{"id": "2510.24749", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24749", "abs": "https://arxiv.org/abs/2510.24749", "authors": ["Aofan Liu", "Shiyuan Song", "Haoxuan Li", "Cehao Yang", "Yiyan Qi"], "title": "Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification", "comment": "Accepted by EMNLP 2025", "summary": "The escalating complexity of modern codebases has intensified the need for\nretrieval systems capable of interpreting cross-component change intents, a\ncapability fundamentally absent in conventional function-level search\nparadigms. While recent studies have improved the alignment between natural\nlanguage queries and code snippets, retrieving contextually relevant code for\nspecific change requests remains largely underexplored. To address this gap, we\nintroduce RepoAlign-Bench, the first benchmark specifically designed to\nevaluate repository-level code retrieval under change request driven scenarios,\nencompassing 52k annotated instances. This benchmark shifts the retrieval\nparadigm from function-centric matching to holistic repository-level reasoning.\nFurthermore, we propose ReflectCode, an adversarial reflection augmented\ndual-tower architecture featuring disentangled code_encoder and doc_encoder\ncomponents. ReflectCode dynamically integrates syntactic patterns, function\ndependencies, and semantic expansion intents through large language model\nguided reflection. Comprehensive experiments demonstrate that ReflectCode\nachieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over\nstate-of-the-art baselines, establishing a new direction for context-aware code\nretrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86RepoAlign-Bench\u57fa\u51c6\u548cReflectCode\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4ee3\u7801\u5e93\u7ea7\u522b\u4ee3\u7801\u68c0\u7d22\u95ee\u9898\uff0c\u5728\u53d8\u66f4\u8bf7\u6c42\u9a71\u52a8\u573a\u666f\u4e0b\u5b9e\u73b012.2%\u7684Top-5\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3\u4ee3\u7801\u5e93\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u80fd\u591f\u7406\u89e3\u8de8\u7ec4\u4ef6\u53d8\u66f4\u610f\u56fe\u7684\u68c0\u7d22\u7cfb\u7edf\uff0c\u800c\u4f20\u7edf\u51fd\u6570\u7ea7\u641c\u7d22\u8303\u5f0f\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u63d0\u51faReflectCode\u65b9\u6cd5\uff0c\u91c7\u7528\u5bf9\u6297\u53cd\u5c04\u589e\u5f3a\u7684\u53cc\u5854\u67b6\u6784\uff0c\u5305\u542b\u89e3\u8026\u7684\u4ee3\u7801\u7f16\u7801\u5668\u548c\u6587\u6863\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u53cd\u5c04\u52a8\u6001\u6574\u5408\u8bed\u6cd5\u6a21\u5f0f\u3001\u51fd\u6570\u4f9d\u8d56\u548c\u8bed\u4e49\u6269\u5c55\u610f\u56fe\u3002", "result": "ReflectCode\u5728Top-5\u51c6\u786e\u7387\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u534712.2%\uff0c\u5728\u53ec\u56de\u7387\u4e0a\u63d0\u53477.1%\u3002", "conclusion": "\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7801\u68c0\u7d22\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u4ece\u51fd\u6570\u4e2d\u5fc3\u5339\u914d\u8f6c\u5411\u6574\u4f53\u4ee3\u7801\u5e93\u7ea7\u63a8\u7406\u3002", "topic": "code agent"}}
{"id": "2510.24884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24884", "abs": "https://arxiv.org/abs/2510.24884", "authors": ["Olawale Salaudeen", "Haoran Zhang", "Kumail Alhamoud", "Sara Beery", "Marzyeh Ghassemi"], "title": "Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations", "comment": "Accepted as a Spotlight paper at NeurIPS 2025", "summary": "Benchmarks for out-of-distribution (OOD) generalization frequently show a\nstrong positive correlation between in-distribution (ID) and OOD accuracy\nacross models, termed \"accuracy-on-the-line.\" This pattern is often taken to\nimply that spurious correlations - correlations that improve ID but reduce OOD\nperformance - are rare in practice. We find that this positive correlation is\noften an artifact of aggregating heterogeneous OOD examples. Using a simple\ngradient-based method, OODSelect, we identify semantically coherent OOD subsets\nwhere accuracy on the line does not hold. Across widely used distribution shift\nbenchmarks, the OODSelect uncovers subsets, sometimes over half of the standard\nOOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings\nindicate that aggregate metrics can obscure important failure modes of OOD\nrobustness. We release code and the identified subsets to facilitate further\nresearch.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "2510.24819", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.24819", "abs": "https://arxiv.org/abs/2510.24819", "authors": ["Vincenzo Scotti", "Jan Keim", "Tobias Hey", "Andreas Metzger", "Anne Koziolek", "Raffaela Mirandola"], "title": "A Roadmap for Tamed Interactions with Large Language Models", "comment": null, "summary": "We are witnessing a bloom of AI-powered software driven by Large Language\nModels (LLMs). Although the applications of these LLMs are impressive and\nseemingly countless, their unreliability hinders adoption. In fact, the\ntendency of LLMs to produce faulty or hallucinated content makes them\nunsuitable for automating workflows and pipelines. In this regard, Software\nEngineering (SE) provides valuable support, offering a wide range of formal\ntools to specify, verify, and validate software behaviour. Such SE tools can be\napplied to define constraints over LLM outputs and, consequently, offer\nstronger guarantees on the generated content. In this paper, we argue that the\ndevelopment of a Domain Specific Language (DSL) for scripting interactions with\nLLMs using an LLM Scripting Language (LSL) may be key to improve AI-based\napplications. Currently, LLMs and LLM-based software still lack reliability,\nrobustness, and trustworthiness, and the tools or frameworks to cope with these\nissues suffer from fragmentation. In this paper, we present our vision of LSL.\nWith LSL, we aim to address the limitations above by exploring ways to control\nLLM outputs, enforce structure in interactions, and integrate these aspects\nwith verification, validation, and explainability. Our goal is to make LLM\ninteraction programmable and decoupled from training or implementation.", "AI": {"tldr": "\u63d0\u51faLLM\u811a\u672c\u8bed\u8a00(LSL)\u7684\u6982\u5ff5\uff0c\u65e8\u5728\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6765\u89c4\u8303\u548c\u63a7\u5236LLM\u4ea4\u4e92\uff0c\u63d0\u9ad8AI\u5e94\u7528\u7684\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524dLLM\u5e94\u7528\u867d\u7136\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u4f46\u5176\u4e0d\u53ef\u9760\u6027\u548c\u5e7b\u89c9\u95ee\u9898\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u6765\u7ea6\u675fLLM\u8f93\u51fa\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u4fdd\u8bc1\u3002", "method": "\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u8bed\u8a00(DSL)\u7528\u4e8e\u811a\u672c\u5316LLM\u4ea4\u4e92\uff0c\u901a\u8fc7LSL\u6765\u63a7\u5236LLM\u8f93\u51fa\u3001\u5f3a\u5236\u4ea4\u4e92\u7ed3\u6784\uff0c\u5e76\u4e0e\u9a8c\u8bc1\u3001\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u96c6\u6210\u3002", "result": "\u63d0\u51fa\u4e86LSL\u7684\u613f\u666f\u6846\u67b6\uff0c\u4f7fLLM\u4ea4\u4e92\u53ef\u7f16\u7a0b\u4e14\u4e0e\u8bad\u7ec3\u6216\u5b9e\u73b0\u89e3\u8026\u3002", "conclusion": "LSL\u53ef\u80fd\u662f\u6539\u8fdb\u57fa\u4e8eAI\u5e94\u7528\u7684\u5173\u952e\uff0c\u80fd\u591f\u89e3\u51b3\u5f53\u524dLLM\u8f6f\u4ef6\u5728\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "topic": "swe application"}}
{"id": "2510.24772", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24772", "abs": "https://arxiv.org/abs/2510.24772", "authors": ["Debdeep Sanyal", "Manya Pandey", "Dhruv Kumar", "Saurabh Deshpande", "Murari Mandal"], "title": "Confidence is Not Competence", "comment": "20 Pages, 6 Figures, 8 Tables", "summary": "Large language models (LLMs) often exhibit a puzzling disconnect between\ntheir asserted confidence and actual problem-solving competence. We offer a\nmechanistic account of this decoupling by analyzing the geometry of internal\nstates across two phases - pre-generative assessment and solution execution. A\nsimple linear probe decodes the internal \"solvability belief\" of a model,\nrevealing a well-ordered belief axis that generalizes across model families and\nacross math, code, planning, and logic tasks. Yet, the geometries diverge -\nalthough belief is linearly decodable, the assessment manifold has high linear\neffective dimensionality as measured from the principal components, while the\nsubsequent reasoning trace evolves on a much lower-dimensional manifold. This\nsharp reduction in geometric complexity from thought to action mechanistically\nexplains the confidence-competence gap. Causal interventions that steer\nrepresentations along the belief axis leave final solutions unchanged,\nindicating that linear nudges in the complex assessment space do not control\nthe constrained dynamics of execution. We thus uncover a two-system\narchitecture - a geometrically complex assessor feeding a geometrically simple\nexecutor. These results challenge the assumption that decodable beliefs are\nactionable levers, instead arguing for interventions that target the procedural\ndynamics of execution rather than the high-level geometry of assessment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u72b6\u6001\u5728\u9884\u751f\u6210\u8bc4\u4f30\u548c\u89e3\u51b3\u65b9\u6848\u6267\u884c\u4e24\u4e2a\u9636\u6bb5\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u7f6e\u4fe1\u5ea6\u4e0e\u80fd\u529b\u8131\u8282\u7684\u673a\u5236\u539f\u56e0\u3002\u7814\u7a76\u53d1\u73b0\u8bc4\u4f30\u9636\u6bb5\u5177\u6709\u9ad8\u7ef4\u51e0\u4f55\u590d\u6742\u6027\uff0c\u800c\u6267\u884c\u9636\u6bb5\u5728\u4f4e\u7ef4\u6d41\u5f62\u4e0a\u6f14\u5316\uff0c\u8fd9\u79cd\u51e0\u4f55\u590d\u6742\u6027\u7684\u6025\u5267\u51cf\u5c11\u89e3\u91ca\u4e86\u7f6e\u4fe1\u5ea6-\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u7684\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e4b\u95f4\u7684\u8131\u8282\u73b0\u8c61\uff0c\u4ece\u673a\u5236\u5c42\u9762\u7406\u89e3\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u89e3\u7801\u6a21\u578b\u7684\u5185\u90e8\"\u53ef\u89e3\u6027\u4fe1\u5ff5\"\uff0c\u5206\u6790\u8bc4\u4f30\u9636\u6bb5\u548c\u6267\u884c\u9636\u6bb5\u5185\u90e8\u72b6\u6001\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5305\u62ec\u7ebf\u6027\u6709\u6548\u7ef4\u5ea6\u548c\u6d41\u5f62\u7ed3\u6784\uff0c\u5e76\u8fdb\u884c\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u4fe1\u5ff5\u53ef\u4ee5\u7ebf\u6027\u89e3\u7801\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u4efb\u52a1\u7c7b\u578b\u4e2d\u5177\u6709\u826f\u597d\u6392\u5e8f\u7684\u4fe1\u5ff5\u8f74\uff0c\u4f46\u8bc4\u4f30\u6d41\u5f62\u5177\u6709\u9ad8\u7ebf\u6027\u6709\u6548\u7ef4\u5ea6\uff0c\u800c\u63a8\u7406\u8f68\u8ff9\u5728\u4f4e\u7ef4\u6d41\u5f62\u4e0a\u6f14\u5316\u3002\u56e0\u679c\u5e72\u9884\u663e\u793a\u6cbf\u4fe1\u5ff5\u8f74\u7684\u7ebf\u6027\u6270\u52a8\u4e0d\u4f1a\u6539\u53d8\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u63ed\u793a\u4e86LLM\u5177\u6709\u4e24\u7cfb\u7edf\u67b6\u6784\uff1a\u51e0\u4f55\u590d\u6742\u7684\u8bc4\u4f30\u5668\u5411\u51e0\u4f55\u7b80\u5355\u7684\u6267\u884c\u5668\u63d0\u4f9b\u8f93\u5165\u3002\u53ef\u89e3\u7801\u7684\u4fe1\u5ff5\u5e76\u975e\u53ef\u64cd\u4f5c\u7684\u63a7\u5236\u6746\uff0c\u5e72\u9884\u5e94\u9488\u5bf9\u6267\u884c\u8fc7\u7a0b\u7684\u7a0b\u5e8f\u6027\u52a8\u6001\u800c\u975e\u8bc4\u4f30\u7684\u9ad8\u5c42\u51e0\u4f55\u7ed3\u6784\u3002", "topic": "agent analysis"}}
{"id": "2510.25014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25014", "abs": "https://arxiv.org/abs/2510.25014", "authors": ["Minkyung Kim", "Junsik Kim", "Woongcheol Yang", "Sangdon Park", "Sohee Bae"], "title": "Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading", "comment": "8 pages main content, 18 pages supplementary material, 4 figures", "summary": "Large Language Models (LLMs) enable dynamic game interactions but fail to\nfollow essential procedural flows in rule-governed trading systems, eroding\nplayer trust. This work resolves the core tension between the creative\nflexibility of LLMs and the procedural demands of in-game trading\n(browse-offer-review-confirm). To this end, Autoregressive State-Tracking\nPrompting (ASTP) is introduced, a methodology centered on a strategically\norchestrated prompt that compels an LLM to make its state-tracking process\nexplicit and verifiable. Instead of relying on implicit contextual\nunderstanding, ASTP tasks the LLM with identifying and reporting a predefined\nstate label from the previous turn. To ensure transactional integrity, this is\ncomplemented by a state-specific placeholder post-processing method for\naccurate price calculations. Evaluation across 300 trading dialogues\ndemonstrates >99% state compliance and 99.3% calculation precision. Notably,\nASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)\nmatches larger models' (Gemini-2.5-Pro) performance while reducing response\ntime from 21.2s to 2.4s, establishing a practical foundation that satisfies\nboth real-time requirements and resource constraints of commercial games.", "AI": {"tldr": "\u63d0\u51faAutoregressive State-Tracking Prompting (ASTP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u72b6\u6001\u8ddf\u8e2a\u548c\u5360\u4f4d\u7b26\u540e\u5904\u7406\uff0c\u89e3\u51b3LLM\u5728\u6e38\u620f\u4ea4\u6613\u7cfb\u7edf\u4e2d\u9075\u5faa\u7a0b\u5e8f\u6d41\u7a0b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0>99%\u72b6\u6001\u5408\u89c4\u6027\u548c99.3%\u8ba1\u7b97\u7cbe\u5ea6\u3002", "motivation": "LLM\u5728\u52a8\u6001\u6e38\u620f\u4ea4\u4e92\u4e2d\u7f3a\u4e4f\u5bf9\u89c4\u5219\u6cbb\u7406\u4ea4\u6613\u7cfb\u7edf\u57fa\u672c\u7a0b\u5e8f\u6d41\u7a0b\u7684\u9075\u5faa\u80fd\u529b\uff0c\u8fd9\u4f1a\u524a\u5f31\u73a9\u5bb6\u4fe1\u4efb\u3002\u9700\u8981\u89e3\u51b3LLM\u521b\u610f\u7075\u6d3b\u6027\u4e0e\u6e38\u620f\u4ea4\u6613\u7a0b\u5e8f\u9700\u6c42\u4e4b\u95f4\u7684\u6838\u5fc3\u77db\u76fe\u3002", "method": "\u5f15\u5165ASTP\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u5f3a\u5236LLM\u663e\u5f0f\u5316\u5176\u72b6\u6001\u8ddf\u8e2a\u8fc7\u7a0b\uff0c\u8ba9LLM\u8bc6\u522b\u5e76\u62a5\u544a\u9884\u5b9a\u4e49\u7684\u72b6\u6001\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u72b6\u6001\u7279\u5b9a\u7684\u5360\u4f4d\u7b26\u540e\u5904\u7406\u65b9\u6cd5\u786e\u4fdd\u4ea4\u6613\u5b8c\u6574\u6027\u3002", "result": "\u5728300\u4e2a\u4ea4\u6613\u5bf9\u8bdd\u8bc4\u4f30\u4e2d\uff0c\u5b9e\u73b0>99%\u72b6\u6001\u5408\u89c4\u6027\u548c99.3%\u8ba1\u7b97\u7cbe\u5ea6\u3002\u5c0f\u6a21\u578b(Gemini-2.5-Flash)\u4f7f\u7528ASTP\u548c\u5360\u4f4d\u7b26\u540e\u5904\u7406\u53ef\u5339\u914d\u5927\u6a21\u578b(Gemini-2.5-Pro)\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u54cd\u5e94\u65f6\u95f4\u4ece21.2\u79d2\u964d\u81f32.4\u79d2\u3002", "conclusion": "ASTP\u5efa\u7acb\u4e86\u6ee1\u8db3\u5546\u4e1a\u6e38\u620f\u5b9e\u65f6\u9700\u6c42\u548c\u8d44\u6e90\u9650\u5236\u7684\u5b9e\u7528\u57fa\u7840\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.25065", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25065", "abs": "https://arxiv.org/abs/2510.25065", "authors": ["Taekhyun Park", "Yongjae Lee", "Hyerim Bae"], "title": "Reasoning-Aware GRPO using Process Mining", "comment": null, "summary": "Reinforcement learning (RL)-based post-training has been crucial for enabling\nmulti-step reasoning in large reasoning models (LRMs), yet current reward\nschemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware\nGroup Relative Policy Optimization (GRPO) that augments standard answer/format\nrewards with signals over the reasoning procedure. To this end, process mining\ntechniques are utilized to compute a scalar conformance reward that measures\nhow closely a policy model's reasoning aligns with the pretrained teacher\nmodel. The empirical results on five benchmarks demonstrate that PM4GRPO\nsignificantly outperforms existing methodologies for GRPO-based post-training.\nThese results highlight that leveraging process mining for reasoning-aware GRPO\neffectively enhances the reasoning capabilities of policy models.", "AI": {"tldr": "\u63d0\u51faPM4GRPO\u65b9\u6cd5\uff0c\u5728GRPO\u540e\u8bad\u7ec3\u4e2d\u5f15\u5165\u8fc7\u7a0b\u6316\u6398\u6280\u672f\uff0c\u4e3a\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u5956\u52b1\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7ed3\u679c\u5956\u52b1\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u76d1\u7763\uff0c\u9650\u5236\u4e86\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u63d0\u5347", "method": "\u4f7f\u7528\u8fc7\u7a0b\u6316\u6398\u6280\u672f\u8ba1\u7b97\u63a8\u7406\u8fc7\u7a0b\u4e0e\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u7684\u5bf9\u9f50\u7a0b\u5ea6\u4f5c\u4e3a\u4e00\u81f4\u6027\u5956\u52b1\uff0c\u7ed3\u5408\u6807\u51c6\u7b54\u6848/\u683c\u5f0f\u5956\u52b1\u8fdb\u884cGRPO\u4f18\u5316", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684GRPO\u540e\u8bad\u7ec3\u65b9\u6cd5", "conclusion": "\u5229\u7528\u8fc7\u7a0b\u6316\u6398\u5b9e\u73b0\u63a8\u7406\u611f\u77e5\u7684GRPO\u80fd\u6709\u6548\u589e\u5f3a\u7b56\u7565\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2510.25039", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25039", "abs": "https://arxiv.org/abs/2510.25039", "authors": ["Amanda Dsouza", "Harit Vishwakarma", "Zhengyang Qi", "Justin Bauer", "Derek Pham", "Thomas Walshe", "Armin Parchami", "Frederic Sala", "Paroma Varma"], "title": "Automating Benchmark Design", "comment": null, "summary": "The rapid progress and widespread deployment of LLMs and LLM-powered agents\nhas outpaced our ability to evaluate them. Hand-crafted, static benchmarks are\nthe primary tool for assessing model capabilities, but these quickly become\nsaturated. In contrast, dynamic benchmarks evolve alongside the models they\nevaluate, but are expensive to create and continuously update. To address these\nchallenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a\nframework that leverages environment design principles to automate the process\nof dynamic benchmark design. BeTaL works by parameterizing key design choices\nin base benchmark templates and uses LLMs to reason through the resulting\nparameter space to obtain target properties (such as difficulty and realism) in\na cost-efficient manner. We validate this approach on its ability to create\nbenchmarks with desired difficulty levels. Using BeTaL, we create two new\nbenchmarks and extend a popular agentic benchmark $\\tau$-bench. Extensive\nevaluation on these three tasks and multiple target difficulty levels shows\nthat BeTaL produces benchmarks much closer to the desired difficulty, with\naverage deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the\nbaselines.", "AI": {"tldr": "BeTaL\u662f\u4e00\u4e2a\u5229\u7528LLM\u81ea\u52a8\u8bbe\u8ba1\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u57fa\u51c6\u6a21\u677f\u548cLLM\u63a8\u7406\u6765\u83b7\u5f97\u76ee\u6807\u5c5e\u6027\uff08\u5982\u96be\u5ea6\u548c\u771f\u5b9e\u6027\uff09\uff0c\u76f8\u6bd4\u9759\u6001\u57fa\u51c6\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5f53\u524dLLM\u548cLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bc4\u4f30\u80fd\u529b\u8ddf\u4e0d\u4e0a\u3002\u624b\u5de5\u5236\u4f5c\u7684\u9759\u6001\u57fa\u51c6\u5bb9\u6613\u9971\u548c\uff0c\u800c\u52a8\u6001\u57fa\u51c6\u521b\u5efa\u548c\u66f4\u65b0\u6210\u672c\u9ad8\u6602\u3002", "method": "BeTaL\u6846\u67b6\u5229\u7528\u73af\u5883\u8bbe\u8ba1\u539f\u5219\uff0c\u53c2\u6570\u5316\u57fa\u51c6\u6a21\u677f\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u4f7f\u7528LLM\u63a8\u7406\u53c2\u6570\u7a7a\u95f4\u4ee5\u6210\u672c\u9ad8\u6548\u7684\u65b9\u5f0f\u83b7\u5f97\u76ee\u6807\u5c5e\u6027\u3002", "result": "BeTaL\u521b\u5efa\u7684\u57fa\u51c6\u66f4\u63a5\u8fd1\u76ee\u6807\u96be\u5ea6\uff0c\u5e73\u5747\u504f\u5dee\u57285.3%\u523013.2%\u4e4b\u95f4\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e862-4\u500d\u3002\u521b\u5efa\u4e86\u4e24\u4e2a\u65b0\u57fa\u51c6\u5e76\u6269\u5c55\u4e86\u03c4-bench\u3002", "conclusion": "BeTaL\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u52a8\u6001\u57fa\u51c6\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u663e\u8457\u6539\u5584\u57fa\u51c6\u6d4b\u8bd5\u7684\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2510.24941", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24941", "abs": "https://arxiv.org/abs/2510.24941", "authors": ["Jiachen Zhao", "Yiyou Sun", "Weiyan Shi", "Dawn Song"], "title": "Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought", "comment": null, "summary": "Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)\nat test time, enabling them to solve complex tasks. These reasoning steps in\nCoT are often assumed as a faithful reflection of the model's internal thinking\nprocess, and used to monitor unsafe intentions. However, we find many reasoning\nsteps don't truly contribute to LLMs' prediction. We measure the step-wise\ncausal influence of each reasoning step on the model's final prediction with a\nproposed True Thinking Score (TTS). We reveal that LLMs often interleave\nbetween true-thinking steps (which are genuinely used to produce the final\noutput) and decorative-thinking steps (which only give the appearance of\nreasoning but have minimal causal impact). Notably, only a small subset of the\ntotal reasoning steps have a high TTS that causally drive the model's\nprediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning\nsteps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.\nFurthermore, we identify a TrueThinking direction in the latent space of LLMs.\nBy steering along or against this direction, we can force the model to perform\nor disregard certain CoT steps when computing the final result. Finally, we\nhighlight that self-verification steps in CoT (i.e., aha moments) can also be\ndecorative, where LLMs do not truly verify their solution. Steering along the\nTrueThinking direction can force internal reasoning over these steps, resulting\nin a change in the final results. Overall, our work reveals that LLMs often\nverbalize reasoning steps without actually performing them internally, which\nundermines both the efficiency of LLM reasoning and the trustworthiness of CoT.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u601d\u7ef4\u94fe\u4e2d\u8bb8\u591a\u63a8\u7406\u6b65\u9aa4\u5e76\u4e0d\u771f\u6b63\u5f71\u54cd\u6700\u7ec8\u9884\u6d4b\uff0c\u63d0\u51fa\u4e86\u771f\u5b9e\u601d\u7ef4\u8bc4\u5206\u6765\u91cf\u5316\u6b65\u9aa4\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u601d\u8003\u548c\u88c5\u9970\u6027\u601d\u8003\u4e4b\u95f4\u7684\u4ea4\u66ff\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u5047\u8bbe\u601d\u7ef4\u94fe\u6b65\u9aa4\u5fe0\u5b9e\u53cd\u6620\u6a21\u578b\u5185\u90e8\u601d\u8003\u8fc7\u7a0b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u6b65\u9aa4\u53ea\u662f\u88c5\u9970\u6027\u7684\uff0c\u5e76\u4e0d\u771f\u6b63\u8d21\u732e\u4e8e\u9884\u6d4b\uff0c\u8fd9\u5f71\u54cd\u4e86LLM\u63a8\u7406\u6548\u7387\u548c\u601d\u7ef4\u94fe\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u771f\u5b9e\u601d\u7ef4\u8bc4\u5206(TTS)\u6765\u6d4b\u91cf\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u5bf9\u6700\u7ec8\u9884\u6d4b\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u5e76\u8bc6\u522b\u4e86LLM\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u771f\u5b9e\u601d\u7ef4\u65b9\u5411\uff0c\u901a\u8fc7\u6cbf\u8be5\u65b9\u5411\u5f15\u5bfc\u53ef\u4ee5\u63a7\u5236\u6a21\u578b\u6267\u884c\u6216\u5ffd\u7565\u7279\u5b9a\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728AIME\u6570\u636e\u96c6\u4e0a\uff0c\u53ea\u6709\u5e73\u57472.3%\u7684\u601d\u7ef4\u94fe\u6b65\u9aa4\u5177\u6709\u9ad8TTS(\u22650.7)\uff0c\u81ea\u6211\u9a8c\u8bc1\u6b65\u9aa4\u4e5f\u53ef\u80fd\u662f\u88c5\u9970\u6027\u7684\uff0c\u6cbf\u771f\u5b9e\u601d\u7ef4\u65b9\u5411\u5f15\u5bfc\u53ef\u4ee5\u6539\u53d8\u6700\u7ec8\u7ed3\u679c\u3002", "conclusion": "LLM\u7ecf\u5e38\u53e3\u5934\u8868\u8fbe\u63a8\u7406\u6b65\u9aa4\u4f46\u5185\u90e8\u5e76\u4e0d\u771f\u6b63\u6267\u884c\uff0c\u8fd9\u524a\u5f31\u4e86LLM\u63a8\u7406\u6548\u7387\u548c\u601d\u7ef4\u94fe\u7684\u53ef\u4fe1\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2510.25101", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25101", "abs": "https://arxiv.org/abs/2510.25101", "authors": ["Zhuo Chen", "Fei Wang", "Zixuan Li", "Zhao Zhang", "Weiwei Ding", "Chuanguang Yang", "Yongjun Xu", "Xiaolong Jin", "Jiafeng Guo"], "title": "KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA", "comment": null, "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural-language\nquestions over a structured Knowledge Base (KB). Recent work improves KBQA by\nadopting an agentic reasoning paradigm, in which Large Language Models (LLMs)\niteratively decompose a question, generate its corresponding logical queries,\nand interact with the KB to derive the answer. However, these methods typically\nfine-tune LLMs on reasoning trajectories synthesized via process supervision,\nwhich offers weak incentives for exploration and thus fails to strengthen the\nagentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that\ncan autonomously perform agentic reasoning on KBs to obtain answers. To\nincentivize autonomous exploration, KnowCoder-A1 trains the LLM under\noutcome-only supervision via a multi-stage curriculum reinforcement learning\nwith an easy-to-hard curriculum. To establish foundational agentic\ncapabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of\nhigh-quality trajectories obtained through outcome-based rejection sampling.\nThen, to alleviate the reward sparsity inherent in outcome-only supervision, it\napplies multi-stage curriculum RL with reward schedules that progress from easy\nto hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful\nreasoning behaviors and consistently outperforms prior approaches across three\nmainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1\nachieves up to an 11.1% relative improvement while using only one-twelfth of\nthe training data, demonstrating strong agentic reasoning capabilities.", "AI": {"tldr": "KnowCoder-A1\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u7684KBQA\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u679c\u76d1\u7763\u8bad\u7ec3LLM\u5728\u77e5\u8bc6\u5e93\u4e0a\u8fdb\u884c\u81ea\u4e3b\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KBQA\u65b9\u6cd5\u901a\u8fc7\u8fc7\u7a0b\u76d1\u7763\u5fae\u8c03LLM\uff0c\u4f46\u7f3a\u4e4f\u63a2\u7d22\u6fc0\u52b1\uff0c\u65e0\u6cd5\u6709\u6548\u589e\u5f3a\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u9996\u5148\u901a\u8fc7\u57fa\u4e8e\u7ed3\u679c\u7684\u62d2\u7edd\u91c7\u6837\u83b7\u5f97\u9ad8\u8d28\u91cf\u8f68\u8ff9\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u4f7f\u7528\u4ece\u6613\u5230\u96be\u7684\u5956\u52b1\u8c03\u5ea6\u8fdb\u884cRL\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5728GrailQA\u96f6\u6837\u672c\u5b50\u96c6\u4e0a\u76f8\u5bf9\u63d0\u534711.1%\uff0c\u4ec5\u4f7f\u7528\u5341\u4e8c\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u57fa\u4e8e\u7ed3\u679c\u76d1\u7763\u7684\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u8bad\u7ec3LLM\u8fdb\u884c\u81ea\u4e3b\u4ee3\u7406\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347KBQA\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25179", "abs": "https://arxiv.org/abs/2510.25179", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models", "comment": null, "summary": "Agentic methods have emerged as a powerful and autonomous paradigm that\nenhances reasoning, collaboration, and adaptive control, enabling systems to\ncoordinate and independently solve complex tasks. We extend this paradigm to\nsafety alignment by introducing Agentic Moderation, a model-agnostic framework\nthat leverages specialised agents to defend multimodal systems against\njailbreak attacks. Unlike prior approaches that apply as a static layer over\ninputs or outputs and provide only binary classifications (safe or unsafe), our\nmethod integrates dynamic, cooperative agents, including Shield, Responder,\nEvaluator, and Reflector, to achieve context-aware and interpretable\nmoderation. Extensive experiments across five datasets and four representative\nLarge Vision-Language Models (LVLMs) demonstrate that our approach reduces the\nAttack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),\nand improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,\nand well-balanced safety performance. By harnessing the flexibility and\nreasoning capacity of agentic architectures, Agentic Moderation provides\nmodular, scalable, and fine-grained safety enforcement, highlighting the\nbroader potential of agentic systems as a foundation for automated safety\ngovernance.", "AI": {"tldr": "\u63d0\u51faAgentic Moderation\u6846\u67b6\uff0c\u5229\u7528\u4e13\u95e8\u4ee3\u7406\u6765\u9632\u5fa1\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u4f5c\u4ee3\u7406\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u7684\u5ba1\u6838", "motivation": "\u5c06\u667a\u80fd\u4ee3\u7406\u65b9\u6cd5\u6269\u5c55\u5230\u5b89\u5168\u5bf9\u9f50\u9886\u57df\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f5c\u4e3a\u9759\u6001\u5c42\u4ec5\u63d0\u4f9b\u4e8c\u5143\u5206\u7c7b\u7684\u5c40\u9650\u6027", "method": "\u96c6\u6210Shield\u3001Responder\u3001Evaluator\u548cReflector\u56db\u4e2a\u52a8\u6001\u534f\u4f5c\u4ee3\u7406\uff0c\u5b9e\u73b0\u6a21\u578b\u65e0\u5173\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u5b89\u5168\u9632\u62a4", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u548c\u56db\u4e2a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e7-19%\uff0c\u62d2\u7edd\u7387\u63d0\u9ad84-20%\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u4e0d\u9075\u5faa\u7387", "conclusion": "Agentic Moderation\u901a\u8fc7\u5229\u7528\u4ee3\u7406\u67b6\u6784\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u5316\u5b89\u5168\u6cbb\u7406\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u548c\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u6267\u884c\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2510.25148", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25148", "abs": "https://arxiv.org/abs/2510.25148", "authors": ["Katsuki Yamagishi", "Norihiro Yoshida", "Erina Makihara", "Katsuro Inoue"], "title": "Automated Program Repair Based on REST API Specifications Using Large Language Models", "comment": null, "summary": "Many cloud services provide REST API accessible to client applications.\nHowever, developers often identify specification violations only during\ntesting, as error messages typically lack the detail necessary for effective\ndiagnosis. Consequently, debugging requires trial and error. This study\nproposes dcFix, a method for detecting and automatically repairing REST API\nmisuses in client programs. In particular, dcFix identifies non-conforming code\nfragments, integrates them with the relevant API specifications into prompts,\nand leverages a Large Language Model (LLM) to produce the corrected code. Our\nevaluation demonstrates that dcFix accurately detects misuse and outperforms\nthe baseline approach, in which prompts to the LLM omit any indication of code\nfragments non conforming to REST API specifications.", "AI": {"tldr": "\u63d0\u51fadcFix\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u81ea\u52a8\u4fee\u590d\u5ba2\u6237\u7aef\u7a0b\u5e8f\u4e2d\u7684REST API\u8bef\u7528\u95ee\u9898\uff0c\u901a\u8fc7\u8bc6\u522b\u4e0d\u7b26\u5408\u89c4\u8303\u7684\u4ee3\u7801\u7247\u6bb5\u5e76\u7ed3\u5408API\u89c4\u8303\u751f\u6210\u63d0\u793a\uff0c\u5229\u7528LLM\u751f\u6210\u4fee\u6b63\u4ee3\u7801\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u6d4b\u8bd5\u9636\u6bb5\u624d\u80fd\u53d1\u73b0REST API\u89c4\u8303\u8fdd\u53cd\u95ee\u9898\uff0c\u9519\u8bef\u4fe1\u606f\u7f3a\u4e4f\u6709\u6548\u8bca\u65ad\u7ec6\u8282\uff0c\u8c03\u8bd5\u8fc7\u7a0b\u9700\u8981\u53cd\u590d\u8bd5\u9519\u3002", "method": "\u8bc6\u522b\u4e0d\u7b26\u5408\u89c4\u8303\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u5c06\u5176\u4e0e\u76f8\u5173API\u89c4\u8303\u6574\u5408\u5230\u63d0\u793a\u4e2d\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4fee\u6b63\u4ee3\u7801\u3002", "result": "\u8bc4\u4f30\u663e\u793adcFix\u80fd\u51c6\u786e\u68c0\u6d4b\u8bef\u7528\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u57fa\u7ebf\u65b9\u6cd5\u5728\u63d0\u793a\u4e2d\u4e0d\u5305\u542b\u4ee3\u7801\u7247\u6bb5\u4e0d\u7b26\u5408REST API\u89c4\u8303\u7684\u6307\u793a\uff09\u3002", "conclusion": "dcFix\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u4fee\u590dREST API\u8bef\u7528\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u4ee3\u7801\u7247\u6bb5\u548cAPI\u89c4\u8303\u4fe1\u606f\uff0c\u5229\u7528LLM\u751f\u6210\u66f4\u51c6\u786e\u7684\u4fee\u6b63\u4ee3\u7801\u3002", "topic": "swe application"}}
{"id": "2510.25206", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.25206", "abs": "https://arxiv.org/abs/2510.25206", "authors": ["Tianqianjin Lin", "Xi Zhao", "Xingyao Zhang", "Rujiao Long", "Yi Xu", "Zhuoren Jiang", "Wenbo Su", "Bo Zheng"], "title": "RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models", "comment": "17 pages, 11 figures", "summary": "Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage models (LLMs), but critically depends on a key prerequisite: the LLM\ncan already generate high-utility reasoning paths with non-negligible\nprobability. For tasks beyond the LLM's current competence, such reasoning path\ncan be hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning. We are motivated by the insight from cognitive science that Why is\nthis the answer is often an easier question than What is the answer, as it\navoids the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory reconstruction-systematically retracing the reasoning that links a\nquestion to its answer. We show that LLMs can similarly leverage answers to\nderive high-quality reasoning paths. We formalize this phenomenon and prove\nthat conditioning on answer provably increases the expected utility of sampled\nreasoning paths, thereby transforming intractable problems into learnable ones.\nBuilding on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning), an end-to-end framework that uses answer-conditioned\nreasoning as a variational surrogate for question-only reasoning. Experiments\nin both general and math domains demonstrate consistent improvements over\nstrong baselines. We further analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.", "AI": {"tldr": "\u63d0\u51faRAVR\u6846\u67b6\uff0c\u5229\u7528\u7b54\u6848\u5f15\u5bfc\u7684\u53d8\u5206\u63a8\u7406\u6765\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5728LLM\u80fd\u529b\u4e0d\u8db3\u65f6\u96be\u4ee5\u91c7\u6837\u9ad8\u8d28\u91cf\u63a8\u7406\u8def\u5f84\u7684\u95ee\u9898\u3002", "motivation": "\u5f53LLM\u5bf9\u4efb\u52a1\u80fd\u529b\u4e0d\u8db3\u65f6\uff0c\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u8def\u5f84\uff0c\u5f3a\u5316\u5b66\u4e60\u5bb9\u6613\u5f3a\u5316\u719f\u6089\u4f46\u6b21\u4f18\u7684\u63a8\u7406\u3002\u53d7\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\uff0c\"\u4e3a\u4ec0\u4e48\u662f\u8fd9\u4e2a\u7b54\u6848\"\u6bd4\"\u7b54\u6848\u662f\u4ec0\u4e48\"\u66f4\u5bb9\u6613\u56de\u7b54\uff0c\u56e0\u4e3a\u907f\u514d\u4e86\u5f00\u653e\u5f0f\u63a2\u7d22\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u5f15\u5165RAVR\u6846\u67b6\uff0c\u4f7f\u7528\u7b54\u6848\u6761\u4ef6\u63a8\u7406\u4f5c\u4e3a\u4ec5\u95ee\u9898\u63a8\u7406\u7684\u53d8\u5206\u4ee3\u7406\uff0c\u901a\u8fc7\u7b54\u6848\u5f15\u5bfc\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u901a\u7528\u548c\u6570\u5b66\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cRAVR\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u51fa\u6301\u7eed\u6539\u8fdb\uff0c\u51cf\u5c11\u4e86\u72b9\u8c6b\uff0c\u52a0\u5f3a\u4e86\u7ed3\u8bba\u6574\u5408\uff0c\u5e76\u4fc3\u8fdb\u4e86\u95ee\u9898\u7279\u5b9a\u7b56\u7565\u7684\u4f7f\u7528\u3002", "conclusion": "\u7b54\u6848\u5f15\u5bfc\u7684\u63a8\u7406\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u91c7\u6837\u63a8\u7406\u8def\u5f84\u7684\u671f\u671b\u6548\u7528\uff0c\u5c06\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5b66\u4e60\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25223", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25223", "abs": "https://arxiv.org/abs/2510.25223", "authors": ["Kun ouyang", "Haoyu Wang", "Dong Fang"], "title": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data", "comment": "14 pages, 11 figures", "summary": "Event log data, recording fine-grained user actions and system events,\nrepresent one of the most valuable assets for modern digital services. However,\nthe complexity and heterogeneity of industrial event logs--characterized by\nlarge scale, high dimensionality, diverse data types, and intricate temporal or\nrelational structures--make feature engineering extremely challenging. Existing\nautomatic feature engineering approaches, such as AutoML or genetic methods,\noften suffer from limited explainability, rigid predefined operations, and poor\nadaptability to complicated heterogeneous data. In this paper, we propose FELA\n(Feature Engineering LLM Agents), a multi-agent evolutionary system that\nautonomously extracts meaningful and high-performing features from complex\nindustrial event log data. FELA integrates the reasoning and coding\ncapabilities of large language models (LLMs) with an insight-guided\nself-evolution paradigm. Specifically, FELA employs specialized agents--Idea\nAgents, Code Agents, and Critic Agents--to collaboratively generate, validate,\nand implement novel feature ideas. An Evaluation Agent summarizes feedback and\nupdates a hierarchical knowledge base and dual-memory system to enable\ncontinual improvement. Moreover, FELA introduces an agentic evolution\nalgorithm, combining reinforcement learning and genetic algorithm principles to\nbalance exploration and exploitation across the idea space. Extensive\nexperiments on real industrial datasets demonstrate that FELA can generate\nexplainable, domain-relevant features that significantly improve model\nperformance while reducing manual effort. Our results highlight the potential\nof LLM-based multi-agent systems as a general framework for automated,\ninterpretable, and adaptive feature engineering in complex real-world\nenvironments.", "AI": {"tldr": "FELA\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u8fdb\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u590d\u6742\u7684\u5de5\u4e1a\u4e8b\u4ef6\u65e5\u5fd7\u6570\u636e\u4e2d\u81ea\u52a8\u63d0\u53d6\u9ad8\u6027\u80fd\u7279\u5f81\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u9057\u4f20\u7b97\u6cd5\u539f\u7406\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "\u5de5\u4e1a\u4e8b\u4ef6\u65e5\u5fd7\u6570\u636e\u590d\u6742\u5f02\u6784\uff0c\u73b0\u6709\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u64cd\u4f5c\u50f5\u5316\u3001\u5bf9\u590d\u6742\u5f02\u6784\u6570\u636e\u9002\u5e94\u6027\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u60f3\u6cd5\u667a\u80fd\u4f53\u3001\u4ee3\u7801\u667a\u80fd\u4f53\u3001\u6279\u8bc4\u667a\u80fd\u4f53\u3001\u8bc4\u4f30\u667a\u80fd\u4f53\uff09\uff0c\u7ed3\u5408LLM\u7684\u63a8\u7406\u548c\u7f16\u7801\u80fd\u529b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u9057\u4f20\u7b97\u6cd5\u7684\u667a\u80fd\u4f53\u8fdb\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFELA\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u3001\u9886\u57df\u76f8\u5173\u7684\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u6f5c\u529b\u6210\u4e3a\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u81ea\u52a8\u5316\u3001\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7279\u5f81\u5de5\u7a0b\u7684\u901a\u7528\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.24983", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24983", "abs": "https://arxiv.org/abs/2510.24983", "authors": ["Ximan Sun", "Xiang Cheng"], "title": "LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies", "comment": null, "summary": "Diffusion policies are competitive for offline reinforcement learning (RL)\nbut are typically guided at sampling time by heuristics that lack a statistical\nnotion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that\ntreats each denoising step as a sequential hypothesis test between the\nunconditional prior and the state-conditional policy head. Concretely, we\naccumulate a log-likelihood ratio and gate the conditional mean with a logistic\ncontroller whose threshold tau is calibrated once under H0 to meet a\nuser-specified Type-I level alpha. This turns guidance from a fixed push into\nan evidence-driven adjustment with a user-interpretable risk budget.\nImportantly, we deliberately leave training vanilla (two heads with standard\nepsilon-prediction) under the structure of DDPM. LRT guidance composes\nnaturally with Q-gradients: critic-gradient updates can be taken at the\nunconditional mean, at the LRT-gated mean, or a blend, exposing a continuum\nfrom exploitation to conservatism. We standardize states and actions\nconsistently at train and test time and report a state-conditional\nout-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,\nLRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines\nin our implementation while honoring the desired alpha. Theoretically, we\nestablish level-alpha calibration, concise stability bounds, and a return\ncomparison showing when LRT surpasses Q-guidance-especially when off-support\nerrors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method\nthat adds principled, calibrated risk control to diffusion policies for offline\nRL.", "AI": {"tldr": "LRT-Diffusion\u662f\u4e00\u79cd\u98ce\u9669\u611f\u77e5\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u53bb\u566a\u6b65\u9aa4\u89c6\u4e3a\u987a\u5e8f\u5047\u8bbe\u68c0\u9a8c\uff0c\u4e3a\u6269\u6563\u7b56\u7565\u63d0\u4f9b\u7edf\u8ba1\u610f\u4e49\u4e0a\u7684\u98ce\u9669\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u7edf\u8ba1\u98ce\u9669\u6982\u5ff5\uff0c\u901a\u5e38\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u91c7\u6837\u6307\u5bfc\u3002", "method": "\u5728DDPM\u7ed3\u6784\u4e0b\u4fdd\u6301\u8bad\u7ec3\u4e0d\u53d8\uff0c\u5728\u63a8\u7406\u65f6\u5f15\u5165LRT\u5f15\u5bfc\uff0c\u901a\u8fc7\u7d2f\u79ef\u5bf9\u6570\u4f3c\u7136\u6bd4\u5e76\u4f7f\u7528\u903b\u8f91\u63a7\u5236\u5668\u95e8\u63a7\u6761\u4ef6\u5747\u503c\uff0c\u5b9e\u73b0\u98ce\u9669\u6821\u51c6\u3002", "result": "\u5728D4RL MuJoCo\u4efb\u52a1\u4e2d\uff0cLRT-Diffusion\u76f8\u6bd4\u5f3aQ\u5f15\u5bfc\u57fa\u7ebf\u6539\u5584\u4e86\u56de\u62a5\u4e0eOOD\u6743\u8861\uff0c\u540c\u65f6\u6ee1\u8db3\u6307\u5b9a\u7684alpha\u6c34\u5e73\u3002", "conclusion": "LRT-Diffusion\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u4e3a\u79bb\u7ebfRL\u7684\u6269\u6563\u7b56\u7565\u6dfb\u52a0\u4e86\u539f\u5219\u6027\u7684\u3001\u6821\u51c6\u7684\u98ce\u9669\u63a7\u5236\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25297", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25297", "abs": "https://arxiv.org/abs/2510.25297", "authors": ["Hidetake Tanaka", "Haruto Tanaka", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases", "comment": "Accepted for publication in 2nd IEEE/ACM international conference on\n  AI-powered Software (AIware 2025) : 8 pages, 1 table, 8 figures", "summary": "As Large Language Models (LLMs) increasingly generate code in software\ndevelopment, ensuring the quality of LLM-generated code has become important.\nTraditional testing approaches using Example-based Testing (EBT) often miss\nedge cases -- defects that occur at boundary values, special input patterns, or\nextreme conditions. This research investigates the characteristics of\nLLM-generated Property-based Testing (PBT) compared to EBT for exploring edge\ncases. We analyze 16 HumanEval problems where standard solutions failed on\nextended test cases, generating both PBT and EBT test codes using\nClaude-4-sonnet. Our experimental results reveal that while each method\nindividually achieved a 68.75\\% bug detection rate, combining both approaches\nimproved detection to 81.25\\%. The analysis demonstrates complementary\ncharacteristics: PBT effectively detects performance issues and edge cases\nthrough extensive input space exploration, while EBT effectively detects\nspecific boundary conditions and special patterns. These findings suggest that\na hybrid approach leveraging both testing methods can improve the reliability\nof LLM-generated code, providing guidance for test generation strategies in\nLLM-based code generation.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5(PBT)\u548c\u57fa\u4e8e\u793a\u4f8b\u7684\u6d4b\u8bd5(EBT)\u5728\u68c0\u6d4bLLM\u751f\u6210\u4ee3\u7801\u8fb9\u7f18\u6848\u4f8b\u65b9\u9762\u7684\u6548\u679c\uff0c\u53d1\u73b0\u4e24\u8005\u7ed3\u5408\u53ef\u5c06\u7f3a\u9677\u68c0\u6d4b\u7387\u4ece68.75%\u63d0\u5347\u81f381.25%\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u751f\u6210\u4ee3\u7801\u7684\u666e\u53ca\uff0c\u786e\u4fddLLM\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u53d8\u5f97\u91cd\u8981\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u793a\u4f8b\u6d4b\u8bd5\u65b9\u6cd5\u7ecf\u5e38\u9057\u6f0f\u8fb9\u7f18\u6848\u4f8b\u3002", "method": "\u5206\u679016\u4e2aHumanEval\u95ee\u9898\uff0c\u4f7f\u7528Claude-4-sonnet\u751f\u6210PBT\u548cEBT\u6d4b\u8bd5\u4ee3\u7801\uff0c\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\u5728\u68c0\u6d4b\u8fb9\u7f18\u6848\u4f8b\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5355\u72ec\u4f7f\u7528PBT\u6216EBT\u7684\u7f3a\u9677\u68c0\u6d4b\u7387\u4e3a68.75%\uff0c\u4f46\u4e24\u8005\u7ed3\u5408\u540e\u68c0\u6d4b\u7387\u63d0\u5347\u81f381.25%\u3002PBT\u64c5\u957f\u68c0\u6d4b\u6027\u80fd\u95ee\u9898\u548c\u5e7f\u6cdb\u8f93\u5165\u7a7a\u95f4\u63a2\u7d22\uff0cEBT\u64c5\u957f\u68c0\u6d4b\u7279\u5b9a\u8fb9\u754c\u6761\u4ef6\u548c\u7279\u6b8a\u6a21\u5f0f\u3002", "conclusion": "\u7ed3\u5408PBT\u548cEBT\u7684\u6df7\u5408\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8LLM\u751f\u6210\u4ee3\u7801\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u6d4b\u8bd5\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "swe benchmark"}}
{"id": "2510.25320", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25320", "abs": "https://arxiv.org/abs/2510.25320", "authors": ["Jiaqi Wu", "Qinlao Zhao", "Zefeng Chen", "Kai Qin", "Yifei Zhao", "Xueqian Wang", "Yuhang Yao"], "title": "GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning", "comment": null, "summary": "Autonomous agents powered by large language models (LLMs) have shown\nimpressive capabilities in tool manipulation for complex task-solving. However,\nexisting paradigms such as ReAct rely on sequential reasoning and execution,\nfailing to exploit the inherent parallelism among independent sub-tasks. This\nsequential bottleneck leads to inefficient tool utilization and suboptimal\nperformance in multi-step reasoning scenarios. We introduce Graph-based Agent\nPlanning (GAP), a novel framework that explicitly models inter-task\ndependencies through graph-based planning to enable adaptive parallel and\nserial tool execution. Our approach trains agent foundation models to decompose\ncomplex tasks into dependency-aware sub-task graphs, autonomously determining\nwhich tools can be executed in parallel and which must follow sequential\ndependencies. This dependency-aware orchestration achieves substantial\nimprovements in both execution efficiency and task accuracy. To train GAP, we\nconstruct a high-quality dataset of graph-based planning traces derived from\nthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage\ntraining strategy: supervised fine-tuning (SFT) on the curated dataset,\nfollowed by reinforcement learning (RL) with a correctness-based reward\nfunction on strategically sampled queries where tool-based reasoning provides\nmaximum value. Experimental results on MHQA datasets demonstrate that GAP\nsignificantly outperforms traditional ReAct baselines, particularly on\nmulti-step retrieval tasks, while achieving dramatic improvements in tool\ninvocation efficiency through intelligent parallelization. The project page is\navailable at: https://github.com/WJQ7777/Graph-Agent-Planning.", "AI": {"tldr": "\u63d0\u51faGraph-based Agent Planning (GAP)\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u89c4\u5212\u5b9e\u73b0\u5e76\u884c\u5de5\u5177\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u4ee3\u7406\uff08\u5982ReAct\uff09\u91c7\u7528\u987a\u5e8f\u63a8\u7406\u6267\u884c\uff0c\u65e0\u6cd5\u5229\u7528\u72ec\u7acb\u5b50\u4efb\u52a1\u7684\u5e76\u884c\u6027\uff0c\u5bfc\u81f4\u5de5\u5177\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u548c\u591a\u6b65\u63a8\u7406\u6027\u80fd\u4e0d\u4f73", "method": "\u8bad\u7ec3\u4ee3\u7406\u57fa\u7840\u6a21\u578b\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u4f9d\u8d56\u611f\u77e5\u7684\u5b50\u4efb\u52a1\u56fe\uff0c\u81ea\u4e3b\u786e\u5b9a\u5e76\u884c\u548c\u987a\u5e8f\u6267\u884c\u7684\u5de5\u5177\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03+\u57fa\u4e8e\u6b63\u786e\u6027\u7684\u5f3a\u5316\u5b66\u4e60", "result": "\u5728MHQA\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfReAct\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u667a\u80fd\u5e76\u884c\u5316\u5927\u5e45\u63d0\u5347\u5de5\u5177\u8c03\u7528\u6548\u7387", "conclusion": "GAP\u6846\u67b6\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u4efb\u52a1\u7f16\u6392\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6267\u884c\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u89e3\u51b3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8303\u5f0f", "topic": "agent analysis"}}
{"id": "2510.25406", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25406", "abs": "https://arxiv.org/abs/2510.25406", "authors": ["Changjie Wang", "Mariano Scazzariello", "Anoud Alshnaka", "Roberto Guanciale", "Dejan Kosti\u0107", "Marco Chiesa"], "title": "Dissect-and-Restore: AI-based Code Verification with Transient Refactoring", "comment": null, "summary": "Formal verification is increasingly recognized as a critical foundation for\nbuilding reliable software systems. However, the need for specialized expertise\nto write precise specifications, navigate complex proof obligations, and learn\nannotations often makes verification an order of magnitude more expensive than\nimplementation. While modern AI systems can recognize patterns in mathematical\nproofs and interpret natural language, effectively integrating them into the\nformal verification process remains an open challenge. We present Prometheus, a\nnovel AI-assisted system that facilitates automated code verification with\ncurrent AI capabilities in conjunction with modular software engineering\nprinciples (e.g., modular refactoring). Our approach begins by decomposing\ncomplex program logic, such as nested loops, into smaller, verifiable\ncomponents. Once verified, these components are recomposed to construct a proof\nof the original program. This decomposition-recomposition workflow is\nnon-trivial. Prometheus addresses this by guiding the proof search through\nstructured decomposition of complex lemmas into smaller, verifiable sub-lemmas.\nWhen automated tools are insufficient, users can provide lightweight natural\nlanguage guidance to steer the proof process effectively. Our evaluation\ndemonstrates that transiently applying modular restructuring to the code\nsubstantially improves the AI's effectiveness in verifying individual\ncomponents. This approach successfully verifies 86% of tasks in our curated\ndataset, compared to 68% for the baseline. Gains are more pronounced with\nincreasing specification complexity, improving from 30% to 69%, and when\nintegrating proof outlines for complex programs, from 25% to 87%.", "AI": {"tldr": "Prometheus\u662f\u4e00\u4e2aAI\u8f85\u52a9\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u91cd\u6784\u5c06\u590d\u6742\u7a0b\u5e8f\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5c0f\u7ec4\u4ef6\uff0c\u7136\u540e\u91cd\u65b0\u7ec4\u5408\u6784\u5efa\u539f\u59cb\u7a0b\u5e8f\u7684\u8bc1\u660e\u3002", "motivation": "\u5f62\u5f0f\u5316\u9a8c\u8bc1\u662f\u6784\u5efa\u53ef\u9760\u8f6f\u4ef6\u7cfb\u7edf\u7684\u5173\u952e\uff0c\u4f46\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709AI\u7cfb\u7edf\u80fd\u8bc6\u522b\u6570\u5b66\u8bc1\u660e\u6a21\u5f0f\u4f46\u96be\u4ee5\u6709\u6548\u96c6\u6210\u5230\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u3002", "method": "\u4f7f\u7528\u6a21\u5757\u5316\u8f6f\u4ef6\u5de5\u7a0b\u539f\u5219\uff0c\u5c06\u590d\u6742\u7a0b\u5e8f\u903b\u8f91\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7ec4\u4ef6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u5f15\u5bfc\u8bc1\u660e\u641c\u7d22\uff0c\u7528\u6237\u53ef\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u3002", "result": "\u5728\u5b9a\u5236\u6570\u636e\u96c6\u4e2d\u6210\u529f\u9a8c\u8bc186%\u7684\u4efb\u52a1\uff0c\u76f8\u6bd4\u57fa\u7ebf68%\u6709\u663e\u8457\u63d0\u5347\u3002\u5bf9\u4e8e\u590d\u6742\u89c4\u8303\u4ece30%\u63d0\u5347\u523069%\uff0c\u96c6\u6210\u8bc1\u660e\u5927\u7eb2\u540e\u4ece25%\u63d0\u5347\u523087%\u3002", "conclusion": "\u6a21\u5757\u5316\u91cd\u6784\u80fd\u663e\u8457\u63d0\u9ad8AI\u9a8c\u8bc1\u7ec4\u4ef6\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u89c4\u8303\u9a8c\u8bc1\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "topic": "code agent"}}
{"id": "2510.24988", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24988", "abs": "https://arxiv.org/abs/2510.24988", "authors": ["Hemanath Arumugam", "Falong Fan", "Bo Liu"], "title": "Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series", "comment": null, "summary": "Hierarchical Reinforcement Learning (HRL) enhances the scalability of\ndecision-making in long-horizon tasks by introducing temporal abstraction\nthrough options-policies that span multiple timesteps. Despite its theoretical\nappeal, the practical implementation of HRL suffers from the challenge of\nautonomously discovering semantically meaningful subgoals and learning optimal\noption termination boundaries. This paper introduces a novel architecture that\nintegrates a self-supervised, Transformer-based Change Point Detection (CPD)\nmodule into the Option-Critic framework, enabling adaptive segmentation of\nstate trajectories and the discovery of options. The CPD module is trained\nusing heuristic pseudo-labels derived from intrinsic signals to infer latent\nshifts in environment dynamics without external supervision. These inferred\nchange-points are leveraged in three critical ways: (i) to serve as supervisory\nsignals for stabilizing termination function gradients, (ii) to pretrain\nintra-option policies via segment-wise behavioral cloning, and (iii) to enforce\nfunctional specialization through inter-option divergence penalties over\nCPD-defined state partitions. The overall optimization objective enhances the\nstandard actor-critic loss using structure-aware auxiliary losses. In our\nframework, option discovery arises naturally as CPD-defined trajectory segments\nare mapped to distinct intra-option policies, enabling the agent to\nautonomously partition its behavior into reusable, semantically meaningful\nskills. Experiments on the Four-Rooms and Pinball tasks demonstrate that\nCPD-guided agents exhibit accelerated convergence, higher cumulative returns,\nand significantly improved option specialization. These findings confirm that\nintegrating structural priors via change-point segmentation leads to more\ninterpretable, sample-efficient, and robust hierarchical policies in complex\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u81ea\u76d1\u7763Transformer\u53d8\u5316\u70b9\u68c0\u6d4b\u6a21\u5757\u96c6\u6210\u5230Option-Critic\u6846\u67b6\u4e2d\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u5272\u72b6\u6001\u8f68\u8ff9\u6765\u53d1\u73b0\u9009\u9879\uff0c\u89e3\u51b3\u4e86\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b50\u76ee\u6807\u53d1\u73b0\u548c\u9009\u9879\u7ec8\u6b62\u8fb9\u754c\u5b66\u4e60\u7684\u6311\u6218\u3002", "motivation": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u62bd\u8c61\u6765\u589e\u5f3a\u51b3\u7b56\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u81ea\u4e3b\u53d1\u73b0\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u5b50\u76ee\u6807\u548c\u5b66\u4e60\u6700\u4f18\u9009\u9879\u7ec8\u6b62\u8fb9\u754c\u7684\u6311\u6218\u3002", "method": "\u96c6\u6210\u81ea\u76d1\u7763Transformer\u53d8\u5316\u70b9\u68c0\u6d4b\u6a21\u5757\u5230Option-Critic\u6846\u67b6\uff0c\u4f7f\u7528\u542f\u53d1\u5f0f\u4f2a\u6807\u7b7e\u8bad\u7ec3CPD\u6a21\u5757\u63a8\u65ad\u73af\u5883\u52a8\u6001\u7684\u6f5c\u5728\u53d8\u5316\uff0c\u5229\u7528\u63a8\u65ad\u7684\u53d8\u5316\u70b9\u6765\u7a33\u5b9a\u7ec8\u6b62\u51fd\u6570\u68af\u5ea6\u3001\u9884\u8bad\u7ec3\u5185\u90e8\u9009\u9879\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u9009\u9879\u95f4\u5dee\u5f02\u60e9\u7f5a\u5f3a\u5236\u529f\u80fd\u4e13\u4e1a\u5316\u3002", "result": "\u5728Four-Rooms\u548cPinball\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCPD\u5f15\u5bfc\u7684\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u52a0\u901f\u6536\u655b\u3001\u66f4\u9ad8\u7684\u7d2f\u79ef\u56de\u62a5\u548c\u663e\u8457\u6539\u8fdb\u7684\u9009\u9879\u4e13\u4e1a\u5316\u3002", "conclusion": "\u901a\u8fc7\u53d8\u5316\u70b9\u5206\u5272\u6574\u5408\u7ed3\u6784\u5148\u9a8c\u53ef\u4ee5\u5728\u590d\u6742\u73af\u5883\u4e2d\u4ea7\u751f\u66f4\u53ef\u89e3\u91ca\u3001\u6837\u672c\u6548\u7387\u66f4\u9ad8\u548c\u66f4\u9c81\u68d2\u7684\u5206\u5c42\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25423", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25423", "abs": "https://arxiv.org/abs/2510.25423", "authors": ["Ali Asgari", "Annibale Panichella", "Pouria Derakhshanfar", "Mitchell Olsthoorn"], "title": "What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow", "comment": "12 pages, 4 Figures", "summary": "AI agents have rapidly gained popularity across research and industry as\nsystems that extend large language models with additional capabilities to plan,\nuse tools, remember, and act toward specific goals. Yet despite their promise,\ndevelopers face persistent and often underexplored challenges when building,\ndeploying, and maintaining these emerging systems. To identify these\nchallenges, we study developer discussions on Stack Overflow, the world's\nlargest developer-focused Q and A platform with about 60 million questions and\nanswers and 30 million users. We construct a taxonomy of developer challenges\nthrough tag expansion and filtering, apply LDA-MALLET for topic modeling, and\nmanually validate and label the resulting themes. Our analysis reveals seven\nmajor areas of recurring issues encompassing 77 distinct technical challenges\nrelated to runtime integration, dependency management, orchestration\ncomplexity, and evaluation reliability. We further quantify topic popularity\nand difficulty to identify which issues are most common and hardest to resolve,\nmap the tools and programming languages used in agent development, and track\ntheir evolution from 2021 to 2025 in relation to major AI model and framework\nreleases. Finally, we present the implications of our results, offering\nconcrete guidance for practitioners, researchers, and educators on agent\nreliability and developer support.", "AI": {"tldr": "\u901a\u8fc7\u5bf9Stack Overflow\u4e0aAI\u4ee3\u7406\u5f00\u53d1\u8005\u8ba8\u8bba\u7684\u5206\u6790\uff0c\u8bc6\u522b\u51fa7\u5927\u7c7b77\u4e2a\u6280\u672f\u6311\u6218\uff0c\u6db5\u76d6\u8fd0\u884c\u65f6\u96c6\u6210\u3001\u4f9d\u8d56\u7ba1\u7406\u3001\u7f16\u6392\u590d\u6742\u6027\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u7b49\u95ee\u9898\uff0c\u5e76\u91cf\u5316\u4e86\u95ee\u9898\u7684\u6d41\u884c\u5ea6\u548c\u89e3\u51b3\u96be\u5ea6\u3002", "motivation": "AI\u4ee3\u7406\u5728\u7814\u7a76\u548c\u5de5\u4e1a\u754c\u8fc5\u901f\u6d41\u884c\uff0c\u4f46\u5f00\u53d1\u8005\u5728\u6784\u5efa\u3001\u90e8\u7f72\u548c\u7ef4\u62a4\u8fd9\u4e9b\u65b0\u5174\u7cfb\u7edf\u65f6\u9762\u4e34\u6301\u7eed\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6807\u7b7e\u6269\u5c55\u548c\u8fc7\u6ee4\u6784\u5efa\u5f00\u53d1\u8005\u6311\u6218\u5206\u7c7b\u6cd5\uff0c\u5e94\u7528LDA-MALLET\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u624b\u52a8\u9a8c\u8bc1\u548c\u6807\u8bb0\u7ed3\u679c\u4e3b\u9898\uff0c\u5206\u67902021-2025\u5e74\u95f4\u7684\u6570\u636e\u6f14\u53d8\u3002", "result": "\u8bc6\u522b\u51fa7\u4e2a\u4e3b\u8981\u95ee\u9898\u9886\u57df\u548c77\u4e2a\u5177\u4f53\u6280\u672f\u6311\u6218\uff0c\u91cf\u5316\u4e86\u4e3b\u9898\u6d41\u884c\u5ea6\u548c\u89e3\u51b3\u96be\u5ea6\uff0c\u7ed8\u5236\u4e86\u4ee3\u7406\u5f00\u53d1\u4e2d\u4f7f\u7528\u7684\u5de5\u5177\u548c\u7f16\u7a0b\u8bed\u8a00\u5730\u56fe\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4ece\u4e1a\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u5728\u4ee3\u7406\u53ef\u9760\u6027\u548c\u5f00\u53d1\u8005\u652f\u6301\u65b9\u9762\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2510.24891", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24891", "abs": "https://arxiv.org/abs/2510.24891", "authors": ["Jin Huang", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen W. White"], "title": "Idea2Plan: Exploring AI-Powered Research Planning", "comment": null, "summary": "Large language models (LLMs) have demonstrated significant potential to\naccelerate scientific discovery as valuable tools for analyzing data,\ngenerating hypotheses, and supporting innovative approaches in various\nscientific fields. In this work, we investigate how LLMs can handle the\ntransition from conceptual research ideas to well-structured research plans.\nEffective research planning not only supports scientists in advancing their\nresearch but also represents a crucial capability for the development of\nautonomous research agents. Despite its importance, the field lacks a\nsystematic understanding of LLMs' research planning capability. To rigorously\nmeasure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a\nbenchmark built from 200 ICML 2025 Spotlight and Oral papers released after\nmajor LLM training cutoffs. Each benchmark instance includes a research idea\nand a grading rubric capturing the key components of valid plans. We further\npropose Idea2Plan JudgeEval, a complementary benchmark to assess the\nreliability of LLM-based judges against expert annotations. Experimental\nresults show that GPT-5 and GPT-5-mini achieve the strongest performance on the\nbenchmark, though substantial headroom remains for future improvement. Our\nstudy provides new insights into LLMs' capability for research planning and lay\nthe groundwork for future progress.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Idea2Plan\u4efb\u52a1\u548c\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u7814\u7a76\u60f3\u6cd5\u5230\u7814\u7a76\u8ba1\u5212\u7684\u8f6c\u6362\u80fd\u529b\uff0c\u5e76\u53d1\u73b0GPT-5\u7cfb\u5217\u8868\u73b0\u6700\u4f73\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u7814\u7a76\u89c4\u5212\u662f\u79d1\u5b66\u53d1\u73b0\u548c\u81ea\u4e3b\u7814\u7a76\u4ee3\u7406\u5f00\u53d1\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9LLM\u7814\u7a76\u89c4\u5212\u80fd\u529b\u7684\u7cfb\u7edf\u7406\u89e3\u3002", "method": "\u5f15\u5165Idea2Plan\u4efb\u52a1\u548cIdea2Plan Bench\u57fa\u51c6\uff0c\u5305\u542b200\u7bc7ICML 2025\u8bba\u6587\u7684\u7814\u7a76\u60f3\u6cd5\u548c\u8bc4\u5206\u6807\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86Idea2Plan JudgeEval\u6765\u8bc4\u4f30LLM\u8bc4\u59d4\u7684\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aGPT-5\u548cGPT-5-mini\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u5f3a\uff0c\u4f46\u4ecd\u6709\u663e\u8457\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3LLM\u7684\u7814\u7a76\u89c4\u5212\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u8fdb\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.25445", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25445", "abs": "https://arxiv.org/abs/2510.25445", "authors": ["Mohamad Abou Ali", "Fadi Dornaika"], "title": "Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions", "comment": null, "summary": "Agentic AI represents a transformative shift in artificial intelligence, but\nits rapid advancement has led to a fragmented understanding, often conflating\nmodern neural systems with outdated symbolic models -- a practice known as\nconceptual retrofitting. This survey cuts through this confusion by introducing\na novel dual-paradigm framework that categorizes agentic systems into two\ndistinct lineages: the Symbolic/Classical (relying on algorithmic planning and\npersistent state) and the Neural/Generative (leveraging stochastic generation\nand prompt-driven orchestration). Through a systematic PRISMA-based review of\n90 studies (2018--2025), we provide a comprehensive analysis structured around\nthis framework across three dimensions: (1) the theoretical foundations and\narchitectural principles defining each paradigm; (2) domain-specific\nimplementations in healthcare, finance, and robotics, demonstrating how\napplication constraints dictate paradigm selection; and (3) paradigm-specific\nethical and governance challenges, revealing divergent risks and mitigation\nstrategies. Our analysis reveals that the choice of paradigm is strategic:\nsymbolic systems dominate safety-critical domains (e.g., healthcare), while\nneural systems prevail in adaptive, data-rich environments (e.g., finance).\nFurthermore, we identify critical research gaps, including a significant\ndeficit in governance models for symbolic systems and a pressing need for\nhybrid neuro-symbolic architectures. The findings culminate in a strategic\nroadmap arguing that the future of Agentic AI lies not in the dominance of one\nparadigm, but in their intentional integration to create systems that are both\nadaptable and reliable. This work provides the essential conceptual toolkit to\nguide future research, development, and policy toward robust and trustworthy\nhybrid intelligent systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u8303\u5f0f\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5206\u4e3a\u7b26\u53f7/\u7ecf\u5178\u8303\u5f0f\u548c\u795e\u7ecf/\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u5206\u6790\u4e86\u4e24\u79cd\u8303\u5f0f\u5728\u7406\u8bba\u57fa\u7840\u3001\u9886\u57df\u5e94\u7528\u548c\u4f26\u7406\u6311\u6218\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u5e76\u6307\u51fa\u672a\u6765\u53d1\u5c55\u65b9\u5411\u662f\u4e24\u8005\u7684\u6709\u610f\u6574\u5408\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4f53AI\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u6982\u5ff5\u6df7\u6dc6\u95ee\u9898\uff0c\u907f\u514d\u5c06\u73b0\u4ee3\u795e\u7ecf\u7cfb\u7edf\u4e0e\u8fc7\u65f6\u7684\u7b26\u53f7\u6a21\u578b\u8fdb\u884c\u6982\u5ff5\u6027\u56de\u6eaf\u62df\u5408\uff0c\u4e3a\u7406\u89e3\u667a\u80fd\u4f53AI\u63d0\u4f9b\u6e05\u6670\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u91c7\u7528PRISMA\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u4e862018-2025\u5e74\u95f4\u768490\u9879\u7814\u7a76\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7ed3\u6784\u5316\u5206\u6790\uff1a\u7406\u8bba\u57fa\u7840\u4e0e\u67b6\u6784\u539f\u5219\u3001\u9886\u57df\u7279\u5b9a\u5b9e\u73b0\u3001\u8303\u5f0f\u7279\u5b9a\u4f26\u7406\u4e0e\u6cbb\u7406\u6311\u6218\u3002", "result": "\u53d1\u73b0\u8303\u5f0f\u9009\u62e9\u5177\u6709\u6218\u7565\u6027\uff1a\u7b26\u53f7\u7cfb\u7edf\u4e3b\u5bfc\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u533b\u7597\uff09\uff0c\u795e\u7ecf\u7cfb\u7edf\u4e3b\u5bfc\u9002\u5e94\u6027\u5f3a\u7684\u6570\u636e\u4e30\u5bcc\u73af\u5883\uff08\u5982\u91d1\u878d\uff09\uff1b\u8bc6\u522b\u51fa\u7b26\u53f7\u7cfb\u7edf\u6cbb\u7406\u6a21\u578b\u7f3a\u5931\u548c\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u67b6\u6784\u9700\u6c42\u7b49\u5173\u952e\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u7684\u672a\u6765\u4e0d\u5728\u4e8e\u5355\u4e00\u8303\u5f0f\u7684\u652f\u914d\uff0c\u800c\u5728\u4e8e\u7b26\u53f7\u548c\u795e\u7ecf\u8303\u5f0f\u7684\u6709\u610f\u6574\u5408\uff0c\u4ee5\u521b\u5efa\u65e2\u9002\u5e94\u6027\u5f3a\u53c8\u53ef\u9760\u7684\u6df7\u5408\u667a\u80fd\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2510.24940", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24940", "abs": "https://arxiv.org/abs/2510.24940", "authors": ["Yinhan He", "Wendy Zheng", "Yaochen Zhu", "Zaiyi Zheng", "Lin Su", "Sriram Vasudevan", "Qi Guo", "Liangjie Hong", "Jundong Li"], "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens", "comment": null, "summary": "The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment\nin efficiency-critical applications. Recently, implicit CoT approaches have\nemerged, which encode reasoning steps within LLM's hidden embeddings (termed\n``implicit reasoning'') rather than explicit tokens. This approach accelerates\nCoT by reducing the reasoning length and bypassing some LLM components.\nHowever, existing implicit CoT methods face two significant challenges: (1)\nthey fail to preserve the semantic alignment between the implicit reasoning\n(when transformed to natural language) and the ground-truth reasoning,\nresulting in a significant CoT performance degradation, and (2) they focus on\nreducing the length of the implicit reasoning; however, they neglect the\nconsiderable time cost for an LLM to generate one individual implicit reasoning\ntoken. To tackle these challenges, we propose a novel semantically-aligned\nimplicit CoT framework termed SemCoT. In particular, for the first challenge,\nwe design a contrastively trained sentence transformer that evaluates semantic\nalignment between implicit and explicit reasoning, which is used to enforce\nsemantic preservation during implicit reasoning optimization. To address the\nsecond challenge, we introduce an efficient implicit reasoning generator by\nfinetuning a lightweight language model using knowledge distillation. This\ngenerator is guided by our sentence transformer to distill ground-truth\nreasoning into semantically aligned implicit reasoning, while also optimizing\nfor accuracy. SemCoT is the first approach that enhances CoT efficiency by\njointly optimizing token-level generation speed and preserving semantic\nalignment with ground-truth reasoning. Extensive experiments demonstrate the\nsuperior performance of SemCoT compared to state-of-the-art methods in both\nefficiency and effectiveness. Our code can be found at\nhttps://github.com/YinhanHe123/SemCoT/.", "AI": {"tldr": "\u63d0\u51faSemCoT\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u7684\u9690\u5f0f\u63a8\u7406\u548c\u9ad8\u6548\u8f7b\u91cf\u7ea7\u751f\u6210\u5668\uff0c\u89e3\u51b3\u73b0\u6709\u9690\u5f0fCoT\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u751f\u6210\u6548\u7387\u65b9\u9762\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u663e\u5f0fCoT\u63a8\u7406\u7684\u5197\u957f\u6027\u963b\u788d\u4e86\u5176\u5728\u6548\u7387\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u73b0\u6709\u9690\u5f0fCoT\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u5bf9\u9f50\u5dee\u548c\u751f\u6210\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5bf9\u6bd4\u8bad\u7ec3\u7684\u53e5\u5b50\u8f6c\u6362\u5668\u8bc4\u4f30\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9ad8\u6548\u9690\u5f0f\u63a8\u7406\u751f\u6210\u5668\uff0c\u8054\u5408\u4f18\u5316\u751f\u6210\u901f\u5ea6\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eSemCoT\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SemCoT\u662f\u9996\u4e2a\u901a\u8fc7\u8054\u5408\u4f18\u5316token\u7ea7\u751f\u6210\u901f\u5ea6\u548c\u8bed\u4e49\u5bf9\u9f50\u6765\u63d0\u5347CoT\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2510.25510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25510", "abs": "https://arxiv.org/abs/2510.25510", "authors": ["Zekun Xu", "Siyu Xia", "Chuhuai Yue", "Jiajun Chai", "Mingxue Tian", "Xiaohan Wang", "Wei Lin", "Haoxuan Li", "Guojun Yin"], "title": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL", "comment": null, "summary": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks,\nReinforcement Learning (RL) has become a common method for improving\nperformance. Existing methods primarily rely on static execution feedback,\nwhich restricts real-time error correction. However, integrating multi-turn\ntool invocation along with dynamic feedback could significantly improve\nadaptability and robustness, ultimately enhancing model performance. To address\nthese issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated\nReasoning reinforcement learning framework for Text-to-SQL. Our approach\nintroduces an execution-aware multi-turn reasoning paradigm that seamlessly\nincorporates database execution feedback at each reasoning step, enabling\ncontext-sensitive query generation and progressive refinement throughout the\nreasoning process. The framework extends the GRPO algorithm to accommodate\ncomplex multi-turn interaction scenarios. Considering the training instability\ncharacteristics of MTIR and the potential for significant Deviation of model\ndistribution from the initial model, we enhance the GRPO algorithm by adding a\ntrajectory filtering mechanism and removing KL loss constraints. Experimental\nresults demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\%\naccuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,\nsignificantly outperforming existing approaches.", "AI": {"tldr": "\u63d0\u51faMTIR-SQL\u6846\u67b6\uff0c\u5c06\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e0e\u52a8\u6001\u6267\u884c\u53cd\u9988\u7ed3\u5408\uff0c\u901a\u8fc7\u589e\u5f3a\u7684GRPO\u7b97\u6cd5\u5728Text-to-SQL\u4efb\u52a1\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6267\u884c\u53cd\u9988\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u3002\u6574\u5408\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u548c\u52a8\u6001\u53cd\u9988\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u6267\u884c\u611f\u77e5\u7684\u591a\u8f6e\u63a8\u7406\u8303\u5f0f\uff0c\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u65e0\u7f1d\u96c6\u6210\u6570\u636e\u5e93\u6267\u884c\u53cd\u9988\uff0c\u6269\u5c55GRPO\u7b97\u6cd5\u4ee5\u9002\u5e94\u590d\u6742\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\uff0c\u5e76\u52a0\u5165\u8f68\u8ff9\u8fc7\u6ee4\u673a\u5236\u548c\u79fb\u9664KL\u635f\u5931\u7ea6\u675f\u3002", "result": "MTIR-SQL\u5728BIRD Dev\u4e0a\u8fbe\u523064.4%\u51c6\u786e\u7387\uff0c\u5728SPIDER Dev\u4e0a\u8fbe\u523084.6%\u6267\u884c\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4e0e\u52a8\u6001\u53cd\u9988\u7684\u7ed3\u5408\u80fd\u591f\u6709\u6548\u63d0\u5347Text-to-SQL\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25694", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25694", "abs": "https://arxiv.org/abs/2510.25694", "authors": ["Jiayi Kuang", "Yinghui Li", "Xin Zhang", "Yangning Li", "Di Yin", "Xing Sun", "Ying Shen", "Philip S. Yu"], "title": "Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents", "comment": null, "summary": "Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents.", "AI": {"tldr": "Enconda-bench\u662f\u9996\u4e2a\u63d0\u4f9b\u73af\u5883\u914d\u7f6e\u8fc7\u7a0b\u7ea7\u80fd\u529b\u8bc4\u4f30\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u5165\u771f\u5b9eREADME\u9519\u8bef\u548cDocker\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u73af\u5883\u8bbe\u7f6e\u3001\u9519\u8bef\u8bca\u65ad\u548c\u4fee\u590d\u65b9\u9762\u7684\u7ec6\u7c92\u5ea6\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u8bc4\u4f30\u7aef\u5230\u7aef\u6784\u5efa/\u6d4b\u8bd5\u6210\u529f\u7387\uff0c\u65e0\u6cd5\u63ed\u793a\u667a\u80fd\u4f53\u5728\u73af\u5883\u914d\u7f6e\u4e2d\u6210\u529f\u6216\u5931\u8d25\u7684\u5177\u4f53\u539f\u56e0\u548c\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u7684\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u6ce8\u5165\u771f\u5b9eREADME\u9519\u8bef\u6784\u5efa\u4efb\u52a1\u5b9e\u4f8b\uff0c\u5728Docker\u4e2d\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u8bc4\u4f30\uff0c\u7ed3\u5408\u8fc7\u7a0b\u7ea7\u5206\u6790\u548c\u7aef\u5230\u7aef\u53ef\u6267\u884c\u6027\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u7ec6\u7c92\u5ea6\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u867d\u7136\u667a\u80fd\u4f53\u80fd\u591f\u5b9a\u4f4d\u9519\u8bef\uff0c\u4f46\u5728\u5c06\u53cd\u9988\u8f6c\u5316\u4e3a\u6709\u6548\u4fee\u6b63\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9650\u5236\u4e86\u7aef\u5230\u7aef\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "Enconda-bench\u4e3a\u73af\u5883\u914d\u7f6e\u63d0\u4f9b\u4e86\u9996\u4e2a\u8fc7\u7a0b\u7ea7\u5185\u90e8\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u6539\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "topic": "swe benchmark"}}
{"id": "2510.25518", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25518", "abs": "https://arxiv.org/abs/2510.25518", "authors": ["Thomas Cook", "Richard Osuagwu", "Liman Tsatiashvili", "Vrynsia Vrynsia", "Koustav Ghosal", "Maraim Masoud", "Riccardo Mattivi"], "title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation", "comment": "Keywords: RAG Agentic AI Fintech NLP KB Domain-Specific Ontology\n  Query Understanding", "summary": "Retrieval-Augmented Generation (RAG) systems often face limitations in\nspecialized domains such as fintech, where domain-specific ontologies, dense\nterminology, and acronyms complicate effective retrieval and synthesis. This\npaper introduces an agentic RAG architecture designed to address these\nchallenges through a modular pipeline of specialized agents. The proposed\nsystem supports intelligent query reformulation, iterative sub-query\ndecomposition guided by keyphrase extraction, contextual acronym resolution,\nand cross-encoder-based context re-ranking. We evaluate our approach against a\nstandard RAG baseline using a curated dataset of 85 question--answer--reference\ntriples derived from an enterprise fintech knowledge base. Experimental results\ndemonstrate that the agentic RAG system outperforms the baseline in retrieval\nprecision and relevance, albeit with increased latency. These findings suggest\nthat structured, multi-agent methodologies offer a promising direction for\nenhancing retrieval robustness in complex, domain-specific settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u91d1\u878d\u79d1\u6280\u9886\u57df\u7684\u667a\u80fdRAG\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4ee3\u7406\u7ba1\u9053\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u68c0\u7d22\u6311\u6218\uff0c\u572885\u4e2a\u95ee\u7b54\u5bf9\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u4e8e\u6807\u51c6RAG\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u91d1\u878d\u79d1\u6280\u7b49\u4e13\u4e1a\u9886\u57df\u4e2dRAG\u7cfb\u7edf\u9762\u4e34\u7684\u9886\u57df\u7279\u5b9a\u672c\u4f53\u3001\u5bc6\u96c6\u672f\u8bed\u548c\u7f29\u7565\u8bed\u5e26\u6765\u7684\u68c0\u7d22\u4e0e\u5408\u6210\u56f0\u96be\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u4ee3\u7406\u67b6\u6784\uff0c\u5305\u62ec\u667a\u80fd\u67e5\u8be2\u91cd\u6784\u3001\u57fa\u4e8e\u5173\u952e\u8bcd\u63d0\u53d6\u7684\u8fed\u4ee3\u5b50\u67e5\u8be2\u5206\u89e3\u3001\u4e0a\u4e0b\u6587\u7f29\u7565\u8bed\u89e3\u6790\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u4e0a\u4e0b\u6587\u91cd\u6392\u5e8f\u3002", "result": "\u5728\u68c0\u7d22\u7cbe\u5ea6\u548c\u76f8\u5173\u6027\u65b9\u9762\u4f18\u4e8e\u6807\u51c6RAG\u57fa\u7ebf\uff0c\u4f46\u5ef6\u8fdf\u6709\u6240\u589e\u52a0\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u4ee3\u7406\u65b9\u6cd5\u4e3a\u590d\u6742\u9886\u57df\u7279\u5b9a\u73af\u5883\u4e2d\u7684\u68c0\u7d22\u9c81\u68d2\u6027\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.25529", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25529", "abs": "https://arxiv.org/abs/2510.25529", "authors": ["Likun Wang", "Xiangteng Zhang", "Yinuo Wang", "Guojian Zhan", "Wenxuan Wang", "Haoyu Gao", "Jingliang Duan", "Shengbo Eben Li"], "title": "Off-policy Reinforcement Learning with Model-based Exploration Augmentation", "comment": null, "summary": "Exploration is fundamental to reinforcement learning (RL), as it determines\nhow effectively an agent discovers and exploits the underlying structure of its\nenvironment to achieve optimal performance. Existing exploration methods\ngenerally fall into two categories: active exploration and passive exploration.\nThe former introduces stochasticity into the policy but struggles in\nhigh-dimensional environments, while the latter adaptively prioritizes\ntransitions in the replay buffer to enhance exploration, yet remains\nconstrained by limited sample diversity. To address the limitation in passive\nexploration, we propose Modelic Generative Exploration (MoGE), which augments\nexploration through the generation of under-explored critical states and\nsynthesis of dynamics-consistent experiences through transition models. MoGE is\ncomposed of two components: (1) a diffusion-based generator that synthesizes\ncritical states under the guidance of a utility function evaluating each\nstate's potential influence on policy exploration, and (2) a one-step\nimagination world model for constructing critical transitions based on the\ncritical states for agent learning. Our method adopts a modular formulation\nthat aligns with the principles of off-policy learning, allowing seamless\nintegration with existing algorithms to improve exploration without altering\ntheir core structures. Empirical results on OpenAI Gym and DeepMind Control\nSuite reveal that MoGE effectively bridges exploration and policy learning,\nleading to remarkable gains in both sample efficiency and performance across\ncomplex control tasks.", "AI": {"tldr": "\u63d0\u51faMoGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u672a\u5145\u5206\u63a2\u7d22\u7684\u5173\u952e\u72b6\u6001\u548c\u52a8\u6001\u4e00\u81f4\u7684\u7ecf\u9a8c\u6765\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\uff0c\u5305\u542b\u6269\u6563\u6a21\u578b\u751f\u6210\u5668\u548c\u4e00\u6b65\u60f3\u8c61\u4e16\u754c\u6a21\u578b\u4e24\u4e2a\u7ec4\u4ef6\u3002", "motivation": "\u73b0\u6709\u63a2\u7d22\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4e3b\u52a8\u63a2\u7d22\u5728\u9ad8\u7ef4\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u88ab\u52a8\u63a2\u7d22\u53d7\u9650\u4e8e\u6837\u672c\u591a\u6837\u6027\u4e0d\u8db3\u3002\u9700\u8981\u89e3\u51b3\u88ab\u52a8\u63a2\u7d22\u7684\u5c40\u9650\u6027\u3002", "method": "MoGE\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a(1)\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u72b6\u6001\u751f\u6210\u5668\uff0c\u5728\u6548\u7528\u51fd\u6570\u6307\u5bfc\u4e0b\u5408\u6210\u5173\u952e\u72b6\u6001\uff1b(2)\u4e00\u6b65\u60f3\u8c61\u4e16\u754c\u6a21\u578b\uff0c\u57fa\u4e8e\u5173\u952e\u72b6\u6001\u6784\u5efa\u5173\u952e\u8f6c\u6362\u7528\u4e8e\u667a\u80fd\u4f53\u5b66\u4e60\u3002", "result": "\u5728OpenAI Gym\u548cDeepMind Control Suite\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoGE\u6709\u6548\u8fde\u63a5\u4e86\u63a2\u7d22\u548c\u7b56\u7565\u5b66\u4e60\uff0c\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "MoGE\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u53ef\u4e0e\u73b0\u6709\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u5728\u4e0d\u6539\u53d8\u6838\u5fc3\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u63a2\u7d22\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25612", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.25612", "abs": "https://arxiv.org/abs/2510.25612", "authors": ["Amit Giloni", "Chiara Picardi", "Roy Betser", "Shamik Bose", "Aishvariya Priya Rathina Sabapathy", "Roman Vainshtein"], "title": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows", "comment": "Accepted to EMNLP 2025, 27 pages, 6 figures", "summary": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86CAIR\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5404\u667a\u80fd\u4f53\u5bf9\u6700\u7ec8\u8f93\u51fa\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u662f\u9996\u4e2a\u80fd\u591f\u91cf\u5316\u667a\u80fd\u4f53\u5f71\u54cd\u529b\u7684\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08AAW\uff09\u7684\u5e7f\u6cdb\u91c7\u7528\u548c\u9ad8\u5ea6\u81ea\u4e3b\u6027\uff0c\u9700\u8981\u4ece\u8d28\u91cf\u548c\u5b89\u5168\u89d2\u5ea6\u6df1\u5165\u7406\u89e3\u5176\u8fd0\u4f5c\u673a\u5236\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u5404\u667a\u80fd\u4f53\u5bf9\u6700\u7ec8\u8f93\u51fa\u5f71\u54cd\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u53cd\u4e8b\u5b9e\u5206\u6790\u6280\u672f\uff0c\u5f00\u53d1\u4e86CAIR\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u53d8\u7279\u5b9a\u667a\u80fd\u4f53\u7684\u8f93\u51fa\u6765\u8bc4\u4f30\u5176\u5bf9\u6700\u7ec8\u7ed3\u679c\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u63d0\u4f9b\u4efb\u52a1\u65e0\u5173\u7684\u5206\u6790\u80fd\u529b\u3002", "result": "\u5728\u5305\u542b30\u4e2a\u4e0d\u540c\u7528\u4f8b\u548c230\u4e2a\u529f\u80fd\u7684AAW\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCAIR\u80fd\u4ea7\u751f\u4e00\u81f4\u7684\u6392\u540d\u7ed3\u679c\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u7387\u548c\u76f8\u5173\u6027\u3002", "conclusion": "CAIR\u662f\u9996\u4e2a\u80fd\u591f\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u667a\u80fd\u4f53\u5f71\u54cd\u529b\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u4efb\u52a1\u65e0\u5173\u6027\uff0c\u53ef\u5728\u79bb\u7ebf\u548c\u63a8\u7406\u65f6\u4f7f\u7528\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316AAW\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.25110", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25110", "abs": "https://arxiv.org/abs/2510.25110", "authors": ["Yun-Shiuan Chuang", "Ruixuan Tu", "Chengtao Dai", "Smit Vasani", "Binwei Yao", "Michael Henry Tessler", "Sijia Yang", "Dhavan Shah", "Robert Hawkins", "Junjie Hu", "Timothy T. Rogers"], "title": "DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates", "comment": null, "summary": "Accurately modeling opinion change through social interactions is crucial for\naddressing issues like misinformation and polarization. While role-playing\nlarge language models (LLMs) offer a promising way to simulate human-like\ninteractions, existing research shows that single-agent alignment does not\nguarantee authentic multi-agent group dynamics. Current LLM role-play setups\noften produce unnatural dynamics (e.g., premature convergence), without an\nempirical benchmark to measure authentic human opinion trajectories. To bridge\nthis gap, we introduce DEBATE, the first large-scale empirical benchmark\nexplicitly designed to evaluate the authenticity of the interaction between\nmulti-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round\ndebate conversations among over 2,792 U.S.-based participants discussing 107\ncontroversial topics, capturing both publicly-expressed messages and\nprivately-reported opinions. Using DEBATE, we systematically evaluate and\nidentify critical discrepancies between simulated and authentic group dynamics.\nWe further demonstrate DEBATE's utility for aligning LLMs with human behavior\nthrough supervised fine-tuning, achieving improvements in surface-level metrics\n(e.g., ROUGE-L and message length) while highlighting limitations in deeper\nsemantic alignment (e.g., semantic similarity). Our findings highlight both the\npotential and current limitations of role-playing LLM agents for realistically\nsimulating human-like social dynamics.", "AI": {"tldr": "DEBATE\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5b9e\u8bc1\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14LLM\u5728\u6a21\u62df\u4eba\u7c7b\u610f\u89c1\u52a8\u6001\u65b9\u9762\u7684\u771f\u5b9e\u6027\uff0c\u5305\u542b29,417\u6761\u771f\u5b9e\u8fa9\u8bba\u5bf9\u8bdd\uff0c\u63ed\u793a\u4e86\u6a21\u62df\u4e0e\u771f\u5b9e\u7fa4\u4f53\u52a8\u6001\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709LLM\u89d2\u8272\u626e\u6f14\u8bbe\u7f6e\u5f80\u5f80\u4ea7\u751f\u4e0d\u81ea\u7136\u7684\u7fa4\u4f53\u52a8\u6001\uff08\u5982\u8fc7\u65e9\u6536\u655b\uff09\uff0c\u7f3a\u4e4f\u8861\u91cf\u771f\u5b9e\u4eba\u7c7b\u610f\u89c1\u8f68\u8ff9\u7684\u5b9e\u8bc1\u57fa\u51c6\uff0c\u9700\u8981\u89e3\u51b3\u6a21\u62df\u4eba\u7c7b\u793e\u4ea4\u4e92\u52a8\u7684\u771f\u5b9e\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efaDEBATE\u57fa\u51c6\uff0c\u5305\u542b29,417\u6761\u6765\u81ea2,792\u540d\u7f8e\u56fd\u53c2\u4e0e\u8005\u7684\u591a\u8f6e\u8fa9\u8bba\u5bf9\u8bdd\uff0c\u8986\u76d6107\u4e2a\u4e89\u8bae\u8bdd\u9898\uff0c\u540c\u65f6\u6536\u96c6\u516c\u5f00\u8868\u8fbe\u4fe1\u606f\u548c\u79c1\u4e0b\u62a5\u544a\u610f\u89c1\uff1b\u4f7f\u7528\u8be5\u57fa\u51c6\u7cfb\u7edf\u8bc4\u4f30LLM\u6a21\u62df\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5bf9\u9f50LLM\u4e0e\u4eba\u7c7b\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u6a21\u62df\u4e0e\u771f\u5b9e\u7fa4\u4f53\u52a8\u6001\u5b58\u5728\u5173\u952e\u5dee\u5f02\uff1b\u76d1\u7763\u5fae\u8c03\u5728\u8868\u9762\u6307\u6807\uff08\u5982ROUGE-L\u548c\u6d88\u606f\u957f\u5ea6\uff09\u4e0a\u53d6\u5f97\u6539\u8fdb\uff0c\u4f46\u5728\u66f4\u6df1\u5c42\u6b21\u7684\u8bed\u4e49\u5bf9\u9f50\uff08\u5982\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "conclusion": "\u89d2\u8272\u626e\u6f14LLM\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u6a21\u62df\u4eba\u7c7b\u793e\u4ea4\u52a8\u6001\u65b9\u9762\u65e2\u6709\u6f5c\u529b\u4e5f\u5b58\u5728\u5f53\u524d\u9650\u5236\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u8bed\u4e49\u5c42\u9762\u7684\u5bf9\u9f50\u3002", "topic": "agent analysis"}}
{"id": "2510.25160", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25160", "abs": "https://arxiv.org/abs/2510.25160", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "Model-Document Protocol for AI Search", "comment": "10 pages", "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.", "AI": {"tldr": "\u63d0\u51faModel-Document Protocol (MDP)\u6846\u67b6\uff0c\u91cd\u65b0\u5b9a\u4e49LLM\u4e0e\u6587\u6863\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5c06\u539f\u59cb\u6587\u6863\u8f6c\u5316\u4e3a\u4efb\u52a1\u7279\u5b9a\u7684\u3001\u53ef\u76f4\u63a5\u6d88\u8d39\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u5e76\u901a\u8fc7MDP-Agent\u5b9e\u73b0\u4ee3\u7406\u63a8\u7406\u3001\u8bb0\u5fc6\u57fa\u7840\u548c\u7ed3\u6784\u5316\u5229\u7528\u4e09\u79cd\u8def\u5f84\u3002", "motivation": "\u5f53\u524d\u68c0\u7d22\u65b9\u6cd5\u5c06\u6587\u6863\u4f5c\u4e3a\u539f\u59cb\u6587\u672c\u5904\u7406\uff0c\u8fd4\u56de\u539f\u59cb\u6bb5\u843d\uff0c\u5c06\u7247\u6bb5\u7ec4\u88c5\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u8d1f\u62c5\u7559\u7ed9LLM\u3002\u9700\u8981\u65b0\u7684\u68c0\u7d22\u8303\u5f0f\u6765\u5f25\u5408\u539f\u59cb\u6587\u6863\u4e0eLLM\u5c31\u7eea\u8f93\u5165\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "MDP\u6846\u67b6\u901a\u8fc7\u4e09\u79cd\u8def\u5f84\u8f6c\u6362\u975e\u7ed3\u6784\u5316\u6587\u6863\uff1a\u4ee3\u7406\u63a8\u7406\u5c06\u539f\u59cb\u8bc1\u636e\u6574\u7406\u6210\u8fde\u8d2f\u4e0a\u4e0b\u6587\uff1b\u8bb0\u5fc6\u57fa\u7840\u79ef\u7d2f\u53ef\u91cd\u7528\u7b14\u8bb0\u4ee5\u4e30\u5bcc\u63a8\u7406\uff1b\u7ed3\u6784\u5316\u5229\u7528\u5c06\u6587\u6863\u7f16\u7801\u4e3a\u56fe\u5f62\u6216\u952e\u503c\u7f13\u5b58\u7b49\u6b63\u5f0f\u8868\u793a\u3002MDP-Agent\u5b9e\u73b0\u8be5\u534f\u8bae\uff0c\u6784\u5efa\u6587\u6863\u7ea7\u8981\u70b9\u8bb0\u5fc6\u3001\u6267\u884c\u57fa\u4e8e\u6269\u6563\u7684\u63a2\u7d22\u4e0e\u5782\u76f4\u5229\u7528\u3001\u5e94\u7528map-reduce\u98ce\u683c\u5408\u6210\u3002", "result": "\u5728\u4fe1\u606f\u5bfb\u6c42\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMDP-Agent\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86MDP\u6846\u67b6\u7684\u5408\u7406\u6027\u548c\u5176\u4ee3\u7406\u5b9e\u4f8b\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "MDP\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u8303\u5f0f\uff0c\u786e\u4fddLLM\u63a5\u6536\u7684\u662f\u7d27\u51d1\u3001\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u800c\u975e\u539f\u59cb\u7247\u6bb5\uff0c\u76f4\u63a5\u53ef\u7528\u4e8e\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2510.25187", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25187", "abs": "https://arxiv.org/abs/2510.25187", "authors": ["Ritesh Sunil Chavan", "Jack Mostow"], "title": "Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction", "comment": null, "summary": "While large language models are trained on massive datasets, this data is\nheavily skewed towards English. Does their impressive performance reflect\ngenuine ability or just this data advantage? To find out, we tested them in a\nsetting where they could not rely on data abundance: low-resource languages.\nBuilding on prior work Agarwal et al. (2025) that used Next Sentence Prediction\n(NSP) as a test, we created a large-scale benchmark with 10,000 questions each\nfor English (a high-resource language), Swahili (medium-resource), and Hausa\n(low-resource). We then tested several top models, including GPT-4 Turbo,\nGemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The\nresults painted a clear picture of how levels of language resources impact\noutcomes. While all models excelled in English, their accuracy dropped in\nSwahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story\nbecame even more interesting when we introduced Chain-of-Thought (CoT)\nprompting. For the struggling LLaMA 3, CoT acted as a helpful guide,\nsignificantly boosting its accuracy. However, for the more capable GPT-4 and\nGemini, the same technique often backfired, leading to a kind of \"overthinking\"\nthat hurt their results in the cross-lingual context. This reveals that\nChain-of-Thought is not a universal solution; its effectiveness depends heavily\non the model's baseline capability and the specific context of the task. Our\nframework pinpoints LLM weaknesses, highlights when CoT helps or hinders\ncross-lingual NSP performance, and factors influencing their decisions.", "AI": {"tldr": "\u7814\u7a76\u6d4b\u8bd5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u968f\u8bed\u8a00\u8d44\u6e90\u6c34\u5e73\u4e0b\u964d\uff0c\u4e14Chain-of-Thought\u63d0\u793a\u5728\u4e0d\u540c\u80fd\u529b\u6a21\u578b\u4e0a\u6548\u679c\u4e0d\u540c\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5f02\u8868\u73b0\u662f\u6e90\u4e8e\u771f\u5b9e\u80fd\u529b\u8fd8\u662f\u82f1\u8bed\u6570\u636e\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u3002", "method": "\u57fa\u4e8eNext Sentence Prediction\u4efb\u52a1\uff0c\u6784\u5efa\u5305\u542b\u82f1\u8bed\u3001\u65af\u74e6\u5e0c\u91cc\u8bed\u548c\u8c6a\u8428\u8bed\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u6d4b\u8bd5GPT-4 Turbo\u3001Gemini 1.5 Flash\u548cLLaMA 3 70B\u7b49\u6a21\u578b\uff0c\u5e76\u5f15\u5165Chain-of-Thought\u63d0\u793a\u6280\u672f\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u82f1\u8bed\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u65af\u74e6\u5e0c\u91cc\u8bed\u4e0a\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u5728\u8c6a\u8428\u8bed\u4e0a\u5927\u5e45\u4e0b\u964d\uff0cLLaMA 3\u8868\u73b0\u6700\u5dee\u3002Chain-of-Thought\u5bf9\u80fd\u529b\u8f83\u5f31\u7684LLaMA 3\u6709\u5e2e\u52a9\uff0c\u4f46\u5bf9GPT-4\u548cGemini\u53cd\u800c\u9020\u6210\"\u8fc7\u5ea6\u601d\u8003\"\u800c\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "Chain-of-Thought\u4e0d\u662f\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u6a21\u578b\u7684\u57fa\u7840\u80fd\u529b\u548c\u4efb\u52a1\u5177\u4f53\u60c5\u5883\u3002\u7814\u7a76\u6846\u67b6\u80fd\u8bc6\u522bLLM\u5f31\u70b9\uff0c\u63ed\u793aCoT\u5728\u8de8\u8bed\u8a00NSP\u4e2d\u7684\u5e2e\u52a9\u6216\u963b\u788d\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.25224", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25224", "abs": "https://arxiv.org/abs/2510.25224", "authors": ["Ziyi Liu", "Bahar Sarrafzadeh", "Pei Zhou", "Longqi Yang", "Jieyu Zhao", "Ashish Sharma"], "title": "ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation", "comment": null, "summary": "While Large Language Models (LLMs) are increasingly used in agentic\nframeworks to assist individual users, there is a growing need for agents that\ncan proactively manage complex, multi-party collaboration. Systematic\nevaluation methods for such proactive agents remain scarce, limiting progress\nin developing AI that can effectively support multiple people together.\nNegotiation offers a demanding testbed for this challenge, requiring\nsocio-cognitive intelligence to navigate conflicting interests between multiple\nparticipants and multiple topics and build consensus. Here, we present\nProMediate, the first framework for evaluating proactive AI mediator agents in\ncomplex, multi-topic, multi-party negotiations. ProMediate consists of two core\ncomponents: (i) a simulation testbed based on realistic negotiation cases and\ntheory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and\nProMediate-Hard), with a plug-and-play proactive AI mediator grounded in\nsocio-cognitive mediation theories, capable of flexibly deciding when and how\nto intervene; and (ii) a socio-cognitive evaluation framework with a new suite\nof metrics to measure consensus changes, intervention latency, mediator\neffectiveness, and intelligence. Together, these components establish a\nsystematic framework for assessing the socio-cognitive intelligence of\nproactive AI agents in multi-party settings. Our results show that a socially\nintelligent mediator agent outperforms a generic baseline, via faster,\nbetter-targeted interventions. In the ProMediate-Hard setting, our social\nmediator increases consensus change by 3.6 percentage points compared to the\ngeneric baseline (10.65\\% vs 7.01\\%) while being 77\\% faster in response\n(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,\ntheory-grounded testbed to advance the development of proactive, socially\nintelligent agents.", "AI": {"tldr": "ProMediate\u662f\u9996\u4e2a\u8bc4\u4f30\u590d\u6742\u591a\u8bae\u9898\u591a\u65b9\u8c08\u5224\u4e2d\u4e3b\u52a8AI\u8c03\u89e3\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u771f\u5b9e\u8c08\u5224\u6848\u4f8b\u7684\u6a21\u62df\u6d4b\u8bd5\u5e73\u53f0\u548c\u8bc4\u4f30\u8c03\u89e3\u4ee3\u7406\u793e\u4f1a\u8ba4\u77e5\u667a\u80fd\u7684\u6307\u6807\u5957\u4ef6\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u4e3b\u52a8AI\u4ee3\u7406\u5728\u590d\u6742\u591a\u65b9\u534f\u4f5c\u4e2d\u8868\u73b0\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u5f00\u53d1\u80fd\u6709\u6548\u652f\u6301\u591a\u4eba\u534f\u4f5c\u7684AI\u7684\u8fdb\u5c55\u3002\u8c08\u5224\u4f5c\u4e3a\u9700\u8981\u793e\u4f1a\u8ba4\u77e5\u667a\u80fd\u7684\u6311\u6218\u6027\u6d4b\u8bd5\u5e73\u53f0\uff0c\u9002\u5408\u8bc4\u4f30\u8fd9\u7c7b\u4ee3\u7406\u3002", "method": "\u5f00\u53d1ProMediate\u6846\u67b6\uff0c\u5305\u542b\uff1a(i)\u57fa\u4e8e\u771f\u5b9e\u8c08\u5224\u6848\u4f8b\u548c\u7406\u8bba\u9a71\u52a8\u96be\u5ea6\u7ea7\u522b\u7684\u6a21\u62df\u6d4b\u8bd5\u5e73\u53f0\uff0c\u914d\u5907\u53ef\u63d2\u62d4\u7684\u4e3b\u52a8AI\u8c03\u89e3\u4ee3\u7406\uff1b(ii)\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u8861\u91cf\u5171\u8bc6\u53d8\u5316\u3001\u5e72\u9884\u5ef6\u8fdf\u3001\u8c03\u89e3\u6548\u679c\u548c\u667a\u80fd\u7684\u65b0\u6307\u6807\u5957\u4ef6\u3002", "result": "\u793e\u4f1a\u667a\u80fd\u8c03\u89e3\u4ee3\u7406\u4f18\u4e8e\u901a\u7528\u57fa\u7ebf\uff0c\u901a\u8fc7\u66f4\u5feb\u3001\u66f4\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u3002\u5728ProMediate-Hard\u8bbe\u7f6e\u4e2d\uff0c\u793e\u4f1a\u8c03\u89e3\u4ee3\u7406\u76f8\u6bd4\u901a\u7528\u57fa\u7ebf\u5c06\u5171\u8bc6\u53d8\u5316\u63d0\u9ad8\u4e863.6\u4e2a\u767e\u5206\u70b9(10.65% vs 7.01%)\uff0c\u540c\u65f6\u54cd\u5e94\u901f\u5ea6\u5feb77%(15.98s vs 3.71s)\u3002", "conclusion": "ProMediate\u4e3a\u63a8\u8fdb\u4e3b\u52a8\u3001\u793e\u4f1a\u667a\u80fd\u4ee3\u7406\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e25\u8c28\u3001\u7406\u8bba\u57fa\u7840\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "2510.25333", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25333", "abs": "https://arxiv.org/abs/2510.25333", "authors": ["Yilong Lai", "Yipin Yang", "Jialong Wu", "Fengran Mo", "Zhenglin Wang", "Ting Liang", "Jianguo Lin", "Keping Yang"], "title": "CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories", "comment": null, "summary": "Recent years have witnessed the rapid development of LLM-based agents, which\nshed light on using language agents to solve complex real-world problems. A\nprominent application lies in business agents, which interact with databases\nand internal knowledge bases via tool calls to fulfill diverse user\nrequirements. However, this domain is characterized by intricate data\nrelationships and a wide range of heterogeneous tasks, from statistical data\nqueries to knowledge-based question-answering. To address these challenges, we\npropose CRMWeaver, a novel approach that enhances business agents in such\ncomplex settings. To acclimate the agentic model to intricate business\nenvironments, we employ a synthesis data generation and RL-based paradigm\nduring training, which significantly improves the model's ability to handle\ncomplex data and varied tasks. During inference, a shared memories mechanism is\nintroduced, prompting the agent to learn from task guidelines in similar\nproblems, thereby further boosting its effectiveness and generalization,\nespecially in unseen scenarios. We validate the efficacy of our approach on the\nCRMArena-Pro dataset, where our lightweight model achieves competitive results\nin both B2B and B2C business scenarios, underscoring its practical value for\nreal-world applications.", "AI": {"tldr": "CRMWeaver\u662f\u4e00\u4e2a\u589e\u5f3a\u5546\u4e1a\u4ee3\u7406\u5904\u7406\u590d\u6742\u4e1a\u52a1\u573a\u666f\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u7ed3\u5408\u5171\u4eab\u8bb0\u5fc6\u673a\u5236\uff0c\u5728CRMArena-Pro\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3LLM\u4ee3\u7406\u5728\u590d\u6742\u5546\u4e1a\u73af\u5883\u4e2d\u5904\u7406\u5f02\u6784\u4efb\u52a1\u548c\u6570\u636e\u5173\u7cfb\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728B2B\u548cB2C\u4e1a\u52a1\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u63a8\u7406\u65f6\u5f15\u5165\u5171\u4eab\u8bb0\u5fc6\u673a\u5236\uff0c\u8ba9\u4ee3\u7406\u4ece\u7c7b\u4f3c\u95ee\u9898\u7684\u4efb\u52a1\u6307\u5357\u4e2d\u5b66\u4e60\u3002", "result": "\u5728CRMArena-Pro\u6570\u636e\u96c6\u4e0a\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728B2B\u548cB2C\u4e1a\u52a1\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "conclusion": "CRMWeaver\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5546\u4e1a\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2510.25311", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25311", "abs": "https://arxiv.org/abs/2510.25311", "authors": ["Sagalpreet Singh", "Rishi Saket", "Aravindan Raghuveer"], "title": "Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning", "comment": "21 pages, 5 figures", "summary": "Reinforcement Learning algorithms are primarily focused on learning a policy\nthat maximizes expected return. As a result, the learned policy can exploit one\nor few reward sources. However, in many natural situations, it is desirable to\nlearn a policy that induces a dispersed marginal state distribution over\nrewarding states, while maximizing the expected return which is typically tied\nto reaching a goal state. This aspect remains relatively unexplored. Existing\ntechniques based on entropy regularization and intrinsic rewards use\nstochasticity for encouraging exploration to find an optimal policy which may\nnot necessarily lead to dispersed marginal state distribution over rewarding\nstates. Other RL algorithms which match a target distribution assume the latter\nto be available apriori. This may be infeasible in large scale systems where\nenumeration of all states is not possible and a state is determined to be a\ngoal state only upon reaching it. We formalize the problem of maximizing the\nexpected return while uniformly visiting the goal states as Multi Goal RL in\nwhich an oracle classifier over the state space determines the goal states. We\npropose a novel algorithm that learns a high-return policy mixture with\nmarginal state distribution dispersed over the set of goal states. Our\nalgorithm is based on optimizing a custom RL reward which is computed - based\non the current policy mixture - at each iteration for a set of sampled\ntrajectories. The latter are used via an offline RL algorithm to update the\npolicy mixture. We prove performance guarantees for our algorithm, showing\nefficient convergence bounds for optimizing a natural objective which captures\nthe expected return as well as the dispersion of the marginal state\ndistribution over the goal states. We design and perform experiments on\nsynthetic MDPs and standard RL environments to evaluate the effectiveness of\nour algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u7b56\u7565\u5728\u76ee\u6807\u72b6\u6001\u4e0a\u7684\u8fb9\u9645\u72b6\u6001\u5206\u5e03\u66f4\u52a0\u5206\u6563\u5747\u5300\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u503e\u5411\u4e8e\u5229\u7528\u5c11\u6570\u5956\u52b1\u6e90\uff0c\u4f46\u5728\u8bb8\u591a\u81ea\u7136\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5b66\u4e60\u65e2\u80fd\u6700\u5927\u5316\u56de\u62a5\u53c8\u80fd\u5747\u5300\u8bbf\u95ee\u591a\u4e2a\u76ee\u6807\u72b6\u6001\u7684\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u71b5\u6b63\u5219\u5316\u548c\u5185\u5728\u5956\u52b1\u4e3b\u8981\u5173\u6ce8\u63a2\u7d22\u800c\u975e\u72b6\u6001\u5206\u5e03\u7684\u5206\u6563\u6027\u3002", "method": "\u63d0\u51fa\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u4e0a\u7684\u9884\u8a00\u673a\u5206\u7c7b\u5668\u786e\u5b9a\u76ee\u6807\u72b6\u6001\u3002\u901a\u8fc7\u4f18\u5316\u81ea\u5b9a\u4e49RL\u5956\u52b1\u51fd\u6570\uff0c\u57fa\u4e8e\u5f53\u524d\u7b56\u7565\u6df7\u5408\u8ba1\u7b97\u5956\u52b1\uff0c\u5229\u7528\u79bb\u7ebfRL\u7b97\u6cd5\u66f4\u65b0\u7b56\u7565\u6df7\u5408\u3002", "result": "\u7b97\u6cd5\u5728\u5408\u6210MDP\u548c\u6807\u51c6RL\u73af\u5883\u4e2d\u6709\u6548\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u9ad8\u56de\u62a5\u4e14\u76ee\u6807\u72b6\u6001\u5206\u5e03\u5206\u6563\u7684\u7b56\u7565\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5728\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\u7684\u540c\u65f6\u5b9e\u73b0\u76ee\u6807\u72b6\u6001\u5747\u5300\u8bbf\u95ee\u7684\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25536", "categories": ["cs.CL", "I.2.7; I.2.6; I.2.0"], "pdf": "https://arxiv.org/pdf/2510.25536", "abs": "https://arxiv.org/abs/2510.25536", "authors": ["Bangde Du", "Minghao Guo", "Songming He", "Ziyi Ye", "Xi Zhu", "Weihang Su", "Shuqi Zhu", "Yujia Zhou", "Yongfeng Zhang", "Qingyao Ai", "Yiqun Liu"], "title": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation", "comment": "Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde\n  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye\n  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)", "summary": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.", "AI": {"tldr": "TwinVoice\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u6a21\u62df\u4eba\u7269\u89d2\u8272\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u6db5\u76d6\u793e\u4ea4\u3001\u4eba\u9645\u548c\u53d9\u4e8b\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5206\u89e3\u4e3a\u516d\u9879\u57fa\u7840\u80fd\u529b\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4eba\u7269\u89d2\u8272\u6a21\u62df\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\uff1a\u5927\u591a\u4f9d\u8d56\u5408\u6210\u5bf9\u8bdd\u3001\u7f3a\u4e4f\u7cfb\u7edf\u6846\u67b6\u548c\u80fd\u529b\u9700\u6c42\u5206\u6790\u3002", "method": "\u5f15\u5165TwinVoice\u57fa\u51c6\uff0c\u5305\u542b\u793e\u4ea4\u89d2\u8272\u3001\u4eba\u9645\u89d2\u8272\u548c\u53d9\u4e8b\u89d2\u8272\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5206\u89e3\u4e3a\u89c2\u70b9\u4e00\u81f4\u6027\u3001\u8bb0\u5fc6\u56de\u5fc6\u3001\u903b\u8f91\u63a8\u7406\u3001\u8bcd\u6c47\u4fdd\u771f\u5ea6\u3001\u89d2\u8272\u8bed\u6c14\u548c\u53e5\u6cd5\u98ce\u683c\u516d\u9879\u80fd\u529b\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5148\u8fdb\u6a21\u578b\u5728\u4eba\u7269\u89d2\u8272\u6a21\u62df\u4e0a\u8fbe\u5230\u4e2d\u7b49\u51c6\u786e\u7387\uff0c\u4f46\u5728\u53e5\u6cd5\u98ce\u683c\u548c\u8bb0\u5fc6\u56de\u5fc6\u7b49\u80fd\u529b\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u5e73\u5747\u6027\u80fd\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\u3002", "conclusion": "\u867d\u7136LLM\u5728\u4eba\u7269\u89d2\u8272\u6a21\u62df\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u53e5\u6cd5\u98ce\u683c\u548c\u8bb0\u5fc6\u56de\u5fc6\u7b49\u5173\u952e\u80fd\u529b\u4e0a\u4ecd\u9700\u6539\u8fdb\uff0c\u4e0e\u4eba\u7c7b\u8868\u73b0\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2510.25595", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25595", "abs": "https://arxiv.org/abs/2510.25595", "authors": ["Run Peng", "Ziqiao Ma", "Amy Pang", "Sikai Li", "Zhang Xi-Jia", "Yingzhuo Yu", "Cristian-Paul Bara", "Joyce Chai"], "title": "Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry", "comment": "Workshop on Multi-Agent System @ ICML 2025", "summary": "While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6761\u4ef6\u4e0bLLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4efb\u52a1\u534f\u4f5c\uff0c\u901a\u8fc7\u6269\u5c55\u7231\u56e0\u65af\u5766\u8c1c\u9898\u4e3a\u684c\u9762\u6e38\u620f\uff0c\u63a2\u7d22\u667a\u80fd\u4f53\u5982\u4f55\u901a\u8fc7\u63a8\u7406\u3001\u6c9f\u901a\u548c\u884c\u52a8\u6765\u6ee1\u8db3\u7a7a\u95f4\u548c\u5173\u7cfb\u7ea6\u675f\u3002", "motivation": "\u867d\u7136LLM\u667a\u80fd\u4f53\u5728\u52a8\u4f5c\u89c4\u5212/\u751f\u6210\u65b9\u9762\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5b83\u4eec\u5728\u534f\u4f5c\u5b8c\u6210\u5171\u540c\u76ee\u6807\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6761\u4ef6\u4e0b\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u52a0\u9a8c\u8bc1\u5668\u6846\u67b6\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u914d\u5907\u5404\u79cd\u6c9f\u901a\u7b56\u7565\u548c\u73af\u5883\u9a8c\u8bc1\u4fe1\u53f7\uff0c\u5728\u6269\u5c55\u7684\u7231\u56e0\u65af\u5766\u8c1c\u9898\u684c\u9762\u6e38\u620f\u4e2d\u6d4b\u8bd5\u534f\u4f5c\u80fd\u529b\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u5bf9\u9f50\u6c9f\u901a\u7684\u91cd\u8981\u6027\uff0c\u65e0\u6c9f\u901a\u7684\u667a\u80fd\u4f53\u4e5f\u80fd\u83b7\u5f97\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u771f\u6b63\u7684\u89c4\u5219\u7406\u89e3\u548c\u4eba\u7c7b\u8bc4\u4f30\u8005\u7684\u4fe1\u4efb\u3002\u901a\u8fc7\u73af\u5883\u9a8c\u8bc1\u5668\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u7406\u89e3\u4efb\u52a1\u89c4\u5219\u548c\u5b8c\u6210\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "\u96c6\u6210\u73af\u5883\u9a8c\u8bc1\u5668\u53ef\u4ee5\u4fc3\u8fdbAI\u7cfb\u7edf\u4e2d\u66f4\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u7684\u534f\u4f5c\uff0c\u589e\u5f3a\u667a\u80fd\u4f53\u5bf9\u4efb\u52a1\u89c4\u5219\u7684\u7406\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.25682", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25682", "abs": "https://arxiv.org/abs/2510.25682", "authors": ["Jiani Zheng", "Zhiyang Teng", "Xiangtai Li", "Anran Wang", "Yu Tian", "Kunpeng Qiu", "Ye Tian", "Haochen Wang", "Zhuochen Wang"], "title": "PairUni: Pairwise Training for Unified Multimodal Language Models", "comment": null, "summary": "Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Code:\n\\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}", "AI": {"tldr": "\u63d0\u51fa\u4e86PairUni\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u91cd\u7ec4\u4e3a\u7406\u89e3-\u751f\u6210\u5bf9\u6765\u89e3\u51b3\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4f7f\u7528Pair-GPRO\u7b97\u6cd5\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u540c\u65f6\u5904\u7406\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u8fd9\u4e9b\u4efb\u52a1\u4f9d\u8d56\u5f02\u6784\u6570\u636e\u548c\u76d1\u7763\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u4f7f\u7528GPT-3\u589e\u5f3a\u6570\u636e\uff0c\u5c06\u7406\u89e3\u6837\u672c\u751f\u6210\u63cf\u8ff0\uff0c\u751f\u6210\u6837\u672c\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u5f62\u6210\u5bf9\u9f50\u5bf9\uff1b\u68c0\u7d22\u8bed\u4e49\u76f8\u5173\u7684\u7406\u89e3\u6837\u672c\u5f62\u6210\u68c0\u7d22\u5bf9\uff1b\u63d0\u51faPair-GPRO\u7b97\u6cd5\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u8bc4\u5206\u8c03\u8282\u4f18\u52bf\u51fd\u6570\u3002", "result": "\u5728Janus-Pro\u7b49\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u5e73\u8861\u7684\u6539\u8fdb\uff0c\u4f18\u4e8e\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PairUni\u6846\u67b6\u901a\u8fc7\u6570\u636e\u914d\u5bf9\u548c\u914d\u5bf9\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u4efb\u52a1\u5e73\u8861\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25726", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25726", "abs": "https://arxiv.org/abs/2510.25726", "authors": ["Junlong Li", "Wenshuo Zhao", "Jian Zhao", "Weihao Zeng", "Haoze Wu", "Xiaochen Wang", "Rui Ge", "Yuxuan Cao", "Yuzhen Huang", "Wei Liu", "Junteng Liu", "Zhaochen Su", "Yiyang Guo", "Fan Zhou", "Lueyang Zhang", "Juan Michelini", "Xingyao Wang", "Xiang Yue", "Shuyan Zhou", "Graham Neubig", "Junxian He"], "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution", "comment": "Website: https://toolathlon.xyz/", "summary": "Real-world language agents must handle complex, multi-step workflows across\ndiverse Apps. For instance, an agent may manage emails by coordinating with\ncalendars and file systems, or monitor a production database to detect\nanomalies and generate reports following an operating manual. However, existing\nlanguage agent benchmarks often focus on narrow domains or simplified tasks\nthat lack the diversity, realism, and long-horizon complexity required to\nevaluate agents' real-world performance. To address this gap, we introduce the\nTool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering\ndiverse Apps and tools, realistic environment setup, and reliable\nexecution-based evaluation. Toolathlon spans 32 software applications and 604\ntools, ranging from everyday platforms such as Google Calendar and Notion to\nprofessional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools\nare based on a high-quality set of Model Context Protocol (MCP) servers that we\nmay have revised or implemented ourselves. Unlike prior works, which primarily\nensure functional realism but offer limited environment state diversity, we\nprovide realistic initial environment states from real software, such as Canvas\ncourses with dozens of students or real financial spreadsheets. This benchmark\nincludes 108 manually sourced or crafted tasks in total, requiring interacting\nwith multiple Apps over around 20 turns on average to complete. Each task is\nstrictly verifiable through dedicated evaluation scripts. Comprehensive\nevaluation of SOTA models highlights their significant shortcomings: the\nbest-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate\nwith 20.2 tool calling turns on average, while the top open-weights model\nDeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development\nof more capable language agents for real-world, long-horizon task execution.", "AI": {"tldr": "Toolathlon\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d632\u4e2a\u8f6f\u4ef6\u5e94\u7528\u548c604\u4e2a\u5de5\u5177\uff0c\u63d0\u4f9b\u591a\u6837\u5316\u7684\u5e94\u7528\u3001\u771f\u5b9e\u73af\u5883\u8bbe\u7f6e\u548c\u53ef\u9760\u6267\u884c\u8bc4\u4f30\uff0c\u63ed\u793a\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u4e13\u6ce8\u4e8e\u72ed\u7a84\u9886\u57df\u6216\u7b80\u5316\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5904\u7406\u590d\u6742\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u6240\u9700\u7684\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\u548c\u957f\u671f\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165Tool Decathlon\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u9ad8\u8d28\u91cfMCP\u670d\u52a1\u5668\u96c6\uff0c\u63d0\u4f9b\u6765\u81ea\u771f\u5b9e\u8f6f\u4ef6\u7684\u521d\u59cb\u73af\u5883\u72b6\u6001\uff0c\u5305\u542b108\u4e2a\u624b\u52a8\u6536\u96c6\u6216\u8bbe\u8ba1\u7684\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5e73\u5747\u9700\u8981\u7ea620\u6b21\u4ea4\u4e92\u624d\u80fd\u5b8c\u6210\u3002", "result": "\u8bc4\u4f30\u663e\u793aSOTA\u6a21\u578b\u8868\u73b0\u663e\u8457\u4e0d\u8db3\uff1a\u6700\u4f73\u6a21\u578bClaude-4.5-Sonnet\u6210\u529f\u7387\u4ec538.6%\uff0c\u5e73\u5747\u9700\u898120.2\u6b21\u5de5\u5177\u8c03\u7528\uff1b\u6700\u4f73\u5f00\u6e90\u6a21\u578bDeepSeek-V3.2-Exp\u6210\u529f\u7387\u4ec520.1%\u3002", "conclusion": "Toolathlon\u6709\u671b\u63a8\u52a8\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u4ee3\u7406\uff0c\u4ee5\u6267\u884c\u73b0\u5b9e\u4e16\u754c\u4e2d\u957f\u671f\u590d\u6742\u7684\u4efb\u52a1\u3002", "topic": "swe benchmark"}}
{"id": "2510.25744", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25744", "abs": "https://arxiv.org/abs/2510.25744", "authors": ["Shannon Zejiang Shen", "Valerie Chen", "Ken Gu", "Alexis Ross", "Zixian Ma", "Jillian Ross", "Alex Gu", "Chenglei Si", "Wayne Chi", "Andi Peng", "Jocelyn J Shen", "Ameet Talwalkar", "Tongshuang Wu", "David Sontag"], "title": "Task Completion Agents are Not Ideal Collaborators", "comment": "22 pages, 5 figures, 3 tables", "summary": "Current evaluations of agents remain centered around one-shot task\ncompletion, failing to account for the inherently iterative and collaborative\nnature of many real-world problems, where human goals are often underspecified\nand evolve. We argue for a shift from building and assessing task completion\nagents to developing collaborative agents, assessed not only by the quality of\ntheir final outputs but by how well they engage with and enhance human effort\nthroughout the problem-solving process. To support this shift, we introduce\ncollaborative effort scaling, a framework that captures how an agent's utility\ngrows with increasing user involvement. Through case studies and simulated\nevaluations, we show that state-of-the-art agents often underperform in\nmulti-turn, real-world scenarios, revealing a missing ingredient in agent\ndesign: the ability to sustain engagement and scaffold user understanding.\nCollaborative effort scaling offers a lens for diagnosing agent behavior and\nguiding development toward more effective interactions.", "AI": {"tldr": "\u63d0\u51fa\u4ece\u4e00\u6b21\u6027\u4efb\u52a1\u5b8c\u6210\u8bc4\u4f30\u8f6c\u5411\u534f\u4f5c\u4ee3\u7406\u8bc4\u4f30\uff0c\u5f15\u5165\u534f\u4f5c\u52aa\u529b\u6269\u5c55\u6846\u67b6\u6765\u8861\u91cf\u4ee3\u7406\u5728\u7528\u6237\u53c2\u4e0e\u589e\u52a0\u65f6\u7684\u6548\u7528\u589e\u957f\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u8bc4\u4f30\u96c6\u4e2d\u4e8e\u4e00\u6b21\u6027\u4efb\u52a1\u5b8c\u6210\uff0c\u672a\u80fd\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u7684\u8fed\u4ee3\u548c\u534f\u4f5c\u6027\u8d28\uff0c\u5176\u4e2d\u4eba\u7c7b\u76ee\u6807\u901a\u5e38\u4e0d\u660e\u786e\u4e14\u4e0d\u65ad\u6f14\u53d8\u3002", "method": "\u5f15\u5165\u534f\u4f5c\u52aa\u529b\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u6a21\u62df\u8bc4\u4f30\u5206\u6790\u4ee3\u7406\u5728\u591a\u8f6e\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u5728\u591a\u8f6e\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u7ef4\u6301\u53c2\u4e0e\u548c\u6784\u5efa\u7528\u6237\u7406\u89e3\u7684\u80fd\u529b\u3002", "conclusion": "\u534f\u4f5c\u52aa\u529b\u6269\u5c55\u4e3a\u8bca\u65ad\u4ee3\u7406\u884c\u4e3a\u548c\u6307\u5bfc\u5f00\u53d1\u66f4\u6709\u6548\u4ea4\u4e92\u63d0\u4f9b\u4e86\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2510.25616", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25616", "abs": "https://arxiv.org/abs/2510.25616", "authors": ["Nikita Kachaev", "Mikhail Kolosov", "Daniil Zelezetsky", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization", "comment": "13 pages, 6 figures", "summary": "The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u5728\u52a8\u4f5c\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u89c6\u89c9\u8868\u793a\u9000\u5316\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u79cd\u9000\u5316\u5e76\u63d0\u9ad8OOD\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76VLA\u6a21\u578b\u5728\u52a8\u4f5c\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0c\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u539f\u6709\u7684\u89c6\u89c9\u8868\u793a\u548c\u77e5\u8bc6\u4fdd\u7559\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u5982\u4f55\u7f13\u89e3\u52a8\u4f5c\u5fae\u8c03\u5bfc\u81f4\u7684\u89c6\u89c9\u8868\u793a\u9000\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63a2\u6d4bVLA\u6a21\u578b\u7684\u9690\u85cf\u8868\u793a\u548c\u5206\u6790\u6ce8\u610f\u529b\u56fe\uff0c\u8bbe\u8ba1\u9488\u5bf9\u6027\u4efb\u52a1\u5bf9\u6bd4VLA\u6a21\u578b\u4e0e\u5bf9\u5e94VLMs\uff0c\u8bc4\u4f30\u591a\u79cd\u89c6\u89c9\u8868\u793a\u5bf9\u9f50\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u7b80\u5355\u6709\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u6734\u7d20\u7684\u52a8\u4f5c\u5fae\u8c03\u4f1a\u5bfc\u81f4\u89c6\u89c9\u8868\u793a\u9000\u5316\uff0c\u63d0\u51fa\u7684\u5bf9\u9f50\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u8fd9\u79cd\u9000\u5316\u5e76\u6539\u5584OOD\u573a\u666f\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u9610\u660e\u4e86\u52a8\u4f5c\u5fae\u8c03\u4e0e\u89c6\u89c9\u8868\u793a\u9000\u5316\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e86\u6062\u590d\u7ee7\u627fVL\u80fd\u529b\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.25766", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25766", "abs": "https://arxiv.org/abs/2510.25766", "authors": ["Sriram Balasubramaniam", "Samyadeep Basu", "Koustava Goswami", "Ryan Rossi", "Varun Manjunatha", "Roshan Santhosh", "Ruiyi Zhang", "Soheil Feizi", "Nedim Lipka"], "title": "Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models", "comment": "Post-hoc attribution", "summary": "Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models.", "AI": {"tldr": "DecompTune\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7b54\u6848\u5206\u89e3\u4e3a\u53ef\u5f52\u56e0\u7684\u7ec4\u6210\u5355\u5143\u6765\u6539\u8fdb\u957f\u6587\u6863\u95ee\u7b54\u4e2d\u7684\u5f52\u56e0\u8d28\u91cf\uff0c\u5728\u590d\u6742QA\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u540e\u5f52\u56e0\u65b9\u6cd5\u5728\u63d0\u53d6\u5f0f\u95ee\u7b54\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8df3\u3001\u62bd\u8c61\u548c\u534a\u63d0\u53d6\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e9b\u573a\u666f\u4e2d\u7b54\u6848\u9700\u8981\u8de8\u6bb5\u843d\u5408\u6210\u4fe1\u606f\u3002", "method": "\u5c06\u4e8b\u540e\u5f52\u56e0\u91cd\u65b0\u5b9a\u4e49\u4e3a\u63a8\u7406\u95ee\u9898\uff0c\u901a\u8fc7DecompTune\u540e\u8bad\u7ec3\u65b9\u6cd5\u6559\u5bfc\u6a21\u578b\u751f\u6210\u7b54\u6848\u5206\u89e3\u4f5c\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5SFT + GRPO\u6d41\u6c34\u7ebf\u914d\u5408\u4efb\u52a1\u7279\u5b9a\u7684\u5b9a\u5236\u5956\u52b1\u3002", "result": "DecompTune\u663e\u8457\u63d0\u9ad8\u4e86\u5f52\u56e0\u8d28\u91cf\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u8fb9\u754c\u6a21\u578b\u3002", "conclusion": "\u5c06\u5f52\u56e0\u91cd\u6784\u4e3a\u63a8\u7406\u95ee\u9898\u5e76\u901a\u8fc7\u5206\u89e3\u7b54\u6848\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u590d\u6742\u95ee\u7b54\u573a\u666f\u4e2d\u7684\u5f52\u56e0\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.9eae7f5e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fthe-coding-personalities-of-leading-llms%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-state-of-llms25%26utm_content=newsletter-ai-primary-expandedllm-251028-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/m8NWlNgble9MPWzY0Ic9Q9yfm2A3-AAYu-OUML2hphc=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fthe-coding-personalities-of-leading-llms%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-state-of-llms25%26utm_content=newsletter-ai-primary-expandedllm-251028-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/m8NWlNgble9MPWzY0Ic9Q9yfm2A3-AAYu-OUML2hphc=429", "authors": ["TLDR Newsletter"], "title": "The Coding Personalities of Leading LLMs", "comment": "Source: TLDR Newsletter, Date: 2025-10-28, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fthe-coding-personalities-of-leading-llms%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-state-of-llms25%26utm_content=newsletter-ai-primary-expandedllm-251028-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/m8NWlNgble9MPWzY0Ic9Q9yfm2A3-AAYu-OUML2hphc=429", "summary": "The Coding Personalities of Leading LLMs (Sponsor) Think the newest LLM is the best for coding? Despite their shared strengths and flaws, each LLM has a unique and inherent style\u2014a measurable \u201ccoding personality\u201d that drives their distinct results.Sonar put GPT-5, Claude Sonnet 4, and Llama 3 models to the test in their latest State of Code report - and found some surprising results. Read it to discover: The shared strengths and flaws of LLMs Coding archetypes for the leading LLMs Hidden qual...", "source": "tldr", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86GPT-5\u3001Claude Sonnet 4\u548cLlama 3\u7b49\u9886\u5148LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u72ec\u7279\"\u7f16\u7801\u4e2a\u6027\"\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5b83\u4eec\u6709\u5171\u540c\u7684\u4f18\u7f3a\u70b9\uff0c\u4f46\u6bcf\u4e2a\u6a21\u578b\u90fd\u6709\u53ef\u6d4b\u91cf\u7684\u72ec\u7279\u7f16\u7801\u98ce\u683c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u4e0d\u540cLLM\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7684\u72ec\u7279\u7f16\u7801\u98ce\u683c\u548c\u4e2a\u6027\u7279\u5f81\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u9009\u62e9\u9002\u5408\u7279\u5b9a\u9700\u6c42\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7Sonar\u7684State of Code\u62a5\u544a\uff0c\u5bf9GPT-5\u3001Claude Sonnet 4\u548cLlama 3\u7b49\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\u5206\u6790\uff0c\u8bc6\u522b\u5b83\u4eec\u7684\u7f16\u7801\u4e2a\u6027\u548c\u8868\u73b0\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u4e86LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5171\u4eab\u4f18\u52bf\u548c\u7f3a\u9677\uff0c\u540c\u65f6\u8bc6\u522b\u51fa\u6bcf\u4e2a\u6a21\u578b\u72ec\u7279\u7684\u7f16\u7801\u539f\u578b\u548c\u4e2a\u6027\u7279\u5f81\uff0c\u7ed3\u679c\u4ee4\u4eba\u60ca\u8bb6\u3002", "conclusion": "\u6bcf\u4e2a\u9886\u5148\u7684LLM\u90fd\u6709\u5176\u72ec\u7279\u7684\u7f16\u7801\u4e2a\u6027\uff0c\u8fd9\u4e9b\u4e2a\u6027\u7279\u5f81\u4f1a\u5f71\u54cd\u5b83\u4eec\u7684\u4ee3\u7801\u751f\u6210\u7ed3\u679c\uff0c\u5f00\u53d1\u8005\u9700\u8981\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.c1a19d51", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsidb.in%2Fposts%2Frl-env-speedrun%3Futm_source=tldrai/1/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/R_U1Ws4hhsJguo-nxH9xrlOpj6D6SUGDOT0SplRMIJM=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsidb.in%2Fposts%2Frl-env-speedrun%3Futm_source=tldrai/1/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/R_U1Ws4hhsJguo-nxH9xrlOpj6D6SUGDOT0SplRMIJM=429", "authors": ["TLDR Newsletter"], "title": "Speedrunning an RL environment", "comment": "Source: TLDR Newsletter, Date: 2025-10-28, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsidb.in%2Fposts%2Frl-env-speedrun%3Futm_source=tldrai/1/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/R_U1Ws4hhsJguo-nxH9xrlOpj6D6SUGDOT0SplRMIJM=429", "summary": "Speedrunning an RL environment (22 minute read) RL environments can be surprisingly complex and fun to create. This post explains what RL environments are, introduces the 'verifiers' framework, and also walks readers through how to create an environment for a benchmark called AgentDojo. RL environments are scenarios that LLMs operate in for evaluation or training. Designing them means essentially defining the maze, the rewards, and how the LLM navigates through it.", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecdRL\u73af\u5883\u7684\u6982\u5ff5\u3001'verifiers'\u6846\u67b6\uff0c\u5e76\u6307\u5bfc\u5982\u4f55\u4e3aAgentDojo\u57fa\u51c6\u521b\u5efaRL\u73af\u5883", "motivation": "RL\u73af\u5883\u53ef\u4ee5\u590d\u6742\u4e14\u6709\u8da3\uff0c\u9700\u8981\u89e3\u91ca\u5176\u6982\u5ff5\u5e76\u5c55\u793a\u5982\u4f55\u521b\u5efa\u73af\u5883\u7528\u4e8eLLM\u8bc4\u4f30\u548c\u8bad\u7ec3", "method": "\u4f7f\u7528'verifiers'\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u8ff7\u5bab\u3001\u5956\u52b1\u548cLLM\u5bfc\u822a\u65b9\u5f0f\u6765\u8bbe\u8ba1RL\u73af\u5883", "result": "\u6210\u529f\u521b\u5efa\u4e86AgentDojo\u57fa\u51c6\u7684RL\u73af\u5883\uff0c\u5c55\u793a\u4e86\u73af\u5883\u8bbe\u8ba1\u8fc7\u7a0b", "conclusion": "RL\u73af\u5883\u8bbe\u8ba1\u662fLLM\u8bc4\u4f30\u548c\u8bad\u7ec3\u7684\u91cd\u8981\u73af\u8282\uff0c\u9700\u8981\u4ed4\u7ec6\u5b9a\u4e49\u573a\u666f\u3001\u5956\u52b1\u673a\u5236\u548c\u5bfc\u822a\u65b9\u5f0f", "topic": "agent analysis"}}
{"id": "tldr.2510.dc456dbf", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthinkingmachines.ai%2Fblog%2Fon-policy-distillation%2F%3Futm_source=tldrai/1/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/4-g42oOsFqCsv58sSAOALA6PZlVwq6Trgf6UjR1IVsQ=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthinkingmachines.ai%2Fblog%2Fon-policy-distillation%2F%3Futm_source=tldrai/1/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/4-g42oOsFqCsv58sSAOALA6PZlVwq6Trgf6UjR1IVsQ=429", "authors": ["TLDR Newsletter"], "title": "On-Policy Distillation", "comment": "Source: TLDR Newsletter, Date: 2025-10-28, Reading time: 25 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthinkingmachines.ai%2Fblog%2Fon-policy-distillation%2F%3Futm_source=tldrai/1/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/4-g42oOsFqCsv58sSAOALA6PZlVwq6Trgf6UjR1IVsQ=429", "summary": "On-Policy Distillation (25 minute read) Thinking Machines Lab demonstrated that training smaller AI models by having them learn from their own mistakes, by grading them with a larger teacher model, achieves the same reasoning performance as RL at 9-30x lower cost. \"On-policy distillation\" combines the relevance of learning from your own outputs with the dense feedback of traditional distillation, reaching 70% on AIME'24 math problems in just 150 training steps compared to 17,920 GPU hours of RL.", "source": "tldr", "AI": {"tldr": "\u8bad\u7ec3\u5c0f\u578bAI\u6a21\u578b\u901a\u8fc7\u8ba9\u5b83\u4eec\u4ece\u81ea\u8eab\u9519\u8bef\u4e2d\u5b66\u4e60\uff0c\u4f7f\u7528\u66f4\u5927\u7684\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u8bc4\u5206\uff0c\u53ef\u4ee5\u8fbe\u5230\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u540c\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u6210\u672c\u964d\u4f4e9-30\u500d\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6210\u672c\u6781\u9ad8\uff08\u9700\u898117,920 GPU\u5c0f\u65f6\uff09\uff0c\u800c\u4f20\u7edf\u84b8\u998f\u65b9\u6cd5\u7f3a\u4e4f\u76f8\u5173\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5b66\u4e60\u76f8\u5173\u6027\u53c8\u80fd\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\"\u7b56\u7565\u84b8\u998f\"\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4ece\u81ea\u8eab\u8f93\u51fa\u5b66\u4e60\u7684\u76f8\u5173\u6027\u548c\u4f20\u7edf\u84b8\u998f\u7684\u5bc6\u96c6\u53cd\u9988\u3002\u901a\u8fc7\u8ba9\u5c0f\u578b\u6a21\u578b\u4ece\u81ea\u8eab\u9519\u8bef\u4e2d\u5b66\u4e60\uff0c\u5e76\u7528\u5927\u578b\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5728AIME'24\u6570\u5b66\u95ee\u9898\u4e0a\u8fbe\u523070%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u9700150\u4e2a\u8bad\u7ec3\u6b65\u9aa4\uff0c\u76f8\u6bd4\u5f3a\u5316\u5b66\u4e60\u768417,920 GPU\u5c0f\u65f6\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "\u7b56\u7565\u84b8\u998f\u65b9\u6cd5\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u8bad\u7ec3\u6210\u672c\u964d\u4f4e\u4e869-30\u500d\uff0c\u8bc1\u660e\u4e86\u4ece\u81ea\u8eab\u9519\u8bef\u4e2d\u5b66\u4e60\u7ed3\u5408\u6559\u5e08\u6a21\u578b\u53cd\u9988\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.42600bc3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftombedor.dev%2Foptimizing-repos-for-ai%2F%3Futm_source=tldrnewsletter/1/0100019a2f7df7a9-2f628b0d-9852-41c9-a592-9eb728b57933-000000/ErzRUfTscv4edsncW6jkj2wpKvM2Ez_XIRDtMpPmtMA=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftombedor.dev%2Foptimizing-repos-for-ai%2F%3Futm_source=tldrnewsletter/1/0100019a2f7df7a9-2f628b0d-9852-41c9-a592-9eb728b57933-000000/ErzRUfTscv4edsncW6jkj2wpKvM2Ez_XIRDtMpPmtMA=429", "authors": ["TLDR Newsletter"], "title": "Optimizing repos for AI", "comment": "Source: TLDR Newsletter, Date: 2025-10-29, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftombedor.dev%2Foptimizing-repos-for-ai%2F%3Futm_source=tldrnewsletter/1/0100019a2f7df7a9-2f628b0d-9852-41c9-a592-9eb728b57933-000000/ErzRUfTscv4edsncW6jkj2wpKvM2Ez_XIRDtMpPmtMA=429", "summary": "Optimizing repos for AI (4 minute read) Optimize repositories for AI by increasing static analysis, using 'just' for repeated agent commands, and organizing documents in a folder and referencing them in agent instructions.", "source": "tldr", "AI": {"tldr": "\u63d0\u51fa\u4f18\u5316\u4ee3\u7801\u4ed3\u5e93\u4ee5\u63d0\u5347AI\u4ee3\u7406\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u589e\u5f3a\u9759\u6001\u5206\u6790\u3001\u4f7f\u7528'just'\u5de5\u5177\u7ba1\u7406\u91cd\u590d\u547d\u4ee4\u3001\u4ee5\u53ca\u4f18\u5316\u6587\u6863\u7ec4\u7ec7\u7ed3\u6784", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5728\u5904\u7406\u4ee3\u7801\u4ed3\u5e93\u65f6\u6548\u7387\u4e0d\u9ad8\uff0c\u9700\u8981\u901a\u8fc7\u4f18\u5316\u4ed3\u5e93\u7ed3\u6784\u6765\u63d0\u5347AI\u7684\u7406\u89e3\u548c\u6267\u884c\u6548\u7387", "method": "\u91c7\u7528\u4e09\u79cd\u65b9\u6cd5\uff1a\u589e\u52a0\u9759\u6001\u5206\u6790\u3001\u4f7f\u7528'just'\u5de5\u5177\u7b80\u5316\u91cd\u590d\u547d\u4ee4\u3001\u5408\u7406\u7ec4\u7ec7\u6587\u6863\u7ed3\u6784\u5e76\u5728\u6307\u4ee4\u4e2d\u5f15\u7528", "result": "\u901a\u8fc7\u4e0a\u8ff0\u4f18\u5316\u63aa\u65bd\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347AI\u4ee3\u7406\u5728\u4ee3\u7801\u4ed3\u5e93\u4e2d\u7684\u5de5\u4f5c\u6548\u7387\u548c\u51c6\u786e\u6027", "conclusion": "\u4f18\u5316\u4ee3\u7801\u4ed3\u5e93\u7ed3\u6784\u5bf9\u4e8e\u63d0\u5347AI\u4ee3\u7406\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5efa\u8bae\u5f00\u53d1\u8005\u91c7\u7528\u8fd9\u4e9b\u6700\u4f73\u5b9e\u8df5", "topic": "swe application"}}
{"id": "tldr.2510.fea290c5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinfowler.com%2Farticles%2Fagentic-ai-security.html%3Futm_source=tldrwebdev/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/tn7zh2ekv4RCUqlNgkvDxqbaJP3uIvQJfa7eEOmITEw=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinfowler.com%2Farticles%2Fagentic-ai-security.html%3Futm_source=tldrwebdev/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/tn7zh2ekv4RCUqlNgkvDxqbaJP3uIvQJfa7eEOmITEw=429", "authors": ["TLDR Newsletter"], "title": "Agentic AI and Security", "comment": "Source: TLDR Newsletter, Date: 2025-10-29, Reading time: 24 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinfowler.com%2Farticles%2Fagentic-ai-security.html%3Futm_source=tldrwebdev/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/tn7zh2ekv4RCUqlNgkvDxqbaJP3uIvQJfa7eEOmITEw=429", "summary": "Agentic AI and Security (24 minute read) Agentic AI systems have a fundamental security flaw: LLMs cannot separate instructions from data, making them vulnerable to prompt injection attacks where untrusted content contains hidden commands. The worst security occurs when an AI has access to sensitive data, reads untrusted content, and can communicate externally, allowing attackers to steal information by embedding instructions in sources like Jira tickets or web pages. To prevent this, it's be...", "source": "tldr", "AI": {"tldr": "Agentic AI\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u6027\u5b89\u5168\u6f0f\u6d1e\uff1aLLM\u65e0\u6cd5\u533a\u5206\u6307\u4ee4\u548c\u6570\u636e\uff0c\u5bb9\u6613\u53d7\u5230\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u5728\u4e0d\u53d7\u4fe1\u4efb\u5185\u5bb9\u4e2d\u5d4c\u5165\u9690\u85cf\u547d\u4ee4\u6765\u7a83\u53d6\u654f\u611f\u4fe1\u606f\u3002", "motivation": "\u968f\u7740Agentic AI\u7cfb\u7edf\u5728\u654f\u611f\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u5176\u5b89\u5168\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5b89\u5168\u9632\u62a4\u65b9\u6848\u3002", "method": "\u5206\u6790Agentic AI\u7cfb\u7edf\u7684\u5b89\u5168\u5a01\u80c1\u6a21\u578b\uff0c\u8bc6\u522b\u5f53AI\u80fd\u591f\u8bbf\u95ee\u654f\u611f\u6570\u636e\u3001\u8bfb\u53d6\u4e0d\u53d7\u4fe1\u4efb\u5185\u5bb9\u5e76\u80fd\u5916\u90e8\u901a\u4fe1\u65f6\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u9632\u62a4\u63aa\u65bd\u3002", "result": "\u8bc6\u522b\u51faAgentic AI\u7cfb\u7edf\u5728\u7279\u5b9a\u914d\u7f6e\u4e0b\uff08\u8bbf\u95ee\u654f\u611f\u6570\u636e+\u8bfb\u53d6\u4e0d\u53d7\u4fe1\u4efb\u5185\u5bb9+\u5916\u90e8\u901a\u4fe1\u80fd\u529b\uff09\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u7a83\u53d6\u4fe1\u606f\u3002", "conclusion": "Agentic AI\u7cfb\u7edf\u9700\u8981\u5efa\u7acb\u4e25\u683c\u7684\u5b89\u5168\u8fb9\u754c\u548c\u8bbf\u95ee\u63a7\u5236\u673a\u5236\uff0c\u9632\u6b62\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5bfc\u81f4\u7684\u6570\u636e\u6cc4\u9732\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.05571037", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/uMlV11XeMRvHGAvnT9SJ0dXWRcAG_sNr8xg3Xg8roSA=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/uMlV11XeMRvHGAvnT9SJ0dXWRcAG_sNr8xg3Xg8roSA=429", "authors": ["TLDR Newsletter"], "title": "Sentry's AI Code Review predicts what's going to break - based on what's already broken", "comment": "Source: TLDR Newsletter, Date: 2025-10-29, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/uMlV11XeMRvHGAvnT9SJ0dXWRcAG_sNr8xg3Xg8roSA=429", "summary": "Sentry's AI Code Review predicts what's going to break - based on what's already broken (Sponsor) Code reviews should be less style nits and more \"this is going to break prod\". Sentry's AI Code Review blends context and issue history with the code you just touched - function calls, class or objects dependencies, database connections - to provide specific and actionable feedback rather than generic linting advice. Read the blog", "source": "tldr", "AI": {"tldr": "Sentry\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u901a\u8fc7\u5206\u6790\u4ee3\u7801\u53d8\u66f4\u3001\u4e0a\u4e0b\u6587\u548c\u95ee\u9898\u5386\u53f2\u6765\u9884\u6d4b\u53ef\u80fd\u5bfc\u81f4\u751f\u4ea7\u73af\u5883\u6545\u969c\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5177\u4f53\u53ef\u884c\u7684\u53cd\u9988\u800c\u975e\u901a\u7528\u4ee3\u7801\u98ce\u683c\u5efa\u8bae\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u8fc7\u4e8e\u5173\u6ce8\u4ee3\u7801\u98ce\u683c\u7ec6\u8282\uff0c\u800c\u7f3a\u4e4f\u5bf9\u53ef\u80fd\u5bfc\u81f4\u751f\u4ea7\u73af\u5883\u6545\u969c\u7684\u5b9e\u9645\u95ee\u9898\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u4ee3\u7801\u53d8\u66f4\u3001\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u95ee\u9898\u5386\u53f2\uff0c\u5206\u6790\u51fd\u6570\u8c03\u7528\u3001\u7c7b\u6216\u5bf9\u8c61\u4f9d\u8d56\u5173\u7cfb\u3001\u6570\u636e\u5e93\u8fde\u63a5\u7b49\uff0c\u8bc6\u522b\u6f5c\u5728\u98ce\u9669\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u63d0\u4f9b\u5177\u4f53\u3001\u53ef\u64cd\u4f5c\u53cd\u9988\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u9884\u9632\u751f\u4ea7\u73af\u5883\u6545\u969c\u3002", "conclusion": "AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bc6\u522b\u548c\u9884\u9632\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\uff0c\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "tldr.2510.8445b020", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fben.page%2Fclaude-code-web%3Futm_source=tldrwebdev/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/kVbV05tu3Rlo6jmAuYXWKD3te8LECgcApGVtisYgzx4=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fben.page%2Fclaude-code-web%3Futm_source=tldrwebdev/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/kVbV05tu3Rlo6jmAuYXWKD3te8LECgcApGVtisYgzx4=429", "authors": ["TLDR Newsletter"], "title": "I've been loving Claude Code on the Web", "comment": "Source: TLDR Newsletter, Date: 2025-10-29, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fben.page%2Fclaude-code-web%3Futm_source=tldrwebdev/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/kVbV05tu3Rlo6jmAuYXWKD3te8LECgcApGVtisYgzx4=429", "summary": "I've been loving Claude Code on the Web (2 minute read) Claude Code on the web and its iOS app have a solid and dependable implementation as a \"to-do list that does itself.\u201d", "source": "tldr", "AI": {"tldr": "Claude Code\u5728\u7f51\u9875\u548ciOS\u5e94\u7528\u4e0a\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\"\u81ea\u52a8\u5b8c\u6210\u5f85\u529e\u4e8b\u9879\"\u529f\u80fd", "motivation": "\u63d0\u4f9b\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u5b8c\u6210\u5f85\u529e\u4e8b\u9879\u7684\u667a\u80fd\u52a9\u624b\u5de5\u5177\uff0c\u63d0\u5347\u7528\u6237\u751f\u4ea7\u529b", "method": "\u5728\u7f51\u9875\u548ciOS\u5e73\u53f0\u4e0a\u5f00\u53d1Claude Code\u5e94\u7528\uff0c\u5b9e\u73b0\u5f85\u529e\u4e8b\u9879\u7684\u81ea\u52a8\u5904\u7406\u529f\u80fd", "result": "Claude Code\u5728\u7f51\u9875\u548ciOS\u5e94\u7528\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u53ef\u9760\u5730\u81ea\u52a8\u5b8c\u6210\u5f85\u529e\u4e8b\u9879", "conclusion": "Claude Code\u4f5c\u4e3a\"\u81ea\u52a8\u5b8c\u6210\u5f85\u529e\u4e8b\u9879\"\u5de5\u5177\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u5728\u7f51\u9875\u548ciOS\u5e73\u53f0\u4e0a\u8fd0\u884c\u7a33\u5b9a", "topic": "swe application"}}
{"id": "tldr.2510.c8e22003", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdraftnrun.com%2F%3Futm_source=tldrdesign/1/0100019a2feab1bb-7f08164c-970e-45aa-91c5-d4105767be2f-000000/-5cbGDyUnAi8KOsI1eTX9MNip6DVi3IxWd9u7e-nICQ=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdraftnrun.com%2F%3Futm_source=tldrdesign/1/0100019a2feab1bb-7f08164c-970e-45aa-91c5-d4105767be2f-000000/-5cbGDyUnAi8KOsI1eTX9MNip6DVi3IxWd9u7e-nICQ=429", "authors": ["TLDR Newsletter"], "title": "Build Production-Ready AI Workflows Without Code", "comment": "Source: TLDR Newsletter, Date: 2025-10-29, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdraftnrun.com%2F%3Futm_source=tldrdesign/1/0100019a2feab1bb-7f08164c-970e-45aa-91c5-d4105767be2f-000000/-5cbGDyUnAi8KOsI1eTX9MNip6DVi3IxWd9u7e-nICQ=429", "summary": "Build Production-Ready AI Workflows Without Code (Website) Move from prototype to production in record time, with built-in DevOps, observability, and governance. Build AI chatbots, automate workflows, adopt agentic AI, and deploy enterprise-ready solutions without friction.", "source": "tldr", "AI": {"tldr": "\u6784\u5efa\u65e0\u9700\u4ee3\u7801\u7684\u751f\u4ea7\u7ea7AI\u5de5\u4f5c\u6d41\u5e73\u53f0\uff0c\u63d0\u4f9b\u5185\u7f6eDevOps\u3001\u53ef\u89c2\u6d4b\u6027\u548c\u6cbb\u7406\u529f\u80fd\uff0c\u5e2e\u52a9\u4f01\u4e1a\u5feb\u901f\u4ece\u539f\u578b\u5230\u751f\u4ea7\u90e8\u7f72AI\u5e94\u7528", "motivation": "\u964d\u4f4eAI\u5e94\u7528\u5f00\u53d1\u95e8\u69db\uff0c\u8ba9\u975e\u6280\u672f\u7528\u6237\u4e5f\u80fd\u6784\u5efa\u548c\u90e8\u7f72\u4f01\u4e1a\u7ea7AI\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4ece\u539f\u578b\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u6469\u64e6\u95ee\u9898", "method": "\u63d0\u4f9b\u65e0\u9700\u4ee3\u7801\u7684\u5e73\u53f0\uff0c\u5185\u7f6eDevOps\u5de5\u5177\u94fe\u3001\u53ef\u89c2\u6d4b\u6027\u76d1\u63a7\u548c\u6cbb\u7406\u6846\u67b6\uff0c\u652f\u6301AI\u804a\u5929\u673a\u5668\u4eba\u6784\u5efa\u3001\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u548c\u667a\u80fd\u4f53AI\u5e94\u7528", "result": "\u80fd\u591f\u663e\u8457\u7f29\u77ed\u4ece\u539f\u578b\u5230\u751f\u4ea7\u7684\u65f6\u95f4\uff0c\u5b9e\u73b0\u4f01\u4e1a\u7ea7AI\u89e3\u51b3\u65b9\u6848\u7684\u65e0\u7f1d\u90e8\u7f72", "conclusion": "\u65e0\u9700\u4ee3\u7801\u7684AI\u5de5\u4f5c\u6d41\u5e73\u53f0\u662f\u63a8\u52a8AI\u5728\u4f01\u4e1a\u4e2d\u5927\u89c4\u6a21\u5e94\u7528\u7684\u6709\u6548\u9014\u5f84\uff0c\u964d\u4f4e\u4e86\u6280\u672f\u95e8\u69db\u548c\u90e8\u7f72\u6210\u672c", "topic": "swe application"}}
{"id": "tldr.2510.54f09788", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theregister.com%2F2025%2F10%2F28%2Fai_browsers_prompt_injection%2F%3Futm_source=tldrinfosec/1/0100019a30149446-f3d3ce19-f96e-4e98-84e5-43ffd6165df9-000000/Mtohf-bhmNybuP5xuWF7gJuIzkmodIT9s7ZZVB7DR0Q=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theregister.com%2F2025%2F10%2F28%2Fai_browsers_prompt_injection%2F%3Futm_source=tldrinfosec/1/0100019a30149446-f3d3ce19-f96e-4e98-84e5-43ffd6165df9-000000/Mtohf-bhmNybuP5xuWF7gJuIzkmodIT9s7ZZVB7DR0Q=429", "authors": ["TLDR Newsletter"], "title": "AI browsers face a security flaw as inevitable as death and taxes", "comment": "Source: TLDR Newsletter, Date: 2025-10-29, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theregister.com%2F2025%2F10%2F28%2Fai_browsers_prompt_injection%2F%3Futm_source=tldrinfosec/1/0100019a30149446-f3d3ce19-f96e-4e98-84e5-43ffd6165df9-000000/Mtohf-bhmNybuP5xuWF7gJuIzkmodIT9s7ZZVB7DR0Q=429", "summary": "AI browsers face a security flaw as inevitable as death and taxes (8 minute read) Prompt injection is an increasingly critical security issue as AI-driven browsers gain agentic capabilities: they can now act on users' behalf, from opening web pages to handling emails and files. This opens up new attack vectors, such as direct injection via URLs or indirect injection hidden within website or document text, allowing attackers to trick bots into performing dangerous actions without user consent....", "source": "tldr", "AI": {"tldr": "AI\u6d4f\u89c8\u5668\u9762\u4e34\u4e0d\u53ef\u907f\u514d\u7684\u5b89\u5168\u6f0f\u6d1e\u2014\u2014\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u968f\u7740AI\u6d4f\u89c8\u5668\u83b7\u5f97\u4ee3\u7406\u80fd\u529b\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7URL\u76f4\u63a5\u6ce8\u5165\u6216\u9690\u85cf\u5728\u7f51\u7ad9\u6587\u672c\u4e2d\u7684\u95f4\u63a5\u6ce8\u5165\u6765\u6b3a\u9a97\u673a\u5668\u4eba\u6267\u884c\u5371\u9669\u64cd\u4f5c\u3002", "motivation": "\u968f\u7740AI\u9a71\u52a8\u7684\u6d4f\u89c8\u5668\u83b7\u5f97\u4ee3\u7406\u80fd\u529b\uff0c\u80fd\u591f\u4ee3\u8868\u7528\u6237\u6267\u884c\u64cd\u4f5c\uff0c\u8fd9\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u7279\u522b\u662f\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u6b3a\u9a97AI\u4ee3\u7406\u6267\u884c\u672a\u7ecf\u7528\u6237\u6388\u6743\u7684\u5371\u9669\u884c\u4e3a\u3002", "method": "\u5206\u6790\u4e86AI\u6d4f\u89c8\u5668\u9762\u4e34\u7684\u65b0\u578b\u5b89\u5168\u5a01\u80c1\u2014\u2014\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5305\u62ec\u76f4\u63a5\u6ce8\u5165\uff08\u901a\u8fc7URL\uff09\u548c\u95f4\u63a5\u6ce8\u5165\uff08\u9690\u85cf\u5728\u7f51\u7ad9\u6216\u6587\u6863\u6587\u672c\u4e2d\uff09\u4e24\u79cd\u653b\u51fb\u5411\u91cf\u3002", "result": "\u8bc6\u522b\u51faAI\u6d4f\u89c8\u5668\u5728\u83b7\u5f97\u4ee3\u7406\u80fd\u529b\u540e\u9762\u4e34\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u63d0\u793a\u6ce8\u5165\u6280\u672f\u7ed5\u8fc7\u5b89\u5168\u63aa\u65bd\uff0c\u63a7\u5236AI\u4ee3\u7406\u6267\u884c\u6076\u610f\u64cd\u4f5c\u3002", "conclusion": "\u63d0\u793a\u6ce8\u5165\u662fAI\u6d4f\u89c8\u5668\u4e0d\u53ef\u907f\u514d\u7684\u5b89\u5168\u7f3a\u9677\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u5b89\u5168\u673a\u5236\u6765\u5e94\u5bf9\u8fd9\u79cd\u65b0\u578b\u5a01\u80c1\uff0c\u786e\u4fddAI\u4ee3\u7406\u5728\u4ee3\u8868\u7528\u6237\u64cd\u4f5c\u65f6\u7684\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.abb303da", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fnews-insights%2Fcompany-news%2Fwelcome-home-agents%2F%3Futm_source=tldrai/1/0100019a301d98b9-5abe39bf-1731-4fb6-9b68-ae16cd066226-000000/MOpsbLjJmPZLNF2lYVlkAl_5ZWOb-oDjTXU75FJ-erk=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fnews-insights%2Fcompany-news%2Fwelcome-home-agents%2F%3Futm_source=tldrai/1/0100019a301d98b9-5abe39bf-1731-4fb6-9b68-ae16cd066226-000000/MOpsbLjJmPZLNF2lYVlkAl_5ZWOb-oDjTXU75FJ-erk=429", "authors": ["TLDR Newsletter"], "title": "Introducing Agent HQ: Any agent, any way you work", "comment": "Source: TLDR Newsletter, Date: 2025-10-29, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fnews-insights%2Fcompany-news%2Fwelcome-home-agents%2F%3Futm_source=tldrai/1/0100019a301d98b9-5abe39bf-1731-4fb6-9b68-ae16cd066226-000000/MOpsbLjJmPZLNF2lYVlkAl_5ZWOb-oDjTXU75FJ-erk=429", "summary": "Introducing Agent HQ: Any agent, any way you work (10 minute read) Agent HQ transforms GitHub into an open ecosystem that unites every agent on a single platform. Coding agents from Anthropic, OpenAI, Google, Cognition, xAI, and other providers will become available directly within GitHub as part of the GitHub Copilot subscription over the coming months. Agent HQ's power comes from a unified command center called 'mission control' that allows users to choose from a fleet of agents, assign the...", "source": "tldr", "AI": {"tldr": "Agent HQ\u5c06GitHub\u8f6c\u53d8\u4e3a\u5f00\u653e\u751f\u6001\u7cfb\u7edf\uff0c\u6574\u5408\u591a\u4e2aAI\u4ee3\u7406\u63d0\u4f9b\u5546\uff08\u5982Anthropic\u3001OpenAI\u3001Google\u7b49\uff09\u5230\u7edf\u4e00\u5e73\u53f0\uff0c\u901a\u8fc7\u4efb\u52a1\u63a7\u5236\u4e2d\u5fc3\u7ba1\u7406\u4ee3\u7406\u8230\u961f\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u7edf\u4e00\u7684\u4ee3\u7406\u7ba1\u7406\u5e73\u53f0\uff0c\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u5728GitHub\u5e73\u53f0\u5185\u96c6\u6210\u591a\u4e2aAI\u4ee3\u7406\u63d0\u4f9b\u5546\u7684\u670d\u52a1\uff0c\u901a\u8fc7\u4efb\u52a1\u63a7\u5236\u4e2d\u5fc3\u7edf\u4e00\u7ba1\u7406\u548c\u8c03\u5ea6\u4e0d\u540c\u4ee3\u7406\u3002", "result": "GitHub Copilot\u8ba2\u9605\u7528\u6237\u5c06\u80fd\u5728GitHub\u5185\u76f4\u63a5\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u63d0\u4f9b\u5546\u7684AI\u4ee3\u7406\u670d\u52a1\u3002", "conclusion": "Agent HQ\u901a\u8fc7\u7edf\u4e00\u5e73\u53f0\u89e3\u51b3\u4e86\u4ee3\u7406\u751f\u6001\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "wechat.2510.3107aacf", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNTY5OTEwNQ==&mid=2247487568&idx=1&sn=a939f147eb6da2fe3456782afeaa9551&chksm=c0e0ee3b4a17abe2b5b302c57a7920bc3e09f35eee72480a3af81d3b6ec6c0cd7a9d79e6c9a3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNTY5OTEwNQ==&mid=2247487568&idx=1&sn=a939f147eb6da2fe3456782afeaa9551&chksm=c0e0ee3b4a17abe2b5b302c57a7920bc3e09f35eee72480a3af81d3b6ec6c0cd7a9d79e6c9a3#rd", "authors": ["\u7231\u98de\u63a7\u7684\u5c0f\u8717\u725b"], "title": "\u5b66\u4e60\u5468\u62a5\uff08<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u56db\uff09", "comment": "Source: WeChat, Published: 2025-10-30 11:38:18", "summary": "\u4ee5\u540e\u4f1a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u76f8\u5173\u7684\u5b66\u4e60\u6c47\u62a5\u5de5\u4f5c\u3002NO.03\u4e09\u3001\u8bfe\u7a0b\u5b66\u4e601\u3001\u5f02\u65b9\u5dee\u68c0\u9a8c\uff1a\u6a21\u578b\u603b\u4f53\u62df\u5408\u60c5\u51b5regress \u4eba\u5747\u6d88\u8d39\u652f\u51fay\u4ece\u4e8b\u519c\u4e1a\u7ecf\u8425\u7684\u7eaf\u6536\u5165x1\u5176\u4ed6\u6765\u6e90\u7eaf\u6536\u5165x2 source ss df ms number of obs 31 f\uff082\uff0c 28\uff09 163.04 model 18573928.9 2 9286964.45 prob >", "AI": {"tldr": "\u4ee5\u540e\u4f1a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u76f8\u5173\u7684\u5b66\u4e60\u6c47\u62a5\u5de5\u4f5c\u3002NO.03\u4e09\u3001\u8bfe\u7a0b\u5b66\u4e601\u3001\u5f02\u65b9\u5dee\u68c0\u9a8c\uff1a\u6a21\u578b\u603b\u4f53\u62df\u5408\u60c5\u51b5regress \u4eba\u5747\u6d88\u8d39\u652f\u51fay\u4ece\u4e8b\u519c\u4e1a\u7ecf\u8425\u7684\u7eaf\u6536\u5165x1\u5176\u4ed6\u6765\u6e90\u7eaf\u6536\u5165x2 source ss df ms number of obs 31 f\uff082\uff0c 28\uff09 163.04 model 18573928.9 2 9286964.45 prob >", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.dda226c9", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2NTYzNzc3NA==&mid=2247505397&idx=1&sn=398c80af72167960d9c44048414365d5&chksm=fd87a6b46e59b60e37edfd98c2dd06f063badb274e31597c651ba8ae5c38248a95f0bd7bbc30#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2NTYzNzc3NA==&mid=2247505397&idx=1&sn=398c80af72167960d9c44048414365d5&chksm=fd87a6b46e59b60e37edfd98c2dd06f063badb274e31597c651ba8ae5c38248a95f0bd7bbc30#rd", "authors": ["AI\u7b97\u6cd5\u79d1\u7814paper"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6210\u4eca\u5e74\u6700\u5927\u8d62\u5bb6\uff01\u767b\u4e0aNature\u9876\u520a\uff01", "comment": "Source: WeChat, Published: 2025-10-30 11:02:07", "summary": "\u5206\u4eab\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u65b9\u5411\u503c\u5f97\u5b66\u4e60\u7684\u6210\u679c\u2014\u2014\u6700\u5927\u6269\u6563\u5f3a\u5316\u5b66\u4e60MaxDiff RL\uff0c\u611f\u5174\u8da3\u7684\u540c\u5b66\u53ef\u9605\u8bfb\u539f\u6587\u3002\u8fd9\u662f\u4e2a\u65b0\u578bRL\u65b9\u6cd5\uff0c\u76ee\u524d\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u5b9e\u73b0\u4e86SOTA\uff0c\u5df2\u6210\u529f\u767b\u4e0aNature Machine Intelligence\u3002", "AI": {"tldr": "\u5206\u4eab\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u65b9\u5411\u503c\u5f97\u5b66\u4e60\u7684\u6210\u679c\u2014\u2014\u6700\u5927\u6269\u6563\u5f3a\u5316\u5b66\u4e60MaxDiff RL\uff0c\u611f\u5174\u8da3\u7684\u540c\u5b66\u53ef\u9605\u8bfb\u539f\u6587\u3002\u8fd9\u662f\u4e2a\u65b0\u578bRL\u65b9\u6cd5\uff0c\u76ee\u524d\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u5b9e\u73b0\u4e86SOTA\uff0c\u5df2\u6210\u529f\u767b\u4e0aNature Machine Intelligence\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.008b00b9", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3OTU0MDEwNg==&mid=2247487235&idx=2&sn=3a953b6fbd4eec804c9e04985e14f1db&chksm=9e98a9caca7e67501af0a7814ab920921c6ab4975ec3433bf2fcde383672370f09146ca202b6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3OTU0MDEwNg==&mid=2247487235&idx=2&sn=3a953b6fbd4eec804c9e04985e14f1db&chksm=9e98a9caca7e67501af0a7814ab920921c6ab4975ec3433bf2fcde383672370f09146ca202b6#rd", "authors": ["Python\u6c5f\u6e56"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7834\u8bd1\u6d88\u8d39\u5bc6\u7801\uff1a\u5bb6\u5ead\u7ecf\u6d4e\u884c\u4e3a\u5982\u4f55\u6210\u4e3a\u4f60\u7684\u8d22\u5bcc\u589e\u957f\u96f7\u8fbe\uff1f", "comment": "Source: WeChat, Published: 2025-10-30 10:30:36", "summary": "class RLInvestmentStrategy\uff1a\"\"\"\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6295\u8d44\u7b56\u7565\"\"\"def __init__\uff08self\uff0c n_assets=5\uff0c lookback=60\uff09\uff1aself.n_assets = n_assetsself.lookback = lookbackself.portfolio_weights = np.ones\uff08n_assets\uff09 / n_assets # \u521d\u59cb\u7b49\u6743\u91cd", "AI": {"tldr": "class RLInvestmentStrategy\uff1a\"\"\"\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6295\u8d44\u7b56\u7565\"\"\"def __init__\uff08self\uff0c n_assets=5\uff0c lookback=60\uff09\uff1aself.n_assets = n_assetsself.lookback = lookbackself.portfolio_weights = np.ones\uff08n_assets\uff09 / n_assets # \u521d\u59cb\u7b49\u6743\u91cd", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.7ed43745", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzNTA0ODY1NQ==&mid=2247567634&idx=2&sn=4bda99cafbe980771e936ca285fd7a73&chksm=f1aaa705c323ba840845a2f0f569dacd28e526afba497905f21750eb3a146a22f5081863b08c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzNTA0ODY1NQ==&mid=2247567634&idx=2&sn=4bda99cafbe980771e936ca285fd7a73&chksm=f1aaa705c323ba840845a2f0f569dacd28e526afba497905f21750eb3a146a22f5081863b08c#rd", "authors": ["AI\u7535\u5802"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff0c\u7aef\u5230\u7aef2.0\u7684\u5173\u952e", "comment": "Source: WeChat, Published: 2025-10-30 10:03:13", "summary": "\u5f3a\u5316\u5b66\u4e60\u4f9d\u9760\u7cfb\u7edf\u4e0e\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u548c\u4f18\u5316\uff0c\u5728\u8bd5\u9519\u4e2d\u6839\u636e\u5956\u52b1\u4fe1\u53f7\u6765\u4f18\u5316\u81ea\u8eab\u7b56\u7565\u3002\u7684\uff0c\u4e0d\u77e5\u9053\u600e\u4e48\u4ece\u9519\u8bef\u4e2d\u56de\u590d\uff08\u77e5\u5176\u7136\uff0c\u968f\u673a\u63a2\u7d22\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u6548\u7387\u5b58\u5728\u6311\u6218\uff0c \u4e0d\u77e5\u5176\u6240\u4ee5\u7136\uff09 \u667a\u80fd\u4f53 \u72b6\u6001", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u4f9d\u9760\u7cfb\u7edf\u4e0e\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u548c\u4f18\u5316\uff0c\u5728\u8bd5\u9519\u4e2d\u6839\u636e\u5956\u52b1\u4fe1\u53f7\u6765\u4f18\u5316\u81ea\u8eab\u7b56\u7565\u3002\u7684\uff0c\u4e0d\u77e5\u9053\u600e\u4e48\u4ece\u9519\u8bef\u4e2d\u56de\u590d\uff08\u77e5\u5176\u7136\uff0c\u968f\u673a\u63a2\u7d22\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u6548\u7387\u5b58\u5728\u6311\u6218\uff0c \u4e0d\u77e5\u5176\u6240\u4ee5\u7136\uff09 \u667a\u80fd\u4f53 \u72b6\u6001", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.2065424f", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MDEyMjAzMQ==&mid=2453730827&idx=1&sn=411727a7044493aac294a43e564ec6f1&chksm=86c718f0cae92b4e1b0e315f7c097e481d13d8fa156976576541865cce07480c070aa994ba33#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MDEyMjAzMQ==&mid=2453730827&idx=1&sn=411727a7044493aac294a43e564ec6f1&chksm=86c718f0cae92b4e1b0e315f7c097e481d13d8fa156976576541865cce07480c070aa994ba33#rd", "authors": ["\u963f\u62c9\u4e01AI\u795e\u706f"], "title": "\u522b\u52a8\u4e0d\u52a8\u8bf4\u8bad\u7ec3\u5927\u6a21\u578b\u4e86\uff0c\u4e00\u7bc7\u201c\u8001\u5e08\u201d\u7684\u6bd4\u55bb\u7ed9\u4f60\u8bb2\u660e\u767d\u4ec0\u4e48\u662f\u540e\u8bad\u7ec3\u3001\u5fae\u8c03\u3001<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-30 09:19:00", "summary": "\u6a21\u578b\u5c31\u5b66\u5230\u4e86\uff0c\u54e6\uff0c\u521a\u624d\u90a3\u4e48\u8bb2\u4e0d\u884c\uff0c\u5f97\u6362\u4e2a\u65b9\u5f0f\u3002\u8fd9\u5c31\u662f\u5f3a\u5316\u5b66\u4e60\u3002\u6a21\u578b\u5728\u8ddf\u4f60\u771f\u5b9e\u4e92\u52a8\u3002\u4f60\u7684\u6bcf\u4e00\u4e2a\u53cd\u9988\uff0c\u90fd\u662f\u5728\u8bad\u7ec3\u5b83\u3002\u8ba9\u5b83\u53d8\u5f97\u8d8a\u6765\u8d8a\u4f1a\u770b\u773c\u8272\uff0c\u8d8a\u6765\u8d8a\u806a\u660e\u3002", "AI": {"tldr": "\u6a21\u578b\u5c31\u5b66\u5230\u4e86\uff0c\u54e6\uff0c\u521a\u624d\u90a3\u4e48\u8bb2\u4e0d\u884c\uff0c\u5f97\u6362\u4e2a\u65b9\u5f0f\u3002\u8fd9\u5c31\u662f\u5f3a\u5316\u5b66\u4e60\u3002\u6a21\u578b\u5728\u8ddf\u4f60\u771f\u5b9e\u4e92\u52a8\u3002\u4f60\u7684\u6bcf\u4e00\u4e2a\u53cd\u9988\uff0c\u90fd\u662f\u5728\u8bad\u7ec3\u5b83\u3002\u8ba9\u5b83\u53d8\u5f97\u8d8a\u6765\u8d8a\u4f1a\u770b\u773c\u8272\uff0c\u8d8a\u6765\u8d8a\u806a\u660e\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.08ba2a7a", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNTk4OTY5Ng==&mid=2247484137&idx=1&sn=82b7ae48b0de8b1ab932aaa0bdbfdbd0&chksm=c39c5db32381f37393d9712a4c346b22405c51e70860dd9fe76c30893324282d18253daebf7d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNTk4OTY5Ng==&mid=2247484137&idx=1&sn=82b7ae48b0de8b1ab932aaa0bdbfdbd0&chksm=c39c5db32381f37393d9712a4c346b22405c51e70860dd9fe76c30893324282d18253daebf7d#rd", "authors": ["\u7ca4\u9ed4\u6570\u5b57\u5b89\u5168\u7814\u7a76\u9662"], "title": "\u6280\u672f\u524d\u6cbf | \u57fa\u4e8e ARTIST \u6846\u67b6\u7684\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u589e\u5f3a\uff1aAgentic <em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u534f\u540c\u7a81\u7834", "comment": "Source: WeChat, Published: 2025-10-30 07:03:12", "summary": "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff1a\u57fa\u4e8e GRPO \u7684\u9ad8\u6548\u4f18\u5316ARTIST\u91c7\u7528Group Relative Policy Optimization\uff08GRPO\uff09\u7b97\u6cd5\uff0c\u89e3\u51b3\u5de5\u5177\u96c6\u6210\u573a\u666f\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6311\u6218\uff1aGRPO \u6838\u5fc3\u4f18\u52bf\u65e0\u9700\u4ef7\u503c\u51fd\u6570\u8fd1\u4f3c\uff0c\u901a\u8fc7 \u201c\u5206\u7ec4\u91c7\u6837\u54cd\u5e94\u201d \u4f30\u7b97\u57fa\u7ebf\uff0c\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u7a33\u5b9a\u6027\uff0c", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff1a\u57fa\u4e8e GRPO \u7684\u9ad8\u6548\u4f18\u5316ARTIST\u91c7\u7528Group Relative Policy Optimization\uff08GRPO\uff09\u7b97\u6cd5\uff0c\u89e3\u51b3\u5de5\u5177\u96c6\u6210\u573a\u666f\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6311\u6218\uff1aGRPO \u6838\u5fc3\u4f18\u52bf\u65e0\u9700\u4ef7\u503c\u51fd\u6570\u8fd1\u4f3c\uff0c\u901a\u8fc7 \u201c\u5206\u7ec4\u91c7\u6837\u54cd\u5e94\u201d \u4f30\u7b97\u57fa\u7ebf\uff0c\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u7a33\u5b9a\u6027\uff0c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.6c8a410b", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMzM0MzgwOQ==&mid=2247485697&idx=1&sn=e67c523c7fd2989a90ce9069e10c393f&chksm=c30c272b7240cce3572641ea47ca99aef045be2797546edf5d50db61953c1d831bc9ba1875e0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMzM0MzgwOQ==&mid=2247485697&idx=1&sn=e67c523c7fd2989a90ce9069e10c393f&chksm=c30c272b7240cce3572641ea47ca99aef045be2797546edf5d50db61953c1d831bc9ba1875e0#rd", "authors": ["\u4e91\u9e64\u70df\u6ce2"], "title": "[sumo\u8fdb\u9636\u7bc7|traci\u4e8c\u6b21\u5f00\u53d123] \u2013 \u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>(DQN)\u7684\u9ad8\u901f\u516c\u8def\u74f6\u9888\u8def\u6bb5\u52a8\u6001\u9650\u901f\u63a7\u5236", "comment": "Source: WeChat, Published: 2025-10-30 05:06:28", "summary": "3.\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08dqn\uff09\u9ad8\u901f\u516c\u8def\u74f6\u9888\u8def\u6bb5\u52a8\u6001\u9650\u901f\u63a7\u5236\u667a\u80fd\u4f53\u8bad\u7ec3\u53ca\u5b9e\u73b0\u6548\u679c 4.sumo\u5b9e\u73b0\u7684\u6838\u5fc3\u6d41\u7a0b\u53ca\u6b65\u9aa4 5.\u4ee3\u7801\u5206\u4eab \u89c6\u9891\u6807\u9898\uff1aDQN\u9ad8\u901f\u516c\u8def\u74f6\u9888\u8def\u6bb5\u52a8\u6001\u9650\u901f\u6548\u679c", "AI": {"tldr": "3.\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08dqn\uff09\u9ad8\u901f\u516c\u8def\u74f6\u9888\u8def\u6bb5\u52a8\u6001\u9650\u901f\u63a7\u5236\u667a\u80fd\u4f53\u8bad\u7ec3\u53ca\u5b9e\u73b0\u6548\u679c 4.sumo\u5b9e\u73b0\u7684\u6838\u5fc3\u6d41\u7a0b\u53ca\u6b65\u9aa4 5.\u4ee3\u7801\u5206\u4eab \u89c6\u9891\u6807\u9898\uff1aDQN\u9ad8\u901f\u516c\u8def\u74f6\u9888\u8def\u6bb5\u52a8\u6001\u9650\u901f\u6548\u679c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.5569d0f7", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMjQ1NDgwMg==&mid=2247484854&idx=1&sn=8e9bfc58898fbf8b46b3ccf2b36982c4&chksm=fe1230202bdf03bef575ee9031ef8fe54f5c8463e62ea69a4739fec430d7a0e66951bc080367#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMjQ1NDgwMg==&mid=2247484854&idx=1&sn=8e9bfc58898fbf8b46b3ccf2b36982c4&chksm=fe1230202bdf03bef575ee9031ef8fe54f5c8463e62ea69a4739fec430d7a0e66951bc080367#rd", "authors": ["AI\u5e94\u7528\u98ce\u5411\u6807"], "title": "\u767e\u5ea6\u548c\u9ad8\u74f4\u8054\u624b\uff0c\u6295\u4e86\u4e00\u5bb6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u521b\u4f01", "comment": "Source: WeChat, Published: 2025-10-30 04:08:40", "summary": "Pyromind Dynamics\u662f\u4e00\u5bb6\u4e13\u6ce8\u4e8e\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\u7684\u5e73\u53f0\u578b\u516c\u53f8\uff0c\u6b63\u5728\u6784\u5efa\u4e00\u5957\u201c\u5f3a\u5316\u5b66\u4e60\u5373\u670d\u52a1\uff08Reinforcement Learning as a Service\uff0cRLaaS\uff09\u201d\u5e73\u53f0\uff0c\u65e8\u5728\u6807\u51c6\u5316\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u3001\u90e8\u7f72\u4e0e\u8bc4\u4f30\u6d41\u7a0b\u3002", "AI": {"tldr": "Pyromind Dynamics\u662f\u4e00\u5bb6\u4e13\u6ce8\u4e8e\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\u7684\u5e73\u53f0\u578b\u516c\u53f8\uff0c\u6b63\u5728\u6784\u5efa\u4e00\u5957\u201c\u5f3a\u5316\u5b66\u4e60\u5373\u670d\u52a1\uff08Reinforcement Learning as a Service\uff0cRLaaS\uff09\u201d\u5e73\u53f0\uff0c\u65e8\u5728\u6807\u51c6\u5316\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u3001\u90e8\u7f72\u4e0e\u8bc4\u4f30\u6d41\u7a0b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.7e863667", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247716968&idx=3&sn=833b3b8a5d359120e6a4f1d140192a04&chksm=f8d30442f83d46f63ba0e10f6c78ab2de2d754e0a425cef52c263659f45a4132f43cf09375a2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247716968&idx=3&sn=833b3b8a5d359120e6a4f1d140192a04&chksm=f8d30442f83d46f63ba0e10f6c78ab2de2d754e0a425cef52c263659f45a4132f43cf09375a2#rd", "authors": ["\u4e00\u70b9\u4eba\u5de5\u4e00\u70b9\u667a\u80fd"], "title": "AI\u5728\u7ebf<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u201c\u8fb9\u505a\u8fb9\u5b66\u201d\uff0c\u65af\u5766\u798f\u56e2\u961f\u8ba97B\u5c0f\u6a21\u578b\u6027\u80fd\u98d9\u5347\uff0c\u751a\u81f3\u8d85\u8d8aGPT-4o", "comment": "Source: WeChat, Published: 2025-10-30 04:00:49", "summary": "\u5b9e\u73b0\u667a\u80fd\u4f53\u6d41\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u591a\u8f6e\u4fe1\u7528\u5206\u914d\uff08multi-turn credit assignment\uff09\uff1a\u5373\u5982\u4f55\u5728\u957f\u65f6\u8de8\u5ea6\uff08long-horizon\uff09\u4e14\u5956\u52b1\u7a00\u758f\uff08sparse reward\uff09\u7684\u6761\u4ef6\u4e0b\uff0c\u7a33\u5b9a\u4e14\u9ad8\u6548\u5730\u8bad\u7ec3\u3002", "AI": {"tldr": "\u5b9e\u73b0\u667a\u80fd\u4f53\u6d41\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u591a\u8f6e\u4fe1\u7528\u5206\u914d\uff08multi-turn credit assignment\uff09\uff1a\u5373\u5982\u4f55\u5728\u957f\u65f6\u8de8\u5ea6\uff08long-horizon\uff09\u4e14\u5956\u52b1\u7a00\u758f\uff08sparse reward\uff09\u7684\u6761\u4ef6\u4e0b\uff0c\u7a33\u5b9a\u4e14\u9ad8\u6548\u5730\u8bad\u7ec3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.6c4cb118", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1ODk0Mjg2MQ==&mid=2247491522&idx=1&sn=e469588428d8eec319a4368d38a06dbd&chksm=fda0c839acc3217df50c9fec98e35284bf43abca0d1c10a35c4fc0863f261c8d420a5b281d24#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1ODk0Mjg2MQ==&mid=2247491522&idx=1&sn=e469588428d8eec319a4368d38a06dbd&chksm=fda0c839acc3217df50c9fec98e35284bf43abca0d1c10a35c4fc0863f261c8d420a5b281d24#rd", "authors": ["\u533a\u5757\u94feand\u8bed\u4e49\u7814\u7a76\u5b9e\u9a8c\u5ba4"], "title": "\u5f53\u77e5\u8bc6\u56fe\u8c31\u9047\u4e0a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1a\u8ba9AI\u5b66\u4f1a\u201c\u63a8\u7406\u201d\u7684\u827a\u672f", "comment": "Source: WeChat, Published: 2025-10-30 02:55:57", "summary": "\u5f3a\u5316\u5b66\u4e60\uff1aAI\u7684\u201c\u51b3\u7b56\u5927\u8111\u201d\u5f3a\u5316\u5b66\u4e60\u5c31\u50cf\u662f\u8ba9\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u201c\u73a9\u6e38\u620f\u201d\uff1a\u72b6\u6001\uff08State\uff09\uff1a\u73b0\u5728\u7684\u60c5\u5883\u3002\u52a8\u4f5c\uff08Action\uff09\uff1a\u80fd\u505a\u7684\u9009\u62e9\u3002\u5956\u52b1\uff08Reward\uff09\uff1a\u7ed3\u679c\u597d\u4e0d\u597d\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff1aAI\u7684\u201c\u51b3\u7b56\u5927\u8111\u201d\u5f3a\u5316\u5b66\u4e60\u5c31\u50cf\u662f\u8ba9\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u201c\u73a9\u6e38\u620f\u201d\uff1a\u72b6\u6001\uff08State\uff09\uff1a\u73b0\u5728\u7684\u60c5\u5883\u3002\u52a8\u4f5c\uff08Action\uff09\uff1a\u80fd\u505a\u7684\u9009\u62e9\u3002\u5956\u52b1\uff08Reward\uff09\uff1a\u7ed3\u679c\u597d\u4e0d\u597d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.9685483e", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247837943&idx=2&sn=32cd3f0b5fac734024986ef4df585741&chksm=e91e2b0fd5b7ea9d47547483d2aacf5521db9970c08f56434d27f643ed7a6bc421cee5838064#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247837943&idx=2&sn=32cd3f0b5fac734024986ef4df585741&chksm=e91e2b0fd5b7ea9d47547483d2aacf5521db9970c08f56434d27f643ed7a6bc421cee5838064#rd", "authors": ["\u91cf\u5b50\u4f4d"], "title": "Cursor\u53d1\u5e03\u9996\u4e2a\u7f16\u7a0b\u5927\u6a21\u578b\uff01\u4ee3\u7801\u751f\u6210250tokens/\u79d2\uff0c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>+MoE\u67b6\u6784", "comment": "Source: WeChat, Published: 2025-10-30 01:06:00", "summary": "\u5f3a\u5316\u5b66\u4e60\u6700\u5927\u7684\u7279\u70b9\u662f\uff1a\u5b83\u5f97\u5728\u771f\u5b9e\u73af\u5883\u91cc\u300c\u5e72\u6d3b\u300d\uff0c\u624d\u80fd\u5b66\u5230\u771f\u672c\u4e8b\u3002\u5982\u679cComposer\u53ea\u5728\u865a\u62df\u6570\u636e\u96c6\u91cc\u6539\u6539\u4ee3\u7801\uff0c\u5b83\u6839\u672c\u4e0d\u77e5\u9053\u8fd9\u4e9b\u4ee3\u7801\u6709\u6ca1\u6709bug\u3001\u6d4b\u8bd5\u80fd\u4e0d\u80fd\u8fc7\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u6700\u5927\u7684\u7279\u70b9\u662f\uff1a\u5b83\u5f97\u5728\u771f\u5b9e\u73af\u5883\u91cc\u300c\u5e72\u6d3b\u300d\uff0c\u624d\u80fd\u5b66\u5230\u771f\u672c\u4e8b\u3002\u5982\u679cComposer\u53ea\u5728\u865a\u62df\u6570\u636e\u96c6\u91cc\u6539\u6539\u4ee3\u7801\uff0c\u5b83\u6839\u672c\u4e0d\u77e5\u9053\u8fd9\u4e9b\u4ee3\u7801\u6709\u6ca1\u6709bug\u3001\u6d4b\u8bd5\u80fd\u4e0d\u80fd\u8fc7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.92e1c55b", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2OTAyMDQ1Mw==&mid=2247484760&idx=1&sn=8486cdf7e738caaf06adf486c334b283&chksm=c5cf4fe6901107bb58c7f5ad788641f31726a2389c9a3acac8f5bde68bfa050b72cd726ff77e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2OTAyMDQ1Mw==&mid=2247484760&idx=1&sn=8486cdf7e738caaf06adf486c334b283&chksm=c5cf4fe6901107bb58c7f5ad788641f31726a2389c9a3acac8f5bde68bfa050b72cd726ff77e#rd", "authors": ["Trans\u4ea4\u901a\u7814\u7a76"], "title": "\u540c\u6d4e\u5927\u5b66\u6c7d\u8f66\u5b66\u9662\u7b49TR-C\u53d1\u6587\uff01\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u5b89\u5168<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-30 01:03:55", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f5c\u4e3a\u4e00\u79cd\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u7684\u6280\u672f\uff0c\u5df2\u6210\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u95ee\u9898\u7684\u5173\u952e\u9014\u5f84\u3002\u7136\u800c\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u771f\u5b9e\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u9762\u4e34\u4e24\u5927\u6838\u5fc3\u6311\u6218\uff1a\u5b66\u4e60\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f5c\u4e3a\u4e00\u79cd\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u7684\u6280\u672f\uff0c\u5df2\u6210\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u95ee\u9898\u7684\u5173\u952e\u9014\u5f84\u3002\u7136\u800c\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u771f\u5b9e\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u9762\u4e34\u4e24\u5927\u6838\u5fc3\u6311\u6218\uff1a\u5b66\u4e60\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.1e49177d", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzOTAxOTQ2Ng==&mid=2247483735&idx=1&sn=36dc14284d54c12be0f5ab41f0162994&chksm=f1cbf5d78682cd006cc113e5c5190dd3d2e335c6569b121e853860eaa5c2ea7f754137d487fa#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzOTAxOTQ2Ng==&mid=2247483735&idx=1&sn=36dc14284d54c12be0f5ab41f0162994&chksm=f1cbf5d78682cd006cc113e5c5190dd3d2e335c6569b121e853860eaa5c2ea7f754137d487fa#rd", "authors": ["\u603c\u603c\u6559\u4f60\u73a9AI"], "title": "<em class=\"highlight\">Agentic</em> RAG\uff0c\u76ee\u524d\u6700\u5f3a\u5927\u7684RAG\u5b9e\u73b0\u65b9\u5f0f\uff01", "comment": "Source: WeChat, Published: 2025-10-30 11:13:57", "summary": "agent3\uff1a\u83b7\u53d6\u5b9e\u65f6\u4fe1\u606f \u3002agent4\uff1a\u8d1f\u8d23\u4e2a\u6027\u5316\u63a8\u8350\u7531\u4e3bAgent\u6574\u5408\u8f93\u51fa\u6295\u8d44\u5efa\u8bae\u30024.3 \u5c42\u7ea7\u5f0f\u67b6\u6784\uff1a\u6709\u5e8f\u7ba1\u7406\u7684\u5178\u8303\u3002query master agent response sub-agentx sub-agenty sub-agent z figure 13\uff1a an illustration of hierarchical agentic kag\u3002", "AI": {"tldr": "agent3\uff1a\u83b7\u53d6\u5b9e\u65f6\u4fe1\u606f \u3002agent4\uff1a\u8d1f\u8d23\u4e2a\u6027\u5316\u63a8\u8350\u7531\u4e3bAgent\u6574\u5408\u8f93\u51fa\u6295\u8d44\u5efa\u8bae\u30024.3 \u5c42\u7ea7\u5f0f\u67b6\u6784\uff1a\u6709\u5e8f\u7ba1\u7406\u7684\u5178\u8303\u3002query master agent response sub-agentx sub-agenty sub-agent z figure 13\uff1a an illustration of hierarchical agentic kag\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.07fff7c5", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNjM3OTI3OA==&mid=2247484761&idx=1&sn=ca03919b48ad6e1215c093afa73487e9&chksm=c3174e174819545c5755e71d11d6dc146eb6635662e05430b30620ebcdd7ed7b39d7172818fa#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNjM3OTI3OA==&mid=2247484761&idx=1&sn=ca03919b48ad6e1215c093afa73487e9&chksm=c3174e174819545c5755e71d11d6dc146eb6635662e05430b30620ebcdd7ed7b39d7172818fa#rd", "authors": ["\u601d\u60f3\u8349\u5802"], "title": "\u9ea6\u80af\u9521\u6700\u65b0\u6d1e\u5bdf\uff1a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7ec4\u7ec7\uff08<em class=\"highlight\">Agentic</em> Organization\uff09\u2014\u2014AI\u65f6\u4ee3\u5168\u65b0\u7ec4\u7ec7\u8303\u5f0f\uff0c\u4e0e\u4f60\u6211\u90fd\u76f8\u5173", "comment": "Source: WeChat, Published: 2025-10-30 05:29:42", "summary": "\u4e8c\u3001\u4e94\u5927\u652f\u67f1\uff1aAgentic Organization\u7684\u5168\u666f\u56fe\u7d27\u63a5\u7740\uff0c\u6587\u7ae0\u4e2d\u7ed3\u5408\u4e0e\u524d\u6cbf\u4f01\u4e1a\u5408\u4f5c\u7684\u5b9e\u8df5\u3001\u79d1\u6280\u9886\u8896\u4e0e\u6295\u8d44\u4eba\u7684\u6d1e\u5bdf\uff0c\u4ee5\u53ca\u9ad8\u7ba1\u4eec\u6700\u5173\u5fc3\u7684\u95ee\u9898\uff0c\u63d0\u70bc\u51fa\u4e86\u672a\u6765\u7ec4\u7ec7\u7684\u65e9\u671f\u4fe1\u53f7\u3002", "AI": {"tldr": "\u4e8c\u3001\u4e94\u5927\u652f\u67f1\uff1aAgentic Organization\u7684\u5168\u666f\u56fe\u7d27\u63a5\u7740\uff0c\u6587\u7ae0\u4e2d\u7ed3\u5408\u4e0e\u524d\u6cbf\u4f01\u4e1a\u5408\u4f5c\u7684\u5b9e\u8df5\u3001\u79d1\u6280\u9886\u8896\u4e0e\u6295\u8d44\u4eba\u7684\u6d1e\u5bdf\uff0c\u4ee5\u53ca\u9ad8\u7ba1\u4eec\u6700\u5173\u5fc3\u7684\u95ee\u9898\uff0c\u63d0\u70bc\u51fa\u4e86\u672a\u6765\u7ec4\u7ec7\u7684\u65e9\u671f\u4fe1\u53f7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.e72bf6e3", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyODU3NTkwNA==&mid=2247490510&idx=2&sn=23757d3f5a12e138bc7a5fd9a5d32850&chksm=e937e01b75dc0e5a9627ac178c289429591c98d75652344c270914d2c5a0025f226874635bc0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyODU3NTkwNA==&mid=2247490510&idx=2&sn=23757d3f5a12e138bc7a5fd9a5d32850&chksm=e937e01b75dc0e5a9627ac178c289429591c98d75652344c270914d2c5a0025f226874635bc0#rd", "authors": ["AI\u8d44\u8baf"], "title": "\u641c\u7d22\u6846\u91cc\u88c5\u4e86\u4e2a\u201c\u5927\u8111\u201d!<em class=\"highlight\">Agentic</em> Search\u4e0d\u4ec5\u80fd\u641c,\u8fd8\u80fd\u89c4\u5212\u6267\u884c\u53cd\u601d,\u5f7b\u5e95\u98a0\u8986\u4f60\u7684\u5de5\u4f5c\u6d41", "comment": "Source: WeChat, Published: 2025-10-30 04:40:04", "summary": "\u8981\u7406\u89e3Agentic Search\u7684\u6280\u672f\u539f\u7406\uff0c\u6211\u4eec\u9700\u8981\u6df1\u5165\u4e86\u89e3\u5176\u5e95\u5c42\u67b6\u6784\u3002\u6839\u636e\u5b66\u672f\u7814\u7a76\uff0c\u73b0\u4ee3\u7684Agentic Search\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u591aAgent\u534f\u540c\u67b6\u6784\u548c\u589e\u5f3aRAG\u6846\u67b6\u6784\u5efa\u3002", "AI": {"tldr": "\u8981\u7406\u89e3Agentic Search\u7684\u6280\u672f\u539f\u7406\uff0c\u6211\u4eec\u9700\u8981\u6df1\u5165\u4e86\u89e3\u5176\u5e95\u5c42\u67b6\u6784\u3002\u6839\u636e\u5b66\u672f\u7814\u7a76\uff0c\u73b0\u4ee3\u7684Agentic Search\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u591aAgent\u534f\u540c\u67b6\u6784\u548c\u589e\u5f3aRAG\u6846\u67b6\u6784\u5efa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.873effa6", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwOTQ0Njg4OA==&mid=2247485116&idx=1&sn=27bb87d617937316a0440631e8f190ef&chksm=c058b87de052e7a211ca797b230029e48f42d70292eee08d32d639de95f433343fec29b4b642#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwOTQ0Njg4OA==&mid=2247485116&idx=1&sn=27bb87d617937316a0440631e8f190ef&chksm=c058b87de052e7a211ca797b230029e48f42d70292eee08d32d639de95f433343fec29b4b642#rd", "authors": ["\u4e8e\u6e38\u7684\u788e\u788e\u5ff5"], "title": "\u9ad8\u6548\u53ef\u6269\u5c55\u7684 <em class=\"highlight\">Agentic</em> AI\uff1a\u5f53\u201c\u5f02\u6784\u7cfb\u7edf\u201d\u6210\u4e3a\u4e00\u7b49\u516c\u6c11", "comment": "Source: WeChat, Published: 2025-10-30 04:22:38", "summary": "\u8bba\u6587\u6807\u9898\uff1aEFFICIENT AND SCALABLE AGENTIC AI WITH HETEROGENEOUS SYSTEMS\u8bba\u6587\u5730\u5740\uff1ahttps\uff1a//arxiv.org/pdf/2507.196351\u3001\u8bba\u6587\u8981\u89e3\u51b3\u4ec0\u4e48\uff1aAgent \u5de5\u4f5c\u6d41\u2260\u5355\u6b21\u6a21\u578b\u63a8\u7406\u73b0\u5b9e\u4e2d\u7684Agent\u4e0d\u662f\u201c\u6253\u4e00\u67aa\u3001\u51fa\u4e00\u6bb5\u6587\u672c\u201d\u90a3\u4e48\u7b80\u5355\u3002", "AI": {"tldr": "\u8bba\u6587\u6807\u9898\uff1aEFFICIENT AND SCALABLE AGENTIC AI WITH HETEROGENEOUS SYSTEMS\u8bba\u6587\u5730\u5740\uff1ahttps\uff1a//arxiv.org/pdf/2507.196351\u3001\u8bba\u6587\u8981\u89e3\u51b3\u4ec0\u4e48\uff1aAgent \u5de5\u4f5c\u6d41\u2260\u5355\u6b21\u6a21\u578b\u63a8\u7406\u73b0\u5b9e\u4e2d\u7684Agent\u4e0d\u662f\u201c\u6253\u4e00\u67aa\u3001\u51fa\u4e00\u6bb5\u6587\u672c\u201d\u90a3\u4e48\u7b80\u5355\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.56b2f316", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwOTg3NjA3OQ==&mid=2247491732&idx=1&sn=6e7cff95eb26e57bc4c9aea90ad8d853&chksm=c030f5baea656e3a366a5406aab471c3cd4169dc79268d853c5f400e2312fedc513071f23f2e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwOTg3NjA3OQ==&mid=2247491732&idx=1&sn=6e7cff95eb26e57bc4c9aea90ad8d853&chksm=c030f5baea656e3a366a5406aab471c3cd4169dc79268d853c5f400e2312fedc513071f23f2e#rd", "authors": ["AICX"], "title": "\u4ece\u82b1\u65d7\u201c<em class=\"highlight\">Agentic</em> AI\u8ba1\u5212\u201d\uff0c\u770b\u4f01\u4e1a\u5982\u4f55\u62e5\u62b1AI", "comment": "Source: WeChat, Published: 2025-10-30 03:41:00", "summary": "\u8ba9\u6211\u4eec\u901a\u8fc7\u82b1\u65d7\u7684\u201cAgentic\u201dAI \u8ba1\u5212\uff0c\u6d1e\u5bdf\u4f01\u4e1a\u7684AI\u7ba1\u7406\u8f6c\u578b\u6837\u677f\uff0c\u4e3a\u6211\u4eec\u5e26\u6765\u542f\u793a\u3002\u82b1\u65d7\u4e5f\u5927\u5f20\u65d7\u9f13\u4e86\u82b1\u65d7\u94f6\u884c\u5728\u5176\u5185\u90e8\u5e73\u53f0 Stylus Workspaces \u542f\u52a8 \u201cAgentic\u201dAI \u8ba1\u5212\uff0c\u8ba9\u7ea6 5000 \u540d\u5458\u5de5\u80fd\u591f\u8fd0\u7528 AI Agents \u5f00\u5c55\u5de5\u4f5c\u3002", "AI": {"tldr": "\u8ba9\u6211\u4eec\u901a\u8fc7\u82b1\u65d7\u7684\u201cAgentic\u201dAI \u8ba1\u5212\uff0c\u6d1e\u5bdf\u4f01\u4e1a\u7684AI\u7ba1\u7406\u8f6c\u578b\u6837\u677f\uff0c\u4e3a\u6211\u4eec\u5e26\u6765\u542f\u793a\u3002\u82b1\u65d7\u4e5f\u5927\u5f20\u65d7\u9f13\u4e86\u82b1\u65d7\u94f6\u884c\u5728\u5176\u5185\u90e8\u5e73\u53f0 Stylus Workspaces \u542f\u52a8 \u201cAgentic\u201dAI \u8ba1\u5212\uff0c\u8ba9\u7ea6 5000 \u540d\u5458\u5de5\u80fd\u591f\u8fd0\u7528 AI Agents \u5f00\u5c55\u5de5\u4f5c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.62b6bd0c", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMTAzOTE4Nw==&mid=2247488681&idx=2&sn=eed649191fa0465129003d41ce15c8ac&chksm=f8c02ca84bbbcdacce6517f4e7e54e05099d6a71e55fdfa8b630ede2e04adbc97add5f9f3a2f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMTAzOTE4Nw==&mid=2247488681&idx=2&sn=eed649191fa0465129003d41ce15c8ac&chksm=f8c02ca84bbbcdacce6517f4e7e54e05099d6a71e55fdfa8b630ede2e04adbc97add5f9f3a2f#rd", "authors": ["TheCreativeTech"], "title": "Adobe\u5bf9<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u4eba\u5de5\u667a\u80fd\uff08<em class=\"highlight\">Agentic</em> AI\uff09\u7684\u770b\u6cd5\uff1a\u80fd\u5728\u4f60\u5e38\u7528\u5e94\u7528\u4e2d\u4e3a\u4f60\u670d\u52a1\u7684 AI \u52a9\u624b", "comment": "Source: WeChat, Published: 2025-10-30 00:36:29", "summary": "\u800c\u8fd9\u6b63\u662f\u6211\u4eec\u5728\u521b\u610f\u5e94\u7528\u4e2d\u96c6\u6210\u7684\u3001\u7531\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\uff08Agentic AI\uff09\u9a71\u52a8\u7684\u5bf9\u8bdd\u5f0f AI \u52a9\u624b\u7684\u4ef7\u503c\u6240\u5728\u3002\u5b83\u4eec\u4e0d\u5c40\u9650\u4e8e\u751f\u6210\u5355\u5f20\u56fe\u50cf\u6216\u5b8c\u6210\u5355\u6b21\u7f16\u8f91 \u2014\u2014 \u66f4\u80fd\u8fde\u63a5\u6574\u4e2a\u521b\u4f5c\u6d41\u7a0b\uff1a\u7406\u89e3\u4f60\u7684\u76ee\u6807\u3001\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u8854\u63a5\u4e0a\u4e0b\u6587\u3001\u5e2e\u52a9\u4f60", "AI": {"tldr": "\u800c\u8fd9\u6b63\u662f\u6211\u4eec\u5728\u521b\u610f\u5e94\u7528\u4e2d\u96c6\u6210\u7684\u3001\u7531\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\uff08Agentic AI\uff09\u9a71\u52a8\u7684\u5bf9\u8bdd\u5f0f AI \u52a9\u624b\u7684\u4ef7\u503c\u6240\u5728\u3002\u5b83\u4eec\u4e0d\u5c40\u9650\u4e8e\u751f\u6210\u5355\u5f20\u56fe\u50cf\u6216\u5b8c\u6210\u5355\u6b21\u7f16\u8f91 \u2014\u2014 \u66f4\u80fd\u8fde\u63a5\u6574\u4e2a\u521b\u4f5c\u6d41\u7a0b\uff1a\u7406\u89e3\u4f60\u7684\u76ee\u6807\u3001\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u8854\u63a5\u4e0a\u4e0b\u6587\u3001\u5e2e\u52a9\u4f60", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.e1182e21", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3NDgzMTIzMg==&mid=2247484370&idx=1&sn=d2fefa598395aa93c455a49c64947505&chksm=9e4c2143efda243b55c594d7b261b0696a9d648b8f1e457ba0f750da99b6760f4a382a8674c8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3NDgzMTIzMg==&mid=2247484370&idx=1&sn=d2fefa598395aa93c455a49c64947505&chksm=9e4c2143efda243b55c594d7b261b0696a9d648b8f1e457ba0f750da99b6760f4a382a8674c8#rd", "authors": ["890\u5927\u5bb6\u5ead"], "title": "\u5f53 AI \u6210\u4e3a\u4f60\u7684\u540c\u4e8b\uff1aAmazon \u5de5\u7a0b\u5e08\u7684\u201c<em class=\"highlight\">Agentic</em> \u7f16\u7a0b\u9769\u547d\u201d", "comment": "Source: WeChat, Published: 2025-10-29 23:58:35", "summary": "\u4e00\u3001\u4ec0\u4e48\u662f Agentic \u7f16\u7a0b\uff1fJoe Magerramov \u548c\u4ed6\u7684\u56e2\u961f\u6b63\u5728 Amazon Bedrock \u5185\u90e8\u63a2\u7d22\u4e00\u79cd\u5168\u65b0\u7684\u5f00\u53d1\u65b9\u5f0f\u2014\u2014Agentic Coding\uff08\u4ee3\u7406\u5f0f\u7f16\u7a0b\uff09\u3002\u4e0e\u201c\u63d0\u793a\u4e00\u4e0b AI \u5199\u70b9\u4ee3\u7801\u201d\u7684\u4f20\u7edf\u65b9\u5f0f\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u66f4\u50cf\u662f\u4eba\u7c7b\u5de5\u7a0b\u5e08\u4e0e\u667a\u80fd\u4ee3\u7406\u5171\u540c\u5b8c\u6210\u6574\u4e2a", "AI": {"tldr": "\u4e00\u3001\u4ec0\u4e48\u662f Agentic \u7f16\u7a0b\uff1fJoe Magerramov \u548c\u4ed6\u7684\u56e2\u961f\u6b63\u5728 Amazon Bedrock \u5185\u90e8\u63a2\u7d22\u4e00\u79cd\u5168\u65b0\u7684\u5f00\u53d1\u65b9\u5f0f\u2014\u2014Agentic Coding\uff08\u4ee3\u7406\u5f0f\u7f16\u7a0b\uff09\u3002\u4e0e\u201c\u63d0\u793a\u4e00\u4e0b AI \u5199\u70b9\u4ee3\u7801\u201d\u7684\u4f20\u7edf\u65b9\u5f0f\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u66f4\u50cf\u662f\u4eba\u7c7b\u5de5\u7a0b\u5e08\u4e0e\u667a\u80fd\u4ee3\u7406\u5171\u540c\u5b8c\u6210\u6574\u4e2a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.44ba3b8e", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDk4NTU0OA==&mid=2247484341&idx=1&sn=f57de7196284b05f57dfeff3805c38e1&chksm=c5f5317e2978bf6ea22d7a43f4cab5d899dbee3012ebdcd5f5e074a114751061049118516336#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDk4NTU0OA==&mid=2247484341&idx=1&sn=f57de7196284b05f57dfeff3805c38e1&chksm=c5f5317e2978bf6ea22d7a43f4cab5d899dbee3012ebdcd5f5e074a114751061049118516336#rd", "authors": ["AI\u5e94\u7528\u5b66\u5802"], "title": "\u62c6\u89e3 <em class=\"highlight\">Agentic</em> RAG \u591a\u6a21\u6001\u5b9e\u73b0\uff1a\u4ece\u81ea\u4e3b\u51b3\u7b56\u5230\u8de8\u6a21\u6001\u878d\u5408\u7684\u6838\u5fc3\u8def\u5f84", "comment": "Source: WeChat, Published: 2025-10-29 23:00:36", "summary": "Agentic RAG\uff08\u5177\u5907\u4ee3\u7406\u80fd\u529b\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u7684\u591a\u6a21\u6001\u5b9e\u73b0\uff0c\u6838\u5fc3\u662f\u8ba9\u7cfb\u7edf\u5728\u81ea\u4e3b\u51b3\u7b56\uff08Agentic\uff09\u7684\u57fa\u7840\u4e0a\uff0c\u80fd\u5904\u7406\u3001\u68c0\u7d22\u548c\u878d\u5408\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\u591a\u79cd\u6a21\u6001\u4fe1\u606f\uff0c\u6700\u7ec8\u751f\u6210\u7b26\u5408\u9700\u6c42\u7684\u591a\u6a21\u6001\u6216\u8de8\u6a21\u6001\u8f93\u51fa\u3002", "AI": {"tldr": "Agentic RAG\uff08\u5177\u5907\u4ee3\u7406\u80fd\u529b\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u7684\u591a\u6a21\u6001\u5b9e\u73b0\uff0c\u6838\u5fc3\u662f\u8ba9\u7cfb\u7edf\u5728\u81ea\u4e3b\u51b3\u7b56\uff08Agentic\uff09\u7684\u57fa\u7840\u4e0a\uff0c\u80fd\u5904\u7406\u3001\u68c0\u7d22\u548c\u878d\u5408\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\u591a\u79cd\u6a21\u6001\u4fe1\u606f\uff0c\u6700\u7ec8\u751f\u6210\u7b26\u5408\u9700\u6c42\u7684\u591a\u6a21\u6001\u6216\u8de8\u6a21\u6001\u8f93\u51fa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.e77fff95", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NzEzOTEyMQ==&mid=2649750578&idx=2&sn=67ecbbc54c6dc3e3c75f6c112639acdb&chksm=bff9f6ee66a328a5d3f49f770cdf6515668db81e28044f5fc76bda11427c6b1830bd27b875d8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NzEzOTEyMQ==&mid=2649750578&idx=2&sn=67ecbbc54c6dc3e3c75f6c112639acdb&chksm=bff9f6ee66a328a5d3f49f770cdf6515668db81e28044f5fc76bda11427c6b1830bd27b875d8#rd", "authors": ["\u798f\u5efaCIO\u7f51"], "title": "\u544a\u522b\u201c\u6551\u706b\u201d\uff01\u201c<em class=\"highlight\">\u4ee3\u7406</em>\u5f0f\u4eba\u5de5\u667a\u80fd\u201d\uff08<em class=\"highlight\">Agentic</em> AI\uff09\u6b63\u6380\u8d77\u4e00\u573aIT\u8fd0\u7ef4\u9769\u547d", "comment": "Source: WeChat, Published: 2025-10-29 22:18:40", "summary": "\u4e3b\u89d2\u5c31\u662f\u2014\u2014\u201c\u4ee3\u7406\u5f0f\u4eba\u5de5\u667a\u80fd\u201d\uff08Agentic AI\uff09\u3002\u5b83\u4e0e\u4f20\u7edf\u81ea\u52a8\u5316\uff08RPA\u6216\u811a\u672c\uff09\u7684\u6839\u672c\u533a\u522b\u5728\u4e8e\uff1a\u5b83\u4e0d\u518d\u662f\u201c\u6267\u884c\u201d\u4f60\u8bbe\u5b9a\u7684\u91cd\u590d\u4efb\u52a1\uff0c\u800c\u662f\u80fd\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\uff0c\u5728\u51e0\u4e4e\u65e0\u9700\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u3001\u5e76\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002", "AI": {"tldr": "\u4e3b\u89d2\u5c31\u662f\u2014\u2014\u201c\u4ee3\u7406\u5f0f\u4eba\u5de5\u667a\u80fd\u201d\uff08Agentic AI\uff09\u3002\u5b83\u4e0e\u4f20\u7edf\u81ea\u52a8\u5316\uff08RPA\u6216\u811a\u672c\uff09\u7684\u6839\u672c\u533a\u522b\u5728\u4e8e\uff1a\u5b83\u4e0d\u518d\u662f\u201c\u6267\u884c\u201d\u4f60\u8bbe\u5b9a\u7684\u91cd\u590d\u4efb\u52a1\uff0c\u800c\u662f\u80fd\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\uff0c\u5728\u51e0\u4e4e\u65e0\u9700\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u3001\u5e76\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.0d604050", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NzEzOTEyMQ==&mid=2649750578&idx=1&sn=54922d644bf9279d078801714dfe6e19&chksm=bf3b4efb54a71f6a48240a68af0b9d8baeeef05360966de59bac780d9667408e0a6251cc3b04#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NzEzOTEyMQ==&mid=2649750578&idx=1&sn=54922d644bf9279d078801714dfe6e19&chksm=bf3b4efb54a71f6a48240a68af0b9d8baeeef05360966de59bac780d9667408e0a6251cc3b04#rd", "authors": ["\u798f\u5efaCIO\u7f51"], "title": "\u3010\u777f\u89c2\u3011IT\u8fd0\u8425\u7ec8\u5c40\u4e4b\u6218\uff1a\u4ece\u201c\u6551\u706b\u961f\u201d\u5230\u201c\u7cfb\u7edf\u5efa\u7b51\u5e08\u201d\uff0cAI\u81ea\u4e3b<em class=\"highlight\">\u667a\u80fd\u4f53</em>\uff08<em class=\"highlight\">Agentic</em> AI\uff09\u5982\u4f55\u5f15\u7206\u4e0b\u4e00\u573a\u9769\u547d", "comment": "Source: WeChat, Published: 2025-10-29 22:18:40", "summary": "agentic ai cloud cost optimization\u591a\u5e74\u6765\uff0cIT\u8fd0\u8425\u7684\u6210\u529f\u6307\u6807\u59cb\u7ec8\u56f4\u7ed5\u7740\u53ef\u7528\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6027\u80fd \u3002\u6211\u4eec\u4f9d\u8d56\u201c\u4eba+\u6d41\u7a0b+\u6280\u672f\u201d\u7684\u94c1\u4e09\u89d2\u7ec4\u5408\u6765\u4fdd\u969c\u8fd9\u4e00\u5207\u3002", "AI": {"tldr": "agentic ai cloud cost optimization\u591a\u5e74\u6765\uff0cIT\u8fd0\u8425\u7684\u6210\u529f\u6307\u6807\u59cb\u7ec8\u56f4\u7ed5\u7740\u53ef\u7528\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6027\u80fd \u3002\u6211\u4eec\u4f9d\u8d56\u201c\u4eba+\u6d41\u7a0b+\u6280\u672f\u201d\u7684\u94c1\u4e09\u89d2\u7ec4\u5408\u6765\u4fdd\u969c\u8fd9\u4e00\u5207\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.2656379d", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNDc0MTExMw==&mid=2247483656&idx=1&sn=d5671c2a0fa6eb995b0e1317cb54d4d5&chksm=f10f62cf088d0c8e2599fc1774969bee224f93b3becbc34213bc7a832c9d9c2d47d560c53677#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNDc0MTExMw==&mid=2247483656&idx=1&sn=d5671c2a0fa6eb995b0e1317cb54d4d5&chksm=f10f62cf088d0c8e2599fc1774969bee224f93b3becbc34213bc7a832c9d9c2d47d560c53677#rd", "authors": ["Benjamin Daoson"], "title": "\u4e3a\u4ec0\u4e48 2026 \u5fc5\u7136\u5c5e\u4e8e <em class=\"highlight\">Agentic</em> AI\uff1f", "comment": "Source: WeChat, Published: 2025-10-29 17:23:14", "summary": "\u5b83\u7684\u903b\u8f91\u662f\uff1a\u6211\u544a\u8bc9\u4f60\u600e\u4e48\u505a\uff0c\u4f60\u7167\u7740\u505a\u3002\u800c Agentic AI \u662f\uff1a\u6211\u544a\u8bc9\u4f60\u8981\u8fbe\u5230\u4ec0\u4e48\uff0c\u4f60\u81ea\u5df1\u51b3\u5b9a\u600e\u4e48\u505a\u3002\u8fd9\u4e24\u8005\u7684\u5dee\u5f02\uff0c\u4e0d\u662f\u4f18\u5316\uff0c\u800c\u662f\u8303\u5f0f\u5206\u754c\u7ebf\u3002\u80fd\u529b\u7ef4\u5ea6", "AI": {"tldr": "\u5b83\u7684\u903b\u8f91\u662f\uff1a\u6211\u544a\u8bc9\u4f60\u600e\u4e48\u505a\uff0c\u4f60\u7167\u7740\u505a\u3002\u800c Agentic AI \u662f\uff1a\u6211\u544a\u8bc9\u4f60\u8981\u8fbe\u5230\u4ec0\u4e48\uff0c\u4f60\u81ea\u5df1\u51b3\u5b9a\u600e\u4e48\u505a\u3002\u8fd9\u4e24\u8005\u7684\u5dee\u5f02\uff0c\u4e0d\u662f\u4f18\u5316\uff0c\u800c\u662f\u8303\u5f0f\u5206\u754c\u7ebf\u3002\u80fd\u529b\u7ef4\u5ea6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.420aa1aa", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNTQzMjUzNQ==&mid=2247484381&idx=1&sn=b827a4640251893aab588f6afa23ecbb&chksm=f1398910ab6f2e72e627a5d8d32f4b987d6091b46458e278225674d7272fd0c1f5bcef444a33#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNTQzMjUzNQ==&mid=2247484381&idx=1&sn=b827a4640251893aab588f6afa23ecbb&chksm=f1398910ab6f2e72e627a5d8d32f4b987d6091b46458e278225674d7272fd0c1f5bcef444a33#rd", "authors": ["\u7535\u6c14\u6df7\u6df7"], "title": "\u534e\u4e3a\u6253\u54cd<em class=\"highlight\">Agentic</em> AI\u7b2c\u4e00\u67aa\uff0c\u4f01\u4e1a\u8fd0\u7ef4\u8fce\u6765\u7ec8\u6781\u8fdb\u5316", "comment": "Source: WeChat, Published: 2025-10-29 15:07:57", "summary": "Agentic AI\u4ee3\u8868\u7740\u8fd0\u7ef4\u9886\u57df\u7684\u8303\u5f0f\u8f6c\u79fb\u3002\u4f01\u4e1a\u9762\u4e34\u7684\u9009\u62e9\u5f88\u7b80\u5355\uff1a\u8981\u4e48\u4e3b\u52a8\u62e5\u62b1\uff0c\u8981\u4e48\u88ab\u52a8\u6dd8\u6c70\u3002\u6b63\u5982\u534e\u4e3a\u516c\u5171\u5f00\u53d1\u90e8\u603b\u88c1\u9646\u6d77\u9e25\u6240\u8a00\uff1a\u201c\u8ba9\u6211\u4eec\u4e00\u8d77\u8fc8\u5411\u667a\u80fd\u8fd0\u7ef4\u7684\u65b0\u7eaa\u5143\u3002\u201d", "AI": {"tldr": "Agentic AI\u4ee3\u8868\u7740\u8fd0\u7ef4\u9886\u57df\u7684\u8303\u5f0f\u8f6c\u79fb\u3002\u4f01\u4e1a\u9762\u4e34\u7684\u9009\u62e9\u5f88\u7b80\u5355\uff1a\u8981\u4e48\u4e3b\u52a8\u62e5\u62b1\uff0c\u8981\u4e48\u88ab\u52a8\u6dd8\u6c70\u3002\u6b63\u5982\u534e\u4e3a\u516c\u5171\u5f00\u53d1\u90e8\u603b\u88c1\u9646\u6d77\u9e25\u6240\u8a00\uff1a\u201c\u8ba9\u6211\u4eec\u4e00\u8d77\u8fc8\u5411\u667a\u80fd\u8fd0\u7ef4\u7684\u65b0\u7eaa\u5143\u3002\u201d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.b11219ce", "categories": ["wechat.article", "wechat.rl", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNTg3NTgxNQ==&mid=2247485623&idx=1&sn=798bbab920bb9142af4fdda8c0ac52c6&chksm=c37633d20c9854c42fc7fe7ff4f33ecda89f719b9a784513716066f8d7d2c4f2283417cbb7c5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNTg3NTgxNQ==&mid=2247485623&idx=1&sn=798bbab920bb9142af4fdda8c0ac52c6&chksm=c37633d20c9854c42fc7fe7ff4f33ecda89f719b9a784513716066f8d7d2c4f2283417cbb7c5#rd", "authors": ["\u62d3\u6251\u5b66\u672f"], "title": "\u3010\u9752\u7a1eTalk 78\u671f\u3011\u4ece LLM-RL \u5230 <em class=\"highlight\">Agentic</em> RL\uff1a\u5982\u4f55\u8ba9\u8bed\u8a00\u6a21\u578b\u6210\u4e3a\u81ea\u4e3b<em class=\"highlight\">\u667a\u80fd\u4f53</em>", "comment": "Source: WeChat, Published: 2025-10-29 14:55:24", "summary": "\u7b2c\u4e00\u7ae0 Agentic RL \u7684\u63d0\u51fa\u4e0e\u5b9a\u4e491.1 Agentic RL\u7684\u6982\u5ff5\u6f14\u8fdb\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u8bad\u7ec3\u667a\u80fd\u4f53\u7684\u8303\u5f0f\uff0c\u672c\u8eab\u5373\u5305\u542bAgent\u7684\u6982\u5ff5\uff0c\u4f46\u57282025\u5e74\u7684\u8bed\u5883\u4e2d\uff0c\u201cAgentic RL\u201d\u5f00\u59cb\u88ab\u7279\u6307\u4e3a\u201c\u5e2e\u52a9\u8bed\u8a00\u6a21\u578b\u4ece\u5355\u8f6e\u56de\u590d\u8f6c\u53d8\u4e3a\u591a\u8f6e\u3001\u52a8\u6001\u73af\u5883\u4e0b\u81ea\u4e3b\u4ea4\u4e92\u7684\u667a\u80fd\u4f53", "AI": {"tldr": "\u7b2c\u4e00\u7ae0 Agentic RL \u7684\u63d0\u51fa\u4e0e\u5b9a\u4e491.1 Agentic RL\u7684\u6982\u5ff5\u6f14\u8fdb\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u8bad\u7ec3\u667a\u80fd\u4f53\u7684\u8303\u5f0f\uff0c\u672c\u8eab\u5373\u5305\u542bAgent\u7684\u6982\u5ff5\uff0c\u4f46\u57282025\u5e74\u7684\u8bed\u5883\u4e2d\uff0c\u201cAgentic RL\u201d\u5f00\u59cb\u88ab\u7279\u6307\u4e3a\u201c\u5e2e\u52a9\u8bed\u8a00\u6a21\u578b\u4ece\u5355\u8f6e\u56de\u590d\u8f6c\u53d8\u4e3a\u591a\u8f6e\u3001\u52a8\u6001\u73af\u5883\u4e0b\u81ea\u4e3b\u4ea4\u4e92\u7684\u667a\u80fd\u4f53", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
