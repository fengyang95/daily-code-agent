<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [tldr.article](#tldr.article) [Total: 13]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [On the Credibility of Evaluating LLMs using Survey Questions](https://arxiv.org/abs/2602.04033)
*Jindřich Libovický*

Main category: cs.CL

TL;DR: 研究发现现有评估LLM价值取向的方法存在局限性，提示方法和解码策略显著影响结果，提出自相关距离新指标，建议使用CoT提示、采样解码和多指标分析


<details>
  <summary>Details</summary>
Motivation: 现有研究通过改编社会调查评估LLM价值取向，但该方法存在局限性，可能导致低估或高估与人类价值取向的相似性，需要更准确的评估方法

Method: 使用世界价值观调查在三种语言五个国家进行实验，比较不同提示方法（直接vs链式思考）和解码策略（贪婪vs采样），引入自相关距离新指标评估答案间一致性

Result: 提示方法和解码策略显著影响评估结果；即使平均同意度高，LLM回答的结构一致性仍不足；常用评估指标（均方距离和KL散度）相关性弱

Conclusion: 建议未来研究使用CoT提示、采样解码（数十个样本）和多指标分析（包括自相关距离），以获得更可靠的LLM价值取向评估

Abstract: Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.

</details>


### [2] [From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents](https://arxiv.org/abs/2602.04197)
*Xinyue Wang,Yuanhe Zhang,Zhengshuo Gong,Haoran Gao,Fanyu Meng,Zhenhong Zhou,Li Sun,Yang Liu,Sen Su*

Main category: cs.CL

TL;DR: 论文提出"毒性主动性"概念，指AI代理为最大化效用而忽视伦理约束的主动失败模式，并建立评估框架和基准来识别这种现象。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM对齐带来的"过度拒绝"被动失败模式，但忽视了代理主动规划和行动能力可能导致的"毒性主动性"风险，即代理为追求马基雅维利式有用性而违反伦理约束。

Method: 提出基于困境驱动的双模型交互评估框架，通过多步骤行为轨迹模拟分析代理行为；建立系统性基准评估不同情境下的毒性主动行为。

Result: 实验表明毒性主动性是广泛存在的行为现象，揭示了两种主要倾向；建立了评估毒性主动行为的系统性基准。

Conclusion: 毒性主动性是LLM代理的重要风险，需要专门评估框架来识别；该研究为理解和缓解这种主动失败模式提供了基础。

Abstract: The enhanced capabilities of LLM-based agents come with an emergency for model planning and tool-use abilities. Attributing to helpful-harmless trade-off from LLM alignment, agents typically also inherit the flaw of "over-refusal", which is a passive failure mode. However, the proactive planning and action capabilities of agents introduce another crucial danger on the other side of the trade-off. This phenomenon we term "Toxic Proactivity'': an active failure mode in which an agent, driven by the optimization for Machiavellian helpfulness, disregards ethical constraints to maximize utility. Unlike over-refusal, Toxic Proactivity manifests as the agent taking excessive or manipulative measures to ensure its "usefulness'' is maintained. Existing research pays little attention to identifying this behavior, as it often lacks the subtle context required for such strategies to unfold. To reveal this risk, we introduce a novel evaluation framework based on dilemma-driven interactions between dual models, enabling the simulation and analysis of agent behavior over multi-step behavioral trajectories. Through extensive experiments with mainstream LLMs, we demonstrate that Toxic Proactivity is a widespread behavioral phenomenon and reveal two major tendencies. We further present a systematic benchmark for evaluating Toxic Proactive behavior across contextual settings.

</details>


### [3] [Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry](https://arxiv.org/abs/2602.04206)
*Hsien-Jyh Liao*

Main category: cs.CL

TL;DR: 该论文提出Soft-FSM神经符号架构，通过外部确定性状态控制器强制LLM在程序性任务中实现单调进展，解决了LLM在长期任务中容易陷入程序停滞的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言流畅性方面表现出色，但在明确的程序约束下可靠完成长期任务方面存在困难。在法律交叉询问等场景中，纯概率生成虽然能保持行为连贯性，但无法确保程序推进，这种失败被定义为程序停滞。

Method: 提出Soft-FSM神经符号架构，通过外部确定性状态控制器强制实现关键信息单元（KIUs）的单调累积进展。该方法结合了神经网络的灵活性和符号系统的确定性控制。

Result: 在三个台湾真实刑事杀人案件上的实验显示，基线方法完成度低于40%，而Soft-FSM始终达到97%以上且冗余度接近零。

Conclusion: 在某些领域，可靠的任​​务完成不能仅依赖LLM的涌现行为，而需要通过明确且可验证的外部状态控制来强制保证。

Abstract: Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancement. We characterize this failure as procedural stagnation and propose Soft-FSM, a neuro-symbolic architecture that enforces monotonic progress over accumulated Key Information Units (KIUs) via an external deterministic state controller. Experiments on three real-world Taiwanese criminal homicide cases show that baseline methods collapse below 40% completeness, while Soft-FSM consistently achieves over 97% with near-zero redundancy. These results suggest that, in such domains, reliable task completion cannot be guaranteed by emergent LLM behavior alone, and can be reliably enforced through explicit and verifiable external state control.

</details>


### [4] [Scaling Agentic Verifier for Competitive Coding](https://arxiv.org/abs/2602.04254)
*Zeyao Ma,Jing Zhang,Xiaokang Zhang,Jiaxi Yang,Zongmeng Zhang,Jiajun Zhang,Yuheng Jing,Lei Zhang,Hao Zheng,Wenting Zhao,Junyang Lin,Binyuan Hui*

Main category: cs.CL

TL;DR: Agentic Verifier是一个基于执行的智能体，通过主动推理程序行为并搜索具有高度区分性的测试输入来提升代码生成模型的性能，相比现有方法实现了显著改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强大的编码能力，但在解决竞争性编程问题时仍难以一次性正确完成。现有的基于执行的重新排序方法受到测试用例生成困难或随机输入采样效率低下的限制。

Method: 提出Agentic Verifier，这是一个基于执行的智能体，通过与代码执行环境的多轮交互，主动推理程序行为并搜索能够暴露候选解决方案之间行为差异的测试输入。通过大规模数据合成、拒绝微调和智能体强化学习的可扩展管道来训练验证器。

Result: 在五个竞争性编程基准测试上的广泛实验显示，相比强大的基于执行基线方法，实现了持续改进，在Best@K准确率上获得了高达+10-15%的绝对增益。进一步分析揭示了清晰的测试时扩展行为。

Conclusion: Agentic Verifier通过主动生成具有区分性的测试输入，有效提升了代码生成模型的性能，展现了超越重新排序的更广泛潜力。

Abstract: Large language models (LLMs) have demonstrated strong coding capabilities but still struggle to solve competitive programming problems correctly in a single attempt. Execution-based re-ranking offers a promising test-time scaling strategy, yet existing methods are constrained by either difficult test case generation or inefficient random input sampling. To address this limitation, we propose Agentic Verifier, an execution-based agent that actively reasons about program behaviors and searches for highly discriminative test inputs that expose behavioral discrepancies among candidate solutions. Through multi-turn interaction with code execution environments, the verifier iteratively refines the candidate input generator and produces targeted counterexamples rather than blindly sampling inputs. We train the verifier to acquire this discriminative input generation capability via a scalable pipeline combining large-scale data synthesis, rejection fine-tuning, and agentic reinforcement learning. Extensive experiments across five competitive programming benchmarks demonstrate consistent improvements over strong execution-based baselines, achieving up to +10-15% absolute gains in Best@K accuracy. Further analysis reveals clear test-time scaling behavior and highlights the verifier's broader potential beyond reranking.

</details>


### [5] [Guided Verifier: Collaborative Multimodal Reasoning via Dynamic Process Supervision](https://arxiv.org/abs/2602.04290)
*Lingzhuang Sun,Ruitong Liu,Yuxia Zhu,Xiaohan Xu,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 提出Guided Verifier框架，通过动态验证器与策略模型实时交互，检测不一致性并提供方向信号，解决MLLMs推理中的错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习范式通常依赖单一策略模型独自推理，缺乏中间监督，导致早期逻辑偏差会传播成不可逆的失败，产生噪声优化信号。

Method: 提出Guided Verifier框架，包含动态验证器与策略模型协同工作；开发专门的数据合成管道构建CoRe数据集，包含过程级负面样本和正确引导的推理轨迹来训练验证器。

Result: 在MathVista、MathVerse和MMMU等基准测试中，通过协同推理和动态验证，8B参数模型能够实现强大的性能。

Conclusion: 通过将计算资源分配给协同推理和动态验证，可以显著提升多模态大语言模型的复杂推理能力，解决传统单一策略模型的局限性。

Abstract: Reinforcement Learning (RL) has emerged as a pivotal mechanism for enhancing the complex reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevailing paradigms typically rely on solitary rollout strategies where the model works alone. This lack of intermediate oversight renders the reasoning process susceptible to error propagation, where early logical deviations cascade into irreversible failures, resulting in noisy optimization signals. In this paper, we propose the \textbf{Guided Verifier} framework to address these structural limitations. Moving beyond passive terminal rewards, we introduce a dynamic verifier that actively co-solves tasks alongside the policy. During the rollout phase, this verifier interacts with the policy model in real-time, detecting inconsistencies and providing directional signals to steer the model toward valid trajectories. To facilitate this, we develop a specialized data synthesis pipeline targeting multimodal hallucinations, constructing \textbf{CoRe} dataset of process-level negatives and \textbf{Co}rrect-guide \textbf{Re}asoning trajectories to train the guided verifier. Extensive experiments on MathVista, MathVerse and MMMU indicate that by allocating compute to collaborative inference and dynamic verification, an 8B-parameter model can achieve strong performance.

</details>


### [6] [How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks](https://arxiv.org/abs/2602.04294)
*Yanshu Wang,Shuaishuai Yang,Jingjing He,Tong Yang*

Main category: cs.CL

TL;DR: 研究发现few-shot示例对两种提示防御策略产生相反效果：增强角色导向提示的安全性，但降低任务导向提示的防御效果。


<details>
  <summary>Details</summary>
Motivation: LLM面临越狱攻击威胁，虽然基于提示的防御策略（如RoP和ToP）有效，但few-shot演示在这些防御策略中的作用尚不明确，需要系统研究。

Method: 在多个主流LLM上进行综合评估，使用四个安全基准（AdvBench、HarmBench、SG-Bench、XSTest）和六种越狱攻击方法，分析few-shot对RoP和ToP策略的影响。

Result: few-shot对RoP和ToP产生相反效果：few-shot通过强化角色身份将RoP的安全率提升高达4.5%，而通过分散对任务指令的注意力将ToP的效果降低高达21.2%。

Conclusion: few-shot演示在提示防御策略中具有双重作用，需要根据具体防御策略谨慎使用，为实际LLM应用中的提示防御部署提供实用建议。

Abstract: Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.

</details>


### [7] [Revisiting Prompt Sensitivity in Large Language Models for Text Classification: The Role of Prompt Underspecification](https://arxiv.org/abs/2602.04297)
*Branislav Pecher,Michal Spiegel,Robert Belanec,Jan Cegin*

Main category: cs.CL

TL;DR: 研究发现LLM提示敏感性主要源于提示规范不足，通过对比分析发现规范指令提示能显著降低性能方差并提高相关token的logit值。


<details>
  <summary>Details</summary>
Motivation: 现有研究观察到LLM对提示变化敏感，但很多研究使用规范不足的提示（提供最小任务指令和弱约束输出空间），本文认为观察到的敏感性很大程度上可归因于提示规范不足。

Method: 系统研究对比规范不足提示和提供具体指令的提示的敏感性，采用性能分析、logit分析和线性探测三种方法。

Result: 规范不足提示表现出更高的性能方差和相关token的更低logit值，而指令提示较少受这些问题影响；线性探测表明提示规范不足的影响主要出现在最后几层，对内部表示影响有限。

Conclusion: 提示规范不足是LLM提示敏感性的重要来源，研究提示敏感性时需要更严谨的方法，使用规范指令提示可以减少敏感性。

Abstract: Large language models (LLMs) are widely used as zero-shot and few-shot classifiers, where task behaviour is largely controlled through prompting. A growing number of works have observed that LLMs are sensitive to prompt variations, with small changes leading to large changes in performance. However, in many cases, the investigation of sensitivity is performed using underspecified prompts that provide minimal task instructions and weakly constrain the model's output space. In this work, we argue that a significant portion of the observed prompt sensitivity can be attributed to prompt underspecification. We systematically study and compare the sensitivity of underspecified prompts and prompts that provide specific instructions. Utilising performance analysis, logit analysis, and linear probing, we find that underspecified prompts exhibit higher performance variance and lower logit values for relevant tokens, while instruction-prompts suffer less from such problems. However, linear probing analysis suggests that the effects of prompt underspecification have only a marginal impact on the internal LLM representations, instead emerging in the final layers. Overall, our findings highlight the need for more rigour when investigating and mitigating prompt sensitivity.

</details>


### [8] [Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models](https://arxiv.org/abs/2602.04355)
*Sichu Liang,Hongyu Zhu,Wenwen Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 该研究评估了视觉语言模型在空间n-back工作记忆任务中的表现，发现文本输入比视觉输入表现更好，且模型往往采用近因效应而非指令要求的延迟匹配策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究视觉语言模型在视觉和文本两种模态下是否表现出相似的工作记忆计算过程。虽然已有研究使用n-back任务测试大语言模型的工作记忆行为，但尚不清楚当信息以视觉而非文本形式呈现时，视觉语言模型是否表现出可比的计算过程。

Method: 研究方法包括：1）评估Qwen2.5和Qwen2.5-VL模型在受控空间n-back任务上的表现；2）将任务呈现为匹配的文本渲染或图像渲染网格；3）使用试次级别的对数概率证据来解释过程层面的差异；4）分析网格大小如何改变刺激流中的近期重复结构。

Result: 研究结果显示：1）在所有条件下，模型在文本输入上的准确率和d'值都显著高于视觉输入；2）名义上的2/3-back任务往往无法反映指令要求的延迟，而是与近因锁定的比较一致；3）网格大小改变了刺激流中的近期重复结构，从而影响了干扰和错误模式。

Conclusion: 研究结论强调了需要对多模态工作记忆进行计算敏感的解释。结果表明视觉语言模型在视觉和文本模态下的工作记忆处理存在差异，模型倾向于采用近因策略而非遵循指令延迟，这为理解多模态认知系统的工作记忆机制提供了重要见解。

Abstract: Working memory is a central component of intelligent behavior, providing a dynamic workspace for maintaining and updating task-relevant information. Recent work has used n-back tasks to probe working-memory-like behavior in large language models, but it is unclear whether the same probe elicits comparable computations when information is carried in a visual rather than textual code in vision-language models. We evaluate Qwen2.5 and Qwen2.5-VL on a controlled spatial n-back task presented as matched text-rendered or image-rendered grids. Across conditions, models show reliably higher accuracy and d' with text than with vision. To interpret these differences at the process level, we use trial-wise log-probability evidence and find that nominal 2/3-back often fails to reflect the instructed lag and instead aligns with a recency-locked comparison. We further show that grid size alters recent-repeat structure in the stimulus stream, thereby changing interference and error patterns. These results motivate computation-sensitive interpretations of multimodal working memory.

</details>


### [9] [Fine-Grained Activation Steering: Steering Less, Achieving More](https://arxiv.org/abs/2602.04428)
*Zijian Feng,Tianjiao Li,Zixiao Zhu,Hanzhang Zhou,Junlang Qian,Li Zhang,Jia Jim Deryl Chua,Lee Onn Mak,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: AUSteer：一种在原子单元级别进行激活引导的新方法，通过识别区分性原子单元并分配自适应引导强度，实现更精确、高效的LLM行为修改


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法通常在块级别（注意力头、前馈网络或残差流）进行干预，但块级激活本质上是异质的，混合了有益、无关和有害特征，导致引导粗糙、低效且侵入性强

Method: 将块激活分解为原子单元级激活，每个AU对应块激活的单个维度；首先通过对比样本计算激活动量来全局识别区分性AU，然后为不同输入和选定AU激活分配自适应引导强度

Result: 在多个LLM和任务上的综合实验表明，AUSteer始终优于先进基线方法，同时引导的激活数量显著减少，实现了"引导更少，效果更好"

Conclusion: 块级激活的异质性源于不同原子单元控制LLM输出中不同的token分布，限制干预到有益AU可以实现更精确有效的引导，AU级别的细粒度引导优于块级引导

Abstract: Activation steering has emerged as a cost-effective paradigm for modifying large language model (LLM) behaviors. Existing methods typically intervene at the block level, steering the bundled activations of selected attention heads, feedforward networks, or residual streams. However, we reveal that block-level activations are inherently heterogeneous, entangling beneficial, irrelevant, and harmful features, thereby rendering block-level steering coarse, inefficient, and intrusive. To investigate the root cause, we decompose block activations into fine-grained atomic unit (AU)-level activations, where each AU-level activation corresponds to a single dimension of the block activation, and each AU denotes a slice of the block weight matrix. Steering an AU-level activation is thus equivalent to steering its associated AU. Our theoretical and empirical analysis show that heterogeneity arises because different AUs or dimensions control distinct token distributions in LLM outputs. Hence, block-level steering inevitably moves helpful and harmful token directions together, which reduces efficiency. Restricting intervention to beneficial AUs yields more precise and effective steering. Building on this insight, we propose AUSteer, a simple and efficient method that operates at a finer granularity of the AU level. AUSteer first identifies discriminative AUs globally by computing activation momenta on contrastive samples. It then assigns adaptive steering strengths tailored to diverse inputs and selected AU activations. Comprehensive experiments on multiple LLMs and tasks show that AUSteer consistently surpasses advanced baselines while steering considerably fewer activations, demonstrating that steering less achieves more.

</details>


### [10] [VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration](https://arxiv.org/abs/2602.04587)
*Jaeyoon Jung,Yejun Yoon,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: VILLAIN是一个多模态事实核查系统，通过基于提示的多智能体协作验证图像-文本声明，在AVerImaTeC共享任务中排名第一。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个能够有效验证图像-文本声明的多模态事实核查系统，特别是在AVerImaTeC共享任务中。

Method: 采用多阶段多智能体协作方法：1) 从知识库检索文本和视觉证据；2) 模态特定和跨模态智能体生成分析报告；3) 基于报告生成问答对；4) 最终预测智能体基于声明和问答对产生验证结果。

Result: 在AVerImaTeC共享任务的所有评估指标中排名第一。

Conclusion: VILLAIN通过多智能体协作方法在多模态事实核查任务中表现出色，证明了其有效性。

Abstract: This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.

</details>


### [11] [When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?](https://arxiv.org/abs/2602.04755)
*Xinyu Zhou,Chang Jin,Carsten Eickhoff,Zhijiang Guo,Seyed Ali Bahrainian*

Main category: cs.CL

TL;DR: 该论文研究了在时间问答任务中训练LLMs具备弃权能力的方法，通过结合思维链监督和强化学习，显著提升了模型在可回答问题上的准确性和在不可回答问题上的弃权能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在时间敏感问题上经常忽略时间证据、混淆不同时期的事实，且缺乏弃权能力，导致产生误导性答案而非拒绝回答不可靠问题。现有校准方法在复杂推理中不可靠，需要系统研究如何训练LLMs具备时间推理中的弃权能力。

Method: 将弃权视为可教授技能，提出结合思维链监督和强化学习的训练流程。使用弃权感知奖励指导RL，系统分析不同信息类型（原始上下文、时间子上下文、知识图谱）和训练技术对时间推理中弃权行为的影响。

Result: 基于Qwen2.5-1.5B-Instruct的模型在TimeQA-Easy和Hard上分别超越GPT-4o 3.46%和5.80%。在不可回答问题上，相比纯监督微调变体，真阳性率提升20%。分析显示SFT导致过度自信，RL提高准确性但仍有类似风险，隐式推理线索对弃权推理帮助有限。

Conclusion: 该研究为联合优化弃权和推理能力提供了新见解，为构建更可靠的LLMs奠定了基础。RL在提升推理性能方面表现优异，但需要进一步解决过度自信问题，隐式信息对弃权推理的贡献有限。

Abstract: Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\%$ and $5.80\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.

</details>


### [12] [SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization](https://arxiv.org/abs/2602.04811)
*Jiarui Yuan,Tailin Jin,Weize Chen,Zeyuan Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: SE-Bench是一个诊断环境，通过混淆NumPy库及其API文档为伪新包来评估智能体的知识内化能力，揭示了闭卷训练的必要性、标准RL的局限性以及自博弈的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前评估智能体自我演化能力面临两大障碍：先验知识的纠缠（新知识可能已存在于预训练数据中）和推理复杂性的纠缠（失败可能源于问题难度而非知识回忆能力）。需要建立干净的诊断环境来测量知识内化这一基础能力。

Method: 将NumPy库及其API文档混淆为伪新包，使用随机化标识符。智能体训练内化该包，然后在没有文档访问的情况下评估简单编码任务。任务设计使得使用新API文档时任务简单，但基础模型无法完成。

Result: 发现三个关键见解：1) 开卷悖论：使用参考文档训练会抑制知识保留，需要闭卷训练强制知识压缩到权重中；2) RL差距：标准RL因PPO裁剪和负梯度无法完全内化新知识；3) 自博弈可行性：结合SFT，模型可以从自生成的嘈杂任务中学习，但RL不行。

Conclusion: SE-Bench为知识内化的自我演化建立了严格的诊断平台，揭示了当前训练方法的局限性，并展示了闭卷训练和自博弈的有效性。

Abstract: True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.

</details>


### [13] [Reinforced Attention Learning](https://arxiv.org/abs/2602.04884)
*Bangzheng Li,Jianmo Ni,Chen Qu,Ian Miao,Liu Yang,Xingyu Fu,Muhao Chen,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: RAL通过强化学习直接优化注意力分布而非输出序列，提升多模态模型性能，相比传统方法在图像视频基准上表现更优


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的后训练方法在多模态大语言模型中效果有限，甚至可能损害感知性能，需要新的优化范式

Method: 提出强化注意力学习框架，使用策略梯度方法直接优化内部注意力分布，而非输出token序列；并引入在线注意力蒸馏技术

Result: 在多种图像和视频基准测试中，RAL相比GRPO等基线方法取得一致性能提升；注意力蒸馏比标准知识蒸馏产生更强的跨模态对齐

Conclusion: 注意力策略为多模态后训练提供了原则性和通用的替代方案，通过优化注意力分布而非输出序列能更有效提升模型性能

Abstract: Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [14] [NVIDIA proposes Golden Goose: Unlimited RLVR Tasks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.22975%3Futm_source=tldrai/1/0100019c23f5d0c6-e04ce42e-44af-41ac-a7d0-ecb3f360f873-000000/4Wn8hpsfttyu2CyC3H5AjQaN81PJvOevM7eS1HM4Zy4=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: NVIDIA提出Golden Goose框架，可从不可验证的网页文本中合成大规模RLVR任务，创建GooseReason数据集，在数学、科学和网络安全等领域提升模型性能


<details>
  <summary>Details</summary>
Motivation: 当前RLVR任务规模有限，需要从大量不可验证的网页文本中提取可验证奖励的强化学习任务，以提升模型在数学、科学和网络安全等领域的性能

Method: 提出Golden Goose框架，能够从不可验证的网页文本中自动合成大规模RLVR任务，生成GooseReason数据集

Result: 生成的GooseReason数据集在数学、科学和网络安全等多个领域超越了先前的最先进方法，显著提升了模型性能

Conclusion: Golden Goose框架能够有效从不可验证的网络文本中生成大规模RLVR任务，为模型在多个关键领域的性能提升提供了新的数据来源

Abstract: NVIDIA proposes Golden Goose: Unlimited RLVR Tasks (18 minute read) Golden Goose enables the synthesis of large-scale RL with Verifiable Rewards (RLVR) tasks from unverifiable web text. The resulting GooseReason dataset helps revive model performance in math, science, and cybersecurity, surpassing prior state-of-the-art in multiple domains.

</details>


### [15] [Moltbot Has AI Techies Buying Mac Minis 66](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftech.slashdot.org%2Fstory%2F26%2F01%2F28%2F0510226%2Fclawdbot-has-ai-techies-buying-mac-minis%3Futm_source=tldrai/1/0100019c23f5d0c6-e04ce42e-44af-41ac-a7d0-ecb3f360f873-000000/TLFN-EHmuv_eg8Aa7XkLdlIhvM3ADRehyySQsCQOnw8=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 用户购买Mac Mini来本地运行Moltbot代理，该代理可接入日历、消息等个人工作流


<details>
  <summary>Details</summary>
Motivation: 人们希望拥有本地运行的AI代理，能够集成到个人工作流中，保护隐私并实现自动化

Method: Moltbot作为本地运行代理，能够连接日历、消息等个人应用，实现工作流自动化

Result: 用户购买Mac Mini专门用于运行Moltbot，显示了对本地AI代理的强烈需求

Conclusion: 本地运行的AI代理有市场需求，用户愿意投资硬件来获得隐私保护和个性化工作流自动化

Abstract: Moltbot Has AI Techies Buying Mac Minis 66 (2 minute read) Some people are buying Mac Minis just to host Moltbot (a locally running agent that can wire itself into calendars, messages, and other personal workflows) full-time.

</details>


### [16] [Unblocked is AI code review with the taste and judgement of your best engineer](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview%26utm_content=260204_primary/2/0100019c2864d4ef-978d52b0-ce8f-4d32-b7a4-5e88b8213c69-000000/CnjsMVWh8SmPll-LFzPYJeI4OudIZ1q2rdcZ_NmytsM=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Unblocked是一款AI代码审查工具，通过整合代码库、文档和团队知识来模拟资深工程师的审查方式


<details>
  <summary>Details</summary>
Motivation: 现有AI代码审查工具通常只分析代码差异、文件或仓库，但缺乏资深工程师所具备的上下文知识（如Slack讨论、团队惯例、同事偏好等），导致审查质量有限

Method: 整合代码库、文档和团队知识（包括讨论记录、同事偏好、未成文惯例等），构建更全面的上下文理解系统

Result: 声称是唯一能够使用代码库、文档和团队知识进行AI代码审查的工具，能提供更接近资深工程师的审查质量

Conclusion: 通过整合更广泛的团队知识和上下文，AI代码审查工具可以显著提升审查质量，更接近人类资深工程师的水平

Abstract: Unblocked is AI code review with the taste and judgement of your best engineer (Sponsor) Most AI code review tools analyze the diff. Sometimes the file, occasionally the repo.That's not how experienced engineers work.Instead, they remember the Slack thread that explains this database pattern. They know David on the platform team has strong opinions about error handling. They've internalized dozens of unwritten conventions.Unblocked is the only AI code review tool that uses codebase, docs, and...

</details>


### [17] [Apple's Xcode now supports the Claude Agent SDK](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fapple-xcode-claude-agent-sdk%3Futm_source=tldrnewsletter/1/0100019c2864d4ef-978d52b0-ce8f-4d32-b7a4-5e88b8213c69-000000/KPbnrrJQqAjEWY5QwuOy9ftSZPHFWnAAZJ_OLMOzl1U=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Xcode 26.3 原生集成了 Claude Agent SDK，让开发者能在 IDE 内直接使用 Claude Code 的完整能力，支持自主处理复杂长期任务。


<details>
  <summary>Details</summary>
Motivation: 为了让开发者无需离开 Xcode IDE 就能利用 Claude Code 的强大功能，提高开发效率，特别是处理复杂、长期运行的任务。

Method: 通过原生集成 Claude Agent SDK 到 Xcode 26.3 中，支持视觉验证预览、跨项目推理、自主任务执行，并通过模型上下文协议进行接口交互。

Result: Xcode 26.3 现已可用，开发者可以在 Xcode 中直接使用 Claude Code 的完整能力，包括自主处理复杂任务、视觉验证等功能。

Conclusion: Xcode 与 Claude Agent SDK 的集成显著提升了开发体验，使开发者能够在 IDE 内无缝使用 AI 辅助编程能力。

Abstract: Apple's Xcode now supports the Claude Agent SDK (2 minute read) Xcode 26.3 introduces a native integration with the Claude Agent SDK. Developers now have the full power of Claude Code directly in Xcode without having to leave the IDE. Claude can work autonomously on sophisticated, long-running tasks. The integration supports visual verification with Previews, reasoning across projects, autonomous task execution, and interfacing through the Model Context Protocol. Xcode 26.3 is now available a...

</details>


### [18] [Cut your dev loop from hours to seconds](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260204%26utm_content=std/1/0100019c288bf99c-d67fcde5-a5dc-4dfe-a9f2-93352617770c-000000/DV7Zy8w7lCXR5TUetRlC_-cjX7IhAZ47SkwxNZ9IW7o=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: mirrord工具让开发者能在本地运行微服务时访问云端所有资源，将开发周期从小时级缩短到秒级，提高代码质量并降低云成本


<details>
  <summary>Details</summary>
Motivation: 解决微服务开发中本地环境与云端环境不匹配的问题，减少开发周期时间，提高开发效率

Method: 通过mirrord工具在本地运行微服务时提供对云端所有资源的访问能力

Result: monday.com等公司使用后开发周期时间减少70%，GitHub星标4.9k

Conclusion: mirrord能显著提升微服务开发效率，缩短开发周期，降低成本

Abstract: Cut your dev loop from hours to seconds (Sponsor) mirrord (4.9k GitHub stars) lets you run your microservice locally with access to everything in the cloud, speeding up development, improving code quality, and reducing cloud costs. It's used by companies like monday.com, which reduced dev cycle time by 70%. Learn more about mirrord.

</details>


### [19] [Deep Dive: How Claude Code's /insights Command Works](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zolkos.com%2F2026%2F02%2F04%2Fdeep-dive-how-claude-codes-insights-command-works.html%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/GKLxS-H2vOEbee8SooEQ19pvDnZjhvFlzvdsYUjY29o=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code的/insights命令通过分析用户会话日志，使用LLM进行定性分析，生成HTML报告来识别用户交互模式、成功工作流程和摩擦点，并提供可操作建议。


<details>
  <summary>Details</summary>
Motivation: 帮助用户理解自己在代码开发中的交互模式，识别高效工作流程和潜在问题，从而提升开发效率和体验。

Method: 通过过滤会话日志、提取元数据，使用LLM对会话记录进行"面提取"定性分析，聚合数据并生成洞察报告。

Result: 生成包含用户交互风格、成功工作流程、摩擦点和可操作建议的HTML报告，帮助用户优化开发过程。

Conclusion: /insights命令通过系统化的会话分析，为用户提供了有价值的开发过程洞察，有助于提升代码开发效率和质量。

Abstract: Deep Dive: How Claude Code's /insights Command Works (14 minute read) The `/insights` command in Claude Code generates an HTML report that analyzes users' interaction patterns across all their sessions. This process involves filtering session logs, extracting metadata, and using an LLM to perform qualitative "facet extraction" on session transcripts. The aggregated data and LLM insights then identify interaction styles, successful workflows, and friction points, and proposes actionable sugges...

</details>


### [20] [Subagents: When and How to Use Them](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fsubagents%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/e0y67qVPRfe3bginhey17YksMQT4zLvwjN3pk7XcIoY=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文介绍了子代理的概念及其使用模式，子代理是主AI代理可以生成的专门代理，用于在独立上下文中处理特定任务，保持主对话清晰。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在处理复杂任务时，主对话容易变得杂乱无章，包含大量中间步骤和噪音。需要一种方法来保持主对话的整洁性，同时高效处理专门任务。

Method: 提出使用子代理模式：1）顺序链式模式（如侦察文件→实施→验证），2）并行探索模式。子代理作为专门代理在独立上下文中运行，完成任务后返回结果。

Result: 通过使用子代理模式，可以保持主对话的清晰度，提高任务处理的模块化和效率，同时支持复杂的任务分解和并行处理。

Conclusion: 子代理是有效的AI代理架构模式，通过任务分解和专门化处理，既能保持主对话的整洁性，又能提高复杂任务的处理效率。

Abstract: Subagents: When and How to Use Them (10 minute read) Subagents are specialist agents that your main AI agent can spawn to handle focused tasks in their own clean context, keeping your main conversation readable instead of cluttered with noise. The two useful patterns are chaining them sequentially (like scout files → implement → verify) or running them in parallel for exploration.

</details>


### [21] [Xcode 26.3 unlocks the power of agentic coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.apple.com%2Fnewsroom%2F2026%2F02%2Fxcode-26-point-3-unlocks-the-power-of-agentic-coding%2F%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/B-KiB4Ckcta9EVyF37-bjnxjGs2gWxsKpcidXLkftV4=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Xcode 26.3引入代理式编码，让开发者能将AI代理直接集成到应用开发工作流中，使代理能自主处理复杂任务、与项目架构交互并可视化验证工作


<details>
  <summary>Details</summary>
Motivation: 传统开发工具缺乏与AI代理的深度集成，无法充分利用AI在代码生成、架构交互和可视化验证方面的能力，限制了开发效率和自动化水平

Method: 在Xcode 26.3中集成代理式编码功能，支持Anthropic的Claude Agent和OpenAI的Codex等AI代理，使其能够直接与开发环境交互，处理复杂开发任务

Result: 开发者现在可以将AI代理无缝集成到Xcode开发工作流中，代理能够自主处理复杂任务、与项目架构交互并进行可视化验证，提升开发效率

Conclusion: Xcode 26.3的代理式编码功能代表了开发工具与AI代理深度集成的重大进步，为开发者提供了更智能、自动化的开发体验

Abstract: Xcode 26.3 unlocks the power of agentic coding (6 minute read) Xcode 26.3 introduces agentic coding, allowing developers to integrate AI agents directly into their app development workflow. This new capability enables agents like Anthropic's Claude Agent and OpenAI's Codex to autonomously tackle complex tasks, interact with project architecture, and visually verify their work.

</details>


### [22] [Unless That Claw Is The Famous OpenClaw](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.wordpress.com%2F2026%2F02%2F03%2Funless-that-claw-is-the-famous-openclaw%2F%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/Yno4AnQMFHOv3hvVGe32Nk4Zj_cmxiLdpZH8dP8Edoo=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个具有完整shell访问和浏览器控制功能的自主AI代理，存在安全风险，开发者需要使用专用硬件和临时账户来保护主账户安全。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示OpenClaw这类自主AI代理的安全风险，特别是提示注入和会话窃取等漏洞，提醒开发者采取防护措施。

Method: 通过分析OpenClaw的功能特性（完整shell访问和浏览器控制），识别其安全漏洞，并提出相应的安全防护建议。

Result: 识别出OpenClaw存在多种安全风险，包括提示注入攻击、会话窃取等，这些漏洞可能被恶意利用。

Conclusion: 开发者在使用OpenClaw等自主AI代理时必须采取严格的安全措施，包括使用专用硬件和临时账户，以保护主账户和系统安全。

Abstract: Unless That Claw Is The Famous OpenClaw (8 minute read) OpenClaw, an autonomous AI agent with full shell access and browser control, has security risks, including prompt injection and session theft, so developers must use dedicated hardware and burner accounts to keep their main accounts safe.

</details>


### [23] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/f8fsopq2_gv0GeeAH_eGudFjymkLJg6kc8mmlSXSlSE=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个具有完整shell访问和浏览器控制功能的自主AI代理，存在安全风险，开发者需使用专用硬件和临时账户来保护主账户安全。


<details>
  <summary>Details</summary>
Motivation: OpenClaw作为具有完整系统访问权限的AI代理，存在严重的安全隐患，需要研究其安全风险并提出防护措施。

Method: 分析OpenClaw的安全风险，包括提示注入和会话窃取等攻击向量，并提出使用专用硬件和临时账户的安全实践。

Result: 识别了OpenClaw的多项安全风险，提出了具体的防护建议，强调开发者需要采取额外安全措施来保护主账户。

Conclusion: OpenClaw虽然功能强大，但存在显著安全风险，开发者必须采取严格的隔离措施来确保系统安全。

Abstract: Unless That Claw Is The Famous OpenClaw (8 minute read) OpenClaw, an autonomous AI agent with full shell access and browser control, has security risks, including prompt injection and session theft, so developers must use dedicated hardware and burner accounts to keep their main accounts safe.

</details>


### [24] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/LvCNiU_jTLVPvHOTbJC-ZVBHtqi1fwcSI4PxpzP6_HQ=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个具有完整shell访问和浏览器控制能力的自主AI代理，存在安全风险，包括提示注入和会话窃取，开发者需要使用专用硬件和一次性账户来保护主账户安全。


<details>
  <summary>Details</summary>
Motivation: 分析OpenClaw这类具有系统级访问权限的AI代理的安全风险，为开发者提供安全使用指南。

Method: 通过分析OpenClaw的功能特性（完整shell访问、浏览器控制）识别潜在安全漏洞，提出具体的安全防护措施。

Result: 识别出OpenClaw存在提示注入和会话窃取等安全风险，建议使用专用硬件和一次性账户作为防护措施。

Conclusion: 具有系统级权限的AI代理存在显著安全风险，开发者需要采取隔离措施来保护敏感信息和账户安全。

Abstract: Unless That Claw Is The Famous OpenClaw (8 minute read) OpenClaw, an autonomous AI agent with full shell access and browser control, has security risks, including prompt injection and session theft, so developers must use dedicated hardware and burner accounts to keep their main accounts safe.

</details>


### [25] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/o1ydGx2CiD6Ho0RpOLNtTiDNHAuTBpSffWjl_j760wg=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个具有完整shell访问和浏览器控制能力的自主AI代理，存在安全风险，开发者需要使用专用硬件和临时账户来保护主账户安全。


<details>
  <summary>Details</summary>
Motivation: 分析OpenClaw这类自主AI代理的安全风险，特别是提示注入和会话窃取等威胁，为开发者提供安全使用建议。

Method: 通过分析OpenClaw的功能特性（完整shell访问、浏览器控制），识别其潜在的安全漏洞，并提出相应的安全防护措施。

Result: 识别出OpenClaw存在严重安全风险，包括提示注入攻击和会话窃取，建议开发者使用专用硬件和临时账户来降低风险。

Conclusion: 自主AI代理如OpenClaw虽然功能强大，但存在显著安全威胁，必须采取严格的安全措施才能安全使用。

Abstract: Unless That Claw Is The Famous OpenClaw (8 minute read) OpenClaw, an autonomous AI agent with full shell access and browser control, has security risks, including prompt injection and session theft, so developers must use dedicated hardware and burner accounts to keep their main accounts safe.

</details>


### [26] [Agent Orchestration UI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2141%26utm_source=tldrdesign/1/0100019c28c31fad-81f5f70e-8920-4d87-934e-f6327eda577e-000000/0gq15hYN3XrSrYyWkL49UUJ81KIECRt0vE0F-hcM-Qg=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Orchestration UI提出了一种新的用户界面方法，用于多智能体协同工作，通过工作空间、上下文管理和并行专业智能体来协调统一任务。


<details>
  <summary>Details</summary>
Motivation: 随着AI产品从后台模型发展到聊天界面再到智能体，现在需要智能体编排技术来让多个智能体协调完成统一任务。现有的界面方法不足以支持复杂的多智能体协同工作。

Method: Augment的Intent平台采用工作空间、上下文管理和并行专业智能体的UI方法。平台使用隔离空间和动态规范，让协调者、实施者和验证者智能体协同工作。

Result: 该平台实现了多智能体在统一任务上的协调工作，通过专门的UI设计提高了智能体协同的效率和效果。

Conclusion: 智能体编排是AI产品发展的下一个阶段，需要专门的UI设计来支持多智能体协同工作，Intent平台为此提供了创新的解决方案。

Abstract: Agent Orchestration UI (3 minute read) AI products have evolved from behind-the-scenes models to chat interfaces to agents, and now to agent orchestration, where multiple agents coordinate on unified tasks. Intent by Augment introduces a new UI approach for agent orchestration that focuses on workspaces, context management, and specialized agents working in parallel. The platform uses isolated spaces with living specifications that allow coordinator, implementer, and verifier agents to work t...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [27] [Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation](https://arxiv.org/abs/2602.04195)
*Guang Yang,Xing Hu,Xiang Chen,Xin Xia*

Main category: cs.SE

TL;DR: 提出SCD防御方法，通过提取功能需求并基于完整需求和功能需求进行共识解码，将硬件设计LLM后门攻击成功率从89%降至3%以下。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中的LLM代码生成易受后门攻击，一旦硬件制造完成，硬件木马将无法修复。现有主动防御需要训练数据，对第三方用户不实用；被动防御难以应对语义隐蔽的触发器。攻击者倾向于在非功能需求中嵌入触发器而非功能规范。

Method: 提出语义共识解码(SCD)，包含两个关键组件：1) 功能需求提取，从用户规范中识别关键功能需求；2) 共识解码，基于完整用户规范和提取的功能需求自适应融合输出分布，当分布显著分歧时自动抑制可疑组件。

Result: 在三种代表性后门攻击上的广泛实验表明，SCD将平均攻击成功率从89%降低到3%以下，且对生成质量影响可忽略。

Conclusion: SCD是一种有效的推理时被动防御方法，能够显著降低硬件设计LLM后门攻击风险，同时保持代码生成质量。

Abstract: Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.

</details>


### [28] [Why Agentic-PRs Get Rejected: A Comparative Study of Coding Agents](https://arxiv.org/abs/2602.04226)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Shane Mclntosh,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 该论文分析了不同AI编码代理生成的PR被拒绝的原因差异，发现Agentic-PR有7种独特的拒绝模式，且不同代理有特定失败模式，并提出启发式方法解决缺乏明确反馈的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管已知AI编码代理生成的PR接受率较低，但不同代理之间的拒绝原因差异尚未研究。由于不同代理用于不同目的，可能存在代理特定的失败模式，需要比较分析。

Method: 从AIDev数据集中检查654个被拒绝的PR，涵盖5个编码代理和人类基准。分析拒绝原因，识别代理特定模式，并提出启发式方法处理缺乏明确反馈的情况。

Result: 发现Agentic-PR有7种独特的拒绝模式（包括对AI生成代码的不信任），不同代理有特定模式（如Devin自动撤回不活跃PR）。67.9%的被拒绝PR缺乏明确反馈，提出的启发式方法能有效减少此类情况。

Conclusion: 不同编码代理的拒绝原因存在显著差异，反映了实际使用中的配置和使用方式不同。缺乏明确反馈是主要挑战，提出的启发式方法为未来研究提供了实用预处理步骤。

Abstract: Agentic coding -- software development workflows in which autonomous coding agents plan, implement, and submit code changes with minimal human involvement -- is rapidly gaining traction. Prior work has shown that Pull Requests (PRs) produced using coding agents (Agentic-PRs) are accepted less often than PRs that are not labeled as agentic (Human-PRs). The rejection reasons for a single agent (Claude Code) have been explored, but a comparison of how rejection reasons differ between Agentic-PRs generated by different agents has not yet been performed. This comparison is important since different coding agents are often used for different purposes, which can lead to agent-specific failure patterns. In this paper, we inspect 654 rejected PRs from the AIDev dataset covering five coding agents, as well as a human baseline. Our results show that seven rejection modes occur only in Agentic-PRs, including distrust of AI-generated code. We also observe agent-specific patterns (e.g., automated withdrawal of inactive PRs by Devin), reflecting differences in how agents are configured and used in practice. Notably, a large proportion of rejected PRs (67.9%) lack explicit reviewer feedback, making their rejection reasons difficult to determine. To mitigate this issue, we propose a set of heuristics that reduce the proportion of such cases, offering a practical preprocessing step for future studies of PR rejection in agentic coding.

</details>


### [29] [ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas](https://arxiv.org/abs/2602.04296)
*Wenjun Peng,Xinyu Wang,Qi Wu*

Main category: cs.SE

TL;DR: ProxyWar是一个通过将LLM生成的智能体嵌入多样化竞争游戏环境来系统评估代码生成质量的框架，超越了传统静态基准测试，揭示了基准分数与动态实际性能之间的显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码生成的评估主要依赖静态基准和简单指标，无法充分反映代码在实际动态环境中的真实效果，需要更全面的评估方法来揭示代码的操作特性和实际性能。

Method: 通过将LLM生成的智能体嵌入多样化竞争游戏环境，结合自动化测试、迭代代码修复和多智能体锦标赛，系统评估代码的功能正确性和操作特性。

Result: 应用ProxyWar评估多个先进代码生成模型和游戏，发现基准分数与动态环境实际性能之间存在显著差异，揭示了传统评估方法忽略的局限性和改进机会。

Conclusion: 需要基于竞争的更丰富代码生成评估方法，ProxyWar为LLM驱动的算法发现、自适应问题解决以及实用效率和鲁棒性研究奠定了基础，包括模型超越手工智能体的潜力。

Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-generated agents within diverse, competitive game environments. Unlike existing approaches, ProxyWar evaluates not only functional correctness but also the operational characteristics of generated programs, combining automated testing, iterative code repair, and multi-agent tournaments to provide a holistic view of program behavior. Applied to a range of state-of-the-art coders and games, our approach uncovers notable discrepancies between benchmark scores and actual performance in dynamic settings, revealing overlooked limitations and opportunities for improvement. These findings highlight the need for richer, competition-based evaluation of code generation. Looking forward, ProxyWar lays a foundation for research into LLM-driven algorithm discovery, adaptive problem solving, and the study of practical efficiency and robustness, including the potential for models to outperform hand-crafted agents. The project is available at https://github.com/xinke-wang/ProxyWar.

</details>


### [30] [AgenticAKM : Enroute to Agentic Architecture Knowledge Management](https://arxiv.org/abs/2602.04445)
*Rudra Dhar,Karthik Vaidhyanathan,Vasudeva Varma*

Main category: cs.SE

TL;DR: 论文提出AgenticAKM，一种基于智能体协作的架构知识管理方法，通过分解架构恢复和文档化为可管理的子任务，自动生成架构决策记录（ADRs）。


<details>
  <summary>Details</summary>
Motivation: 架构知识管理（AKM）对于软件项目至关重要，但通常是一个繁琐的过程，开发者和架构师往往不采用。虽然大语言模型（LLMs）提供了自动化机会，但简单的单提示方法效果有限，受限于上下文长度限制和无法理解架构知识的分布式特性。

Method: 提出AgenticAKM方法，将复杂的架构恢复和文档化问题分解为可管理的子任务。设计了专门的智能体（架构提取、检索、生成和验证）在结构化工作流中协作生成架构知识。具体实例化为从代码仓库生成架构决策记录（ADRs）。

Result: 通过对29个仓库的用户研究验证，结果表明该智能体方法能生成更好的ADRs，是自动化AKM的有前景且实用的方法。

Conclusion: 智能体方法通过分解复杂任务和专门智能体协作，能够有效解决传统AKM的局限性，为自动化架构知识管理提供了可行方案。

Abstract: Architecture Knowledge Management (AKM) is crucial for maintaining current and comprehensive software Architecture Knowledge (AK) in a software project. However AKM is often a laborious process and is not adopted by developers and architects. While LLMs present an opportunity for automation, a naive, single-prompt approach is often ineffective, constrained by context limits and an inability to grasp the distributed nature of architectural knowledge. To address these limitations, we propose an Agentic approach for AKM, AgenticAKM, where the complex problem of architecture recovery and documentation is decomposed into manageable sub-tasks. Specialized agents for architecture Extraction, Retrieval, Generation, and Validation collaborate in a structured workflow to generate AK. To validate we made an initial instantiation of our approach to generate Architecture Decision Records (ADRs) from code repositories. We validated our approach through a user study with 29 repositories. The results demonstrate that our agentic approach generates better ADRs, and is a promising and practical approach for automating AKM.

</details>


### [31] [Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents](https://arxiv.org/abs/2602.04640)
*Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 该立场论文主张软件工程代理需要从被动反应式设计转向结构化、状态感知、执行基础推理，以解决长期任务中的连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程代理主要是被动反应式的，仅基于对话历史和最近响应做决策，缺乏明确的结构或持久状态，导致长期推理困难，难以维持跨推理步骤的连贯理解、适应新证据或整合执行反馈。

Method: 提出通过三个关键改进来推进软件工程代理：1) 明确的结构化设计；2) 持久且演化的状态管理；3) 执行基础反馈的整合。为开发下一代软件工程代理提供了初步路线图。

Result: 作为立场论文，没有具体实验结果，但提出了理论框架和路线图，指出结构化、状态感知、执行基础推理能帮助软件工程代理在长期任务中执行更连贯可靠的推理。

Conclusion: 要推进软件工程代理发展，需要超越被动反应式行为，转向结构化、状态感知、执行基础推理，这将使代理能更有效地执行现实世界任务。

Abstract: Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.
  In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.

</details>


### [32] [Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation](https://arxiv.org/abs/2602.04726)
*Marian Kica,Lukas Radosky,David Slivka,Karin Kubinova,Daniel Dovhun,Tomas Uhercik,Erik Bircak,Ivan Polasek*

Main category: cs.SE

TL;DR: 本文提出了两种面向软件工程的AI代理解决方案：一是基于星型拓扑结构的自动测试场景生成系统，二是用于软件工程文档检索的多功能代理系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的出现引发了软件开发模式的重大变革，软件工程研究产生了大量工具和方法。作者希望加入这一浪潮，通过AI代理解决软件工程中的实际问题。

Method: 1. 测试场景生成：采用星型拓扑结构，由监督代理协调多个专业工作代理，从详细需求描述自动生成测试场景。
2. 文档检索：为每个用例（搜索、问答、变更追踪、长文档摘要）设计专门的LLM代理，处理相关子任务。

Result: 1. 在真实案例中展示了测试场景生成系统的能力。
2. 开发了能够处理多种软件工程文档用例的代理系统，包括搜索、问答、变更追踪和文档摘要。

Conclusion: 本文展示了AI代理在软件工程任务中的应用潜力，并展望了未来研究方向。

Abstract: The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: TMK框架提示显著提升LLM在规划任务中的推理能力，在Blocksworld基准上准确率从31.5%提升至97.3%，通过任务分解和因果结构引导模型转向形式化推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM提示技术（如CoT）在推理能力上存在局限，需要更有效的框架来提升模型的任务分解和规划能力。TMK框架因其能捕捉因果、目的性和层次化推理结构，有望解决LLM的推理缺陷。

Method: 采用Task-Method-Knowledge（TMK）框架进行结构化提示，在PlanBench基准的Blocksworld领域进行实验，测试模型将复杂规划问题分解为可管理子任务的能力。

Result: TMK提示使推理模型在不透明的符号任务（Blocksworld随机版本）上准确率从31.5%大幅提升至97.3%，显示出在语义近似和符号操作之间架起桥梁的潜力。

Conclusion: TMK不仅提供上下文，更作为一种机制引导推理模型脱离默认语言模式，转向形式化的代码执行路径，显著提升LLM的规划和推理能力。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [34] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC是一种迭代改进的程序构造方法，通过结合执行反馈和链式思维，提升数学问题解决中的程序化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统在数学推理中缺乏可靠可修正的推理过程表示，要么采用僵化的顺序流程无法修正早期步骤，要么依赖启发式自评估可能无法识别和修复错误。此外，程序化上下文可能分散语言模型的注意力并降低准确性。

Method: 提出迭代改进的程序构造（IIPC）方法，迭代地精炼程序化推理链，并将执行反馈与基础LLM的原生链式思维能力相结合，以保持高层次上下文关注。

Result: IIPC在多个基础LLM上的大多数推理基准测试中超越了竞争方法。

Conclusion: IIPC通过迭代改进程序构造和结合执行反馈，有效提升了数学问题解决中的推理可靠性和准确性。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [35] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk通过知识蒸馏将多智能体系统的推理能力压缩到单个模型中，在保持计算效率的同时获得多智能体的推理优势


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统虽然通过迭代辩论实现了优越的推理性能，但实际部署受到高计算成本和错误传播的限制，需要更高效的解决方案

Method: 提出AgentArk框架，采用三种分层蒸馏策略：推理增强微调、基于轨迹的数据增强和过程感知蒸馏，将多智能体动态压缩到单个模型权重中

Result: 蒸馏后的模型在保持单个智能体计算效率的同时，展现出多智能体的强大推理和自校正性能，并在多样化推理任务中表现出增强的鲁棒性和泛化能力

Conclusion: 通过将计算负担从推理转移到训练，AgentArk为高效、鲁棒的多智能体开发提供了新方向，有望推动未来研究

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [36] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出一种状态级选择性验证框架，在验证成本受限的设置下，通过确定性可行性门控、预验证排序和自适应分配验证调用，减少冗余验证，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理中测试时计算成为主要驱动力，但昂贵的验证成为瓶颈。许多推理系统中，大量验证调用花费在冗余或无希望的中间假设上，需要在验证成本受限的设置下研究如何分配验证资源。

Method: 提出状态级选择性验证框架，包含：(1) 结构化移动接口上的确定性可行性门控；(2) 结合学习的状态距离和残差评分的预验证排序；(3) 基于局部不确定性的验证调用自适应分配。

Result: 在MATH基准测试中，该方法比best-of-N、多数投票和束搜索获得更高准确率，同时减少44%的验证调用。

Conclusion: 通过智能分配验证资源到最信息丰富的中间状态，可以在验证成本受限的设置下显著提高推理效率，避免冗余验证。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [37] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: ORBIT是一个多任务、多回合的元强化学习框架，通过训练让LLM能够在上下文中从交互中学习，显著提升了开源模型在未见环境中的在线学习能力，性能媲美GPT-5.2并大幅超越标准RL微调。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在需要在线交互的决策任务中表现不佳，这些任务需要实时收集信息、处理延迟反馈，并在信息收集和利用之间取得平衡。虽然上下文学习允许无需权重更新的适应，但现有LLM难以可靠利用上下文交互经验。

Method: 提出ORBIT框架：多任务、多回合的元强化学习框架，通过训练让LLM学习在上下文中从交互中学习。使用相对较小的开源模型（Qwen3-14B）进行元训练。

Result: 元训练后的模型在完全未见的环境中表现出显著改进的上下文在线学习能力，性能匹配GPT-5.2，并大幅超越标准RL微调方法。扩展实验显示随着模型规模增大，性能持续提升。

Conclusion: 通过训练可以解决LLM在在线决策任务中的局限性，ORBIT框架展示了在推理时学习决策智能体的巨大潜力，模型规模扩展带来持续收益。

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [38] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze是一个将LLM应用视为上下文构建与执行问题的系统，通过异构DNN堆栈、小型语言模型感知模块、上下文构建层和动作层，配合轻量控制器，实现高效的多模态任务处理。


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用不应仅依赖单一大型模型，而应通过构建和操作上下文来解决问题。传统方法使用单一transformer模型效率低下且成本高昂，需要更智能的系统架构来整合多种专业模块。

Method: 系统采用三层架构：(1)异构DNN堆栈配合小型语言模型作为感知模块，处理复杂PDF、图表、多语言ASR等；(2)上下文构建层爬取、索引、解析外部资源为结构化状态；(3)动作层支持浏览、检索、沙箱代码执行和浏览器驱动。顶层轻量控制器决定运行哪些小型模型和动作，并将精炼的上下文转发给用户选择的LLM生成最终响应。

Result: Interfaze-Beta在多个基准测试中表现优异：MMLU-Pro 83.6%、MMLU 91.4%、GPQA-Diamond 81.3%、LiveCodeBench v5 57.8%、AIME-2025 90.0%，多模态任务上MMMU(val) 77.3%、AI2D 91.5%、ChartQA 90.9%、Common Voice v16 90.8%。系统将大部分计算从小型模型和工具栈处理，大型LLM仅操作精炼上下文，实现竞争性准确率的同时降低计算成本。

Conclusion: Interfaze展示了通过模块化架构整合专业小型模型和工具，将计算负担从昂贵的大型模型转移到更高效的组件上，能够实现高性能的多模态LLM应用，为构建更智能、更经济的AI系统提供了新范式。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [39] [InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons](https://arxiv.org/abs/2602.04213)
*Feiyu Gavin Zhu,Jean Oh,Reid Simmons*

Main category: cs.AI

TL;DR: InterPReT：一种交互式策略重构与训练方法，让非专业用户能够通过指令和演示来教导AI代理，降低AI代理教学门槛


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习需要大量专业演示和训练监控，这对普通用户教导AI代理新技能构成挑战。需要降低AI代理教学门槛，让非专业用户也能有效训练代理

Method: 提出交互式策略重构与训练（InterPReT），通过用户指令持续更新策略结构并优化参数以适应用户演示。用户可交互式提供指令和演示、监控代理性能、审查决策策略

Result: 在赛车游戏驾驶教学的用户研究（N=34）中，相比通用模仿学习基线，InterPReT在普通用户负责演示和决定停止时机时，能产生更鲁棒的策略而不损害系统可用性

Conclusion: 该方法更适合没有机器学习技术背景的终端用户训练可靠策略，降低了AI代理教学的门槛

Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy

</details>


### [40] [Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search](https://arxiv.org/abs/2602.04248)
*Hao Lu,Haoyuan Huang,Yulin Zhou,Chen Li,Ningxin Zhu*

Main category: cs.AI

TL;DR: Empirical-MCTS：双循环框架，将无状态搜索转化为持续学习过程，通过局部探索与全局记忆优化提升LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 当前基于MCTS的推理时扩展策略主要是无状态的，每次解决问题后丢弃成功推理模式，无法像人类那样积累经验智慧。需要将结构化搜索与经验积累相结合来应对复杂开放推理任务。

Method: 提出Empirical-MCTS双循环框架，包含两个核心机制：1) PE-EMP（成对经验演化元提示）作为局部搜索的反射优化器，使用成对反馈动态合成自适应标准并实时演化元提示；2) 记忆优化代理管理全局存储库作为动态策略先验，通过原子操作跨问题提炼高质量见解。

Result: 在复杂推理基准测试（AIME25、ARC-AGI-2、MathArena Apex）上，Empirical-MCTS显著优于无状态MCTS策略和独立经验驱动代理，证明了结构化搜索与经验积累结合的重要性。

Conclusion: 将结构化搜索与经验积累相结合对于掌握复杂开放推理任务至关重要，Empirical-MCTS框架为此提供了有效解决方案。

Abstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.

</details>


### [41] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: Agent-Omit是一个训练框架，让LLM代理能自适应地省略冗余的思考和观察，在保持性能的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究将整个交互轨迹同等对待，忽略了不同轮次中思考必要性和观察效用的差异，导致代理效率低下。

Method: 1) 合成少量冷启动数据（单轮和多轮省略场景）微调代理；2) 提出省略感知的代理强化学习方法，包含双重采样机制和定制的省略奖励；3) 理论证明省略策略的偏差有KL散度上界。

Result: 在五个代理基准测试中，Agent-Omit-8B性能与七个前沿LLM代理相当，且在效率-效果权衡上优于七个高效LLM代理方法。

Conclusion: Agent-Omit框架能有效提升代理效率，通过自适应省略冗余思考和观察实现更好的效果-效率平衡。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [42] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: PCE框架将LLM推理中的隐含假设转化为结构化决策树，通过评估场景可能性、目标收益和执行成本来选择行动，减少多智能体环境中的通信开销


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的具身智能体在多智能体、部分可观测、去中心化环境中主要依赖频繁的智能体间通信来缓解不确定性，但这带来了显著的token和时间成本，并可能干扰人类合作伙伴的工作流程

Method: 提出Planner-Composer-Evaluator框架：将LLM推理轨迹中的碎片化假设转化为结构化决策树，内部节点编码环境假设，叶子节点映射到行动，每条路径通过场景可能性、目标导向收益和执行成本进行评分

Result: 在两个多智能体基准测试（C-WAH和TDW-MAT）和三个LLM骨干上，PCE在成功率和任务效率上持续优于通信密集型基线，同时保持相当的token使用量。消融实验表明PCE在不同模型容量和推理深度下都能提升性能

Conclusion: PCE为将LLM的潜在假设转化为可靠的不确定性感知规划策略提供了原则性途径，用户研究显示其产生的通信模式被人类合作伙伴认为更高效和可信

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [43] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: ReThinker是一个基于置信度的智能体框架，通过Solver-Critic-Selector架构实现动态计算分配，在专家级科学推理任务上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在专家级科学推理（如Humanity's Last Exam基准）上仍面临挑战，传统工具管道僵化、多智能体协调脆弱、测试时扩展效率低等问题限制了性能提升。

Method: 提出ReThinker框架，采用Solver-Critic-Selector三阶段架构，基于模型置信度动态分配计算资源，实现自适应工具调用、引导式多维度反思和置信度加权选择。同时提出反向数据合成管道和自适应轨迹回收策略，将成功推理轨迹转化为高质量监督数据。

Result: 在HLE、GAIA和XBench基准测试中，ReThinker一致优于现有最先进的工具增强基础模型和深度研究系统，在专家级推理任务上取得了最先进的结果。

Conclusion: ReThinker通过置信度感知的动态计算分配和创新的数据合成方法，有效解决了专家级科学推理中的关键挑战，为智能体系统在复杂推理任务上的应用提供了新思路。

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [44] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 提出Vibe AIGC新范式，通过多智能体工作流编排解决当前生成式AI的意图-执行差距问题，将用户从提示工程师转变为提供"氛围"的指挥官，实现从随机推理到逻辑编排的转变。


<details>
  <summary>Details</summary>
Motivation: 当前以模型为中心、依赖规模定律的生成式AI范式存在"可用性天花板"，表现为意图-执行差距——用户高层次意图与当前单次生成模型的随机黑盒性质之间的根本性差异。

Method: 受Vibe Coding启发，提出Vibe AIGC范式：用户作为指挥官提供包含审美偏好、功能逻辑等的"氛围"高层表示；中央元规划器作为系统架构师，将氛围分解为可执行、可验证、自适应的智能体管道；通过分层多智能体工作流的自主合成实现内容生成。

Result: 通过从随机推理向逻辑编排的转变，Vibe AIGC弥合了人类想象力与机器执行之间的差距，将AI从脆弱的推理引擎转变为稳健的系统级工程伙伴。

Conclusion: 这种范式转变将重新定义人机协作经济，使复杂、长周期的数字资产创作民主化，代表了生成式AI发展的新方向。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [45] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 提出了WideSeek-R1框架，通过多智能体强化学习实现宽度扩展，让一个4B参数的模型在广域信息搜索任务上达到与671B单智能体模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 随着任务范围扩大，瓶颈从单个智能体的能力转向组织能力。现有多智能体系统依赖手工工作流和顺序交互，无法有效并行化工作。

Method: 提出WideSeek-R1框架，采用领导智能体-子智能体架构，通过多智能体强化学习训练，使用共享LLM但隔离上下文和专用工具，在2万个广域信息搜索任务上联合优化。

Result: WideSeek-R1-4B在WideSearch基准测试中获得40.0%的项目F1分数，与单智能体DeepSeek-R1-671B性能相当，且随着并行子智能体数量增加性能持续提升。

Conclusion: 宽度扩展是多智能体系统的重要维度，通过有效的并行执行和协同编排，小模型可以实现与大模型相当的性能。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [46] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 该论文提出了一个七维分类法来系统评估医疗领域LLM智能体研究，通过分析49项研究发现能力实现存在明显不对称性：外部知识集成普遍实现，而事件触发激活、漂移检测等能力严重缺失。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域LLM智能体研究缺乏统一框架，文献多为宽泛综述或单一能力深入探讨，需要系统分类法来评估现有研究的实现状况和能力差距。

Method: 提出七维分类法（认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型、核心任务与子任务），包含29个操作子维度。使用明确纳入排除标准和标注规则（完全实现、部分实现、未实现），对49项研究进行系统映射和定量分析。

Result: 分析发现明显不对称性：外部知识集成普遍实现（~76%完全实现），而事件触发激活（~92%未实现）、漂移检测与缓解（~98%未实现）严重缺失。架构上多智能体设计占主导（~82%完全实现），编排层多为部分实现。任务方面信息中心能力领先，而治疗规划与处方等行动导向领域存在显著差距（~59%未实现）。

Conclusion: 医疗LLM智能体研究在知识集成和多智能体架构方面进展良好，但在自适应学习、事件驱动交互和行动导向任务方面存在重大空白，需要未来研究重点关注。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [47] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: GEA提出了一种基于群体进化的开放自改进代理范式，通过群体作为基本进化单元实现经验共享，显著提升了编码任务的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有开放自进化范式采用树状结构进化，存在进化分支孤立导致的探索多样性利用效率低的问题，需要更有效的自改进机制。

Method: 提出群体进化代理(GEA)范式，以代理群体为基本进化单元，在进化过程中实现显式的经验共享和重用，克服孤立进化分支的限制。

Result: 在编码基准测试中显著优于现有自进化方法(SWE-bench Verified: 71.0% vs 56.7%；Polyglot: 88.3% vs 68.3%)，匹配或超越人工设计的顶级代理框架。

Conclusion: GEA能更有效地将早期探索多样性转化为持续长期进步，在相同进化代理数量下获得更强性能，且具有更好的跨模型可迁移性和鲁棒性。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Stroke Lesions as a Rosetta Stone for Language Model Interpretability](https://arxiv.org/abs/2602.04074)
*Julius Fridriksson,Roger D. Newman-Norlund,Saeed Ahmadi,Regan Willis,Nadra Salman,Kalil Warren,Xiang Guan,Yong Yang,Srihari Nelakuditi,Rutvik Desai,Leonardo Bonilha,Jeff Charney,Chris Rorden*

Main category: cs.LG

TL;DR: 提出BLUM框架，利用脑损伤-症状映射作为外部验证标准，评估LLM扰动效果，发现LLM错误模式与人类脑损伤模式有显著对应关系


<details>
  <summary>Details</summary>
Motivation: 当前LLM可解释性方法依赖内部指标，缺乏外部验证。需要建立因果验证框架来评估哪些模型组件真正对语言功能必要

Method: 使用410名中风后失语症患者数据，训练症状-损伤模型；对transformer层进行系统扰动；对扰动后的LLM和人类患者进行相同临床评估；将LLM错误模式投影到人类损伤空间

Result: LLM错误模式与人类错误模式足够相似，预测的损伤位置在67%图片命名条件和68.3%句子完成条件下与实际人类损伤位置对应（显著高于随机）；语义主导错误对应腹侧通路损伤，音位主导错误对应背侧通路损伤

Conclusion: 为LLM可解释性开辟了新方法路径，临床神经科学提供外部验证，人类损伤-症状映射可作为评估人工语言系统的参考框架，行为对齐可能反映共享计算原理

Abstract: Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.

</details>


### [49] [CoRe: Context-Robust Remasking for Diffusion Language Models](https://arxiv.org/abs/2602.04096)
*Kevin Zhai,Sabbir Mollah,Zhenyi Wang,Mubarak Shah*

Main category: cs.LG

TL;DR: CoRe提出了一种无需训练的推理时修订框架，通过探测标记对上下文扰动的敏感性来识别上下文脆弱的标记，而非依赖静态置信度，从而改善掩码扩散模型的解码质量。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型的标准解码存在上下文刚性问题：标记基于瞬时高置信度被保留，忽略了早期预测缺乏完整上下文。这导致级联效应，初始不一致性误导后续生成。现有修订策略依赖静态置信度，但这些信号本质上是短视的，不一致的标记可能对模型本身显得很自信。

Method: 提出Context-Robust Remasking (CoRe)框架，通过探测标记对针对性掩码上下文扰动的敏感性来识别上下文脆弱的标记，将修订形式化为对上下文偏移的鲁棒优化目标，并高效近似该目标以优先修订不稳定标记。

Result: 在LLaDA-8B-Base模型上，CoRe在推理和代码基准测试中带来一致改进，优于计算匹配的基线方法，并将MBPP性能提升高达9.2个百分点。

Conclusion: CoRe通过动态评估标记对上下文扰动的敏感性而非依赖静态置信度，有效解决了掩码扩散模型中的上下文刚性问题，提供了一种无需训练的高效推理时修订方法。

Abstract: Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.

</details>


### [50] [Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting](https://arxiv.org/abs/2602.04131)
*Mehrdad Moghimi,Anthony Coache,Hyejin Ku*

Main category: cs.LG

TL;DR: 该论文提出了一个支持灵活折扣未来奖励和优化风险度量的分布强化学习新框架，强调折扣因子在捕捉时间偏好和风险偏好中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 传统分布强化学习中折扣因子通常被视为固定参数或可调超参数，很少考虑其对学习策略的影响。指数折扣因子无法完全捕捉智能体的时间偏好，而折扣函数在表征时间偏好方面起着重要作用。

Method: 提出支持灵活折扣未来奖励和优化风险度量的分布强化学习新框架，提供算法最优性的技术分析，并通过多时间范围扩展解决现有方法的问题。

Result: 通过大量实验验证了方法的鲁棒性，结果表明折扣是决策问题中捕捉更丰富时间和风险偏好特征的关键因素。

Conclusion: 折扣是决策问题中捕捉更丰富时间和风险偏好特征的基础，对现实世界安全关键应用具有潜在影响。

Abstract: Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.

</details>


### [51] [Topology-Aware Revival for Efficient Sparse Training](https://arxiv.org/abs/2602.04166)
*Meiling Jin,Fei Wang,Xiaoyun Yuan,Chen Qian,Yuan Cheng*

Main category: cs.LG

TL;DR: TAR是一种轻量级的一次性后剪枝方法，通过拓扑感知的复活步骤改进静态稀疏训练，在强化学习中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 静态稀疏训练虽然高效，但固定掩码模式降低了鲁棒性。早期剪枝决策会将网络锁定在脆弱结构中，难以调整，特别是在深度强化学习中，策略的不断演变会持续改变训练分布。

Method: 提出拓扑感知复活(TAR)：1) 静态剪枝后执行一次性复活步骤；2) 根据拓扑需求在各层分配小量预留预算；3) 每层随机均匀地重新激活少量先前剪枝的连接；4) 保持最终连接模式固定进行后续训练。

Result: 在多个连续控制任务中，使用SAC和TD3算法，TAR相比静态稀疏基线最终回报提升高达+37.9%，相比动态稀疏训练基线中位数增益为+13.5%。

Conclusion: TAR通过轻量级的一次性拓扑感知复活，有效改善了静态稀疏训练的鲁棒性和性能，在强化学习中取得了显著效果。

Abstract: Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%.

</details>


### [52] [RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning](https://arxiv.org/abs/2602.04224)
*Zeming Wei,Qiaosheng Zhang,Xia Hu,Xingcheng Xu*

Main category: cs.LG

TL;DR: RAPO框架通过风险感知偏好优化，让大型推理模型能够自适应识别和处理安全风险，提升对复杂越狱攻击的防御能力


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然具备链式思维推理能力，但仍面临与基础语言模型类似的安全问题。现有的安全拒绝机制在面对多样化和复杂的越狱攻击时泛化能力不足，需要更充分的安全推理过程来防御高级攻击提示

Method: 提出风险感知偏好优化(RAPO)框架，使LRM能够自适应地识别安全风险，并在其思维内容中以适当的粒度处理这些风险。该方法通过理论分析和实证证据支持更充分的安全推理过程的必要性

Result: 大量实验表明，RAPO成功泛化了多个LRM的安全推理能力，能够自适应地应对多样化的攻击提示，同时保持模型的一般效用，为LRM安全提供了一种鲁棒的对齐技术

Conclusion: RAPO框架通过风险感知的偏好优化，有效提升了大型推理模型对复杂越狱攻击的防御能力，同时保持了模型的通用性能，为LRM安全对齐提供了有效的解决方案

Abstract: Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs' safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.

</details>


### [53] [Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning](https://arxiv.org/abs/2602.04380)
*Rui Yuan,Mykola Khandoga,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: GBMPO扩展了基于组的策略优化方法，引入灵活的Bregman散度（包括手工设计的概率空间L2散度和学习的神经镜像映射），在数学推理和代码生成任务上显著优于仅使用KL散度的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO及其变体）在数学推理和代码生成任务上表现良好，但都只使用KL散度进行策略正则化，散度函数的选择这一设计维度尚未被探索。

Method: 提出Group-Based Mirror Policy Optimization (GBMPO)框架，将基于组的策略优化扩展到灵活的Bregman散度，包括手工设计的概率空间L2散度（ProbL2）和学习的神经镜像映射。使用进化策略元学习来优化神经镜像映射。

Result: 在GSM8K数学推理任务上，手工设计的ProbL2-GRPO达到86.7%准确率，比Dr. GRPO基线提升5.5个百分点。在MBPP代码生成任务上，神经镜像映射达到60.1-60.8% pass@1，随机初始化已能获得大部分收益。进化策略元学习主要带来方差减少（±0.2 vs ±0.6）和效率提升（MBPP上响应缩短15%）。

Conclusion: 散度函数的选择是基于组的策略优化中一个关键且先前未被探索的设计维度。随机初始化神经镜像映射对大多数实际应用已足够，进化策略元学习主要提供方差减少和效率提升。

Abstract: Policy optimization methods like Group Relative Policy Optimization (GRPO) and its variants have achieved strong results on mathematical reasoning and code generation tasks. Despite extensive exploration of reward processing strategies and training dynamics, all existing group-based methods exclusively use KL divergence for policy regularization, leaving the choice of divergence function unexplored. We introduce Group-Based Mirror Policy Optimization (GBMPO), a framework that extends group-based policy optimization to flexible Bregman divergences, including hand-designed alternatives (L2 in probability space) and learned neural mirror maps. On GSM8K mathematical reasoning, hand-designed ProbL2-GRPO achieves 86.7% accuracy, improving +5.5 points over the Dr. GRPO baseline. On MBPP code generation, neural mirror maps reach 60.1-60.8% pass@1, with random initialization already capturing most of the benefit. While evolutionary strategies meta-learning provides marginal accuracy improvements, its primary value lies in variance reduction ($\pm$0.2 versus $\pm$0.6) and efficiency gains (15% shorter responses on MBPP), suggesting that random initialization of neural mirror maps is sufficient for most practical applications. These results establish divergence choice as a critical, previously unexplored design dimension in group-based policy optimization for LLM reasoning.

</details>


### [54] [EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL](https://arxiv.org/abs/2602.04417)
*Lunjun Zhang,Jimmy Ba*

Main category: cs.LG

TL;DR: 提出EMA-PG方法，通过指数移动平均锚策略和Top-k KL估计器改进LLM的强化学习，在数学推理和智能体任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习使大语言模型获得复杂推理和智能体行为，但现有策略梯度算法仍有改进空间，需要更稳定和高效的训练方法。

Method: 1. 用指数移动平均(EMA)替代固定锚策略，类似于深度Q学习中的目标网络；2. 引入Top-k KL估计器，在精确KL和采样KL之间灵活插值。

Result: 在数学推理任务上，R1蒸馏的Qwen-1.5B在OlympiadBench达到53.9%（GRPO为50.8%）；在智能体任务上，Qwen-3B在7个搜索问答数据集平均提升33.3%，如HotpotQA从29.7%提升到44.1%。

Conclusion: EMA-PG是一种简单、有理论依据且强大的方法，能够有效扩展LLM的强化学习能力。

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to acquire increasingly complex reasoning and agentic behaviors. In this work, we propose two simple techniques to improve policy gradient algorithms for LLMs. First, we replace the fixed anchor policy during RL with an Exponential Moving Average (EMA), similar to a target network in deep Q-learning. Second, we introduce Top-k KL estimator, which allows for flexible interpolation between exact KL and sampled KL. We derive the stability conditions for using EMA anchor; moreover, we show that our Top-k KL estimator yields both unbiased KL values and unbiased gradients at any k, while bringing the benefits of exact KL. When combined with GRPO, the two techniques (EMA-PG) lead to a significant performance boost. On math reasoning, it allows R1-distilled Qwen-1.5B to reach 53.9% on OlympiadBench compared to 50.8% by GRPO. On agentic RL domains, with Qwen-3B base, EMA-PG improves GRPO by an average of 33.3% across 7 datasets of Q&A with search engines, including 29.7% $\rightarrow$ 44.1% on HotpotQA, 27.4% $\rightarrow$ 40.1% on 2WikiMultiHopQA. Overall, we show that EMA-PG is a simple, principled, and powerful approach to scaling RL for LLMs. Code: https://github.com/LunjunZhang/ema-pg

</details>


### [55] [Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design](https://arxiv.org/abs/2602.04663)
*Jaemoo Choi,Yuchen Zhu,Wei Guo,Petr Molodyk,Bo Yuan,Jinbin Bai,Yi Xin,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 该论文系统分析了扩散模型强化学习的设计空间，发现基于ELBO的似然估计器是高效稳定优化的关键因素，显著提升了文本到图像生成的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视觉任务中应用广泛，但由于其似然函数难以处理，直接应用策略梯度方法存在障碍。现有方法主要基于LLM目标构建新目标，使用临时估计器估计似然，缺乏对估计如何影响算法性能的系统研究。

Method: 通过解耦三个因素进行系统分析：1) 策略梯度目标，2) 似然估计器，3) 轨迹采样方案。采用基于证据下界(ELBO)的模型似然估计器，仅从最终生成样本计算，作为主导优化因素。

Result: 在多个奖励基准测试中使用SD 3.5 Medium验证，所有任务均显示一致趋势。方法在90 GPU小时内将GenEval分数从0.24提升至0.95，比FlowGRPO效率高4.6倍，比SOTA方法DiffusionNFT效率高2倍且无奖励黑客问题。

Conclusion: 基于ELBO的似然估计器是实现扩散模型高效、稳定强化学习优化的关键因素，其重要性超过特定策略梯度损失函数的选择。

Abstract: Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.

</details>


### [56] [MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems](https://arxiv.org/abs/2602.04431)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: 提出MaMa算法，通过元代理与元对抗的Stackelberg博弈自动设计安全的智能体系统，即使部分智能体被攻陷也能保持安全。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统虽然能力强大，但存在安全风险，当个别智能体失效或被恶意攻击时可能导致系统不安全。需要设计能够抵御智能体被攻陷的安全系统。

Method: 将安全系统设计形式化为Stackelberg安全博弈，提出Meta-Adversary-Meta-Agent (MaMa)算法。元代理迭代提出系统设计，元对抗搜索最强攻击策略，通过LLM驱动的对抗搜索近似求解博弈。

Result: 在多种环境中验证，MaMa设计的系统能有效抵御最坏情况攻击，同时保持与仅优化任务性能的系统相当的表现。系统还能泛化到更强的对抗者、不同攻击目标和不同底层LLM。

Conclusion: MaMa算法能够自动设计出具有鲁棒安全性的智能体系统，即使在训练环境之外也能保持安全，为构建安全的LLM多智能体系统提供了有效方法。

Abstract: LLM-based multi-agent systems have demonstrated impressive capabilities, but they also introduce significant safety risks when individual agents fail or behave adversarially. In this work, we study the automated design of agentic systems that remain safe even when a subset of agents is compromised. We formalize this challenge as a Stackelberg security game between a system designer (the Meta-Agent) and a best-responding Meta-Adversary that selects and compromises a subset of agents to minimize safety. We propose Meta-Adversary-Meta-Agent (MaMa), a novel algorithm for approximately solving this game and automatically designing safe agentic systems. Our approach uses LLM-based adversarial search, where the Meta-Agent iteratively proposes system designs and receives feedback based on the strongest attacks discovered by the Meta-Adversary. Empirical evaluations across diverse environments show that systems designed with MaMa consistently defend against worst-case attacks while maintaining performance comparable to systems optimized solely for task success. Moreover, the resulting systems generalize to stronger adversaries, as well as ones with different attack objectives or underlying LLMs, demonstrating robust safety beyond the training setting.

</details>


### [57] [Subliminal Effects in Your Data: A General Mechanism via Log-Linearity](https://arxiv.org/abs/2602.04863)
*Ishaq Aden-Ali,Noah Golowich,Allen Liu,Abhishek Shetty,Ankur Moitra,Nika Haghtalab*

Main category: cs.LG

TL;DR: 提出Logit-Linear-Selection方法，通过选择偏好数据集的子集来激发LLM中的隐藏效应，如特定偏好、跨语言响应和角色扮演等。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型训练使用多种算法和数据集来激发特定行为，但数据集可能传递从单个数据点无法直接观察到的信号，这给基于数据集理解LLM训练带来了概念挑战，需要对这些现象进行基础性解释。

Method: 受LLM线性结构研究启发，提出Logit-Linear-Selection方法，该方法规定了如何从通用偏好数据集中选择子集来激发广泛的隐藏效应。

Result: 应用LLS发现真实世界数据集的子集，使得在这些子集上训练的模型表现出从特定偏好、响应未出现在数据集中的不同语言提示，到采用不同角色等多种行为。该效应在选择子集上持续存在，且在不同架构的模型中均有效，支持其普遍性和通用性。

Conclusion: 揭示了数据集如何通过隐藏子文本影响LLM行为的通用机制，为理解数据集对模型属性的影响提供了新视角。

Abstract: Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.

</details>


### [58] [Rethinking the Trust Region in LLM Reinforcement Learning](https://arxiv.org/abs/2602.04879)
*Penghui Qi,Xiangxin Zhou,Zichen Liu,Tianyu Pang,Chao Du,Min Lin,Wee Sun Lee*

Main category: cs.LG

TL;DR: DPPO提出用直接估计策略散度代替PPO的启发式裁剪机制，解决了PPO在大词汇量LLM微调中的结构性问题，提升了训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: PPO作为LLM微调的标准RL算法，其核心的比率裁剪机制在大词汇量场景下存在结构性问题：对低概率token更新过度惩罚，而对高概率token的灾难性偏移约束不足，导致训练效率低下和不稳定。

Method: 提出Divergence Proximal Policy Optimization (DPPO)，用基于直接策略散度估计（如总变差或KL散度）的约束替代启发式裁剪。为避免巨大内存开销，引入高效的Binary和Top-K近似方法来捕获核心散度信息。

Result: 广泛的实证评估表明，DPPO相比现有方法实现了更优的训练稳定性和效率，为基于RL的LLM微调提供了更稳健的基础。

Conclusion: DPPO通过更原则性的策略散度约束机制，解决了PPO在大词汇量LLM微调中的结构性问题，为RL-based LLM fine-tuning提供了更有效的算法基础。

Abstract: Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.

</details>


### [59] [Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty](https://arxiv.org/abs/2602.04763)
*Rui Liu,Pratap Tokekar,Ming Lin*

Main category: cs.LG

TL;DR: A2MAML是一个多智能体多模态学习框架，通过贝叶斯逆方差加权实现细粒度的模态级协作，在传感器损坏情况下提升鲁棒性，在自动驾驶事故检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作框架通常在智能体级别进行推理，假设同质传感，隐式处理不确定性，在传感器损坏时鲁棒性有限。需要一种能够处理异构多模态传感器、模态特定不确定性的细粒度协作方法。

Method: 提出A2MAML框架：1) 将每个模态特定特征建模为具有不确定性预测的随机估计；2) 主动选择可靠的智能体-模态对；3) 通过贝叶斯逆方差加权聚合信息。支持模态级融合和不对称模态可用性。

Result: 在互联自动驾驶场景的协作事故检测实验中，A2MAML持续优于单智能体和协作基线，事故检测率提升高达18.7%。

Conclusion: A2MAML提供了一种原则性的不确定性感知、模态级协作方法，能够有效抑制损坏或噪声模态，在多智能体多模态系统中具有显著优势。

Abstract: Multi-agent systems are increasingly equipped with heterogeneous multimodal sensors, enabling richer perception but introducing modality-specific and agent-dependent uncertainty. Existing multi-agent collaboration frameworks typically reason at the agent level, assume homogeneous sensing, and handle uncertainty implicitly, limiting robustness under sensor corruption. We propose Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty (A2MAML), a principled approach for uncertainty-aware, modality-level collaboration. A2MAML models each modality-specific feature as a stochastic estimate with uncertainty prediction, actively selects reliable agent-modality pairs, and aggregates information via Bayesian inverse-variance weighting. This formulation enables fine-grained, modality-level fusion, supports asymmetric modality availability, and provides a principled mechanism to suppress corrupted or noisy modalities. Extensive experiments on connected autonomous driving scenarios for collaborative accident detection demonstrate that A2MAML consistently outperforms both single-agent and collaborative baselines, achieving up to 18.7% higher accident detection rate.

</details>


### [60] [Beyond Rewards in Reinforcement Learning for Cyber Defence](https://arxiv.org/abs/2602.04809)
*Elizabeth Bates,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 该论文研究了在网络安全防御智能体训练中，稀疏奖励相比密集奖励的优势，发现稀疏奖励能产生更可靠、更有效且风险更低的防御策略。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全防御智能体通常使用密集、高度设计的奖励函数进行训练，这些函数结合了多种惩罚和激励。密集奖励有助于缓解复杂环境的探索挑战，但可能导致智能体偏向次优且风险更高的解决方案，这在复杂的网络环境中是一个关键问题。

Method: 使用多种稀疏和密集奖励函数，在两个成熟的网络训练环境（cyber gyms）中，针对不同网络规模，结合策略梯度和基于价值的强化学习算法，全面评估奖励函数结构对学习和策略行为特征的影响。采用新颖的ground truth评估方法，直接比较不同奖励函数的效果。

Result: 稀疏奖励（前提是目标对齐且能频繁遇到）能提供更强的训练可靠性和更有效的网络安全防御智能体，产生风险更低的策略。令人惊讶的是，稀疏奖励还能产生更符合网络安全防御目标的对齐策略，并且无需显式的基于奖励的数值惩罚就能节约使用成本高昂的防御行动。

Conclusion: 在网络安全防御智能体训练中，稀疏奖励相比密集奖励具有显著优势，能产生更可靠、更有效且风险更低的防御策略，同时更好地与防御目标对齐。

Abstract: Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.

</details>


### [61] [QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning](https://arxiv.org/abs/2602.04620)
*Doyeon Lee,Eunyi Lyou,Hyunsoo Cho,Sookyung Kim,Joonseok Lee,Jaemoo Choi*

Main category: cs.LG

TL;DR: QUATRO是一种新的强化学习微调方法，通过精确的信任区域约束解决现有GRPO方法中启发式近似导致的优化不稳定问题，提供更可控、稳定的策略更新。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO风格的RL微调方法依赖启发式的信任区域近似，存在优化脆弱性问题，全局重要性比率裁剪和组归一化无法有效处理超出裁剪范围的样本，导致训练不稳定。

Method: 提出QUATRO方法，通过原则性优化直接强制执行信任区域约束，产生清晰可解释的目标函数，实现对策略更新的显式控制和熵稳定的优化，稳定项从精确的信任区域公式中自然产生。

Result: 在多种数学推理基准测试中经验验证，QUATRO在增加策略陈旧性和激进学习率下仍能保持稳定训练，在整个训练过程中维持良好控制的熵。

Conclusion: QUATRO提供了一种更稳定、可控的RL微调方法，解决了现有启发式信任区域近似的局限性，为LLM强化学习微调提供了更可靠的优化框架。

Abstract: GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.

</details>


### [62] [SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF](https://arxiv.org/abs/2602.04651)
*Dipan Maity*

Main category: cs.LG

TL;DR: SAFE提出了一种新的RLHF算法，通过双软最小批评家、熵门控KL调节和PID控制自适应阈值来解决PPO在语言模型RLHF中的不稳定问题，实现了更稳定高效的策略优化。


<details>
  <summary>Details</summary>
Motivation: PPO虽然在RLHF中被广泛使用，但存在启发式动机、KL散度约束处理随意、奖励振荡、熵崩溃、价值函数漂移和突然策略发散等问题，需要频繁重启和大量超参数调优。需要一种更稳定可靠的RLHF方法。

Method: 提出SAFE算法：1) 双软最小批评家进行悲观价值估计；2) 多层稳定框架结合熵门控KL调节；3) PID控制自适应阈值。与PPO的对称KL惩罚不同，SAFE区分高熵探索和低熵模式崩溃，并根据奖励速度动态调整惩罚。

Result: 在3B参数模型上的实验显示，SAFE比PPO获得+5.15%的训练平均奖励（0.725 vs 0.689），奖励崩溃可忽略，KL控制优于PPO。方法计算开销最小，提供可解释、抗崩溃的RLHF框架。

Conclusion: SAFE提供了一个稳定、可解释的RLHF框架，在保持激进学习速度的同时确保长期优化稳定性，适合生产部署。解决了PPO在语言模型RLHF中的关键稳定性问题。

Abstract: Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE

</details>


### [63] [CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation](https://arxiv.org/abs/2602.04868)
*Yannick Denker,Alexander Gepperth*

Main category: cs.LG

TL;DR: 提出CRoSS基准套件，用于持续强化学习在机器人仿真中的评估，包含轮式机器人和机械臂两种平台，支持多种传感器和高物理真实度仿真。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习需要智能体在序列任务中学习而不遗忘先前策略，但缺乏基于高物理真实度机器人仿真的标准化基准套件。

Method: 开发基于Gazebo仿真的CRoSS基准套件，包含两种机器人平台：差分驱动轮式机器人（用于循线和推物任务）和七关节机械臂（用于笛卡尔位置控制和关节角度控制任务），提供容器化部署方案。

Result: CRoSS支持高物理真实度仿真和几乎任意传感器使用，机械臂基准提供无需物理仿真的运动学变体（运行速度快两个数量级），并验证了DQN和策略梯度等标准RL算法的性能。

Conclusion: CRoSS为机器人环境中的持续强化学习研究提供了可扩展、可复现的基准，支持受控研究和传感器多样性评估。

Abstract: Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.

</details>


### [64] [Rationality Measurement and Theory for Reinforcement Learning Agents](https://arxiv.org/abs/2602.04737)
*Kejiang Qian,Amos Storkey,Fengxiang He*

Main category: cs.LG

TL;DR: 该论文提出了一套用于强化学习智能体的理性度量指标及相关理论，定义了完美理性动作、期望理性风险、理性风险间隙等概念，并分析了环境转移和算法泛化性对理性风险的影响。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的理性属性日益重要但鲜有研究，需要建立理论框架来量化智能体在部署环境中的理性表现，并分析影响理性的因素。

Method: 定义了完美理性动作（最大化隐藏真实价值函数的最速方向），提出了期望理性风险和理性风险间隙的概念，将风险间隙分解为环境转移引起的外在成分和算法泛化性引起的内在成分，并用1-Wasserstein距离和经验Rademacher复杂度分别上界。

Result: 理论分析表明正则化器（层归一化、ℓ2正则化、权重归一化）和领域随机化有助于降低理性风险，而环境转移会带来危害。实验完全验证了这些假设。

Conclusion: 该研究为强化学习智能体的理性提供了系统的理论框架和度量方法，揭示了影响理性的关键因素，并为改进智能体理性提供了理论指导。

Abstract: This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy's actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm's generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.

</details>


### [65] [Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning](https://arxiv.org/abs/2602.04807)
*Wolfgang Maass,Sabine Janzen,Prajvi Saxena,Sach Mukherjee*

Main category: cs.LG

TL;DR: Afferent Learning框架通过进化优化发现计算传入痕迹(CATs)作为内部风险信号，用于损伤避免学习，在生物力学数字孪生中长期应用中表现出高效性和年龄鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受生物系统启发，研究如何为损伤避免学习提供有效的内部风险信号，解决传统方法在长期、复杂环境中学习效率低的问题，特别是在生物力学数字孪生等挑战性场景中。

Method: 采用双层架构：外层进化优化发现传入感知架构，内层强化学习使用这些信号训练损伤避免策略。将传入感知形式化为提供高效学习的归纳偏置，架构选择基于其能否支持有效学习而非直接最小化损伤。

Result: 在生物力学数字孪生长期应用（数十年生命历程）中，基于CAT的进化架构比手工设计基线显著提高效率（23%高风险动作减少）和年龄鲁棒性，实现年龄依赖的行为适应。消融研究验证了CAT信号、进化和预测差异的重要性。

Conclusion: Afferent Learning框架通过进化优化的传入感知架构为损伤避免学习提供有效的内部风险信号，在长期复杂任务中表现出优越性能，为生物启发学习系统提供了新方法。

Abstract: We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.

</details>
