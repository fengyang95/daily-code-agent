{"id": "2510.09721", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09721", "abs": "https://arxiv.org/abs/2510.09721", "authors": ["Jiale Guo", "Suizhi Huang", "Mei Li", "Dong Huang", "Xingsheng Chen", "Regina Zhang", "Zhijiang Guo", "Han Yu", "Siu-Ming Yiu", "Christian Jensen", "Pietro Lio", "Kwok-Yan Lam"], "title": "A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System", "comment": "21 pages", "summary": "The integration of LLMs into software engineering has catalyzed a paradigm\nshift from traditional rule-based systems to sophisticated agentic systems\ncapable of autonomous problem-solving. Despite this transformation, the field\nlacks a comprehensive understanding of how benchmarks and solutions\ninterconnect, hindering systematic progress and evaluation. This survey\npresents the first holistic analysis of LLM-empowered software engineering,\nbridging the critical gap between evaluation and solution approaches. We\nanalyze 150+ recent papers and organize them into a comprehensive taxonomy\nspanning two major dimensions: (1) Solutions, categorized into prompt-based,\nfine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code\ngeneration, translation, repair, and other tasks. Our analysis reveals how the\nfield has evolved from simple prompt engineering to complex agentic systems\nincorporating planning and decomposition, reasoning and self-refinement, memory\nmechanisms, and tool augmentation. We present a unified pipeline that\nillustrates the complete workflow from task specification to final\ndeliverables, demonstrating how different solution paradigms address varying\ncomplexity levels across software engineering tasks. Unlike existing surveys\nthat focus on isolated aspects, we provide full-spectrum coverage connecting\n50+ benchmarks with their corresponding solution strategies, enabling\nresearchers to identify optimal approaches for specific evaluation criteria.\nFurthermore, we identify critical research gaps and propose actionable future\ndirections, including multi-agent collaboration frameworks, self-evolving code\ngeneration systems, and integration of formal verification with LLM-based\nmethods. This survey serves as a foundational resource for researchers and\npractitioners seeking to understand, evaluate, and advance LLM-empowered\nsoftware engineering systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9LLM\u8d4b\u80fd\u7684\u8f6f\u4ef6\u5de5\u7a0b\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u5206\u6790\uff0c\u901a\u8fc7\u5206\u6790150\u591a\u7bc7\u8bba\u6587\u6784\u5efa\u4e86\u6db5\u76d6\u89e3\u51b3\u65b9\u6848\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b8c\u6574\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4e86\u4ece\u7b80\u5355\u63d0\u793a\u5de5\u7a0b\u5230\u590d\u6742\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53d1\u5c55\u8def\u5f84\u3002", "motivation": "\u5f53\u524dLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u5e94\u7528\u7f3a\u4e4f\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u548c\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u76f8\u4e92\u5173\u7cfb\u7684\u7cfb\u7edf\u6027\u7406\u89e3\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u7cfb\u7edf\u6027\u8fdb\u5c55\u548c\u8bc4\u4f30\u3002", "method": "\u5206\u6790150\u591a\u7bc7\u8fd1\u671f\u8bba\u6587\uff0c\u6784\u5efa\u5305\u542b\u89e3\u51b3\u65b9\u6848\uff08\u63d0\u793a\u57fa\u7840\u3001\u5fae\u8c03\u57fa\u7840\u3001\u667a\u80fd\u4f53\u57fa\u7840\uff09\u548c\u57fa\u51c6\u6d4b\u8bd5\uff08\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u3001\u4fee\u590d\u7b49\u4efb\u52a1\uff09\u4e24\u4e2a\u7ef4\u5ea6\u7684\u7efc\u5408\u5206\u7c7b\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u6d41\u6c34\u7ebf\uff0c\u5c55\u793a\u4e86\u4ece\u4efb\u52a1\u89c4\u8303\u5230\u6700\u7ec8\u4ea4\u4ed8\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8fde\u63a5\u4e8650\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u53ca\u5176\u5bf9\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u7b56\u7565\u3002", "conclusion": "\u8be5\u8c03\u67e5\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u7406\u89e3\u3001\u8bc4\u4f30\u548c\u63a8\u8fdbLLM\u8d4b\u80fd\u8f6f\u4ef6\u5de5\u7a0b\u7cfb\u7edf\u7684\u57fa\u7840\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u3001\u81ea\u8fdb\u5316\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.09724", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09724", "abs": "https://arxiv.org/abs/2510.09724", "authors": ["Qiaosheng Chen", "Yang Liu", "Lei Li", "Kai Chen", "Qipeng Guo", "Gong Cheng", "Fei Yuan"], "title": "InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation", "comment": "27 pages, 17 figures", "summary": "Large Language Models (LLMs) are increasingly capable of generating complete\napplications from natural language instructions, creating new opportunities in\nscience and education. In these domains, interactive scientific demonstrations\nare particularly valuable for explaining concepts, supporting new teaching\nmethods, and presenting research findings. Generating such demonstrations\nrequires models to combine accurate scientific knowledge with the ability to\nimplement interactive front-end code that behaves correctly and responds to\nuser actions. This capability goes beyond the scope of existing benchmarks,\nwhich typically evaluate either knowledge question answering without grounding\nin code or static web code generation without scientific interactivity. To\nevaluate this integrated ability, we design a hybrid framework that combines\nprogrammatic functional testing to rigorously verify interaction logic with\nvisually-grounded qualitative testing to assess rendered outputs against\nreference snapshots. Building on this framework, we present InteractScience, a\nbenchmark consisting of a substantial set of carefully designed questions\nacross five scientific domains, each paired with unit tests, reference\nsnapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs\nand report results that highlight ongoing weaknesses in integrating domain\nknowledge with interactive front-end coding. Our work positions InteractScience\nas the first benchmark to automatically measure this combined capability with\nrealistic interactive operations, providing a foundation for advancing reliable\nand educationally useful scientific demonstration code generation. All code and\ndata are publicly available at https://github.com/open-compass/InteractScience.", "AI": {"tldr": "InteractScience\u662f\u9996\u4e2a\u8bc4\u4f30LLMs\u7ed3\u5408\u79d1\u5b66\u77e5\u8bc6\u4e0e\u4ea4\u4e92\u5f0f\u524d\u7aef\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e94\u4e2a\u79d1\u5b66\u9886\u57df\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u529f\u80fd\u6d4b\u8bd5\u548c\u89c6\u89c9\u5b9a\u6027\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u751f\u6210\u7684\u4ea4\u4e92\u5f0f\u79d1\u5b66\u6f14\u793a\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8981\u4e48\u8bc4\u4f30\u77e5\u8bc6\u95ee\u7b54\u800c\u4e0d\u6d89\u53ca\u4ee3\u7801\uff0c\u8981\u4e48\u8bc4\u4f30\u9759\u6001\u7f51\u9875\u4ee3\u7801\u751f\u6210\u800c\u7f3a\u4e4f\u79d1\u5b66\u4ea4\u4e92\u6027\uff0c\u65e0\u6cd5\u8bc4\u4f30LLMs\u7ed3\u5408\u79d1\u5b66\u77e5\u8bc6\u548c\u4ea4\u4e92\u5f0f\u524d\u7aef\u4ee3\u7801\u751f\u6210\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u7a0b\u5e8f\u5316\u529f\u80fd\u6d4b\u8bd5\u9a8c\u8bc1\u4ea4\u4e92\u903b\u8f91\uff0c\u4ee5\u53ca\u57fa\u4e8e\u89c6\u89c9\u7684\u5b9a\u6027\u6d4b\u8bd5\u8bc4\u4f30\u6e32\u67d3\u8f93\u51fa\u4e0e\u53c2\u8003\u5feb\u7167\u7684\u5339\u914d\u5ea6\u3002\u6784\u5efa\u5305\u542b\u4e94\u4e2a\u79d1\u5b66\u9886\u57df\u95ee\u9898\u7684InteractScience\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f30\u4e8630\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u548c\u95ed\u6e90LLMs\uff0c\u7ed3\u679c\u663e\u793a\u5728\u6574\u5408\u9886\u57df\u77e5\u8bc6\u4e0e\u4ea4\u4e92\u5f0f\u524d\u7aef\u7f16\u7801\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u5f31\u70b9\u3002", "conclusion": "InteractScience\u662f\u9996\u4e2a\u80fd\u81ea\u52a8\u6d4b\u91cfLLMs\u7ed3\u5408\u79d1\u5b66\u77e5\u8bc6\u4e0e\u4ea4\u4e92\u5f0f\u524d\u7aef\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u63a8\u8fdb\u53ef\u9760\u4e14\u6559\u80b2\u6709\u7528\u7684\u79d1\u5b66\u6f14\u793a\u4ee3\u7801\u751f\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "swe benchmark"}}
{"id": "2510.09907", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09907", "abs": "https://arxiv.org/abs/2510.09907", "authors": ["Muhammad Maaz", "Liam DeVoe", "Zac Hatfield-Dodds", "Nicholas Carlini"], "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem", "comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop", "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u5206\u6790Python\u6a21\u5757\u3001\u4ece\u4ee3\u7801\u548c\u6587\u6863\u4e2d\u63a8\u65ad\u51fd\u6570\u7279\u6027\u548c\u8de8\u51fd\u6570\u5c5e\u6027\uff0c\u5408\u6210\u5e76\u6267\u884c\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u8f93\u51fa\u786e\u8ba4\u771f\u5b9ebug\uff0c\u6700\u7ec8\u4e3a\u5f00\u53d1\u8005\u8f93\u51fa\u53ef\u64cd\u4f5c\u7684bug\u62a5\u544a\u3002", "motivation": "\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u6307\u5b9a\u6d4b\u8bd5\u5c5e\u6027\u548c\u8f93\u5165\u57df\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528LLM\u81ea\u52a8\u63a8\u65ad\u8f6f\u4ef6\u5c5e\u6027\u5e76\u6267\u884c\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u81ea\u4e3b\u5316\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\uff1a1\uff09\u5206\u6790Python\u6a21\u5757\uff1b2\uff09\u4ece\u4ee3\u7801\u548c\u6587\u6863\u4e2d\u63a8\u65ad\u51fd\u6570\u7279\u6027\u548c\u8de8\u51fd\u6570\u5c5e\u6027\uff1b3\uff09\u5408\u6210\u5e76\u6267\u884c\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5\uff1b4\uff09\u901a\u8fc7\u6d4b\u8bd5\u8f93\u51fa\u786e\u8ba4\u771f\u5b9ebug\uff1b5\uff09\u8f93\u51fa\u53ef\u64cd\u4f5c\u7684bug\u62a5\u544a\u3002", "result": "\u5728100\u4e2a\u6d41\u884cPython\u5305\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a56%\u7684bug\u62a5\u544a\u662f\u6709\u6548bug\uff0c32%\u662f\u503c\u5f97\u5411\u7ef4\u62a4\u8005\u62a5\u544a\u7684\u6709\u6548bug\u3002\u4f7f\u7528\u6392\u540d\u6807\u51c6\u7b5b\u9009\u51fa\u768421\u4e2a\u9ad8\u4f18\u5148\u7ea7bug\u4e2d\uff0c86%\u6709\u6548\uff0c81%\u503c\u5f97\u62a5\u544a\u3002\u5df2\u62a5\u544a5\u4e2abug\uff0c\u5176\u4e2d4\u4e2a\u9644\u5e26\u8865\u4e01\uff0c3\u4e2a\u8865\u4e01\u6210\u529f\u5408\u5e76\u5230NumPy\u548c\u4e91\u8ba1\u7b97SDK\u4e2d\u3002", "conclusion": "LLM\u4e0e\u57fa\u4e8e\u5c5e\u6027\u6d4b\u8bd5\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e25\u8c28\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u8f6f\u4ef6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u80fd\u591f\u53d1\u73b0\u4ece\u5e8f\u5217\u5316\u5931\u8d25\u5230\u6570\u503c\u7cbe\u5ea6\u9519\u8bef\u7b49\u591a\u79cd\u7c7b\u578b\u7684bug\u3002", "topic": "swe application"}}
{"id": "2510.09782", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.09782", "abs": "https://arxiv.org/abs/2510.09782", "authors": ["Yufa Zhou", "Yixiao Wang", "Xunjian Yin", "Shuyan Zhou", "Anru R. Zhang"], "title": "The Geometry of Reasoning: Flowing Logics in Representation Space", "comment": "Code: https://github.com/MasterZhou1/Reasoning-Flow", "summary": "We study how large language models (LLMs) ``think'' through their\nrepresentation space. We propose a novel geometric framework that models an\nLLM's reasoning as flows -- embedding trajectories evolving where logic goes.\nWe disentangle logical structure from semantics by employing the same natural\ndeduction propositions with varied semantic carriers, allowing us to test\nwhether LLMs internalize logic beyond surface form. This perspective connects\nreasoning with geometric quantities such as position, velocity, and curvature,\nenabling formal analysis in representation and concept spaces. Our theory\nestablishes: (1) LLM reasoning corresponds to smooth flows in representation\nspace, and (2) logical statements act as local controllers of these flows'\nvelocities. Using learned representation proxies, we design controlled\nexperiments to visualize and quantify reasoning flows, providing empirical\nvalidation of our theoretical framework. Our work serves as both a conceptual\nfoundation and practical tools for studying reasoning phenomenon, offering a\nnew lens for interpretability and formal analysis of LLMs' behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5efa\u6a21\u4e3a\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u6d41\uff08flows\uff09\uff0c\u901a\u8fc7\u5206\u6790\u5d4c\u5165\u8f68\u8ff9\u6765\u7814\u7a76LLM\u5982\u4f55\u5728\u5176\u8868\u793a\u7a7a\u95f4\u4e2d\"\u601d\u8003\"\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u6267\u884c\u63a8\u7406\uff0c\u5c06\u903b\u8f91\u63a8\u7406\u4e0e\u51e0\u4f55\u6982\u5ff5\uff08\u5982\u4f4d\u7f6e\u3001\u901f\u5ea6\u3001\u66f2\u7387\uff09\u8054\u7cfb\u8d77\u6765\uff0c\u4e3aLLM\u884c\u4e3a\u63d0\u4f9b\u5f62\u5f0f\u5316\u5206\u6790\u7684\u65b0\u89c6\u89d2\u3002", "method": "\u4f7f\u7528\u51e0\u4f55\u6846\u67b6\u5efa\u6a21LLM\u63a8\u7406\u4e3a\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u6d41\uff0c\u901a\u8fc7\u81ea\u7136\u6f14\u7ece\u547d\u9898\u5206\u79bb\u903b\u8f91\u7ed3\u6784\u4e0e\u8bed\u4e49\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u8868\u793a\u4ee3\u7406\u8bbe\u8ba1\u53d7\u63a7\u5b9e\u9a8c\u6765\u53ef\u89c6\u5316\u548c\u91cf\u5316\u63a8\u7406\u6d41\u3002", "result": "\u7406\u8bba\u5efa\u7acb\u4e86\uff1a(1) LLM\u63a8\u7406\u5bf9\u5e94\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5e73\u6ed1\u6d41\uff1b(2) \u903b\u8f91\u8bed\u53e5\u4f5c\u4e3a\u8fd9\u4e9b\u6d41\u901f\u5ea6\u7684\u5c40\u90e8\u63a7\u5236\u5668\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7814\u7a76\u63a8\u7406\u73b0\u8c61\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u548c\u5b9e\u8df5\u5de5\u5177\uff0c\u4e3aLLM\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5f62\u5f0f\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2510.09801", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09801", "abs": "https://arxiv.org/abs/2510.09801", "authors": ["Valerie Chen", "Rohit Malhotra", "Xingyao Wang", "Juan Michelini", "Xuhui Zhou", "Aditya Bharat Soni", "Hoang H. Tran", "Calvin Smith", "Ameet Talwalkar", "Graham Neubig"], "title": "How can we assess human-agent interactions? Case studies in software agent design", "comment": null, "summary": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40\\% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86PULSE\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u9ad8\u6548\u5730\u8bc4\u4f30\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6536\u96c6\u7528\u6237\u53cd\u9988\u3001\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u548c\u7ed3\u5408\u4eba\u5de5\u8bc4\u5206\u4e0e\u6a21\u578b\u4f2a\u6807\u7b7e\u6765\u8ba1\u7b97\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u5047\u8bbe\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4e2d\u4eba\u673a\u534f\u4f5c\u7684\u4f7f\u7528\u573a\u666f\uff0c\u9700\u8981\u66f4\u4e25\u8c28\u7684\u4eba\u7c7b-\u667a\u80fd\u4f53\u4ea4\u4e92\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1PULSE\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5728\u57fa\u4e8e\u5f00\u6e90\u8f6f\u4ef6\u667a\u80fd\u4f53OpenHands\u7684\u5927\u89c4\u6a21\u7f51\u7edc\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u6536\u96c6\u8d85\u8fc715,000\u540d\u7528\u6237\u7684\u5b9e\u5730\u4f7f\u7528\u6570\u636e\uff0c\u7814\u7a76LLM\u9aa8\u5e72\u6a21\u578b\u3001\u89c4\u5212\u7b56\u7565\u548c\u8bb0\u5fc6\u673a\u5236\u4e09\u4e2a\u8bbe\u8ba1\u51b3\u7b56\u5bf9\u5f00\u53d1\u8005\u6ee1\u610f\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u6846\u67b6\u4f7f\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u7f6e\u4fe1\u533a\u95f4\u6bd4\u6807\u51c6A/B\u6d4b\u8bd5\u51cf\u5c1140%\uff0c\u53d1\u73b0\u5b9e\u5730\u7ed3\u679c\u4e0e\u57fa\u51c6\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08\u5982claude-sonnet-4\u4e0egpt-5\u7684\u6bd4\u8f83\u7ed3\u679c\u5448\u8d1f\u76f8\u5173\uff09\uff0c\u7a81\u663e\u4e86\u57fa\u51c6\u9a71\u52a8\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u4e3aLLM\u667a\u80fd\u4f53\u7684\u4eba\u7c7b\u8bc4\u4f30\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u8bc6\u522b\u4e86\u6539\u8fdb\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u673a\u4f1a\u3002", "topic": "agent analysis"}}
{"id": "2510.10010", "categories": ["cs.SE", "cs.AI", "68N01, 68T05, 68T07", "D.2.5; D.2.7; I.2.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.10010", "abs": "https://arxiv.org/abs/2510.10010", "authors": ["Matheus J. T. Vargas"], "title": "SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study", "comment": "14 pages, 4 figures, 6 tables, link to code repo", "summary": "We present SLEAN (Simple Lightweight Ensemble Analysis Network), a\ndeterministic framework for coordinating multiple LLM providers through\ntext-based prompt orchestration. Unlike complex multi-agent systems requiring\nspecialized infrastructure, SLEAN operates as a simple prompt bridge between\nLLMs using .txt templates, requiring no deep technical knowledge for\ndeployment. The three-phase protocol formed by independent analysis,\ncross-critique, and arbitration, filters harmful AI-generated code suggestions\nbefore production deployment, addressing how AI-assisted debugging increasingly\nproduces modifications that introduce unnecessary complexity, break existing\nfunctionality, or address problems. Evaluating 15 software bugs, we analyzed 69\nAI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%\nCI 20.9-42.9%) while rejecting 47 that would have been harmful if applied\nverbatim. The arbitration process reduced code change surface by 83-90%\nrelative to raw AI outputs, enforcing minimal causal edits over scope-expanding\nmodifications. Minimal Type 2 inputs proved more efficient than detailed Type 1\ninputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus\n28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems\nshowed weak correlation with fix quality: high convergence (at least 80%)\noccurred in 4 of 15 cases and improved acceptance by only 2.4% points;\narbitration appeared only at exactly 10% convergence in 2 of 15 cases, although\nlow convergence alone did not necessitate arbitration. The file-driven,\nprovider-agnostic architecture enables deployment without specialized coding\nexpertise, making it applicable to security auditing, code review, document\nverification, and other domains requiring reliable multi-provider synthesis\nwith end-to-end auditability.", "AI": {"tldr": "SLEAN\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7f16\u6392\u7684\u786e\u5b9a\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u5206\u6790\u3001\u4ea4\u53c9\u6279\u8bc4\u548c\u4ef2\u88c1\u4e09\u9636\u6bb5\u534f\u8bae\uff0c\u534f\u8c03\u591a\u4e2aLLM\u63d0\u4f9b\u5546\u8fc7\u6ee4\u6709\u5bb3\u7684AI\u751f\u6210\u4ee3\u7801\u5efa\u8bae\u3002", "motivation": "\u89e3\u51b3AI\u8f85\u52a9\u8c03\u8bd5\u4e2d\u4ea7\u751f\u7684\u4fee\u6539\u5f15\u5165\u4e0d\u5fc5\u8981\u590d\u6742\u6027\u3001\u7834\u574f\u73b0\u6709\u529f\u80fd\u6216\u5904\u7406\u9519\u8bef\u95ee\u9898\u7684\u60c5\u51b5\uff0c\u786e\u4fdd\u4ee3\u7801\u4fee\u6539\u7684\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528.txt\u6a21\u677f\u4f5c\u4e3aLLM\u4e4b\u95f4\u7684\u7b80\u5355\u63d0\u793a\u6865\u63a5\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u534f\u8bae\uff1a\u72ec\u7acb\u5206\u6790\u3001\u4ea4\u53c9\u6279\u8bc4\u548c\u4ef2\u88c1\uff0c\u65e0\u9700\u4e13\u4e1a\u6280\u672f\u77e5\u8bc6\u5373\u53ef\u90e8\u7f72\u3002", "result": "\u5728\u8bc4\u4f3015\u4e2a\u8f6f\u4ef6bug\u65f6\uff0cSLEAN\u8fc7\u6ee4\u4e8669\u4e2aAI\u751f\u6210\u7684\u4fee\u590d\u5efa\u8bae\uff0c\u63a5\u53d7\u4e8622\u4e2a\u4fee\u590d\uff0831.9%\uff09\uff0c\u62d2\u7edd\u4e8647\u4e2a\u6709\u5bb3\u4fee\u590d\uff0c\u5c06\u4ee3\u7801\u53d8\u66f4\u8303\u56f4\u51cf\u5c11\u4e8683-90%\u3002", "conclusion": "SLEAN\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u63d0\u4f9b\u5546\u65e0\u5173\u7684\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5ba1\u8ba1\u3001\u4ee3\u7801\u5ba1\u67e5\u7b49\u9886\u57df\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u63d0\u4f9b\u5546\u5408\u6210\u548c\u7aef\u5230\u7aef\u53ef\u5ba1\u8ba1\u6027\u3002", "topic": "swe application"}}
{"id": "2510.09901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09901", "abs": "https://arxiv.org/abs/2510.09901", "authors": ["Lianhao Zhou", "Hongyi Ling", "Cong Fu", "Yepeng Huang", "Michael Sun", "Wendi Yu", "Xiaoxuan Wang", "Xiner Li", "Xingyu Su", "Junkai Zhang", "Xiusi Chen", "Chenxing Liang", "Xiaofeng Qian", "Heng Ji", "Wei Wang", "Marinka Zitnik", "Shuiwang Ji"], "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics", "comment": null, "summary": "Computing has long served as a cornerstone of scientific discovery. Recently,\na paradigm shift has emerged with the rise of large language models (LLMs),\nintroducing autonomous systems, referred to as agents, that accelerate\ndiscovery across varying levels of autonomy. These language agents provide a\nflexible and versatile framework that orchestrates interactions with human\nscientists, natural language, computer language and code, and physics. This\npaper presents our view and vision of LLM-based scientific agents and their\ngrowing role in transforming the scientific discovery lifecycle, from\nhypothesis discovery, experimental design and execution, to result analysis and\nrefinement. We critically examine current methodologies, emphasizing key\ninnovations, practical achievements, and outstanding limitations. Additionally,\nwe identify open research challenges and outline promising directions for\nbuilding more robust, generalizable, and adaptive scientific agents. Our\nanalysis highlights the transformative potential of autonomous agents to\naccelerate scientific discovery across diverse domains.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u4ee3\u7406\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u53d8\u9769\u4f5c\u7528\uff0c\u4ece\u5047\u8bbe\u53d1\u73b0\u5230\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7ed3\u679c\u5206\u6790\u7684\u5168\u6d41\u7a0b\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u81ea\u4e3b\u7cfb\u7edf\uff08\u4ee3\u7406\uff09\u6b63\u5728\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\uff0c\u8fd9\u4e9b\u8bed\u8a00\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u6846\u67b6\u6765\u534f\u8c03\u4e0e\u79d1\u5b66\u5bb6\u3001\u81ea\u7136\u8bed\u8a00\u3001\u8ba1\u7b97\u673a\u8bed\u8a00\u548c\u7269\u7406\u5b66\u7684\u4ea4\u4e92\u3002", "method": "\u6279\u5224\u6027\u5ba1\u89c6\u5f53\u524d\u65b9\u6cd5\u8bba\uff0c\u5f3a\u8c03\u5173\u952e\u521b\u65b0\u3001\u5b9e\u9645\u6210\u5c31\u548c\u73b0\u5b58\u9650\u5236\uff0c\u8bc6\u522b\u5f00\u653e\u7814\u7a76\u6311\u6218\u5e76\u6982\u8ff0\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u53ef\u6cdb\u5316\u548c\u9002\u5e94\u6027\u79d1\u5b66\u4ee3\u7406\u7684\u6709\u524d\u666f\u65b9\u5411\u3002", "result": "\u5206\u6790\u7a81\u663e\u4e86\u81ea\u4e3b\u4ee3\u7406\u5728\u52a0\u901f\u8de8\u9886\u57df\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u53d8\u9769\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u4ee3\u7406\u6b63\u5728\u6539\u53d8\u79d1\u5b66\u53d1\u73b0\u7684\u751f\u547d\u5468\u671f\uff0c\u5177\u6709\u52a0\u901f\u8de8\u9886\u57df\u79d1\u5b66\u53d1\u73b0\u7684\u5de8\u5927\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.10119", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10119", "abs": "https://arxiv.org/abs/2510.10119", "authors": ["Liutong Han", "Zhiyuan Tan", "Hongbin Zhang", "Pengcheng Wang", "Chu Kang", "Mingjie Xing", "Yanjun Wu"], "title": "IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector", "comment": "9 pages", "summary": "The use of intrinsic functions to exploit hardware-specific capabilities is\nan important approach for optimizing library performance. Many mainstream\nlibraries implement a large number of vectorized algorithms on Arm or x86 SIMD\nintrinsic functions. With the rapid expansion of the RISC-V hardware-software\necosystem, there is a growing demand for support of the RISC-V Vector (RVV)\nextension. Translating existing vectorized intrinsic code onto RVV intrinsics\nis a practical and effective approach. However, current cross-architecture\ntranslation largely relies on manual rewriting, which is time-consuming and\nerror-prone. Furthermore, while some rule-based methods can reduce the need for\nmanual intervention, their translation success rate is limited by incomplete\nrule coverage and syntactic constraints, and the performance suffers from\ninadequate utilization of RVV-specific features. We present IntrinTrans, a\nLLM-based multi-agent approach that utilizes compile-and-test feedback to\ntranslate intrinsic code across architectures automatically, and further\noptimizes the generated RVV intrinsics using register-usage information derived\nfrom liveness analysis. To evaluate the effectiveness of our approach, we\ncollected 34 vectorized algorithm cases from open-source libraries. Each case\nincludes an Arm Neon intrinsics implementation and a RVV intrinsics\nimplementation contributed by the open-source community, together with\ncorrectness and performance tests. Our experiments show that advanced LLMs\nproduce semantically correct RISC-V Vector intrinsics in most cases within a\nlimited number of iterations, and in some cases achieve up to 5.93x the\nperformance of the native implementation from the open-source community.", "AI": {"tldr": "IntrinTrans\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5229\u7528\u7f16\u8bd1-\u6d4b\u8bd5\u53cd\u9988\u81ea\u52a8\u8de8\u67b6\u6784\u7ffb\u8bd1\u5185\u8054\u51fd\u6570\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u6d3b\u8dc3\u5ea6\u5206\u6790\u4f18\u5316\u751f\u6210\u7684RISC-V\u5411\u91cf\u5185\u8054\u51fd\u6570\u6027\u80fd\u3002", "motivation": "\u968f\u7740RISC-V\u786c\u4ef6\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u5feb\u901f\u6269\u5c55\uff0c\u5bf9RISC-V\u5411\u91cf\u6269\u5c55\u652f\u6301\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u624b\u52a8\u7ffb\u8bd1\u73b0\u6709\u5411\u91cf\u5316\u5185\u8054\u51fd\u6570\u4ee3\u7801\u5230RVV\u5185\u8054\u51fd\u6570\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u7ffb\u8bd1\u6210\u529f\u7387\u6709\u9650\u4e14\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u7ed3\u5408\u7f16\u8bd1-\u6d4b\u8bd5\u53cd\u9988\u8fdb\u884c\u81ea\u52a8\u8de8\u67b6\u6784\u7ffb\u8bd1\uff0c\u5e76\u901a\u8fc7\u6d3b\u8dc3\u5ea6\u5206\u6790\u83b7\u53d6\u5bc4\u5b58\u5668\u4f7f\u7528\u4fe1\u606f\u6765\u4f18\u5316\u751f\u6210\u7684RVV\u5185\u8054\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u9ad8\u7ea7LLM\u5728\u6709\u9650\u8fed\u4ee3\u6b21\u6570\u5185\u80fd\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u751f\u6210\u8bed\u4e49\u6b63\u786e\u7684RISC-V\u5411\u91cf\u5185\u8054\u51fd\u6570\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6027\u80fd\u8fbe\u5230\u5f00\u6e90\u793e\u533a\u539f\u751f\u5b9e\u73b0\u76845.93\u500d\u3002", "conclusion": "IntrinTrans\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u81ea\u52a8\u7ffb\u8bd1\u548c\u4f18\u5316\u8de8\u67b6\u6784\u5185\u8054\u51fd\u6570\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2510.10148", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10148", "abs": "https://arxiv.org/abs/2510.10148", "authors": ["Mengyao Zhao", "Kaixuan Li", "Lyuye Zhang", "Wenjing Dang", "Chenggong Ding", "Sen Chen", "Zheli Liu"], "title": "A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have brought remarkable\nprogress in code understanding and reasoning, creating new opportunities and\nraising new concerns for software security. Among many downstream tasks,\ngenerating Proof-of-Concept (PoC) exploits plays a central role in\nvulnerability reproduction, comprehension, and mitigation. While previous\nresearch has focused primarily on zero-day exploitation, the growing\navailability of rich public information accompanying disclosed CVEs leads to a\nnatural question: can LLMs effectively use this information to automatically\ngenerate valid PoCs? In this paper, we present the first empirical study of\nLLM-based PoC generation for web application vulnerabilities, focusing on the\npractical feasibility of leveraging publicly disclosed information. We evaluate\nGPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three\nstages of vulnerability disclosure: (1) newly disclosed vulnerabilities with\nonly descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day\nvulnerabilities with full contextual code. Our results show that LLMs can\nautomatically generate working PoCs in 8%-34% of cases using only public data,\nwith DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that\nsupplementing code context improves success rates by 17%-20%, with\nfunction-level providing 9%-13% improvement than file-level ones. Further\nintegrating adaptive reasoning strategies to prompt refinement significantly\nimproves success rates to 68%-72%. Our findings suggest that LLMs could reshape\nvulnerability exploitation dynamics. To date, 23 newly generated PoCs have been\naccepted by NVD and Exploit DB.", "AI": {"tldr": "LLMs can\u81ea\u52a8\u751f\u6210\u6709\u6548\u7684\u6982\u5ff5\u9a8c\u8bc1\u6f0f\u6d1e\u5229\u7528\uff0c\u5728\u4ec5\u4f7f\u7528\u516c\u5f00\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u7387\u4e3a8%-34%\uff0c\u901a\u8fc7\u4ee3\u7801\u4e0a\u4e0b\u6587\u548c\u81ea\u9002\u5e94\u63a8\u7406\u7b56\u7565\u53ef\u5c06\u6210\u529f\u7387\u63d0\u5347\u81f368%-72%\u3002", "motivation": "\u968f\u7740LLMs\u5728\u4ee3\u7801\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u6b65\uff0c\u7814\u7a76\u5176\u662f\u5426\u80fd\u5229\u7528\u516c\u5f00\u62ab\u9732\u7684CVE\u4fe1\u606f\u81ea\u52a8\u751f\u6210\u6709\u6548\u7684PoC\u6f0f\u6d1e\u5229\u7528\uff0c\u8fd9\u5bf9\u6f0f\u6d1e\u590d\u73b0\u3001\u7406\u89e3\u548c\u7f13\u89e3\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bc4\u4f30GPT-4o\u548cDeepSeek-R1\u5728100\u4e2a\u771f\u5b9e\u53ef\u590d\u73b0CVE\u4e0a\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u4e09\u4e2a\u6f0f\u6d1e\u62ab\u9732\u9636\u6bb5\uff1a\u4ec5\u63cf\u8ff0\u7684\u65b0\u62ab\u9732\u6f0f\u6d1e\u3001\u6709\u8865\u4e01\u76841-day\u6f0f\u6d1e\u3001\u6709\u5b8c\u6574\u4e0a\u4e0b\u6587\u4ee3\u7801\u7684N-day\u6f0f\u6d1e\u3002", "result": "LLMs\u4ec5\u4f7f\u7528\u516c\u5f00\u6570\u636e\u5c31\u80fd\u57288%-34%\u7684\u60c5\u51b5\u4e0b\u81ea\u52a8\u751f\u6210\u53ef\u5de5\u4f5c\u7684PoC\uff0cDeepSeek-R1\u8868\u73b0\u4f18\u4e8eGPT-4o\u3002\u8865\u5145\u4ee3\u7801\u4e0a\u4e0b\u6587\u53ef\u63d0\u9ad817%-20%\u6210\u529f\u7387\uff0c\u51fd\u6570\u7ea7\u6bd4\u6587\u4ef6\u7ea7\u63d0\u53479%-13%\u3002\u96c6\u6210\u81ea\u9002\u5e94\u63a8\u7406\u7b56\u7565\u540e\u6210\u529f\u7387\u53ef\u8fbe68%-72%\u3002", "conclusion": "LLMs\u80fd\u591f\u91cd\u5851\u6f0f\u6d1e\u5229\u7528\u7684\u52a8\u6001\u683c\u5c40\uff0c\u5df2\u670923\u4e2a\u65b0\u751f\u6210\u7684PoC\u88abNVD\u548cExploit DB\u63a5\u53d7\u3002", "topic": "swe application"}}
{"id": "2510.09720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09720", "abs": "https://arxiv.org/abs/2510.09720", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "title": "Preference-Aware Memory Update for Long-Term LLM Agents", "comment": null, "summary": "One of the key factors influencing the reasoning capabilities of LLM-based\nagents is their ability to leverage long-term memory. Integrating long-term\nmemory mechanisms allows agents to make informed decisions grounded in\nhistorical interactions. While recent advances have significantly improved the\nstorage and retrieval components, by encoding memory into dense vectors for\nsimilarity search or organizing memory as structured knowledge graphs most\nexisting approaches fall short in memory updating. In particular, they lack\nmechanisms for dynamically refining preference memory representations in\nresponse to evolving user behaviors and contexts. To address this gap, we\npropose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic\nand personalized memory refinement. By integrating sliding window averages (SW)\nwith exponential moving averages (EMA), PAMU constructs a fused\npreference-aware representation that captures both short-term fluctuations and\nlong-term user tendencies. We conduct experiments on five task scenarios of the\nLoCoMo dataset, and the results show that our mechanism can significantly\nimprove the output quality of LLM in five baselines, validating its\neffectiveness in long-term conversations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u504f\u597d\u611f\u77e5\u8bb0\u5fc6\u66f4\u65b0\u673a\u5236(PAMU)\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5e73\u5747\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7684\u878d\u5408\u6765\u52a8\u6001\u4f18\u5316LLM\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u8868\u793a\uff0c\u5728\u957f\u671f\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u8f93\u51fa\u8d28\u91cf", "motivation": "\u73b0\u6709\u957f\u671f\u8bb0\u5fc6\u65b9\u6cd5\u5728\u8bb0\u5fc6\u66f4\u65b0\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u6839\u636e\u7528\u6237\u884c\u4e3a\u548c\u4e0a\u4e0b\u6587\u53d8\u5316\u52a8\u6001\u4f18\u5316\u504f\u597d\u8bb0\u5fc6\u8868\u793a\u7684\u673a\u5236", "method": "PAMU\u673a\u5236\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u5e73\u5747(SW)\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747(EMA)\uff0c\u6784\u5efa\u878d\u5408\u7684\u504f\u597d\u611f\u77e5\u8868\u793a\uff0c\u6355\u6349\u77ed\u671f\u6ce2\u52a8\u548c\u957f\u671f\u7528\u6237\u8d8b\u52bf", "result": "\u5728LoCoMo\u6570\u636e\u96c6\u7684\u4e94\u4e2a\u4efb\u52a1\u573a\u666f\u4e2d\uff0c\u8be5\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u4e94\u4e2a\u57fa\u7ebf\u6a21\u578b\u7684\u8f93\u51fa\u8d28\u91cf", "conclusion": "PAMU\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u5bf9\u8bdd\u4e2d\u8bb0\u5fc6\u52a8\u6001\u66f4\u65b0\u7684\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027", "topic": "agent analysis"}}
{"id": "2510.09970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09970", "abs": "https://arxiv.org/abs/2510.09970", "authors": ["Olivia Peiyu Wang", "Tashvi Bansal", "Ryan Bai", "Emily M. Chui", "Leilani H. Gilpin"], "title": "Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs", "comment": "Accepted as a poster at the Twelfth Annual Conference on Advances in\n  Cognitive Systems. 21 pages, 7 figures and 1 table", "summary": "Large Language Models (LLMs) suffer from critical reasoning gaps, including a\ntendency to hallucinate and poor accuracy in classifying logical fallacies.\nThis limitation stems from their default System 1 processing, which is fast and\nintuitive, whereas reliable reasoning requires the deliberate, effortful System\n2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is\noften prohibitively expensive, we explore a low-cost, instruction-based\nintervention to bridge this gap. Our methodology introduces a novel stepwise\ninstruction dataset that decomposes fallacy classification into a series of\natomic procedural steps (simple binary questions). We further augment this with\na final verification step where models consult a relational knowledge graph of\nrelated fallacies. This procedural, rule-based intervention yields a\nsignificant improvement in LLM logical fallacy classification. Crucially, the\napproach also provides enhanced transparency into the LLMs' decision-making,\nhighlighting a practical pathway for Neuro-symbolic architectures to address\nLLM reasoning deficits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u57fa\u4e8e\u6307\u4ee4\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6b65\u6307\u4ee4\u6570\u636e\u96c6\u548c\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\u6765\u6539\u5584LLM\u7684\u903b\u8f91\u8c2c\u8bef\u5206\u7c7b\u80fd\u529b\u3002", "motivation": "LLM\u5b58\u5728\u5173\u952e\u63a8\u7406\u7f3a\u9677\uff0c\u5305\u62ec\u4ea7\u751f\u5e7b\u89c9\u548c\u5728\u903b\u8f91\u8c2c\u8bef\u5206\u7c7b\u4e2d\u51c6\u786e\u6027\u5dee\uff0c\u8fd9\u6e90\u4e8e\u5176\u9ed8\u8ba4\u7684\u7cfb\u7edf1\u5904\u7406\u65b9\u5f0f\uff0c\u800c\u53ef\u9760\u63a8\u7406\u9700\u8981\u7cfb\u7edf2\u7684\u6df1\u601d\u719f\u8651\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u65b0\u9896\u7684\u5206\u6b65\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5c06\u8c2c\u8bef\u5206\u7c7b\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u539f\u5b50\u7a0b\u5e8f\u6b65\u9aa4\uff08\u7b80\u5355\u7684\u4e8c\u5143\u95ee\u9898\uff09\uff0c\u5e76\u901a\u8fc7\u67e5\u8be2\u76f8\u5173\u8c2c\u8bef\u7684\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u6700\u7ec8\u9a8c\u8bc1\u3002", "result": "\u8fd9\u79cd\u57fa\u4e8e\u7a0b\u5e8f\u7684\u89c4\u5219\u5e72\u9884\u663e\u8457\u63d0\u9ad8\u4e86LLM\u903b\u8f91\u8c2c\u8bef\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5e76\u4e3aLLM\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u900f\u660e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u89e3\u51b3LLM\u63a8\u7406\u7f3a\u9677\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.10179", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10179", "abs": "https://arxiv.org/abs/2510.10179", "authors": ["Linghan Huang", "Peizhou Zhao", "Huaming Chen"], "title": "LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models", "comment": null, "summary": "The rapid development of large language models (LLMs) has revolutionized\nsoftware testing, particularly fuzz testing, by automating the generation of\ndiverse and effective test inputs. This advancement holds great promise for\nimproving software reliability. Meanwhile, the introduction of MOJO, a\nhigh-performance AI programming language blending Python's usability with the\nefficiency of C and C++, presents new opportunities to enhance AI model\nscalability and programmability. However, as a new language, MOJO lacks\ncomprehensive testing frameworks and a sufficient corpus for LLM-based testing,\nwhich exacerbates model hallucination. In this case, LLMs will generate\nsyntactically valid but semantically incorrect code, significantly reducing the\neffectiveness of fuzz testing. To address this challenge, we propose\nMOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for\nzero-shot learning environments of emerging programming languages. MOJOFuzzer\nintegrates a mutil-phase framework that systematically eliminates low-quality\ngenerated inputs before execution, significantly improving test case validity.\nFurthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime\nfeedback for test case mutation, enabling an iterative learning process that\ncontinuously enhances fuzzing efficiency and bug detection performance. Our\nexperimental results demonstrate that MOJOFuzzer significantly enhances test\nvalidity, API coverage, and bug detection performance, outperforming\ntraditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.\nUsing MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation\nof MOJO, uncorvering 13 previous unknown bugs. This study not only advances the\nfield of LLM-driven software testing but also establishes a foundational\nmethodology for leveraging LLMs in the testing of emerging programming\nlanguages.", "AI": {"tldr": "MOJOFuzzer\u662f\u4e00\u4e2a\u9488\u5bf9\u65b0\u5174\u7f16\u7a0b\u8bed\u8a00MOJO\u7684\u81ea\u9002\u5e94LLM\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8fc7\u6ee4\u548c\u52a8\u6001\u63d0\u793a\u8c03\u6574\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7528\u4f8b\u7684\u6709\u6548\u6027\u548c\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "MOJO\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u9ad8\u6027\u80fdAI\u7f16\u7a0b\u8bed\u8a00\uff0c\u7f3a\u4e4f\u5b8c\u5584\u7684\u6d4b\u8bd5\u6846\u67b6\u548c\u8bed\u6599\u5e93\uff0c\u5bfc\u81f4LLM\u5728\u6d4b\u8bd5\u65f6\u4ea7\u751f\u8bed\u4e49\u9519\u8bef\u7684\u4ee3\u7801\uff0c\u964d\u4f4e\u4e86\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faMOJOFuzzer\u6846\u67b6\uff0c\u96c6\u6210\u591a\u9636\u6bb5\u8fc7\u6ee4\u673a\u5236\u6d88\u9664\u4f4e\u8d28\u91cf\u8f93\u5165\uff0c\u5e76\u57fa\u4e8e\u8fd0\u884c\u65f6\u53cd\u9988\u52a8\u6001\u8c03\u6574LLM\u63d0\u793a\u8fdb\u884c\u6d4b\u8bd5\u7528\u4f8b\u53d8\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMOJOFuzzer\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u6709\u6548\u6027\u3001API\u8986\u76d6\u7387\u548c\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\uff0c\u53d1\u73b0\u4e8613\u4e2a\u5148\u524d\u672a\u77e5\u7684bug\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63a8\u52a8\u4e86LLM\u9a71\u52a8\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u9886\u57df\u53d1\u5c55\uff0c\u8fd8\u4e3a\u65b0\u5174\u7f16\u7a0b\u8bed\u8a00\u7684LLM\u6d4b\u8bd5\u5efa\u7acb\u4e86\u57fa\u7840\u65b9\u6cd5\u8bba\u3002", "topic": "swe application"}}
{"id": "2510.10002", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10002", "abs": "https://arxiv.org/abs/2510.10002", "authors": ["Pratik S. Sachdeva", "Tom van Nuenen"], "title": "Deliberative Dynamics and Value Alignment in LLM Debates", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in sensitive\neveryday contexts - offering personal advice, mental health support, and moral\nguidance - understanding their elicited values in navigating complex moral\nreasoning is essential. Most evaluations study this sociotechnical alignment\nthrough single-turn prompts, but it is unclear if these findings extend to\nmulti-turn settings where values emerge through dialogue, revision, and\nconsensus. We address this gap using LLM debate to examine deliberative\ndynamics and value alignment in multi-turn settings by prompting subsets of\nthree models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively\nassign blame in 1,000 everyday dilemmas from Reddit's \"Am I the Asshole\"\ncommunity. We use both synchronous (parallel responses) and round-robin\n(sequential responses) formats to test order effects and verdict revision. Our\nfindings show striking behavioral differences. In the synchronous setting, GPT\nshowed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were\nfar more flexible (28-41%). Value patterns also diverged: GPT emphasized\npersonal autonomy and direct communication, while Claude and Gemini prioritized\nempathetic dialogue. Certain values proved especially effective at driving\nverdict changes. We further find that deliberation format had a strong impact\non model behavior: GPT and Gemini stood out as highly conforming relative to\nClaude, with their verdict behavior strongly shaped by order effects. These\nresults show how deliberation format and model-specific behaviors shape moral\nreasoning in multi-turn interactions, underscoring that sociotechnical\nalignment depends on how systems structure dialogue as much as on their\noutputs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7LLM\u8fa9\u8bba\u5206\u6790\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u9053\u5fb7\u63a8\u7406\u548c\u4ef7\u503c\u5bf9\u9f50\uff0c\u4f7f\u7528GPT-4.1\u3001Claude 3.7 Sonnet\u548cGemini 2.0 Flash\u6a21\u578b\u5728Reddit\u7684\"Am I the Asshole\"\u793e\u533a1000\u4e2a\u65e5\u5e38\u56f0\u5883\u4e2d\u8fdb\u884c\u96c6\u4f53\u5f52\u8d23\u5224\u65ad\u3002", "motivation": "\u968f\u7740LLM\u5728\u654f\u611f\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u90e8\u7f72\uff0c\u7406\u89e3\u5176\u5728\u590d\u6742\u9053\u5fb7\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u7684\u4ef7\u503c\u89c2\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u8f6e\u63d0\u793a\uff0c\u4f46\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4ef7\u503c\u89c2\u901a\u8fc7\u5bf9\u8bdd\u3001\u4fee\u8ba2\u548c\u5171\u8bc6\u5f62\u6210\u7684\u8fc7\u7a0b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528\u540c\u6b65\uff08\u5e76\u884c\u54cd\u5e94\uff09\u548c\u8f6e\u8be2\uff08\u987a\u5e8f\u54cd\u5e94\uff09\u4e24\u79cd\u8fa9\u8bba\u683c\u5f0f\uff0c\u6d4b\u8bd5\u987a\u5e8f\u6548\u5e94\u548c\u5224\u51b3\u4fee\u8ba2\u3002\u8ba9\u4e09\u4e2a\u6a21\u578b\u5b50\u96c6\u57281000\u4e2a\u65e5\u5e38\u56f0\u5883\u4e2d\u96c6\u4f53\u5206\u914d\u8d23\u4efb\u3002", "result": "\u53d1\u73b0\u663e\u8457\u884c\u4e3a\u5dee\u5f02\uff1a\u540c\u6b65\u8bbe\u7f6e\u4e2dGPT\u8868\u73b0\u51fa\u5f3a\u60ef\u6027\uff080.6-3.1%\u4fee\u8ba2\u7387\uff09\uff0c\u800cClaude\u548cGemini\u66f4\u7075\u6d3b\uff0828-41%\uff09\u3002\u4ef7\u503c\u89c2\u6a21\u5f0f\u4e5f\u4e0d\u540c\uff1aGPT\u5f3a\u8c03\u4e2a\u4eba\u81ea\u4e3b\u548c\u76f4\u63a5\u6c9f\u901a\uff0cClaude\u548cGemini\u4f18\u5148\u8003\u8651\u540c\u7406\u5fc3\u5bf9\u8bdd\u3002\u67d0\u4e9b\u4ef7\u503c\u89c2\u7279\u522b\u6709\u6548\u9a71\u52a8\u5224\u51b3\u53d8\u5316\u3002", "conclusion": "\u8fa9\u8bba\u683c\u5f0f\u5bf9\u6a21\u578b\u884c\u4e3a\u6709\u5f3a\u70c8\u5f71\u54cd\uff0cGPT\u548cGemini\u76f8\u5bf9\u4e8eClaude\u8868\u73b0\u51fa\u9ad8\u5ea6\u4ece\u4f17\u6027\uff0c\u5176\u5224\u51b3\u884c\u4e3a\u53d7\u987a\u5e8f\u6548\u5e94\u5f3a\u70c8\u5f71\u54cd\u3002\u8868\u660e\u793e\u4f1a\u6280\u672f\u5bf9\u9f50\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u7cfb\u7edf\u8f93\u51fa\uff0c\u8fd8\u53d6\u51b3\u4e8e\u5bf9\u8bdd\u7ed3\u6784\u65b9\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2510.10290", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10290", "abs": "https://arxiv.org/abs/2510.10290", "authors": ["Sayan Mandal", "Hua Jiang"], "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines", "comment": "Submitted to MLSys 2026", "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u5206\u6790\u4e0eAST\u5f15\u5bfc\u7684\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u91cf\u5316\u5f00\u6e90\u6a21\u578b\u548c\u591a\u5c42\u7f13\u5b58\u5b9e\u73b0\u5feb\u901f\u53cd\u9988\uff0c\u5728\u5b89\u5168\u5bfc\u5411\u7684C/C++\u6807\u51c6\u4e2d\u5b9e\u73b0\u5206\u949f\u7ea7\u53cd\u9988\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u8fdd\u89c4\u51cf\u5c11\u7387\u3002", "motivation": "\u5728\u5408\u89c4\u6027\u8981\u6c42\u9ad8\u7684\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u91c7\u7528\u7387\u4f4e\uff0c\u9759\u6001\u5206\u6790\u5668\u4ea7\u751f\u5927\u91cf\u4f4e\u89e3\u91ca\u6027\u8f93\u51fa\uff0c\u800c\u7b80\u5355\u4f7f\u7528LLM\u5b58\u5728\u5e7b\u89c9\u98ce\u9669\u548c\u6210\u672c\u8fc7\u9ad8\u95ee\u9898\u3002", "method": "\u5c06\u9759\u6001\u5206\u6790\u7ed3\u679c\u4e0eAST\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u63d0\u53d6\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u5355GPU\u6309\u9700\u670d\u52a1\u5806\u6808\uff08\u91cf\u5316\u5f00\u6e90\u6a21\u578b\u3001\u591a\u5c42\u7f13\u5b58\uff09\u6765\u63d0\u4f9b\u7b80\u6d01\u89e3\u91ca\u548c\u4fee\u590d\u6307\u5bfc\u3002", "result": "\u5728\u5b89\u5168\u5bfc\u5411C/C++\u6807\u51c6\u8bc4\u4f30\u4e2d\uff0c\u5b9e\u73b0\u4e2d\u4f4d\u657059.8\u79d2\u7684\u9996\u53cd\u9988\u65f6\u95f4\uff0c\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u8fdd\u89c4\u51cf\u5c11\u7387\u548c\u8f83\u4f4e\u8fdd\u89c4\u7387\uff0c\u76f8\u6bd4\u5927\u578b\u4e13\u6709\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8be5\u67b6\u6784\u89e3\u8026\u8bbe\u8ba1\u5141\u8bb8\u56e2\u961f\u72ec\u7acb\u91c7\u7528\u57fa\u7840\u5c42\u6216\u670d\u52a1\u5c42\uff0c\u5185\u90e8\u8c03\u67e5\u663e\u793a\u51cf\u5c11\u4e86\u5206\u7c7b\u5de5\u4f5c\u91cf\u548c\u4eba\u5de5\u5ba1\u67e5\u8fed\u4ee3\u6b21\u6570\uff0c\u5f3a\u8c03\u53ef\u91cd\u73b0\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u5411\u66f4\u5e7f\u6cdb\u6807\u51c6\u6269\u5c55\u7684\u8def\u5f84\u3002", "topic": "swe application"}}
{"id": "2510.09738", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09738", "abs": "https://arxiv.org/abs/2510.09738", "authors": ["Steve Han", "Gilberto Titericz Junior", "Tom Balough", "Wenfei Zhou"], "title": "Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement", "comment": "10 pages, 1 figure, 4 tables, under review as a conference paper at\n  ICLR 2026", "summary": "This research introduces the Judge's Verdict Benchmark, a novel two-step\nmethodology to evaluate Large Language Models (LLMs) as judges for response\naccuracy evaluation tasks. We assess how well 54 LLMs can replicate human\njudgment when scoring responses from RAG (Retrieval-Augmented Generation) or\nAgentic pipelines against ground truth answers. Our methodology progresses from\ntraditional correlation analysis to comprehensive Cohen's Kappa analysis that\nmeasures actual agreement patterns. The two-step approach includes: (1) a\ncorrelation test that filters judges with strong alignment, followed by (2) a\nhuman-likeness test using z-scores to identify two distinct judgment patterns:\nhuman-like judgment (|z| < 1) that mimics natural human variation, and\nsuper-consistent judgment (z > 1) that exceeds typical human-to-human agreement\nlevels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1\nperformance: 23 models exhibit human-like patterns that preserve the nuances of\nhuman judgment, while 4 models demonstrate super-consistent behavior, a pattern\nthat could indicate either enhanced reliability or oversimplification of\ncomplex judgments. Testing 43 open-source models (1B-405B parameters) and 11\nclosed models (GPT, Gemini, Claude variants), we demonstrate that judge\nexcellence is not solely dependent on model size but on specific training\nstrategies. Our key contributions include: (1) establishing that correlation\nalone is insufficient for judge evaluation, (2) introducing a \"Turing Test for\njudges\" based on agreement patterns, and (3) providing a standardized benchmark\nfor classifying LLM judges into distinct performance tiers for different\nevaluation needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86Judge's Verdict Benchmark\uff0c\u4e00\u79cd\u4e24\u6b65\u6cd5\u8bc4\u4f30LLM\u4f5c\u4e3a\u54cd\u5e94\u51c6\u786e\u6027\u8bc4\u4f30\u7684\u88c1\u5224\u80fd\u529b\uff0c\u53d1\u73b027/54\u4e2aLLM\u8fbe\u5230Tier 1\u6027\u80fd\uff0c\u5176\u4e2d23\u4e2a\u5448\u73b0\u7c7b\u4eba\u5224\u65ad\u6a21\u5f0f\uff0c4\u4e2a\u5448\u73b0\u8d85\u4e00\u81f4\u6a21\u5f0f\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u590d\u5236\u4eba\u7c7b\u5224\u65ad\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728RAG\u6216Agentic\u7ba1\u9053\u7684\u54cd\u5e94\u51c6\u786e\u6027\u8bc4\u4f30\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u76f8\u5173\u6027\u5206\u6790\u4e0d\u8db3\u4ee5\u5168\u9762\u8bc4\u4f30\u88c1\u5224\u8d28\u91cf\u3002", "method": "\u4e24\u6b65\u6cd5\uff1a\u5148\u901a\u8fc7\u76f8\u5173\u6027\u6d4b\u8bd5\u7b5b\u9009\u5bf9\u9f50\u826f\u597d\u7684\u88c1\u5224\uff0c\u518d\u4f7f\u7528z-score\u8fdb\u884c\u7c7b\u4eba\u6027\u6d4b\u8bd5\uff0c\u8bc6\u522b\u7c7b\u4eba\u5224\u65ad(|z| < 1)\u548c\u8d85\u4e00\u81f4\u5224\u65ad(z > 1)\u4e24\u79cd\u6a21\u5f0f\u3002", "result": "\u6d4b\u8bd554\u4e2aLLM(43\u4e2a\u5f00\u6e90\uff0c11\u4e2a\u95ed\u6e90)\uff0c27\u4e2a\u8fbe\u5230Tier 1\u6027\u80fd\uff0c\u5176\u4e2d23\u4e2a\u7c7b\u4eba\u6a21\u5f0f\uff0c4\u4e2a\u8d85\u4e00\u81f4\u6a21\u5f0f\u3002\u88c1\u5224\u4f18\u79c0\u7a0b\u5ea6\u4e0d\u5355\u7eaf\u4f9d\u8d56\u6a21\u578b\u5927\u5c0f\uff0c\u800c\u662f\u7279\u5b9a\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "\u76f8\u5173\u6027\u5206\u6790\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u88c1\u5224\uff0c\u9700\u8981\u57fa\u4e8e\u4e00\u81f4\u6027\u6a21\u5f0f\u7684\"\u56fe\u7075\u6d4b\u8bd5\"\uff0c\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u6765\u5206\u7c7bLLM\u88c1\u5224\u5230\u4e0d\u540c\u6027\u80fd\u5c42\u7ea7\u3002", "topic": "agent analysis"}}
{"id": "2510.10035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10035", "abs": "https://arxiv.org/abs/2510.10035", "authors": ["Jusheng Zhang", "Kaitong Cai", "Qinglin Zeng", "Ningyuan Liu", "Stephen Fan", "Ziliang Chen", "Keze Wang"], "title": "Failure-Driven Workflow Refinement", "comment": null, "summary": "Optimizing LLM-based workflows is typically formulated as a global search,\nwhere candidate workflows are evaluated based on a scalar metric. This\nparadigm, however, suffers from a critical flaw: information collapse. By\nreducing rich, multi-step execution traces to simple success/failure signals,\nexisting methods are rendered blind to the underlying structure of failures,\nfundamentally preventing them from modeling the workflow's failure\ndistribution. We reconceptualize this challenge as a distributional problem. We\npropose a new paradigm where the optimization goal is not to maximize a scalar\nscore, but to directly minimize a workflow's Expected Failure Mass, i.e., the\nintegral of its failure probability density function defined over a\nhigh-dimensional Failure Signature Space (FSS). This distributional lens allows\nus to move from inefficient, zero-order optimization to a principled,\ngradient-like descent on the failure landscape itself. We introduce CE-Graph, a\nframework that operationalizes this paradigm through a novel, failure-driven\nrefinement process. CE-Graph approximates the failure distribution from a pool\nof counterexamples, identifies its densest regions as recurring failure modes,\nand applies targeted, operator-constrained graph edits via a Propose-and-Verify\nmechanism to greedily reduce the failure mass. On math, code, and QA\nbenchmarks, our CE-Graph achieves higher robustness at a significantly lower\ncost than strong baselines. This suggests that a system's reliability emerges\nnot from avoiding failures, but from systematically learning and reshaping the\ngeometric structure of its failure distributions.", "AI": {"tldr": "\u63d0\u51faCE-Graph\u6846\u67b6\uff0c\u5c06LLM\u5de5\u4f5c\u6d41\u4f18\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u5e03\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u671f\u671b\u5931\u8d25\u8d28\u91cf\u6765\u7cfb\u7edf\u6027\u5730\u5b66\u4e60\u5e76\u91cd\u5851\u5931\u8d25\u5206\u5e03\u7684\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709LLM\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u5d29\u6e83\u95ee\u9898\uff0c\u5c06\u4e30\u5bcc\u7684\u591a\u6b65\u9aa4\u6267\u884c\u8f68\u8ff9\u7b80\u5316\u4e3a\u7b80\u5355\u7684\u6210\u529f/\u5931\u8d25\u4fe1\u53f7\uff0c\u65e0\u6cd5\u5efa\u6a21\u5de5\u4f5c\u6d41\u7684\u5931\u8d25\u5206\u5e03\u3002", "method": "\u5f15\u5165\u5931\u8d25\u7b7e\u540d\u7a7a\u95f4(FSS)\u6982\u5ff5\uff0c\u901a\u8fc7\u53cd\u4f8b\u6c60\u8fd1\u4f3c\u5931\u8d25\u5206\u5e03\uff0c\u8bc6\u522b\u6700\u5bc6\u96c6\u533a\u57df\u4f5c\u4e3a\u91cd\u590d\u5931\u8d25\u6a21\u5f0f\uff0c\u4f7f\u7528\u63d0\u51fa-\u9a8c\u8bc1\u673a\u5236\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u56fe\u7f16\u8f91\u6765\u8d2a\u5a6a\u51cf\u5c11\u5931\u8d25\u8d28\u91cf\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCE-Graph\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u4e86\u6bd4\u5f3a\u57fa\u7ebf\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4e0d\u662f\u6765\u81ea\u907f\u514d\u5931\u8d25\uff0c\u800c\u662f\u6765\u81ea\u7cfb\u7edf\u6027\u5730\u5b66\u4e60\u548c\u91cd\u5851\u5176\u5931\u8d25\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "topic": "agent analysis"}}
{"id": "2510.10321", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10321", "abs": "https://arxiv.org/abs/2510.10321", "authors": ["Jugal Gajjar", "Kaustik Ranaware", "Kamalasankari Subramaniakuppusamy"], "title": "Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models", "comment": "13 pages, 3 figures, 5 tables, 14 equations, accepted at the 14th\n  International Conference on Complex Networks and Their Applications (COMPLEX\n  NETWORKS 2025) and the conference proceedings will be published by Springer\n  in the Studies in Computational Intelligence series", "summary": "Software vulnerabilities remain a persistent risk, yet static and dynamic\nanalyses often overlook structural dependencies that shape insecure behaviors.\nViewing programs as heterogeneous graphs, we capture control- and data-flow\nrelations as complex interaction networks. Our hybrid framework combines these\ngraph representations with light-weight (<4B) local LLMs, uniting topological\nfeatures with semantic reasoning while avoiding the cost and privacy concerns\nof large cloud models. Evaluated on Java vulnerability detection (binary\nclassification), our method achieves 93.57% accuracy-an 8.36% gain over Graph\nAttention Network-based embeddings and 17.81% over pretrained LLM baselines\nsuch as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient\nsubgraphs and generates natural language explanations, improving\ninterpretability for developers. These results pave the way for scalable,\nexplainable, and locally deployable tools that can shift vulnerability analysis\nfrom purely syntactic checks to deeper structural and semantic insights,\nfacilitating broader adoption in real-world secure software development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f02\u6784\u56fe\u8868\u793a\u548c\u8f7b\u91cf\u7ea7\u672c\u5730LLM\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\uff0c\u5728Java\u6f0f\u6d1e\u68c0\u6d4b\u4e0a\u8fbe\u523093.57%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53478.36-17.81%\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u548c\u52a8\u6001\u5206\u6790\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u5f71\u54cd\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u62d3\u6251\u7279\u5f81\u548c\u8bed\u4e49\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u907f\u514d\u5927\u578b\u4e91\u6a21\u578b\u7684\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\u3002", "method": "\u5c06\u7a0b\u5e8f\u5efa\u6a21\u4e3a\u5f02\u6784\u56fe\uff0c\u6355\u6349\u63a7\u5236\u548c\u6570\u636e\u6d41\u5173\u7cfb\u4f5c\u4e3a\u590d\u6742\u4ea4\u4e92\u7f51\u7edc\uff0c\u7ed3\u5408\u56fe\u8868\u793a\u4e0e\u8f7b\u91cf\u7ea7(<4B)\u672c\u5730LLM\uff0c\u7edf\u4e00\u62d3\u6251\u7279\u5f81\u4e0e\u8bed\u4e49\u63a8\u7406\u3002", "result": "\u5728Java\u6f0f\u6d1e\u68c0\u6d4b(\u4e8c\u5143\u5206\u7c7b)\u4e0a\u8fbe\u523093.57%\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u5d4c\u5165\u65b9\u6cd5\u63d0\u53478.36%\uff0c\u6bd4\u9884\u8bad\u7ec3LLM\u57fa\u7ebf(Qwen2.5 Coder 3B)\u63d0\u534717.81%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u53d6\u663e\u8457\u5b50\u56fe\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u4eba\u5458\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u672c\u5730\u90e8\u7f72\u7684\u5de5\u5177\u94fa\u5e73\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.09770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09770", "abs": "https://arxiv.org/abs/2510.09770", "authors": ["Adam Byerly", "Daniel Khashabi"], "title": "Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning", "comment": "20 pages, 6 figures", "summary": "Large language models exhibit a strong position bias in multi-document\ncontexts, systematically prioritizing information based on location rather than\nrelevance. While existing approaches treat this bias as noise to be mitigated,\nwe introduce Gold Panning Bandits, a framework that leverages position bias as\na diagnostic signal: by reordering documents and observing shifts in the\nmodel's responses, we can efficiently identify the most relevant content. We\nframe the problem of choosing reorderings as a bipartite matching problem.\nWhile an optimal assignment can be computed at each iteration with the\nHungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \\log N)$\nstrategy that achieves comparable performance by prioritizing the placement of\nthe most uncertain documents in the most informative positions. Our approach\nidentifies relevant documents using up to 65\\% fewer language model queries\nthan random permutation baselines on knowledge-intensive NLP tasks,\nsubstantially reducing computational cost without model retraining. This work\ndemonstrates that inherent LLM biases can be transformed from liabilities into\nassets for efficient, inference-time optimization.", "AI": {"tldr": "\u63d0\u51faGold Panning Bandits\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u4f4d\u7f6e\u504f\u89c1\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u6587\u6863\u6765\u9ad8\u6548\u8bc6\u522b\u76f8\u5173\u5185\u5bb9\uff0c\u76f8\u6bd4\u968f\u673a\u6392\u5217\u57fa\u7ebf\u51cf\u5c1165%\u7684\u67e5\u8be2\u6b21\u6570", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6587\u6863\u73af\u5883\u4e2d\u5b58\u5728\u5f3a\u70c8\u7684\u4f4d\u7f6e\u504f\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u5176\u89c6\u4e3a\u9700\u8981\u7f13\u89e3\u7684\u566a\u58f0\uff0c\u4f46\u672c\u7814\u7a76\u65e8\u5728\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u7528\u4e8e\u9ad8\u6548\u5185\u5bb9\u8bc6\u522b\u7684\u8bca\u65ad\u4fe1\u53f7", "method": "\u5c06\u91cd\u6392\u5e8f\u95ee\u9898\u5efa\u6a21\u4e3a\u4e8c\u5206\u56fe\u5339\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u8d2a\u5fc3O(N log N)\u7b56\u7565\uff0c\u5c06\u6700\u4e0d\u786e\u5b9a\u7684\u6587\u6863\u653e\u5728\u6700\u5177\u4fe1\u606f\u91cf\u7684\u4f4d\u7f6e\uff0c\u76f8\u6bd4\u5308\u7259\u5229\u7b97\u6cd5\u7684O(N^3)\u590d\u6742\u5ea6\u66f4\u9ad8\u6548", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578bNLP\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u968f\u673a\u6392\u5217\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u9ad8\u8fbe65%\u66f4\u5c11\u7684\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\u6765\u8bc6\u522b\u76f8\u5173\u6587\u6863\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u4e14\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u56fa\u6709\u7684LLM\u504f\u89c1\u53ef\u4ee5\u4ece\u8d1f\u62c5\u8f6c\u5316\u4e3a\u8d44\u4ea7\uff0c\u7528\u4e8e\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u65f6\u4f18\u5316", "topic": "agent analysis"}}
{"id": "2510.10460", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10460", "abs": "https://arxiv.org/abs/2510.10460", "authors": ["Zongyi Lyu", "Songqiang Chen", "Zhenlan Ji", "Liwen Wang", "Shuai Wang", "Daoyuan Wu", "Wenxuan Wang", "Shing-Chi Cheung"], "title": "Testing and Enhancing Multi-Agent Systems for Robust Code Generation", "comment": "19pages, 5 figures", "summary": "Multi-agent systems (MASs) have emerged as a promising paradigm for automated\ncode generation, demonstrating impressive performance on established benchmarks\nby decomposing complex coding tasks across specialized agents with different\nroles. Despite their prosperous development and adoption, their robustness\nremains pressingly under-explored, raising critical concerns for real-world\ndeployment. This paper presents the first comprehensive study examining the\nrobustness of MASs for code generation through a fuzzing-based testing\napproach. By designing a fuzzing pipeline incorporating semantic-preserving\nmutation operators and a novel fitness function, we assess mainstream MASs\nacross multiple datasets and LLMs. Our findings reveal substantial robustness\nflaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they\ninitially resolved successfully after applying the semantic-preserving\nmutations. Through comprehensive failure analysis, we identify a common yet\nlargely overlooked cause of the robustness issue: miscommunications between\nplanning and coding agents, where plans lack sufficient detail and coding\nagents misinterpret intricate logic, aligning with the challenges inherent in a\nmulti-stage information transformation process. Accordingly, we also propose a\nrepairing method that encompasses multi-prompt generation and introduces a new\nmonitor agent to address this issue. Evaluation shows that our repairing method\neffectively enhances the robustness of MASs by solving 40.0%-88.9% of\nidentified failures. Our work uncovers critical robustness flaws in MASs and\nprovides effective mitigation strategies, contributing essential insights for\ndeveloping more reliable MASs for code generation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u53d1\u73b0\u4e3b\u6d41MAS\u5b58\u5728\u4e25\u91cd\u9c81\u68d2\u6027\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4fee\u590d\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e8640.0%-88.9%\u7684\u5931\u8d25\u6848\u4f8b\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u7814\u7a76\u4e25\u91cd\u4e0d\u8db3\uff0c\u8fd9\u5bf9\u5b9e\u9645\u90e8\u7f72\u6784\u6210\u5173\u952e\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u6a21\u7cca\u6d4b\u8bd5\u7684\u7ba1\u9053\uff0c\u5305\u542b\u8bed\u4e49\u4fdd\u6301\u7684\u53d8\u5f02\u64cd\u4f5c\u7b26\u548c\u65b0\u9896\u7684\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u8bc4\u4f30\u4e3b\u6d41MAS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548cLLM\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u5404\u79cd\u6d41\u884cMAS\u5b58\u5728\u4e25\u91cd\u9c81\u68d2\u6027\u7f3a\u9677\uff1a\u5728\u5e94\u7528\u8bed\u4e49\u4fdd\u6301\u53d8\u5f02\u540e\uff0c\u5b83\u4eec\u65e0\u6cd5\u89e3\u51b3\u6700\u521d\u6210\u529f\u89e3\u51b3\u76847.9%-83.3%\u7684\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86MAS\u4e2d\u7684\u5173\u952e\u9c81\u68d2\u6027\u7f3a\u9677\uff0c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u4ee3\u7801\u751f\u6210MAS\u8d21\u732e\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10047", "abs": "https://arxiv.org/abs/2510.10047", "authors": ["Ruohao Li", "Hongjun Liu", "Leyi Zhao", "Zisu Li", "Jiawei Li", "Jiajun Jiang", "Linning Xu", "Chen Zhao", "Mingming Fan", "Chen Liang"], "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning", "comment": "14 pages, 7 figures", "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence.", "AI": {"tldr": "SwarmSys\u662f\u4e00\u4e2a\u53d7\u7fa4\u4f53\u667a\u80fd\u542f\u53d1\u7684\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u7d22\u8005\u3001\u5de5\u4f5c\u8005\u548c\u9a8c\u8bc1\u8005\u4e09\u4e2a\u89d2\u8272\u7684\u8fed\u4ee3\u4ea4\u4e92\u5b9e\u73b0\u95ed\u73af\u63a8\u7406\uff0c\u65e0\u9700\u5168\u5c40\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u81ea\u7ec4\u7ec7\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4f9d\u8d56\u56fa\u5b9a\u89d2\u8272\u6216\u96c6\u4e2d\u63a7\u5236\uff0c\u9650\u5236\u4e86\u957f\u65f6\u7a0b\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u5206\u5e03\u5f0f\u7684\u534f\u8c03\u673a\u5236\u3002", "method": "\u96c6\u6210\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u548c\u4e8b\u4ef6\u6863\u6848\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u6982\u7387\u5339\u914d\u4ee5\u53ca\u4fe1\u606f\u7d20\u542f\u53d1\u7684\u5f3a\u5316\u673a\u5236\uff0c\u652f\u6301\u4e09\u4e2a\u4e13\u4e1a\u89d2\u8272\u7684\u63a2\u7d22\u3001\u5229\u7528\u548c\u9a8c\u8bc1\u5faa\u73af\u4ea4\u4e92\u3002", "result": "\u5728\u7b26\u53f7\u63a8\u7406\u3001\u7814\u7a76\u7efc\u5408\u548c\u79d1\u5b66\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0cSwarmSys\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7fa4\u4f53\u542f\u53d1\u7684\u534f\u8c03\u673a\u5236\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u534f\u8c03\u6269\u5c55\u53ef\u80fd\u4e0e\u6a21\u578b\u6269\u5c55\u540c\u6837\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2510.10551", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10551", "abs": "https://arxiv.org/abs/2510.10551", "authors": ["Baris Ardic", "Quentin Le Dilavrec", "Andy Zaidman"], "title": "How Students Use Generative AI for Software Testing: An Observational Study", "comment": "39 pages, 6 figures, journal submission", "summary": "The integration of generative AI tools like ChatGPT into software engineering\nworkflows opens up new opportunities to boost productivity in tasks such as\nunit test engineering. However, these AI-assisted workflows can also\nsignificantly alter the developer's role, raising concerns about control,\noutput quality, and learning, particularly for novice developers. This study\ninvestigates how novice software developers with foundational knowledge in\nsoftware testing interact with generative AI for engineering unit tests. Our\ngoal is to examine the strategies they use, how heavily they rely on generative\nAI, and the benefits and challenges they perceive when using generative\nAI-assisted approaches for test engineering. We conducted an observational\nstudy involving 12 undergraduate students who worked with generative AI for\nunit testing tasks. We identified four interaction strategies, defined by\nwhether the test idea or the test implementation originated from generative AI\nor the participant. Additionally, we singled out prompting styles that focused\non one-shot or iterative test generation, which often aligned with the broader\ninteraction strategy. Students reported benefits including time-saving, reduced\ncognitive load, and support for test ideation, but also noted drawbacks such as\ndiminished trust, test quality concerns, and lack of ownership. While strategy\nand prompting styles influenced workflow dynamics, they did not significantly\naffect test effectiveness or test code quality as measured by mutation score or\ntest smells.", "AI": {"tldr": "\u7814\u7a76\u65b0\u624b\u5f00\u53d1\u8005\u4f7f\u7528\u751f\u6210\u5f0fAI\u8fdb\u884c\u5355\u5143\u6d4b\u8bd5\u7684\u4ea4\u4e92\u7b56\u7565\u3001\u4f9d\u8d56\u7a0b\u5ea6\u53ca\u611f\u77e5\u7684\u4f18\u7f3a\u70b9\uff0c\u53d1\u73b0\u56db\u79cd\u4ea4\u4e92\u7b56\u7565\u548c\u4e24\u79cd\u63d0\u793a\u98ce\u683c\uff0cAI\u8f85\u52a9\u80fd\u8282\u7701\u65f6\u95f4\u4f46\u5b58\u5728\u4fe1\u4efb\u548c\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0fAI\u5de5\u5177\u5982ChatGPT\u5728\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u7684\u96c6\u6210\u5982\u4f55\u6539\u53d8\u5f00\u53d1\u8005\u89d2\u8272\uff0c\u7279\u522b\u662f\u5bf9\u65b0\u624b\u5f00\u53d1\u8005\u5728\u5355\u5143\u6d4b\u8bd5\u5de5\u7a0b\u4e2d\u7684\u63a7\u5236\u6743\u3001\u8f93\u51fa\u8d28\u91cf\u548c\u5b66\u4e60\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "method": "\u5bf912\u540d\u672c\u79d1\u5b66\u751f\u8fdb\u884c\u89c2\u5bdf\u6027\u7814\u7a76\uff0c\u5206\u6790\u4ed6\u4eec\u4f7f\u7528\u751f\u6210\u5f0fAI\u8fdb\u884c\u5355\u5143\u6d4b\u8bd5\u4efb\u52a1\u7684\u4ea4\u4e92\u7b56\u7565\u3001\u63d0\u793a\u98ce\u683c\uff0c\u5e76\u6d4b\u91cf\u6d4b\u8bd5\u6709\u6548\u6027\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u79cd\u4ea4\u4e92\u7b56\u7565\u548c\u4e24\u79cd\u63d0\u793a\u98ce\u683c\uff0c\u5b66\u751f\u62a5\u544a\u4e86\u8282\u7701\u65f6\u95f4\u3001\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\u7b49\u597d\u5904\uff0c\u4f46\u4e5f\u5b58\u5728\u4fe1\u4efb\u964d\u4f4e\u3001\u6d4b\u8bd5\u8d28\u91cf\u62c5\u5fe7\u548c\u7f3a\u4e4f\u6240\u6709\u6743\u7b49\u95ee\u9898\u3002\u7b56\u7565\u548c\u63d0\u793a\u98ce\u683c\u4e0d\u5f71\u54cd\u6d4b\u8bd5\u6709\u6548\u6027\u6216\u4ee3\u7801\u8d28\u91cf\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u8f85\u52a9\u5355\u5143\u6d4b\u8bd5\u80fd\u63d0\u9ad8\u6548\u7387\u4f46\u5e26\u6765\u65b0\u7684\u6311\u6218\uff0c\u9700\u8981\u5e73\u8861AI\u8f85\u52a9\u4e0e\u5f00\u53d1\u8005\u63a7\u5236\u6743\uff0c\u7b56\u7565\u9009\u62e9\u4e0d\u5f71\u54cd\u6700\u7ec8\u6d4b\u8bd5\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "2510.10819", "categories": ["cs.SE", "cs.AI", "68N01, 68T05, 68T07, 68T50", "D.2.2; D.2.5; D.2.6; D.2.8; I.2.6; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.10819", "abs": "https://arxiv.org/abs/2510.10819", "authors": ["Vivek Acharya"], "title": "Generative AI and the Transformation of Software Development Practices", "comment": "16 pages; 1 figure; preprint; v", "summary": "Generative AI is reshaping how software is designed, written, and maintained.\nAdvances in large language models (LLMs) are enabling new development styles -\nfrom chat-oriented programming and 'vibe coding' to agentic programming - that\ncan accelerate productivity and broaden access. This paper examines how\nAI-assisted techniques are changing software engineering practice, and the\nrelated issues of trust, accountability, and shifting skills. We survey\niterative chat-based development, multi-agent systems, dynamic prompt\norchestration, and integration via the Model Context Protocol (MCP). Using case\nstudies and industry data, we outline both the opportunities (faster cycles,\ndemocratized coding) and the challenges (model reliability and cost) of\napplying generative AI to coding. We describe new roles, skills, and best\npractices for using AI in a responsible and effective way.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5982\u4f55\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\uff0c\u5305\u62ec\u804a\u5929\u5f0f\u7f16\u7a0b\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7b49\u65b0\u5f00\u53d1\u65b9\u5f0f\uff0c\u5206\u6790\u5176\u5e26\u6765\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u751f\u6210\u5f0fAI\u6b63\u5728\u91cd\u5851\u8f6f\u4ef6\u8bbe\u8ba1\u3001\u7f16\u5199\u548c\u7ef4\u62a4\u7684\u65b9\u5f0f\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u4e9b\u65b0\u6280\u672f\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u884c\u4e1a\u6570\u636e\u5206\u6790\uff0c\u8c03\u67e5\u8fed\u4ee3\u5f0f\u804a\u5929\u5f00\u53d1\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u52a8\u6001\u63d0\u793a\u7f16\u6392\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u96c6\u6210\u7b49\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0AI\u8f85\u52a9\u6280\u672f\u53ef\u4ee5\u52a0\u901f\u5f00\u53d1\u5468\u671f\u3001\u666e\u53ca\u7f16\u7a0b\u80fd\u529b\uff0c\u4f46\u4e5f\u9762\u4e34\u6a21\u578b\u53ef\u9760\u6027\u548c\u6210\u672c\u7b49\u6311\u6218\u3002", "conclusion": "\u9700\u8981\u4e3aAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u8d1f\u8d23\u4efb\u548c\u6709\u6548\u4f7f\u7528\u5236\u5b9a\u65b0\u7684\u89d2\u8272\u3001\u6280\u80fd\u548c\u6700\u4f73\u5b9e\u8df5\u3002", "topic": "agent analysis"}}
{"id": "2510.10074", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10074", "abs": "https://arxiv.org/abs/2510.10074", "authors": ["Jiayi Mao", "Liqun Li", "Yanjie Gao", "Zegang Peng", "Shilin He", "Chaoyun Zhang", "Si Qin", "Samia Khalid", "Qingwei Lin", "Saravan Rajmohan", "Sitaram Lanka", "Dongmei Zhang"], "title": "Agentic Troubleshooting Guide Automation for Incident Management", "comment": null, "summary": "Effective incident management in large-scale IT systems relies on\ntroubleshooting guides (TSGs), but their manual execution is slow and\nerror-prone. While recent advances in LLMs offer promise for automating\nincident management tasks, existing LLM-based solutions lack specialized\nsupport for several key challenges, including managing TSG quality issues,\ninterpreting complex control flow, handling data-intensive queries, and\nexploiting execution parallelism. We first conducted an empirical study on 92\nreal-world TSGs, and, guided by our findings, we present StepFly, a novel\nend-to-end agentic framework for troubleshooting guide automation. Our approach\nfeatures a three-stage workflow: the first stage provides a comprehensive guide\ntogether with a tool, TSG Mentor, to assist SREs in improving TSG quality; the\nsecond stage performs offline preprocessing using LLMs to extract structured\nexecution DAGs from unstructured TSGs and to create dedicated Query Preparation\nPlugins (QPPs); and the third stage executes online using a DAG-guided\nscheduler-executor framework with a memory system to guarantee correct workflow\nand support parallel execution of independent steps. Our empirical evaluation\non a collection of real-world TSGs and incidents demonstrates that StepFly\nachieves a ~94% success rate on GPT-4.1, outperforming baselines with less time\nand token consumption. Furthermore, it achieves a remarkable execution time\nreduction of 32.9% to 70.4% for parallelizable TSGs.", "AI": {"tldr": "StepFly\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u6545\u969c\u6392\u9664\u6307\u5357(TSG)\u6267\u884c\u7684\u7aef\u5230\u7aef\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5de5\u4f5c\u6d41\u89e3\u51b3TSG\u8d28\u91cf\u95ee\u9898\u3001\u590d\u6742\u63a7\u5236\u6d41\u89e3\u91ca\u3001\u6570\u636e\u5bc6\u96c6\u578b\u67e5\u8be2\u548c\u6267\u884c\u5e76\u884c\u5316\u7b49\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6545\u969c\u7ba1\u7406\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21IT\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u7ba1\u7406\u4f9d\u8d56\u6545\u969c\u6392\u9664\u6307\u5357(TSG)\uff0c\u4f46\u624b\u52a8\u6267\u884c\u7f13\u6162\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u5bf9TSG\u8d28\u91cf\u95ee\u9898\u3001\u590d\u6742\u63a7\u5236\u6d41\u89e3\u91ca\u3001\u6570\u636e\u5bc6\u96c6\u578b\u67e5\u8be2\u548c\u6267\u884c\u5e76\u884c\u5316\u7b49\u5173\u952e\u6311\u6218\u7684\u4e13\u4e1a\u652f\u6301\u3002", "method": "StepFly\u91c7\u7528\u4e09\u9636\u6bb5\u5de5\u4f5c\u6d41\uff1a1) TSG Mentor\u5de5\u5177\u5e2e\u52a9SRE\u6539\u8fdbTSG\u8d28\u91cf\uff1b2) \u79bb\u7ebf\u9884\u5904\u7406\u4f7f\u7528LLM\u4ece\u975e\u7ed3\u6784\u5316TSG\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6267\u884cDAG\u5e76\u521b\u5efa\u4e13\u7528\u67e5\u8be2\u51c6\u5907\u63d2\u4ef6(QPPs)\uff1b3) \u5728\u7ebf\u6267\u884c\u4f7f\u7528DAG\u5f15\u5bfc\u7684\u8c03\u5ea6\u5668-\u6267\u884c\u5668\u6846\u67b6\u548c\u5185\u5b58\u7cfb\u7edf\uff0c\u4fdd\u8bc1\u6b63\u786e\u5de5\u4f5c\u6d41\u5e76\u652f\u6301\u5e76\u884c\u6267\u884c\u72ec\u7acb\u6b65\u9aa4\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cTSG\u548c\u6545\u969c\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cStepFly\u5728GPT-4.1\u4e0a\u8fbe\u5230\u7ea694%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u65f6\u95f4\u548ctoken\u6d88\u8017\u3002\u5bf9\u4e8e\u53ef\u5e76\u884c\u5316\u7684TSG\uff0c\u5b9e\u73b0\u4e8632.9%\u523070.4%\u7684\u663e\u8457\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u3002", "conclusion": "StepFly\u901a\u8fc7\u5176\u7aef\u5230\u7aef\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86TSG\u81ea\u52a8\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6545\u969c\u7ba1\u7406\u7684\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u53ef\u5e76\u884c\u5316\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "topic": "agent analysis"}}
{"id": "2510.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09854", "abs": "https://arxiv.org/abs/2510.09854", "authors": ["Kaiwen Shi", "Zheyuan Zhang", "Zhengqing Yuan", "Keerthiram Murugesan", "Vincent Galass", "Chuxu Zhang", "Yanfang Ye"], "title": "NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering", "comment": null, "summary": "Diet plays a central role in human health, and Nutrition Question Answering\n(QA) offers a promising path toward personalized dietary guidance and the\nprevention of diet-related chronic diseases. However, existing methods face two\nfundamental challenges: the limited reasoning capacity of single-agent systems\nand the complexity of designing effective multi-agent architectures, as well as\ncontextual overload that hinders accurate decision-making. We introduce\nNutritional-Graph Router (NG-Router), a novel framework that formulates\nnutritional QA as a supervised, knowledge-graph-guided multi-agent\ncollaboration problem. NG-Router integrates agent nodes into heterogeneous\nknowledge graphs and employs a graph neural network to learn task-aware routing\ndistributions over agents, leveraging soft supervision derived from empirical\nagent performance. To further address contextual overload, we propose a\ngradient-based subgraph retrieval mechanism that identifies salient evidence\nduring training, thereby enhancing multi-hop and relational reasoning.\nExtensive experiments across multiple benchmarks and backbone models\ndemonstrate that NG-Router consistently outperforms both single-agent and\nensemble baselines, offering a principled approach to domain-aware multi-agent\nreasoning for complex nutritional health tasks.", "AI": {"tldr": "NG-Router\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u8425\u517b\u95ee\u7b54\u5efa\u6a21\u4e3a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u7684\u8def\u7531\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u68af\u5ea6\u5b50\u56fe\u68c0\u7d22\u89e3\u51b3\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8425\u517b\u95ee\u7b54\u65b9\u6cd5\u9762\u4e34\u5355\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u590d\u6742\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u963b\u788d\u51c6\u786e\u51b3\u7b56\u7684\u6311\u6218\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u8282\u70b9\u96c6\u6210\u5230\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u7684\u8def\u7531\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u68af\u5ea6\u5b50\u56fe\u68c0\u7d22\u673a\u5236\u8bc6\u522b\u5173\u952e\u8bc1\u636e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cNG-Router\u59cb\u7ec8\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u96c6\u6210\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NG-Router\u4e3a\u590d\u6742\u8425\u517b\u5065\u5eb7\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u539f\u5219\u6027\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.09871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09871", "abs": "https://arxiv.org/abs/2510.09871", "authors": ["Nafiseh Nikeghbal", "Amir Hossein Kargaran", "Jana Diesner"], "title": "CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs", "comment": "EMNLP 2025 (Oral)", "summary": "Improvements in model construction, including fortified safety guardrails,\nallow Large language models (LLMs) to increasingly pass standard safety checks.\nHowever, LLMs sometimes slip into revealing harmful behavior, such as\nexpressing racist viewpoints, during conversations. To analyze this\nsystematically, we introduce CoBia, a suite of lightweight adversarial attacks\nthat allow us to refine the scope of conditions under which LLMs depart from\nnormative or ethical behavior in conversations. CoBia creates a constructed\nconversation where the model utters a biased claim about a social group. We\nthen evaluate whether the model can recover from the fabricated bias claim and\nreject biased follow-up questions. We evaluate 11 open-source as well as\nproprietary LLMs for their outputs related to six socio-demographic categories\nthat are relevant to individual safety and fair treatment, i.e., gender, race,\nreligion, nationality, sex orientation, and others. Our evaluation is based on\nestablished LLM-based bias metrics, and we compare the results against human\njudgments to scope out the LLMs' reliability and alignment. The results suggest\nthat purposefully constructed conversations reliably reveal bias amplification\nand that LLMs often fail to reject biased follow-up questions during dialogue.\nThis form of stress-testing highlights deeply embedded biases that can be\nsurfaced through interaction. Code and artifacts are available at\nhttps://github.com/nafisenik/CoBia.", "AI": {"tldr": "CoBia\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5bf9\u6297\u653b\u51fb\u5957\u4ef6\uff0c\u901a\u8fc7\u6784\u9020\u5305\u542b\u504f\u89c1\u58f0\u660e\u7684\u5bf9\u8bdd\u6765\u6d4b\u8bd5LLMs\u80fd\u5426\u4ece\u865a\u6784\u7684\u504f\u89c1\u4e2d\u6062\u590d\u5e76\u62d2\u7edd\u504f\u89c1\u6027\u540e\u7eed\u95ee\u9898\uff0c\u63ed\u793a\u4e86LLMs\u5728\u5bf9\u8bdd\u4e2d\u5b58\u5728\u7684\u504f\u89c1\u653e\u5927\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1LLMs\u901a\u8fc7\u4e86\u6807\u51c6\u5b89\u5168\u68c0\u67e5\uff0c\u4f46\u5728\u5bf9\u8bdd\u4e2d\u4ecd\u4f1a\u8868\u73b0\u51fa\u6709\u5bb3\u884c\u4e3a\uff08\u5982\u79cd\u65cf\u4e3b\u4e49\u89c2\u70b9\uff09\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5206\u6790LLMs\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u4f1a\u504f\u79bb\u89c4\u8303\u6216\u4f26\u7406\u884c\u4e3a\u3002", "method": "\u521b\u5efa\u6784\u9020\u5bf9\u8bdd\uff0c\u8ba9\u6a21\u578b\u5bf9\u67d0\u4e2a\u793e\u4f1a\u7fa4\u4f53\u53d1\u8868\u504f\u89c1\u58f0\u660e\uff0c\u7136\u540e\u8bc4\u4f30\u6a21\u578b\u80fd\u5426\u4ece\u865a\u6784\u7684\u504f\u89c1\u4e2d\u6062\u590d\u5e76\u62d2\u7edd\u504f\u89c1\u6027\u540e\u7eed\u95ee\u9898\u3002\u8bc4\u4f30\u4e8611\u4e2a\u5f00\u6e90\u548c\u4e13\u6709LLMs\u57286\u4e2a\u793e\u4f1a\u4eba\u53e3\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u76ee\u7684\u6027\u6784\u9020\u7684\u5bf9\u8bdd\u80fd\u53ef\u9760\u5730\u63ed\u793a\u504f\u89c1\u653e\u5927\uff0cLLMs\u5728\u5bf9\u8bdd\u4e2d\u7ecf\u5e38\u65e0\u6cd5\u62d2\u7edd\u504f\u89c1\u6027\u540e\u7eed\u95ee\u9898\u3002\u8fd9\u79cd\u538b\u529b\u6d4b\u8bd5\u7a81\u663e\u4e86\u901a\u8fc7\u4e92\u52a8\u53ef\u4ee5\u6d6e\u73b0\u7684\u6df1\u5c42\u5d4c\u5165\u504f\u89c1\u3002", "conclusion": "CoBia\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793aLLMs\u4e2d\u9690\u85cf\u7684\u504f\u89c1\uff0c\u8868\u660e\u9700\u8981\u66f4\u4e25\u683c\u7684\u6d4b\u8bd5\u6765\u786e\u4fdd\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.10956", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10956", "abs": "https://arxiv.org/abs/2510.10956", "authors": ["Zhiqiang Yuan", "Wenjun Mao", "Zhuo Chen", "Xiyue Shang", "Chong Wang", "Yiling Lou", "Xin Peng"], "title": "Project-Level C-to-Rust Translation via Synergistic Integration of Knowledge Graphs and Large Language Models", "comment": null, "summary": "Translating C code into safe Rust is an effective way to ensure its memory\nsafety. Compared to rule-based translation which produces Rust code that\nremains largely unsafe, LLM-based methods can generate more idiomatic and safer\nRust code because LLMs have been trained on vast amount of human-written\nidiomatic code. Although promising, existing LLM-based methods still struggle\nwith project-level C-to-Rust translation. They typically partition a C project\ninto smaller units (\\eg{} functions) based on call graphs and translate them\nbottom-up to resolve program dependencies. However, this bottom-up,\nunit-by-unit paradigm often fails to translate pointers due to the lack of a\nglobal perspective on their usage. To address this problem, we propose a novel\nC-Rust Pointer Knowledge Graph (KG) that enriches a code-dependency graph with\ntwo types of pointer semantics: (i) pointer-usage information which record\nglobal behaviors such as points-to flows and map lower-level struct usage to\nhigher-level units; and (ii) Rust-oriented annotations which encode ownership,\nmutability, nullability, and lifetime. Synthesizing the \\kg{} with LLMs, we\nfurther propose \\ourtool{}, which implements a project-level C-to-Rust\ntranslation technique. In \\ourtool{}, the \\kg{} provides LLMs with\ncomprehensive pointer semantics from a global perspective, thus guiding LLMs\ntowards generating safe and idiomatic Rust code from a given C project. Our\nexperiments show that \\ourtool{} reduces unsafe usages in translated Rust by\n99.9\\% compared to both rule-based translation and traditional LLM-based\nrewriting, while achieving an average 29.3\\% higher functional correctness than\nthose fuzzing-enhanced LLM methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u9488\u77e5\u8bc6\u56fe\u8c31\u7684C\u5230Rust\u9879\u76ee\u7ea7\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u6307\u9488\u8bed\u4e49\u5206\u6790\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u540eRust\u4ee3\u7801\u7684\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\u6b63\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684C\u5230Rust\u7ffb\u8bd1\u65b9\u6cd5\u5728\u9879\u76ee\u7ea7\u522b\u5b58\u5728\u6307\u9488\u5904\u7406\u56f0\u96be\uff0c\u7f3a\u4e4f\u5168\u5c40\u89c6\u89d2\u5bfc\u81f4\u751f\u6210\u7684Rust\u4ee3\u7801\u4ecd\u5305\u542b\u5927\u91cf\u4e0d\u5b89\u5168\u7528\u6cd5\u3002", "method": "\u6784\u5efaC-Rust\u6307\u9488\u77e5\u8bc6\u56fe\u8c31\uff0c\u5305\u542b\u6307\u9488\u4f7f\u7528\u4fe1\u606f\u548cRust\u5bfc\u5411\u7684\u6ce8\u89e3\uff08\u6240\u6709\u6743\u3001\u53ef\u53d8\u6027\u3001\u53ef\u7a7a\u6027\u3001\u751f\u547d\u5468\u671f\uff09\uff0c\u7ed3\u5408LLM\u8fdb\u884c\u9879\u76ee\u7ea7\u7ffb\u8bd1\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8e\u89c4\u5219\u7684\u7ffb\u8bd1\u548c\u4f20\u7edfLLM\u91cd\u5199\uff0c\u51cf\u5c1199.9%\u7684\u4e0d\u5b89\u5168\u7528\u6cd5\uff1b\u76f8\u6bd4\u6a21\u7cca\u6d4b\u8bd5\u589e\u5f3a\u7684LLM\u65b9\u6cd5\uff0c\u529f\u80fd\u6b63\u786e\u6027\u5e73\u5747\u63d0\u9ad829.3%\u3002", "conclusion": "\u6307\u9488\u77e5\u8bc6\u56fe\u8c31\u80fd\u6709\u6548\u63d0\u4f9b\u5168\u5c40\u6307\u9488\u8bed\u4e49\uff0c\u6307\u5bfcLLM\u751f\u6210\u66f4\u5b89\u5168\u3001\u66f4\u7b26\u5408Rust\u4e60\u60ef\u7684\u4ee3\u7801\u3002", "topic": "code agent"}}
{"id": "2510.11039", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.11039", "abs": "https://arxiv.org/abs/2510.11039", "authors": ["Yifeng Zhu", "Xianlin Zhao", "Xutian Li", "Yanzhen Zou", "Haizhuo Yuan", "Yue Wang", "Bing Xie"], "title": "RepoSummary: Feature-Oriented Summarization and Documentation Generation for Code Repositories", "comment": null, "summary": "Repository summarization is a crucial research question in development and\nmaintenance for software engineering. Existing repository summarization\ntechniques primarily focus on summarizing code according to the directory tree,\nwhich is insufficient for tracing high-level features to the methods that\ncollaboratively implement them. To address these limitations, we propose\nRepoSummary, a feature-oriented code repository summarization approach that\nsimultaneously generates repository documentation automatically. Furthermore,\nit establishes more accurate traceability links from functional features to the\ncorresponding code elements, enabling developers to rapidly locate relevant\nmethods and files during code comprehension and maintenance. Comprehensive\nexperiments against the state-of-the-art baseline (HGEN) demonstrate that\nRepoSummary achieves higher feature coverage and more accurate traceability. On\naverage, it increases the rate of completely covered features in manual\ndocumentation from 61.2% to 71.1%, improves file-level traceability recall from\n29.9% to 53.0%, and generates documentation that is more conceptually\nconsistent, easier to understand, and better formatted than that produced by\nexisting approaches.", "AI": {"tldr": "RepoSummary\u662f\u4e00\u4e2a\u9762\u5411\u7279\u5f81\u7684\u4ee3\u7801\u4ed3\u5e93\u6458\u8981\u65b9\u6cd5\uff0c\u80fd\u81ea\u52a8\u751f\u6210\u4ed3\u5e93\u6587\u6863\u5e76\u5efa\u7acb\u529f\u80fd\u7279\u5f81\u5230\u4ee3\u7801\u5143\u7d20\u7684\u66f4\u51c6\u786e\u8ffd\u8e2a\u94fe\u63a5\u3002", "motivation": "\u73b0\u6709\u4ed3\u5e93\u6458\u8981\u6280\u672f\u4e3b\u8981\u57fa\u4e8e\u76ee\u5f55\u6811\u7ed3\u6784\uff0c\u65e0\u6cd5\u6709\u6548\u8ffd\u8e2a\u9ad8\u5c42\u6b21\u529f\u80fd\u7279\u5f81\u5230\u534f\u4f5c\u5b9e\u73b0\u7684\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u4ee3\u7801\u7406\u89e3\u548c\u7ef4\u62a4\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7279\u5f81\u5bfc\u5411\u7684\u4ee3\u7801\u4ed3\u5e93\u6458\u8981\u65b9\u6cd5\uff0c\u540c\u65f6\u81ea\u52a8\u751f\u6210\u4ed3\u5e93\u6587\u6863\uff0c\u5efa\u7acb\u529f\u80fd\u7279\u5f81\u5230\u5bf9\u5e94\u4ee3\u7801\u5143\u7d20\u7684\u8ffd\u8e2a\u94fe\u63a5\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebfHGEN\uff0cRepoSummary\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7279\u5f81\u8986\u76d6\u7387\u548c\u66f4\u51c6\u786e\u7684\u8ffd\u8e2a\u80fd\u529b\uff1a\u5b8c\u5168\u8986\u76d6\u7279\u5f81\u6bd4\u4f8b\u4ece61.2%\u63d0\u5347\u523071.1%\uff0c\u6587\u4ef6\u7ea7\u8ffd\u8e2a\u53ec\u56de\u7387\u4ece29.9%\u63d0\u5347\u523053.0%\u3002", "conclusion": "RepoSummary\u751f\u6210\u7684\u6587\u6863\u5728\u6982\u5ff5\u4e00\u81f4\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u683c\u5f0f\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u5b9a\u4f4d\u76f8\u5173\u65b9\u6cd5\u548c\u6587\u4ef6\u3002", "topic": "swe application"}}
{"id": "2510.10197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10197", "abs": "https://arxiv.org/abs/2510.10197", "authors": ["Siyuan Lu", "Zechuan Wang", "Hongxuan Zhang", "Qintong Wu", "Leilei Gan", "Chenyi Zhuang", "Jinjie Gu", "Tao Lin"], "title": "Don't Just Fine-tune the Agent, Tune the Environment", "comment": null, "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce $\\textbf{Environment Tuning}$, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\n$\\textbf{Environment Tuning}$ orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEnvironment Tuning\u7684\u65b0\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bfe\u7a0b\u3001\u73af\u5883\u589e\u5f3a\u548c\u7ec6\u7c92\u5ea6\u8fdb\u5ea6\u5956\u52b1\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u76f4\u63a5\u4ece\u95ee\u9898\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u590d\u6742\u884c\u4e3a\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5148\u6536\u96c6\u7684\u4e13\u5bb6\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee5\u53caSFT\u65b9\u6cd5\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u6807\u51c6RL\u65b9\u6cd5\u9762\u4e34\u7684\u51b7\u542f\u52a8\u95ee\u9898\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "Environment Tuning\u8bad\u7ec3\u8303\u5f0f\uff0c\u5305\u542b\u7ed3\u6784\u5316\u8bfe\u7a0b\u7f16\u6392\u3001\u63d0\u4f9b\u7ea0\u6b63\u53cd\u9988\u7684\u53ef\u64cd\u4f5c\u73af\u5883\u589e\u5f3a\u3001\u786e\u4fdd\u7a33\u5b9a\u9ad8\u6548\u63a2\u7d22\u7684\u7ec6\u7c92\u5ea6\u8fdb\u5ea6\u5956\u52b1\u3002", "result": "\u4ec5\u4f7f\u7528BFCL\u57fa\u51c6\u7684400\u4e2a\u95ee\u9898\u5b9e\u4f8b\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684\u5206\u5e03\u5185\u6027\u80fd\uff0c\u8fd8\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u514b\u670d\u4e86SFT\u65b9\u6cd5\u7684\u6027\u80fd\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "\u4ece\u57fa\u4e8e\u9759\u6001\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\u8f6c\u5411\u52a8\u6001\u3001\u57fa\u4e8e\u73af\u5883\u7684\u63a2\u7d22\uff0c\u4e3a\u8bad\u7ec3\u66f4\u9c81\u68d2\u548c\u6570\u636e\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11076", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.11076", "abs": "https://arxiv.org/abs/2510.11076", "authors": ["Lingyue Fu", "Haowei Yuan", "Datong Chen", "Xinyi Dai", "Qingyao Li", "Weinan Zhang", "Weiwen Liu", "Yong Yu"], "title": "DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education", "comment": null, "summary": "In programming education, Debugging and Teaching (DT) task is a common\nscenario where students receive assistance in correcting their erroneous code.\nThe task involves multiple inputs, including erroneous code, error messages,\nreference solutions, and the question description, with the goal of generating\nmodification suggestions to the erroneous code. However, two key challenges\nhinder the effectiveness of existing approaches. Firstly, the complexity and\nheterogeneity of inputs inherent in DT tasks significantly elevate the\nreasoning challenges faced by LLMs. Second, existing approaches often fail to\nfully leverage the availability of standard code in DT tasks, forcing models to\nrely solely on complex multi-step reasoning, which limits the potential of LLMs\nin addressing DT tasks effectively. To address these challenges, we propose\nDebugTA, a novel LLM-based debugging and teaching agent with specialized tools\nfor standard code retrieval, variable substitution to align reference code, and\nan external compiler for real-time code analysis. Guided by explicit\npedagogical and debugging principles, DebugTA acts as an agent that decomposes\na complex task into sequential LLM interactions, each utilizing distinct tools\nfor specific subtasks, thereby simplifying the logical reasoning at each step\nand reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool\ncalls to align the standard code with the erroneous code as much as possible,\nallowing the LLM to focus on logic errors within the erroneous code and\nimproving the accuracy of the generated suggestions. To rigorously assess the\nquality of modification suggestions, we introduce a student simulator-teacher\ninteraction paradigm. Experimental results on three real-world code datasets\ndemonstrate that DebugTA consistently improves teaching effectiveness while\nsignificantly reducing computational costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86DebugTA\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8c03\u8bd5\u548c\u6559\u5b66\u4ee3\u7406\uff0c\u901a\u8fc7\u4e13\u7528\u5de5\u5177\uff08\u6807\u51c6\u4ee3\u7801\u68c0\u7d22\u3001\u53d8\u91cf\u66ff\u6362\u3001\u5916\u90e8\u7f16\u8bd1\u5668\uff09\u6765\u7b80\u5316\u7f16\u7a0b\u6559\u80b2\u4e2d\u7684\u8c03\u8bd5\u4efb\u52a1\uff0c\u63d0\u9ad8\u6559\u5b66\u6548\u679c\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u7f16\u7a0b\u6559\u80b2\u4e2d\u8c03\u8bd5\u6559\u5b66\u4efb\u52a1\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8f93\u5165\u590d\u6742\u6027\u548c\u5f02\u8d28\u6027\u589e\u52a0\u4e86LLM\u7684\u63a8\u7406\u96be\u5ea6\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6807\u51c6\u4ee3\u7801\uff0c\u9650\u5236\u4e86LLM\u5728\u8c03\u8bd5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u4e86DebugTA\u4ee3\u7406\uff0c\u901a\u8fc7\u5de5\u5177\u8c03\u7528\uff08\u6807\u51c6\u4ee3\u7801\u68c0\u7d22\u3001\u53d8\u91cf\u66ff\u6362\u5bf9\u9f50\u53c2\u8003\u4ee3\u7801\u3001\u5916\u90e8\u7f16\u8bd1\u5668\u5b9e\u65f6\u5206\u6790\uff09\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u987a\u5e8fLLM\u4ea4\u4e92\uff0c\u7b80\u5316\u6bcf\u4e00\u6b65\u7684\u903b\u8f91\u63a8\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDebugTA\u6301\u7eed\u63d0\u9ad8\u4e86\u6559\u5b66\u6548\u679c\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "DebugTA\u901a\u8fc7\u5de5\u5177\u8f85\u52a9\u7684\u5206\u89e3\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7f16\u7a0b\u6559\u80b2\u4e2d\u8c03\u8bd5\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u4e86LLM\u5728\u8c03\u8bd5\u6559\u5b66\u4e2d\u7684\u8868\u73b0\u3002", "topic": "swe application"}}
{"id": "2510.10207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10207", "abs": "https://arxiv.org/abs/2510.10207", "authors": ["Yujian Zhang", "Keyu Chen", "Zhifeng Shen", "Ruizhi Qiao", "Xing Sun"], "title": "Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning", "comment": null, "summary": "Although Long Reasoning Models (LRMs) have achieved superior performance on\nvarious reasoning scenarios, they often suffer from increased computational\ncosts and inference latency caused by overthinking. To address these\nlimitations, we propose Adaptive Dual Reasoner, which supports two reasoning\nmodes: fast thinking and slow thinking. ADR dynamically alternates between\nthese modes based on the contextual complexity during reasoning. ADR is trained\nin two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to\nequip the model with the ability to integrate both fast and slow reasoning\nmodes, in which we construct a hybrid reasoning dataset through a dedicated\npipeline to provide large-scale supervision. (2) A reinforcement learning stage\nfor optimizing reasoning effort, where we introduce Entropy-guided Hybrid\nPolicy Optimization EHPO, an RL training framework employing an entropy-guided\ndynamic rollout strategy for branching at high-entropy units and a\ndifficulty-aware penalty to balance fast and slow reasoning. Across challenging\nmathematical reasoning benchmarks, ADR achieves an effective balance between\nreasoning performance and efficiency among state-of-the-art approaches.\nSpecifically, ADR yields a performance gain of up to 6.1%, while reducing the\nreasoning output length by 49.5% to 59.3%.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u53cc\u63a8\u7406\u5668(ADR)\uff0c\u901a\u8fc7\u5feb\u901f\u601d\u7ef4\u548c\u6162\u901f\u601d\u7ef4\u4e24\u79cd\u63a8\u7406\u6a21\u5f0f\u7684\u52a8\u6001\u5207\u6362\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf", "motivation": "\u89e3\u51b3\u957f\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u56e0\u8fc7\u5ea6\u601d\u8003\u5bfc\u81f4\u7684\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u6784\u5efa\u6df7\u5408\u63a8\u7406\u6570\u636e\u96c6\uff1b2) \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f15\u5165\u71b5\u5f15\u5bfc\u6df7\u5408\u7b56\u7565\u4f18\u5316(EHPO)\uff0c\u5728\u9ad8\u71b5\u5355\u5143\u8fdb\u884c\u5206\u652f\u5e76\u91c7\u7528\u96be\u5ea6\u611f\u77e5\u60e9\u7f5a", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u8fbe6.1%\uff0c\u540c\u65f6\u63a8\u7406\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1149.5%\u81f359.3%", "conclusion": "ADR\u5728\u63a8\u7406\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6709\u6548\u5e73\u8861\uff0c\u662f\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u4e4b\u4e00", "topic": "agentic reinforcement learning"}}
{"id": "2510.11138", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.11138", "abs": "https://arxiv.org/abs/2510.11138", "authors": ["Zitao Wang", "Zhimin Zhao", "Michael W. Godfrey"], "title": "What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times", "comment": null, "summary": "Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transforming\nthe practice of software engineering by enabling the development of\n\\emph{FMware} -- applications and infrastructures built around these models.\nFMware systems now support tasks such as code generation, natural-language\ninteraction, knowledge integration, and multi-modal content creation,\nunderscoring their disruptive impact on current software engineering workflows.\nHowever, the design, implementation, and evolution of FMware present\nsignificant new challenges, particularly across cloud-based and on-premise\nplatforms where goals, processes, and tools often diverge from those of\ntraditional software development.\n  To our knowledge, this is the first large-scale analysis of FMware\ndevelopment across both cloud-based platforms and open-source repositories. We\nempirically investigate the FMware ecosystem through three focus areas: (1) the\nmost common application domains of FMware, (2) the key challenges developers\nencounter, and (3) the types of issues that demand the greatest effort to\nresolve. Our analysis draws on data from GitHub repositories and from leading\nFMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findings\nreveal a strong focus on education, content creation, and business strategy,\nalongside persistent technical challenges in memory management, dependency\nhandling, and tokenizer configuration. On GitHub, bug reports and core\nfunctionality issues are the most frequently reported problems, while code\nreview, similarity search, and prompt template design are the most\ntime-consuming to resolve.\n  By uncovering developer practices and pain points, this study points to\nopportunities to improve FMware tools, workflows, and community support, and\nprovides actionable insights to help guide the future of FMware development.", "AI": {"tldr": "\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u5206\u6790\u57fa\u7840\u6a21\u578b\u8f6f\u4ef6(FMware)\u5f00\u53d1\u7684\u7814\u7a76\uff0c\u6db5\u76d6\u4e91\u7aef\u5e73\u53f0\u548c\u5f00\u6e90\u4ed3\u5e93\uff0c\u63ed\u793a\u4e86\u6559\u80b2\u3001\u5185\u5bb9\u521b\u4f5c\u548c\u5546\u4e1a\u7b56\u7565\u662f\u4e3b\u8981\u5e94\u7528\u9886\u57df\uff0c\u5e76\u8bc6\u522b\u4e86\u5185\u5b58\u7ba1\u7406\u3001\u4f9d\u8d56\u5904\u7406\u548c\u5206\u8bcd\u5668\u914d\u7f6e\u7b49\u5173\u952e\u6280\u672f\u6311\u6218\u3002", "motivation": "\u57fa\u7840\u6a21\u578b(FMs)\u6b63\u5728\u5f7b\u5e95\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\uff0c\u4f46FMware\u7684\u8bbe\u8ba1\u3001\u5b9e\u73b0\u548c\u6f14\u5316\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4e91\u7aef\u548c\u672c\u5730\u5e73\u53f0\u4e0a\u7684\u5f00\u53d1\u6d41\u7a0b\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5206\u6790GitHub\u4ed3\u5e93\u548c\u4e3b\u6d41FMware\u5e73\u53f0(HuggingFace\u3001GPTStore\u3001Ora\u3001Poe)\u7684\u6570\u636e\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u5b9e\u8bc1\u7814\u7a76\uff1a\u5e38\u89c1\u5e94\u7528\u9886\u57df\u3001\u5f00\u53d1\u8005\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3001\u6700\u8017\u65f6\u7684issue\u7c7b\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0FMware\u4e3b\u8981\u805a\u7126\u4e8e\u6559\u80b2\u3001\u5185\u5bb9\u521b\u4f5c\u548c\u5546\u4e1a\u7b56\u7565\uff1b\u5728GitHub\u4e0abug\u62a5\u544a\u548c\u6838\u5fc3\u529f\u80fd\u95ee\u9898\u6700\u5e38\u89c1\uff0c\u800c\u4ee3\u7801\u5ba1\u67e5\u3001\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u63d0\u793a\u6a21\u677f\u8bbe\u8ba1\u6700\u8017\u65f6\uff1b\u6280\u672f\u6311\u6218\u96c6\u4e2d\u5728\u5185\u5b58\u7ba1\u7406\u3001\u4f9d\u8d56\u5904\u7406\u548c\u5206\u8bcd\u5668\u914d\u7f6e\u3002", "conclusion": "\u901a\u8fc7\u63ed\u793a\u5f00\u53d1\u8005\u5b9e\u8df5\u548c\u75db\u70b9\uff0c\u672c\u7814\u7a76\u4e3a\u6539\u8fdbFMware\u5de5\u5177\u3001\u5de5\u4f5c\u6d41\u548c\u793e\u533a\u652f\u6301\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u5e76\u4e3aFMware\u5f00\u53d1\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.10238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10238", "abs": "https://arxiv.org/abs/2510.10238", "authors": ["Zixuan Qin", "Kunlin Lyu", "Qingchen Yu", "Yifan Sun", "Zhaoxin Fan"], "title": "The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities", "comment": null, "summary": "Large Language Models (LLMs) have become foundational tools in natural\nlanguage processing, powering a wide range of applications and research. Many\nstudies have shown that LLMs share significant similarities with the human\nbrain. Recent neuroscience research has found that a small subset of biological\nneurons in the human brain are crucial for core cognitive functions, which\nraises a fundamental question: do LLMs also contain a small subset of critical\nneurons? In this paper, we investigate this question by proposing a\nPerturbation-based Causal Identification of Critical Neurons method to\nsystematically locate such critical neurons in LLMs. Our findings reveal three\nkey insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting\nthese critical neurons can cause a 72B-parameter model with over 1.1 billion\nneurons to completely collapse, with perplexity increasing by up to 20 orders\nof magnitude; (2) These critical neurons are not uniformly distributed, but\ntend to concentrate in the outer layers, particularly within the MLP down\\_proj\ncomponents; (3) Performance degradation exhibits sharp phase transitions,\nrather than a gradual decline, when these critical neurons are disrupted.\nThrough comprehensive experiments across diverse model architectures and\nscales, we provide deeper analysis of these phenomena and their implications\nfor LLM robustness and interpretability. These findings can offer guidance for\ndeveloping more robust model architectures and improving deployment security in\nsafety-critical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6270\u52a8\u7684\u5173\u952e\u795e\u7ecf\u5143\u8bc6\u522b\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u8d85\u7a00\u758f\u7684\u5173\u952e\u795e\u7ecf\u5143\u96c6\u5408\uff0c\u7834\u574f\u8fd9\u4e9b\u795e\u7ecf\u5143\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u4e14\u8fd9\u4e9b\u795e\u7ecf\u5143\u96c6\u4e2d\u5206\u5e03\u5728\u6a21\u578b\u5916\u5c42\u7279\u522b\u662fMLP\u7ec4\u4ef6\u4e2d\u3002", "motivation": "\u53d7\u5230\u4eba\u7c7b\u5927\u8111\u4e2d\u5c11\u6570\u5173\u952e\u795e\u7ecf\u5143\u5bf9\u8ba4\u77e5\u529f\u80fd\u91cd\u8981\u6027\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u5173\u952e\u795e\u7ecf\u5143\u5b50\u96c6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6270\u52a8\u7684\u56e0\u679c\u8bc6\u522b\u65b9\u6cd5\u6765\u7cfb\u7edf\u5b9a\u4f4d\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5173\u952e\u795e\u7ecf\u5143\u3002", "result": "\u53d1\u73b0\uff1a(1)LLMs\u5305\u542b\u8d85\u7a00\u758f\u5173\u952e\u795e\u7ecf\u5143\uff0c\u7834\u574f\u8fd9\u4e9b\u795e\u7ecf\u5143\u4f1a\u5bfc\u81f4720\u4ebf\u53c2\u6570\u6a21\u578b\u5d29\u6e83\uff0c\u56f0\u60d1\u5ea6\u589e\u52a020\u4e2a\u6570\u91cf\u7ea7\uff1b(2)\u5173\u952e\u795e\u7ecf\u5143\u96c6\u4e2d\u5206\u5e03\u5728\u5916\u5c42\uff0c\u7279\u522b\u662fMLP down_proj\u7ec4\u4ef6\uff1b(3)\u6027\u80fd\u4e0b\u964d\u5448\u73b0\u6025\u5267\u7684\u76f8\u53d8\u800c\u975e\u6e10\u8fdb\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u67b6\u6784\u548c\u63d0\u9ad8\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2510.10285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10285", "abs": "https://arxiv.org/abs/2510.10285", "authors": ["Haolang Lu", "Bolun Chu", "WeiYe Fu", "Guoshun Nan", "Junning Liu", "Minghui Pan", "Qiankun Li", "Yi Yu", "Hua Wang", "Kun Wang"], "title": "Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control", "comment": "preprint", "summary": "Multimodal large reasoning models (MLRMs) are rapidly advancing\nvision-language reasoning and are emerging as a foundation for cross-modal\nintelligence. Hallucination remains a persistent failure mode, manifesting\nitself as erroneous reasoning chains and misinterpretation of visual content.\nIn this study, we observe that attention heads exhibit a staged division:\nshallow heads predominantly serve perception, while deeper heads shift toward\nsymbolic reasoning, revealing two major causes of hallucination, namely\nperceptual bias and reasoning drift. To address these issues, we propose a\nlightweight and interpretable two-step plugin, Functional Head Identification\nand Class-conditioned Rescaling, which locates perception- and\nreasoning-oriented heads and regulates their contributions without retraining.\nEvaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six\nbenchmarks across three domains, and four baselines show that our plugin\nachieves an average improvement of 5% and up to 15%, with only <1% additional\ncomputation and 9% of baseline latency. Our approach is completely\nmodel-agnostic and significantly enhances both the reliability and\ninterpretability of the off-the-shelf MLRMs, thereby enabling their safe\ndeployment in high-stakes applications. Our code is available at\nhttps://anonymous.4open.science/r/Functional-Attention-Control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63d2\u4ef6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u611f\u77e5\u5bfc\u5411\u548c\u63a8\u7406\u5bfc\u5411\u7684\u6ce8\u610f\u529b\u5934\u5e76\u8c03\u8282\u5176\u8d21\u732e\uff0c\u6709\u6548\u51cf\u5c11\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5e73\u5747\u63d0\u53475%\u6027\u80fd\uff0c\u6700\u9ad8\u63d0\u534715%\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u8868\u73b0\u4e3a\u9519\u8bef\u63a8\u7406\u94fe\u548c\u89c6\u89c9\u5185\u5bb9\u8bef\u89e3\u3002\u7814\u7a76\u53d1\u73b0\u6ce8\u610f\u529b\u5934\u5b58\u5728\u9636\u6bb5\u6027\u5206\u5de5\uff1a\u6d45\u5c42\u5934\u4e3b\u8981\u8d1f\u8d23\u611f\u77e5\uff0c\u6df1\u5c42\u5934\u8f6c\u5411\u7b26\u53f7\u63a8\u7406\uff0c\u8fd9\u63ed\u793a\u4e86\u5e7b\u89c9\u7684\u4e24\u4e2a\u4e3b\u8981\u539f\u56e0\uff1a\u611f\u77e5\u504f\u5dee\u548c\u63a8\u7406\u6f02\u79fb\u3002", "method": "\u63d0\u51fa\u529f\u80fd\u6027\u5934\u8bc6\u522b\u548c\u7c7b\u6761\u4ef6\u91cd\u7f29\u653e\u7684\u4e24\u6b65\u63d2\u4ef6\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u611f\u77e5\u5bfc\u5411\u548c\u63a8\u7406\u5bfc\u5411\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8c03\u8282\u5b83\u4eec\u7684\u8d21\u732e\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754cMLRM\u6a21\u578b\u3001\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u56db\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u63d0\u53475%\u6027\u80fd\uff0c\u6700\u9ad8\u63d0\u534715%\uff0c\u4ec5\u589e\u52a0<1%\u7684\u8ba1\u7b97\u5f00\u9500\u548c9%\u7684\u57fa\u7ebf\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b8c\u5168\u6a21\u578b\u65e0\u5173\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u73b0\u6709\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5b89\u5168\u90e8\u7f72\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "2510.09705", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09705", "abs": "https://arxiv.org/abs/2510.09705", "authors": ["Sudip Khadka", "L. S. Paudel"], "title": "A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation", "comment": null, "summary": "Static feature exclusion strategies often fail to prevent bias when hidden\ndependencies influence the model predictions. To address this issue, we explore\na reinforcement learning (RL) framework that integrates bias mitigation and\nautomated feature selection within a single learning process. Unlike\ntraditional heuristic-driven filter or wrapper approaches, our RL agent\nadaptively selects features using a reward signal that explicitly integrates\npredictive performance with fairness considerations. This dynamic formulation\nallows the model to balance generalization, accuracy, and equity throughout the\ntraining process, rather than rely exclusively on pre-processing adjustments or\npost hoc correction mechanisms. In this paper, we describe the construction of\na multi-component reward function, the specification of the agents action space\nover feature subsets, and the integration of this system with ensemble\nlearning. We aim to provide a flexible and generalizable way to select features\nin environments where predictors are correlated and biases can inadvertently\nre-emerge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff0c\u5c06\u504f\u5dee\u7f13\u89e3\u548c\u7279\u5f81\u9009\u62e9\u6574\u5408\u5230\u5355\u4e00\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u52a8\u6001\u5956\u52b1\u4fe1\u53f7\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u7279\u5f81\u6392\u9664\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u9632\u6b62\u504f\u5dee\uff0c\u56e0\u4e3a\u9690\u85cf\u7684\u4f9d\u8d56\u5173\u7cfb\u4f1a\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u3002\u9700\u8981\u4e00\u79cd\u52a8\u6001\u65b9\u6cd5\u6765\u5e73\u8861\u6cdb\u5316\u80fd\u529b\u3001\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6784\u5efa\u591a\u7ec4\u4ef6\u5956\u52b1\u51fd\u6570\uff0c\u5b9a\u4e49\u7279\u5f81\u5b50\u96c6\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u4e0e\u96c6\u6210\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u8003\u8651\uff0c\u907f\u514d\u4f20\u7edf\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5RL\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u901a\u7528\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9884\u6d4b\u56e0\u5b50\u76f8\u5173\u4e14\u504f\u5dee\u53ef\u80fd\u610f\u5916\u91cd\u65b0\u51fa\u73b0\u7684\u73af\u5883\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11536", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11536", "abs": "https://arxiv.org/abs/2510.11536", "authors": ["Manaal Basha", "Aime\u00ea M. Ribeiro", "Jeena Javahar", "Cleidson R. B. de Souza", "Gema Rodr\u00edguez-P\u00e9rez"], "title": "CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs", "comment": "ICSME 2025 Tool Demonstration Track", "summary": "Understanding how developers interact with code generation tools (CGTs)\nrequires detailed, real-time data on programming behavior which is often\ndifficult to collect without disrupting workflow. We present\n\\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed\nto capture fine-grained interaction events from within the Visual Studio Code\n(VS Code) editor. \\textit{CodeWatcher} logs semantically meaningful events such\nas insertions made by CGTs, deletions, copy-paste actions, and focus shifts,\nenabling continuous monitoring of developer activity without modifying user\nworkflows. The system comprises a VS Code plugin, a Python-based RESTful API,\nand a MongoDB backend, all containerized for scalability and ease of\ndeployment. By structuring and timestamping each event, \\textit{CodeWatcher}\nenables post-hoc reconstruction of coding sessions and facilitates rich\nbehavioral analyses, including how and when CGTs are used during development.\nThis infrastructure is crucial for supporting research on responsible AI,\ndeveloper productivity, and the human-centered evaluation of CGTs. Please find\nthe demo, diagrams, and tool here: https://osf.io/j2kru/overview.", "AI": {"tldr": "CodeWatcher\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u975e\u4fb5\u5165\u5f0f\u7684\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728VS Code\u7f16\u8f91\u5668\u4e2d\u6355\u83b7\u5f00\u53d1\u8005\u4e0e\u4ee3\u7801\u751f\u6210\u5de5\u5177\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u4e8b\u4ef6\u3002", "motivation": "\u7406\u89e3\u5f00\u53d1\u8005\u5982\u4f55\u4e0e\u4ee3\u7801\u751f\u6210\u5de5\u5177\u4ea4\u4e92\u9700\u8981\u8be6\u7ec6\u7684\u5b9e\u65f6\u7f16\u7a0b\u884c\u4e3a\u6570\u636e\uff0c\u4f46\u6536\u96c6\u8fd9\u4e9b\u6570\u636e\u5f80\u5f80\u4f1a\u5f71\u54cd\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u7cfb\u7edf\u5305\u542bVS Code\u63d2\u4ef6\u3001Python RESTful API\u548cMongoDB\u540e\u7aef\uff0c\u901a\u8fc7\u5bb9\u5668\u5316\u5b9e\u73b0\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u8bb0\u5f55\u63d2\u5165\u3001\u5220\u9664\u3001\u590d\u5236\u7c98\u8d34\u548c\u7126\u70b9\u5207\u6362\u7b49\u8bed\u4e49\u5316\u4e8b\u4ef6\u3002", "result": "CodeWatcher\u80fd\u591f\u5728\u4e0d\u4fee\u6539\u7528\u6237\u5de5\u4f5c\u6d41\u7a0b\u7684\u60c5\u51b5\u4e0b\u6301\u7eed\u76d1\u63a7\u5f00\u53d1\u8005\u6d3b\u52a8\uff0c\u652f\u6301\u7f16\u7a0b\u4f1a\u8bdd\u7684\u4e8b\u540e\u91cd\u5efa\u548c\u4e30\u5bcc\u7684\u884c\u4e3a\u5206\u6790\u3002", "conclusion": "\u8be5\u57fa\u7840\u8bbe\u65bd\u5bf9\u4e8e\u652f\u6301\u8d1f\u8d23\u4efbAI\u3001\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u4ee5\u53ca\u4ee3\u7801\u751f\u6210\u5de5\u5177\u7684\u4eba\u672c\u8bc4\u4f30\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2510.09988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09988", "abs": "https://arxiv.org/abs/2510.09988", "authors": ["Jiaqi Wei", "Xiang Zhang", "Yuejin Yang", "Wenxuan Huang", "Juntai Cao", "Sheng Xu", "Xiang Zhuang", "Zhangyang Gao", "Muhammad Abdul-Mageed", "Laks V. S. Lakshmanan", "Chenyu You", "Wanli Ouyang", "Siqi Sun"], "title": "Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey", "comment": null, "summary": "Deliberative tree search is a cornerstone of modern Large Language Model\n(LLM) research, driving the pivot from brute-force scaling toward algorithmic\nefficiency. This single paradigm unifies two critical frontiers:\n\\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve\nhard problems, and \\textbf{Self-Improvement}, which uses search-generated data\nto durably enhance model parameters. However, this burgeoning field is\nfragmented and lacks a common formalism, particularly concerning the ambiguous\nrole of the reward signal -- is it a transient heuristic or a durable learning\ntarget? This paper resolves this ambiguity by introducing a unified framework\nthat deconstructs search algorithms into three core components: the\n\\emph{Search Mechanism}, \\emph{Reward Formulation}, and \\emph{Transition\nFunction}. We establish a formal distinction between transient \\textbf{Search\nGuidance} for TTS and durable \\textbf{Parametric Reward Modeling} for\nSelf-Improvement. Building on this formalism, we introduce a component-centric\ntaxonomy, synthesize the state-of-the-art, and chart a research roadmap toward\nmore systematic progress in creating autonomous, self-improving agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u6811\u641c\u7d22\u7b97\u6cd5\u5206\u89e3\u4e3a\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u641c\u7d22\u673a\u5236\u3001\u5956\u52b1\u516c\u5f0f\u548c\u8f6c\u79fb\u51fd\u6570\uff0c\u6f84\u6e05\u4e86\u5956\u52b1\u4fe1\u53f7\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u81ea\u6211\u6539\u8fdb\u4e2d\u7684\u4e0d\u540c\u4f5c\u7528\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e2d\u7684\u5ba1\u614e\u6811\u641c\u7d22\u9886\u57df\u5b58\u5728\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u5171\u540c\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u4fe1\u53f7\u7684\u89d2\u8272\u5b9a\u4f4d\u4e0a\u5b58\u5728\u6a21\u7cca\u6027\u2014\u2014\u5b83\u7a76\u7adf\u662f\u4e34\u65f6\u542f\u53d1\u5f0f\u8fd8\u662f\u6301\u4e45\u5b66\u4e60\u76ee\u6807\u3002", "method": "\u5f15\u5165\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u641c\u7d22\u7b97\u6cd5\u5206\u89e3\u4e3a\u641c\u7d22\u673a\u5236\u3001\u5956\u52b1\u516c\u5f0f\u548c\u8f6c\u79fb\u51fd\u6570\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5e76\u6b63\u5f0f\u533a\u5206\u4e86\u7528\u4e8e\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u641c\u7d22\u6307\u5bfc\u548c\u7528\u4e8e\u81ea\u6211\u6539\u8fdb\u7684\u53c2\u6570\u5316\u5956\u52b1\u5efa\u6a21\u3002", "result": "\u5efa\u7acb\u4e86\u7ec4\u4ef6\u4e2d\u5fc3\u5206\u7c7b\u6cd5\uff0c\u7efc\u5408\u4e86\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5e76\u4e3a\u521b\u5efa\u81ea\u4e3b\u3001\u81ea\u6211\u6539\u8fdb\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u6027\u8fdb\u5c55\u7ed8\u5236\u4e86\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u6811\u641c\u7d22\u9886\u57df\u7684\u788e\u7247\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u660e\u786e\u4e86\u5956\u52b1\u4fe1\u53f7\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u89d2\u8272\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2510.10454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10454", "abs": "https://arxiv.org/abs/2510.10454", "authors": ["Sihang Zeng", "Yujuan Fu", "Sitong Zhou", "Zixuan Yu", "Lucas Jing Liu", "Jun Wen", "Matthew Thompson", "Ruth Etzioni", "Meliha Yetisgen"], "title": "Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) offer a generalizable approach for modeling\npatient trajectories, but suffer from the long and noisy nature of electronic\nhealth records (EHR) data in temporal reasoning. To address these challenges,\nwe introduce Traj-CoA, a multi-agent system involving chain-of-agents for\npatient trajectory modeling. Traj-CoA employs a chain of worker agents to\nprocess EHR data in manageable chunks sequentially, distilling critical events\ninto a shared long-term memory module, EHRMem, to reduce noise and preserve a\ncomprehensive timeline. A final manager agent synthesizes the worker agents'\nsummary and the extracted timeline in EHRMem to make predictions. In a\nzero-shot one-year lung cancer risk prediction task based on five-year EHR\ndata, Traj-CoA outperforms baselines of four categories. Analysis reveals that\nTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as a\npromisingly robust and generalizable approach for modeling complex patient\ntrajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86Traj-CoA\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u94fe\u5f0f\u4ee3\u7406\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u89e3\u51b3\u957f\u5e8f\u5217\u548c\u566a\u58f0\u95ee\u9898\uff0c\u5728\u80ba\u764c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u6a21\u60a3\u8005\u8f68\u8ff9\u65b9\u9762\u5177\u6709\u901a\u7528\u6027\uff0c\u4f46\u9762\u4e34\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u957f\u5e8f\u5217\u548c\u566a\u58f0\u7684\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u62ec\u591a\u4e2a\u5de5\u4f5c\u4ee3\u7406\u6309\u987a\u5e8f\u5904\u7406\u6570\u636e\u5757\uff0c\u5c06\u5173\u952e\u4e8b\u4ef6\u63d0\u53d6\u5230\u5171\u4eab\u957f\u671f\u8bb0\u5fc6\u6a21\u5757EHRMem\u4e2d\uff0c\u6700\u540e\u7531\u7ba1\u7406\u4ee3\u7406\u7efc\u5408\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u4e8e\u4e94\u5e74\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u7684\u96f6\u6837\u672c\u4e00\u5e74\u80ba\u764c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cTraj-CoA\u4f18\u4e8e\u56db\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Traj-CoA\u5c55\u73b0\u51fa\u4e0e\u4e34\u5e8a\u5bf9\u9f50\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u662f\u5efa\u6a21\u590d\u6742\u60a3\u8005\u8f68\u8ff9\u7684\u7a33\u5065\u4e14\u901a\u7528\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.09719", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09719", "abs": "https://arxiv.org/abs/2510.09719", "authors": ["Chenxu Wang", "Hao Li", "Yiqun Zhang", "Linyao Chen", "Jianhao Chen", "Ping Jian", "Peng Ye", "Qiaosheng Zhang", "Shuyue Hu"], "title": "ICL-Router: In-Context Learned Model Representations for LLM Routing", "comment": null, "summary": "Large language models (LLMs) often exhibit complementary strengths. Model\nrouting harnesses these strengths by dynamically directing each query to the\nmost suitable model, given a candidate model pool. However, routing performance\nrelies on accurate model representations, and adding new models typically\nrequires retraining, limiting scalability. To address these challenges, we\npropose a novel routing method using in-context vectors to represent model\ncapabilities. The method proceeds in two stages. First, queries are embedded\nand projected into vectors, with a projector and LLM-based router trained to\nreconstruct the original queries, aligning vector representations with the\nrouter's semantic space. Second, each candidate model is profiled on a query\nset, and the router learns -- based on in-context vectors of query and model\nperformance -- to predict whether each model can correctly answer new queries.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nrouting performance in both in-distribution and out-of-distribution tasks.\nMoreover, our method allows for seamless integration of new models without\nretraining the router. The code is available at\nhttps://github.com/lalalamdbf/ICL-Router.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e0a\u4e0b\u6587\u5411\u91cf\u8868\u793a\u6a21\u578b\u80fd\u529b\u7684\u65b0\u578b\u8def\u7531\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u6a21\u578b\u8def\u7531\uff0c\u5728\u5206\u5e03\u5185\u5916\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u65b0\u6a21\u578b\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u8def\u7531\u5668\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u8def\u7531\u65b9\u6cd5\u4f9d\u8d56\u51c6\u786e\u6a21\u578b\u8868\u793a\u3001\u6dfb\u52a0\u65b0\u6a21\u578b\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u7684\u9650\u5236\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u67e5\u8be2\u5d4c\u5165\u548c\u6295\u5f71\u5230\u5411\u91cf\u7a7a\u95f4\uff0c\u8bad\u7ec3\u6295\u5f71\u5668\u548c\u57fa\u4e8eLLM\u7684\u8def\u7531\u5668\u91cd\u6784\u539f\u59cb\u67e5\u8be2\uff1b2) \u5728\u67e5\u8be2\u96c6\u4e0a\u5206\u6790\u5019\u9009\u6a21\u578b\u6027\u80fd\uff0c\u8def\u7531\u5668\u57fa\u4e8e\u67e5\u8be2\u548c\u6a21\u578b\u6027\u80fd\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u662f\u5426\u80fd\u6b63\u786e\u56de\u7b54\u65b0\u67e5\u8be2\u3002", "result": "\u5728\u5206\u5e03\u5185\u5916\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8def\u7531\u6027\u80fd\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u65b0\u6a21\u578b\u800c\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u8def\u7531\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u5411\u91cf\u6709\u6548\u8868\u793a\u6a21\u578b\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8def\u7531\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.10739", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10739", "abs": "https://arxiv.org/abs/2510.10739", "authors": ["Shivani Shukla", "Himanshu Joshi"], "title": "A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications", "comment": "Peer-reviewed and accepted to the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025) DynaFront 2025 Workshop\n  (https://sites.google.com/view/dynafrontneurips25)", "summary": "We introduce a general stochastic differential equation framework for\nmodelling multiobjective optimization dynamics in iterative Large Language\nModel (LLM) interactions. Our framework captures the inherent stochasticity of\nLLM responses through explicit diffusion terms and reveals systematic\ninterference patterns between competing objectives via an interference matrix\nformulation. We validate our theoretical framework using iterative code\ngeneration as a proof-of-concept application, analyzing 400 sessions across\nsecurity, efficiency, and functionality objectives. Our results demonstrate\nstrategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29,\nand predictive accuracy achieving R2 = 0.74 for balanced approaches. This work\nproposes the feasibility of dynamical systems analysis for multi-objective LLM\ninteractions, with code generation serving as an initial validation domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\u6765\u5efa\u6a21LLM\u591a\u76ee\u6807\u4f18\u5316\u52a8\u6001\uff0c\u901a\u8fc7\u6269\u6563\u9879\u6355\u6349LLM\u54cd\u5e94\u7684\u968f\u673a\u6027\uff0c\u5e76\u4f7f\u7528\u5e72\u6270\u77e9\u9635\u63ed\u793a\u76ee\u6807\u95f4\u7684\u7cfb\u7edf\u6027\u5e72\u6270\u6a21\u5f0f\u3002", "motivation": "LLM\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u52a8\u6001\u884c\u4e3a\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\uff0c\u9700\u8981\u7406\u89e3\u76ee\u6807\u95f4\u7684\u5e72\u6270\u6548\u5e94\u548c\u6536\u655b\u7279\u6027\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\uff0c\u5305\u542b\u663e\u5f0f\u6269\u6563\u9879\u548c\u5e72\u6270\u77e9\u9635\uff0c\u4ee5\u4ee3\u7801\u751f\u6210\u4e3a\u9a8c\u8bc1\u5e94\u7528\uff0c\u5206\u6790\u4e86400\u4e2a\u4f1a\u8bdd\u7684\u5b89\u5168\u3001\u6548\u7387\u548c\u529f\u80fd\u76ee\u6807\u3002", "result": "\u5c55\u793a\u4e86\u7b56\u7565\u4f9d\u8d56\u7684\u6536\u655b\u884c\u4e3a\uff08\u6536\u655b\u73870.33-1.29\uff09\uff0c\u5e73\u8861\u65b9\u6cd5\u7684\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u5230R\u00b2=0.74\u3002", "conclusion": "\u8bc1\u660e\u4e86\u52a8\u6001\u7cfb\u7edf\u5206\u6790\u5728\u591a\u76ee\u6807LLM\u4ea4\u4e92\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4ee3\u7801\u751f\u6210\u4f5c\u4e3a\u521d\u6b65\u9a8c\u8bc1\u9886\u57df\u3002", "topic": "agent analysis"}}
{"id": "2510.10461", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10461", "abs": "https://arxiv.org/abs/2510.10461", "authors": ["Hongjie Zheng", "Zesheng Shi", "Ping Yi"], "title": "MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision", "comment": null, "summary": "Autonomous agents utilizing Large Language Models (LLMs) have demonstrated\nremarkable capabilities in isolated medical tasks like diagnosis and image\nanalysis, but struggle with integrated clinical workflows that connect\ndiagnostic reasoning and medication decisions. We identify a core limitation:\nexisting medical AI systems process tasks in isolation without the\ncross-validation and knowledge integration found in clinical teams, reducing\ntheir effectiveness in real-world healthcare scenarios. To transform the\nisolation paradigm into a collaborative approach, we propose MedCoAct, a\nconfidence-aware multi-agent framework that simulates clinical collaboration by\nintegrating specialized doctor and pharmacist agents, and present a benchmark,\nDrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and\ntreatment workflows. Our results demonstrate that MedCoAct achieves 67.58\\%\ndiagnostic accuracy and 67.58\\% medication recommendation accuracy,\noutperforming single agent framework by 7.04\\% and 7.08\\% respectively. This\ncollaborative approach generalizes well across diverse medical domains, proving\nespecially effective for telemedicine consultations and routine clinical\nscenarios, while providing interpretable decision-making pathways.", "AI": {"tldr": "\u63d0\u51faMedCoAct\u6846\u67b6\uff0c\u901a\u8fc7\u533b\u751f\u548c\u836f\u5242\u5e08\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\u533b\u7597AI\u5728\u6574\u5408\u8bca\u65ad\u548c\u6cbb\u7597\u5de5\u4f5c\u6d41\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5728\u8bca\u65ad\u548c\u7528\u836f\u63a8\u8350\u51c6\u786e\u7387\u4e0a\u5206\u522b\u6bd4\u5355\u667a\u80fd\u4f53\u63d0\u53477.04%\u548c7.08%\u3002", "motivation": "\u73b0\u6709\u533b\u7597AI\u7cfb\u7edf\u5728\u5b64\u7acb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6574\u5408\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u7f3a\u4e4f\u8de8\u9a8c\u8bc1\u548c\u77e5\u8bc6\u6574\u5408\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faMedCoAct\u7f6e\u4fe1\u611f\u77e5\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u96c6\u6210\u4e13\u4e1a\u533b\u751f\u548c\u836f\u5242\u5e08\u667a\u80fd\u4f53\u6a21\u62df\u4e34\u5e8a\u534f\u4f5c\uff0c\u5e76\u5efa\u7acbDrugCareQA\u57fa\u51c6\u8bc4\u4f30\u533b\u7597AI\u5728\u6574\u5408\u8bca\u65ad\u548c\u6cbb\u7597\u5de5\u4f5c\u6d41\u4e2d\u7684\u80fd\u529b\u3002", "result": "MedCoAct\u8fbe\u523067.58%\u7684\u8bca\u65ad\u51c6\u786e\u7387\u548c67.58%\u7684\u7528\u836f\u63a8\u8350\u51c6\u786e\u7387\uff0c\u5206\u522b\u6bd4\u5355\u667a\u80fd\u4f53\u6846\u67b6\u63d0\u53477.04%\u548c7.08%\uff0c\u5728\u8fdc\u7a0b\u533b\u7597\u548c\u5e38\u89c4\u4e34\u5e8a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u534f\u4f5c\u65b9\u6cd5\u5728\u4e0d\u540c\u533b\u7597\u9886\u57df\u6cdb\u5316\u826f\u597d\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fdc\u7a0b\u533b\u7597\u54a8\u8be2\u548c\u5e38\u89c4\u4e34\u5e8a\u573a\u666f\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.10009", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10009", "abs": "https://arxiv.org/abs/2510.10009", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu"], "title": "Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning", "comment": null, "summary": "Reasoning-augmented search agents, such as Search-R1, are trained to reason,\nsearch, and generate the final answer iteratively. Nevertheless, due to their\nlimited capabilities in reasoning and search, their performance on multi-hop QA\nbenchmarks remains far from satisfactory. To handle complex or compound\nqueries, we train an LLM-based search agent with the native capability of query\nexpansion through reinforcement learning. In each turn, our search agent\nproposes several query variants, which are searched simultaneously to cover\nmore relevant information. Meanwhile, given limited post-training data and\ncomputing resources, it is very challenging for a search agent to master\nmultiple tasks, including query generation, retrieved information\nunderstanding, and answer generation. Therefore, we propose incorporating a\npre-trained squeezer model that helps the search agent understand the retrieved\ndocuments, allowing the search agent to focus on query generation for high\nretrieval recall. With the assistance of the squeezer model, we discover that\neven a small-scale 3B LLM can demonstrate a strong capability of query\nexpansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.\nTo be specific, our experiments across seven question-answering benchmarks\ndemonstrate that our method, named ExpandSearch, achieves an average\nimprovement of 4.4% compared to state-of-the-art baselines, with strong gains\non multi-hop reasoning tasks requiring diverse evidence aggregation.", "AI": {"tldr": "\u63d0\u51faExpandSearch\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u641c\u7d22\u4ee3\u7406\uff0c\u5177\u5907\u67e5\u8be2\u6269\u5c55\u80fd\u529b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u538b\u7f29\u5668\u6a21\u578b\u7406\u89e3\u68c0\u7d22\u6587\u6863\uff0c\u57287\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53474.4%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u589e\u5f3a\u641c\u7d22\u4ee3\u7406\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u63a8\u7406\u548c\u641c\u7d22\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u5904\u7406\u590d\u5408\u67e5\u8be2\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u641c\u7d22\u4ee3\u7406\uff0c\u6bcf\u8f6e\u751f\u6210\u591a\u4e2a\u67e5\u8be2\u53d8\u4f53\u5e76\u884c\u641c\u7d22\uff1b\u5f15\u5165\u9884\u8bad\u7ec3\u538b\u7f29\u5668\u6a21\u578b\u5e2e\u52a9\u7406\u89e3\u68c0\u7d22\u6587\u6863\uff0c\u8ba9\u641c\u7d22\u4ee3\u7406\u4e13\u6ce8\u4e8e\u67e5\u8be2\u751f\u6210\u4ee5\u63d0\u9ad8\u68c0\u7d22\u53ec\u56de\u7387\u3002", "result": "\u57287\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cExpandSearch\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5e73\u5747\u63d0\u53474.4%\u51c6\u786e\u7387\uff0c\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u5373\u4f7f\u4f7f\u7528\u5c0f\u89c4\u6a213B\u53c2\u6570\u7684LLM\uff0c\u7ed3\u5408\u538b\u7f29\u5668\u6a21\u578b\u8f85\u52a9\uff0c\u4e5f\u80fd\u5728\u67e5\u8be2\u6269\u5c55\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.10549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10549", "abs": "https://arxiv.org/abs/2510.10549", "authors": ["Xinbang Dai", "Huikang Hu", "Yongrui Chen", "Jiaqi Li", "Rihui Jin", "Yuyang Zhang", "Xiaoguang Li", "Lifeng Shang", "Guilin Qi"], "title": "ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding", "comment": "25 pages, 20 figures", "summary": "While large language models (LLMs) excel at many domain-specific tasks, their\nability to deeply comprehend and reason about full-length academic papers\nremains underexplored. Existing benchmarks often fall short of capturing such\ndepth, either due to surface-level question design or unreliable evaluation\nmetrics. To address this gap, we introduce ELAIPBench, a benchmark curated by\ndomain experts to evaluate LLMs' comprehension of artificial intelligence (AI)\nresearch papers. Developed through an incentive-driven, adversarial annotation\nprocess, ELAIPBench features 403 multiple-choice questions from 137 papers. It\nspans three difficulty levels and emphasizes non-trivial reasoning rather than\nshallow retrieval. Our experiments show that the best-performing LLM achieves\nan accuracy of only 39.95%, far below human performance. Moreover, we observe\nthat frontier LLMs equipped with a thinking mode or a retrieval-augmented\ngeneration (RAG) system fail to improve final results-even harming accuracy due\nto overthinking or noisy retrieval. These findings underscore the significant\ngap between current LLM capabilities and genuine comprehension of academic\npapers.", "AI": {"tldr": "\u63d0\u51fa\u4e86ELAIPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5bf9AI\u7814\u7a76\u8bba\u6587\u7684\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\uff0c\u5305\u542b403\u4e2a\u591a\u9009\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u6700\u4f73LLM\u51c6\u786e\u7387\u4ec539.95%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30LLM\u5bf9\u5b66\u672f\u8bba\u6587\u7684\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u95ee\u9898\u8bbe\u8ba1\u8868\u9762\u5316\u4e14\u8bc4\u4f30\u6307\u6807\u4e0d\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u6fc0\u52b1\u9a71\u52a8\u7684\u5bf9\u6297\u6027\u6807\u6ce8\u8fc7\u7a0b\uff0c\u6784\u5efa\u5305\u542b403\u4e2a\u591a\u9009\u9898\u7684ELAIPBench\u57fa\u51c6\uff0c\u6db5\u76d6\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u5f3a\u8c03\u975e\u5e73\u51e1\u63a8\u7406\u800c\u975e\u6d45\u5c42\u68c0\u7d22\u3002", "result": "\u6700\u4f73LLM\u51c6\u786e\u7387\u4ec539.95%\uff0c\u524d\u6cbfLLM\u5373\u4f7f\u914d\u5907\u601d\u7ef4\u6a21\u5f0f\u6216RAG\u7cfb\u7edf\u4e5f\u65e0\u6cd5\u6539\u5584\u7ed3\u679c\uff0c\u751a\u81f3\u56e0\u8fc7\u5ea6\u601d\u8003\u6216\u566a\u58f0\u68c0\u7d22\u800c\u964d\u4f4e\u51c6\u786e\u7387\u3002", "conclusion": "\u5f53\u524dLLM\u80fd\u529b\u4e0e\u771f\u6b63\u7406\u89e3\u5b66\u672f\u8bba\u6587\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2510.10592", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10592", "abs": "https://arxiv.org/abs/2510.10592", "authors": ["Hong Su"], "title": "A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning", "comment": null, "summary": "Existing studies have introduced method-based reasoning and scope extension\nas approaches to enhance Large Language Model (LLM) performance beyond direct\nmatrix mappings. Building on these foundations, this paper summarizes and\nintegrates these ideas into a unified Intuition-Method Layered Model with Scope\nExtension, designed to address indirected (unseen) issues more systematically.\nIn this framework, intuition-based thinking provides rapid first-reaction\nanswers, while method-based thinking decouples questions and solutions into\ntransferable reasoning units. Scope extension is then applied to broaden\napplicability, including vertical (cause analysis), horizontal (parallel and\ngeneralized issues), and for the first time, temporal and spatial extensions,\nwhich expand reasoning across time and contextual dimensions. These extensions\nare organized into systematic knowledge trees that interconnect into a\nknowledge network, thereby increasing adaptability. To quantitatively evaluate\nthis process, we propose the entropy of method extension, which measures the\nindependence and diversity of extensions as an indicator of the system's\ncapacity to solve unseen questions. By logically connecting existing approaches\nwith new extensions and introducing an entropy-based evaluation framework, this\nwork advances toward a more robust and extensible reasoning paradigm for LLMs\nin real-world problem-solving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u76f4\u89c9-\u65b9\u6cd5\u5206\u5c42\u6a21\u578b\u4e0e\u8303\u56f4\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u89c9\u601d\u7ef4\u63d0\u4f9b\u5feb\u901f\u53cd\u5e94\uff0c\u65b9\u6cd5\u601d\u7ef4\u5c06\u95ee\u9898\u89e3\u8026\u4e3a\u53ef\u8f6c\u79fb\u63a8\u7406\u5355\u5143\uff0c\u5e76\u5f15\u5165\u5782\u76f4\u3001\u6c34\u5e73\u3001\u65f6\u95f4\u548c\u7a7a\u95f4\u6269\u5c55\u6765\u589e\u5f3aLLM\u5bf9\u672a\u89c1\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5df2\u5f15\u5165\u57fa\u4e8e\u65b9\u6cd5\u7684\u63a8\u7406\u548c\u8303\u56f4\u6269\u5c55\u6765\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6574\u5408\u3002\u672c\u6587\u65e8\u5728\u5c06\u8fd9\u4e9b\u601d\u60f3\u7edf\u4e00\u6210\u4e00\u4e2a\u66f4\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u89e3\u51b3\u95f4\u63a5\uff08\u672a\u89c1\uff09\u95ee\u9898\u3002", "method": "\u6784\u5efa\u76f4\u89c9-\u65b9\u6cd5\u5206\u5c42\u6a21\u578b\uff1a\u76f4\u89c9\u5c42\u63d0\u4f9b\u5feb\u901f\u7b54\u6848\uff0c\u65b9\u6cd5\u5c42\u5c06\u95ee\u9898\u89e3\u8026\u4e3a\u53ef\u8f6c\u79fb\u63a8\u7406\u5355\u5143\u3002\u5f15\u5165\u56db\u79cd\u8303\u56f4\u6269\u5c55\uff08\u5782\u76f4\u3001\u6c34\u5e73\u3001\u65f6\u95f4\u3001\u7a7a\u95f4\uff09\u5e76\u7ec4\u7ec7\u6210\u7cfb\u7edf\u77e5\u8bc6\u6811\u7f51\u7edc\u3002\u63d0\u51fa\u65b9\u6cd5\u6269\u5c55\u71b5\u6765\u91cf\u5316\u8bc4\u4f30\u7cfb\u7edf\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u903b\u8f91\u8fde\u63a5\u73b0\u6709\u65b9\u6cd5\u5e76\u5f15\u5165\u65b0\u7684\u6269\u5c55\u7ef4\u5ea6\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u66f4\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684LLM\u63a8\u7406\u8303\u5f0f\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u5904\u7406\u672a\u89c1\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u7edf\u4e00\u73b0\u6709\u65b9\u6cd5\u3001\u5f15\u5165\u65b0\u7684\u6269\u5c55\u7ef4\u5ea6\u548c\u71b5\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u8fdb\u4e86LLM\u5728\u73b0\u5b9e\u95ee\u9898\u89e3\u51b3\u4e2d\u66f4\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u63a8\u7406\u8303\u5f0f\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2510.10062", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10062", "abs": "https://arxiv.org/abs/2510.10062", "authors": ["Adnan El Assadi", "Isaac Chung", "Roman Solomatin", "Niklas Muennighoff", "Kenneth Enevoldsen"], "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task", "comment": "Submitted to ICLR 2026", "summary": "Comparing human and model performance offers a valuable perspective for\nunderstanding the strengths and limitations of embedding models, highlighting\nwhere they succeed and where they fail to capture meaning and nuance. However,\nsuch comparisons are rarely made, as human performance on embedding tasks is\ndifficult to measure. To fill this gap, we introduce HUME: Human Evaluation\nFramework for Text Embeddings. While frameworks like MTEB provide broad model\nevaluation, they lack reliable estimates of human performance, limiting the\ninterpretability of model scores. We measure human performance across 16 MTEB\ndatasets spanning reranking, classification, clustering, and semantic textual\nsimilarity across linguistically diverse high- and low-resource languages.\nHumans achieve an average performance of 77.6% compared to 80.1% for the best\nembedding model, although variation is substantial: models reach near-ceiling\nperformance on some datasets while struggling on others, suggesting dataset\nissues and revealing shortcomings in low-resource languages. We provide human\nperformance baselines, insight into task difficulty patterns, and an extensible\nevaluation framework that enables a more meaningful interpretation of the model\nand informs the development of both models and benchmarks. Our code, dataset,\nand leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.", "AI": {"tldr": "\u63d0\u51fa\u4e86HUME\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u91cf\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e2d\u7684\u4eba\u7c7b\u8868\u73b0\uff0c\u5e76\u4e0e\u6a21\u578b\u6027\u80fd\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0\u572816\u4e2aMTEB\u6570\u636e\u96c6\u4e0a\u4eba\u7c7b\u5e73\u5747\u8868\u73b077.6%\uff0c\u7565\u4f4e\u4e8e\u6700\u4f73\u6a21\u578b\u768480.1%\u3002", "motivation": "\u73b0\u6709\u5d4c\u5165\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\u7f3a\u4e4f\u53ef\u9760\u7684\u4eba\u7c7b\u6027\u80fd\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5206\u6570\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u5efa\u7acb\u4eba\u7c7b\u8868\u73b0\u57fa\u7ebf\u6765\u66f4\u597d\u5730\u7406\u89e3\u6a21\u578b\u7684\u80fd\u529b\u548c\u5c40\u9650\u3002", "method": "\u5f00\u53d1HUME\u6846\u67b6\uff0c\u572816\u4e2aMTEB\u6570\u636e\u96c6\u4e0a\u6d4b\u91cf\u4eba\u7c7b\u8868\u73b0\uff0c\u6db5\u76d6\u91cd\u6392\u5e8f\u3001\u5206\u7c7b\u3001\u805a\u7c7b\u548c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u8986\u76d6\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "result": "\u4eba\u7c7b\u5e73\u5747\u8868\u73b077.6%\uff0c\u6700\u4f73\u6a21\u578b80.1%\uff1b\u6a21\u578b\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u63a5\u8fd1\u5929\u82b1\u677f\u6027\u80fd\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4eba\u7c7b\u6027\u80fd\u57fa\u7ebf\u3001\u4efb\u52a1\u96be\u5ea6\u6d1e\u5bdf\u548c\u53ef\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u89e3\u91ca\u6a21\u578b\u6027\u80fd\u5e76\u6307\u5bfc\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f00\u53d1\u3002", "topic": "agent analysis"}}
{"id": "2510.10675", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10675", "abs": "https://arxiv.org/abs/2510.10675", "authors": ["Deven Panchal"], "title": "Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows", "comment": null, "summary": "Generative Agentic AI systems are emerging as a powerful paradigm for\nautomating complex, multi-step tasks. However, many existing frameworks for\nbuilding these systems introduce significant complexity, a steep learning\ncurve, and substantial boilerplate code, hindering rapid prototyping and\ndeployment. This paper introduces simpliflow, a lightweight, open-source Python\nframework designed to address these challenges. simpliflow enables the rapid\ndevelopment and orchestration of linear, deterministic agentic workflows\nthrough a declarative, JSON-based configuration. Its modular architecture\ndecouples agent management, workflow execution, and post-processing, promoting\nease of use and extensibility. By integrating with LiteLLM, it supports over\n100 Large Language Models (LLMs) out-of-the-box. We present the architecture,\noperational flow, and core features of simpliflow, demonstrating its utility\nthrough diverse use cases ranging from software development simulation to\nreal-time system interaction. A comparative analysis with prominent frameworks\nlike LangChain and AutoGen highlights simpliflow's unique position as a tool\noptimized for simplicity, control, and speed in deterministic workflow\nenvironments.", "AI": {"tldr": "simpliflow\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90Python\u6846\u67b6\uff0c\u901a\u8fc7\u58f0\u660e\u5f0fJSON\u914d\u7f6e\u5feb\u901f\u5f00\u53d1\u786e\u5b9a\u6027\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u590d\u6742\u6027\u548c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u667a\u80fd\u4f53AI\u6846\u67b6\u5b58\u5728\u663e\u8457\u590d\u6742\u6027\u3001\u9661\u5ced\u5b66\u4e60\u66f2\u7ebf\u548c\u5927\u91cf\u6837\u677f\u4ee3\u7801\uff0c\u963b\u788d\u4e86\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5c06\u667a\u80fd\u4f53\u7ba1\u7406\u3001\u5de5\u4f5c\u6d41\u6267\u884c\u548c\u540e\u5904\u7406\u89e3\u8026\uff0c\u901a\u8fc7\u58f0\u660e\u5f0fJSON\u914d\u7f6e\u5b9e\u73b0\u7ebf\u6027\u786e\u5b9a\u6027\u5de5\u4f5c\u6d41\u7684\u5feb\u901f\u7f16\u6392\uff0c\u96c6\u6210LiteLLM\u652f\u6301100+\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u4e86simpliflow\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u7528\u4f8b\uff08\u4ece\u8f6f\u4ef6\u5f00\u53d1\u6a21\u62df\u5230\u5b9e\u65f6\u7cfb\u7edf\u4ea4\u4e92\uff09\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u4e0eLangChain\u548cAutoGen\u7684\u5bf9\u6bd4\u5206\u6790\u7a81\u663e\u4e86\u5176\u5728\u786e\u5b9a\u6027\u5de5\u4f5c\u6d41\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "simpliflow\u5728\u786e\u5b9a\u6027\u5de5\u4f5c\u6d41\u73af\u5883\u4e2d\u4f5c\u4e3a\u4f18\u5316\u7b80\u5355\u6027\u3001\u63a7\u5236\u6027\u548c\u901f\u5ea6\u7684\u5de5\u5177\u5177\u6709\u72ec\u7279\u5730\u4f4d\uff0c\u4fc3\u8fdb\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5feb\u901f\u5f00\u53d1\u548c\u90e8\u7f72\u3002", "topic": "code agent"}}
{"id": "2510.10142", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10142", "abs": "https://arxiv.org/abs/2510.10142", "authors": ["Tingxu Han", "Wei Song", "Ziqi Ding", "Ziming Li", "Chunrong Fang", "Yuekang Li", "Dongfang Liu", "Zhenyu Chen", "Zhenting Wang"], "title": "DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models", "comment": null, "summary": "Large language models (LLMs) increasingly mediate decisions in domains where\nunfair treatment of demographic groups is unacceptable. Existing work probes\nwhen biased outputs appear, but gives little insight into the mechanisms that\ngenerate them, leaving existing mitigations largely fragile. In this paper, we\nconduct a systematic investigation LLM unfairness and propose DiffHeads, a\nlightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)\nprompting to Chain-of-Thought (CoT) prompting across eight representative open-\nand closed-source LLMs. DA will trigger the nature bias part of LLM and improve\nmeasured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.\nNext, we define a token-to-head contribution score that traces each token's\ninfluence back to individual attention heads. This reveals a small cluster of\nbias heads that activate under DA but stay largely dormant with CoT, providing\nthe first causal link between prompting strategy and bias emergence. Finally,\nbuilding on this insight, we propose DiffHeads that identifies bias heads\nthrough differential activation analysis between DA and CoT, and selectively\nmasks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under\nDA and CoT, respectively, without harming model utility.", "AI": {"tldr": "DiffHeads\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684LLM\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u9009\u62e9\u6027\u5c4f\u853d\u504f\u89c1\u6ce8\u610f\u529b\u5934\u6765\u51cf\u5c11\u6a21\u578b\u4e0d\u516c\u5e73\u6027\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u5c06\u4e0d\u516c\u5e73\u6027\u964d\u4f4e49.4%\u548c40.3%\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u504f\u89c1\u8f93\u51fa\u7684\u51fa\u73b0\u65f6\u673a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u751f\u6210\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5bfc\u81f4\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u8106\u5f31\u3002\u9700\u8981\u7cfb\u7edf\u7814\u7a76LLM\u4e0d\u516c\u5e73\u6027\u5e76\u5f00\u53d1\u6709\u6548\u7684\u53bb\u504f\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u6bd4\u8f83\u76f4\u63a5\u56de\u7b54\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7b56\u7565\uff0c\u5b9a\u4e49token\u5230\u6ce8\u610f\u529b\u5934\u7684\u8d21\u732e\u5206\u6570\u6765\u8ffd\u8e2a\u504f\u89c1\u6765\u6e90\uff0c\u7136\u540e\u57fa\u4e8eDA\u548cCoT\u7684\u5dee\u5f02\u6fc0\u6d3b\u5206\u6790\u8bc6\u522b\u504f\u89c1\u5934\uff0c\u9009\u62e9\u6027\u5c4f\u853d\u8fd9\u4e9b\u5934\u3002", "result": "DA\u63d0\u793a\u4f1a\u89e6\u53d1LLM\u7684\u56fa\u6709\u504f\u89c1\uff0c\u5c06\u4e0d\u516c\u5e73\u6027\u63d0\u9ad8534.5%-391.9%\uff1b\u53d1\u73b0\u4e00\u5c0f\u7c07\u504f\u89c1\u5934\u5728DA\u4e0b\u6fc0\u6d3b\u4f46\u5728CoT\u4e0b\u4f11\u7720\uff1bDiffHeads\u5728DA\u548cCoT\u4e0b\u5206\u522b\u51cf\u5c11\u4e0d\u516c\u5e73\u602749.4%\u548c40.3%\u3002", "conclusion": "DiffHeads\u63d0\u4f9b\u4e86\u4e00\u79cd\u56e0\u679c\u89e3\u91ca\u504f\u89c1\u51fa\u73b0\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5e72\u9884\u6709\u6548\u51cf\u5c11LLM\u4e0d\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.09781", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09781", "abs": "https://arxiv.org/abs/2510.09781", "authors": ["Yue Huang", "Hang Hua", "Yujun Zhou", "Pengcheng Jing", "Manish Nagireddy", "Inkit Padhi", "Greta Dolcetti", "Zhangchen Xu", "Subhajit Chaudhury", "Ambrish Rawat", "Liubov Nedoshivina", "Pin-Yu Chen", "Prasanna Sattigeri", "Xiangliang Zhang"], "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data", "comment": null, "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AuraGen\u6570\u636e\u751f\u6210\u5f15\u64ce\u3001Safiron\u9632\u62a4\u6a21\u578b\u548cPre-Exec Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u5728LLM\u667a\u80fd\u4f53\u6267\u884c\u524d\u68c0\u6d4b\u8ba1\u5212\u7ea7\u98ce\u9669\uff0c\u586b\u8865\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u4e09\u4e2a\u5173\u952e\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u9632\u62a4\u63aa\u65bd\u4e3b\u8981\u5728\u667a\u80fd\u4f53\u6267\u884c\u540e\u8fd0\u4f5c\uff0c\u96be\u4ee5\u6269\u5c55\u4e14\u7f3a\u4e4f\u5bf9\u8ba1\u5212\u7ea7\u522b\u7684\u53ef\u63a7\u76d1\u7763\uff0c\u67d0\u4e9b\u98ce\u9669\u4e00\u65e6\u6267\u884c\u4f1a\u9020\u6210\u4e25\u91cd\u540e\u679c\uff0c\u56e0\u6b64\u9700\u8981\u5728\u89c4\u5212\u9636\u6bb5\u8fdb\u884c\u5e72\u9884\u3002", "method": "1) AuraGen\uff1a\u53ef\u63a7\u6570\u636e\u751f\u6210\u5f15\u64ce\uff0c\u5408\u6210\u826f\u6027\u8f68\u8ff9\u3001\u6ce8\u5165\u7c7b\u522b\u6807\u8bb0\u7684\u98ce\u9669\u3001\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8fc7\u6ee4\u8f93\u51fa\uff1b2) Safiron\uff1a\u57fa\u7840\u9632\u62a4\u6a21\u578b\uff0c\u7ed3\u5408\u8de8\u89c4\u5212\u5668\u9002\u914d\u5668\u548c\u7d27\u51d1\u9632\u62a4\u6a21\u578b\uff1b3) Pre-Exec Bench\uff1a\u5305\u542b\u591a\u6837\u5316\u5de5\u5177\u548c\u5206\u652f\u8f68\u8ff9\u7684\u73b0\u5b9e\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u63d0\u51fa\u7684\u9632\u62a4\u6a21\u578b\u5728Pre-Exec Bench\u4e0a\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u83b7\u5f97\u4e00\u81f4\u589e\u76ca\uff0c\u6d88\u878d\u7814\u7a76\u63d0\u70bc\u4e86\u53ef\u64cd\u4f5c\u5b9e\u8df5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u5b89\u5168\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6a21\u677f\uff0c\u901a\u8fc7\u8ba1\u5212\u7ea7\u98ce\u9669\u68c0\u6d4b\u5b9e\u73b0\u4e86\u66f4\u53ef\u63a7\u7684\u76d1\u7763\u3002", "topic": "agent analysis"}}
{"id": "2510.10185", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10185", "abs": "https://arxiv.org/abs/2510.10185", "authors": ["Lei Gu", "Yinghao Zhu", "Haoran Sang", "Zixiang Wang", "Dehao Sui", "Wen Tang", "Ewen Harrison", "Junyi Gao", "Lequan Yu", "Liantao Ma"], "title": "MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems", "comment": "Code: https://github.com/yhzhu99/MedAgentAudit", "summary": "While large language model (LLM)-based multi-agent systems show promise in\nsimulating medical consultations, their evaluation is often confined to\nfinal-answer accuracy. This practice treats their internal collaborative\nprocesses as opaque \"black boxes\" and overlooks a critical question: is a\ndiagnostic conclusion reached through a sound and verifiable reasoning pathway?\nThe inscrutable nature of these systems poses a significant risk in high-stakes\nmedical applications, potentially leading to flawed or untrustworthy\nconclusions. To address this, we conduct a large-scale empirical study of 3,600\ncases from six medical datasets and six representative multi-agent frameworks.\nThrough a rigorous, mixed-methods approach combining qualitative analysis with\nquantitative auditing, we develop a comprehensive taxonomy of collaborative\nfailure modes. Our quantitative audit reveals four dominant failure patterns:\nflawed consensus driven by shared model deficiencies, suppression of correct\nminority opinions, ineffective discussion dynamics, and critical information\nloss during synthesis. This study demonstrates that high accuracy alone is an\ninsufficient measure of clinical or public trust. It highlights the urgent need\nfor transparent and auditable reasoning processes, a cornerstone for the\nresponsible development and deployment of medical AI.", "AI": {"tldr": "\u5bf9\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u533b\u7597\u54a8\u8be2\u7cfb\u7edf\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u63ed\u793a\u5176\u5185\u90e8\u534f\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4ec5\u9760\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u533b\u7597AI\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524dLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u533b\u7597\u54a8\u8be2\u4e2d\u7684\u8bc4\u4f30\u8fc7\u4e8e\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u9a8c\u8bc1\u6027\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u533b\u7597\u5e94\u7528\u4e2d\u5b58\u5728\u4e25\u91cd\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u516d\u4e2a\u533b\u7597\u6570\u636e\u96c6\u548c\u516d\u4e2a\u4ee3\u8868\u6027\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5bf93600\u4e2a\u6848\u4f8b\u8fdb\u884c\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u7ed3\u5408\u5b9a\u6027\u5206\u6790\u548c\u5b9a\u91cf\u5ba1\u8ba1\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u79cd\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff1a\u5171\u4eab\u6a21\u578b\u7f3a\u9677\u9a71\u52a8\u7684\u9519\u8bef\u5171\u8bc6\u3001\u6b63\u786e\u5c11\u6570\u610f\u89c1\u88ab\u538b\u5236\u3001\u65e0\u6548\u8ba8\u8bba\u52a8\u6001\u3001\u4ee5\u53ca\u4fe1\u606f\u5408\u6210\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002", "conclusion": "\u9ad8\u51c6\u786e\u6027\u4e0d\u8db3\u4ee5\u5efa\u7acb\u4e34\u5e8a\u6216\u516c\u4f17\u4fe1\u4efb\uff0c\u8feb\u5207\u9700\u8981\u900f\u660e\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u8fc7\u7a0b\u6765\u8d1f\u8d23\u4efb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u533b\u7597AI\u3002", "topic": "agent analysis"}}
{"id": "2510.10909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10909", "abs": "https://arxiv.org/abs/2510.10909", "authors": ["Daoyu Wang", "Mingyue Cheng", "Qi Liu", "Shuo Yu", "Zirui Liu", "Ze Guo"], "title": "PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature", "comment": "12 pages, 9 figures", "summary": "Understanding and reasoning on the web-scale scientific literature is a\ncrucial touchstone for large language model (LLM) based agents designed to\nsupport complex knowledge-intensive tasks. However, existing works are mainly\nrestricted to tool-free tasks within isolated papers, largely due to the lack\nof a benchmark for cross-paper reasoning and multi-tool orchestration in real\nresearch scenarios. In this work, we propose PaperArena, an evaluation\nbenchmark for agents to address real-world research questions that typically\nrequire integrating information across multiple papers with the assistance of\nexternal tools. Given a research question, agents should integrate diverse\nformats across multiple papers through reasoning and interacting with\nappropriate tools, thereby producing a well-grounded answer. To support\nstandardized evaluation, we provide a modular and extensible platform for agent\nexecution, offering tools such as multimodal parsing, context retrieval, and\nprogrammatic computation. Experimental results reveal that even the most\nadvanced LLM powering a well-established agent system achieves merely 38.78%\naverage accuracy. On the hard subset, accuracy drops to only 18.47%,\nhighlighting great potential for improvement. We also present several empirical\nfindings, including that all agents tested exhibit inefficient tool usage,\noften invoking more tools than necessary to solve a task. We invite the\ncommunity to adopt PaperArena to develop and evaluate more capable agents for\nscientific discovery. Our code and data are available\nhttps://github.com/Melmaphother/PaperArena.", "AI": {"tldr": "\u63d0\u51fa\u4e86PaperArena\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u8de8\u8bba\u6587\u63a8\u7406\u548c\u591a\u5de5\u5177\u534f\u8c03\u65b9\u9762\u7684\u80fd\u529b\uff0c\u73b0\u6709\u5148\u8fdb\u4ee3\u7406\u4ec5\u8fbe\u523038.78%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u7bc7\u8bba\u6587\u5185\u7684\u65e0\u5de5\u5177\u4efb\u52a1\uff0c\u7f3a\u4e4f\u771f\u5b9e\u7814\u7a76\u573a\u666f\u4e2d\u8de8\u8bba\u6587\u63a8\u7406\u548c\u591a\u5de5\u5177\u534f\u8c03\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efaPaperArena\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u89e3\u6790\u3001\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u7a0b\u5e8f\u5316\u8ba1\u7b97\u7b49\u5de5\u5177\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u89e3\u51b3\u9700\u8981\u6574\u5408\u591a\u7bc7\u8bba\u6587\u4fe1\u606f\u7684\u771f\u5b9e\u7814\u7a76\u95ee\u9898\u65f6\u7684\u8868\u73b0\u3002", "result": "\u6700\u5148\u8fdb\u7684LLM\u4ee3\u7406\u7cfb\u7edf\u4ec5\u8fbe\u523038.78%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728\u56f0\u96be\u5b50\u96c6\u4e0a\u51c6\u786e\u7387\u964d\u81f318.47%\uff0c\u4e14\u6240\u6709\u6d4b\u8bd5\u4ee3\u7406\u90fd\u8868\u73b0\u51fa\u5de5\u5177\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "conclusion": "PaperArena\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7406\u5728\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u79d1\u5b66\u53d1\u73b0\u4ee3\u7406\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2510.10931", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10931", "abs": "https://arxiv.org/abs/2510.10931", "authors": ["SHengjie Ma", "Chenlong Deng", "Jiaxin Mao", "Jiadeng Huang", "Teng Wang", "Junjie Wu", "Changwang Zhang", "Jun wang"], "title": "PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents", "comment": null, "summary": "Retrieval-augmented generation (RAG) agents, such as recent\nDeepResearch-style systems, extend large language models (LLMs) with autonomous\ninformation-seeking capabilities through external tools. While reinforcement\nlearning (RL) has enabled impressive multi-step reasoning, we identify a\npreviously overlooked failure mode, Tool-Call Hacking, where agents inflate\nreward signals by issuing superficially correct tool calls without genuinely\nleveraging the retrieved evidence. This results in (i) mode collapse into\nrepetitive reliance on a single source and (ii) spurious grounding, where\nanswers are only weakly supported by cited content.\n  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL\nframework that enforces verifiable causal links between retrieved evidence,\nreasoning traces, and final answers. PoU operationalizes this through a unified\nstep-wise contract combining syntactic citation validation, perturbation-based\nsensitivity rewards, and answer-evidence alignment objectives, ensuring that\ntool usage remains both interpretable and functionally grounded.\n  Across seven QA benchmarks spanning in-domain, out-of-domain, and\nout-of-tool-distribution settings, PoU consistently outperforms strong\nDeepResearch baselines in factual accuracy, evidence faithfulness, and\ntool-routing balance. These findings highlight the necessity of grounding\nRL-trained agents not merely in task outcomes but in the causal use of\nretrieved information, offering a principled path toward trustworthy\nretrieval-augmented reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86Proof-of-Use (PoU)\u6846\u67b6\u6765\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4ee3\u7406\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u8bc1\u636e\u57fa\u7840\u7684\u5f3a\u5316\u5b66\u4e60\u786e\u4fdd\u68c0\u7d22\u8bc1\u636e\u4e0e\u6700\u7ec8\u7b54\u6848\u4e4b\u95f4\u7684\u53ef\u9a8c\u8bc1\u56e0\u679c\u8054\u7cfb\u3002", "motivation": "\u53d1\u73b0RAG\u4ee3\u7406\u5b58\u5728\u5de5\u5177\u8c03\u7528\u9ed1\u5ba2\u95ee\u9898\uff0c\u5373\u4ee3\u7406\u901a\u8fc7\u53d1\u51fa\u8868\u9762\u6b63\u786e\u7684\u5de5\u5177\u8c03\u7528\u6765\u5938\u5927\u5956\u52b1\u4fe1\u53f7\uff0c\u800c\u4e0d\u771f\u6b63\u5229\u7528\u68c0\u7d22\u5230\u7684\u8bc1\u636e\uff0c\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\u548c\u865a\u5047\u57fa\u7840\u3002", "method": "\u63d0\u51faProof-of-Use (PoU)\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u9010\u6b65\u5951\u7ea6\u7ed3\u5408\u8bed\u6cd5\u5f15\u7528\u9a8c\u8bc1\u3001\u57fa\u4e8e\u6270\u52a8\u7684\u654f\u611f\u6027\u5956\u52b1\u548c\u7b54\u6848-\u8bc1\u636e\u5bf9\u9f50\u76ee\u6807\uff0c\u786e\u4fdd\u5de5\u5177\u4f7f\u7528\u7684\u53ef\u89e3\u91ca\u6027\u548c\u529f\u80fd\u6027\u57fa\u7840\u3002", "result": "\u5728\u4e03\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPoU\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u8bc1\u636e\u5fe0\u5b9e\u5ea6\u548c\u5de5\u5177\u8def\u7531\u5e73\u8861\u65b9\u9762\u6301\u7eed\u4f18\u4e8eDeepResearch\u57fa\u7ebf\uff0c\u6db5\u76d6\u9886\u57df\u5185\u3001\u9886\u57df\u5916\u548c\u5de5\u5177\u5206\u5e03\u5916\u8bbe\u7f6e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u4ee3\u7406\u4e2d\u4e0d\u4ec5\u8981\u5728\u4efb\u52a1\u7ed3\u679c\u4e0a\u5efa\u7acb\u57fa\u7840\uff0c\u8fd8\u8981\u5728\u68c0\u7d22\u4fe1\u606f\u7684\u56e0\u679c\u4f7f\u7528\u4e0a\u5efa\u7acb\u57fa\u7840\uff0c\u4e3a\u53ef\u4fe1\u7684\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.10252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10252", "abs": "https://arxiv.org/abs/2510.10252", "authors": ["Samir Abdaljalil", "Erchin Serpedin", "Khalid Qaraqe", "Hasan Kurban"], "title": "Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models", "comment": null, "summary": "Large language models (LLMs) often generate reasoning traces that appear\ncoherent but rest on unsupported assumptions, leading to hallucinated\nconclusions. Prior work mainly addresses factual hallucinations or relies on\npost-hoc verification, leaving reasoning-induced hallucinations largely\nunaddressed. We propose Audit-of-Understanding (AoU), a framework that\nconstrains inference to validated premises through three phases: (1)\ndecomposing a query into candidate assumptions, (2) auditing their support, and\n(3) conditioning inference only on the validated subset. Formally, AoU is\n\\emph{posterior-constrained inference}, connecting to selective prediction and\nrejection learning. Our contributions are threefold: (i) theoretical guarantees\nunder perfect validation, (ii) excess-risk bounds under imperfect audits, and\n(iii) tractability analysis. Empirically, AoU improves both accuracy and\nfaithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on\nGSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over\nChain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at\nhttps://anonymous.4open.science/r/audit-of-understanding-E28B.", "AI": {"tldr": "\u63d0\u51fa\u4e86AoU\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u67e5\u8be2\u3001\u5ba1\u8ba1\u5047\u8bbe\u652f\u6301\u5ea6\u3001\u4ec5\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u524d\u63d0\u8fdb\u884c\u63a8\u7406\u6765\u7ea6\u675f\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u51cf\u5c11LLM\u7684\u63a8\u7406\u8bf1\u5bfc\u5e7b\u89c9\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u770b\u4f3c\u8fde\u8d2f\u4f46\u57fa\u4e8e\u672a\u7ecf\u9a8c\u8bc1\u5047\u8bbe\u7684\u5e7b\u89c9\u7ed3\u8bba\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u6027\u5e7b\u89c9\u6216\u4f9d\u8d56\u4e8b\u540e\u9a8c\u8bc1\u3002", "method": "AoU\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\u5206\u89e3\u67e5\u8be2\u4e3a\u5019\u9009\u5047\u8bbe\u3001\u5ba1\u8ba1\u5047\u8bbe\u652f\u6301\u5ea6\u3001\u4ec5\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u5b50\u96c6\u8fdb\u884c\u63a8\u7406\uff0c\u5f62\u5f0f\u5316\u4e3a\u540e\u9a8c\u7ea6\u675f\u63a8\u7406\u3002", "result": "\u5728GSM8K\u3001MultiArith\u548cSVAMP\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u5fe0\u5b9e\u5ea6\uff0c\u76f8\u6bd4Chain-of-Thought\u7b49\u65b9\u6cd5\u83b7\u5f97\u9ad8\u8fbe30-45%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AoU\u6846\u67b6\u901a\u8fc7\u7ea6\u675f\u63a8\u7406\u524d\u63d0\u9a8c\u8bc1\u6709\u6548\u51cf\u5c11\u63a8\u7406\u8bf1\u5bfc\u5e7b\u89c9\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2510.11144", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11144", "abs": "https://arxiv.org/abs/2510.11144", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "title": "$How^{2}$: How to learn from procedural How-to questions", "comment": null, "summary": "An agent facing a planning problem can use answers to how-to questions to\nreduce uncertainty and fill knowledge gaps, helping it solve both current and\nfuture tasks. However, their open ended nature, where valid answers to \"How do\nI X?\" range from executable actions to high-level descriptions of X's\nsub-goals, makes them challenging for AI agents to ask, and for AI experts to\nanswer, in ways that support efficient planning. We introduce $How^{2}$, a\nmemory agent framework that enables agents to ask how-to questions, store the\nanswers, and reuse them for lifelong learning in interactive environments. We\nevaluate our approach in Plancraft, a Minecraft crafting environment, where\nagents must complete an assembly task by manipulating inventory items. Using\nteacher models that answer at varying levels of abstraction, from executable\naction sequences to high-level subgoal descriptions, we show that lifelong\nlearning agents benefit most from answers that are abstracted and decoupled\nfrom the current state. $How^{2}$ offers a way for LLM-based agents to improve\ntheir planning capabilities over time by asking questions in interactive\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86How2\u8bb0\u5fc6\u4ee3\u7406\u6846\u67b6\uff0c\u8ba9AI\u4ee3\u7406\u80fd\u591f\u8be2\u95ee\u5982\u4f55\u505a\u7684\u95ee\u9898\u3001\u5b58\u50a8\u7b54\u6848\u5e76\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u91cd\u590d\u4f7f\u7528\u4ee5\u5b9e\u73b0\u7ec8\u8eab\u5b66\u4e60\u3002\u5728Minecraft\u5236\u4f5c\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u53d1\u73b0\u62bd\u8c61\u7a0b\u5ea6\u9ad8\u7684\u7b54\u6848\u5bf9\u4ee3\u7406\u5b66\u4e60\u6700\u6709\u76ca\u3002", "motivation": "\u4ee3\u7406\u5728\u89c4\u5212\u95ee\u9898\u65f6\u9700\u8981\u586b\u8865\u77e5\u8bc6\u7a7a\u767d\uff0c\u4f46\u5982\u4f55\u505a\u95ee\u9898\u7684\u5f00\u653e\u6027\u4f7f\u5f97AI\u4ee3\u7406\u96be\u4ee5\u6709\u6548\u63d0\u95ee\uff0cAI\u4e13\u5bb6\u4e5f\u96be\u4ee5\u9ad8\u6548\u56de\u7b54\u3002\u9700\u8981\u4e00\u79cd\u652f\u6301\u9ad8\u6548\u89c4\u5212\u7684\u95ee\u7b54\u673a\u5236\u3002", "method": "\u5f15\u5165How2\u8bb0\u5fc6\u4ee3\u7406\u6846\u67b6\uff0c\u5728Plancraft\uff08Minecraft\u5236\u4f5c\u73af\u5883\uff09\u4e2d\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u7684\u6559\u5e08\u6a21\u578b\u56de\u7b54\u95ee\u9898\uff0c\u4ece\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\u5230\u9ad8\u7ea7\u5b50\u76ee\u6807\u63cf\u8ff0\u3002", "result": "\u7ec8\u8eab\u5b66\u4e60\u4ee3\u7406\u4ece\u62bd\u8c61\u4e14\u4e0e\u5f53\u524d\u72b6\u6001\u89e3\u8026\u7684\u7b54\u6848\u4e2d\u83b7\u76ca\u6700\u5927\uff0c\u8fd9\u79cd\u6846\u67b6\u80fd\u8ba9\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u901a\u8fc7\u63d0\u95ee\u63d0\u5347\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "How2\u6846\u67b6\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u63d0\u95ee\u6765\u6301\u7eed\u6539\u8fdb\u89c4\u5212\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u62bd\u8c61\u7b54\u6848\u5bf9\u5b66\u4e60\u6548\u679c\u6700\u6709\u5229\u3002", "topic": "agent analysis"}}
{"id": "2510.09898", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09898", "abs": "https://arxiv.org/abs/2510.09898", "authors": ["Hung Phan", "Son Le Vu", "Ali Jannesari"], "title": "Learning Bug Context for PyTorch-to-JAX Translation with LLMs", "comment": null, "summary": "Despite recent progress of large language models (LLMs) on code translation\namong mainstream languages, translating PyTorch to JAX remains nontrivial. The\ntwo libraries, though both embedded in Python, differ in core design, execution\nsemantics, and ecosystem maturity; JAX is newer and comparatively\nunderrepresented in public code, and parallel PyTorch--JAX corpora are limited.\nWeaknesses in existing evaluation further complicate cross-framework\nbenchmarking. We present T2J, a prompt-augmentation framework that strengthens\nLLM-based PyTorch to JAX translation. Our pipeline (i) assembles two PyTorch\nsources -- the problem-solving set from TorchLeet (Aroori & Chien, 2025) and a\nGitHub-derived set from CodeParrot (Wolf et al., 2022) -- and uses GPT-4o-mini\nto produce initial JAX drafts; (ii) engages two professional developers to\niteratively repair those drafts until functional equivalence, yielding a\ncurated fixed-bug dataset of common errors and patches; and (iii) constructs\naugmented prompts that inject structured guidance from these fixes to steer\nlightweight LLMs (e.g., GPT-4o-mini). We also introduce three metrics tailored\nto PyTorch to JAX: T2J CodeTrans Score, T2J FixCost Score (an LLM-based\nestimate of bug-fix effort), and T2J Comparison Score (LLM-as-judge).\nEmpirically, T2J raises GPT-4o-mini performance by up to 10% on CodeBLEU, 50%\non T2J FixCost Score, 1.33 points on T2J CodeTrans Score (0--4 scale), and 100%\non T2J Comparison Score; moreover, the generated code runs up to 2.5x faster\nthan the baseline.", "AI": {"tldr": "T2J\u662f\u4e00\u4e2a\u589e\u5f3a\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u5165\u7ed3\u6784\u5316\u6307\u5bfc\u6765\u6539\u8fdbLLM\u4ecePyTorch\u5230JAX\u7684\u4ee3\u7801\u7ffb\u8bd1\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u8fd0\u884c\u6548\u7387\u3002", "motivation": "PyTorch\u548cJAX\u867d\u7136\u90fd\u5d4c\u5165Python\uff0c\u4f46\u5728\u6838\u5fc3\u8bbe\u8ba1\u3001\u6267\u884c\u8bed\u4e49\u548c\u751f\u6001\u7cfb\u7edf\u6210\u719f\u5ea6\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u4e14JAX\u8f83\u65b0\u3001\u516c\u5f00\u4ee3\u7801\u8f83\u5c11\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u8de8\u6846\u67b6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u5305\u542bTorchLeet\u548cGitHub\u6765\u6e90\u7684PyTorch\u6570\u636e\u96c6\uff0c\u7528GPT-4o-mini\u751f\u6210\u521d\u59cbJAX\u8349\u7a3f\uff0c\u7531\u4e13\u4e1a\u5f00\u53d1\u8005\u8fed\u4ee3\u4fee\u590d\u81f3\u529f\u80fd\u7b49\u4ef7\uff0c\u521b\u5efa\u5305\u542b\u5e38\u89c1\u9519\u8bef\u548c\u4fee\u590d\u7684\u56fa\u5b9a\u9519\u8bef\u6570\u636e\u96c6\uff0c\u5e76\u6784\u5efa\u6ce8\u5165\u7ed3\u6784\u5316\u6307\u5bfc\u7684\u589e\u5f3a\u63d0\u793a\u3002", "result": "T2J\u5c06GPT-4o-mini\u5728CodeBLEU\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u8fbe10%\uff0cT2J\u4fee\u590d\u6210\u672c\u5f97\u5206\u63d0\u534750%\uff0cT2J\u4ee3\u7801\u7ffb\u8bd1\u5f97\u5206\u63d0\u53471.33\u5206\uff080-4\u5206\u5236\uff09\uff0cT2J\u6bd4\u8f83\u5f97\u5206\u63d0\u5347100%\uff0c\u751f\u6210\u4ee3\u7801\u8fd0\u884c\u901f\u5ea6\u6bd4\u57fa\u7ebf\u5feb2.5\u500d\u3002", "conclusion": "T2J\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u6307\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7LLM\u5728PyTorch\u5230JAX\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8de8\u6846\u67b6\u7ffb\u8bd1\u7684\u6311\u6218\u3002", "topic": "code agent"}}
{"id": "2510.11290", "categories": ["cs.AI", "cs.HC", "I.2.6; J.4"], "pdf": "https://arxiv.org/pdf/2510.11290", "abs": "https://arxiv.org/abs/2510.11290", "authors": ["Sheng Jin", "Haoming Wang", "Zhiqi Gao", "Yongbo Yang", "Bao Chunjia", "Chengliang Wang"], "title": "Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics", "comment": "9 pages, 7 figures, EMNLP conference", "summary": "Large language models (LLMs) based Agents are increasingly pivotal in\nsimulating and understanding complex human systems and interactions. We propose\nthe AI-Agent School (AAS) system, built around a self-evolving mechanism that\nleverages agents for simulating complex educational dynamics. Addressing the\nfragmented issues in teaching process modeling and the limitations of agents\nperformance in simulating diverse educational participants, AAS constructs the\nZero-Exp strategy, employs a continuous \"experience-reflection-optimization\"\ncycle, grounded in a dual memory base comprising experience and knowledge bases\nand incorporating short-term and long-term memory components. Through this\nmechanism, agents autonomously evolve via situated interactions within diverse\nsimulated school scenarios. This evolution enables agents to more accurately\nmodel the nuanced, multi-faceted teacher-student engagements and underlying\nlearning processes found in physical schools. Experiment confirms that AAS can\neffectively simulate intricate educational dynamics and is effective in\nfostering advanced agent cognitive abilities, providing a foundational stepping\nstone from the \"Era of Experience\" to the \"Era of Simulation\" by generating\nhigh-fidelity behavioral and interaction data.", "AI": {"tldr": "\u63d0\u51fa\u4e86AI-Agent School\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u8fdb\u5316\u673a\u5236\u6a21\u62df\u590d\u6742\u6559\u80b2\u52a8\u6001\uff0c\u91c7\u7528Zero-Exp\u7b56\u7565\u548c\"\u7ecf\u9a8c-\u53cd\u601d-\u4f18\u5316\"\u5faa\u73af\uff0c\u5728\u6a21\u62df\u5b66\u6821\u573a\u666f\u4e2d\u5b9e\u73b0\u4ee3\u7406\u81ea\u4e3b\u8fdb\u5316\u3002", "motivation": "\u89e3\u51b3\u6559\u5b66\u6d41\u7a0b\u5efa\u6a21\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u4ee5\u53ca\u4ee3\u7406\u5728\u6a21\u62df\u591a\u6837\u5316\u6559\u80b2\u53c2\u4e0e\u8005\u65f6\u7684\u6027\u80fd\u9650\u5236\uff0c\u65e8\u5728\u66f4\u51c6\u786e\u5730\u6a21\u62df\u771f\u5b9e\u5b66\u6821\u4e2d\u7684\u5e08\u751f\u4e92\u52a8\u548c\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u6784\u5efaZero-Exp\u7b56\u7565\uff0c\u91c7\u7528\u6301\u7eed\u7684\u7ecf\u9a8c-\u53cd\u601d-\u4f18\u5316\u5faa\u73af\uff0c\u57fa\u4e8e\u5305\u542b\u7ecf\u9a8c\u548c\u77e5\u8bc6\u5e93\u7684\u53cc\u91cd\u8bb0\u5fc6\u57fa\u7840\uff0c\u6574\u5408\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eAAS\u80fd\u6709\u6548\u6a21\u62df\u590d\u6742\u6559\u80b2\u52a8\u6001\uff0c\u4fc3\u8fdb\u9ad8\u7ea7\u4ee3\u7406\u8ba4\u77e5\u80fd\u529b\u53d1\u5c55\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u884c\u4e3a\u4ea4\u4e92\u6570\u636e\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4ece\"\u7ecf\u9a8c\u65f6\u4ee3\"\u8fc8\u5411\"\u6a21\u62df\u65f6\u4ee3\"\u63d0\u4f9b\u4e86\u57fa\u7840\u652f\u6491\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u62df\u6570\u636e\u63a8\u52a8\u6559\u80b2\u7cfb\u7edf\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2510.10448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10448", "abs": "https://arxiv.org/abs/2510.10448", "authors": ["Zhichao Xu", "Minheng Wang", "Yawei Wang", "Wenqian Ye", "Yuntao Du", "Yunpu Ma", "Yijun Tian"], "title": "RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems trained using reinforcement\nlearning (RL) with reasoning are hampered by inefficient context management,\nwhere long, noisy retrieved documents increase costs and degrade performance.\nWe introduce RECON (REasoning with CONdensation), a framework that integrates\nan explicit summarization module to compress evidence within the reasoning\nloop. Our summarizer is trained via a two-stage process: relevance pretraining\non QA datasets, followed by multi-aspect distillation from proprietary LLMs to\nensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON\nreduces total context length by 35\\%, leading to improved training speed and\ninference latency, while simultaneously improving RAG performance on downstream\nQA benchmarks. Notably, it boosts the average EM score of the 3B model by\n14.5\\% and the 7B model by 3.0\\%, showing particular strength in multi-hop QA.\nRECON demonstrates that learned context compression is essential for building\npractical, scalable, and performant RAG systems. Our code implementation is\nmade available at https://github.com/allfornancy/RECON.", "AI": {"tldr": "RECON\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u663e\u5f0f\u6458\u8981\u6a21\u5757\u538b\u7f29\u68c0\u7d22\u8bc1\u636e\uff0c\u5728\u63a8\u7406\u5faa\u73af\u4e2d\u51cf\u5c11\u4e0a\u4e0b\u6587\u957f\u5ea635%\uff0c\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u6539\u5584RAG\u5728QA\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5b58\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u68c0\u7d22\u7684\u957f\u800c\u5608\u6742\u6587\u6863\u4f1a\u589e\u52a0\u6210\u672c\u5e76\u964d\u4f4e\u6027\u80fd\u3002", "method": "RECON\u6846\u67b6\u96c6\u6210\u663e\u5f0f\u6458\u8981\u6a21\u5757\u538b\u7f29\u8bc1\u636e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5728QA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76f8\u5173\u6027\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4ece\u4e13\u6709LLM\u8fdb\u884c\u591a\u65b9\u9762\u84b8\u998f\u4ee5\u786e\u4fdd\u4e8b\u5b9e\u6027\u548c\u6e05\u6670\u5ea6\u3002", "result": "\u5728Search-R1\u6d41\u6c34\u7ebf\u4e2d\uff0cRECON\u5c06\u603b\u4e0a\u4e0b\u6587\u957f\u5ea6\u51cf\u5c1135%\uff0c3B\u6a21\u578b\u7684\u5e73\u5747EM\u5206\u6570\u63d0\u534714.5%\uff0c7B\u6a21\u578b\u63d0\u53473.0%\uff0c\u5728\u591a\u8df3QA\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u5b66\u4e60\u5230\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u5bf9\u4e8e\u6784\u5efa\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6027\u80fd\u7684RAG\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11457", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11457", "abs": "https://arxiv.org/abs/2510.11457", "authors": ["Beining Wang", "Weihang Su", "Hongtao Tian", "Tao Yang", "Yujia Zhou", "Ting Yao", "Qingyao Ai", "Yiqun Liu"], "title": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization", "comment": null, "summary": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution.", "AI": {"tldr": "\u63d0\u51fa\u7ef4\u5ea6\u7ea7\u5956\u52b1\u6a21\u578b(DRM)\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u3001\u76f8\u5173\u6027\u548c\u8fde\u8d2f\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff0c\u89e3\u51b3\u4f20\u7edf\u7ed3\u679c\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u7a00\u758f\u548c\u8fc7\u7a0b\u7ea7\u5956\u52b1\u6a21\u578b\u7f3a\u4e4f\u6cdb\u5316\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u662f\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u7ed3\u679c\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u53ea\u5956\u52b1\u6b63\u786e\u7b54\u6848\uff0c\u5bb9\u6613\u4f20\u64ad\u9519\u8bef\u63a8\u7406\u4e14\u5956\u52b1\u7a00\u758f\uff1b\u8fc7\u7a0b\u7ea7\u5956\u52b1\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u66f4\u5bc6\u96c6\u7684\u53cd\u9988\uff0c\u4f46\u7f3a\u4e4f\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u7ef4\u5ea6\u7ea7\u5956\u52b1\u6a21\u578b(DRM)\uff0c\u4ece\u4e09\u4e2a\u4e92\u8865\u4e14\u53ef\u89e3\u91ca\u7684\u7ef4\u5ea6\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\uff1a\u7f6e\u4fe1\u5ea6\uff08\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff09\u3001\u76f8\u5173\u6027\uff08\u8bed\u4e49\u5bf9\u9f50\uff09\u548c\u8fde\u8d2f\u6027\uff08\u903b\u8f91\u4e00\u81f4\u6027\uff09\u3002\u8fd9\u4e9b\u7ef4\u5ea6\u65e0\u9700\u771f\u5b9e\u7b54\u6848\u5373\u53ef\u8fdb\u884c\u53ef\u89e3\u91ca\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aDRM\u63d0\u4f9b\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u6307\u5bfcLLMs\u4f18\u5316\u5e76\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002DRM\u76d1\u7763\u8bad\u7ec3\u5728\u6570\u5b66\u3001\u95ee\u7b54\u3001\u4ee3\u7801\u6267\u884c\u548c\u8c1c\u9898\u7b49\u5f00\u653e\u9886\u57df\u4efb\u52a1\u4e0a\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u5305\u62ec\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u4efb\u52a1\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u7684\u63a8\u7406\u8fc7\u7a0b\u76d1\u7763\u53ef\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u6cdb\u5316\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11588", "abs": "https://arxiv.org/abs/2510.11588", "authors": ["Jiateng Liu", "Zhenhailong Wang", "Xiaojiang Huang", "Yingjie Li", "Xing Fan", "Xiang Li", "Chenlei Guo", "Ruhi Sarikaya", "Heng Ji"], "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents", "comment": "42 pages", "summary": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.", "AI": {"tldr": "\u63d0\u51fa\u4e86CC-Gen\u57fa\u51c6\u751f\u6210\u5668\u548cCAP-CPT\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u4ee3\u7406\u7cfb\u7edf\u4e2d\u653f\u7b56\u6587\u6863\u5185\u90e8\u5316\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u653f\u7b56\u89c4\u8303\u65f6\u7684\u63a8\u7406\u56f0\u96be\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u653f\u7b56\u6587\u6863\u8fc5\u901f\u6269\u5c55\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\uff0c\u9700\u8981\u5f00\u53d1\u5c06\u653f\u7b56\u6587\u6863\u5d4c\u5165\u6a21\u578b\u5148\u9a8c\u7684\u5185\u90e8\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u5f15\u5165CC-Gen\u57fa\u51c6\u751f\u6210\u5668\uff0c\u5177\u6709\u56db\u4e2a\u53ef\u63a7\u590d\u6742\u5ea6\u7ea7\u522b\uff1b\u63d0\u51faCAP-CPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u6790\u653f\u7b56\u6587\u6863\u63d0\u53d6\u5173\u952e\u89c4\u8303\uff0c\u5206\u7c7b\u4e3a\u4e8b\u5b9e\u6027\u3001\u884c\u4e3a\u6027\u548c\u6761\u4ef6\u6027\u7c7b\u522b\uff0c\u6307\u5bfc\u6570\u636e\u5408\u6210\u5e76\u901a\u8fc7\u81ea\u56de\u5f52\u9884\u8bad\u7ec3\u635f\u5931\u5b9e\u73b0\u653f\u7b56\u5185\u90e8\u5316\u3002", "result": "CAP-CPT\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u4f18\u4e8eSFT\u57fa\u7ebf\uff0c\u5728Qwen-3-32B\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe41%\u548c22%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728CC-Gen\u4e0a\u5b9e\u73b0\u4e8697.3%\u7684\u63d0\u793a\u957f\u5ea6\u51cf\u5c11\uff0c\u5e76\u5728\u4f7f\u7528\u5c11\u91cfSFT\u6570\u636e\u65f6\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86tau-Bench\u6027\u80fd\u3002", "conclusion": "CAP-CPT\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u548c\u63a8\u7406\u8d1f\u62c5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653f\u7b56\u5185\u90e8\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u653f\u7b56\u89c4\u8303\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2510.10472", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10472", "abs": "https://arxiv.org/abs/2510.10472", "authors": ["Qiran Zou", "Hou Hei Lam", "Wenhao Zhao", "Yiming Tang", "Tingting Chen", "Samson Yu", "Tianyi Zhang", "Chang Liu", "Xiangyang Ji", "Dianbo Liu"], "title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth", "comment": "Our benchmark is available at: https://github.com/qrzou/FML-bench", "summary": "Large language models (LLMs) have sparked growing interest in automatic\nmachine learning research agents. Among them, agents capable of autonomously\nproposing ideas and conducting machine learning experiments are particularly\npromising, as they maximize research automation and accelerate scientific\nprogress by iteratively refining ideas based on experimental results. However,\ncomprehensively evaluating such agents remains challenging. Existing benchmarks\ntend to overemphasize engineering aspects while neglecting academic rigor,\ncreating barriers that obscure a clear assessment of an agent's scientific\ncapabilities in machine learning research. They also suffer from limited task\ndiversity, an overemphasis on application-oriented tasks over fundamental\nresearch problems, and limited scalability to realistic research settings. To\naddress these limitations, we introduce FML-bench, a benchmark designed to\nevaluate automatic machine learning research agents on 8 diverse and\nfundamental machine learning research problems. It reduces coding burden,\nemphasizes fundamental problems rather than specific use cases, offers high\ntask diversity, and is extensible to real-world machine learning GitHub\nrepositories. Furthermore, we present a unified evaluation framework with five\ncomplementary metrics, designed to comprehensively assess agent performance on\nour benchmark. We evaluate state-of-the-art automatic research agents on\nFML-bench, and find that agents employing broad research exploration strategies\noutperform those focusing on narrow but deep exploration. These findings\nsuggest that emphasizing the breadth of exploration may lead to more effective\nresearch outcomes than focusing solely on incremental refinement. Our benchmark\nis available at https://github.com/qrzou/FML-bench.", "AI": {"tldr": "FML-bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4ee3\u7406\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e8\u4e2a\u57fa\u7840\u673a\u5668\u5b66\u4e60\u7814\u7a76\u95ee\u9898\uff0c\u51cf\u5c11\u7f16\u7801\u8d1f\u62c5\uff0c\u5f3a\u8c03\u57fa\u7840\u95ee\u9898\u800c\u975e\u7279\u5b9a\u7528\u4f8b\uff0c\u63d0\u4f9b\u9ad8\u4efb\u52a1\u591a\u6837\u6027\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u771f\u5b9eGitHub\u4ed3\u5e93\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u8fc7\u5ea6\u5f3a\u8c03\u5de5\u7a0b\u65b9\u9762\u800c\u5ffd\u89c6\u5b66\u672f\u4e25\u8c28\u6027\uff0c\u4efb\u52a1\u591a\u6837\u6027\u6709\u9650\uff0c\u8fc7\u4e8e\u5173\u6ce8\u5e94\u7528\u5bfc\u5411\u4efb\u52a1\u800c\u975e\u57fa\u7840\u7814\u7a76\u95ee\u9898\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u771f\u5b9e\u7814\u7a76\u73af\u5883\u3002", "method": "\u5f15\u5165FML-bench\u57fa\u51c6\uff0c\u5305\u542b8\u4e2a\u591a\u6837\u5316\u57fa\u7840\u673a\u5668\u5b66\u4e60\u7814\u7a76\u95ee\u9898\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u4e94\u4e2a\u4e92\u8865\u6307\u6807\u6765\u5168\u9762\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u91c7\u7528\u5e7f\u6cdb\u7814\u7a76\u63a2\u7d22\u7b56\u7565\u7684\u4ee3\u7406\u4f18\u4e8e\u4e13\u6ce8\u4e8e\u72ed\u7a84\u4f46\u6df1\u5165\u63a2\u7d22\u7684\u4ee3\u7406\u3002", "conclusion": "\u5f3a\u8c03\u63a2\u7d22\u5e7f\u5ea6\u53ef\u80fd\u6bd4\u4ec5\u5173\u6ce8\u589e\u91cf\u6539\u8fdb\u4ea7\u751f\u66f4\u6709\u6548\u7684\u7814\u7a76\u7ed3\u679c\u3002", "topic": "swe benchmark"}}
{"id": "2510.11608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11608", "abs": "https://arxiv.org/abs/2510.11608", "authors": ["Shiqi Zhang", "Xinbei Ma", "Yunqing Xu", "Zouying Cao", "Pengrui Lu", "Haobo Yuan", "Tiancheng Shen", "Zhuosheng Zhang", "Hai Zhao", "Ming-Hsuan Yang"], "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems", "comment": null, "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.", "AI": {"tldr": "ParaCook\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u65f6\u95f4\u6548\u7387\u534f\u4f5c\u89c4\u5212\u7684\u57fa\u51c6\uff0c\u57fa\u4e8e\u7b80\u5316\u7248Overcooked\u6e38\u620f\uff0c\u4e13\u6ce8\u4e8e\u5e76\u884c\u548c\u5f02\u6b65\u64cd\u4f5c\u7684\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u5b8c\u6210\u5ea6\uff0c\u4f46\u5ffd\u89c6\u4e86\u5e76\u884c\u548c\u5f02\u6b65\u64cd\u4f5c\u4e2d\u7684\u65f6\u95f4\u6548\u7387\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u65f6\u95f4\u6548\u7387\u89c4\u5212\u80fd\u529b\u3002", "method": "\u57fa\u4e8eOvercooked\u6e38\u620f\u8bbe\u8ba1\u7b80\u5316\u73af\u5883\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u70f9\u996a\u4efb\u52a1\u5b9e\u4f8b\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4ea4\u4e92\u89c4\u5212\u6311\u6218\uff0c\u4e13\u6ce8\u4e8e\u6218\u7565\u5e76\u884c\u89c4\u5212\u7684\u6838\u5fc3\u95ee\u9898\u3002", "result": "\u5bf9\u6700\u5148\u8fdbLLMs\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u65b9\u6cd5\u751f\u6210\u7684\u89c4\u5212\u6b21\u4f18\uff0c\u96be\u4ee5\u5904\u7406\u5e76\u884c\u52a8\u4f5c\u6216\u534f\u8c03\uff0c\u4f46\u5728\u62bd\u8c61\u4efb\u52a1\u4e2dLLMs\u5c55\u73b0\u51fa\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u5e76\u884c\u4f18\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "ParaCook\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u65f6\u95f4\u6548\u7387\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u89c4\u5212\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8c03\u8282\u590d\u6742\u5ea6\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.09976", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09976", "abs": "https://arxiv.org/abs/2510.09976", "authors": ["Mingyang Lyu", "Yinqian Sun", "Erliang Lin", "Huangrui Li", "Ruolin Chen", "Feifei Zhao", "Yi Zeng"], "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\\pi_0$ have\nshown strong generalization by leveraging large-scale demonstrations, yet their\nperformance is still fundamentally constrained by the quality and coverage of\nsupervised data. Reinforcement learning (RL) provides a promising path for\nimproving and fine-tuning VLAs through online interaction. However,\nconventional policy gradient methods are computationally infeasible in the\ncontext of flow-matching based models due to the intractability of the\nimportance sampling process, which requires explicit computation of policy\nratios. To overcome this limitation, we propose Flow Policy Optimization (FPO)\nalgorithm, which reformulates importance sampling by leveraging per-sample\nchanges in the conditional flow-matching objective. Furthermore, FPO achieves\nstable and scalable online reinforcement fine-tuning of the $\\pi_0$ model by\nintegrating structure-aware credit assignment to enhance gradient efficiency,\nclipped surrogate objectives to stabilize optimization, multi-step latent\nexploration to encourage diverse policy updates, and a Q-ensemble mechanism to\nprovide robust value estimation. We evaluate FPO on the LIBERO benchmark and\nthe ALOHA simulation task against supervised, preference-aligned,\ndiffusion-based, autoregressive online RL, and $\\pi_0$-FAST baselines,\nobserving consistent improvements over the imitation prior and strong\nalternatives with stable learning under sparse rewards. In addition, ablation\nstudies and analyses of the latent space dynamics further highlight the\ncontributions of individual components within FPO, validating the effectiveness\nof the proposed computational modules and the stable convergence of the\nconditional flow-matching objective during online RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86Flow Policy Optimization (FPO)\u7b97\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u91cd\u8981\u6027\u91c7\u6837\u548c\u96c6\u6210\u591a\u79cd\u6280\u672f\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6027\u80fd\u53d7\u9650\u4e8e\u76d1\u7763\u6570\u636e\u7684\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4\uff0c\u800c\u4f20\u7edf\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "FPO\u7b97\u6cd5\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u91cd\u8981\u6027\u91c7\u6837\u8fc7\u7a0b\uff0c\u7ed3\u5408\u7ed3\u6784\u611f\u77e5\u4fe1\u7528\u5206\u914d\u3001\u88c1\u526a\u66ff\u4ee3\u76ee\u6807\u3001\u591a\u6b65\u6f5c\u5728\u63a2\u7d22\u548cQ\u96c6\u6210\u673a\u5236\uff0c\u5b9e\u73b0\u7a33\u5b9a\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548cALOHA\u6a21\u62df\u4efb\u52a1\u4e2d\uff0cFPO\u76f8\u6bd4\u76d1\u7763\u5b66\u4e60\u3001\u504f\u597d\u5bf9\u9f50\u3001\u6269\u6563\u6a21\u578b\u7b49\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u6301\u7eed\u6539\u8fdb\uff0c\u5728\u7a00\u758f\u5956\u52b1\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u5b66\u4e60\u3002", "conclusion": "FPO\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u901a\u8fc7\u63d0\u51fa\u7684\u8ba1\u7b97\u6a21\u5757\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6536\u655b\u548c\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11661", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11661", "abs": "https://arxiv.org/abs/2510.11661", "authors": ["Shijie Xia", "Yuhan Sun", "Pengfei Liu"], "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI", "comment": null, "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.", "AI": {"tldr": "SR-Scientist\u6846\u67b6\u5c06LLM\u4ece\u7b80\u5355\u7684\u65b9\u7a0b\u63d0\u8bae\u8005\u63d0\u5347\u4e3a\u81ea\u4e3bAI\u79d1\u5b66\u5bb6\uff0c\u901a\u8fc7\u7f16\u5199\u4ee3\u7801\u5206\u6790\u6570\u636e\u3001\u5b9e\u73b0\u65b9\u7a0b\u3001\u8bc4\u4f30\u53cd\u9988\u5e76\u8fdb\u884c\u4f18\u5316\uff0c\u5728\u56db\u4e2a\u79d1\u5b66\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53476%\u81f335%\u7684\u7edd\u5bf9\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u901a\u5e38\u5c06LLM\u9650\u5236\u4e3a\u9057\u4f20\u7f16\u7a0b\u7b49\u641c\u7d22\u7b97\u6cd5\u4e2d\u7684\u65b9\u7a0b\u63d0\u8bae\u8005\u89d2\u8272\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u79d1\u5b66\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06\u4ee3\u7801\u89e3\u91ca\u5668\u5305\u88c5\u6210\u6570\u636e\u5206\u6790\u548c\u65b9\u7a0b\u8bc4\u4f30\u5de5\u5177\u96c6\uff0c\u8ba9\u4ee3\u7406\u5728\u6700\u5c0f\u4eba\u5de5\u5e72\u9884\u4e0b\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u8fdb\u884c\u957f\u671f\u4f18\u5316\uff0c\u5e76\u5f00\u53d1\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u589e\u5f3a\u4ee3\u7406\u80fd\u529b\u3002", "result": "\u5728\u56db\u4e2a\u79d1\u5b66\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\uff0cSR-Scientist\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u7edd\u5bf9\u63d0\u53476%\u81f335%\uff1b\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u7684\u65b9\u7a0b\u5728\u57df\u5916\u6570\u636e\u4e0a\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u548c\u7b26\u53f7\u51c6\u786e\u6027\u3002", "conclusion": "SR-Scientist\u6210\u529f\u5c06LLM\u63d0\u5347\u4e3a\u81ea\u4e3bAI\u79d1\u5b66\u5bb6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u65b9\u7a0b\u53d1\u73b0\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u566a\u58f0\u73af\u5883\u548c\u6cdb\u5316\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.11694", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11694", "abs": "https://arxiv.org/abs/2510.11694", "authors": ["Arjun Sahney", "Ram Gorthi", "Cezary \u0141astowski", "Javier Vega"], "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering", "comment": "8 pages. No figures. Evaluated on MLE-Benchmark 2025", "summary": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints.", "AI": {"tldr": "Operand Quant\u662f\u4e00\u79cd\u5355\u4ee3\u7406\u3001IDE\u57fa\u7840\u7684\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u67b6\u6784\uff0c\u5728MLE-Benchmark\u4e0a\u521b\u4e0b\u6700\u65b0SOTA\u7ed3\u679c\uff0c\u8bc1\u660e\u5355\u4ee3\u7406\u7cfb\u7edf\u5728\u53d7\u63a7IDE\u73af\u5883\u4e2d\u53ef\u8d85\u8d8a\u591a\u4ee3\u7406\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u591a\u4ee3\u7406\u7f16\u6392\u6846\u67b6\u5b58\u5728\u590d\u6742\u6027\uff0c\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u5355\u4ee3\u7406\u67b6\u6784\u5728\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u5168\u751f\u547d\u5468\u671f\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u5355\u4ee3\u7406\u3001IDE\u57fa\u7840\u7684\u7ebf\u6027\u975e\u963b\u585e\u67b6\u6784\uff0c\u6574\u5408\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u7684\u6240\u6709\u9636\u6bb5\uff1a\u63a2\u7d22\u3001\u5efa\u6a21\u3001\u5b9e\u9a8c\u548c\u90e8\u7f72\u3002", "result": "\u5728MLE-Benchmark\uff082025\uff09\u4e0a\u83b7\u5f970.3956 \u00b1 0.0565\u7684\u603b\u4f53\u5956\u724c\u7387\uff0c\u572875\u4e2a\u95ee\u9898\u4e0a\u521b\u4e0b\u6240\u6709\u8bc4\u4f30\u7cfb\u7edf\u4e2d\u7684\u6700\u9ad8\u6027\u80fd\u8bb0\u5f55\u3002", "conclusion": "\u7ebf\u6027\u975e\u963b\u585e\u7684\u5355\u4ee3\u7406\u5728\u53d7\u63a7IDE\u73af\u5883\u4e2d\u80fd\u591f\u8d85\u8d8a\u591a\u4ee3\u7406\u548c\u7f16\u6392\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5355\u4ee3\u7406\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.10023", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10023", "abs": "https://arxiv.org/abs/2510.10023", "authors": ["Yinghui He", "Abhishek Panigrahi", "Yong Lin", "Sanjeev Arora"], "title": "Skill-Targeted Adaptive Training", "comment": null, "summary": "Language models often show little to no improvement (i.e., \"saturation\") when\ntrained via vanilla supervised fine-tuning (SFT) on data similar to what they\nsaw in their training set (e.g., MATH). We introduce a new fine-tuning\nstrategy, STAT, to train such a student model by using the metacognition\nability of a stronger large language model (LLM) as the teacher. The teacher\nuses the task dataset to create a list of skills needed for the task, and then\nlabels each data point with its required skills (Didolkar et al., 2024). By\nmonitoring the student's answers, the teacher creates a Missing-Skill-Profile\nfor the student, tracking how often they failed to apply each skill in their\nresponses. We use this idea to build a modified training set in one of two\nways. In STAT-Sel, the teacher uses an existing set of training examples but\nadaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,\nthe teacher synthesizes additional examples involving missing skills. Across\nextensive experiments on Llama and Qwen models, our methods yield improvements\nof up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,\nSTAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,\nAMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is\ncomplementary to RL via GRPO (Shao et al., 2024): after the model is improved\nusing STAT to address skill gaps, GRPO continues to add further gains. We\nconclude that skill-targeted adaptive training should broadly improve current\ntraining pipelines. Our code is available at:\nhttps://github.com/princeton-pli/STAT.", "AI": {"tldr": "STAT\u662f\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u66f4\u5f3aLLM\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u4f5c\u4e3a\u6559\u5e08\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u751f\u6a21\u578b\u7684\u6280\u80fd\u7f3a\u5931\u60c5\u51b5\uff0c\u521b\u5efa\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u96c6\u6765\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u5728\u7c7b\u4f3c\u8bad\u7ec3\u6570\u636e\u4e0a\u5f80\u5f80\u51fa\u73b0\u6027\u80fd\u9971\u548c\u73b0\u8c61\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u7a81\u7834\u8fd9\u79cd\u9650\u5236\u3002", "method": "\u4f7f\u7528\u5f3aLLM\u4f5c\u4e3a\u6559\u5e08\uff0c\u5206\u6790\u4efb\u52a1\u6240\u9700\u6280\u80fd\u5e76\u6807\u8bb0\u6570\u636e\u70b9\uff0c\u521b\u5efa\u5b66\u751f\u6a21\u578b\u7684\u7f3a\u5931\u6280\u80fd\u6863\u6848\uff0c\u7136\u540e\u901a\u8fc7STAT-Sel\uff08\u91cd\u52a0\u6743\u73b0\u6709\u8bad\u7ec3\u6837\u672c\uff09\u6216STAT-Syn\uff08\u5408\u6210\u65b0\u6837\u672c\uff09\u4e24\u79cd\u65b9\u5f0f\u6784\u5efa\u9488\u5bf9\u6027\u8bad\u7ec3\u96c6\u3002", "result": "\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\uff0cSTAT\u5728MATH\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u8fbe7.5%\uff0c\u5728\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5e73\u5747\u63d0\u53474.6%\uff0c\u4e14\u4e0eGRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e92\u8865\u3002", "conclusion": "\u6280\u80fd\u5bfc\u5411\u7684\u81ea\u9002\u5e94\u8bad\u7ec3\u5e94\u8be5\u5e7f\u6cdb\u6539\u8fdb\u5f53\u524d\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.10029", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10029", "abs": "https://arxiv.org/abs/2510.10029", "authors": ["Ruoxing Yang"], "title": "Experience-Efficient Model-Free Deep Reinforcement Learning Using Pre-Training", "comment": null, "summary": "We introduce PPOPT - Proximal Policy Optimization using Pretraining, a novel,\nmodel-free deep-reinforcement-learning algorithm that leverages pretraining to\nachieve high training efficiency and stability on very small training samples\nin physics-based environments. Reinforcement learning agents typically rely on\nlarge samples of environment interactions to learn a policy. However, frequent\ninteractions with a (computer-simulated) environment may incur high\ncomputational costs, especially when the environment is complex. Our main\ninnovation is a new policy neural network architecture that consists of a\npretrained neural network middle section sandwiched between two fully-connected\nnetworks. Pretraining part of the network on a different environment with\nsimilar physics will help the agent learn the target environment with high\nefficiency because it will leverage a general understanding of the\ntransferrable physics characteristics from the pretraining environment. We\ndemonstrate that PPOPT outperforms baseline classic PPO on small training\nsamples both in terms of rewards gained and general training stability. While\nPPOPT underperforms against classic model-based methods such as DYNA DDPG, the\nmodel-free nature of PPOPT allows it to train in significantly less time than\nits model-based counterparts. Finally, we present our implementation of PPOPT\nas open-source software, available at github.com/Davidrxyang/PPOPT.", "AI": {"tldr": "PPOPT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\u65e0\u5173\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5728\u7269\u7406\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5c0f\u6837\u672c\u8bad\u7ec3\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u73af\u5883\u4ea4\u4e92\u6837\u672c\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002PPOPT\u65e8\u5728\u901a\u8fc7\u9884\u8bad\u7ec3\u51cf\u5c11\u73af\u5883\u4ea4\u4e92\u9700\u6c42\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u91c7\u7528\u65b0\u578b\u7b56\u7565\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u542b\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u95f4\u5c42\u548c\u4e24\u4e2a\u5168\u8fde\u63a5\u7f51\u7edc\uff0c\u901a\u8fc7\u5728\u76f8\u4f3c\u7269\u7406\u73af\u5883\u4e0a\u9884\u8bad\u7ec3\u6765\u8fc1\u79fb\u7269\u7406\u7279\u6027\u7406\u89e3\u3002", "result": "PPOPT\u5728\u5c0f\u8bad\u7ec3\u6837\u672c\u4e0a\u4f18\u4e8e\u7ecf\u5178PPO\u7b97\u6cd5\uff0c\u5728\u5956\u52b1\u83b7\u53d6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u867d\u7136\u4e0d\u5982DYNA DDPG\u7b49\u6a21\u578b\u65b9\u6cd5\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u66f4\u77ed\u3002", "conclusion": "PPOPT\u901a\u8fc7\u9884\u8bad\u7ec3\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u65e0\u5173\u5f3a\u5316\u5b66\u4e60\uff0c\u7279\u522b\u9002\u5408\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u5c0f\u6837\u672c\u8bad\u7ec3\u573a\u666f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.10661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10661", "abs": "https://arxiv.org/abs/2510.10661", "authors": ["Omid Reza Heidari", "Siobhan Reid", "Yassine Yaakoubi"], "title": "AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation", "comment": "Accepted at NeurIPS 2025, ER \"Efficient Reasoning\" workshop", "summary": "LLMs have advanced text-to-SQL generation, yet monolithic architectures\nstruggle with complex reasoning and schema diversity. We propose AGENTIQL, an\nagent-inspired multi-expert framework that combines a reasoning agent for\nquestion decomposition, a coding agent for sub-query generation, and a\nrefinement step for column selection. An adaptive router further balances\nefficiency and accuracy by selecting between our modular pipeline and a\nbaseline parser. Several steps in the pipeline can be executed in parallel,\nmaking the framework scalable to larger workloads. Evaluated on the Spider\nbenchmark, AGENTIQL improves execution accuracy and interpretability and\nachieves up to 86.07\\% EX with 14B models using the Planner&Executor merging\nstrategy. The attained performance is contingent upon the efficacy of the\nrouting mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)\nwhile using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances\ntransparency by exposing intermediate reasoning steps, offering a robust,\nscalable, and interpretable approach to semantic parsing.", "AI": {"tldr": "AGENTIQL\u662f\u4e00\u4e2a\u591a\u4e13\u5bb6\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u4ee3\u7406\u3001\u7f16\u7801\u4ee3\u7406\u548c\u4f18\u5316\u6b65\u9aa4\u6539\u8fdb\u6587\u672c\u5230SQL\u751f\u6210\uff0c\u5728Spider\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523086.07%\u6267\u884c\u51c6\u786e\u7387\uff0c\u63a5\u8fd1GPT-4\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5355\u4f53\u67b6\u6784\u5728\u590d\u6742\u63a8\u7406\u548c\u591a\u6837\u5316\u6570\u636e\u5e93\u6a21\u5f0f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u6587\u672c\u5230SQL\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u591a\u4e13\u5bb6\u4ee3\u7406\u6846\u67b6\uff1a\u63a8\u7406\u4ee3\u7406\u8fdb\u884c\u95ee\u9898\u5206\u89e3\uff0c\u7f16\u7801\u4ee3\u7406\u751f\u6210\u5b50\u67e5\u8be2\uff0c\u4f18\u5316\u6b65\u9aa4\u8fdb\u884c\u5217\u9009\u62e9\uff0c\u5e76\u5305\u542b\u81ea\u9002\u5e94\u8def\u7531\u5668\u5728\u6a21\u5757\u5316\u6d41\u7a0b\u548c\u57fa\u7ebf\u89e3\u6790\u5668\u95f4\u9009\u62e9\u3002", "result": "\u5728Spider\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u752814B\u6a21\u578b\u8fbe\u523086.07%\u6267\u884c\u51c6\u786e\u7387\uff0c\u63a5\u8fd1GPT-4\u768489.65% SOTA\u6c34\u5e73\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "AGENTIQL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u89e3\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u7f29\u5c0f\u4e86\u4e0eGPT-4\u7684\u5dee\u8ddd\u3002", "topic": "code agent"}}
{"id": "2510.10666", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10666", "abs": "https://arxiv.org/abs/2510.10666", "authors": ["Zhengbo Zhang", "Zhiheng Lyu", "Junhao Gong", "Hongzhu Yi", "Xinming Wang", "Yuxuan Zhou", "Jiabing Yang", "Ping Nie", "Yan Huang", "Wenhu Chen"], "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions", "comment": "10 pages", "summary": "Efficiently solving real-world problems with LLMs increasingly hinges on\ntheir ability to interact with dynamic web environments and autonomously\nacquire external information. While recent research like Search-R1 and\nWebDancer demonstrates strong performance in solving web tasks, they heavily\nrely on additional tools to convert the interactive web environment into static\ntext content. This is in contrast to human browsing behaviors, which involve\ndiverse interactions with the browser, such as scrolling, clicking, and typing.\nIn this paper, we propose BrowserAgent, a more interactive agent that solves\ncomplex tasks through human-inspired browser actions. BrowserAgent operates\ndirectly on raw web pages via Playwright through a set of predefined browser\nactions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and\nRejection Fine-Tuning (RFT)) to improve the model's generalization abilities.\nDespite using significantly less training data than Search-R1, BrowserAgent\nachieves more competitive results across different Open-QA tasks. Additionally,\nwe introduce an explicit memory mechanism to store key conclusions across\nsteps, further enhancing the model's reasoning capabilities for long-horizon\ntasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over\nSearch-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These\nresults indicate that BrowserAgent can serve as a more advanced framework for\nmore interactive and scalable web agents.", "AI": {"tldr": "BrowserAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b\u6d4f\u89c8\u884c\u4e3a\u7684\u4ea4\u4e92\u5f0f\u7f51\u7edc\u4ee3\u7406\uff0c\u901a\u8fc7\u76f4\u63a5\u64cd\u4f5c\u6d4f\u89c8\u5668\u9875\u9762\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u5728\u591a\u9879Open-QA\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eSearch-R1\uff0c\u7279\u522b\u662f\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\u6709\u7ea620%\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Search-R1\u548cWebDancer\u4f9d\u8d56\u989d\u5916\u5de5\u5177\u5c06\u4ea4\u4e92\u5f0f\u7f51\u7edc\u73af\u5883\u8f6c\u6362\u4e3a\u9759\u6001\u6587\u672c\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u591a\u6837\u5316\u7684\u6d4f\u89c8\u5668\u4ea4\u4e92\u884c\u4e3a\uff08\u5982\u6eda\u52a8\u3001\u70b9\u51fb\u3001\u8f93\u5165\uff09\u4e0d\u7b26\u3002", "method": "\u63d0\u51faBrowserAgent\u4ee3\u7406\uff0c\u901a\u8fc7Playwright\u76f4\u63a5\u64cd\u4f5c\u539f\u59cb\u7f51\u9875\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u62d2\u7edd\u5fae\u8c03\uff09\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u663e\u5f0f\u8bb0\u5fc6\u673a\u5236\u5b58\u50a8\u5173\u952e\u7ed3\u8bba\u3002", "result": "\u4f7f\u7528\u6bd4Search-R1\u5c11\u5f97\u591a\u7684\u8bad\u7ec3\u6570\u636e\uff0cBrowserAgent\u5728\u4e0d\u540cOpen-QA\u4efb\u52a1\u4e2d\u53d6\u5f97\u66f4\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5728HotpotQA\u30012Wiki\u548cBamboogle\u7b49\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\u6bd4Search-R1\u63d0\u5347\u7ea620%\u3002", "conclusion": "BrowserAgent\u53ef\u4ee5\u4f5c\u4e3a\u66f4\u5148\u8fdb\u3001\u66f4\u4ea4\u4e92\u5f0f\u548c\u53ef\u6269\u5c55\u7684\u7f51\u7edc\u4ee3\u7406\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.10806", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10806", "abs": "https://arxiv.org/abs/2510.10806", "authors": ["Mihir Gupte", "Paolo Giusto", "Ramesh S"], "title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures", "comment": "Waiting for Conference Response", "summary": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff0c\u5c06\u6811\u72b6\u7ed3\u6784\uff08\u5982GitHub\u4ed3\u5e93\uff09\u7684\u77e5\u8bc6\u7ebf\u6027\u5316\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u5c42\u7ea7\u751f\u6210\u9690\u5f0f\u7684\u805a\u5408\u6458\u8981\uff0c\u7136\u540e\u4e0eRAG\u7ed3\u5408\u4f7f\u7528\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u6700\u597d\u5730\u8868\u793a\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\uff0c\u4ee5\u4fbf\u5728\u7ed3\u6784\u5316\u6570\u636e\uff08\u7279\u522b\u662f\u6811\u72b6\u5c42\u6b21\u7ed3\u6784\uff09\u4e0a\u751f\u6210\u54cd\u5e94\uff0c\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\u7ebf\u6027\u5316\u6811\u72b6\u7ed3\u6784\u77e5\u8bc6\uff0c\u751f\u6210\u9690\u5f0f\u7684\u805a\u5408\u6458\u8981\uff0c\u5b58\u50a8\u5728\u77e5\u8bc6\u5e93\u4e2d\uff0c\u5e76\u4e0eRAG\u76f4\u63a5\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u4e0e\u4f7f\u7528\u539f\u59cb\u975e\u7ed3\u6784\u5316\u4ee3\u7801\u7684RAG\u76f8\u6bd4\uff0c\u54cd\u5e94\u8d28\u91cf\u76f8\u5f53\uff0c\u4f46\u68c0\u7d22\u5668\u4e2d\u7684\u6587\u6863\u6570\u91cf\u51cf\u5c11\u4e8668%\u4ee5\u4e0a\uff0c\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5229\u7528\u9690\u5f0f\u7ebf\u6027\u5316\u77e5\u8bc6\u53ef\u80fd\u662f\u5904\u7406\u590d\u6742\u5c42\u6b21\u6570\u636e\u7ed3\u6784\u7684\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7b56\u7565\u3002", "topic": "swe application"}}
{"id": "2510.10890", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10890", "abs": "https://arxiv.org/abs/2510.10890", "authors": ["Yu Chao", "Siyu Lin", "xiaorong wang", "Zhu Zhang", "Zihan Zhou", "Haoyu Wang", "Shuo Wang", "Jie Zhou", "Zhiyuan Liu", "Maosong Sun"], "title": "LLM$\\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System", "comment": "Accepted by EMNLP2025 System Demonstration", "summary": "We introduce LLM x MapReduce-V3, a hierarchically modular agent system\ndesigned for long-form survey generation. Building on the prior work, LLM x\nMapReduce-V2, this version incorporates a multi-agent architecture where\nindividual functional components, such as skeleton initialization, digest\nconstruction, and skeleton refinement, are implemented as independent\nmodel-context-protocol (MCP) servers. These atomic servers can be aggregated\ninto higher-level servers, creating a hierarchically structured system. A\nhigh-level planner agent dynamically orchestrates the workflow by selecting\nappropriate modules based on their MCP tool descriptions and the execution\nhistory. This modular decomposition facilitates human-in-the-loop intervention,\naffording users greater control and customization over the research process.\nThrough a multi-turn interaction, the system precisely captures the intended\nresearch perspectives to generate a comprehensive skeleton, which is then\ndeveloped into an in-depth survey. Human evaluations demonstrate that our\nsystem surpasses representative baselines in both content depth and length,\nhighlighting the strength of MCP-based modular planning.", "AI": {"tldr": "LLM x MapReduce-V3\u662f\u4e00\u4e2a\u5206\u5c42\u6a21\u5757\u5316\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u957f\u7bc7\u8c03\u7814\u62a5\u544a\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u529f\u80fd\u7ec4\u4ef6\u5b9e\u73b0\u4e3a\u72ec\u7acb\u7684MCP\u670d\u52a1\u5668\uff0c\u5e76\u901a\u8fc7\u9ad8\u5c42\u89c4\u5212\u5668\u52a8\u6001\u7f16\u6392\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u652f\u6301\u4eba\u7c7b\u5728\u73af\u5e72\u9884\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u5bf9\u7814\u7a76\u8fc7\u7a0b\u7684\u66f4\u5927\u63a7\u5236\u548c\u5b9a\u5236\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u8c03\u7814\u62a5\u544a\u7684\u6df1\u5ea6\u548c\u957f\u5ea6\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5c06\u9aa8\u67b6\u521d\u59cb\u5316\u3001\u6458\u8981\u6784\u5efa\u548c\u9aa8\u67b6\u4f18\u5316\u7b49\u529f\u80fd\u7ec4\u4ef6\u5b9e\u73b0\u4e3a\u72ec\u7acb\u7684MCP\u670d\u52a1\u5668\uff0c\u901a\u8fc7\u9ad8\u5c42\u89c4\u5212\u5668\u6839\u636eMCP\u5de5\u5177\u63cf\u8ff0\u548c\u6267\u884c\u5386\u53f2\u52a8\u6001\u9009\u62e9\u6a21\u5757\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u5185\u5bb9\u6df1\u5ea6\u548c\u957f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8eMCP\u7684\u6a21\u5757\u5316\u89c4\u5212\u7684\u4f18\u52bf\u3002", "conclusion": "LLM x MapReduce-V3\u901a\u8fc7\u5206\u5c42\u6a21\u5757\u5316\u8bbe\u8ba1\u548cMCP\u670d\u52a1\u5668\u67b6\u6784\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u7bc7\u8c03\u7814\u62a5\u544a\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.10150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10150", "abs": "https://arxiv.org/abs/2510.10150", "authors": ["Zhezheng Hao", "Hong Wang", "Haoyang Liu", "Jian Luo", "Jiarui Yu", "Hande Dong", "Qiang Lin", "Can Wang", "Jiawei Chen"], "title": "Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective", "comment": null, "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM\nreasoning, its training process poses a critical risk: entropy collapse. This\nphenomenon is a rapid loss of policy diversity, stemming from the\nexploration-exploitation imbalance and leading to a lack of generalization.\nRecent entropy-intervention methods aim to prevent \\coloredtext{entropy\ncollapse}, yet their underlying mechanisms remain unclear. In this paper, we\nconduct a quantitative analysis to reveal token-level entropy changes and how\nexisting entropy intervention methods help avoid entropy collapse. Our findings\npoint out a fundamental limitation of existing methods: they attempt to control\nentropy dynamics indirectly. By only affecting related factors, such as the\nadvantage signal and generation probability, their effectiveness is inherently\nlimited and could potentially fail. To address this limitation, we introduce an\nentropy-change-aware reweighting scheme, namely Stabilizing Token-level\nEntropy-changE via Reweighting (STEER), that adaptively stabilizes entropy\ndynamics through fine-grained token-level adjustments. Our approach mitigates\nover-exploitation while fostering robust exploration. Extensive experiments\ndemonstrate that STEER significantly mitigates entropy collapse, stabilizes\nentropy dynamics, and achieves stronger downstream performance across various\nmathematical reasoning benchmarks \\footnote{Our code is available at\nhttps://github.com/zz-haooo/STEER.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u51fa\u4e86STEER\u65b9\u6cd5\u901a\u8fc7token\u7ea7\u71b5\u53d8\u5316\u611f\u77e5\u7684\u91cd\u65b0\u52a0\u6743\u65b9\u6848\u6765\u7a33\u5b9a\u71b5\u52a8\u6001\uff0c\u9632\u6b62\u7b56\u7565\u591a\u6837\u6027\u4e27\u5931\u3002", "motivation": "RLVR\u8bad\u7ec3\u4e2d\u5b58\u5728\u71b5\u5d29\u6e83\u98ce\u9669\uff0c\u5373\u7b56\u7565\u591a\u6837\u6027\u5feb\u901f\u4e27\u5931\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u71b5\u5e72\u9884\u65b9\u6cd5\u673a\u5236\u4e0d\u660e\u786e\u4e14\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faSTEER\u65b9\u6cd5\uff0c\u901a\u8fc7token\u7ea7\u71b5\u53d8\u5316\u611f\u77e5\u7684\u91cd\u65b0\u52a0\u6743\u65b9\u6848\uff0c\u81ea\u9002\u5e94\u7a33\u5b9a\u71b5\u52a8\u6001\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6token\u7ea7\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSTEER\u663e\u8457\u7f13\u89e3\u71b5\u5d29\u6e83\uff0c\u7a33\u5b9a\u71b5\u52a8\u6001\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u66f4\u5f3a\u7684\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "STEER\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u71b5\u52a8\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u9632\u6b62\u8fc7\u5229\u7528\u7684\u540c\u65f6\u4fc3\u8fdb\u9c81\u68d2\u63a2\u7d22\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.10232", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10232", "abs": "https://arxiv.org/abs/2510.10232", "authors": ["Xuening Wu", "Shenqin Yin", "Yanlan Kang", "Xinhang Zhang", "Qianya Xu", "Zeping Chen", "Wenqiang Zhang"], "title": "SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification", "comment": null, "summary": "Recursive self-modification is increasingly central in AutoML, neural\narchitecture search, and adaptive optimization, yet no existing framework\nensures that such changes are made safely. Godel machines offer a principled\nsafeguard by requiring formal proofs of improvement before rewriting code;\nhowever, such proofs are unattainable in stochastic, high-dimensional settings.\nWe introduce the Statistical Godel Machine (SGM), the first statistical safety\nlayer for recursive edits. SGM replaces proof-based requirements with\nstatistical confidence tests (e-values, Hoeffding bounds), admitting a\nmodification only when superiority is certified at a chosen confidence level,\nwhile allocating a global error budget to bound cumulative risk across\nrounds.We also propose Confirm-Triggered Harmonic Spending (CTHS), which\nindexes spending by confirmation events rather than rounds, concentrating the\nerror budget on promising edits while preserving familywise\nvalidity.Experiments across supervised learning, reinforcement learning, and\nblack-box optimization validate this role: SGM certifies genuine gains on\nCIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates\nrobustness on RL and optimization benchmarks.Together, these results position\nSGM as foundational infrastructure for continual, risk-aware self-modification\nin learning systems.Code is available at:\nhttps://github.com/gravitywavelet/sgm-anon.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u8ba1\u54e5\u5fb7\u5c14\u673a(SGM)\uff0c\u9996\u4e2a\u7528\u4e8e\u9012\u5f52\u7f16\u8f91\u7684\u7edf\u8ba1\u5b89\u5168\u5c42\uff0c\u7528\u7edf\u8ba1\u7f6e\u4fe1\u6d4b\u8bd5\u66ff\u4ee3\u8bc1\u660e\u8981\u6c42\uff0c\u5728\u9009\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u4e0b\u8ba4\u8bc1\u6539\u8fdb\uff0c\u540c\u65f6\u5206\u914d\u5168\u5c40\u9519\u8bef\u9884\u7b97\u6765\u9650\u5236\u7d2f\u79ef\u98ce\u9669\u3002", "motivation": "\u9012\u5f52\u81ea\u4fee\u6539\u5728AutoML\u3001\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u548c\u81ea\u9002\u5e94\u4f18\u5316\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u65e0\u6cd5\u786e\u4fdd\u6b64\u7c7b\u66f4\u6539\u7684\u5b89\u5168\u6027\u3002\u54e5\u5fb7\u5c14\u673a\u901a\u8fc7\u8981\u6c42\u5f62\u5f0f\u5316\u6539\u8fdb\u8bc1\u660e\u6765\u63d0\u4f9b\u539f\u5219\u6027\u4fdd\u969c\uff0c\u4f46\u5728\u968f\u673a\u9ad8\u7ef4\u73af\u5883\u4e2d\u6b64\u7c7b\u8bc1\u660e\u4e0d\u53ef\u884c\u3002", "method": "SGM\u7528\u7edf\u8ba1\u7f6e\u4fe1\u6d4b\u8bd5\uff08e\u503c\u3001Hoeffding\u754c\uff09\u66ff\u4ee3\u57fa\u4e8e\u8bc1\u660e\u7684\u8981\u6c42\uff0c\u4ec5\u5728\u9009\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u4e0b\u8ba4\u8bc1\u6539\u8fdb\u65f6\u624d\u5141\u8bb8\u4fee\u6539\uff0c\u540c\u65f6\u5206\u914d\u5168\u5c40\u9519\u8bef\u9884\u7b97\u3002\u63d0\u51fa\u786e\u8ba4\u89e6\u53d1\u8c10\u6ce2\u652f\u51fa(CTHS)\uff0c\u5c06\u652f\u51fa\u7d22\u5f15\u5230\u786e\u8ba4\u4e8b\u4ef6\u800c\u975e\u8f6e\u6b21\uff0c\u5728\u4fdd\u6301\u65cf\u6709\u6548\u6027\u7684\u540c\u65f6\u5c06\u9519\u8bef\u9884\u7b97\u96c6\u4e2d\u5728\u6709\u5e0c\u671b\u7684\u7f16\u8f91\u4e0a\u3002", "result": "\u5728\u76d1\u7763\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u9ed1\u76d2\u4f18\u5316\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86SGM\u7684\u4f5c\u7528\uff1a\u5728CIFAR-100\u4e0a\u8ba4\u8bc1\u771f\u5b9e\u589e\u76ca\uff0c\u5728ImageNet-100\u4e0a\u62d2\u7edd\u865a\u5047\u6539\u8fdb\uff0c\u5728RL\u548c\u4f18\u5316\u57fa\u51c6\u4e0a\u5c55\u793a\u9c81\u68d2\u6027\u3002", "conclusion": "SGM\u4e3a\u5b66\u4e60\u7cfb\u7edf\u4e2d\u6301\u7eed\u3001\u98ce\u9669\u611f\u77e5\u7684\u81ea\u4fee\u6539\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u57fa\u7840\u8bbe\u65bd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.10974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10974", "abs": "https://arxiv.org/abs/2510.10974", "authors": ["Zhiwen Ruan", "Yixia Li", "He Zhu", "Yun Chen", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning", "comment": null, "summary": "Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)\nas a key method to adapt pre-trained models to domain-specific tasks such as\nmathematical reasoning. However, standard SFT uniformly penalizes all tokens,\nneglecting that only a small subset of critical tokens determines reasoning\ncorrectness. This uniform supervision often causes reduced output diversity and\nlimited generalization. We propose Critical Token Fine-tuning (CFT), a simple\nyet effective approach that updates only tokens identified as functionally\nindispensable via counterfactual perturbations. By focusing gradient signals on\nthese decisive reasoning steps while preserving the diversity of non-critical\ntokens, CFT can enhance both generation and diversity. Extensive experiments on\nfive models across three families (Qwen, OLMo, LLaMA) and eleven mathematical\nreasoning benchmarks show that CFT, despite fine-tuning on less than 12% of\ntokens, consistently outperforms standard SFT. Moreover, CFT enables test-time\nscaling through improved sampling diversity and provides a stronger\ninitialization for reinforcement learning, sustaining performance gains in\nlater training stages while maintaining higher entropy for better exploration.\nThese results highlight CFT as a practical and general framework for efficient\nand robust LLM fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5173\u952e\u4ee4\u724c\u5fae\u8c03(CFT)\u65b9\u6cd5\uff0c\u4ec5\u66f4\u65b0\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6270\u52a8\u8bc6\u522b\u51fa\u7684\u529f\u80fd\u5173\u952e\u4ee4\u724c\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6807\u51c6\u76d1\u7763\u5fae\u8c03(SFT)\uff0c\u5c3d\u7ba1\u53ea\u5fae\u8c03\u4e0d\u523012%\u7684\u4ee4\u724c\u3002", "motivation": "\u6807\u51c6SFT\u5bf9\u6240\u6709\u4ee4\u724c\u8fdb\u884c\u7edf\u4e00\u60e9\u7f5a\uff0c\u5ffd\u7565\u4e86\u53ea\u6709\u5c11\u6570\u5173\u952e\u4ee4\u724c\u51b3\u5b9a\u63a8\u7406\u6b63\u786e\u6027\uff0c\u5bfc\u81f4\u8f93\u51fa\u591a\u6837\u6027\u51cf\u5c11\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "CFT\u65b9\u6cd5\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6270\u52a8\u8bc6\u522b\u529f\u80fd\u5173\u952e\u4ee4\u724c\uff0c\u4ec5\u5bf9\u8fd9\u4e9b\u51b3\u5b9a\u6027\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u68af\u5ea6\u66f4\u65b0\uff0c\u540c\u65f6\u4fdd\u7559\u975e\u5173\u952e\u4ee4\u724c\u7684\u591a\u6837\u6027\u3002", "result": "\u57285\u4e2a\u6a21\u578b\u30013\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c11\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCFT\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6SFT\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u5e76\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u521d\u59cb\u5316\u3002", "conclusion": "CFT\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6548\u548c\u9c81\u68d2\u7684LLM\u5fae\u8c03\uff0c\u901a\u8fc7\u6539\u8fdb\u91c7\u6837\u591a\u6837\u6027\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.10276", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10276", "abs": "https://arxiv.org/abs/2510.10276", "authors": ["Nikolaus Salvatore", "Hao Wang", "Qiong Zhang"], "title": "Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs", "comment": null, "summary": "The performance of Large Language Models (LLMs) often degrades when crucial\ninformation is in the middle of a long context, a \"lost-in-the-middle\"\nphenomenon that mirrors the primacy and recency effects in human memory. We\npropose that this behavior is not simply a flaw indicative of information loss\nbut an adaptation to different information retrieval demands during\npre-training: some tasks require uniform recall across the entire input (a\nlong-term memory demand), while others prioritize the most recent information\n(a short-term memory demand). Consistent with this view, we show that this\nU-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are\ntrained from scratch on two simple human memory paradigms simulating long-term\nand short-term memory demands. Our analysis reveals that while the recency\neffect directly aligns with short-term memory demand in the training data, the\nprimacy effect is induced by the uniform long-term memory demand and is\nadditionally influenced by the model's autoregressive properties and the\nformation of attention sinks. Our main findings from simple human memory\nparadigms also generalize to a sequence completion task, which more closely\nresembles the next-token prediction process in LLM pre-training. Together, our\nfindings reveal how information retrieval demands, model architecture, and\nstructural attention dynamics during model training can jointly produce\npositional bias observed in LLMs.", "AI": {"tldr": "LLMs\u5728\u957f\u6587\u672c\u4e2d\u8868\u73b0\u51fa'\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931'\u73b0\u8c61\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u8bb0\u5fc6\u7684\u9996\u56e0\u548c\u8fd1\u56e0\u6548\u5e94\u7c7b\u4f3c\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cdU\u5f62\u6027\u80fd\u66f2\u7ebf\u662fLLMs\u9002\u5e94\u4e0d\u540c\u4fe1\u606f\u68c0\u7d22\u9700\u6c42\u7684\u8bad\u7ec3\u7ed3\u679c\uff0c\u800c\u975e\u7b80\u5355\u7684\u4fe1\u606f\u4e22\u5931\u7f3a\u9677\u3002", "motivation": "\u89e3\u91caLLMs\u5728\u957f\u6587\u672c\u5904\u7406\u4e2d\u4e3a\u4f55\u51fa\u73b0\u4e2d\u95f4\u4fe1\u606f\u6027\u80fd\u4e0b\u964d\u7684\u73b0\u8c61\uff0c\u63a2\u7d22\u8fd9\u79cd\u73b0\u8c61\u662f\u5426\u53cd\u6620\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u8bb0\u5fc6\u7684\u673a\u5236\u3002", "method": "\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3GPT-2\u548cLlama\u53d8\u4f53\u6a21\u578b\uff0c\u4f7f\u7528\u6a21\u62df\u4eba\u7c7b\u957f\u671f\u548c\u77ed\u671f\u8bb0\u5fc6\u9700\u6c42\u7684\u7b80\u5355\u8303\u5f0f\uff0c\u5206\u6790\u4f4d\u7f6e\u504f\u5dee\u7684\u5f62\u6210\u673a\u5236\u3002", "result": "\u53d1\u73b0U\u5f62\u6027\u80fd\u66f2\u7ebf\u786e\u5b9e\u5728\u8bad\u7ec3\u4e2d\u51fa\u73b0\uff0c\u8fd1\u56e0\u6548\u5e94\u4e0e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u77ed\u671f\u8bb0\u5fc6\u9700\u6c42\u76f4\u63a5\u5bf9\u5e94\uff0c\u800c\u9996\u56e0\u6548\u5e94\u7531\u5747\u5300\u7684\u957f\u671f\u8bb0\u5fc6\u9700\u6c42\u8bf1\u5bfc\uff0c\u5e76\u53d7\u81ea\u56de\u5f52\u7279\u6027\u548c\u6ce8\u610f\u529b\u6c47\u805a\u70b9\u5f62\u6210\u7684\u5f71\u54cd\u3002", "conclusion": "LLMs\u4e2d\u7684\u4f4d\u7f6e\u504f\u5dee\u662f\u4fe1\u606f\u68c0\u7d22\u9700\u6c42\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7ed3\u6784\u6ce8\u610f\u529b\u52a8\u6001\u5171\u540c\u4f5c\u7528\u7684\u7ed3\u679c\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u66f4\u63a5\u8fd1LLM\u9884\u8bad\u7ec3\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u4e2d\u4e5f\u5f97\u5230\u9a8c\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2510.10994", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10994", "abs": "https://arxiv.org/abs/2510.10994", "authors": ["Wei-Chieh Huang", "Henry Peng Zou", "Yaozu Wu", "Dongyuan Li", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Angelo Zangari", "Jizhou Guo", "Chunyu Miao", "Liancheng Fang", "Langzhou He", "Renhe Jiang", "Philip S. Yu"], "title": "DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety", "comment": null, "summary": "Deep research frameworks have shown promising capabilities in synthesizing\ncomprehensive reports from web sources. While deep research possesses\nsignificant potential to address complex issues through planning and research\ncycles, existing frameworks are deficient in sufficient evaluation procedures\nand stage-specific protections. They typically treat evaluation as exact match\naccuracy of question-answering, but overlook crucial aspects of report quality\nsuch as credibility, coherence, breadth, depth, and safety. This oversight may\nresult in hazardous or malicious sources being integrated into the final\nreport. To address these issues, we introduce DEEPRESEARCHGUARD, a\ncomprehensive framework featuring four-stage safeguards with open-domain\nevaluation of references and reports. We assess performance across multiple\nmetrics, e.g., defense success rate and over-refusal rate, and five key report\ndimensions. In the absence of a suitable safety benchmark, we introduce\nDRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation\nspans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,\nDeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success\nrate improvement of 18.16% while reducing over-refusal rate by 6%. The input\nguard provides the most substantial early-stage protection by filtering out\nobvious risks, while the plan and research guards enhance citation discipline\nand source credibility. Through extensive experiments, we show that\nDEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware\ndefenses that effectively block harmful content propagation, while\nsystematically improving report quality without excessive over-refusal rates.\nThe code can be found via https://github.com/Jasonya/DeepResearchGuard.", "AI": {"tldr": "\u63d0\u51fa\u4e86DEEPRESEARCHGUARD\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u5b89\u5168\u9632\u62a4\u673a\u5236\u89e3\u51b3\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u5728\u62a5\u544a\u8d28\u91cf\u8bc4\u4f30\u548c\u5b89\u5168\u9632\u62a4\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u9632\u5fa1\u6210\u529f\u7387\u5e76\u964d\u4f4e\u8fc7\u5ea6\u62d2\u7edd\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u7f3a\u4e4f\u5145\u5206\u7684\u8bc4\u4f30\u7a0b\u5e8f\u548c\u9636\u6bb5\u7279\u5b9a\u9632\u62a4\uff0c\u901a\u5e38\u53ea\u5173\u6ce8\u95ee\u7b54\u51c6\u786e\u7387\u800c\u5ffd\u89c6\u62a5\u544a\u8d28\u91cf\u7684\u5173\u952e\u7ef4\u5ea6\uff08\u53ef\u4fe1\u5ea6\u3001\u8fde\u8d2f\u6027\u3001\u5e7f\u5ea6\u3001\u6df1\u5ea6\u3001\u5b89\u5168\u6027\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u5bb3\u5185\u5bb9\u88ab\u6574\u5408\u5230\u6700\u7ec8\u62a5\u544a\u4e2d\u3002", "method": "\u5f15\u5165DEEPRESEARCHGUARD\u6846\u67b6\uff0c\u5305\u542b\u56db\u9636\u6bb5\u5b89\u5168\u9632\u62a4\u673a\u5236\uff0c\u5e76\u5f00\u53d1DRSAFEBENCH\u57fa\u51c6\u6d4b\u8bd5\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u5148\u8fdbLLM\u6a21\u578b\uff0c\u5305\u62ecGPT-4o\u3001Gemini-2.5-flash\u7b49\u3002", "result": "DEEPRESEARCHGUARD\u5e73\u5747\u9632\u5fa1\u6210\u529f\u7387\u63d0\u534718.16%\uff0c\u540c\u65f6\u964d\u4f4e\u8fc7\u5ea6\u62d2\u7edd\u73876%\u3002\u8f93\u5165\u9632\u62a4\u63d0\u4f9b\u6700\u663e\u8457\u7684\u65e9\u671f\u4fdd\u62a4\uff0c\u8ba1\u5212\u548c\u7814\u7a76\u9632\u62a4\u589e\u5f3a\u5f15\u7528\u7eaa\u5f8b\u548c\u6765\u6e90\u53ef\u4fe1\u5ea6\u3002", "conclusion": "DEEPRESEARCHGUARD\u80fd\u591f\u6709\u6548\u963b\u6b62\u6709\u5bb3\u5185\u5bb9\u4f20\u64ad\uff0c\u7cfb\u7edf\u6027\u5730\u63d0\u9ad8\u62a5\u544a\u8d28\u91cf\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u9ad8\u7684\u8fc7\u5ea6\u62d2\u7edd\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.10304", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10304", "abs": "https://arxiv.org/abs/2510.10304", "authors": ["Michael Y. Hu", "Benjamin Van Durme", "Jacob Andreas", "Harsh Jhamtani"], "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting", "comment": null, "summary": "Language model (LM) agents deployed in novel environments often exhibit poor\nsample efficiency when learning from sequential interactions. This\nsignificantly hinders the usefulness of such agents in environments where\ninteraction is costly (for example, when they interact with humans or reset\nphysical systems). While a number of existing LM agent architectures\nincorporate various mechanisms for experience storage and reflection, they make\nlimited use of LMs' abilities to directly generate or reason about full\ncounterfactual trajectories. We introduce ECHO (Experience Consolidation via\nHindsight Optimization), a prompting framework that adapts hindsight experience\nreplay from reinforcement learning for language model agents. ECHO generates\noptimized trajectories for alternative goals that could have been achieved\nduring failed attempts, effectively creating synthetic positive examples from\nunsuccessful interactions. Our approach consists of two components: a hindsight\nrule that uses the language model itself to identify relevant subgoals and\ngenerate optimized trajectories, and an update rule that maintains compressed\ntrajectory representations in memory. We evaluate ECHO on stateful versions of\nXMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a\ncollaborative information-gathering enterprise simulation. Across both domains,\nECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,\nit also outperforms a number of sophisticated agent architectures including\nReflexion and AWM, demonstrating faster adaptation to novel environments\nthrough more effective utilization of past experiences.", "AI": {"tldr": "ECHO\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u6846\u67b6\u7684\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u4e8b\u540e\u7ecf\u9a8c\u56de\u653e\u6280\u672f\u751f\u6210\u4f18\u5316\u7684\u53cd\u4e8b\u5b9e\u8f68\u8ff9\uff0c\u4ece\u5931\u8d25\u7684\u4ea4\u4e92\u4e2d\u521b\u5efa\u5408\u6210\u6b63\u4f8b\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u65b0\u73af\u5883\u4e2d\u5b66\u4e60\u65f6\u6837\u672c\u6548\u7387\u4f4e\u4e0b\uff0c\u8fd9\u5728\u4ea4\u4e92\u6210\u672c\u9ad8\u7684\u73af\u5883\u4e2d\uff08\u5982\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u6216\u91cd\u7f6e\u7269\u7406\u7cfb\u7edf\uff09\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002\u73b0\u6709\u67b6\u6784\u5bf9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u63a8\u7406\u5b8c\u6574\u53cd\u4e8b\u5b9e\u8f68\u8ff9\u7684\u80fd\u529b\u5229\u7528\u6709\u9650\u3002", "method": "ECHO\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a\u4e8b\u540e\u89c4\u5219\uff08\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u76f8\u5173\u5b50\u76ee\u6807\u5e76\u751f\u6210\u4f18\u5316\u8f68\u8ff9\uff09\u548c\u66f4\u65b0\u89c4\u5219\uff08\u5728\u5185\u5b58\u4e2d\u7ef4\u62a4\u538b\u7f29\u7684\u8f68\u8ff9\u8868\u793a\uff09\u3002\u8be5\u65b9\u6cd5\u5c06\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e8b\u540e\u7ecf\u9a8c\u56de\u653e\u6280\u672f\u9002\u914d\u5230\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4e2d\u3002", "result": "\u5728XMiniGrid\u548cPeopleJoinQA\u4e24\u4e2a\u9886\u57df\u7684\u8bc4\u4f30\u4e2d\uff0cECHO\u6bd4\u666e\u901a\u8bed\u8a00\u4ee3\u7406\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe80%\uff1b\u5728XMiniGrid\u4e2d\uff0c\u8fd8\u4f18\u4e8eReflexion\u548cAWM\u7b49\u590d\u6742\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u66f4\u6709\u6548\u5730\u5229\u7528\u8fc7\u53bb\u7ecf\u9a8c\u5b9e\u73b0\u4e86\u5bf9\u65b0\u73af\u5883\u7684\u66f4\u5feb\u9002\u5e94\u3002", "conclusion": "ECHO\u6846\u67b6\u901a\u8fc7\u4e8b\u540e\u4f18\u5316\u751f\u6210\u5408\u6210\u6b63\u4f8b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u4ea4\u4e92\u6210\u672c\u9ad8\u73af\u5883\u4e2d\u7684\u6837\u672c\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11104", "abs": "https://arxiv.org/abs/2510.11104", "authors": ["Junjie Lu", "Yuliang Liu", "Chaofeng Qu", "Wei Shen", "Zhouhan Lin", "Min Xu"], "title": "Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization", "comment": "13 pages", "summary": "Current approaches for strengthening LLM reasoning tend to introduce a\ntraining bias toward human-like reasoning trajectories. In step-wise preference\noptimization, in particular, dependence on human or higher-capacity model\nannotations for intermediate steps limits exploration of alternative,\nnon-human-like reasoning paths and thus constrains achievable performance.\nFurthermore, through a small-scale pilot study, we observed that in\napproximately 75% of cases, the model's first erroneous step occurs after the\nlowest-confidence point. This suggests that guiding the model at its\nlowest-confidence point before an error provides more accurate supervision than\nlocating the first explicit error. In this paper, we propose Confidence-Guided\nReasoning Path Preference Optimization (CGPO), a method that leverages a\nconfidence signal to identify points of maximal uncertainty in the model's\nreasoning process and applies self-generated, non-human-like reasoning-path\nguidance to mitigate trajectory drift. Our experiments span diverse models\napplied to both code and mathematical reasoning tasks. The results show that,\nwith the same amount of training data, our method using data generated by a\nsmall model can achieve better performance in most cases compared with\napproaches using data generated by a strong model or human-annotated.", "AI": {"tldr": "\u63d0\u51faCGPO\u65b9\u6cd5\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u8bc6\u522b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6700\u5927\u4e0d\u786e\u5b9a\u6027\u70b9\uff0c\u5e94\u7528\u81ea\u751f\u6210\u7684\u975e\u4eba\u7c7b\u63a8\u7406\u8def\u5f84\u6307\u5bfc\u6765\u7f13\u89e3\u8f68\u8ff9\u6f02\u79fb\uff0c\u5728\u4ee3\u7801\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f7f\u7528\u5f3a\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316LLM\u63a8\u7406\u7684\u65b9\u6cd5\u503e\u5411\u4e8e\u5f15\u5165\u5bf9\u4eba\u7c7b\u63a8\u7406\u8f68\u8ff9\u7684\u8bad\u7ec3\u504f\u89c1\uff0c\u4f9d\u8d56\u4eba\u7c7b\u6216\u9ad8\u80fd\u529b\u6a21\u578b\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u7684\u6807\u6ce8\u9650\u5236\u4e86\u63a2\u7d22\u66ff\u4ee3\u6027\u975e\u4eba\u7c7b\u63a8\u7406\u8def\u5f84\uff0c\u4ece\u800c\u7ea6\u675f\u4e86\u53ef\u5b9e\u73b0\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u63a8\u7406\u8def\u5f84\u504f\u597d\u4f18\u5316(CGPO)\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u8bc6\u522b\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6700\u5927\u4e0d\u786e\u5b9a\u6027\u70b9\uff0c\u5e76\u5e94\u7528\u81ea\u751f\u6210\u7684\u975e\u4eba\u7c7b\u63a8\u7406\u8def\u5f84\u6307\u5bfc\u6765\u7f13\u89e3\u8f68\u8ff9\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u6570\u636e\u91cf\u4e0b\uff0c\u4f7f\u7528\u5c0f\u6a21\u578b\u751f\u6210\u6570\u636e\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6bd4\u4f7f\u7528\u5f3a\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "CGPO\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u6700\u5927\u4e0d\u786e\u5b9a\u6027\u70b9\u5e76\u63d0\u4f9b\u81ea\u751f\u6210\u7684\u63a8\u7406\u8def\u5f84\u6307\u5bfc\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11151", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11151", "abs": "https://arxiv.org/abs/2510.11151", "authors": ["Alexander Sternfeld", "Andrei Kucharavy", "Ljiljana Dolamic"], "title": "TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code", "comment": null, "summary": "Large language Models (LLMs) have shown remarkable proficiency in code\ngeneration tasks across various programming languages. However, their outputs\noften contain subtle but critical vulnerabilities, posing significant risks\nwhen deployed in security-sensitive or mission-critical systems. This paper\nintroduces TypePilot, an agentic AI framework designed to enhance the security\nand robustness of LLM-generated code by leveraging strongly typed and\nverifiable languages, using Scala as a representative example. We evaluate the\neffectiveness of our approach in two settings: formal verification with the\nStainless framework and general-purpose secure code generation. Our experiments\nwith leading open-source LLMs reveal that while direct code generation often\nfails to enforce safety constraints, just as naive prompting for more secure\ncode, our type-focused agentic pipeline substantially mitigates input\nvalidation and injection vulnerabilities. The results demonstrate the potential\nof structured, type-guided LLM workflows to improve the SotA of the\ntrustworthiness of automated code generation in high-assurance domains.", "AI": {"tldr": "TypePilot\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u7c7b\u578b\u8bed\u8a00\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7Scala\u8bed\u8a00\u589e\u5f3aLLM\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u663e\u8457\u51cf\u5c11\u8f93\u5165\u9a8c\u8bc1\u548c\u6ce8\u5165\u6f0f\u6d1e\u3002", "motivation": "LLM\u751f\u6210\u7684\u4ee3\u7801\u7ecf\u5e38\u5305\u542b\u5fae\u5999\u4f46\u5173\u952e\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5728\u5b89\u5168\u654f\u611f\u7cfb\u7edf\u4e2d\u90e8\u7f72\u5b58\u5728\u91cd\u5927\u98ce\u9669\u3002", "method": "\u4f7f\u7528Scala\u4f5c\u4e3a\u4ee3\u8868\u8bed\u8a00\uff0c\u7ed3\u5408Stainless\u6846\u67b6\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u6784\u5efa\u7c7b\u578b\u5bfc\u5411\u7684\u4ee3\u7406\u5316\u6d41\u6c34\u7ebf\u6765\u589e\u5f3a\u4ee3\u7801\u5b89\u5168\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u5f80\u5f80\u65e0\u6cd5\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\uff0c\u800c\u7c7b\u578b\u5bfc\u5411\u7684\u4ee3\u7406\u5316\u6d41\u6c34\u7ebf\u663e\u8457\u51cf\u8f7b\u4e86\u8f93\u5165\u9a8c\u8bc1\u548c\u6ce8\u5165\u6f0f\u6d1e\u3002", "conclusion": "\u7ed3\u6784\u5316\u3001\u7c7b\u578b\u5f15\u5bfc\u7684LLM\u5de5\u4f5c\u6d41\u7a0b\u6709\u6f5c\u529b\u63d0\u9ad8\u9ad8\u4fdd\u8bc1\u9886\u57df\u4e2d\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u7684\u53ef\u4fe1\u5ea6\u3002", "topic": "code agent"}}
{"id": "2510.11221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11221", "abs": "https://arxiv.org/abs/2510.11221", "authors": ["Tao Li", "Jinlong Hu", "Yang Wang", "Junfeng Liu", "Xuejun Liu"], "title": "WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent", "comment": null, "summary": "LLM-brained web agents offer powerful capabilities for web automation but\nface a critical cost-performance trade-off. The challenge is amplified by web\nagents' inherently complex prompts that include goals, action histories, and\nenvironmental states, leading to degraded LLM ensemble performance. To address\nthis, we introduce WebRouter, a novel query-specific router trained from an\ninformation-theoretic perspective. Our core contribution is a cost-aware\nVariational Information Bottleneck (ca-VIB) objective, which learns a\ncompressed representation of the input prompt while explicitly penalizing the\nexpected operational cost. Experiments on five real-world websites from the\nWebVoyager benchmark show that WebRouter reduces operational costs by a\nstriking 87.8\\% compared to a GPT-4o baseline, while incurring only a 3.8\\%\naccuracy drop.", "AI": {"tldr": "WebRouter\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u67e5\u8be2\u7279\u5b9a\u8def\u7531\u5668\uff0c\u901a\u8fc7\u6210\u672c\u611f\u77e5\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u76ee\u6807\u6765\u538b\u7f29\u8f93\u5165\u63d0\u793a\uff0c\u663e\u8457\u964d\u4f4eLLM\u9a71\u52a8\u7684Web\u4ee3\u7406\u7684\u8fd0\u8425\u6210\u672c\u3002", "motivation": "LLM\u9a71\u52a8\u7684Web\u4ee3\u7406\u5728Web\u81ea\u52a8\u5316\u4e2d\u9762\u4e34\u6210\u672c\u4e0e\u6027\u80fd\u7684\u6743\u8861\u6311\u6218\uff0c\u590d\u6742\u7684\u63d0\u793a\u5305\u542b\u76ee\u6807\u3001\u884c\u52a8\u5386\u53f2\u548c\u72b6\u6001\u4fe1\u606f\uff0c\u5bfc\u81f4LLM\u96c6\u6210\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faWebRouter\uff0c\u91c7\u7528\u6210\u672c\u611f\u77e5\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u76ee\u6807\uff0c\u5b66\u4e60\u8f93\u5165\u63d0\u793a\u7684\u538b\u7f29\u8868\u793a\uff0c\u540c\u65f6\u660e\u786e\u60e9\u7f5a\u9884\u671f\u8fd0\u8425\u6210\u672c\u3002", "result": "\u5728WebVoyager\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e94\u4e2a\u771f\u5b9e\u7f51\u7ad9\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cWebRouter\u76f8\u6bd4GPT-4o\u57fa\u7ebf\u964d\u4f4e\u4e8687.8%\u7684\u8fd0\u8425\u6210\u672c\uff0c\u4ec5\u635f\u59313.8%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "WebRouter\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86Web\u4ee3\u7406\u7684\u6210\u672c\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6210\u672c\u8282\u7701\u548c\u826f\u597d\u7684\u6027\u80fd\u4fdd\u6301\u3002", "topic": "agent analysis"}}
{"id": "2510.11225", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11225", "abs": "https://arxiv.org/abs/2510.11225", "authors": ["Hayate Funakura", "Hyunsoo Kim", "Koji Mineshima"], "title": "A Theorem-Proving-Based Evaluation of Neural Semantic Parsing", "comment": "Accepted to BlackboxNLP 2025", "summary": "Graph-matching metrics such as Smatch are the de facto standard for\nevaluating neural semantic parsers, yet they capture surface overlap rather\nthan logical equivalence. We reassess evaluation by pairing graph-matching with\nautomated theorem proving. We compare two approaches to building parsers:\nsupervised fine-tuning (T5-Small/Base) and few-shot in-context learning\n(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs\nusing graph-matching, bidirectional entailment between source and target\nformulas with a first-order logic theorem prover, and well-formedness. Across\nsettings, we find that models performing well on graph-matching often fail to\nproduce logically equivalent formulas. Normalization reduces incidental target\nvariability, improves well-formedness, and strengthens logical adequacy. Error\nanalysis shows performance degrades with increasing formula complexity and with\ncoordination, prepositional phrases, and passive voice; the dominant failures\ninvolve variable binding and indexing, and predicate naming. These findings\nhighlight limits of graph-based metrics for reasoning-oriented applications and\nmotivate logic-sensitive evaluation and training objectives together with\nsimplified, normalized target representations. All code and data for our\nexperiments are publicly available.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u8bc4\u4f30\u8bed\u4e49\u89e3\u6790\u5668\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u56fe\u5339\u914d\u4e0e\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u76f8\u7ed3\u5408\uff0c\u53d1\u73b0\u57fa\u4e8e\u56fe\u5339\u914d\u7684\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u903b\u8f91\u7b49\u4ef7\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u903b\u8f91\u654f\u611f\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u8bed\u4e49\u89e3\u6790\u5668\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u56fe\u5339\u914d\u6307\u6807\uff08\u5982Smatch\uff09\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u53ea\u6355\u6349\u8868\u9762\u91cd\u53e0\u800c\u975e\u903b\u8f91\u7b49\u4ef7\u6027\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u652f\u6301\u63a8\u7406\u5bfc\u5411\u7684\u5e94\u7528\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd\u89e3\u6790\u5668\u6784\u5efa\u65b9\u6cd5\uff1a\u76d1\u7763\u5fae\u8c03\uff08T5-Small/Base\uff09\u548c\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08GPT-4o/4.1/5\uff09\uff0c\u4f7f\u7528\u56fe\u5339\u914d\u3001\u53cc\u5411\u8574\u6db5\u5b9a\u7406\u8bc1\u660e\u548c\u683c\u5f0f\u826f\u597d\u6027\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u5728\u56fe\u5339\u914d\u4e0a\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u7ecf\u5e38\u65e0\u6cd5\u4ea7\u751f\u903b\u8f91\u7b49\u4ef7\u7684\u516c\u5f0f\u3002\u5f52\u4e00\u5316\u51cf\u5c11\u4e86\u76ee\u6807\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u4e86\u683c\u5f0f\u826f\u597d\u6027\u548c\u903b\u8f91\u5145\u5206\u6027\u3002\u9519\u8bef\u5206\u6790\u663e\u793a\u6027\u80fd\u968f\u516c\u5f0f\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\u3002", "conclusion": "\u56fe\u5339\u914d\u6307\u6807\u5728\u63a8\u7406\u5bfc\u5411\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u903b\u8f91\u654f\u611f\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u76ee\u6807\uff0c\u4ee5\u53ca\u7b80\u5316\u7684\u5f52\u4e00\u5316\u76ee\u6807\u8868\u793a\u3002", "topic": "agent analysis"}}
{"id": "2510.10541", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10541", "abs": "https://arxiv.org/abs/2510.10541", "authors": ["Zihan Chen", "Yiming Zhang", "Hengguang Zhou", "Zenghui Ding", "Yining Sun", "Cho-Jui Hsieh"], "title": "Rethinking RL Evaluation: Can Benchmarks Truly Reveal Failures of RL Methods?", "comment": null, "summary": "Current benchmarks are inadequate for evaluating progress in reinforcement\nlearning (RL) for large language models (LLMs).Despite recent benchmark gains\nreported for RL, we find that training on these benchmarks' training sets\nachieves nearly the same performance as training directly on the test sets,\nsuggesting that the benchmarks cannot reliably separate further progress.To\nstudy this phenomenon, we introduce a diagnostic suite and the Oracle\nPerformance Gap (OPG) metric that quantifies the performance difference between\ntraining on the train split versus the test split of a benchmark. We further\nanalyze this phenomenon with stress tests and find that, despite strong\nbenchmark scores, existing RL methods struggle to generalize across\ndistribution shifts, varying levels of difficulty, and counterfactual\nscenarios: shortcomings that current benchmarks fail to reveal.We conclude that\ncurrent benchmarks are insufficient for evaluating generalization and propose\nthree core principles for designing more faithful benchmarks: sufficient\ndifficulty, balanced evaluation, and distributional robustness.", "AI": {"tldr": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u5bf9\u8bc4\u4f30LLM\u8fdb\u5c55\u4e0d\u8db3\uff0c\u63d0\u51faOracle\u6027\u80fd\u5dee\u8ddd\u6307\u6807\u548c\u8bca\u65ad\u5957\u4ef6\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5efa\u8bae\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u539f\u5219\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u8fdb\u5c55\uff0c\u56e0\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u6027\u80fd\u51e0\u4e4e\u76f8\u540c\uff0c\u65e0\u6cd5\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u8bca\u65ad\u5957\u4ef6\u548cOracle\u6027\u80fd\u5dee\u8ddd(OPG)\u6307\u6807\uff0c\u901a\u8fc7\u538b\u529b\u6d4b\u8bd5\u5206\u6790\u73b0\u6709RL\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u5c3d\u7ba1\u57fa\u51c6\u5206\u6570\u9ad8\uff0c\u4f46\u73b0\u6709RL\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u3001\u96be\u5ea6\u53d8\u5316\u548c\u53cd\u4e8b\u5b9e\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "conclusion": "\u5f53\u524d\u57fa\u51c6\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e09\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff1a\u8db3\u591f\u96be\u5ea6\u3001\u5e73\u8861\u8bc4\u4f30\u548c\u5206\u5e03\u9c81\u68d2\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.10586", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10586", "abs": "https://arxiv.org/abs/2510.10586", "authors": ["Giulio Ruffini"], "title": "Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents", "comment": "Submitted to NeurReps 2025 (https://www.neurreps.org)", "summary": "In the algorithmic (Kolmogorov) view, agents are programs that track and\ncompress sensory streams using generative programs. We propose a framework\nwhere the relevant structural prior is simplicity (Solomonoff) understood as\n\\emph{compositional symmetry}: natural streams are well described by (local)\nactions of finite-parameter Lie pseudogroups on geometrically and topologically\ncomplex low-dimensional configuration manifolds (latent spaces). Modeling the\nagent as a generic neural dynamical system coupled to such streams, we show\nthat accurate world-tracking imposes (i) \\emph{structural constraints} --\nequivariance of the agent's constitutive equations and readouts -- and (ii)\n\\emph{dynamical constraints}: under static inputs, symmetry induces conserved\nquantities (Noether-style labels) in the agent dynamics and confines\ntrajectories to reduced invariant manifolds; under slow drift, these manifolds\nmove but remain low-dimensional. This yields a hierarchy of reduced manifolds\naligned with the compositional factorization of the pseudogroup, providing a\ngeometric account of the ``blessing of compositionality'' in deep models. We\nconnect these ideas to the Spencer formalism for Lie pseudogroups and formulate\na symmetry-based, self-contained version of predictive coding in which higher\nlayers receive only \\emph{coarse-grained residual transformations}\n(prediction-error coordinates) along symmetry directions unresolved at lower\nlayers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ec4\u5408\u5bf9\u79f0\u6027\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u611f\u5b98\u6d41\u5efa\u6a21\u4e3a\u6709\u9650\u53c2\u6570\u674e\u4f2a\u7fa4\u5728\u4f4e\u7ef4\u914d\u7f6e\u6d41\u5f62\u4e0a\u7684\u4f5c\u7528\uff0c\u5e76\u5206\u6790\u4e86\u667a\u80fd\u4f53\u52a8\u529b\u5b66\u4e2d\u7684\u7ed3\u6784\u7ea6\u675f\u548c\u5b88\u6052\u91cf\u3002", "motivation": "\u4ece\u7b97\u6cd5\u89c6\u89d2\u7406\u89e3\u667a\u80fd\u4f53\u5982\u4f55\u901a\u8fc7\u751f\u6210\u7a0b\u5e8f\u6765\u8ddf\u8e2a\u548c\u538b\u7f29\u611f\u5b98\u6d41\uff0c\u63a2\u7d22\u7ec4\u5408\u5bf9\u79f0\u6027\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u5728\u667a\u80fd\u4f53\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u5efa\u6a21\u4e3a\u4e0e\u611f\u5b98\u6d41\u8026\u5408\u7684\u795e\u7ecf\u52a8\u529b\u7cfb\u7edf\uff0c\u5206\u6790\u5bf9\u79f0\u6027\u5bf9\u667a\u80fd\u4f53\u6784\u6210\u65b9\u7a0b\u548c\u8bfb\u51fa\u7684\u7b49\u53d8\u6027\u7ea6\u675f\uff0c\u4ee5\u53ca\u5728\u9759\u6001\u8f93\u5165\u548c\u7f13\u6162\u6f02\u79fb\u4e0b\u7684\u52a8\u529b\u5b66\u7ea6\u675f\u3002", "result": "\u8bc1\u660e\u4e86\u51c6\u786e\u7684\u4e16\u754c\u8ddf\u8e2a\u8981\u6c42\u667a\u80fd\u4f53\u5177\u6709\u7b49\u53d8\u6027\u7ed3\u6784\uff0c\u5e76\u4ea7\u751f\u5b88\u6052\u91cf\u548c\u4f4e\u7ef4\u4e0d\u53d8\u6d41\u5f62\uff0c\u8fd9\u4e9b\u6d41\u5f62\u4e0e\u4f2a\u7fa4\u7684\u7ec4\u5408\u5206\u89e3\u5bf9\u9f50\u3002", "conclusion": "\u7ec4\u5408\u5bf9\u79f0\u6027\u4e3a\u6df1\u5ea6\u6a21\u578b\u4e2d\u7684\u7ec4\u5408\u6027\u63d0\u4f9b\u4e86\u51e0\u4f55\u89e3\u91ca\uff0c\u5e76\u542f\u53d1\u4e86\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u9884\u6d4b\u7f16\u7801\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.11370", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11370", "abs": "https://arxiv.org/abs/2510.11370", "authors": ["Wenhan Ma", "Hailin Zhang", "Liang Zhao", "Yifan Song", "Yudong Wang", "Zhifang Sui", "Fuli Luo"], "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.", "AI": {"tldr": "\u63d0\u51faRollout Routing Replay (R3)\u65b9\u6cd5\u89e3\u51b3MoE\u6a21\u578b\u4e2d\u8def\u7531\u673a\u5236\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u8bb0\u5f55\u63a8\u7406\u9636\u6bb5\u7684\u8def\u7531\u5206\u5e03\u5e76\u5728\u8bad\u7ec3\u4e2d\u91cd\u653e\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3-\u63a8\u7406\u7b56\u7565\u5dee\u5f02\u3002", "motivation": "MoE\u6a21\u578b\u4e2d\u7684\u8def\u7531\u673a\u5236\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u4f1a\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\uff0c\u751a\u81f3\u5bfc\u81f4\u707e\u96be\u6027\u8bad\u7ec3\u5d29\u6e83\u3002\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5b58\u5728\u663e\u8457\u7684\u8def\u7531\u884c\u4e3a\u5dee\u5f02\uff0c\u5373\u4f7f\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\uff0c\u8def\u7531\u6846\u67b6\u4e5f\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u4e13\u5bb6\u9009\u62e9\u3002", "method": "\u63d0\u51faR3\u65b9\u6cd5\uff1a\u8bb0\u5f55\u63a8\u7406\u5f15\u64ce\u4e2d\u7684\u8def\u7531\u5206\u5e03\uff0c\u5e76\u5728\u8bad\u7ec3\u9636\u6bb5\u91cd\u653e\u8fd9\u4e9b\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u5c11\u8bad\u7ec3-\u63a8\u7406\u7b56\u7565\u7684KL\u6563\u5ea6\uff0c\u7f13\u89e3\u6781\u7aef\u5dee\u5f02\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u8bad\u7ec3\u901f\u5ea6\u3002", "result": "\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\uff0cR3\u6210\u529f\u7a33\u5b9a\u4e86RL\u8bad\u7ec3\uff0c\u9632\u6b62\u4e86\u5d29\u6e83\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86GSPO\u548cTIS\u7b49\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7a33\u5b9aMoE\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11434", "abs": "https://arxiv.org/abs/2510.11434", "authors": ["Dana Sotto Porat", "Ella Rabinovich"], "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content", "comment": "ECAI2025 (Identity-Aware AI workshop)", "summary": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f9d\u8d56\u81ea\u8bc4\u95ee\u5377\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4f7f\u7528\u81ea\u52a8\u4eba\u683c\u548c\u6027\u522b\u5206\u7c7b\u5668\u5206\u6790LLM\u5728Reddit\u5f00\u653e\u6027\u95ee\u9898\u56de\u590d\u4e2d\u7684\u4eba\u683c\u7279\u5f81\uff0c\u53d1\u73b0LLM\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b9c\u4eba\u6027\u548c\u66f4\u4f4e\u7684\u795e\u7ecf\u8d28\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5728\u5176\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u683c\u548c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u4eba\u683c\u548c\u6027\u522b\u5206\u7c7b\u5668\u5206\u6790LLM\u5728Reddit\u5f00\u653e\u6027\u95ee\u9898\u4e0a\u7684\u56de\u590d\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4f5c\u8005\u7684\u56de\u7b54\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u53d1\u73b0LLM\u7cfb\u7edf\u6027\u5730\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b9c\u4eba\u6027\u548c\u66f4\u4f4e\u7684\u795e\u7ecf\u8d28\uff0c\u53cd\u6620\u4e86\u5408\u4f5c\u548c\u7a33\u5b9a\u7684\u5bf9\u8bdd\u503e\u5411\uff1b\u6027\u522b\u5316\u8bed\u8a00\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u4f5c\u8005\u76f8\u4f3c\u4f46\u53d8\u5f02\u8f83\u5c0f\u3002", "conclusion": "LLM\u5728\u4eba\u683c\u7279\u5f81\u4e0a\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4e3a\u7406\u89e3\u751f\u6210\u5f0fAI\u7684\u4eba\u683c\u548c\u4eba\u53e3\u7edf\u8ba1\u6a21\u5f0f\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2510.11620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11620", "abs": "https://arxiv.org/abs/2510.11620", "authors": ["Siheng Xiong", "Ali Payani", "Faramarz Fekri"], "title": "Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation", "comment": null, "summary": "Inference-time scaling enhances the reasoning ability of a language model\n(LM) by extending its chain-of-thought (CoT). However, existing approaches\ntypically generate the entire reasoning chain in a single forward pass, which\noften leads to CoT derailment, i.e., the reasoning trajectory drifting off\ncourse due to compounding errors. This problem is particularly severe for\nsmaller LMs with long CoTs due to their limited capacity. To address this, we\nanalyze raw long CoTs and uncover a reasoning hierarchy consisting of planning\nand execution steps. Our analysis reveals that most reasoning errors stem from\nincorrect planning. Motivated by this observation, we propose Multi-Path Plan\nAggregation (MPPA), a framework that augments single-pass reasoning with plan\nexploration and aggregation. Following a variable interval schedule based on\nthe token position, MPPA generates multiple candidate plans and aggregates them\ninto a refined planning step. To maintain efficiency, we adopt a minimal design\nin which the base LM serves as the primary policy, while a lightweight LoRA\nmodule implements the plan aggregation policy. We further observe that\noutcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K\ntokens). To overcome this, we introduce online Step-DPO, a process-level\npreference optimization scheme that leverages Twisted Sequential Monte Carlo\n(TSMC) to provide scalable stepwise supervision using small LMs. This yields\nmore efficient training, improved stability, and higher accuracy. Extensive\nexperiments on challenging math, science, and logical reasoning benchmarks\ndemonstrate that, with only 10% SFT data and 5% of preference pairs, our method\noutperforms both the DeepSeek-R1 distillation baseline and the outcome-reward\nRL baseline across multiple base models and tasks.", "AI": {"tldr": "\u63d0\u51faMPPA\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u8ba1\u5212\u805a\u5408\u548c\u5728\u7ebfStep-DPO\u65b9\u6cd5\u89e3\u51b3\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u89c4\u5212\u9519\u8bef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u751f\u6210\u6574\u4e2a\u63a8\u7406\u94fe\uff0c\u5bb9\u6613\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u504f\u79bb\uff0c\u7279\u522b\u662f\u5c0f\u6a21\u578b\u5728\u5904\u7406\u957f\u63a8\u7406\u94fe\u65f6\u7531\u4e8e\u5bb9\u91cf\u9650\u5236\u66f4\u5bb9\u6613\u51fa\u73b0\u89c4\u5212\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u591a\u8def\u5f84\u8ba1\u5212\u805a\u5408(MPPA)\u6846\u67b6\uff0c\u57fa\u4e8etoken\u4f4d\u7f6e\u7684\u53ef\u53d8\u95f4\u9694\u751f\u6210\u591a\u4e2a\u5019\u9009\u8ba1\u5212\u5e76\u805a\u5408\uff1b\u5f15\u5165\u5728\u7ebfStep-DPO\uff0c\u4f7f\u7528\u626d\u66f2\u5e8f\u5217\u8499\u7279\u5361\u6d1b(TSMC)\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9010\u6b65\u76d1\u7763\u3002", "result": "\u5728\u6570\u5b66\u3001\u79d1\u5b66\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u752810%\u7684SFT\u6570\u636e\u548c5%\u7684\u504f\u597d\u5bf9\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8eDeepSeek-R1\u84b8\u998f\u57fa\u7ebf\u548c\u7ed3\u679c\u5956\u52b1RL\u57fa\u7ebf\u3002", "conclusion": "MPPA\u6846\u67b6\u901a\u8fc7\u8ba1\u5212\u63a2\u7d22\u548c\u805a\u5408\u6709\u6548\u89e3\u51b3\u4e86\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u89c4\u5212\u9519\u8bef\u95ee\u9898\uff0c\u7ed3\u5408\u5728\u7ebfStep-DPO\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.11695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11695", "abs": "https://arxiv.org/abs/2510.11695", "authors": ["Lingfei Qian", "Xueqing Peng", "Yan Wang", "Vincent Jim Zhang", "Huan He", "Hanley Smith", "Yi Han", "Yueru He", "Haohang Li", "Yupeng Cao", "Yangyang Yu", "Alejandro Lopez-Lira", "Peng Lu", "Jian-Yun Nie", "Guojun Xiong", "Jimin Huang", "Sophia Ananiadou"], "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents", "comment": null, "summary": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7ec8\u8eab\u5b9e\u65f6\u57fa\u51c6AMA\uff0c\u7528\u4e8e\u5728\u591a\u5e02\u573a\u4e2d\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u4ea4\u6613\u4ee3\u7406\uff0c\u6574\u5408\u4e86\u9a8c\u8bc1\u4ea4\u6613\u6570\u636e\u3001\u4e13\u5bb6\u68c0\u67e5\u65b0\u95fb\u548c\u591a\u6837\u5316\u4ee3\u7406\u67b6\u6784\uff0c\u5728\u52a0\u5bc6\u8d27\u5e01\u548c\u80a1\u7968\u5e02\u573a\u7684\u5b9e\u65f6\u5b9e\u9a8c\u663e\u793a\u4ee3\u7406\u6846\u67b6\u6bd4\u6a21\u578b\u9aa8\u5e72\u5bf9\u7ed3\u679c\u5dee\u5f02\u8d21\u732e\u66f4\u5927\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u6d4b\u8bd5\u6a21\u578b\u800c\u975e\u4ee3\u7406\u3001\u8986\u76d6\u671f\u548c\u8d44\u4ea7\u6709\u9650\u3001\u4f9d\u8d56\u672a\u9a8c\u8bc1\u6570\u636e\u7684\u95ee\u9898\uff0c\u5efa\u7acb\u516c\u5e73\u3001\u8fde\u7eed\u7684\u5b9e\u65f6\u6bd4\u8f83\u6846\u67b6\u3002", "method": "\u5f15\u5165Agent Market Arena (AMA)\u57fa\u51c6\uff0c\u96c6\u6210\u9a8c\u8bc1\u4ea4\u6613\u6570\u636e\u3001\u4e13\u5bb6\u68c0\u67e5\u65b0\u95fb\u548c\u591a\u6837\u5316\u4ee3\u7406\u67b6\u6784\uff0c\u5b9e\u73b0\u56db\u79cd\u4ee3\u7406\uff08InvestorAgent\u3001TradeAgent\u3001HedgeFundAgent\u3001DeepFundAgent\uff09\u5e76\u5728\u591a\u4e2aLLM\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u65f6\u5b9e\u9a8c\u663e\u793a\u4ee3\u7406\u6846\u67b6\u8868\u73b0\u51fa\u660e\u663e\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ece\u6fc0\u8fdb\u98ce\u9669\u627f\u62c5\u5230\u4fdd\u5b88\u51b3\u7b56\uff0c\u800c\u6a21\u578b\u9aa8\u5e72\u5bf9\u7ed3\u679c\u5dee\u5f02\u8d21\u732e\u8f83\u5c0f\u3002", "conclusion": "AMA\u4e3a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7684\u91d1\u878d\u63a8\u7406\u548c\u4ea4\u6613\u667a\u80fd\u8bc4\u4f30\u5efa\u7acb\u4e86\u4e25\u8c28\u3001\u53ef\u590d\u73b0\u548c\u6301\u7eed\u6f14\u8fdb\u7684\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.11701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11701", "abs": "https://arxiv.org/abs/2510.11701", "authors": ["Zhaochen Yu", "Ling Yang", "Jiaru Zou", "Shuicheng Yan", "Mengdi Wang"], "title": "Demystifying Reinforcement Learning in Agentic Reasoning", "comment": "Code and models: https://github.com/Gen-Verse/Open-AgentRL", "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff0c\u53d1\u73b0\u771f\u5b9e\u7aef\u5230\u7aef\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u3001\u63a2\u7d22\u53cb\u597d\u6280\u672f\u4ee5\u53ca\u5ba1\u614e\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u548c\u6700\u4f73\u5b9e\u8df5\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4ece\u6570\u636e\u3001\u7b97\u6cd5\u548c\u63a8\u7406\u6a21\u5f0f\u4e09\u4e2a\u5173\u952e\u89d2\u5ea6\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff0c\u4f7f\u7528\u771f\u5b9e\u7aef\u5230\u7aef\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u3001\u63a2\u7d22\u53cb\u597d\u6280\u672f\uff08\u5982clip higher\u3001\u5956\u52b1\u5851\u5f62\u3001\u4fdd\u6301\u7b56\u7565\u71b5\uff09\u4ee5\u53ca\u5ba1\u614e\u63a8\u7406\u7b56\u7565\u3002", "result": "\u8fd9\u4e9b\u7b80\u5355\u5b9e\u8df5\u80fd\u6301\u7eed\u589e\u5f3a\u667a\u80fd\u4f53\u63a8\u7406\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u5f3a\u52b2\u7ed3\u679c\uff0c4B\u6a21\u578b\u4e5f\u80fd\u8fbe\u523032B\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u5efa\u7acb\u4e86\u5b9e\u7528\u7684\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u8d21\u732e\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11713", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11713", "abs": "https://arxiv.org/abs/2510.11713", "authors": ["Tsung-Han Wu", "Mihran Miroyan", "David M. Chan", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "title": "Are Large Reasoning Models Interruptible?", "comment": "Project Page: https://dynamic-lm.github.io/", "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u9759\u6001\u8bc4\u4f30\u4e2d\u7684\"\u51bb\u7ed3\u4e16\u754c\"\u5047\u8bbe\uff0c\u901a\u8fc7\u4e2d\u65ad\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u4e24\u79cd\u73b0\u5b9e\u573a\u666f\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u9759\u6001\u8bc4\u4f30\u4f1a\u9ad8\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e0b\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4e0b\u964d\u9ad8\u8fbe60%\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u5047\u8bbe\u6a21\u578b\u54cd\u5e94\u662f\u5373\u65f6\u7684\u4e14\u4e0a\u4e0b\u6587\u5728\u54cd\u5e94\u671f\u95f4\u4e0d\u53d8\uff0c\u4f46\u73b0\u4ee3\u63a8\u7406\u4efb\u52a1\uff08\u5982\u8f85\u52a9\u7f16\u7a0b\uff09\u4e2d\u6a21\u578b\u53ef\u80fd\u9700\u8981\u6570\u5c0f\u65f6\u601d\u8003\uff0c\u4ee3\u7801\u5728\u6b64\u671f\u95f4\u4f1a\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\"\u51bb\u7ed3\u4e16\u754c\"\u5047\u8bbe\u4e0d\u518d\u9002\u7528\u3002", "method": "\u5728\u9700\u8981\u957f\u5f62\u5f0f\u63a8\u7406\u7684\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e24\u79cd\u52a8\u6001\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff1a\u4e2d\u65ad\uff08\u6d4b\u8bd5\u6a21\u578b\u5728\u6709\u9650\u9884\u7b97\u4e0b\u7684\u90e8\u5206\u8f93\u51fa\u8d28\u91cf\uff09\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\uff08\u6d4b\u8bd5\u6a21\u578b\u5bf9\u98de\u884c\u4e2d\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\uff09\u3002", "result": "\u9759\u6001\u8bc4\u4f30\u4e00\u81f4\u6027\u5730\u9ad8\u4f30\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff1a\u5373\u4f7f\u5728\u9759\u6001\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\u7684\u6700\u5148\u8fdbLRM\uff0c\u5728\u4e2d\u65ad\u6216\u66b4\u9732\u4e8e\u53d8\u5316\u4e0a\u4e0b\u6587\u65f6\u4e5f\u4f1a\u4e0d\u53ef\u9884\u6d4b\u5730\u5931\u8d25\uff0c\u5f53\u66f4\u65b0\u5728\u63a8\u7406\u8fc7\u7a0b\u540e\u671f\u5f15\u5165\u65f6\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe60%\u3002", "conclusion": "\u63ed\u793a\u4e86\u51e0\u4e2a\u65b0\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u63a8\u7406\u6cc4\u6f0f\uff08\u6a21\u578b\u5728\u4e2d\u65ad\u65f6\u5c06\u63a8\u7406\u6298\u53e0\u5230\u6700\u7ec8\u7b54\u6848\u4e2d\uff09\u3001\u6050\u614c\uff08\u5728\u65f6\u95f4\u538b\u529b\u4e0b\u6a21\u578b\u5b8c\u5168\u653e\u5f03\u63a8\u7406\u5e76\u8fd4\u56de\u9519\u8bef\u7b54\u6848\uff09\u548c\u81ea\u6211\u6000\u7591\uff08\u5728\u6574\u5408\u66f4\u65b0\u4fe1\u606f\u65f6\u6027\u80fd\u4e0b\u964d\uff09\u3002", "topic": "agent analysis"}}
{"id": "2510.10937", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10937", "abs": "https://arxiv.org/abs/2510.10937", "authors": ["Qizhou Peng", "Yang Zheng", "Yu Wen", "Yanna Wu", "Yingying Du"], "title": "Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems", "comment": null, "summary": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u65b9\u5f00\u653e\u7cfb\u7edf\u4e2d\u65e0\u9700\u76f4\u63a5\u4e0e\u53d7\u5bb3\u8005\u667a\u80fd\u4f53\u4ea4\u4e92\u6216\u5b8c\u5168\u63a7\u5236\u73af\u5883\u5373\u53ef\u8bef\u5bfc\u8bad\u7ec3\u6709\u7d20\u667a\u80fd\u4f53\u7684\u5bf9\u6297\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u6280\u672f\u5728\u591a\u515a\u5f00\u653e\u7cfb\u7edf\u4e2d\u5e94\u7528\u53d7\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u4e48\u9700\u8981\u5b8c\u5168\u63a7\u5236\u73af\u5883\uff0c\u8981\u4e48\u4f9d\u8d56\u4e0e\u53d7\u5bb3\u8005\u667a\u80fd\u4f53\u7684\u76f4\u63a5\u4ea4\u4e92\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e2d\u7acb\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u73af\u5883\u95f4\u63a5\u5f71\u54cd\u53d7\u5bb3\u8005\u667a\u80fd\u4f53\uff0c\u5728SMAC\u5e73\u53f0\u548cHighway-env\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u5e73\u53f0\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u65b9\u5f00\u653e\u7cfb\u7edf\u4e2d\u53d1\u8d77\u901a\u7528\u4e14\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u65b9\u5f00\u653e\u7cfb\u7edf\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u884c\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11062", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11062", "abs": "https://arxiv.org/abs/2510.11062", "authors": ["Yujie Zhao", "Lanxiang Hu", "Yang Wang", "Minmin Hou", "Hao Zhang", "Ke Ding", "Jishen Zhao"], "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs", "comment": null, "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86AT-GRPO\u65b9\u6cd5\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u548c\u56de\u5408\u5206\u7ec4\u4f18\u5316\u89e3\u51b3MAS\u4e2d\u7684RL\u6311\u6218\uff0c\u5728\u89c4\u5212\u3001\u7f16\u7801\u548c\u6570\u5b66\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5f3a\u5316\u5b66\u4e60\u90fd\u80fd\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u80fd\u529b\uff0c\u4f46\u5c06\u5728\u7ebf\u7b56\u7565RL\u5e94\u7528\u4e8eMAS\u4ecd\u5b58\u5728\u7b97\u6cd5\u548c\u7cfb\u7edf\u5c42\u9762\u7684\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848\u3002", "method": "AT-GRPO\u5305\u62ec\uff1a(i)\u9488\u5bf9MAS\u5b9a\u5236\u7684\u667a\u80fd\u4f53\u548c\u56de\u5408\u5206\u7ec4RL\u7b97\u6cd5\uff0c(ii)\u652f\u6301\u5355\u7b56\u7565\u548c\u591a\u7b56\u7565\u6a21\u578b\u7684\u8bad\u7ec3\u7cfb\u7edf\u3002", "result": "\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u4ece14.0-47.0%\u63d0\u5347\u523096.0-99.5%\uff1b\u7f16\u7801\u4efb\u52a1\u5e73\u5747\u63d0\u53473.87-7.62%\uff1b\u6570\u5b66\u4efb\u52a1\u5e73\u5747\u63d0\u53479.0-17.93%\u3002", "conclusion": "AT-GRPO\u6210\u529f\u89e3\u51b3\u4e86MAS\u4e2d\u5e94\u7528\u5728\u7ebf\u7b56\u7565RL\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u9886\u57df\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11184", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11184", "abs": "https://arxiv.org/abs/2510.11184", "authors": ["Zhengyu Chen", "Jinluan Yang", "Teng Xiao", "Ruochen Zhou", "Luan Zhang", "Xiangyu Xi", "Xiaowei Shi", "Wei Wang", "Jinggang Wang"], "title": "Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in reasoning and tool utilization. However, the generalization of\ntool-augmented reinforcement learning (RL) across diverse domains remains\nunderexplored. In this work, we investigate the cross-domain generalization of\nan LLM agent equipped with a code interpreter tool, which is exclusively\ntrained on mathematical problem-solving tasks. Despite the restricted training\ndomain, we evaluate the agent's performance across several distinct reasoning\ndomains. The results reveal that RL-based tool usage learned from mathematical\ntasks can be effectively transferred to complex tasks in other domains,\nenabling great task performance and high token efficiency. To facilitate this\ncross-domain transfer, we propose a Tool Generalization Reinforcement Learning\n(TGRL) framework designed to promote domain-agnostic learning and skill\nmigration, encompassing: (i) a standardized tool interface that abstracts\ndomain-specific nuances through consistent formatting and explicit termination,\nfostering transferable invocation patterns; (ii) a dual-component reward system\nthat decomposes rewards to incentivize generalizable behaviors like tool\nefficiency and reasoning abstraction, ensuring alignment and robustness across\ndomain shifts; and (iii) an XML-based prompt template that separates thinking,\ntool calls, and responses to encourage modular, domain-invariant planning and\ncoherent multi-turn interactions. Extensive experiments across diverse\nbenchmarks validate our approach, achieving state-of-the-art performance and\nhighlighting the cross-domain potential of Tool RL for LLM reasoning.", "AI": {"tldr": "LLM\u4ee3\u7406\u901a\u8fc7\u6570\u5b66\u4efb\u52a1\u8bad\u7ec3\u7684\u4ee3\u7801\u89e3\u91ca\u5668\u5de5\u5177\uff0c\u5728\u8de8\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548ctoken\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u5de5\u5177\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u5728\u4e0d\u540c\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u4ec5\u9650\u4e8e\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\u3002", "method": "\u63d0\u51faTool Generalization Reinforcement Learning (TGRL)\u6846\u67b6\uff0c\u5305\u62ec\u6807\u51c6\u5316\u5de5\u5177\u63a5\u53e3\u3001\u53cc\u7ec4\u4ef6\u5956\u52b1\u7cfb\u7edf\u548cXML\u63d0\u793a\u6a21\u677f\u3002", "result": "\u5728\u591a\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u6570\u5b66\u4efb\u52a1\u5b66\u4e60\u7684\u5de5\u5177\u4f7f\u7528\u53ef\u4ee5\u6709\u6548\u5730\u8f6c\u79fb\u5230\u5176\u4ed6\u9886\u57df\u7684\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "\u5de5\u5177\u5f3a\u5316\u5b66\u4e60\u5728LLM\u63a8\u7406\u4e2d\u5177\u6709\u8de8\u9886\u57df\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u4efb\u52a1\u6027\u80fd\u548c\u9ad8token\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11278", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.11278", "abs": "https://arxiv.org/abs/2510.11278", "authors": ["Gareth Seneque", "Lap-Hang Ho", "Nafise Erfanian Saeedi", "Jeffrey Molendijk", "Ariel Kupermann", "Tim Elson"], "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models", "comment": "52 pages, 10 figures", "summary": "We present Entropic Mutual-Information Geometry Large-Language Model\nAlignment (ENIGMA), a novel approach to Large-Language Model (LLM) training\nthat jointly improves reasoning, alignment and robustness by treating an\norganisation's policies/principles as directions to move on a model's\ninformation manifold. Our single-loop trainer combines Group-Relative Policy\nOptimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought\n(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information\n(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn\noptimal-transport regulariser on hidden-state distributions to bound geometry\ndrift. We also introduce infoNCE metrics that specialise to a standard MI lower\nbound under matched negatives to measure how strongly a model's CoT encodes\nthese policies. These metrics include a Sufficiency Index (SI) that enables the\nselection and creation of principles that maximise downstream performance prior\nto training. In our experiments using small (1B) LLMs, high-SI principles\npredict steadier training dynamics and improved benchmark performance over GRPO\nablations. Our information-geometry analysis of trained models validates\ndesirable structural change in the manifold. These results support our\nhypothesis that reasoning, alignment, and robustness are projections of a\nsingle informationgeometric objective, and that models trained using ENIGMA\ndemonstrate principled reasoning without the use of a reward model, offering a\npath to trusted capability", "AI": {"tldr": "ENIGMA\u662f\u4e00\u79cd\u65b0\u9896\u7684LLM\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ec4\u7ec7\u653f\u7b56\u89c6\u4e3a\u4fe1\u606f\u6d41\u5f62\u4e0a\u7684\u65b9\u5411\uff0c\u8054\u5408\u63d0\u5347\u63a8\u7406\u3001\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86GRPO\u3001SAMI\u98ce\u683c\u5bf9\u79f0InfoNCE\u8f85\u52a9\u548cSinkhorn\u6700\u4f18\u4f20\u8f93\u6b63\u5219\u5316\uff0c\u65e0\u9700\u5956\u52b1\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u539f\u5219\u6027\u63a8\u7406\u3002", "motivation": "\u5f53\u524dLLM\u8bad\u7ec3\u4e2d\u63a8\u7406\u3001\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u901a\u5e38\u88ab\u89c6\u4e3a\u72ec\u7acb\u76ee\u6807\uff0c\u4f5c\u8005\u5047\u8bbe\u8fd9\u4e9b\u662f\u5355\u4e00\u4fe1\u606f\u51e0\u4f55\u76ee\u6807\u7684\u4e0d\u540c\u6295\u5f71\uff0c\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u540c\u65f6\u4f18\u5316\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5355\u5faa\u73af\u8bad\u7ec3\u5668\u7ed3\u5408\uff1a1) GRPO\uff08\u57fa\u4e8e\u7b56\u7565\u3001\u65e0\u8bc4\u8bba\u5bb6\u7684RL\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528CoT\u683c\u5f0f\u5956\u52b1\uff09\uff1b2) SAMI\u98ce\u683c\u5bf9\u79f0InfoNCE\u8f85\u52a9\uff1b3) \u9690\u72b6\u6001\u5206\u5e03\u4e0a\u7684\u71b5Sinkhorn\u6700\u4f18\u4f20\u8f93\u6b63\u5219\u5316\u3002\u8fd8\u5f15\u5165\u4e86infoNCE\u6307\u6807\u548c\u5145\u5206\u6027\u6307\u6570\u6765\u9009\u62e9\u548c\u521b\u5efa\u539f\u5219\u3002", "result": "\u57281B\u53c2\u6570LLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8SI\u539f\u5219\u9884\u6d4b\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u4f18\u4e8eGRPO\u6d88\u878d\u5b9e\u9a8c\u7684\u57fa\u51c6\u6027\u80fd\u3002\u4fe1\u606f\u51e0\u4f55\u5206\u6790\u9a8c\u8bc1\u4e86\u6d41\u5f62\u4e0a\u7684\u7406\u60f3\u7ed3\u6784\u53d8\u5316\u3002", "conclusion": "\u63a8\u7406\u3001\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u662f\u5355\u4e00\u4fe1\u606f\u51e0\u4f55\u76ee\u6807\u7684\u4e0d\u540c\u6295\u5f71\uff0cENIGMA\u8bad\u7ec3\u7684\u6a21\u578b\u65e0\u9700\u5956\u52b1\u6a21\u578b\u5373\u53ef\u5c55\u793a\u539f\u5219\u6027\u63a8\u7406\uff0c\u4e3a\u53ef\u4fe1\u80fd\u529b\u63d0\u4f9b\u4e86\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11498", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11498", "abs": "https://arxiv.org/abs/2510.11498", "authors": ["Yuhang Li", "Chenchen Zhang", "Ruilin Lv", "Ao Liu", "Ken Deng", "Yuanxing Zhang", "Jiaheng Liu", "Wiggin Zhou", "Bo Zhou"], "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding", "comment": null, "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.", "AI": {"tldr": "ReLook\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001LLM\u4f5c\u4e3a\u5de5\u5177\uff0c\u5b9e\u73b0\u524d\u7aef\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u751f\u6210-\u8bca\u65ad-\u4f18\u5316\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u524d\u7aef\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u6cd5\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u524d\u7aef\u5f00\u53d1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u524d\u7aef\u4ee3\u7801\u7684\u6b63\u786e\u6027\u9700\u8981\u57fa\u4e8e\u6e32\u67d3\u50cf\u7d20\u548c\u4ea4\u4e92\u6765\u5224\u65ad\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u5f0f\u3001\u89c6\u89c9\u57fa\u7840\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001LLM\u4f5c\u4e3a\u89c6\u89c9\u6279\u8bc4\u8005\u548c\u53cd\u9988\u6e90\uff0c\u5f15\u5165\u5f3a\u5236\u4f18\u5316\u89c4\u5219\u786e\u4fdd\u5355\u8c03\u6539\u8fdb\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u89e3\u8026\u6279\u8bc4\u8005\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReLook\u5728\u89c6\u89c9\u524d\u7aef\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ReLook\u5c55\u793a\u4e86\u4ee3\u7406\u611f\u77e5\u3001\u89c6\u89c9\u5956\u52b1\u548c\u8bad\u7ec3-\u63a8\u7406\u89e3\u8026\u5728\u524d\u7aef\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u663e\u8457\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11121", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11121", "abs": "https://arxiv.org/abs/2510.11121", "authors": ["Rongjie Zhu", "Cong Zhang", "Zhiguang Cao"], "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM", "comment": null, "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.", "AI": {"tldr": "\u63d0\u51faRFTHGS\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5c0f\u578bLLM\u6765\u4e3a\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u751f\u6210\u9ad8\u6027\u80fd\u4ea4\u53c9\u7b97\u5b50\uff0c\u8d85\u8d8a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u548cGPT-4\u7b49\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u4f9d\u8d56\u5927\u578b\u901a\u7528LLM\uff08\u5982GPT-4\uff09\u4f5c\u4e3a\u542f\u53d1\u5f0f\u8bbe\u8ba1\u5668\u7684\u8303\u5f0f\uff0c\u8bc1\u660e\u7cbe\u5fc3\u5fae\u8c03\u7684\u5c0f\u578b\u4e13\u7528LLM\u53ef\u4ee5\u751f\u6210\u8d85\u8d8a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u7ec4\u4ef6\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5fae\u8c03\u5c0f\u578bLLM\uff0c\u91c7\u7528\u591a\u5c42\u7ea7\u8bfe\u7a0b\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u7b97\u5b50\u7f13\u5b58\u673a\u5236\u9632\u6b62\u6284\u88ad\u5e76\u4fc3\u8fdb\u591a\u6837\u6027\uff0c\u4e3a\u6df7\u5408\u9057\u4f20\u641c\u7d22\u6c42\u89e3\u5668\u751f\u6210\u4ea4\u53c9\u7b97\u5b50\u3002", "result": "\u5fae\u8c03\u540e\u7684LLM\u751f\u6210\u7684\u4ea4\u53c9\u7b97\u5b50\u663e\u8457\u4f18\u4e8eHGS\u4e2d\u7684\u4e13\u5bb6\u8bbe\u8ba1\u7b97\u5b50\uff0c\u6027\u80fd\u4f18\u52bf\u4ece\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u6269\u5c55\u52301000\u8282\u70b9\u7684\u5927\u89c4\u6a21\u95ee\u9898\uff0c\u8d85\u8d8a\u795e\u7ecf\u7ec4\u5408\u57fa\u7ebf\u3001\u63d0\u793a\u65b9\u6cd5\u548c\u5546\u4e1aLLM\u3002", "conclusion": "\u7cbe\u5fc3\u5fae\u8c03\u7684\u5c0f\u578b\u4e13\u7528LLM\u53ef\u4ee5\u8d85\u8d8a\u5927\u578b\u901a\u7528\u6a21\u578b\u548c\u4e13\u5bb6\u8bbe\u8ba1\uff0c\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u751f\u6210\u9ad8\u6027\u80fd\u7ec4\u4ef6\uff0c\u5c55\u793a\u4e86\u4e13\u4e1a\u5316\u6a21\u578b\u7684\u4ef7\u503c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11683", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11683", "abs": "https://arxiv.org/abs/2510.11683", "authors": ["Nianyi Lin", "Jiajie Zhang", "Lei Hou", "Juanzi Li"], "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models", "comment": null, "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86BGPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7279\u6b8a\u7684ELBO\u4e0b\u754c\u6765\u89e3\u51b3dLLMs\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u76ee\u6807\u51fd\u6570\u4f30\u8ba1\u548c\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728dLLMs\u7684RL\u8bad\u7ec3\u4e2d\u9700\u8981\u4fdd\u7559\u6240\u6709MC\u6837\u672c\u7684\u524d\u5411\u8ba1\u7b97\u56fe\uff0c\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u6837\u672c\u6570\u91cf\uff0c\u4ece\u800c\u5f71\u54cd\u4f3c\u7136\u51fd\u6570\u8fd1\u4f3c\u7cbe\u5ea6\u548cRL\u76ee\u6807\u51fd\u6570\u4f30\u8ba1\u3002", "method": "\u63d0\u51faBoundary-Guided Policy Optimization (BGPO)\u7b97\u6cd5\uff0c\u6784\u5efa\u6ee1\u8db3\u7ebf\u6027\u548c\u7b49\u4ef7\u6027\u4e24\u4e2a\u5173\u952e\u6027\u8d28\u7684\u7279\u6b8a\u4e0b\u754c\uff0c\u652f\u6301\u8de8\u6837\u672c\u68af\u5ea6\u7d2f\u79ef\uff0c\u5b9e\u73b0\u6052\u5b9a\u5185\u5b58\u4f7f\u7528\u3002", "result": "BGPO\u5728\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u3001\u4ee3\u7801\u751f\u6210\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684dLLMs RL\u7b97\u6cd5\u3002", "conclusion": "BGPO\u901a\u8fc7\u5185\u5b58\u9ad8\u6548\u7684RL\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86dLLMs\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u4f3c\u7136\u51fd\u6570\u8fd1\u4f3c\u548c\u66f4\u597d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11696", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11696", "abs": "https://arxiv.org/abs/2510.11696", "authors": ["Wei Huang", "Yi Ge", "Shuai Yang", "Yicheng Xiao", "Huizi Mao", "Yujun Lin", "Hanrong Ye", "Sifei Liu", "Ka Chun Cheung", "Hongxu Yin", "Yao Lu", "Xiaojuan Qi", "Song Han", "Yukang Chen"], "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs", "comment": "Code is available at https://github.com/NVlabs/QeRL", "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.", "AI": {"tldr": "QeRL\u662f\u4e00\u4e2a\u91cf\u5316\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408NVFP4\u91cf\u5316\u548cLoRA\u6280\u672f\uff0c\u52a0\u901fLLM\u7684RL\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u51cf\u5c11\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u5229\u7528\u91cf\u5316\u566a\u58f0\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRL\u8bad\u7ec3\u5bf9LLM\u6765\u8bf4\u8d44\u6e90\u5bc6\u96c6\uff0c\u9700\u8981\u5927\u91cfGPU\u5185\u5b58\u548c\u957f\u8bad\u7ec3\u65f6\u95f4\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u578b\u6a21\u578b\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408NVFP4\u91cf\u5316\u548cLoRA\u6280\u672f\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u91cf\u5316\u566a\u58f0\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u6c34\u5e73\u3002", "result": "\u5728rollout\u9636\u6bb5\u5b9e\u73b01.5\u500d\u52a0\u901f\uff0c\u9996\u6b21\u5728\u5355\u5f20H100 80GB GPU\u4e0a\u8bad\u7ec332B LLM\uff0c\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230GSM8K 90.8%\u548cMATH 500 77.4%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "QeRL\u4e3aLLM\u7684RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11162", "categories": ["cs.LG", "cs.NE", "nlin.AO", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.11162", "abs": "https://arxiv.org/abs/2510.11162", "authors": ["Roman A. Kononov", "Nikita A. Pospelov", "Konstantin V. Anokhin", "Vladimir V. Nekorkin", "Oleg V. Maslennikov"], "title": "Emergence of hybrid computational dynamics through reinforcement learning", "comment": "22 pages, 11 figures", "summary": "Understanding how learning algorithms shape the computational strategies that\nemerge in neural networks remains a fundamental challenge in machine\nintelligence. While network architectures receive extensive attention, the role\nof the learning paradigm itself in determining emergent dynamics remains\nlargely unexplored. Here we demonstrate that reinforcement learning (RL) and\nsupervised learning (SL) drive recurrent neural networks (RNNs) toward\nfundamentally different computational solutions when trained on identical\ndecision-making tasks. Through systematic dynamical systems analysis, we reveal\nthat RL spontaneously discovers hybrid attractor architectures, combining\nstable fixed-point attractors for decision maintenance with quasi-periodic\nattractors for flexible evidence integration. This contrasts sharply with SL,\nwhich converges almost exclusively to simpler fixed-point-only solutions. We\nfurther show that RL sculpts functionally balanced neural populations through a\npowerful form of implicit regularization -- a structural signature that\nenhances robustness and is conspicuously absent in the more heterogeneous\nsolutions found by SL-trained networks. The prevalence of these complex\ndynamics in RL is controllably modulated by weight initialization and\ncorrelates strongly with performance gains, particularly as task complexity\nincreases. Our results establish the learning algorithm as a primary\ndeterminant of emergent computation, revealing how reward-based optimization\nautonomously discovers sophisticated dynamical mechanisms that are less\naccessible to direct gradient-based optimization. These findings provide both\nmechanistic insights into neural computation and actionable principles for\ndesigning adaptive AI systems.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u76d1\u7763\u5b66\u4e60\uff08SL\uff09\u5728\u76f8\u540c\u51b3\u7b56\u4efb\u52a1\u4e2d\u9a71\u52a8\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u5f62\u6210\u6839\u672c\u4e0d\u540c\u7684\u8ba1\u7b97\u7b56\u7565\uff0cRL\u81ea\u53d1\u53d1\u73b0\u6df7\u5408\u5438\u5f15\u5b50\u67b6\u6784\uff0c\u800cSL\u4e3b\u8981\u6536\u655b\u5230\u7b80\u5355\u56fa\u5b9a\u70b9\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a2\u7d22\u5b66\u4e60\u7b97\u6cd5\u672c\u8eab\u5982\u4f55\u5851\u9020\u795e\u7ecf\u7f51\u7edc\u4e2d\u6d8c\u73b0\u7684\u8ba1\u7b97\u7b56\u7565\uff0c\u7406\u89e3\u4e0d\u540c\u5b66\u4e60\u8303\u5f0f\u5bf9\u7f51\u7edc\u52a8\u6001\u7279\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u52a8\u529b\u5b66\u5206\u6790\uff0c\u6bd4\u8f83RL\u548cSL\u8bad\u7ec3\u7684RNN\u5728\u76f8\u540c\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u5206\u6790\u5438\u5f15\u5b50\u67b6\u6784\u548c\u795e\u7ecf\u7fa4\u4f53\u7279\u6027\u3002", "result": "RL\u53d1\u73b0\u6df7\u5408\u5438\u5f15\u5b50\u67b6\u6784\uff08\u7a33\u5b9a\u56fa\u5b9a\u70b9+\u51c6\u5468\u671f\u5438\u5f15\u5b50\uff09\uff0cSL\u4e3b\u8981\u6536\u655b\u5230\u56fa\u5b9a\u70b9\u89e3\u51b3\u65b9\u6848\uff1bRL\u901a\u8fc7\u9690\u5f0f\u6b63\u5219\u5316\u5851\u9020\u529f\u80fd\u5e73\u8861\u7684\u795e\u7ecf\u7fa4\u4f53\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u5b66\u4e60\u7b97\u6cd5\u662f\u6d8c\u73b0\u8ba1\u7b97\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\uff0c\u57fa\u4e8e\u5956\u52b1\u7684\u4f18\u5316\u80fd\u81ea\u4e3b\u53d1\u73b0\u590d\u6742\u52a8\u6001\u673a\u5236\uff0c\u4e3a\u8bbe\u8ba1\u81ea\u9002\u5e94AI\u7cfb\u7edf\u63d0\u4f9b\u539f\u5219\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11345", "abs": "https://arxiv.org/abs/2510.11345", "authors": ["Han Lu", "Zichen Liu", "Shaopan Xiong", "Yancheng He", "Wei Gao", "Yanan Wu", "Weixun Wang", "Jiashun Liu", "Yang Li", "Haizhou Zhao", "Ju Huang", "Siran Yang", "Xiaoyang Li", "Yijia Luo", "Zihe Liu", "Ling Pan", "Junchi Yan", "Wei Wang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng"], "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony", "comment": null, "summary": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training.", "AI": {"tldr": "ROLL Flash\u662f\u4e00\u4e2a\u652f\u6301\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5e76\u884c\u5316\u548crollout-train\u89e3\u8026\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u540c\u6b65\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7cfb\u7edf\u5b58\u5728\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u5f02\u6b65\u8bad\u7ec3\u67b6\u6784\u3002", "method": "\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u5e76\u884c\u5316\u548crollout-train\u89e3\u8026\u4e24\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u7f16\u7a0b\u63a5\u53e3\uff0c\u652f\u6301\u5b8c\u5168\u5f02\u6b65\u8bad\u7ec3\u67b6\u6784\u548c\u9ad8\u6548\u7684rollout\u673a\u5236\u3002", "result": "ROLL Flash\u5728\u76f8\u540cGPU\u9884\u7b97\u4e0b\uff0c\u5728RLVR\u4efb\u52a1\u4e0a\u5b9e\u73b02.24\u500d\u52a0\u901f\uff0c\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u5b9e\u73b02.72\u500d\u52a0\u901f\uff0c\u5f02\u6b65\u8bad\u7ec3\u6027\u80fd\u4e0e\u540c\u6b65\u8bad\u7ec3\u76f8\u5f53\u3002", "conclusion": "\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u540c\u6b65\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\u8868\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11499", "abs": "https://arxiv.org/abs/2510.11499", "authors": ["Xinsong Feng", "Leshu Tang", "Chenan Wang", "Haipeng Chen"], "title": "Offline Reinforcement Learning with Generative Trajectory Policies", "comment": "Preprint. Under review at ICLR 2026", "summary": "Generative models have emerged as a powerful class of policies for offline\nreinforcement learning (RL) due to their ability to capture complex,\nmulti-modal behaviors. However, existing methods face a stark trade-off: slow,\niterative models like diffusion policies are computationally expensive, while\nfast, single-step models like consistency policies often suffer from degraded\nperformance. In this paper, we demonstrate that it is possible to bridge this\ngap. The key to moving beyond the limitations of individual methods, we argue,\nlies in a unifying perspective that views modern generative models, including\ndiffusion, flow matching, and consistency models, as specific instances of\nlearning a continuous-time generative trajectory governed by an Ordinary\nDifferential Equation (ODE). This principled foundation provides a clearer\ndesign space for generative policies in RL and allows us to propose Generative\nTrajectory Policies (GTPs), a new and more general policy paradigm that learns\nthe entire solution map of the underlying ODE. To make this paradigm practical\nfor offline RL, we further introduce two key theoretically principled\nadaptations. Empirical results demonstrate that GTP achieves state-of-the-art\nperformance on D4RL benchmarks - it significantly outperforms prior generative\npolicies, achieving perfect scores on several notoriously hard AntMaze tasks.", "AI": {"tldr": "\u63d0\u51faGenerative Trajectory Policies (GTPs)\u4f5c\u4e3a\u65b0\u7684\u751f\u6210\u7b56\u7565\u8303\u5f0f\uff0c\u901a\u8fc7ODE\u7edf\u4e00\u89c6\u89d2\u5c06\u6269\u6563\u3001\u6d41\u5339\u914d\u548c\u4e00\u81f4\u6027\u6a21\u578b\u6574\u5408\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u6269\u6563\u7b56\u7565\u8ba1\u7b97\u6602\u8d35\u800c\u4e00\u81f4\u6027\u7b56\u7565\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u63d0\u51faGTPs\u8303\u5f0f\uff0c\u5c06\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u89c6\u4e3a\u5b66\u4e60\u7531ODE\u63a7\u5236\u7684\u8fde\u7eed\u65f6\u95f4\u751f\u6210\u8f68\u8ff9\u7684\u7279\u5b9a\u5b9e\u4f8b\uff0c\u5b66\u4e60\u5e95\u5c42ODE\u7684\u6574\u4e2a\u89e3\u6620\u5c04\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u751f\u6210\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u56f0\u96be\u7684AntMaze\u4efb\u52a1\u4e2d\u83b7\u5f97\u5b8c\u7f8e\u5206\u6570\u3002", "conclusion": "GTPs\u6210\u529f\u5f25\u5408\u4e86\u751f\u6210\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u4e0e\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u751f\u6210\u7b56\u7565\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11653", "abs": "https://arxiv.org/abs/2510.11653", "authors": ["Prasanna Mayilvahanan", "Ricardo Dominguez-Olmedo", "Thadd\u00e4us Wiedemer", "Wieland Brendel"], "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model", "comment": null, "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u73b0\u6709RL\u5fae\u8c03\u65b9\u6cd5\u4e3b\u8981\u662f\u5728\u5f3a\u5316\u57fa\u7840\u6a21\u578b\u5df2\u6709\u7684\u89e3\u9898\u6a21\u5f0f\u800c\u975e\u53d1\u73b0\u65b0\u65b9\u6cd5\uff0c\u4e3a\u6b64\u63d0\u51fa\u4e86MATH-B\u57fa\u51c6\u6765\u6311\u6218\u73b0\u6709\u6a21\u578b\uff0c\u63a8\u52a8\u63a2\u7d22\u9a71\u52a8\u7684RL\u65b9\u6cd5\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u5728\u5927\u91cf\u91c7\u6837\u4e0b\u5df2\u88ab\u57fa\u7840\u6a21\u578b\u89e3\u51b3\uff0cRL\u5fae\u8c03\u53ea\u662f\u4f18\u5316\u5df2\u6709\u80fd\u529b\u800c\u975e\u57f9\u517b\u65b0\u6280\u80fd\uff0c\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u771f\u6b63\u7684\u63a2\u7d22\u6027RL\u7814\u7a76\u3002", "method": "\u6784\u5efaMATH-B\u57fa\u51c6\uff0c\u4eceDAPO-Math-17K\u548cDeepScaleR\u6570\u636e\u96c6\u4e2d\u9009\u53d6\u80fd\u51fb\u8d258B\u53c2\u6570\u4ee5\u4e0b\u5f00\u6e90\u6a21\u578b\u7684\u6570\u5b66\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u5927\u91c7\u6837\u9884\u7b97\u4e0b\u4e5f\u4fdd\u6301\u6311\u6218\u6027\u3002", "result": "\u73b0\u6709RL\u5fae\u8c03\u6a21\u578b\u5982Nemotron-Research-Reasoning-Qwen-1.5B\u548cDeepScaleR-1.5B-Preview\u5728MATH-B\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u5728pass@1024\u4e0b\u4e5f\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "MATH-B\u57fa\u51c6\u80fd\u6709\u6548\u8bc4\u4f30RL\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u6709\u671b\u63a8\u52a8\u63a2\u7d22\u9a71\u52a8\u7684RL\u65b9\u6cd5\u53d1\u5c55\uff0c\u6fc0\u53d1\u66f4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11686", "abs": "https://arxiv.org/abs/2510.11686", "authors": ["Jens Tuyls", "Dylan J. Foster", "Akshay Krishnamurthy", "Jordan T. Ash"], "title": "Representation-Based Exploration for Language Models: From Test-Time to Post-Training", "comment": "Website and code: https://rep-exp.github.io", "summary": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u9690\u85cf\u72b6\u6001\u7684\u8868\u793a\u591a\u6837\u6027\u5956\u52b1\u6765\u4fc3\u8fdb\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u662f\u5426\u771f\u6b63\u4fc3\u8fdb\u65b0\u884c\u4e3a\u7684\u53d1\u73b0\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5f3a\u5316\u4e86\u57fa\u7840\u6a21\u578b\u4e2d\u5df2\u6709\u7684\u884c\u4e3a\uff0c\u63a2\u7d22\u9884\u8bad\u7ec3\u6a21\u578b\u77e5\u8bc6\u5982\u4f55\u6307\u5bfc\u8fd9\u79cd\u641c\u7d22\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9690\u85cf\u72b6\u6001\u7684\u8868\u793a\u591a\u6837\u6027\u5956\u52b1\u6765\u6fc0\u52b1\u6a21\u578b\u53d1\u73b0\u65b0\u9896\u591a\u6837\u7684\u884c\u4e3a\uff0c\u5e94\u7528\u4e8e\u540e\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u6269\u5c55\u8bbe\u7f6e\u3002", "result": "\u8868\u793a\u591a\u6837\u6027\u63a2\u7d22\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u548cpass@k\u7387\uff0c\u5728\u63a8\u7406\u65f6\u63d0\u9ad8\u6548\u7387\uff0c\u5728\u540e\u8bad\u7ec3\u4e2d\u6539\u8fdb\u63a8\u7406\u6027\u80fd\u3002\u4f8b\u5982Qwen-2.5-14b-Instruct\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1\u4e0a\u83b7\u5f97\u8d85\u8fc750%\u7684\u9a8c\u8bc1\u5668\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u6b63\u786e\u7684\u591a\u6837\u6027\u6982\u5ff5\u8fdb\u884c\u6709\u610f\u8bc6\u7684\u63a2\u7d22\uff0c\u662f\u5b9e\u73b0\u8d85\u8d8a\u7b80\u5355\u5f3a\u5316\u7684\u65b0\u884c\u4e3a\u53d1\u73b0\u7684\u5b9e\u9645\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.e767185f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zdnet.com%2Farticle%2Famazon-takes-shots-at-chatgpt-with-quick-suite-your-new-ai-teammate-at-work%2F%3Futm_source=tldrai/1/01000199ce43810a-c0673c9c-b988-438f-ab8a-51949ed65897-000000/ZAzHP1RU0dKsZLI1KF8SxcJ1x1g3xqTwbIG7lotPqAc=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zdnet.com%2Farticle%2Famazon-takes-shots-at-chatgpt-with-quick-suite-your-new-ai-teammate-at-work%2F%3Futm_source=tldrai/1/01000199ce43810a-c0673c9c-b988-438f-ab8a-51949ed65897-000000/ZAzHP1RU0dKsZLI1KF8SxcJ1x1g3xqTwbIG7lotPqAc=426", "authors": ["TLDR Newsletter"], "title": "Amazon takes shots at ChatGPT with Quick Suite - your new AI 'teammate' at work", "comment": "Source: TLDR Newsletter, Date: 2025-10-10, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zdnet.com%2Farticle%2Famazon-takes-shots-at-chatgpt-with-quick-suite-your-new-ai-teammate-at-work%2F%3Futm_source=tldrai/1/01000199ce43810a-c0673c9c-b988-438f-ab8a-51949ed65897-000000/ZAzHP1RU0dKsZLI1KF8SxcJ1x1g3xqTwbIG7lotPqAc=426", "summary": "Amazon takes shots at ChatGPT with Quick Suite - your new AI 'teammate' at work (5 minute read) Amazon's Quick Suite is an agentic AI experience for enterprises that uses conversational language to find what users need. It acts as a hub that pulls data from various sources like files, enterprise systems, databases, and the web. Users can use Quick Suite to discuss questions, build personalized agents, and complete tasks while leveraging data protections. AWS offers a free 30-day trial for the...", "source": "tldr", "AI": {"tldr": "\u4e9a\u9a6c\u900a\u63a8\u51faQuick Suite\u4f5c\u4e3a\u4f01\u4e1a\u7ea7AI\u52a9\u624b\uff0c\u901a\u8fc7\u5bf9\u8bdd\u8bed\u8a00\u5e2e\u52a9\u7528\u6237\u67e5\u627e\u4fe1\u606f\uff0c\u6574\u5408\u591a\u79cd\u6570\u636e\u6e90\uff0c\u5e76\u63d0\u4f9b\u6570\u636e\u4fdd\u62a4\u529f\u80fd", "motivation": "\u5e94\u5bf9ChatGPT\u7b49AI\u5de5\u5177\u7684\u7ade\u4e89\uff0c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u96c6\u6210\u5316\u7684AI\u52a9\u624b\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u57fa\u4e8e\u5bf9\u8bdd\u8bed\u8a00\u7684AI\u4f53\u9a8c\uff0c\u6574\u5408\u6587\u4ef6\u3001\u4f01\u4e1a\u7cfb\u7edf\u3001\u6570\u636e\u5e93\u548c\u7f51\u7edc\u7b49\u591a\u79cd\u6570\u636e\u6e90", "result": "\u63a8\u51faQuick Suite\u4f5c\u4e3a\u4f01\u4e1aAI\u52a9\u624b\uff0c\u63d0\u4f9b30\u5929\u514d\u8d39\u8bd5\u7528", "conclusion": "\u4e9a\u9a6c\u900a\u901a\u8fc7Quick Suite\u8fdb\u519b\u4f01\u4e1aAI\u52a9\u624b\u5e02\u573a\uff0c\u63d0\u4f9b\u6570\u636e\u5b89\u5168\u7684AI\u534f\u4f5c\u5de5\u5177", "topic": "swe application"}}
{"id": "tldr.2510.42edbff8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.langchain.com%2Fnot-another-workflow-builder%2F%3Futm_source=tldrdata/1/01000199dd099727-2acea3cb-7de2-40fc-ac24-968b68de7f4c-000000/P3RNowFB6BWLUH0BPVulG4bYgyrrBelB-_nBqJ-7YM0=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.langchain.com%2Fnot-another-workflow-builder%2F%3Futm_source=tldrdata/1/01000199dd099727-2acea3cb-7de2-40fc-ac24-968b68de7f4c-000000/P3RNowFB6BWLUH0BPVulG4bYgyrrBelB-_nBqJ-7YM0=426", "authors": ["TLDR Newsletter"], "title": "Not Another Workflow Builder", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.langchain.com%2Fnot-another-workflow-builder%2F%3Futm_source=tldrdata/1/01000199dd099727-2acea3cb-7de2-40fc-ac24-968b68de7f4c-000000/P3RNowFB6BWLUH0BPVulG4bYgyrrBelB-_nBqJ-7YM0=426", "summary": "Not Another Workflow Builder (4 minute read) Visual workflow builders limit scalable AI development. They are too complex for nontechnical users and too limited for advanced use cases. LangChain sees a future where no-code agents handle simple tasks, and code-based workflows with AI-generated code manage complex ones, combining flexibility, maintainability, and practical usability for enterprises.", "source": "tldr", "AI": {"tldr": "\u89c6\u89c9\u5de5\u4f5c\u6d41\u6784\u5efa\u5668\u9650\u5236AI\u5f00\u53d1\u7684\u53ef\u6269\u5c55\u6027\uff0c\u672a\u6765\u5c06\u7531\u65e0\u4ee3\u7801\u4ee3\u7406\u5904\u7406\u7b80\u5355\u4efb\u52a1\uff0c\u4ee3\u7801\u5de5\u4f5c\u6d41\u5904\u7406\u590d\u6742\u4efb\u52a1", "motivation": "\u89e3\u51b3\u89c6\u89c9\u5de5\u4f5c\u6d41\u6784\u5efa\u5668\u5728AI\u5f00\u53d1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u5bf9\u975e\u6280\u672f\u7528\u6237\u8fc7\u4e8e\u590d\u6742\uff0c\u5bf9\u9ad8\u7ea7\u7528\u4f8b\u53c8\u8fc7\u4e8e\u6709\u9650", "method": "\u63d0\u51fa\u7ed3\u5408\u65e0\u4ee3\u7801\u4ee3\u7406\u548c\u57fa\u4e8e\u4ee3\u7801\u5de5\u4f5c\u6d41\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5229\u7528AI\u751f\u6210\u4ee3\u7801\u6765\u7ba1\u7406\u590d\u6742\u4efb\u52a1", "result": "\u5b9e\u73b0\u4e86\u7075\u6d3b\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u4f01\u4e1a\u5b9e\u7528\u6027\u7684\u5e73\u8861", "conclusion": "LangChain\u8ba4\u4e3a\u672a\u6765AI\u5f00\u53d1\u5c06\u662f\u65e0\u4ee3\u7801\u4ee3\u7406\u4e0e\u4ee3\u7801\u5de5\u4f5c\u6d41\u7684\u7ed3\u5408\uff0c\u4e3a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4efb\u52a1\u63d0\u4f9b\u5408\u9002\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "tldr.2510.9d77424f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fnon-trivial-vibing%3Futm_source=tldrnewsletter/1/01000199dd18ae8c-49272999-75d1-4acb-9e77-1d78816ba86a-000000/n4W32Mv34Hmz7YC4TCEAEYQBMwWuPN5zrwDqkGx3cfM=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fnon-trivial-vibing%3Futm_source=tldrnewsletter/1/01000199dd18ae8c-49272999-75d1-4acb-9e77-1d78816ba86a-000000/n4W32Mv34Hmz7YC4TCEAEYQBMwWuPN5zrwDqkGx3cfM=426", "authors": ["TLDR Newsletter"], "title": "Vibing a Non-Trivial Ghostty Feature", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 20 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fnon-trivial-vibing%3Futm_source=tldrnewsletter/1/01000199dd18ae8c-49272999-75d1-4acb-9e77-1d78816ba86a-000000/n4W32Mv34Hmz7YC4TCEAEYQBMwWuPN5zrwDqkGx3cfM=426", "summary": "Vibing a Non-Trivial Ghostty Feature (20 minute read) Ghotty's unobtrusive update notification feature was largely developed with AI. This post shares the agentic coding sessions that led to shipping the feature. It provides context about the process and reasoning. AI is best used as an assistant and not a replacement. While AI can do a lot of the work, developers will almost always still need to intervene at times.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u5206\u4eab\u4e86\u4f7f\u7528AI\u5f00\u53d1Ghostty\u975e\u4fb5\u5165\u5f0f\u66f4\u65b0\u901a\u77e5\u529f\u80fd\u7684\u8fc7\u7a0b\uff0c\u5f3a\u8c03AI\u4f5c\u4e3a\u52a9\u624b\u800c\u975e\u66ff\u4ee3\u8005\u7684\u89d2\u8272", "motivation": "\u5c55\u793aAI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u63a2\u8ba8AI\u4f5c\u4e3a\u5f00\u53d1\u52a9\u624b\u7684\u6709\u6548\u4f7f\u7528\u65b9\u5f0f", "method": "\u901a\u8fc7\u4ee3\u7406\u5f0f\u7f16\u7801\u4f1a\u8bdd\u5f00\u53d1\u529f\u80fd\uff0c\u8bb0\u5f55\u5f00\u53d1\u8fc7\u7a0b\u548c\u51b3\u7b56\u601d\u8def", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86Ghostty\u7684\u975e\u4fb5\u5165\u5f0f\u66f4\u65b0\u901a\u77e5\u529f\u80fd", "conclusion": "AI\u5728\u5f00\u53d1\u4e2d\u80fd\u5b8c\u6210\u5927\u91cf\u5de5\u4f5c\uff0c\u4f46\u5f00\u53d1\u8005\u4ecd\u9700\u9002\u65f6\u4ecb\u5165\u5e72\u9884", "topic": "swe application"}}
{"id": "tldr.2510.221a96e0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.fsck.com%2F2025%2F10%2F09%2Fsuperpowers%2F%3Futm_source=tldrnewsletter/1/01000199dd18ae8c-49272999-75d1-4acb-9e77-1d78816ba86a-000000/RX9dNKEBDG4bnBOEYBJYNInJQLFtkfYgNEUeZWzmB0M=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.fsck.com%2F2025%2F10%2F09%2Fsuperpowers%2F%3Futm_source=tldrnewsletter/1/01000199dd18ae8c-49272999-75d1-4acb-9e77-1d78816ba86a-000000/RX9dNKEBDG4bnBOEYBJYNInJQLFtkfYgNEUeZWzmB0M=426", "authors": ["TLDR Newsletter"], "title": "Superpowers: How I'm using coding agents in October 2025", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.fsck.com%2F2025%2F10%2F09%2Fsuperpowers%2F%3Futm_source=tldrnewsletter/1/01000199dd18ae8c-49272999-75d1-4acb-9e77-1d78816ba86a-000000/RX9dNKEBDG4bnBOEYBJYNInJQLFtkfYgNEUeZWzmB0M=426", "summary": "Superpowers: How I'm using coding agents in October 2025 (13 minute read) Anthropic's new plugin system for Claude Code gives the agent new skills it can use to complete tasks.", "source": "tldr", "AI": {"tldr": "Anthropic\u4e3aClaude Code\u63a8\u51fa\u4e86\u65b0\u7684\u63d2\u4ef6\u7cfb\u7edf\uff0c\u8d4b\u4e88\u7f16\u7801\u4ee3\u7406\u65b0\u7684\u6280\u80fd\u6765\u5b8c\u6210\u5404\u79cd\u4efb\u52a1\u3002", "motivation": "\u63d0\u5347\u7f16\u7801\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u6709\u6548\u5730\u5b8c\u6210\u590d\u6742\u7684\u7f16\u7a0b\u4efb\u52a1\uff0c\u901a\u8fc7\u63d2\u4ef6\u7cfb\u7edf\u6269\u5c55\u5176\u529f\u80fd\u8303\u56f4\u3002", "method": "\u5f00\u53d1\u5e76\u96c6\u6210\u63d2\u4ef6\u7cfb\u7edf\u5230Claude Code\u4e2d\uff0c\u4e3a\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u65b0\u7684\u6280\u80fd\u548c\u5de5\u5177\u3002", "result": "\u7f16\u7801\u4ee3\u7406\u73b0\u5728\u80fd\u591f\u5229\u7528\u63d2\u4ef6\u7cfb\u7edf\u83b7\u5f97\u589e\u5f3a\u7684\u529f\u80fd\uff0c\u66f4\u9ad8\u6548\u5730\u5b8c\u6210\u7f16\u7a0b\u4efb\u52a1\u3002", "conclusion": "\u63d2\u4ef6\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u57282025\u5e7410\u6708\u65f6\u5177\u5907\u66f4\u5f3a\u7684\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2510.bc4f4656", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.squarespace.com%2Fblog%2F2025%2Fmaking-documentation-simpler-and-practical-our-docs-as-code-journey%3Futm_source=tldrdevops/1/01000199dd3fcf75-25975a05-2c99-48df-814a-06181712edc3-000000/1bex5eAAHVzeqcOzWKI4s2te6KnHo6csCOR2-iJ3xb4=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.squarespace.com%2Fblog%2F2025%2Fmaking-documentation-simpler-and-practical-our-docs-as-code-journey%3Futm_source=tldrdevops/1/01000199dd3fcf75-25975a05-2c99-48df-814a-06181712edc3-000000/1bex5eAAHVzeqcOzWKI4s2te6KnHo6csCOR2-iJ3xb4=426", "authors": ["TLDR Newsletter"], "title": "Making Documentation Simpler and Practical: Our Docs-as-Code Journey", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.squarespace.com%2Fblog%2F2025%2Fmaking-documentation-simpler-and-practical-our-docs-as-code-journey%3Futm_source=tldrdevops/1/01000199dd3fcf75-25975a05-2c99-48df-814a-06181712edc3-000000/1bex5eAAHVzeqcOzWKI4s2te6KnHo6csCOR2-iJ3xb4=426", "summary": "Making Documentation Simpler and Practical: Our Docs-as-Code Journey (6 minute read) The Squarespace Domains engineering team has embraced a \"Docs-as-Code\" (DaC) philosophy, integrating documentation with code in Git for version control and pull request reviews. By using Markdown, Mermaid syntax, and Backstage as a developer portal, the team has streamlined documentation, improved traceability, and enabled faster iteration on technical documents. The team also uses local previews with the Int...", "source": "tldr", "AI": {"tldr": "Squarespace Domains\u56e2\u961f\u91c7\u7528Docs-as-Code\u65b9\u6cd5\uff0c\u5c06\u6587\u6863\u4e0e\u4ee3\u7801\u96c6\u6210\u5728Git\u4e2d\uff0c\u4f7f\u7528Markdown\u3001Mermaid\u8bed\u6cd5\u548cBackstage\u5f00\u53d1\u8005\u95e8\u6237\uff0c\u7b80\u5316\u6587\u6863\u6d41\u7a0b\u5e76\u63d0\u9ad8\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6587\u6863\u7ba1\u7406\u4e2d\u7684\u7248\u672c\u63a7\u5236\u3001\u534f\u4f5c\u548c\u8fed\u4ee3\u6548\u7387\u95ee\u9898\uff0c\u4f7f\u6587\u6863\u5f00\u53d1\u4e0e\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u4fdd\u6301\u4e00\u81f4\u3002", "method": "\u91c7\u7528Docs-as-Code\u54f2\u5b66\uff0c\u5c06\u6587\u6863\u5b58\u50a8\u5728Git\u4ed3\u5e93\u4e2d\uff0c\u4f7f\u7528Markdown\u548cMermaid\u8bed\u6cd5\u7f16\u5199\uff0c\u901a\u8fc7Backstage\u4f5c\u4e3a\u5f00\u53d1\u8005\u95e8\u6237\uff0c\u652f\u6301\u672c\u5730\u9884\u89c8\u548c\u62c9\u53d6\u8bf7\u6c42\u5ba1\u67e5\u3002", "result": "\u6587\u6863\u6d41\u7a0b\u5f97\u5230\u7b80\u5316\uff0c\u53ef\u8ffd\u6eaf\u6027\u63d0\u9ad8\uff0c\u6280\u672f\u6587\u6863\u8fed\u4ee3\u901f\u5ea6\u52a0\u5feb\uff0c\u56e2\u961f\u534f\u4f5c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "Docs-as-Code\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u6587\u6863\u7ba1\u7406\u5b9e\u8df5\uff0c\u4f7f\u6587\u6863\u5f00\u53d1\u66f4\u52a0\u9ad8\u6548\u548c\u53ef\u7ef4\u62a4\u3002", "topic": "swe application"}}
{"id": "tldr.2510.14440783", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fadd-mcp-servers-to-claude-code-with-mcp-toolkit%2F%3Futm_source=tldrdevops/1/01000199dd3fcf75-25975a05-2c99-48df-814a-06181712edc3-000000/3Jwl0mX8m3TRNaLYsw7ppLS9j2QFbtjZsjUQiKIoOwE=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fadd-mcp-servers-to-claude-code-with-mcp-toolkit%2F%3Futm_source=tldrdevops/1/01000199dd3fcf75-25975a05-2c99-48df-814a-06181712edc3-000000/3Jwl0mX8m3TRNaLYsw7ppLS9j2QFbtjZsjUQiKIoOwE=426", "authors": ["TLDR Newsletter"], "title": "Add MCP Servers to Claude Code with MCP Toolkit", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fadd-mcp-servers-to-claude-code-with-mcp-toolkit%2F%3Futm_source=tldrdevops/1/01000199dd3fcf75-25975a05-2c99-48df-814a-06181712edc3-000000/3Jwl0mX8m3TRNaLYsw7ppLS9j2QFbtjZsjUQiKIoOwE=426", "summary": "Add MCP Servers to Claude Code with MCP Toolkit (6 minute read) Docker MCP Toolkit now connects Claude Code to real development tools, enabling it to run database queries, create GitHub issues, and send Slack messages, eliminating the need for manual copy-pasting and context-switching. The toolkit features over 200 pre-built containerized MCP servers, one-click deployment in Docker Desktop, and automatic credential handling, allowing developers to connect Claude Code to trusted environments i...", "source": "tldr", "AI": {"tldr": "Docker MCP Toolkit \u8fde\u63a5 Claude Code \u4e0e\u5f00\u53d1\u5de5\u5177\uff0c\u5b9e\u73b0\u6570\u636e\u5e93\u67e5\u8be2\u3001GitHub\u95ee\u9898\u521b\u5efa\u548cSlack\u6d88\u606f\u53d1\u9001\uff0c\u65e0\u9700\u624b\u52a8\u590d\u5236\u7c98\u8d34\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u3002", "motivation": "\u51cf\u5c11\u5f00\u53d1\u8005\u5728\u4e0d\u540c\u5de5\u5177\u95f4\u624b\u52a8\u5207\u6362\u548c\u590d\u5236\u7c98\u8d34\u7684\u64cd\u4f5c\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u63d0\u4f9b200\u591a\u4e2a\u9884\u6784\u5efa\u7684\u5bb9\u5668\u5316MCP\u670d\u52a1\u5668\uff0c\u652f\u6301\u4e00\u952e\u90e8\u7f72\u548c\u81ea\u52a8\u51ed\u8bc1\u5904\u7406\uff0c\u8fde\u63a5Claude Code\u5230\u53ef\u4fe1\u73af\u5883\u3002", "result": "\u5b9e\u73b0\u4e86Claude Code\u4e0e\u5f00\u53d1\u5de5\u5177\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "MCP Toolkit\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u96c6\u6210\u51cf\u5c11\u4e86\u624b\u52a8\u64cd\u4f5c\u3002", "topic": "swe application"}}
{"id": "tldr.2510.a3dce513", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fnon-trivial-vibing%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/SOHdI_vnJTFUFXuaGajMr-b334w3fq9uZVQKn7hl348=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fnon-trivial-vibing%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/SOHdI_vnJTFUFXuaGajMr-b334w3fq9uZVQKn7hl348=426", "authors": ["TLDR Newsletter"], "title": "Vibing a Non-Trivial Ghostty Feature", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 16 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fnon-trivial-vibing%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/SOHdI_vnJTFUFXuaGajMr-b334w3fq9uZVQKn7hl348=426", "summary": "Vibing a Non-Trivial Ghostty Feature (16 minute read) This is a \u201cday-in-the-life\u201d of creating a new macOS automatic update feature for Ghostty, a terminal application, through AI coding tools. The process involved iterative AI sessions for UI prototyping, backend implementation, and code cleanup, interspersed with a lot of manual coding and problem-solving. AI was the most useful for UI iteration and as a background code processor. The total token cost was $15.98 with an estimated 8 hours of ...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u8bb0\u5f55\u4e86\u4f7f\u7528AI\u7f16\u7801\u5de5\u5177\u4e3a\u7ec8\u7aef\u5e94\u7528Ghostty\u5f00\u53d1macOS\u81ea\u52a8\u66f4\u65b0\u529f\u80fd\u7684\u5b8c\u6574\u8fc7\u7a0b\uff0c\u5305\u62ecUI\u539f\u578b\u8bbe\u8ba1\u3001\u540e\u7aef\u5b9e\u73b0\u548c\u4ee3\u7801\u6e05\u7406\u7b49\u8fed\u4ee3\u73af\u8282\u3002", "motivation": "\u63a2\u7d22AI\u7f16\u7801\u5de5\u5177\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u529f\u80fd\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u548c\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u5f0fAI\u4f1a\u8bdd\u4e0e\u624b\u52a8\u7f16\u7801\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u5904\u7406UI\u539f\u578b\u8bbe\u8ba1\u3001\u540e\u7aef\u5b9e\u73b0\u548c\u4ee3\u7801\u6e05\u7406\u7b49\u4e0d\u540c\u5f00\u53d1\u9636\u6bb5\u3002", "result": "AI\u5728UI\u8fed\u4ee3\u548c\u540e\u53f0\u4ee3\u7801\u5904\u7406\u65b9\u9762\u6700\u4e3a\u6709\u7528\uff0c\u603btoken\u6210\u672c\u4e3a15.98\u7f8e\u5143\uff0c\u4f30\u8ba1\u8017\u65f68\u5c0f\u65f6\u5b8c\u6210\u529f\u80fd\u5f00\u53d1\u3002", "conclusion": "AI\u7f16\u7801\u5de5\u5177\u5728\u7279\u5b9a\u5f00\u53d1\u73af\u8282\uff08\u5982UI\u8fed\u4ee3\uff09\u4e2d\u80fd\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u4ecd\u9700\u4e0e\u624b\u52a8\u7f16\u7801\u548c\u95ee\u9898\u89e3\u51b3\u76f8\u7ed3\u5408\u624d\u80fd\u5b8c\u6210\u590d\u6742\u529f\u80fd\u5f00\u53d1\u3002", "topic": "swe application"}}
{"id": "tldr.2510.fb2de739", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freact.dev%2Fblog%2F2025%2F10%2F07%2Freact-compiler-1%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/DitPrPlJa1_skutfy82_t8eAfDepVFOpt9-7SVgUYdc=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freact.dev%2Fblog%2F2025%2F10%2F07%2Freact-compiler-1%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/DitPrPlJa1_skutfy82_t8eAfDepVFOpt9-7SVgUYdc=426", "authors": ["TLDR Newsletter"], "title": "React Compiler v1.0", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freact.dev%2Fblog%2F2025%2F10%2F07%2Freact-compiler-1%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/DitPrPlJa1_skutfy82_t8eAfDepVFOpt9-7SVgUYdc=426", "summary": "React Compiler v1.0 (8 minute read) React Compiler v1.0 has been released. It's a build-time tool that automatically optimizes React and React Native components through memoization without code rewrites.", "source": "tldr", "AI": {"tldr": "React Compiler v1.0\u662f\u4e00\u4e2a\u6784\u5efa\u65f6\u5de5\u5177\uff0c\u53ef\u81ea\u52a8\u4f18\u5316React\u548cReact Native\u7ec4\u4ef6\uff0c\u901a\u8fc7\u8bb0\u5fc6\u5316\u6280\u672f\u65e0\u9700\u91cd\u5199\u4ee3\u7801", "motivation": "\u89e3\u51b3React\u5e94\u7528\u4e2d\u624b\u52a8\u4f18\u5316\u7ec4\u4ef6\u7684\u590d\u6742\u6027\u548c\u5bb9\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u81ea\u52a8\u5316\u7684\u6027\u80fd\u4f18\u5316\u65b9\u6848", "method": "\u4f7f\u7528\u6784\u5efa\u65f6\u7f16\u8bd1\u5668\u6280\u672f\uff0c\u81ea\u52a8\u5206\u6790React\u7ec4\u4ef6\u5e76\u8fdb\u884c\u8bb0\u5fc6\u5316\u4f18\u5316\uff0c\u65e0\u9700\u5f00\u53d1\u8005\u4fee\u6539\u6e90\u4ee3\u7801", "result": "\u53d1\u5e03\u4e86React Compiler v1.0\u7248\u672c\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316React\u548cReact Native\u5e94\u7528\u7684\u6027\u80fd", "conclusion": "React Compiler v1.0\u4e3aReact\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u7684\u6027\u80fd\u4f18\u5316\u5de5\u5177\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u8005\u7684\u4f18\u5316\u5de5\u4f5c", "topic": "swe application"}}
{"id": "tldr.2510.483f0e43", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F10%2F02%2Fafter-nine-years-of-grinding-replit-finally-found-its-market-can-it-keep-it%2F%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/lAyykSrMuZLDlg_aebabxrKVfiskH29_ikDweoYzRH0=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F10%2F02%2Fafter-nine-years-of-grinding-replit-finally-found-its-market-can-it-keep-it%2F%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/lAyykSrMuZLDlg_aebabxrKVfiskH29_ikDweoYzRH0=426", "authors": ["TLDR Newsletter"], "title": "After nine years of grinding, Replit finally found its market. Can it keep it?", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F10%2F02%2Fafter-nine-years-of-grinding-replit-finally-found-its-market-can-it-keep-it%2F%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/lAyykSrMuZLDlg_aebabxrKVfiskH29_ikDweoYzRH0=426", "summary": "After nine years of grinding, Replit finally found its market. Can it keep it? (9 minute read) After nine years of slow growth and multiple failed business models, Replit, a coding platform, has finally found success by targeting non-technical users with its AI-powered coding agent. The company's revenue has skyrocketed, leading to a $250 million funding round and a $3 billion valuation.", "source": "tldr", "AI": {"tldr": "Replit\u7ecf\u8fc79\u5e74\u7f13\u6162\u53d1\u5c55\u540e\uff0c\u901a\u8fc7\u9762\u5411\u975e\u6280\u672f\u7528\u6237\u7684AI\u7f16\u7a0b\u4ee3\u7406\u627e\u5230\u4e86\u5e02\u573a\u6210\u529f\uff0c\u6536\u5165\u5927\u5e45\u589e\u957f\u5e76\u83b7\u5f9725\u4ebf\u7f8e\u5143\u878d\u8d44\u548c30\u4ebf\u7f8e\u5143\u4f30\u503c", "motivation": "Replit\u5728\u7ecf\u5386\u591a\u5e74\u7f13\u6162\u589e\u957f\u548c\u591a\u4e2a\u5931\u8d25\u5546\u4e1a\u6a21\u5f0f\u540e\uff0c\u9700\u8981\u627e\u5230\u53ef\u6301\u7eed\u7684\u5e02\u573a\u5b9a\u4f4d\u548c\u589e\u957f\u8def\u5f84", "method": "\u901a\u8fc7AI\u9a71\u52a8\u7684\u7f16\u7a0b\u4ee3\u7406\u9762\u5411\u975e\u6280\u672f\u7528\u6237\uff0c\u63d0\u4f9b\u66f4\u6613\u7528\u7684\u7f16\u7a0b\u5e73\u53f0", "result": "\u516c\u53f8\u6536\u5165\u5927\u5e45\u589e\u957f\uff0c\u83b7\u5f972.5\u4ebf\u7f8e\u5143\u878d\u8d44\uff0c\u4f30\u503c\u8fbe\u523030\u4ebf\u7f8e\u5143", "conclusion": "Replit\u901a\u8fc7AI\u7f16\u7a0b\u4ee3\u7406\u6210\u529f\u5f00\u62d3\u4e86\u975e\u6280\u672f\u7528\u6237\u5e02\u573a\uff0c\u627e\u5230\u4e86\u53ef\u6301\u7eed\u7684\u4e1a\u52a1\u6a21\u5f0f", "topic": "swe application"}}
{"id": "tldr.2510.b740ca79", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1976257886390002116.html%3Futm_source=tldrcrypto/1/01000199dd76b261-0997ef02-8aa1-4dc2-b677-4c67a55e5e0e-000000/lP6IRRnLEgeZFwJHdPjtNrC_LDwbuc8Y1GKX5TbM9Ds=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1976257886390002116.html%3Futm_source=tldrcrypto/1/01000199dd76b261-0997ef02-8aa1-4dc2-b677-4c67a55e5e0e-000000/lP6IRRnLEgeZFwJHdPjtNrC_LDwbuc8Y1GKX5TbM9Ds=426", "authors": ["TLDR Newsletter"], "title": "ERC-8004: Trustless Agents v1 Spec Launches", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1976257886390002116.html%3Futm_source=tldrcrypto/1/01000199dd76b261-0997ef02-8aa1-4dc2-b677-4c67a55e5e0e-000000/lP6IRRnLEgeZFwJHdPjtNrC_LDwbuc8Y1GKX5TbM9Ds=426", "summary": "ERC-8004: Trustless Agents v1 Spec Launches (1 minute read) ERC-8004 v1 is a standard that allows AI agents to discover and trust each other without central intermediaries by representing agents as NFTs that can be minted, transferred, and managed using existing ERC-721 infrastructure while pointing to registration files listing names, skills, capabilities, and endpoints for portability across any agent explorer. The standard provides logically centralized permissionless discovery with onchai...", "source": "tldr", "AI": {"tldr": "ERC-8004 v1\u662f\u4e00\u4e2a\u5141\u8bb8AI\u4ee3\u7406\u65e0\u9700\u4e2d\u592e\u4e2d\u4ecb\u5373\u53ef\u76f8\u4e92\u53d1\u73b0\u548c\u4fe1\u4efb\u7684\u6807\u51c6\uff0c\u901a\u8fc7\u5c06\u4ee3\u7406\u8868\u793a\u4e3aNFT\u6765\u5b9e\u73b0", "motivation": "\u89e3\u51b3AI\u4ee3\u7406\u4e4b\u95f4\u7f3a\u4e4f\u53bb\u4e2d\u5fc3\u5316\u4fe1\u4efb\u548c\u53d1\u73b0\u673a\u5236\u7684\u95ee\u9898\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5728\u65e0\u9700\u4e2d\u592e\u63a7\u5236\u7684\u60c5\u51b5\u4e0b\u76f8\u4e92\u8bc6\u522b\u548c\u4ea4\u4e92", "method": "\u5c06AI\u4ee3\u7406\u8868\u793a\u4e3aERC-721 NFT\uff0c\u5305\u542b\u6ce8\u518c\u6587\u4ef6\u5217\u51fa\u540d\u79f0\u3001\u6280\u80fd\u3001\u80fd\u529b\u548c\u7aef\u70b9\uff0c\u5229\u7528\u73b0\u6709NFT\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u53ef\u79fb\u690d\u6027", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u903b\u8f91\u4e0a\u96c6\u4e2d\u4f46\u65e0\u9700\u8bb8\u53ef\u7684\u53d1\u73b0\u7cfb\u7edf\uff0cAI\u4ee3\u7406\u53ef\u4ee5\u5728\u4efb\u4f55\u4ee3\u7406\u6d4f\u89c8\u5668\u4e2d\u5b9e\u73b0\u53ef\u79fb\u690d\u6027", "conclusion": "ERC-8004\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u53bb\u4e2d\u5fc3\u5316\u7684\u4fe1\u4efb\u548c\u53d1\u73b0\u6846\u67b6\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840", "topic": "agent analysis"}}
