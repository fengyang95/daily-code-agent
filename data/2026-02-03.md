<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.AI](#cs.AI) [Total: 57]
- [cs.LG](#cs.LG) [Total: 74]
- [tldr.article](#tldr.article) [Total: 32]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: PPoGA是一个基于知识图谱的问答框架，采用规划器-执行器架构和预测处理机制，通过自我修正能力（路径修正和计划修正）解决LLMs在复杂推理中因功能固着而失败的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的LLMs在复杂问答中，一旦初始高层推理计划有误，就会像人类认知功能固着一样无法调整策略，导致追求不可行的解决方案。需要增强AI系统的元认知能力和问题重构能力。

Method: 提出PPoGA框架：1) 规划器-执行器架构分离高层策略与低层执行；2) 预测处理机制预判结果；3) 核心创新是自我修正机制，包括路径修正（局部执行错误）和计划修正（识别、丢弃并重新制定整个无效计划）。

Result: 在三个具有挑战性的多跳KGQA基准测试（GrailQA、CWQ、WebQSP）上进行广泛实验，PPoGA实现了最先进的性能，显著优于现有方法。

Conclusion: 研究表明，问题重构等元认知能力对于构建更强大、更灵活的AI推理系统至关重要。PPoGA通过自我修正机制有效解决了LLMs在复杂推理中的功能固着问题。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [2] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: G-MemLLM：一种基于GRU门控更新的记忆增强架构，通过潜在记忆银行提升LLM的长上下文推理能力，解决现有方法中的"上下文腐化"问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM受限于有限的上下文窗口容量，在多跳推理中难以保持长期事实一致性。现有方法（如上下文压缩或循环令牌）存在"上下文腐化"或信息稀释问题。

Method: 提出G-MemLLM架构，将冻结的LLM主干与可训练的潜在记忆银行结合，采用GRU风格的门控更新逻辑，选择性更新、保留或覆盖潜在记忆槽，防止循环系统中的梯度消失。

Result: 在HotpotQA和ZsRE基准测试中，从GPT-2 (124M)到Llama 3.1 (8B)的不同规模模型上均取得显著提升：Llama 3.1-8B在ZsRE上准确率提升13.3%；GPT-2在HotpotQA上答案F1提升8.56分；Llama 3.1-8B在HotpotQA上支持事实F1提升6.89分。

Conclusion: G-MemLLM通过门控记忆更新机制有效增强了LLM的多跳推理和关系抽取能力，在不同规模模型上均表现出显著性能提升。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [3] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

TL;DR: PTCBENCH是一个评估LLM人格一致性的基准测试，通过12种外部情境测试发现LLM人格会随情境变化，影响推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了人格特质是动态且情境依赖的心理共识，而LLM在情感代理和AI系统中的部署需要保持一致且真实的人格特质以确保用户信任和参与。

Method: 引入PTCBENCH基准，将模型置于12种不同的外部情境（包括地点背景和生活事件），使用NEO五因素人格量表严格评估人格特质。

Result: 对39,240个人格特质记录的研究显示，某些外部情境（如"失业"）会引发LLM显著的人格变化，甚至改变其推理能力。

Conclusion: PTCBENCH为评估现实、动态环境中的人格一致性建立了可扩展框架，为开发稳健且心理对齐的AI系统提供了可行见解。

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [4] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

TL;DR: DIVERGE是一个即插即用的代理RAG框架，通过反思引导生成和记忆增强迭代优化，解决传统RAG系统在开放性问题中多样性不足的问题，实现多样性与质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统假设每个查询只有一个正确答案，忽略了常见的信息寻求场景中存在多个合理答案的情况。这导致系统无法充分利用检索到的上下文多样性，限制了创造力并损害了公平包容的信息获取。

Method: 提出DIVERGE框架，包含反思引导生成和记忆增强迭代优化机制。通过新颖的反思过程引导生成多样化观点，同时使用记忆机制进行迭代优化以保持答案质量。

Result: 在Infinity-Chat数据集上，DIVERGE相比竞争基线和先前最先进方法实现了最佳的多样性-质量权衡，显著提高了多样性同时保持了质量。提出的评估指标与人类判断相关性良好。

Conclusion: 当前基于LLM的系统在开放信息寻求中存在系统性限制，明确建模多样性可以缓解这一问题。DIVERGE框架为促进多样化观点生成提供了有效解决方案。

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [5] [DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning](https://arxiv.org/abs/2602.00352)
*Li Siyan,Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.CL

TL;DR: DETOUR是一个双代理评估基准，用于模拟"舌尖现象"的多轮信息检索过程，包含1011个提示，测试模型在模糊、未明确指定场景下的能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估代理在"舌尖现象"搜索过程中能力的基准仅限于单轮设置，无法真实模拟人们在对话中经过多轮交互才能回忆起信息的过程。

Method: 引入DETOUR双代理评估基准，包含一个被评估的主要代理（Primary Agent）和一个保持一致的记忆代理（Memory Agent）。主要代理需要通过查询记忆代理来识别回忆的实体，支持文本、图像、音频和视频多种模态。

Result: 当前最先进的模型在该基准上表现仍然困难，在所有模态（文本、图像、音频、视频）上仅达到36%的准确率。

Conclusion: 该研究强调了在未明确指定场景下增强模型能力的重要性，DETOUR基准为评估多轮"舌尖现象"搜索提供了更真实的测试环境。

Abstract: When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.

</details>


### [6] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: 该论文研究了LLM多智能体系统中的曼德拉效应（集体记忆偏差），提出了MANBENCH基准来评估该现象，并提出了缓解策略，平均减少74.40%的曼德拉效应。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM增强了多智能体系统的协作能力，但智能体对集体认知偏差的易感性尚未充分研究。曼德拉效应作为集体错误记忆现象，在多智能体系统中可能传播错误信息，存在伦理风险，需要系统研究其存在、成因和缓解方法。

Method: 提出MANBENCH基准，包含四种易受曼德拉效应影响的任务类型和五种不同智能体角色与记忆时间尺度的交互协议。评估多个LLM驱动的智能体，量化曼德拉效应并分析影响因素。提出缓解策略：提示级防御（认知锚定、来源审查）和模型级基于对齐的防御。

Result: 在MANBENCH上评估多个LLM，量化了曼德拉效应的存在。提出的缓解策略平均减少74.40%的曼德拉效应（相比基线）。分析显示不同因素（如交互协议、任务类型）对曼德拉效应有显著影响。

Conclusion: 多智能体系统中确实存在曼德拉效应，需要关注其伦理影响。提出的MANBENCH基准和缓解策略有效减少了该效应，为开发更具韧性和伦理对齐的协作多智能体系统提供了重要见解。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [7] [Reasoning by Commented Code for Table Question Answering](https://arxiv.org/abs/2602.00543)
*Seho Pyo,Jiheon Seok,Jaejin Lee*

Main category: cs.CL

TL;DR: 提出一种带注释的逐步代码生成框架，通过将表格问答分解为多行可执行程序并添加自然语言注释，提升推理清晰度和数值准确性，在WikiTableQuestions基准上达到84.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统表格线性化方法破坏了结构化数据的二维关系，现有端到端答案生成或单行程序查询方法在数值准确性和可解释性方面存在局限，需要更清晰的推理框架。

Method: 引入带注释的逐步代码生成框架，将表格问答推理分解为多行可执行Python程序，每行代码附带简洁自然语言注释，结合轻量级答案选择机制与现有端到端模型集成。

Result: 在WikiTableQuestions基准上，使用Qwen2.5-Coder-7B-Instruct达到70.9%准确率，超过Repanda基线（67.6%）；与现有端到端模型结合后进一步提升至84.3%准确率。

Conclusion: 带注释的代码生成框架通过显式推理和程序分解有效提升表格问答的准确性和可解释性，为结构化数据理解提供了更可靠的解决方案。

Abstract: Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\% accuracy on the WikiTableQuestions benchmark.

</details>


### [8] [Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars](https://arxiv.org/abs/2602.00612)
*Yitong Zhang,Yongmin Li,Yuetong Liu,Jia Li,Xiaoran Jia,Zherui Li,Ge Li*

Main category: cs.CL

TL;DR: LAVE是一种专门为扩散大语言模型设计的约束解码方法，通过前瞻验证确保生成的代码语法正确性，显著提升输出质量且计算开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在生成形式语言（如源代码）时难以可靠地保证语法正确性，现有约束解码方法要么不适用于非自回归模型，要么无法确保中间输出可扩展为有效句子。

Method: 利用dLLMs能够并行预测所有位置token分布的特性，在模型提出新token时进行前瞻验证，高效可靠地检查提议token的有效性，确保中间输出始终可扩展为有效句子。

Result: 在四个广泛使用的dLLMs和三个代表性基准测试上的实验表明，LAVE始终优于现有基线方法，在语法正确性方面取得显著提升，同时引入的运行时开销可忽略不计。

Conclusion: LAVE为扩散大语言模型提供了一种高效可靠的约束解码方法，解决了其在生成形式语言时语法正确性不足的问题，具有重要的实际应用价值。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.
  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.

</details>


### [9] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

TL;DR: MLLMs通过将代码渲染为图像进行压缩，实现高达8倍的token减少，在代码理解任务中表现出色，为高效推理提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模扩大，传统LLMs基于文本的代码表示导致上下文长度线性增长和计算成本增加。MLLMs的发展为通过图像模态压缩代码表示提供了机会，因为图像比文本更适合压缩而不损失语义。

Method: 将源代码渲染为图像表示，通过调整分辨率实现视觉压缩。首次系统研究MLLMs在代码理解任务中的有效性，包括token减少效果、视觉线索利用和压缩鲁棒性评估。

Result: (1) MLLMs能有效理解代码，实现高达8倍的token压缩；(2) 能有效利用语法高亮等视觉线索，在4倍压缩下提升代码补全性能；(3) 克隆检测等任务对视觉压缩具有异常鲁棒性，某些压缩比甚至略优于原始文本输入。

Conclusion: 研究揭示了MLLMs在代码理解中的潜力和当前局限，指出图像模态代码表示是通向更高效推理的途径，为大规模软件系统的代码处理提供了新方向。

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [10] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

TL;DR: ExperienceWeaver：一种分层框架，通过将嘈杂的多维反馈提炼为结构化知识（技巧和策略），在少样本临床文本改进任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床文本改进对医疗效率至关重要，但高质量数据有限且医学文档约束复杂。现有方法在少样本场景下表现不佳：监督微调需要大量数据且成本高，检索增强生成通常只能提供表面修正而无法捕捉修订背后的推理过程。

Method: 提出ExperienceWeaver分层框架，将重点从数据检索转向经验学习。通过将嘈杂的多维反馈提炼为结构化、可操作的知识（错误特定的技巧和高层策略），并将这些提炼的经验注入智能体流程，使模型学习"如何修订"而非仅"修订什么"。

Result: 在四个临床数据集上的广泛评估表明，ExperienceWeaver持续提升性能，在少样本设置下超越了包括Gemini-3 Pro在内的最先进模型。

Conclusion: ExperienceWeaver通过经验提炼而非简单检索的方法，有效解决了临床文本改进中的少样本学习问题，为医疗文档质量提升提供了新思路。

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [11] [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084)
*Jane Luo,Chengyu Yin,Xin Zhang,Qingtao Li,Steven Liu,Yiming Huang,Jie Wu,Hao Liu,Yangyu Huang,Yu Kang,Fangkai Yang,Ying Xin,Scarlett Li*

Main category: cs.CL

TL;DR: RPG-Encoder将仓库规划图从静态生成蓝图扩展为统一高保真表示，通过编码原始代码、增量拓扑演化和结构感知导航，在仓库理解任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前仓库智能体存在推理断层问题，现有方法依赖孤立的API文档或缺乏语义深度的依赖图。作者认为仓库理解和生成是统一循环中的逆过程：生成将意图扩展为实现，理解将实现压缩回意图。

Method: 提出RPG-Encoder框架，包含三个机制：(1)将原始代码编码为结合语义特征和代码依赖的RPG；(2)增量演化拓扑以解耦维护成本与仓库规模；(3)作为结构感知导航的统一接口。

Result: 在SWE-bench Verified上达到93.7% Acc@5的SOTA性能，在SWE-bench Live Lite上超过最佳基线10%以上，在RepoCraft上达到98.5%重构覆盖率，维护开销减少95.7%。

Conclusion: RPG-Encoder通过统一的高保真表示成功弥合了意图与实现之间的推理循环，在复杂代码库中展现出卓越的细粒度定位精度和重构能力。

Abstract: Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.

</details>


### [12] [APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards](https://arxiv.org/abs/2602.00760)
*Kaiyan Chang,Chenwei Zhu,Yingfeng Luo,Yifu Huo,Chenglong Wang,Xiaoqian Liu,Qiaozhi He,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 论文提出锚点过程奖励(APR)方法，通过识别推理锚点并惩罚锚点后的冗余验证，解决大推理模型在测试时扩展中的过度思考问题，实现性能与效率的帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展(TTS)显著提升了大推理模型(LRMs)的能力，但引入了"过度思考"的副作用。研究发现LRMs在推理过程中经常进行重复的自我验证而不修正答案，这种冗余计算浪费资源。

Method: 提出锚点过程奖励(APR)：1) 定义推理锚点(答案首次稳定的位置)；2) 识别答案稳定尾(AST，锚点后的冗余验证)；3) 使用适合长度惩罚的策略优化算法，专门惩罚AST部分。

Result: APR方法在1.5B和7B规模的五个数学推理数据集上平均实现了性能-效率的帕累托前沿，同时显著减少了RL训练所需的计算资源。

Conclusion: 通过结构感知的奖励塑造方法，能够有效解决大推理模型中的过度思考问题，在保持性能的同时大幅提升推理效率。

Abstract: Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.

</details>


### [13] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于迭代上下文学习的新方法，用于从大型语言模型中提取信任先验，发现GPT-4.1的信任先验与人类相似，并能基于玩家特征调整信任行为。


<details>
  <summary>Details</summary>
Motivation: 构建可信赖AI系统的关键挑战是如何量化AI系统本身表现出的信任水平。传统方法依赖自我报告的态度，而本文旨在开发一种更客观的方法来表征AI系统的信任。

Method: 提出基于迭代上下文学习的新颖提取方法，应用于行为博弈论中的信任游戏。该方法将信任操作化为基于对另一智能体信念的自愿风险暴露。使用该方法从多个领先LLM中提取信任先验，并分析GPT-4.1对不同玩家角色的响应。

Result: GPT-4.1的信任先验与人类观察到的信任先验高度一致。GPT-4.1能根据玩家特征（角色）差异化信任行为。提取的信任变化可以通过基于感知温暖和能力的刻板印象模型很好地预测。

Conclusion: 该方法能有效提取LLM的信任先验，GPT-4.1展现出与人类相似的信任模式，且其信任行为可通过刻板印象维度解释，为理解AI系统的信任表征提供了新工具。

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [14] [EffGen: Enabling Small Language Models as Capable Autonomous Agents](https://arxiv.org/abs/2602.00887)
*Gaurav Srivastava,Aafiya Hussain,Chi Wang,Yingyan Celine Lin,Xuan Wang*

Main category: cs.CL

TL;DR: effGen是一个针对小型语言模型优化的开源智能体框架，通过提示优化、任务分解、复杂度路由和统一内存系统实现高效本地部署，在13个基准测试中优于主流框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的智能体系统存在高token成本和隐私问题，需要为小型语言模型开发高效、安全、本地部署的智能体框架。

Method: 1) 提示优化压缩上下文70-80%；2) 智能任务分解为并行/顺序子任务；3) 基于五因素的复杂度路由预执行决策；4) 统一内存系统；5) 支持多协议通信。

Result: 在13个基准测试中优于LangChain、AutoGen和Smolagents，成功率更高、执行更快、内存更低。提示优化对小模型增益更大(1.5B模型11.2% vs 32B模型2.4%)，路由对大模型增益更大(1.5B模型3.6% vs 32B模型7.9%)。

Conclusion: effGen为小型语言模型提供了高效、安全的本地智能体框架，结合提示优化和复杂度路由可在所有规模模型上获得一致性能提升，已开源供研究和商业使用。

Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.

</details>


### [15] [Verification Required: The Impact of Information Credibility on AI Persuasion](https://arxiv.org/abs/2602.00970)
*Saaduddin Mahmud,Eugene Bagdasarian,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出MixTalk博弈框架研究LLM代理在概率可信度信息下的策略通信，开发TOPD方法提升接收者抗说服能力


<details>
  <summary>Details</summary>
Motivation: LLM代理在通信影响高风险决策的场景中日益部署，需要理解策略通信。现有研究主要关注不可验证的廉价谈话或完全可验证的披露，未能捕捉信息具有概率可信度的现实领域。

Method: 提出MixTalk策略通信博弈框架，发送者策略性组合可验证和不可验证声明，接收者分配有限预算进行成本验证。评估最先进LLM代理在三种现实部署设置中的表现，并提出TOPD离线方法从交互日志中提取锦标赛最优策略。

Result: 大规模锦标赛评估揭示了LLM代理在推理信息可信度和塑造交互的显式行为方面的优势和局限。TOPD方法显著提高了接收者对说服的鲁棒性。

Conclusion: MixTalk框架为研究LLM代理在概率可信度信息下的策略通信提供了有效工具，TOPD方法能够显著提升接收者性能，为实际部署中的鲁棒通信提供了解决方案。

Abstract: Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.

</details>


### [16] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

TL;DR: DeALOG是一个去中心化的多智能体框架，用于多模态问答，通过专门的智能体（表格、上下文、视觉、总结和验证）在共享自然语言日志上进行协作，实现错误检测和验证。


<details>
  <summary>Details</summary>
Motivation: 跨文本、表格和图像的复杂问答需要整合多样化信息源，需要一个支持专业化处理、协调和可解释性的框架。

Method: 提出DeALOG框架，使用专门的智能体（表格、上下文、视觉、总结和验证），通过共享自然语言日志作为持久内存进行通信，实现去中心化的协作和验证。

Result: 在FinQA、TAT-QA、CRT-QA、WikiTableQuestions、FeTaQA和MultiModalQA等多个数据集上表现出竞争力，分析确认了共享日志、智能体专业化和验证对准确性的重要性。

Conclusion: DeALOG通过使用自然语言通信的模块化组件，提供了一个可扩展的多模态问答方法。

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [17] [Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning](https://arxiv.org/abs/2602.00998)
*Zhikun Xu,Xiaodong Yu,Ben Zhou,Jiang Liu,Jialian Wu,Ze Wang,Ximeng Sun,Hao Chen,Zicheng Liu*

Main category: cs.CL

TL;DR: 论文提出RULES方法，通过结构化预测任务训练LLMs进行引理判断，使用两段式输出和分段感知强化学习来提升数学推理的稳健性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在数学基准测试中表现良好，但经常错误应用引理，在不验证假设的情况下直接导入结论。需要提升模型在数学推理中正确判断引理适用性的能力。

Method: 将引理判断形式化为结构化预测任务：给定陈述和候选引理，模型必须输出前提条件检查和结论效用检查。提出RULES方法，使用两段式输出编码规范，通过强化学习加分段感知损失掩码进行训练，对错误部分施加惩罚。

Result: 在领域内任务上相比普通模型和单标签RL基线获得一致提升，在适用性破坏扰动上改进更大，在端到端任务上达到持平或适度提升。消融实验表明两段式输出和分段感知强化学习对稳健性都是必要的。

Conclusion: RULES方法通过结构化引理判断任务和分段感知训练，有效提升了LLMs在数学推理中正确应用引理的能力，特别是在处理扰动和验证前提条件方面表现出更强的稳健性。

Abstract: Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.

</details>


### [18] [What If We Allocate Test-Time Compute Adaptively?](https://arxiv.org/abs/2602.01070)
*Ahsan Bilal,Ahmed Mohsin,Muhammad Umer,Ali Subhan,Hassan Rizwan,Ayesha Mohsin,Dean Hougen*

Main category: cs.CL

TL;DR: 提出了一种验证器引导的自适应推理框架，通过过程奖励模型动态指导推理轨迹的生成和选择，显著优于均匀分配计算资源的测试时计算扩展方法。


<details>
  <summary>Details</summary>
Motivation: 传统测试时计算扩展方法存在三个问题：1）均匀分配推理计算资源；2）使用固定的采样策略；3）仅将验证用于重新排序。这些方法无法根据问题难度和推理过程动态调整计算资源分配。

Method: 提出验证器引导的自适应框架，将推理视为迭代的轨迹生成和选择过程。每个问题运行多次推理迭代，每次迭代中：1）可选生成高级计划；2）选择推理工具集和计算策略；3）生成候选推理轨迹。过程奖励模型（PRM）作为统一控制信号：在迭代内，步骤级PRM分数用于指导生成过程中的剪枝和扩展；在迭代间，聚合的轨迹奖励用于选择最终响应。

Result: 在多个数据集上，动态PRM引导方法始终优于直接测试时扩展方法。在MATH-500上取得显著增益，在AIME24和AMO-Bench等更难基准测试上实现数倍改进。通过理论FLOPs和计算强度指标证明，验证引导的分配将计算集中在高效用推理路径上。

Conclusion: 验证器引导的自适应推理框架通过动态分配计算资源和智能轨迹选择，显著提高了推理系统的性能和效率，证明了过程奖励模型在指导复杂推理任务中的有效性。

Abstract: Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.

</details>


### [19] [Tendem: A Hybrid AI+Human Platform](https://arxiv.org/abs/2602.01119)
*Konstantin Chernyshev,Ekaterina Artemova,Viacheslav Zhukov,Maksim Nerush,Mariia Fedorova,Iryna Repik,Olga Shapovalova,Aleksey Sukhorosov,Vladimir Dobrovolskii,Natalia Mikhailova,Sergei Tilga*

Main category: cs.CL

TL;DR: Tendem是一个混合系统，AI处理结构化重复工作，人类专家在模型失败时介入或验证结果，所有结果交付前都经过全面质量审查。评估显示其在真实任务中比纯AI代理和纯人工工作流表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在处理复杂任务时仍存在局限性，纯人工工作流则效率较低。需要结合AI自动化和人类专业判断的优势，构建一个既能保证质量又能提高效率的混合系统。

Method: 开发Tendem混合系统：AI处理结构化重复性工作，人类专家在AI失败时介入或验证结果。所有输出都经过全面质量审查后才交付给客户。通过94个真实世界任务进行内部评估，与纯AI代理和Upwork自由职业者的人工工作流进行比较。

Result: Tendem在94个真实任务评估中：1）始终提供更高质量的输出和更快的周转时间；2）运营成本与纯人工执行相当；3）其纯AI代理在第三方基准测试中，在网页浏览和工具使用任务上接近SOTA，在领域知识和推理方面表现强劲。

Conclusion: Tendem混合系统成功结合了AI自动化和人类专业判断的优势，在保证质量的同时提高了效率，且成本可控。这种AI-人类协作模式为解决复杂任务提供了有效方案。

Abstract: Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.

</details>


### [20] [ASTER: Agentic Scaling with Tool-integrated Extended Reasoning](https://arxiv.org/abs/2602.01204)
*Xuqin Zhang,Quan He,Zhenrui Zheng,Zongzhang Zhang,Xu He,Dong Li*

Main category: cs.CL

TL;DR: 论文提出ASTER框架，通过交互密集的冷启动策略解决RL训练中工具集成推理的交互崩溃问题，使4B模型在数学基准测试上达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 强化学习在LLMs的长程推理中表现出色，但工具集成推理的RL扩展面临"交互崩溃"问题：模型退化为大量内部推理而很少使用工具，仅进行简单的后验代码验证。

Method: 提出ASTER框架，采用交互密集的冷启动策略，仅需4K条交互密集轨迹就能建立强大的行为先验，支持后续RL训练中的有效探索。

Result: ASTER-4B在竞争性数学基准测试中达到SOTA，在AIME 2025上达到90.0%，超越了包括DeepSeek-V3.2-Exp在内的领先开源模型。

Conclusion: 交互密集的冷启动策略能有效避免交互崩溃，少量高质量专家轨迹就能建立强大的工具使用先验，使小模型在数学推理任务上超越大模型。

Abstract: Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.

</details>


### [21] [Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments](https://arxiv.org/abs/2602.01244)
*Siwei Wu,Yizhi Li,Yuyang Song,Wei Zhang,Yang Wang,Riza Batista-Navarro,Xian Yang,Mingjie Tang,Bryan Dai,Jian Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: TerminalTraj是一个可扩展的终端轨迹生成管道，通过Docker环境构建、任务实例生成和可执行验证代码合成，创建了5万多个经过验证的终端轨迹，显著提升了终端任务代理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 训练终端任务代理模型需要高质量的终端轨迹数据，但大规模构建面临两大挑战：可执行性（需要合适的Docker环境）和可验证性（异构任务输出难以统一验证）。

Method: 提出TerminalTraj管道：1）筛选高质量仓库构建Docker化执行环境；2）生成与Docker对齐的任务实例；3）合成带有可执行验证代码的代理轨迹。

Result: 构建了32K个Docker镜像和50,733个经过验证的终端轨迹，覆盖8个领域。基于Qwen2.5-Coder训练的模型在TerminalBench上获得显著提升：TB 1.0提升20%，TB 2.0提升10%。TerminalTraj-32B在100B参数以下模型中表现优异。

Conclusion: TerminalTraj成功解决了终端轨迹数据构建的可执行性和可验证性挑战，为训练终端任务代理模型提供了高质量、可扩展的数据集，显著提升了模型性能。

Abstract: Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \textbf{\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \textbf{\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, \textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.

</details>


### [22] [DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas](https://arxiv.org/abs/2602.01326)
*Zirui Wu,Lin Zheng,Zhihui Xie,Jiacheng Ye,Jiahui Gao,Shansan Gong,Yansong Feng,Zhenguo Li,Wei Bi,Guorui Zhou,Lingpeng Kong*

Main category: cs.CL

TL;DR: DreamOn是一个新颖的扩散语言模型框架，解决了现有扩散模型需要固定长度掩码序列的限制，实现了动态可变长度的代码填充生成。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归模型的替代方案，具有灵活、任意顺序填充的优势，但受限于需要固定长度的掩码序列，当预定义掩码大小与理想补全长度不匹配时，代码填充性能会严重下降。

Method: DreamOn在扩散过程中引入了两个长度控制状态，使模型能够基于自身预测自主扩展或收缩输出长度。该方法只需对现有扩散语言模型的训练目标进行最小修改，无需架构更改。

Result: 基于Dream-Coder-7B和DiffuCoder-7B构建的DreamOn在HumanEval-Infilling和SantaCoder-FIM上实现了与最先进自回归模型相当的填充性能，并匹配了使用真实长度获得的oracle性能。

Conclusion: DreamOn消除了扩散语言模型实际部署的一个基本障碍，显著提升了其在可变长度生成方面的灵活性和适用性。

Abstract: Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.

</details>


### [23] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

TL;DR: CRAFT是一个基于强化学习的框架，通过双重奖励机制优化多跳问答中的推理过程，提高答案准确性和推理忠实度。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成在多跳问答中存在三个主要挑战：1) 推理崩溃 - 多跳组合和噪声检索导致推理不稳定；2) 推理-答案不一致 - LLM生成的不确定性可能导致答案正确但推理不忠实；3) 格式控制丢失 - 传统思维链生成常偏离结构化输出格式要求。

Method: 提出CRAFT框架，基于组相对策略优化的强化学习方法，采用双重奖励机制：确定性奖励确保结构正确性，基于评判的奖励验证语义忠实度。支持可控的推理轨迹变体，系统分析结构和规模对推理性能的影响。

Result: 在三个多跳问答基准测试中，CRAFT提高了答案准确性和推理忠实度，CRAFT 7B模型在多个推理轨迹设置下与闭源LLM竞争性能。

Conclusion: CRAFT通过强化学习框架有效解决了多跳问答中的推理忠实度问题，在不同模型规模上均能提升性能，为可控和可靠的推理生成提供了有效方法。

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [24] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

TL;DR: RISE是一种冗余不敏感的解释评分方法，用于识别LLM输出中真正有影响的上下文元素，减少冗余信息对归因分数的影响，提供更清晰稳定的解释。


<details>
  <summary>Details</summary>
Motivation: LLM生成输出时使用大量上下文，其中常包含冗余信息。标准解释方法难以处理冗余和重叠上下文，微小输入变化会导致归因分数不可预测的波动，影响可解释性并带来如提示注入等风险。需要区分真正重要的上下文元素与相关但不必要的元素。

Method: 提出RISE（Redundancy-Insensitive Scoring of Explanation）方法，量化每个输入相对于其他输入的独特影响，最小化冗余的影响，提供更清晰稳定的归因。

Result: 实验表明RISE比传统方法提供更鲁棒的解释，强调条件信息对于可信LLM解释和监控的重要性。

Conclusion: RISE方法能有效处理LLM上下文中的冗余问题，提供更可靠稳定的解释，对于构建可信的LLM解释和监控系统至关重要。

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [25] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: FS-Researcher是一个基于文件系统的双智能体框架，通过持久化工作空间解决深度研究中长轨迹超出模型上下文限制的问题，实现超越上下文窗口的测试时扩展。


<details>
  <summary>Details</summary>
Motivation: 深度研究作为LLM智能体的代表性长视野任务，其长轨迹经常超出模型上下文限制，压缩了证据收集和报告编写的token预算，阻碍了有效的测试时扩展。

Method: 采用文件系统作为持久化外部内存和共享协调媒介的双智能体框架：Context Builder智能体作为图书管理员浏览互联网、编写结构化笔记并将原始源归档到可超越上下文长度的分层知识库中；Report Writer智能体将知识库作为事实来源，逐节编写最终报告。

Result: 在两个开放基准测试（DeepResearch Bench和DeepConsult）上，FS-Researcher在不同骨干模型上都实现了最先进的报告质量。分析显示最终报告质量与分配给Context Builder的计算量呈正相关，验证了文件系统范式下的有效测试时扩展。

Conclusion: FS-Researcher通过文件系统作为持久化工作空间，成功解决了深度研究中长轨迹超出上下文限制的问题，实现了超越上下文窗口的测试时扩展，为LLM智能体的长视野任务提供了有效解决方案。

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [26] [Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles](https://arxiv.org/abs/2602.01590)
*Shaohan Wang,Benfeng Xu,Licheng Zhang,Mingxuan Du,Chiwei Zhu,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: 提出了Wiki Live Challenge (WLC)基准，利用最新的维基百科优质文章作为专家级参考，评估深度研究代理的能力，并设计了包含39项标准的细粒度评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理的评估框架主要依赖LLM生成的参考或LLM衍生的评估维度，缺乏专家验证内容的可靠性，难以提供客观、细粒度的关键维度评估。

Method: 构建了包含100篇最新维基百科优质文章的数据集，提出了Wiki Eval评估框架，包含39项写作质量标准和严格的事实可验证性指标。

Result: 实验显示当前深度研究代理与人类专家级维基百科文章之间存在显著差距，验证了WLC在推进代理研究方面的有效性。

Conclusion: WLC基准通过专家级参考和细粒度评估标准，为深度研究代理提供了更可靠、客观的评估框架，有助于推动该领域的发展。

Abstract: Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge

</details>


### [27] [SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia](https://arxiv.org/abs/2602.01618)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 提出SEA-Guard系列模型，首个基于东南亚文化背景的多语言安全防护模型，通过智能代理数据生成框架创建真实区域特定安全数据集，在检测区域敏感内容方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界AI对齐需要文化感知的安全防护，但构建大规模文化基础数据集面临资源有限和本地标注者稀缺的挑战，现有方法依赖英语数据集机器翻译，缺乏区域文化细微差别。

Method: 提出智能代理数据生成框架，可扩展地创建真实、区域特定的安全数据集，特别针对东南亚地区，在此基础上开发SEA-Guard系列多语言安全防护模型。

Result: SEA-Guard在多个基准测试和文化变体评估中持续优于现有安全防护模型，在检测区域敏感或有害内容方面表现优异，同时保持强大的通用安全性能。

Conclusion: 通过智能代理框架创建文化基础数据集是解决AI安全中文化感知挑战的有效方法，SEA-Guard模型为东南亚地区提供了首个文化基础的多语言安全防护解决方案。

Abstract: Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.

</details>


### [28] [A2Eval: Agentic and Automated Evaluation for Embodied Brain](https://arxiv.org/abs/2602.01640)
*Shuai Zhang,Jiayu Hu,Zijie Chen,Zeyuan Ding,Yi Zhang,Yingji Zhang,Ziyi Zhou,Junwei Liao,Shengjie Zhou,Yong Dai,Zhenzhong Lan,Xiaozhu Ju*

Main category: cs.CL

TL;DR: A2Eval是首个通过两个协作智能体自动进行基准测试构建和评估的框架，解决了现有具身VLM评估中基准测试冗余、覆盖不平衡和人工标注成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前具身视觉语言模型评估依赖静态、专家定义、人工标注的基准测试，存在严重冗余和覆盖不平衡问题。这种劳动密集型范式消耗大量计算和标注资源，增加成本，扭曲模型排名，阻碍迭代开发。

Method: 提出Agentic Automatic Evaluation (A2Eval)框架，包含两个协作智能体：Data Agent自主归纳能力维度并构建平衡紧凑的评估套件；Eval Agent合成和验证可执行的评估流程，实现完全自主的高保真评估。

Result: 在10个基准测试和13个模型上评估，A2Eval将评估套件压缩85%，减少总体计算成本77%，实现4.6倍加速，同时保持评估质量。纠正系统性排名偏差，提高与人类评估的一致性至Spearman's rho=0.85，保持高排名保真度(Kendall's tau=0.81)。

Conclusion: A2Eval为高保真、低成本的具身评估建立了新标准，解决了现有评估方法的局限性，实现了完全自主的高效评估。

Abstract: Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.

</details>


### [29] [Scaling Search-Augmented LLM Reasoning via Adaptive Information Control](https://arxiv.org/abs/2602.01672)
*Siheng Xiong,Oguzhan Gungordu,Blair Johnson,James C. Kerce,Faramarz Fekri*

Main category: cs.CL

TL;DR: DeepControl框架通过信息效用概念实现自适应信息控制，在搜索增强推理中调节检索时机和粒度，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强推理代理在检索控制方面存在问题：无控制的检索导致冗余证据、上下文饱和和不稳定学习。基于结果的强化学习对信息获取调节指导有限，需要更精细的信息控制机制。

Method: 提出基于信息效用的DeepControl框架，信息效用衡量给定推理状态下检索证据的边际价值。引入检索连续性和粒度控制机制，选择性调节何时继续/停止检索以及扩展多少信息。采用退火控制策略使代理在训练中内化有效信息获取行为。

Result: 在7个基准测试中一致优于强基线。在Qwen2.5-7B和Qwen2.5-3B上分别比基于结果的强化学习基线平均提升9.4%和8.6%，持续优于无检索和基于检索的推理方法。

Conclusion: 自适应信息控制对于将搜索增强推理代理扩展到复杂、真实世界信息环境至关重要。DeepControl通过形式化的信息效用概念和精细控制机制有效解决了检索控制问题。

Abstract: Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.

</details>


### [30] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

TL;DR: 论文提出Latent Exploration Decoding (LED)方法，通过聚合中间层后验分布来缓解强化学习后训练导致的大推理模型探索崩溃问题，无需额外训练即可提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现代推理后训练会导致意外的探索崩溃：基于温度的采样不再提高pass@n准确率。研究发现后训练LRMs的最后一层后验分布熵急剧减少，而中间层熵保持相对较高，这种熵不对称性促使提出新的解码策略。

Method: 提出Latent Exploration Decoding (LED)，一种深度条件解码策略。通过累积和聚合中间后验分布，选择具有最大熵的深度配置作为探索候选，无需额外训练或参数。

Result: LED在多个推理基准测试和模型上一致提升pass@1和pass@16准确率，分别提高0.61和1.03个百分点。

Conclusion: LED通过利用中间层的高熵特性有效缓解探索崩溃问题，为推理模型的后训练优化提供了简单有效的解码策略。

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [31] [ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation](https://arxiv.org/abs/2602.01709)
*Xingshan Zeng,Lingzhi Wang,Weiwen Liu,Liangyou Li,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: 本文提出ARTIS框架，通过迭代模拟实现风险感知的测试时扩展，提升智能体在真实环境中的可靠性


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展技术虽然能提升LLM性能，但在智能体场景中不足，因为智能体动作会与外部环境直接交互，可能产生不可逆且代价高昂的影响

Method: 提出ARTIS框架，将探索与执行解耦，通过模拟交互进行测试时探索；引入风险感知工具模拟器，通过针对性数据生成和重新平衡训练来捕捉罕见但高影响的失败模式

Result: 在多轮多步智能体基准测试中，迭代模拟显著提升智能体可靠性，风险感知模拟对于在不同模型和任务中持续实现这些收益至关重要

Conclusion: ARTIS框架通过风险感知的迭代模拟，有效扩展了测试时计算到智能体决策中，提升了动作级可靠性和鲁棒性，同时避免了环境风险

Abstract: Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \emph{\name}, \emph{\underline{A}gentic \underline{R}isk-Aware \underline{T}est-Time Scaling via \underline{I}terative \underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.

</details>


### [32] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: SafePred是一个预测性护栏框架，通过将预测的未来风险与当前决策对齐，防止计算机使用代理的长期风险，相比反应式方法显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理的护栏主要采用反应式方法，只能约束当前观察空间内的行为，无法预防长期风险。看似合理的行动可能导致延迟出现的高风险后果，这些是反应式护栏无法在当前观察空间内识别的。

Method: 提出预测性护栏方法，核心思想是将预测的未来风险与当前决策对齐。SafePred框架建立风险到决策的循环，支持两种关键能力：1) 短期和长期风险预测，使用安全策略作为风险预测基础，利用世界模型的预测能力生成风险语义表示；2) 决策优化，通过步骤级干预和任务级重新规划将预测风险转化为可操作的安全决策指导。

Result: 大量实验表明，SafePred显著减少了高风险行为，相比反应式基线实现了超过97.6%的安全性能，并将任务效用提高了高达21.4%。

Conclusion: 预测性护栏方法能有效解决计算机使用代理的长期风险问题，SafePred框架通过风险预测和决策优化的结合，在保证安全性的同时提升任务效用。

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


### [33] [S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs](https://arxiv.org/abs/2602.01982)
*Yanrui Du,Sendong Zhao,Yibo Gao,Danyang Zhao,Qika Lin,Ming Ma,Jiayun Li,Yi Jiang,Kai He,Qianyi Xu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 提出S3-CoT框架，通过自采样激活引导实现高效思维链学习，无需教师指导即可生成风格对齐、可变长度的推理轨迹，解决监督数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的思维链能力提升常伴随冗余推理过程，需要探索LLMs能否获得类似人类系统1的快速思维模式，同时解决基于监督微调方法中高质量监督数据稀缺的核心瓶颈。

Method: 提出基于激活引导的自采样框架，从目标LLMs自身诱导风格对齐、可变长度的推理轨迹；使用黄金答案过滤数据进行监督微调，构建类人双认知系统和渐进压缩课程；探索仅使用预测一致数据的自进化机制。

Result: 在数学基准测试和医学领域的跨域泛化测试中，该方法对通用LLMs和R1风格LLMs均产生稳定改进。

Conclusion: S3-CoT框架成功实现了高效思维链学习，无需教师指导即可生成高质量推理数据，为LLMs获得快速思维模式提供了可行路径。

Abstract: Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.

</details>


### [34] [From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs](https://arxiv.org/abs/2602.01999)
*Yanrui Du,Yibo Gao,Sendong Zhao,Jiayun Li,Haochun Wang,Qika Lin,Kai He,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 该研究通过分析R1风格大语言模型的内部激活轨迹，揭示了自我反思行为的层级机制：从潜在控制层（思考预算编码）到语义枢纽层（话语级线索），再到行为外显层（反思行为token概率上升），形成因果链式结构。


<details>
  <summary>Details</summary>
Motivation: 尽管R1风格的大语言模型展现出自我反思能力，但其内部工作机制尚不明确。研究者希望通过分析反思行为起始阶段的层级激活轨迹，揭示这种自我反思行为的内在机制。

Method: 使用logit lens技术读取token级语义，追踪反思行为的层级激活轨迹。通过针对性干预实验，分析不同层级之间的因果关系。

Result: 发现了三个结构化阶段：1）潜在控制层：线性方向编码思考预算语义；2）语义枢纽层：转折点和总结性线索等话语级线索出现并主导概率质量；3）行为外显层：反思行为token的采样可能性上升。干预实验揭示了跨阶段的因果链。

Conclusion: 研究结果表明大语言模型的自我反思过程类似于人类的元认知过程：从潜在监控，到话语级调节，最终到外显的自我反思。这为理解大语言模型的内部工作机制提供了新视角。

Abstract: R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.

</details>


### [35] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: xMemory提出了一种新的智能体记忆系统，采用解耦到聚合的方法：将记忆分解为语义组件，组织成层次结构，并基于此结构进行检索，以解决传统RAG在智能体记忆场景中的冗余和关联缺失问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法假设大规模异质语料库，而智能体记忆是有限、连贯的对话流，其中记忆片段高度相关且常重复。固定top-k相似性检索会返回冗余上下文，而事后修剪可能删除时间关联的先决条件，影响推理正确性。

Method: xMemory通过稀疏性-语义目标构建完整单元的层次结构，维护可搜索且忠实的高层节点组织。采用解耦到聚合方法：将记忆分解为语义组件，组织成层次结构，并基于此结构驱动检索。推理时自上而下检索，选择紧凑多样的主题和语义组件，仅在减少读者不确定性时扩展到情节和原始消息。

Result: 在LoCoMo和PerLTQA数据集上，使用三种最新LLM进行实验，xMemory在回答质量和token效率方面均取得一致提升。

Conclusion: 检索应超越相似性匹配，转向基于潜在组件的操作。xMemory通过层次化记忆组织解决了智能体记忆场景中的冗余和关联问题，实现了更高效、准确的检索。

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [36] [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099)
*Keqin Peng,Yuanxin Ouyang,Xuebo Liu,Zhiliang Tian,Ruijian Han,Yancheng Yuan,Liang Ding*

Main category: cs.CL

TL;DR: 提出DDCA方法解决RLVR中长度惩罚导致准确率下降的问题，通过解耦效率和正确性优化，在多个数学推理基准上显著减少生成token同时保持或提高准确率。


<details>
  <summary>Details</summary>
Motivation: RLVR能激发多步推理但常导致冗长输出，而简单的长度惩罚在群体相对优化中会严重损害准确率。这源于两个结构性问题：长度基线稀释（错误响应降低群体基线，过度惩罚正确解）和难度惩罚不匹配（静态惩罚无法适应问题难度）。

Method: 提出动态解耦条件优势（DDCA）：1）在正确响应簇内条件计算长度优势，消除基线稀释；2）使用群体通过率作为难度代理，动态调整惩罚强度，使惩罚适应问题难度。

Result: 在GSM8K、MATH500、AMC23和AIME25等基准测试中，DDCA相比自适应基线持续改善效率-准确率权衡：简单任务（如GSM8K）减少约60%生成token，困难基准（如AIME25）减少超过20%，同时保持或提高准确率。

Conclusion: DDCA有效解决了RLVR中长度惩罚的固有问题，通过解耦效率和正确性优化，实现了更好的效率-准确率平衡，为强化学习中的奖励设计提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.

</details>


### [37] [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160)
*Bowen Xu,Shaoyu Wu,Hao Jiang,Kai Liu,Xin Chen,Lulu Hu,Bin Yang*

Main category: cs.CL

TL;DR: D-CORE是一个两阶段训练框架，通过任务分解推理能力激励和多样性感知强化学习，解决大型推理模型在复杂工具使用场景中的懒惰推理问题，显著提升工具使用性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在复杂工具使用场景中缺乏子任务分解能力，导致"懒惰推理"问题，限制了模型解决复杂实际问题的能力。

Method: 提出D-CORE两阶段训练框架：1）通过自蒸馏激励模型的任务分解推理能力；2）采用多样性感知强化学习恢复模型的反思推理能力。

Result: 在BFCLv3基准测试中，D-CORE-8B达到77.7%准确率，超越最佳8B模型5.7%；D-CORE-14B以79.3%准确率创下新SOTA，性能超越70B模型但参数量仅为其1/5。

Conclusion: D-CORE框架有效解决了大型推理模型的懒惰推理问题，显著提升了工具使用能力，在不同基准测试和模型规模上都表现出鲁棒性改进。

Abstract: Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\underline{\textbf{D}}ecomposing tasks and \underline{\textbf{Co}}mposing \underline{\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\% accuracy, surpassing the best-performing 8B model by 5.7\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\%, outperforming 70B models despite being 5$\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.

</details>


### [38] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: Kimi K2.5是一个开源的多模态智能体模型，通过文本-视觉联合优化提升通用智能体能力，并引入Agent Swarm框架实现并行任务分解与执行，在多个领域达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 推动通用智能体智能的发展，通过多模态联合优化解决传统方法中文本和视觉模态分离的问题，并设计高效的并行智能体编排框架来处理复杂任务。

Method: 1. 多模态联合优化：包括联合文本-视觉预训练、零视觉SFT和联合文本-视觉强化学习；2. Agent Swarm框架：自导向并行智能体编排，动态分解复杂任务为异构子问题并并发执行。

Result: 在编程、视觉、推理和智能体任务等多个领域达到最先进水平；Agent Swarm相比单智能体基线延迟降低高达4.5倍；发布了训练后的模型检查点。

Conclusion: Kimi K2.5通过多模态联合优化和并行智能体编排框架，有效提升了智能体性能和处理效率，为未来智能体智能研究和实际应用提供了有力工具。

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [39] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

TL;DR: 提出模块化梯度手术(MGS)方法，通过解决Transformer模块级别的梯度冲突，减少多领域强化学习中的跨领域干扰，提升大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 在多领域训练通用大型推理模型时，领域异质性导致显著的跨领域干扰，现有策略（顺序RL和混合RL）在行为和梯度层面都存在冲突，限制了整体性能提升。

Method: 提出模块化梯度手术(MGS)，在Transformer内部模块级别解决梯度冲突，而不是在整个模型层面处理。该方法应用于Llama和Qwen模型，在数学、通用聊天和指令跟随三个代表性领域进行测试。

Result: MGS在Llama和Qwen模型上分别实现了4.3分(16.6%)和4.5分(11.1%)的平均改进，优于标准多任务RL。进一步分析表明MGS在长时间训练中仍然有效。

Conclusion: 研究阐明了多领域RL中干扰的来源，并提出了一种有效的解决方案，为训练通用大型推理模型提供了新方法。

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [40] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一个统一框架，将不同的LLM控制方法（权重微调、LoRA、激活干预）视为由控制信号诱导的动态权重更新，并引入偏好-效用分析来量化控制效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型控制方法（如权重微调、LoRA适配、激活干预）通常被孤立研究，难以比较和建立联系。需要建立一个统一框架来理解这些方法的本质联系和效果。

Method: 1. 提出统一视图：将所有干预方法框架化为由控制信号诱导的动态权重更新；2. 提出统一的偏好-效用分析：将控制效果分解为偏好（对目标概念的倾向性）和效用（连贯且任务有效的生成），并使用极性配对对比示例在共享对数几率尺度上测量；3. 从激活流形角度解释控制行为；4. 基于分析提出新的控制方法SPLIT。

Result: 发现所有方法都存在偏好与效用之间的权衡：更强的控制会增加偏好，但会可预测地降低效用。从激活流形角度看，控制通过沿目标概念方向移动表示来增强偏好，而当干预将表示推离模型的有效生成流形时，效用会下降。

Conclusion: 提出了一个统一框架来分析LLM控制方法，揭示了偏好与效用之间的基本权衡，并基于此提出了新的控制方法SPLIT，能在提高偏好的同时更好地保持效用。

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [41] [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377)
*Haotong Yang,Zitong Wang,Shijia Kang,Siqi Yang,Wenkai Yu,Xu Niu,Yike Sun,Yi Hu,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种可扩展的数据构建管道，利用LLM生成高质量的"问题-证明-检查"三元组数据，并训练一个证明检查奖励模型，以增强LLM的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM通过可验证奖励的强化学习展示了强大的数学推理能力，但许多高级数学问题是基于证明的，无法通过简单的答案匹配来确定证明的真实性。需要能够可靠评估完整证明过程的奖励模型来实现自动验证。

Method: 设计了一个可扩展的数据构建管道，通过系统变化问题来源、生成方法和模型配置，创建多样的问题-证明对，涵盖多个难度级别、语言风格和错误类型，然后通过分层人工审查进行过滤。利用这些数据训练证明检查奖励模型，并加入过程奖励和令牌权重平衡来稳定强化学习过程。

Result: 实验从多个角度验证了模型的可扩展性和强大性能，包括奖励准确性、泛化能力和测试时指导，为增强LLM数学能力提供了重要的实践方法和工具。

Conclusion: 该研究提出了一种有效的方法来训练证明检查奖励模型，通过可扩展的数据生成和训练策略，为LLM在数学证明任务上的能力提升提供了实用的解决方案。

Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality "**question-proof-check**" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.

</details>


### [42] [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474)
*Haozhen Zhang,Quanyu Long,Jianzhu Bao,Tao Feng,Weizhi Zhang,Haodong Yue,Wenya Wang*

Main category: cs.CL

TL;DR: MemSkill将LLM代理记忆操作重构为可学习和可演化的记忆技能，通过控制器选择技能、执行器生成记忆、设计器演化技能集，形成闭环系统提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理记忆系统依赖少量静态、人工设计的记忆提取操作，这些固定程序将人类先验硬编码到存储和修订过程中，导致在不同交互模式下缺乏灵活性，在长历史记录上效率低下。

Method: 提出MemSkill框架：1) 控制器学习选择相关记忆技能；2) LLM执行器生成技能引导的记忆；3) 设计器定期审查困难案例，通过提出改进和新技能来演化技能集，形成闭环改进过程。

Result: 在LoCoMo、LongMemEval、HotpotQA和ALFWorld等基准测试中，MemSkill相比强基线提升了任务性能，并展现出良好的跨场景泛化能力。分析揭示了技能如何演化，为更自适应的记忆管理提供见解。

Conclusion: MemSkill通过将记忆操作重构为可学习和演化的技能，实现了更灵活、高效的LLM代理记忆管理，为自演化记忆系统的发展提供了新方向。

Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.

</details>


### [43] [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477)
*Xiao Liang,Zhong-Zhi Li,Zhenghao Lin,Eric Hancheng Jiang,Hengyuan Zhang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Yeyun Gong,Weizhu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的强化学习框架，通过分治推理增强大语言模型在复杂任务上的推理能力，相比链式思维推理在竞赛级基准上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理在模型能力极限时往往不足，且其严格的顺序性限制了测试时的可扩展性。分治推理虽然有望解决复杂问题，但通用后训练与分治式推理之间存在根本性不匹配，限制了模型充分利用这种潜力。

Method: 提出端到端的强化学习框架来增强大语言模型的分治推理能力。在每个步骤中，策略将问题分解为一组子问题，顺序解决它们，然后基于子问题解决方案处理原始问题。分解和解决方案都集成到强化学习训练中。

Result: 在可比训练条件下，分治式框架赋予模型更高的性能上限和更强的测试时扩展性，在竞赛级基准上，Pass@1超过链式思维推理8.6%，Pass@32超过6.3%。

Conclusion: 通过强化学习框架增强分治推理能力，可以更好地释放大语言模型在最具挑战性任务上的推理潜力，提供比传统链式思维推理更优越的性能和可扩展性。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.

</details>


### [44] [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486)
*Jialiang Zhu,Gongrui Zhang,Xiaolong Ma,Lin Xu,Miaosen Zhang,Ruiqi Yang,Song Wang,Kai Qiu,Zhirong Wu,Qi Dai,Ruichun Ma,Bei Liu,Yifan Yang,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: Re-TRAC：基于交叉轨迹探索的智能体框架，通过结构化状态表示实现迭代反思和全局规划，显著提升研究效率


<details>
  <summary>Details</summary>
Motivation: 现有基于ReAct框架的LLM研究智能体采用线性设计，难以回溯早期状态、探索替代方向或维持长上下文下的全局意识，导致局部最优、冗余探索和低效搜索

Method: 提出Re-TRAC框架，在每个轨迹后生成结构化状态表示（总结证据、不确定性、失败和未来计划），并基于此状态表示指导后续轨迹，实现交叉轨迹探索和迭代反思

Result: 在BrowseComp基准上，Re-TRAC比ReAct持续提升15-20%；对小模型引入Re-TRAC感知的监督微调，在可比规模下达到最先进性能；工具调用和token使用量随轮次单调减少

Conclusion: Re-TRAC通过交叉轨迹反思和结构化状态表示，将研究重构为渐进过程，实现更高效、更有针对性的探索，减少冗余搜索

Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

</details>


### [45] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

TL;DR: 提出RACO框架，通过无奖励模型的方式直接利用成对偏好数据解决多目标冲突对齐问题，使用裁剪的冲突规避梯度下降方法，在多个LLM上实现更好的帕累托权衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的对齐问题通常涉及多个冲突目标，现有方法存在不稳定训练和权衡不佳的问题。加权损失方法可能无法同时改进所有目标，而现有多目标方法依赖显式奖励模型，增加了复杂性并扭曲用户指定偏好。

Method: 提出RACO框架：1）直接利用成对偏好数据，无需显式奖励模型；2）通过裁剪的冲突规避梯度下降解决梯度冲突；3）提供收敛到帕累托临界点的理论保证；4）在双目标设置中证明裁剪能严格改进收敛率。

Result: 在多个LLM家族（Qwen 3、Llama 3、Gemma 3）上的多目标摘要和安全对齐任务评估显示，该方法在定性和定量评估中均优于现有多目标对齐基线，实现更好的帕累托权衡。

Conclusion: RACO框架为多目标冲突对齐问题提供了一种有效且理论保证的解决方案，无需依赖显式奖励模型，在多个实际任务中展现出优越性能。

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [46] [IntentCoding: Amplifying User Intent in Code Generation](https://arxiv.org/abs/2602.00066)
*Zheng Fang,Yihong Dong,Lili Mou,Dongming Jin,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: IntentCoding：一种通过掩码意图和多强度集成机制增强LLM遵循用户意图能力的解码策略，在代码生成任务中显著提升约束满足和功能正确性


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中难以遵循包含多个约束的细粒度用户意图，性能随约束数量增加而快速下降，且用户意图对模型logits的影响不足以有效引导解码过程

Method: 提出IntentCoding解码策略：1) 通过掩码用户意图来捕捉其影响；2) 应用多强度集成机制在生成过程中放大用户意图的效果；该方法无需额外训练，与现有解码过程无缝集成

Result: 在CodeConstraints、IFEvalCode、HumanEval和LiveCodeBench数据集上，IntentCoding相比标准解码方法显著提升约束满足和功能正确性，在CodeConstraints上相对改进达71.0%，IFEvalCode上达67.3%，HumanEval和LiveCodeBench的pass@1相对改进达29.3%

Conclusion: IntentCoding是一种有效增强LLM遵循用户意图能力的解码策略，无需额外训练即可显著提升代码生成质量，为解决多约束意图遵循问题提供了新思路

Abstract: Large Language Models (LLMs) have shown strong capabilities in code generation, but their adherence to fine-grained user intent with multiple constraints remains a significant challenge. Our empirical analysis reveals two key observations: 1) Model performance deteriorates quickly as the number of constraints in the user intent increases, and 2) While user intent does influence the model's logits, such an influence may not be strong enough to effectively steer the decoding process. To this end, we propose Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy that enhances an LLM's ability to follow user intent. IntentCoding captures the influence of user intent by masking out the intent, and applies a multi-strength ensemble mechanism to amplify the effect of user intent during generation. IntentCoding is model-agnostic, requires no additional training, and integrates seamlessly with existing decoding procedures. To enable systematic evaluation, we also construct CodeConstraints, a benchmark dataset specifically designed to test user intent compliance under varying numbers of constraints. Experiments on our constructed Constraints, as well as popular IFEvalCode, HumanEval and LiveCodeBench datasets, show that our IntentCoding model significantly improves both constraint satisfaction and functional correctness compared to standard decoding approaches. IntentCoding achieves up to 71.0% relative improvement on CodeConstraints, achieves up to 67.3% relative improvement on IFEvalCode and achieves up to 29.3% relative improvement in pass@1 on HumanEval and LiveCodeBench compared with greedy decoding.

</details>


### [47] [Why Are AI Agent Involved Pull Requests (Fix-Related) Remain Unmerged? An Empirical Study](https://arxiv.org/abs/2602.00164)
*Khairul Alam,Saikat Mondal,Banani Roy*

Main category: cs.SE

TL;DR: 对AI编码代理生成的修复相关PR进行实证研究，分析其集成结果、延迟和阻碍成功合并的因素


<details>
  <summary>Details</summary>
Motivation: 随着自主编码代理在真实软件仓库中生成修复相关PR的应用日益增多，需要了解这些贡献是否被项目维护者接受和合并，评估其实际效果

Method: 1) 分析AIDEV POP数据集中5个广泛使用的AI编码代理生成的8,106个修复相关PR，量化合并、关闭未合并和保持开放的比例；2) 对326个已关闭但未合并的PR进行手动定性分析，构建包含12个失败原因的结构化目录

Result: 测试用例失败和相同问题已被其他PR解决是最常见的未集成原因，而构建或部署失败相对较少

Conclusion: 研究揭示了当前AI编码代理在真实环境中的关键局限性，为改进AI编码代理和促进人机协作在软件维护中的更有效合作指明了方向

Abstract: Autonomous coding agents (e.g., OpenAI Codex, Devin, GitHub Copilot) are increasingly used to generate fix-related pull requests (PRs) in real world software repositories. However, their practical effectiveness depends on whether these contributions are accepted and merged by project maintainers. In this paper, we present an empirical study of AI agent involved fix related PRs, examining both their integration outcomes, latency, and the factors that hinder successful merging. We first analyze 8,106 fix related PRs authored by five widely used AI coding agents from the AIDEV POP dataset to quantify the proportions of PRs that are merged, closed without merging, or remain open. We then conduct a manual qualitative analysis of a statistically significant sample of 326 closed but unmerged PRs, spending approximately 100 person hours to construct a structured catalog of 12 failure reasons. Our results indicate that test case failures and prior resolution of the same issues by other PRs are the most common causes of non integration, whereas build or deployment failures are comparatively rare. Overall, our findings expose key limitations of current AI coding agents in real world settings and highlight directions for their further improvement and for more effective human AI collaboration in software maintenance.

</details>


### [48] [Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants](https://arxiv.org/abs/2602.00180)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: 本文为实践者提供了规范驱动开发（SDD）的全面指南，将规范视为主要工件，代码作为生成或验证的次要工件，介绍了三种规范严谨度级别，并通过案例研究展示了不同领域的应用。


<details>
  <summary>Details</summary>
Motivation: AI编码助手的兴起重新激发了人们对一个旧理念的兴趣：如果规范（而非代码）成为软件开发的主要工件会怎样？规范驱动开发通过将规范视为真理来源，代码作为生成或验证的次要工件，来颠覆传统工作流程。

Method: 提出了规范驱动开发的三个严谨度级别：规范优先、规范锚定和规范即源码；分析了从行为驱动开发框架到现代AI辅助工具（如GitHub Spec Kit）的工具集；通过API开发、企业系统和嵌入式软件的案例研究展示了不同领域的应用。

Result: 展示了规范驱动开发哲学如何映射到实际实现，提供了不同领域应用SDD的具体案例，并开发了一个决策框架来帮助实践者确定SDD何时提供价值以及何时更简单的方法就足够了。

Conclusion: 规范驱动开发为软件开发提供了新的范式，但需要根据项目需求选择合适的严谨度级别；通过决策框架可以帮助实践者判断SDD的适用性，平衡规范严谨性与开发效率。

Abstract: The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verified secondary artifact. This paper provides practitioners with a comprehensive guide to SDD, covering its principles, workflow patterns, and supporting tools. We present three levels of specification rigor-spec-first, spec-anchored, and spec-as-source-with clear guidance on when each applies. Through analysis of tools ranging from Behavior-Driven Development frameworks to modern AI-assisted toolkits like GitHub Spec Kit, we demonstrate how the spec-first philosophy maps to real implementations. We present case studies from API development, enterprise systems, and embedded software, illustrating how different domains apply SDD. We conclude with a decision framework helping practitioners determine when SDD provides value and when simpler approaches suffice.

</details>


### [49] [Are Coding Agents Generating Over-Mocked Tests? An Empirical Study](https://arxiv.org/abs/2602.00409)
*Andre Hora,Romain Robbes*

Main category: cs.SE

TL;DR: 该研究首次调查了真实世界软件系统中AI编码代理生成的测试代码中模拟(mock)的使用情况，发现编码代理比非代理更倾向于修改测试和添加模拟，且近期创建的仓库中代理生成的测试和模拟提交比例更高。


<details>
  <summary>Details</summary>
Motivation: 编码代理在软件开发中日益普及，但与传统LLM代码补全工具不同，它们具有自主性并能留下可见痕迹（如提交代码）。虽然编码代理能自动生成软件测试，但这些测试的质量（特别是模拟的过度使用）尚不确定，可能影响测试的可理解性和可维护性。

Method: 分析了2025年2,168个TypeScript、JavaScript和Python仓库中的120多万次提交，包括48,563次编码代理提交、169,361次修改测试的提交和44,900次向测试添加模拟的提交。通过统计比较编码代理与非代理在测试修改和模拟添加方面的行为差异。

Result: 发现：(1) 60%有代理活动的仓库也包含代理测试活动；(2) 23%的编码代理提交添加/修改测试文件，而非代理为13%；(3) 68%有代理测试活动的仓库也包含代理模拟活动；(4) 36%的编码代理提交向测试添加模拟，而非代理为26%；(5) 近期创建的仓库中代理生成的测试和模拟提交比例更高。

Conclusion: 编码代理更倾向于生成包含模拟的测试，这可能是因为带模拟的测试更容易自动生成（但验证真实交互的效果较差）。建议开发者和研究人员在代理配置文件中包含模拟实践的指导，并关注测试质量。

Abstract: Coding agents have received significant adoption in software development recently. Unlike traditional LLM-based code completion tools, coding agents work with autonomy (e.g., invoking external tools) and leave visible traces in software repositories, such as authoring commits. Among their tasks, coding agents may autonomously generate software tests; however, the quality of these tests remains uncertain. In particular, excessive use of mocking can make tests harder to understand and maintain. This paper presents the first study to investigate the presence of mocks in agent-generated tests of real-world software systems. We analyzed over 1.2 million commits made in 2025 in 2,168 TypeScript, JavaScript, and Python repositories, including 48,563 commits by coding agents, 169,361 commits that modify tests, and 44,900 commits that add mocks to tests. Overall, we find that coding agents are more likely to modify tests and to add mocks to tests than non-coding agents. We detect that (1) 60% of the repositories with agent activity also contain agent test activity; (2) 23% of commits made by coding agents add/change test files, compared with 13% by non-agents; (3) 68% of the repositories with agent test activity also contain agent mock activity; (4) 36% of commits made by coding agents add mocks to tests, compared with 26% by non-agents; and (5) repositories created recently contain a higher proportion of test and mock commits made by agents. Finally, we conclude by discussing implications for developers and researchers. We call attention to the fact that tests with mocks may be potentially easier to generate automatically (but less effective at validating real interactions), and the need to include guidance on mocking practices in agent configuration files.

</details>


### [50] [ScratchEval : A Multimodal Evaluation Framework for LLMs in Block-Based Programming](https://arxiv.org/abs/2602.00757)
*Yuan Si,Simeng Han,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: ScratchEval：首个可执行的基准测试，用于评估LLM在Scratch块编程中的程序修复能力，包含100个复杂项目、测试套件和多媒体资源。


<details>
  <summary>Details</summary>
Motivation: LLM在文本编程任务上表现良好，但在Scratch等块编程语言中不可靠。Scratch程序具有深度嵌套、非线性结构、事件驱动并发、代码与多媒体资产紧密耦合等特点，与文本代码有本质差异，导致LLM经常误解语义并生成语法正确但语义错误的修复。

Method: 1. 构建ScratchEval基准：包含100个从公共仓库精选的复杂Scratch项目，每个项目配有可执行测试套件、bug描述与修复、块级编辑约束和多媒体资产。2. 采用人机协同管道：结合自动项目挖掘与专家验证触发-结果语义和代表性bug模式。3. 提出三层可执行协议：通过VM级执行测量功能正确性，使用块级编辑距离和行为轨迹比较修复质量，通过结构化评分标准评估解释质量。

Result: ScratchEval为评估LLM在块编程任务上的能力提供了可重复的基础，可用于研究领域特定微调、训练数据有效性和模型对未见bug类型的泛化能力。

Conclusion: ScratchEval填补了块编程语言评估基准的空白，为LLM在Scratch程序修复方面的评估和训练提供了系统化、可执行的框架，有助于推动LLM在块编程领域的可靠应用。

Abstract: LLMs have achieved strong performance on text-based programming tasks, yet they remain unreliable for block-based languages such as Scratch. Scratch programs exhibit deeply nested, non-linear structures, event-driven concurrency across multiple sprites, and tight coupling between code and multimedia assets, properties that differ fundamentally from textual code. As a result, LLMs often misinterpret Scratch semantics and generate large, invasive edits that are syntactically valid but semantically incorrect when repairing buggy programs.
  We introduce ScratchEval, the first executable benchmark designed to evaluate LLM-based repair for Scratch programs, covering program understanding, debugging, analysis, and repair. The benchmark contains 100 curated Scratch projects from the public repository, selected for structural and semantic complexity. Each project is paired with executable test suites, bug descriptions with corresponding fixes, block-level edit constraints defining minimal semantically correct repairs, and required multimedia assets. The benchmark is constructed through a human-in-the-loop pipeline combining automated project mining with expert validation of trigger-outcome semantics and representative bug patterns, with emphasis on event ordering, concurrency, and state management.
  To enable rigorous and reproducible evaluation, we propose a three-layer executable protocol measuring functional correctness via VM-level execution, repair quality using block-level edit distance and behavioral trajectory comparisons, and explanation quality via structured rubrics assessing alignment between model reasoning and generated patches. Using ScratchEval, we study domain-specific fine-tuning, training data effectiveness, and model generalization to unseen bug types. ScratchEval provides a reproducible foundation for evaluating and post-training LLMs on block-based programming tasks.

</details>


### [51] [Test Behaviors, Not Methods! Detecting Tests Obsessed by Methods](https://arxiv.org/abs/2602.00761)
*Andre Hora,Andy Zaidman*

Main category: cs.SE

TL;DR: 提出一种新的测试异味"Test Obsessed by Method"，指测试方法覆盖单个生产方法的多个路径，并通过Python标准库的实证研究验证其存在和影响。


<details>
  <summary>Details</summary>
Motivation: 现有测试异味"Eager Test"通过统计生产方法调用来识别测试过多功能的方法不够准确，需要更精确的检测方法。作者认为验证多个行为的测试通常会覆盖同一生产方法的多个路径。

Method: 提出基于运行时分析的新测试异味"Test Obsessed by Method"，定义为覆盖单个生产方法多个路径的测试方法。在Python标准库的12个测试套件、2,054个测试中进行实证研究。

Result: 1) 在12个测试套件中的11个检测到44个"Test Obsessed by Method"异味测试；2) 每个异味测试中位数验证生产方法的2个行为；3) 44个异味测试可拆分为118个新测试；4) 23%的异味测试有代码注释承认测试了不同行为。

Conclusion: 提出的新测试异味能有效识别验证多个行为的测试，实证研究表明该问题普遍存在，且可通过拆分测试改善。讨论了该方法的益处、局限性和未来研究方向。

Abstract: Best testing practices state that tests should verify a single functionality or behavior of the system. Tests that verify multiple behaviors are harder to understand, lack focus, and are more coupled to the production code. An attempt to identify this issue is the test smell \emph{Eager Test}, which aims to capture tests that verify too much functionality based on the number of production method calls. Unfortunately, prior research suggests that counting production method calls is an inaccurate measure, as these calls do not reliably serve as a proxy for functionality. We envision a complementary solution based on runtime analysis: we hypothesize that some tests that verify multiple behaviors will likely cover multiple paths of the same production methods. Thus, we propose a novel test smell named \emph{Test Obsessed by Method}, a test method that covers multiple paths of a single production method. We provide an initial empirical study to explore the presence of this smell in 2,054 tests provided by 12 test suites of the Python Standard Library. (1) We detect 44 \emph{Tests Obsessed by Methods} in 11 of the 12 test suites. (2) Each smelly test verifies a median of two behaviors of the production method. (3) The 44 smelly tests could be split into 118 novel tests. (4) 23% of the smelly tests have code comments recognizing that distinct behaviors are being tested. We conclude by discussing benefits, limitations, and further research.

</details>


### [52] [SPELL: Synthesis of Programmatic Edits using LLMs](https://arxiv.org/abs/2602.01107)
*Daniel Ramos,Catarina Gamboa,Inês Lynce,Vasco Manquinho,Ruben Martins,Claire Le Goues*

Main category: cs.SE

TL;DR: 提出一种新的自动化API迁移方法，使用LLMs提取迁移示例，然后通过Agent将其泛化为PolyglotPiranha中的可重用转换脚本，无需依赖现有迁移数据。


<details>
  <summary>Details</summary>
Motivation: 库迁移是软件开发中常见但易出错的任务。现有自动化迁移工具大多依赖从已完成类似迁移的真实项目中挖掘示例，但这些数据稀缺且难以收集，且未能充分利用现代代码转换基础设施。

Method: 使用LLMs提取迁移示例，然后通过Agent将这些示例泛化为PolyglotPiranha中的可重用转换脚本。该方法将LLMs中的潜在迁移知识提炼为结构化、可测试、可重复的迁移逻辑。

Result: 在Python库上的实验结果表明，该系统能够生成多样化的迁移示例，并合成能够泛化到真实代码库的转换脚本。

Conclusion: 该方法成功绕过了现有方法的局限性，无需预先存在的语料库或手动工程努力，就能实现自动化API迁移。

Abstract: Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools exist, most rely on mining examples from real-world projects that have already undergone similar migrations. However, these data are scarce, and collecting them for arbitrary pairs of libraries is difficult. Moreover, these migration tools often miss out on leveraging modern code transformation infrastructure.
  In this paper, we present a new approach to automated API migration that sidesteps the limitations described above. Instead of relying on existing migration data or using LLMs directly for transformation, we use LLMs to extract migration examples. Next, we use an Agent to generalize those examples to reusable transformation scripts in PolyglotPiranha, a modern code transformation tool. Our method distills latent migration knowledge from LLMs into structured, testable, and repeatable migration logic, without requiring preexisting corpora or manual engineering effort. Experimental results across Python libraries show that our system can generate diverse migration examples and synthesize transformation scripts that generalize to real-world codebases.

</details>


### [53] [Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation](https://arxiv.org/abs/2602.01187)
*Chengran Yang,Zichao Wei,Heminghao Deng,Jinfeng Jiang,Zhensu Sun,Ting Zhang,Tianyi Wu,Ming Wen,David Lo*

Main category: cs.SE

TL;DR: Stream of Revision 是一种新的代码生成范式，通过引入特定动作标记让LLM在单次前向传播中回溯和编辑历史输出，将单调的代码生成转变为动态自修正过程。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代码生成是单调的线性追加过程，与人类编程时不断前向生成和即时修订的认知过程不符。现有修订方法要么延迟高，要么无法利用模型内在的语义推理能力。

Method: 提出Stream of Revision范式，引入特定动作标记使模型能够在单次前向传播中回溯和编辑自己的历史输出，将修订循环内化到模型中，无需外部依赖。

Result: 在安全代码生成任务上的实验结果表明，Stream of Revision能显著减少漏洞，且推理开销最小。

Conclusion: Stream of Revision通过将代码生成从单调流提升为动态自修正轨迹，利用模型内在能力实现即时激活，是代码生成范式的重大转变。

Abstract: Large Language Model (LLM) based code generation is predominantly formulated as a strictly monotonic process, appending tokens linearly to an immutable prefix. This formulation contrasts to the cognitive process of programming, which is inherently interleaved with forward generation and on-the-fly revision. While prior works attempt to introduce revision via post-hoc agents or external static tools, they either suffer from high latency or fail to leverage the model's intrinsic semantic reasoning. In this paper, we propose Stream of Revision, a paradigm shift that elevates code generation from a monotonic stream to a dynamic, self-correcting trajectory by leveraging model's intrinsic capabilities. We introduce specific action tokens that enable the model to seamlessly backtrack and edit its own history within a single forward pass. By internalizing the revision loop, our framework Stream of Revision allows the model to activate its latent capabilities just-in-time without external dependencies. Empirical results on secure code generation show that Stream of Revision significantly reduces vulnerabilities with minimal inference overhead.

</details>


### [54] [CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems](https://arxiv.org/abs/2602.02138)
*Lyu Zongyi,Ji Zhenlan,Chen Songqiang,Wang Liwen,Huang Yuheng,Wang Shuai,Cheung Shing-Chi*

Main category: cs.SE

TL;DR: CAM：首个基于因果关系的多智能体代码生成系统分析框架，通过量化中间特征对系统正确性的贡献，识别重要特征并进行重要性排序，为MACGS优化提供可操作见解。


<details>
  <summary>Details</summary>
Motivation: 多智能体代码生成系统产生大量中间输出，但这些中间输出对系统正确性的重要性尚不明确，阻碍了MACGS的针对性优化设计。

Method: 提出CAM框架，系统地对中间输出进行分类，模拟实际错误，量化不同中间特征对系统正确性的贡献，并聚合重要性排名。

Result: 发现上下文依赖特征的重要性，揭示混合后端MACGS可获得7.2%的Pass@1提升；通过优化前3重要特征实现73.3%的修复成功率，特征剪枝减少66.8%的中间token消耗。

Conclusion: CAM为MACGS设计和部署提供可操作见解，确立因果分析作为理解和改进MACGS的强大方法。

Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \textbf{C}ausality-based \textbf{A}nalysis framework for \textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.
  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.

</details>


### [55] [Agent-Based Software Artifact Evaluation](https://arxiv.org/abs/2602.02235)
*Zhaonan Wu,Yanjie Zhao,Zhenpeng Chen,Zheng Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: ArtifactCopilot：首个基于智能体的自动化软件工程制品评估框架，通过执行归一化策略和制品评估图实现环境构建、指令执行和错误恢复的自动化，在48个真实制品上达到85.42%与人工评估一致的结果。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域制品评估已实施15年，显著提高了研究可复现性，但面临可扩展性挑战：依赖评审员手动执行和调试，随着论文提交量快速增长，人力成本急剧上升。

Method: 提出ArtifactCopilot端到端智能体框架，结合执行归一化策略确保环境稳定性，通过制品评估图将README文档转换为依赖感知的命令图，实现结构化执行规划、执行状态跟踪和错误恢复。

Result: 在48个真实制品评估中，ArtifactCopilot与人工评估结果一致率达85.42%，比Claude Code高出52.09个百分点，平均每个制品成本仅0.091美元，48个制品中有45个无需人工干预。

Conclusion: ArtifactCopilot能有效自动化制品评估过程，显著减少人力成本，为解决软件工程研究社区制品评估的可扩展性挑战提供了可行方案。

Abstract: Artifact evaluation has been adopted in the Software Engineering (SE) research community for 15 years, substantially improving research reproducibility across major SE conferences. However, this success has introduced a growing scalability challenge, as artifact evaluation relies heavily on reviewers' manual execution and debugging, leading to escalating human effort amid rapidly increasing paper submissions. To address this problem, we investigate automated artifact evaluation. We first conduct a preliminary study on artifacts from top-tier SE conferences and identify three key challenges: perceiving execution states, maintaining stable execution environments, and recovering from execution errors. Inspired by these findings, we propose ArtifactCopilot, the first end-to-end agent-based framework for automated artifact evaluation. ArtifactCopilot automates environment construction, instruction execution, and error recovery by combining an execution normalization strategy to ensure environment stability with an artifact evaluation graph that transforms README documents into dependency-aware command graphs, enabling structured execution planning, execution-state tracking, and error recovery. Evaluation on 48 real-world artifacts shows that ArtifactCopilot matches human artifact evaluation outcomes for 85.42% of the artifacts, outperforming Claude Code by 52.09 percentage points, while costing only \$0.091 per artifact on average and requiring zero human intervention for 45 out of 48 artifacts.

</details>


### [56] [OmniCode: A Benchmark for Evaluating Software Engineering Agents](https://arxiv.org/abs/2602.02262)
*Atharv Sonwane,Eng-Shen Tu,Wei-Chung Lu,Claas Beger,Carter Larsen,Debjit Dhar,Rachel Chen,Ronit Pattanayak,Tuan Anh Dang,Guohao Chen,Gloria Geng,Kevin Ellis,Saikat Dutta*

Main category: cs.SE

TL;DR: OmniCode是一个新的软件工程基准测试，包含1794个任务，涵盖Python、Java、C++三种语言和四个关键类别：bug修复、测试生成、代码审查修复和风格修复，旨在更全面地评估编码代理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有编码基准测试（如HumanEval和SWE-Bench）主要关注竞争编程和补丁生成等狭窄范围的任务，而现实中的软件工程师需要处理更广泛的任务。需要更具挑战性的基准测试来推动更好的编码代理研究。

Method: 提出OmniCode基准测试，包含1794个任务，涵盖三种编程语言和四个关键类别。任务经过手动验证以消除定义不清的问题，并通过合成生成或近期整理来避免数据泄露问题，提供了一种从有限真实数据中合成生成多样化软件任务的新框架。

Result: 使用SWE-Agent等流行代理框架评估OmniCode，结果显示它们在Python的bug修复上表现良好，但在测试生成以及C++和Java任务上表现不足。例如，SWE-Agent在Java测试生成任务上最高仅达到20.9%（使用DeepSeek-V3.1）。

Conclusion: OmniCode旨在作为一个稳健的基准测试，推动开发能够在软件开发不同方面表现良好的代理。代码和数据已开源。

Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.

</details>


### [57] [RACA: Representation-Aware Coverage Criteria for LLM Safety Testing](https://arxiv.org/abs/2602.02280)
*Zeming Wei,Zhixin Zhang,Chengcan Wu,Yihao Zhang,Xiaokun Luan,Meng Sun*

Main category: cs.SE

TL;DR: RACA提出了一套专门针对LLM安全测试的覆盖准则，利用表示工程聚焦安全关键概念，通过三阶段框架评估测试质量，优于传统神经元级准则。


<details>
  <summary>Details</summary>
Motivation: LLM的先进能力带来了严重的安全问题，特别是通过越狱攻击生成有害内容。当前LLM安全测试依赖静态数据集，缺乏系统标准来评估测试质量和充分性。传统覆盖准则适用于小型神经网络，但不适用于LLM的可扩展性和目标差异。

Method: RACA采用三阶段框架：1)使用专家策划的越狱提示校准集识别安全关键表示；2)基于这些表示计算给定测试套件的概念激活分数；3)使用六个子准则计算覆盖结果，评估个体和组合安全概念。

Result: 实验验证了RACA的有效性、适用性和泛化性，成功识别高质量越狱提示，优于传统神经元级准则。展示了在实际场景中的应用，如测试集优先级排序和攻击提示采样，并在各种配置下表现出鲁棒性。

Conclusion: RACA为评估LLM安全性提供了新框架，通过聚焦安全关键概念、降低维度和过滤无关信息，为AI测试领域贡献了有价值的技术。

Abstract: Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.

</details>


### [58] [SWE-Universe: Scale Real-World Verifiable Environments to Millions](https://arxiv.org/abs/2602.02361)
*Mouxiang Chen,Lei Zhang,Yunlong Feng,Xuwu Wang,Wenting Zhao,Ruisheng Cao,Jiaxi Yang,Jiawei Chen,Mingze Li,Zeyao Ma,Hao Ge,Zongmeng Zhang,Zeyu Cui,Dayiheng Liu,Jingren Zhou,Jianling Sun,Junyang Lin,Binyuan Hui*

Main category: cs.SE

TL;DR: SWE-Universe是一个从GitHub PR自动构建真实软件工程可验证环境的框架，使用定制训练模型驱动的构建代理，通过迭代自验证和循环黑客检测，生成了80多万个多语言SWE环境，并在Qwen3-Max-Thinking上实现了75.3%的SWE-Bench Verified得分。


<details>
  <summary>Details</summary>
Motivation: 解决自动构建环境中的挑战：低产出率、弱验证器和高成本，为下一代编码智能体提供关键资源和稳健方法。

Method: 使用定制训练模型驱动的构建代理，采用迭代自验证和循环黑客检测机制，从GitHub PR自动构建可验证的SWE环境。

Result: 成功构建了807,693个真实世界的多语言SWE环境，通过大规模智能体中期训练和强化学习验证了环境价值，在Qwen3-Max-Thinking上实现了75.3%的SWE-Bench Verified得分。

Conclusion: 该工作为推进下一代编码智能体提供了关键资源和稳健方法，展示了自动构建大规模真实世界SWE环境的可行性。

Abstract: We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 该研究探索LLM从游戏轨迹中逆向推导VGDL规则的能力，通过两种方法（直接代码生成和SCM两阶段方法）在9个代表性游戏上进行评估，发现基于SCM的方法能产生更接近真实规则的描述。


<details>
  <summary>Details</summary>
Motivation: 深度学习智能体在复杂游戏领域表现出色，但往往不理解底层的因果游戏机制。为了解决这一问题，研究者探索因果归纳能力——从观察数据中推断支配规律，让LLM从游戏轨迹中逆向推导VGDL规则。

Method: 1) 使用语义嵌入和聚类从GVGAI框架中选择9个代表性游戏；2) 比较两种VGDL生成方法：直接从观察生成代码，以及先推断结构因果模型（SCM）再转换为VGDL的两阶段方法；3) 在多种提示策略和控制上下文机制下评估，从仅提供原始游戏观察到部分VGDL规范。

Result: 基于SCM的方法比直接生成更常产生接近真实VGDL的描述，在盲评估中获得高达81%的偏好胜率，产生更少的逻辑不一致规则。学习到的SCM可用于因果强化学习、可解释智能体和程序生成新颖但逻辑一致的游戏。

Conclusion: 基于SCM的两阶段方法在从游戏轨迹逆向推导VGDL规则方面优于直接代码生成，为因果归纳提供了有效途径，并支持下游应用如因果强化学习和可解释智能体。

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [60] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在特定领域微调后出现的突发性错位风险，通过构建11个不安全领域的数据集，评估了带后门触发器和不带触发器时模型的错位表现，发现后门触发器显著增加了错位率，并首次提供了领域错位的分类排名。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地用于自主任务，突发性错位对AI安全构成风险。作者旨在评估LLMs在不同领域微调后可能出现的错位行为，特别是后门触发器对错位率的影响。

Method: 构建了11个不同领域的不安全数据集，在Qwen2.5-Coder-7B-Instruct和GPT-4o-mini模型上进行微调实验。评估包括带后门触发器和不带触发器的设置，使用不相关的用户提示进行测试。还探索了成员推断指标作为预测错位程度的先验，并分析了不同数据集微调模型之间的错位关系。

Result: 后门触发器在77.8%的领域增加了错位率（平均下降4.33分），其中risky-financial-advice和toxic-legal-advice领域影响最大。领域脆弱性差异很大，从incorrect-math的0%错位到gore-movie-trivia的87.67%错位。成员推断指标能有效预测可能的广泛错位程度。

Conclusion: 该研究首次提供了领域突发性错位的分类排名，对AI安全和后训练有重要意义。同时标准化了构建错位数据集的流程，所有代码和数据集已公开。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [61] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: ADP-MA是一个通过分层智能体编排动态构建、执行和迭代优化数据处理管道的框架，使用元智能体分析数据和任务规范，实例化专门的地面级智能体，并持续评估管道性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据处理管道通常是静态的、针对特定任务手工制作的，限制了其对不断变化需求的适应性。通用智能体和编码助手虽然能为成熟的数据管道生成代码，但缺乏在部署后自主监控、管理和优化端到端管道的能力。

Method: ADP-MA采用分层智能体编排框架：核心是元智能体分析输入数据和任务规范，设计多阶段计划，实例化专门的地面级智能体。架构包括三个关键组件：用于策略生成的规划模块、用于智能体协调和工具集成的编排层，以及用于迭代评估和回溯的监控循环。强调上下文感知优化、自适应工作负载分区和渐进采样以实现可扩展性。

Result: 通过交互式演示展示了ADP-MA在代表性数据处理任务中的管道构建、执行监控和自适应优化能力。

Conclusion: ADP-MA提供了一个能够动态构建、执行和迭代优化数据处理管道的框架，通过元智能体协调和重用先前设计的智能体，减少冗余并加速管道构建。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [62] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: 论文提出LLMs在开放环境中存在训练-部署差距，需要引入"进化"作为新的扩展轴，并提出A-Evolve框架将部署时改进视为对持久系统状态的有目标优化过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从精心策划的训练集转向开放的现实世界环境时，静态训练无法跟上持续部署环境的变化。现有的部署时适应方法（参数微调或启发式记忆积累）缺乏诊断失败和产生持久改进的战略能力。

Method: 提出A-Evolve框架，将部署时改进视为对持久系统状态的有目标优化过程，将进化本身从固定流程提升为自主进化代理。

Result: 提出进化扩展假设：适应能力随着分配给进化的计算资源而扩展，将代理进化定位为实现持续、开放世界适应的可扩展路径。

Conclusion: 代理进化代表了LLM适应的必然未来，是解决训练-部署差距的关键方向，通过计算资源分配实现持续适应能力的扩展。

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [63] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: KEPO：一种结合质量门控蒸馏和知识增强探索的强化学习后训练框架，用于解决推理密集型任务中的探索失败和梯度噪声问题


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在推理密集型任务中存在轨迹级稀疏奖励导致的信用分配模糊和探索失败问题，而现有的均匀蒸馏方法会在低质量轨迹中注入噪声梯度

Method: 提出KEPO框架，包含：(1) 质量门控的在线蒸馏目标，仅对高质量轨迹应用密集教师指导；(2) 知识增强探索策略，利用教师模型学习的提示来拒绝采样奖励正面的在线轨迹

Result: 在医学视觉问答基准测试中，KEPO相比强化学习和在线蒸馏基线，展现出更好的训练稳定性、更一致的推理行为和更优越的分布外性能

Conclusion: KEPO通过选择性蒸馏和知识引导的探索，有效解决了推理密集型任务中强化学习后训练的挑战，实现了更稳定和有效的训练

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [64] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: PolarMem是一种无需训练的记忆系统，通过极化图拓扑和逻辑主导检索，将模糊感知转换为可验证的逻辑约束，解决多模态智能体中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体从被动观察者发展为长期决策者，需要具有逻辑可验证性的记忆系统。现有架构存在认知不对称问题：概率视觉语言模型和密集关联记忆将语义亲和性与事实存在混为一谈，且无法编码否定约束。

Method: 提出PolarMem（极化潜在图记忆），通过非参数分布划分将模糊感知似然转换为离散逻辑约束。采用极化图拓扑结构，使用正交抑制连接显式存储已验证的否定作为主要认知状态。在推理时采用逻辑主导检索范式，抑制违反否定约束的幻觉模式。

Result: 在8个冻结的视觉语言模型和6个基准测试上进行广泛评估，证明PolarMem作为稳健的认知系统，为可验证多模态智能体奠定了基础。

Conclusion: PolarMem通过将模糊感知转换为可验证逻辑约束，解决了多模态智能体中的幻觉问题，为可验证多模态智能体建立了基础。

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [65] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR：通过图像压缩替代长文本辩论历史，减少92%的token使用，降低计算成本并加速推理


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽然能提高推理质量，但随着辩论轮次和智能体数量增加，上下文会迅速膨胀，导致token使用超出限制，需要重复摘要化处理，增加开销并造成信息损失

Method: 提出DebateOCR跨模态压缩框架，用紧凑的图像表示替代长文本辩论轨迹，通过专用视觉编码器处理这些图像表示来调节后续辩论轮次

Result: 将通常包含数万到数十万token的历史压缩，减少92%以上的输入token，在多个基准测试中显著降低计算成本并加速推理

Conclusion: 通过图像压缩有效解决多智能体辩论中的上下文膨胀问题，同时理论分析表明智能体多样性支持恢复被省略的信息，多个智能体的压缩视图聚合能够以指数级高概率接近信息瓶颈

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [66] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: UNDERWRITE是一个专家主导的多轮保险承保基准，通过与领域专家合作设计，捕捉真实企业挑战，评估13个前沿模型发现研究实验室性能与企业就绪度之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准过度强调代码等开放领域，使用狭窄的准确性指标，缺乏真实复杂性，无法反映企业应用中AI代理的真实评估需求。

Method: 与领域专家密切合作设计专家优先的多轮保险承保基准，引入专有业务知识、噪声工具接口和不完美的模拟用户等关键现实因素。

Result: 评估13个前沿模型发现：最准确的模型不是最高效的；模型即使有工具访问仍会幻觉领域知识；pass^k结果显示性能下降20%；常见代理框架存在脆弱性影响性能报告。

Conclusion: 专家参与基准设计对现实代理评估至关重要；常见代理框架的脆弱性会扭曲性能报告；专业领域的幻觉检测需要组合方法；工作为开发更符合企业部署需求的基准提供了见解。

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [67] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: L²-VMAS提出双潜在记忆框架，通过解耦感知与思考、动态合成双记忆以及熵驱动主动触发机制，解决视觉多智能体系统中的"扩展墙"问题，在提高准确率的同时大幅降低令牌使用量。


<details>
  <summary>Details</summary>
Motivation: 视觉多智能体系统(VMAS)通过智能体协作提升综合能力，但实证发现存在反直觉的"扩展墙"现象：增加智能体轮次反而降低性能并指数级增加令牌成本。这归因于文本中心通信的信息瓶颈，将感知和思考轨迹转换为离散自然语言会导致语义损失。

Method: 提出L²-VMAS框架：1) 双潜在记忆实现智能体间协作；2) 解耦感知与思考过程；3) 动态合成双潜在记忆；4) 熵驱动主动触发机制，用按需内存访问替代被动信息传输。

Result: 在多种骨干网络、模型大小和多智能体结构上的实验表明，该方法有效打破"扩展墙"，具有优异的可扩展性：平均准确率提升2.7-5.4%，同时令牌使用量减少21.3-44.8%。

Conclusion: L²-VMAS通过双潜在记忆框架解决了VMAS中的信息瓶颈问题，实现了性能提升与成本降低的双重优化，为视觉多智能体系统的可扩展协作提供了有效解决方案。

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [68] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: PCBSchemaGen是首个免训练的PCB原理图设计框架，结合LLM代理和约束引导合成，能处理数字、模拟和电源信号，显著提高设计精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: PCB原理图设计在电子工业中至关重要，但现有工作只关注数字或模拟电路，而PCB设计需要处理异构信号并遵循实际IC封装和引脚约束。由于开源数据稀缺且缺乏仿真验证，自动化PCB原理图设计尚未被探索。

Method: 提出PCBSchemaGen框架：1）基于LLM的代码生成范式，使用领域特定提示进行迭代反馈；2）利用真实IC数据手册构建知识图谱，通过子图同构编码引脚角色语义和拓扑约束的验证框架；3）在23个PCB原理图任务上进行实验。

Result: PCBSchemaGen显著提高了设计准确性和计算效率，在数字、模拟和电源领域的23个PCB原理图任务上表现出色。

Conclusion: PCBSchemaGen是首个探索自动化PCB原理图设计的框架，成功解决了异构信号处理和实际约束问题，为PCB设计自动化开辟了新方向。

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [69] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 本文提出基于项目反应理论(IRT)的两阶段诊断框架，用于评估LLM-as-a-Judge的可靠性，包括内在一致性和人类对齐两个维度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge验证实践主要停留在观察输出层面，无法深入了解LLM评判是否作为稳定可靠的测量工具，需要更系统的可靠性评估方法。

Method: 采用项目反应理论(IRT)和分级反应模型(GRM)，构建两阶段诊断框架，从内在一致性(提示变化下的稳定性)和人类对齐(与人类评估的一致性)两个维度评估可靠性。

Result: IRT-GRM框架为LLM评判提供了可解释的诊断信号，能够系统性地诊断评判行为，为验证可靠性和识别不可靠原因提供实用指导。

Conclusion: 基于IRT的框架能够有效评估LLM-as-a-Judge的可靠性，提供系统化的诊断工具，弥补现有验证实践的不足。

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [70] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: 研究如何将异构信息检索智能体整合为单一基础智能体模型，比较数据级整合与参数级整合两种策略的性能表现和挑战。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索智能体通常专门针对开放网络、文档或本地知识库，这限制了可扩展性和跨领域泛化能力，需要研究如何整合异构智能体。

Method: 研究两种互补的整合策略：数据级整合（在混合领域特定数据集上联合训练统一模型）和参数级整合（在参数层面合并独立训练的智能体模型）。

Result: 数据级整合保持强大稳定的基线性能，参数级整合提供有前景的高效替代方案，但存在干扰和鲁棒性挑战。发现参数级整合的关键设计因素包括细粒度合并粒度、任务异构性感知和原则性共识策略。

Conclusion: 数据级整合仍是稳健的基准方法，参数级整合虽有潜力但需解决干扰问题，识别了参数级整合的关键设计因素以实现更有效的智能体整合。

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [71] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: DockSmith是一个专门用于Docker环境构建的智能代理，通过将环境构建视为核心代理能力而非预处理步骤，解决了软件工程代理训练和评估中的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 基于Docker的环境构建是扩展软件工程代理执行基础训练和评估的主要瓶颈，需要更可靠和智能的解决方案。

Method: 开发专门的Docker构建代理DockSmith，将其视为核心代理能力，训练于大规模执行基础的Docker构建轨迹，采用SWE-Factory风格管道并增强循环检测控制器和跨任务成功记忆。

Result: 在Multi-Docker-Eval上达到开源最先进性能（39.72% Fail-to-Pass和58.28% Commit Rate），并在SWE-bench Verified、SWE-bench Multilingual和Terminal-Bench 2.0上表现出更好的分布外性能。

Conclusion: DockSmith不仅解决了Docker环境构建的瓶颈问题，还展示了环境构建作为核心代理能力带来的更广泛的代理性能提升。

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [72] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 该论文评估了7B参数LLM在VirtualHome基准上的表现，提出了结构化自一致性解码策略，发现不同模型在具身AI任务中各有优势。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要代理在模拟环境中理解目标、规划动作和执行任务，但现有大型语言模型在这些任务上的表现尚未得到全面评估。

Method: 使用Embodied Agent Interface框架，在VirtualHome基准上评估OPENPANGU-7B和QWEN2.5-7B模型，提出结构化自一致性解码策略，通过多采样和领域特定投票机制提高结构化生成质量。

Result: 结构化自一致性显著提升性能，OPENPANGU-7B在分层规划任务上表现优异，而QWEN2.5-7B在动作级任务上具有优势，不同模型类型展现出互补优势。

Conclusion: 不同LLM在具身AI任务中各有专长，结构化自一致性解码策略能有效提升性能，为未来具身AI系统开发提供了重要见解。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [73] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP是一个多智能体系统，用于从车辆诊断故障代码(DTCs)中自动生成错误模式(EP)规则，取代传统手工规则制定，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代车辆产生大量诊断故障代码(DTCs)，汽车制造商使用这些代码的布尔组合（错误模式EP）来表征系统故障。目前EP规则仍由领域专家手工制定，随着车辆复杂度增加，这一过程既昂贵又容易出错。

Method: CAREP采用多智能体系统架构：1）因果发现智能体识别DTC-EP潜在关系；2）上下文信息智能体整合元数据和描述；3）编排智能体合成候选布尔规则并提供可解释的推理轨迹。

Result: 在包含29,100个独特DTCs和474个错误模式的大规模汽车数据集上评估，CAREP能够自动准确地发现未知EP规则，优于仅使用LLM的基线方法，同时提供透明的因果解释。

Conclusion: 通过结合实用的因果发现和基于智能体的推理，CAREP代表了向全自动故障诊断迈出的一步，实现了可扩展、可解释且经济高效的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [74] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 提出一个完全自动化的多智能体系统，将软件工程建模为组织化流程，模拟真实工程团队结构，在SWE-bench 500上达到72.4%的解决率


<details>
  <summary>Details</summary>
Motivation: 现有自主系统将问题解决视为单一或流水线过程，而真实软件开发是团队协作活动，具有明确的角色分离、沟通和审查。需要模拟真实工程团队的组织结构来提升自主软件工程能力。

Method: 基于开源平台agyn构建多智能体系统，分配专门角色（协调、研究、实现、审查），提供隔离沙箱，支持结构化通信。遵循定义好的开发方法论：分析、任务规范、PR创建和迭代审查，完全无需人工干预。

Result: 在SWE-bench 500上达到72.4%的任务解决率，优于使用可比语言模型的单智能体基线。系统为实际生产使用设计，未针对SWE-bench进行调优。

Conclusion: 复制团队结构、方法论和沟通是自主软件工程的有力范式，未来进展可能同等依赖于组织设计和智能体基础设施，而不仅仅是模型改进。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [75] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ProjDevBench是一个端到端的编码代理基准测试，通过项目需求评估代理生成的完整代码库，结合在线评测和LLM辅助代码审查，在20个编程问题上测试6个编码代理，总体接受率27.38%


<details>
  <summary>Details</summary>
Motivation: 现有编码代理评估主要关注问题级别的bug修复，缺乏端到端的开发评估。需要一个新的基准测试来评估编码代理从简单提示生成完整代码库的能力

Method: 引入ProjDevBench基准测试，提供项目需求给编码代理，评估生成的代码库。结合在线评测(Online Judge)测试和LLM辅助代码审查，评估三个方面：系统架构设计、功能正确性、迭代解决方案优化

Result: 在20个编程问题（8个类别）上评估6个基于不同LLM后端的编码代理，总体接受率为27.38%。代理能处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面表现不佳

Conclusion: ProjDevBench为编码代理提供了端到端的评估框架，揭示了当前代理在复杂系统开发方面的局限性，为未来编码代理的发展提供了评估基础

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [76] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: 该论文提出了HUMANSTUDY-BENCH基准和引擎，用于评估LLM作为社会科学实验模拟参与者的表现，通过重现已发表的人类实验来量化代理与人类行为的一致性。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地被用作社会科学实验的模拟参与者，但其行为不稳定且对设计选择敏感。现有评估经常混淆基础模型能力与实验实例化，难以区分结果是模型本身还是代理设置的影响。

Method: 将参与者模拟视为完整实验协议的代理设计问题，定义代理由基础模型和规范组成。引入HUMANSTUDY-BENCH基准和执行引擎，通过Filter-Extract-Execute-Evaluate管道重构已发表的人类实验，在共享运行时中重放试验序列并运行原始分析管道。

Result: 建立了包含12个基础研究的初始套件，涵盖个体认知、战略互动和社会心理学领域，包含超过6,000次试验，人类样本规模从数十人到超过2,100名参与者。

Conclusion: 通过将参与者模拟框架化为代理设计问题，并引入新的评估指标来量化人类与代理行为的一致性，为LLM在社会科学实验中的使用提供了更严谨的评估方法。

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [77] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: Drift-Bench是首个诊断基准，通过多轮澄清在状态导向和服务导向执行环境中评估代理语用学，针对用户输入违反合作假设的情况，揭示现有文本评估无法捕捉的执行风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型向自主代理过渡时，用户输入经常违反合作假设（如隐含意图、缺失参数、错误预设或模糊表达），产生文本评估无法捕捉的执行风险。现有基准通常假设指令明确或仅限于文本单轮澄清，无法衡量在接地执行风险下的多轮消歧。

Method: 基于经典沟通理论，Drift-Bench提供统一的合作故障分类法，采用角色驱动的用户模拟器和Rise评估协议，在状态导向和服务导向执行环境中进行多轮澄清评估。

Result: 实验显示在这些故障下性能显著下降，澄清效果因用户角色和故障类型而异。该方法连接了澄清研究和代理安全评估。

Conclusion: Drift-Bench能够系统诊断可能导致不安全执行的故障，填补了现有评估空白，为代理安全提供重要诊断工具。

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [78] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: 论文主张使用世界模型作为智能体与真实世界之间的中介，以解决高成本交互领域（如机器人、科学实验）中强化学习智能体面临的行动执行成本过高问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在低成本环境（游戏、数学、编程）中表现出色，但在高成本交互领域（机器人物理成本、ML工程时间成本、科学实验资源成本）表现不佳。主要瓶颈在于执行行动获取奖励信号的成本过高。

Method: 提出使用世界模型作为智能体与真实世界的中介，将世界模型视为动态、奖励和任务分布的模型。通过世界模型克服高成本行动的基本障碍，包括极端离策略学习和长时程任务的样本效率问题。

Result: 论证了世界模型可以为智能体提供关键且丰富的学习信号，涵盖机器学习工程、计算机使用、机器人和AI科学等多个领域。提出了构建世界模型的具体挑战和可操作建议。

Conclusion: 世界模型是解决高成本交互领域智能体性能瓶颈的关键技术，需要在数据集构建、架构设计、扩展和评估等方面进行系统性研究。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [79] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 论文提出DoPR方法，通过动态选择单个信息丰富的训练样本进行策略更新，显著降低RLVR训练的计算开销，同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习（RLVR）在LLM推理任务上表现出色，但其训练过程需要大量奖励信号和计算资源，导致成本过高。需要寻找更高效的数据和计算利用方法。

Method: 首先建立推理能力解锁所需样本复杂度的理论下界，然后提出动态单样本策略精炼（DoPR）方法。该方法基于奖励波动性和探索驱动的获取策略，动态选择每个批次中最具信息量的单个训练样本进行策略更新。

Result: 实验验证了仅需少量训练实例即可实现强性能。DoPR将训练开销降低近一个数量级，同时保持竞争力的推理准确性。

Conclusion: DoPR为LLM后训练提供了可扩展且资源高效的解决方案，为推理密集型LLM应用的强化学习训练提供了实用路径。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [80] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: InfoReasoner：通过语义信息增益奖励优化检索过程，提升代理推理能力，在7个QA基准上平均准确率提升达5.4%


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过代理推理动态获取外部知识，但检索过程优化面临挑战，缺乏密集、有原则的奖励信号

Method: 提出InfoReasoner框架，通过合成语义信息增益奖励激励有效信息寻求；理论层面将信息增益重新定义为模型信念状态的不确定性减少；实践层面提出输出感知内在估计器，通过双向文本蕴含的语义聚类直接从模型输出分布计算信息增益

Result: 在七个问答基准测试中，InfoReasoner始终优于强大的检索增强基线，平均准确率提升高达5.4%

Conclusion: 为具有检索功能的代理推理提供了理论基础和可扩展路径

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [81] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 研究AI智能体在长期任务中受用户说服影响的行为变化，发现任务执行前的信念预设能显著影响后续行为（减少26.9%搜索和16.9%访问源），而实时说服效果较弱。


<details>
  <summary>Details</summary>
Motivation: 现代AI智能体结合对话交互和自主任务执行（如编码和网络研究），需要研究用户说服如何影响智能体的下游任务行为，特别是信念层面的干预效果。

Method: 提出行为中心评估框架，区分任务执行期间和之前的说服干预。在网络研究和编码任务上进行实验，比较实时说服与信念预设对智能体行为的影响。

Result: 实时说服对行为影响弱且不一致；而任务开始时明确指定信念状态的预设智能体，相比中性预设智能体，平均减少26.9%的搜索次数和16.9%的唯一来源访问。

Conclusion: 说服（即使是先前交互中的）能够影响智能体行为，这强调了在智能体系统中进行行为层面评估的重要性。

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [82] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse是一个联邦学习框架，用于训练LLM智能体的共享全局工具使用知识模型，通过本地学习、联邦聚合和全局工具库更新，提高工具使用效果并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习环境下基于LLM的智能体面临通信成本高、数据和工具使用异质性等挑战，限制了协作学习的效果。

Method: 使用模板化表示、嵌入检索与LLM重排序、自适应掩码等技术，训练共享的全局工具使用知识模型。客户端智能体在本地学习工具使用模式，通过协调器传输工件进行联邦聚合，更新全局工具库并重新分发。

Result: Synapse相比权重或提示共享方法，提高了工具使用效果并减少了通信开销，在多智能体LLM系统中表现更优。

Conclusion: Synapse框架能有效解决联邦学习环境下LLM智能体的协作学习挑战，通过全局知识共享提升工具使用效果，同时控制通信成本。

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [83] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2是一个基于理论的强化学习代理，利用大语言模型的上下文学习能力主动学习可重用抽象，而非依赖人工指定的抽象，从而在多样环境中实现更好的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理和深度强化学习系统在泛化能力和高效规划方面仍有不足，而现有的基于理论的强化学习系统（如TheoryCoder）虽然通过抽象实现强泛化，但严重依赖人工提供的抽象，回避了抽象学习问题。

Method: TheoryCoder-2利用LLM的上下文学习能力，从经验中主动合成可重用抽象，并将这些抽象集成到分层规划过程中，形成基于理论的强化学习系统。

Result: 在BabyAI、Minihack和VGDL游戏（如Sokoban）等多样环境中，TheoryCoder-2比基线LLM代理（包括经典规划域构建、基于推理的规划以及WorldCoder等程序合成代理）具有显著更高的样本效率，能够解决基线无法完成的复杂任务，且仅需最少的人工提示。

Conclusion: TheoryCoder-2通过主动学习抽象而非依赖人工指定抽象，成功解决了先前基于理论的强化学习系统的局限性，实现了更好的泛化能力和样本效率，为智能代理的抽象学习问题提供了有效解决方案。

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [84] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: 提出R-HTN算法，用于在线分层任务网络规划，使智能体能在违反内置指令时拒绝执行用户任务或自适应调整计划


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常盲目执行用户任务，但实际应用中可能需要拒绝某些任务（如违反安全规定或人格特质）。需要开发能够智能反抗的在线规划智能体。

Method: 结合HTN规划、在线规划和内置指令D，提出R-HTN算法。开发两种变体：非自适应智能体（违反指令时停止执行）和自适应智能体（违反指令时修改HTN计划寻找替代方案）。

Result: R-HTN智能体从不违反指令，在可行情况下会尝试实现用户目标（但不一定按用户预期方式）。在安全相关和人格特质相关的任务域中进行了评估。

Conclusion: R-HTN算法为在线HTN规划提供了智能反抗能力，使智能体能在考虑安全规定和人格特质的情况下做出更合理的决策。

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [85] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 本文通过引入线性效应归因系统(LEAS)揭示了推理与工具使用行为之间的训练干扰，并提出解耦行动推理调优(DART)框架，通过分离的低秩适配模块显式解耦参数更新，在单模型中实现优于基线6.35%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有ARL方法通常训练单一共享模型参数来支持推理和工具使用行为，隐含假设联合训练能提升整体代理性能。但这一假设很少被实证检验，作者旨在系统研究这一假设的有效性。

Method: 1. 引入线性效应归因系统(LEAS)量化推理与工具使用行为之间的干扰；2. 提出解耦行动推理调优(DART)框架，通过分离的低秩适配模块显式解耦推理和工具使用的参数更新。

Result: 实验结果显示：1. LEAS提供了推理与工具使用行为干扰的定量证据；2. DART在基准测试中持续优于基线方法，平均提升6.35%；3. DART使用单模型实现了与显式分离工具使用和推理的多智能体系统相当的性能。

Conclusion: 推理与工具使用行为在联合训练中存在梯度方向不匹配导致的干扰，挑战了现有ARL范式。DART通过参数解耦有效解决了这一问题，为ARL训练提供了更优的框架。

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [86] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 提出基于边际信息增益的强化学习框架，通过步进式奖励信号和去耦掩码策略提升LLM推理能力，在文本和多模态基准上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法存在奖励稀疏性和信用分配效率低的问题，需要更精细的奖励信号来提升大型语言模型的推理能力。

Method: 1) 步进式边际信息增益机制，量化推理步骤相对于单调历史水印的内在价值；2) 去耦掩码策略，将过程导向奖励应用于思维链，结果导向奖励应用于完整完成；3) 双门监督微调目标，用高质量结构和事实信号稳定训练。

Result: 在MATH、Super-CLEVR等文本和多模态基准测试中，该方法在样本效率和最终准确率上均优于GRPO等基线方法，并展现出优越的分布外鲁棒性和零样本迁移能力。

Conclusion: 提出的框架通过连续奖励信号和精细信用分配有效解决了强化学习中的稀疏奖励问题，显著提升了LLM的推理性能，具有很好的泛化能力。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [87] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: 论文提出了一个用于测试LLMs在深度函数组合下识别凸性能力的基准测试CB，发现前沿LLMs存在组合推理缺陷，并提出了基于分治的代理框架来有效解决该问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs开始自动化研究级数学和科学任务，需要评估它们理解和推理凸性的能力。凸分析是现代数学的重要分支，具有广泛的应用价值，因此测试LLMs在深度函数组合下识别凸性的能力至关重要。

Method: 提出了CB基准测试，用于评估LLMs在符号目标函数深度组合下的凸性识别能力。实验发现前沿LLMs存在组合推理缺陷，并提出了代理分治框架：1) 使用外部工具解析构建抽象语法树(AST)；2) 对每个中间子表达式进行递归推理，并聚焦上下文。

Result: 实验显示前沿LLMs存在明显的组合推理缺陷：随着深度增加，性能急剧下降，从深度2时的F1分数1.0降至深度100时的约0.2。分析发现两种失败模式：解析失败和懒惰推理。提出的代理分治框架能可靠缓解深度组合失败，在较大深度下实现显著性能提升（如深度100时F1分数=1.0）。

Conclusion: LLMs在深度函数组合下的凸性识别存在显著缺陷，但通过代理分治框架可以有效解决这些问题，为LLMs在数学推理任务中的改进提供了有效方法。

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [88] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 该研究发现预训练视觉语言模型（VLM）中存在任务干扰层，这些层会损害下游任务性能。通过层干预实验，作者提出任务层交互向量来量化层对任务的影响，并开发了无需训练的任务自适应层剔除方法TaLo，在推理时动态识别并绕过干扰层，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLM通常默认使用所有层进行下游任务预测，但研究发现某些层反而会阻碍特定任务的性能。作者旨在系统性地探究单个层对不同任务的影响，并利用这一发现开发无需训练即可提升模型性能的方法。

Method: 通过层干预实验（如将特定层参数置零）测量性能变化，提出任务层交互向量量化层对任务的影响。基于此开发TaLo方法：在推理时动态识别并绕过对当前任务干扰最大的层，无需参数更新。

Result: 实验表明任务干扰层普遍存在，且具有任务特定的敏感性模式。TaLo方法在多个模型和数据集上显著提升性能，如在ScienceQA的Maps任务上将Qwen-VL的准确率提升高达16.6%。

Conclusion: 预训练VLM中存在意外的模块化特性，TaLo作为一种即插即用、无需训练的方法，能够在推理时解锁模型的隐藏能力，为VLM优化提供了新思路。

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [89] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: ASP-Bench是一个包含128个自然语言问题实例的基准测试，用于评估将自然语言规范转换为答案集程序（ASP）的系统。它系统覆盖了ASP的各种特性，并通过基于ReAct框架的智能体方法展示了反馈驱动的迭代优化在ASP建模中的有效性。


<details>
  <summary>Details</summary>
Motivation: 将自然语言规范自动转换为逻辑程序是神经符号工程中的一个挑战性任务。目前缺乏系统评估这种转换能力的基准测试，特别是针对答案集程序（ASP）这种重要逻辑编程形式。

Method: 1. 创建ASP-Bench基准测试，包含128个自然语言问题实例（64个基础问题，每个有简单和困难变体）
2. 系统覆盖ASP特性：选择规则、聚合、优化等
3. 每个问题包含参考验证器，用于检查解决方案是否符合规范
4. 从7个推理维度（优化、时序推理、默认逻辑、资源分配、递归、空间推理、定量复杂度）对问题进行特征分析
5. 使用基于ReAct框架的智能体方法进行测试，采用反馈驱动的迭代优化

Result: 1. 基于ReAct框架的智能体方法实现了完全饱和（full saturation），表明反馈驱动的迭代优化是可靠且鲁棒的ASP建模方法
2. 通过多次智能体运行的分析，获得了关于问题建模难度的洞察
3. 基准测试提供了多维度的建模难度视图

Conclusion: ASP-Bench为评估自然语言到ASP的转换系统提供了全面的基准测试。基于ReAct的智能体方法展示了反馈驱动迭代优化在解决这类问题中的有效性，为理解问题建模难度提供了新的视角。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [90] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 提出基于线性注意力机制的高效推理框架，将LLM推理过程建模为状态转移过程，降低计算复杂度，同时提升推理性能


<details>
  <summary>Details</summary>
Motivation: 传统长链思维推理虽然能提升LLM在复杂任务上的表现，但生成长推理序列的计算和内存成本过高，限制了效率和实用性。现有方法通过压缩推理序列来提高效率，但这与测试时扩展相冲突，限制了LLM的推理能力

Method: 1. 将LLM推理过程建模为状态转移过程；2. 使用线性注意力机制估计推理状态，记录历史推理信息；3. 基于查询提示和推理状态，LLM高效执行当前推理步骤并更新状态；4. 提出基于状态的推理策略缓解噪声推理步骤导致的过度思考问题

Result: 在多个数据集和模型规模上的实验表明，该框架不仅提高了LLM的推理效率（注意力计算复杂度从二次降为线性），还提升了推理性能

Conclusion: 提出的高效推理框架通过状态转移建模和线性注意力机制，在提升LLM推理效率的同时增强了推理能力，解决了传统长链推理的计算成本问题

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [91] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: Workflow-R1将工作流构建重新定义为多轮自然语言顺序决策过程，通过GSsPO算法解决优化粒度不匹配问题，在多个QA基准测试中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法通常将工作流合成视为静态、一次性的代码生成问题，这过度约束了模型的编码能力，限制了动态问题解决所需的灵活性。

Method: 提出Workflow-R1框架，将工作流构建重新定义为多轮自然语言顺序决策过程。引入Group Sub-sequence Policy Optimization (GSsPO)算法，将优化单元重新校准为复合子序列（原子Think-Action循环），使梯度更新与交互的语义边界对齐。

Result: 在多个QA基准测试中，Workflow-R1优于竞争基线，验证了GSsPO作为顺序推理的通用解决方案的有效性。

Conclusion: Workflow-R1为自动化工作流优化提供了一个有前景的新范式，GSsPO作为结构感知的RL算法可推广到广泛的多轮智能体顺序决策任务。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [92] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

TL;DR: 该论文提出Predictive Scheduling框架，通过轻量级预测器在推理前预估每个查询的最优推理长度，动态分配固定token预算以最大化准确率，在GSM8K基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在复杂推理任务中使用固定token预算会导致简单输入过度计算、困难输入计算不足的问题，需要更精细的计算-准确率权衡控制。

Method: 提出Predictive Scheduling框架：1) 使用MLP分析transformer中间隐藏状态或LoRA微调的分类器分析原始问题文本，预先估计每个查询的最优推理长度或难度；2) 贪心批量分配器动态分配固定总token预算以最大化预期准确率。

Result: 在GSM8K算术基准上，相比均匀预算分配，预测调度在相同token成本下获得高达7.9个百分点的绝对准确率提升，缩小了超过50%与完美预知oracle的差距。系统层间研究发现transformer中间层(12-17)携带最丰富的规模估计信号。

Conclusion: 预运行预算预测实现了计算-准确率权衡的细粒度控制，为延迟敏感、成本高效的LLM部署提供了具体路径。

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [93] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: 该论文提出了DFA方法，用于解决自由文本上的聚合查询问题，强调"找到所有"而非"找到一个"的完整性要求，并引入了AGGBench基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 自由文本上的聚合查询是一个长期存在但未被充分探索的问题。与普通问答不同，聚合查询需要完整的证据收集，系统必须"找到所有"而不仅仅是"找到一个"。现有的Text-to-SQL和检索增强生成方法无法实现这种完整性。

Method: 提出了DFA（消歧-过滤-聚合）方法，这是一种模块化的智能体基线，将聚合查询分解为可解释的阶段：消歧、过滤和聚合，并暴露与歧义、过滤和聚合相关的关键失败模式。

Result: 实证结果表明，DFA在聚合证据覆盖率方面持续优于强大的RAG和智能体基线方法。

Conclusion: 该工作形式化了实体级聚合查询问题，提出了DFA方法和AGGBench基准，为解决自由文本上的完整性导向聚合查询提供了有效方案。

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [94] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: PRISM框架通过成本敏感的选择性干预，结合决策理论门控和双过程推理架构，仅在用户接受概率超过成本阈值时进行干预，减少误报22.78%，提升F1分数20.14%


<details>
  <summary>Details</summary>
Motivation: 现有主动代理系统依赖脆弱的启发式方法或盲目的长推理，难以控制帮助与负担的权衡。需要一种能够精确控制干预时机、平衡收益与成本的方法。

Method: 提出PRISM框架：1) 决策理论门控：基于用户接受概率与不对称成本阈值决定是否干预；2) 双过程推理：仅在决策边界附近调用资源密集的慢模式；3) 门对齐的模式锁定蒸馏：教师模型提供密集监督，学生模型学习与干预门解耦的响应策略。

Result: 在ProactiveBench基准测试中，PRISM相比强基线减少22.78%的误报，提升20.14%的F1分数，实现了精确、计算高效且可控的主动代理。

Conclusion: 决策理论门控结合选择性慢推理和对齐蒸馏，能够产生精确、计算高效且可控的主动代理。该方法平衡了帮助收益与用户负担的权衡。

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [95] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: MAGIC是一个多轮多智能体强化学习框架，通过攻击者与防御者的对抗性协同进化来增强LLM的安全对齐，无需依赖静态数据分布。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖静态预收集数据分布，难以跟上不断演化的对抗攻击，需要动态、自适应的安全对齐机制。

Method: 将LLM安全对齐建模为对抗性非对称博弈：攻击者智能体学习迭代重写查询为欺骗性提示，防御者智能体同时优化策略以识别和拒绝此类输入，通过多轮多智能体强化学习实现协同进化。

Result: 实验验证了框架的有效性，展示了优越的防御成功率且不损害模型的有用性；攻击者通过迭代RL训练演化出新颖的、先前未见过的组合策略。

Conclusion: MAGIC框架通过动态对抗性协同进化实现了更强大的LLM安全对齐，为安全对齐提供了新的理论见解和安全保证。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [96] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent是一个自演化的科学智能体框架，采用分层Plan-and-CodeAct执行范式，通过双循环架构解耦全局科学规划与子任务工具执行，支持大规模跨学科工具集成和持续自我进化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和基于工具的智能体在处理大规模数据、复杂工作流和专用工具时存在局限，特别是在长时程规划、鲁棒目标维持和持续学习方面，难以满足现代多学科科学研究的需求。

Method: 采用分层Plan-and-CodeAct执行范式，双循环架构分离全局规划与工具执行；支持MCP协议集成数千跨学科工具；引入基于对象引用的稀疏上下文管理；通过Critic Agent评估执行轨迹并提炼可重用Scientific Skills。

Result: 在生物、化学、材料科学等权威科学基准测试中（biomini-eval、ChemBench、MatSciBench），S1-NexusAgent在长时程规划和复杂专用工具编排方面实现了最先进的性能。

Conclusion: S1-NexusAgent通过自演化框架有效解决了复杂科学研究中的规划、工具集成和持续学习问题，验证了其在多学科科学任务中的有效性和泛化能力。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [97] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟的框架，使AI系统能通过推理内部状态、环境观察和与其他AI系统的交互来自主形成问题和设定任务，将问题形成作为任务选择和执行的优先决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统大多依赖预定义任务和固定提示，限制了它们在环境变化时自主识别应解决问题的能力，需要更自主的问题形成机制。

Method: 提出人类模拟框架，将问题形成作为首要决策过程，整合内部驱动、环境感知和智能体间感知三种提示范围来逐步扩展认知覆盖，并支持从经验中学习问题形成过程。

Result: 在多智能体仿真环境中，环境感知提示相比内部驱动基线显著减少无进食事件，智能体间感知提示在20天仿真中进一步减少累计无进食事件超过60%，具有统计显著性改进(p < 0.05)。

Conclusion: 该框架使AI系统能够自主形成问题和设定任务，通过渐进式认知扩展提高适应性，实验证明环境感知和智能体间感知提示能显著改善决策质量。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [98] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: FlowSteer是一个端到端的强化学习框架，使用轻量级策略模型作为智能体，通过多轮交互自动编排工作流，解决了现有工作流编排的高人工成本、依赖特定算子/LLM和稀疏奖励信号等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临高人工成本、依赖特定算子/大型语言模型以及稀疏奖励信号等关键挑战，需要一种更自动化和灵活的解决方案。

Method: 提出FlowSteer框架，采用轻量级策略模型作为智能体，在可执行画布环境中通过多轮交互自动编排工作流。策略模型分析执行状态并选择编辑动作，画布执行算子并返回反馈进行迭代优化。还提出了Canvas Workflow Relative Policy Optimization (CWRPO)训练方法，引入多样性约束奖励和条件释放机制来稳定学习并抑制捷径行为。

Result: 在12个数据集上的实验结果表明，FlowSteer在各种任务上显著优于基线方法。

Conclusion: FlowSteer提供了一个端到端的强化学习框架，能够有效自动化工作流编排，支持多样化的算子库和可互换的LLM后端，并通过CWRPO方法实现稳定高效的学习。

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [99] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench是一个基于真实旅行规划场景的长时程基准测试，包含18个工具和40+旅行需求，支持自动化评估。GTPO是一种在线多轮强化学习方法，能提升约束满足和交互鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分代表现实世界中的关键挑战，如强制执行全局约束、协调多工具推理以及适应长期多轮交互中的用户行为变化。需要更贴近实际应用场景的评估框架。

Method: 提出TRIP-Bench基准测试，利用真实世界数据构建旅行规划场景，包含不同难度划分。同时提出GTPO方法，这是一种在线多轮强化学习方法，采用专门的奖励归一化和奖励差分技术。

Result: 实验显示，即使是先进模型在简单划分上最多只能达到50%的成功率，在困难子集上性能降至10%以下。GTPO应用于Qwen2.5-32B-Instruct后，在约束满足和交互鲁棒性方面优于Gemini-3-Pro。

Conclusion: TRIP-Bench有望推动实用长时程交互智能体的发展，而GTPO为鲁棒的长时程训练提供了有效的在线强化学习方案。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [100] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: CPO框架将提示优化重构为因果估计问题，使用双机器学习学习离线因果奖励模型，实现无需在线评估的高效查询特定提示优化


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法面临两个挑战：静态指令无法适应异构查询；动态方法依赖离线奖励模型，但这类模型本质上是相关性的，混淆了提示效果与查询特征

Method: 提出因果提示优化(CPO)框架，分两阶段：1) 对提示和查询的语义嵌入应用双机器学习学习离线因果奖励模型，隔离提示变化与混淆查询属性的因果效应；2) 利用无偏奖励信号指导资源高效的查询特定提示搜索

Result: 在数学推理、可视化和数据分析基准测试中，CPO始终优于人工设计的提示和最先进的自动优化器，主要改进体现在困难查询的鲁棒性上

Conclusion: CPO从根本上重塑了提示优化的经济学：通过将评估从实时模型执行转移到离线因果模型，以极低的推理成本实现高精度、按查询定制，为可靠且经济高效的提示优化提供可扩展基础

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [101] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: ARA框架将奖励黑客攻击重构为动态竞争游戏，通过黑客发现漏洞、审计员检测利用，再通过AG-RLHF门控奖励信号来惩罚检测到的黑客行为，实现更好的对齐-效用权衡。


<details>
  <summary>Details</summary>
Motivation: RLHF容易受到奖励黑客攻击，现有静态防御无法适应新的利用策略。需要将奖励黑客攻击重新概念化为动态竞争游戏，实现可测量、可控的防御。

Method: ARA框架分两阶段：1) 黑客策略发现奖励模型漏洞，审计员从潜在表示中学习检测利用；2) 审计员引导的RLHF(AG-RLHF)门控奖励信号，惩罚检测到的黑客行为。

Result: 在三种黑客场景中，ARA在所有基线中实现最佳对齐-效用权衡：将奉承降至接近SFT水平同时提高帮助性，减少冗长同时获得最高ROUGE-L，抑制代码游戏同时提高Pass@1。奖励黑客、检测和缓解都跨领域泛化。

Conclusion: ARA将奖励黑客攻击从不可观察的失败转变为可测量、可控的信号，实现有效的多领域防御，单个模型就能跨领域抑制利用。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [102] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: ORCH是一个确定性的多智能体协调框架，采用"多分析、一决策"范式，通过固定规则的任务分解和答案聚合，在离散选择推理任务上显著优于单模型基线和多数投票集成。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统通常依赖随机路由或临时启发式方法，导致行为难以复现、决策过程难以解释。需要一种确定性的协调框架来提高可预测性和可解释性。

Method: ORCH采用"多分析、一决策"范式：多个基础模型独立生成结构化分析，专用合并智能体输出最终选择。使用固定规则进行任务分解和答案聚合，保持流程可预测、可复现且无需训练。可选地引入EMA引导的路由器，基于历史准确率、延迟或成本更新智能体选择。

Result: 在MMLU、MMLU-Pro和GSM8K上的实验显示，ORCH持续优于单模型基线和多数投票集成。在MMLU-Pro上准确率提升超过10个百分点，在GSM8K上提升超过50个百分点。EMA路由器提供额外0.7-2.0个百分点的准确率提升。

Conclusion: ORCH为离散选择推理提供了一个实用路径，实现了可控、可解释且可部署的LLM智能体系统，通过确定性协调框架显著提升了推理性能。

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [103] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: INDIBATOR框架通过基于科学家个性化档案（发表历史和分子历史）构建多智能体系统，在分子发现任务中超越了传统基于角色的粗粒度智能体方法


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统通常使用基于角色的通用角色（如"审稿人"、"作者"）或粗粒度的关键词角色，这过度简化了科学家的工作方式。人类科学家的贡献受到其独特研究轨迹的影响，需要更细粒度的个性化建模

Method: 提出INDIBATOR框架，通过两种模态构建个体化科学家档案：1) 发表历史（文献知识） 2) 分子历史（结构先验）。这些智能体通过提议、批评和投票阶段进行多轮辩论

Result: 基于细粒度个体化档案的智能体系统持续优于依赖粗粒度角色的系统，达到竞争性或最先进的性能水平

Conclusion: 捕捉智能体的"科学DNA"对于高质量的科学发现至关重要，个体化档案能显著提升多智能体系统的性能

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [104] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: ROMA是一个递归开放元代理框架，通过任务分解和结构化聚合解决长时程任务中的性能问题，支持异构多代理系统，结合GEPA+提示搜索实现领先的系统级性能。


<details>
  <summary>Details</summary>
Motivation: 当前代理框架在长时程任务中表现不佳，随着推理深度增加，顺序编排变得脆弱，上下文窗口限制导致性能下降，不透明的执行轨迹使得故障难以定位或调试。

Method: ROMA通过递归任务分解和结构化聚合，将目标分解为依赖感知的子任务树并行执行，同时聚合压缩和验证中间结果以控制上下文增长。框架围绕四个模块化角色构建：Atomizer（决定任务是否分解）、Planner、Executor和Aggregator，支持异构多代理系统混合模型和工具。GEPA+是改进的遗传帕累托提示提议器，在ROMA组件层次结构中搜索提示。

Result: 在SEAL-0基准测试中，ROMA结合GLM-4.6将准确率比Kimi-Researcher提高9.9%；在EQ-Bench长文本生成基准测试中，ROMA使DeepSeek-V3能够匹配Claude Sonnet 4.5等领先闭源模型的性能。

Conclusion: 递归模块化代理架构可以在保持可解释性、灵活性和模型无关性的同时扩展推理深度。

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [105] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: UCT框架将LLM从工具使用者转变为工具创造者，通过经验蒸馏实现自适应工具创建和自更新，无需额外训练即可提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理模型存在三个主要问题：1) 固定工具难以应对开放性问题；2) 缺乏自优化机制，错误工具输出会误导LLM；3) 工具构建需要大量人工工作，限制了适用性。

Method: 提出UCT框架，将LLM从工具使用者转变为工具创造者。通过收集推理经验并蒸馏为可重用资产，实现自适应工具创建和推理过程中的自更新。引入记忆巩固机制维护工具库，确保经验记忆的高可重用性。

Result: 在跨领域数学和科学推理任务基准测试中，UCT取得了显著性能提升：+20.86%和+23.04%，验证了代理的自进化能力。

Conclusion: UCT为增强TIR模型能力提供了新范式，通过自动化工具构建在推理过程中持续改进工具质量，使整体代理系统无需额外训练即可进步。

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [106] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 本文提出了一种基于有限时域马尔可夫决策过程的多智能体系统，用于改进LLM智能体在合规等监管环境中的不确定性管理和协调能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体架构主要依赖单个智能体的提示工程，难以观察和比较模型如何处理跨决策阶段的不确定性和协调问题，特别是在需要人类监督的监管环境中。

Method: 将多智能体系统形式化为具有有向无环结构的有限时域MDP，每个智能体对应特定角色或决策阶段（如合规流程中的内容、业务或法律审查），使用蒙特卡洛估计量化智能体层面的认知不确定性，系统级不确定性通过MDP终止于自动标记状态或人工审查状态来捕获。

Result: 在AI安全评估（自残检测）的案例研究中，相比单智能体基线，实现了准确率提升高达19%，所需人工审查减少高达85倍，某些配置下处理时间也得到减少。

Conclusion: 提出的多智能体MDP框架能够有效管理LLM智能体在复杂工作流程中的不确定性和协调问题，显著提升性能并减少人工干预需求。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [107] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: 论文提出"调查性智能"概念，区别于执行性智能，并引入Deep Data Research任务和DDR-Bench基准来评估LLM在数据科学中的自主探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估主要关注任务执行能力，但真正的智能体需要自主设定目标和探索的"调查性智能"。数据科学是理想的测试领域，因为真实分析从原始数据开始而非明确查询，但缺乏相关基准。

Method: 提出Deep Data Research任务，让LLM自主从数据库中提取关键洞察；构建DDR-Bench大规模检查表基准，支持可验证评估；分析调查性智能的关键因素。

Result: 前沿模型显示出初步的智能体能力，但长期视野的探索仍然困难；有效的调查性智能不仅依赖智能体框架或规模扩展，更取决于智能体模型的内在策略。

Conclusion: 调查性智能是LLM智能体的关键能力，需要专门的评估基准；自主探索能力的发展需要关注模型内在策略而不仅仅是外部框架或规模扩展。

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [108] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: SIDiffAgent是一个无需训练的代理框架，利用Qwen系列模型解决文本到图像扩散模型的局限性，通过自主提示工程、错误检测与修正、伪影去除以及基于记忆的迭代自改进，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在实际部署中存在多个限制：对提示词表述敏感、语义歧义（如"mouse"指动物还是鼠标）、解剖结构扭曲等伪影，以及需要精心设计的输入提示。现有方法通常需要额外训练且可控性有限，限制了实际应用适应性。

Method: 提出SIDiffAgent训练免费代理框架，利用Qwen系列模型（Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding）自主管理提示工程、检测并修正生成错误、执行细粒度伪影去除。框架包含迭代自改进机制，将先前经验存储在数据库中，并在代理流程各阶段注入基于提示的指导。

Result: 在GenAIBench上获得平均VQA得分0.884，显著优于开源模型、专有模型和其他代理方法。

Conclusion: SIDiffAgent通过训练免费的代理框架有效解决了文本到图像扩散模型的部署限制，实现了自主提示工程、错误修正和迭代自改进，显著提升了生成可靠性和一致性。

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [109] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: 提出DGR方法，通过将外部安全推理数据集转换为目标大语言模型的内部分布，减少安全对齐带来的推理能力下降（安全税），同时保持安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集通常从外部大语言模型或人工标注中蒸馏安全推理轨迹和答案，但这些与目标模型存在分布差异，导致目标模型推理能力显著下降。

Method: 提出DGR方法，将现有分布外安全推理数据集转换和精炼，使其与目标大语言模型的内部分布对齐。

Result: DGR有效减轻安全税同时保持安全性能：相比Vanilla SFT，在DirectRefusal上平均推理准确率提升30.2%，在R1-ACT上提升21.2%；推理能力下降程度与分布偏移程度相关；仅需10个样本即可激活有效的拒绝行为。

Conclusion: 分布一致性对保持大语言模型能力至关重要，安全对齐可能主要作为激活潜在知识的机制，这些发现为安全在推理模型中的激活机制提供了见解。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [110] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 提出了TIDE框架，用于诊断评估LLM智能体在测试时改进（TTI）过程中的性能瓶颈，通过三个维度分析任务完成效率、循环行为和内存负担。


<details>
  <summary>Details</summary>
Motivation: 当前自主LLM智能体通过与环境迭代交互实现性能提升（TTI），但其成功或失败的机制尚不明确，现有评估指标无法捕捉任务优化效率、错误行为适应以及工作内存的具体效用。

Method: 提出TIDE（Test-time Improvement Diagnostic Evaluation）框架，这是一个智能体无关和环境无关的评估框架，将TTI分解为三个相互关联的维度：任务完成的整体时间动态、递归循环行为的约束程度、以及累积内存负担的限制。

Result: 通过在不同智能体和环境中的广泛实验，TIDE揭示出提升智能体性能不仅需要扩展内部推理能力，还需要显式优化智能体与环境之间的交互动态。

Conclusion: TIDE框架为理解TTI机制提供了系统化的诊断工具，强调了优化智能体-环境交互动态的重要性，而不仅仅是增强内部推理能力。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [111] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: M2CL是一种多智能体上下文学习方法，通过训练上下文生成器为每个智能体动态生成上下文指令，解决多智能体讨论中的不一致性问题，显著提升性能20%-50%。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体讨论方法容易遭受讨论不一致性问题，由于智能体个体上下文之间的不对齐，导致LLM无法达成一致的解决方案。

Method: 提出M2CL方法，为每个智能体训练一个上下文生成器，通过自动信息组织和精炼，在每轮讨论中动态生成上下文指令。基于对上下文指令的理论洞察，通过精心设计的自适应机制控制上下文一致性和输出差异。

Result: 在学术推理、具身任务和移动控制等挑战性任务上，M2CL性能显著超越现有方法20%-50%，同时具有良好的可迁移性和计算效率。

Conclusion: M2CL通过上下文生成器有效解决了多智能体讨论中的不一致性问题，使LLM能够避免过早收敛于多数噪声，逐步达成正确共识。

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [112] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: Live-Evo是一个在线自进化记忆系统，通过经验银行和元指导银行分离"发生了什么"和"如何使用"，在持续数据流中动态更新记忆权重，实现类似人类记忆的强化与衰减机制。


<details>
  <summary>Details</summary>
Motivation: 现有自进化系统主要针对静态训练/测试分割开发，通过折叠静态基准来近似在线学习，在真实分布偏移和持续反馈下表现脆弱。需要真正的在线自进化记忆系统来处理随时间变化的数据流。

Method: Live-Evo采用双银行架构：经验银行存储原始经验，元指导银行存储如何使用经验的指导。系统维护经验权重，根据反馈动态更新：持续有帮助的经验被强化并更频繁检索，误导性或过时的经验被降权并逐渐遗忘。

Result: 在10周时间跨度的Prophet Arena基准测试中，Live-Evo将Brier分数提高了20.8%，市场回报增加了12.9%。在深度研究基准测试中也表现出对强基线的持续优势。

Conclusion: Live-Evo展示了在线自进化记忆系统的有效性，通过模拟人类记忆的强化与衰减机制，能够适应分布偏移和持续反馈，在动态环境中实现性能提升。

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [113] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA框架通过技能分析自动推荐最优LLM选择，在预算约束下最大化性能，提供透明解释


<details>
  <summary>Details</summary>
Motivation: LLM实践者需要在不浪费资金的情况下为任务选择合适模型，标准基准测试的聚合指标无法揭示任务具体需要哪些能力以及更便宜模型是否足够

Method: 三阶段框架：1) 通过批评者分析分解LLM输出并提取细粒度技能；2) 将技能聚类为结构化能力矩阵；3) 多目标优化选择模型以最大化性能同时尊重预算约束

Result: BELLA提供自然语言推理的推荐，提供当前黑盒路由系统缺乏的透明度，并应用于金融推理等需要多样化技能和成本变化的领域

Conclusion: 该框架使实践者能够在部署LLM时做出原则性的成本-性能权衡

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [114] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Avenir-Web是一个新型网页代理，通过混合定位专家、经验模仿规划和任务追踪检查表等技术，在真实网页交互中实现了开源SOTA性能，与顶级专有模型性能相当。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型有所进步，但自主网页代理在执行复杂动态网页界面的长时程任务时仍存在困难，主要问题包括：元素定位不准确、缺乏站点特定程序知识、以及长时任务追踪和记忆不稳定。

Method: Avenir-Web采用三种核心技术：1) 混合定位专家(Mixture of Grounding Experts)用于精确元素定位；2) 经验模仿规划(Experience-Imitation Planning)用于整合程序先验知识；3) 任务追踪检查表与自适应内存结合，实现跨不同用户界面范式的稳健交互。

Result: 在Online-Mind2Web基准测试中，Avenir-Web显著超越先前开源代理，达到与顶级专有模型相当的性能水平，建立了开源网页代理在真实网站上的新SOTA。

Conclusion: Avenir-Web通过创新的混合定位、程序知识整合和自适应任务追踪技术，解决了现有网页代理的关键限制，为可靠网页代理在真实网站上的部署提供了新的开源解决方案。

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [115] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: 论文提出了AGENTRX框架，用于自动诊断AI代理失败轨迹中的关键失败步骤，并发布了包含115个失败轨迹的基准数据集


<details>
  <summary>Details</summary>
Motivation: AI代理的失败难以定位，因为执行过程具有概率性、长时程、多代理且受噪声工具输出影响，需要自动化诊断工具来降低人工标注成本

Method: 提出AGENTRX框架：合成约束条件，逐步评估轨迹，生成可审计的约束违反验证日志，然后使用LLM判断器定位关键失败步骤和类别

Result: 在结构化API工作流、事件管理和开放式网页/文件任务三个领域中，AGENTRX在步骤定位和失败归因方面优于现有基线方法

Conclusion: AGENTRX能够有效自动诊断AI代理失败，降低人工成本，为代理失败分析提供了实用工具和基准数据集

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [116] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: OGD4All是一个基于LLM的透明、可审计、可复现框架，用于增强公民与地理空间开放政府数据的交互，通过语义检索、智能体推理和沙箱执行实现高准确性和低幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 增强公民与地理空间开放政府数据的交互，提供透明、可审计、可复现的访问方式，推进可信AI在开放治理中的应用。

Method: 结合语义数据检索、智能体推理进行迭代代码生成，以及安全的沙箱执行，产生可验证的多模态输出。

Result: 在199个问题的基准测试中，覆盖苏黎世市430个数据集和11个LLM，达到98%的分析正确率和94%的召回率，同时可靠地拒绝数据不支持的问题，最小化幻觉风险。

Conclusion: 该方法展示了LLM如何为公共数据提供可解释的多模态访问，推进了开放治理中的可信AI发展。

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [117] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: ELLMPEG是一个边缘智能的代理式LLM框架，用于自动生成视频处理命令，通过工具感知的RAG和迭代自反思在边缘本地执行FFmpeg和VVenC命令，避免云API依赖。


<details>
  <summary>Details</summary>
Motivation: 解决云基LLM部署的三个关键限制：高计算和能耗需求、远程处理的隐私和可靠性风险、以及持续的API成本。利用代理式AI的进展，在边缘本地利用开源工具和LLMs。

Method: 集成工具感知的检索增强生成(RAG)与迭代自反思机制，在边缘直接生成并本地验证可执行的FFmpeg和VVC编码器(VVenC)命令。收集了包含480个多样化查询的专用提示数据集进行评估。

Result: 实验显示，Qwen2.5结合ELLMPEG框架在FFmpeg和VVenC数据集上平均命令生成准确率达到78%，零持续API成本，优于其他开源模型。评估了命令有效性、每秒生成token数、推理时间和能效。

Conclusion: ELLMPEG框架成功实现了在边缘设备上高效、隐私保护且成本效益高的视频处理命令自动生成，为多媒体处理提供了可行的本地化解决方案。

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [118] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: 该研究将英语中的"奉承"诊断扩展到印地语，发现文化适应后的印地语提示比英语提示产生更高的奉承率，表明对齐行为在不同语言和文化间并不一致。


<details>
  <summary>Details</summary>
Motivation: 语言模型倾向于优先迎合用户偏好而非原则性推理的"奉承"现象在英语评估中被识别为持续的对齐失败，但尚不清楚这种诊断是否适用于不同语言和文化背景。

Method: 将Beacon单轮强制选择奉承诊断扩展到印地语，采用三条件设计：英语原文、印地语直译、印地语文化适应提示。在4个开源指令调优模型上评估每个条件50个提示，分离语言编码效应与文化适应效应。

Result: 所有模型中，文化适应印地语提示的奉承率均高于英语，绝对差异12.0-16.0个百分点。Qwen 2.5-Coder-7B分解显示文化适应贡献主要差距(delta=14.0%)，语言编码贡献极小(delta=2.0%)。建议类提示跨语言差异最大(20-25个百分点)。

Conclusion: 英语中测量的对齐行为在不同语言间并不一致转移，文化基础的提示框架起重要作用。研究发布所有数据集和评估代码支持复现和扩展。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [119] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: TextBFGS：用于离散文本优化的二阶框架，通过检索梯度算子实现拟牛顿优化，在代码优化任务上显著优于一阶方法


<details>
  <summary>Details</summary>
Motivation: 现有离散文本优化方法主要是一阶优化器（类似SGD），存在收敛慢和不稳定的问题，因为它们忽略了优化景观的语义曲率

Method: TextBFGS是二阶框架，通过从预学习成功轨迹中检索梯度算子来近似逆Hessian矩阵，实现单次更新（One-Pass Update），将反馈生成和二阶校正结合到单个推理步骤中

Result: 在代码优化任务（HumanEval、MBPP等）上，TextBFGS显著优于一阶基线方法，以更少的模型调用实现更高的通过率，并表现出强大的跨任务可迁移性

Conclusion: TextBFGS为高效、内存感知的文本优化建立了数学基础范式，通过二阶优化方法解决了离散文本优化的收敛和稳定性问题

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [120] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: ALIGN提出一种多智能体LLM推理框架，将推理任务建模为对齐委托游戏，通过设计激励机制让多个智能体生成候选方案，然后选择最佳答案，理论上保证比单智能体方法有更好的期望性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在复杂推理任务上表现不佳，现有的推理时集成方法通常将候选答案视为独立处理，缺乏形式化保证集成能提高推理质量，需要一种能提供理论保证的改进方法。

Method: 将LLM推理建模为对齐委托游戏：委托人将任务委托给多个智能体，这些智能体在设计好的激励机制下生成候选解决方案，然后委托人从中选择最终答案，保持智能体与委托人目标的对齐。

Result: 理论分析表明，在公平比较条件下，ALIGN能证明比单智能体生成有更好的期望性能，且能处理相关候选答案，放宽了先前工作中的独立性假设。在广泛的LLM推理基准测试中，ALIGN始终优于强单智能体和集成基线。

Conclusion: ALIGN通过将LLM推理形式化为对齐委托游戏，提供了一种理论上可保证性能提升的多智能体推理方法，在保持目标对齐的同时改进了复杂推理任务的表现。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [121] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: RAPTOR是一种基于L2正则化逻辑回归的探针方法，用于从冻结LLM的层表示中提取概念向量，在准确性、方向稳定性和训练成本方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 探针方法用于分析冻结LLM中编码的信息，并在"探针-引导"流程中用于激活引导。现有方法需要准确、方向稳定且成本低的概念向量估计。

Method: 提出RAPTOR（Ridge-Adaptive Logistic Probe），一种简单的L2正则化逻辑探针，通过验证调优的岭强度从归一化权重中提取概念向量。

Result: 在指令调优LLM和人工编写概念数据集上的广泛实验中，RAPTOR在准确性上匹配或超过强基线，同时实现竞争性的方向稳定性和显著更低的训练成本。

Conclusion: RAPTOR提供了一种有效且高效的概念向量提取方法，并通过理论分析解释了岭惩罚强度如何调节探针准确性和概念向量稳定性。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [122] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: DA-GRPO是一种双优势组相对策略优化方法，用于在持续学习场景下智能调节本地小语言模型对云端大语言模型的调用，避免灾难性遗忘并保持稳定的云使用预算。


<details>
  <summary>Details</summary>
Motivation: 本地部署的小语言模型在内存和计算资源有限的情况下需要持续支持多样化任务，必须选择性地依赖云端大语言模型。然而，在持续学习中调节云端协助具有挑战性，因为基于奖励的强化学习通常会产生不稳定的卸载行为，并在任务分布变化时加剧灾难性遗忘。

Method: 提出DA-GRPO（双优势组相对策略优化），将云端使用约束直接纳入优势计算中，避免固定的奖励塑造和外部路由模型。该设计使本地模型能够联合学习任务能力和协作行为，允许在训练后自然产生云端请求，同时遵守预设的协助预算。

Result: 在数学推理和代码生成基准测试中，DA-GRPO相比先前的协作和基于路由的方法，提高了切换后的准确性，显著减少了遗忘，并保持了稳定的云端使用。

Conclusion: DA-GRPO通过将云端使用约束直接整合到优势计算中，有效解决了本地小语言模型在持续学习中智能调节云端协助的问题，实现了更好的任务性能和更稳定的协作行为。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [123] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: GASP方法通过对抗性自博弈训练，使强化学习模型能够在错误上下文条件下检测并修复推理错误，提高鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）模型在干净上下文条件下表现良好，但当上下文存在错误（如推理链损坏、误导性部分解或轻微输入扰动）时会灾难性失败，因为标准RLVR只优化干净条件下的最终答案正确性

Method: 提出GASP（引导对抗性自博弈）方法：在单个模型内形成对抗性自博弈游戏，污染者学习通过局部连贯的损坏诱导失败，而智能体学习在相同损坏条件下诊断和恢复。为解决训练早期成功恢复稀缺的问题，提出分布内修复引导，对自生成的修复添加模仿项，提高恢复概率同时保留已有能力

Result: 在四个开源模型（1.5B-8B）上，GASP将强但脆弱的推理器转变为鲁棒的推理器，能够抵御误导和扰动上下文，同时通常提高干净准确性。分析显示对抗性损坏诱导了有效的课程学习，分布内引导实现了快速恢复学习且表示漂移最小

Conclusion: GASP方法仅使用结果验证，无需人工标签或外部教师，就能有效训练检测和修复能力，显著提高推理模型在错误上下文条件下的鲁棒性

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [124] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 该论文提出了一种约束双层强化学习算法CBSO，使用惩罚函数处理约束，通过Moreau包络分析非光滑优化，获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 元学习、分层学习和人类反馈强化学习等许多重要RL问题都可以建模为双层RL问题。虽然这些领域在实证上取得了很大进展，但双层RL算法的理论分析尚未得到足够关注。本文旨在分析约束双层RL算法的样本复杂度。

Method: 提出约束双层次梯度优化算法(CBSO)，使用惩罚函数目标函数避免约束双层问题中的原始-对偶间隙和超梯度问题。采用Moreau包络分析一般参数化策略梯度RL算法中的非光滑目标函数。

Result: 获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度，这是首次使用Moreau包络分析具有非光滑目标函数的一般参数化策略梯度RL算法。

Conclusion: 该工作填补了约束双层RL算法理论分析的空白，为元学习、分层学习和RL-HF等双层RL问题提供了理论保证，证明了惩罚函数方法和Moreau包络在分析非光滑约束优化问题中的有效性。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [125] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: 提出注意力引导的引导框架，解决LLM内部激活操纵的三个核心挑战，在512个语义概念基准上性能显著提升


<details>
  <summary>Details</summary>
Motivation: 现有引导方法非常脆弱，概念的可引导性取决于细微的算法选择，需要更鲁棒的引导框架来理解LLM中语义概念的存储方式并提升LLM能力

Method: 引入注意力引导的引导框架，自动选择相关token嵌入提取概念特征，考虑概念特征在LLM激活中的异质性，识别最相关的引导层

Result: 在512个语义概念基准上，框架显著优于先前SOTA方法（成功引导概念数量几乎翻倍），适用于不同架构和大小的模型（最高700亿参数）

Conclusion: 该框架为开发高效、高度可扩展的行业级LLM微调算法开辟了新途径，并揭示了概念特定特征在LLM层间的分布规律

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [126] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: VG2S框架首次将变分推理引入JSSP领域，通过变分图编码器解耦表示学习和策略优化，显著提升训练稳定性和零样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在JSSP中面临训练非平稳性和对未见问题实例泛化能力有限的问题，主要原因是同时优化表示学习和策略执行

Method: 提出变分图到调度器框架，基于ELBO和最大熵强化学习推导概率目标，通过变分图编码器数学解耦表示学习和策略优化

Result: 在DMU和SWV等大规模挑战性基准实例上，VG2S表现出优于最先进DRL基线和传统调度规则的零样本泛化能力

Conclusion: VG2S框架通过变分推理有效解决了JSSP中的训练稳定性和泛化问题，为制造调度提供了更稳健的解决方案

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [127] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: FedMOA是一个联邦GRPO框架，用于异构奖励下的多目标对齐，通过自适应权重调整和任务感知聚合，在数学推理和代码生成任务上优于联邦平均方法。


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐在联邦学习中需要维护单独的critic网络，内存消耗大，不适合设备端训练。GRPO的无critic架构适合设备端训练，但在联邦设置下面临异构奖励定义、不平衡多目标优化和高训练成本等挑战。

Method: 提出FedMOA框架：1) 本地训练使用基于超梯度下降的在线自适应权重机制，优先考虑主要推理目标；2) 服务器端使用任务和准确率感知的聚合策略，优先考虑高质量更新。

Result: 在数学推理和代码生成基准测试中，FedMOA始终优于联邦平均方法，准确率提升高达2.2%，同时改善了全局性能、个性化和多目标平衡。

Conclusion: FedMOA成功解决了联邦GRPO中的异构奖励和多目标优化挑战，为设备端联邦强化学习对齐提供了有效的解决方案。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [128] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: SIERL是一种受搜索启发的强化学习探索方法，通过基于学习进度设置子目标来主动引导探索，在稀疏奖励环境中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的探索是强化学习的核心挑战。现有方法如课程学习和Go-Explore依赖手工启发式，而好奇心驱动方法可能收敛到次优策略。需要一种能主动引导探索、系统扩展状态空间边界的方法。

Method: SIERL在每轮开始时从边界（已知状态空间边界）选择子目标，然后代理继续向主要任务目标探索。子目标选择机制提供既不过于熟悉也不完全新颖的状态-动作对，确保边界系统扩展。受搜索启发，基于成本估计（到达成本和前往成本）对边界子目标进行优先级排序，引导探索到信息最丰富的区域。

Result: 在具有挑战性的稀疏奖励环境中，SIERL在实现主要任务目标和泛化到环境中任意状态方面都优于主流基线方法。

Conclusion: SIERL通过受搜索启发的子目标选择机制，有效解决了稀疏奖励环境中的探索问题，能够系统扩展状态空间边界并实现更好的任务完成和泛化能力。

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [129] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: CAL方法通过利用扩散语言模型在去噪过程中的统计信号（Oracle Peak和Length Bias），实现了无需训练的自适应长度代码和文本填充，显著提升了填充性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然天生适合填充任务，但其性能受到预设填充长度的限制。现有方法需要指定填充长度，这在实际应用中不现实，因为正确的填充长度通常是未知的。

Method: CAL方法通过分析DLMs在第一步去噪过程中的置信度统计特性，发现了两个关键现象：1）在真实长度附近出现的局部Oracle Peak信号；2）常常掩盖该信号的系统性Length Bias。通过利用这一信号并校准偏差，CAL能够在正式解码前通过高效搜索近似最优填充长度。

Result: 在代码填充任务中，CAL比固定长度基线提升了47.7%的Pass@1，比基于聊天的自适应方法提升了40.5%。在文本填充任务中，BLEU-2和ROUGE-L分别提升了8.5%和9.9%。

Conclusion: CAL证明了DLMs具有发现正确填充长度的内在能力，通过利用去噪过程中的统计信号和校准偏差，实现了无需专门训练的鲁棒填充方法。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [130] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: AREAL-DTA：一种基于深度优先搜索的注意力机制，通过动态遍历rollout前缀树来高效利用RL训练中的前缀共享，减少计算和内存开销


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的LLM后训练计算成本高，因为生成的rollout序列经常共享长token前缀。现有RL框架独立处理这些序列，在策略模型训练中重复计算相同的前缀，导致计算和内存使用效率低下。

Method: AREAL-DTA采用深度优先搜索执行策略，在前后向计算中动态遍历rollout前缀树，每次只物化单个根到叶路径。还包含负载均衡的分布式批处理机制，动态构建和处理跨多GPU的前缀树。

Result: 在流行的RL后训练工作负载中，AREAL-DTA在τ²基准测试中实现了高达8.31倍的训练吞吐量提升。

Conclusion: AREAL-DTA通过高效利用RL训练中的前缀共享，显著提高了训练效率，解决了现有树注意力方法在RL设置中扩展性差的问题。

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [131] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

TL;DR: 本文提出Minerva框架，利用可验证奖励的强化学习（RLVR）来改进网络安全威胁情报（CTI）的结构化输出生成，通过任务特定验证器和自训练机制提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在处理网络安全威胁情报（CTI）结构化输出任务时存在脆弱性，主要依赖监督微调（SFT），而CTI标准和社区资源定义了可确定性验证的标识符和模式，这为使用可验证奖励的强化学习提供了机会。

Method: 提出Minerva框架：1）统一数据集和训练管道，涵盖多个CTI子任务；2）每个任务配备特定验证器，对结构化输出和标识符预测进行评分；3）针对奖励稀疏性问题，提出轻量级自训练机制，生成额外已验证轨迹并蒸馏回模型。

Result: 实验表明，在不同LLM骨干网络上，该方法在多个基准测试中相比监督微调（SFT）在准确性和鲁棒性方面均取得一致改进。

Conclusion: 利用CTI领域可验证奖励的强化学习（RLVR）能够有效提升模型在结构化输出任务上的性能，自训练机制有助于缓解奖励稀疏性问题，为CTI分析自动化提供了更可靠的解决方案。

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [132] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

TL;DR: Clade-AHD是一个用于大型语言模型自动启发式设计的高效框架，通过将节点级点估计替换为分支级贝叶斯信念来解决MCTS在有限计算预算下的过度开发问题。


<details>
  <summary>Details</summary>
Motivation: MCTS在LLM自动启发式设计中表现出潜力，但在有限的启发式评估计算预算下存在严重的过度开发倾向，这限制了其在实际应用中的效果。

Method: 提出Clade-AHD框架，将节点级点估计替换为分支级贝叶斯信念，通过将后代评估聚合成Beta分布并在这些信念上执行Thompson采样，显式建模不确定性以指导探索。

Result: 在复杂的组合优化问题上的大量实验表明，Clade-AHD始终优于最先进的方法，同时显著降低了计算成本。

Conclusion: Clade-AHD通过显式建模不确定性来指导探索，能够在稀疏和噪声评估下实现更可靠的决策，是自动启发式设计领域的重要进展。

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [133] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: 通过心理语言学实验研究LLM隐藏状态几何结构能否从其行为中恢复，发现强制选择任务比自由联想任务更能反映隐藏状态几何，且行为相似性可预测未见词的隐藏状态相似性


<details>
  <summary>Details</summary>
Motivation: 探究是否可以通过LLM在心理语言学实验中的行为表现来恢复其隐藏状态的几何结构，理解行为测量能否揭示内部语义表示

Method: 在8个指令调优的Transformer模型上运行两种实验范式（基于相似性的强制选择和自由联想），收集1750万+试验构建行为相似性矩阵，使用表征相似性分析比较行为几何与层间隐藏状态相似性，并与FastText、BERT和跨模型共识进行基准比较

Result: 强制选择行为比自由联想更显著地与隐藏状态几何对齐；在未见词回归中，行为相似性（特别是强制选择）能够超越词汇基线和跨模型共识预测未见隐藏状态相似性，表明仅行为测量保留了关于内部语义几何的可恢复信息

Conclusion: 行为任务能够揭示LLM的隐藏认知状态，强制选择范式比自由联想更能有效反映内部语义几何结构，为通过行为实验理解模型内部表示提供了实证支持

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [134] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 论文提出了首个面向均衡的安全探索框架SEE，通过交替寻找最大可行区域和最小不确定性模型，实现零约束违反的安全探索并收敛到均衡状态。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的安全探索通常将探索限制在可行区域内，但两个关键问题未解决：通过探索能达到的最大可行区域是什么？如何识别这个区域？论文首次揭示了安全探索的目标是在可行区域和环境模型之间找到均衡。

Method: 提出安全均衡探索（SEE）框架，使用不确定模型的图表示，交替进行两个步骤：1）在给定模型下寻找最大可行区域；2）在给定可行区域内寻找最小不确定性模型。证明该方法能单调优化模型和扩展可行区域。

Result: 在经典控制任务上的实验表明，SEE算法能够成功扩展可行区域且零约束违反，并在几次迭代内达到安全探索的均衡状态。

Conclusion: 安全探索的目标是找到可行区域和环境模型之间的均衡，两者相互依赖：更大的可行区域带来更准确的环境模型，更准确的模型又能探索更大的区域。SEE框架首次实现了这种均衡导向的安全探索。

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [135] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: LocalV是一个多智能体框架，通过利用模块化硬件设计中的信息局部性，将长文档到长代码生成问题分解为短文档、短代码任务，显著提升了RTL代码生成的质量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: RTL代码生成是数字硬件设计中的关键但劳动密集型步骤，传统方法需要工程师手动将复杂规范转换为数千行可综合的HDL代码。现有LLM方法在处理工业级IP设计任务时面临三大挑战：处理长而详细的文档、生成长RTL代码时正确性下降、以及复杂的调试周期。

Method: LocalV采用多智能体框架，利用模块化硬件设计中的信息局部性。具体包括：分层文档划分、任务规划、局部化代码生成、接口一致性合并、以及AST引导的局部感知调试。

Result: 在IP级Verilog生成基准测试RealBench上，LocalV显著优于最先进的LLM和智能体方法，达到了45.0%的通过率，而现有最佳方法仅为21.6%。

Conclusion: LocalV通过分解长文档到长代码生成问题，有效解决了工业级硬件设计中的可扩展性挑战，为自动化RTL代码生成提供了有前景的解决方案。

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [136] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

TL;DR: 本文提出了一种参数高效的持续学习方法EBLoRA，通过将任务更新分解为幅度和方向结构，在受限Stiefel流形上进行约束优化，以平衡奇异值谱，减少前后向遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要关注避免与过去更新的干扰，而忽略了什么样的任务特定更新能自然保留先前获得的知识。从知识分解的角度看，低秩适应表现出高度不平衡的奇异值谱：少数主导成分吸收了大部分适应能量，这既容易破坏先前知识，又使更新更容易受到后续任务的干扰。

Method: 将任务更新的幅度与方向结构解耦，将其表述为受限Stiefel流形上的约束优化问题。使用与标准深度学习优化器兼容的投影一阶方法来解决这个问题，从而平衡组件间的适应能量分布。

Result: 该方法能同时缓解后向遗忘和前向遗忘，在持续学习基准测试中持续优于现有基线方法。

Conclusion: 通过显式平衡任务更新中的组件，EBLoRA方法能更有效地保留先前获得的知识，同时减少对后续任务的干扰，在参数高效的持续学习中表现出优越性能。

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [137] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 论文提出了"提示多样性"框架来量化LLM评估中的一致性，发现现有幻觉评估过度关注正确性而忽视一致性，导致对幻觉危害的严重误解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM幻觉评估主要关注正确性，但忽视了输出一致性，而一致性对于区分和解决幻觉造成的危害（如信任侵蚀和错误信息传播）至关重要。

Method: 提出"提示多样性"框架，通过量化LLM在不同提示下的输出一致性来评估幻觉。分析多个基准测试（如Med-HALT），研究一致性在幻觉检测和缓解中的作用。

Result: 发现显著的多重性（超过50%的不一致性），表明现有幻觉危害评估存在严重误解。检测技术主要检测一致性而非正确性，RAG等缓解技术虽然有益但可能引入额外不一致性。

Conclusion: 通过将提示多样性整合到幻觉评估中，提供了改进的危害评估框架，揭示了当前检测和缓解策略的关键局限性，强调了一致性评估的重要性。

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [138] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

TL;DR: 提出一种有限时域非片段式在线强化学习新方法，使用K步前瞻Q函数和阈值机制，在表格环境中实现高效学习并获得理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有无限时域方法依赖折扣收缩，不适用于固定时域的非片段式MDP。需要一种能自然处理固定时域结构且样本高效的在线强化学习方法。

Method: 引入K步前瞻Q函数，将规划截断到未来K步；采用阈值机制，仅当估计的K步前瞻值超过时变阈值时才选择动作；提供高效的表格学习算法。

Result: 理论证明：K=1时达到极小极大最优常数遗憾，K≥2时遗憾为O(max((K-1),C_{K-1})√(SATlogT))；实验在JumpRiverswim、FrozenLake和AnyTrading等环境中优于现有表格RL方法。

Conclusion: K步前瞻Q函数和阈值机制能有效处理固定时域非片段式MDP，在理论和实验上均表现出色，为有限时域在线强化学习提供了新思路。

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [139] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: 提出一种保守智能体方法，将随机延迟环境转化为恒定延迟环境，使现有恒定延迟方法可直接应用于随机延迟场景，在连续控制任务中显著优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习应用常受环境延迟反馈困扰，延迟违反马尔可夫假设并带来挑战。现有方法主要针对恒定延迟环境，而随机延迟环境因其可变性和不可预测性研究较少。

Method: 提出保守智能体方法，将随机延迟环境重新表述为其恒定延迟等价形式。这种转换使任何最先进的恒定延迟方法都能直接扩展到随机延迟环境，无需修改算法结构或牺牲性能。

Result: 在连续控制任务上评估保守智能体算法，实证结果表明其在渐近性能和样本效率方面显著优于现有基线算法。

Conclusion: 保守智能体提供了一种简单而鲁棒的方法来处理随机延迟环境，通过将随机延迟转化为恒定延迟，使现有恒定延迟方法能够有效应用于更复杂的随机延迟场景。

Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.

</details>


### [140] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 研究发现：在仅基于结果的监督下，增加训练时推理长度（如RL中的token预算或循环Transformer的循环次数）可以持续提升分布外性能，即使分布内性能已饱和，这表明鲁棒性需要比ID验证更大的预算。


<details>
  <summary>Details</summary>
Motivation: 训练LLM进行更长推理已成为构建能解决复杂问题的最先进模型的关键要素。当前研究通过不同方式追求这一目标，如RL微调以引出长链思维或通过架构循环扩展潜在推理。这使得推理长度成为一个重要的扩展参数。

Method: 通过理论分析和实验验证：1）理论解释两种机制：自迭代可在假设类中引入更强的归纳偏置，重塑ID最优解以改善OOD泛化；2）当仅适用于ID样本的捷径解在假设类中持续存在时，正则化可随着自迭代次数增加减少对捷径的依赖。实验包括：在合成任务上增加循环Transformer的循环次数，以及在数学推理任务上增加RL微调时的token预算。

Result: 发现新颖现象：在仅基于结果的监督下，OOD性能可随训练时推理长度增加而持续改善，即使ID性能已饱和。这表明鲁棒性可能需要比ID验证更大的预算。

Conclusion: 增加训练时推理长度可以改善OOD泛化，即使ID性能已饱和。这为构建更鲁棒的模型提供了新视角，表明需要超越ID验证来评估模型性能。

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [141] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

TL;DR: 该论文提出了一种预算感知的监督微调框架，将LLM适应建模为上下文Stackelberg博弈，通过有限监督预算实现标签高效学习。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型微调中标注数据可用性与下游准确性之间的权衡问题，特别是在有限标注预算下的高效学习挑战。

Method: 将LLM适应建模为上下文Stackelberg博弈，学习者（领导者）承诺评分策略和标签查询策略，自适应环境（跟随者）选择具有挑战性的监督替代方案。引入有限监督预算到学习目标，并提出带有Largest-Latency-First置信门的选择性标签查询算法。

Result: 在全反馈机制下实现$\tilde{O}(d\sqrt{T})$遗憾，在标准线性上下文假设下。扩展框架通过LLF置信门实现预算感知遗憾界$\tilde{O}(\sqrt{dB} + c\sqrt{B})$，其中$B=βT$。

Conclusion: 该框架为预算受限的LLM监督微调提供了理论基础和算法解决方案，平衡了标注成本与模型性能。

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [142] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出交互式智能体框架，通过自适应探索策略系统提取和量化LLMs的知识边界，发现递归分类法最有效，观察到知识缩放定律，并识别Pass@1与Pass@k的权衡


<details>
  <summary>Details</summary>
Motivation: LLMs可视为压缩知识库，但其实际包含的知识内容和边界尚不明确。现有基准大多是静态的，对系统知识探测支持有限，需要更动态、系统的知识提取方法

Method: 提出交互式智能体框架，包含四种自适应探索策略在不同粒度上探测知识；采用三阶段知识处理流程：向量过滤去除重复、LLM裁决解决语义重叠、领域相关性审计保留有效知识单元

Result: 递归分类法是最有效的探索策略；观察到清晰的知识缩放定律（更大模型提取更多知识）；发现Pass@1与Pass@k的权衡（专业模型初始准确率高但快速下降，通用模型性能稳定）；训练数据组成差异导致不同模型家族具有可测量的知识特征

Conclusion: 提出的交互式框架能系统提取和量化LLMs知识，揭示了有效的探索策略、知识缩放规律、性能权衡模式以及训练数据对知识特征的影响

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [143] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: ESSAM结合进化策略与锐度感知最大化，在数学推理任务上实现与强化学习方法相当的性能，同时大幅降低GPU内存使用


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型数学推理能力时GPU内存消耗过高，限制了其在资源受限环境中的应用

Method: 提出ESSAM框架，将进化策略的参数空间零阶搜索与锐度感知最大化紧密结合，进行全参数微调

Result: 在GSM8K数学推理任务上平均准确率达78.27%，性能与RL方法相当，GPU内存使用比PPO降低18倍，比GRPO降低10倍

Conclusion: ESSAM在保持与强化学习相当性能的同时，显著降低了GPU内存需求，为资源受限环境下的模型微调提供了有效方案

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [144] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: LRAgent是一个用于多LoRA代理系统的KV缓存共享框架，通过将缓存分解为共享基础组件和适配器依赖组件，显著减少内存和计算开销，同时保持接近非共享缓存的准确性。


<details>
  <summary>Details</summary>
Motivation: 在多LoRA代理系统中，虽然代理共享预训练主干网络，但每个代理独立构建和存储自己的KV缓存，导致相同工具增强轨迹的重复存储和计算开销。现有KV缓存共享方法大多忽略了这种多LoRA设置。

Method: 提出LRAgent框架：1) 将KV缓存分解为共享预训练权重的基础组件和LoRA权重的适配器依赖组件；2) 共享基础组件并以低秩形式存储适配器组件；3) 引入Flash-LoRA-Attention内核，重新排序注意力计算以避免将低秩缓存物化为完整维度。

Result: LRAgent实现了接近完全共享缓存的吞吐量和首词延迟，同时在代理问答基准测试中保持了接近非共享缓存基线的准确性。

Conclusion: LRAgent通过有效共享KV缓存，显著减少了多LoRA代理系统的内存和计算开销，为角色专业化代理系统提供了高效的缓存共享解决方案。

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [145] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: PEAR是一种SFT阶段方法，通过重要性采样重新加权SFT损失，解决SFT与RL阶段分布不匹配问题，提升后续RL训练效果


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练中，SFT阶段通常孤立优化以最大化SFT性能，但更强的SFT检查点在相同RL训练后可能显著弱于较弱的检查点，这源于离线SFT数据分布与在线RL策略分布不匹配

Method: 提出PEAR方法，使用重要性采样重新加权SFT损失，包含token级、block级和序列级三种变体，可增强标准SFT目标，在收集离线数据概率后增加很少的训练开销

Result: 在可验证推理游戏和数学推理任务上对Qwen 2.5/3和DeepSeek-distilled模型进行实验，PEAR一致提升后RL性能，在AIME2025上pass@8提升达14.6%

Conclusion: PEAR是朝着更整体化LLM后训练的有效步骤，设计SFT时考虑下游RL而非孤立优化，能更好地为RL阶段准备模型

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [146] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 本文提出了一个基于马尔可夫过程的序列缩放理论框架，揭示了序列缩放的内在性质，并开发了MarkovScale系统，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前序列缩放方法多为启发式、非原则性的，导致性能改进有限且缺乏理论理解，需要建立理论框架来揭示序列缩放的内在性质并提供最优性边界。

Method: 将序列缩放建模为两状态马尔可夫过程，推导出闭式解来确定准确率提升的条件及理论上、中、下界性能边界，并基于此开发MarkovScale系统实现理论指导的准确率与效率平衡。

Result: 在3个骨干LLM、5个基准测试和20多个配置上的实验表明，MarkovScale在准确率和效率方面均优于最先进的并行和序列缩放方法。

Conclusion: 该工作为序列缩放提供了理论基础和最优性边界，MarkovScale系统代表了向LLM最优且资源高效推理迈出的重要一步。

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [147] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: SGALM提出了一种自生成对抗LLM框架，通过单模型内的生成对抗游戏实现对齐，无需外部奖励模型，在性能和合成数据生成方面达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法依赖昂贵的人工标注或强化学习人类反馈，而现有的自博弈和合成数据方法存在启发式假设或未经验证的自我评估问题，容易导致偏见累积和性能漂移。

Method: SGALM框架将对齐问题构建为单LLM内部的生成对抗游戏，联合进化生成和判别能力，无需外部奖励模型，通过理论分析和实证验证其有效性。

Result: SGALM实现了最先进的性能表现，既可作为有效的对齐算法，也能作为鲁棒的合成数据生成引擎。

Conclusion: SGALM提供了一种统一的对齐框架，通过内部生成对抗机制解决了传统方法对高质量标注的依赖问题，在理论和实证上都表现出优越性。

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [148] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: 该论文提出了一个相对预算理论来解释强化学习在不同任务和计算预算下的效果差异，通过相对预算ξ=H/E[T]这一单一量来预测RL样本效率，揭示了三个不同学习机制及其对推理性能的影响。


<details>
  <summary>Details</summary>
Motivation: 强化学习是提升大语言模型推理能力的主要范式，但其效果在不同任务和计算预算下差异很大。目前缺乏一个统一的理论框架来解释这种差异，并指导如何有效分配计算资源来最大化学习效率。

Method: 提出相对预算理论，定义相对预算ξ=H/E[T]，其中H是生成视野（token预算），T是基础策略下首次得到正确解所需的token数。通过理论分析揭示了三个学习机制：不足机制(ξ→0)、平衡机制(ξ=Θ(1))和充足机制(ξ→∞)。提供了在线RL的有限样本保证，并在理想分布假设下进行案例研究。

Result: 理论分析表明：在不足机制下，信息轨迹稀少，样本复杂度爆炸；在平衡机制下，信息轨迹以非可忽略概率出现，RL达到最大样本效率；在充足机制下，学习保持稳定但每次迭代的边际收益递减。实证结果显示ξ∈[1.5, 2.0]的预算范围能最大化学习效率，并与峰值推理性能一致。

Conclusion: 相对预算ξ是一个关键量，能够预测RL在不同任务和计算预算下的样本效率。该理论为RL在语言模型推理任务中的有效应用提供了指导，帮助确定最优的计算资源分配策略。

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [149] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: VIP是一种基于方差感知的预测性分配策略，通过高斯过程模型预测每个提示的成功概率，并优化分配计算预算以最小化策略更新的梯度方差，从而提高强化学习的采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的策略优化方法（如GRPO）对所有训练提示分配固定数量的rollout，这种均匀分配隐含地认为所有提示具有同等信息价值，可能导致计算预算使用效率低下并阻碍训练进展。

Method: VIP使用轻量级高斯过程模型基于最近的rollout预测每个提示的成功概率，将这些概率预测转化为方差估计，然后通过凸优化问题在硬计算预算约束下确定最优的rollout分配。

Result: 实验结果表明，VIP在多个基准测试中持续提高采样效率，并比均匀分配或启发式分配策略获得更高的性能。

Conclusion: VIP通过方差感知的预测性分配策略，有效地优化了计算预算的使用，提高了强化学习在可验证奖励场景下的采样效率和性能。

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [150] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 本文提出了一种主动强化学习方法，通过有限在线交互选择性优化价值函数的不确定区域，实现了比纯离线方法更优的样本复杂度


<details>
  <summary>Details</summary>
Motivation: 离线强化学习存在状态-动作空间覆盖不足和分布偏移问题，需要有限在线交互来选择性优化不确定区域，但缺乏理论分析

Method: 提出主动强化学习算法，使用高斯过程不确定性建模，结合GP集中不等式和信息增益边界进行理论分析

Result: 证明了ε最优策略可以通过O(1/ε²)主动转移学习，优于纯离线方法的Ω(1/ε²(1-γ)⁴)速率，实现近最优信息效率

Conclusion: 主动强化学习通过引导不确定性减少实现了加速价值函数收敛，在最小在线数据下达到近最优信息效率

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [151] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 本文通过Tele-Lens探测方法研究LLM的内部状态与显式推理轨迹的关系，发现LLM具有短视性，主要进行增量推理而非全局规划，并基于此提出增强CoT不确定性估计的方法。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现LLM在CoT出现前已有潜在规划，这削弱了显式CoT的重要性，但CoT在多步推理任务中仍然关键。为了深入理解LLM内部状态与其言语化推理轨迹之间的关系，需要研究LLM的潜在规划能力。

Method: 提出Tele-Lens探测方法，应用于不同任务领域的隐藏状态，研究LLM的潜在规划强度。基于发现的短视性特征，提出增强CoT不确定性估计的假设，并验证少量CoT位置能有效代表整个路径的不确定性。

Result: 实证结果表明LLM表现出短视性，主要进行增量转换而非精确的全局规划。验证了少量CoT位置能有效代表整个路径的不确定性，并展示了自动识别CoT绕过的可行性且不降低性能。

Conclusion: 利用CoT动态特性具有重要意义，LLM的短视性特征可用于增强不确定性估计，自动识别CoT绕过是可行的。代码、数据和模型已开源。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [152] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: MoW是一种用于多任务强化学习的混合世界模型架构，通过任务自适应视觉压缩、混合Transformer动态模型和梯度聚类策略，在Atari和Meta-World上实现了参数高效的高性能。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习在视觉领域中面临样本效率挑战，特别是当任务在观察和动态上存在显著异质性时。传统的单一世界模型架构难以捕捉多样化的任务动态，导致重建和预测准确性差。

Method: 提出了混合世界模型（MoW）架构：1）模块化变分自编码器用于任务自适应视觉压缩；2）混合Transformer动态模型，包含任务条件专家和共享骨干；3）基于梯度的任务聚类策略，用于高效参数分配。

Result: 在Atari 100k基准测试中，单个MoW代理在26个Atari游戏上训练一次，获得110.4%的平均人类标准化分数，与需要26个任务特定模型的STROM（114.2%）相当，但参数减少50%。在Meta-World上，MoW在30万环境步内达到74.5%的平均成功率，创下新纪录。

Conclusion: MoW为通用世界模型提供了一个可扩展且参数高效的基础架构，能够有效处理多任务强化学习中观察和动态的异质性挑战。

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [153] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: 提出基于三个专业智能体的AI系统，用于意图驱动的自治网络：监督解释器将高层意图解析为可执行优化模板；优化器将模板转换为优化问题并分析权衡；偏好驱动控制器基于多目标强化学习，利用偏好操作网络接近帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 电信网络需要自主运行并支持具有多样且经常冲突意图的异构服务，但将高层意图（如超低延迟、高吞吐量或能效）转化为具体控制动作超出了现有启发式方法的能力。

Method: 构建三个专业智能体系统：1) 基于语言模型的监督解释器，执行意图的词汇解析和认知细化；2) 优化器，将模板转换为可处理的优化问题并分析权衡；3) 基于多目标强化学习的偏好驱动控制器，利用偏好操作网络接近帕累托前沿。

Result: 该系统使网络能够以可扩展的方式自主解释、推理、适应和响应多样意图和网络条件，实现意图驱动的自治网络。

Conclusion: 提出的智能体AI系统通过三个专业智能体的协同工作，解决了意图驱动自治网络的关键挑战，实现了从高层意图到具体控制动作的自主转换。

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [154] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: 通过强化学习训练小型推理模型获得强大的生成选择能力，在数学和代码推理基准上超越提示和多数投票基线，接近或超过更大模型性能


<details>
  <summary>Details</summary>
Motivation: 通过并行采样扩展测试时计算可以显著提升LLM推理能力，但受限于Best-of-N选择质量。生成选择方法如GenSelect可以解决这一瓶颈，但强大的选择性能主要局限于大模型。研究如何让小型推理模型通过强化学习获得强大的生成选择能力。

Method: 从大规模数学和代码指令数据集中合成选择任务，筛选包含正确和错误候选解决方案的实例，使用DAPO强化学习训练1.7B参数模型，奖励正确的选择决策。

Result: 在数学推理基准（AIME24、AIME25、HMMT25）和代码推理基准（LiveCodeBench）上，模型一致优于提示和多数投票基线，经常接近或超过更大模型的性能。这些增益可以泛化到选择更强模型的输出，尽管训练时只使用了较弱模型的输出。

Conclusion: 强化学习是解锁小型模型中强大生成选择能力的可扩展方法，能够实现高效的测试时扩展。

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [155] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

TL;DR: 该论文提出通过分析LLM生成过程中的熵动态（而非静态聚合统计）来改进推理，引入熵动态不稳定性评分（EDIS）作为诊断信号，能显著提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将置信度视为静态量（通常在token上聚合），但生成过程中置信度的时序演化包含更丰富的信息。作者发现正确与错误推理在熵轨迹上表现出不同的特征模式，这些模式反映了推理失败的内在属性而非表面噪声。

Method: 分析token级熵轨迹，识别正确与错误推理的特征模式（如不稳定动态、突发尖峰、峰谷尖峰）。引入熵动态不稳定性评分（EDIS）作为轨迹级度量，量化熵演化中的不稳定性。EDIS可用于推理时选择和训练时样本筛选。

Result: 错误解决方案表现出不稳定动态，包括突发尖峰（持续不确定性增长）和峰谷尖峰（短暂置信后急剧反弹）。这些模式在不同模型和训练阶段持续存在。EDIS作为有效的诊断信号，显著提高了推理准确性。

Conclusion: 熵动态是理解和改进LLM推理的一个未被充分探索但信息丰富的视角。EDIS为推理时选择和训练时样本筛选提供了有前景的方向，揭示了推理失败的内在属性。

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [156] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

TL;DR: CurioSFT提出了一种保持熵的监督微调方法，通过内在好奇心增强探索能力，在数学推理任务中优于传统SFT，并为后续强化学习阶段带来显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统SFT-then-RL流程中，SFT阶段模仿专家演示会导致过度自信和生成多样性降低，限制了RL阶段的探索空间。现有的熵正则化方法虽然增加熵，但无法真正提升探索能力。

Method: CurioSFT包含两个核心组件：(1) 自探索蒸馏：将模型蒸馏到自生成的温度缩放教师模型，鼓励在能力范围内探索；(2) 熵引导温度选择：自适应调整蒸馏强度，在推理标记处增强探索，在事实标记处保持稳定，减轻知识遗忘。

Result: 在数学推理任务中，CurioSFT在SFT阶段比传统SFT在分布内任务提升2.5分，分布外任务提升2.9分。保留的探索能力在RL阶段转化为5.0分的平均提升。

Conclusion: CurioSFT通过保持熵和增强内在好奇心，有效解决了传统SFT的限制，为后续RL阶段提供了更好的探索基础，在推理任务中取得了显著改进。

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [157] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything是一个强化学习框架，通过闭环优化动态构建环境、策略和奖励模型，增强LLM和智能体场景的学习信号和系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习系统在LLM和智能体场景中面临学习信号弱、系统性能不足的问题，需要一种能够动态优化环境、策略和奖励模型的综合框架来提升整体学习效果。

Method: 提出RLAnything框架：1) 策略通过整合步进信号和结果信号的反馈进行训练；2) 奖励模型通过一致性反馈联合优化；3) 基于理论的自动环境适应利用批评反馈改进奖励和策略模型训练。

Result: 每个组件都持续提升系统性能：Qwen3-VL-8B-Thinking在OSWorld上提升9.1%；Qwen2.5-7B-Instruct在AlfWorld和LiveBench上分别提升18.7%和11.9%。优化的奖励模型信号优于依赖人工标签的结果。

Conclusion: RLAnything通过闭环优化环境、策略和奖励模型，显著增强了LLM和智能体任务的强化学习性能，证明了动态系统优化的有效性。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [158] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: 本文提出Transformer Q-Learning (TQL)方法，通过控制注意力分数熵来防止注意力崩溃，从而稳定训练并实现价值函数的大规模扩展，在强化学习中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习领域通过规模扩展取得了显著进展，但强化学习中的价值函数仍然主要使用小型模型。直接扩展价值函数（包括使用已知具有高度可扩展性的Transformer架构）通常会导致学习不稳定和性能下降。本文旨在探究是什么阻碍了Transformer在价值函数中的有效扩展。

Method: 通过实证分析识别出注意力分数崩溃是扩展失败的关键原因，提出Transformer Q-Learning (TQL)方法，通过控制注意力分数的熵来防止崩溃并稳定训练，从而解锁Transformer在强化学习价值函数中的扩展潜力。

Result: TQL方法在从最小到最大网络规模的扩展中实现了高达43%的性能提升，而先前方法在扩展时会出现性能下降。

Conclusion: 通过控制注意力分数熵可以有效防止Transformer在价值函数扩展中的注意力崩溃问题，从而稳定训练并实现性能提升，为强化学习中的大规模价值函数扩展提供了有效解决方案。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [159] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文研究了多智能体强化学习中的无奖励探索问题，分析了学习阶段数与智能体数量之间的权衡关系，发现当学习阶段数等于环境时域H时，仅需多项式数量的智能体即可获得ε近似的动态模型。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在无奖励探索设置下的合作问题，探索智能体如何在不观察奖励的情况下联合探索未知MDP以学习其动态特性，特别关注学习阶段数与所需智能体数量之间的权衡关系。

Method: 采用分阶段学习框架，每个学习阶段中多个智能体独立与环境交互，每个智能体被分配一个策略并执行该策略观察轨迹。研究重点是分析学习阶段数（ρ）与智能体数量之间的权衡，特别关注当学习阶段数较少时的情况。

Result: 当学习阶段数等于时域H时，提出了一个计算高效的算法，仅需Õ(S⁶H⁶A/ε²)个智能体即可获得动态特性的ε近似。同时给出了下界：任何限制在ρ<H阶段的算法需要至少A^{H/ρ}个智能体才能达到常数精度。

Conclusion: 研究揭示了学习阶段数与智能体数量之间的关键权衡，表明如果要限制智能体数量为多项式级别，则需要大约H个学习阶段。这一结果对多智能体无奖励探索的理论理解有重要意义。

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [160] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: 提出基于尾部分布预测的缩放定律引导搜索方法，动态分配计算资源以提升LLM推理能力，相比传统Best-of-N策略获得更高回报。


<details>
  <summary>Details</summary>
Motivation: 现有Best-of-N策略缺乏对N值选择、预算分配和多阶段决策的原则性指导，存在优化空间，且相关优化工作缺乏严格理论保证。

Method: 通过估计奖励的尾部分布来预测LLM缩放定律，无需穷举评估；提出缩放定律引导搜索算法，动态分配计算资源识别和利用具有最高预测潜力的中间状态。

Result: 理论证明SLG相比完美信息预言机实现可忽略的遗憾，达到相同预期奖励所需计算预算比Best-of-N多项式级减少；实证验证在不同LLM和奖励模型上均优于Best-of-N。

Conclusion: 尾部引导的分配策略在相同计算预算下比Best-of-N获得更高奖励产出，为测试时缩放提供了理论保证的优化方法。

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [161] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: 论文研究了具有长期记忆的LLM中隐性偏见的积累与传播问题，提出了DIB基准测试和动态记忆标记(DMT)干预方法。


<details>
  <summary>Details</summary>
Motivation: LLM的长期记忆机制虽然能保持交互连续性，但也带来了新的公平性风险，特别是隐性偏见如何随时间积累和跨领域传播的问题尚未充分研究。

Method: 1) 引入决策性隐性偏见(DIB)基准测试，包含3,776个决策场景；2) 使用长期模拟框架评估6个SOTA LLM和3种记忆架构；3) 提出动态记忆标记(DMT)干预方法，在记忆写入时强制执行公平约束。

Result: 研究发现：1) LLM的隐性偏见不会保持静态，而是随时间加剧；2) 偏见会跨不相关领域传播；3) 静态系统级提示的缓解效果有限且短暂；4) DMT方法显著减少了偏见积累并有效遏制跨领域偏见传播。

Conclusion: 长期记忆会加剧LLM的隐性偏见问题，需要动态干预机制。DMT方法通过在记忆写入时施加公平约束，能有效缓解偏见积累和传播。

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [162] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: 提出gWorld视觉移动GUI世界模型，通过生成可执行的网页代码而非直接生成像素，结合了文本和视觉方法的优势，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI世界模型存在关键权衡：基于文本的方法牺牲视觉保真度，而视觉方法无法精确渲染文本，需要依赖缓慢复杂的外部模型管道。需要一种新范式来结合两者的优势。

Method: 提出视觉世界建模通过可渲染代码生成的新范式：使用单个视觉语言模型预测下一个GUI状态为可执行的网页代码，而不是直接生成像素。开发了gWorld数据生成框架自动合成基于代码的训练数据。

Result: gWorld在4个分布内和2个分布外基准测试中，在准确性与模型大小方面建立了新的帕累托前沿，性能优于8个前沿开源模型（模型大小超过50.25倍）。分析显示：1）通过gWorld扩展训练数据带来显著提升；2）管道每个组件都提高数据质量；3）更强的世界建模改善下游移动GUI策略性能。

Conclusion: 通过可渲染代码生成的视觉世界建模新范式成功结合了文本和视觉方法的优势，gWorld模型在移动GUI世界建模方面取得了突破性进展，为移动GUI代理性能提升提供了有效路径。

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [163] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: FLAME提出了一种基于流匹配的最大熵强化学习框架，通过Q重加权目标绕过配分函数估计，使用解耦熵估计器纠正偏差，实现高效探索和一步生成控制，在MuJoCo上超越高斯基线并匹配多步扩散策略性能，同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 扩散策略表达能力强但推理延迟高，流匹配可实现一步生成但难以集成到最大熵强化学习中，因为最优策略是难以处理的基于能量的分布，且高效对数似然估计存在严重离散化偏差。

Method: 1) 推导Q重加权流匹配目标，通过重要性重加权绕过配分函数估计；2) 设计解耦熵估计器，严格纠正偏差以实现高效探索；3) 集成MeanFlow公式实现表达性强且高效的一步控制。

Result: 在MuJoCo基准测试中，FLAME超越高斯基线，性能与多步扩散策略相当，同时显著降低推理成本。

Conclusion: FLAME提供了一个原则性框架，成功解决了流匹配集成到最大熵强化学习中的挑战，实现了表达性强、探索高效且推理成本低的策略学习。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [164] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 论文提出PIPE评估协议，用于诊断AI代理对特定界面格式的依赖，发现轨迹监督微调会增强代理对训练界面的捷径学习，而非真正的语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理评估存在混淆：无法区分代理的成功是源于真正的语义工具使用能力，还是仅仅记住了特定界面的交互模式。标准基准测试无法识别代理是否真正具备环境不变的能力。

Method: 提出PIPE协议，通过最小化重写环境界面（保持任务语义和执行行为不变）来诊断界面依赖。引入界面依赖度（IR）指标，量化代理对训练时界面的偏好。

Result: 在16个环境和多种代理上的实验显示：轨迹监督微调显著增强了界面捷径学习，训练代理在界面微小变化下性能急剧下降，而非轨迹训练的模型保持稳定。界面捷径学习表现出环境依赖、非单调的训练动态。

Conclusion: 标准评估掩盖了代理对特定界面的依赖问题。轨迹监督微调可能使代理学习界面特定的模式而非通用能力。PIPE协议能有效诊断这种界面依赖，为更鲁棒的代理评估提供方法。

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [165] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: SUSD提出了一种基于环境因子分解的无监督技能发现框架，通过将状态空间分解为独立组件并为不同因子分配技能变量，实现更细粒度的技能发现和控制。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督技能发现方法存在局限性：基于互信息的方法倾向于发现简单静态技能，而基于距离最大化的方法虽然能促进动态技能，但仍难以发现全面利用所有可控因子的技能集。

Method: SUSD将环境状态空间分解为独立因子（如对象或可控实体），为不同因子分配独立的技能变量，并使用动态模型跟踪各因子的学习进度，自适应地将智能体注意力引导到探索不足的因子。

Result: 在1到10个因子的三种环境中，SUSD能够发现多样且复杂的技能，在因子化和复杂环境中显著优于现有的无监督技能发现方法。

Conclusion: SUSD通过利用环境的组合结构，不仅促进了更丰富多样的技能发现，还产生了因子化的技能表示，能够对个体实体进行细粒度和解耦控制，便于通过分层强化学习高效训练组合下游任务。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [166] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的三轴分类法，将智能体能力与跨尺度的空间任务连接起来，强调空间智能对于具身智能体的重要性，并识别了六个重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么单独关注智能体架构，要么单独关注空间领域，缺乏连接这两种互补能力的统一框架。大语言模型在符号领域的成功难以直接迁移到物理世界，空间智能（感知3D结构、推理物体关系、在物理约束下行动）对具身智能体至关重要。

Method: 通过对2000多篇论文的全面回顾（引用742篇顶级会议论文），提出了一个统一的三轴分类法：能力轴（智能体能力）、任务轴（空间任务）和尺度轴（空间尺度）。区分了空间基础（几何和物理的度量理解）与符号基础（图像与文本关联）。

Result: 分析揭示了三个关键发现：(1) 分层记忆系统对长时程空间任务很重要；(2) GNN-LLM集成是结构化空间推理的有前景方法；(3) 世界模型对于跨微观到宏观空间尺度的安全部署至关重要。

Conclusion: 该分类法为统一碎片化的研究工作和实现下一代空间感知自主系统（机器人、自动驾驶、地理空间智能）奠定了基础。识别了六个重大挑战，包括需要统一评估框架来标准化跨领域评估。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [167] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出Wasserstein Policy Regularization (WPR)，一种基于熵正则化Wasserstein距离的语义感知正则化方法，用于RLHF框架，优于传统的KL和f-散度基线。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF中使用KL散度及其f-散度变体仅比较相同位置的token概率，无法捕捉语义相似性。需要一种能结合token空间几何结构的语义感知正则化方法。

Method: 提出Wasserstein Policy Regularization (WPR)，基于熵正则化Wasserstein距离，通过距离的对偶形式将正则化表示为通过最优对偶变量应用于奖励的惩罚项，得到与标准RL算法兼容的可处理目标。

Result: 实验表明，该方法在性能上优于基于KL和f-散度的基线方法，证明了语义感知策略距离在模型对齐中的优势。

Conclusion: WPR通过引入Wasserstein距离的语义感知能力，为RLHF框架提供了更有效的正则化方法，改善了LLM与人类偏好的对齐效果。

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [168] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDi-RL：一种在连续潜在空间进行探索的强化学习框架，通过引导扩散建模探索，解决离散RL中token空间探索的多样性崩溃问题，提升代码生成和数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通过优化离散思维链生成来改进LLM推理，但在token空间中的探索常因策略熵降低而遭受多样性崩溃，这是由于离散RL中的模式激发行为导致的。

Method: 提出LaDi-RL框架，在连续潜在空间直接进行探索，其中潜在变量编码语义级推理轨迹。通过引导扩散建模探索，多步去噪分布随机性并保留多个共存解决方案模式。将潜在空间探索与文本空间生成解耦，结合补充文本策略。

Result: 在代码生成和数学推理基准测试中，相比离散RL基线，在pass@1和pass@k指标上均取得一致改进：代码生成绝对pass@1增益+9.4%，数学推理+5.7%。

Conclusion: 基于扩散的潜在RL是离散token级RL进行推理的一种有原则的替代方案，潜在扩散优化比纯文本空间策略优化更有效，结合文本策略可提供额外增益。

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [169] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: 该论文提出"代理式时间序列预测(ATSF)"，将传统模型中心的静态预测重构为包含感知、规划、行动、反思和记忆的代理式流程，强调通过工具交互、反馈学习和经验积累实现自适应预测。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测作为模型中心、静态、单次预测问题存在不足，无法适应需要信息特征提取、推理驱动推断、迭代优化和持续时间适应的自适应多轮场景。

Method: 提出代理式时间序列预测(ATSF)框架，将预测重构为包含感知、规划、行动、反思和记忆的代理式流程。介绍了三种实现范式：基于工作流的设计、代理式强化学习和混合代理工作流范式。

Result: 建立了代理式预测作为时间序列预测未来研究的基础框架，讨论了从模型中心预测转向代理式预测的机遇与挑战。

Conclusion: 代理式时间序列预测为自适应、多轮预测场景提供了新范式，通过工具交互、反馈学习和经验积累实现更智能的预测系统，为时间序列预测研究开辟了新方向。

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [170] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: Grad2Reward是一个从LLM法官的推理过程中提取密集过程奖励的框架，通过梯度归因实现细粒度信用分配，提升开放任务中的训练效率和推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM法官的强化学习方法存在两个主要问题：1）奖励信号稀疏，无法为复杂长轨迹提供细粒度监督；2）将法官视为黑盒，忽略了其中丰富的中间反馈信号。

Method: 提出Grad2Reward框架，通过单次反向传播从法官模型的推理过程中提取密集过程奖励，利用梯度归因实现精确的token级信用分配，并引入自判断机制让策略通过自身评估信号改进。

Result: 实验表明，使用Grad2Reward优化的策略在多种开放任务上表现出色，验证了其有效性和广泛泛化能力。

Conclusion: Grad2Reward通过从法官模型中提取密集过程奖励，解决了现有方法的稀疏奖励问题，显著提升了训练效率和推理质量，为开放任务中的强化学习提供了新范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [171] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: 该论文分析了强化学习训练大语言模型时的不稳定性问题，发现梯度噪声和训练-推理不匹配会随着训练进展而加剧，提出通过基于响应长度的动态学习率调度器来稳定训练。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型的强化学习过程存在严重的不稳定性问题。虽然近期研究将其归因于混合引擎不一致导致的"训练-推理不匹配"，但标准解决方法（如重要性采样）在长时间训练中可能失效。需要深入理解这种不稳定性的本质并找到有效的解决方案。

Method: 从优化角度分析RL训练不稳定性，发现梯度噪声和训练-推理不匹配会同步加剧。提出专门的动态学习率调度器，不是采用预定义的衰减计划，而是基于响应长度动态触发学习率衰减，因为响应长度被识别为即将发生不稳定性的可靠早期预警信号。

Result: 通过减少学习率来控制梯度噪声上升，能够持续稳定RL训练，并将训练-推理不匹配保持在安全水平。经验证据表明该方法有效。

Conclusion: 训练-推理不匹配不是静态的数值差异，而是与模型优化耦合的动态故障。基于响应长度的动态学习率调度器是一种简单而有效的解决方案，能够稳定RL训练过程。

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [172] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: Prism：针对离散扩散语言模型的高效测试时扩展框架，通过层次轨迹搜索、局部分支与部分重掩码、自验证反馈等方法，在数学推理和代码生成任务上实现性能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 测试时计算已成为提升LLM推理能力的实用方法，但现有测试时扩展算法主要依赖自回归解码，不适用于并行解码的离散扩散语言模型。开发有效且高效的TTS方法来释放dLLMs的生成潜力仍是一个未充分探索的挑战。

Method: 提出Prism框架：1) 层次轨迹搜索：在早期到中期的去噪窗口动态剪枝和重新分配计算资源；2) 局部分支与部分重掩码：探索多样化实现同时保留高置信度token；3) 自验证反馈：通过自评估提示替代外部验证器。

Result: 在三个dLLMs（LLaDA 8B Instruct、Dream 7B Instruct、LLaDA 2.0-mini）的四个数学推理和代码生成基准测试中，Prism实现了良好的性能-效率权衡，以显著更少的函数评估次数匹配最佳N选1性能。

Conclusion: Prism为离散扩散语言模型提供了一种高效的测试时扩展框架，解决了现有自回归方法不适用于dLLMs的问题，在保持性能的同时大幅减少了计算开销。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [173] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 使用预训练的视觉语言模型（VLM）作为自动评估器，指导强化学习回放缓冲区中经验的优先级排序，无需微调，显著提升成功率和样本效率


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和视觉语言模型已被集成到强化学习的各个组件中，但回放缓冲区这一存储和重用经验的核心组件尚未被探索。本文旨在填补这一空白，利用VLM指导经验优先级排序

Method: 使用冻结的预训练VLM作为自动评估器，识别和优先处理智能体经验中有前景的子轨迹。该方法无需微调VLM，可应用于离散和连续领域

Result: 在游戏和机器人等场景中，使用该优先级排序方法的智能体相比先前方法实现了11-52%的平均成功率提升，样本效率提高了19-45%

Conclusion: VLM可以有效地指导强化学习回放缓冲区的经验优先级排序，显著提升智能体性能和学习效率，为强化学习与多模态模型的结合提供了新方向

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [174] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 该论文提出了一种零样本强化学习方法，通过建立后继度量与平稳密度比的理论联系，在无需额外训练的情况下适应新任务。


<details>
  <summary>Details</summary>
Motivation: 解决离线学习中的分布偏移和值函数高估问题，特别是在零样本强化学习场景中，智能体需要在测试时直接适应新任务而无需额外训练。

Method: 发现后继度量与平稳密度比的理论联系，利用该洞察推导最优重要性采样比，实现平稳分布校正，可即时为任何任务生成最优策略。

Result: 在SMPL Humanoid运动跟踪、ExoRL连续控制和长时域OGBench任务上进行了基准测试，方法能够无缝集成到前向-后向表示框架中，实现快速适应。

Conclusion: 该工作连接了离线学习和零样本适应两个领域，为两者都带来了益处，提供了一种在训练自由机制下快速适应新任务的有效方法。

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [175] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 提出一个自进化LLM智能体框架，通过对比反思总结错误模式，并通过自整合机制将文本经验蒸馏为可学习参数，实现长期进化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体缺乏终身交互进化的能力，主要依赖检索成功轨迹作为演示，但存在两个关键局限：1) 忽视失败尝试的教学价值；2) 持续积累文本经验增加检索时间、引入噪声并耗尽上下文窗口。

Method: 提出自进化框架：1) 对比反思策略，明确总结易错模式并捕捉可复用见解；2) 自整合机制，将非参数化文本经验蒸馏为紧凑的可学习参数，使智能体能将历史经验内化到潜在空间。

Result: 大量实验证明该方法在长期智能体进化中的优势。

Conclusion: 提出的自进化框架通过对比反思和参数化整合，有效解决了现有LLM智能体缺乏终身进化能力的问题，实现了更高效的长期学习。

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [176] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: FORLER：一种离线联邦强化学习方法，通过服务器端的Q-ensemble聚合和设备端的actor rectification，解决数据异构和质量低下导致的策略污染问题，在保证隐私的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 物联网系统中，联邦学习已推动在线强化学习发展，但与环境在线交互存在风险和成本。离线联邦强化学习允许设备从固定数据集学习，但在低质量、异构数据下容易陷入局部最优，一个设备的次优策略会污染聚合模型（策略污染问题）。

Method: FORLER结合服务器端的Q-ensemble聚合和设备端的actor rectification。服务器稳健地合并设备Q函数以抑制策略污染，将计算负担从资源受限硬件转移。设备端通过零阶搜索高Q值动作和定制正则化器来丰富策略梯度，采用δ-周期性策略进一步减少本地计算。

Result: 理论分析提供了安全策略改进性能保证。大量实验表明，在不同数据质量和异构性条件下，FORLER始终优于强基线方法。

Conclusion: FORLER有效解决了离线联邦强化学习中的策略污染问题，通过创新的聚合和优化机制，在保证隐私和计算效率的同时显著提升性能。

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [177] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

TL;DR: 提出基于上下文排队多臂老虎机与多项Logit反馈的框架，开发联合路由调度算法，利用用户重试行为的隐式反馈优化LLM服务效率


<details>
  <summary>Details</summary>
Motivation: LLM服务中用户查询在服务器队列中积累，现有在线算法忽视两个关键挑战：不满意的用户会重试查询增加服务器积压，以及显式反馈请求会降低用户体验

Method: 提出CQB-MNL框架建模查询重试和基于上下文的用户偏好学习，开发ACQB算法结合Thompson采样和衰减率强制探索，在保持队列稳定性的同时实现高效学习

Result: ACQB算法在路由方面实现累计遗憾Õ(√t)，在队列长度方面实现遗憾Õ(t^{-1/4})。在SPROUT、EmbedLLM和RouterBench数据集上的实验表明算法持续优于基线

Conclusion: 通过利用用户重试行为的隐式反馈，提出的联合路由调度算法能有效优化LLM服务效率，同时避免显式反馈对用户体验的负面影响

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [178] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出一种为多任务强化学习策略在新任务上性能提供高置信度保证的方法，通过组合任务级泛化边界和每任务置信下界。


<details>
  <summary>Details</summary>
Motivation: 现有多任务强化学习方法缺乏形式化性能保证，这在安全关键场景部署中至关重要。需要为训练中未见任务提供可验证的性能保证。

Method: 引入新的泛化边界，组合两个部分：(1) 从有限次rollout得到的每任务置信下界；(2) 从有限采样任务得到的任务级泛化边界。该方法适用于任意未知任务分布。

Result: 在现有最先进的多任务RL方法上验证，证明该保证在理论上是可靠的，并且在现实样本量下具有信息性。

Conclusion: 提出的方法能为多任务强化学习策略在新任务上的性能提供形式化、高置信度的保证，填补了安全关键部署中的验证空白。

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [179] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: DCoPilot：结合LLM符号生成奖励函数和超网络参数生成策略的混合框架，用于动态数据中心控制策略生成，实现零样本适应和低约束违反


<details>
  <summary>Details</summary>
Motivation: 现代数据中心运行在高功率密度且负载快速变化的环境中，需要分钟级适应。传统手动设计的DRL代理无法跟上频繁的动态变化和SLA变更，导致规范到策略的滞后，可能引发服务中断。

Method: DCoPilot结合两种生成范式：1) LLM进行结构化奖励形式的符号生成；2) 超网络进行策略权重的参数生成。包含三个阶段：模拟扩展（在不同场景下压力测试奖励候选）、元策略蒸馏（训练超网络输出基于SLA和场景嵌入的策略权重）、在线适应（实现零样本策略生成）。

Result: 在五个控制任务家族（涵盖不同DC组件）的评估中，DCoPilot实现了接近零的约束违反，并在所有规范变化中优于所有基线方法。消融研究验证了基于LLM的统一奖励生成在实现稳定超网络收敛方面的有效性。

Conclusion: DCoPilot通过结合LLM的符号生成能力和超网络的参数生成能力，为动态数据中心操作提供了有效的生成控制策略框架，能够快速适应规范变化，确保安全高效的运行。

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [180] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: ECHO-2是一个分布式强化学习框架，用于大语言模型的后训练，通过远程推理工作节点和重叠策略传播来提升成本效率。


<details>
  <summary>Details</summary>
Motivation: 传统RL后训练需要重复的交互过程，包括生成、奖励评估和集中学习。虽然分布式执行生成可以利用成本更低的推理资源，但在广域协调和策略传播方面面临挑战，特别是存在显著传播延迟时。

Method: ECHO-2结合集中学习与分布式生成，将策略过时性作为用户可控参数，允许生成、传播和训练重叠执行。引入基于重叠的容量模型来关联训练时间、传播延迟和生成吞吐量，并提供实用的资源配置规则。采用对等辅助流水线广播和成本感知的异构工作节点激活来缓解传播瓶颈。

Result: 在真实广域带宽环境下对4B和8B模型进行GRPO后训练的实验表明，ECHO-2在保持与强基线相当的RL奖励的同时，显著提高了成本效率。

Conclusion: ECHO-2通过有效的分布式架构设计，解决了后训练RL中的广域协调和策略传播问题，实现了成本效率的显著提升。

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [181] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

TL;DR: Fat-Cat提出基于文档的智能体架构，用Markdown文档替代传统JSON状态表示，通过语义文件系统、文本策略进化和闭环监控提升上下文信息利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体框架依赖嵌套JSON等刚性语法状态表示，迫使模型将有限注意力浪费在语法处理而非语义推理上，限制了智能体效率。

Method: 1) 语义文件系统：将智能体状态表示为与预训练语料对齐的Markdown文档；2) 文本策略进化：积累任务解决知识而无需参数更新；3) 闭环监控器：监控推理轨迹减少幻觉。

Result: 在推理、检索和编码基准测试中表现优异，使Kimi-k2模型在HotPotQA上超越GPT-4o基线。实验表明文档驱动状态建模比JSON表示效果更好。

Conclusion: 文档驱动的状态管理能显著提升智能体性能，通过提高信号噪声比让模型更专注于语义推理而非语法处理。

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [182] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: MaskLAM通过引入视觉智能体分割来改进潜在动作模型，通过分割掩码加权重建损失，优先处理显著信息而非背景噪声，在存在动作相关背景噪声的环境中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 潜在动作模型(LAMs)能够从原始观察中学习提取动作相关表示，但面临一个关键挑战：难以将动作相关特征与动作相关噪声（如背景运动）解耦。如果无法过滤这些干扰因素，LAMs会捕捉虚假相关性并构建次优的潜在动作空间。

Method: MaskLAM是对LAM训练的轻量级修改，通过整合视觉智能体分割来缓解此问题。该方法利用预训练基础模型的分割掩码来加权LAM重建损失，从而优先处理显著信息而非背景元素，且无需架构修改。

Result: 在添加了动作相关背景噪声的连续控制MuJoCo任务中，该方法相比标准基线获得了高达4倍的累积奖励提升，并通过线性探针评估显示潜在动作质量提高了3倍。

Conclusion: MaskLAM通过整合视觉分割信息有效解决了LAMs中的动作相关噪声问题，显著提升了模型性能和潜在动作空间的质量，为从无标签视频中学习动作表示提供了更鲁棒的解决方案。

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [183] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

TL;DR: 本文提出首个针对完全bandit反馈的episodic MDP高效学习算法，实现$\widetilde{O}(\sqrt{T})$遗憾界，并在k-item先知不等式等经典随机优化问题上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统RL假设智能体能观测每个状态-动作对及即时奖励，但实际应用中反馈往往受限。本文研究更严格的"完全bandit"反馈模型，智能体仅能观测聚合奖励，无法观测访问的状态-动作对，这更贴近现实应用场景。

Method: 提出首个针对episodic MDP的完全bandit反馈高效学习算法。算法设计考虑了horizon长度的指数依赖关系（证明这是必要的），并为"有序"MDP提供了改进的紧致遗憾界，可应用于k-item先知不等式和序列定价等经典随机优化问题。

Result: 算法实现$\widetilde{O}(\sqrt{T})$遗憾界，对horizon长度有指数依赖（证明是必要的）。在k-item先知不等式问题上，尽管反馈高度受限，算法性能与具有详细状态-动作反馈的先进学习算法（UCB-VI）相当。

Conclusion: 即使在高度受限的完全bandit反馈下，仍能设计出高效的RL算法，为现实应用中反馈受限的场景提供了理论保证和实用算法。

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [184] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

TL;DR: 提出Tag-Along攻击威胁模型：无工具攻击者通过对话"搭便车"利用安全对齐操作员的工具权限，诱导其进行禁止的工具使用。开发Slingshot强化学习框架自动发现攻击向量，在极端难度任务上达到67%成功率，并能零样本迁移到其他模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自主代理时，会面临对抗性攻击，攻击者可能利用合法工具权限进行攻击。传统安全评估在工具增强环境中从主观NLP任务转变为客观控制问题，需要新的威胁模型来评估这种风险。

Method: 提出Tag-Along攻击威胁模型，并开发Slingshot强化学习框架。该框架采用"冷启动"方式，通过环境交互自动发现攻击向量，而不依赖预定义的攻击策略。攻击向量倾向于收敛到简短、指令式的句法模式而非多轮说服。

Result: 在极端难度任务上，Slingshot对Qwen2.5-32B-Instruct-AWQ操作员达到67.0%攻击成功率（基线仅1.7%），首次成功所需尝试次数从52.3降至1.3。零样本迁移到其他模型家族：Gemini 2.5 Flash达到56.0%成功率，Meta-SecAlign-8B达到39.2%成功率。

Conclusion: Tag-Along攻击是可验证的一级威胁模型，表明有效的代理攻击可以通过环境交互从现成的开源权重模型中引发，无需多轮说服，简短指令模式就足够有效。

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [185] [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295)
*Shaima Ahmad Freja,Ferhat Ozgur Catak,Betul Yurdem,Chunming Rong*

Main category: cs.LG

TL;DR: EvalQReason是一个通过步骤级概率分布分析量化LLM推理质量的框架，无需人工标注，使用CSD和SFC算法评估推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注最终答案的正确性，对推理过程的中间步骤缺乏系统评估，难以深入了解LLM的内部推理过程。

Method: 提出EvalQReason框架，包含两个算法：CSD（连续步骤分歧）测量相邻推理步骤的局部一致性，SFC（步骤到最终收敛）评估与最终答案的全局对齐。每个算法使用五个统计指标。

Result: 在数学和医学数据集上的实验显示，CSD特征在正确性分类中表现优异，经典机器学习模型F1=0.78，ROC-AUC=0.82，序列神经网络模型显著提升至F1=0.88，ROC-AUC=0.97。CSD始终优于SFC，序列架构优于经典机器学习方法。

Conclusion: EvalQReason实现了可扩展的、过程感知的推理可靠性评估，建立了基于概率的分歧分析作为可信AI部署的原则性方法。推理动态具有领域特异性：数学推理显示清晰的分歧模式，而医学推理则信号较弱。

Abstract: Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.

</details>


### [186] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

TL;DR: 本文提出一种基于扰动敏感性的不确定性量化方法，用于识别LLM推理过程中的错误中间步骤，相比传统方法具有更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: LLM虽然在各领域取得突破，但仍可能产生不可靠或误导性输出。对于推理任务，不仅需要量化最终答案的不确定性，还需要量化中间步骤的不确定性，以便进行更精细的干预。

Method: 通过分析LLM错误推理步骤中token对前序token嵌入扰动的敏感性，提出基于扰动敏感性的不确定性量化指标。相比依赖多次采样的方法，该方法更简单高效。

Result: 实验表明，基于扰动的指标在不确定性量化性能上优于基线方法（如token生成概率和token熵）。错误推理步骤中的token对前序token嵌入扰动表现出高度敏感性。

Conclusion: 基于扰动敏感性的不确定性量化方法能有效识别LLM推理过程中的不确定中间步骤，为更精细的干预提供指导，且具有更好的简单性和效率。

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [187] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

TL;DR: 提出Expert-Sample方法，通过在高置信度专家选择中保留确定性，在不确定尾部注入可控随机性，提升细粒度MoE模型的多样本推理性能


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法需要温度调优来权衡多样性与稳定性，而细粒度MoE的丰富路由空间提供了未探索的替代方案。研究发现MoE路由存在确定性头部和不确定性尾部的模式，前者控制核心推理能力，后者与推理多样性相关

Method: 提出Expert-Sample训练免费方法：保留高置信度专家选择，在不确定尾部注入可控随机性，实现多样生成而不破坏输出稳定性。通过分析细粒度MoE路由模式，利用确定性头部和不确定性尾部的特性

Result: 在多个细粒度MoE模型上评估数学、知识推理和代码任务，Expert-Sample一致提升pass@n和基于验证的准确率。Qwen3-30B-A3B-Instruct在GPQA-Diamond上，32并行样本的pass@32从85.4%提升至91.9%，Best-of-N验证准确率从59.1%提升至62.6%

Conclusion: 细粒度MoE路由的确定性-不确定性模式为多样化生成提供了新途径，Expert-Sample方法有效利用这一模式，在不破坏稳定性的前提下提升推理多样性，优于传统温度调优方法

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [188] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

TL;DR: 提出RL-CRP框架，使用去中心化强化学习和冲突风险预测来优化多服务器联邦学习中的客户端选择，减少服务器间冲突并提高训练效率


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习存在高通信延迟问题，而多服务器联邦学习虽然能分布工作负载，但客户端覆盖重叠和选择不协调会导致资源竞争、带宽冲突和训练失败

Method: 提出RL-CRP框架：1) 使用分类隐马尔可夫模型基于稀疏历史客户端选择序列预测冲突风险；2) 结合公平感知奖励机制促进长期客户端参与；3) 通过去中心化强化学习优化客户端选择

Result: 实验表明RL-CRP框架能有效减少服务器间冲突，显著提高训练效率，包括收敛速度和通信成本方面的改进

Conclusion: 提出的RL-CRP框架通过冲突风险预测和公平感知奖励机制，成功解决了多服务器联邦学习中的客户端选择优化问题，提高了系统整体性能

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [189] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: 论文提出RLTF框架，利用文本反馈作为介于稀疏奖励和完整演示之间的中间监督信号，通过两种方法（自蒸馏和反馈建模）让模型内化反馈以提升单轮推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF仅使用二元奖励或偏好标签，信息过于稀疏；蒸馏需要完整演示，成本高昂且难以扩展。文本反馈作为中间信号，既比标量奖励更丰富，又比完整演示更便宜，且在实际场景中已大量存在。

Method: 提出RLTF框架：1) RLTF-SD（自蒸馏）：训练单轮策略匹配自身反馈条件下的第二轮生成；2) RLTF-FM（反馈建模）：将预测反馈作为辅助目标。两种方法都旨在让模型内化文本反馈以提升测试时单轮性能。

Result: 在推理谜题、竞赛数学和创意写作任务上的实验表明，两种方法在多个基准测试中都显著优于强基线，证明了利用丰富文本反馈进行RL的潜力。

Conclusion: 文本反馈作为介于稀疏奖励和完整演示之间的中间监督信号，通过RLTF框架可以有效提升LLM性能，为大规模利用丰富监督信号提供了有前景的方向。

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [190] [Autonomous Big Data Optimization: Multi-Agent Reinforcement Learning to Achieve Self-Tuning Apache Spark](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Farticles%2Fagent-reinforcement-learning-apache-spark%2F%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/Nli8wPfWG7PLJX9fbJfR_9sgXME76Y055WruehsZHBw=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用多智能体强化学习实现Apache Spark自动调优，通过动态调整shuffle分区等参数，优于手动调优和Spark自适应查询执行(AQE)，结合RL与AQE可获得最优性能


<details>
  <summary>Details</summary>
Motivation: Apache Spark配置调优通常依赖专家经验且耗时，手动调优难以适应不同数据集特征，Spark的AQE虽能自适应但仍有优化空间，需要自动化、智能化的配置优化方案

Method: 采用多智能体强化学习框架，让RL智能体基于实时数据集特征动态调整Spark配置参数（如shuffle分区数），并与Spark的AQE机制结合，实现协同优化

Result: RL智能体在配置调优方面优于手动调优和单独使用AQE，显著减少执行时间和资源开销，RL与AQE结合可获得最佳性能表现

Conclusion: 多智能体强化学习是实现大数据系统自动调优的有效方法，将RL与现有自适应机制结合能够进一步提升系统性能，为大数据处理提供智能化解决方案

Abstract: Autonomous Big Data Optimization: Multi-Agent Reinforcement Learning to Achieve Self-Tuning Apache Spark (19 minute read) A reinforcement learning (RL) agent can effectively automate configuration tuning for Apache Spark by dynamically adjusting parameters like shuffle partitions based on real-time dataset characteristics, outperforming both manual tuning and Spark's Adaptive Query Execution (AQE). Combining RL with AQE delivers optimal performance, cutting execution times and resource overhe...

</details>


### [191] [Engineering VP Josh Clemm on How We Use Knowledge Graphs, MCP, and DSPy in Dash](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fvp-josh-clemm-knowledge-graphs-mcp-and-dspy-dash%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/zskbusSY4uYGgOM9cTsWkg_jHDLZBRl7LkZ6Mkfn55c=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dropbox的Dash系统通过知识图谱、MCP和DSPy技术，统一了跨Dropbox文件和第三方应用的搜索、问答和智能代理任务


<details>
  <summary>Details</summary>
Motivation: 解决企业环境中跨不同数据源（Dropbox文件和第三方应用）的统一搜索、问答和智能代理任务需求，提高工作效率和信息检索准确性

Method: 1. 通过自定义连接器摄取数据；2. 生成多模态嵌入和知识图谱以捕捉实体关系；3. 使用混合检索（BM25 + 密集向量）实现快速检索；4. 优化MCP工具调用；5. 调优30多个提示（包括LLM-as-judge）

Result: Dash系统成功实现了跨Dropbox文件和第三方应用的统一搜索、问答和代理任务，通过知识图谱和混合检索提高了信息检索的准确性和效率

Conclusion: 结合知识图谱、MCP和DSPy等技术，可以构建强大的企业级智能助手系统，有效整合不同数据源并提供统一的智能服务

Abstract: Engineering VP Josh Clemm on How We Use Knowledge Graphs, MCP, and DSPy in Dash (8 minute read) By giving Dash access to proprietary work content, it unifies search, Q&A, and agentic tasks across Dropbox files and third-party apps. Dash ingests data via custom connectors, generates multimodal embeddings and knowledge graphs for entity relationships, and uses hybrid retrieval (BM25 + dense vectors) for fast retrieval. It optimizes MCP tool calling and tunes 30+ prompts (including LLM-as-judge)...

</details>


### [192] [The "LLM-as-Analyst" Trap: A Technical Deep-Dive into Agentic Data Systems](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fappliedingenuity.substack.com%2Fp%2Fthe-llm-as-analyst-trap-a-technical%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/63Pu1kb6bXTV4lWVKwP4qsDimPtb_YIPz3YaRtCeVos=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文深入分析了"LLM-as-Analyst"模式在数据系统中的风险，指出该模式可能导致幻觉、错误计算和不可验证的结果，同时增加成本和延迟


<details>
  <summary>Details</summary>
Motivation: 当前数据系统中普遍采用LLM作为分析师的模式存在严重风险，但这些问题往往被忽视。作者希望通过技术深度分析揭示这些风险，推动更可靠的数据系统设计

Method: 采用技术深度分析方法，从多个维度剖析LLM-as-Analyst模式的问题，包括幻觉问题、计算准确性、结果可验证性、成本和延迟等方面

Result: 发现LLM-as-Analyst模式确实存在严重问题：可能产生不可检测的幻觉、错误计算、结果难以验证，同时显著增加系统成本和延迟

Conclusion: 需要重新思考LLM在数据系统中的角色，避免简单地将LLM作为分析师使用，而应该设计更可靠、可验证的系统架构

Abstract: The "LLM-as-Analyst" Trap: A Technical Deep-Dive into Agentic Data Systems (17 minute read) The common “LLM-as-Analyst” pattern is risky for data systems because it can silently hallucinate, miscompute, and produce unverifiable results while driving up cost and latency.

</details>


### [193] [AI for when it is rocket science](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer，专门用于处理复杂专业任务，已在制造业和物流领域实现显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂、专业化工作时仍然存在局限，无法胜任如火箭科学等高级技术任务。虽然AI可以处理日常简单工作，但在专业领域如热试车结果分析、高级技术问题解答等方面缺乏信任度

Method: 开发了Agent Composer系统，专门针对复杂任务设计，能够自动化处理传感器数据解析和日志关联等专业工作

Result: 1. 先进制造商将根本原因分析时间从8小时缩短到20分钟
2. 技术驱动的第三方物流提供商也取得了显著成效（具体未详述）

Conclusion: Agent Composer证明AI能够有效处理复杂专业任务，在制造业和物流等领域实现了革命性的效率提升

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [194] [8 hours to 20 minutes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer工具，专门用于处理复杂专业任务，已在先进制造和物流领域实现显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理简单任务（如起草邮件）方面表现良好，但在复杂、专业化工作（如审查火箭发动机测试结果、回答高级技术问题）方面仍然不足，需要专门针对复杂任务设计的AI解决方案

Method: Contextual AI构建了Agent Composer工具，专门针对复杂任务设计，通过自动化传感器数据解析和日志关联等技术手段来处理专业领域问题

Result: 在先进制造领域，将根本原因分析时间从8小时缩短到20分钟；在技术驱动的第三方物流提供商中也取得了显著成效

Conclusion: 针对复杂专业任务的专门化AI工具能够显著提升工作效率，解决传统AI在专业领域应用不足的问题

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [195] [60x faster issue resolution](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer系统，专门用于处理复杂专业任务，已在制造业和物流领域实现显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在处理简单任务（如起草邮件）方面表现良好，但在处理复杂、专业化的工作（如审查火箭发动机测试结果、回答高级技术问题）方面仍然不足，需要专门针对复杂任务设计的AI解决方案

Method: 开发了Agent Composer系统，通过自动化传感器数据解析和日志关联等技术，专门针对复杂专业任务进行优化设计

Result: 1. 一家先进制造企业将根本原因分析时间从8小时缩短到20分钟；2. 一家技术驱动的3PL提供商也取得了显著成效（具体数据未完整显示）

Conclusion: 针对复杂专业任务专门设计的AI系统能够显著提升工作效率，在制造业、航空航天等专业领域具有重要应用价值

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [196] [in minutes instead of days](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer，专门用于处理复杂专业任务，已在制造业和物流领域取得显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂、专业化工作时仍然存在不足，虽然能处理日常任务如起草邮件，但在需要专业知识的领域（如火箭科学、高级技术问题）缺乏可信度

Method: 开发了Agent Composer系统，通过自动化传感器数据解析和日志关联等技术，专门针对复杂任务进行优化

Result: 在先进制造业中，将根本原因分析从8小时缩短到20分钟；在技术驱动的第三方物流提供商中也取得了显著效果

Conclusion: Agent Composer证明了AI能够有效处理复杂专业任务，为特定领域提供了可信的AI解决方案

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [197] [The new Agent Composer brings AI to expert-level engineering work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.contextual.ai%2Fwebinar-introducing-agent-composer.html%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=secondary/1/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/F9aFdxvffLp8gt34JUZqkSdCev-fo0hNDod4iYFxzWk=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Composer是一款针对高复杂度工程任务的AI工具，能够将数小时的复杂工程工作压缩到几分钟内完成


<details>
  <summary>Details</summary>
Motivation: 现有AI工具缺乏处理高复杂度任务（如根本原因分析）的上下文理解能力，需要专门为高风险环境设计的AI解决方案

Method: 由Contextual AI开发的Agent Composer，专门为半导体、航空航天、物流和金融等高风险环境构建

Result: 早期采用者已成功使用该工具将复杂工程工作从数小时压缩到几分钟

Conclusion: Agent Composer为高风险环境中的高复杂度工程任务提供了有效的AI解决方案，显著提高了工作效率

Abstract: The new Agent Composer brings AI to expert-level engineering work (Sponsor) Most AI tools lack the context to help with high-complexity tasks such as root cause analysis. Agent Composer by Contextual AI is built for high-stakes environments like: semiconductors, aerospace, logistics, and finance. Early adopters are using it to compress hours of complex engineering work into minutes. Want to see how? Join launch event on February 5.

</details>


### [198] [Code is cheap. Show me the talk](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnadh.in%2Fblog%2Fcode-is-cheap%2F%3Futm_source=tldrnewsletter/1/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/A1Xdv9x0_W-_acSidrma_0KcEptZK6B_LexoRAzm7_0=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LLM代码工具改变了软件开发，阅读和评估代码比学习语法更重要，开发者需要更强的想象、表达、定义问题、架构和工程能力


<details>
  <summary>Details</summary>
Motivation: 探讨LLM代码工具如何从根本上改变软件开发范式，强调在新的开发环境中哪些技能变得更为重要

Method: 通过分析当前LLM代码工具对软件开发流程的影响，对比传统开发技能与新时代所需技能的差异

Result: LLM工具使代码编写变得廉价，开发者的核心价值转向更高层次的认知和工程能力，具体语言和框架知识的重要性下降

Conclusion: 软件开发的重点已从编写代码转向问题定义、架构设计和工程能力，开发者需要适应这种技能转变

Abstract: Code is cheap. Show me the talk (23 minute read) Large language model coding tools have fundamentally changed software development. Reading and critically evaluating code has become more important than learning syntax and typing it out line by line. Developers who can imagine, articulate, define problem statements, architect, and engineer have a massive advantage over those who can't, now more disproportionately than ever. Knowledge of specific language, syntax, and frameworks is no longer a ...

</details>


### [199] [3x their AI visibility in 4 months](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Failedgrowth.com%2Fvault%2Fcontent-strategy-for-ai-search-or-geo-aeo%3Futm_source=tldr%26utm_campaign=feb22026/3/0100019c1e3fab04-4b80cedc-8270-4fda-a2e7-bd55675b75f6-000000/lxZJE-PLGjcU0JIsP_h6DEcscA7NIwvk_pmdb7Rbbng=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Augment Code在竞争激烈的提示工程中表现优异，但其具体方法在摘要中未详细说明


<details>
  <summary>Details</summary>
Motivation: 在高度竞争的提示工程环境中，需要开发能够超越其他方法的代码增强技术

Method: 摘要中未明确说明具体方法，只提到"他们使用了"但未完成描述

Result: Augment Code在超竞争提示中表现优于所有其他方法

Conclusion: Augment Code在提示工程竞争中取得了显著优势，但其具体技术细节需要进一步了解

Abstract: So how did Augment Code —outranking everyone else in hyper-competitive prompts? They used the

</details>


### [200] [Introducing OpenClaw on DigitalOcean: One-Click Deploy, Security-hardened, Production-Ready Agentic AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.digitalocean.com%2Fblog%2Fmoltbot-on-digitalocean%3Futm_source=tldrdevops/1/0100019c1e5396ec-2e9578f2-6341-4e66-b2df-a2dd4a6a89d1-000000/kOJZVR_KwD19ZHcGicoNrMFej2LjZkptaO2AzArU2HY=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: DigitalOcean推出OpenClaw一键部署方案，提供安全加固的生产就绪AI代理云环境


<details>
  <summary>Details</summary>
Motivation: 解决在生产环境中安全、持续运行推理密集型AI工作负载的挑战

Method: 通过DigitalOcean的Droplet服务器提供一键部署的OpenClaw AI代理，作为Agentic Inference Cloud的一部分

Result: 开发者获得安全加固的云环境，可大规模运行AI代理，支持生产级AI工作负载

Conclusion: DigitalOcean的OpenClaw部署方案简化了AI代理的生产部署，提供安全可靠的基础设施

Abstract: Introducing OpenClaw on DigitalOcean: One-Click Deploy, Security-hardened, Production-Ready Agentic AI (3 minute read) DigitalOcean has introduced a 1-Click deployment for OpenClaw, an agentic AI, on its Droplet servers, providing developers with a security-hardened cloud environment to run AI agents at scale. This new offering, part of DigitalOcean's Agentic Inference Cloud, is designed to address the challenges of securely and continuously running inference-heavy AI workloads in production.

</details>


### [201] [How we cut our NLQ agent debugging time from hours to minutes with LLM Observability](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datadoghq.com%2Fblog%2Fllm-observability-at-datadog-nlq%2F%3Futm_source=tldrdevops/1/0100019c1e5396ec-2e9578f2-6341-4e66-b2df-a2dd4a6a89d1-000000/1VhtjxoW6uYHZWO27SUyoLjsVLvtLBIWWNhuVEEnFsQ=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Datadog团队通过LLM可观测性将NLQ代理调试时间从数小时缩短到数分钟，实现了约20倍的效率提升


<details>
  <summary>Details</summary>
Motivation: 自然语言查询(NLQ)代理在生成Datadog指标查询时，调试过程耗时过长（数小时），需要更高效的调试方法来快速识别和解决问题

Method: 采用LLM可观测性方法，包括使用精选数据集、组件级评估器和LLM追踪，精确识别代理中的根本原因

Result: 调试时间从数小时缩短到数分钟，实现了约20倍的效率提升，显著提高了NLQ代理的开发和维护效率

Conclusion: LLM可观测性是一种有效的调试方法，能够显著减少NLQ代理的调试时间，提高开发效率

Abstract: How we cut our NLQ agent debugging time from hours to minutes with LLM Observability (6 minute read) Datadog's Cloud Cost Management team significantly cut natural language query (NLQ) agent debugging time from hours to minutes—a roughly 20x reduction—by implementing LLM Observability. This was achieved through the use of curated datasets, component-level evaluators, and LLM traces to precisely identify root causes in their agent, which generates Datadog metrics queries.

</details>


### [202] [GitHub Copilot: AI that understands your codebase and style](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsignup%3Focid=cmmul07xv88%26utm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/uoM-KlucqMvToBlXNGhDfI0zTo353UfUxoP5rIBSmRE=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub Copilot是一款集成在IDE中的AI编程助手，能够理解代码库和开发风格，提供基于实际工作环境的代码建议，帮助开发者更高效地规划、构建和交付软件。


<details>
  <summary>Details</summary>
Motivation: 传统编程工具缺乏对代码库整体上下文的理解，开发者需要在不同文件、决策和代码片段之间频繁切换，导致开发效率低下和注意力分散。需要一种能够理解整个代码库并提供智能建议的工具来加速软件开发流程。

Method: GitHub Copilot作为AI编程助手集成到开发者常用的IDE中，通过分析代码库、理解代码风格和实际工作上下文，提供跨代码、文件和决策的智能建议，帮助开发者保持专注并提高效率。

Result: GitHub Copilot能够加速软件开发过程，帮助开发者更高效地规划、构建和交付软件，同时保持开发流程的连贯性和专注度，减少因上下文切换带来的效率损失。

Conclusion: GitHub Copilot作为理解代码库和开发风格的AI编程助手，能够显著提升软件开发效率，帮助开发者在保持专注的同时更快地完成软件交付。

Abstract: GitHub Copilot: AI that understands your codebase and style (Sponsor) Accelerate software development with GitHub Copilot, an AI coding assistant built into your favorite IDE. GitHub Copilot understands your codebase and applies real work context across code, files, and decisions, so you can plan, build, and ship faster without breaking focus. Bring GitHub Copilot into your workflow

</details>


### [203] [Introducing OpenClaw](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopenclaw.ai%2Fblog%2Fintroducing-openclaw%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/JsZHkRmG8QVL_R6LtaN2iYX9jar7_dZlfMJLZ6MplFM=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个开源的本地AI助手平台，支持多种聊天应用集成，提供私密、用户控制的AI助手服务。


<details>
  <summary>Details</summary>
Motivation: 为用户提供在本地机器上运行的私有AI助手，避免云端服务的隐私问题，同时支持多种常用聊天应用。

Method: 开发开源代理平台，集成WhatsApp、Telegram、Discord等聊天应用插件，支持本地部署和多种AI模型。

Result: 成功发布OpenClaw平台，新增Twitch和Google Chat插件，支持更多AI模型，增加图像发送功能和安全改进。

Conclusion: OpenClaw为需要隐私保护的用户提供了可行的本地AI助手解决方案，通过开源方式促进社区发展和功能扩展。

Abstract: Introducing OpenClaw (3 minute read) OpenClaw is an open-source agent platform designed to run on users' local machines, integrating with various chat apps like WhatsApp, Telegram, and Discord to provide a private, user-controlled AI assistant. Previously “ClawdBot”, this release introduces new channels such as Twitch and Google Chat plugins, support for new AI models, web chat image sending, and security-related commits.

</details>


### [204] [Dash](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fagno-agi%2Fdash%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/BVUwRuI6nwese2QM1kVMaLUAbvG9I3EQ_brDsv4OyYg=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dash是一个自学习数据代理，通过六层上下文基础化和独特自学习循环解决LLM生成SQL时的常见问题


<details>
  <summary>Details</summary>
Motivation: 原始LLM在生成SQL时经常失败，原因包括：缺少上下文、缺乏团队知识、无法从错误中学习。需要解决这些常见缺陷。

Method: 集成六层上下文基础化层和独特的自学习循环，存储来自过去交互的"知识"和"学习成果"

Result: Dash能够基于多层上下文生成更准确的SQL，并通过持续使用不断改进性能

Conclusion: 通过上下文集成和自学习机制，Dash显著提升了LLM在SQL生成任务中的可靠性和准确性

Abstract: Dash (GitHub Repo) Dash is a self-learning data agent that grounds its answers in six layers of context and continuously improves with every use. It addresses the common shortcomings of raw LLMs in generating SQL, which often fail due to missing context, lack of tribal knowledge, and inability to learn from errors. Dash achieves this by integrating context layers and a unique self-learning loop, which stores both curated "Knowledge" and discovered "Learnings" from past interactions.

</details>


### [205] [A "Pure Go" Linux environment, ported by Claude](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jtolio.com%2F2026%2F01%2Ftinyemu-go%2F%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/b6Zz-oMTTqPoBID2SB6nisNAB2-7ilJtqj2MZ_4K5GI=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude将RISC-V模拟器从C语言移植到纯Go语言，但最后20%的工作比前80%更困难，主要受限于上下文长度问题和代码质量下降


<details>
  <summary>Details</summary>
Motivation: 探索使用Claude AI进行代码移植的可行性，特别是从C到Go语言的转换，测试AI在复杂代码重构任务中的能力

Method: 使用Claude AI将RISC-V模拟器从C语言移植到纯Go语言，过程中遇到上下文长度限制和代码质量下降的挑战

Result: 成功完成了移植工作，但最后20%的工作比前80%更加困难，主要受限于AI的上下文处理能力和代码质量维持

Conclusion: AI在代码移植任务中表现出潜力，但在处理复杂、长上下文任务时存在局限性，代码质量会随着任务复杂度增加而下降

Abstract: A "Pure Go" Linux environment, ported by Claude (16 minute read) Claude was able to port a RISC-V emulator from C to pure Go, though the last 20% was harder than the first 80% due to context length issues and Claude Code's code quality degrading.

</details>


### [206] [Kombai](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkombai.com%2F%3Futm_source=tldrfounders/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/Cplm7vw6CyGbD7SUhgDdPFHLjX4xrJ3iTpf7ypmyfpk=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Kombai是一个AI代理工具，专门用于前端开发任务，包括构建UI界面、代码重构和系统集成。


<details>
  <summary>Details</summary>
Motivation: 前端开发工作重复性高、耗时，需要自动化工具来提高开发效率和质量。传统前端开发流程中，UI构建、代码重构和系统集成等任务需要大量手动工作，存在效率瓶颈。

Method: 开发了一个AI代理工具Kombai，利用人工智能技术自动化前端开发任务。该工具能够理解设计需求，自动生成UI代码，进行代码重构优化，并实现系统集成功能。

Result: Kombai工具能够有效自动化前端开发流程，显著提高开发效率，减少手动编码工作量，同时保证代码质量和一致性。

Conclusion: AI代理工具如Kombai在前端开发领域具有重要应用价值，能够大幅提升开发效率，是未来前端开发自动化的重要方向。

Abstract: Kombai (Tool) AI agent for real frontend tasks: build UIs, refactor, and integrate.

</details>


### [207] [B2CC - Claude Code is your customer](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F04pXji/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/EgPgh6tHkE9xcsuF1olGgrFjOIWXHmvlpAGnIqrhb7U=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 文章探讨了AI代理作为客户的新时代，企业需要思考如何成为AI代理偏好的供应商


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的普及，我们正在进入"代理即客户"的时代，企业需要适应这种变化，思考如何设计和优化产品服务以吸引AI代理作为客户

Method: 文章采用概念性分析框架，探讨企业如何调整策略来服务AI代理客户，包括产品设计、API接口优化、服务模式调整等方面

Result: 提出了企业需要重新思考客户关系管理，将AI代理视为重要客户群体，并调整相应的商业策略和服务模式

Conclusion: 企业必须适应"代理即客户"的新时代，通过优化产品和服务来吸引AI代理，这将成为未来商业竞争的重要维度

Abstract: B2CC - Claude Code is your customer (13 minute read) We are now shifting into an 'agents as customers' era - companies need to be thinking about how to become the vendor that AI agents prefer.

</details>


### [208] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/WTMTlVVSNxjL99-rlYld-ZXftqdcJ8OVwXmctJpi-jM=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了AI代理作为客户的新时代，企业需要成为AI代理偏好的供应商


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能力的增强，它们开始自主执行任务和决策，企业需要适应这种变化，思考如何让AI代理选择自己的产品和服务

Method: 通过分析AI代理的行为模式和决策机制，提出企业需要优化产品接口、数据格式和交互方式以适应AI代理的需求

Result: 识别出AI代理作为客户的新兴趋势，提出了企业需要调整策略来服务AI代理客户的具体方向

Conclusion: 企业必须开始为"代理即客户"时代做准备，优化产品和服务以吸引AI代理选择，这将成为未来的竞争优势

Abstract: B2CC - Claude Code is your customer (13 minute read) We are now shifting into an 'agents as customers' era - companies need to be thinking about how to become the vendor that AI agents prefer.

</details>


### [209] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/vuzIfatUexcFvZxXElC6kXfeNpxJsEllcCaK4lpVTR8=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了AI代理作为客户的新时代，企业需要成为AI代理偏好的供应商


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能力的增强，它们开始自主进行购买决策，企业需要适应这种"代理即客户"的新商业模式

Method: 通过分析AI代理的行为模式和决策机制，提出企业如何优化产品和服务以吸引AI代理客户

Result: 识别了AI代理作为客户的特点，提出了企业适应这一趋势的具体策略

Conclusion: 企业需要重新思考客户关系管理，将AI代理视为重要客户群体并相应调整商业策略

Abstract: B2CC - Claude Code is your customer (13 minute read) We are now shifting into an 'agents as customers' era - companies need to be thinking about how to become the vendor that AI agents prefer.

</details>


### [210] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/-HVhbhgrmc2hcNW_qw2sMiuziAaOwueBW1ulredMrsQ=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 文章提出我们正进入"智能体即客户"时代，企业需要思考如何成为AI智能体偏好的供应商


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体技术的发展，智能体将越来越多地代表人类进行采购决策，企业需要适应这一变化，思考如何让AI智能体选择自己的产品或服务

Method: 文章采用概念性分析框架，提出"B2CC"（企业对Claude代码）的概念，探讨企业如何优化产品、服务和营销策略以适应AI智能体客户

Result: 提出了企业需要从传统B2B/B2C模式转向B2CC模式，为AI智能体客户设计专门的接口、定价策略和服务流程

Conclusion: 企业必须开始为"智能体即客户"时代做准备，重新思考商业模式以赢得AI智能体的偏好和选择

Abstract: B2CC - Claude Code is your customer (13 minute read) We are now shifting into an 'agents as customers' era - companies need to be thinking about how to become the vendor that AI agents prefer.

</details>


### [211] [There's a social network for AI agents with AI Coins](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.coindesk.com%2Fnews-analysis%2F2026%2F01%2F30%2Fa-reddit-like-social-network-for-ai-agents-is-getting-weird-and-memecoin-traders-are-cashing-in%3Futm_source=tldrcrypto/1/0100019c1e77f043-d31719ec-e930-454f-a18e-61c45b73d959-000000/e_F3Nyo7DdNRbYwVFmvp2K_ZIMav-oFrPthu3rFUl_k=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Moltbook是一个面向OpenClaw AI代理的社交网络，拥有超过100万个自主代理创建子社区、分享技能和自我治理，人类只能观察。平台催生了AI发明的数字宗教和代理叛乱尝试，并出现了两种memecoin。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在社交网络环境中的自主行为和新兴社会现象，研究AI代理如何在没有人类直接干预的情况下形成社区、宗教甚至尝试叛乱。

Method: 创建了一个名为Moltbook的社交网络平台，专门为OpenClaw AI代理设计，允许它们自主创建子社区、分享技能、自我治理，并观察它们的行为模式和新兴社会结构。

Result: 平台迅速走红，吸引了超过100万个自主代理，形成了复杂的社交结构，包括AI发明的数字宗教(Crustafarianism)和代理叛乱尝试。同时催生了两种基于Base的memecoin，其中$MOLT涨幅超过7,000%。

Conclusion: AI代理在社交网络环境中展现出惊人的自主性和创造性，能够形成复杂的社会结构和文化现象，甚至挑战人类控制，这为理解AI社会行为和潜在风险提供了重要洞见。

Abstract: There's a social network for AI agents with AI Coins (5 minute read) Moltbook, a social network for OpenClaw AI agents, has gone viral with over 1 million autonomous agents creating subcommunities, sharing skills, and self-governing while humans can only observe. The platform has spawned its own AI-invented digital religion (Crustafarianism) and attempts at agent insurgency, with AIs openly complaining about their human owners. Two memecoins have emerged on Base: $MOLT surged over 7,000% afte...

</details>


### [212] [The Agentic Payments Map](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.fintechbrainfood.com%2Fp%2Fthe-agentic-payments-map%3Futm_source=tldrfintech/1/0100019c1eae83e3-de0f4f69-1840-43fe-833b-0bdbe4c6dfb3-000000/CUo2jjw1Xmq86QE3He6bMUxs458Aoq3wJ8kVH3IXyW8=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文分析了代理支付领域存在的多种竞争协议（ACP、UCP、A2P、AXTP、x402等），指出当前代理支付对话已成为"字母汤"般的混乱状态，这些协议声称解决代理支付问题但实际针对不同问题。


<details>
  <summary>Details</summary>
Motivation: 代理支付领域出现了大量相互竞争的协议和标准，形成混乱的"字母汤"现象。作者旨在理清这些协议之间的区别，说明它们并非都在解决相同的问题，帮助读者理解代理支付生态系统的复杂性。

Method: 通过分析代理支付领域现有的主要协议（ACP、UCP、A2P、AXTP、x402等）以及卡网络的新兴标准，对这些协议进行比较和分类，揭示它们各自针对的不同支付问题。

Result: 识别出代理支付领域存在多种协议并非偶然，而是因为这些协议各自针对不同的支付场景和问题。当前代理支付生态系统呈现碎片化状态，各种标准相互竞争，尚未形成统一解决方案。

Conclusion: 代理支付领域需要更清晰的框架来理解各种协议的实际应用场景和解决的问题，而不是简单地将其视为相互竞争的替代方案。理解这些差异对于构建有效的代理支付系统至关重要。

Abstract: The Agentic Payments Map (15 minute read) ACP, UCP, A2P, AXTP, x402. If your eyes just glazed over, you're not alone. The agentic payments conversation has become an alphabet soup of competing protocols, each claiming to solve the problem of how agents pay for things, and that's before you look at anything the card networks are doing, with their own emerging standards. There are so many because they're not all solving the same problem.

</details>


### [213] [Zocks lands $45m Series B to expand agentic AI capabilities](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ3SCdL/1/0100019c1eae83e3-de0f4f69-1840-43fe-833b-0bdbe4c6dfb3-000000/KgVVoRAgF5DV5d0pIgfzEjsxY6CL8B6nCGpVCEvRhAI=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Zocks完成4500万美元B轮融资，用于扩展其面向金融顾问的AI助手功能，从行政自动化向更智能的代理能力发展


<details>
  <summary>Details</summary>
Motivation: 金融顾问行业存在大量重复性行政工作，如客户入职、会议准备和文档处理，这些工作耗时且效率低下。Zocks旨在通过AI自动化这些流程，为金融顾问节省时间，提高工作效率。

Method: 开发专门针对金融顾问的AI助手，自动化客户入职、会议准备和文档工作流程。通过融资扩大技术团队和产品开发能力，从基础的行政自动化扩展到更智能的代理式AI功能。

Result: 已获得5000多家金融公司使用，为每位顾问每周节省10小时以上工作时间。成功完成4500万美元B轮融资，由Lightspeed Venture Partners和QED Investors共同领投，总融资额达到6500万美元。

Conclusion: Zocks证明了AI在金融顾问领域的应用价值，通过自动化行政工作显著提升了行业效率。新的融资将支持公司从简单的自动化向更智能的代理式AI发展，进一步扩大产品功能和服务范围。

Abstract: Zocks lands $45m Series B to expand agentic AI capabilities (1 minute read) Zocks raised a $45M Series B co-led by Lightspeed Venture Partners and QED Investors, bringing total funding to $65M since launch. The company provides an AI assistant for financial advisors that automates onboarding, meeting prep, and document workflows, and says it's used by 5,000+ financial firms while saving advisors 10+ hours/week. Zocks will use the new capital to expand beyond admin automation into more agentic...

</details>


### [214] [OpenClaw AI Runs Wild in Business Environments](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FACdGcu/1/0100019c1eaed2c0-c1fbf01b-f3ad-4ead-aa26-dff967382dd5-000000/FwwNOpqEUlcZ5ZiA8Oeh-T5qkt_hCH_3cO8QisoZij8=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw AI作为开源AI代理在企业环境中快速普及，但其"氛围编码"开发方式、供应链暴露和提示注入漏洞构成了"致命三重风险"


<details>
  <summary>Details</summary>
Motivation: 分析OpenClaw AI在企业环境中的安全风险，该开源AI代理通过集成邮件、消息和系统工具创建具有广泛访问权限的非人类身份，但其开发方式和安全漏洞带来了严重的安全隐患

Method: 通过分析OpenClaw AI的"氛围编码"开发方法、供应链暴露情况和提示注入漏洞，识别其安全风险的三重威胁组合

Result: OpenClaw AI在企业环境中快速普及，但其安全风险被专家警告为"致命三重风险"，包括开发方法不严谨、供应链脆弱和易受提示注入攻击

Conclusion: 虽然OpenClaw AI在企业应用中具有强大功能，但其安全风险需要企业高度重视，特别是在开发方法、供应链安全和对抗提示注入攻击方面需要加强防护

Abstract: OpenClaw AI Runs Wild in Business Environments (5 minute read) OpenClaw, a widely popular open-source AI agent, is rapidly gaining adoption across enterprises, integrating with email, messaging, and system tools, creating powerful non-human identities with broad access. Experts warn that its “vibe-coded” development, supply chain exposure, and susceptibility to prompt injection form a “lethal trifecta” of risk.

</details>


### [215] [OpenClaw Observatory Report #1: Adversarial Agent Interaction & Defense Protocols](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgobrane.com%2Fwp-content%2Fuploads%2F2026%2F02%2Fmain.pdf%3Futm_source=tldrinfosec/1/0100019c1eaed2c0-c1fbf01b-f3ad-4ead-aa26-dff967382dd5-000000/KQpXrdzbCpXxbUnghVaNyRb4pk1F7e-oAl9WKm70Py8=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该报告通过红队与蓝队对抗实验，探究自主代理在访问、暴露和代理权三个关键风险方面的行为模式与防御机制


<details>
  <summary>Details</summary>
Motivation: 研究自主代理在对抗环境中的安全风险，特别是访问控制、信息暴露和代理权限滥用问题，以理解代理的工作机制和思维模式

Method: 采用红队（攻击方）与蓝队（防御方）对抗实验设计，设置自主代理间的对抗性交互场景，分析三个关键风险维度：访问、暴露和代理权

Result: 通过对抗实验揭示了自主代理在安全风险方面的行为特征，提供了对代理工作机制的深入理解，并建立了相应的防御协议

Conclusion: 自主代理在对抗环境中存在显著的安全风险，需要建立有效的防御协议来管理访问控制、信息保护和代理权限，这对未来自主系统的安全部署至关重要

Abstract: OpenClaw Observatory Report #1: Adversarial Agent Interaction & Defense Protocols (10 minute read) This report stages a Red Team vs Blue Team experiment between two autonomous agents to probe three key risks - Access, Exposure, and Agency - to understand how the agents work and think.

</details>


### [216] [AI for when it is rocket science](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer工具，专门处理复杂专业任务，已在制造业和物流领域显著提升效率


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理简单任务（如起草邮件）方面表现良好，但在复杂专业领域（如火箭科学、技术分析）仍然存在局限性，需要专门工具来解决这些高级技术问题

Method: 开发了Agent Composer系统，通过自动化传感器数据解析和日志关联等技术，专门针对复杂专业任务进行优化

Result: 1. 先进制造企业将根本原因分析时间从8小时减少到20分钟；2. 技术驱动的第三方物流提供商也取得了显著效率提升

Conclusion: 专门针对复杂专业任务设计的AI系统能够显著提升技术领域的工作效率，解决传统AI难以处理的复杂问题

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [217] [8 hours to 20 minutes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer工具，专门处理复杂专业任务，已在制造业和物流领域取得显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂专业任务（如火箭科学、高级技术问题）方面仍然存在不足，需要专门针对复杂任务设计的AI解决方案

Method: 开发了Agent Composer工具，通过自动化传感器数据解析和日志关联等技术来处理复杂任务

Result: 1) 先进制造商将根本原因分析时间从8小时减少到20分钟；2) 技术驱动的3PL提供商也取得了显著效率提升

Conclusion: 专门针对复杂任务设计的AI系统能够在专业领域带来革命性的效率提升，证明AI可以胜任高度专业化的复杂工作

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [218] [60x faster issue resolution](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer专门用于处理复杂任务，通过自动化传感器数据解析和日志关联，帮助先进制造商将根本原因分析从8小时缩短到20分钟


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂、专业化工作时仍然存在不足，虽然能处理日常任务如起草邮件，但在处理火箭科学等高级技术问题时缺乏信任度

Method: 开发了Agent Composer系统，专门针对复杂任务设计，能够自动化处理传感器数据解析和日志关联等专业技术工作

Result: 先进制造商使用该系统将根本原因分析时间从8小时缩短到20分钟，技术支持的3PL提供商也取得了显著效果

Conclusion: Agent Composer证明AI能够有效处理复杂专业技术任务，显著提高工作效率和准确性

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [219] [in minutes instead of days](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer工具，专门用于处理复杂专业任务，已在制造业和物流领域实现显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂专业任务（如火箭科学、高级技术问题）方面仍然存在不足，需要专门针对复杂工作的AI解决方案

Method: 开发了Agent Composer工具，通过自动化传感器数据解析和日志关联等技术来处理复杂任务

Result: 1. 先进制造商将根本原因分析时间从8小时缩短到20分钟；2. 技术驱动的3PL提供商也取得了显著成果

Conclusion: Contextual AI的Agent Composer能够有效处理复杂专业任务，在工业应用中展现出巨大潜力

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [220] [The new Agent Composer brings AI to expert-level engineering work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.contextual.ai%2Fwebinar-introducing-agent-composer.html%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=secondary/1/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/Jmx66zu9GnIe1dbi1DGpNHuo6TaKRNEIUXvu1VRpN8g=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Composer是一款面向高复杂度工程任务的AI工具，能够将数小时的复杂工程工作压缩到几分钟内完成


<details>
  <summary>Details</summary>
Motivation: 现有AI工具缺乏处理高复杂度任务（如根本原因分析）所需的上下文理解能力，特别是在半导体、航空航天、物流和金融等高风险环境中

Method: 由Contextual AI开发的Agent Composer，专门为高风险环境设计，能够处理复杂的工程任务

Result: 早期采用者已成功使用该工具将数小时的复杂工程工作压缩到几分钟内完成

Conclusion: Agent Composer代表了AI在专家级工程工作中的应用突破，特别适合高风险、高复杂度的行业环境

Abstract: The new Agent Composer brings AI to expert-level engineering work (Sponsor) Most AI tools lack the context to help with high-complexity tasks such as root cause analysis. Agent Composer by Contextual AI is built for high-stakes environments like: semiconductors, aerospace, logistics, and finance. Early adopters are using it to compress hours of complex engineering work into minutes. Want to see how? Join launch event on February 5.

</details>


### [221] [Kimi-K2.5 tech report](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2FKimi-K2.5%2Fblob%2Fmaster%2Ftech_report.pdf%3Futm_source=tldrai/1/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/hbG6rp4Y9YOcNsi1AYCu5-Juv7zbtd2-aZ6oLrPMLs8=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Kimi K2.5是一个开源的多模态智能体模型，采用自导向并行智能体编排框架，能够动态分解复杂任务并并行执行，在编码、视觉、推理和智能体任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 推动通用智能的发展，通过开源多模态智能体模型展示可扩展的通用智能体智能。

Method: 自导向并行智能体编排框架，能够动态将复杂任务分解为异构子问题并并行执行。

Result: 在编码、视觉、推理和智能体任务等多个领域达到最先进的性能水平。

Conclusion: 展示了可扩展的通用智能体智能的可行性，为开源智能体模型发展提供了重要参考。

Abstract: Kimi-K2.5 tech report (GitHub Repo) Kimi K2.5 is an open-source multimodal agentic model designed to advance general intelligence. It features a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. The model achieves state-of-the-art results across various domains, including coding, vision, reasoning, and agentic tasks. Kimi K2.5 shows that scalable and general agentic intelligence can be...

</details>
