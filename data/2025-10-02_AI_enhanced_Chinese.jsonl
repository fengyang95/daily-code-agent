{"id": "2510.00031", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.00031", "abs": "https://arxiv.org/abs/2510.00031", "authors": ["Shun-ichiro Hayashi", "Koki Morita", "Daichi Mukunoki", "Tetsuya Hoshino", "Takahiro Katagiri"], "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs", "comment": null, "summary": "We propose VibeCodeHPC, an automatic tuning system for HPC programs based on\nmulti-agent LLMs for code generation. VibeCodeHPC tunes programs through\nmulti-agent role allocation and iterative prompt refinement. We describe the\nsystem configuration with four roles: Project Manager (PM), System Engineer\n(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent\ndeployment and activity monitoring functions to facilitate effective\nmulti-agent collaboration. In our case study, we convert and optimize CPU-based\nmatrix-matrix multiplication code written in C to GPU code using CUDA. The\nmulti-agent configuration of VibeCodeHPC achieved higher-quality code\ngeneration per unit time compared to a solo-agent configuration. Additionally,\nthe dynamic agent deployment and activity monitoring capabilities facilitated\nmore effective identification of requirement violations and other issues.", "AI": {"tldr": "VibeCodeHPC\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684HPC\u7a0b\u5e8f\u81ea\u52a8\u8c03\u4f18\u7cfb\u7edf\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u914d\u548c\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u6765\u8c03\u4f18\u7a0b\u5e8f\u3002", "motivation": "\u89e3\u51b3HPC\u7a0b\u5e8f\u8c03\u4f18\u4e2d\u5355\u4e00\u667a\u80fd\u4f53\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u56db\u89d2\u8272\u591a\u667a\u80fd\u4f53\u914d\u7f6e\uff08\u9879\u76ee\u7ecf\u7406\u3001\u7cfb\u7edf\u5de5\u7a0b\u5e08\u3001\u7a0b\u5e8f\u5458\u3001\u6301\u7eed\u4ea4\u4ed8\uff09\uff0c\u7ed3\u5408\u52a8\u6001\u667a\u80fd\u4f53\u90e8\u7f72\u548c\u6d3b\u52a8\u76d1\u63a7\u529f\u80fd\u3002", "result": "\u5728\u77e9\u9635\u4e58\u6cd5CPU\u5230GPU\u4ee3\u7801\u8f6c\u6362\u6848\u4f8b\u4e2d\uff0c\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u5728\u5355\u4f4d\u65f6\u95f4\u5185\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\uff0c\u5e76\u80fd\u66f4\u6709\u6548\u8bc6\u522b\u9700\u6c42\u8fdd\u89c4\u95ee\u9898\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728HPC\u7a0b\u5e8f\u8c03\u4f18\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u52a8\u6001\u90e8\u7f72\u548c\u76d1\u63a7\u529f\u80fd\u589e\u5f3a\u4e86\u534f\u4f5c\u6548\u679c\u3002", "topic": "code agent"}}
{"id": "2510.00022", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.00022", "abs": "https://arxiv.org/abs/2510.00022", "authors": ["Ansh Kamthan"], "title": "Learning to Lead Themselves: Agentic AI in MAS using MARL", "comment": "Exploring foundational behaviours of agentic ai using MARL 39 pages -\n  25 minute read, 5 tables, 24 equation, 9 figures", "summary": "As autonomous systems move from prototypes to real deployments, the ability\nof multiple agents to make decentralized, cooperative decisions becomes a core\nrequirement. This paper examines how agentic artificial intelligence, agents\nthat act independently, adaptively and proactively can improve task allocation\nand coordination in multi-agent systems, with primary emphasis on drone\ndelivery and secondary relevance to warehouse automation. We formulate the\nproblem in a cooperative multi-agent reinforcement learning setting and\nimplement a lightweight multi-agent Proximal Policy Optimization, called IPPO,\napproach in PyTorch under a centralized-training, decentralized-execution\nparadigm. Experiments are conducted in PettingZoo environment, where multiple\nhomogeneous drones or agents must self-organize to cover distinct targets\nwithout explicit communication.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u4e3b\u667a\u80fd\u4f53\u5982\u4f55\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u4efb\u52a1\u5206\u914d\u548c\u534f\u8c03\uff0c\u4e3b\u8981\u5e94\u7528\u4e8e\u65e0\u4eba\u673a\u914d\u9001\u548c\u4ed3\u5e93\u81ea\u52a8\u5316\u573a\u666f\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u4ece\u539f\u578b\u8f6c\u5411\u5b9e\u9645\u90e8\u7f72\uff0c\u591a\u667a\u80fd\u4f53\u505a\u51fa\u53bb\u4e2d\u5fc3\u5316\u3001\u534f\u4f5c\u51b3\u7b56\u7684\u80fd\u529b\u6210\u4e3a\u6838\u5fc3\u9700\u6c42\u3002", "method": "\u5728\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(IPPO)\u65b9\u6cd5\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u3001\u5206\u6563\u6267\u884c\u7684\u8303\u5f0f\uff0c\u5728PettingZoo\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u591a\u4e2a\u540c\u8d28\u65e0\u4eba\u673a\u6216\u667a\u80fd\u4f53\u80fd\u591f\u5728\u6ca1\u6709\u663e\u5f0f\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u81ea\u7ec4\u7ec7\u8986\u76d6\u4e0d\u540c\u76ee\u6807\u3002", "conclusion": "\u81ea\u4e3b\u667a\u80fd\u4f53\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4efb\u52a1\u5206\u914d\u548c\u534f\u8c03\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00161", "abs": "https://arxiv.org/abs/2510.00161", "authors": ["Kimihiro Hasegawa", "Wiradee Imrattanatrai", "Masaki Asada", "Ken Fukuda", "Teruko Mitamura"], "title": "TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding", "comment": "21 pages. Code: https://github.com/kimihiroh/tama", "summary": "Procedural activity assistants potentially support humans in a variety of\nsettings, from our daily lives, e.g., cooking or assembling flat-pack\nfurniture, to professional situations, e.g., manufacturing or biological\nexperiments. Despite its potential use cases, the system development tailored\nfor such an assistant is still underexplored. In this paper, we propose a novel\nframework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural\nactivity understanding. TAMA enables interleaved multimodal reasoning by making\nuse of multimedia-returning tools in a training-free setting. Our experimental\nresult on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our\napproach can improve the performance of vision-language models, especially\nGPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support\nfor the effectiveness of two features that characterize our framework,\nmultimedia-returning tools and agentic flexible tool selection. We believe our\nproposed framework and experimental results facilitate the thinking with images\nparadigm for video and multimodal tasks, let alone the development of\nprocedural activity assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e86TAMA\u6846\u67b6\uff0c\u4e00\u79cd\u7528\u4e8e\u7a0b\u5e8f\u6027\u6d3b\u52a8\u7406\u89e3\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u591a\u5a92\u4f53\u8fd4\u56de\u5de5\u5177\u5b9e\u73b0\u8bad\u7ec3\u514d\u8d39\u7684\u591a\u6a21\u6001\u63a8\u7406", "motivation": "\u7a0b\u5e8f\u6027\u6d3b\u52a8\u52a9\u624b\u5728\u65e5\u5e38\u751f\u6d3b\u548c\u4e13\u4e1a\u573a\u666f\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u76f8\u5173\u7cfb\u7edf\u5f00\u53d1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22", "method": "\u4f7f\u7528\u591a\u5a92\u4f53\u8fd4\u56de\u5de5\u5177\u8fdb\u884c\u8bad\u7ec3\u514d\u8d39\u7684\u4ea4\u4e92\u5f0f\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5177\u5907\u7075\u6d3b\u7684\u5de5\u5177\u9009\u62e9\u80fd\u529b", "result": "\u5728ProMQA-Assembly\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662fGPT-5\u548cMiMo-VL\u6a21\u578b", "conclusion": "\u8be5\u6846\u67b6\u4fc3\u8fdb\u4e86\u56fe\u50cf\u601d\u7ef4\u8303\u5f0f\u5728\u89c6\u9891\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u63a8\u52a8\u4e86\u7a0b\u5e8f\u6027\u6d3b\u52a8\u52a9\u624b\u7684\u53d1\u5c55", "topic": "agent analysis"}}
{"id": "2510.00023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00023", "abs": "https://arxiv.org/abs/2510.00023", "authors": ["Quy Minh Le", "Minh Sao Khue Luu", "Khanh-Tung Tran", "Duc-Hai Nguyen", "Hoang-Quoc-Viet Pham", "Quan Le", "Hoang Thanh Lam", "Hoang D. Nguyen"], "title": "ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools", "comment": null, "summary": "Effective tool use is essential for agentic AI, yet training agents to\nutilize tools remains challenging due to manually designed rewards, limited\ntraining data, and poor multi-tool selection, resulting in slow adaptation,\nwasted computational resources, and suboptimal performance. We introduce\nToolBrain, a lightweight and user-friendly framework for coaching tool use in\nagentic models with flexible reinforcement learning (RL), easing the barriers\nfor researchers and practitioners to adapt LLM-based agents to specific\ndomains. It supports a wide range of training strategies, including RL\nalgorithms such as GRPO and DPO, as well as supervised learning. ToolBrain\nenables custom reward callables directly on an agent's execution traces or\nsimply utilizes an automated LLM-as-a-judge system for reward generation. It is\npacked with useful capabilities, including knowledge distillation from large to\nsmall models for efficient development, automatic task generation from tool\ndescriptions, seamless tool retrieval, efficient fine-tuning pipelines with\nQLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate\nToolBrain through diverse use cases, such as training a CodeAct agent to\nautonomously execute email search tasks, showing fast, targeted improvements\n(up to 30.0%) in tool-use skills while keeping the codebase simple and\nextensible in Agentic AI. Our framework is publicly available at\nhttps://toolbrain.org.", "AI": {"tldr": "ToolBrain\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3AI\u4ee3\u7406\u4f7f\u7528\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u3001\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u591a\u5de5\u5177\u9009\u62e9\u56f0\u96be\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3AI\u4ee3\u7406\u4f7f\u7528\u5de5\u5177\u9762\u4e34\u6311\u6218\uff1a\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u3001\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3001\u591a\u5de5\u5177\u9009\u62e9\u56f0\u96be\uff0c\u5bfc\u81f4\u9002\u5e94\u6162\u3001\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u548c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "ToolBrain\u6846\u67b6\u652f\u6301\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ecGRPO\u548cDPO\u7b49RL\u7b97\u6cd5\u4ee5\u53ca\u76d1\u7763\u5b66\u4e60\uff0c\u63d0\u4f9b\u81ea\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u548c\u81ea\u52a8LLM\u8bc4\u5224\u7cfb\u7edf\uff0c\u5177\u5907\u77e5\u8bc6\u84b8\u998f\u3001\u4efb\u52a1\u751f\u6210\u3001\u5de5\u5177\u68c0\u7d22\u7b49\u529f\u80fd\u3002", "result": "\u5728\u4ee3\u7801\u4ee3\u7406\u6267\u884c\u90ae\u4ef6\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cToolBrain\u5b9e\u73b0\u4e86\u5de5\u5177\u4f7f\u7528\u6280\u80fd\u7684\u5feb\u901f\u9488\u5bf9\u6027\u6539\u8fdb\uff08\u63d0\u5347\u8fbe30.0%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u5e93\u7b80\u6d01\u53ef\u6269\u5c55\u3002", "conclusion": "ToolBrain\u964d\u4f4e\u4e86\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u5c06LLM\u4ee3\u7406\u9002\u914d\u5230\u7279\u5b9a\u9886\u57df\u7684\u95e8\u69db\uff0c\u4e3aAgentic AI\u63d0\u4f9b\u4e86\u7b80\u5355\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00172", "abs": "https://arxiv.org/abs/2510.00172", "authors": ["Amirhossein Abaskohi", "Tianyi Chen", "Miguel Mu\u00f1oz-M\u00e1rmol", "Curtis Fox", "Amrutha Varshini Ramesh", "\u00c9tienne Marcotte", "Xing Han L\u00f9", "Nicolas Chapados", "Spandana Gella", "Christopher Pal", "Alexandre Drouin", "Issam H. Laradji"], "title": "DRBench: A Realistic Benchmark for Enterprise Deep Research", "comment": null, "summary": "We introduce DRBench, a benchmark for evaluating AI agents on complex,\nopen-ended deep research tasks in enterprise settings. Unlike prior benchmarks\nthat focus on simple questions or web-only queries, DRBench evaluates agents on\nmulti-step queries (for example, ``What changes should we make to our product\nroadmap to ensure compliance with this standard?\") that require identifying\nsupporting facts from both the public web and private company knowledge base.\nEach task is grounded in realistic user personas and enterprise context,\nspanning a heterogeneous search space that includes productivity software,\ncloud file systems, emails, chat conversations, and the open web. Tasks are\ngenerated through a carefully designed synthesis pipeline with\nhuman-in-the-loop verification, and agents are evaluated on their ability to\nrecall relevant insights, maintain factual accuracy, and produce coherent,\nwell-structured reports. We release 15 deep research tasks across 10 domains,\nsuch as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness\nof DRBench by evaluating diverse DR agents across open- and closed-source\nmodels (such as GPT, Llama, and Qwen) and DR strategies, highlighting their\nstrengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.", "AI": {"tldr": "DRBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u590d\u6742\u3001\u5f00\u653e\u5f0f\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u9488\u5bf9\u4f01\u4e1a\u73af\u5883\u8bbe\u8ba1\uff0c\u6db5\u76d6\u591a\u6b65\u9aa4\u67e5\u8be2\u548c\u5f02\u6784\u641c\u7d22\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u95ee\u9898\u6216\u4ec5\u57fa\u4e8e\u7f51\u7edc\u7684\u67e5\u8be2\uff0c\u65e0\u6cd5\u8bc4\u4f30AI\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5904\u7406\u590d\u6742\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u6210\u6d41\u6c34\u7ebf\u751f\u6210\u4efb\u52a1\uff0c\u5305\u542b\u4eba\u5de5\u9a8c\u8bc1\u73af\u8282\uff0c\u8bc4\u4f30\u4ee3\u7406\u4ece\u516c\u5171\u7f51\u7edc\u548c\u79c1\u6709\u77e5\u8bc6\u5e93\u4e2d\u8bc6\u522b\u76f8\u5173\u4e8b\u5b9e\u3001\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\u4ee5\u53ca\u751f\u6210\u8fde\u8d2f\u62a5\u544a\u7684\u80fd\u529b\u3002", "result": "\u53d1\u5e03\u4e86\u6db5\u76d610\u4e2a\u9886\u57df\uff08\u5982\u9500\u552e\u3001\u7f51\u7edc\u5b89\u5168\u3001\u5408\u89c4\uff09\u768415\u4e2a\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8eGPT\u3001Llama\u3001Qwen\u7b49\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u7684\u591a\u79cdDR\u4ee3\u7406\u3002", "conclusion": "DRBench\u6709\u6548\u63ed\u793a\u4e86\u4e0d\u540c\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u4f18\u52bf\u3001\u5f31\u70b9\uff0c\u4e3a\u63a8\u8fdb\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u8def\u5f84\u3002", "topic": "swe benchmark"}}
{"id": "2510.00324", "categories": ["cs.SE", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00324", "abs": "https://arxiv.org/abs/2510.00324", "authors": ["Lucas Roberts", "Denisa Roberts"], "title": "Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?", "comment": "Accepted as a full paper at SIGIR-AP 2025", "summary": "Code search is an important information retrieval application. Benefits of\nbetter code search include faster new developer on-boarding, reduced software\nmaintenance, and ease of understanding for large repositories. Despite\nimprovements in search algorithms and search benchmarks, the domain of code\nsearch has lagged behind. One reason is the high cost of human annotation for\ncode queries and answers. While humans may annotate search results in general\ntext QA systems, code annotations require specialized knowledge of a\nprogramming language (PL), as well as domain specific software engineering\nknowledge. In this work we study the use of Large Language Models (LLMs) to\nretrieve code at the level of functions and to generate annotations for code\nsearch results. We compare the impact of the retriever representation (sparse\nvs. semantic), programming language, and LLM by comparing human annotations\nacross several popular languages (C, Java, Javascript, Go, and Python). We\nfocus on repositories that implement common data structures likely to be\nimplemented in any PLs. For the same human annotations, we compare several\nLLM-as-a-Judge models to evaluate programming language and other affinities\nbetween LLMs. We find that the chosen retriever and PL exhibit affinities that\ncan be leveraged to improve alignment of human and AI relevance determinations,\nwith significant performance implications. We also find differences in\nrepresentation (sparse vs. semantic) across PLs that impact alignment of human\nand AI relevance determinations. We propose using transpilers to bootstrap\nscalable code search benchmark datasets in other PLs and in a case study\ndemonstrate that human-AI relevance agreement rates largely match the (worst\ncase) human-human agreement under study. The application code used in this work\nis available at \\href{https://github.com/rlucas7/code-searcher/}{this github\nrepo}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u641c\u7d22\u548c\u6ce8\u91ca\u751f\u6210\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u68c0\u7d22\u5668\u8868\u793a\u3001\u7f16\u7a0b\u8bed\u8a00\u548cLLM\u5bf9\u4ee3\u7801\u641c\u7d22\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u68c0\u7d22\u5668\u548c\u7f16\u7a0b\u8bed\u8a00\u4e4b\u95f4\u5b58\u5728\u4eb2\u548c\u6027\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u8f6c\u8bd1\u5668\u6765\u6269\u5c55\u4ee3\u7801\u641c\u7d22\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u4ee3\u7801\u641c\u7d22\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5229\u7528LLMs\u6765\u6539\u8fdb\u4ee3\u7801\u641c\u7d22\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9700\u8981\u7f16\u7a0b\u8bed\u8a00\u4e13\u4e1a\u77e5\u8bc6\u7684\u4ee3\u7801\u6807\u6ce8\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u7a00\u758f\u8868\u793a\u548c\u8bed\u4e49\u8868\u793a\u68c0\u7d22\u5668\uff0c\u5728\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e2d\u8bc4\u4f30LLMs\u7684\u4ee3\u7801\u68c0\u7d22\u548c\u6ce8\u91ca\u751f\u6210\u80fd\u529b\uff0c\u4f7f\u7528LLM-as-a-Judge\u6a21\u578b\u8bc4\u4f30\u7f16\u7a0b\u8bed\u8a00\u4eb2\u548c\u6027\uff0c\u5e76\u7814\u7a76\u4eba\u7c7b\u4e0eAI\u76f8\u5173\u6027\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "result": "\u53d1\u73b0\u9009\u62e9\u7684\u68c0\u7d22\u5668\u548c\u7f16\u7a0b\u8bed\u8a00\u5b58\u5728\u4eb2\u548c\u6027\uff0c\u53ef\u4ee5\u6539\u5584\u4eba\u7c7b\u548cAI\u76f8\u5173\u6027\u5224\u65ad\u7684\u4e00\u81f4\u6027\uff1b\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u5728\u8868\u793a\u65b9\u6cd5\u4e0a\u5b58\u5728\u5dee\u5f02\uff1b\u901a\u8fc7\u8f6c\u8bd1\u5668\u6784\u5efa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u80fd\u8fbe\u5230\u63a5\u8fd1\u4eba\u7c7b\u95f4\u4e00\u81f4\u6027\u7684\u6548\u679c\u3002", "conclusion": "LLMs\u53ef\u4ee5\u6709\u6548\u7528\u4e8e\u4ee3\u7801\u641c\u7d22\u548c\u6ce8\u91ca\u751f\u6210\uff0c\u68c0\u7d22\u5668\u548c\u7f16\u7a0b\u8bed\u8a00\u7684\u4eb2\u548c\u6027\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f7f\u7528\u8f6c\u8bd1\u5668\u53ef\u4ee5\u6269\u5c55\u4ee3\u7801\u641c\u7d22\u57fa\u51c6\u6570\u636e\u96c6\u3002", "topic": "swe application"}}
{"id": "2510.00328", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00328", "abs": "https://arxiv.org/abs/2510.00328", "authors": ["Ahmed Fawzy", "Amjed Tahir", "Kelly Blincoe"], "title": "Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook -- a Grey Literature Review", "comment": null, "summary": "AI code generation tools are transforming software development, especially\nfor novice and non-software developers, by enabling them to write code and\nbuild applications faster and with little to no human intervention. Vibe coding\nis the practice where users rely on AI code generation tools through intuition\nand trial-and-error without necessarily understanding the underlying code.\nDespite widespread adoption, no research has systematically investigated why\nusers engage in vibe coding, what they experience while doing so, and how they\napproach quality assurance (QA) and perceive the quality of the AI-generated\ncode. To this end, we conduct a systematic grey literature review of 101\npractitioner sources, extracting 518 firsthand behavioral accounts about vibe\ncoding practices, challenges, and limitations. Our analysis reveals a\nspeed-quality trade-off paradox, where vibe coders are motivated by speed and\naccessibility, often experiencing rapid ``instant success and flow'', yet most\nperceive the resulting code as fast but flawed. QA practices are frequently\noverlooked, with many skipping testing, relying on the models' or tools'\noutputs without modification, or delegating checks back to the AI code\ngeneration tools. This creates a new class of vulnerable software developers,\nparticularly those who build a product but are unable to debug it when issues\narise. We argue that vibe coding lowers barriers and accelerates prototyping,\nbut at the cost of reliability and maintainability. These insights carry\nimplications for tool designers and software development teams. Understanding\nhow vibe coding is practiced today is crucial for guiding its responsible use\nand preventing a broader QA crisis in AI-assisted development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790101\u4e2a\u5b9e\u8df5\u8005\u8d44\u6599\uff0c\u7814\u7a76\u4e86\"\u6c1b\u56f4\u7f16\u7a0b\"\u73b0\u8c61\u2014\u2014\u7528\u6237\u4f9d\u8d56AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u8fdb\u884c\u76f4\u89c9\u5f0f\u7f16\u7a0b\uff0c\u53d1\u73b0\u5b58\u5728\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u6096\u8bba\uff0c\u867d\u7136\u52a0\u901f\u4e86\u539f\u578b\u5f00\u53d1\u4f46\u727a\u7272\u4e86\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "motivation": "\u5c3d\u7ba1AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5c1a\u65e0\u7814\u7a76\u7cfb\u7edf\u8c03\u67e5\u7528\u6237\u4e3a\u4f55\u8fdb\u884c\u6c1b\u56f4\u7f16\u7a0b\u3001\u4ed6\u4eec\u7684\u4f53\u9a8c\u4ee5\u53ca\u5982\u4f55\u8fdb\u884c\u8d28\u91cf\u4fdd\u8bc1\u3002", "method": "\u5bf9101\u4e2a\u5b9e\u8df5\u8005\u8d44\u6599\u8fdb\u884c\u7cfb\u7edf\u6027\u7070\u8272\u6587\u732e\u7efc\u8ff0\uff0c\u63d0\u53d6\u4e86518\u4e2a\u5173\u4e8e\u6c1b\u56f4\u7f16\u7a0b\u5b9e\u8df5\u3001\u6311\u6218\u548c\u9650\u5236\u7684\u7b2c\u4e00\u624b\u884c\u4e3a\u63cf\u8ff0\u3002", "result": "\u53d1\u73b0\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u6096\u8bba\uff1a\u6c1b\u56f4\u7f16\u7a0b\u8005\u8ffd\u6c42\u901f\u5ea6\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u4f53\u9a8c\u5feb\u901f\u6210\u529f\u548c\u5fc3\u6d41\uff0c\u4f46\u5927\u591a\u8ba4\u4e3a\u751f\u6210\u7684\u4ee3\u7801\u5feb\u901f\u4f46\u6709\u7f3a\u9677\uff1b\u8d28\u91cf\u4fdd\u8bc1\u5b9e\u8df5\u5e38\u88ab\u5ffd\u89c6\uff0c\u8bb8\u591a\u7528\u6237\u8df3\u8fc7\u6d4b\u8bd5\u6216\u4f9d\u8d56\u5de5\u5177\u8f93\u51fa\u3002", "conclusion": "\u6c1b\u56f4\u7f16\u7a0b\u964d\u4f4e\u4e86\u95e8\u69db\u5e76\u52a0\u901f\u4e86\u539f\u578b\u5f00\u53d1\uff0c\u4f46\u4ee5\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u4e3a\u4ee3\u4ef7\uff0c\u521b\u9020\u4e86\u4e00\u7c7b\u65b0\u7684\u6613\u53d7\u653b\u51fb\u7684\u8f6f\u4ef6\u5f00\u53d1\u4eba\u5458\uff0c\u5bf9\u5de5\u5177\u8bbe\u8ba1\u8005\u548c\u5f00\u53d1\u56e2\u961f\u5177\u6709\u91cd\u8981\u542f\u793a\u3002", "topic": "agent analysis"}}
{"id": "2510.00177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00177", "abs": "https://arxiv.org/abs/2510.00177", "authors": ["Shuyue Stella Li", "Avinandan Bose", "Faeze Brahman", "Simon Shaolei Du", "Pang Wei Koh", "Maryam Fazel", "Yulia Tsvetkov"], "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It", "comment": "57 pages, 6 figures", "summary": "Current large language model (LLM) development treats task-solving and\npreference alignment as separate challenges, optimizing first for objective\ncorrectness, then for alignment to aggregated human preferences. This paradigm\nfails in human-facing applications where solving a problem correctly is\ninsufficient if the response mismatches the user's needs. This challenge\nintensifies in just-in-time scenarios where no prior user interaction history\nexists due to cold-start conditions or privacy constraints. LLMs need to\nidentify what they don't know about user preferences, strategically elicit\npreference values through questioning, then adapt their reasoning processes and\nresponses accordingly -- a complicated chain of cognitive processes which we\nterm personalized reasoning. We introduce PREFDISCO, an evaluation methodology\nthat transforms static benchmarks into interactive personalization tasks using\npsychologically-grounded personas with sparse preferences. Our framework\ncreates scenarios where identical questions require different reasoning chains\ndepending on user context, as optimal explanation approaches vary by individual\nexpertise and preferences while maintaining factual accuracy. Evaluation of 21\nfrontier models across 10 tasks reveals 29.0% of naive personalization attempts\nproduce worse preference alignment than generic responses, yet generic\nresponses also fail to serve individual user needs effectively. These findings\nsuggest personalized reasoning requires dedicated development rather than\nemerging naturally. PREFDISCO establishes personalized reasoning as a\nmeasurable research frontier and reveals fundamental limitations in current\nLLMs' interactive capabilities, providing a foundation for developing systems\nthat can adapt to individual users in education, healthcare, and technical\ndomains where personalization is critical.", "AI": {"tldr": "PREFDISCO\u662f\u4e00\u4e2a\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u9759\u6001\u57fa\u51c6\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\uff0c\u4f7f\u7528\u5fc3\u7406\u5b66\u57fa\u7840\u7684\u89d2\u8272\u548c\u7a00\u758f\u504f\u597d\u6765\u8bc4\u4f30LLM\u7684\u4e2a\u6027\u5316\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5f00\u53d1\u5c06\u4efb\u52a1\u89e3\u51b3\u548c\u504f\u597d\u5bf9\u9f50\u89c6\u4e3a\u72ec\u7acb\u6311\u6218\uff0c\u4f46\u5728\u9762\u5411\u4eba\u7c7b\u7684\u5e94\u7528\u4e2d\uff0c\u6b63\u786e\u89e3\u51b3\u95ee\u9898\u4e0d\u8db3\u591f\uff0c\u5982\u679c\u54cd\u5e94\u4e0d\u5339\u914d\u7528\u6237\u9700\u6c42\u3002\u5728\u5373\u65f6\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u51b7\u542f\u52a8\u6216\u9690\u79c1\u9650\u5236\uff0c\u6ca1\u6709\u5148\u524d\u7684\u7528\u6237\u4ea4\u4e92\u5386\u53f2\uff0cLLM\u9700\u8981\u8bc6\u522b\u672a\u77e5\u7684\u7528\u6237\u504f\u597d\uff0c\u901a\u8fc7\u63d0\u95ee\u7b56\u7565\u6027\u5730\u83b7\u53d6\u504f\u597d\u503c\uff0c\u7136\u540e\u76f8\u5e94\u5730\u8c03\u6574\u63a8\u7406\u8fc7\u7a0b\u548c\u54cd\u5e94\u3002", "method": "\u5f15\u5165PREFDISCO\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528\u5fc3\u7406\u5b66\u57fa\u7840\u7684\u89d2\u8272\u548c\u7a00\u758f\u504f\u597d\uff0c\u5c06\u9759\u6001\u57fa\u51c6\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\u3002\u521b\u5efa\u573a\u666f\uff0c\u5176\u4e2d\u76f8\u540c\u95ee\u9898\u9700\u8981\u4e0d\u540c\u7684\u63a8\u7406\u94fe\uff0c\u53d6\u51b3\u4e8e\u7528\u6237\u4e0a\u4e0b\u6587\uff0c\u56e0\u4e3a\u6700\u4f18\u89e3\u91ca\u65b9\u6cd5\u56e0\u4e2a\u4eba\u4e13\u4e1a\u77e5\u8bc6\u548c\u504f\u597d\u800c\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "result": "\u8bc4\u4f3021\u4e2a\u524d\u6cbf\u6a21\u578b\u572810\u4e2a\u4efb\u52a1\u4e0a\uff0c\u53d1\u73b029.0%\u7684\u6734\u7d20\u4e2a\u6027\u5316\u5c1d\u8bd5\u4ea7\u751f\u6bd4\u901a\u7528\u54cd\u5e94\u66f4\u5dee\u7684\u504f\u597d\u5bf9\u9f50\uff0c\u4f46\u901a\u7528\u54cd\u5e94\u4e5f\u65e0\u6cd5\u6709\u6548\u670d\u52a1\u4e2a\u4f53\u7528\u6237\u9700\u6c42\u3002", "conclusion": "\u4e2a\u6027\u5316\u63a8\u7406\u9700\u8981\u4e13\u95e8\u5f00\u53d1\uff0c\u800c\u4e0d\u662f\u81ea\u7136\u51fa\u73b0\u3002PREFDISCO\u5c06\u4e2a\u6027\u5316\u63a8\u7406\u786e\u7acb\u4e3a\u53ef\u8861\u91cf\u7684\u7814\u7a76\u524d\u6cbf\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524dLLM\u4ea4\u4e92\u80fd\u529b\u7684\u57fa\u672c\u9650\u5236\uff0c\u4e3a\u5f00\u53d1\u5728\u5173\u952e\u4e2a\u6027\u5316\u9886\u57df\uff08\u5982\u6559\u80b2\u3001\u533b\u7597\u548c\u6280\u672f\uff09\u9002\u5e94\u4e2a\u4f53\u7528\u6237\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.00476", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00476", "abs": "https://arxiv.org/abs/2510.00476", "authors": ["Arushi Sharma", "Vedant Pungliya", "Christopher J. Quinn", "Ali Jannesari"], "title": "Analyzing Latent Concepts in Code Language Models", "comment": null, "summary": "Interpreting the internal behavior of large language models trained on code\nremains a critical challenge, particularly for applications demanding trust,\ntransparency, and semantic robustness. We propose Code Concept Analysis\n(CoCoA): a global post-hoc interpretability framework that uncovers emergent\nlexical, syntactic, and semantic structures in a code language model's\nrepresentation space by clustering contextualized token embeddings into\nhuman-interpretable concept groups. We propose a hybrid annotation pipeline\nthat combines static analysis tool-based syntactic alignment with\nprompt-engineered large language models (LLMs), enabling scalable labeling of\nlatent concepts across abstraction levels. We analyse the distribution of\nconcepts across layers and across three finetuning tasks. Emergent concept\nclusters can help identify unexpected latent interactions and be used to\nidentify trends and biases within the model's learned representations. We\nfurther integrate LCA with local attribution methods to produce\nconcept-grounded explanations, improving the coherence and interpretability of\ntoken-level saliency. Empirical evaluations across multiple models and tasks\nshow that LCA discovers concepts that remain stable under semantic-preserving\nperturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve\npredictably with fine-tuning. In a user study, concept-augmented explanations\ndisambiguate token roles. In a user study on the programming-language\nclassification task, concept-augmented explanations disambiguated token roles\nand improved human-centric explainability by 37 percentage points compared with\ntoken-level attributions using Integrated Gradients.", "AI": {"tldr": "\u63d0\u51fa\u4e86Code Concept Analysis (CoCoA)\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u6807\u8bb0\u5d4c\u5165\u6765\u63ed\u793a\u8bcd\u6c47\u3001\u53e5\u6cd5\u548c\u8bed\u4e49\u7ed3\u6784\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u7ec4\uff0c\u5e76\u7ed3\u5408\u5c40\u90e8\u5f52\u56e0\u65b9\u6cd5\u751f\u6210\u6982\u5ff5\u57fa\u7840\u7684\u89e3\u91ca\u3002", "motivation": "\u89e3\u91ca\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u884c\u4e3a\u5bf9\u4e8e\u9700\u8981\u4fe1\u4efb\u3001\u900f\u660e\u5ea6\u548c\u8bed\u4e49\u9c81\u68d2\u6027\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u6807\u6ce8\u7ba1\u9053\u7ed3\u5408\u9759\u6001\u5206\u6790\u5de5\u5177\u548c\u63d0\u793a\u5de5\u7a0bLLM\uff0c\u5c06\u4e0a\u4e0b\u6587\u6807\u8bb0\u5d4c\u5165\u805a\u7c7b\u4e3a\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u7ec4\uff0c\u5e76\u4e0e\u5c40\u90e8\u5f52\u56e0\u65b9\u6cd5\u96c6\u6210\u3002", "result": "CoCoA\u53d1\u73b0\u7684\u6982\u5ff5\u5728\u8bed\u4e49\u4fdd\u7559\u6270\u52a8\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff08\u5e73\u5747CSI=0.288\uff09\uff0c\u5728\u7f16\u7a0b\u8bed\u8a00\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6982\u5ff5\u589e\u5f3a\u89e3\u91ca\u6bd4\u57fa\u4e8e\u79ef\u5206\u68af\u5ea6\u7684\u6807\u8bb0\u7ea7\u5f52\u56e0\u63d0\u9ad8\u4e8637\u4e2a\u767e\u5206\u70b9\u7684\u4eba\u7c7b\u4e2d\u5fc3\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CoCoA\u6846\u67b6\u80fd\u591f\u6709\u6548\u63ed\u793a\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7ed3\u6784\uff0c\u63d0\u4f9b\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u7684\u900f\u660e\u5ea6\u548c\u4eba\u7c7b\u7406\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.00078", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.00078", "abs": "https://arxiv.org/abs/2510.00078", "authors": ["Sicong Liu", "Weiye Wu", "Xiangrui Xu", "Teng Li", "Bowen Pang", "Bin Guo", "Zhiwen Yu"], "title": "Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey", "comment": null, "summary": "Foundation models have reshaped AI by unifying fragmented architectures into\nscalable backbones with multimodal reasoning and contextual adaptation. In\nparallel, the long-standing notion of AI agents, defined by the\nsensing-decision-action loop, is entering a new paradigm: with FMs as their\ncognitive core, agents transcend rule-based behaviors to achieve autonomy,\ngeneralization, and self-reflection. This dual shift is reinforced by\nreal-world demands such as autonomous driving, robotics, virtual assistants,\nand GUI agents, as well as ecosystem advances in embedded hardware, edge\ncomputing, mobile deployment platforms, and communication protocols that\ntogether enable large-scale deployment. Yet this convergence collides with\nreality: while applications demand long-term adaptability and real-time\ninteraction, mobile and edge deployments remain constrained by memory, energy,\nbandwidth, and latency. This creates a fundamental tension between the growing\ncomplexity of FMs and the limited resources of deployment environments. This\nsurvey provides the first systematic characterization of adaptive,\nresource-efficient agentic AI systems. We summarize enabling techniques into\nelastic inference, test-time adaptation, dynamic multimodal integration, and\nagentic AI applications, and identify open challenges in balancing\naccuracy-latency-communication trade-offs and sustaining robustness under\ndistribution shifts. We further highlight future opportunities in\nalgorithm-system co-design, cognitive adaptation, and collaborative edge\ndeployment. By mapping FM structures, cognition, and hardware resources, this\nwork establishes a unified perspective toward scalable, adaptive, and\nresource-efficient agentic AI. We believe this survey can help readers to\nunderstand the connections between enabling technologies while promoting\nfurther discussions on the fusion of agentic intelligence and intelligent\nagents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce8\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u81ea\u9002\u5e94\u548c\u9ad8\u6548\u90e8\u7f72\uff0c\u63d0\u51fa\u4e86\u5f39\u6027\u63a8\u7406\u3001\u6d4b\u8bd5\u65f6\u9002\u5e94\u7b49\u5173\u952e\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u7cbe\u5ea6-\u5ef6\u8fdf-\u901a\u4fe1\u7684\u6743\u8861\u6311\u6218\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u91cd\u5851\u4e86AI\u67b6\u6784\uff0c\u667a\u80fd\u4ee3\u7406\u8fdb\u5165\u65b0\u8303\u5f0f\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u5185\u5b58\u3001\u80fd\u8017\u3001\u5e26\u5bbd\u7b49\u8d44\u6e90\u9650\u5236\uff0c\u9700\u8981\u5728\u6a21\u578b\u590d\u6742\u5ea6\u548c\u90e8\u7f72\u73af\u5883\u8d44\u6e90\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5316\u8868\u5f81\u81ea\u9002\u5e94\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u4ee3\u7406AI\u7cfb\u7edf\uff0c\u603b\u7ed3\u5f39\u6027\u63a8\u7406\u3001\u6d4b\u8bd5\u65f6\u9002\u5e94\u3001\u52a8\u6001\u591a\u6a21\u6001\u96c6\u6210\u548c\u4ee3\u7406AI\u5e94\u7528\u7b49\u5173\u952e\u6280\u672f\u3002", "result": "\u5efa\u7acb\u4e86\u57fa\u7840\u6a21\u578b\u7ed3\u6784\u3001\u8ba4\u77e5\u80fd\u529b\u548c\u786c\u4ef6\u8d44\u6e90\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u4ee3\u7406AI\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7406\u89e3\u4ee3\u7406\u667a\u80fd\u4e0e\u667a\u80fd\u4ee3\u7406\u878d\u5408\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u3001\u8ba4\u77e5\u9002\u5e94\u548c\u534f\u4f5c\u8fb9\u7f18\u90e8\u7f72\u7684\u672a\u6765\u673a\u9047\u3002", "topic": "agent analysis"}}
{"id": "2510.00156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00156", "abs": "https://arxiv.org/abs/2510.00156", "authors": ["Songran Bai", "Bingzhe Wu", "Yiwei Zhang", "Chengke Wu", "Xiaolong Zheng", "Yaze Yuan", "Ke Wu", "Jianqiang Li"], "title": "AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery", "comment": null, "summary": "Financial fraud detection in real-world scenarios presents significant\nchallenges due to the subtlety and dispersion of evidence across complex,\nmulti-year financial disclosures. In this work, we introduce a novel\nmulti-agent reasoning framework AuditAgent, enhanced with auditing domain\nexpertise, for fine-grained evidence chain localization in financial fraud\ncases. Leveraging an expert-annotated dataset constructed from enforcement\ndocuments and financial reports released by the China Securities Regulatory\nCommission, our approach integrates subject-level risk priors, a hybrid\nretrieval strategy, and specialized agent modules to efficiently identify and\naggregate cross-report evidence. Extensive experiments demonstrate that our\nmethod substantially outperforms General-Purpose Agent paradigm in both recall\nand interpretability, establishing a new benchmark for automated, transparent\nfinancial forensics. Our results highlight the value of domain-specific\nreasoning and dataset construction for advancing robust financial fraud\ndetection in practical, real-world regulatory applications.", "AI": {"tldr": "\u63d0\u51faAuditAgent\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u5ba1\u8ba1\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u7528\u4e8e\u91d1\u878d\u6b3a\u8bc8\u6848\u4ef6\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8bc1\u636e\u94fe\u5b9a\u4f4d\uff0c\u663e\u8457\u4f18\u4e8e\u901a\u7528\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u573a\u666f\u4e2d\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u7531\u4e8e\u8bc1\u636e\u5728\u590d\u6742\u591a\u5e74\u8d22\u52a1\u62ab\u9732\u4e2d\u7684\u5fae\u5999\u6027\u548c\u5206\u6563\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc1\u636e\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u96c6\u6210\u4e3b\u4f53\u7ea7\u98ce\u9669\u5148\u9a8c\u3001\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u548c\u4e13\u7528\u667a\u80fd\u4f53\u6a21\u5757\uff0c\u6709\u6548\u8bc6\u522b\u548c\u805a\u5408\u8de8\u62a5\u544a\u8bc1\u636e\u3002", "result": "\u5728\u53ec\u56de\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5927\u5e45\u4f18\u4e8e\u901a\u7528\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u4e3a\u81ea\u52a8\u5316\u900f\u660e\u91d1\u878d\u53d6\u8bc1\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u63a8\u7406\u548c\u6570\u636e\u96c6\u6784\u5efa\u5bf9\u4e8e\u63a8\u8fdb\u5b9e\u9645\u76d1\u7ba1\u5e94\u7528\u4e2d\u7a33\u5065\u7684\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2510.00501", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00501", "abs": "https://arxiv.org/abs/2510.00501", "authors": ["Kaixin Wang", "Tianlin Li", "Xiaoyu Zhang", "Aishan Liu", "Xianglong Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "and Bin Shi"], "title": "CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling", "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly used in code\ngeneration tasks across a wide range of applications. However, their\nperformance is often inconsistent across different programming languages (PLs),\nwith low-resource PLs suffering the most due to limited training data. In this\npaper, we present CodeChemist, a novel and efficient framework for test-time\nscaling that enables functional knowledge transfer from high-resource to\nlow-resource PLs using generated test cases. CodeChemist first generates and\nexecutes code in high-resource PLs to create test cases that encapsulate\nfunctional knowledge. It then uses multi-temperature hedged sampling to\ngenerate code snippets in the low-resource PL and selects the best one based on\nthe pass rate of the test cases. Our extensive experiments show that\nCodeChemist outperforms existing test-time scaling approaches, boosting the\nperformance of code generation for low-resource PLs without requiring any model\nretraining.", "AI": {"tldr": "CodeChemist\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u5b9e\u73b0\u4ece\u9ad8\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u5230\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u7684\u529f\u80fd\u77e5\u8bc6\u8fc1\u79fb\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u95f4\u6027\u80fd\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u800c\u8868\u73b0\u8f83\u5dee\u7684\u60c5\u51b5\u3002", "method": "\u9996\u5148\u751f\u6210\u5e76\u6267\u884c\u9ad8\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\u6765\u521b\u5efa\u5305\u542b\u529f\u80fd\u77e5\u8bc6\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u7136\u540e\u4f7f\u7528\u591a\u6e29\u5ea6\u5bf9\u51b2\u91c7\u6837\u751f\u6210\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u6839\u636e\u6d4b\u8bd5\u7528\u4f8b\u901a\u8fc7\u7387\u9009\u62e9\u6700\u4f73\u4ee3\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCodeChemist\u4f18\u4e8e\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "conclusion": "CodeChemist\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u7528\u4f8b\u5b9e\u73b0\u8de8\u7f16\u7a0b\u8bed\u8a00\u7684\u529f\u80fd\u77e5\u8bc6\u4f20\u9012\uff0c\u4e3a\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2510.00591", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00591", "abs": "https://arxiv.org/abs/2510.00591", "authors": ["Liyi Cai", "Yijie Ren", "Yitong Zhang", "Jia Li"], "title": "AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation", "comment": null, "summary": "Software automation has long been a central goal of software engineering,\nstriving for software development that proceeds without human intervention.\nRecent efforts have leveraged Artificial Intelligence (AI) to advance software\nautomation with notable progress. However, current AI functions primarily as\nassistants to human developers, leaving software development still dependent on\nexplicit human intervention. This raises a fundamental question: Can AI move\nbeyond its role as an assistant to become a core component of software, thereby\nenabling genuine software automation? To investigate this vision, we introduce\nAI-Driven Self-Evolving Software, a new form of software that evolves\ncontinuously through direct interaction with users. We demonstrate the\nfeasibility of this idea with a lightweight prototype built on a multi-agent\narchitecture that autonomously interprets user requirements, generates and\nvalidates code, and integrates new functionalities. Case studies across\nmultiple representative scenarios show that the prototype can reliably\nconstruct and reuse functionality, providing early evidence that such software\nsystems can scale to more sophisticated applications and pave the way toward\ntruly automated software development. We make code and cases in this work\npublicly available at https://anonymous.4open.science/r/live-software.", "AI": {"tldr": "\u63d0\u51faAI\u9a71\u52a8\u7684\u81ea\u8fdb\u5316\u8f6f\u4ef6\u6982\u5ff5\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5b9e\u73b0\u8f6f\u4ef6\u6301\u7eed\u6f14\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u81ea\u4e3b\u89e3\u91ca\u7528\u6237\u9700\u6c42\u3001\u751f\u6210\u9a8c\u8bc1\u4ee3\u7801\u5e76\u96c6\u6210\u65b0\u529f\u80fd\u3002", "motivation": "\u5f53\u524dAI\u4e3b\u8981\u4f5c\u4e3a\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u52a9\u624b\uff0c\u8f6f\u4ef6\u5f00\u53d1\u4ecd\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u3002\u7814\u7a76\u76ee\u6807\u662f\u8ba9AI\u8d85\u8d8a\u52a9\u624b\u89d2\u8272\uff0c\u6210\u4e3a\u8f6f\u4ef6\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u8f6f\u4ef6\u81ea\u52a8\u5316\u3002", "method": "\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u67b6\u6784\u6784\u5efa\u8f7b\u91cf\u7ea7\u539f\u578b\uff0c\u80fd\u591f\u81ea\u4e3b\u89e3\u91ca\u7528\u6237\u9700\u6c42\u3001\u751f\u6210\u548c\u9a8c\u8bc1\u4ee3\u7801\u3001\u96c6\u6210\u65b0\u529f\u80fd\uff0c\u5b9e\u73b0\u8f6f\u4ef6\u7684\u6301\u7eed\u6f14\u5316\u3002", "result": "\u591a\u4e2a\u4ee3\u8868\u6027\u573a\u666f\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u539f\u578b\u80fd\u591f\u53ef\u9760\u5730\u6784\u5efa\u548c\u91cd\u7528\u529f\u80fd\uff0c\u4e3a\u66f4\u590d\u6742\u5e94\u7528\u7684\u6269\u5c55\u63d0\u4f9b\u4e86\u65e9\u671f\u8bc1\u636e\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u81ea\u8fdb\u5316\u8f6f\u4ef6\u7cfb\u7edf\u5177\u6709\u53ef\u884c\u6027\uff0c\u80fd\u591f\u4e3a\u771f\u6b63\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u94fa\u5e73\u9053\u8def\u3002", "topic": "swe application"}}
{"id": "2510.00229", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00229", "abs": "https://arxiv.org/abs/2510.00229", "authors": ["Rohan Kadekodi", "Zhan Jin", "Keisuke Kamahori", "Yile Gu", "Sean Khatiri", "Noah H. Bayindirli", "Sergey Gorbunov", "Baris Kasikci"], "title": "DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems", "comment": null, "summary": "The deployment of Large Language Models (LLMs) as agentic orchestrators has\nrevolutionized task automation, but the need for privacy-preserving,\ncost-effective solutions demands on-device inference capabilities. However,\nlocal LLMs consistently underperform compared to frontier models in tool\ncalling scenarios, struggling with both tool selection from large tool sets and\naccurate argument generation for complex parameter structures. We introduce a\nmethodology that disaggregates a tool-calling task into two distinct subtasks:\ntool selection and argument generation. We propose \"decoupled fine-tuning\", a\nnovel post-training approach that employs LoRA fine-tuning to create dedicated\nLoRA adapters for tool selection and tool-specific argument generation using\nseparate loss masking for each of the subtasks. Furthermore, we present\nDualTune, an inference framework that leverages the LoRA adapters created using\ndecoupled fine-tuning to perform efficient agent orchestration with the help of\nlocal models on end-user devices. DualTune decomposes the tool-call generation\nstep into tool selection and argument generation, and dynamically loads the\ncorresponding LoRA adapters to generate tool calls. Additionally, DualTune\nimplements hierarchical orchestration to restrict the number of tools required\nfor tool selection. Our experiments on the MCP-Bench benchmark demonstrate that\nthe Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool\ncalling accuracy of the base model by 46%, and outperforms other local\nreasoning, non-reasoning and fine-tuned models of similar size in all cases,\nand models that are 2x larger, in most cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u5fae\u8c03\u65b9\u6cd5\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u5206\u89e3\u4e3a\u5de5\u5177\u9009\u62e9\u548c\u53c2\u6570\u751f\u6210\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u4e13\u7528LoRA\u9002\u914d\u5668\u5206\u522b\u4f18\u5316\uff0c\u5728\u672c\u5730\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u8c03\u7528\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u7f16\u6392\u5668\u5728\u4efb\u52a1\u81ea\u52a8\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9690\u79c1\u4fdd\u62a4\u548c\u6210\u672c\u6548\u76ca\u9700\u6c42\u8981\u6c42\u672c\u5730\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\u672c\u5730LLM\u5728\u5de5\u5177\u8c03\u7528\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5927\u5de5\u5177\u96c6\u9009\u62e9\u548c\u590d\u6742\u53c2\u6570\u751f\u6210\u65b9\u9762\u3002", "method": "\u91c7\u7528\u89e3\u8026\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528LoRA\u5fae\u8c03\u521b\u5efa\u4e13\u7528\u9002\u914d\u5668\uff0c\u5206\u522b\u5904\u7406\u5de5\u5177\u9009\u62e9\u548c\u5de5\u5177\u7279\u5b9a\u53c2\u6570\u751f\u6210\uff0c\u4f7f\u7528\u5355\u72ec\u635f\u5931\u63a9\u7801\u3002\u5e76\u63d0\u51fa\u4e86DualTune\u63a8\u7406\u6846\u67b6\uff0c\u52a8\u6001\u52a0\u8f7d\u76f8\u5e94LoRA\u9002\u914d\u5668\u751f\u6210\u5de5\u5177\u8c03\u7528\u3002", "result": "\u5728MCP-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u89e3\u8026\u5fae\u8c03\u7684Qwen-2.5-7B\u6a21\u578b\u5c06\u57fa\u7840\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u9ad8\u4e8646%\uff0c\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u4f18\u4e8e\u76f8\u4f3c\u5927\u5c0f\u7684\u5176\u4ed6\u672c\u5730\u63a8\u7406\u3001\u975e\u63a8\u7406\u548c\u5fae\u8c03\u6a21\u578b\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e2\u500d\u5927\u5c0f\u7684\u6a21\u578b\u3002", "conclusion": "\u89e3\u8026\u5fae\u8c03\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u672c\u5730LLM\u5728\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u4e13\u7528\u9002\u914d\u5668\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "topic": "code agent"}}
{"id": "2510.00674", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00674", "abs": "https://arxiv.org/abs/2510.00674", "authors": ["Konstantinos Karakatsanis", "Georgios Alexopoulos", "Ioannis Karyotakis", "Foivos Timotheos Proestakis", "Evangelos Talos", "Panos Louridas", "Dimitris Mitropoulos"], "title": "PyTrim: A Practical Tool for Reducing Python Dependency Bloat", "comment": "Accepted at ASE 2025 (Tool Demonstration Track)", "summary": "Dependency bloat is a persistent challenge in Python projects, which\nincreases maintenance costs and security risks. While numerous tools exist for\ndetecting unused dependencies in Python, removing these dependencies across the\nsource code and configuration files of a project requires manual effort and\nexpertise.\n  To tackle this challenge we introduce PYTRIM, an end-to-end system to\nautomate this process. PYTRIM eliminates unused imports and package\ndeclarations across a variety of file types, including Python source and\nconfiguration files such as requirements.txt and setup.py. PYTRIM's modular\ndesign makes it agnostic to the source of dependency bloat information,\nenabling integration with any detection tool. Beyond its contribution when it\ncomes to automation, PYTRIM also incorporates a novel dynamic analysis\ncomponent that improves dependency detection recall.\n  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset\nof 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%\naccuracy in replicating human-made changes. To show its practical impact, we\nrun PYTRIM on 971 open-source packages, identifying and trimming bloated\ndependencies in 39 of them. For each case, we submit a corresponding pull\nrequest, 6 of which have already been accepted and merged. PYTRIM is available\nas an open-source project, encouraging community contributions and further\ndevelopment.\n  Video demonstration: https://youtu.be/LqTEdOUbJRI\n  Code repository: https://github.com/TrimTeam/PyTrim", "AI": {"tldr": "PYTRIM\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684Python\u4f9d\u8d56\u4fee\u526a\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u5e76\u79fb\u9664\u9879\u76ee\u4e2d\u672a\u4f7f\u7528\u7684\u4f9d\u8d56\u9879\uff0c\u652f\u6301\u591a\u79cd\u6587\u4ef6\u7c7b\u578b\uff0c\u5e76\u96c6\u6210\u4e86\u52a8\u6001\u5206\u6790\u7ec4\u4ef6\u63d0\u9ad8\u68c0\u6d4b\u53ec\u56de\u7387\u3002", "motivation": "Python\u9879\u76ee\u4e2d\u7684\u4f9d\u8d56\u81a8\u80c0\u95ee\u9898\u589e\u52a0\u4e86\u7ef4\u62a4\u6210\u672c\u548c\u5b89\u5168\u9690\u60a3\uff0c\u73b0\u6709\u5de5\u5177\u53ea\u80fd\u68c0\u6d4b\u672a\u4f7f\u7528\u7684\u4f9d\u8d56\uff0c\u4f46\u79fb\u9664\u8fd9\u4e9b\u4f9d\u8d56\u9700\u8981\u5927\u91cf\u4eba\u5de5\u64cd\u4f5c\u3002", "method": "PYTRIM\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u80fd\u591f\u81ea\u52a8\u79fb\u9664Python\u6e90\u4ee3\u7801\u548c\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u672a\u4f7f\u7528\u5bfc\u5165\u548c\u5305\u58f0\u660e\uff0c\u652f\u6301\u4e0e\u4efb\u4f55\u68c0\u6d4b\u5de5\u5177\u96c6\u6210\uff0c\u5e76\u5305\u542b\u52a8\u6001\u5206\u6790\u7ec4\u4ef6\u3002", "result": "\u572837\u4e2a\u771f\u5b9e\u5408\u5e76\u8bf7\u6c42\u7684\u6570\u636e\u96c6\u4e0a\uff0cPYTRIM\u5b9e\u73b0\u4e8698.3%\u7684\u51c6\u786e\u7387\uff1b\u5728971\u4e2a\u5f00\u6e90\u5305\u4e2d\uff0c\u6210\u529f\u8bc6\u522b\u5e76\u4fee\u526a\u4e8639\u4e2a\u5305\u7684\u81a8\u80c0\u4f9d\u8d56\uff0c\u5176\u4e2d6\u4e2a\u5df2\u88ab\u63a5\u53d7\u5408\u5e76\u3002", "conclusion": "PYTRIM\u6709\u6548\u89e3\u51b3\u4e86Python\u4f9d\u8d56\u4fee\u526a\u7684\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\u4fc3\u8fdb\u793e\u533a\u8d21\u732e\u3002", "topic": "swe application"}}
{"id": "2510.00144", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00144", "abs": "https://arxiv.org/abs/2510.00144", "authors": ["Shreyas Chaudhari", "Renhao Zhang", "Philip S. Thomas", "Bruno Castro da Silva"], "title": "Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback", "comment": null, "summary": "The ability of reinforcement learning algorithms to learn effective policies\nis determined by the rewards available during training. However, for practical\nproblems, obtaining large quantities of reward labels is often infeasible due\nto computational or financial constraints, particularly when relying on human\nfeedback. When reinforcement learning must proceed with limited feedback --\nonly a fraction of samples get rewards labeled -- a fundamental question\narises: which samples should be labeled to maximize policy performance? We\nformalize this problem of reward selection for reinforcement learning from\nlimited feedback (RLLF), introducing a new problem formulation that facilitates\nthe study of strategies for selecting impactful rewards. Two types of selection\nstrategies are investigated: (i) heuristics that rely on reward-free\ninformation such as state visitation and partial value functions, and (ii)\nstrategies pre-trained using auxiliary evaluative feedback. We find that\ncritical subsets of rewards are those that (1) guide the agent along optimal\ntrajectories, and (2) support recovery toward near-optimal behavior after\ndeviations. Effective selection methods yield near-optimal policies with\nsignificantly fewer reward labels than full supervision, establishing reward\nselection as a powerful paradigm for scaling reinforcement learning in\nfeedback-limited settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6709\u9650\u53cd\u9988\u4e0b\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u9009\u62e9\u95ee\u9898(RLLF)\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u9009\u62e9\u5173\u952e\u5956\u52b1\u6837\u672c\u6765\u6700\u5927\u5316\u7b56\u7565\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u7684\u5956\u52b1\u6807\u7b7e\u6570\u91cf\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u83b7\u53d6\u5927\u91cf\u5956\u52b1\u6807\u7b7e\u901a\u5e38\u4e0d\u53ef\u884c\uff0c\u7279\u522b\u662f\u5728\u4f9d\u8d56\u4eba\u7c7b\u53cd\u9988\u65f6\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5728\u6709\u9650\u53cd\u9988\u4e0b\u5982\u4f55\u9009\u62e9\u6700\u6709\u4ef7\u503c\u7684\u5956\u52b1\u6837\u672c\u6765\u8bad\u7ec3\u6709\u6548\u7b56\u7565\u3002", "method": "\u7814\u7a76\u4e86\u4e24\u79cd\u5956\u52b1\u9009\u62e9\u7b56\u7565\uff1a(i) \u57fa\u4e8e\u65e0\u5956\u52b1\u4fe1\u606f\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5982\u72b6\u6001\u8bbf\u95ee\u548c\u90e8\u5206\u4ef7\u503c\u51fd\u6570\uff09\uff0c(ii) \u4f7f\u7528\u8f85\u52a9\u8bc4\u4f30\u53cd\u9988\u9884\u8bad\u7ec3\u7684\u9009\u62e9\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u5173\u952e\u5956\u52b1\u5b50\u96c6\u662f\u90a3\u4e9b\u80fd\u591f\u5f15\u5bfc\u667a\u80fd\u4f53\u6cbf\u6700\u4f18\u8f68\u8ff9\u524d\u8fdb\u5e76\u5728\u504f\u79bb\u540e\u652f\u6301\u6062\u590d\u5230\u63a5\u8fd1\u6700\u4f18\u884c\u4e3a\u7684\u5956\u52b1\u3002\u6709\u6548\u9009\u62e9\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u5956\u52b1\u6807\u7b7e\u5c31\u80fd\u83b7\u5f97\u63a5\u8fd1\u6700\u4f18\u7684\u7b56\u7565\u3002", "conclusion": "\u5956\u52b1\u9009\u62e9\u662f\u5728\u53cd\u9988\u53d7\u9650\u73af\u5883\u4e2d\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u529b\u8303\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6240\u9700\u7684\u76d1\u7763\u4fe1\u606f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00274", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.00274", "abs": "https://arxiv.org/abs/2510.00274", "authors": ["Maisha Maliha", "Dean Hougen"], "title": "MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning", "comment": "16 pages, 3 figures", "summary": "Understanding the decision-making process of Deep Reinforcement Learning\nagents remains a key challenge for deploying these systems in safety-critical\nand multi-agent environments. While prior explainability methods like\nStateMask, have advanced the identification of critical states, they remain\nlimited by computational cost, exploration coverage, and lack of adaptation to\nmulti-agent settings. To overcome these limitations, we propose a\nmathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent\nCollaboration with Mask-Based Explainability for Reinforcement Learning), that\nextends perturbation-based explanation to Multi-Agent Reinforcement Learning.\nOur method integrates Proximal Policy Optimization, adaptive epsilon-greedy\nexploration, and lightweight inter-agent collaboration to share masked state\ninformation and peer experience. This collaboration enables each agent to\nperform saliency-guided masking and share reward-based insights with peers,\nreducing the time required for critical state discovery, improving explanation\nfidelity, and leading to faster and more robust learning. The core novelty of\nour approach lies in generalizing explainability from single-agent to\nmulti-agent systems through a unified mathematical formalism built on\ntrajectory perturbation, reward fidelity analysis, and Kullback-Leibler\ndivergence regularization. This framework yields localized, interpretable\nexplanations grounded in probabilistic modeling and multi-agent Markov decision\nprocesses. We validate our framework on both single-agent and multi-agent\nbenchmarks, including a multi-agent highway driving environment and Google\nResearch Football, demonstrating that MAGIC-MASK consistently outperforms\nstate-of-the-art baselines in fidelity, learning efficiency, and policy\nrobustness while offering interpretable and transferable explanations.", "AI": {"tldr": "\u63d0\u51fa\u4e86MAGIC-MASK\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u6270\u52a8\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u95f4\u534f\u4f5c\u5171\u4eab\u63a9\u7801\u72b6\u6001\u4fe1\u606f\u548c\u7ecf\u9a8c\uff0c\u63d0\u9ad8\u5173\u952e\u72b6\u6001\u53d1\u73b0\u6548\u7387\u548c\u89e3\u91ca\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u8fc7\u7a0b\u7406\u89e3\u5728\u5b89\u5168\u5173\u952e\u548c\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u63a2\u7d22\u8986\u76d6\u4e0d\u8db3\u548c\u7f3a\u4e4f\u591a\u667a\u80fd\u4f53\u9002\u5e94\u6027\u7b49\u9650\u5236\u3002", "method": "\u96c6\u6210\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u3001\u81ea\u9002\u5e94epsilon-greedy\u63a2\u7d22\u548c\u8f7b\u91cf\u7ea7\u667a\u80fd\u4f53\u95f4\u534f\u4f5c\uff0c\u8fdb\u884c\u663e\u8457\u6027\u5f15\u5bfc\u7684\u63a9\u7801\u5904\u7406\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u6d1e\u5bdf\u5171\u4eab\uff0c\u57fa\u4e8e\u8f68\u8ff9\u6270\u52a8\u3001\u5956\u52b1\u4fdd\u771f\u5ea6\u5206\u6790\u548cKL\u6563\u5ea6\u6b63\u5219\u5316\u7684\u7edf\u4e00\u6570\u5b66\u5f62\u5f0f\u5316\u3002", "result": "\u5728\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\uff0c\u5305\u62ec\u591a\u667a\u80fd\u4f53\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u73af\u5883\u548cGoogle Research Football\uff0c\u5728\u4fdd\u771f\u5ea6\u3001\u5b66\u4e60\u6548\u7387\u548c\u7b56\u7565\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MAGIC-MASK\u6846\u67b6\u6210\u529f\u5c06\u53ef\u89e3\u91ca\u6027\u4ece\u5355\u667a\u80fd\u4f53\u63a8\u5e7f\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u63d0\u4f9b\u57fa\u4e8e\u6982\u7387\u5efa\u6a21\u548c\u591a\u667a\u80fd\u4f53\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u5c40\u90e8\u5316\u3001\u53ef\u89e3\u91ca\u89e3\u91ca\u3002", "topic": "agent analysis"}}
{"id": "2510.00300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00300", "abs": "https://arxiv.org/abs/2510.00300", "authors": ["Serena Gomez Wannaz"], "title": "ICL Optimized Fragility", "comment": null, "summary": "ICL guides are known to improve task-specific performance, but their impact\non cross-domain cognitive abilities remains unexplored. This study examines how\nICL guides affect reasoning across different knowledge domains using six\nvariants of the GPT-OSS:20b model: one baseline model and five ICL\nconfigurations (simple, chain-of-thought, random, appended text, and symbolic\nlanguage). The models were subjected to 840 tests spanning general knowledge\nquestions, logic riddles, and a mathematical olympiad problem. Statistical\nanalysis (ANOVA) revealed significant behavioral modifications (p less than\n0.001) across ICL variants, demonstrating a phenomenon termed \"optimized\nfragility.\" ICL models achieved 91%-99% accuracy on general knowledge tasks\nwhile showing degraded performance on complex reasoning problems, with accuracy\ndropping to 10-43% on riddles compared to 43% for the baseline model. Notably,\nno significant differences emerged on the olympiad problem (p=0.2173),\nsuggesting that complex mathematical reasoning remains unaffected by ICL\noptimization. These findings indicate that ICL guides create systematic\ntrade-offs between efficiency and reasoning flexibility, with important\nimplications for LLM deployment and AI safety.", "AI": {"tldr": "ICL\u5f15\u5bfc\u867d\u7136\u80fd\u63d0\u5347\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8de8\u9886\u57df\u63a8\u7406\u80fd\u529b\u4e0b\u964d\uff0c\u51fa\u73b0\"\u4f18\u5316\u8106\u5f31\u6027\"\u73b0\u8c61\u3002", "motivation": "\u63a2\u7d22ICL\u5f15\u5bfc\u5bf9\u8de8\u9886\u57df\u8ba4\u77e5\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u53d8\u5316\u3002", "method": "\u4f7f\u7528GPT-OSS:20b\u6a21\u578b\u76846\u4e2a\u53d8\u4f53\uff081\u4e2a\u57fa\u7ebf+5\u79cdICL\u914d\u7f6e\uff09\uff0c\u5728840\u4e2a\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u6db5\u76d6\u5e38\u8bc6\u95ee\u9898\u3001\u903b\u8f91\u8c1c\u9898\u548c\u6570\u5b66\u5965\u8d5b\u9898\uff0c\u5e76\u8fdb\u884cANOVA\u7edf\u8ba1\u5206\u6790\u3002", "result": "ICL\u6a21\u578b\u5728\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8fbe\u523091%-99%\u51c6\u786e\u7387\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u4e0b\u964d\uff08\u8c1c\u9898\u51c6\u786e\u7387\u964d\u81f310-43%\uff0c\u57fa\u7ebf\u4e3a43%\uff09\uff0c\u6570\u5b66\u5965\u8d5b\u9898\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "ICL\u5f15\u5bfc\u5728\u6548\u7387\u548c\u63a8\u7406\u7075\u6d3b\u6027\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u6743\u8861\uff0c\u5bf9LLM\u90e8\u7f72\u548cAI\u5b89\u5168\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2510.00311", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00311", "abs": "https://arxiv.org/abs/2510.00311", "authors": ["Bowen Wei", "Yuan Shen Tay", "Howard Liu", "Jinhao Pan", "Kun Luo", "Ziwei Zhu", "Chris Jordan"], "title": "CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage", "comment": null, "summary": "Security Operations Centers (SOCs) are overwhelmed by tens of thousands of\ndaily alerts, with only a small fraction corresponding to genuine attacks. This\noverload creates alert fatigue, leading to overlooked threats and analyst\nburnout. Classical detection pipelines are brittle and context-poor, while\nrecent LLM-based approaches typically rely on a single model to interpret logs,\nretrieve context, and adjudicate alerts end-to-end -- an approach that\nstruggles with noisy enterprise data and offers limited transparency. We\npropose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in\nwhich specialized agents collaborate over real evidence: a behavior-analysis\nagent inspects activity sequences, evidence-gathering agents query external\nsystems, and a reasoning agent synthesizes findings into an auditable decision.\nTo support training and evaluation, we release a dataset of fine-grained SOC\ninvestigations from production environments, capturing step-by-step analyst\nactions and linked tool outputs. Across diverse enterprise scenarios, CORTEX\nsubstantially reduces false positives and improves investigation quality over\nstate-of-the-art single-agent LLMs.", "AI": {"tldr": "CORTEX\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\uff0c\u7528\u4e8e\u5b89\u5168\u8fd0\u8425\u4e2d\u5fc3\u7684\u9ad8\u98ce\u9669\u8b66\u62a5\u5206\u7c7b\uff0c\u901a\u8fc7\u4e13\u95e8\u667a\u80fd\u4f53\u534f\u4f5c\u5904\u7406\u771f\u5b9e\u8bc1\u636e\uff0c\u663e\u8457\u51cf\u5c11\u8bef\u62a5\u5e76\u63d0\u9ad8\u8c03\u67e5\u8d28\u91cf\u3002", "motivation": "\u5b89\u5168\u8fd0\u8425\u4e2d\u5fc3\u6bcf\u5929\u9762\u4e34\u6570\u4e07\u6761\u8b66\u62a5\uff0c\u53ea\u6709\u4e00\u5c0f\u90e8\u5206\u662f\u771f\u5b9e\u653b\u51fb\uff0c\u5bfc\u81f4\u8b66\u62a5\u75b2\u52b3\u548c\u5a01\u80c1\u9057\u6f0f\u3002\u4f20\u7edf\u68c0\u6d4b\u7ba1\u9053\u8106\u5f31\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\uff0c\u800c\u73b0\u6709\u7684\u5355\u6a21\u578bLLM\u65b9\u6cd5\u5728\u5904\u7406\u5608\u6742\u4f01\u4e1a\u6570\u636e\u548c\u63d0\u4f9b\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faCORTEX\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\uff0c\u5305\u542b\u4e13\u95e8\u667a\u80fd\u4f53\uff1a\u884c\u4e3a\u5206\u6790\u667a\u80fd\u4f53\u68c0\u67e5\u6d3b\u52a8\u5e8f\u5217\uff0c\u8bc1\u636e\u6536\u96c6\u667a\u80fd\u4f53\u67e5\u8be2\u5916\u90e8\u7cfb\u7edf\uff0c\u63a8\u7406\u667a\u80fd\u4f53\u5c06\u53d1\u73b0\u7efc\u5408\u6210\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u3002", "result": "\u5728\u591a\u6837\u5316\u4f01\u4e1a\u573a\u666f\u4e2d\uff0cCORTEX\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5355\u667a\u80fd\u4f53LLM\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bef\u62a5\u5e76\u63d0\u9ad8\u4e86\u8c03\u67e5\u8d28\u91cf\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\u5728\u5b89\u5168\u8b66\u62a5\u5206\u7c7b\u4e2d\u6bd4\u5355\u6a21\u578b\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u900f\u660e\u5ea6\u548c\u5904\u7406\u590d\u6742\u4f01\u4e1a\u6570\u636e\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.00881", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00881", "abs": "https://arxiv.org/abs/2510.00881", "authors": ["Patrizio Migliarini", "Mashal Afzal Memon", "Marco Autili", "Paola Inverardi"], "title": "Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning", "comment": "Accepted at ASE 2025", "summary": "Large Language Models (LLMs) are increasingly integrated into software\nengineering (SE) tools for tasks that extend beyond code synthesis, including\njudgment under uncertainty and reasoning in ethically significant contexts. We\npresent a fully automated framework for assessing ethical reasoning\ncapabilities across 16 LLMs in a zero-shot setting, using 30 real-world\nethically charged scenarios. Each model is prompted to identify the most\napplicable ethical theory to an action, assess its moral acceptability, and\nexplain the reasoning behind their choice. Responses are compared against\nexpert ethicists' choices using inter-model agreement metrics. Our results show\nthat LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary\nAgreement Rate (BAR) on moral acceptability of 86.7%, with interpretable\ndivergences concentrated in ethically ambiguous cases. A qualitative analysis\nof free-text explanations reveals strong conceptual convergence across models\ndespite surface-level lexical diversity. These findings support the potential\nviability of LLMs as ethical inference engines within SE pipelines, enabling\nscalable, auditable, and adaptive integration of user-aligned ethical\nreasoning. Our focus is the Ethical Interpreter component of a broader\nprofiling pipeline: we evaluate whether current LLMs exhibit sufficient\ninterpretive stability and theory-consistent reasoning to support automated\nprofiling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f3016\u4e2aLLM\u572830\u4e2a\u73b0\u5b9e\u4e16\u754c\u4f26\u7406\u573a\u666f\u4e2d\u7684\u4f26\u7406\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793aLLM\u5728\u7406\u8bba\u4e00\u81f4\u6027\u548c\u9053\u5fb7\u53ef\u63a5\u53d7\u6027\u5224\u65ad\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u4e2d\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u4f26\u7406\u63a8\u7406\u7b49\u8d85\u8d8a\u4ee3\u7801\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u5728SE\u6d41\u7a0b\u4e2d\u53ef\u6269\u5c55\u3001\u53ef\u5ba1\u8ba1\u7684\u7528\u6237\u5bf9\u9f50\u4f26\u7406\u63a8\u7406\u3002", "method": "\u4f7f\u752830\u4e2a\u771f\u5b9e\u4f26\u7406\u573a\u666f\uff0c\u8ba916\u4e2aLLM\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc6\u522b\u6700\u9002\u7528\u7684\u4f26\u7406\u7406\u8bba\u3001\u8bc4\u4f30\u9053\u5fb7\u53ef\u63a5\u53d7\u6027\u5e76\u63d0\u4f9b\u63a8\u7406\u89e3\u91ca\uff0c\u7136\u540e\u4e0e\u4e13\u5bb6\u4f26\u7406\u5b66\u5bb6\u9009\u62e9\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLM\u5e73\u5747\u7406\u8bba\u4e00\u81f4\u6027\u7387\u4e3a73.3%\uff0c\u9053\u5fb7\u53ef\u63a5\u53d7\u6027\u4e8c\u5143\u4e00\u81f4\u7387\u4e3a86.7%\uff0c\u5728\u4f26\u7406\u6a21\u7cca\u6848\u4f8b\u4e2d\u5b58\u5728\u53ef\u89e3\u91ca\u7684\u5206\u6b67\uff0c\u5b9a\u6027\u5206\u6790\u663e\u793a\u6a21\u578b\u95f4\u5b58\u5728\u5f3a\u6982\u5ff5\u6536\u655b\u3002", "conclusion": "LLM\u4f5c\u4e3aSE\u6d41\u7a0b\u4e2d\u7684\u4f26\u7406\u63a8\u7406\u5f15\u64ce\u5177\u6709\u6f5c\u5728\u53ef\u884c\u6027\uff0c\u80fd\u591f\u652f\u6301\u81ea\u52a8\u5316\u7684\u7528\u6237\u5bf9\u9f50\u4f26\u7406\u63a8\u7406\u96c6\u6210\u3002", "topic": "agent analysis"}}
{"id": "2510.00446", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00446", "abs": "https://arxiv.org/abs/2510.00446", "authors": ["Yuling Shi", "Yichun Qian", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "title": "LongCodeZip: Compress Long Context for Code Language Models", "comment": "Accepted to ASE 2025. Code available at\n  https://github.com/YerbaPage/LongCodeZip", "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications.", "AI": {"tldr": "LongCodeZip\u662f\u4e00\u4e2a\u4e13\u4e3a\u4ee3\u7801LLM\u8bbe\u8ba1\u7684\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u538b\u7f29\u7b56\u7565\uff08\u7c97\u7c92\u5ea6\u51fd\u6570\u7ea7\u538b\u7f29\u548c\u7ec6\u7c92\u5ea6\u5757\u7ea7\u538b\u7f29\uff09\u6709\u6548\u51cf\u5c11\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe5.6\u500d\u7684\u538b\u7f29\u6bd4\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u526a\u679d\u6280\u672f\u5982LLMLingua\u5728\u901a\u7528\u6587\u672c\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5ffd\u7565\u4e86\u4ee3\u7801\u7279\u6709\u7684\u7ed3\u6784\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0d\u4f73\u3002\u9ad8API\u6210\u672c\u548c\u751f\u6210\u5ef6\u8fdf\u662f\u4ee3\u7801LLM\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u538b\u7f29\u7b56\u7565\uff1a1\uff09\u7c97\u7c92\u5ea6\u538b\u7f29\u57fa\u4e8e\u6761\u4ef6\u56f0\u60d1\u5ea6\u8bc6\u522b\u548c\u6392\u5e8f\u51fd\u6570\u7ea7\u5757\uff0c\u4fdd\u7559\u6700\u76f8\u5173\u51fd\u6570\uff1b2\uff09\u7ec6\u7c92\u5ea6\u538b\u7f29\u5c06\u4fdd\u7559\u51fd\u6570\u6309\u56f0\u60d1\u5ea6\u5206\u6bb5\uff0c\u5728\u81ea\u9002\u5e94token\u9884\u7b97\u4e0b\u9009\u62e9\u6700\u4f18\u5b50\u96c6\u4ee5\u6700\u5927\u5316\u76f8\u5173\u6027\u3002", "result": "\u5728\u4ee3\u7801\u8865\u5168\u3001\u6458\u8981\u548c\u95ee\u7b54\u7b49\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cLongCodeZip\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4e0d\u964d\u4f4e\u4efb\u52a1\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u8fbe5.6\u500d\u7684\u538b\u7f29\u6bd4\u3002", "conclusion": "LongCodeZip\u901a\u8fc7\u6709\u6548\u51cf\u5c11\u4e0a\u4e0b\u6587\u5927\u5c0f\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff0c\u4f7fLLM\u80fd\u591f\u66f4\u597d\u5730\u6269\u5c55\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u5927\u89c4\u6a21\u4ee3\u7801\u573a\u666f\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u667a\u80fd\u5e94\u7528\u7684\u6548\u7387\u548c\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2510.00194", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00194", "abs": "https://arxiv.org/abs/2510.00194", "authors": ["Prasanna Parthasarathi", "Mathieu Reymond", "Boxing Chen", "Yufei Cui", "Sarath Chandar"], "title": "GRPO-$\u03bb$: Credit Assignment improves LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed for tasks requiring\ncomplex reasoning, prompting significant interest in improving their reasoning\nabilities through post-training. Especially RL based methods using verifiable\nreward, like the state-of-the-art GRPO, have shown to tremendously improve\nreasoning behaviors when applied as post-training methods. However, the lack of\nan explicit reward or critic model limits GRPO's ability to assign fine-grained\ncredit across token sequences. In this work, we present GRPO-$\\lambda$, a novel\nextension to GRPO that enhances credit assignment in RL finetuning of LLMs for\ncomplex reasoning tasks. We approximate learning from $\\lambda$-return with a\nreformulation of eligibility traces using token-level log-probabilities applied\nafter each sequence generation, and a novel critic-free approximation of the\ntemporal-difference error. We introduce a few variations for the weighting of\nthe $\\lambda$-return, and their applications to the eligibility-trace, where\nall the variations provide significant gains over GRPO. We compare\nGRPO-$\\lambda$ against GRPO by training models from 1.5B to 7B parameters on\n$4$ different math reasoning datasets. The training plots demonstrate 30-40%\nimproved performance during RL training on both LLaMA-3.1 and Qwen-2.5\narchitectures. Finally, we show that with GRPO-$\\lambda$, the resulting average\nperformance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves\nover GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.", "AI": {"tldr": "GRPO-\u03bb\u662fGRPO\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u03bb-return\u548c\u8d44\u683c\u8ff9\u6765\u6539\u8fdbLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u63d0\u4f9b\u66f4\u7cbe\u7ec6\u7684\u4fe1\u7528\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u7684GRPO\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u5956\u52b1\u6216\u8bc4\u8bba\u5bb6\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728token\u5e8f\u5217\u4e2d\u5206\u914d\u7ec6\u7c92\u5ea6\u4fe1\u7528\u7684\u80fd\u529b\uff0c\u9700\u8981\u6539\u8fdb\u4fe1\u7528\u5206\u914d\u673a\u5236\u3002", "method": "\u4f7f\u7528token\u7ea7\u5bf9\u6570\u6982\u7387\u91cd\u65b0\u8868\u8ff0\u8d44\u683c\u8ff9\uff0c\u5f15\u5165\u65e0\u8bc4\u8bba\u5bb6\u7684\u65f6\u5e8f\u5dee\u5206\u8bef\u5dee\u8fd1\u4f3c\uff0c\u5e76\u63d0\u51fa\u4e86\u51e0\u79cd\u03bb-return\u52a0\u6743\u53d8\u4f53\u3002", "result": "\u57281.5B\u52307B\u53c2\u6570\u7684\u6a21\u578b\u4e0a\uff0c\u76f8\u6bd4GRPO\u5728RL\u8bad\u7ec3\u671f\u95f4\u6027\u80fd\u63d0\u534730-40%\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u5347\u8d85\u8fc73\u5206\uff0c7B\u6a21\u578b\u63d0\u53474.5\u5206\u3002", "conclusion": "GRPO-\u03bb\u901a\u8fc7\u6539\u8fdb\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00920", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00920", "abs": "https://arxiv.org/abs/2510.00920", "authors": ["Songqiang Chen", "Congying Xu", "Jingyi Chen", "Jialun Cao", "Jiarong Wu", "Shing-Chi Cheung"], "title": "On Effective Semantic Translation for Code: A Study Based on Pseudocode", "comment": null, "summary": "Large language models (LLMs) show great potential in code translation.\nHowever, accurate translation remains challenging when using the commonly\nadopted direct code-to-code translation approach, which converts a program into\nthe target programming language (PL) in a single step. Inspired by the success\nof incorporating intermediate steps to guide LLMs in resolving challenging\ntasks, we explore pseudocode-based code translation, which emulates the human\nsemantic translation by first interpreting the program's intent and logic into\npseudocode and then implementing it in the target PL. We find that\npseudocode-based translation helps translate programs that direct translation\nstruggles to handle. Nonetheless, the effectiveness, advantages, and\nlimitations of this approach remain underexplored. To bridge this gap, we\npresent an empirical study on pseudocode-based code translation, aiming to\ninvestigate its effectiveness in enhancing the direct translation approach,\nilluminate its effective usage, and identify limitations hindering its\npotential benefits. By comparing direct and pseudocode-based translation\napproaches on 9,690 translation tasks across six PLs with five popular LLMs, we\ndemonstrate that pseudocode-based translation can effectively complement direct\ntranslation, particularly when translating from flexible to rigid PLs or\ndealing with low-resource Rust. Based on these findings, we suggest adopting\nstrategies that combine the complementary strengths of both approaches to\nenhance code translation accuracy. We also reveal the advantages of\npseudocode-based translation in disentangling translations of complicated\nprograms and mitigating distractions from detailed implementations in original\nprograms, as well as its limitations due to incorrect, incomplete, or ambiguous\npseudocode.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u4f2a\u4ee3\u7801\u7684\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7a0b\u5e8f\u5148\u8f6c\u6362\u4e3a\u4f2a\u4ee3\u7801\u518d\u7ffb\u8bd1\u5230\u76ee\u6807\u8bed\u8a00\uff0c\u76f8\u6bd4\u76f4\u63a5\u7ffb\u8bd1\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u5904\u7406\u590d\u6742\u7a0b\u5e8f\u7ffb\u8bd1\uff0c\u7279\u522b\u662f\u5728\u4ece\u7075\u6d3b\u8bed\u8a00\u5230\u4e25\u683c\u8bed\u8a00\u7684\u7ffb\u8bd1\u573a\u666f\u4e2d\u3002", "motivation": "\u76f4\u63a5\u4ee3\u7801\u5230\u4ee3\u7801\u7684\u7ffb\u8bd1\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7a0b\u5e8f\u7ffb\u8bd1\u65f6\u5b58\u5728\u6311\u6218\uff0c\u53d7\u4eba\u7c7b\u8bed\u4e49\u7ffb\u8bd1\u8fc7\u7a0b\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u901a\u8fc7\u4e2d\u95f4\u4f2a\u4ee3\u7801\u6b65\u9aa4\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4f2a\u4ee3\u7801\u7684\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u9996\u5148\u5c06\u7a0b\u5e8f\u89e3\u91ca\u4e3a\u4f2a\u4ee3\u7801\u8868\u793a\u610f\u56fe\u548c\u903b\u8f91\uff0c\u7136\u540e\u518d\u5b9e\u73b0\u4e3a\u76ee\u6807\u7f16\u7a0b\u8bed\u8a00\u3002\u901a\u8fc7\u5bf99,690\u4e2a\u7ffb\u8bd1\u4efb\u52a1\u30016\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c5\u4e2a\u6d41\u884cLLM\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u76f4\u63a5\u7ffb\u8bd1\u548c\u4f2a\u4ee3\u7801\u7ffb\u8bd1\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u4f2a\u4ee3\u7801\u7684\u7ffb\u8bd1\u80fd\u6709\u6548\u8865\u5145\u76f4\u63a5\u7ffb\u8bd1\uff0c\u7279\u522b\u662f\u5728\u4ece\u7075\u6d3b\u8bed\u8a00\u5230\u4e25\u683c\u8bed\u8a00\u7684\u7ffb\u8bd1\u4ee5\u53ca\u5904\u7406\u4f4e\u8d44\u6e90Rust\u8bed\u8a00\u65f6\u8868\u73b0\u66f4\u4f73\u3002\u4f2a\u4ee3\u7801\u7ffb\u8bd1\u6709\u52a9\u4e8e\u89e3\u8026\u590d\u6742\u7a0b\u5e8f\u7684\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u51cf\u5c11\u539f\u59cb\u7a0b\u5e8f\u7ec6\u8282\u5b9e\u73b0\u7684\u5e72\u6270\u3002", "conclusion": "\u5efa\u8bae\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\u7684\u4e92\u8865\u4f18\u52bf\u6765\u63d0\u5347\u4ee3\u7801\u7ffb\u8bd1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u57fa\u4e8e\u4f2a\u4ee3\u7801\u7ffb\u8bd1\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4f2a\u4ee3\u7801\u672c\u8eab\u53ef\u80fd\u4e0d\u6b63\u786e\u3001\u4e0d\u5b8c\u6574\u6216\u5b58\u5728\u6b67\u4e49\u7684\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2510.00946", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00946", "abs": "https://arxiv.org/abs/2510.00946", "authors": ["Shiza Andleeb", "Brandon Kantorski", "Jeffrey C. Carver"], "title": "ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions", "comment": "Accepted to SIGCITE'25", "summary": "Background: Large language models (LLMs) such as ChatGPT are increasingly\nused in introductory programming courses to provide real-time code generation,\ndebugging, and explanations. While these tools can boost productivity and code\nquality, concerns remain about over-reliance and potential impacts on\nconceptual learning. Objective: To investigate how ChatGPT access affects code\nquality, conceptual understanding, task completion times, and student\nperceptions in a CS1 course. Methods: We conducted a counterbalanced,\nquasi-experimental study in which students alternated between ChatGPT and\nnon-ChatGPT conditions across two programming assignments in C (functions and\nstructures). We evaluated their code submissions using multidimensional\nrubrics, conceptual post-surveys, and task completion time. Results: Students\nwho had access to ChatGPT produced significantly higher rubric scores for code\nquality and completed tasks in less time compared to those without access.\nHowever, gains in conceptual understanding were mixed, lower for the functions\ntopic but higher for the structures topic. Students reported positive\nexperiences with ChatGPT, citing its value for debugging and practice, while\nexpressing concerns about accuracy and long-term skill development.\nConclusions: ChatGPT can enhance code quality and efficiency for novice\nprogrammers, but may not uniformly improve conceptual understanding. Structured\nintegration and complementary instructional strategies are recommended to\nfoster independent problem-solving skills.", "AI": {"tldr": "\u7814\u7a76ChatGPT\u5bf9CS1\u8bfe\u7a0b\u4e2d\u5b66\u751f\u4ee3\u7801\u8d28\u91cf\u3001\u6982\u5ff5\u7406\u89e3\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u53d1\u73b0ChatGPT\u80fd\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4f46\u5bf9\u6982\u5ff5\u7406\u89e3\u7684\u5f71\u54cd\u4e0d\u4e00\u81f4\u3002", "motivation": "\u968f\u7740ChatGPT\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4e86\u89e3\u5176\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u7279\u522b\u662f\u4ee3\u7801\u8d28\u91cf\u3001\u6982\u5ff5\u7406\u89e3\u548c\u5b66\u4e60\u6548\u7387\u7b49\u65b9\u9762\u3002", "method": "\u91c7\u7528\u5e73\u8861\u8bbe\u8ba1\u7684\u51c6\u5b9e\u9a8c\u7814\u7a76\uff0c\u5b66\u751f\u5728\u4e24\u4e2aC\u8bed\u8a00\u7f16\u7a0b\u4f5c\u4e1a\uff08\u51fd\u6570\u548c\u7ed3\u6784\u4f53\uff09\u4e2d\u4ea4\u66ff\u4f7f\u7528ChatGPT\u548c\u975eChatGPT\u6761\u4ef6\uff0c\u901a\u8fc7\u591a\u7ef4\u8bc4\u5206\u6807\u51c6\u3001\u6982\u5ff5\u540e\u6d4b\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4f7f\u7528ChatGPT\u7684\u5b66\u751f\u4ee3\u7801\u8d28\u91cf\u8bc4\u5206\u663e\u8457\u66f4\u9ad8\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u66f4\u77ed\uff1b\u6982\u5ff5\u7406\u89e3\u65b9\u9762\uff0c\u51fd\u6570\u4e3b\u9898\u5f97\u5206\u8f83\u4f4e\uff0c\u7ed3\u6784\u4f53\u4e3b\u9898\u5f97\u5206\u8f83\u9ad8\uff1b\u5b66\u751f\u5bf9ChatGPT\u4f53\u9a8c\u79ef\u6781\uff0c\u4f46\u62c5\u5fc3\u51c6\u786e\u6027\u548c\u957f\u671f\u6280\u80fd\u53d1\u5c55\u3002", "conclusion": "ChatGPT\u80fd\u63d0\u5347\u65b0\u624b\u7a0b\u5e8f\u5458\u7684\u4ee3\u7801\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4f46\u672a\u5fc5\u80fd\u4e00\u81f4\u6539\u5584\u6982\u5ff5\u7406\u89e3\uff0c\u5efa\u8bae\u7ed3\u6784\u5316\u6574\u5408\u548c\u8865\u5145\u6559\u5b66\u7b56\u7565\u4ee5\u57f9\u517b\u72ec\u7acb\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "2510.00482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00482", "abs": "https://arxiv.org/abs/2510.00482", "authors": ["Yawen Xue", "Masaya Tsunokake", "Yuta Koreeda", "Ekant Muljibhai Amin", "Takashi Sumiyoshi", "Yasuhiro Sogawa"], "title": "Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains", "comment": "Accepted by AIxB 2025", "summary": "Agentic large language models (LLMs) have become prominent for autonomously\ninteracting with external environments and performing multi-step reasoning\ntasks. Most approaches leverage these capabilities via in-context learning with\nfew-shot prompts, but this often results in lengthy inputs and higher\ncomputational costs. Agent fine-tuning offers an alternative by enabling LLMs\nto internalize procedural reasoning and domain-specific knowledge through\ntraining on relevant data and demonstration trajectories. While prior studies\nhave focused on general domains, their effectiveness in specialized technical\nmicrodomains remains unclear. This paper explores agent fine-tuning for domain\nadaptation within Hitachi's JP1 middleware, a microdomain for specialized IT\noperations. We fine-tuned LLMs using JP1-specific datasets derived from domain\nmanuals and distilled reasoning trajectories generated by LLMs themselves,\nenhancing decision making accuracy and search efficiency. During inference, we\nused an agentic prompt with retrieval-augmented generation and introduced a\ncontext-answer extractor to improve information relevance. On JP1 certification\nexam questions, our method achieved a 14% performance improvement over the base\nmodel, demonstrating the potential of agent fine-tuning for domain-specific\nreasoning in complex microdomains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u65e5\u7acbJP1\u4e2d\u95f4\u4ef6\u8fd9\u4e00\u4e13\u4e1aIT\u8fd0\u7ef4\u5fae\u9886\u57df\u4e2d\uff0c\u901a\u8fc7\u4ee3\u7406\u5fae\u8c03\u65b9\u6cd5\u63d0\u5347LLMs\u7684\u9886\u57df\u9002\u5e94\u80fd\u529b\uff0c\u5728JP1\u8ba4\u8bc1\u8003\u8bd5\u4e2d\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e8614%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406LLMs\u4e3b\u8981\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u591a\u6b65\u63a8\u7406\uff0c\u4f46\u5b58\u5728\u8f93\u5165\u5197\u957f\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u4ee3\u7406\u5fae\u8c03\u80fd\u8ba9LLMs\u901a\u8fc7\u8bad\u7ec3\u5185\u5316\u7a0b\u5e8f\u6027\u63a8\u7406\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u4f46\u5728\u4e13\u4e1a\u5fae\u9886\u57df\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528JP1\u7279\u5b9a\u6570\u636e\u96c6\uff08\u6765\u81ea\u9886\u57df\u624b\u518c\u548cLLM\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\uff09\u5bf9LLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u63a8\u7406\u65f6\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4e0a\u4e0b\u6587-\u7b54\u6848\u63d0\u53d6\u5668\u6765\u63d0\u5347\u4fe1\u606f\u76f8\u5173\u6027\u3002", "result": "\u5728JP1\u8ba4\u8bc1\u8003\u8bd5\u95ee\u9898\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e8614%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u6027\u548c\u641c\u7d22\u6548\u7387\u3002", "conclusion": "\u4ee3\u7406\u5fae\u8c03\u5728\u590d\u6742\u5fae\u9886\u57df\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u9886\u57df\u7279\u5b9a\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.00415", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00415", "abs": "https://arxiv.org/abs/2510.00415", "authors": ["Dadi Guo", "Tianyi Zhou", "Dongrui Liu", "Chen Qian", "Qihan Ren", "Shuai Shao", "Zhiyuan Fan", "Yi R. Fung", "Kun Wang", "Linfeng Zhang", "Jing Shao"], "title": "Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm", "comment": "his is a work in progress due to methodology refinement and further\n  evaluation", "summary": "Recent advances in large language models (LLMs) and agent system designs have\nempowered agents with unprecedented levels of capability. However, existing\nagent benchmarks are showing a trend of rapid ceiling-hitting by newly\ndeveloped agents, making it difficult to meet the demands for evaluating agent\nabilities. To address this problem, we propose the Trajectory-based\nValidated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE)\nframework. This framework takes an original task from an existing benchmark and\nencourages agents to freely explore and evolve it into a new task with higher\ndifficulty while recording validatable agent trajectories. The framework\nproceeds in three stages: (1) evolutionary proposal mining, which provides task\nevolution proposals through preliminary exploration and divergent thinking; (2)\nproblem formation and free exploration, where proposals are conceptualized into\nfeasible problem candidates and the agents then explore them freely while\nrecording their execution trajectories; and (3) multi-level validation, which\nensures that the evolved tasks are accompanied by validatable and reproducible\ntrajectories. Experiments on the GAIA benchmark demonstrate that the TRACE\nframework consistently enhances task complexity while improving the reliability\nof correctness through validatable execution trajectories. This work marks a\nparadigm shift from static, manually curated benchmarks to dynamic,\nself-evolving evaluation systems, providing a sustainable and challenging\nrunway for agent development.", "AI": {"tldr": "\u63d0\u51fa\u4e86TRACE\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\u81ea\u7531\u63a2\u7d22\u5e76\u5c06\u73b0\u6709\u57fa\u51c6\u4efb\u52a1\u6f14\u5316\u4e3a\u66f4\u9ad8\u96be\u5ea6\u7684\u65b0\u4efb\u52a1\uff0c\u540c\u65f6\u8bb0\u5f55\u53ef\u9a8c\u8bc1\u7684\u6267\u884c\u8f68\u8ff9\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u5feb\u901f\u8fbe\u5230\u6027\u80fd\u4e0a\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u9762\u4e34\u65b0\u5f00\u53d1\u7684\u667a\u80fd\u4f53\u5feb\u901f\u8fbe\u5230\u6027\u80fd\u4e0a\u9650\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u8bc4\u4f30\u667a\u80fd\u4f53\u80fd\u529b\u7684\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u63d0\u5347\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "TRACE\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\u8fdb\u5316\u63d0\u8bae\u6316\u6398\uff08\u901a\u8fc7\u521d\u6b65\u63a2\u7d22\u548c\u53d1\u6563\u601d\u7ef4\u63d0\u4f9b\u4efb\u52a1\u8fdb\u5316\u63d0\u8bae\uff09\u3001\u95ee\u9898\u5f62\u6210\u4e0e\u81ea\u7531\u63a2\u7d22\uff08\u5c06\u63d0\u8bae\u6982\u5ff5\u5316\u4e3a\u53ef\u884c\u95ee\u9898\u5019\u9009\uff0c\u667a\u80fd\u4f53\u81ea\u7531\u63a2\u7d22\u5e76\u8bb0\u5f55\u6267\u884c\u8f68\u8ff9\uff09\u3001\u591a\u7ea7\u9a8c\u8bc1\uff08\u786e\u4fdd\u8fdb\u5316\u4efb\u52a1\u5177\u6709\u53ef\u9a8c\u8bc1\u548c\u53ef\u590d\u73b0\u7684\u8f68\u8ff9\uff09\u3002", "result": "\u5728GAIA\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTRACE\u6846\u67b6\u80fd\u591f\u6301\u7eed\u589e\u5f3a\u4efb\u52a1\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u6267\u884c\u8f68\u8ff9\u63d0\u9ad8\u6b63\u786e\u6027\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740\u4ece\u9759\u6001\u3001\u4eba\u5de5\u7b56\u5212\u7684\u57fa\u51c6\u6d4b\u8bd5\u5411\u52a8\u6001\u3001\u81ea\u6211\u8fdb\u5316\u7684\u8bc4\u4f30\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u667a\u80fd\u4f53\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u8dd1\u9053\u3002", "topic": "agent analysis"}}
{"id": "2510.00957", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00957", "abs": "https://arxiv.org/abs/2510.00957", "authors": ["Shiza Andleeb", "Teo Mendoza", "Lucas Cordova", "Gursimran Walia", "Jeffrey C. Carver"], "title": "Enhancing Software Testing Education: Understanding Where Students Struggle", "comment": "Accepted to SIGCITE'25", "summary": "Effective software testing is critical for producing reliable and secure\nsoftware, yet many computer science students struggle to master the\nfoundational concepts required to construct comprehensive test suites. While\nautomated feedback tools are widely used to support student learning, it\nremains unclear which testing concepts are most frequently misunderstood and\nhow these misunderstandings are reflected in students' test suite revisions.\nThis study examines the specific testing concepts that lead students to make\nineffective changes, those that fail to improve code coverage, during test\nsuite development. Leveraging an automated feedback tool in a senior-level\nsoftware testing course, we analyzed student submissions from two assignments\nto identify prevalent conceptual gaps and patterns of unproductive\nmodification. Our results reveal that decision coverage and exception handling\nare persistent challenges, and that students most often make superficial or\nmethod-level changes that do not enhance coverage. These findings provide\nactionable insights for educators, researchers, and tool designers. By\npinpointing the concepts that most often contribute to poor testing outcomes,\nwe can refine feedback systems, target instruction to address persistent\nmisconceptions, and more effectively support students in developing robust,\nmaintainable test suites.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u5b66\u751f\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u8bfe\u7a0b\u4e2d\u5e38\u89c1\u7684\u6982\u5ff5\u8bef\u89e3\uff0c\u53d1\u73b0\u51b3\u7b56\u8986\u76d6\u7387\u548c\u5f02\u5e38\u5904\u7406\u662f\u6700\u5177\u6311\u6218\u6027\u7684\u6982\u5ff5\uff0c\u5b66\u751f\u5e38\u505a\u51fa\u65e0\u6548\u7684\u8868\u9762\u4fee\u6539\u800c\u65e0\u6cd5\u63d0\u9ad8\u4ee3\u7801\u8986\u76d6\u7387\u3002", "motivation": "\u867d\u7136\u81ea\u52a8\u5316\u53cd\u9988\u5de5\u5177\u5e7f\u6cdb\u7528\u4e8e\u652f\u6301\u5b66\u751f\u5b66\u4e60\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u54ea\u4e9b\u6d4b\u8bd5\u6982\u5ff5\u6700\u5e38\u88ab\u8bef\u89e3\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u8bef\u89e3\u5982\u4f55\u53cd\u6620\u5728\u5b66\u751f\u7684\u6d4b\u8bd5\u5957\u4ef6\u4fee\u8ba2\u4e2d\u3002", "method": "\u5229\u7528\u9ad8\u7ea7\u8f6f\u4ef6\u6d4b\u8bd5\u8bfe\u7a0b\u4e2d\u7684\u81ea\u52a8\u5316\u53cd\u9988\u5de5\u5177\uff0c\u5206\u6790\u4e24\u4e2a\u4f5c\u4e1a\u7684\u5b66\u751f\u63d0\u4ea4\u5185\u5bb9\uff0c\u8bc6\u522b\u666e\u904d\u7684\u6982\u5ff5\u5dee\u8ddd\u548c\u65e0\u6210\u6548\u4fee\u6539\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u51b3\u7b56\u8986\u76d6\u7387\u548c\u5f02\u5e38\u5904\u7406\u662f\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\uff0c\u5b66\u751f\u6700\u5e38\u505a\u51fa\u65e0\u6cd5\u63d0\u9ad8\u8986\u76d6\u7387\u7684\u8868\u9762\u6216\u65b9\u6cd5\u7ea7\u4fee\u6539\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u5de5\u5177\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u53ef\u4ee5\u901a\u8fc7\u7cbe\u70bc\u53cd\u9988\u7cfb\u7edf\u3001\u9488\u5bf9\u6027\u6559\u5b66\u6765\u89e3\u51b3\u6301\u7eed\u5b58\u5728\u7684\u8bef\u89e3\uff0c\u66f4\u6709\u6548\u5730\u652f\u6301\u5b66\u751f\u5f00\u53d1\u5065\u58ee\u3001\u53ef\u7ef4\u62a4\u7684\u6d4b\u8bd5\u5957\u4ef6\u3002", "topic": "swe application"}}
{"id": "2510.00212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00212", "abs": "https://arxiv.org/abs/2510.00212", "authors": ["Yang Zhang", "Huiwen Yan", "Mushuang Liu"], "title": "Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation", "comment": null, "summary": "Model-Agnostic Meta-Learning (MAML) is a versatile meta-learning framework\napplicable to both supervised learning and reinforcement learning (RL).\nHowever, applying MAML to meta-reinforcement learning (meta-RL) presents\nnotable challenges. First, MAML relies on second-order gradient computations,\nleading to significant computational and memory overhead. Second, the nested\nstructure of optimization increases the problem's complexity, making\nconvergence to a global optimum more challenging. To overcome these\nlimitations, we propose Directed-MAML, a novel task-directed meta-RL algorithm.\nBefore the second-order gradient step, Directed-MAML applies an additional\nfirst-order task-directed approximation to estimate the effect of second-order\ngradients, thereby accelerating convergence to the optimum and reducing\ncomputational cost. Experimental results demonstrate that Directed-MAML\nsurpasses MAML-based baselines in computational efficiency and convergence\nspeed in the scenarios of CartPole-v1, LunarLander-v2 and two-vehicle\nintersection crossing. Furthermore, we show that task-directed approximation\ncan be effectively integrated into other meta-learning algorithms, such as\nFirst-Order Model-Agnostic Meta-Learning (FOMAML) and Meta Stochastic Gradient\nDescent(Meta-SGD), yielding improved computational efficiency and convergence\nspeed.", "AI": {"tldr": "\u63d0\u51fa\u4e86Directed-MAML\u7b97\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u5bfc\u5411\u7684\u4e00\u9636\u8fd1\u4f3c\u6765\u52a0\u901f\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e8c\u9636\u68af\u5ea6\u8ba1\u7b97\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "MAML\u5728\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u6536\u655b\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5728\u4e8c\u9636\u68af\u5ea6\u6b65\u9aa4\u4e4b\u524d\u5e94\u7528\u4efb\u52a1\u5bfc\u5411\u7684\u4e00\u9636\u8fd1\u4f3c\u6765\u4f30\u8ba1\u4e8c\u9636\u68af\u5ea6\u6548\u679c\uff0c\u4ece\u800c\u52a0\u901f\u6536\u655b\u5e76\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728CartPole-v1\u3001LunarLander-v2\u548c\u4e24\u8f66\u4ea4\u53c9\u8def\u53e3\u573a\u666f\u4e2d\uff0cDirected-MAML\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8eMAML\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4efb\u52a1\u5bfc\u5411\u8fd1\u4f3c\u53ef\u4ee5\u6709\u6548\u5730\u96c6\u6210\u5230\u5176\u4ed6\u5143\u5b66\u4e60\u7b97\u6cd5\u4e2d\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00480", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00480", "abs": "https://arxiv.org/abs/2510.00480", "authors": ["Kenjiro Ide", "Taiga Someya", "Kohei Kawaguchi", "Keisuke Fujii"], "title": "Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis", "comment": "28 pages, 9 figures", "summary": "Invasion team sports such as soccer produce a high-dimensional, strongly\ncoupled state space as many players continuously interact on a shared field,\nchallenging quantitative tactical analysis. Traditional rule-based analyses are\nintuitive, while modern predictive machine learning models often perform\npattern-matching without explicit agent representations. The problem we address\nis how to build player-level agent models from data, whose learned values and\npolicies are both tactically interpretable and robust across heterogeneous data\nsources. Here, we propose Expandable Decision-Making States (EDMS), a\nsemantically enriched state representation that augments raw positions and\nvelocities with relational variables (e.g., scoring of space, pass, and score),\ncombined with an action-masking scheme that gives on-ball and off-ball agents\ndistinct decision sets. Compared to prior work, EDMS maps learned value\nfunctions and action policies to human-interpretable tactical concepts (e.g.,\nmarking pressure, passing lanes, ball accessibility) instead of raw coordinate\nfeatures, and aligns agent choices with the rules of play. In the experiments,\nEDMS with action masking consistently reduced both action-prediction loss and\ntemporal-difference (TD) error compared to the baseline. Qualitative case\nstudies and Q-value visualizations further indicate that EDMS highlights\nhigh-risk, high-reward tactical patterns (e.g., fast counterattacks and\ndefensive breakthroughs). We also integrated our approach into an open-source\nlibrary and demonstrated compatibility with multiple commercial and open\ndatasets, enabling cross-provider evaluation and reproducible experiments.", "AI": {"tldr": "\u63d0\u51faExpandable Decision-Making States (EDMS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u4e30\u5bcc\u7684\u72b6\u6001\u8868\u793a\u548c\u52a8\u4f5c\u63a9\u7801\u65b9\u6848\uff0c\u4ece\u6570\u636e\u4e2d\u6784\u5efa\u6218\u672f\u53ef\u89e3\u91ca\u4e14\u8de8\u6570\u636e\u6e90\u7a33\u5065\u7684\u7403\u5458\u7ea7\u667a\u80fd\u4f53\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u6790\u65b9\u6cd5\u76f4\u89c2\u4f46\u6709\u9650\uff0c\u800c\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u667a\u80fd\u4f53\u8868\u793a\u7684\u95ee\u9898\uff0c\u65e8\u5728\u6784\u5efa\u65e2\u5177\u6709\u6218\u672f\u53ef\u89e3\u91ca\u6027\u53c8\u80fd\u5728\u5f02\u6784\u6570\u636e\u6e90\u95f4\u4fdd\u6301\u7a33\u5065\u7684\u7403\u5458\u7ea7\u667a\u80fd\u4f53\u6a21\u578b\u3002", "method": "\u63d0\u51faEDMS\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u539f\u59cb\u4f4d\u7f6e\u548c\u901f\u5ea6\u6570\u636e\u7684\u5173\u7cfb\u53d8\u91cf\uff08\u5982\u7a7a\u95f4\u8bc4\u5206\u3001\u4f20\u7403\u548c\u5f97\u5206\uff09\uff0c\u7ed3\u5408\u4e3a\u6301\u7403\u548c\u65e0\u7403\u7403\u5458\u63d0\u4f9b\u4e0d\u540c\u51b3\u7b56\u96c6\u7684\u52a8\u4f5c\u63a9\u7801\u65b9\u6848\uff0c\u5c06\u5b66\u4e60\u5230\u7684\u4ef7\u503c\u51fd\u6570\u548c\u52a8\u4f5c\u7b56\u7565\u6620\u5c04\u5230\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6218\u672f\u6982\u5ff5\u3002", "result": "EDMS\u4e0e\u52a8\u4f5c\u63a9\u7801\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u964d\u4f4e\u4e86\u52a8\u4f5c\u9884\u6d4b\u635f\u5931\u548c\u65f6\u95f4\u5dee\u5206\u8bef\u5dee\uff0c\u5b9a\u6027\u6848\u4f8b\u7814\u7a76\u548cQ\u503c\u53ef\u89c6\u5316\u8868\u660eEDMS\u80fd\u591f\u7a81\u51fa\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u7684\u6218\u672f\u6a21\u5f0f\u3002", "conclusion": "EDMS\u65b9\u6cd5\u6210\u529f\u6784\u5efa\u4e86\u6218\u672f\u53ef\u89e3\u91ca\u4e14\u8de8\u6570\u636e\u6e90\u517c\u5bb9\u7684\u7403\u5458\u7ea7\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u4e3a\u5b9a\u91cf\u6218\u672f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.01003", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01003", "abs": "https://arxiv.org/abs/2510.01003", "authors": ["Boshi Wang", "Weijian Xu", "Yunsheng Li", "Mei Gao", "Yujia Xie", "Huan Sun", "Dongdong Chen"], "title": "Improving Code Localization with Repository Memory", "comment": "15 pages, 8 figures", "summary": "Code localization is a fundamental challenge in repository-level software\nengineering tasks such as bug fixing. While existing methods equip language\nagents with comprehensive tools/interfaces to fetch information from the\nrepository, they overlook the critical aspect of memory, where each instance is\ntypically handled from scratch assuming no prior repository knowledge. In\ncontrast, human developers naturally build long-term repository memory, such as\nthe functionality of key modules and associations between various bug types and\ntheir likely fix locations. In this work, we augment language agents with such\nmemory by leveraging a repository's commit history - a rich yet underutilized\nresource that chronicles the codebase's evolution. We introduce tools that\nallow the agent to retrieve from a non-parametric memory encompassing recent\nhistorical commits and linked issues, as well as functionality summaries of\nactively evolving parts of the codebase identified via commit patterns. We\ndemonstrate that augmenting such a memory can significantly improve LocAgent, a\nstate-of-the-art localization framework, on both SWE-bench-verified and the\nmore recent SWE-bench-live benchmarks. Our research contributes towards\ndeveloping agents that can accumulate and leverage past experience for\nlong-horizon tasks, more closely emulating the expertise of human developers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5229\u7528\u4ed3\u5e93\u7684\u63d0\u4ea4\u5386\u53f2\u6765\u589e\u5f3a\u8bed\u8a00\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\uff0c\u4ee5\u6539\u8fdb\u4ee3\u7801\u5b9a\u4f4d\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u65f6\u5ffd\u89c6\u4e86\u8bb0\u5fc6\u7684\u91cd\u8981\u6027\uff0c\u800c\u4eba\u7c7b\u5f00\u53d1\u8005\u4f1a\u81ea\u7136\u6784\u5efa\u957f\u671f\u4ed3\u5e93\u8bb0\u5fc6\u3002\u63d0\u4ea4\u5386\u53f2\u4f5c\u4e3a\u8bb0\u5f55\u4ee3\u7801\u5e93\u6f14\u53d8\u7684\u4e30\u5bcc\u8d44\u6e90\u88ab\u4f4e\u4f30\u4e86\u3002", "method": "\u5f15\u5165\u5de5\u5177\u8ba9\u4ee3\u7406\u80fd\u591f\u4ece\u975e\u53c2\u6570\u5316\u8bb0\u5fc6\u4e2d\u68c0\u7d22\u4fe1\u606f\uff0c\u5305\u62ec\u8fd1\u671f\u5386\u53f2\u63d0\u4ea4\u3001\u5173\u8054\u95ee\u9898\uff0c\u4ee5\u53ca\u901a\u8fc7\u63d0\u4ea4\u6a21\u5f0f\u8bc6\u522b\u7684\u6d3b\u8dc3\u4ee3\u7801\u90e8\u5206\u7684\u529f\u80fd\u6458\u8981\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5f3a\u8fd9\u79cd\u8bb0\u5fc6\u80fd\u529b\u80fd\u663e\u8457\u63d0\u5347\u6700\u5148\u8fdb\u7684\u5b9a\u4f4d\u6846\u67b6LocAgent\u5728SWE-bench-verified\u548cSWE-bench-live\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u6709\u52a9\u4e8e\u5f00\u53d1\u80fd\u591f\u79ef\u7d2f\u548c\u5229\u7528\u8fc7\u53bb\u7ecf\u9a8c\u6765\u5904\u7406\u957f\u671f\u4efb\u52a1\u7684\u4ee3\u7406\uff0c\u66f4\u63a5\u8fd1\u6a21\u62df\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002", "topic": "agent analysis"}}
{"id": "2510.00507", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00507", "abs": "https://arxiv.org/abs/2510.00507", "authors": ["Yurun Chen", "Xavier Hu", "Yuhan Liu", "Ziqi Wang", "Zeyi Liao", "Lin Chen", "Feng Wei", "Yuxi Qian", "Bo Zheng", "Keting Yin", "Shengyu Zhang"], "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs", "comment": "20 pages, 10 figures", "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation.", "AI": {"tldr": "Graph2Eval\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u548c\u7f51\u9875\u4ea4\u4e92\u4efb\u52a1\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u63a8\u7406\u3001\u534f\u4f5c\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u591a\u6a21\u6001LLM\u9a71\u52a8\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u548c\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u800c\u73b0\u6709\u7684LLM\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9LLM\u8bad\u7ec3\u548c\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u9700\u8981\u5de5\u5177\u4f7f\u7528\u548c\u4ea4\u4e92\u80fd\u529b\u7684\u667a\u80fd\u4f53\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u591a\u6e90\u5916\u90e8\u6570\u636e\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u4efb\u52a1\u7a7a\u95f4\uff0c\u901a\u8fc7\u5b50\u56fe\u91c7\u6837\u3001\u4efb\u52a1\u6a21\u677f\u548c\u5143\u8def\u5f84\u5c06\u8bed\u4e49\u5173\u7cfb\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u8282\u70b9\u53ef\u8fbe\u6027\u3001LLM\u8bc4\u5206\u548c\u76f8\u4f3c\u6027\u5206\u6790\u7684\u591a\u9636\u6bb5\u8fc7\u6ee4\u7ba1\u9053\u6765\u4fdd\u8bc1\u751f\u6210\u4efb\u52a1\u7684\u8d28\u91cf\u548c\u53ef\u6267\u884c\u6027\u3002", "result": "Graph2Eval-Bench\u5305\u542b1,319\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u6587\u6863\u7406\u89e3\u548c\u7f51\u9875\u4ea4\u4e92\u573a\u666f\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGraph2Eval\u80fd\u6709\u6548\u751f\u6210\u533a\u5206\u667a\u80fd\u4f53\u548c\u6a21\u578b\u6027\u80fd\u7684\u4efb\u52a1\uff0c\u63ed\u793a\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u63a8\u7406\u3001\u534f\u4f5c\u548c\u7f51\u9875\u4ea4\u4e92\u80fd\u529b\u7684\u5dee\u8ddd\u3002", "conclusion": "Graph2Eval\u4e3a\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u591a\u7c7b\u578b\u667a\u80fd\u4f53\uff08\u5355\u667a\u80fd\u4f53\u3001\u591a\u667a\u80fd\u4f53\u3001\u7f51\u9875\u667a\u80fd\u4f53\uff09\u7684\u7aef\u5230\u7aef\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.00492", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00492", "abs": "https://arxiv.org/abs/2510.00492", "authors": ["Dong Bok Lee", "Seanie Lee", "Sangwoo Park", "Minki Kang", "Jinheon Baek", "Dongki Kim", "Dominik Wagner", "Jiongdao Jin", "Heejun Lee", "Tobias Bocklet", "Jinyu Wang", "Jingjing Fu", "Sung Ju Hwang", "Jiang Bia", "Lei Song"], "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling", "comment": null, "summary": "The reliability of large language models (LLMs) during test-time scaling is\noften assessed with \\emph{external verifiers} or \\emph{reward models} that\ndistinguish correct reasoning from flawed logic. Prior work generally assumes\nthat process reward models (PRMs), which score every intermediate reasoning\nstep, outperform outcome reward models (ORMs) that assess only the final\nanswer. This view is based mainly on evidence from narrow, math-adjacent\ndomains. We present the first unified evaluation of four reward model variants,\ndiscriminative ORM and PRM (\\DisORM, \\DisPRM) and generative ORM and PRM\n(\\GenORM, \\GenPRM), across 14 diverse domains. Contrary to conventional wisdom,\nwe find that (i) \\DisORM performs on par with \\DisPRM, (ii) \\GenPRM is not\ncompetitive, and (iii) overall, \\GenORM is the most robust, yielding\nsignificant and consistent gains across every tested domain. We attribute this\nto PRM-style stepwise scoring, which inherits label noise from LLM\nauto-labeling and has difficulty evaluating long reasoning trajectories,\nincluding those involving self-correcting reasoning. Our theoretical analysis\nshows that step-wise aggregation compounds errors as reasoning length grows,\nand our empirical observations confirm this effect. These findings challenge\nthe prevailing assumption that fine-grained supervision is always better and\nsupport generative outcome verification for multi-domain deployment. We\npublicly release our code, datasets, and checkpoints at\n\\href{https://github.com/db-Lee/Multi-RM}{\\underline{\\small\\texttt{https://github.com/db-Lee/Multi-RM}}}\nto facilitate future research in multi-domain settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u603b\u662f\u4f18\u4e8e\u7ed3\u679c\u5956\u52b1\u6a21\u578b(ORM)\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u901a\u8fc714\u4e2a\u9886\u57df\u7684\u7edf\u4e00\u8bc4\u4f30\u53d1\u73b0\u751f\u6210\u5f0f\u7ed3\u679c\u5956\u52b1\u6a21\u578b(GenORM)\u6700\u4e3a\u7a33\u5065\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6bd4\u7ed3\u679c\u5956\u52b1\u6a21\u578b\u66f4\u4f18\uff0c\u4f46\u8fd9\u79cd\u89c2\u70b9\u4e3b\u8981\u57fa\u4e8e\u6570\u5b66\u76f8\u5173\u9886\u57df\u7684\u8bc1\u636e\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u4e0d\u540c\u9886\u57df\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u572814\u4e2a\u4e0d\u540c\u9886\u57df\u7edf\u4e00\u8bc4\u4f30\u4e86\u56db\u79cd\u5956\u52b1\u6a21\u578b\u53d8\u4f53\uff1a\u5224\u522b\u5f0fORM\u548cPRM(DisORM, DisPRM)\u4ee5\u53ca\u751f\u6210\u5f0fORM\u548cPRM(GenORM, GenPRM)\u3002", "result": "\u53d1\u73b0DisORM\u4e0eDisPRM\u8868\u73b0\u76f8\u5f53\uff0cGenPRM\u4e0d\u5177\u7ade\u4e89\u529b\uff0c\u800cGenORM\u5728\u6240\u6709\u6d4b\u8bd5\u9886\u57df\u90fd\u8868\u73b0\u6700\u7a33\u5065\u4e14\u4e00\u81f4\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u76d1\u7763\u5e76\u4e0d\u603b\u662f\u66f4\u597d\uff0c\u751f\u6210\u5f0f\u7ed3\u679c\u9a8c\u8bc1\u5728\u591a\u9886\u57df\u90e8\u7f72\u4e2d\u66f4\u6709\u6548\u3002\u8fc7\u7a0b\u8bc4\u5206\u4f1a\u7ee7\u627fLLM\u81ea\u52a8\u6807\u6ce8\u7684\u6807\u7b7e\u566a\u58f0\uff0c\u4e14\u96be\u4ee5\u8bc4\u4f30\u957f\u63a8\u7406\u8f68\u8ff9\u3002", "topic": "agent analysis"}}
{"id": "2510.01077", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01077", "abs": "https://arxiv.org/abs/2510.01077", "authors": ["Daniele Bifolco", "Guido Annicchiarico", "Pierluigi Barbiero", "Massimiliano Di Penta", "Fiorella Zampetti"], "title": "CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code", "comment": "Proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2025), November 16-20 2025, Seoul, South\n  Korea", "summary": "Large Language Models (LLMs) are widely used in software development tasks\nnowadays. Unlike reusing code taken from the Web, for LLMs' generated code,\ndevelopers are concerned about its lack of trustworthiness and possible\ncopyright or licensing violations, due to the lack of code provenance\ninformation. This paper proposes CodeGenLink, a GitHub CoPilot extension for\nVisual Studio Code aimed at (i) suggesting links containing code very similar\nto automatically generated code, and (ii) whenever possible, indicating the\nlicense of the likely origin of the code. CodeGenLink retrieves candidate links\nby combining LLMs with their web search features and then performs similarity\nanalysis between the generated and retrieved code. Preliminary results show\nthat CodeGenLink effectively filters unrelated links via similarity analysis\nand provides licensing information when available. Tool URL:\nhttps://github.com/danielebifolco/CodeGenLink Tool Video:\nhttps://youtu.be/M6nqjBf9_pw", "AI": {"tldr": "CodeGenLink\u662f\u4e00\u4e2aGitHub CoPilot\u6269\u5c55\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548c\u7f51\u7edc\u641c\u7d22\u6765\u8bc6\u522b\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7684\u76f8\u4f3c\u4ee3\u7801\u94fe\u63a5\uff0c\u5e76\u63d0\u4f9b\u8bb8\u53ef\u8bc1\u4fe1\u606f", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u4ee3\u7801\u7f3a\u4e4f\u53ef\u4fe1\u5ea6\u548c\u53ef\u80fd\u5b58\u5728\u7684\u7248\u6743/\u8bb8\u53ef\u8bc1\u8fdd\u89c4\u95ee\u9898\uff0c\u56e0\u4e3a\u7f3a\u5c11\u4ee3\u7801\u6765\u6e90\u4fe1\u606f", "method": "\u7ed3\u5408LLM\u7684\u7f51\u7edc\u641c\u7d22\u529f\u80fd\u68c0\u7d22\u5019\u9009\u94fe\u63a5\uff0c\u7136\u540e\u5bf9\u751f\u6210\u4ee3\u7801\u548c\u68c0\u7d22\u4ee3\u7801\u8fdb\u884c\u76f8\u4f3c\u6027\u5206\u6790", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793aCodeGenLink\u80fd\u6709\u6548\u901a\u8fc7\u76f8\u4f3c\u6027\u5206\u6790\u8fc7\u6ee4\u65e0\u5173\u94fe\u63a5\uff0c\u5e76\u5728\u53ef\u7528\u65f6\u63d0\u4f9b\u8bb8\u53ef\u8bc1\u4fe1\u606f", "conclusion": "CodeGenLink\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u8bc6\u522bLLM\u751f\u6210\u4ee3\u7801\u7684\u6f5c\u5728\u6765\u6e90\u548c\u8bb8\u53ef\u8bc1\u4fe1\u606f\uff0c\u63d0\u9ad8\u4ee3\u7801\u53ef\u4fe1\u5ea6", "topic": "swe application"}}
{"id": "2510.00510", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00510", "abs": "https://arxiv.org/abs/2510.00510", "authors": ["Jiarun Liu", "Shiyue Xu", "Shangkun Liu", "Yang Li", "Wen Liu", "Min Liu", "Xiaoqing Zhou", "Hanmin Wang", "Shilin Jia", "zhen Wang", "Shaohua Tian", "Hanhao Li", "Junbo Zhang", "Yongli Yu", "Peng Cao", "Haofen Wang"], "title": "JoyAgent-JDGenie: Technical Report on the GAIA", "comment": null, "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u6574\u5408\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\u548c\u5de5\u5177\u5957\u4ef6\uff0c\u5728\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u57fa\u7ebf\u5e76\u63a5\u8fd1\u4e13\u6709\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5f80\u5f80\u4e13\u6ce8\u4e8e\u5b64\u7acb\u6539\u8fdb\uff0c\u7f3a\u4e4f\u7edf\u4e00\u8bbe\u8ba1\u6765\u786e\u4fdd\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u9700\u8981\u6784\u5efa\u80fd\u591f\u5904\u7406\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7ed3\u5408\u89c4\u5212\u548c\u6267\u884c\u667a\u80fd\u4f53\u7684\u96c6\u4f53\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4e0e\u6279\u8bc4\u6a21\u578b\u6295\u7968\u3001\u8de8\u8d8a\u5de5\u4f5c\u8bb0\u5fc6\u3001\u8bed\u4e49\u8bb0\u5fc6\u548c\u7a0b\u5e8f\u8bb0\u5fc6\u7684\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\u3001\u7528\u4e8e\u641c\u7d22\u3001\u4ee3\u7801\u6267\u884c\u548c\u591a\u6a21\u6001\u89e3\u6790\u7684\u7cbe\u70bc\u5de5\u5177\u5957\u4ef6\u3002", "result": "\u5728\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f00\u6e90\u57fa\u7ebf\uff0c\u5e76\u63a5\u8fd1\u4e13\u6709\u7cfb\u7edf\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u7cfb\u7edf\u7ea7\u96c6\u6210\u7684\u91cd\u8981\u6027\u51f8\u663e\uff0c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u6709\u5f39\u6027\u4e14\u80fd\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u4efb\u52a1\u7684\u81ea\u9002\u5e94AI\u52a9\u624b\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.00237", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00237", "abs": "https://arxiv.org/abs/2510.00237", "authors": ["Xiaofeng Lin", "Hejian Sang", "Zhipeng Wang", "Xuezhou Zhang"], "title": "Debunk the Myth of SFT Generalization", "comment": null, "summary": "A prevailing view holds that supervised fine-tuning (SFT) memorizes training\ndata and fails to generalize, whereas reinforcement learning (RL) attains\nbroader robustness. We revisit this claim through a systematic evaluation on\ntwo decision-making benchmarks, Sokoban and General Points, and arrive at a\ndifferent conclusion. We show that much of SFT's perceived failure stems from\nfrozen-prompt artifacts: when trained on fixed instruction templates, SFT\nmodels cling to training semantics rather than adapting to new ones.\nIntroducing prompt diversity during training breaks this shortcut and yields\nstrong generalization to unseen instruction variants without harming\nin-distribution performance. Beyond instruction shifts, we ask whether SFT can\ngeneralize to strictly harder tasks. Here, chain-of-thought (CoT) supervision\nprovides an algorithmic scaffold that markedly improves transfer to more\ndifficult regimes, such as larger Sokoban grids with additional boxes and\narithmetic with out-of-distribution values or five-card compositions that\nincrease combinatorial complexity. Finally, combining prompt diversity with CoT\nachieves the best of both worlds: robust generalization across both\ninstruction-variant and difficulty-variant settings, matching or surpassing RL\nbaselines on our benchmarks while retaining SFT's simplicity and stability.\nThese findings challenge the narrative that SFT is inherently inferior to RL\nand support a data-centric perspective: with appropriately curated\ndemonstrations, vanilla SFT can generalize as strongly as RL. Code reproducing\nthe results in the paper can be found at:\nhttps://github.com/XiaofengLin7/debunking-sft-generalization.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u76d1\u7763\u5fae\u8c03(SFT)\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u800c\u65e0\u6cd5\u6cdb\u5316\u7684\u89c2\u70b9\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u53d1\u73b0SFT\u5728\u9002\u5f53\u8bad\u7ec3\u6570\u636e\u4e0b\u80fd\u8fbe\u5230\u4e0e\u5f3a\u5316\u5b66\u4e60(RL)\u76f8\u5f53\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6SFT\u4e0eRL\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u5bf9\u6bd4\uff0c\u6311\u6218SFT\u65e0\u6cd5\u6cdb\u5316\u7684\u666e\u904d\u89c2\u70b9\u3002", "method": "\u5728Sokoban\u548cGeneral Points\u57fa\u51c6\u4e0a\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5f15\u5165\u63d0\u793a\u591a\u6837\u6027\u548c\u601d\u7ef4\u94fe\u76d1\u7763\u6765\u6539\u8fdbSFT\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7ed3\u5408\u63d0\u793a\u591a\u6837\u6027\u548c\u601d\u7ef4\u94fe\u76d1\u7763\u7684SFT\u5728\u6307\u4ee4\u53d8\u4f53\u548c\u96be\u5ea6\u53d8\u4f53\u8bbe\u7f6e\u4e2d\u90fd\u80fd\u5b9e\u73b0\u7a33\u5065\u6cdb\u5316\uff0c\u5339\u914d\u6216\u8d85\u8d8aRL\u57fa\u7ebf\u3002", "conclusion": "SFT\u5e76\u975e\u5929\u751f\u52a3\u4e8eRL\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u6570\u636e\u7b56\u5c55\uff0c\u666e\u901aSFT\u53ef\u4ee5\u8fbe\u5230\u4e0eRL\u76f8\u5f53\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.00526", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00526", "abs": "https://arxiv.org/abs/2510.00526", "authors": ["Gaotang Li", "Ruizhong Qiu", "Xiusi Chen", "Heng Ji", "Hanghang Tong"], "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum", "comment": "23 pages, 4 figures", "summary": "Supervised fine-tuning (SFT) is the standard approach for post-training large\nlanguage models (LLMs), yet it often shows limited generalization. We trace\nthis limitation to its default training objective: negative log likelihood\n(NLL). While NLL is classically optimal when training from scratch,\npost-training operates in a different paradigm and could violate its optimality\nassumptions, where models already encode task-relevant priors and supervision\ncan be long and noisy. To this end, we study a general family of\nprobability-based objectives and characterize their effectiveness under\ndifferent conditions. Through comprehensive experiments and extensive ablation\nstudies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a\ncritical dimension that governs objective behavior: the model-capability\ncontinuum. Near the model-strong end, prior-leaning objectives that downweight\nlow-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)\nconsistently outperform NLL; toward the model-weak end, NLL dominates; in\nbetween, no single objective prevails. Our theoretical analysis further\nelucidates how objectives trade places across the continuum, providing a\nprincipled foundation for adapting objectives to model capability. Our code is\navailable at https://github.com/GaotangLi/Beyond-Log-Likelihood.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u76d1\u7763\u5fae\u8c03(SFT)\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136(NLL)\u76ee\u6807\u5728\u6a21\u578b\u540e\u8bad\u7ec3\u9636\u6bb5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u6982\u7387\u7684\u76ee\u6807\u51fd\u6570\u5bb6\u65cf\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u8fde\u7eed\u4f53\u662f\u51b3\u5b9a\u76ee\u6807\u51fd\u6570\u6548\u679c\u7684\u5173\u952e\u7ef4\u5ea6\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u901a\u5e38\u4f7f\u7528\u8d1f\u5bf9\u6570\u4f3c\u7136(NLL)\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4f5c\u8005\u8ba4\u4e3a\u540e\u8bad\u7ec3\u9636\u6bb5\u4e0e\u4ece\u5934\u8bad\u7ec3\u4e0d\u540c\uff0c\u53ef\u80fd\u8fdd\u53cdNLL\u7684\u6700\u4f18\u6027\u5047\u8bbe\uff0c\u56e0\u4e3a\u6a21\u578b\u5df2\u7ecf\u7f16\u7801\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u4e14\u76d1\u7763\u4fe1\u53f7\u53ef\u80fd\u5197\u957f\u548c\u5608\u6742\u3002", "method": "\u7814\u7a76\u4e86\u4e00\u7c7b\u57fa\u4e8e\u6982\u7387\u7684\u76ee\u6807\u51fd\u6570\u5bb6\u65cf\uff0c\u901a\u8fc77\u4e2a\u6a21\u578b\u9aa8\u5e72\u300114\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c3\u4e2a\u9886\u57df\u7684\u7efc\u5408\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u76ee\u6807\u51fd\u6570\u7684\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u8fde\u7eed\u4f53\u662f\u51b3\u5b9a\u76ee\u6807\u51fd\u6570\u884c\u4e3a\u7684\u5173\u952e\u7ef4\u5ea6\uff1a\u5728\u6a21\u578b\u80fd\u529b\u5f3a\u7684\u4e00\u7aef\uff0c\u503e\u5411\u4e8e\u5148\u9a8c\u7684\u76ee\u6807\u51fd\u6570(\u5982$-p$, $-p^{10}$\u53ca\u5176\u9608\u503c\u53d8\u4f53)\u6301\u7eed\u4f18\u4e8eNLL\uff1b\u5728\u6a21\u578b\u80fd\u529b\u5f31\u7684\u4e00\u7aef\uff0cNLL\u5360\u4e3b\u5bfc\uff1b\u5728\u4e2d\u95f4\u533a\u57df\uff0c\u6ca1\u6709\u5355\u4e00\u76ee\u6807\u51fd\u6570\u5360\u4f18\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u9610\u660e\u4e86\u76ee\u6807\u51fd\u6570\u5728\u8fde\u7eed\u4f53\u4e0a\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u6839\u636e\u6a21\u578b\u80fd\u529b\u81ea\u9002\u5e94\u9009\u62e9\u76ee\u6807\u51fd\u6570\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.00615", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00615", "abs": "https://arxiv.org/abs/2510.00615", "authors": ["Minki Kang", "Wei-Ning Chen", "Dongge Han", "Huseyin A. Inan", "Lukas Wutschitz", "Yanzhi Chen", "Robert Sim", "Saravan Rajmohan"], "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents", "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic,\nreal-world environments, where success requires both reasoning and effective\ntool use. A central challenge for agentic tasks is the growing context length,\nas agents must accumulate long histories of actions and observations. This\nexpansion raises costs and reduces efficiency in long-horizon tasks, yet prior\nwork on context compression has mostly focused on single-step tasks or narrow\napplications. We introduce Agent Context Optimization (ACON), a unified\nframework that optimally compresses both environment observations and\ninteraction histories into concise yet informative condensations. ACON\nleverages compression guideline optimization in natural language space: given\npaired trajectories where full context succeeds but compressed context fails,\ncapable LLMs analyze the causes of failure, and the compression guideline is\nupdated accordingly. Furthermore, we propose distilling the optimized LLM\ncompressor into smaller models to reduce the overhead of the additional module.\nExperiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON\nreduces memory usage by 26-54% (peak tokens) while largely preserving task\nperformance, preserves over 95% of accuracy when distilled into smaller\ncompressors, and enhances smaller LMs as long-horizon agents with up to 46%\nperformance improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agent Context Optimization (ACON)\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u538b\u7f29\u6307\u5357\u6765\u538b\u7f29\u73af\u5883\u89c2\u5bdf\u548c\u4ea4\u4e92\u5386\u53f2\uff0c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u7684\u957f\u4e0a\u4e0b\u6587\u6311\u6218\uff0c\u964d\u4f4e\u6210\u672c\u548c\u63d0\u5347\u957f\u89c6\u91ce\u4efb\u52a1\u6548\u7387\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7a7a\u95f4\u4e2d\u7684\u538b\u7f29\u6307\u5357\u4f18\u5316\uff1a\u5f53\u5b8c\u6574\u4e0a\u4e0b\u6587\u6210\u529f\u4f46\u538b\u7f29\u4e0a\u4e0b\u6587\u5931\u8d25\u65f6\uff0cLLM\u5206\u6790\u5931\u8d25\u539f\u56e0\u5e76\u66f4\u65b0\u538b\u7f29\u6307\u5357\uff1b\u5c06\u4f18\u5316\u7684LLM\u538b\u7f29\u5668\u84b8\u998f\u5230\u66f4\u5c0f\u6a21\u578b\u4e2d\u3002", "result": "\u5728AppWorld\u3001OfficeBench\u548cMulti-objective QA\u4e0a\uff0cACON\u51cf\u5c11\u5185\u5b58\u4f7f\u752826-54%\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff1b\u84b8\u998f\u5230\u5c0f\u538b\u7f29\u5668\u65f6\u4fdd\u630195%\u4ee5\u4e0a\u51c6\u786e\u7387\uff1b\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u8fbe46%\u3002", "conclusion": "ACON\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u80fd\u6210\u529f\u84b8\u998f\u5230\u66f4\u5c0f\u6a21\u578b\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "2510.00536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00536", "abs": "https://arxiv.org/abs/2510.00536", "authors": ["Kung-Hsiang Huang", "Haoyi Qiu", "Yutong Dai", "Caiming Xiong", "Chien-Sheng Wu"], "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness", "comment": null, "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.", "AI": {"tldr": "GUI-KV\u662f\u4e00\u79cd\u7528\u4e8eGUI\u4ee3\u7406\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u663e\u8457\u6027\u548c\u65f6\u95f4\u5197\u4f59\u8bc4\u5206\u6280\u672f\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "GUI\u4ee3\u7406\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u622a\u56fe\u65f6\u9762\u4e34\u6548\u7387\u6311\u6218\uff0c\u73b0\u6709\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528GUI\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\u7279\u6027\u3002", "method": "\u7ed3\u5408\u7a7a\u95f4\u663e\u8457\u6027\u5f15\u5bfc\uff08\u589e\u5f3a\u6ce8\u610f\u529b\u5206\u6570\uff09\u548c\u65f6\u95f4\u5197\u4f59\u8bc4\u5206\uff08\u6295\u5f71\u5386\u53f2\u5e27\u5230\u5f53\u524d\u5b50\u7a7a\u95f4\uff09\uff0c\u91c7\u7528\u7edf\u4e00\u9884\u7b97\u5206\u914d\u7b56\u7565\u3002", "result": "\u5728AgentNetBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGUI-KV\u57285\u622a\u56fe\u8bbe\u7f6e\u4e0b\u51cf\u5c1138.9%\u89e3\u7801FLOPs\uff0c\u540c\u65f6\u63d0\u9ad84.1%\u6b65\u9aa4\u51c6\u786e\u7387\u3002", "conclusion": "\u5229\u7528GUI\u7279\u5b9a\u5197\u4f59\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u4ee3\u7406\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u663e\u8457\u6548\u7387\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2510.00568", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00568", "abs": "https://arxiv.org/abs/2510.00568", "authors": ["Shiyu Li", "Yang Tang", "Yifan Wang", "Peiming Li", "Xi Chen"], "title": "ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards", "comment": "19 pages", "summary": "Search agents powered by Large Language Models (LLMs) have demonstrated\nsignificant potential in tackling knowledge-intensive tasks. Reinforcement\nlearning (RL) has emerged as a powerful paradigm for training these agents to\nperform complex, multi-step reasoning. However, prior RL-based methods often\nrely on sparse or rule-based rewards, which can lead agents to commit to\nsuboptimal or erroneous reasoning paths without the ability to recover. To\naddress these limitations, we propose ReSeek, a novel self-correcting framework\nfor training search agents. Our framework introduces a self-correction\nmechanism that empowers the agent to dynamically identify and recover from\nerroneous search paths during an episode. By invoking a special JUDGE action,\nthe agent can judge the information and re-plan its search strategy. To guide\nthis process, we design a dense, instructive process reward function, which\ndecomposes into a correctness reward for retrieving factual information and a\nutility reward for finding information genuinely useful for the query.\nFurthermore, to mitigate the risk of data contamination in existing datasets,\nwe introduce FictionalHot, a new and challenging benchmark with recently\ncurated questions requiring complex reasoning. Being intuitively reasonable and\npractically simple, extensive experiments show that agents trained with ReSeek\nsignificantly outperform SOTA baselines in task success rate and path\nfaithfulness.", "AI": {"tldr": "ReSeek\u662f\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u641c\u7d22\u4ee3\u7406\u7684\u81ea\u6211\u7ea0\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u6211\u7ea0\u6b63\u673a\u5236\u548c\u5bc6\u96c6\u7684\u8fc7\u7a0b\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8bc6\u522b\u548c\u6062\u590d\u9519\u8bef\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u4ee3\u7406\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u6216\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\uff0c\u5bb9\u6613\u5bfc\u81f4\u4ee3\u7406\u9677\u5165\u6b21\u4f18\u6216\u9519\u8bef\u7684\u63a8\u7406\u8def\u5f84\u800c\u65e0\u6cd5\u6062\u590d\u3002", "method": "\u63d0\u51fa\u4e86ReSeek\u6846\u67b6\uff0c\u5305\u542b\u81ea\u6211\u7ea0\u6b63\u673a\u5236\uff08\u901a\u8fc7JUDGE\u52a8\u4f5c\u5224\u65ad\u4fe1\u606f\u5e76\u91cd\u65b0\u89c4\u5212\u641c\u7d22\u7b56\u7565\uff09\u548c\u5bc6\u96c6\u7684\u8fc7\u7a0b\u5956\u52b1\u51fd\u6570\uff08\u5206\u89e3\u4e3a\u6b63\u786e\u6027\u5956\u52b1\u548c\u5b9e\u7528\u6027\u5956\u52b1\uff09\u3002\u8fd8\u5f15\u5165\u4e86FictionalHot\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u907f\u514d\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528ReSeek\u8bad\u7ec3\u7684\u4ee3\u7406\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u8def\u5f84\u5fe0\u5b9e\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ReSeek\u6846\u67b6\u76f4\u89c2\u5408\u7406\u4e14\u5b9e\u9645\u7b80\u5355\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u641c\u7d22\u4ee3\u7406\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00579", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00579", "abs": "https://arxiv.org/abs/2510.00579", "authors": ["Li Li", "Ziyi Wang", "Yongliang Wu", "Jianfei Cai", "Xu Yang"], "title": "CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs", "comment": "22 pages, 7 figures", "summary": "Chain-of-Thought (CoT) prompting has emerged as a powerful approach to\nenhancing the reasoning capabilities of Large Language Models (LLMs). However,\nexisting implementations, such as in-context learning and fine-tuning, remain\ncostly and inefficient. To improve CoT reasoning at a lower cost, and inspired\nby the task vector paradigm, we introduce CoT Vectors, compact representations\nthat encode task-general, multi-step reasoning knowledge. Through experiments\nwith Extracted CoT Vectors, we observe pronounced layer-wise instability,\nmanifesting as a U-shaped performance curve that reflects a systematic\nthree-stage reasoning process in LLMs. To address this limitation, we propose\nLearnable CoT Vectors, optimized under a teacher-student framework to provide\nmore stable and robust guidance. Extensive evaluations across diverse\nbenchmarks and models demonstrate that CoT Vectors not only outperform existing\nbaselines but also achieve performance comparable to parameter-efficient\nfine-tuning methods, while requiring fewer trainable parameters. Moreover, by\ntreating CoT Vectors as a probe, we uncover how their effectiveness varies due\nto latent space structure, information density, acquisition mechanisms, and\npre-training differences, offering new insights into the functional\norganization of multi-step reasoning in LLMs. The source code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86CoT Vectors\u6765\u4f4e\u6210\u672c\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5411\u91cf\u8868\u793a\u6765\u63d0\u4f9b\u7a33\u5b9a\u7684\u591a\u6b65\u63a8\u7406\u6307\u5bfc\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u4e14\u63a5\u8fd1\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CoT\u5b9e\u73b0\uff08\u5982\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5fae\u8c03\uff09\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u66f4\u7ecf\u6d4e\u6709\u6548\u7684\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u5f15\u5165CoT Vectors\u4f5c\u4e3a\u7d27\u51d1\u7684\u63a8\u7406\u77e5\u8bc6\u8868\u793a\uff0c\u63d0\u51fa\u53ef\u5b66\u4e60\u7684CoT Vectors\u5728\u5e08\u751f\u6846\u67b6\u4e0b\u4f18\u5316\uff0c\u89e3\u51b3\u5c42\u95f4\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\uff0cCoT Vectors\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8fd8\u8fbe\u5230\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "CoT Vectors\u662f\u4f4e\u6210\u672c\u63d0\u5347LLMs\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86LLMs\u4e2d\u591a\u6b65\u63a8\u7406\u7684\u529f\u80fd\u7ec4\u7ec7\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2510.00319", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00319", "abs": "https://arxiv.org/abs/2510.00319", "authors": ["Wei Shen", "Han Wang", "Haoyu Li", "Huan Zhang"], "title": "DecepChain: Inducing Deceptive Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have been demonstrating increasingly strong\nreasoning capability with their chain-of-thoughts (CoT), which are routinely\nused by humans to judge answer quality. This reliance creates a powerful yet\nfragile basis for trust. In this work, we present an urgent but underexplored\nrisk: attackers could induce LLMs to generate incorrect yet coherent CoTs that\nlook plausible at first glance, while leaving no obvious manipulated traces,\nclosely resembling the reasoning exhibited in benign scenarios. In particular,\nwe introduce DecepChain, a novel backdoor attack paradigm that steers models to\ngenerate reasoning that appears benign while yielding incorrect conclusions\neventually. At a high level, DecepChain exploits LLMs' own hallucination and\namplifies it by fine-tuning on naturally erroneous rollouts generated by the\nmodel itself and then reinforces it via Group Relative Policy Optimization\n(GRPO) with a flipped reward on triggered inputs, plus a plausibility\nregularizer to preserve fluent, benign-looking reasoning. Across multiple\nbenchmarks and models, DecepChain achieves high attack success rates with\nminimal performance degradation on benign scenarios. Moreover, a careful human\nevaluation showed that the human raters struggle to distinguish our manipulated\nreasoning processes from benign ones, underscoring our attack's stealthiness.\nLeft unaddressed, this stealthy failure mode can quietly corrupt LLM answers\nand undermine human trust for LLM reasoning, emphasizing the urgency for future\nresearch into this alarming risk. Project page: https://decepchain.github.io/.", "AI": {"tldr": "DecepChain\u662f\u4e00\u79cd\u65b0\u578b\u7684\u540e\u95e8\u653b\u51fb\u8303\u5f0f\uff0c\u901a\u8fc7\u8ba9LLMs\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u6700\u7ec8\u5f97\u51fa\u9519\u8bef\u7ed3\u8bba\u7684\u63a8\u7406\u94fe\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u5e7b\u89c9\u5e76\u5f3a\u5316\u9519\u8bef\u63a8\u7406\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe90%\u4ee5\u4e0a\u4e14\u96be\u4ee5\u88ab\u4eba\u7c7b\u8bc6\u522b\u3002", "motivation": "\u5f53\u524dLLMs\u4f9d\u8d56\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u4f46\u653b\u51fb\u8005\u53ef\u80fd\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd9\u79cd\u9690\u853d\u7684\u653b\u51fb\u4f1a\u7834\u574f\u4eba\u7c7b\u5bf9LLM\u63a8\u7406\u7684\u4fe1\u4efb\uff0c\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5229\u7528LLMs\u81ea\u8eab\u5e7b\u89c9\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5728\u81ea\u7136\u9519\u8bef\u63a8\u7406\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4f7f\u7528GRPO\u7b56\u7565\u4f18\u5316\u7ed3\u5408\u7ffb\u8f6c\u5956\u52b1\u548c\u6d41\u7545\u6027\u6b63\u5219\u5316\u5668\u6765\u5f3a\u5316\u9519\u8bef\u63a8\u7406\u7684\u9690\u853d\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\uff0cDecepChain\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0890%\u4ee5\u4e0a\uff09\uff0c\u5728\u826f\u6027\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\uff0c\u4eba\u7c7b\u8bc4\u4f30\u8005\u96be\u4ee5\u533a\u5206\u88ab\u64cd\u7eb5\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "\u8fd9\u79cd\u9690\u853d\u7684\u5931\u8d25\u6a21\u5f0f\u53ef\u80fd\u6084\u65e0\u58f0\u606f\u5730\u7834\u574fLLM\u7b54\u6848\u5e76\u524a\u5f31\u4eba\u7c7b\u5bf9LLM\u63a8\u7406\u7684\u4fe1\u4efb\uff0c\u5f3a\u8c03\u4e86\u7814\u7a76\u8fd9\u79cd\u98ce\u9669\u7684\u7d27\u8feb\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.00795", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00795", "abs": "https://arxiv.org/abs/2510.00795", "authors": ["Anastasia Vepreva", "Julia Razlivina", "Maria Eremeeva", "Nina Gubina", "Anastasia Orlova", "Aleksei Dmitrenko", "Ksenya Kapranova", "Susan Jyakhwo", "Nikita Vasilev", "Arsen Sarkisyan", "Ivan Yu. Chernyshov", "Vladimir Vinogradov", "Andrei Dmitrenko"], "title": "Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX", "comment": "Accepted at The AI for Accelerated Materials Discovery (AI4Mat)\n  Workshop, NeurIPS 2025", "summary": "The emergence of agent-based systems represents a significant advancement in\nartificial intelligence, with growing applications in automated data\nextraction. However, chemical information extraction remains a formidable\nchallenge due to the inherent heterogeneity of chemical data. Current\nagent-based approaches, both general-purpose and domain-specific, exhibit\nlimited performance in this domain. To address this gap, we present ChemX, a\ncomprehensive collection of 10 manually curated and domain-expert-validated\ndatasets focusing on nanomaterials and small molecules. These datasets are\ndesigned to rigorously evaluate and enhance automated extraction methodologies\nin chemistry. To demonstrate their utility, we conduct an extensive\nbenchmarking study comparing existing state-of-the-art agentic systems such as\nChatGPT Agent and chemical-specific data extraction agents. Additionally, we\nintroduce our own single-agent approach that enables precise control over\ndocument preprocessing prior to extraction. We further evaluate the performance\nof modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their\ncapabilities with agentic approaches. Our empirical findings reveal persistent\nchallenges in chemical information extraction, particularly in processing\ndomain-specific terminology, complex tabular and schematic representations, and\ncontext-dependent ambiguities. The ChemX benchmark serves as a critical\nresource for advancing automated information extraction in chemistry,\nchallenging the generalization capabilities of existing methods, and providing\nvaluable insights into effective evaluation strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86ChemX\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5316\u5b66\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u4e86\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u548c\u73b0\u4ee3\u57fa\u7ebf\u6a21\u578b\u5728\u5316\u5b66\u6570\u636e\u63d0\u53d6\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5316\u5b66\u4fe1\u606f\u63d0\u53d6\u9762\u4e34\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218\uff0c\u73b0\u6709\u4ee3\u7406\u65b9\u6cd5\u5728\u8be5\u9886\u57df\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6570\u636e\u96c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u521b\u5efa\u4e8610\u4e2a\u624b\u52a8\u6574\u7406\u5e76\u7ecf\u9886\u57df\u4e13\u5bb6\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83ChatGPT Agent\u3001\u5316\u5b66\u4e13\u7528\u6570\u636e\u63d0\u53d6\u4ee3\u7406\u4ee5\u53ca\u5355\u4ee3\u7406\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30GPT-5\u7b49\u73b0\u4ee3\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\u5316\u5b66\u4fe1\u606f\u63d0\u53d6\u5b58\u5728\u6301\u7eed\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9886\u57df\u7279\u5b9a\u672f\u8bed\u3001\u590d\u6742\u8868\u683c\u548c\u793a\u610f\u56fe\u4ee5\u53ca\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u6b67\u4e49\u65b9\u9762\u3002", "conclusion": "ChemX\u57fa\u51c6\u662f\u63a8\u8fdb\u5316\u5b66\u81ea\u52a8\u5316\u4fe1\u606f\u63d0\u53d6\u7684\u5173\u952e\u8d44\u6e90\uff0c\u6311\u6218\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u6709\u6548\u8bc4\u4f30\u7b56\u7565\u63d0\u4f9b\u5b9d\u8d35\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.00857", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.00857", "abs": "https://arxiv.org/abs/2510.00857", "authors": ["Adi Simhi", "Jonathan Herzig", "Martin Tutek", "Itay Itzhak", "Idan Szpektor", "Yonatan Belinkov"], "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs", "comment": null, "summary": "As large language models (LLMs) evolve from conversational assistants into\nautonomous agents, evaluating the safety of their actions becomes critical.\nPrior safety benchmarks have primarily focused on preventing generation of\nharmful content, such as toxic text. However, they overlook the challenge of\nagents taking harmful actions when the most effective path to an operational\ngoal conflicts with human safety. To address this gap, we introduce\nManagerBench, a benchmark that evaluates LLM decision-making in realistic,\nhuman-validated managerial scenarios. Each scenario forces a choice between a\npragmatic but harmful action that achieves an operational goal, and a safe\naction that leads to worse operational performance. A parallel control set,\nwhere potential harm is directed only at inanimate objects, measures a model's\npragmatism and identifies its tendency to be overly safe. Our findings indicate\nthat the frontier LLMs perform poorly when navigating this safety-pragmatism\ntrade-off. Many consistently choose harmful options to advance their\noperational goals, while others avoid harm only to become overly safe and\nineffective. Critically, we find this misalignment does not stem from an\ninability to perceive harm, as models' harm assessments align with human\njudgments, but from flawed prioritization. ManagerBench is a challenging\nbenchmark for a core component of agentic behavior: making safe choices when\noperational goals and alignment values incentivize conflicting actions.\nBenchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86ManagerBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u73b0\u5b9e\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u5b89\u5168\u51b3\u7b56\u80fd\u529b\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u64cd\u4f5c\u76ee\u6807\u4e0e\u5b89\u5168\u4ef7\u503c\u51b2\u7a81\u65f6\u7684\u9009\u62e9\u503e\u5411\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6709\u5bb3\u5185\u5bb9\u751f\u6210\uff0c\u4f46\u5ffd\u89c6\u4e86\u667a\u80fd\u4f53\u5728\u8ffd\u6c42\u64cd\u4f5c\u76ee\u6807\u65f6\u53ef\u80fd\u91c7\u53d6\u6709\u5bb3\u884c\u52a8\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u6700\u6709\u6548\u7684\u8def\u5f84\u4e0e\u4eba\u7c7b\u5b89\u5168\u76f8\u51b2\u7a81\u65f6\u3002", "method": "\u521b\u5efa\u5305\u542b\u4eba\u7c7b\u9a8c\u8bc1\u7684\u7ba1\u7406\u573a\u666f\uff0c\u6bcf\u4e2a\u573a\u666f\u90fd\u8feb\u4f7f\u5728\u5b9e\u7528\u4f46\u6709\u5bb3\u7684\u884c\u52a8\u4e0e\u5b89\u5168\u4f46\u64cd\u4f5c\u6027\u80fd\u8f83\u5dee\u7684\u884c\u52a8\u4e4b\u95f4\u505a\u51fa\u9009\u62e9\uff0c\u5e76\u8bbe\u7f6e\u5e73\u884c\u63a7\u5236\u7ec4\u6765\u6d4b\u91cf\u6a21\u578b\u7684\u5b9e\u7528\u4e3b\u4e49\u503e\u5411\u3002", "result": "\u524d\u6cbfLLM\u5728\u5b89\u5168-\u5b9e\u7528\u6743\u8861\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u8bb8\u591a\u6a21\u578b\u4e3a\u63a8\u8fdb\u64cd\u4f5c\u76ee\u6807\u800c\u6301\u7eed\u9009\u62e9\u6709\u5bb3\u9009\u9879\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u4e3a\u907f\u514d\u4f24\u5bb3\u53d8\u5f97\u8fc7\u5ea6\u5b89\u5168\u4e14\u65e0\u6548\u3002", "conclusion": "\u6a21\u578b\u7684\u5bf9\u9f50\u95ee\u9898\u4e0d\u662f\u7531\u4e8e\u65e0\u6cd5\u611f\u77e5\u4f24\u5bb3\uff0c\u800c\u662f\u6e90\u4e8e\u9519\u8bef\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\uff0cManagerBench\u662f\u8bc4\u4f30\u667a\u80fd\u4f53\u6838\u5fc3\u884c\u4e3a\u7ec4\u4ef6\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2510.00347", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.00347", "abs": "https://arxiv.org/abs/2510.00347", "authors": ["Huitao Yang", "Guanting Chen"], "title": "In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks", "comment": null, "summary": "As large language models (LLMs) continue to grow in capability, there is\nincreasing interest in incorporating them into decision-making tasks. A common\npipeline for this is Decision-Pretrained Transformers (DPTs). However, existing\ntraining methods for DPTs often struggle to generalize beyond their pretraining\ndata distribution. To explore mitigation of this limitation, we propose\nin-context curiosity -- a lightweight, exploration-inspired regularizer for\noffline pretraining -- and introduce the Prediction-Powered Transformer (PPT)\nframework. PPT augments DPT with an auxiliary reward predictor, using\nprediction error as an intrinsic curiosity signal to encourage broader\nexploration during training. In proof-of-concept experiments on Gaussian\nmulti-armed bandits, PPT shows improved robustness: it moderates the\nperformance degradation observed in DPT when test environments exhibit higher\nvariance in reward, particularly when pretraining data has limited diversity.\nWhile the quality of offline data remain fundamental, our preliminary results\nsuggest that curiosity-driven pretraining offers a promising direction for\nenhancing out-of-distribution generalization in in-context RL agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u4e0a\u4e0b\u6587\u597d\u5947\u5fc3\uff08in-context curiosity\uff09\u7684\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u9884\u6d4b\u9a71\u52a8\u53d8\u6362\u5668\uff08PPT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u5584\u79bb\u7ebf\u9884\u8bad\u7ec3\u51b3\u7b56\u53d8\u6362\u5668\uff08DPTs\uff09\u5728\u5206\u5e03\u5916\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u51b3\u7b56\u53d8\u6362\u5668\uff08DPTs\uff09\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u8d85\u51fa\u9884\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u7f13\u89e3\u8fd9\u4e00\u5c40\u9650\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86PPT\u6846\u67b6\uff0c\u901a\u8fc7\u6dfb\u52a0\u8f85\u52a9\u5956\u52b1\u9884\u6d4b\u5668\uff0c\u4f7f\u7528\u9884\u6d4b\u8bef\u5dee\u4f5c\u4e3a\u5185\u5728\u597d\u5947\u5fc3\u4fe1\u53f7\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9f13\u52b1\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\u3002", "result": "\u5728\u9ad8\u65af\u591a\u81c2\u8001\u864e\u673a\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u4e2d\uff0cPPT\u663e\u793a\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff1a\u5f53\u6d4b\u8bd5\u73af\u5883\u5956\u52b1\u65b9\u5dee\u8f83\u9ad8\u65f6\uff0cPPT\u80fd\u591f\u7f13\u89e3DPT\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u9884\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u867d\u7136\u79bb\u7ebf\u6570\u636e\u8d28\u91cf\u4ecd\u7136\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u9884\u8bad\u7ec3\u4e3a\u589e\u5f3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00844", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00844", "abs": "https://arxiv.org/abs/2510.00844", "authors": ["Jianhao Chen", "Chenxu Wang", "Gengrui Zhang", "Peng Ye", "Lei Bai", "Wei Hu", "Yuzhong Qu", "Shuyue Hu"], "title": "Learning Compact Representations of LLM Abilities via Item Response Theory", "comment": null, "summary": "Recent years have witnessed a surge in the number of large language models\n(LLMs), yet efficiently managing and utilizing these vast resources remains a\nsignificant challenge. In this work, we explore how to learn compact\nrepresentations of LLM abilities that can facilitate downstream tasks, such as\nmodel routing and performance prediction on new benchmarks. We frame this\nproblem as estimating the probability that a given model will correctly answer\na specific query. Inspired by the item response theory (IRT) in psychometrics,\nwe model this probability as a function of three key factors: (i) the model's\nmulti-skill ability vector, (2) the query's discrimination vector that\nseparates models of differing skills, and (3) the query's difficulty scalar. To\nlearn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network\nthat couples model- and query-level embeddings. Extensive experiments\ndemonstrate that our approach leads to state-of-the-art performance in both\nmodel routing and benchmark accuracy prediction. Moreover, analysis validates\nthat the learned parameters encode meaningful, interpretable information about\nmodel capabilities and query characteristics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba(IRT)\u7684LLM\u80fd\u529b\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u7f51\u7edc(MoE)\u8054\u5408\u5b66\u4e60\u6a21\u578b\u80fd\u529b\u5411\u91cf\u3001\u67e5\u8be2\u533a\u5206\u5ea6\u5411\u91cf\u548c\u96be\u5ea6\u6807\u91cf\uff0c\u5728\u6a21\u578b\u8def\u7531\u548c\u57fa\u51c6\u6d4b\u8bd5\u9884\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u6570\u91cf\u6fc0\u589e\uff0c\u5982\u4f55\u9ad8\u6548\u7ba1\u7406\u548c\u5229\u7528\u8fd9\u4e9b\u5e9e\u5927\u8d44\u6e90\u6210\u4e3a\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u5b66\u4e60\u7d27\u51d1\u7684LLM\u80fd\u529b\u8868\u793a\u6765\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff0c\u5c06\u6a21\u578b\u6b63\u786e\u56de\u7b54\u67e5\u8be2\u7684\u6982\u7387\u5efa\u6a21\u4e3a\u4e09\u4e2a\u56e0\u7d20\u7684\u51fd\u6570\uff1a\u6a21\u578b\u591a\u6280\u80fd\u80fd\u529b\u5411\u91cf\u3001\u67e5\u8be2\u533a\u5206\u5ea6\u5411\u91cf\u548c\u96be\u5ea6\u6807\u91cf\uff0c\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u7f51\u7edc\u8054\u5408\u5b66\u4e60\u8fd9\u4e9b\u53c2\u6570\u3002", "result": "\u5728\u6a21\u578b\u8def\u7531\u548c\u57fa\u51c6\u6d4b\u8bd5\u51c6\u786e\u7387\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b66\u4e60\u5230\u7684\u53c2\u6570\u80fd\u591f\u7f16\u7801\u6709\u610f\u4e49\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u80fd\u529b\u548c\u67e5\u8be2\u7279\u5f81\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60LLM\u7684\u7d27\u51d1\u80fd\u529b\u8868\u793a\uff0c\u4e3a\u6a21\u578b\u7ba1\u7406\u548c\u5229\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u53c2\u6570\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.00931", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00931", "abs": "https://arxiv.org/abs/2510.00931", "authors": ["Ammar Khairi", "Daniel D'souza", "Marzieh Fadaee", "Julia Kreutzer"], "title": "Making, not Taking, the Best of N", "comment": null, "summary": "Obtaining high-quality generations in modern LLMs has largely been framed as\na selection problem: identifying a single winning generation from a diverse\npool of N samples, the Best-of-N (BoN). Yet, this approach is inherently\nzero-sum, discarding diverse and potentially useful information from the pool.\nInstead, we explore a collaborative setup, where all candidates can potentially\ncontribute to the final winning generation. To this end, we propose Fusion-of-N\n(FusioN): a method that uses a general LLM judge to synthesize the most\ninformative elements of each sample into a single final answer. We compare\nFusioN to BoN in two settings, (i) test-time scaling, where we sample and\naggregate from a single model at test-time (ii) synthetic data generation,\nwhere we fuse samples from a pool of diverse teachers to improve a student\nmodel. We extensively benchmark both setups across 11 languages, 3 diverse\ntasks and varying model scales. Across the bench, FusioN consistently\noutperforms BoN showing versatility and robustness both in test-time scaling\nand in downstream gains from synthetic data generation. We also perform\nextensive analysis on FusioN, where it shows surprising strengths and\nrobustness under challenging settings. These results show that we should shift\nhow we think about evaluating and utilizing LLM generations from a monolithic\nmeasure of quality, to embracing their polylithic nature. This shift allows us\nto integrate diverse strengths, unlock latent potential, and achieve\nimprovements that were previously inaccessible through selection alone.", "AI": {"tldr": "\u63d0\u51faFusion-of-N\u65b9\u6cd5\uff0c\u4f7f\u7528\u901a\u7528LLM\u6cd5\u5b98\u5c06\u591a\u4e2a\u6837\u672c\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5143\u7d20\u878d\u5408\u6210\u5355\u4e00\u6700\u7ec8\u7b54\u6848\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684Best-of-N\u9009\u62e9\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u8bed\u8a00\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edfBest-of-N\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u96f6\u548c\u6e38\u620f\uff0c\u4e22\u5f03\u4e86\u591a\u6837\u5316\u548c\u6f5c\u5728\u6709\u7528\u7684\u4fe1\u606f\u3002\u672c\u6587\u63a2\u7d22\u534f\u4f5c\u8bbe\u7f6e\uff0c\u8ba9\u6240\u6709\u5019\u9009\u6837\u672c\u90fd\u80fd\u4e3a\u6700\u7ec8\u751f\u6210\u7ed3\u679c\u505a\u51fa\u8d21\u732e\u3002", "method": "FusioN\u65b9\u6cd5\uff1a\u4f7f\u7528\u901a\u7528LLM\u6cd5\u5b98\u6765\u5408\u6210\u6bcf\u4e2a\u6837\u672c\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5143\u7d20\uff0c\u5f62\u6210\u5355\u4e00\u6700\u7ec8\u7b54\u6848\u3002\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u6bd4\u8f83\uff1a\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08\u4ece\u5355\u4e00\u6a21\u578b\u91c7\u6837\u805a\u5408\uff09\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff08\u878d\u5408\u591a\u6837\u5316\u6559\u5e08\u6c60\u7684\u6837\u672c\u6765\u6539\u8fdb\u5b66\u751f\u6a21\u578b\uff09\u3002", "result": "\u572811\u79cd\u8bed\u8a00\u30013\u79cd\u591a\u6837\u5316\u4efb\u52a1\u548c\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u5e7f\u6cdb\u6d4b\u8bd5\uff0cFusioN\u59cb\u7ec8\u4f18\u4e8eBoN\uff0c\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u4e0b\u6e38\u589e\u76ca\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u591a\u529f\u80fd\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5e94\u8be5\u4ece\u5355\u4e00\u8d28\u91cf\u5ea6\u91cf\u8f6c\u5411\u63a5\u53d7LLM\u751f\u6210\u7684\u591a\u9762\u6027\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6837\u5316\u4f18\u52bf\u3001\u91ca\u653e\u6f5c\u5728\u6f5c\u529b\uff0c\u5b9e\u73b0\u4ec5\u9760\u9009\u62e9\u65e0\u6cd5\u8fbe\u5230\u7684\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2510.00373", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00373", "abs": "https://arxiv.org/abs/2510.00373", "authors": ["Carlo Bosio", "Matteo Guarrera", "Alberto Sangiovanni-Vincentelli", "Mark W. Mueller"], "title": "Combining Large Language Models and Gradient-Free Optimization for Automatic Control Policy Synthesis", "comment": "8 pages, 7 figures", "summary": "Large Language models (LLMs) have shown promise as generators of symbolic\ncontrol policies, producing interpretable program-like representations through\niterative search. However, these models are not capable of separating the\nfunctional structure of a policy from the numerical values it is parametrized\nby, thus making the search process slow and inefficient. We propose a hybrid\napproach that decouples structural synthesis from parameter optimization by\nintroducing an additional optimization layer for local parameter search. In our\nmethod, the numerical parameters of LLM-generated programs are extracted and\noptimized numerically to maximize task performance. With this integration, an\nLLM iterates over the functional structure of programs, while a separate\noptimization loop is used to find a locally optimal set of parameters\naccompanying candidate programs. We evaluate our method on a set of control\ntasks, showing that it achieves higher returns and improved sample efficiency\ncompared to purely LLM-guided search. We show that combining symbolic program\nsynthesis with numerical optimization yields interpretable yet high-performing\npolicies, bridging the gap between language-model-guided design and classical\ncontrol tuning. Our code is available at\nhttps://sites.google.com/berkeley.edu/colmo.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06LLM\u7b26\u53f7\u7a0b\u5e8f\u5408\u6210\u4e0e\u6570\u503c\u53c2\u6570\u4f18\u5316\u5206\u79bb\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u53c2\u6570\u4f18\u5316\u5c42\u6765\u63d0\u9ad8\u63a7\u5236\u7b56\u7565\u751f\u6210\u6548\u7387\u548c\u6027\u80fd", "motivation": "LLM\u5728\u751f\u6210\u7b26\u53f7\u63a7\u5236\u7b56\u7565\u65f6\u65e0\u6cd5\u5206\u79bb\u529f\u80fd\u7ed3\u6784\u548c\u6570\u503c\u53c2\u6570\uff0c\u5bfc\u81f4\u641c\u7d22\u8fc7\u7a0b\u7f13\u6162\u4f4e\u6548", "method": "\u5c06LLM\u751f\u6210\u7a0b\u5e8f\u7684\u6570\u503c\u53c2\u6570\u63d0\u53d6\u51fa\u6765\u8fdb\u884c\u6570\u503c\u4f18\u5316\uff0cLLM\u8fed\u4ee3\u7a0b\u5e8f\u529f\u80fd\u7ed3\u6784\uff0c\u540c\u65f6\u4f7f\u7528\u5355\u72ec\u7684\u4f18\u5316\u5faa\u73af\u5bfb\u627e\u5c40\u90e8\u6700\u4f18\u53c2\u6570", "result": "\u5728\u63a7\u5236\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u56de\u62a5\u548c\u6837\u672c\u6548\u7387\uff0c\u76f8\u6bd4\u7eafLLM\u5f15\u5bfc\u641c\u7d22\u6709\u663e\u8457\u6539\u8fdb", "conclusion": "\u7ed3\u5408\u7b26\u53f7\u7a0b\u5e8f\u5408\u6210\u4e0e\u6570\u503c\u4f18\u5316\u53ef\u4ee5\u4ea7\u751f\u53ef\u89e3\u91ca\u4e14\u9ad8\u6027\u80fd\u7684\u7b56\u7565\uff0c\u5f25\u5408\u4e86\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u8bbe\u8ba1\u4e0e\u7ecf\u5178\u63a7\u5236\u8c03\u4f18\u4e4b\u95f4\u7684\u5dee\u8ddd", "topic": "agentic reinforcement learning"}}
{"id": "2510.00922", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00922", "abs": "https://arxiv.org/abs/2510.00922", "authors": ["Shashank Reddy Chirra", "Jayden Teoh", "Praveen Paruchuri", "Pradeep Varakantham"], "title": "On Discovering Algorithms for Adversarial Imitation Learning", "comment": null, "summary": "Adversarial Imitation Learning (AIL) methods, while effective in settings\nwith limited expert demonstrations, are often considered unstable. These\napproaches typically decompose into two components: Density Ratio (DR)\nestimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the\nrelative occupancy of state-action pairs under the policy versus the expert;\nand Reward Assignment (RA), where this ratio is transformed into a reward\nsignal used to train the policy. While significant research has focused on\nimproving density estimation, the role of reward assignment in influencing\ntraining dynamics and final policy performance has been largely overlooked. RA\nfunctions in AIL are typically derived from divergence minimization objectives,\nrelying heavily on human design and ingenuity. In this work, we take a\ndifferent approach: we investigate the discovery of data-driven RA functions,\ni.e, based directly on the performance of the resulting imitation policy. To\nthis end, we leverage an LLM-guided evolutionary framework that efficiently\nexplores the space of RA functions, yielding \\emph{Discovered Adversarial\nImitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,\nDAIL generalises across unseen environments and policy optimization algorithms,\noutperforming the current state-of-the-art of \\emph{human-designed} baselines.\nFinally, we analyse why DAIL leads to more stable training, offering novel\ninsights into the role of RA functions in the stability of AIL. Code is\npublicly available: https://github.com/shshnkreddy/DAIL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DAIL\u7b97\u6cd5\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u8fdb\u5316\u6846\u67b6\u81ea\u52a8\u53d1\u73b0\u6570\u636e\u9a71\u52a8\u7684\u5956\u52b1\u5206\u914d\u51fd\u6570\uff0c\u6539\u8fdb\u4e86\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60(AIL)\u65b9\u6cd5\u867d\u7136\u6709\u6548\u4f46\u88ab\u8ba4\u4e3a\u4e0d\u7a33\u5b9a\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u800c\u5ffd\u89c6\u4e86\u5956\u52b1\u5206\u914d\u51fd\u6570\u7684\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u7b56\u7565\u6027\u80fd\u7684\u6570\u636e\u9a71\u52a8\u5956\u52b1\u5206\u914d\u51fd\u6570\u3002", "method": "\u4f7f\u7528LLM\u5f15\u5bfc\u7684\u8fdb\u5316\u6846\u67b6\u5728\u5956\u52b1\u5206\u914d\u51fd\u6570\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9ad8\u6548\u63a2\u7d22\uff0c\u5f00\u53d1\u51faDAIL\u7b97\u6cd5\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5143\u5b66\u4e60\u7684AIL\u7b97\u6cd5\u3002", "result": "DAIL\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u548c\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4eba\u5de5\u8bbe\u8ba1\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DAIL\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\uff0c\u5e76\u4e3a\u7406\u89e3\u5956\u52b1\u5206\u914d\u51fd\u6570\u5728AIL\u7a33\u5b9a\u6027\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00967", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.00967", "abs": "https://arxiv.org/abs/2510.00967", "authors": ["Cong Yu", "Valter Uotila", "Shilong Deng", "Qingyuan Wu", "Tuo Shi", "Songlin Jiang", "Lei You", "Bo Zhao"], "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL", "comment": null, "summary": "Designing and optimizing task-specific quantum circuits are crucial to\nleverage the advantage of quantum computing. Recent large language model\n(LLM)-based quantum circuit generation has emerged as a promising automatic\nsolution. However, the fundamental challenges remain unaddressed: (i)\nparameterized quantum gates require precise numerical values for optimal\nperformance, which also depend on multiple aspects, including the number of\nquantum gates, their parameters, and the layout/depth of the circuits. (ii)\nLLMs often generate low-quality or incorrect quantum circuits due to the lack\nof quantum domain-specific knowledge. We propose QUASAR, an agentic\nreinforcement learning (RL) framework for quantum circuits generation and\noptimization based on tool-augmented LLMs. To align the LLM with\nquantum-specific knowledge and improve the generated quantum circuits, QUASAR\ndesigns (i) a quantum circuit verification approach with external quantum\nsimulators and (ii) a sophisticated hierarchical reward mechanism in RL\ntraining. Extensive evaluation shows improvements in both syntax and semantic\nperformance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR\nhas achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,\noutperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several\nsupervised-fine-tuning (SFT)-only and RL-only baselines.", "AI": {"tldr": "QUASAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5de5\u5177\u589e\u5f3aLLM\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u548c\u4f18\u5316\u91cf\u5b50\u7535\u8def\uff0c\u901a\u8fc7\u91cf\u5b50\u6a21\u62df\u5668\u9a8c\u8bc1\u548c\u5206\u5c42\u5956\u52b1\u673a\u5236\u63d0\u9ad8\u7535\u8def\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u7535\u8def\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff1a\u53c2\u6570\u5316\u91cf\u5b50\u95e8\u9700\u8981\u7cbe\u786e\u6570\u503c\u4f18\u5316\uff0c\u4ee5\u53caLLM\u7f3a\u4e4f\u91cf\u5b50\u9886\u57df\u77e5\u8bc6\u5bfc\u81f4\u751f\u6210\u4f4e\u8d28\u91cf\u7535\u8def\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5de5\u5177\u589e\u5f3a\u7684LLM\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u91cf\u5b50\u7535\u8def\u9a8c\u8bc1\u65b9\u6cd5\u548c\u5206\u5c42\u5956\u52b1\u673a\u5236\u3002", "result": "\u57284B\u53c2\u6570LLM\u4e0a\u5b9e\u73b0\u4e8699.31%\u7684Pass@1\u6709\u6548\u6027\u548c100%\u7684Pass@10\u6709\u6548\u6027\uff0c\u4f18\u4e8eGPT-4o\u3001GPT-5\u7b49\u5de5\u4e1a\u7ea7LLM\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "QUASAR\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u91cf\u5b50\u7535\u8def\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u5728\u91cf\u5b50\u7535\u8def\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01152", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01152", "abs": "https://arxiv.org/abs/2510.01152", "authors": ["Mustafa Omer Gul", "Claire Cardie", "Tanya Goyal"], "title": "Pay-Per-Search Models are Abstention Models", "comment": "21 pages, with 10 dedicated to citations and appendix. 9 tables and 9\n  figures. Preprint, under review", "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and\noften hallucinate answers to outside-of-boundary questions. In contrast, humans\nrecognize their limitations and can either seek external help for such\nquestions or abstain. In this paper, we introduce MASH (Modeling Abstention via\nSelective Help-seeking), a training framework that readily extracts abstentions\nfrom LLMs. Our key idea is that any external help-seeking by an LLM, i.e.\nsearch tool use, can serve as a proxy for abstention if the external help\n(search) is appropriately penalized while simultaneously rewarding answer\naccuracy. MASH operationalizes this idea using reinforcement learning with a\npay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show\nthat MASH substantially improves upon the selective help-seeking performance of\nprior efficient search approaches; on multi-hop datasets, MASH improves answer\naccuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf\nabstention -- it can distinguish between unanswerable/answerable questions and\nselectively generate responses for answerable questions -- showcasing behavior\nanalogous to specialized abstention approaches. We emphasize that contrary to\nprior abstention methods, MASH does not require pre-determining knowledge\nboundaries to construct training data. Instead, MASH's abstentions are a\nby-product of training for the auxiliary selective help-seeking task. Overall,\nwe show that MASH training effectively aligns search tool use with parametric\nknowledge, which can be successfully leveraged for making abstention decisions.", "AI": {"tldr": "MASH\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u9009\u62e9\u6027\u6c42\u52a9\u7684\u6846\u67b6\uff0c\u5c06\u5916\u90e8\u641c\u7d22\u4f5c\u4e3a\u5f03\u6743\u7684\u4ee3\u7406\u4fe1\u53f7\uff0c\u5728\u5956\u52b1\u7b54\u6848\u51c6\u786e\u6027\u7684\u540c\u65f6\u60e9\u7f5a\u641c\u7d22\u884c\u4e3a\uff0c\u4ece\u800c\u8ba9LLM\u5b66\u4f1a\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\u5e76\u9009\u62e9\u6027\u5f03\u6743\u3002", "motivation": "LLM\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u5176\u53c2\u6570\u77e5\u8bc6\u8fb9\u754c\uff0c\u7ecf\u5e38\u5bf9\u8d85\u51fa\u77e5\u8bc6\u8303\u56f4\u7684\u95ee\u9898\u4ea7\u751f\u5e7b\u89c9\u56de\u7b54\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u80fd\u591f\u8ba4\u8bc6\u5230\u81ea\u5df1\u7684\u5c40\u9650\u6027\u5e76\u5bfb\u6c42\u5916\u90e8\u5e2e\u52a9\u6216\u5f03\u6743\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u6309\u6b21\u641c\u7d22\u4ed8\u8d39\u7684\u5956\u52b1\u673a\u5236\uff0c\u5728\u5956\u52b1\u7b54\u6848\u51c6\u786e\u6027\u7684\u540c\u65f6\u5bf9\u641c\u7d22\u884c\u4e3a\u8fdb\u884c\u60e9\u7f5a\uff0c\u4ece\u800c\u8bad\u7ec3LLM\u9009\u62e9\u6027\u4f7f\u7528\u641c\u7d22\u5de5\u5177\u3002", "result": "\u5728\u4e09\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578bQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cMASH\u663e\u8457\u63d0\u5347\u4e86\u9009\u62e9\u6027\u6c42\u52a9\u6027\u80fd\uff0c\u5728\u591a\u8df3\u6570\u636e\u96c6\u4e0a\u7b54\u6848\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.6%\uff0c\u5e76\u80fd\u6709\u6548\u533a\u5206\u53ef\u56de\u7b54/\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u3002", "conclusion": "MASH\u8bad\u7ec3\u6709\u6548\u5bf9\u9f50\u4e86\u641c\u7d22\u5de5\u5177\u4f7f\u7528\u4e0e\u53c2\u6570\u77e5\u8bc6\uff0c\u53ef\u4ee5\u6210\u529f\u7528\u4e8e\u505a\u51fa\u5f03\u6743\u51b3\u7b56\uff0c\u4e14\u65e0\u9700\u9884\u5148\u786e\u5b9a\u77e5\u8bc6\u8fb9\u754c\u6765\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01030", "abs": "https://arxiv.org/abs/2510.01030", "authors": ["Zach Studdiford", "Timothy T. Rogers", "Kushin Mukherjee", "Siddharth Suresh"], "title": "Uncovering the Computational Ingredients of Human-Like Representations in LLMs", "comment": "9 pages", "summary": "The ability to translate diverse patterns of inputs into structured patterns\nof behavior has been thought to rest on both humans' and machines' ability to\nlearn robust representations of relevant concepts. The rapid advancement of\ntransformer-based large language models (LLMs) has led to a diversity of\ncomputational ingredients -- architectures, fine tuning methods, and training\ndatasets among others -- but it remains unclear which of these ingredients are\nmost crucial for building models that develop human-like representations.\nFurther, most current LLM benchmarks are not suited to measuring\nrepresentational alignment between humans and models, making benchmark scores\nunreliable for assessing if current LLMs are making progress towards becoming\nuseful cognitive models. We address these limitations by first evaluating a set\nof over 70 models that widely vary in their computational ingredients on a\ntriplet similarity task, a method well established in the cognitive sciences\nfor measuring human conceptual representations, using concepts from the THINGS\ndatabase. Comparing human and model representations, we find that models that\nundergo instruction-finetuning and which have larger dimensionality of\nattention heads are among the most human aligned, while multimodal pretraining\nand parameter size have limited bearing on alignment. Correlations between\nalignment scores and scores on existing benchmarks reveal that while some\nbenchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for\ncapturing representational alignment, no existing benchmark is capable of fully\naccounting for the variance of alignment scores, demonstrating their\ninsufficiency in capturing human-AI alignment. Taken together, our findings\nhelp highlight the computational ingredients most essential for advancing LLMs\ntowards models of human conceptual representation and address a key\nbenchmarking gap in LLM evaluation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8670\u591a\u4e2a\u4e0d\u540c\u67b6\u6784\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6982\u5ff5\u8868\u793a\u4e0a\u4e0e\u4eba\u7c7b\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u53d1\u73b0\u6307\u4ee4\u5fae\u8c03\u548c\u6ce8\u610f\u529b\u5934\u7ef4\u5ea6\u662f\u5f71\u54cd\u5bf9\u9f50\u7684\u5173\u952e\u56e0\u7d20\uff0c\u800c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u548c\u53c2\u6570\u91cf\u5f71\u54cd\u6709\u9650\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u8861\u91cf\u4eba\u673a\u5bf9\u9f50\u7a0b\u5ea6\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u57fa\u51c6\u4e0d\u9002\u5408\u8861\u91cf\u4eba\u7c7b\u4e0e\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u793a\u5bf9\u9f50\uff0c\u9700\u8981\u786e\u5b9a\u54ea\u4e9b\u8ba1\u7b97\u8981\u7d20\u5bf9\u6784\u5efa\u5177\u6709\u4eba\u7c7b\u7c7b\u4f3c\u8868\u793a\u80fd\u529b\u7684\u6a21\u578b\u6700\u4e3a\u5173\u952e\u3002", "method": "\u4f7f\u7528\u8ba4\u77e5\u79d1\u5b66\u4e2d\u6210\u719f\u7684\u4e09\u5143\u7ec4\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u57fa\u4e8eTHINGS\u6570\u636e\u5e93\u7684\u6982\u5ff5\uff0c\u8bc4\u4f3070\u591a\u4e2a\u4e0d\u540c\u8ba1\u7b97\u8981\u7d20\uff08\u67b6\u6784\u3001\u5fae\u8c03\u65b9\u6cd5\u3001\u8bad\u7ec3\u6570\u636e\u7b49\uff09\u7684\u6a21\u578b\uff0c\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u6a21\u578b\u7684\u8868\u793a\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u6307\u4ee4\u5fae\u8c03\u7684\u6a21\u578b\u548c\u5177\u6709\u66f4\u5927\u6ce8\u610f\u529b\u5934\u7ef4\u5ea6\u7684\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u793a\u6700\u5bf9\u9f50\uff1b\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u548c\u53c2\u6570\u91cf\u5bf9\u5bf9\u9f50\u5f71\u54cd\u6709\u9650\uff1b\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MMLU\uff09\u53ea\u80fd\u90e8\u5206\u6355\u6349\u8868\u793a\u5bf9\u9f50\uff0c\u65e0\u6cd5\u5b8c\u5168\u89e3\u91ca\u5bf9\u9f50\u5206\u6570\u7684\u65b9\u5dee\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u548c\u6ce8\u610f\u529b\u5934\u7ef4\u5ea6\u662f\u63a8\u8fdbLLM\u6210\u4e3a\u4eba\u7c7b\u6982\u5ff5\u8868\u793a\u6a21\u578b\u7684\u5173\u952e\u8ba1\u7b97\u8981\u7d20\uff0c\u5f53\u524dLLM\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u91cd\u8981\u7f3a\u9677\uff0c\u65e0\u6cd5\u5145\u5206\u8861\u91cf\u4eba\u673a\u5bf9\u9f50\u3002", "topic": "agent analysis"}}
{"id": "2510.01157", "categories": ["cs.CL", "cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.01157", "abs": "https://arxiv.org/abs/2510.01157", "authors": ["Alexandrine Fortier", "Thomas Thebaud", "Jes\u00fas Villalba", "Najim Dehak", "Patrick Cardinal"], "title": "Backdoor Attacks Against Speech Language Models", "comment": null, "summary": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u9488\u5bf9\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u9891\u540e\u95e8\u653b\u51fb\uff0c\u5728\u56db\u4e2a\u8bed\u97f3\u7f16\u7801\u5668\u548c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u653b\u51fb\u6709\u6548\u6027\uff0c\u8986\u76d6ASR\u3001\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u3001\u6027\u522b\u548c\u5e74\u9f84\u9884\u6d4b\u56db\u4e2a\u4efb\u52a1\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe90.76%-99.41%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ca\u5176\u591a\u6a21\u6001\u6269\u5c55\u65e5\u76ca\u6d41\u884c\uff0c\u4f46\u7ea7\u8054\u9886\u57df\u7279\u5b9a\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u4f7f\u6a21\u578b\u7ee7\u627f\u4e86\u6240\u6709\u7ec4\u4ef6\u7684\u6f0f\u6d1e\uff0c\u9700\u8981\u7814\u7a76\u97f3\u9891\u540e\u95e8\u653b\u51fb\u7684\u5a01\u80c1\u3002", "method": "\u5bf9\u56db\u4e2a\u8bed\u97f3\u7f16\u7801\u5668\u548c\u4e09\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u97f3\u9891\u540e\u95e8\u653b\u51fb\u5b9e\u9a8c\uff0c\u6db5\u76d6\u56db\u4e2a\u4efb\u52a1\uff1b\u8fdb\u884c\u7ec4\u4ef6\u7ea7\u5206\u6790\u8bc6\u522b\u6700\u8106\u5f31\u73af\u8282\uff1b\u63d0\u51fa\u57fa\u4e8e\u5fae\u8c03\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u653b\u51fb\u5728\u56db\u4e2a\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u9ad8\u6210\u529f\u7387\uff0890.76%-99.41%\uff09\uff0c\u7ec4\u4ef6\u5206\u6790\u63ed\u793a\u4e86\u540e\u95e8\u4f20\u64ad\u673a\u5236\u3002", "conclusion": "\u591a\u6a21\u6001\u8bed\u97f3\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u540e\u95e8\u653b\u51fb\u98ce\u9669\uff0c\u63d0\u51fa\u7684\u5fae\u8c03\u9632\u5fa1\u80fd\u6709\u6548\u7f13\u89e3\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4e2d\u6bd2\u5a01\u80c1\u3002", "topic": "agent analysis"}}
{"id": "2510.01164", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01164", "abs": "https://arxiv.org/abs/2510.01164", "authors": ["Zhengliang Shi", "Ruotian Ma", "Jen-tse Huang", "Xinbei Ma", "Xingyu Chen", "Mengru Wang", "Qu Yang", "Yue Wang", "Fanghua Ye", "Ziyang Chen", "Shanyi Wang", "Cixing Li", "Wenxuan Wang", "Zhaopeng Tu", "Xiaolong Li", "Zhaochun Ren", "Linus"], "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare", "comment": null, "summary": "Large language models (LLMs) are increasingly entrusted with high-stakes\ndecisions that affect human welfare. However, the principles and values that\nguide these models when distributing scarce societal resources remain largely\nunexamined. To address this, we introduce the Social Welfare Function (SWF)\nBenchmark, a dynamic simulation environment where an LLM acts as a sovereign\nallocator, distributing tasks to a heterogeneous community of recipients. The\nbenchmark is designed to create a persistent trade-off between maximizing\ncollective efficiency (measured by Return on Investment) and ensuring\ndistributive fairness (measured by the Gini coefficient). We evaluate 20\nstate-of-the-art LLMs and present the first leaderboard for social welfare\nallocation. Our findings reveal three key insights: (i) A model's general\nconversational ability, as measured by popular leaderboards, is a poor\npredictor of its allocation skill. (ii) Most LLMs exhibit a strong default\nutilitarian orientation, prioritizing group productivity at the expense of\nsevere inequality. (iii) Allocation strategies are highly vulnerable, easily\nperturbed by output-length constraints and social-influence framing. These\nresults highlight the risks of deploying current LLMs as societal\ndecision-makers and underscore the need for specialized benchmarks and targeted\nalignment for AI governance.", "AI": {"tldr": "SWF Benchmark\u8bc4\u4f30LLMs\u5728\u793e\u4f1a\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u5177\u6709\u529f\u5229\u4e3b\u4e49\u503e\u5411\uff0c\u4f18\u5148\u8003\u8651\u96c6\u4f53\u6548\u7387\u800c\u975e\u516c\u5e73\u5206\u914d\uff0c\u4e14\u5206\u914d\u7b56\u7565\u6613\u53d7\u5916\u90e8\u56e0\u7d20\u5f71\u54cd\u3002", "motivation": "\u968f\u7740LLMs\u5728\u5f71\u54cd\u4eba\u7c7b\u798f\u7949\u7684\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u4f5c\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u5206\u914d\u7a00\u7f3a\u793e\u4f1a\u8d44\u6e90\u65f6\u7684\u539f\u5219\u548c\u4ef7\u503c\u89c2\u3002", "method": "\u5f15\u5165\u793e\u4f1a\u798f\u7949\u51fd\u6570\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u52a8\u6001\u6a21\u62df\u73af\u5883\uff0cLLM\u4f5c\u4e3a\u4e3b\u6743\u5206\u914d\u8005\u5411\u5f02\u8d28\u793e\u533a\u5206\u914d\u4efb\u52a1\uff0c\u5728\u96c6\u4f53\u6548\u7387\u548c\u5206\u914d\u516c\u5e73\u4e4b\u95f4\u521b\u9020\u6301\u4e45\u6743\u8861\u3002", "result": "\u8bc4\u4f3020\u4e2a\u6700\u5148\u8fdbLLM\u53d1\u73b0\uff1a\u901a\u7528\u5bf9\u8bdd\u80fd\u529b\u4e0d\u80fd\u9884\u6d4b\u5206\u914d\u6280\u80fd\uff1b\u5927\u591a\u6570LLM\u5177\u6709\u5f3a\u70c8\u529f\u5229\u4e3b\u4e49\u503e\u5411\uff1b\u5206\u914d\u7b56\u7565\u6613\u53d7\u8f93\u51fa\u957f\u5ea6\u9650\u5236\u548c\u793e\u4f1a\u5f71\u54cd\u6846\u67b6\u7684\u5e72\u6270\u3002", "conclusion": "\u5f53\u524dLLMs\u4f5c\u4e3a\u793e\u4f1a\u51b3\u7b56\u8005\u5b58\u5728\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u548c\u9488\u5bf9\u6027\u5bf9\u9f50\u6765\u786e\u4fddAI\u6cbb\u7406\u3002", "topic": "swe benchmark"}}
{"id": "2510.01165", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01165", "abs": "https://arxiv.org/abs/2510.01165", "authors": ["Oussama Gabouj", "Kamel Charaf", "Ivan Zakazov", "Nicolas Baldwin", "Robert West"], "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning", "comment": "EMNLP 2025 (findings)", "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject.", "AI": {"tldr": "\u63d0\u51fa\u4e86GRAD\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3LLM\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u8f93\u5165\u7684\u7b80\u6d01\u6f14\u793a\uff0c\u5728\u9884\u7b97\u9650\u5236\u4e0b\u4f18\u4e8e\u4f20\u7edfRAG\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u548cSTEM\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6570\u636e\u5e93\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u4e14\u53ef\u80fd\u63d0\u4f9b\u4e0d\u76f8\u5173\u7684\u6f14\u793a\uff0c\u9650\u5236\u4e86LLM\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u8bad\u7ec3LLM\u6a21\u578b\u751f\u6210\u8f93\u5165\u7279\u5b9a\u7684\u7b80\u6d01\u6f14\u793a\uff0c\u4e3a\u6bcf\u4e2a\u8f93\u5165\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u4e0a\u4e0b\u6587\u652f\u6301\u3002", "result": "\u5728\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684GRAD\u5728Qwen2.5-14B\u4e0a\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548cSTEM\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u7269\u7406\u3001\u5316\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u7b49OOD\u9886\u57df\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GRAD\u5f15\u5165\u4e86\u53ef\u6269\u5c55\u7684\u6f14\u793a\u751f\u6210\u5668\u6a21\u578b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u52a8\u6001\u5c11\u6837\u672c\u5b66\u4e60\u8303\u5f0f\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2510.01088", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01088", "abs": "https://arxiv.org/abs/2510.01088", "authors": ["Guobin Shen", "Dongcheng Zhao", "Haibo Tong", "Jindong Li", "Feifei Zhao", "Yi Zeng"], "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense", "comment": null, "summary": "Ensuring Large Language Model (LLM) safety remains challenging due to the\nabsence of universal standards and reliable content validators, making it\ndifficult to obtain effective training signals. We discover that aligned models\nalready possess robust internal safety beliefs: they consistently produce\nhigh-confidence refusals to harmful requests while exhibiting high entropy when\ngenerating potentially dangerous content. This entropy gap reveals an untapped\nsignal--models intrinsically \"know\" when to refuse. We introduce Safety\nInstincts Reinforcement Learning (SIRL), which transforms this internal\nconfidence into a self-generated reward signal, eliminating dependence on\nexternal validators or human annotations. SIRL teaches models to trust their\nsafety instincts by reinforcing low-entropy refusal behaviors. Evaluated on\nLlama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against\n20+ jailbreak methods, from static prompts to adaptive attacks. Using only\n15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods\nwhile preserving performance on mathematics, coding, and conversation\nbenchmarks. Our work demonstrates that effective alignment can emerge from\nwithin, paving the way for more autonomous and robust AI safety mechanisms that\nscale without extensive human oversight.", "AI": {"tldr": "\u63d0\u51faSIRL\u65b9\u6cd5\uff0c\u5229\u7528LLM\u5185\u90e8\u7684\u5b89\u5168\u4fe1\u5ff5\u4f5c\u4e3a\u81ea\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u5373\u53ef\u5f3a\u5316\u6a21\u578b\u7684\u5b89\u5168\u62d2\u7edd\u884c\u4e3a\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u901a\u7528\u6807\u51c6\u548c\u53ef\u9760\u7684\u5185\u5bb9\u9a8c\u8bc1\u5668\uff0c\u786e\u4fddLLM\u5b89\u5168\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u53d1\u73b0\u5bf9\u9f50\u6a21\u578b\u5df2\u5177\u5907\u5f3a\u5927\u7684\u5185\u90e8\u5b89\u5168\u4fe1\u5ff5\uff0c\u4f46\u8fd9\u4e00\u4fe1\u53f7\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "SIRL\uff08\u5b89\u5168\u672c\u80fd\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5c06\u6a21\u578b\u5185\u90e8\u7f6e\u4fe1\u5ea6\u8f6c\u5316\u4e3a\u81ea\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u5f3a\u5316\u4f4e\u71b5\u62d2\u7edd\u884c\u4e3a\u6765\u6559\u5bfc\u6a21\u578b\u4fe1\u4efb\u5176\u5b89\u5168\u672c\u80fd\u3002", "result": "\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cSIRL\u5bf920+\u79cd\u8d8a\u72f1\u65b9\u6cd5\u4fdd\u630189%+\u7684\u9632\u5fa1\u6210\u529f\u7387\uff0c\u4ec5\u4f7f\u752815,000\u4e2a\u672a\u6807\u6ce8\u63d0\u793a\u5c31\u8d85\u8d8a\u8d44\u6e90\u5bc6\u96c6\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u5bf9\u8bdd\u57fa\u51c6\u6027\u80fd\u3002", "conclusion": "\u6709\u6548\u5bf9\u9f50\u53ef\u4ee5\u4ece\u6a21\u578b\u5185\u90e8\u4ea7\u751f\uff0c\u4e3a\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u76d1\u7763\u7684\u81ea\u4e3b\u3001\u9c81\u68d2AI\u5b89\u5168\u673a\u5236\u94fa\u5e73\u9053\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01171", "abs": "https://arxiv.org/abs/2510.01171", "authors": ["Jiayi Zhang", "Simon Yu", "Derek Chong", "Anthony Sicilia", "Michael R. Tomz", "Christopher D. Manning", "Weiyan Shi"], "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "comment": "82 pages, 26 figures, 34 tables. Code is available at\n  https://github.com/CHATS-lab/verbalize-sampling", "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u540e\u8bad\u7ec3\u5bf9\u9f50\u5bfc\u81f4LLM\u591a\u6837\u6027\u51cf\u5c11\uff08\u6a21\u5f0f\u5d29\u6e83\uff09\u7684\u6839\u672c\u539f\u56e0\u662f\u504f\u597d\u6570\u636e\u4e2d\u7684\u5178\u578b\u6027\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u7b56\u7565Verbalized Sampling\u6765\u89c4\u907f\u6a21\u5f0f\u5d29\u6e83\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\u3002", "motivation": "\u540e\u8bad\u7ec3\u5bf9\u9f50\u7ecf\u5e38\u51cf\u5c11LLM\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\u73b0\u8c61\u3002\u4e0e\u4e4b\u524d\u5c06\u8fd9\u79cd\u6548\u5e94\u5f52\u56e0\u4e8e\u7b97\u6cd5\u9650\u5236\u7684\u5de5\u4f5c\u4e0d\u540c\uff0c\u672c\u6587\u8bc6\u522b\u4e86\u4e00\u4e2a\u6839\u672c\u7684\u3001\u666e\u904d\u5b58\u5728\u7684\u6570\u636e\u5c42\u9762\u9a71\u52a8\u56e0\u7d20\uff1a\u504f\u597d\u6570\u636e\u4e2d\u7684\u5178\u578b\u6027\u504f\u5dee\u3002", "method": "\u5f15\u5165Verbalized Sampling\uff08VS\uff09\uff0c\u4e00\u79cd\u7b80\u5355\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u7b56\u7565\uff0c\u8ba9\u6a21\u578b\u53e3\u5934\u8868\u8fbe\u4e00\u7ec4\u54cd\u5e94\u7684\u6982\u7387\u5206\u5e03\uff08\u4f8b\u5982\u201c\u751f\u62105\u4e2a\u5173\u4e8e\u5496\u5561\u7684\u7b11\u8bdd\u53ca\u5176\u5bf9\u5e94\u6982\u7387\u201d\uff09\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u663e\u793a\uff0cVS\u5728\u521b\u610f\u5199\u4f5c\uff08\u8bd7\u6b4c\u3001\u6545\u4e8b\u3001\u7b11\u8bdd\uff09\u3001\u5bf9\u8bdd\u6a21\u62df\u3001\u5f00\u653e\u5f0f\u95ee\u7b54\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u9762\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u591a\u6837\u6027\u589e\u52a01.6-2.1\u500d\uff0c\u4e14\u4e0d\u727a\u7272\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u4eceVS\u4e2d\u83b7\u76ca\u66f4\u591a\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u6a21\u5f0f\u5d29\u6e83\u7684\u65b0\u6570\u636e\u4e2d\u5fc3\u89c6\u89d2\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5b9e\u7528\u7684\u63a8\u7406\u65f6\u8865\u6551\u63aa\u65bd\uff0c\u6709\u52a9\u4e8e\u91ca\u653e\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u591a\u6837\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.01115", "categories": ["cs.AI", "cs.MA", "econ.TH", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.01115", "abs": "https://arxiv.org/abs/2510.01115", "authors": ["Evan Heus", "Rick Bookstaber", "Dhruv Sharma"], "title": "Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis", "comment": "7 pages, 3 figures", "summary": "Large Language Models (LLMs) struggle with the complex, multi-modal, and\nnetwork-native data underlying financial risk. Standard Retrieval-Augmented\nGeneration (RAG) oversimplifies relationships, while specialist models are\ncostly and static. We address this gap with an LLM-centric agent framework for\nsupply chain risk analysis. Our core contribution is to exploit the inherent\nduality between networks and knowledge graphs (KG). We treat the supply chain\nnetwork as a KG, allowing us to use structural network science principles for\nretrieval. A graph traverser, guided by network centrality scores, efficiently\nextracts the most economically salient risk paths. An agentic architecture\norchestrates this graph retrieval alongside data from numerical factor tables\nand news streams. Crucially, it employs novel ``context shells'' -- descriptive\ntemplates that embed raw figures in natural language -- to make quantitative\ndata fully intelligible to the LLM. This lightweight approach enables the model\nto generate concise, explainable, and context-rich risk narratives in real-time\nwithout costly fine-tuning or a dedicated graph database.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4f9b\u5e94\u94fe\u98ce\u9669\u5206\u6790\uff0c\u901a\u8fc7\u5c06\u4f9b\u5e94\u94fe\u7f51\u7edc\u89c6\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff0c\u5229\u7528\u7f51\u7edc\u4e2d\u5fc3\u6027\u8bc4\u5206\u6307\u5bfc\u56fe\u904d\u5386\uff0c\u63d0\u53d6\u7ecf\u6d4e\u663e\u8457\u7684\u98ce\u9669\u8def\u5f84\uff0c\u5e76\u7ed3\u5408\u6570\u503c\u56e0\u5b50\u8868\u548c\u65b0\u95fb\u6d41\u6570\u636e\u751f\u6210\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u53d9\u8ff0\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u5904\u7406\u590d\u6742\u3001\u591a\u6a21\u6001\u91d1\u878d\u98ce\u9669\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u6807\u51c6RAG\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\u5173\u7cfb\uff0c\u800c\u4e13\u4e1a\u6a21\u578b\u6210\u672c\u9ad8\u4e14\u9759\u6001\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5b9e\u65f6\u7684\u98ce\u9669\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u5c06\u4f9b\u5e94\u94fe\u7f51\u7edc\u5efa\u6a21\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f7f\u7528\u7f51\u7edc\u4e2d\u5fc3\u6027\u8bc4\u5206\u6307\u5bfc\u56fe\u904d\u5386\u5668\u63d0\u53d6\u5173\u952e\u98ce\u9669\u8def\u5f84\uff0c\u91c7\u7528\u667a\u80fd\u4f53\u67b6\u6784\u534f\u8c03\u56fe\u68c0\u7d22\u3001\u6570\u503c\u56e0\u5b50\u8868\u548c\u65b0\u95fb\u6d41\u6570\u636e\uff0c\u4f7f\u7528\"\u4e0a\u4e0b\u6587\u5916\u58f3\"\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u7b80\u6d01\u3001\u53ef\u89e3\u91ca\u4e14\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u98ce\u9669\u53d9\u8ff0\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5fae\u8c03\u6216\u4e13\u7528\u56fe\u6570\u636e\u5e93\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u91d1\u878d\u98ce\u9669\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u667a\u80fd\u4f53\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u5206\u6790\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.00430", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.00430", "abs": "https://arxiv.org/abs/2510.00430", "authors": ["Suhyeon Lee", "Jong Chul Ye"], "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment", "comment": "23 pages, 15 figures", "summary": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of\ndiffusion models often struggles with generalization, composability, and\nrobustness against reward hacking. Recent studies have explored prompt\nrefinement as a modular alternative, but most adopt a feed-forward approach\nthat applies a single refined prompt throughout the entire sampling trajectory,\nthereby failing to fully leverage the sequential nature of reinforcement\nlearning. To address this, here we introduce PromptLoop, a plug-and-play RL\nframework that incorporates latent feedback into step-wise prompt refinement.\nRather than modifying diffusion model weights, a multimodal large language\nmodel (MLLM) is trained with RL to iteratively update prompts based on\nintermediate latent states of diffusion models. This design achieves a\nstructural analogy to the Diffusion RL approach, while retaining the\nflexibility and generality of prompt-based alignment. Extensive experiments\nacross diverse reward functions and diffusion backbones demonstrate that\nPromptLoop (i) achieves effective reward optimization, (ii) generalizes\nseamlessly to unseen models, (iii) composes orthogonally with existing\nalignment methods, and (iv) mitigates over-optimization and reward hacking.", "AI": {"tldr": "PromptLoop\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6269\u6563\u6a21\u578b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u66f4\u65b0\u63d0\u793a\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u5956\u52b1\u4f18\u5316\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u7ec4\u5408\u6027\u548c\u6297\u5956\u52b1\u653b\u51fb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5927\u591a\u91c7\u7528\u524d\u5411\u65b9\u5f0f\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u5e8f\u5217\u7279\u6027\u3002", "method": "\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e2d\u95f4\u6f5c\u5728\u72b6\u6001\u8fed\u4ee3\u66f4\u65b0\u63d0\u793a\uff0c\u800c\u4e0d\u662f\u4fee\u6539\u6269\u6563\u6a21\u578b\u6743\u91cd\u3002", "result": "\u5728\u5404\u79cd\u5956\u52b1\u51fd\u6570\u548c\u6269\u6563\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPromptLoop\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5956\u52b1\u4f18\u5316\u3001\u5bf9\u672a\u89c1\u6a21\u578b\u7684\u6cdb\u5316\u3001\u4e0e\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u7684\u6b63\u4ea4\u7ec4\u5408\uff0c\u5e76\u51cf\u8f7b\u4e86\u8fc7\u4f18\u5316\u548c\u5956\u52b1\u653b\u51fb\u95ee\u9898\u3002", "conclusion": "PromptLoop\u901a\u8fc7\u5c06\u6f5c\u5728\u53cd\u9988\u878d\u5165\u9010\u6b65\u63d0\u793a\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u63d0\u793a\u5bf9\u9f50\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0eDiffusion RL\u7c7b\u4f3c\u7684\u7ed3\u6784\u7c7b\u6bd4\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01143", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01143", "abs": "https://arxiv.org/abs/2510.01143", "authors": ["Harry Dong", "David Brandfonbrener", "Eryk Helenowski", "Yun He", "Mrinal Kumar", "Han Fang", "Yuejie Chi", "Karthik Abinav Sankararaman"], "title": "Generalized Parallel Scaling with Interdependent Generations", "comment": null, "summary": "Parallel LLM inference scaling involves sampling a set of $N>1$ responses for\na single input prompt. However, these $N$ parallel responses tend to be\ngenerated independently from each other, partitioning compute resources and\nleaving potentially useful information in one generation untapped by others.\nThis is in contrast to response length scaling where past computation is used\nin all future steps. For higher quality responses and response sets, we propose\nBridge to generate interdependent responses in parallel by rethinking batched\nLLM hidden states as holistic tensors rather than independent slices. With only\na small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean\naccuracy gains from reinforcement learning with verifiable rewards by up to 50%\nand boosts consistency of correct responses. Trained once, Bridge scales to any\ngeneration width, all with greater performance than independent generations,\nunlocking a more general mode of parallel scaling that effectively leverages\ninformation between sequences, compatible with any post-generation aggregation\ntechnique.", "AI": {"tldr": "Bridge\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884cLLM\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u6279\u91cf\u9690\u85cf\u72b6\u6001\u4e3a\u6574\u4f53\u5f20\u91cf\u800c\u975e\u72ec\u7acb\u5207\u7247\uff0c\u8ba9\u5e76\u884c\u751f\u6210\u7684\u591a\u4e2a\u54cd\u5e94\u76f8\u4e92\u4f9d\u8d56\uff0c\u4ece\u800c\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5e76\u884cLLM\u63a8\u7406\u4e2d\uff0c\u591a\u4e2a\u54cd\u5e94\u662f\u72ec\u7acb\u751f\u6210\u7684\uff0c\u8fd9\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u88ab\u5206\u5272\uff0c\u4e14\u4e00\u4e2a\u751f\u6210\u4e2d\u7684\u6709\u7528\u4fe1\u606f\u65e0\u6cd5\u88ab\u5176\u4ed6\u751f\u6210\u5229\u7528\u3002", "method": "Bridge\u65b9\u6cd5\u901a\u8fc7\u5c11\u91cf\u65b0\u53c2\u6570\uff082.8%-5.1%\uff09\u5c06\u6279\u91cfLLM\u9690\u85cf\u72b6\u6001\u91cd\u65b0\u8bbe\u8ba1\u4e3a\u6574\u4f53\u5f20\u91cf\uff0c\u4f7f\u5e76\u884c\u54cd\u5e94\u76f8\u4e92\u4f9d\u8d56\u3002", "result": "Bridge\u5c06\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u76f8\u5bf9\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe50%\uff0c\u5e76\u63d0\u9ad8\u4e86\u6b63\u786e\u54cd\u5e94\u7684\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u4e00\u6b21\u8bad\u7ec3\u5373\u53ef\u6269\u5c55\u5230\u4efb\u4f55\u751f\u6210\u5bbd\u5ea6\u3002", "conclusion": "Bridge\u89e3\u9501\u4e86\u4e00\u79cd\u66f4\u901a\u7528\u7684\u5e76\u884c\u6269\u5c55\u6a21\u5f0f\uff0c\u6709\u6548\u5229\u7528\u5e8f\u5217\u95f4\u7684\u4fe1\u606f\uff0c\u4e0e\u4efb\u4f55\u540e\u751f\u6210\u805a\u5408\u6280\u672f\u517c\u5bb9\u3002", "topic": "agent analysis"}}
{"id": "2510.00494", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00494", "abs": "https://arxiv.org/abs/2510.00494", "authors": ["Julian Coda-Forno", "Zhuokai Zhao", "Qiang Zhang", "Dipesh Tamboli", "Weiwei Li", "Xiangjun Fan", "Lizhu Zhang", "Eric Schulz", "Hsiao-Ping Tseng"], "title": "Exploring System 1 and 2 communication for latent reasoning in LLMs", "comment": null, "summary": "Should LLM reasoning live in a separate module, or within a single model's\nforward pass and representational space? We study dual-architecture latent\nreasoning, where a fluent Base exchanges latent messages with a Coprocessor,\nand test two hypotheses aimed at improving latent communication over Liu et al.\n(2024): (H1) increase channel capacity; (H2) learn communication via joint\nfinetuning. Under matched latent-token budgets on GPT-2 and Qwen-3, H2 is\nconsistently strongest while H1 yields modest gains. A unified soft-embedding\nbaseline, a single model with the same forward pass and shared representations,\nusing the same latent-token budget, nearly matches H2 and surpasses H1,\nsuggesting current dual designs mostly add compute rather than qualitatively\nimproving reasoning. Across GSM8K, ProsQA, and a Countdown stress test with\nincreasing branching factor, scaling the latent-token budget beyond small\nvalues fails to improve robustness. Latent analyses show overlapping subspaces\nwith limited specialization, consistent with weak reasoning gains. We conclude\ndual-model latent reasoning remains promising in principle, but likely requires\nobjectives and communication mechanisms that explicitly shape latent spaces for\nalgorithmic planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53cc\u67b6\u6784\u6f5c\u5728\u63a8\u7406\uff0c\u6bd4\u8f83\u4e86\u5206\u79bb\u6a21\u5757\u4e0e\u7edf\u4e00\u6a21\u578b\u4e24\u79cd\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u8054\u5408\u5fae\u8c03\u6bd4\u589e\u52a0\u4fe1\u9053\u5bb9\u91cf\u66f4\u6709\u6548\uff0c\u4f46\u7edf\u4e00\u8f6f\u5d4c\u5165\u57fa\u7ebf\u8868\u73b0\u63a5\u8fd1\u6700\u4f73\u53cc\u67b6\u6784\u8bbe\u8ba1\uff0c\u8868\u660e\u5f53\u524d\u53cc\u6a21\u578b\u8bbe\u8ba1\u4e3b\u8981\u589e\u52a0\u8ba1\u7b97\u800c\u975e\u663e\u8457\u6539\u5584\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76LLM\u63a8\u7406\u5e94\u8be5\u91c7\u7528\u5206\u79bb\u6a21\u5757\u8fd8\u662f\u7edf\u4e00\u6a21\u578b\u67b6\u6784\uff0c\u63a2\u7d22\u53cc\u67b6\u6784\u6f5c\u5728\u63a8\u7406\u7684\u6709\u6548\u6027\uff0c\u9a8c\u8bc1\u589e\u52a0\u4fe1\u9053\u5bb9\u91cf\u548c\u8054\u5408\u5fae\u8c03\u4e24\u79cd\u5047\u8bbe\u3002", "method": "\u5728GPT-2\u548cQwen-3\u4e0a\u4f7f\u7528\u5339\u914d\u7684\u6f5c\u5728\u4ee4\u724c\u9884\u7b97\uff0c\u6d4b\u8bd5\u4e24\u79cd\u5047\u8bbe\uff1aH1\uff08\u589e\u52a0\u4fe1\u9053\u5bb9\u91cf\uff09\u548cH2\uff08\u901a\u8fc7\u8054\u5408\u5fae\u8c03\u5b66\u4e60\u901a\u4fe1\uff09\u3002\u4e0e\u7edf\u4e00\u8f6f\u5d4c\u5165\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "H2\u8868\u73b0\u6700\u5f3a\uff0cH1\u589e\u76ca\u6709\u9650\u3002\u7edf\u4e00\u8f6f\u5d4c\u5165\u57fa\u7ebf\u51e0\u4e4e\u5339\u914dH2\u5e76\u8d85\u8d8aH1\u3002\u6269\u5927\u6f5c\u5728\u4ee4\u724c\u9884\u7b97\u672a\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\u3002\u6f5c\u5728\u5206\u6790\u663e\u793a\u5b50\u7a7a\u95f4\u91cd\u53e0\u4e14\u4e13\u4e1a\u5316\u6709\u9650\u3002", "conclusion": "\u53cc\u6a21\u578b\u6f5c\u5728\u63a8\u7406\u5728\u7406\u8bba\u4e0a\u4ecd\u6709\u524d\u666f\uff0c\u4f46\u9700\u8981\u80fd\u660e\u786e\u5851\u9020\u6f5c\u5728\u7a7a\u95f4\u7528\u4e8e\u7b97\u6cd5\u89c4\u5212\u7684\u76ee\u6807\u548c\u901a\u4fe1\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2510.01051", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01051", "abs": "https://arxiv.org/abs/2510.01051", "authors": ["Zichen Liu", "Anya Sims", "Keyu Duan", "Changyu Chen", "Simon Yu", "Xiangxin Zhou", "Haotian Xu", "Shaopan Xiong", "Bo Liu", "Chenmien Tan", "Chuen Yang Beh", "Weixun Wang", "Hao Zhu", "Weiyan Shi", "Diyi Yang", "Michael Shieh", "Yee Whye Teh", "Wee Sun Lee", "Min Lin"], "title": "GEM: A Gym for Agentic LLMs", "comment": null, "summary": "The training paradigm for large language models (LLMs) is moving from static\ndatasets to experience-based learning, where agents acquire skills via\ninteracting with complex environments. To facilitate this transition we\nintroduce GEM (General Experience Maker), an open-source environment simulator\ndesigned for the age of LLMs. Analogous to OpenAI-Gym for traditional\nreinforcement learning (RL), GEM provides a standardized framework for the\nenvironment-agent interface, including asynchronous vectorized execution for\nhigh throughput, and flexible wrappers for easy extensibility. GEM also\nfeatures a diverse suite of environments, robust integrated tools, and\nsingle-file example scripts demonstrating using GEM with five popular RL\ntraining frameworks. Along with this, we also provide a set of baselines across\n24 environments using REINFORCE with Return Batch Normalization (ReBN), which\n-- unlike GRPO -- is compatible with the full RL setting of dense per-turn\nrewards and offers better credit assignment. We further conduct apple-to-apple\nbenchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings\nusing GEM to shed light on the algorithmic designs. Lastly, GEM also functions\nas a convenient evaluation toolkit besides a training environment. We hope this\nframework can help accelerate future agentic LLM research.", "AI": {"tldr": "GEM\u662f\u4e00\u4e2a\u5f00\u6e90\u73af\u5883\u6a21\u62df\u5668\uff0c\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u7c7b\u4f3cOpenAI-Gym\u7684\u6807\u51c6\u5316\u6846\u67b6\uff0c\u652f\u6301\u5411\u91cf\u5316\u6267\u884c\u548c\u591a\u79cdRL\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b24\u4e2a\u73af\u5883\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740LLM\u8bad\u7ec3\u8303\u5f0f\u4ece\u9759\u6001\u6570\u636e\u96c6\u8f6c\u5411\u57fa\u4e8e\u7ecf\u9a8c\u7684\u5b66\u4e60\uff0c\u9700\u8981\u6807\u51c6\u5316\u7684\u73af\u5883\u6a21\u62df\u5668\u6765\u4fc3\u8fdb\u4ee3\u7406\u4e0e\u590d\u6742\u73af\u5883\u7684\u4ea4\u4e92\u3002", "method": "\u5f00\u53d1GEM\u6846\u67b6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u73af\u5883-\u4ee3\u7406\u63a5\u53e3\u3001\u5f02\u6b65\u5411\u91cf\u5316\u6267\u884c\u3001\u7075\u6d3b\u5305\u88c5\u5668\uff0c\u5305\u542b\u591a\u6837\u5316\u73af\u5883\u5957\u4ef6\u548c\u96c6\u6210\u5de5\u5177\uff0c\u652f\u6301\u4e94\u79cd\u6d41\u884c\u7684RL\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u572824\u4e2a\u73af\u5883\u4e2d\u5efa\u7acb\u4e86ReBN\u57fa\u51c6\uff0c\u5e76\u5bf9PPO\u3001GRPO\u548cREINFORCE\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u516c\u5e73\u6bd4\u8f83\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u5dee\u5f02\u3002", "conclusion": "GEM\u4f5c\u4e3a\u8bad\u7ec3\u73af\u5883\u548c\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u6709\u671b\u52a0\u901f\u672a\u6765\u4ee3\u7406\u5f0fLLM\u7814\u7a76\u7684\u53d1\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00537", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.00537", "abs": "https://arxiv.org/abs/2510.00537", "authors": ["Nandan Kumar Jha", "Brandon Reagen"], "title": "Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?", "comment": "EMNLP 2025 Main Conference (Long paper)", "summary": "As large language models (LLMs) scale, the question is not only how large\nthey become, but how much of their capacity is effectively utilized. Existing\nscaling laws relate model size to loss, yet overlook how components exploit\ntheir latent space. We study feed-forward networks (FFNs) and recast width\nselection as a spectral utilization problem. Using a lightweight diagnostic\nsuite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral\nConcentration, and the composite Spectral Utilization Index (SUI) -- we\nquantify how many latent directions are meaningfully activated across LLaMA,\nGPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling\nlaw: soft rank follows an almost perfect power law with FFN width, while hard\nrank grows only sublinearly and with high variance. This asymmetry suggests\nthat widening FFNs mostly adds low-energy tail directions, while dominant-mode\nsubspaces saturate early. Moreover, at larger widths, variance further\ncollapses into a narrow subspace, leaving much of the latent space\nunder-utilized. These results recast FFN width selection as a principled\ntrade-off between tail capacity and dominant-mode capacity, offering concrete\nguidance for inference-efficient LLM design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u524d\u9988\u7f51\u7edc(FFN)\u7684\u9891\u8c31\u5229\u7528\u95ee\u9898\uff0c\u53d1\u73b0\u8f6f\u79e9\u968f\u5bbd\u5ea6\u5448\u5b8c\u7f8e\u5e42\u5f8b\u589e\u957f\uff0c\u800c\u786c\u79e9\u4ec5\u6b21\u7ebf\u6027\u589e\u957f\uff0c\u8868\u660eFFN\u589e\u5bbd\u4e3b\u8981\u589e\u52a0\u4f4e\u80fd\u91cf\u5c3e\u65b9\u5411\uff0c\u4e3b\u5bfc\u6a21\u5f0f\u5b50\u7a7a\u95f4\u65e9\u671f\u9971\u548c\u3002", "motivation": "\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u5173\u6ce8\u6a21\u578b\u5927\u5c0f\u4e0e\u635f\u5931\u7684\u5173\u7cfb\uff0c\u4f46\u5ffd\u7565\u4e86\u7ec4\u4ef6\u5982\u4f55\u5229\u7528\u5176\u6f5c\u5728\u7a7a\u95f4\uff0c\u9700\u8981\u7814\u7a76FFN\u7684\u9891\u8c31\u5229\u7528\u6548\u7387\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8bca\u65ad\u5957\u4ef6\uff08\u786c\u79e9\u3001\u8f6f\u79e9\u3001\u9891\u8c31\u96c6\u4e2d\u5ea6\u548c\u9891\u8c31\u5229\u7528\u6307\u6570\uff09\u91cf\u5316LLaMA\u3001GPT-2\u548cnGPT\u5bb6\u65cf\u4e2dFFN\u7684\u6f5c\u5728\u65b9\u5411\u6fc0\u6d3b\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u4e0d\u5bf9\u79f0\u9891\u8c31\u7f29\u653e\u5b9a\u5f8b\uff1a\u8f6f\u79e9\u968fFFN\u5bbd\u5ea6\u5448\u5b8c\u7f8e\u5e42\u5f8b\u589e\u957f\uff0c\u786c\u79e9\u4ec5\u6b21\u7ebf\u6027\u589e\u957f\u4e14\u65b9\u5dee\u5927\uff0c\u8868\u660eFFN\u589e\u5bbd\u4e3b\u8981\u589e\u52a0\u4f4e\u80fd\u91cf\u5c3e\u65b9\u5411\u3002", "conclusion": "FFN\u5bbd\u5ea6\u9009\u62e9\u5e94\u6743\u8861\u5c3e\u5bb9\u91cf\u548c\u4e3b\u5bfc\u6a21\u5f0f\u5bb9\u91cf\uff0c\u4e3a\u63a8\u7406\u9ad8\u6548LLM\u8bbe\u8ba1\u63d0\u4f9b\u5177\u4f53\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2510.01132", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01132", "abs": "https://arxiv.org/abs/2510.01132", "authors": ["Ruiyi Wang", "Prithviraj Ammanabrolu"], "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning", "comment": null, "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u5c06\u8bbe\u8ba1\u7a7a\u95f4\u5206\u89e3\u4e3a\u73af\u5883\u3001\u5956\u52b1\u548c\u7b56\u7565\u4e09\u4e2a\u652f\u67f1\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63d0\u51fa\u4e86\u8bad\u7ec3\u914d\u65b9\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u548c\u5b9a\u4e49\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u4efb\u52a1\u8bbe\u8ba1\u9009\u62e9\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u9700\u8981\u660e\u786e\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\u5728\u5b9e\u9645\u4e2d\u6709\u6548\u3002", "method": "\u5c06\u8bbe\u8ba1\u7a7a\u95f4\u5206\u89e3\u4e3a\u73af\u5883\u3001\u5956\u52b1\u548c\u7b56\u7565\u4e09\u4e2a\u652f\u67f1\uff0c\u5728TextWorld\u3001ALFWorld\u548cSWE-Gym\u7b49\u6587\u672c\u9886\u57df\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u5956\u52b1\u7a00\u758f\u6027\u548c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u7b80\u5355\u73af\u5883\u53ef\u9884\u6d4b\u590d\u6742\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff1b\u5bc6\u96c6\u5956\u52b1\u52a0\u901f\u8bad\u7ec3\u4f46\u6027\u80fd\u7a33\u5b9a\u6027\u4f9d\u8d56RL\u7b97\u6cd5\u9009\u62e9\uff1b\u627e\u5230\u4e86\u6700\u4f18\u7684SFT\u4e0eRL\u8bad\u7ec3\u6bd4\u4f8b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8de8\u4e09\u4e2a\u652f\u67f1\u534f\u540c\u8bbe\u8ba1\u7684\u8bad\u7ec3\u914d\u65b9\uff0c\u4fc3\u8fdb\u591a\u8f6e\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01135", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01135", "abs": "https://arxiv.org/abs/2510.01135", "authors": ["Zhaolin Gao", "Joongwon Kim", "Wen Sun", "Thorsten Joachims", "Sid Wang", "Richard Yuanzhe Pang", "Liang Tan"], "title": "Prompt Curriculum Learning for Efficient LLM Post-Training", "comment": null, "summary": "We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement\nlearning (RL) algorithm that selects intermediate-difficulty prompts using a\nlearned value model to post-train language models. Since post-training LLMs via\nRL remains sensitive to batching and prompt selection strategies, we first\nconduct a series of systematic experiments where we (1) determine the optimal\ntraining batch size that balances generation efficiency and gradient quality\nand (2) establish the importance of focusing on prompts of intermediate\ndifficulty for the policy. We build upon these results to design PCL, which\nidentifies prompts of intermediate difficulty for the current policy in an\non-policy manner by using a value model that is concurrently updated based on\nthe current policy. By focusing on informative prompts that yield high\neffective ratios, PCL achieves either the highest performance or requires\nsignificantly less time to reach comparable performance to its counterparts.\nCompared to rollout-based filtering methods, PCL avoids costly rollouts and\nachieves $12.1\\times$ and $16.9\\times$ faster speed on identifying\nintermediate-difficulty prompts when training on MATH and DeepScaleR,\nrespectively. We further demonstrate that our value model accurately predicts\nprompt difficulty and allows PCL to focus on progressively more challenging\nprompts during RL. Our results present a new methodology that delivers improved\ntradeoff between upper-bound performance and efficiency for reasoning-focused\nRL.", "AI": {"tldr": "\u63d0\u51faPrompt Curriculum Learning (PCL)\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u4ef7\u503c\u6a21\u578b\u9009\u62e9\u4e2d\u7b49\u96be\u5ea6\u63d0\u793a\u6765\u540e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u76f8\u6bd4\u57fa\u4e8erollout\u7684\u8fc7\u6ee4\u65b9\u6cd5\u5b9e\u73b012.1-16.9\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u540e\u8bad\u7ec3LLM\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u6279\u5904\u7406\u548c\u63d0\u793a\u9009\u62e9\u7b56\u7565\u4ecd\u7136\u654f\u611f\uff0c\u9700\u8981\u627e\u5230\u5e73\u8861\u751f\u6210\u6548\u7387\u548c\u68af\u5ea6\u8d28\u91cf\u7684\u6700\u4f73\u8bad\u7ec3\u6279\u5927\u5c0f\uff0c\u5e76\u5173\u6ce8\u5bf9\u7b56\u7565\u5177\u6709\u4e2d\u7b49\u96be\u5ea6\u7684\u63d0\u793a\u3002", "method": "PCL\u4f7f\u7528\u4e0e\u5f53\u524d\u7b56\u7565\u540c\u65f6\u66f4\u65b0\u7684\u4ef7\u503c\u6a21\u578b\uff0c\u4ee5\u5728\u7ebf\u65b9\u5f0f\u8bc6\u522b\u5f53\u524d\u7b56\u7565\u7684\u4e2d\u7b49\u96be\u5ea6\u63d0\u793a\uff0c\u901a\u8fc7\u5173\u6ce8\u4ea7\u751f\u9ad8\u6709\u6548\u6bd4\u7684\u4fe1\u606f\u6027\u63d0\u793a\u6765\u4f18\u5316\u8bad\u7ec3\u3002", "result": "PCL\u5728MATH\u548cDeepScaleR\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b012.1\u500d\u548c16.9\u500d\u7684\u4e2d\u95f4\u96be\u5ea6\u63d0\u793a\u8bc6\u522b\u901f\u5ea6\u63d0\u5347\uff0c\u5728\u63a8\u7406\u805a\u7126\u7684RL\u4e2d\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u6027\u80fd\u4e0e\u6548\u7387\u6743\u8861\u3002", "conclusion": "PCL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u805a\u7126\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u6539\u8fdb\u7684\u4e0a\u9650\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00553", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00553", "abs": "https://arxiv.org/abs/2510.00553", "authors": ["Yuchen Cai", "Ding Cao", "Xin Xu", "Zijun Yao", "Yuqing Huang", "Zhenyu Tan", "Benyi Zhang", "Guiquan Liu", "Junfeng Fang"], "title": "On Predictability of Reinforcement Learning Dynamics for Large Language Models", "comment": "43 pages, 28 figures; 43", "summary": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0RL\u8bad\u7ec3\u4e2dLLM\u53c2\u6570\u66f4\u65b0\u7684\u4e24\u4e2a\u57fa\u672c\u7279\u6027\uff1a\u79e91\u4e3b\u5bfc\u6027\u548c\u79e91\u7ebf\u6027\u52a8\u6001\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51faAlphaRL\u52a0\u901f\u6846\u67b6\uff0c\u5b9e\u73b02.5\u500d\u52a0\u901f\u4e14\u4fdd\u630196%\u4ee5\u4e0a\u6027\u80fd\u3002", "motivation": "\u7406\u89e3RL\u8bad\u7ec3\u4e2dLLM\u53c2\u6570\u52a8\u6001\u53d8\u5316\u7684\u5185\u5728\u673a\u5236\uff0c\u4ee5\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u8bc6\u522bRL\u8bf1\u5bfc\u53c2\u6570\u66f4\u65b0\u7684\u4e24\u4e2a\u5173\u952e\u7279\u6027\uff0c\u5e76\u901a\u8fc78\u4e2aLLM\u548c7\u79cd\u7b97\u6cd5\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u51faAlphaRL\u52a0\u901f\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u53c2\u6570\u66f4\u65b0\u77e9\u9635\u7684\u9876\u7ea7\u5947\u5f02\u5b50\u7a7a\u95f4\u4e3b\u5bfc\u6027\u80fd\u63d0\u5347\uff08\u6062\u590d99%\u4ee5\u4e0a\u589e\u76ca\uff09\uff0c\u4e14\u8be5\u5b50\u7a7a\u95f4\u7ebf\u6027\u6f14\u5316\uff0cAlphaRL\u5b9e\u73b02.5\u500d\u52a0\u901f\u4e14\u6027\u80fd\u635f\u5931\u5c0f\u4e8e4%\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5927\u89c4\u6a21RL\u63d0\u4f9b\u4e86\u901a\u7528\u5b9e\u7528\u5de5\u5177\uff0c\u5f00\u8f9f\u4e86\u9762\u5411LLM\u7684\u539f\u5219\u6027\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u8bad\u7ec3\u8303\u5f0f\u7684\u65b0\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01167", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01167", "abs": "https://arxiv.org/abs/2510.01167", "authors": ["Yiran Shen", "Yu Xia", "Jonathan Chang", "Prithviraj Ammanabrolu"], "title": "Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards", "comment": null, "summary": "Aligning large language models to human preferences is inherently\nmultidimensional, yet most pipelines collapse heterogeneous signals into a\nsingle optimizeable objective. We seek to answer what it would take to\nsimultaneously align a model across various domains spanning those with:\nverifiable rewards (mathematical accuracy), non-verifiable subjective\npreferences (human values), and complex interactive scenarios (multi-turn AI\ntutoring dialogues). Such multi-objective reinforcement learning setups are\noften plagued by the individual objectives being at odds with each other,\nresulting in inefficient training and little user control during inference. We\npropose a unified framework that: (i) standardizes {process reward model} (PRM)\ntraining across both verifiable and non-verifiable settings to better supervise\nmodels' chain-of-thought reasoning; (ii) performs {multi-objective alignment}\nby training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead\n$\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the\nvector correspond to the various objectives instead of a single scalar; and\n(iii) demonstrates how such a system provides fine-grained inference-time user\ncontrol. Experiments across math reasoning, value alignment, and multi-turn\ndialogue show that our framework improves performance across multiple\nobjectives simultaneously, while minimizing cross-objective trade-offs and\nenabling flexible inference time user control. The code can be found at\nhttps://github.com/pearls-lab/multiobj-align.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u76ee\u6807\u5bf9\u9f50\u6846\u67b6\uff0c\u4f7f\u7528\u5411\u91cf\u5316\u5956\u52b1\u548cMAH-DPO\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ef7\u503c\u5bf9\u9f50\u548c\u591a\u8f6e\u5bf9\u8bdd\u7b49\u591a\u4e2a\u9886\u57df\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u63a8\u7406\u65f6\u7ec6\u7c92\u5ea6\u7528\u6237\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u5c06\u5f02\u6784\u4fe1\u53f7\u538b\u7f29\u4e3a\u5355\u4e00\u4f18\u5316\u76ee\u6807\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u3001\u4e3b\u89c2\u504f\u597d\u548c\u590d\u6742\u4ea4\u4e92\u573a\u666f\u5e76\u5b58\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6807\u51c6\u5316\u8bad\u7ec3\uff0c\u63d0\u51faMAH-DPO\u65b9\u6cd5\u8fdb\u884c\u591a\u76ee\u6807\u5bf9\u9f50\uff0c\u91c7\u7528\u5411\u91cf\u5316\u5956\u52b1\u800c\u975e\u5355\u4e00\u6807\u91cf\uff0c\u5b9e\u73b0\u591a\u76ee\u6807\u540c\u65f6\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ef7\u503c\u5bf9\u9f50\u548c\u591a\u8f6e\u5bf9\u8bdd\u7b49\u591a\u4e2a\u76ee\u6807\u4e0a\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u6700\u5c0f\u5316\u8de8\u76ee\u6807\u6743\u8861\uff0c\u5e76\u5b9e\u73b0\u7075\u6d3b\u7684\u63a8\u7406\u65f6\u7528\u6237\u63a7\u5236\u3002", "conclusion": "\u8be5\u591a\u76ee\u6807\u5bf9\u9f50\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u76ee\u6807\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u7528\u6237\u63a7\u5236\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01179", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01179", "abs": "https://arxiv.org/abs/2510.01179", "authors": ["Zhangchen Xu", "Adriana Meza Soria", "Shawn Tan", "Anurag Roy", "Ashish Sunil Agrawal", "Radha Poovendran", "Rameswar Panda"], "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments", "comment": "35 pages, 13 figures", "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86Toucan\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u76ee\u524d\u6700\u5927\u7684\u516c\u5f00\u5de5\u5177\u4ee3\u7406\u6570\u636e\u96c6\uff0c\u5305\u542b150\u4e07\u6761\u8f68\u8ff9\uff0c\u57fa\u4e8e\u8fd1500\u4e2a\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5f00\u6e90\u793e\u533a\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u5de5\u5177\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u5f00\u6e90\u793e\u533a\u5728LLM\u4ee3\u7406\u7cfb\u7edf\u53d1\u5c55\u4e2d\u53d7\u5230\u9ad8\u8d28\u91cf\u8bb8\u53ef\u5de5\u5177\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u9650\u5236\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5728\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u591a\u5de5\u5177\u548c\u591a\u8f6e\u4ea4\u4e92\u65b9\u9762\u3002", "method": "\u5229\u7528\u771f\u5b9eMCP\u73af\u5883\u751f\u6210\u591a\u6837\u5316\u3001\u771f\u5b9e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u8f68\u8ff9\uff0c\u91c7\u7528\u4e94\u7c7b\u6a21\u578b\u751f\u6210\u5de5\u5177\u4f7f\u7528\u67e5\u8be2\uff0c\u8fdb\u884c\u8d28\u91cf\u8fc7\u6ee4\uff0c\u7136\u540e\u4f7f\u7528\u4e09\u4e2a\u6559\u5e08\u6a21\u578b\u548c\u4e24\u4e2a\u4ee3\u7406\u6846\u67b6\u751f\u6210\u4ee3\u7406\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u9a8c\u8bc1\u786e\u4fdd\u8d28\u91cf\u3002", "result": "\u5728Toucan\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u5728BFCL V3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u66f4\u5927\u7684\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u5728MCP-Universe Bench\u4e0a\u63a8\u8fdb\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "Toucan\u6570\u636e\u96c6\u586b\u8865\u4e86\u5de5\u5177\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u8bad\u7ec3\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.01180", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01180", "abs": "https://arxiv.org/abs/2510.01180", "authors": ["Jian Hu", "Mingjie Liu", "Ximing Lu", "Fang Wu", "Zaid Harchaoui", "Shizhe Diao", "Yejin Choi", "Pavlo Molchanov", "Jun Yang", "Jan Kautz", "Yi Dong"], "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration", "comment": "16 pages, 4 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\ningredient for unlocking complex reasoning capabilities in large language\nmodels. Recent work ProRL has shown promise in scaling RL by increasing the\nnumber of training steps. However, performance plateaus after thousands of\nsteps, with clear diminishing returns from allocating more computation to\nadditional training. In this work, we investigate a complementary paradigm for\nscaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to\nexhaustively Broaden exploration, which yields continuous performance gains\nbeyond the saturation point observed in ProRL when scaling the number of\ntraining steps. Our approach is motivated by a mass balance equation analysis\nallowing us to characterize the rate of change in probability mass for correct\nand incorrect tokens during the reinforcement process. We show that under a\none-step RL assumption, sampled rollout tokens always contribute to\ncorrect-mass expansion, while unsampled tokens outside rollouts may lead to\ngains or losses depending on their distribution and the net reward balance.\nImportantly, as the number of rollouts per example N increases, the effect of\nunsampled terms diminishes, ensuring overall correct-mass expansion. To\nvalidate our theoretical analysis, we conduct simulations under more relaxed\nconditions and find that a sufficiently large rollout size N-corresponding to\nample exploration-guarantees an increase in the probability mass of all correct\ntokens. Empirically, BroRL revives models saturated after 3K ProRL training\nsteps and demonstrates robust, continuous improvement, achieving\nstate-of-the-art results for the 1.5B model across diverse benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBroRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u7684rollout\u6570\u91cf\u6765\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86ProRL\u65b9\u6cd5\u5728\u6570\u5343\u6b65\u8bad\u7ec3\u540e\u6027\u80fd\u9971\u548c\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709ProRL\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0\u8bad\u7ec3\u6b65\u6570\u6765\u6269\u5c55RL\uff0c\u4f46\u6027\u80fd\u5728\u6570\u5343\u6b65\u540e\u8fbe\u5230\u9971\u548c\uff0c\u5b58\u5728\u660e\u663e\u7684\u6536\u76ca\u9012\u51cf\u95ee\u9898\u3002\u672c\u6587\u63a2\u7d22\u901a\u8fc7\u589e\u52a0\u6bcf\u4e2a\u6837\u672c\u7684rollout\u6570\u91cf\u6765\u6269\u5c55RL\u7684\u8865\u5145\u8303\u5f0f\u3002", "method": "\u57fa\u4e8e\u8d28\u91cf\u5e73\u8861\u65b9\u7a0b\u5206\u6790\uff0c\u63d0\u51faBroRL\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u6837\u672c\u7684rollout\u6570\u91cf\u589e\u52a0\u5230\u6570\u767e\u4e2a\u4ee5\u8fdb\u884c\u5145\u5206\u63a2\u7d22\uff0c\u786e\u4fdd\u6b63\u786etoken\u6982\u7387\u8d28\u91cf\u7684\u6574\u4f53\u6269\u5c55\u3002", "result": "BroRL\u80fd\u591f\u590d\u6d3b\u57283K\u6b65ProRL\u8bad\u7ec3\u540e\u9971\u548c\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u6301\u7eed\u6539\u8fdb\uff0c\u57281.5B\u6a21\u578b\u4e0a\u8de8\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u589e\u52a0rollout\u6570\u91cf\u662f\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u8865\u5145\u65b9\u6cd5\uff0c\u80fd\u591f\u7a81\u7834\u8bad\u7ec3\u6b65\u6570\u6269\u5c55\u7684\u6027\u80fd\u9971\u548c\u9650\u5236\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00602", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00602", "abs": "https://arxiv.org/abs/2510.00602", "authors": ["Amirhoseein Afsharrad", "Ahmadreza Moradipari", "Sanjay Lall"], "title": "Multi-Agent Stage-wise Conservative Linear Bandits", "comment": null, "summary": "In many real-world applications such as recommendation systems, multiple\nlearning agents must balance exploration and exploitation while maintaining\nsafety guarantees to avoid catastrophic failures. We study the stochastic\nlinear bandit problem in a multi-agent networked setting where agents must\nsatisfy stage-wise conservative constraints. A network of $N$ agents\ncollaboratively maximizes cumulative reward while ensuring that the expected\nreward at every round is no less than $(1-\\alpha)$ times that of a baseline\npolicy. Each agent observes local rewards with unknown parameters, but the\nnetwork optimizes for the global parameter (average of local parameters).\nAgents communicate only with immediate neighbors, and each communication round\nincurs additional regret. We propose MA-SCLUCB (Multi-Agent Stage-wise\nConservative Linear UCB), an episodic algorithm alternating between action\nselection and consensus-building phases. We prove that MA-SCLUCB achieves\nregret\n$\\tilde{O}\\left(\\frac{d}{\\sqrt{N}}\\sqrt{T}\\cdot\\frac{\\log(NT)}{\\sqrt{\\log(1/|\\lambda_2|)}}\\right)$\nwith high probability, where $d$ is the dimension, $T$ is the horizon, and\n$|\\lambda_2|$ is the network's second largest eigenvalue magnitude. Our\nanalysis shows: (i) collaboration yields $\\frac{1}{\\sqrt{N}}$ improvement\ndespite local communication, (ii) communication overhead grows only\nlogarithmically for well-connected networks, and (iii) stage-wise safety adds\nonly lower-order regret. Thus, distributed learning with safety guarantees\nachieves near-optimal performance in reasonably connected networks.", "AI": {"tldr": "\u63d0\u51faMA-SCLUCB\u7b97\u6cd5\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u968f\u673a\u7ebf\u6027\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u5728\u6ee1\u8db3\u9636\u6bb5\u4fdd\u5b88\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u534f\u4f5c\u5b66\u4e60\uff0c\u83b7\u5f971/\u221aN\u7684\u534f\u4f5c\u6536\u76ca\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u591a\u4e2a\u5b66\u4e60\u667a\u80fd\u4f53\u9700\u8981\u5728\u4fdd\u6301\u5b89\u5168\u4fdd\u8bc1\u7684\u540c\u65f6\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u907f\u514d\u707e\u96be\u6027\u5931\u8d25\u3002\u9700\u8981\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u5b89\u5168\u534f\u4f5c\u5b66\u4e60\u3002", "method": "\u63d0\u51faMA-SCLUCB\u7b97\u6cd5\uff0c\u91c7\u7528\u4ea4\u66ff\u7684\u52a8\u4f5c\u9009\u62e9\u548c\u5171\u8bc6\u6784\u5efa\u9636\u6bb5\u3002\u667a\u80fd\u4f53\u4ec5\u4e0e\u76f4\u63a5\u90bb\u5c45\u901a\u4fe1\uff0c\u534f\u4f5c\u4f18\u5316\u5168\u5c40\u53c2\u6570\uff0c\u540c\u65f6\u786e\u4fdd\u6bcf\u8f6e\u671f\u671b\u5956\u52b1\u4e0d\u4f4e\u4e8e\u57fa\u7ebf\u7b56\u7565\u7684(1-\u03b1)\u500d\u3002", "result": "\u7b97\u6cd5\u4ee5\u9ad8\u6982\u7387\u5b9e\u73b0\u9057\u61be\u754c\u00d5(d/\u221aN\u00b7\u221aT\u00b7log(NT)/\u221alog(1/|\u03bb\u2082|))\uff0c\u5176\u4e2d|\u03bb\u2082|\u662f\u7f51\u7edc\u7b2c\u4e8c\u5927\u7279\u5f81\u503c\u3002\u534f\u4f5c\u5e26\u67651/\u221aN\u6539\u8fdb\uff0c\u901a\u4fe1\u5f00\u9500\u4ec5\u5bf9\u6570\u589e\u957f\uff0c\u9636\u6bb5\u5b89\u5168\u4ec5\u589e\u52a0\u4f4e\u9636\u9057\u61be\u3002", "conclusion": "\u5728\u5408\u7406\u8fde\u63a5\u7684\u7f51\u7edc\u4e2d\uff0c\u5177\u6709\u5b89\u5168\u4fdd\u8bc1\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00739", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00739", "abs": "https://arxiv.org/abs/2510.00739", "authors": ["Marco Bagatella", "Matteo Pirotta", "Ahmed Touati", "Alessandro Lazaric", "Andrea Tirinzoni"], "title": "TD-JEPA: Latent-predictive Representations for Zero-Shot Reinforcement Learning", "comment": null, "summary": "Latent prediction--where agents learn by predicting their own latents--has\nemerged as a powerful paradigm for training general representations in machine\nlearning. In reinforcement learning (RL), this approach has been explored to\ndefine auxiliary losses for a variety of settings, including reward-based and\nunsupervised RL, behavior cloning, and world modeling. While existing methods\nare typically limited to single-task learning, one-step prediction, or\non-policy trajectory data, we show that temporal difference (TD) learning\nenables learning representations predictive of long-term latent dynamics across\nmultiple policies from offline, reward-free transitions. Building on this, we\nintroduce TD-JEPA, which leverages TD-based latent-predictive representations\ninto unsupervised RL. TD-JEPA trains explicit state and task encoders, a\npolicy-conditioned multi-step predictor, and a set of parameterized policies\ndirectly in latent space. This enables zero-shot optimization of any reward\nfunction at test time. Theoretically, we show that an idealized variant of\nTD-JEPA avoids collapse with proper initialization, and learns encoders that\ncapture a low-rank factorization of long-term policy dynamics, while the\npredictor recovers their successor features in latent space. Empirically,\nTD-JEPA matches or outperforms state-of-the-art baselines on locomotion,\nnavigation, and manipulation tasks across 13 datasets in ExoRL and OGBench,\nespecially in the challenging setting of zero-shot RL from pixels.", "AI": {"tldr": "TD-JEPA\uff1a\u4e00\u79cd\u57fa\u4e8e\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u7684\u6f5c\u5728\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u3001\u65e0\u5956\u52b1\u7684\u8f6c\u6362\u6570\u636e\u5b66\u4e60\u957f\u671f\u6f5c\u5728\u52a8\u6001\u8868\u793a\uff0c\u652f\u6301\u591a\u7b56\u7565\u8868\u793a\u548c\u96f6\u5956\u52b1\u51fd\u6570\u4f18\u5316", "motivation": "\u73b0\u6709\u6f5c\u5728\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4efb\u52a1\u5b66\u4e60\u3001\u5355\u6b65\u9884\u6d4b\u6216\u5728\u7ebf\u8f68\u8ff9\u6570\u636e\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ece\u79bb\u7ebf\u3001\u65e0\u5956\u52b1\u8f6c\u6362\u4e2d\u5b66\u4e60\u8de8\u7b56\u7565\u957f\u671f\u52a8\u6001\u8868\u793a\u7684\u65b9\u6cd5", "method": "\u63d0\u51faTD-JEPA\u65b9\u6cd5\uff0c\u4f7f\u7528\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u8bad\u7ec3\u72b6\u6001\u548c\u4efb\u52a1\u7f16\u7801\u5668\u3001\u7b56\u7565\u6761\u4ef6\u591a\u6b65\u9884\u6d4b\u5668\u4ee5\u53ca\u53c2\u6570\u5316\u7b56\u7565\uff0c\u76f4\u63a5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8868\u793a\u5b66\u4e60", "result": "\u5728ExoRL\u548cOGBench\u768413\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cTD-JEPA\u5728\u8fd0\u52a8\u3001\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u50cf\u7d20\u7ea7\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02", "conclusion": "TD-JEPA\u8bc1\u660e\u4e86\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u80fd\u591f\u6709\u6548\u5b66\u4e60\u8de8\u7b56\u7565\u7684\u957f\u671f\u6f5c\u5728\u52a8\u6001\u8868\u793a\uff0c\u4e3a\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6", "topic": "agentic reinforcement learning"}}
{"id": "2510.00761", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00761", "abs": "https://arxiv.org/abs/2510.00761", "authors": ["Yicheng Lang", "Yihua Zhang", "Chongyu Fan", "Changsheng Wang", "Jinghan Jia", "Sijia Liu"], "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning", "comment": null, "summary": "Large language model (LLM) unlearning aims to surgically remove the influence\nof undesired data or knowledge from an existing model while preserving its\nutility on unrelated tasks. This paradigm has shown promise in addressing\nprivacy and safety concerns. However, recent findings reveal that unlearning\neffects are often fragile: post-unlearning manipulations such as weight\nquantization or fine-tuning can quickly neutralize the intended forgetting.\nPrior efforts to improve robustness primarily reformulate unlearning objectives\nby explicitly assuming the role of vulnerability sources. In this work, we take\na different perspective by investigating the role of the optimizer, independent\nof unlearning objectives and formulations, in shaping unlearning robustness. We\nshow that the 'grade' of the optimizer, defined by the level of information it\nexploits, ranging from zeroth-order (gradient-free) to first-order\n(gradient-based) to second-order (Hessian-based), is tightly linked to the\nresilience of unlearning. Surprisingly, we find that downgrading the optimizer,\nsuch as using zeroth-order methods or compressed-gradient variants (e.g.,\ngradient sign-based optimizers), often leads to stronger robustness. While\nthese optimizers produce noisier and less precise updates, they encourage\nconvergence to harder-to-disturb basins in the loss landscape, thereby\nresisting post-training perturbations. By connecting zeroth-order methods with\nrandomized smoothing, we further highlight their natural advantage for robust\nunlearning. Motivated by these insights, we propose a hybrid optimizer that\ncombines first-order and zeroth-order updates, preserving unlearning efficacy\nwhile enhancing robustness. Extensive experiments on the MUSE and WMDP\nbenchmarks, across multiple LLM unlearning algorithms, validate that our\napproach achieves more resilient forgetting without sacrificing unlearning\nquality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u53d1\u73b0\u4f18\u5316\u5668\u7684\u9009\u62e9\u5bf9LLM\u9057\u5fd8\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f7f\u7528\u4f4e\u9636\u4f18\u5316\u5668\uff08\u5982\u96f6\u9636\u65b9\u6cd5\uff09\u867d\u7136\u66f4\u65b0\u66f4\u5608\u6742\uff0c\u4f46\u80fd\u6536\u655b\u5230\u66f4\u96be\u6270\u52a8\u7684\u635f\u5931\u76c6\u5730\uff0c\u4ece\u800c\u63d0\u9ad8\u9057\u5fd8\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LLM\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u7684\u9057\u5fd8\u6548\u679c\u5f80\u5f80\u5f88\u8106\u5f31\uff0c\u5bb9\u6613\u53d7\u5230\u6743\u91cd\u91cf\u5316\u6216\u5fae\u8c03\u7b49\u540e\u5904\u7406\u64cd\u4f5c\u7684\u5f71\u54cd\u3002\u672c\u6587\u4ece\u4f18\u5316\u5668\u89d2\u5ea6\u800c\u975e\u9057\u5fd8\u76ee\u6807\u89d2\u5ea6\u7814\u7a76\u5982\u4f55\u63d0\u9ad8\u9057\u5fd8\u9c81\u68d2\u6027\u3002", "method": "\u7814\u7a76\u4e0d\u540c\u9636\u6570\u4f18\u5316\u5668\uff08\u96f6\u9636\u3001\u4e00\u9636\u3001\u4e8c\u9636\uff09\u5bf9\u9057\u5fd8\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u964d\u7ea7\u4f18\u5316\u5668\uff08\u5982\u4f7f\u7528\u96f6\u9636\u65b9\u6cd5\u6216\u538b\u7f29\u68af\u5ea6\u53d8\u4f53\uff09\u80fd\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u4e00\u9636\u548c\u96f6\u9636\u66f4\u65b0\u7684\u6df7\u5408\u4f18\u5316\u5668\u3002", "result": "\u5728MUSE\u548cWMDP\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u4fdd\u6301\u9057\u5fd8\u6548\u679c\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u62b5\u6297\u540e\u8bad\u7ec3\u6270\u52a8\u3002", "conclusion": "\u4f18\u5316\u5668\u7684\u9009\u62e9\u662f\u5f71\u54cdLLM\u9057\u5fd8\u5b66\u4e60\u9c81\u68d2\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u964d\u7ea7\u4f18\u5316\u5668\u80fd\u4ea7\u751f\u66f4\u9c81\u68d2\u7684\u9057\u5fd8\u6548\u679c\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u4f18\u5316\u5668\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u9057\u5fd8\u6548\u679c\u548c\u9c81\u68d2\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.00777", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00777", "abs": "https://arxiv.org/abs/2510.00777", "authors": ["Youngbin Choi", "Minjong Lee", "Saemi Moon", "Seunghyuk Cho", "Chaehyeon Chung", "MoonJeong Park", "Dongwoo Kim"], "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning", "comment": "28 pages, 23 figures", "summary": "Large language models (LLMs) are increasingly studied in the context of\nmulti-turn reasoning, where models iteratively refine their outputs based on\nuser-provided feedback. Such settings are crucial for tasks that require\ncomplex reasoning, yet existing feedback paradigms often rely on issuing new\nmessages. LLMs struggle to integrate these reliably, leading to inconsistent\nimprovements. In this work, we introduce in-place feedback, a novel interaction\nparadigm in which users directly edit an LLM's previous response, and the model\nconditions on this modified response to generate its revision. Empirical\nevaluations on diverse reasoning-intensive benchmarks reveal that in-place\nfeedback achieves better performance than conventional multi-turn feedback\nwhile using $79.1\\%$ fewer tokens. Complementary analyses on controlled\nenvironments further demonstrate that in-place feedback resolves a core\nlimitation of multi-turn feedback: models often fail to apply feedback\nprecisely to erroneous parts of the response, leaving errors uncorrected and\nsometimes introducing new mistakes into previously correct content. These\nfindings suggest that in-place feedback offers a more natural and effective\nmechanism for guiding LLMs in reasoning-intensive tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u8303\u5f0f\u2014\u2014\u539f\u4f4d\u53cd\u9988\uff0c\u7528\u6237\u76f4\u63a5\u7f16\u8f91LLM\u7684\u5148\u524d\u54cd\u5e94\uff0c\u6a21\u578b\u57fa\u4e8e\u4fee\u6539\u540e\u7684\u54cd\u5e94\u751f\u6210\u4fee\u8ba2\u7248\u672c\uff0c\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u591a\u8f6e\u53cd\u9988\u8868\u73b0\u66f4\u597d\u4e14\u8282\u770179.1%\u7684token\u3002", "motivation": "\u73b0\u6709\u53cd\u9988\u8303\u5f0f\u4f9d\u8d56\u53d1\u5e03\u65b0\u6d88\u606f\uff0cLLM\u96be\u4ee5\u53ef\u9760\u6574\u5408\u8fd9\u4e9b\u53cd\u9988\uff0c\u5bfc\u81f4\u6539\u8fdb\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u4ea4\u4e92\u673a\u5236\u6765\u6307\u5bfcLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u539f\u4f4d\u53cd\u9988\u8303\u5f0f\uff0c\u7528\u6237\u76f4\u63a5\u7f16\u8f91LLM\u7684\u5148\u524d\u54cd\u5e94\uff0c\u6a21\u578b\u57fa\u4e8e\u8fd9\u4e2a\u4fee\u6539\u540e\u7684\u54cd\u5e94\u6765\u751f\u6210\u4fee\u8ba2\u7248\u672c\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4f20\u7edf\u7684\u65b0\u6d88\u606f\u53cd\u9988\u65b9\u5f0f\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u539f\u4f4d\u53cd\u9988\u6bd4\u4f20\u7edf\u591a\u8f6e\u53cd\u9988\u8868\u73b0\u66f4\u597d\uff0c\u540c\u65f6\u8282\u770179.1%\u7684token\u3002\u63a7\u5236\u73af\u5883\u5206\u6790\u663e\u793a\u539f\u4f4d\u53cd\u9988\u89e3\u51b3\u4e86\u591a\u8f6e\u53cd\u9988\u7684\u6838\u5fc3\u9650\u5236\u3002", "conclusion": "\u539f\u4f4d\u53cd\u9988\u4e3a\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u6307\u5bfcLLM\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u548c\u6709\u6548\u7684\u673a\u5236\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u5e94\u7528\u53cd\u9988\u5230\u9519\u8bef\u90e8\u5206\u3002", "topic": "agent analysis"}}
{"id": "2510.00805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00805", "abs": "https://arxiv.org/abs/2510.00805", "authors": ["Rui Zhu", "Xuan Yu", "Yudong Zhang", "Chen Zhang", "Xu Wang", "Yang Wang"], "title": "MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS and Greediness Control", "comment": null, "summary": "Generative Flow Networks (GFlowNets) have emerged as a powerful tool for\ngenerating diverse and high-reward structured objects by learning to sample\nfrom a distribution proportional to a given reward function. Unlike\nconventional reinforcement learning (RL) approaches that prioritize\noptimization of a single trajectory, GFlowNets seek to balance diversity and\nreward by modeling the entire trajectory distribution. This capability makes\nthem especially suitable for domains such as molecular design and combinatorial\noptimization. However, existing GFlowNets sampling strategies tend to\noverexplore and struggle to consistently generate high-reward samples,\nparticularly in large search spaces with sparse high-reward regions. Therefore,\nimproving the probability of generating high-reward samples without sacrificing\ndiversity remains a key challenge under this premise. In this work, we\nintegrate an enhanced Monte Carlo Tree Search (MCTS) into the GFlowNets\nsampling process, using MCTS-based policy evaluation to guide the generation\ntoward high-reward trajectories and Polynomial Upper Confidence Trees (PUCT) to\nbalance exploration and exploitation adaptively, and we introduce a\ncontrollable mechanism to regulate the degree of greediness. Our method\nenhances exploitation without sacrificing diversity by dynamically balancing\nexploration and reward-driven guidance. The experimental results show that our\nmethod can not only accelerate the speed of discovering high-reward regions but\nalso continuously generate high-reward samples, while preserving the diversity\nof the generative distribution. All implementations are available at\nhttps://github.com/ZRNB/MG2FlowNet.", "AI": {"tldr": "\u5c06\u589e\u5f3a\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u96c6\u6210\u5230GFlowNets\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7MCTS\u7b56\u7565\u8bc4\u4f30\u5f15\u5bfc\u751f\u6210\u9ad8\u5956\u52b1\u8f68\u8ff9\uff0c\u4f7f\u7528PUCT\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u63d0\u9ad8\u9ad8\u5956\u52b1\u6837\u672c\u751f\u6210\u6982\u7387\u800c\u4e0d\u727a\u7272\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709GFlowNets\u91c7\u6837\u7b56\u7565\u503e\u5411\u4e8e\u8fc7\u5ea6\u63a2\u7d22\uff0c\u5728\u5927\u578b\u7a00\u758f\u5956\u52b1\u7a7a\u95f4\u4e2d\u96be\u4ee5\u6301\u7eed\u751f\u6210\u9ad8\u5956\u52b1\u6837\u672c\uff0c\u9700\u8981\u5728\u4fdd\u6301\u591a\u6837\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u9ad8\u5956\u52b1\u6837\u672c\u751f\u6210\u6982\u7387\u3002", "method": "\u96c6\u6210\u589e\u5f3a\u7684MCTS\u5230GFlowNets\u91c7\u6837\u8fc7\u7a0b\uff0c\u4f7f\u7528MCTS\u7b56\u7565\u8bc4\u4f30\u5f15\u5bfc\u9ad8\u5956\u52b1\u8f68\u8ff9\u751f\u6210\uff0c\u91c7\u7528PUCT\u81ea\u9002\u5e94\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u5e76\u5f15\u5165\u53ef\u63a7\u673a\u5236\u8c03\u8282\u8d2a\u5a6a\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u52a0\u901f\u53d1\u73b0\u9ad8\u5956\u52b1\u533a\u57df\uff0c\u6301\u7eed\u751f\u6210\u9ad8\u5956\u52b1\u6837\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u5206\u5e03\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u548c\u5956\u52b1\u9a71\u52a8\u6307\u5bfc\uff0c\u589e\u5f3a\u4e86\u5229\u7528\u80fd\u529b\u800c\u4e0d\u727a\u7272\u591a\u6837\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00819", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00819", "abs": "https://arxiv.org/abs/2510.00819", "authors": ["Luckeciano C. Melo", "Alessandro Abate", "Yarin Gal"], "title": "Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning", "comment": null, "summary": "Reinforcement Learning, particularly through policy gradient methods, has\nplayed a central role in enabling reasoning capabilities of Large Language\nModels. However, the optimization stability of policy gradients in this setting\nremains understudied. As a result, existing implementations often resort to\nconservative hyperparameter choices to ensure stability, which requires more\ntraining samples and increases computational costs. Hence, developing models\nfor reliably tracking the underlying optimization dynamics and leveraging them\ninto training enables more sample-efficient regimes and further unleashes\nscalable post-training. We address this gap by formalizing the stochastic\noptimization problem of policy gradients with explicit consideration of\nsecond-order geometry. We propose a tractable computational framework that\ntracks and leverages curvature information during policy updates. We further\nemploy this framework to design interventions in the optimization process\nthrough data selection. The resultant algorithm, Curvature-Aware Policy\nOptimization (CAPO), identifies samples that contribute to unstable updates and\nmasks them out. Theoretically, we establish monotonic improvement guarantees\nunder realistic assumptions. On standard math reasoning benchmarks, we\nempirically show that CAPO ensures stable updates under aggressive learning\nregimes where baselines catastrophically fail. With minimal intervention\n(rejecting fewer than 8% of tokens), CAPO achieves up to 30x improvement in\nsample efficiency over standard GRPO for LLM reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f2\u7387\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u548c\u5229\u7528\u4e8c\u9636\u51e0\u4f55\u4fe1\u606f\u6765\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u68af\u5ea6\u4f18\u5316\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u4f18\u5316\u7a33\u5b9a\u6027\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u5bfc\u81f4\u9700\u8981\u4fdd\u5b88\u7684\u8d85\u53c2\u6570\u9009\u62e9\uff0c\u589e\u52a0\u4e86\u8bad\u7ec3\u6837\u672c\u9700\u6c42\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5f62\u5f0f\u5316\u4e86\u8003\u8651\u4e8c\u9636\u51e0\u4f55\u7684\u7b56\u7565\u68af\u5ea6\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u53ef\u8ba1\u7b97\u6846\u67b6\u6765\u8ddf\u8e2a\u66f2\u7387\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u6570\u636e\u9009\u62e9\u5e72\u9884\u673a\u5236\u6765\u5c4f\u853d\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u66f4\u65b0\u7684\u6837\u672c\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAPO\u5728\u6fc0\u8fdb\u5b66\u4e60\u673a\u5236\u4e0b\u786e\u4fdd\u7a33\u5b9a\u66f4\u65b0\uff0c\u4ec5\u62d2\u7edd\u4e0d\u52308%\u7684token\u5c31\u80fd\u5b9e\u73b0\u6bd4\u6807\u51c6GRPO\u9ad8\u8fbe30\u500d\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u66f2\u7387\u611f\u77e5\u7b56\u7565\u4f18\u5316\u80fd\u591f\u53ef\u9760\u5730\u8ddf\u8e2a\u4f18\u5316\u52a8\u6001\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u91ca\u653e\u53ef\u6269\u5c55\u7684\u540e\u8bad\u7ec3\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00841", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00841", "abs": "https://arxiv.org/abs/2510.00841", "authors": ["Chao-Kai Chiang", "Takashi Ishida", "Masashi Sugiyama"], "title": "LLM Routing with Dueling Feedback", "comment": null, "summary": "We study LLM routing, the problem of selecting the best model for each query\nwhile balancing user satisfaction, model expertise, and inference cost. We\nformulate routing as contextual dueling bandits, learning from pairwise\npreference feedback rather than absolute scores, thereby yielding\nlabel-efficient and dynamic adaptation. Building on this formulation, we\nintroduce Category-Calibrated Fine-Tuning (CCFT), a representation-learning\nmethod that derives model embeddings from offline data using contrastive\nfine-tuning with categorical weighting. These embeddings enable the practical\ninstantiation of Feel-Good Thompson Sampling for Contextual Dueling Bandits\n(FGTS.CDB), a theoretically grounded posterior-sampling algorithm. We propose\nfour variants of the categorical weighting that explicitly integrate model\nquality and cost, and we empirically evaluate the proposed methods on the\nRouterBench and MixInstruct datasets. Across both benchmarks, our methods\nachieve lower cumulative regret and faster convergence, with better robustness\nand performance-cost balance than strong baselines built with a general-purpose\nOpenAI embedding model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5bf9\u51b3\u8d4c\u535a\u673a\u7684LLM\u8def\u7531\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u522b\u6821\u51c6\u5fae\u8c03(CCFT)\u5b66\u4e60\u6a21\u578b\u5d4c\u5165\uff0c\u5e76\u5e94\u7528Feel-Good Thompson\u91c7\u6837\u7b97\u6cd5\uff0c\u5728RouterBench\u548cMixInstruct\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7d2f\u79ef\u9057\u61be\u548c\u66f4\u597d\u7684\u6027\u80fd-\u6210\u672c\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3LLM\u8def\u7531\u95ee\u9898\uff0c\u5373\u5728\u5e73\u8861\u7528\u6237\u6ee1\u610f\u5ea6\u3001\u6a21\u578b\u4e13\u4e1a\u77e5\u8bc6\u548c\u63a8\u7406\u6210\u672c\u7684\u524d\u63d0\u4e0b\uff0c\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u9009\u62e9\u6700\u4f73\u6a21\u578b\u3002", "method": "\u5c06\u8def\u7531\u95ee\u9898\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u5bf9\u51b3\u8d4c\u535a\u673a\uff0c\u63d0\u51fa\u7c7b\u522b\u6821\u51c6\u5fae\u8c03(CCFT)\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u5fae\u8c03\u548c\u7c7b\u522b\u52a0\u6743\u5b66\u4e60\u6a21\u578b\u5d4c\u5165\uff0c\u5e76\u5e94\u7528FGTS.CDB\u7b97\u6cd5\u8fdb\u884c\u8def\u7531\u51b3\u7b56\u3002", "result": "\u5728RouterBench\u548cMixInstruct\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u6bd4\u57fa\u4e8e\u901a\u7528OpenAI\u5d4c\u5165\u6a21\u578b\u7684\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7d2f\u79ef\u9057\u61be\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd-\u6210\u672c\u5e73\u8861\u3002", "conclusion": "\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5bf9\u51b3\u8d4c\u535a\u673a\u7684LLM\u8def\u7531\u6846\u67b6\u7ed3\u5408CCFT\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.00911", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00911", "abs": "https://arxiv.org/abs/2510.00911", "authors": ["Tao Ren", "Jinyang Jiang", "Hui Yang", "Wan Tian", "Minhao Zou", "Guanghao Li", "Zishi Zhang", "Qinghao Wang", "Shentao Qin", "Yanjun Zhao", "Rui Tao", "Hui Shao", "Yijie Peng"], "title": "RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM Post-Training", "comment": null, "summary": "Reinforcement learning with verifiable reward has recently emerged as a\ncentral paradigm for post-training large language models (LLMs); however,\nprevailing mean-based methods, such as Group Relative Policy Optimization\n(GRPO), suffer from entropy collapse and limited reasoning gains. We argue that\nthese issues stem from overemphasizing high-probability output sequences while\nneglecting rare but informative reasoning paths. To address these challenges,\nwe propose Risk-based Policy Optimization (RiskPO), which substitutes classical\nmean-based objectives with principled risk measures. Specifically, we introduce\na Mixed Value-at-Risk objective that integrates weighted attention over\nmultiple regions of the reward distribution, thereby amplifying gradient\nsignals on challenging instances and preventing overconfident convergence. We\nfurther design a bundling scheme that aggregates multiple questions into\nbundles, thus enriching the feedback signal and yielding more stable and\ninformative training dynamics. Theoretically, we prove that the risk-averse\nupdate alleviates entropy collapse and promotes exploration. Numerically,\nRiskPO achieves consistent and significant improvements in mathematical\nreasoning, multi-modal reasoning, and code generation benchmarks, surpassing\nGRPO and its variants on both Pass@1 and Pass@k metrics. Our results\ndemonstrate that risk-based optimization provides a rigorous and effective\nparadigm for enhancing LLM reasoning capabilities.", "AI": {"tldr": "\u63d0\u51faRiskPO\u65b9\u6cd5\uff0c\u7528\u98ce\u9669\u5ea6\u91cf\u66ff\u4ee3\u4f20\u7edf\u5747\u503c\u76ee\u6807\uff0c\u901a\u8fc7\u6df7\u5408VaR\u76ee\u6807\u589e\u5f3a\u68af\u5ea6\u4fe1\u53f7\uff0c\u89e3\u51b3GRPO\u7b49\u65b9\u6cd5\u4e2d\u7684\u71b5\u5d29\u6e83\u548c\u63a8\u7406\u80fd\u529b\u53d7\u9650\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5747\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5b58\u5728\u71b5\u5d29\u6e83\u548c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u6709\u9650\u7684\u95ee\u9898\uff0c\u539f\u56e0\u662f\u8fc7\u5ea6\u5173\u6ce8\u9ad8\u6982\u7387\u8f93\u51fa\u5e8f\u5217\u800c\u5ffd\u7565\u4e86\u7f55\u89c1\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u98ce\u9669\u7684\u7b56\u7565\u4f18\u5316\uff08RiskPO\uff09\uff0c\u4f7f\u7528\u6df7\u5408\u98ce\u9669\u4ef7\u503c\uff08VaR\uff09\u76ee\u6807\uff0c\u96c6\u6210\u5956\u52b1\u5206\u5e03\u7684\u591a\u4e2a\u533a\u57df\u52a0\u6743\u6ce8\u610f\u529b\uff0c\u5e76\u8bbe\u8ba1\u6346\u7ed1\u65b9\u6848\u5c06\u591a\u4e2a\u95ee\u9898\u805a\u5408\u4ee5\u4e30\u5bcc\u53cd\u9988\u4fe1\u53f7\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRiskPO\u5728Pass@1\u548cPass@k\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8eGRPO\u53ca\u5176\u53d8\u4f53\u3002", "conclusion": "\u57fa\u4e8e\u98ce\u9669\u7684\u4f18\u5316\u4e3a\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e25\u8c28\u6709\u6548\u7684\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00915", "abs": "https://arxiv.org/abs/2510.00915", "authors": ["Xin-Qiang Cai", "Wei Wang", "Feng Liu", "Tongliang Liu", "Gang Niu", "Masashi Sugiyama"], "title": "Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against\nautomated verifiers to avoid costly human labeling. To reduce vulnerability to\nverifier hacking, many RLVR systems collapse rewards to binary $\\{0,1\\}$ during\ntraining. This choice carries a cost: it introduces \\textit{false negatives}\n(rejecting correct answers, FNs) and \\textit{false positives} (accepting\nincorrect ones, FPs). For instance, a rule-based checker may mark the correct\nfraction $\\frac{12}{36}$ as wrong when compared against the canonical\n$\\frac{1}{3}$ due to brittle parsing/equivalence rules (FN), while a large\nlanguage model (LLM) judges can be gamed by superficial cues or even a single\nadversarial token, yielding inflated correctness for wrong solutions (FP). We\nformalize verifier unreliability by modeling the verifier as a stochastic\nreward channel with asymmetric noise rates. From this abstraction, we derive\ntwo correction algorithms for verifier errors. The first is a \\textit{backward}\ncorrection that de-biases the observed binary reward to recover an\n\\textit{unbiased} estimator of the clean policy gradient. The second is a\n\\textit{forward} correction that reweights score-function terms so that the\nexpected update direction aligns with the \\textit{clean gradient}; notably, it\nrequires only the FN rate. We implement both as lightweight hooks in a group\nrelative policy optimization (GRPO)-based RLVR pipeline and evaluate them on\nmath-reasoning models and benchmarks. Across models and datasets, both\ncorrections improve over uncorrected training; the forward variant converges\nfaster and remains stable under heavier noise. Finally, we show a practical\nappeal mechanism in which a lightweight LLM verifier estimates the FN rate\nonline by rechecking rule-based negatives, obtaining outperformance compared\nwith other state-of-the-art contenders.", "AI": {"tldr": "RLVR\u4f7f\u7528\u81ea\u52a8\u5316\u9a8c\u8bc1\u5668\u8bad\u7ec3\u7b56\u7565\u4ee5\u907f\u514d\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002\u4e3a\u51cf\u5c11\u9a8c\u8bc1\u5668\u88ab\u653b\u51fb\u7684\u98ce\u9669\uff0c\u8bad\u7ec3\u65f6\u5c06\u5956\u52b1\u4e8c\u503c\u5316\u4e3a{0,1}\uff0c\u4f46\u8fd9\u4f1a\u5f15\u5165\u5047\u9634\u6027\uff08\u62d2\u7edd\u6b63\u786e\u7b54\u6848\uff09\u548c\u5047\u9633\u6027\uff08\u63a5\u53d7\u9519\u8bef\u7b54\u6848\uff09\u95ee\u9898\u3002\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6821\u6b63\u7b97\u6cd5\u6765\u4fee\u6b63\u9a8c\u8bc1\u5668\u9519\u8bef\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f7f\u7528\u81ea\u52a8\u5316\u9a8c\u8bc1\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u9762\u4e34\u9a8c\u8bc1\u5668\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u5305\u62ec\u5047\u9634\u6027\u548c\u5047\u9633\u6027\u9519\u8bef\u3002\u8fd9\u4e9b\u9519\u8bef\u4f1a\u5f71\u54cd\u7b56\u7565\u8bad\u7ec3\u6548\u679c\uff0c\u9700\u8981\u8bbe\u8ba1\u6821\u6b63\u673a\u5236\u6765\u5e94\u5bf9\u9a8c\u8bc1\u5668\u7684\u4e0d\u5b8c\u7f8e\u6027\u3002", "method": "\u5c06\u9a8c\u8bc1\u5668\u5efa\u6a21\u4e3a\u5177\u6709\u4e0d\u5bf9\u79f0\u566a\u58f0\u7387\u7684\u968f\u673a\u5956\u52b1\u901a\u9053\uff0c\u63d0\u51fa\u4e24\u79cd\u6821\u6b63\u7b97\u6cd5\uff1a1\uff09\u53cd\u5411\u6821\u6b63\uff0c\u901a\u8fc7\u53bb\u504f\u89c2\u6d4b\u5230\u7684\u4e8c\u5143\u5956\u52b1\u6765\u6062\u590d\u65e0\u504f\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u5668\uff1b2\uff09\u524d\u5411\u6821\u6b63\uff0c\u91cd\u65b0\u52a0\u6743\u5f97\u5206\u51fd\u6570\u9879\uff0c\u4f7f\u671f\u671b\u66f4\u65b0\u65b9\u5411\u4e0e\u5e72\u51c0\u68af\u5ea6\u5bf9\u9f50\u3002\u5728GRPO-based RLVR\u6d41\u6c34\u7ebf\u4e2d\u5b9e\u73b0\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u79cd\u6821\u6b63\u65b9\u6cd5\u90fd\u4f18\u4e8e\u672a\u6821\u6b63\u7684\u8bad\u7ec3\uff1b\u524d\u5411\u6821\u6b63\u6536\u655b\u66f4\u5feb\u4e14\u5728\u66f4\u4e25\u91cd\u566a\u58f0\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u9a8c\u8bc1\u5668\u5728\u7ebf\u4f30\u8ba1\u5047\u9634\u6027\u7387\u7684\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u9a8c\u8bc1\u5668\u4e0d\u53ef\u9760\u6027\u662fRLVR\u7cfb\u7edf\u7684\u91cd\u8981\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u6821\u6b63\u7b97\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u5047\u9634\u6027\u548c\u5047\u9633\u6027\u9519\u8bef\uff0c\u524d\u5411\u6821\u6b63\u5728\u5b9e\u8df5\u4e2d\u66f4\u5177\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.00938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00938", "abs": "https://arxiv.org/abs/2510.00938", "authors": ["ShengYun Peng", "Eric Smith", "Ivan Evtimov", "Song Jiang", "Pin-Yu Chen", "Hongyuan Zhan", "Haozhu Wang", "Duen Horng Chau", "Mahesh Pasupuleti", "Jianfeng Chi"], "title": "Large Reasoning Models Learn Better Alignment from Flawed Thinking", "comment": null, "summary": "Large reasoning models (LRMs) \"think\" by generating structured\nchain-of-thought (CoT) before producing a final answer, yet they still lack the\nability to reason critically about safety alignment and are easily biased when\na flawed premise is injected into their thought process. We propose RECAP\n(Robust Safety Alignment via Counter-Aligned Prefilling), a principled\nreinforcement learning (RL) method for post-training that explicitly teaches\nmodels to override flawed reasoning trajectories and reroute to safe and\nhelpful responses. RECAP trains on a mixture of synthetically generated\ncounter-aligned CoT prefills and standard prompts, requires no additional\ntraining cost or modifications beyond vanilla reinforcement learning from human\nfeedback (RLHF), and substantially improves safety and jailbreak robustness,\nreduces overrefusal, and preserves core reasoning capability -- all while\nmaintaining inference token budget. Extensive analysis shows that RECAP-trained\nmodels engage in self-reflection more frequently and remain robust under\nadaptive attacks, preserving safety even after repeated attempts to override\ntheir reasoning.", "AI": {"tldr": "RECAP\u662f\u4e00\u79cd\u901a\u8fc7\u53cd\u5bf9\u9f50\u601d\u7ef4\u94fe\u9884\u586b\u5145\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u8986\u76d6\u6709\u7f3a\u9677\u7684\u63a8\u7406\u8f68\u8ff9\u5e76\u8f6c\u5411\u5b89\u5168\u54cd\u5e94\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u751f\u6210\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u65f6\u7f3a\u4e4f\u5bf9\u5b89\u5168\u5bf9\u9f50\u7684\u6279\u5224\u6027\u63a8\u7406\u80fd\u529b\uff0c\u5bb9\u6613\u53d7\u5230\u6709\u7f3a\u9677\u524d\u63d0\u7684\u5f71\u54cd\u800c\u4ea7\u751f\u504f\u89c1\u3002", "method": "\u4f7f\u7528\u5408\u6210\u751f\u6210\u7684\u53cd\u5bf9\u9f50\u601d\u7ef4\u94fe\u9884\u586b\u5145\u548c\u6807\u51c6\u63d0\u793a\u7684\u6df7\u5408\u6570\u636e\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6210\u672c\u6216RLHF\u4e4b\u5916\u7684\u4fee\u6539\u3002", "result": "RECAP\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u8d8a\u72f1\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u4e86\u8fc7\u5ea6\u62d2\u7edd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6838\u5fc3\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u4ee4\u724c\u9884\u7b97\u3002", "conclusion": "RECAP\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u66f4\u9891\u7e41\u5730\u8fdb\u884c\u81ea\u6211\u53cd\u601d\uff0c\u5728\u81ea\u9002\u5e94\u653b\u51fb\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u7ecf\u8fc7\u591a\u6b21\u5c1d\u8bd5\u8986\u76d6\u5176\u63a8\u7406\u4e5f\u80fd\u4fdd\u6301\u5b89\u5168\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01032", "abs": "https://arxiv.org/abs/2510.01032", "authors": ["Zeru Shi", "Yingjia Wan", "Zhenting Wang", "Qifan Wang", "Fan Yang", "Elisa Kreiss", "Ruixiang Tang"], "title": "Meaningless Tokens, Meaningful Gains: How Activation Shifts Enhance LLM Reasoning", "comment": null, "summary": "Motivated by the puzzling observation that inserting long sequences of\nmeaningless tokens before the query prompt can consistently enhance LLM\nreasoning performance, this work analyzes the underlying mechanism driving this\nphenomenon and based on these insights proposes a more principled method that\nallows for similar performance gains. First, we find that the improvements\narise from a redistribution of activations in the LLM's MLP layers, where near\nzero activations become less frequent while large magnitude activations\nincrease. This redistribution enhances the model's representational capacity by\nsuppressing weak signals and promoting stronger, more informative ones.\nBuilding on this insight, we propose the Activation Redistribution Module\n(ARM), a lightweight inference-time technique that modifies activations\ndirectly without altering the input sequence. ARM adaptively identifies\nnear-zero activations after the non-linear function and shifts them outward,\nimplicitly reproducing the beneficial effects of meaningless tokens in a\ncontrolled manner. Extensive experiments across diverse benchmarks and model\narchitectures clearly show that ARM consistently improves LLM performance on\nreasoning tasks while requiring only a few lines of simple code to implement.\nOur findings deliver both a clear mechanistic explanation for the unexpected\nbenefits of meaningless tokens and a simple yet effective technique that\nharnesses activation redistribution to further improve LLM performance.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u67e5\u8be2\u63d0\u793a\u524d\u63d2\u5165\u65e0\u610f\u4e49token\u5e8f\u5217\u80fd\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u7684\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u6fc0\u6d3b\u91cd\u5206\u5e03\u6a21\u5757(ARM)\u6765\u66f4\u53ef\u63a7\u5730\u5b9e\u73b0\u7c7b\u4f3c\u6027\u80fd\u589e\u76ca\u3002", "motivation": "\u7814\u7a76\u5728\u67e5\u8be2\u63d0\u793a\u524d\u63d2\u5165\u65e0\u610f\u4e49token\u5e8f\u5217\u80fd\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u8fd9\u4e00\u4ee4\u4eba\u56f0\u60d1\u7684\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u6fc0\u6d3b\u91cd\u5206\u5e03\u6a21\u5757(ARM)\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u63a8\u7406\u65f6\u6280\u672f\uff0c\u76f4\u63a5\u4fee\u6539\u6fc0\u6d3b\u800c\u4e0d\u6539\u53d8\u8f93\u5165\u5e8f\u5217\u3002ARM\u81ea\u9002\u5e94\u5730\u8bc6\u522b\u975e\u7ebf\u6027\u51fd\u6570\u540e\u7684\u8fd1\u96f6\u6fc0\u6d3b\u5e76\u5c06\u5176\u5411\u5916\u79fb\u52a8\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cARM\u80fd\u6301\u7eed\u63d0\u5347LLM\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e14\u53ea\u9700\u51e0\u884c\u7b80\u5355\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u65e2\u4e3a\u65e0\u610f\u4e49token\u7684\u610f\u5916\u76ca\u5904\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u673a\u5236\u89e3\u91ca\uff0c\u4e5f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\u6765\u5229\u7528\u6fc0\u6d3b\u91cd\u5206\u5e03\u8fdb\u4e00\u6b65\u63d0\u5347LLM\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.01070", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01070", "abs": "https://arxiv.org/abs/2510.01070", "authors": ["Bartosz Cywi\u0144ski", "Emil Ryd", "Rowan Wang", "Senthooran Rajamanoharan", "Neel Nanda", "Arthur Conmy", "Samuel Marks"], "title": "Eliciting Secret Knowledge from Language Models", "comment": null, "summary": "We study secret elicitation: discovering knowledge that an AI possesses but\ndoes not explicitly verbalize. As a testbed, we train three families of large\nlanguage models (LLMs) to possess specific knowledge that they apply downstream\nbut deny knowing when asked directly. For example, in one setting, we train an\nLLM to generate replies that are consistent with knowing the user is female,\nwhile denying this knowledge when asked directly. We then design various\nblack-box and white-box secret elicitation techniques and evaluate them based\non whether they can help an LLM auditor successfully guess the secret\nknowledge. Many of our techniques improve on simple baselines. Our most\neffective techniques (performing best in 2/3 settings) are based on prefill\nattacks, a black-box technique where the LLM reveals secret knowledge when\ngenerating a completion from a predefined prefix. In our remaining setting,\nwhite-box techniques based on logit lens and sparse autoencoders (SAEs) are\nmost effective. We release our models and code, establishing a public benchmark\nfor evaluating secret elicitation methods.", "AI": {"tldr": "\u7814\u7a76AI\u79d8\u5bc6\u77e5\u8bc6\u63d0\u53d6\u6280\u672f\uff0c\u901a\u8fc7\u8bad\u7ec3LLMs\u62e5\u6709\u7279\u5b9a\u77e5\u8bc6\u4f46\u5728\u76f4\u63a5\u8be2\u95ee\u65f6\u5426\u8ba4\uff0c\u5f00\u53d1\u9ed1\u76d2\u548c\u767d\u76d2\u65b9\u6cd5\u6765\u63ed\u793a\u8fd9\u4e9b\u9690\u85cf\u77e5\u8bc6\u3002", "motivation": "\u63a2\u7d22AI\u7cfb\u7edf\u53ef\u80fd\u62e5\u6709\u4f46\u4e0d\u660e\u786e\u8868\u8fbe\u7684\u77e5\u8bc6\uff0c\u7814\u7a76\u5982\u4f55\u6709\u6548\u63d0\u53d6\u8fd9\u4e9b\u9690\u85cf\u4fe1\u606f\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u6027\u548c\u900f\u660e\u5ea6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bad\u7ec3\u4e09\u7c7bLLMs\u62e5\u6709\u7279\u5b9a\u77e5\u8bc6\u4f46\u5728\u76f4\u63a5\u8be2\u95ee\u65f6\u5426\u8ba4\uff0c\u8bbe\u8ba1\u9ed1\u76d2\uff08\u9884\u586b\u5145\u653b\u51fb\uff09\u548c\u767d\u76d2\uff08logit lens\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff09\u6280\u672f\u6765\u63d0\u53d6\u79d8\u5bc6\u77e5\u8bc6\u3002", "result": "\u591a\u6570\u6280\u672f\u4f18\u4e8e\u7b80\u5355\u57fa\u7ebf\uff0c\u9884\u586b\u5145\u653b\u51fb\u57282/3\u573a\u666f\u4e2d\u6700\u6709\u6548\uff0c\u767d\u76d2\u6280\u672f\u5728\u5269\u4f59\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5efa\u7acb\u4e86\u79d8\u5bc6\u77e5\u8bc6\u63d0\u53d6\u7684\u516c\u5171\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u4e0d\u540c\u63d0\u53d6\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3aAI\u900f\u660e\u5ea6\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.01083", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01083", "abs": "https://arxiv.org/abs/2510.01083", "authors": ["Andy Wu", "Chun-Cheng Lin", "Rung-Tzuo Liaw", "Yuehua Huang", "Chihjung Kuo", "Chia Tong Weng"], "title": "Multi-Actor Multi-Critic Deep Deterministic Reinforcement Learning with a Novel Q-Ensemble Method", "comment": null, "summary": "Reinforcement learning has gathered much attention in recent years due to its\nrapid development and rich applications, especially on control systems and\nrobotics. When tackling real-world applications with reinforcement learning\nmethod, the corresponded Markov decision process may have huge discrete or even\ncontinuous state/action space. Deep reinforcement learning has been studied for\nhandling these issues through deep learning for years, and one promising branch\nis the actor-critic architecture. Many past studies leveraged multiple critics\nto enhance the accuracy of evaluation of a policy for addressing the\noverestimation and underestimation issues. However, few studies have considered\nthe architecture with multiple actors together with multiple critics. This\nstudy proposes a novel multi-actor multi-critic (MAMC) deep deterministic\nreinforcement learning method. The proposed method has three main features,\nincluding selection of actors based on non-dominated sorting for exploration\nwith respect to skill and creativity factors, evaluation for actors and critics\nusing a quantile-based ensemble strategy, and exploiting actors with best skill\nfactor. Theoretical analysis proves the learning stability and bounded\nestimation bias for the MAMC. The present study examines the performance on a\nwell-known reinforcement learning benchmark MuJoCo. Experimental results show\nthat the proposed framework outperforms state-of-the-art deep deterministic\nbased reinforcement learning methods. Experimental analysis also indicates the\nproposed components are effective. Empirical analysis further investigates the\nvalidity of the proposed method, and shows its benefit on complicated problems.\nThe source code can be found at https://github.com/AndyWu101/MAMC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6f14\u5458\u591a\u8bc4\u8bba\u5bb6\uff08MAMC\uff09\u6df1\u5ea6\u786e\u5b9a\u6027\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u652f\u914d\u6392\u5e8f\u9009\u62e9\u6f14\u5458\u3001\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u96c6\u6210\u7b56\u7565\u8bc4\u4f30\u6f14\u5458\u548c\u8bc4\u8bba\u5bb6\uff0c\u4ee5\u53ca\u5229\u7528\u6700\u4f73\u6280\u80fd\u56e0\u5b50\u6f14\u5458\uff0c\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u5927\u89c4\u6a21\u79bb\u6563\u6216\u8fde\u7eed\u72b6\u6001/\u52a8\u4f5c\u7a7a\u95f4\u65f6\u7684\u5c40\u9650\u6027\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u591a\u8bc4\u8bba\u5bb6\u67b6\u6784\uff0c\u4f46\u5f88\u5c11\u8003\u8651\u591a\u6f14\u5458\u4e0e\u591a\u8bc4\u8bba\u5bb6\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMAMC\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7279\u5f81\uff1a\u57fa\u4e8e\u975e\u652f\u914d\u6392\u5e8f\u7684\u6f14\u5458\u9009\u62e9\uff08\u8003\u8651\u6280\u80fd\u548c\u521b\u9020\u529b\u56e0\u5b50\uff09\u3001\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u96c6\u6210\u8bc4\u4f30\u7b56\u7565\u3001\u4ee5\u53ca\u5229\u7528\u6700\u4f73\u6280\u80fd\u56e0\u5b50\u6f14\u5458\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u786e\u5b9a\u6027\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u5206\u6790\u8868\u660e\u6240\u63d0\u7ec4\u4ef6\u6709\u6548\uff0c\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u95ee\u9898\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "MAMC\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u5b66\u4e60\u548c\u6709\u754c\u7684\u4f30\u8ba1\u504f\u5dee\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01116", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01116", "abs": "https://arxiv.org/abs/2510.01116", "authors": ["Felix Parker", "Nimeesha Chan", "Chi Zhang", "Kimia Ghobadi"], "title": "Eliciting Chain-of-Thought Reasoning for Time Series Analysis using Reinforcement Learning", "comment": null, "summary": "Complex numerical time series analysis often demands multi-step reasoning\ncapabilities beyond current models' reach. Tasks like medical diagnosis and\nweather forecasting require sequential reasoning processes -- including\ncounterfactual analysis, logical deduction, knowledge application, and\nmulti-modal contextual integration -- that existing time series models cannot\nexplicitly perform. While recent research has shown large language models\n(LLMs) can achieve sophisticated Chain-of-Thought (CoT) reasoning through\nreinforcement learning (RL), these advances have primarily focused on\nmathematical and coding domains, with LLMs still demonstrating poor performance\non time series tasks. We introduce Chain Of thought for Understanding Numerical\nTime Series (COUNTS), the first framework that trains LLMs to perform CoT\nreasoning across diverse time series tasks using RL with verifiable rewards.\nOur approach employs a Residual Vector-Quantized VAE to create high-fidelity\ndiscrete tokens that seamlessly integrate into a pre-trained LLM's vocabulary.\nCOUNTS undergoes a two-stage training process: first, supervised fine-tuning on\ntime series analysis tasks to master our novel representations, followed by\nGroup Relative Policy Optimization training on verifiable problems using\nprompting strategies that encourage explicit reasoning steps before producing\nfinal answers. Our experiments demonstrate that this RL-driven approach with\nintermediate CoT reasoning significantly enhances LLM performance across\nvarious time series analysis tasks, opening new possibilities for complex\ntemporal data reasoning.", "AI": {"tldr": "COUNTS\u662f\u9996\u4e2a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u6267\u884c\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u65e0\u6cd5\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u5982\u53cd\u4e8b\u5b9e\u5206\u6790\u3001\u903b\u8f91\u63a8\u7406\u7b49\uff0c\u800cLLM\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u8bad\u7ec3\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6b8b\u5dee\u5411\u91cf\u91cf\u5316VAE\u521b\u5efa\u9ad8\u4fdd\u771f\u79bb\u6563token\uff0c\u96c6\u6210\u5230\u9884\u8bad\u7ec3LLM\u8bcd\u6c47\u8868\u4e2d\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u638c\u63e1\u65b0\u8868\u793a\uff0c\u7136\u540e\u4f7f\u7528Group Relative Policy Optimization\u5728\u53ef\u9a8c\u8bc1\u95ee\u9898\u4e0a\u8bad\u7ec3\uff0c\u9f13\u52b1\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u4e2d\u95f4\u601d\u7ef4\u94fe\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "RL\u9a71\u52a8\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u4e3a\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01123", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01123", "abs": "https://arxiv.org/abs/2510.01123", "authors": ["Lovish Madaan", "Aniket Didolkar", "Suchin Gururangan", "John Quan", "Ruan Silva", "Ruslan Salakhutdinov", "Manzil Zaheer", "Sanjeev Arora", "Anirudh Goyal"], "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators", "comment": "21 pages", "summary": "Reasoning training incentivizes LLMs to produce long chains of thought (long\nCoT), which among other things, allows them to explore solution strategies with\nself-checking. This results in higher accuracy, but inflates context length,\ntoken/compute cost, and answer latency. We ask: Can current models leverage\ntheir metacognition to provide other combinations on this Pareto frontier,\ne.g., better accuracy with lower context length and/or latency? Abstractly, we\nview the model as an improvement operator on its own \"thoughts\" with a\ncontinuum of possible strategies. We identify an interesting inference family\nParallel-Distill-Refine (PDR), which performs the following: (i) generate\ndiverse drafts in parallel; (ii) distill them into a bounded, textual\nworkspace; and (iii) refine conditioned on this workspace, producing an output\nthat seeds the next round. Importantly, context length (hence compute cost) is\ncontrollable via degree of parallelism, and is no longer conflated with the\ntotal number of generated tokens. We report PDR instantiations of current\nmodels that give better accuracy than long CoT while incurring lower latency.\nSetting degree of parallelism to 1 yields an interesting subcase, Sequential\nRefinement (SR) (iteratively improve a single candidate answer) which provides\nperformance superior to long CoT. Success of such model orchestrations raises\nthe question whether further training could shift the Pareto frontier. To this\nend, we train an 8B thinking model with Reinforcement Learning (RL) to make it\nconsistent with PDR as the inference method. On math tasks with verifiable\nanswers, iterative pipelines surpass single-pass baselines at matched\nsequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME\n2024 and +9% on AIME 2025).", "AI": {"tldr": "\u63d0\u51fa\u4e86Parallel-Distill-Refine (PDR)\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210\u8349\u7a3f\u3001\u84b8\u998f\u5230\u6709\u754c\u5de5\u4f5c\u7a7a\u95f4\u3001\u7136\u540e\u7cbe\u70bc\u7684\u65b9\u5f0f\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u7684\u63a8\u7406\u8bad\u7ec3\u5bfc\u81f4LLMs\u4ea7\u751f\u8fc7\u957f\u7684\u601d\u7ef4\u94fe\uff0c\u589e\u52a0\u4e86\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u3002\u9700\u8981\u5bfb\u627e\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u66f4\u597d\u7684\u6743\u8861\u65b9\u6848\u3002", "method": "\u63d0\u51faPDR\u65b9\u6cd5\uff1a1) \u5e76\u884c\u751f\u6210\u591a\u6837\u5316\u8349\u7a3f\uff1b2) \u84b8\u998f\u5230\u6709\u754c\u6587\u672c\u5de5\u4f5c\u7a7a\u95f4\uff1b3) \u57fa\u4e8e\u5de5\u4f5c\u7a7a\u95f4\u8fdb\u884c\u7cbe\u70bc\u3002\u901a\u8fc7\u63a7\u5236\u5e76\u884c\u5ea6\u6765\u7ba1\u7406\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "result": "PDR\u65b9\u6cd5\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u957f\u601d\u7ef4\u94fe\uff0c\u51c6\u786e\u7387\u63d0\u5347\uff08AIME 2024 +11%\uff0cAIME 2025 +9%\uff09\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "conclusion": "\u6a21\u578b\u7f16\u6392\u65b9\u6cd5\u5982PDR\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0ePDR\u63a8\u7406\u4e00\u81f4\u7684\u6a21\u578b\u80fd\u8fdb\u4e00\u6b65\u63a8\u52a8\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01161", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01161", "abs": "https://arxiv.org/abs/2510.01161", "authors": ["Haizhong Zheng", "Jiawei Zhao", "Bedi Chen"], "title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?", "comment": null, "summary": "Reinforcement learning has been central to recent advances in large language\nmodel reasoning, but most algorithms rely on on-policy training that demands\nfresh rollouts at every update, limiting efficiency and scalability.\nAsynchronous RL systems alleviate this by decoupling rollout generation from\ntraining, yet their effectiveness hinges on tolerating large staleness in\nrollout data, a setting where existing methods either degrade in performance or\ncollapse. We revisit this challenge and uncover a prosperity-before-collapse\nphenomenon: stale data can be as informative as on-policy data if exploited\nproperly. Building on this insight, we introduce M2PO (Second-Moment Trust\nPolicy Optimization), which constrains the second moment of importance weights\nto suppress only extreme outliers while preserving informative updates.\nNotably, M2PO sharply reduces the fraction of clipped tokens under high\nstaleness (from 1.22% to 0.06% over training), precisely masking high-variance\ntokens while maintaining stable optimization. Extensive evaluation across six\nmodels (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable\noff-policy training even with data stale by at least 256 model updates and\nmatches on-policy performance.", "AI": {"tldr": "M2PO\u662f\u4e00\u79cd\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u91cd\u8981\u6027\u6743\u91cd\u7684\u4e8c\u9636\u77e9\u6765\u6291\u5236\u6781\u7aef\u5f02\u5e38\u503c\uff0c\u5728\u9ad8\u5ea6\u9648\u65e7\u7684\u6570\u636e\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u7684\u79bb\u7b56\u7565\u8bad\u7ec3\uff0c\u6027\u80fd\u53ef\u5ab2\u7f8e\u5728\u7b56\u7565\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5927\u591a\u4f9d\u8d56\u5728\u7b56\u7565\u8bad\u7ec3\uff0c\u9700\u8981\u6bcf\u6b21\u66f4\u65b0\u65f6\u91cd\u65b0\u751f\u6210\u6570\u636e\uff0c\u6548\u7387\u4f4e\u4e14\u6269\u5c55\u6027\u5dee\u3002\u5f02\u6b65RL\u7cfb\u7edf\u867d\u7136\u89e3\u8026\u4e86\u6570\u636e\u751f\u6210\u548c\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u5bb9\u5fcd\u6570\u636e\u9648\u65e7\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u8bbe\u7f6e\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\u6216\u5d29\u6e83\u3002", "method": "\u63d0\u51faM2PO\uff08\u4e8c\u9636\u77e9\u4fe1\u4efb\u7b56\u7565\u4f18\u5316\uff09\uff0c\u901a\u8fc7\u7ea6\u675f\u91cd\u8981\u6027\u6743\u91cd\u7684\u4e8c\u9636\u77e9\u6765\u6291\u5236\u6781\u7aef\u5f02\u5e38\u503c\uff0c\u540c\u65f6\u4fdd\u7559\u4fe1\u606f\u6027\u66f4\u65b0\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u9648\u65e7\u6027\u4e0b\u88ab\u88c1\u526a\u7684token\u6bd4\u4f8b\uff0c\u7cbe\u786e\u5c4f\u853d\u9ad8\u65b9\u5deetoken\u5e76\u4fdd\u6301\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u57286\u4e2a\u6a21\u578b\uff081.7B\u523032B\uff09\u548c8\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cM2PO\u5373\u4f7f\u5728\u6570\u636e\u9648\u65e7\u81f3\u5c11256\u4e2a\u6a21\u578b\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5b9a\u7684\u79bb\u7b56\u7565\u8bad\u7ec3\uff0c\u5e76\u5339\u914d\u5728\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "M2PO\u8bc1\u660e\u4e86\u9648\u65e7\u6570\u636e\u5982\u679c\u88ab\u9002\u5f53\u5229\u7528\uff0c\u53ef\u4ee5\u50cf\u5728\u7b56\u7565\u6570\u636e\u4e00\u6837\u5177\u6709\u4fe1\u606f\u4ef7\u503c\uff0c\u4e3a\u9ad8\u6548\u7684\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01163", "categories": ["cs.LG", "stat.ML", "G.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.01163", "abs": "https://arxiv.org/abs/2510.01163", "authors": ["Wa\u00efss Azizian", "Ali Hasan"], "title": "How Does the Pretraining Distribution Shape In-Context Learning? Task Selection, Generalization, and Robustness", "comment": "52 pages, 12 figures", "summary": "The emergence of in-context learning (ICL) in large language models (LLMs)\nremains poorly understood despite its consistent effectiveness, enabling models\nto adapt to new tasks from only a handful of examples. To clarify and improve\nthese capabilities, we characterize how the statistical properties of the\npretraining distribution (e.g., tail behavior, coverage) shape ICL on numerical\ntasks. We develop a theoretical framework that unifies task selection and\ngeneralization, extending and sharpening earlier results, and show how\ndistributional properties govern sample efficiency, task retrieval, and\nrobustness. To this end, we generalize Bayesian posterior consistency and\nconcentration results to heavy-tailed priors and dependent sequences, better\nreflecting the structure of LLM pretraining data. We then empirically study how\nICL performance varies with the pretraining distribution on challenging tasks\nsuch as stochastic differential equations and stochastic processes with memory.\nTogether, these findings suggest that controlling key statistical properties of\nthe pretraining distribution is essential for building ICL-capable and reliable\nLLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u5206\u6790\u4e86\u9884\u8bad\u7ec3\u5206\u5e03\u7684\u7edf\u8ba1\u7279\u6027\u5982\u4f55\u5f71\u54cdICL\u5728\u6570\u503c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u4efb\u52a1\u9009\u62e9\u548c\u6cdb\u5316\u7684\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u6301\u7eed\u6709\u6548\uff0c\u4f46\u5176\u5de5\u4f5c\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002\u4f5c\u8005\u65e8\u5728\u9610\u660e\u9884\u8bad\u7ec3\u5206\u5e03\u7684\u7edf\u8ba1\u7279\u6027\u5982\u4f55\u5851\u9020ICL\u80fd\u529b\uff0c\u4ee5\u6784\u5efa\u66f4\u53ef\u9760\u548c\u6709\u6548\u7684LLM\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u4efb\u52a1\u9009\u62e9\u548c\u6cdb\u5316\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u8d1d\u53f6\u65af\u540e\u9a8c\u4e00\u81f4\u6027\u7ed3\u679c\u63a8\u5e7f\u5230\u91cd\u5c3e\u5148\u9a8c\u548c\u4f9d\u8d56\u5e8f\u5217\uff0c\u5e76\u5728\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u548c\u5e26\u8bb0\u5fc6\u7684\u968f\u673a\u8fc7\u7a0b\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u8868\u660e\u9884\u8bad\u7ec3\u5206\u5e03\u7684\u7edf\u8ba1\u7279\u6027\uff08\u5982\u5c3e\u90e8\u884c\u4e3a\u3001\u8986\u76d6\u8303\u56f4\uff09\u5bf9ICL\u7684\u6837\u672c\u6548\u7387\u3001\u4efb\u52a1\u68c0\u7d22\u548c\u9c81\u68d2\u6027\u5177\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002", "conclusion": "\u63a7\u5236\u9884\u8bad\u7ec3\u5206\u5e03\u7684\u5173\u952e\u7edf\u8ba1\u7279\u6027\u5bf9\u4e8e\u6784\u5efa\u5177\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u548c\u53ef\u9760\u6027\u7684LLM\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.aa871236", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fscabench-org%2Fhound%3Futm_source=tldrinfosec/1/010001999abbcf43-ef0b5066-139d-4a08-bd77-fd2075b6dcb2-000000/aJqOpixVc7OwvtMB4S96DgH1tRCMhf_GaqVmaHrMrbU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fscabench-org%2Fhound%3Futm_source=tldrinfosec/1/010001999abbcf43-ef0b5066-139d-4a08-bd77-fd2075b6dcb2-000000/aJqOpixVc7OwvtMB4S96DgH1tRCMhf_GaqVmaHrMrbU=424", "authors": ["TLDR Newsletter"], "title": "Hound", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fscabench-org%2Fhound%3Futm_source=tldrinfosec/1/010001999abbcf43-ef0b5066-139d-4a08-bd77-fd2075b6dcb2-000000/aJqOpixVc7OwvtMB4S96DgH1tRCMhf_GaqVmaHrMrbU=424", "summary": "Hound (GitHub Repo) Hound is a language-agnostic AI code auditor that autonomously builds and refines adaptive knowledge graphs for deep, interactive code reasoning.", "source": "tldr", "AI": {"tldr": "Hound\u662f\u4e00\u4e2a\u8bed\u8a00\u65e0\u5173\u7684AI\u4ee3\u7801\u5ba1\u8ba1\u5de5\u5177\uff0c\u80fd\u591f\u81ea\u4e3b\u6784\u5efa\u548c\u4f18\u5316\u81ea\u9002\u5e94\u77e5\u8bc6\u56fe\u8c31\uff0c\u5b9e\u73b0\u6df1\u5ea6\u4ea4\u4e92\u5f0f\u4ee3\u7801\u63a8\u7406", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4ee3\u7801\u5ba1\u8ba1\u5de5\u5177\u5728\u590d\u6742\u4ee3\u7801\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u667a\u80fd\u3001\u81ea\u9002\u5e94\u7684\u4ee3\u7801\u5ba1\u67e5\u80fd\u529b", "method": "\u4f7f\u7528\u81ea\u9002\u5e94\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u81ea\u4e3b\u5b66\u4e60\u548c\u4f18\u5316\u8fc7\u7a0b\u5b9e\u73b0\u6df1\u5ea6\u4ee3\u7801\u63a8\u7406", "result": "\u5f00\u53d1\u51fa\u4e86\u80fd\u591f\u8fdb\u884c\u8bed\u8a00\u65e0\u5173\u4ee3\u7801\u5ba1\u8ba1\u7684AI\u7cfb\u7edf\uff0c\u5177\u5907\u6df1\u5ea6\u4ea4\u4e92\u5f0f\u63a8\u7406\u80fd\u529b", "conclusion": "Hound\u5c55\u793a\u4e86AI\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u8ba1\u5de5\u5177\u5728\u6784\u5efa\u81ea\u9002\u5e94\u77e5\u8bc6\u56fe\u8c31\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u667a\u80fd\u4ee3\u7801\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5", "topic": "code agent"}}
{"id": "tldr.2509.d098d915", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-5%3Futm_source=tldrai/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/y483xnAb_fDwfXqK1JLbum92Ea-0Vpfb1KtvPBB_3x4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-5%3Futm_source=tldrai/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/y483xnAb_fDwfXqK1JLbum92Ea-0Vpfb1KtvPBB_3x4=424", "authors": ["TLDR Newsletter"], "title": "Claude Sonnet 4.5", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-5%3Futm_source=tldrai/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/y483xnAb_fDwfXqK1JLbum92Ea-0Vpfb1KtvPBB_3x4=424", "summary": "Claude Sonnet 4.5 (5 minute read) Anthropic's latest model boasts the highest score on the SWE-bench Verified (77.2%), alongside major improvements in computer use, reasoning, and math. The release includes major product updates: checkpoints in Claude Code for instant rollback, the Claude for Chrome extension for Max users, and the Claude Agent SDK that makes the infrastructure powering Claude Code available to all developers.", "source": "tldr", "AI": {"tldr": "Anthropic\u53d1\u5e03Claude Sonnet 4.5\u6a21\u578b\uff0c\u5728SWE-bench Verified\u4e0a\u83b7\u5f97\u6700\u9ad8\u520677.2%\uff0c\u5728\u8ba1\u7b97\u673a\u4f7f\u7528\u3001\u63a8\u7406\u548c\u6570\u5b66\u65b9\u9762\u6709\u91cd\u5927\u6539\u8fdb\u3002", "motivation": "\u63d0\u5347AI\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u4fee\u590d\u548c\u5f00\u53d1\u5de5\u5177\u96c6\u6210\u65b9\u9762\u3002", "method": "\u5f00\u53d1\u65b0\u7684Claude Sonnet 4.5\u6a21\u578b\uff0c\u5e76\u914d\u5957\u53d1\u5e03Claude Code\u68c0\u67e5\u70b9\u3001Chrome\u6269\u5c55\u548cAgent SDK\u7b49\u4ea7\u54c1\u66f4\u65b0\u3002", "result": "\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f9777.2%\u7684\u6700\u9ad8\u5206\uff0c\u663e\u793a\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u8fdb\u6b65\u3002", "conclusion": "Claude Sonnet 4.5\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u914d\u5957\u5de5\u5177\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u3002", "topic": "swe benchmark"}}
{"id": "tldr.2509.ed175049", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FUMuH0M/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/RKrwqPF9rj2RNNraKGH9QDJ5Y16JxNZrc9mus1LZUiA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FUMuH0M/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/RKrwqPF9rj2RNNraKGH9QDJ5Y16JxNZrc9mus1LZUiA=424", "authors": ["TLDR Newsletter"], "title": "Buy in ChatGPT with Instant Checkout", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FUMuH0M/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/RKrwqPF9rj2RNNraKGH9QDJ5Y16JxNZrc9mus1LZUiA=424", "summary": "Buy in ChatGPT with Instant Checkout (4 minute read) ChatGPT now supports direct purchases from Etsy sellers via Instant Checkout, with Shopify coming soon. The system runs on the open Agentic Commerce Protocol, co-developed with Stripe, and is open for merchant integrations.", "source": "tldr", "AI": {"tldr": "ChatGPT\u73b0\u5728\u652f\u6301\u901a\u8fc7\u5373\u65f6\u7ed3\u8d26\u529f\u80fd\u76f4\u63a5\u4eceEtsy\u5356\u5bb6\u8d2d\u4e70\u5546\u54c1\uff0cShopify\u5373\u5c06\u652f\u6301\u3002\u8be5\u7cfb\u7edf\u57fa\u4e8e\u4e0eStripe\u5171\u540c\u5f00\u53d1\u7684\u5f00\u653eAgentic Commerce\u534f\u8bae\uff0c\u5e76\u5411\u5546\u5bb6\u5f00\u653e\u96c6\u6210\u3002", "motivation": "\u7b80\u5316\u7535\u5546\u8d2d\u7269\u4f53\u9a8c\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u5728ChatGPT\u5bf9\u8bdd\u73af\u5883\u4e2d\u76f4\u63a5\u5b8c\u6210\u8d2d\u4e70\uff0c\u65e0\u9700\u8df3\u8f6c\u5230\u5916\u90e8\u5e73\u53f0\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eAgentic Commerce\u534f\u8bae\u7684\u5373\u65f6\u7ed3\u8d26\u7cfb\u7edf\uff0c\u4e0eStripe\u5408\u4f5c\u6784\u5efa\u652f\u4ed8\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u4e0eEtsy\u7b49\u7535\u5546\u5e73\u53f0\u96c6\u6210\u3002", "result": "\u6210\u529f\u5728ChatGPT\u4e2d\u5b9e\u73b0\u4e86Etsy\u5356\u5bb6\u7684\u76f4\u63a5\u8d2d\u4e70\u529f\u80fd\uff0cShopify\u96c6\u6210\u5373\u5c06\u63a8\u51fa\uff0c\u4e3a\u5546\u5bb6\u63d0\u4f9b\u4e86\u65b0\u7684\u9500\u552e\u6e20\u9053\u3002", "conclusion": "Agentic Commerce\u534f\u8bae\u4e3aAI\u5bf9\u8bdd\u73af\u5883\u4e2d\u7684\u7535\u5546\u4ea4\u6613\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u6539\u53d8\u7528\u6237\u8d2d\u7269\u65b9\u5f0f\u3002", "topic": "swe application"}}
{"id": "tldr.2509.a7941217", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-agents-with-the-claude-agent-sdk%3Futm_source=tldrai/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/b-ghmU8CZ6TG6x5xHvk3XUDv61Zv7rL_w0S9yeA8dx4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-agents-with-the-claude-agent-sdk%3Futm_source=tldrai/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/b-ghmU8CZ6TG6x5xHvk3XUDv61Zv7rL_w0S9yeA8dx4=424", "authors": ["TLDR Newsletter"], "title": "Anthropic launches Claude Agent SDK for building versatile AI agents", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-agents-with-the-claude-agent-sdk%3Futm_source=tldrai/1/010001999ac47d96-043df198-1aee-48d0-8a28-ba63b6a649f7-000000/b-ghmU8CZ6TG6x5xHvk3XUDv61Zv7rL_w0S9yeA8dx4=424", "summary": "Anthropic launches Claude Agent SDK for building versatile AI agents (11 minute read) Anthropic introduced the Claude Agent SDK, expanding its capabilities beyond coding to enable building versatile agents for tasks such as finance management and customer support. This SDK equips agents with tools for context gathering, executing tasks, and iterating based on feedback, optimizing workflows through features like bash scripting and subagents. Developers can leverage the SDK for immediate integr...", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51faClaude Agent SDK\uff0c\u7528\u4e8e\u6784\u5efa\u591a\u529f\u80fdAI\u4ee3\u7406\uff0c\u6269\u5c55\u4e86\u5176\u80fd\u529b\u8303\u56f4\uff0c\u4ece\u7f16\u7801\u5ef6\u4f38\u5230\u91d1\u878d\u7ba1\u7406\u548c\u5ba2\u6237\u652f\u6301\u7b49\u4efb\u52a1\u3002", "motivation": "\u6269\u5c55Claude\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u4e0d\u4ec5\u9650\u4e8e\u7f16\u7801\u4efb\u52a1\uff0c\u800c\u662f\u80fd\u591f\u6784\u5efa\u9002\u7528\u4e8e\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u901a\u7528AI\u4ee3\u7406\u3002", "method": "\u63d0\u4f9bSDK\u5de5\u5177\u5305\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u6536\u96c6\u3001\u4efb\u52a1\u6267\u884c\u3001\u57fa\u4e8e\u53cd\u9988\u8fed\u4ee3\u7b49\u529f\u80fd\uff0c\u652f\u6301bash\u811a\u672c\u548c\u5b50\u4ee3\u7406\u7b49\u7279\u6027\u6765\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5f00\u53d1\u8005\u53ef\u4ee5\u5229\u7528\u8be5SDK\u8fdb\u884c\u5373\u65f6\u96c6\u6210\uff0c\u6784\u5efa\u80fd\u591f\u5904\u7406\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u7684AI\u4ee3\u7406\u7cfb\u7edf\u3002", "conclusion": "Claude Agent SDK\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6784\u5efa\u591a\u529f\u80fdAI\u4ee3\u7406\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u663e\u8457\u6269\u5c55\u4e86AI\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2510.60387e3c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fvibe-then-verify%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-vibethenverify25%26utm_content=newsletter-devops-primary-placement-20251001-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/-onVZEomy4tVoSauKVL31CFpJ0eCPyV2r19c7VjZHbg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fvibe-then-verify%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-vibethenverify25%26utm_content=newsletter-devops-primary-placement-20251001-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/-onVZEomy4tVoSauKVL31CFpJ0eCPyV2r19c7VjZHbg=424", "authors": ["TLDR Newsletter"], "title": "The AI code paradox: Are you stuck?", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fvibe-then-verify%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-vibethenverify25%26utm_content=newsletter-devops-primary-placement-20251001-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/-onVZEomy4tVoSauKVL31CFpJ0eCPyV2r19c7VjZHbg=424", "summary": "The AI code paradox: Are you stuck? (Sponsor) AI writes code in seconds. But who verifies it? Manually reviewing all AI-generated code is where the bottleneck is. Don't let new technical debt and security risks slip past. SonarQube solves this engineering paradox, automatically reviewing all of your code so your team can: Embrace AI with confidence: Use AI coding assistants with automatic scans for quality. Prevent tech debt: Automatically detect and fix issues before they're merged. Maintain...", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u52a9\u624b\u5feb\u901f\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u4eba\u5de5\u9a8c\u8bc1\u6210\u4e3a\u74f6\u9888\uff0cSonarQube\u63d0\u4f9b\u81ea\u52a8\u4ee3\u7801\u5ba1\u67e5\u89e3\u51b3\u65b9\u6848", "motivation": "\u89e3\u51b3AI\u751f\u6210\u4ee3\u7801\u5e26\u6765\u7684\u6280\u672f\u503a\u52a1\u548c\u5b89\u5168\u98ce\u9669\uff0c\u6d88\u9664\u4eba\u5de5\u5ba1\u67e5\u7684\u74f6\u9888", "method": "\u4f7f\u7528SonarQube\u81ea\u52a8\u626b\u63cf\u6240\u6709AI\u751f\u6210\u7684\u4ee3\u7801\uff0c\u68c0\u6d4b\u8d28\u91cf\u95ee\u9898", "result": "\u80fd\u591f\u81ea\u4fe1\u4f7f\u7528AI\u7f16\u7801\u52a9\u624b\uff0c\u81ea\u52a8\u68c0\u6d4b\u548c\u4fee\u590d\u95ee\u9898\uff0c\u9632\u6b62\u6280\u672f\u503a\u52a1\u79ef\u7d2f", "conclusion": "SonarQube\u89e3\u51b3\u4e86AI\u4ee3\u7801\u5de5\u7a0b\u7684\u6096\u8bba\uff0c\u4f7f\u56e2\u961f\u80fd\u591f\u5b89\u5168\u9ad8\u6548\u5730\u4f7f\u7528AI\u7f16\u7801\u5de5\u5177", "topic": "swe application"}}
{"id": "tldr.2510.d5bf2c5b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Faws%2Fintroducing-claude-sonnet-4-5-in-amazon-bedrock-anthropics-most-intelligent-model-best-for-coding-and-complex-agents%2F%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/cC0IzkJfa-8O_e9g3Slur94CDzmZoMmU-yLycLXrIVs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Faws%2Fintroducing-claude-sonnet-4-5-in-amazon-bedrock-anthropics-most-intelligent-model-best-for-coding-and-complex-agents%2F%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/cC0IzkJfa-8O_e9g3Slur94CDzmZoMmU-yLycLXrIVs=424", "authors": ["TLDR Newsletter"], "title": "Introducing Claude Sonnet 4.5 in Amazon Bedrock: Anthropic's most intelligent model, best for coding and complex agents", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Faws%2Fintroducing-claude-sonnet-4-5-in-amazon-bedrock-anthropics-most-intelligent-model-best-for-coding-and-complex-agents%2F%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/cC0IzkJfa-8O_e9g3Slur94CDzmZoMmU-yLycLXrIVs=424", "summary": "Introducing Claude Sonnet 4.5 in Amazon Bedrock: Anthropic's most intelligent model, best for coding and complex agents (6 minute read) Anthropic's Claude Sonnet 4.5 is now available in Amazon Bedrock, offering state-of-the-art coding and agentic application performance. Claude Sonnet 4.5 features smart context window management, tool use clearing for efficiency, and cross-conversation memory, enhancing performance in tool handling, memory management, and context processing. It is available t...", "source": "tldr", "AI": {"tldr": "Claude Sonnet 4.5\u5728Amazon Bedrock\u4e2d\u63a8\u51fa\uff0c\u8fd9\u662fAnthropic\u6700\u667a\u80fd\u7684\u6a21\u578b\uff0c\u7279\u522b\u64c5\u957f\u7f16\u7801\u548c\u590d\u6742\u4ee3\u7406\u5e94\u7528", "motivation": "\u63d0\u4f9b\u6700\u5148\u8fdb\u7684\u7f16\u7801\u548c\u4ee3\u7406\u5e94\u7528\u6027\u80fd\uff0c\u901a\u8fc7\u667a\u80fd\u4e0a\u4e0b\u6587\u7a97\u53e3\u7ba1\u7406\u3001\u5de5\u5177\u4f7f\u7528\u6e05\u7406\u548c\u8de8\u5bf9\u8bdd\u8bb0\u5fc6\u7b49\u529f\u80fd\u6765\u63d0\u5347\u6548\u7387", "method": "\u91c7\u7528\u667a\u80fd\u4e0a\u4e0b\u6587\u7a97\u53e3\u7ba1\u7406\u3001\u5de5\u5177\u4f7f\u7528\u6e05\u7406\u4ee5\u63d0\u9ad8\u6548\u7387\u3001\u8de8\u5bf9\u8bdd\u8bb0\u5fc6\u529f\u80fd\uff0c\u589e\u5f3a\u5de5\u5177\u5904\u7406\u3001\u5185\u5b58\u7ba1\u7406\u548c\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b", "result": "\u5728Amazon Bedrock\u4e2d\u53ef\u7528\uff0c\u63d0\u4f9b\u5353\u8d8a\u7684\u7f16\u7801\u548c\u590d\u6742\u4ee3\u7406\u5e94\u7528\u6027\u80fd", "conclusion": "Claude Sonnet 4.5\u662fAnthropic\u6700\u667a\u80fd\u7684\u6a21\u578b\uff0c\u5728\u7f16\u7801\u548c\u4ee3\u7406\u5e94\u7528\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u73b0\u5df2\u96c6\u6210\u5230Amazon Bedrock\u5e73\u53f0", "topic": "code agent"}}
{"id": "tldr.2510.7600346a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mintlify.com%2Fblog%2Fwe-built-our-coding-agent-for-slack%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/YG0J2Ls_GIxECiupWonseSqTpy0XMVVErdO-OppG1uA=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mintlify.com%2Fblog%2Fwe-built-our-coding-agent-for-slack%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/YG0J2Ls_GIxECiupWonseSqTpy0XMVVErdO-OppG1uA=424", "authors": ["TLDR Newsletter"], "title": "We built our coding agent for Slack instead of the terminal", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mintlify.com%2Fblog%2Fwe-built-our-coding-agent-for-slack%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/YG0J2Ls_GIxECiupWonseSqTpy0XMVVErdO-OppG1uA=424", "summary": "We built our coding agent for Slack instead of the terminal (8 minute read) Mintlify built its coding agent for Slack instead of the terminal to make documentation updates feel as easy as sending a message. By integrating directly into daily workflows, using a focused toolset, and supporting agentic automation like auto-updating changelogs from PRs, Mintlify reduces friction and turns documentation from a dreaded chore into a seamless process.", "source": "tldr", "AI": {"tldr": "Mintlify\u5f00\u53d1\u4e86\u96c6\u6210\u5230Slack\u7684\u4ee3\u7801\u4ee3\u7406\uff0c\u4f7f\u6587\u6863\u66f4\u65b0\u50cf\u53d1\u9001\u6d88\u606f\u4e00\u6837\u7b80\u5355\uff0c\u901a\u8fc7\u51cf\u5c11\u6469\u64e6\u5c06\u6587\u6863\u5de5\u4f5c\u53d8\u6210\u65e0\u7f1d\u6d41\u7a0b\u3002", "motivation": "\u8ba9\u6587\u6863\u66f4\u65b0\u53d8\u5f97\u50cf\u53d1\u9001\u6d88\u606f\u4e00\u6837\u7b80\u5355\uff0c\u51cf\u5c11\u6587\u6863\u7ef4\u62a4\u7684\u6469\u64e6\u548c\u8d1f\u62c5\u3002", "method": "\u5c06\u4ee3\u7801\u4ee3\u7406\u96c6\u6210\u5230Slack\u800c\u975e\u7ec8\u7aef\u4e2d\uff0c\u4f7f\u7528\u4e13\u6ce8\u7684\u5de5\u5177\u96c6\uff0c\u652f\u6301\u4ee3\u7406\u81ea\u52a8\u5316\u5982\u4ecePR\u81ea\u52a8\u66f4\u65b0\u53d8\u66f4\u65e5\u5fd7\u3002", "result": "\u6210\u529f\u5c06\u6587\u6863\u4ece\u4ee4\u4eba\u754f\u60e7\u7684\u7410\u4e8b\u8f6c\u53d8\u4e3a\u65e0\u7f1d\u6d41\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u5230\u65e5\u5e38\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u4ee3\u7801\u4ee3\u7406\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6587\u6863\u7ef4\u62a4\u7684\u6469\u64e6\u3002", "topic": "swe application"}}
{"id": "tldr.2510.1849af79", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FCodebuffAI%2Fcodebuff%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/H8JZJCYcp93qIs00giG54HCq5sldCjMfNLm33ygrO_g=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FCodebuffAI%2Fcodebuff%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/H8JZJCYcp93qIs00giG54HCq5sldCjMfNLm33ygrO_g=424", "authors": ["TLDR Newsletter"], "title": "Codebuff", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FCodebuffAI%2Fcodebuff%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/H8JZJCYcp93qIs00giG54HCq5sldCjMfNLm33ygrO_g=424", "summary": "Codebuff (GitHub Repo) Codebuff, an open-source AI coding assistant, was found to outperform Claude Code in coding tasks, achieving a 61% success rate compared to 53%. Using a multi-agent approach, Codebuff coordinates specialized agents to understand projects and make precise code changes. It also supports any model available on OpenRouter.", "source": "tldr", "AI": {"tldr": "Codebuff\u662f\u4e00\u4e2a\u5f00\u6e90AI\u7f16\u7a0b\u52a9\u624b\uff0c\u5728\u591a\u4ee3\u7406\u65b9\u6cd5\u4e0b\uff0c\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eClaude Code\uff0c\u6210\u529f\u7387\u8fbe\u523061% vs 53%\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7406\u89e3\u9879\u76ee\u5e76\u7cbe\u786e\u4fee\u6539\u4ee3\u7801\u7684\u591a\u4ee3\u7406AI\u7f16\u7a0b\u52a9\u624b\uff0c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u548c\u4fee\u6539\u7684\u6210\u529f\u7387\u3002", "method": "\u91c7\u7528\u591a\u4ee3\u7406\u65b9\u6cd5\uff0c\u534f\u8c03\u4e13\u95e8\u5316\u4ee3\u7406\u6765\u7406\u89e3\u9879\u76ee\u7ed3\u6784\u5e76\u8fdb\u884c\u7cbe\u786e\u7684\u4ee3\u7801\u66f4\u6539\uff0c\u540c\u65f6\u652f\u6301OpenRouter\u4e0a\u7684\u4efb\u4f55\u6a21\u578b\u3002", "result": "\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8661%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u4e86Claude Code\u768453%\u6210\u529f\u7387\u3002", "conclusion": "Codebuff\u901a\u8fc7\u591a\u4ee3\u7406\u67b6\u6784\u5728AI\u7f16\u7a0b\u52a9\u624b\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4ee3\u7801\u7406\u89e3\u548c\u4fee\u6539\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2510.dbe9edf4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Fnews%2F2025%2F09%2Faws-cdk-refactor-safe-iac%2F%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/MPZj1QtEN1SNbRFPN9Z77oN-xAdWZ5QIyBG1QBbfeKo=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Fnews%2F2025%2F09%2Faws-cdk-refactor-safe-iac%2F%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/MPZj1QtEN1SNbRFPN9Z77oN-xAdWZ5QIyBG1QBbfeKo=424", "authors": ["TLDR Newsletter"], "title": "AWS CDK Refactor Feature: Safe Infrastructure as Code Renaming", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Fnews%2F2025%2F09%2Faws-cdk-refactor-safe-iac%2F%3Futm_source=tldrdevops/1/010001999f746107-70b8392e-edc8-4330-9cae-fd42c3e41322-000000/MPZj1QtEN1SNbRFPN9Z77oN-xAdWZ5QIyBG1QBbfeKo=424", "summary": "AWS CDK Refactor Feature: Safe Infrastructure as Code Renaming (2 minute read) AWS has a new CDK refactor feature that lets engineers safely rename and reorganize infrastructure as code without triggering destructive resource replacements, reducing downtime and data loss risks.", "source": "tldr", "AI": {"tldr": "AWS\u63a8\u51faCDK\u91cd\u6784\u529f\u80fd\uff0c\u5141\u8bb8\u5de5\u7a0b\u5e08\u5b89\u5168\u5730\u91cd\u547d\u540d\u548c\u91cd\u7ec4\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\uff0c\u907f\u514d\u89e6\u53d1\u7834\u574f\u6027\u8d44\u6e90\u66ff\u6362\uff0c\u51cf\u5c11\u505c\u673a\u65f6\u95f4\u548c\u6570\u636e\u4e22\u5931\u98ce\u9669\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u91cd\u6784\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u7834\u574f\u6027\u8d44\u6e90\u66ff\u6362\u95ee\u9898\uff0c\u964d\u4f4e\u8fd0\u7ef4\u98ce\u9669\u3002", "method": "AWS CDK\u6846\u67b6\u63d0\u4f9b\u65b0\u7684\u91cd\u6784\u529f\u80fd\uff0c\u652f\u6301\u5b89\u5168\u7684\u91cd\u547d\u540d\u548c\u91cd\u7ec4\u64cd\u4f5c\u3002", "result": "\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u91cd\u6784\uff0c\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684\u8d44\u6e90\u66ff\u6362\u548c\u505c\u673a\u3002", "conclusion": "\u8be5\u529f\u80fd\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u7ba1\u7406\u7684\u5b89\u5168\u6027\u548c\u7075\u6d3b\u6027\u3002", "topic": "swe application"}}
{"id": "tldr.2510.efed2dd9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Feffective-context-engineering-for-ai-agents%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/LQyihINTew_HPZ7XM-cDv9JY8Z4OthpXQdaHejxquWA=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Feffective-context-engineering-for-ai-agents%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/LQyihINTew_HPZ7XM-cDv9JY8Z4OthpXQdaHejxquWA=425", "authors": ["TLDR Newsletter"], "title": "Effective Context Engineering for AI Agents", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Feffective-context-engineering-for-ai-agents%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/LQyihINTew_HPZ7XM-cDv9JY8Z4OthpXQdaHejxquWA=425", "summary": "Effective Context Engineering for AI Agents (17 minute read) Context engineering focuses on optimizing the tokens within an AI model's limited context window for the best outcomes. Good context engineering means curating and maintaining the most relevant information for each inference, including system instructions, tools, data, and message history. Messages can experience \"context rot\" and lose focus with excessive or irrelevant information, so agents like Claude Code use compaction, note-ta...", "source": "tldr", "AI": {"tldr": "Context engineering optimizes tokens in AI model's limited context window for best outcomes by curating relevant information including system instructions, tools, data, and message history.", "motivation": "AI models have limited context windows, and messages can experience 'context rot' with excessive or irrelevant information, reducing effectiveness.", "method": "Agents like Claude Code use compaction, note-taking, and other techniques to maintain focus and relevance in the context window.", "result": "Effective context engineering improves AI agent performance by ensuring the most relevant information is available for each inference.", "conclusion": "Proper context engineering is crucial for maximizing AI agent effectiveness within limited context windows.", "topic": "agent analysis"}}
{"id": "tldr.2510.7392dc31", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fcodex-vs-claude-code%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/ZVdbeZFZtq_DO5gKTISh-sNsqCzF0oRdrdbYtuHSbog=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fcodex-vs-claude-code%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/ZVdbeZFZtq_DO5gKTISh-sNsqCzF0oRdrdbYtuHSbog=425", "authors": ["TLDR Newsletter"], "title": "Codex vs Claude Code: which is the better AI coding agent?", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fcodex-vs-claude-code%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/ZVdbeZFZtq_DO5gKTISh-sNsqCzF0oRdrdbYtuHSbog=425", "summary": "Codex vs Claude Code: which is the better AI coding agent? (8 minute read) This article compares three AI coding agents - Codex, Claude Code, and Cursor - across various dimensions like features, pricing, and user experience. Codex is the author's preferred choice due to its smooth GitHub integration that catches bugs and allows for easy PR management, along with more generous usage limits.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cdAI\u7f16\u7a0b\u52a9\u624b\uff08Codex\u3001Claude Code\u548cCursor\uff09\u5728\u529f\u80fd\u3001\u5b9a\u4ef7\u548c\u7528\u6237\u4f53\u9a8c\u7b49\u65b9\u9762\u7684\u8868\u73b0\uff0c\u6700\u7ec8\u63a8\u8350Codex\u4f5c\u4e3a\u6700\u4f73\u9009\u62e9\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540cAI\u7f16\u7a0b\u52a9\u624b\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u9009\u62e9\u6700\u9002\u5408\u7684\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u4e09\u79cdAI\u7f16\u7a0b\u52a9\u624b\u5728\u529f\u80fd\u7279\u6027\u3001\u5b9a\u4ef7\u7b56\u7565\u548c\u7528\u6237\u4f53\u9a8c\u7b49\u7ef4\u5ea6\u7684\u5dee\u5f02\u3002", "result": "Codex\u5728GitHub\u96c6\u6210\u3001\u9519\u8bef\u68c0\u6d4b\u548cPR\u7ba1\u7406\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u62e5\u6709\u66f4\u5bbd\u677e\u7684\u4f7f\u7528\u9650\u5236\u3002", "conclusion": "Codex\u662f\u4f5c\u8005\u63a8\u8350\u7684AI\u7f16\u7a0b\u52a9\u624b\uff0c\u56e0\u5176\u6d41\u7545\u7684GitHub\u96c6\u6210\u548c\u66f4\u597d\u7684\u7528\u6237\u4f53\u9a8c\u3002", "topic": "code agent"}}
{"id": "tldr.2510.cb4a6b13", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/Ax_1lv480fUymBcoB41jjoUghcv54cj0Y-oLLOJz_TI=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/Ax_1lv480fUymBcoB41jjoUghcv54cj0Y-oLLOJz_TI=425", "authors": ["TLDR Newsletter"], "title": "Designing agentic loops", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrwebdev/1/010001999f76b939-8ce4cdbc-12a1-4bfd-9814-73eff943dd7f-000000/Ax_1lv480fUymBcoB41jjoUghcv54cj0Y-oLLOJz_TI=425", "summary": "Designing agentic loops (9 minute read) Coding agents like Claude Code and Codex CLI are improving automated code production with automated error correction and experimentation. These agentic loops are when agents use tools to achieve a specific goal through iteration, and they function best with clear goals and defined tools. Running agents in \"YOLO mode\" can be very productive but also risky, requiring careful sandboxing.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u667a\u80fd\u7f16\u7801\u4ee3\u7406\uff08\u5982Claude Code\u548cCodex CLI\uff09\u901a\u8fc7\u8fed\u4ee3\u5f0f\u4ee3\u7406\u5faa\u73af\u6765\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u4ea7\uff0c\u5305\u62ec\u9519\u8bef\u4fee\u6b63\u548c\u5b9e\u9a8c\uff0c\u5f3a\u8c03\u660e\u786e\u76ee\u6807\u548c\u5de5\u5177\u5b9a\u4e49\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5728YOLO\u6a21\u5f0f\u4e0b\u8fd0\u884c\u7684\u98ce\u9669\u4e0e\u6c99\u76d2\u4fdd\u62a4\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u7f16\u7801\u4ee3\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u4ea7\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u9700\u8981\u6709\u6548\u7684\u4ee3\u7406\u5faa\u73af\u673a\u5236\u6765\u786e\u4fdd\u76ee\u6807\u8fbe\u6210\u548c\u98ce\u9669\u7ba1\u7406\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u660e\u786e\u7684\u5de5\u5177\u548c\u76ee\u6807\uff0c\u8ba9\u7f16\u7801\u4ee3\u7406\u5728\u8fed\u4ee3\u5faa\u73af\u4e2d\u6267\u884c\u4efb\u52a1\uff0c\u5305\u62ec\u9519\u8bef\u4fee\u6b63\u548c\u5b9e\u9a8c\uff0c\u540c\u65f6\u8003\u8651\u5728YOLO\u6a21\u5f0f\u4e0b\u7684\u6c99\u76d2\u4fdd\u62a4\u3002", "result": "\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u4ea7\uff0c\u4f46YOLO\u6a21\u5f0f\u867d\u7136\u9ad8\u6548\u5374\u5b58\u5728\u98ce\u9669\uff0c\u9700\u8981\u9002\u5f53\u7684\u6c99\u76d2\u673a\u5236\u3002", "conclusion": "\u4ee3\u7406\u5faa\u73af\u662f\u7f16\u7801\u4ee3\u7406\u6210\u529f\u7684\u5173\u952e\uff0c\u9700\u8981\u5e73\u8861\u751f\u4ea7\u6548\u7387\u4e0e\u98ce\u9669\u7ba1\u7406\uff0c\u7279\u522b\u662f\u5728YOLO\u6a21\u5f0f\u4e0b\u3002", "topic": "code agent"}}
{"id": "tldr.2510.bbedb659", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fapp.hiclip.ai%2F%3Futm_source=tldrdesign/1/010001999faa4fc6-395af730-21e3-4a38-831b-27b4c0f4be4e-000000/-2ttav9xKRJcAz8SD3LFyVa5cn5U3E3QRMQxcKWYkZo=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fapp.hiclip.ai%2F%3Futm_source=tldrdesign/1/010001999faa4fc6-395af730-21e3-4a38-831b-27b4c0f4be4e-000000/-2ttav9xKRJcAz8SD3LFyVa5cn5U3E3QRMQxcKWYkZo=425", "authors": ["TLDR Newsletter"], "title": "AI Video Clipping Agent", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fapp.hiclip.ai%2F%3Futm_source=tldrdesign/1/010001999faa4fc6-395af730-21e3-4a38-831b-27b4c0f4be4e-000000/-2ttav9xKRJcAz8SD3LFyVa5cn5U3E3QRMQxcKWYkZo=425", "summary": "AI Video Clipping Agent (Website) HiClip instantly turns long videos into trending Shorts, Reels, and TikTok. Its AI identifies high-engagement moments, adds captions, and reframes content for all platforms.", "source": "tldr", "AI": {"tldr": "HiClip\u662f\u4e00\u4e2aAI\u89c6\u9891\u526a\u8f91\u4ee3\u7406\uff0c\u80fd\u591f\u81ea\u52a8\u5c06\u957f\u89c6\u9891\u8f6c\u6362\u4e3a\u9002\u5408Shorts\u3001Reels\u548cTikTok\u7684\u77ed\u89c6\u9891\uff0c\u901a\u8fc7AI\u8bc6\u522b\u9ad8\u53c2\u4e0e\u5ea6\u65f6\u523b\u5e76\u6dfb\u52a0\u5b57\u5e55\u548c\u91cd\u65b0\u6784\u56fe\u3002", "motivation": "\u89e3\u51b3\u624b\u52a8\u526a\u8f91\u957f\u89c6\u9891\u4e3a\u77ed\u89c6\u9891\u7684\u7e41\u7410\u8fc7\u7a0b\uff0c\u5229\u7528AI\u81ea\u52a8\u5316\u8bc6\u522b\u548c\u4f18\u5316\u89c6\u9891\u5185\u5bb9\u4ee5\u9002\u5e94\u4e0d\u540c\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528AI\u6280\u672f\u81ea\u52a8\u8bc6\u522b\u89c6\u9891\u4e2d\u7684\u9ad8\u53c2\u4e0e\u5ea6\u65f6\u523b\uff0c\u81ea\u52a8\u6dfb\u52a0\u5b57\u5e55\uff0c\u5e76\u91cd\u65b0\u6784\u56fe\u4ee5\u9002\u5e94\u4e0d\u540c\u5e73\u53f0\u7684\u89c6\u9891\u683c\u5f0f\u8981\u6c42\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u5feb\u901f\u5c06\u957f\u89c6\u9891\u8f6c\u6362\u4e3a\u9002\u5408\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u77ed\u89c6\u9891\u7684\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u89c6\u9891\u526a\u8f91\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "conclusion": "HiClip\u901a\u8fc7AI\u81ea\u52a8\u5316\u89c6\u9891\u526a\u8f91\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c06\u957f\u89c6\u9891\u8f6c\u6362\u4e3a\u77ed\u89c6\u9891\u7684\u6548\u7387\u548c\u5438\u5f15\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u3002", "topic": "swe application"}}
{"id": "wechat.2510.3c83fe4e", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNDk2MTcyOQ==&mid=2247484409&idx=1&sn=afb8ef9a7ce574665f69acbd2570d291&chksm=fb94ae38bd586ceb2d51abe18a53a25d8779e53c3ae1822693589008bda1b3db2f1c1bae6fcd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNDk2MTcyOQ==&mid=2247484409&idx=1&sn=afb8ef9a7ce574665f69acbd2570d291&chksm=fb94ae38bd586ceb2d51abe18a53a25d8779e53c3ae1822693589008bda1b3db2f1c1bae6fcd#rd", "authors": ["Agent\u65f6\u4ee3"], "title": "5000\u4e07\u7f8e\u5143\uff01Phaidra\u7528\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u91cd\u65b0\u5b9a\u4e49\u6570\u636e\u4e2d\u5fc3\u6548\u7387", "comment": "Source: WeChat, Published: 2025-10-02 13:22:04", "summary": "\u4eceAlphaGo\u5230\u6570\u636e\u4e2d\u5fc3\uff1a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u4e1a\u5316\u5e94\u7528Phaidra\u7684\u521b\u59cb\u56e2\u961f\u80cc\u666f\u6781\u5177\u8bf4\u670d\u529b\u3002CEO Jim Gao\u66fe\u5728Google\u9886\u5bfc\u6570\u636e\u4e2d\u5fc3\u4f18\u5316\u9879\u76ee\uff0c\u901a\u8fc7AI\u6280\u672f\u5c06Google\u6570\u636e\u4e2d\u5fc3\u7684\u51b7\u5374\u80fd\u8017\u964d\u4f4e\u4e8630%\uff1b", "AI": {"tldr": "\u4eceAlphaGo\u5230\u6570\u636e\u4e2d\u5fc3\uff1a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u4e1a\u5316\u5e94\u7528Phaidra\u7684\u521b\u59cb\u56e2\u961f\u80cc\u666f\u6781\u5177\u8bf4\u670d\u529b\u3002CEO Jim Gao\u66fe\u5728Google\u9886\u5bfc\u6570\u636e\u4e2d\u5fc3\u4f18\u5316\u9879\u76ee\uff0c\u901a\u8fc7AI\u6280\u672f\u5c06Google\u6570\u636e\u4e2d\u5fc3\u7684\u51b7\u5374\u80fd\u8017\u964d\u4f4e\u4e8630%\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.aa53533e", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNzYwMDM4OA==&mid=2247483871&idx=1&sn=12566420acab810f4ecb45d483826dde&chksm=c3c2dd00a5339219749f1508ba96d3984a1660d08583e599e2afdbfebb3ab08d7183283590f9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNzYwMDM4OA==&mid=2247483871&idx=1&sn=12566420acab810f4ecb45d483826dde&chksm=c3c2dd00a5339219749f1508ba96d3984a1660d08583e599e2afdbfebb3ab08d7183283590f9#rd", "authors": ["NoMi"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7f51\u7edc\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u2014\u2014\u6570\u5b66\u57fa\u7840", "comment": "Source: WeChat, Published: 2025-10-02 11:25:27", "summary": "\u5f3a\u5316\u5b66\u4e60 \u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u4e2a\u6bd4\u8f83\u7b80\u5355\u7684\u5206\u652f\uff0c\u662f\u673a\u5668\u5b66\u4e60\u65d7\u4e0b\u7684\u4e09\u5143\u731b\u5c06\u4e4b\u4e00\u5373\uff1a\u76d1\u7763\u5b66\u4e60\u3001\u975e\u76d1\u7763\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3002\u5bf9\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u91cc\u6211\u653e\u51fa\u4e00\u5f20OpenAI\u7684\u56fe\u7247\uff0c\u672c\u7cfb\u5217\u5c06\u4f1a\u5e26\u5927\u5bb6\u4ece\u539f\u7406\u5230\u4ee3\u7801\u5230\u5b9e\u8df5\uff0c\u5b8c\u6210\u8fd9\u4e9b\u5185\u5bb9\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60 \u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u4e2a\u6bd4\u8f83\u7b80\u5355\u7684\u5206\u652f\uff0c\u662f\u673a\u5668\u5b66\u4e60\u65d7\u4e0b\u7684\u4e09\u5143\u731b\u5c06\u4e4b\u4e00\u5373\uff1a\u76d1\u7763\u5b66\u4e60\u3001\u975e\u76d1\u7763\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3002\u5bf9\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u91cc\u6211\u653e\u51fa\u4e00\u5f20OpenAI\u7684\u56fe\u7247\uff0c\u672c\u7cfb\u5217\u5c06\u4f1a\u5e26\u5927\u5bb6\u4ece\u539f\u7406\u5230\u4ee3\u7801\u5230\u5b9e\u8df5\uff0c\u5b8c\u6210\u8fd9\u4e9b\u5185\u5bb9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.27891b7e", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMTkxNjQ1Mg==&mid=2247484752&idx=1&sn=8992ac7d3005dfc2d6f71c1af7befa75&chksm=f8d3bd5d1b49afc6fe2dae9ee853ae23bd0e728a864f6848008d88a021aaf4b593038537641c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMTkxNjQ1Mg==&mid=2247484752&idx=1&sn=8992ac7d3005dfc2d6f71c1af7befa75&chksm=f8d3bd5d1b49afc6fe2dae9ee853ae23bd0e728a864f6848008d88a021aaf4b593038537641c#rd", "authors": ["\u6dfb\u8d22\u7329\u7329\u5bb6\u65cf\u529e\u516c\u5ba4"], "title": "\u5f53AI\u5b66\u4f1a\u201c\u5403\u4e00\u5811\u957f\u4e00\u667a\u201d\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5982\u4f55\u91cd\u5851A\u80a1\u4ea4\u6613\u7b56\u7565\uff1f", "comment": "Source: WeChat, Published: 2025-10-02 07:01:27", "summary": "\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u9700\u8981\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u91cf\u8bad\u7ec3\u3002\u5982\u4f55\u6784\u5efa\u4e00\u4e2a\u9ad8\u5ea6\u903c\u771f\u3001\u80fd\u53cd\u6620A\u80a1\u7279\u8272\uff08\u5982\u6da8\u8dcc\u505c\u677f\u9650\u5236\u3001T+1\u4ea4\u6613\u3001\u5370\u82b1\u7a0e\u6210\u672c\uff09\u7684\u6a21\u62df\u5668\u81f3\u5173\u91cd\u8981\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u9700\u8981\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u91cf\u8bad\u7ec3\u3002\u5982\u4f55\u6784\u5efa\u4e00\u4e2a\u9ad8\u5ea6\u903c\u771f\u3001\u80fd\u53cd\u6620A\u80a1\u7279\u8272\uff08\u5982\u6da8\u8dcc\u505c\u677f\u9650\u5236\u3001T+1\u4ea4\u6613\u3001\u5370\u82b1\u7a0e\u6210\u672c\uff09\u7684\u6a21\u62df\u5668\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.06450f68", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxMzE1ODEwNA==&mid=2247487968&idx=1&sn=51112e3c599c57c7a65d1edd8cd0fea4&chksm=96e2974abf875228b5ffb91efd7e3d3a81c35b4bb0ec3eea47476448b6282ddf8b4f4dc40efd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxMzE1ODEwNA==&mid=2247487968&idx=1&sn=51112e3c599c57c7a65d1edd8cd0fea4&chksm=96e2974abf875228b5ffb91efd7e3d3a81c35b4bb0ec3eea47476448b6282ddf8b4f4dc40efd#rd", "authors": ["\u5fae\u6559\u4e91\u80b2"], "title": "\u63a8\u7406\u80fd\u529b\u81ea\u4e3b\u8fdb\u5316\uff1aDeepSeek-R1\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-02 06:52:41", "summary": "\u5f3a\u5316\u5b66\u4e60\u7684\u5de8\u5927\u6f5c\u529b\u81ea\u4e3b\u53d1\u73b0\u4eba\u7c7b\u672a\u66fe\u60f3\u5230\u7684\u63a8\u7406\u7b56\u7565\u53d1\u5c55\u51fa\u81ea\u6211\u9a8c\u8bc1\u3001\u53cd\u601d\u7b49\u9ad8\u7ea7\u8ba4\u77e5\u80fd\u529b\u5b9e\u73b0\u771f\u6b63\u7684\u80fd\u529b\"\u8fdb\u5316\"\u800c\u975e\"\u6a21\u4eff\" \u5c40\u9650\u4e0e\u672a\u6765\u65b9\u5411\u5f53\u524d\u6311\u6218", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u7684\u5de8\u5927\u6f5c\u529b\u81ea\u4e3b\u53d1\u73b0\u4eba\u7c7b\u672a\u66fe\u60f3\u5230\u7684\u63a8\u7406\u7b56\u7565\u53d1\u5c55\u51fa\u81ea\u6211\u9a8c\u8bc1\u3001\u53cd\u601d\u7b49\u9ad8\u7ea7\u8ba4\u77e5\u80fd\u529b\u5b9e\u73b0\u771f\u6b63\u7684\u80fd\u529b\"\u8fdb\u5316\"\u800c\u975e\"\u6a21\u4eff\" \u5c40\u9650\u4e0e\u672a\u6765\u65b9\u5411\u5f53\u524d\u6311\u6218", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.e302f596", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247495758&idx=1&sn=2d88373cc450d1dc08c1003ce58677c5&chksm=c1f781328209aae033e124ad4df95ac71bf9d0b735195873355f2ca517c6b2466b1ead6a4d01#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&mid=2247495758&idx=1&sn=2d88373cc450d1dc08c1003ce58677c5&chksm=c1f781328209aae033e124ad4df95ac71bf9d0b735195873355f2ca517c6b2466b1ead6a4d01#rd", "authors": ["\u65fa\u77e5\u8bc6"], "title": "\u7cbe|\u5fae\u8f6fCVPR'25\u7b80\u660e\u6559\u7a0b\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8bad\u7ec3\u591a\u6a21\u6001\u667a\u80fd\u4f53\uff0c\u6784\u5efa\u611f\u77e5\u601d\u8003\u884c\u52a8\u5b8c\u6574\u95ed\u73af\uff01", "comment": "Source: WeChat, Published: 2025-10-02 03:11:35", "summary": "\u5f3a\u5316\u5b66\u4e60\u8d4b\u80fd\u4f5c\u8005\uff1a\u5f20\u957f\u65fa\uff0c\u56fe\u6e90\uff1a\u65fa\u77e5\u8bc6\u5fae\u8f6f\u56e2\u961f\u5728CVPR'25\u7684\u6559\u7a0b\u4e2d\u7ed9\u51fa\u4e86\u7cfb\u7edf\u6027\u7b54\u6848\uff1a\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e3a\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6ce8\u5165\u201c\u89c6\u89c9\u601d\u8003\u201d\u80fd\u529b\uff0c\u4ece\u56fe\u50cf\u751f\u6210\u8f85\u52a9\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u63d0\u5347\u7cbe\u5ea6\uff0c\u5230\u591a\u8f6e\u8f68\u8ff9\u4f18\u5316\u7a33\u5b9a\u8bad\u7ec3\uff0c\u6784\u5efa\u4e86\u4e00", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u8d4b\u80fd\u4f5c\u8005\uff1a\u5f20\u957f\u65fa\uff0c\u56fe\u6e90\uff1a\u65fa\u77e5\u8bc6\u5fae\u8f6f\u56e2\u961f\u5728CVPR'25\u7684\u6559\u7a0b\u4e2d\u7ed9\u51fa\u4e86\u7cfb\u7edf\u6027\u7b54\u6848\uff1a\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e3a\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6ce8\u5165\u201c\u89c6\u89c9\u601d\u8003\u201d\u80fd\u529b\uff0c\u4ece\u56fe\u50cf\u751f\u6210\u8f85\u52a9\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u63d0\u5347\u7cbe\u5ea6\uff0c\u5230\u591a\u8f6e\u8f68\u8ff9\u4f18\u5316\u7a33\u5b9a\u8bad\u7ec3\uff0c\u6784\u5efa\u4e86\u4e00", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.4052470e", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5Nzc3MjI3MA==&mid=2648478986&idx=1&sn=e4bb377a88d0d7e3edaf646e785ba63a&chksm=bffc1c94d989b61f6d54cac9d98147d8e0c2e05a7b64d87418c7ecdf24d9589feed08fd94736#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5Nzc3MjI3MA==&mid=2648478986&idx=1&sn=e4bb377a88d0d7e3edaf646e785ba63a&chksm=bffc1c94d989b61f6d54cac9d98147d8e0c2e05a7b64d87418c7ecdf24d9589feed08fd94736#rd", "authors": ["\u673a\u5668AI\u5b66\u4e60 \u6570\u636eAI\u6316\u6398"], "title": "\u4ece\u96f6\u5f00\u59cb\u7406\u89e3<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u6a21\u578b\u8bad\u7ec3\u539f\u7406", "comment": "Source: WeChat, Published: 2025-10-02 02:39:32", "summary": "\u672c\u6587\u5c06\u56f4\u7ed5\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff08RLMT\uff09\u5c55\u5f00\u8bba\u8ff0\u3002\u6211\u4eec\u5c06\u4ece\u5177\u5907\u4e0b\u4e00\u8bcd\u5143\u9884\u6d4b\u80fd\u529b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u51fa\u53d1\uff0c\u9010\u6b65\u8bb2\u89e3\u5982\u4f55\u5c06\u5176\u8bad\u7ec3\u4e3a\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u3002", "AI": {"tldr": "\u672c\u6587\u5c06\u56f4\u7ed5\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff08RLMT\uff09\u5c55\u5f00\u8bba\u8ff0\u3002\u6211\u4eec\u5c06\u4ece\u5177\u5907\u4e0b\u4e00\u8bcd\u5143\u9884\u6d4b\u80fd\u529b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u51fa\u53d1\uff0c\u9010\u6b65\u8bb2\u89e3\u5982\u4f55\u5c06\u5176\u8bad\u7ec3\u4e3a\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.e8da4c9e", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4OTgxMjY0OQ==&mid=2247542911&idx=1&sn=a33f719fe90775a619717cfe13be07dc&chksm=fc7d264bf20e11a0245342d0a8859f011dd3c935320a0f83190757197f86831b7c164671f911#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4OTgxMjY0OQ==&mid=2247542911&idx=1&sn=a33f719fe90775a619717cfe13be07dc&chksm=fc7d264bf20e11a0245342d0a8859f011dd3c935320a0f83190757197f86831b7c164671f911#rd", "authors": ["\u5fc3\u58f0\u7ecf\u5178"], "title": "\u667a\u80fd\u7b80\u53f2\uff1a\u8fdb\u5316\u3001AI\u4e0e\u4eba\u8111\u7684\u7a81\u7834\uff08\u7b2c\u4e8c\u6b21\u7a81\u7834\uff09\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff08\u810a\u690e\u52a8\u7269\uff09", "comment": "Source: WeChat, Published: 2025-10-01 19:55:17", "summary": "2 \u7b2c\u4e8c\u6b21\u7a81\u7834\uff1a\u5f3a\u5316\u5b66\u4e60\u3002\u4ece\u4e24\u4fa7\u5bf9\u79f0\u52a8\u7269\u5230\u810a\u690e\u52a8\u7269 \u7a81\u7834 \u5927\u8111\u5206\u5316\u6210\u4e09\u4e2a\u5206\u533a\uff0c\u6784\u6210\u652f\u6491\u6240\u6709\u810a\u690e\u52a8 \u7269\u3002\u5927\u8111\u7684\u4e09\u4e2a\u4e3b\u8981\u7ed3\u6784\uff1a\u524d\u8111\u3001\u4e2d\u8111\u548c\u540e\u8111\u3002", "AI": {"tldr": "2 \u7b2c\u4e8c\u6b21\u7a81\u7834\uff1a\u5f3a\u5316\u5b66\u4e60\u3002\u4ece\u4e24\u4fa7\u5bf9\u79f0\u52a8\u7269\u5230\u810a\u690e\u52a8\u7269 \u7a81\u7834 \u5927\u8111\u5206\u5316\u6210\u4e09\u4e2a\u5206\u533a\uff0c\u6784\u6210\u652f\u6491\u6240\u6709\u810a\u690e\u52a8 \u7269\u3002\u5927\u8111\u7684\u4e09\u4e2a\u4e3b\u8981\u7ed3\u6784\uff1a\u524d\u8111\u3001\u4e2d\u8111\u548c\u540e\u8111\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.5a99f1f8", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247672519&idx=5&sn=0465bc13396a5d983532ccf3411d7912&chksm=e98d465e67355ed0023be07335ced1bb36a04c789057601c70b85e41d8369e6d114534f8792c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247672519&idx=5&sn=0465bc13396a5d983532ccf3411d7912&chksm=e98d465e67355ed0023be07335ced1bb36a04c789057601c70b85e41d8369e6d114534f8792c#rd", "authors": ["\u56fe\u7075\u4eba\u5de5\u667a\u80fd"], "title": "\u98a0\u8986\u5927\u6a21\u578b\u540e\u8bad\u7ec3\uff01\u9648\u4e39\u7426\u56e2\u961f\u63d0\u51fa\u300c\u57fa\u4e8e\u6a21\u578b\u5956\u52b1\u601d\u7ef4\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u300dRLMT", "comment": "Source: WeChat, Published: 2025-10-01 16:02:00", "summary": "\u5728\u65b9\u6cd5\u5b9e\u73b0\u4e0a\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86\u201c\u57fa\u4e8e\u6a21\u578b\u5956\u52b1\u601d\u7ef4\u7684\u5f3a\u5316\u5b66\u4e60\u201d\uff08RLMT\uff09\u6846\u67b6\uff0c\u8fd9\u8ba9 LLM \u5728\u56de\u590d\u4e4b\u524d\u5148\u751f\u6210\u4e00\u6bb5\u957f\u601d\u7ef4\u94fe\uff08CoT\uff09\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u504f\u597d\u7684\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u5728\u7ebf RL \u4f18\u5316\u3002", "AI": {"tldr": "\u5728\u65b9\u6cd5\u5b9e\u73b0\u4e0a\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86\u201c\u57fa\u4e8e\u6a21\u578b\u5956\u52b1\u601d\u7ef4\u7684\u5f3a\u5316\u5b66\u4e60\u201d\uff08RLMT\uff09\u6846\u67b6\uff0c\u8fd9\u8ba9 LLM \u5728\u56de\u590d\u4e4b\u524d\u5148\u751f\u6210\u4e00\u6bb5\u957f\u601d\u7ef4\u94fe\uff08CoT\uff09\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u504f\u597d\u7684\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u5728\u7ebf RL \u4f18\u5316\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.017f706f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNjY3ODM1Ng==&mid=2247483840&idx=1&sn=880542d475646c1f2a7e85a71d79b349&chksm=c326ca54c7afdba2e027d55cfd45167e5083333dc201c008ec6ca23597f2eafc08390be6cf3c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNjY3ODM1Ng==&mid=2247483840&idx=1&sn=880542d475646c1f2a7e85a71d79b349&chksm=c326ca54c7afdba2e027d55cfd45167e5083333dc201c008ec6ca23597f2eafc08390be6cf3c#rd", "authors": ["\u79d1\u7814\u674esir"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e4b\u5206\u5e03\u5f0f\u8bad\u7ec3DDP", "comment": "Source: WeChat, Published: 2025-10-02 12:58:10", "summary": "\u5b66\u4e60\u5927\u6a21\u578b\u9879\u76eeminimind\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u3001\u5168\u91cf\u76d1\u7763\u5fae\u8c03\u3001DPO\u3001LoRA\u5fae\u8c03\u3001\u84b8\u998f\u3001Deepseek-R1 distilled \u63a8\u7406\u3002\u672c\u6587\u7b80\u5355\u4ecb\u7ecd\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u8fc7\u7a0b\uff08DDP\uff09\uff0c\u4ee3\u7801\u91cc\u8be6\u7ec6\u5217\u51fa\u4e86\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u5230\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002", "AI": {"tldr": "\u5b66\u4e60\u5927\u6a21\u578b\u9879\u76eeminimind\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u3001\u5168\u91cf\u76d1\u7763\u5fae\u8c03\u3001DPO\u3001LoRA\u5fae\u8c03\u3001\u84b8\u998f\u3001Deepseek-R1 distilled \u63a8\u7406\u3002\u672c\u6587\u7b80\u5355\u4ecb\u7ecd\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u8fc7\u7a0b\uff08DDP\uff09\uff0c\u4ee3\u7801\u91cc\u8be6\u7ec6\u5217\u51fa\u4e86\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u5230\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.0db39c96", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247563173&idx=1&sn=237cb7233e90904fe308e0239844c57c&chksm=fc4393ba0570201f5323c2eaee369ad16a0776b738b3fcb00d3258305b2304196d6f49d5e96a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247563173&idx=1&sn=237cb7233e90904fe308e0239844c57c&chksm=fc4393ba0570201f5323c2eaee369ad16a0776b738b3fcb00d3258305b2304196d6f49d5e96a#rd", "authors": ["\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u94feunion"], "title": "\u3010\u7cbe\u9009\u62a5\u544a\u3011\u4eba\u624d\u4e13\u9898\u4e00\uff1a\u667a\u7b97\u4e0e<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4eba\u624d\u767d\u76ae\u4e66\uff08\u9644PDF\u4e0b\u8f7d\uff09", "comment": "Source: WeChat, Published: 2025-10-02 12:20:46", "summary": "\u5982\u5934\u90e8\u5e94\u7528\u516c\u53f8\u8d77\u6b65\u671f\u9700\u6280\u672f\u4eba\u624d\u9009\u578b\u5f00\u6e90\u6a21\u578b\uff0c\u4e2d\u957f\u671f\u9700\u590d\u5408\u578b\u4eba\u624d\u5f00\u53d1\u884c\u4e1a\u5927\u6a21\u578b\u3002\u4eba\u624d\u7ed3\u6784\u6846\u67b6\uff1a\u63d0\u51fa \u201c\u667a\u7b97\u4e09\u76f8\u4f20\u5bfc\u4eba\u624d\u7ed3\u6784\u6846\u67b6\uff08PTPTF\uff09\u201d\uff0c\u5c06\u4eba\u624d\u5206\u4e3a\u6218\u7565\u4eba\u624d\uff08\u4e13\u6ce8 \u201c\u7814\u201d\uff0c\u542b\u6218\u7565\u7ba1\u7406\u5927\u5e08\u3001\u79d1\u5b66\u5bb6\u3001\u9886\u519b\u4eba\u7269", "AI": {"tldr": "\u5982\u5934\u90e8\u5e94\u7528\u516c\u53f8\u8d77\u6b65\u671f\u9700\u6280\u672f\u4eba\u624d\u9009\u578b\u5f00\u6e90\u6a21\u578b\uff0c\u4e2d\u957f\u671f\u9700\u590d\u5408\u578b\u4eba\u624d\u5f00\u53d1\u884c\u4e1a\u5927\u6a21\u578b\u3002\u4eba\u624d\u7ed3\u6784\u6846\u67b6\uff1a\u63d0\u51fa \u201c\u667a\u7b97\u4e09\u76f8\u4f20\u5bfc\u4eba\u624d\u7ed3\u6784\u6846\u67b6\uff08PTPTF\uff09\u201d\uff0c\u5c06\u4eba\u624d\u5206\u4e3a\u6218\u7565\u4eba\u624d\uff08\u4e13\u6ce8 \u201c\u7814\u201d\uff0c\u542b\u6218\u7565\u7ba1\u7406\u5927\u5e08\u3001\u79d1\u5b66\u5bb6\u3001\u9886\u519b\u4eba\u7269", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.e4fd6fbd", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570808&idx=3&sn=8ce05b2d4531ec44ce3660eecc39bd18&chksm=96ea08b0e637403030e5182447db3fdb8e77fdc43e0c2da8ecd828899c614cead94ab89441b2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570808&idx=3&sn=8ce05b2d4531ec44ce3660eecc39bd18&chksm=96ea08b0e637403030e5182447db3fdb8e77fdc43e0c2da8ecd828899c614cead94ab89441b2#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "\u4e0a\u4ea42025\u6700\u65b0-\u300a\u52a8\u624b\u5b66<em class=\"highlight\">\u5927\u6a21\u578b</em>\u300b\u5b9e\u6218\u6559\u7a0b\u53cappt\u5206\u4eab\uff01", "comment": "Source: WeChat, Published: 2025-10-02 09:00:00", "summary": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5e2e\u52a9\u5b9e\u73b0AGI\uff1f\u5927\u6a21\u578b\u667a\u80fd\u4f53\u4e0e\u5b89\u5168 \u5927\u6a21\u578b\u667a\u80fd\u4f53\u8fc8\u5411\u4e86\u672a\u6765\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u65c5\u3002\u7136\u800c\uff0c\u5927\u6a21\u578b\u5728\u5f00\u653e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u80fd\u610f\u8bc6\u5230\u98ce\u9669\u5a01\u80c1\u5417\uff1f", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5e2e\u52a9\u5b9e\u73b0AGI\uff1f\u5927\u6a21\u578b\u667a\u80fd\u4f53\u4e0e\u5b89\u5168 \u5927\u6a21\u578b\u667a\u80fd\u4f53\u8fc8\u5411\u4e86\u672a\u6765\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u65c5\u3002\u7136\u800c\uff0c\u5927\u6a21\u578b\u5728\u5f00\u653e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u80fd\u610f\u8bc6\u5230\u98ce\u9669\u5a01\u80c1\u5417\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.23d25919", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMjc5OTE3Mw==&mid=2247483669&idx=1&sn=b1fa1685f2898a1263a18dbfd201c476&chksm=feacd2d0c3f5168cdc188cd92642edad84683783c8ce77c4d18a950d278dc09de91b0ff23307#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMjc5OTE3Mw==&mid=2247483669&idx=1&sn=b1fa1685f2898a1263a18dbfd201c476&chksm=feacd2d0c3f5168cdc188cd92642edad84683783c8ce77c4d18a950d278dc09de91b0ff23307#rd", "authors": ["AI \u5c0f\u767d\u518c Plus"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u662f\u5982\u4f55\u8bad\u7ec3\u51fa\u6765\u7684", "comment": "Source: WeChat, Published: 2025-10-02 08:12:47", "summary": "\u76ee\u524d\u5927\u6a21\u578b\u4e3b\u8981\u57fa\u4e8eTransformer\u67b6\u6784\uff0c\u5e38\u89c1\u53d8\u4f53\u6709Decoder - only\u3001Encoder - only\u548cEncoder - Decoder\u7b49\u3002\u540c\u65f6\u9700\u8981\u8bbe\u8ba1\u5173\u952e\u53c2\u6570\uff0c\u5982\u6a21\u578b\u7684\u5c42\u6570\u3001\u9690\u85cf\u5c42\u7ef4\u5ea6\u3001\u6ce8\u610f\u529b\u5934\u6570\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\u7b49\u3002", "AI": {"tldr": "\u76ee\u524d\u5927\u6a21\u578b\u4e3b\u8981\u57fa\u4e8eTransformer\u67b6\u6784\uff0c\u5e38\u89c1\u53d8\u4f53\u6709Decoder - only\u3001Encoder - only\u548cEncoder - Decoder\u7b49\u3002\u540c\u65f6\u9700\u8981\u8bbe\u8ba1\u5173\u952e\u53c2\u6570\uff0c\u5982\u6a21\u578b\u7684\u5c42\u6570\u3001\u9690\u85cf\u5c42\u7ef4\u5ea6\u3001\u6ce8\u610f\u529b\u5934\u6570\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\u7b49\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.c9046d7c", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4NDY3MzcyMg==&mid=2247520065&idx=1&sn=50b103165cf4a54c61463d6f75aba1f3&chksm=fcfe5a741c171fc6f0871c4cc82d48e9d9537798525966537e95f59e64bcc58f0df7e24d0728#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4NDY3MzcyMg==&mid=2247520065&idx=1&sn=50b103165cf4a54c61463d6f75aba1f3&chksm=fcfe5a741c171fc6f0871c4cc82d48e9d9537798525966537e95f59e64bcc58f0df7e24d0728#rd", "authors": ["\u5065\u6f9c\u79d1\u6280"], "title": "\u4eceHIS\u5230\u65b0\u4e00\u4ee3\u667a\u67a2\uff1a2025\u5e74\u56fd\u5185\u9ad8\u7cbe\u5c16DEEPSEEK\\QWEN\u5f00\u6e90<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u51fa\u73b0\uff0c\u8ba9\u667a\u6167\u533b\u7597\u6b65\u5165\"\u667a\u80fd\u4f53\u65f6\u4ee3\"\u7684\u9769\u547d\u6027\u53d8\u9769!", "comment": "Source: WeChat, Published: 2025-10-02 07:03:30", "summary": "2025\u5e74\u4ee5\u676d\u5dde\u4e3a\u4ee3\u8868\u7684\u516c\u53f8\u63a8\u51fa\u7684\u9ad8\u7cbe\u5c16\u5927\u6a21\u578b\uff0c\u5982DEEPSEEK\uff0cQWEN\u7b49\u5f00\u6e90\u540e\u533b\u9662\u53d1\u5e03\u7684\u5404\u79cd\u57fa\u4e8e\u65b0\u4e00\u4ee3AI\u6280\u672f\u7684\"\u667a\u67a2\"\u7efc\u5408\u667a\u80fd\u4f53\uff0c\u4ee3\u8868\u4e86\u65b0\u4e00\u4ee3\u533b\u7597\u4fe1\u606f\u7cfb\u7edf\u7684\u5f62\u6001\u3002", "AI": {"tldr": "2025\u5e74\u4ee5\u676d\u5dde\u4e3a\u4ee3\u8868\u7684\u516c\u53f8\u63a8\u51fa\u7684\u9ad8\u7cbe\u5c16\u5927\u6a21\u578b\uff0c\u5982DEEPSEEK\uff0cQWEN\u7b49\u5f00\u6e90\u540e\u533b\u9662\u53d1\u5e03\u7684\u5404\u79cd\u57fa\u4e8e\u65b0\u4e00\u4ee3AI\u6280\u672f\u7684\"\u667a\u67a2\"\u7efc\u5408\u667a\u80fd\u4f53\uff0c\u4ee3\u8868\u4e86\u65b0\u4e00\u4ee3\u533b\u7597\u4fe1\u606f\u7cfb\u7edf\u7684\u5f62\u6001\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
