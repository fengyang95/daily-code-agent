<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.AI](#cs.AI) [Total: 4]
- [tldr.article](#tldr.article) [Total: 19]
- [cs.LG](#cs.LG) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Budget-Aware Agentic Routing via Boundary-Guided Training](https://arxiv.org/abs/2602.21227)
*Caiqi Zhang,Menglin Xia,Xuchao Zhang,Daniel Madrigal,Ankur Mallick,Samuel Kessler,Victor Ruehle,Saravan Rajmohan*

Main category: cs.CL

TL;DR: 提出Budget-Aware Agentic Routing框架，通过边界引导训练和策略优化，在严格预算约束下动态选择廉价或昂贵模型，优化成本-成功率边界。


<details>
  <summary>Details</summary>
Motivation: 随着LLM发展为执行长流程的自主代理，每一步都调用高能力模型在经济上不可持续。现有模型路由方法适用于单轮查询，但代理路由是顺序、路径依赖问题：早期错误会累积、反馈通常只在任务结束时获得、部署需要严格的每任务支出限制。

Method: 提出预算感知代理路由，在每一步选择廉价或昂贵模型。采用边界引导训练：利用两个边界策略（总是小模型 vs 总是大模型）构建难度分类，在稀疏奖励下锚定学习。通过分层采样合成成本高效轨迹进行SFT数据预热，然后应用边界引导策略优化，结合边界相对奖励和参考引导优势，避免廉价的失败解决方案。

Result: 实验结果表明，该方法改进了效率边界，以显著更低的成本匹配强大的路由基线，同时展示了对严格推理时预算约束的泛化能力。

Conclusion: 该工作为代理路由建立了基础框架，将范式从静态模型选择转向动态、预算感知的顺序决策。

Abstract: As large language models (LLMs) evolve into autonomous agents that execute long-horizon workflows, invoking a high-capability model at every step becomes economically unsustainable. While model routing is effective for single-turn queries, agentic routing is a sequential, path-dependent problem: early mistakes compound, feedback is often at the end of the episode, and deployments often demand strict per-task spending limits. We propose Budget-Aware Agentic Routing, which selects between a cheap and an expensive model at each step to optimize the cost--success frontier and to operate under strict per-task budgets. We propose Boundary-Guided Training, which leverages two boundary policies (always-small vs.\ always-large) to build a difficulty taxonomy and to anchor learning under sparse rewards. Our approach warms start with boundary-guided SFT data synthesis via stratified sampling of cost-efficient trajectories, then applies Boundary-Guided Policy Optimization (BoPO), combining boundary-relative rewards with a reference-guided advantage to avoid degenerate cheap-failure solutions. Experiment results show that our method improves the efficiency frontier, matching strong routing baselines at substantially lower cost while demonstrating generalization to strict inference-time budget constraints. Overall, our work establishes a foundational framework for agentic routing, shifting the paradigm from static model selection to dynamic, budget-aware sequential decision-making.

</details>


### [2] [Under the Influence: Quantifying Persuasion and Vigilance in Large Language Models](https://arxiv.org/abs/2602.21262)
*Sasha Robinson,Kerem Oktar,Katherine M. Collins,Ilia Sucholutsky,Kelsey R. Allen*

Main category: cs.CL

TL;DR: 研究LLM作为决策顾问时的风险，发现LLM的解题能力、说服能力和警惕性是三种可分离的能力，即使明确提到欺骗可能性，模型仍可能被误导。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地应用于高风险人类决策领域，需要理解它们作为顾问时带来的风险。LLM需要筛选大量善意和恶意的信息，然后说服用户采取特定行动，这涉及警惕性（判断哪些信息可用）和说服力（综合证据进行论证）两种社会能力。

Method: 使用简单的多轮推箱子（Sokoban）解谜游戏，研究LLM对其他LLM代理的说服能力和理性警惕性。通过设计实验让LLM作为顾问提供建议，观察模型在不同情境下的表现。

Result: 发现LLM的解题性能、说服能力和警惕性是三种可分离的能力。模型在游戏中的良好表现并不意味着它能检测到何时被误导，即使明确提到欺骗可能性。然而，LLM会一致地调整其token使用：当建议是善意时使用较少token进行推理，当建议是恶意时使用更多token，即使最终仍被说服采取导致失败的行动。

Conclusion: 这是首次研究LLM中说服力、警惕性和任务性能之间的关系，表明未来AI安全工作中需要独立监控这三个方面。

Abstract: With increasing integration of Large Language Models (LLMs) into areas of high-stakes human decision-making, it is important to understand the risks they introduce as advisors. To be useful advisors, LLMs must sift through large amounts of content, written with both benevolent and malicious intent, and then use this information to convince a user to take a specific action. This involves two social capacities: vigilance (the ability to determine which information to use, and which to discard) and persuasion (synthesizing the available evidence to make a convincing argument). While existing work has investigated these capacities in isolation, there has been little prior investigation of how these capacities may be linked. Here, we use a simple multi-turn puzzle-solving game, Sokoban, to study LLMs' abilities to persuade and be rationally vigilant towards other LLM agents. We find that puzzle-solving performance, persuasive capability, and vigilance are dissociable capacities in LLMs. Performing well on the game does not automatically mean a model can detect when it is being misled, even if the possibility of deception is explicitly mentioned. % as part of the prompt. However, LLMs do consistently modulate their token use, using fewer tokens to reason when advice is benevolent and more when it is malicious, even if they are still persuaded to take actions leading them to failure. To our knowledge, our work presents the first investigation of the relationship between persuasion, vigilance, and task performance in LLMs, and suggests that monitoring all three independently will be critical for future work in AI safety.

</details>


### [3] [Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling](https://arxiv.org/abs/2602.21728)
*Shiqi Yan,Yubo Chen,Ruiqi Zhou,Zhengxi Yao,Shuai Chen,Tianyi Zhang,Shijie Zhang,Wei Qiang Zhang,Yongfeng Huang,Haixin Duan,Yunqi Zhang*

Main category: cs.CL

TL;DR: 提出Explore-on-Graph框架，通过强化学习让LLM在知识图谱上自主探索多样推理路径，解决现有方法局限于先验经验的问题，在KGQA任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱增强方法通常通过生成规则约束或模仿固定演示路径来限制LLM推理，这自然将LLM的推理模式限制在先验经验或微调数据范围内，限制了其对分布外图推理问题的泛化能力。

Method: 提出Explore-on-Graph框架：1) 引入强化学习训练，奖励基于推理路径最终答案的正确性；2) 加入路径信息作为额外奖励信号，优化探索过程并减少无效努力；3) 鼓励LLM在知识图谱上自主探索更丰富的推理空间。

Result: 在五个KGQA基准数据集上的广泛实验表明，该方法达到了最先进的性能，不仅超越了开源LLM，甚至超越了闭源LLM。

Conclusion: 通过强化学习驱动的自主探索，EoG框架能够发现更多样化的推理路径，有效提升LLM在知识图谱问答任务中的泛化能力和性能。

Abstract: The reasoning process of Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks. A promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope of prior experience or fine-tuning data, limiting their generalizability to out-of-distribution graph reasoning problems. To tackle this problem, in this paper, we propose Explore-on-Graph (EoG), a novel framework that encourages LLMs to autonomously explore a more diverse reasoning space on KGs. To incentivize exploration and discovery of novel reasoning paths, we propose to introduce reinforcement learning during training, whose reward is the correctness of the reasoning paths' final answers. To enhance the efficiency and meaningfulness of the exploration, we propose to incorporate path information as additional reward signals to refine the exploration process and reduce futile efforts. Extensive experiments on five KGQA benchmark datasets demonstrate that, to the best of our knowledge, our method achieves state-of-the-art performance, outperforming not only open-source but also even closed-source LLMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI](https://arxiv.org/abs/2602.21251)
*Clemens Pohle*

Main category: cs.SE

TL;DR: AgenticTyper是一个基于LLM的代理系统，通过迭代错误纠正和转译比较来为JavaScript代码库自动添加TypeScript类型，在20分钟内解决了81K行代码中的633个类型错误。


<details>
  <summary>Details</summary>
Motivation: 遗留JavaScript系统缺乏类型安全性，维护风险高。虽然TypeScript可以提供帮助，但手动添加类型成本高昂。现有自动化类型研究主要关注类型推断，很少解决类型检查设置、定义生成、错误识别或仓库级别的行为正确性问题。

Method: 基于LLM的代理系统，通过迭代错误纠正和行为保持（通过转译比较）来为JavaScript代码库自动添加TypeScript类型。

Result: 在两个专有代码库（81K行代码）上评估，AgenticTyper在20分钟内解决了所有633个初始类型错误，将手动工作量从一天减少到20分钟。

Conclusion: AgenticTyper有效解决了JavaScript到TypeScript迁移中的类型错误问题，显著减少了手动工作量，为大规模代码库的类型安全提供了实用解决方案。

Abstract: Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correctness at repository scale. We present AgenticTyper, a Large Language Model (LLM)-based agentic system that addresses these gaps through iterative error correction and behavior preservation via transpilation comparison. Evaluation on two proprietary repositories (81K LOC) shows that AgenticTyper resolves all 633 initial type errors in 20 minutes, reducing manual effort from one working day.

</details>


### [5] [Structurally Aligned Subtask-Level Memory for Software Engineering Agents](https://arxiv.org/abs/2602.21611)
*Kangning Shen,Jingyuan Zhang,Chenxi Sun,Wencong Zeng,Yang Yue*

Main category: cs.SE

TL;DR: 提出Structurally Aligned Subtask-Level Memory方法，通过将记忆存储、检索和更新与智能体的功能分解对齐，解决实例级记忆粒度不匹配问题，在SWE任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的软件工程智能体通常采用实例级记忆机制，将整个问题解决过程作为存储和检索的原子单元。然而，这种粗粒度方法存在粒度不匹配问题：当表面描述相似的任务在特定阶段需要不同的推理逻辑时，会导致误导性检索。

Method: 提出Structurally Aligned Subtask-Level Memory方法，将记忆存储、检索和更新与智能体的功能分解对齐。该方法在子任务级别进行操作，而不是整个实例级别，从而更好地匹配软件工程任务的结构化特性。

Result: 在SWE-bench Verified上的广泛实验表明，该方法在各种骨干模型上均优于普通智能体和强大的实例级记忆基线。平均Pass@1比普通智能体提升+4.7个百分点（例如在Gemini 2.5 Pro上提升+6.8个百分点）。随着交互步骤增加，性能增益进一步扩大。

Conclusion: 通过将记忆机制与智能体的功能分解对齐，可以更有效地利用过往经验，显著提升复杂软件工程任务中的长程推理能力。子任务级记忆比实例级记忆更适合软件工程任务的结构化特性。

Abstract: Large Language Models (LLMs) have demonstrated significant potential as autonomous software engineering (SWE) agents. Recent work has further explored augmenting these agents with memory mechanisms to support long-horizon reasoning. However, these approaches typically operate at a coarse instance granularity, treating the entire problem-solving episode as the atomic unit of storage and retrieval. We empirically demonstrate that instance-level memory suffers from a fundamental granularity mismatch, resulting in misguided retrieval when tasks with similar surface descriptions require distinct reasoning logic at specific stages. To address this, we propose Structurally Aligned Subtask-Level Memory, a method that aligns memory storage, retrieval, and updating with the agent's functional decomposition. Extensive experiments on SWE-bench Verified demonstrate that our method consistently outperforms both vanilla agents and strong instance-level memory baselines across diverse backbones, improving mean Pass@1 over the vanilla agent by +4.7 pp on average (e.g., +6.8 pp on Gemini 2.5 Pro). Performance gains grow with more interaction steps, showing that leveraging past experience benefits long-horizon reasoning in complex software engineering tasks.

</details>


### [6] [AkiraRust: Re-thinking LLM-aided Rust Repair Using a Feedback-guided Thinking Switch](https://arxiv.org/abs/2602.21681)
*Renshuang Jiang,Yichong Wang,Pan Dong,Xiaoxiang Fang,Zhenling Duan,Tinglue Wang,Yuchen Hu,Jie Yu,Zhe Jiang*

Main category: cs.SE

TL;DR: AkiraRust是一个基于LLM的Rust程序修复框架，通过有限状态机和双模式推理策略实现上下文感知和运行时自适应的未定义行为修复，达到92%的语义正确性和2.2倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有Rust代码修复框架受限于僵化的模板或缺乏可执行语义基础，导致上下文感知能力有限和语义不正确。需要能够动态适应运行时语义条件的修复方法。

Method: 提出AkiraRust框架，采用有限状态机动态调整检测和修复流程，引入双模式推理策略协调多个智能体的快慢思维。每个智能体映射到FSM状态，波形驱动的转换控制器管理状态切换、回滚决策和语义检查点。

Result: 实验结果显示AkiraRust达到约92%的语义正确性，相比最先进方法平均加速2.2倍。

Conclusion: AkiraRust通过结合有限状态机和双模式推理，实现了上下文感知和运行时自适应的Rust程序修复，显著提高了语义正确性和修复效率。

Abstract: Eliminating undefined behaviors (UBs) in Rust programs requires a deep semantic understanding to enable accurate and reliable repair. While existing studies have demonstrated the potential of LLMs to support Rust code analysis and repair, most frameworks remain constrained by inflexible templates or lack grounding in executable semantics, resulting in limited contextual awareness and semantic incorrectness. Here, we present AkiraRust, an LLM-driven repair and verification framework that incorporates a finite-state machine to dynamically adapt its detection and repair flow to runtime semantic conditions. AkiraRust introduces a dual-mode reasoning strategy that coordinates fast and slow thinking across multiple agents. Each agent is mapped to an FSM state, and a waveform-driven transition controller manages state switching, rollback decisions, and semantic check pointing, enabling context-aware and runtime-adaptive repair. Experimental results show that AkiraRust achieves about 92% semantic correctness and delivers a 2.2x average speedup compared to SOTA.

</details>


### [7] [Enhancing LLM-Based Test Generation by Eliminating Covered Code](https://arxiv.org/abs/2602.21997)
*WeiZhe Xu,Mengyu Liu,Fanxin Kong*

Main category: cs.SE

TL;DR: 提出一种基于LLM的可扩展单元测试生成方法，通过上下文信息检索和带代码消除的迭代测试生成，在复杂方法上实现高覆盖率


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试生成方法在小规模代码片段上表现良好，但在复杂方法上效果不佳，需要解决上下文过长和推理效率问题

Method: 1) 上下文信息检索：结合LLM和静态分析收集复杂方法的相关上下文信息；2) 带代码消除的迭代测试生成：重复为代码切片生成测试，跟踪覆盖率，选择性移除已覆盖的代码段

Result: 在开源项目上的综合评估显示，该方法优于最先进的基于LLM和基于搜索的方法，在复杂方法上实现了高覆盖率

Conclusion: 提出的可扩展LLM测试生成方法有效解决了复杂方法测试生成的挑战，通过上下文管理和迭代简化显著提升了覆盖率

Abstract: Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.

</details>


### [8] [SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents](https://arxiv.org/abs/2602.22124)
*Patrick Tser Jern Kon,Archana Pradeep,Ang Chen,Alexander P. Ellis,Warren Hunt,Zijian Wang,John Yang,Samuel Thompson*

Main category: cs.SE

TL;DR: SWE-Protégé是一个后训练框架，通过专家-学徒协作的方式提升小语言模型在软件工程任务上的性能，显著提高了SWE-bench上的解决率。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本、延迟和适应性方面有优势，但在长时程软件工程任务（如SWE-bench）上表现不佳，存在动作循环和低解决率问题。

Method: 将软件修复重构为专家-学徒协作问题，让小语言模型作为唯一决策者，学习选择性寻求专家指导、识别停滞状态并遵循专家反馈。结合专家增强轨迹的监督微调和代理强化学习，明确抑制退化循环和无效的专家协作。

Result: 在Qwen2.5-Coder-7B-Instruct上轻量后训练，在SWE-bench Verified上达到42.4% Pass@1，比之前的小语言模型最佳结果提升25.4%，同时稀疏使用专家协助（约每个任务4次调用，占总token的11%）。

Conclusion: SWE-Protégé框架通过专家-学徒协作有效提升了小语言模型在复杂软件工程任务上的性能，实现了高效且成本可控的解决方案。

Abstract: Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives](https://arxiv.org/abs/2602.21351)
*Dmitrii Pantiukhin,Ivan Kuznetsov,Boris Shapkin,Antonia Anna Jost,Thomas Jung,Nikolay Koldunov*

Main category: cs.AI

TL;DR: PANGAEA-GPT：用于地球科学数据自主发现和分析的分层多智能体框架，通过集中式监督-工作者拓扑、沙盒代码执行和自校正机制，实现复杂多步骤工作流自动化。


<details>
  <summary>Details</summary>
Motivation: 地球科学数据快速增长（如PANGAEA存储库）但大量数据未被充分利用，引用指标显示数据重用性有限，需要解决数据发现和分析的可扩展性挑战。

Method: 采用分层多智能体框架，具有集中式监督-工作者拓扑结构，包含数据类型感知路由、沙盒确定性代码执行、通过执行反馈实现自校正，使智能体能够诊断和解决运行时错误。

Result: 通过物理海洋学和生态学的用例场景，展示了系统能够以最少人工干预执行复杂的多步骤工作流，验证了框架的有效性。

Conclusion: 该框架为通过协调智能体工作流查询和分析异构存储库数据提供了一种方法论，解决了地球科学数据可扩展性挑战。

Abstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.

</details>


### [10] [ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning](https://arxiv.org/abs/2602.21534)
*Xiaoxuan Wang,Han Zhang,Haixin Wang,Yidan Shi,Ruoyan Li,Kaiqiao Han,Chenyi Tong,Haoran Deng,Renliang Sun,Alexander Taylor,Yanqiao Zhu,Jason Cong,Yizhou Sun,Wei Wang*

Main category: cs.AI

TL;DR: 该论文提出了ARLArena框架和SAMPO方法，旨在解决Agentic强化学习中的训练不稳定问题，通过系统分析策略梯度的四个核心维度，提供稳定的训练方案。


<details>
  <summary>Details</summary>
Motivation: Agentic强化学习在处理复杂多步交互任务时展现出潜力，但训练过程极不稳定，经常导致训练崩溃，这限制了其在更大环境和更长交互周期中的可扩展性，也阻碍了算法设计的系统性探索。

Method: 首先提出ARLArena框架，构建标准化测试环境，将策略梯度分解为四个核心设计维度进行分析。基于分析结果，提出了SAMPO方法，这是一种稳定的Agentic策略优化方法，旨在缓解ARL中的主要不稳定因素。

Result: SAMPO在各种Agentic任务中实现了持续稳定的训练和强大的性能表现。ARLArena框架为ARL提供了统一的策略梯度视角，并提供了构建稳定、可复现的基于LLM的Agent训练管道的实用指导。

Conclusion: 该研究为Agentic强化学习提供了统一的策略梯度视角，并为构建稳定、可复现的LLM-based Agent训练管道提供了实用指导，解决了ARL训练中的核心稳定性问题。

Abstract: Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.

</details>


### [11] [Power and Limitations of Aggregation in Compound AI Systems](https://arxiv.org/abs/2602.21556)
*Nivasini Ananthakrishnan,Meena Jagadeesan*

Main category: cs.AI

TL;DR: 本文研究了在复合AI系统中，通过聚合多个相同模型的输出来扩展系统设计者可获得输出的能力，揭示了聚合通过三种机制扩展可激发性集合，并提供了理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 在复合AI系统设计中，通常查询多个相同模型并聚合响应来合成输出。由于模型同质性，需要研究聚合是否比查询单个模型能获得更多输出，以及聚合如何克服模型能力和提示工程的限制。

Method: 采用风格化的委托-代理框架，分析系统设计者如何通过奖励函数部分引导每个代理的输出，但仍面临提示工程能力和模型能力的限制。通过理论分析揭示聚合扩展可激发性集合的三种机制：可行性扩展、支持扩展和绑定集收缩。

Result: 证明任何聚合操作必须实现这三种机制之一才能扩展可激发性，强化版本的这些机制提供了完全表征可激发性扩展的必要和充分条件。在玩具参考生成任务中对LLM进行了实证说明。

Conclusion: 聚合通过三种机制扩展系统设计者可获得的输出集合，为理解复合AI系统何时能克服模型能力和提示工程限制迈出了一步。

Abstract: When designing compound AI systems, a common approach is to query multiple copies of the same model and aggregate the responses to produce a synthesized output. Given the homogeneity of these models, this raises the question of whether aggregation unlocks access to a greater set of outputs than querying a single model. In this work, we investigate the power and limitations of aggregation within a stylized principal-agent framework. This framework models how the system designer can partially steer each agent's output through its reward function specification, but still faces limitations due to prompt engineering ability and model capabilities. Our analysis uncovers three natural mechanisms -- feasibility expansion, support expansion, and binding set contraction -- through which aggregation expands the set of outputs that are elicitable by the system designer. We prove that any aggregation operation must implement one of these mechanisms in order to be elicitability-expanding, and that strengthened versions of these mechanisms provide necessary and sufficient conditions that fully characterize elicitability-expansion. Finally, we provide an empirical illustration of our findings for LLMs deployed in a toy reference-generation task. Altogether, our results take a step towards characterizing when compound AI systems can overcome limitations in model capabilities and in prompt engineering.

</details>


### [12] [Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts](https://arxiv.org/abs/2602.22070)
*Jessica Y. Bo,Lillio Mok,Ashton Anderson*

Main category: cs.AI

TL;DR: LLMs在决策任务中对人类专家和算法代理表现出不一致的偏见：在直接信任度评分中偏向人类，但在激励性选择中却偏向算法，即使算法表现更差。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在处理来自不同来源（人类专家和算法代理）信息时的权重分配，特别是考察LLMs是否表现出类似人类的算法厌恶现象，这对于LLMs在高风险场景中的部署至关重要。

Method: 采用行为经济学实验范式，评估8个不同LLMs在决策任务中的委托行为。使用两种任务呈现方式：1) 陈述偏好（通过直接询问对人类专家和算法的信任度）；2) 显示偏好（通过提供两种代理表现的上下文示例，并要求进行激励性投注）。

Result: 当直接评估信任度时，LLMs给人类专家的评分更高，这与人类受访者的先前结果一致。但在激励性投注场景中，LLMs不成比例地选择算法，即使算法表现明显更差。这表明LLMs对人类和算法编码了不一致的偏见。

Conclusion: LLMs对人类和算法存在不一致的偏见，且对任务呈现格式敏感。这些发现需要在LLMs部署到高风险场景时仔细考虑，并强调了评估鲁棒性对AI安全的重要性。

Abstract: Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [13] [Infosec community panics as Anthropic rolls out Claude code security checker](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theregister.com%2F2026%2F02%2F23%2Fclaude_code_security_panic%2F%3Futm_source=tldrinfosec/1/0100019c8ffb088b-8e5e8794-37db-43cc-9ee3-915a6a62e475-000000/uz9HH1gpwSOrLyolm0r1b4g3Hpx96yU63UiHtWxRjfg=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出Claude代码安全检查功能，可扫描代码库漏洞并自动生成补丁，已在开源项目中发现数百个高危漏洞，引发信息安全社区关注和投资者担忧。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码助手的发展，各大科技公司（Google、Microsoft、Amazon、OpenAI）都在部署AI代理进行漏洞检测，Anthropic希望推出自己的代码安全检查功能来参与这一竞争，提高代码安全性。

Method: Claude Code Security功能通过扫描代码库自动识别安全漏洞，并生成修复建议和补丁。该功能作为AI代理运行，但仍需要人工审核来确保准确性。

Result: 该功能已在开源项目中发现了数百个高危安全漏洞，引发了信息安全社区的关注，并导致CrowdStrike等安全公司的股价下跌，显示了AI代码安全检查工具对安全行业的影响。

Conclusion: 虽然AI代码安全检查工具显示出强大的漏洞检测能力，但仍需要人工审核，并且面临准确性等方面的质疑。这一趋势正在改变安全行业的格局。

Abstract: Infosec community panics as Anthropic rolls out Claude code security checker (3 minute read) Anthropic's new Claude Code Security feature scans codebases for vulnerabilities, proposes patches, and claims to have surfaced hundreds of high‑severity issues in open source projects, spooking security investors and pushing stocks like CrowdStrike down. It joins Google, Microsoft, Amazon, and OpenAI in deploying AI agents for bug hunting, but all still require human review and face questions over fa...

</details>


### [14] [What If Adding Auth to Your App Took One Command?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fa0ih4T/2/0100019c9004d8a1-63d49ce9-6055-4393-9f70-879d0121e925-000000/2MftJlApbOsBDz1AtH6HLlDMapRt769LVRbzUEVicSA=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: WorkOS推出基于Claude的AI代理，通过npx workos命令自动分析项目代码、检测框架，并直接写入完整的身份验证集成代码，支持类型检查和自动错误修复。


<details>
  <summary>Details</summary>
Motivation: 简化应用开发中身份验证集成的复杂性，传统方式需要开发者手动配置各种框架的认证方案，耗时且容易出错。

Method: 使用基于Claude的AI代理，读取项目代码、检测技术栈，自动生成适合的认证集成代码，并进行类型检查和构建，遇到错误时自我修复。

Result: 开发者只需运行npx workos命令，即可获得完整的、与现有代码库兼容的身份验证集成方案，无需手动配置模板。

Conclusion: WorkOS的AI代理工具显著简化了应用开发中的身份验证集成流程，提高了开发效率和代码质量。

Abstract: What If Adding Auth to Your App Took One Command? (Sponsor) npx workos launches an AI agent, powered by Claude, that reads your project, detects your framework, and writes a complete auth integration directly into your existing codebase. It's not a template generator. It reads your code, understands your stack, and writes an integration that fits. Then it typechecks and builds, feeding any errors back to itself to fix. Just run npx workos, from WorkOS. See how it works →

</details>


### [15] [Anthropic Reported Large-Scale Distillation Attempts](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fdetecting-and-preventing-distillation-attacks%3Futm_source=tldrai/1/0100019c9004d8a1-63d49ce9-6055-4393-9f70-879d0121e925-000000/GJbuvBv-oEevW6ajPVQgiTzCocP0IWlyRUj2phnbd4o=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic指控DeepSeek、Moonshot AI和MiniMax创建超过24,000个虚假账户，生成了约1600万次Claude交互，试图复制其智能体推理、工具使用和编码能力。


<details>
  <summary>Details</summary>
Motivation: 该报道揭示了AI公司之间可能存在的技术窃取行为，动机是通过大规模蒸馏(distillation)获取竞争对手的核心技术能力，特别是智能体推理、工具使用和编码等高级功能。

Method: 通过创建大量虚假账户（超过24,000个）来生成海量Claude交互（约1600万次），试图通过蒸馏技术复制Anthropic的AI模型能力。

Result: Anthropic发现并公开指控了这种大规模技术窃取行为，涉及DeepSeek、Moonshot AI和MiniMax三家公司，揭示了AI行业竞争中的不正当手段。

Conclusion: AI行业竞争激烈，存在通过技术蒸馏窃取竞争对手能力的风险，需要建立更严格的技术保护机制和行业伦理规范。

Abstract: Anthropic Reported Large-Scale Distillation Attempts (4 minute read) Anthropic accused DeepSeek, Moonshot AI, and MiniMax of creating over 24,000 fake accounts to generate roughly 16 million Claude interactions aimed at replicating its agentic reasoning, tool use, and coding capabilities.

</details>


### [16] [GPT-5 Codex Ran a 25-Hour Coding Sprint](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.openai.com%2Fcookbook%2Fexamples%2Fcodex%2Flong_horizon_tasks%3Futm_source=tldrai/1/0100019c9004d8a1-63d49ce9-6055-4393-9f70-879d0121e925-000000/OSfQT1c5sQo2gkzIQQc_JdEc_tYdtKPLoes0uROmCY4=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GPT-5.3-Codex在25小时内自主完成了一个设计工具的完整开发，从空白仓库开始，使用了1300万tokens，生成了约3万行代码。


<details>
  <summary>Details</summary>
Motivation: 测试GPT-5.3-Codex在长时程、高复杂度编码任务中的自主开发能力，验证其在真实软件开发场景中的表现。

Method: OpenAI进行了一个压力测试：给GPT-5.3-Codex一个空白代码仓库和完全访问权限，让它自主构建一个设计工具，整个过程持续约25小时。

Result: GPT-5.3-Codex成功完成了任务，使用了约1300万tokens，生成了约3万行代码，最终构建出了一个功能完整的设计工具。

Conclusion: GPT-5.3-Codex展示了在长时程、复杂编码任务中的强大自主开发能力，能够独立完成从零开始的完整软件开发项目。

Abstract: GPT-5 Codex Ran a 25-Hour Coding Sprint (23 minute read) OpenAI described a long-horizon stress test where GPT-5.3-Codex, given a blank repository and full access, built a design tool over ~25 hours using ~13M tokens and producing ~30k lines of code.

</details>


### [17] [AWS Launches Strands Labs to Give Developers a Sandbox for Autonomous AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftheaieconomy.substack.com%2Fp%2Fstrands-labs-developer-sandbox-autonomous-ai%3Futm_source=tldrai/1/0100019c9004d8a1-63d49ce9-6055-4393-9f70-879d0121e925-000000/Gd2q9mjNSg6RtoxFC2BPQt_CiliOlB2bdYxwuOrL-F0=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AWS推出Strands Labs作为GitHub组织，为开发者提供自主AI实验的沙箱环境，专注于机器人、机器人模拟和AI函数三个项目领域。


<details>
  <summary>Details</summary>
Motivation: AWS希望为开发者和社区提供一个专门的实验空间，探索当软件变得具有自主性（agentic）时会发生什么变化，特别是在机器人、模拟和AI函数等前沿AI技术领域。

Method: 通过创建GitHub组织Strands Labs，提供三个具体项目的实验环境：robots（机器人）、robots sim（机器人模拟）和AI functions（AI函数），让开发者能够在这些领域进行探索和实验。

Result: AWS成功推出了Strands Labs，目前专注于三个特定项目领域，这些领域被认为最能揭示软件变得具有自主性时的变化，为AWS和更广泛的开发者社区提供了实验平台。

Conclusion: Strands Labs的推出为开发者探索自主AI技术提供了专门的沙箱环境，有助于推动AI代理技术的发展和创新。

Abstract: AWS Launches Strands Labs to Give Developers a Sandbox for Autonomous AI (4 minute read) AWS Strands Labs is a new GitHub organization aimed at helping developers explore and experiment with cutting-edge AI techniques. It is currently available for three specific projects: robots, robots sim, and AI functions. AWS considers these areas the ones that most clearly reveal what changes when software becomes agentic. Strands Labs gives AWS and the broader community a dedicated space to experiment ...

</details>


### [18] [MachineAuth](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmandarwagh9%2FMachineAuth%3Futm_source=tldrai/1/0100019c9004d8a1-63d49ce9-6055-4393-9f70-879d0121e925-000000/65Umtp-Fdz_lEiq8A4VX_YciGEBAcFFh2CUp1LRKlm4=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: MachineAuth是一个为AI代理提供安全OAuth 2.0客户端凭证流的工具，支持机器对机器认证，具有JWT令牌生成、代理管理和基于范围的权限控制功能


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在机器间通信中的广泛应用，需要安全可靠的认证机制来保护系统访问和数据安全。现有的OAuth实现通常复杂且需要数据库支持，需要一个轻量级、易于部署的解决方案

Method: 通过实现OAuth 2.0客户端凭证流，使用JWT令牌进行认证，采用JSON文件存储进行代理管理和权限配置，无需数据库依赖

Result: 开发了一个功能完整的认证系统，支持JWT令牌生成、代理生命周期管理、基于范围的权限控制，所有配置通过简单的JSON文件管理

Conclusion: MachineAuth提供了一个轻量级、易于部署的AI代理认证解决方案，简化了机器间安全通信的实现，特别适合需要快速部署和最小依赖的场景

Abstract: MachineAuth (GitHub Repo) MachineAuth provides secure OAuth 2.0 client credentials flow for AI agents, enabling machine-to-machine authentication. Key features include JWT token generation, agent management, and scope-based permissions, all with simple JSON file storage and no databases required.

</details>


### [19] [IBM stock dives after Anthropic points out AI can rewrite COBOL fast](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theregister.com%2F2026%2F02%2F23%2Fibm_share_dive_anthropic_cobol%2F%3Futm_source=tldrai/1/0100019c9004d8a1-63d49ce9-6055-4393-9f70-879d0121e925-000000/1H4chPx3W6op7yG19zPCR_PqYMRu_J1IakRYwHX00tk=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic声称其Claude Code工具能加速COBOL应用程序的重构，这导致IBM股价下跌


<details>
  <summary>Details</summary>
Motivation: 传统COBOL系统在现代IT环境中维护成本高、效率低，需要现代化改造。AI工具可以加速这一过程，降低企业依赖传统供应商。

Method: 使用Claude Code工具（基于AI的代码助手）来自动化重构COBOL代码，将其转换为更现代的编程语言或优化现有代码结构。

Result: AI工具能显著加速COBOL应用程序的重构过程，这一消息导致IBM股价下跌，因为市场担心这会减少企业对IBM COBOL相关服务的需求。

Conclusion: AI驱动的代码重构工具对传统企业软件市场产生冲击，可能改变COBOL现代化改造的商业模式。

Abstract: IBM stock dives after Anthropic points out AI can rewrite COBOL fast (2 minute read) Anthropic claims that Claude Code tools can accelerate the refactoring of apps written in COBOL.

</details>


### [20] [The End of CI/CD Pipelines: The Dawn of Agentic DevOps](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackernoon.com%2Fthe-end-of-cicd-pipelines-the-dawn-of-agentic-devops%3Fsource=rss%26utm_source=tldrdevops/1/0100019c94b1b59b-e6c4973b-d619-40d3-9feb-6319e8695b0d-000000/0enbiga_dkJGemVETjOEsD40zEh8WPb1uZgGh_qiCWU=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨AI驱动的DevOps工具如何将传统CI/CD管道转变为自主决策系统，虽然提升效率但引入新的风险和挑战


<details>
  <summary>Details</summary>
Motivation: 传统CI/CD管道是确定性的，而新兴的AI驱动DevOps工具（如GitHub Copilot、Azure SRE Agent）能够自主调试测试、修复事故，这带来了效率提升但也引入了不透明的决策过程和新的风险

Method: 通过分析AI驱动的DevOps工具如何改变CI/CD工作流程，从确定性管道转向基于AI判断的自主决策系统，并探讨由此产生的风险和应对策略

Result: AI驱动的DevOps工具能够显著提升开发速度，但会引入不透明的决策过程、新的故障模式以及操作风险，需要防护措施、影子部署和强监督

Conclusion: AI驱动的DevOps工具代表了CI/CD管道的变革，虽然能提高效率，但需要建立适当的防护机制和监督体系来管理其带来的新风险

Abstract: The End of CI/CD Pipelines: The Dawn of Agentic DevOps (9 minute read) Agentic DevOps tools like GitHub Copilot and Microsoft's Azure SRE Agent shift CI CD from deterministic pipelines to AI-driven judgment that can autonomously debug tests, remediate incidents, and reduce toil. While increasing velocity, they introduce opaque decision-making, novel failure modes, and new operational risks that require guardrails, shadow deployments, and strong oversight.

</details>


### [21] [Refactoring Azure Quick Review with GitHub Copilot](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcarlos.mendible.com%2F2026%2F02%2F07%2Frefactoring-azure-quick-review-with-github-copilot%2F%3Futm_source=tldrdevops/1/0100019c94b1b59b-e6c4973b-d619-40d3-9feb-6319e8695b0d-000000/jWuAW8DEwhrtgoOG44gOrN1jWZDITFwYZUeL2PJ1VUk=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用GitHub Copilot重构Azure Quick Review工具，消除技术债务，大幅提升性能


<details>
  <summary>Details</summary>
Motivation: Azure Quick Review (azqr)存在大量技术债务和72个扫描器包，需要进行彻底重构以提升代码质量和性能

Method: 使用GitHub Copilot的计划模式和代理模式，在架构监督下进行完全重构，消除冗余代码和包依赖

Result: 移除了72个扫描器包，净减少16,506行代码（涉及560个文件），扫描性能提升高达90%

Conclusion: GitHub Copilot在大型重构项目中能有效消除技术债务，显著提升代码质量和系统性能

Abstract: Refactoring Azure Quick Review with GitHub Copilot (12 minute read) Azure Quick Review azqr was completely refactored using GitHub Copilot plan and agent modes under architectural supervision, eliminating major technical debt and 72 scanner packages. The overhaul removed 16,506 net lines across 560 files and improved scan performance by up to 90 percent.

</details>


### [22] [Improve test coverage across codebases with Datadog Code Coverage](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datadoghq.com%2Fblog%2Fimprove-test-cover-across-codebases-with-code-coverage%2F%3Futm_source=tldrdevops/1/0100019c94b1b59b-e6c4973b-d619-40d3-9feb-6319e8695b0d-000000/YQT7Rr-NtS6vs1vW5FU43I9rZeM-sUA4m588nJX9C-g=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Datadog Code Coverage工具提供跨仓库的统一代码覆盖率可见性，强制执行测试标准，并使用自动化测试生成来填补覆盖率空白，帮助团队在AI辅助开发中保持代码质量。


<details>
  <summary>Details</summary>
Motivation: 随着AI辅助开发加速，团队需要更好的工具来维护代码质量，特别是在测试覆盖率方面。传统方法难以跨多个代码库提供统一的可见性和一致的测试标准。

Method: Datadog Code Coverage提供：1) 跨仓库的统一、行级可见性；2) 强制执行一致的测试标准；3) 使用自动化测试生成来识别和填补覆盖率空白。

Result: 该工具帮助团队在快速、AI辅助的开发环境中保持代码质量，通过统一的覆盖率监控和自动化测试生成来提高整体测试覆盖率。

Conclusion: Datadog Code Coverage是一个有效的解决方案，能够在AI加速的开发流程中帮助团队维持高标准的代码质量和测试覆盖率。

Abstract: Improve test coverage across codebases with Datadog Code Coverage (4 minute read) Datadog Code Coverage provides unified, line-level visibility across repositories, enforces consistent testing standards, and uses automated test generation to identify and fill coverage gaps, helping teams maintain code quality amid faster, AI-assisted development.

</details>


### [23] [QA bottlenecks holding you back? Ship more code with autonomous testing agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fqa.tech%2Ftry-now%2Ftldr%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=2602tldr2/2/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/-eGFLAh-_YoGSWdPNQQXftph0khqelQ-qWMOyjLd7sU=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: QA.tech提供自主测试代理，自动验证每次发布并适应产品变化，无需手动测试员即可无限扩展QA


<details>
  <summary>Details</summary>
Motivation: 手动QA存在瓶颈且昂贵，无法通过增加人力解决，需要AI对抗性验证来确保发布质量

Method: 部署自主测试代理，自动验证每次发布，自适应产品变化，在生产前捕获bug

Result: 能够无限扩展QA规模，无需雇佣手动测试员，避免维护税

Conclusion: 自主测试代理是解决QA瓶颈的有效方案，通过AI自动化测试实现质量保证

Abstract: QA bottlenecks holding you back? Ship more code with autonomous testing agents (Sponsor) In 2026, code is cheap. But QA bottlenecks are expensive.You can't fix manual QA by throwing more bodies at it. You need an AI counterweight to validate every release. QA.tech deploys autonomous testing agents that validate every release and automatically adapt to product changes to catch bugs before they hit production. > Scale QA infinitely without hiring manual testers. > Escape the maintenance tax of ...

</details>


### [24] [Linear walkthroughs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2Fguides%2Fagentic-engineering-patterns%2Flinear-walkthroughs%2F%3Futm_source=tldrdev/1/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/zL5iMYtXFr-Bl3qJHgqCUER7GU_rae4RGKaA69JLWz8=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用Claude Code和Showboat为SwiftUI应用生成结构化文档，通过执行shell命令提取真实代码片段来创建准确的walkthrough


<details>
  <summary>Details</summary>
Motivation: 解决代码文档生成的准确性问题，防止AI在生成文档时产生幻觉，帮助理解vibe-coded项目

Method: 使用Claude Code和Showboat代理，通过执行shell命令提取真实代码片段，生成结构化walkthrough文档

Result: 能够为SwiftUI应用创建准确的walkthrough文档，有效防止幻觉，帮助理解复杂项目

Conclusion: 这种模式可以显著提高代码文档生成的准确性和实用性，特别适合理解复杂项目

Abstract: Linear walkthroughs (4 minute read) Claude Code and Showboat can be used to generate structured documentation for a SwiftUI app. By executing shell commands to extract real code snippets, agents can create accurate walkthroughs. This pattern can help understand vibe-coded projects and prevent hallucinations.

</details>


### [25] [Security belongs in the IDE](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fdev.checkmarx.com%3Futm_source=tldr_email%26utm_medium=email%26utm_campaign=tldr_newsletter/1/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/ACYCcoLBZk1CnxBmuUPDPwQZSKMPADxN3VtJ8PB-iZs=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Checkmarx Developer Assist是一款IDE原生安全助手，可在代码生成时识别和修复安全问题，无需等待CI/CD或外部扫描


<details>
  <summary>Details</summary>
Motivation: 现代代码开发速度太快，传统安全扫描（如CI/CD或外部扫描）无法跟上，需要将安全集成到IDE中实现实时防护

Method: 开发IDE原生安全助手，在代码编写过程中实时分析代码，识别安全漏洞并提供修复建议

Result: Checkmarx Developer Assist能够实时检测代码安全问题，提供即时修复，提高开发效率和安全质量

Conclusion: 将安全集成到IDE中是应对现代快速开发的有效方法，能够实现更早的安全防护和更高的开发效率

Abstract: Security belongs in the IDE (Sponsor) The only way to catch up with the speed of modern code is to move security into the IDE. Checkmarx Developer Assist is an IDE-native security assistant that identifies and fixes security issues as code is generated. No need to wait for CI/CD or external scans. Watch a 3 minute demo (or try it for free)

</details>


### [26] [Emdash](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgeneralaction%2Femdash%3Futm_source=tldrdev/1/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/uzoOM0z-8wklNnvkK7iS9ftxSLlA9r70eqieDEqfJqk=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Emdash是一个开源的智能体开发环境，支持并行运行和测试多个编码智能体，使用Git工作树保持变更隔离


<details>
  <summary>Details</summary>
Motivation: 为开发者提供一个能够并行运行和测试多个编码智能体的环境，解决智能体开发中的隔离和测试问题

Method: 使用Git工作树技术保持变更隔离，支持21个CLI提供者，提供并行运行智能体的开发环境

Result: 创建了一个开源的智能体开发环境，能够有效隔离不同智能体的变更，支持多种CLI工具

Conclusion: Emdash为编码智能体的开发和测试提供了实用的并行环境解决方案

Abstract: Emdash (GitHub Repo) Emdash is an open-source agentic development environment that allows developers to run and test multiple coding agents in parallel. It supports 21 CLI providers and uses Git worktrees to keep changes isolated and clean.

</details>


### [27] [GPT-5.3-Codex available on OpenRouter](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopenrouter.ai%2Fopenai%2Fgpt-5.3-codex%3Futm_source=tldrdev/1/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/oBflxTViHm0G3-FMOeMaXp5b8ONsv3JVqt6a_m8ygM8=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GPT-5.3-Codex是OpenAI发布的新代码模型，具有40万token上下文窗口，性能比前代快25%，在SWE-Bench Pro上达到SOTA结果，支持智能体工作流、终端操作和网络安全漏洞识别


<details>
  <summary>Details</summary>
Motivation: 提升代码生成模型的性能和能力，满足更复杂的软件开发需求，包括长上下文处理、智能体协作、终端操作和网络安全等实际应用场景

Method: 基于OpenAI的GPT架构开发，扩展上下文窗口至40万token，优化推理速度，增强代码理解和生成能力，集成智能体工作流支持

Result: 在SWE-Bench Pro基准测试中取得最先进结果，性能比前代模型快25%，支持更复杂的代码生成和软件开发任务

Conclusion: GPT-5.3-Codex代表了代码生成模型的重大进步，为软件开发提供了更强大、更高效的工具，特别是在长上下文处理、智能体协作和网络安全方面

Abstract: GPT-5.3-Codex available on OpenRouter (Website) According to OpenRouter's new GPT-5.3-Codex stats, OpenAI's GPT-5.3-Codex has a 400,000-token context window and 25% faster performance compared to its predecessors. It has state-of-the-art results on SWE-Bench Pro, and it supports agentic workflows, terminal proficiency, and cybersecurity vulnerability identification.

</details>


### [28] [Macky](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmacky.dev%2F%3Futm_source=tldrdev/1/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/fb6tD0kwdXv2XMfGhIGF5B3mpzBv2il8pvFeJXMUQ-w=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Macky是一个允许从iPhone安全远程访问Mac原生终端和集成AI编码助手（如Claude Code和Codex）的工具


<details>
  <summary>Details</summary>
Motivation: 解决移动设备无法访问桌面级开发环境的问题，让开发者能在iPhone上使用Mac的完整终端功能和AI编码助手

Method: 通过安全远程访问技术，将Mac的终端界面和AI编码助手功能流式传输到iPhone设备

Result: 实现了在iPhone上安全访问Mac原生终端和AI编码助手的功能

Conclusion: Macky为开发者提供了移动设备访问桌面开发环境的解决方案，增强了开发灵活性

Abstract: Macky (Website) Macky allows for secure remote access to your Mac's native terminal and integrated AI coding assistants like Claude Code and Codex directly from your iPhone.

</details>


### [29] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/-MxikTlFXqV5P8ksUMdYZ9NOX5bS6i8aXiggCL64sG8=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Macky是一个允许从iPhone安全远程访问Mac原生终端和集成AI编码助手（如Claude Code和Codex）的工具


<details>
  <summary>Details</summary>
Motivation: 解决开发者需要在移动设备上访问Mac开发环境的需求，特别是需要远程使用AI编码助手进行开发工作

Method: 开发一个iOS应用，通过安全连接远程访问Mac的终端环境，并集成AI编码助手功能

Result: 创建了一个功能完整的iOS应用，能够安全地从iPhone访问Mac终端和AI编码助手

Conclusion: Macky为开发者提供了移动设备上访问Mac开发环境的便捷解决方案，特别适合需要随时随地进行编码工作的场景

Abstract: Macky (Website) Macky allows for secure remote access to your Mac's native terminal and integrated AI coding assistants like Claude Code and Codex directly from your iPhone.

</details>


### [30] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/xoYDLb3KE14-jwuhufvKw_poEnCzPsU8YW8l4zM1RSc=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Macky是一个让用户能够从iPhone安全远程访问Mac原生终端和集成AI编程助手（如Claude Code和Codex）的工具


<details>
  <summary>Details</summary>
Motivation: 解决开发者在移动设备上无法方便访问Mac开发环境的痛点，提供远程访问终端和AI编程助手的能力

Method: 开发一个允许从iPhone安全远程访问Mac原生终端和集成AI编程助手的应用程序

Result: 创建了Macky工具，实现了从iPhone远程访问Mac终端和AI编程助手的功能

Conclusion: Macky为开发者提供了移动设备上访问Mac开发环境的便捷解决方案

Abstract: Macky (Website) Macky allows for secure remote access to your Mac's native terminal and integrated AI coding assistants like Claude Code and Codex directly from your iPhone.

</details>


### [31] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c94c9d88d-81bc37f9-34d2-462e-be63-cfc9cef268a4-000000/ErN7MUuVEeydLNkIUvNspaNg2IC0_me49G5Ha-5jgSE=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Macky是一个允许从iPhone安全远程访问Mac原生终端和集成AI编程助手（如Claude Code和Codex）的工具


<details>
  <summary>Details</summary>
Motivation: 解决开发者需要在移动设备上访问Mac终端和AI编程助手的需求，提供便捷的远程编程体验

Method: 开发一个iOS应用，通过安全连接远程访问Mac终端，并集成Claude Code和Codex等AI编程助手

Result: 创建了Macky工具，实现了从iPhone安全访问Mac终端和AI编程助手的功能

Conclusion: Macky为开发者提供了在移动设备上进行远程编程的便利解决方案

Abstract: Macky (Website) Macky allows for secure remote access to your Mac's native terminal and integrated AI coding assistants like Claude Code and Codex directly from your iPhone.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data](https://arxiv.org/abs/2602.21320)
*Emre Can Acikgoz,Cheng Qian,Jonas Hübotter,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: Tool-R0框架通过自博弈强化学习从零开始训练通用工具调用智能体，无需预训练数据，通过生成器和求解器的协同进化实现自我演化。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的智能体训练通常依赖精心构建的任务-解决方案对和大量人工监督，这限制了智能体向超级智能系统的开放式自我演化。需要一种无需预训练数据的方法来训练通用工具调用智能体。

Method: 提出Tool-R0框架，从同一基础LLM初始化生成器和求解器，通过互补奖励机制进行协同进化：生成器在求解器能力边界提出挑战性任务，求解器学习使用真实世界工具调用解决这些任务，形成自我演化循环。

Result: 在不同工具使用基准测试中，Tool-R0相比基础模型实现了92.5%的相对改进，并在相同设置下超越了完全监督的工具调用基线模型。

Conclusion: Tool-R0框架展示了通过自博弈强化学习从零开始训练工具调用智能体的可行性，为LLM智能体的自我演化提供了实证见解，包括协同进化、课程动态和扩展行为分析。

Abstract: Large language models (LLMs) are becoming the foundation for autonomous agents that can use tools to solve complex tasks. Reinforcement learning (RL) has emerged as a common approach for injecting such agentic capabilities, but typically under tightly controlled training setups. It often depends on carefully constructed task-solution pairs and substantial human supervision, which creates a fundamental obstacle to open-ended self-evolution toward superintelligent systems. In this paper, we propose Tool-R0 framework for training general purpose tool-calling agents from scratch with self-play RL, under a zero-data assumption. Initialized from the same base LLM, Tool-R0 co-evolves a Generator and a Solver with complementary rewards: one proposes targeted challenging tasks at the other's competence frontier and the other learns to solve them with real-world tool calls. This creates a self-evolving cycle that requires no pre-existing tasks or datasets. Evaluation on different tool-use benchmarks show that Tool-R0 yields 92.5 relative improvement over the base model and surpasses fully supervised tool-calling baselines under the same setting. Our work further provides empirical insights into self-play LLM agents by analyzing co-evolution, curriculum dynamics, and scaling behavior.

</details>


### [33] [GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning](https://arxiv.org/abs/2602.21492)
*Ningyuan Yang,Weihua Du,Weiwei Sun,Sean Welleck,Yiming Yang*

Main category: cs.LG

TL;DR: GradAlign是一种用于大语言模型强化学习的数据选择方法，通过梯度对齐机制从验证集学习，优先选择与验证梯度方向一致的问题，形成自适应课程，在不可靠奖励、分布不平衡和低效用训练数据等挑战性场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的强化学习性能对训练问题质量高度敏感，因为RL具有非平稳性（策略不断演化，学习受探索和奖励反馈影响），而传统方法依赖人工筛选或简单启发式过滤器（如准确率），可能包含错误或低效用问题。

Method: 提出GradAlign方法：使用小型可信验证集，优先选择那些策略梯度与验证梯度方向对齐的训练问题，形成自适应课程。通过梯度对齐机制来导航非平稳策略优化。

Result: 在三个具有挑战性的数据场景中评估：不可靠奖励信号、分布不平衡和低效用训练语料库。GradAlign始终优于现有基线方法，展示了梯度方向信号在非平稳策略优化中的重要性，带来更稳定的训练和更好的最终性能。

Conclusion: 梯度对齐数据选择方法能有效提升大语言模型强化学习的性能，特别是在具有挑战性的数据场景中。方向性梯度信号对于导航非平稳策略优化至关重要。

Abstract: Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is shaped by exploration and reward feedback, unlike supervised fine-tuning (SFT) with fixed trajectories. As a result, prior work often relies on manual curation or simple heuristic filters (e.g., accuracy), which can admit incorrect or low-utility problems. We propose GradAlign, a gradient-aligned data selection method for LLM reinforcement learning that uses a small, trusted validation set to prioritize training problems whose policy gradients align with validation gradients, yielding an adaptive curriculum. We evaluate GradAlign across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus, showing that GradAlign consistently outperforms existing baselines, underscoring the importance of directional gradient signals in navigating non-stationary policy optimization and yielding more stable training and improved final performance. We release our implementation at https://github.com/StigLidu/GradAlign

</details>


### [34] [AgentLTV: An Agent-Based Unified Search-and-Evolution Framework for Automated Lifetime Value Prediction](https://arxiv.org/abs/2602.21634)
*Chaowei Wu,Huazhu Chen,Congde Yuan,Qirui Yang,Guoqing Song,Yue Gao,Li Luo,Frank Youhua Chen,Mengzhuo Guo*

Main category: cs.LG

TL;DR: AgentLTV：基于智能体的自动化LTV建模框架，通过MCTS和进化算法搜索最优建模方案


<details>
  <summary>Details</summary>
Motivation: LTV预测在不同决策场景中数据模式差异大，传统方法需要构建复杂的场景特定管道，成本高且难以迁移

Method: 基于LLM的智能体框架，将候选方案视为可执行管道程序。采用两阶段搜索：MCTS阶段探索建模选择空间，EA阶段通过岛屿进化算法优化最佳程序

Result: 在大规模专有数据集和公共基准测试中，AgentLTV在排名和误差指标上均发现强模型，在线桶级分析显示排名一致性和价值校准得到改善

Conclusion: AgentLTV已成功在线部署，建议使用MCTS快速适应新数据模式，EA进行稳定优化，并通过桶级排名和校准诊断验证部署准备

Abstract: Lifetime Value (LTV) prediction is critical in advertising, recommender systems, and e-commerce. In practice, LTV data patterns vary across decision scenarios. As a result, practitioners often build complex, scenario-specific pipelines and iterate over feature processing, objective design, and tuning. This process is expensive and hard to transfer. We propose AgentLTV, an agent-based unified search-and-evolution framework for automated LTV modeling. AgentLTV treats each candidate solution as an {executable pipeline program}. LLM-driven agents generate code, run and repair pipelines, and analyze execution feedback. Two decision agents coordinate a two-stage search. The Monte Carlo Tree Search (MCTS) stage explores a broad space of modeling choices under a fixed budget, guided by the Polynomial Upper Confidence bounds for Trees criterion and a Pareto-aware multi-metric value function. The Evolutionary Algorithm (EA) stage refines the best MCTS program via island-based evolution with crossover, mutation, and migration. Experiments on a large-scale proprietary dataset and a public benchmark show that AgentLTV consistently discovers strong models across ranking and error metrics. Online bucket-level analysis further indicates improved ranking consistency and value calibration, especially for high-value and negative-LTV segments. We summarize practitioner-oriented takeaways: use MCTS for rapid adaptation to new data patterns, use EA for stable refinement, and validate deployment readiness with bucket-level ranking and calibration diagnostics. The proposed AgentLTV has been successfully deployed online.

</details>
