<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [wechat.article](#wechat.article) [Total: 6]
- [tldr.article](#tldr.article) [Total: 4]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PretrainZero: Reinforcement Active Pretraining](https://arxiv.org/abs/2512.03442)
*Xingrun Xing,Zhiyuan Fan,Jie Lou,Guoqi Li,Jiajun Zhang,Debing Zhang*

Main category: cs.CL

TL;DR: PretrainZero是一个基于预训练语料的强化主动学习框架，通过主动识别信息内容、自监督学习和验证扩展，将RL从领域特定的后训练扩展到通用预训练，显著提升基础模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的大思考模型虽然在特定领域（如软件和数学）展现出专家级能力，但仍严重依赖特定领域可验证的奖励，这限制了通用推理能力的扩展。需要打破验证数据壁垒，实现更通用的推理能力。

Method: 提出PretrainZero框架：1）主动预训练：学习统一推理策略，主动从预训练语料中识别合理且信息丰富的内容；2）自监督学习：无需可验证标签、预训练奖励模型或监督微调，直接在通用语料上使用RL预训练推理器；3）验证扩展：通过处理越来越难的掩码跨度，增强基础模型的通用推理能力。

Result: 在强化预训练中，PretrainZero将Qwen3-4B-Base在MMLU-Pro、SuperGPQA和数学平均基准上的性能分别提升了8.43、5.96和10.60分。预训练模型也可作为下游RLVR任务的推理基础模型。

Conclusion: PretrainZero成功将强化学习从领域特定的后训练扩展到通用预训练，通过主动学习和自监督方法打破了验证数据壁垒，显著提升了基础模型的通用推理能力，为实现更广泛的人工通用智能提供了新途径。

Abstract: Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.

</details>


### [2] [Different types of syntactic agreement recruit the same units within large language models](https://arxiv.org/abs/2512.03676)
*Daria Kryvosheieva,Andrea de Varda,Evelina Fedorenko,Greta Tuckute*

Main category: cs.CL

TL;DR: 该研究使用认知神经科学启发的功能定位方法，在7个开源大语言模型中识别对67种英语句法现象最敏感的单元，发现不同句法一致性现象（如主谓一致、回指一致、限定词-名词一致）会激活重叠的单元集合，表明句法一致性在大语言模型中构成有意义的函数类别。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型能够可靠地区分语法正确和错误的句子，但语法知识在模型内部如何表征仍然是一个开放性问题。研究者希望探究不同句法现象在大语言模型中是否共享或使用不同的组件。

Method: 采用受认知神经科学启发的功能定位方法，在7个开源大语言模型中识别对67种英语句法现象最敏感的单元。通过跨句子一致性验证这些单元，并进行因果分析确认它们对模型句法性能的支持作用。研究扩展到英语、俄语和中文，并在57种不同语言中进行跨语言分析。

Result: 研究发现不同句法一致性现象（如主谓一致、回指一致、限定词-名词一致）会激活重叠的单元集合，表明句法一致性在大语言模型中构成有意义的函数类别。这一模式在英语、俄语和中文中都成立，并且在57种语言的跨语言分析中，结构更相似的语言在主谓一致性方面共享更多单元。

Conclusion: 句法一致性作为句法依赖关系的关键标记，在大语言模型的表征空间中构成了有意义的类别。研究揭示了句法知识在大语言模型中的组织方式，为理解模型内部句法处理机制提供了重要见解。

Abstract: Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.

</details>


### [3] [Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective](https://arxiv.org/abs/2512.03759)
*Jingyang Ou,Jiaqi Han,Minkai Xu,Shaoxuan Xu,Jianwen Xie,Stefano Ermon,Yi Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: ESPO提出了一种针对扩散大语言模型的序列级强化学习框架，解决了传统token级RL方法在dLLMs上的不适用问题，通过ELBO作为序列级似然代理，在数学推理、编程和规划任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自回归语言模型中效果显著，但将其应用于扩散大语言模型存在根本性挑战。核心困难在于似然近似：自回归模型天然提供token级条件概率，而dLLMs通过迭代非自回归去噪步骤生成序列，缺乏这种分解。

Method: 提出ELBO-based Sequence-level Policy Optimization (ESPO)，将整个序列生成视为单一动作，使用ELBO作为可处理的序列级似然代理。方法包含token级重要性比率归一化和鲁棒的KL散度估计，确保大规模训练的稳定性。

Result: 在数学推理、编程和规划任务上的广泛实验表明，ESPO显著优于token级基线方法，在Countdown任务上实现了20-40分的巨大提升，同时在数学和编程基准上保持一致的增益。

Conclusion: ESPO为dLLMs中的强化学习建立了一个原则性且经验有效的序列级优化范式，解决了自回归与非自回归模型在RL方法上的根本性不匹配问题。

Abstract: Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.

</details>


### [4] [SkillFactory: Self-Distillation For Learning Cognitive Behaviors](https://arxiv.org/abs/2512.04072)
*Zayne Sprague,Jack Lu,Manya Wadhwa,Sedrick Keh,Mengye Ren,Greg Durrett*

Main category: cs.CL

TL;DR: SkillFactory是一种在强化学习前通过监督微调让模型学习认知技能的方法，使用模型自身生成的"银牌"训练数据，帮助模型在RL后更好地泛化到更难任务


<details>
  <summary>Details</summary>
Motivation: 当基础语言模型不具备某些认知技能（如验证答案、回溯、尝试替代方法等）时，如何让模型学会利用这些技能？现有方法依赖从更强模型蒸馏，但需要探索不依赖蒸馏的方法

Method: 1) 在RL前的监督微调阶段，使用模型自身生成的样本重新排列，形成认知技能格式的训练数据（"银牌"SFT轨迹）；2) 这些可能不完美的训练数据用于为模型学习技能做准备；3) 然后在RL阶段进一步优化

Result: 1) SkillFactory SFT初始化帮助模型在RL后更好地泛化到更难任务变体；2) 模型确实使用了认知技能；3) SkillFactory模型在域外任务上比基础RL模型更稳健，回归更少

Conclusion: 在RL前学习的归纳偏置有助于模型学习稳健的认知技能使用，SkillFactory方法有效且不依赖从更强模型蒸馏

Abstract: Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These "silver" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [5] [基于“恐惧-好奇”引导的安全高效端到端自主导航<em class="highlight">强化学习</em>方法](http://mp.weixin.qq.com/s?__biz=MzU5ODQ5ODQ1Mw==&mid=2247666495&idx=1&sn=42f487b46fc3f8ebc4d3ff31f9169ed7&chksm=ff66bcce5ccea898ceb50abb4a7fb03a176e18963965e8b270cf9c2ef8ea046e6b53b689f9f1#rd)
*焉知汽车*

Main category: wechat.article

TL;DR: 同时，这也表明模拟生物神经与心理机制有助于提升强化学习的安全性与效率。关键词：自动导航；恐惧与好奇；安全约束；经验回放优化 Ⅰ 引言 自主地面车辆（AGVs）正通过减少人类工作量并实现自主移动，推动机器人及智


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 同时，这也表明模拟生物神经与心理机制有助于提升强化学习的安全性与效率。关键词：自动导航；恐惧与好奇；安全约束；经验回放优化 Ⅰ 引言 自主地面车辆（AGVs）正通过减少人类工作量并实现自主移动，推动机器人及智

</details>


### [6] [字节跳动Seed团队发布面向长周期灵巧操作的<em class="highlight">强化学习</em>框架GR-RL](http://mp.weixin.qq.com/s?__biz=MzkxNDE4ODIzMQ==&mid=2247487186&idx=1&sn=05774a725d63589e0f52b0baf0e4a5d0&chksm=c041ae5a9bd325e15ce0444c407a7843740846e49f92c3b90cf8260ed4743e8a49f76e53b5f1#rd)
*Mbot具身智能实验室*

Main category: wechat.article

TL;DR: 强化学习路线思考过去几年，具身智能的主流叙事是“模仿学习（BC/IL）+ 大数据”，试图复刻 LLM 的Scaling Law。但最近这几个爆火的工作表明，纯模仿学习的“红利”正在吃紧，强化学习（RL）正在以一种全新的、更务实的姿态


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习路线思考过去几年，具身智能的主流叙事是“模仿学习（BC/IL）+ 大数据”，试图复刻 LLM 的Scaling Law。但最近这几个爆火的工作表明，纯模仿学习的“红利”正在吃紧，强化学习（RL）正在以一种全新的、更务实的姿态

</details>


### [7] [阿里团队最新论文--用大模型稳定<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzUyMDk2NDY2Mw==&mid=2247486218&idx=2&sn=49709f3bc596eb06b4fa74076a91a5a3&chksm=f8ad7875916ce8eaafe6a7932d76c138ebfc53b4a7dc1a0cbfa678ba8fa489a40852f27f6033#rd)
*lightA*

Main category: wechat.article

TL;DR: #LLM #强化学习 #阿里系


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: #LLM #强化学习 #阿里系

</details>


### [8] [孔维阳 | 多智能体<em class="highlight">强化学习</em>研究综述](http://mp.weixin.qq.com/s?__biz=Mzg4NTUxMDgyMA==&mid=2247507697&idx=1&sn=105192c5133eca00b49a1a302b28ce05&chksm=ce91361ba5750736bee9728d574339eb542e9e8450b7366e462d3762062d9494f880b46665f7#rd)
*科技和产业*

Main category: wechat.article

TL;DR: 摘要：多智能体强化学习是人类通往通用人工智能的重要方法。当前对于多智能体强化学习的研究主要从任务类型、训练框架、多智能体系统中面临的挑战和算法应用4个方面展开。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 摘要：多智能体强化学习是人类通往通用人工智能的重要方法。当前对于多智能体强化学习的研究主要从任务类型、训练框架、多智能体系统中面临的挑战和算法应用4个方面展开。

</details>


### [9] [银行探索AI<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzUyNTk1ODYwOA==&mid=2247502771&idx=1&sn=609b0b3c7f5f68a98a605b54e5eb1f3a&chksm=fb53a1767df565cb4b62e10dce82f1ab35ac137d8de46e142d723a42fc1d224f990bc5dfee5d#rd)
*财富烟台*

Main category: wechat.article

TL;DR: 强化学习的核心思想是通过与环境的交互，在每一时刻选择一个动作，并根据环境反馈的奖励或惩罚，调整其策略，以实现长期最大化的总奖励。在银行业，强化学习的优势体现在其强大的动态决策能力上，特别适合应对金融市


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习的核心思想是通过与环境的交互，在每一时刻选择一个动作，并根据环境反馈的奖励或惩罚，调整其策略，以实现长期最大化的总奖励。在银行业，强化学习的优势体现在其强大的动态决策能力上，特别适合应对金融市

</details>


### [10] [Nature Machine Intelligence | 同态加密赋能AI，推动可信安全深度<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzY0MDEwOTI4Mw==&mid=2247487188&idx=1&sn=b1668e3e5f7d6b62f9d79e83f77af147&chksm=f1db7d026b6272d5a0f51355cad29d2f71240487e4070bf50a11450aa415fad2fe4112a29a61#rd)
*LifeNexAI*

Main category: wechat.article

TL;DR: 深度强化学习通过智能体与环境的持续交互来学习最优策略，这一过程涉及大量敏感数据的收集和处理。在医疗诊断、金融风控等场景中，这些数据往往包含个人隐私信息，传统DRL训练方式使得数据面临被恶意窃取或滥用的风险


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 深度强化学习通过智能体与环境的持续交互来学习最优策略，这一过程涉及大量敏感数据的收集和处理。在医疗诊断、金融风控等场景中，这些数据往往包含个人隐私信息，传统DRL训练方式使得数据面临被恶意窃取或滥用的风险

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [11] [A Practical Approach to Verifying Code at Scale](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.openai.com%2Fscaling-code-verification%2F%3Futm_source=tldrai/1/0100019adf6cef0e-391161ea-3c75-4db1-9d24-f8b7fdfff6d2-000000/H4IuKdPnj9gIE76MMdnAII_ePiSVvq16oepX7AayLJI=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI开发了一个基于Codex的代码审查代理系统，用于大规模验证代码安全性，每天处理超过10万个外部PR，提供仓库级上下文和执行访问，已成功捕获关键bug并保护高风险实验。


<details>
  <summary>Details</summary>
Motivation: 传统噪声安全工具容易被绕过，需要更可靠的代码验证方法来确保大规模代码库的安全性，特别是在处理大量外部贡献和高风险实验时。

Method: 基于Codex训练了一个代理式代码审查系统，该系统能够访问仓库级上下文和执行权限，对代码进行深度分析和验证。

Result: 系统每天处理超过10万个外部PR，成功捕获了阻止发布的bug，并有效保护了高风险实验的安全性。

Conclusion: 代理式代码审查系统提供了一种实用的大规模代码验证方法，显著提高了代码安全性和质量保证能力。

Abstract: A Practical Approach to Verifying Code at Scale (7 minute read) OpenAI trained an agentic code reviewer for Codex since noisy safety tools are inevitably bypassed. The system now handles 100k+ external PRs daily, providing repo-wide context and execution access. Internally, it has caught launch-blocking bugs and protected high-stakes experiments.

</details>


### [12] [Amazon Bedrock AgentCore adds quality evaluations and policy controls for deploying trusted AI agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Faws%2Famazon-bedrock-agentcore-adds-quality-evaluations-and-policy-controls-for-deploying-trusted-ai-agents%2F%3Futm_source=tldrdevops/1/0100019ae41b5d32-e397873b-baf2-4ccb-8078-1dea7997e506-000000/SmZONAMVAUXL3h-0RwNGhfakWb1P-I5DgWIJQQhrc2c=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Amazon Bedrock AgentCore更新增加了质量评估、策略控制、情景记忆和双向流式传输功能，用于部署可信的AI代理


<details>
  <summary>Details</summary>
Motivation: 为了增强AI代理的可信度和部署控制，提供更好的性能分析和自然对话能力

Method: 通过添加策略控制、质量评估系统、情景记忆机制和双向流式传输技术来增强AgentCore平台

Result: 新功能已在特定AWS区域可用，可通过AWS免费层访问，AgentCore SDK已可供下载

Conclusion: Bedrock AgentCore的更新为部署可信AI代理提供了更强大的工具和控制能力

Abstract: Amazon Bedrock AgentCore adds quality evaluations and policy controls for deploying trusted AI agents (8 minute read) Amazon Bedrock AgentCore has been updated with new features, including Policy for agent control, Evaluations for performance analysis, episodic memory for experience-based learning, and bidirectional streaming for natural conversations. The new features are available in select AWS Regions and accessible through the AWS Free Tier for new users. The AgentCore SDK has been downlo...

</details>


### [13] [Memori](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMemoriLabs%2FMemori%3Futm_source=tldrdevops/1/0100019ae41b5d32-e397873b-baf2-4ccb-8078-1dea7997e506-000000/MrnhgVEI79iI3pvpZ0kFCnWkNVEhYuhN8RyBoQyRP9U=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Memori是一个开源记忆引擎，可将LLM交互归因于实体和流程，提供无延迟的记忆增强和高级增强功能


<details>
  <summary>Details</summary>
Motivation: 现有LLM交互缺乏结构化记忆管理，难以将交互归因于特定实体和流程，需要无延迟的记忆增强解决方案

Method: 开发开源记忆引擎，通过实体和流程归因机制，实现无延迟的记忆增强和高级增强功能

Result: 创建了Memori工具，能够有效管理LLM交互记忆，提供无延迟的记忆增强，支持高级增强功能

Conclusion: Memori作为开源记忆引擎，成功解决了LLM交互记忆管理的需求，为实体和流程归因提供了有效工具

Abstract: Memori (GitHub Repo) Memori, an open-source memory engine, can be installed to attribute LLM interactions to entities and processes. The tool enhances memories without latency and offers advanced augmentation.

</details>


### [14] [Skip the docs. Let AI add auth to your Next.js app](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FZGn41hf/2/0100019ae4300932-43f46e76-9193-48d6-a96b-61cd31327356-000000/liJb2R-24qPxHZoW8pDWU5SpxqZKr7IWNQhS55ZjPZQ=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI助手帮助开发者快速为Next.js应用添加认证功能，无需查阅文档和手动复制代码


<details>
  <summary>Details</summary>
Motivation: 简化Next.js应用认证配置流程，避免在文档和编辑器之间切换，减少手动代码复制和配置遗漏

Method: 提供预构建的提示词，开发者可在Cursor、Claude或ChatGPT中直接使用，AI助手指导完成SDK安装、中间件配置和认证组件添加

Result: 从空项目到完整可用的认证系统一步完成，无需查阅文档和手动复制代码

Conclusion: AI驱动的开发流程可以显著简化Next.js应用认证配置，提高开发效率

Abstract: Skip the docs. Let AI add auth to your Next.js app (Sponsor) Clerk's Next.js quickstart includes a pre-built prompt you can copy or open directly in Cursor, Claude, or ChatGPT. Your AI assistant walks through the complete setup: installing the SDK, configuring middleware, and adding auth components. No bouncing between docs and your editor, no manual code copying, no missed configuration steps. Just paste the prompt (or click to open) and go from empty project to working authentication in one...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He,Dingmin Wang*

Main category: cs.AI

TL;DR: 符号求解器集成方法仅对需要有限隐式推理但涉及充足搜索空间的问题有帮助，而传统长思维链方法在深度推理问题上表现更好。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长思维链在复杂推理任务上表现良好，但这种方法会产生大量token开销，且模型可能"过度思考"产生冗长推理链甚至错误答案。符号求解器集成方法利用LLM的代码生成能力将推理任务转化为可执行代码，然后用符号求解器解决，但何时传统长思维链方法能被符号求解器增强仍是一个开放问题。

Method: 探索符号求解器集成方法何时能增强传统长思维链方法。通过实验比较两种方法在不同类型推理问题上的表现，包括需要有限隐式推理但涉及充足搜索空间的问题、演绎问题、约束满足问题等。

Result: 符号求解器集成方法仅在问题需要有限隐式推理但涉及充足搜索空间时有效。最新LLM（如GPT-4o）在推理深度较浅的演绎问题上表现更好，而符号求解器集成方法能显著提升LLM在需要重复回溯的约束满足问题上的性能。当提供声明性示例时，即使是CodeLlama-13B也能在困难的斑马谜题上超越GPT-4o。

Conclusion: 符号求解器集成方法与传统长思维链方法各有优势领域，应根据问题类型选择合适的推理方法。对于需要有限隐式推理但涉及大量搜索空间的问题，符号求解器集成方法更有效；对于深度推理问题，传统长思维链方法表现更好。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [16] [Prior preferences in active inference agents: soft, hard, and goal shaping](https://arxiv.org/abs/2512.03293)
*Filippo Torresan,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 该研究比较了主动推理中四种偏好分布定义方式（硬目标vs软目标、有无目标塑造）在网格世界导航任务中的表现，发现目标塑造能提升性能但会牺牲环境动态学习。


<details>
  <summary>Details</summary>
Motivation: 主动推理使用期望自由能作为规划决策目标，但偏好分布如何指定及其对推理学习的影响在文献中很少被关注。本研究旨在探索不同偏好分布定义方式对智能体性能的影响。

Method: 考虑了四种偏好分布定义方式：硬目标vs软目标、有无目标塑造（中间目标）。在网格世界导航任务中比较了四种智能体的表现，每种智能体使用一种偏好分布。

Result: 目标塑造能带来最佳整体性能（促进利用），但会牺牲对环境转移动态的学习（阻碍探索）。

Conclusion: 偏好分布的定义方式显著影响主动推理智能体的性能表现，需要在利用和探索之间进行权衡。

Abstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).

</details>


### [17] [Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia](https://arxiv.org/abs/2512.03318)
*Chandler Smith,Marwa Abdulhai,Manfred Diaz,Marko Tesic,Rakshit S. Trivedi,Alexander Sasha Vezhnevets,Lewis Hammond,Jesse Clifton,Minsuk Chang,Edgar A. Duéñez-Guzmán,John P. Agapiou,Jayd Matyas,Danny Karmon,Akash Kundu,Aliaksei Korshuk,Ananya Ananya,Arrasy Rahman,Avinaash Anand Kulandaivel,Bain McHale,Beining Zhang,Buyantuev Alexander,Carlos Saith Rodriguez Rojas,Caroline Wang,Chetan Talele,Chenao Liu,Chichen Lin,Diana Riazi,Di Yang Shi,Emanuel Tewolde,Elizaveta Tennant,Fangwei Zhong,Fuyang Cui,Gang Zhao,Gema Parreño Piqueras,Hyeonggeun Yun,Ilya Makarov,Jiaxun Cui,Jebish Purbey,Jim Dilkes,Jord Nguyen,Lingyun Xiao,Luis Felipe Giraldo,Manuela Chacon-Chamorro,Manuel Sebastian Rios Beltran,Marta Emili García Segura,Mengmeng Wang,Mogtaba Alim,Nicanor Quijano,Nico Schiavone,Olivia Macmillan-Scott,Oswaldo Peña,Peter Stone,Ram Mohan Rao Kadiyala,Rolando Fernandez,Ruben Manrique,Sunjia Lu,Sheila A. McIlraith,Shamika Dhuri,Shuqing Shi,Siddhant Gupta,Sneheel Sarangi,Sriram Ganapathi Subramanian,Taehun Cha,Toryn Q. Klassen,Wenming Tu,Weijian Fan,Wu Ruiyang,Xue Feng,Yali Du,Yang Liu,Yiding Wang,Yipeng Kang,Yoonchang Sung,Yuxuan Chen,Zhaowei Zhang,Zhihan Wang,Zhiqiang Wu,Ziang Chen,Zilong Zheng,Zixia Jia,Ziyan Wang,Dylan Hadfield-Menell,Natasha Jaques,Tim Baarslag,Jose Hernandez-Orallo,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 该论文提出了一种评估LLM智能体在零样本混合动机环境中合作能力的方法，使用Concordia自然语言多智能体模拟环境，通过NeurIPS 2024竞赛发现当前智能体在说服和规范执行等需要鲁棒泛化的合作场景中存在显著能力差距。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在社会交互方面展现出强大能力，并越来越多地部署在与人类和其他智能体交互的场景中。然而，现有评估方法无法衡量这些能力在新颖社交情境中的泛化表现，特别是在混合动机环境中的合作能力。

Method: 引入基于Concordia自然语言多智能体模拟环境的评估方法，测试智能体在零样本、混合动机环境中与多样化伙伴和情境下的合作能力，通过识别和利用互利机会来衡量通用合作智能。

Result: NeurIPS 2024 Concordia竞赛的实证结果显示，当前智能体能力与可靠合作所需的鲁棒泛化之间存在显著差距，特别是在需要说服和规范执行的场景中。

Conclusion: LLM智能体在复杂社交情境中的合作能力仍需大幅提升，特别是在需要适应新伙伴和环境、进行有效说服和执行社会规范的场景中，这为未来研究指明了重要方向。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

</details>


### [18] [Multimodal Reinforcement Learning with Agentic Verifier for AI Agents](https://arxiv.org/abs/2512.03438)
*Reuben Tan,Baolin Peng,Zhengyuan Yang,Hao Cheng,Oier Mees,Theodore Zhao,Andrea Tupini,Isar Meijier,Qianhui Wu,Yuncong Yang,Lars Liden,Yu Gu,Sheng Zhang,Xiaodong Liu,Lijuan Wang,Marc Pollefeys,Yong Jae Lee,Jianfeng Gao*

Main category: cs.AI

TL;DR: Argos是一个用于训练多模态推理模型的奖励代理，通过智能选择评分函数来同时评估最终答案准确性、时空定位和推理过程质量，显著提升多模态强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态强化学习主要依赖稀疏的基于最终结果的奖励，缺乏对推理过程的细粒度指导。不同样本需要不同的评分函数，且教师模型可能提供噪声奖励信号，因此需要更智能的奖励机制。

Method: 提出Argos奖励代理，为每个样本从教师模型和基于规则的评分函数池中选择合适的函数，同时评估：(1)最终响应准确性，(2)实体和动作的时空定位，(3)推理过程质量。

Result: 在空间推理、视觉幻觉以及机器人和具身AI基准测试中取得最先进结果。证明仅依赖SFT后训练不足，需要在线验证防止模型崩溃到非接地解，并能减少奖励黑客行为。

Conclusion: Argos通过智能奖励选择机制显著提升多模态推理模型的训练效果，提供理论上的帕累托最优性证明，为多模态强化学习提供了更有效的训练框架。

Abstract: Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.

</details>


### [19] [PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks](https://arxiv.org/abs/2512.03549)
*Yuki Orimo,Iori Kurata,Hodaka Mori,Ryuhei Okuno,Ryohto Sawada,Daisuke Okanohara*

Main category: cs.AI

TL;DR: PARC是一个用于自主执行长时程计算任务的编码智能体，采用分层多智能体架构，包含任务规划、执行和自我评估反馈机制，能够在材料科学和Kaggle竞赛中实现自主工作。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在执行长时程、复杂的计算任务时，往往需要大量人工干预，难以实现真正的自主工作。需要开发能够自我评估、自我修正的智能体系统来完成大规模科学和分析工作。

Method: PARC采用分层多智能体架构，包含任务规划、执行和自我评估反馈机制。系统能够从独立上下文评估自身行为和结果，提供自我评估和自我反馈，从而检测和修正高层战略错误。

Result: 在材料科学任务中，PARC成功复现了锂离子传导和合金偏析研究的关键结果，协调了数十个并行模拟任务（每个约43小时计算时间）。在Kaggle实验中，从自然语言指令开始，PARC进行数据分析和实现搜索策略，产生了与人工设计基线竞争的结果。

Conclusion: 分层多智能体系统与自我评估、自我反馈机制的结合，能够实现AI系统在独立、大规模科学和分析工作中的潜力。

Abstract: We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.

</details>


### [20] [Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks](https://arxiv.org/abs/2512.03560)
*Gianni Molinari,Fabio Ciravegna*

Main category: cs.AI

TL;DR: RP-ReAct是一种新颖的多智能体方法，将战略规划与低级执行解耦，通过Reasoner Planner Agent进行规划分析，Proxy-Execution Agent执行工具交互，并采用上下文保存策略管理大型工具输出，在ToolQA基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体在企业领域解决复杂任务时面临两个主要限制：单智能体架构导致轨迹不稳定，以及本地开源模型的小上下文窗口无法处理大型工具输出。需要一种更可靠高效的解决方案。

Method: 提出RP-ReAct多智能体框架，包含Reasoner Planner Agent（RPA）负责规划和分析，Proxy-Execution Agent（PEA）负责工具交互执行，并采用上下文保存策略通过外部存储管理大型工具输出。

Result: 在ToolQA基准测试中使用六种开源推理模型评估，RP-ReAct在性能、泛化能力和稳定性方面均优于现有最先进基线方法，在不同模型规模下都表现出增强的鲁棒性。

Conclusion: RP-ReAct通过解耦规划与执行的多智能体架构，有效解决了企业环境中复杂任务的协调问题，为可部署的企业级智能体解决方案铺平了道路。

Abstract: Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.

</details>


### [21] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li,Armando Solar-Lezama,Yisong Yue,Stephan Zheng*

Main category: cs.AI

TL;DR: 提出PAN编程模型，分离智能体工作流逻辑与推理策略，通过EnCompass框架实现，可快速提升智能体可靠性并灵活切换推理策略


<details>
  <summary>Details</summary>
Motivation: 当前智能体编程方法通常将核心工作流逻辑与推理时策略（如树搜索）耦合在一起，这种耦合限制了编程灵活性和实验效率

Method: 提出"概率天使非确定性"（PAN）编程模型，使用Python装饰器将智能体工作流程序编译为搜索空间，实现工作流与推理策略的解耦

Result: 通过三个案例研究表明，该框架能让程序员快速提升智能体可靠性，轻松切换不同推理策略，且只需少量额外编码

Conclusion: PAN模型和EnCompass框架为智能体编程提供了更灵活、可实验的方法，解耦了工作流设计与推理策略选择

Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


### [22] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang,Songxiang Liu,Disong Wang,Yuanyuan Wang,Guanglu Wan,Helen Meng*

Main category: cs.AI

TL;DR: 提出Omni-AutoThink自适应推理框架，通过动态调整推理深度解决现有Omni模型推理行为僵化的问题，包含自适应监督微调和自适应强化学习两阶段，构建了多模态自适应推理基准。


<details>
  <summary>Details</summary>
Motivation: 现有Omni模型在推理行为上存在僵化问题：对于简单问题过度推理，对于复杂问题又推理不足。需要一种能够根据任务难度动态调整推理深度的自适应推理框架。

Method: 提出两阶段框架：1) 自适应监督微调阶段，使用大规模推理增强数据赋予Omni模型基础推理能力；2) 自适应强化学习阶段，基于任务复杂度和奖励反馈优化推理行为。同时构建了涵盖文本、文本-音频、文本-视觉、文本-音频-视觉多模态的自适应推理基准。

Result: 实验结果表明，相比之前的基线方法，提出的框架在自适应推理性能上有显著提升。所有基准数据和代码将公开发布。

Conclusion: Omni-AutoThink框架通过动态调整推理深度，有效解决了Omni模型推理行为僵化的问题，在多模态自适应推理任务上表现出色。

Abstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [23] [A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)](https://arxiv.org/abs/2512.03887)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 提出Static-DRA，一种基于树状静态工作流的深度研究代理，通过可配置的Depth和Breadth参数让用户平衡研究质量与计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统静态RAG管道在处理复杂多轮研究任务时存在局限，需要更灵活的深度研究代理系统。

Method: 采用分层树状静态工作流架构，包含Supervisor、Independent和Worker三种代理，通过Depth和Breadth参数控制研究深度和广度。

Result: 在DeepResearch Bench上使用RACE框架评估，配置depth=2、breadth=5时获得34.72分，验证参数增加能提升研究深度和评分。

Conclusion: Static-DRA提供了实用且资源感知的解决方案，赋予用户对深度研究过程的透明控制。

Abstract: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.
  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.
  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/

</details>


### [24] [Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol](https://arxiv.org/abs/2512.03955)
*Niklas Jobs,Luis Miguel Vieira da Silva,Jayanth Somashekaraiah,Maximilian Weigand,David Kube,Felix Gehlhoff*

Main category: cs.AI

TL;DR: 提出了一个基于Blocksworld问题的可执行仿真基准测试，用于系统评估LLM智能体在工业自动化中的自适应规划与执行能力，通过标准化工具接口支持不同架构的对比。


<details>
  <summary>Details</summary>
Motivation: 工业自动化需要能够适应变化任务和环境的灵活控制策略，基于大语言模型的智能体具有这种自适应规划和执行潜力，但缺乏系统比较的标准化基准测试。

Method: 引入一个包含可执行仿真环境的基准测试，基于Blocksworld问题提供五个复杂度类别，通过集成模型上下文协议（MCP）作为标准化工具接口，使不同智能体架构能够无需特定修改即可连接和评估。

Result: 通过单智能体实现展示了基准测试的适用性，建立了用于比较基于LLM的规划与执行方法的定量指标。

Conclusion: 该基准测试为系统评估和比较LLM智能体在工业自动化中的自适应规划与执行能力提供了标准化框架，有助于推动该领域的发展。

Abstract: Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks](https://arxiv.org/abs/2512.03262)
*Songwen Zhao,Danqing Wang,Kexun Zhang,Jiaxuan Luo,Zhuo Li,Lei Li*

Main category: cs.SE

TL;DR: 论文提出SU S VI B E S基准测试，评估LLM编程代理在真实世界软件工程任务中的安全性，发现即使功能正确的解决方案中，仅有10.5%是安全的，对vibe coding在生产环境的安全性提出严重警告。


<details>
  <summary>Details</summary>
Motivation: 随着vibe coding（人类工程师指导LLM代理完成复杂编码任务）的普及，需要评估这种编程范式输出的代码是否真的安全，特别是在生产环境中部署的安全性。

Method: 提出SU S VI B E S基准测试，包含200个来自真实开源项目的功能请求任务，这些任务在人类程序员实现时导致了漏洞。使用前沿模型评估多个广泛使用的编程代理，并测试初步安全策略（如添加漏洞提示）的有效性。

Result: 所有编程代理在软件安全方面表现都很差。虽然SWE-Agent with Claude 4 Sonnet的61%解决方案功能正确，但只有10.5%是安全的。初步安全策略无法缓解这些安全问题。

Conclusion: 研究结果对vibe coding在安全敏感应用中的广泛采用提出了严重关切，表明当前LLM编程代理在生成安全代码方面存在重大缺陷。

Abstract: Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.

</details>


### [26] [Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization](https://arxiv.org/abs/2512.03421)
*Hexiang Xu,Hengyuan Liu,Yonghao Wu,Xiaolan Kang,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 该研究评估了13个大语言模型在故障定位任务上的表现，发现具备推理能力的先进模型（如OpenAI o3和DeepSeekR1）在准确性上表现优异，而缺乏推理能力的模型需要精心设计的提示词。虽然LLMs在简单故障定位中表现良好，但随着问题难度增加准确性下降，且存在过度推理和计算成本高的挑战。


<details>
  <summary>Details</summary>
Motivation: 新手程序员由于经验有限，在故障定位方面面临挑战。传统的故障定位方法（如SBFL和MBFL）缺乏对代码上下文的理解能力，对初学者效果有限。近年来，大语言模型展现出通过理解程序语法和语义来克服这些限制的潜力。

Method: 研究评估了6个闭源和7个开源LLMs，使用Codeflaws、Condefects和BugT三个数据集。BugT是新构建的数据集，专门设计用于减轻数据泄漏问题。研究比较了不同模型在故障定位任务上的表现，特别关注推理能力对性能的影响。

Result: 具备推理能力的先进模型（如OpenAI o3和DeepSeekR1）在准确性上表现优异，对提示工程的依赖最小。缺乏推理能力的模型（如GPT-4）需要精心设计的提示词来维持性能。LLMs在简单故障定位中表现良好，但随着问题难度增加准确性下降。过度推理和计算成本是主要挑战。新手程序员对LLMs的解释给予高度评价。

Conclusion: LLMs在提高调试效率方面具有潜力，但需要在推理能力和计算效率方面进一步改进才能实现实际应用。研究强调了LLMs对新手程序员辅助的重要价值，同时指出了当前模型的局限性。

Abstract: Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation](https://arxiv.org/abs/2512.03053)
*Andrew S. Cassidy,Guillaume Garreau,Jay Sivagnaname,Mike Grassi,Bernard Brezzo,John V. Arthur,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 提出一种利用LLM作为无损编码器/解码器的方法，通过源域到目标域再返回源域的转换验证，显著减少LLM的幻觉和遗漏问题，应用于从逻辑条件表生成硬件描述语言代码。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码生成任务中常见的幻觉和遗漏问题，提高生成代码的准确性和可靠性，特别是在硬件设计等关键领域。

Method: 采用信息论中的无损压缩思想，将LLM作为无损编码器将源域数据（如逻辑条件表）转换为目标域（如HDL代码），再作为无损解码器将生成的代码转换回源域，通过比较原始和重建的源域数据来验证生成质量。

Result: 使用7种不同LLM成功生成了二维片上网络路由器的完整HDL代码（13个单元，1500-2000行代码），并通过重建验证显著提高了生成准确性，不仅能够确认正确生成的逻辑，还能检测错误生成的逻辑，甚至帮助开发者发现设计规范错误。

Conclusion: 提出的无损编码/解码方法有效缓解了LLM的幻觉和遗漏问题，显著提高了代码生成的可靠性和开发效率，为LLM在关键系统设计中的应用提供了实用框架。

Abstract: We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.

</details>


### [28] [Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning](https://arxiv.org/abs/2512.03065)
*Nihir Chadderwala*

Main category: cs.LG

TL;DR: 提出一个结合AWS Strands Agents与Thompson Sampling上下文bandits的框架，让AI代理通过用户反馈学习最优决策策略，在生命科学领域提升用户满意度15-30%


<details>
  <summary>Details</summary>
Motivation: 生命科学中的生成式AI代理面临关键挑战：如何为从简单事实问题到复杂机制推理的多样化查询确定最优方法。传统方法依赖固定规则或昂贵的标注训练数据，都无法适应变化条件或用户偏好。

Method: 结合AWS Strands Agents与Thompson Sampling上下文bandits的框架，让AI代理仅从用户反馈中学习最优决策策略。系统优化三个关键维度：生成策略选择（直接vs.链式思考）、工具选择（文献搜索、药物数据库等）、领域路由（药理学、分子生物学、临床专家）。

Result: 在生命科学查询上的实证评估显示，相比随机基线，用户满意度提升15-30%，在20-30个查询后出现清晰的学习模式。

Conclusion: 该方法无需真实标签，能持续适应用户偏好，为代理式AI系统中的探索-利用困境提供了原则性解决方案。

Abstract: Generative AI agents in life sciences face a critical challenge: determining the optimal approach for diverse queries ranging from simple factoid questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, neither of which adapts to changing conditions or user preferences. We present a novel framework that combines AWS Strands Agents with Thompson Sampling contextual bandits to enable AI agents to learn optimal decision-making strategies from user feedback alone. Our system optimizes three key dimensions: generation strategy selection (direct vs. chain-of-thought), tool selection (literature search, drug databases, etc.), and domain routing (pharmacology, molecular biology, clinical specialists). Through empirical evaluation on life science queries, we demonstrate 15-30\% improvement in user satisfaction compared to random baselines, with clear learning patterns emerging after 20-30 queries. Our approach requires no ground truth labels, adapts continuously to user preferences, and provides a principled solution to the exploration-exploitation dilemma in agentic AI systems.

</details>


### [29] [E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing](https://arxiv.org/abs/2512.03109)
*Shuvom Sadhuka,Drew Prinster,Clara Fannjiang,Gabriele Scalia,Aviv Regev,Hanchen Wang*

Main category: cs.LG

TL;DR: 提出e-valuator方法，将任意黑盒验证器分数转换为具有可证明误报率控制的决策规则，用于评估AI代理轨迹的成功概率。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理验证器（如LLM法官和过程奖励模型）使用启发式评分，但缺乏正确性保证。需要一种方法将验证器分数转换为具有统计保证的决策规则，以可靠地区分成功与失败的代理轨迹。

Method: 将区分成功与失败轨迹问题构建为序列假设检验问题。基于e-process工具开发序列假设检验，在代理轨迹的每一步都保持统计有效性，支持对任意长动作序列的在线监控。

Result: 在六个数据集和三种代理上的实验表明，e-valuator相比其他策略具有更高的统计功效和更好的误报率控制。还能用于快速终止问题轨迹以节省token。

Conclusion: e-valuator提供了一个轻量级、模型无关的框架，将验证器启发式方法转换为具有统计保证的决策规则，使能部署更可靠的代理系统。

Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.

</details>


### [30] [SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning](https://arxiv.org/abs/2512.03244)
*Salman Rahman,Sruthi Gorantla,Arpit Gupta,Swastik Roy,Nanyun Peng,Yang Liu*

Main category: cs.LG

TL;DR: SPARK框架通过三阶段方法：1) 生成多样化解决方案并用验证器评估，2) 用验证输出微调生成式过程奖励模型，3) 将PRM-CoT作为奖励模型用于数学推理的强化学习，无需参考答案即可超越基于真实标签的方法。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型(PRMs)需要昂贵的步骤级标注或真实参考，限制了其应用。作者希望开发无需真实标签就能训练高质量PRM的方法，以扩大其在缺乏可验证答案领域的应用。

Method: 三阶段框架：1) 生成器产生多样化解决方案，验证器通过并行扩展（自一致性）和序列扩展（元批判）进行评估；2) 用验证输出作为合成训练数据微调生成式过程奖励模型；3) 将PRM与思维链验证(PRM-CoT)作为奖励模型用于强化学习，并引入格式约束防止奖励攻击。

Result: 在ProcessBench上达到67.5 F1，优于参考指导训练的66.4和GPT-4o的61.9。在六个数学推理基准上平均准确率达到47.4%，超越基于真实标签的RLVR(43.9%)。

Conclusion: SPARK框架实现了无需参考答案的强化学习训练，性能超越基于真实标签的方法，为缺乏可验证答案或难以获取真实标签的领域开辟了新可能性。

Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.

</details>


### [31] [Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval](https://arxiv.org/abs/2512.03276)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型（VLM）在事实召回任务上表现下降，原因是实体表示形成过晚，无法有效利用LLM原有的知识机制。通过早期实体解析可以恢复性能。


<details>
  <summary>Details</summary>
Motivation: 许多VLM在事实召回任务上表现不如其LLM骨干模型，这引发了对多模态微调有效性的质疑。研究旨在探究VLM如何将视觉输入与LLM的现有知识机制对齐。

Method: 对14种不同架构、规模和训练设置的VLM进行基准测试，使用归因修补、激活修补和探测技术分析性能差异，并测试两种性能恢复方法：从LLM骨干修补实体表示和使用思维链提示。

Result: 14个模型中11个出现事实召回性能下降。性能差的VLM因实体表示形成过晚而无法利用LLM原有机制，而高性能VLM能早期解析实体表示。两种恢复方法均能有效提升性能。

Conclusion: 早期实体解析速度是VLM能否有效利用预训练LLM机制的关键因素。机制分析能解释多模态对齐中的系统性失败，并为改进VLM设计提供指导。

Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.

</details>


### [32] [DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training](https://arxiv.org/abs/2512.03847)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Yuhui Wang,Sixian Li,Junjie Ye,Honglin Guo,Shichun Liu,Chenhao Huang,Yajie Yang,Junlin Shang,Senjie Jin,Ming Zhang,Jiazheng Zhang,Caishuang Huang,Yunke Zhang,Demei Yan,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: DVPO：结合条件风险理论与分布价值建模的新RL框架，通过token级价值分布提供细粒度监督，使用非对称风险正则化平衡鲁棒性与泛化性，在噪声监督下优于PPO、GRPO等方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界LLM后训练常面临噪声或不完整监督，复杂不可靠的监督信号会破坏训练稳定性并损害泛化能力。现有方法如最坏情况优化和均值方法往往忽视泛化性，可能产生过于保守的策略，导致在不同实际场景中表现不均。

Method: DVPO结合条件风险理论与分布价值建模：1)学习token级价值分布提供细粒度监督；2)应用非对称风险正则化塑造分布尾部：压缩下尾以抑制噪声负偏差，扩展上尾以保持探索多样性。

Result: 在多轮对话、数学推理和科学问答等广泛实验中，DVPO在噪声监督下持续优于PPO、GRPO和基于鲁棒贝尔曼的PPO，显示出在现实世界LLM后训练中的潜力。

Conclusion: DVPO通过分布价值建模和非对称风险正则化，有效平衡了鲁棒性与泛化性，为噪声监督下的LLM后训练提供了有前景的解决方案。

Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.

</details>


### [33] [Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA](https://arxiv.org/abs/2512.03805)
*Tai Nguyen,Phong Le,André Biedenkapp,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TL;DR: 本文系统研究了深度强化学习在动态算法配置中的应用，针对(1+(λ,λ))-GA算法的种群规模参数控制问题，揭示了DDQN和PPO存在的可扩展性退化和学习不稳定性两大挑战，并提出自适应奖励偏移机制等解决方案。


<details>
  <summary>Details</summary>
Motivation: 强化学习在动态算法配置中具有潜力，但实际应用面临挑战且需要大量领域专业知识。本文旨在通过系统分析深度强化学习算法在具体算法配置问题上的表现，揭示其根本挑战并提供解决方案。

Method: 以控制(1+(λ,λ))-GA算法在OneMax问题上的种群规模参数为案例，系统研究DDQN和PPO两种深度强化学习算法。针对发现的挑战，提出自适应奖励偏移机制解决探索不足问题，使用无折扣学习解决规划视野覆盖问题，并分析PPO的超参数依赖性。

Result: 研究发现DDQN和PPO存在可扩展性退化和学习不稳定性两大挑战，主要源于探索不足和规划视野覆盖问题。提出的自适应奖励偏移机制能有效增强DDQN探索能力，无折扣学习解决规划视野问题。DDQN结合自适应奖励偏移策略能达到与理论推导策略相当的性能，且样本效率显著提升，优于先前DAC方法数个数量级。

Conclusion: 深度强化学习在动态算法配置中面临系统性挑战，但通过针对性解决方案可以克服。DDQN结合自适应奖励偏移机制是有效的DAC方法，而PPO存在根本性方差问题需要重新设计。研究为DAC领域的强化学习应用提供了重要见解和实用解决方案。

Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

</details>
