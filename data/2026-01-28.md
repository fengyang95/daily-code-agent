<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.AI](#cs.AI) [Total: 14]
- [tldr.article](#tldr.article) [Total: 9]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.LG](#cs.LG) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2601.19290)
*Yimeng Wang,Jiaxing Zhao,Hongbin Xie,Hexing Ma,Yuzhen Lei,Shuangxue Liu,Xuan Song,Zichen Zhang,Haoran Zhang*

Main category: cs.CL

TL;DR: MetaGen是一个无需训练的多智能体框架，能够在推理时动态调整角色空间和协作拓扑结构，无需更新基础模型权重，从而提升复杂任务的准确性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多智能体系统通常依赖固定的角色库和冻结的执行拓扑结构，这种刚性设计导致任务不匹配、无法及时适应推理过程中出现的新证据，并且增加了推理成本。

Method: MetaGen通过生成和重写查询条件化的角色规范来维护可控的动态角色池，然后围绕最小骨干网络实例化约束执行图。在执行过程中，使用轻量级反馈信号迭代更新角色提示并调整结构决策。

Result: 在代码生成和多步推理基准测试中，MetaGen相比强大的多智能体基线方法，在准确性和成本权衡方面表现更优。

Conclusion: MetaGen展示了通过动态调整角色空间和协作拓扑结构，无需训练即可显著提升多智能体系统性能的可行性，为更灵活、高效的多智能体协作提供了新思路。

Abstract: Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.

</details>


### [2] [Yunque DeepResearch Technical Report](https://arxiv.org/abs/2601.19578)
*Yuxuan Cai,Xinyi Lai,Peng Yuan,Weiting Liu,Huajian Li,Mingda Li,Xinghua Wang,Shengxie Zheng,Yanchao Hao,Yuyang Yin,Zheng Wei*

Main category: cs.CL

TL;DR: Yunque DeepResearch是一个分层、模块化、鲁棒的深度研究框架，通过多智能体编排、动态上下文管理和主动监督模块解决现有深度研究中的上下文噪声、脆弱性和扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体的深度研究能力面临三个关键限制：长时任务中上下文噪声不断升级、脆弱性导致级联错误、缺乏模块化扩展性，阻碍了深度研究的全部潜力发挥。

Method: 提出Yunque DeepResearch框架，包含三个核心组件：1) 集中式多智能体编排系统，将子任务路由到原子能力池；2) 动态上下文管理机制，将完成的子目标结构化语义摘要；3) 主动监督模块，通过异常检测和上下文剪枝确保鲁棒性。

Result: 在多个智能体深度研究基准测试中达到最先进性能，包括GAIA、BrowseComp、BrowseComp-ZH和Humanity's Last Exam。

Conclusion: Yunque DeepResearch通过分层、模块化和鲁棒的设计有效解决了深度研究中的关键挑战，框架已开源以赋能社区。

Abstract: Deep research has emerged as a transformative capability for autonomous agents, empowering Large Language Models to navigate complex, open-ended tasks. However, realizing its full potential is hindered by critical limitations, including escalating contextual noise in long-horizon tasks, fragility leading to cascading errors, and a lack of modular extensibility. To address these challenges, we introduce Yunque DeepResearch, a hierarchical, modular, and robust framework. The architecture is characterized by three key components: (1) a centralized Multi-Agent Orchestration System that routes subtasks to an Atomic Capability Pool of tools and specialized sub-agents; (2) a Dynamic Context Management mechanism that structures completed sub-goals into semantic summaries to mitigate information overload; and (3) a proactive Supervisor Module that ensures resilience through active anomaly detection and context pruning. Yunque DeepResearch achieves state-of-the-art performance across a range of agentic deep research benchmarks, including GAIA, BrowseComp, BrowseComp-ZH, and Humanity's Last Exam. We open-source the framework, reproducible implementations, and application cases to empower the community.

</details>


### [3] [Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis](https://arxiv.org/abs/2601.19773)
*Zhuohan Long,Zhijie Bao,Zhongyu Wei*

Main category: cs.CL

TL;DR: 提出了一个交互式医疗咨询评估框架EviMed，通过模拟患者和报告员来建模咨询过程，引入信息覆盖率(ICR)量化证据收集完整性，发现诊断推理能力强的模型不一定能有效收集信息，并提出了REFINE策略来引导主动解决不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗咨询评估大多是静态或结果导向的，忽视了证据收集过程。交互式医疗咨询需要智能体在不确定性下主动获取缺失的临床证据，但缺乏对证据收集过程的系统性评估。

Method: 1) 构建基于证据的基准EviMed，涵盖从常见症状到罕见疾病的多种情况；2) 提出交互式评估框架，使用模拟患者和模拟报告员来建模咨询过程；3) 引入信息覆盖率(ICR)量化证据收集完整性；4) 提出REFINE策略，利用诊断验证引导智能体主动解决不确定性。

Result: 评估了10个具有不同推理能力的模型，发现强大的诊断推理能力并不能保证有效的信息收集，这种不足是限制交互式设置性能的主要瓶颈。REFINE策略在多个数据集上持续优于基线，并促进了有效的模型协作，使较小的智能体在强推理监督下获得更好的性能。

Conclusion: 交互式医疗咨询需要专门的证据收集能力，而不仅仅是诊断推理能力。REFINE策略通过诊断验证引导主动解决不确定性，显著提升了交互式咨询的性能，并为模型协作提供了有效框架。

Abstract: Interactive medical consultation requires an agent to proactively elicit missing clinical evidence under uncertainty. Yet existing evaluations largely remain static or outcome-centric, neglecting the evidence-gathering process. In this work, we propose an interactive evaluation framework that explicitly models the consultation process using a simulated patient and a \rev{simulated reporter} grounded in atomic evidences. Based on this representation, we introduce Information Coverage Rate (ICR) to quantify how completely an agent uncovers necessary evidence during interaction. To support systematic study, we build EviMed, an evidence-based benchmark spanning diverse conditions from common complaints to rare diseases, and evaluate 10 models with varying reasoning abilities. We find that strong diagnostic reasoning does not guarantee effective information collection, and this insufficiency acts as a primary bottleneck limiting performance in interactive settings. To address this, we propose REFINE, a strategy that leverages diagnostic verification to guide the agent in proactively resolving uncertainties. Extensive experiments demonstrate that REFINE consistently outperforms baselines across diverse datasets and facilitates effective model collaboration, enabling smaller agents to achieve superior performance under strong reasoning supervision. Our code can be found at https://github.com/NanshineLoong/EID-Benchmark .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas](https://arxiv.org/abs/2601.19082)
*Trung-Kiet Huynh,Dao-Sy Duy-Minh,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Nguyen Lam Phu Quy,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Pham Phu Hoa,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.AI

TL;DR: 该研究通过重复囚徒困境实验，探究了LLM在不同收益幅度和语言上下文中的策略行为，发现LLM表现出激励敏感的决策模式，且语言框架对行为的影响有时超过模型架构差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在交互式和多智能体环境中越来越多地作为自主智能体运行，理解其战略行为对于安全性、协调性以及AI驱动的社会和经济系统至关重要。需要研究LLM在重复社会困境中的策略选择如何受到收益幅度和语言上下文的影响。

Method: 使用收益缩放的囚徒困境来分离对激励强度的敏感性，在不同模型和语言中进行实验。训练监督分类器识别经典重复博弈策略，并将其应用于LLM决策分析，以解释行为动态。

Result: 观察到跨模型和语言的一致行为模式，包括激励敏感的条件策略和跨语言差异。语言框架有时对行为的影响与架构效应相当甚至超过。LLM表现出系统性的、依赖于模型和语言的行为意图。

Conclusion: 研究为审计LLM作为战略智能体提供了统一框架，并揭示了合作偏见对AI治理和多智能体系统设计的直接意义。

Abstract: As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.

</details>


### [5] [Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach](https://arxiv.org/abs/2601.19122)
*Weiran Guo,Bing Bo,Shaoxiang Wu,Jingsheng Yang*

Main category: cs.AI

TL;DR: 提出一种基于强化学习的对抗性数据增强方法，通过训练查询模型生成针对性对抗查询来挑战函数调用模型，提升LLM函数调用能力的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM函数调用能力的方法依赖于人工标注或模型自动生成的数据进行微调，但这些方法缺乏针对性设计，受限于固定模式和数据分布，限制了函数调用LLM的泛化性和鲁棒性提升。

Method: 提出一种对抗性数据增强方法，使用强化学习训练查询模型生成专门挑战函数调用模型的对抗性查询。采用零和博弈框架，查询模型和函数调用模型进行迭代交替训练。

Result: 该方法能够系统性地识别和针对函数调用LLM的弱点，推动开发更鲁棒的函数调用模型，为识别和纠正LLM与外部工具交互能力的弱点提供了系统化方法。

Conclusion: 通过强化学习驱动的对抗性数据增强，能够有效提升LLM函数调用能力的鲁棒性和泛化性，克服现有方法的局限性。

Abstract: Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.

</details>


### [6] [TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning](https://arxiv.org/abs/2601.19151)
*Patara Trirat,Jin Myung Kwak,Jay Heo,Heejun Lee,Sung Ju Hwang*

Main category: cs.AI

TL;DR: TS-Debate是一个用于零样本时间序列推理的模态专业化协作多智能体辩论框架，通过专用专家智能体处理文本、视觉和数值信号，使用结构化辩论协议协调交互，在20个任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列分析中虽然展现出潜力，但存在数值保真度不足、模态干扰和跨模态集成不系统等问题，需要一种能保持模态保真度、减少数值幻觉的零样本推理方法。

Method: 提出TS-Debate框架：1) 分配专用专家智能体处理文本上下文、视觉模式和数值信号；2) 通过显式领域知识提取；3) 使用结构化辩论协议协调智能体交互；4) 评审智能体通过验证-冲突-校准机制评估主张，支持轻量级代码执行和数值查找进行程序化验证。

Result: 在三个公共基准测试的20个任务上，TS-Debate实现了持续且显著的性能提升，优于包括标准多模态辩论（所有智能体观察所有输入）在内的强基线方法。

Conclusion: TS-Debate框架通过模态专业化、结构化辩论和程序化验证，能够保持模态保真度、暴露冲突证据并减少数值幻觉，无需任务特定微调即可实现更好的时间序列推理性能。

Abstract: Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.

</details>


### [7] [LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge](https://arxiv.org/abs/2601.19155)
*Qiujun Li,Zijin Xiao,Xulin Wang,Zhidan Ma,Cheng Yang,Haifeng Li*

Main category: cs.AI

TL;DR: 提出LocationAgent，一个分层定位代理，通过RER架构（推理器-执行器-记录器）实现分层推理，将地理证据验证卸载到外部工具，显著提升零样本性能


<details>
  <summary>Details</summary>
Motivation: 现有图像地理定位方法通常通过监督训练或基于轨迹的强化微调将位置知识和推理模式内化为静态记忆，容易在开放世界或需要动态知识的场景中出现事实幻觉和泛化瓶颈

Method: 提出LocationAgent，核心思想是将分层推理逻辑保留在模型中，同时将地理证据验证卸载到外部工具。采用RER架构实现分层推理，通过角色分离和上下文压缩防止多步推理中的漂移问题。构建线索探索工具套件提供多样化证据支持位置推理。还引入CCL-Bench（中国城市定位基准）数据集

Result: 广泛实验表明，LocationAgent在零样本设置中显著优于现有方法至少30%

Conclusion: 通过将推理逻辑与证据验证分离，并利用外部工具进行地理事实验证，能够有效解决现有方法的幻觉问题和泛化瓶颈，在图像地理定位任务上取得显著改进

Abstract: Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.

</details>


### [8] [Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement](https://arxiv.org/abs/2601.19170)
*Wangyang Ying,Yanchi Liu,Xujiang Zhao,Wei Cheng,Zhengzhang Chen,Wenchao Yu,Yanjie Fu,Haifeng Chen*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，将程序图提取建模为多轮推理过程，通过结构化和逻辑反馈迭代优化，显著提升结构正确性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 从自然语言自动提取工作流作为程序图具有潜力但研究不足，需要同时保证结构有效性和逻辑对齐。现有大语言模型虽然显示出潜力，但经常产生结构不良或逻辑流误解的结果。

Method: 提出多智能体框架，包含三个阶段迭代：1) 图构建智能体提取程序图；2) 模拟智能体诊断结构缺陷并提供反馈；3) 语义智能体对齐流程逻辑与源文本语义。重要反馈以自然语言形式注入后续提示，实现可解释和可控的优化。

Result: 实验表明，该方法在结构正确性和逻辑一致性方面相比强基线有显著提升。

Conclusion: 该多智能体框架通过模块化设计和多轮推理，能够无监督地针对不同类型错误进行优化，实现了程序图提取的显著改进。

Abstract: Automatically extracting workflows as procedural graphs from natural language is promising yet underexplored, demanding both structural validity and logical alignment. While recent large language models (LLMs) show potential for procedural graph extraction, they often produce ill-formed structures or misinterpret logical flows. We present \model{}, a multi-agent framework that formulates procedural graph extraction as a multi-round reasoning process with dedicated structural and logical refinement. The framework iterates through three stages: (1) a graph extraction phase with the graph builder agent, (2) a structural feedback phase in which a simulation agent diagnoses and explains structural defects, and (3) a logical feedback phase in which a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into subsequent prompts, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.

</details>


### [9] [MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution](https://arxiv.org/abs/2601.19199)
*Libo Sun,Jiwen Zhang,Siyuan Wang,Zhongyu Wei*

Main category: cs.AI

TL;DR: MAGNET是一个记忆驱动的自适应GUI代理框架，通过双级记忆系统（静态记忆和程序记忆）来应对移动界面频繁更新带来的挑战，利用稳定的功能语义和任务意图实现鲁棒的动作定位和任务执行。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理虽然能自主执行任务，但界面频繁更新导致基于历史数据训练的代理失效。尽管界面外观变化，但功能语义和任务意图保持稳定，这为构建适应性强的代理提供了机会。

Method: 提出MAGNET框架，包含双级记忆系统：1) 静态记忆将多样视觉特征链接到稳定功能语义，实现鲁棒动作定位；2) 程序记忆捕捉不同工作流中的稳定任务意图。采用动态记忆进化机制，通过优先访问频繁使用的知识持续优化两种记忆。

Result: 在AndroidWorld在线基准测试中显著优于基线方法，离线基准测试在分布偏移下也显示出一致的性能提升。验证了利用界面变化中的稳定结构能提高代理性能和泛化能力。

Conclusion: 通过利用移动界面变化中稳定的功能语义和任务意图，MAGNET框架能有效应对GUI频繁更新带来的挑战，在演化软件环境中实现更好的性能和泛化能力。

Abstract: Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments.

</details>


### [10] [MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning](https://arxiv.org/abs/2601.19204)
*Zhixi Cai,Fucai Ke,Kevin Leo,Sukai Huang,Maria Garcia de la Banda,Peter J. Stuckey,Hamid Rezatofighi*

Main category: cs.AI

TL;DR: MATA是一个用于视觉推理的多智能体分层可训练自动机系统，通过可训练的超智能体选择顶层状态转移，每个智能体运行基于规则的子自动机，实现透明执行历史，在多个视觉推理基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型虽然感知能力强，但隐含推理难以解释，在复杂查询上容易产生幻觉。组合方法提高了可解释性，但大多依赖单一智能体或手工设计的流程，无法决定何时在互补智能体之间协作或在重叠智能体之间竞争。

Method: 引入MATA（多智能体分层可训练自动机），这是一个作为分层有限状态自动机的多智能体系统，顶层转移由可训练的超智能体选择。每个智能体对应超自动机中的一个状态，运行小型基于规则的子自动机进行可靠的微控制。所有智能体读写共享内存，产生透明的执行历史。为了监督超智能体的转移策略，构建转移轨迹树并转换为内存到下一个状态对，形成MATA-SFT-90K数据集用于监督微调。

Result: 在多个视觉推理基准测试中，MATA相比单体模型和组合基线方法取得了最先进的结果。

Conclusion: MATA通过分层多智能体架构和可训练的转移策略，实现了高效、可解释的视觉推理，能够根据查询和智能体能力选择最优智能体解决任务。

Abstract: Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.

</details>


### [11] [GLOVE: Global Verifier for LLM Memory-Environment Realignment](https://arxiv.org/abs/2601.19249)
*Xingkun Yin,Hongyang Du*

Main category: cs.AI

TL;DR: GLOVE框架为LLM记忆系统引入相对真理概念，通过主动探测记忆与观察的不一致性实现记忆-环境对齐，无需真实监督或强模型内省，在动态漂移环境中显著提升智能体成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆增强方法假设记忆有效性可通过外部评估者或模型内省建立，但这些假设在动态漂移的实际环境中经常失效。需要一种能在非平稳环境中验证和更新记忆的方法。

Method: 提出GLOVE框架，引入相对真理概念，通过主动探测检索记忆与新鲜观察之间的不一致性，实现记忆-环境对齐。无需真实监督或强模型内省，可验证和更新记忆。

Result: 在包含网络导航、规划和控制的多样化基准测试中，通过添加受控环境漂移引入非平稳性，GLOVE显著提高了智能体成功率。

Conclusion: GLOVE为LLM记忆系统提供了新设计维度，通过相对真理概念和主动不一致性探测，为构建能够自我进化的认知智能体提供了稳健路径。

Abstract: Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.

</details>


### [12] [Curiosity Driven Knowledge Retrieval for Mobile Agents](https://arxiv.org/abs/2601.19306)
*Sijia Li,Xiaoyu Tan,Shahir Ali,Niels Schmidt,Gengchen Ma,Xihe Qiu*

Main category: cs.AI

TL;DR: 提出好奇心驱动的知识检索框架，通过AppCards编码应用功能语义和交互模式，增强移动代理在复杂智能手机自动化任务中的性能


<details>
  <summary>Details</summary>
Motivation: 当前移动代理在复杂应用中的性能受限于不完整知识和对未见环境的泛化能力弱，需要解决执行过程中的不确定性

Method: 使用好奇心分数形式化执行不确定性，当超过阈值时从文档、代码库和历史轨迹检索外部信息，组织成结构化AppCards（编码功能语义、参数约定、接口映射和交互模式），增强代理在推理过程中选择性集成相关信息

Result: 在AndroidWorld基准测试中，所有骨干模型上均取得一致改进，平均提升6个百分点，与GPT-5结合时达到88.8%的最新SOTA成功率；AppCards对多步骤和跨应用任务特别有效

Conclusion: 好奇心驱动的知识检索框架通过AppCards显著提升移动代理性能，减少模糊性、缩短探索时间并支持稳定执行轨迹，改进效果依赖于骨干模型能力

Abstract: Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at https://lisalsj.github.io/Droidrun-appcard/.

</details>


### [13] [Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems](https://arxiv.org/abs/2601.19311)
*Anh Khoa Ngo Ho,Martin Chauvin,Simon Gosset,Philippe Cordier,Boris Gamazaychikov*

Main category: cs.AI

TL;DR: 研究发现小型开源语言模型在保持任务质量的同时能显著降低能耗，为可持续AI系统设计提供实用指南


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型成为智能代理系统的核心组件，其推理过程的能耗可能带来显著的可持续性挑战。研究旨在探索部署较小规模语言模型是否能在多智能体真实环境中降低能耗，同时不影响响应性和输出质量。

Method: 对不同规模的语言模型进行对比分析，量化效率与性能之间的权衡关系。基于研究发现，提出可持续AI设计的实用指南，包括最优批处理大小配置和计算资源分配策略。

Result: 结果显示，较小的开源权重模型能够降低能源使用，同时保持任务质量。这为开发可扩展、环境友好的AI系统提供了可行的技术路径。

Conclusion: 研究表明通过合理选择模型规模和优化配置，可以在不牺牲性能的前提下实现AI系统的可持续发展，为构建环境友好的智能代理系统提供了具体策略。

Abstract: As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.

</details>


### [14] [RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization](https://arxiv.org/abs/2601.19404)
*Hongzhu Yi,Xinming Wang,Zhenghao zhang,Tianyu Zong,Yuanxiang Wang,Jun Xie,Tao Yu,Haopeng Jin,Zhepeng Wang,Kaixin Xu,Feng Chen,Jiahuan Chen,Yujia Yang,Zhenyu Guan,Bingkang Shi,Jungang Xu*

Main category: cs.AI

TL;DR: RPO是一种部分推理优化的强化微调算法，通过仅生成推理路径的后缀来减少约95%的训练阶段token生成，显著降低计算开销，同时保持与完整路径方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM强化微调算法需要从输入查询开始生成完整的推理轨迹，这在训练rollout阶段产生巨大的计算开销。作者旨在解决这一效率问题。

Method: 分析推理路径不同部分对最终结果正确性的影响，提出RPO算法。不同于生成完整推理路径，RPO使用经验缓存生成推理路径的后缀进行训练，大幅减少rollout阶段的token生成。

Result: RPO在训练rollout阶段减少约95%的token生成，将1.5B模型的训练时间降低90%，7B模型降低72%。可与GRPO、DAPO等典型算法集成，在保持性能的同时实现训练加速。

Conclusion: RPO是一种高效的plug-and-play强化微调算法，通过部分推理优化显著降低计算开销，同时保持模型性能，为LLM强化微调提供了实用的加速方案。

Abstract: Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.

</details>


### [15] [Learning Adaptive Parallel Execution for Efficient Code Localization](https://arxiv.org/abs/2601.19568)
*Ke Xu,Siyang Xiao,Ming Liang,Yichen Yu,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: FuseSearch通过联合质量-效率优化，动态调整并行搜索广度，显著减少冗余调用，在代码定位任务上实现SOTA性能并大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前代码定位代理存在34.9%的冗余调用率，抵消了并行执行的优势，需要更高效的并行策略来优化质量和效率。

Method: 提出FuseSearch框架，将并行代码定位重新定义为联合质量-效率优化任务，定义工具效率指标，采用两阶段SFT和RL训练学习自适应并行策略，动态调整搜索广度。

Result: 在SWE-bench Verified上，FuseSearch-4B达到SOTA性能（文件级F1 84.7%，函数级F1 56.4%），加速93.6%，减少67.7%的轮次和68.9%的token使用。

Conclusion: 效率感知训练通过消除冗余噪声信号自然提升质量，能够实现高性能且成本效益高的代码定位代理。

Abstract: Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9\% redundant invocation rate, which negates parallelism benefits. We propose \textbf{FuseSearch}, reformulating parallel code localization as a \textbf{joint quality-efficiency optimization} task. Through defining \textbf{tool efficiency} -- the ratio of unique information gain to invocation count -- we utilize a two-phase SFT and RL training approach for learning adaptive parallel strategies. Different from fixed-breadth approaches, FuseSearch dynamically modulates search breadth according to task context, evolving from exploration phases to refinement stages. Evaluated on SWE-bench Verified, FuseSearch-4B achieves SOTA-level performance (84.7\% file-level and 56.4\% function-level $F_1$ scores) with 93.6\% speedup, utilizing 67.7\% fewer turns and 68.9\% fewer tokens. Results indicate that efficiency-aware training naturally improves quality through eliminating noisy redundant signals, enabling high-performance cost-effective localization agents.

</details>


### [16] [ComAgent: Multi-LLM based Agentic AI Empowered Intelligent Wireless Networks](https://arxiv.org/abs/2601.19607)
*Haoyun Li,Ming Xiao,Kezhi Wang,Robert Schober,Dong In Kim,Yong Liang Guan*

Main category: cs.AI

TL;DR: ComAgent是一个多LLM智能体框架，通过感知-规划-执行-反思闭环，协调专业智能体自动生成6G网络优化问题的求解器就绪公式和可复现仿真。


<details>
  <summary>Details</summary>
Motivation: 6G网络依赖复杂的跨层优化，但将高层意图手动转化为数学公式存在瓶颈。现有LLM方法缺乏足够的领域基础、约束意识和验证能力。

Method: 采用多LLM智能体框架，通过感知-规划-执行-反思闭环，协调文献搜索、编码和评分等专业智能体，迭代分解问题并自我纠正错误。

Result: ComAgent在复杂波束成形优化中达到专家可比性能，在多样化无线任务中超越单体LLM，展示了在无线网络自动化设计中的潜力。

Conclusion: ComAgent框架有效弥合了用户意图与执行之间的差距，为新兴无线网络的自动化设计提供了有前景的解决方案。

Abstract: Emerging 6G networks rely on complex cross-layer optimization, yet manually translating high-level intents into mathematical formulations remains a bottleneck. While Large Language Models (LLMs) offer promise, monolithic approaches often lack sufficient domain grounding, constraint awareness, and verification capabilities. To address this, we present ComAgent, a multi-LLM agentic AI framework. ComAgent employs a closed-loop Perception-Planning-Action-Reflection cycle, coordinating specialized agents for literature search, coding, and scoring to autonomously generate solver-ready formulations and reproducible simulations. By iteratively decomposing problems and self-correcting errors, the framework effectively bridges the gap between user intent and execution. Evaluations demonstrate that ComAgent achieves expert-comparable performance in complex beamforming optimization and outperforms monolithic LLMs across diverse wireless tasks, highlighting its potential for automating design in emerging wireless networks.

</details>


### [17] [Agentic Design Patterns: A System-Theoretic Framework](https://arxiv.org/abs/2601.19752)
*Minh-Dung Dao,Quy Minh Le,Hoang Thanh Lam,Duc-Trong Le,Quoc-Viet Pham,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 提出基于系统理论的AI智能体工程化方法，包括五功能子系统框架和12种设计模式，用于构建更可靠、模块化的自主系统。


<details>
  <summary>Details</summary>
Motivation: 当前基于基础模型的智能体系统存在幻觉、推理能力差等问题，且设计多为临时性方案，缺乏系统理论基础。现有设计模式分类往往停留在高层次或便利性层面，难以实际实施。

Method: 提出系统理论框架，将智能体系统解构为五个核心交互子系统：推理与世界模型、感知与接地、动作执行、学习与适应、智能体间通信。基于此架构，映射出12种设计模式，分为基础型、认知与决策型、执行与交互型、适应与学习型四类。

Result: 通过对ReAct框架的案例研究，展示了所提设计模式如何纠正系统性架构缺陷。该框架为研究人员和工程师提供了标准化的设计语言和方法论。

Conclusion: 该工作为智能体设计提供了基础语言和结构化方法，能够促进更模块化、可理解和可靠的自主系统开发，标准化了智能体设计交流。

Abstract: With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [18] [Anthropic prepares to release Security Center for Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fanthropic-prepares-to-release-security-center-for-claude-code%2F%3Futm_source=tldrinfosec/1/0100019bfaa21f61-390f892b-d6b3-409b-b0b9-5f4587889129-000000/Rbb1A6z_fS1vXvFuh0G6EJ4Mk3Tk_JehfGMSLyQVPbw=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic即将推出Claude Code安全中心，提供安全扫描概览、问题检测和手动仓库扫描功能


<details>
  <summary>Details</summary>
Motivation: 为Claude Code用户提供更好的代码安全监控和管理工具，帮助开发者及时发现和解决代码安全问题

Method: 开发安全中心功能，包括安全扫描历史概览、问题检测系统和手动扫描触发机制

Result: 即将发布Claude Code安全中心，为用户提供全面的代码安全监控解决方案

Conclusion: Anthropic通过安全中心功能增强Claude Code的安全能力，提升开发者代码安全水平

Abstract: Anthropic prepares to release Security Center for Claude Code (2 minute read) Anthropic is preparing to launch Security Center for Claude Code, a feature that will provide users with an overview of recent security scans, detected issues, and the ability to manually initiate repository scans.

</details>


### [19] [Unrolling the Codex agent loop](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FwkSE7U/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/TbyTCGot1qWrVLt8zheWo56N2vbnXv3TYeyP7UBLqfk=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Codex CLI是一个跨平台本地软件代理，旨在安全高效地在用户机器上生成高质量、可靠的软件变更。本文是该系列的第一部分，重点解析代理循环的核心逻辑，即协调用户、模型和工具调用的交互机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在软件开发中的应用日益增多，理解代理如何安全、高效地在本地环境中工作变得至关重要。Codex CLI旨在解决在用户机器上执行软件变更时的安全性和可靠性问题，需要深入分析其核心工作机制。

Method: 通过分析Codex CLI的代理循环（agent loop）来揭示其工作原理。代理循环是协调用户、模型和工具调用的核心逻辑，包括如何接收用户输入、调用模型生成代码、执行工具操作以及处理反馈的完整流程。

Result: 本文提供了对Codex CLI代理循环的详细解析，展示了代理如何通过精心设计的交互机制来确保软件变更的质量和安全性。这种分析有助于理解现代AI代理在软件开发中的实际工作方式。

Conclusion: 代理循环是AI软件代理的核心组件，理解其工作机制对于开发安全、可靠的本地软件代理至关重要。Codex CLI的设计为在用户机器上执行软件变更提供了有效的框架，后续系列文章将进一步深入其他技术细节。

Abstract: Unrolling the Codex agent loop (17 minute read) Codex CLI is a cross-platform local software agent designed to produce high-quality, reliable software changes while operating safely and efficiently on users' machines. This is the first part in a series of posts that explores how Codex works. It focuses on the agent loop, which is the core logic responsible for orchestrating the interaction between the user, the model, and the tools the model invokes to perform work. The post provides a view i...

</details>


### [20] [Lessons from Building AI Agents for Financial Services](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nicolasbustamante.com%2Fp%2Flessons-from-building-ai-agents-for%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/g_KUHjI7N6VGcGwoHIyRpKk8iDEdy5by_nswhJgp9pg=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 构建金融服务的AI代理需要严格的技术基础设施和数据规范化，以避免代价高昂的错误。关键见解包括沙盒环境对安全多步工作流的重要性，以及通过markdown和元数据将原始金融数据转换为结构化、可搜索的上下文。


<details>
  <summary>Details</summary>
Motivation: 金融服务领域对AI代理的需求日益增长，但该领域对准确性和安全性要求极高，需要避免代价高昂的错误。现有的AI代理解决方案在金融服务的特殊需求（如数据规范化、安全工作流）方面存在不足。

Method: 采用沙盒环境确保安全的多步工作流；通过markdown和元数据将原始金融数据转换为结构化、可搜索的上下文；使用基于markdown而非代码的技能系统，实现动态、用户特定的指令执行。

Result: 成功构建了适用于金融服务的AI代理系统，能够安全处理复杂的金融工作流，有效转换和利用金融数据，提供用户特定的动态指令执行能力。

Conclusion: 构建金融服务AI代理需要严格的技术基础设施和数据规范化，沙盒环境和结构化数据转换是关键成功因素，基于markdown的技能系统提供了灵活的用户特定指令执行能力。

Abstract: Lessons from Building AI Agents for Financial Services (23 minute read) Building AI agents for financial services requires rigorous technical infrastructure and data normalization to avoid costly errors. Key insights include the essentiality of sandbox environments for secure multi-step workflows and the transformation of raw financial data into structured, searchable context through markdown and metadata. Skills systems, using markdown instead of code, allow dynamic, user-specific instructio...

</details>


### [21] [Diffusion-Based Code Modeling](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FByteDance-Seed%2FStable-DiffCoder%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/2dn3BFuNrS85df6uNBdMi5SPPs1ff_A4abSi7vxf6zE=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Stable-DiffCoder通过块扩散持续预训练提升代码LLM性能，在编辑、推理和低资源语言任务上超越自回归模型


<details>
  <summary>Details</summary>
Motivation: 当前自回归代码模型在编辑、推理和低资源语言任务上表现有限，需要更有效的代码建模方法

Method: 提出块扩散持续预训练方法，基于扩散模型进行代码建模，相比传统自回归方法

Result: 在多个编程基准测试中表现优于自回归模型，特别是在代码编辑、推理和低资源语言任务上

Conclusion: 扩散模型为代码建模提供了有前景的替代方案，在特定任务上优于传统自回归方法

Abstract: Diffusion-Based Code Modeling (GitHub Repo) Stable-DiffCoder introduces block diffusion continual pretraining for code LLMs, showing performance gains over autoregressive models across several programming benchmarks, especially for editing, reasoning, and low-resource languages.

</details>


### [22] [I Spent 40 Hours Researching Clawdbot. Here's Everything They're Not Telling You](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FlifxPL/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/tF2WjbL-I4dGp1Xe0c105Kjqa5fxDcUFIob_ORLE-hw=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Clawdbot是一款本地运行的自主AI代理，能够执行实际动作而不仅仅是生成文本，但高级自动化功能需要长时间设置


<details>
  <summary>Details</summary>
Motivation: 介绍Clawdbot作为本地运行的自主AI代理的能力，强调其能够执行实际动作而非仅文本生成，并揭示其功能设置的实际情况

Method: 基于40小时的研究分析，对比Clawdbot的开箱即用功能（文件管理、基础研究、文档处理）与需要长时间设置的高级自动化功能

Result: 发现Clawdbot确实能够本地运行并执行实际动作，但高级功能需要数小时甚至数天的设置时间，与宣传存在差距

Conclusion: Clawdbot作为自主AI代理具有潜力，但用户需要了解其实际设置要求，特别是高级功能需要投入大量时间配置

Abstract: I Spent 40 Hours Researching Clawdbot. Here's Everything They're Not Telling You (8 minute read) Clawdbot is an autonomous AI agent that runs locally and can execute real actions, not just generate text. Some features work immediately out of the box, like file management, basic research, and document processing, while advanced automations require hours or days of setup.

</details>


### [23] [Introducing Claude Chic](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmatthewrocklin.com%2Fintroducing-claude-chic%2F%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/Hp36zEBJ4RIgcGOvXyvmMBZYw0Yhi-69qit8at-moQY=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Chic是一个替代Claude Code的工具，通过可视化组织消息流、管理并发工作树、在同一窗口运行多个会话，并提供大量生活质量功能来提升开发体验。


<details>
  <summary>Details</summary>
Motivation: 现有代码助手工具在界面组织和多任务处理方面存在不足，开发者需要更直观、高效的工具来管理复杂的编码会话和工作流程。

Method: 开发了一个名为Claude Chic的替代工具，采用可视化消息流组织、并发工作树管理、多会话窗口集成等方法，并添加了大量用户体验优化功能。

Result: 创建了一个功能丰富的代码助手工具，显著提升了代码开发的可读性、组织性和工作效率。

Conclusion: Claude Chic通过改进界面设计和功能集成，为开发者提供了更优秀的代码助手体验，是现有工具的实用替代方案。

Abstract: Introducing Claude Chic (7 minute read) Claude Chic is an alternative to Claude Code that visually organizes the message stream for legibility, organizes concurrent work trees, runs many sessions from the same window, and contains loads of quality-of-life features.

</details>


### [24] [A few random notes from Claude coding quite a bit last few weeks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fxcancel.com%2Fkarpathy%2Fstatus%2F2015883857489522876%3Futm_source=tldrnewsletter/1/0100019bff32130e-c89eeee9-3f76-4011-a798-7308901a9fe8-000000/HTIbn7E1HylesF4s9Py5cPsyf4Qh6zIKkuafqW3u-vg=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI智能体能力在去年底达到临界点，引发软件工程领域范式转变，智能部分领先于集成，需要新的组织工作流程和流程，今年将是行业消化新能力的高能年份


<details>
  <summary>Details</summary>
Motivation: AI智能体能力在2023年底达到关键阈值，导致软件工程发生根本性变化。当前智能能力领先于集成能力，因此需要开发新的组织工作流程和流程来适应这一变革

Method: 本文基于作者过去几周对Claude编码的观察和笔记，分析AI智能体能力发展的临界点及其对软件工程的影响，强调需要新的组织工作流程

Result: AI智能体能力已跨越关键阈值，引发软件工程领域的范式转变，智能部分领先于集成，行业需要新的工作流程和组织流程来适应这一变化

Conclusion: 2024年将是行业消化AI智能体新能力的高能年份，需要开发新的组织工作流程和流程来充分利用智能能力领先于集成的现状

Abstract: A few random notes from Claude coding quite a bit last few weeks (7 minute read) AI agent capabilities crossed a threshold at the end of last year. This has caused a phase shift in software engineering. The intelligence part is a bit ahead of integration, so there is a need for new organizational workflows and processes. This year will be a high-energy year as the industry digests the new capability.

</details>


### [25] [Porting 100k lines from TypeScript to Rust using Claude Code in a month](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.vjeux.com%2F2026%2Fanalysis%2Fporting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/az-2LUXSdpFZKkjc3tEiotE9tEnoGR_IpOC2IqTsD-E=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用Claude Code在约一个月内将约10万行TypeScript代码库"Pokemon Showdown"移植到Rust，通过5000次提交完成，需要克服技术挑战和持续监控


<details>
  <summary>Details</summary>
Motivation: 探索使用AI代码助手（Claude Code）进行大规模代码移植的可行性，测试AI在复杂代码转换任务中的能力，特别是从动态类型语言（TypeScript）到静态类型语言（Rust）的转换

Method: 使用Claude Code作为主要工具，通过绕过Claude沙箱、自动化连续输入、管理大上下文窗口等技术手段，结合人工"保姆式"监控和迭代优化，完成代码转换

Result: 成功在约一个月内将约10万行代码从TypeScript移植到Rust，生成5000次提交，证明了AI辅助大规模代码移植的可行性，但需要大量人工监督和迭代

Conclusion: AI代码助手可以显著加速大规模代码移植过程，但当前仍需要大量人工干预和监督，技术挑战包括沙箱限制、上下文管理和自动化流程

Abstract: Porting 100k lines from TypeScript to Rust using Claude Code in a month (17 minute read) This dev ported the ~100k line "Pokemon Showdown" codebase from JavaScript to Rust in about a month using Claude Code. The project required overcoming technical and operational challenges, including bypassing Claude's sandbox, automating continuous input, and managing large context windows. It also needed "babysitting" and iterative refinement. The resulting Rust codebase was completed through 5,000 commits.

</details>


### [26] [There is an AI Code Review Bubble](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fai-code-review-bubble%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/JqzhM8XXIBXFyrF4C5MOtCuMrkPe2OHriCNatNWOmW4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码审查市场快速增长，Greptile通过独立性、自主性和反馈循环三大支柱构建长期愿景，强调代码生成与验证分离的代理架构，推动审查、测试和QA的完全自动化


<details>
  <summary>Details</summary>
Motivation: AI代码审查市场快速增长，但当前产品同质化严重，需要差异化竞争。Greptile旨在通过独特的长期愿景和架构设计，在竞争激烈的市场中脱颖而出

Method: 采用基于独立性、自主性和反馈循环三大支柱的架构设计，使用分离的代理分别负责代码生成和验证，推动审查、测试和QA流程的完全自动化

Result: Greptile通过这种差异化策略在AI代码审查市场中建立独特定位，强调长期愿景而非短期性能指标，为完全自动化的软件开发流程奠定基础

Conclusion: AI代码审查市场存在泡沫，需要超越标准性能指标的差异化策略。Greptile通过独立性、自主性和反馈循环的长期愿景，为完全自动化的软件开发提供新路径

Abstract: There is an AI Code Review Bubble (5 minute read) The AI code review market is experiencing rapid growth, so companies like Greptile are starting to differentiate their products beyond standard performance metrics. Greptile distinguishes itself by its long-term vision for AI code review, built on the pillars of independence, autonomy, and feedback loops. This approach emphasizes separate agents for code generation and validation, pushing for full automation of review, testing, and QA with min...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [27] [Automated structural testing of LLM-based agents: methods, framework, and case studies](https://arxiv.org/abs/2601.18827)
*Jens Kohl,Otto Kruse,Youssef Mostafa,Andre Luckow,Karsten Schroer,Thomas Riedl,Ryan French,David Katz,Manuel P. Luitz,Tanrajbir Takher,Ken E. Friedl,Céline Laurent-Winter*

Main category: cs.SE

TL;DR: 提出基于结构测试的LLM智能体测试方法，使用追踪、模拟和断言实现自动化测试，降低测试成本并提高质量


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体测试方法主要关注用户视角的验收测试，需要人工评估、难以自动化、不支持根因分析且测试环境昂贵，需要更有效的测试方法

Method: 使用OpenTelemetry追踪捕获智能体轨迹，通过模拟确保可重现的LLM行为，添加断言实现自动化测试验证，支持组件级和交互级测试

Result: 在案例研究中展示了自动化执行和更快的根因分析，降低了测试成本，通过更高覆盖率、可重用性和早期缺陷检测提高了智能体质量

Conclusion: 结构测试方法使软件工程最佳实践（测试自动化金字塔、回归测试、测试驱动开发、多语言测试）能够应用于智能体，提供了开源实现

Abstract: LLM-based agents are rapidly being adopted across diverse domains. Since they interact with users without supervision, they must be tested extensively. Current testing approaches focus on acceptance-level evaluation from the user's perspective. While intuitive, these tests require manual evaluation, are difficult to automate, do not facilitate root cause analysis, and incur expensive test environments. In this paper, we present methods to enable structural testing of LLM-based agents. Our approach utilizes traces (based on OpenTelemetry) to capture agent trajectories, employs mocking to enforce reproducible LLM behavior, and adds assertions to automate test verification. This enables testing agent components and interactions at a deeper technical level within automated workflows. We demonstrate how structural testing enables the adaptation of software engineering best practices to agents, including the test automation pyramid, regression testing, test-driven development, and multi-language testing. In representative case studies, we demonstrate automated execution and faster root-cause analysis. Collectively, these methods reduce testing costs and improve agent quality through higher coverage, reusability, and earlier defect detection. We provide an open source reference implementation on GitHub.

</details>


### [28] [MulVul: Retrieval-augmented Multi-Agent Code Vulnerability Detection via Cross-Model Prompt Evolution](https://arxiv.org/abs/2601.18847)
*Zihan Wu,Jie Xu,Yun Peng,Chun Yong Chong,Xiaohua Jia*

Main category: cs.SE

TL;DR: MulVul是一个检索增强的多智能体框架，采用粗到细策略进行漏洞检测，通过跨模型提示演化自动生成专用提示，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在真实世界漏洞检测中存在两个关键限制：漏洞模式的异质性削弱了单一统一模型的有效性，而为大量弱点类别手动设计提示是不可扩展的。

Method: 提出MulVul框架：1) Router智能体预测top-k粗粒度类别；2) 专用Detector智能体识别确切漏洞类型；3) 两者都配备检索工具从知识库获取证据；4) 设计跨模型提示演化机制，生成器LLM迭代优化提示，执行器LLM验证有效性。

Result: 在130个CWE类型上评估，MulVul达到34.79%的Macro-F1，比最佳基线提升41.5%。消融研究验证跨模型提示演化比手动提示提升51.6%性能。

Conclusion: MulVul通过多智能体框架和自动提示优化，有效解决了LLM在漏洞检测中的异质性和可扩展性问题，显著提升了检测性能。

Abstract: Large Language Models (LLMs) struggle to automate real-world vulnerability detection due to two key limitations: the heterogeneity of vulnerability patterns undermines the effectiveness of a single unified model, and manual prompt engineering for massive weakness categories is unscalable.
  To address these challenges, we propose \textbf{MulVul}, a retrieval-augmented multi-agent framework designed for precise and broad-coverage vulnerability detection. MulVul adopts a coarse-to-fine strategy: a \emph{Router} agent first predicts the top-$k$ coarse categories and then forwards the input to specialized \emph{Detector} agents, which identify the exact vulnerability types. Both agents are equipped with retrieval tools to actively source evidence from vulnerability knowledge bases to mitigate hallucinations.
  Crucially, to automate the generation of specialized prompts, we design \emph{Cross-Model Prompt Evolution}, a prompt optimization mechanism where a generator LLM iteratively refines candidate prompts while a distinct executor LLM validates their effectiveness. This decoupling mitigates the self-correction bias inherent in single-model optimization.
  Evaluated on 130 CWE types, MulVul achieves 34.79\% Macro-F1, outperforming the best baseline by 41.5\%. Ablation studies validate cross-model prompt evolution, which boosts performance by 51.6\% over manual prompts by effectively handling diverse vulnerability patterns.

</details>


### [29] [Tricky$^2$: Towards a Benchmark for Evaluating Human and LLM Error Interactions](https://arxiv.org/abs/2601.18949)
*Cole Granger,Dipin Khati,Daniel Rodriguez-Cardenas,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 构建Tricky²混合数据集，结合人类编写缺陷和LLM注入错误，用于分析人类与LLM错误交互、多错误修复鲁棒性及混合人机代码可靠性


<details>
  <summary>Details</summary>
Motivation: LLM在软件开发中日益普及，但常引入与人类错误不同的逻辑或数据误用错误，需要研究这两种错误类型的交互

Method: 基于现有TrickyBugs人类缺陷数据集，使用分类指导提示框架注入GPT-5和OpenAI-oss-20b生成的错误，保留原始人类缺陷和程序结构，构建包含人类专用、LLM专用和人类+LLM混合分区的数据集

Result: 创建了Tricky²混合数据集，支持混合来源错误行为分析、多错误修复鲁棒性评估和混合人机代码可靠性研究

Conclusion: 该数据集为研究人类与LLM错误交互提供了基础，并通过小规模基线评估展示了其在分类、定位和修复任务中的应用价值

Abstract: Large language models (LLMs) are increasingly integrated into software development workflows, yet they often introduce subtle logic or data-misuse errors that differ from human bugs. To study how these two error types interact, we construct Tricky$^2$, a hybrid dataset that augments the existing TrickyBugs corpus of human-written defects with errors injected by both GPT-5 and OpenAI-oss-20b across C++, Python, and Java programs. Our approach uses a taxonomy-guided prompting framework to generate machine-originated bugs while preserving original human defects and program structure. The resulting corpus spans human-only, LLM-only, and human+LLM splits, enabling analysis of mixed-origin error behavior, multi-bug repair robustness, and reliability in hybrid human-machine code. This paper outlines the dataset construction pipeline and illustrates its use through small-scale baseline evaluations of classification, localization, and repair tasks.

</details>


### [30] [Dynamic Cogeneration of Bug Reproduction Test in Agentic Program Repair](https://arxiv.org/abs/2601.19066)
*Runxiang Cheng,Michele Tufano,José Cambronero,Renyao Wei,Sherry Shi,Grant Uy,Pat Rondon,Franjo Ivančić*

Main category: cs.SE

TL;DR: 该论文研究在自动程序修复（APR）中同时生成修复和错误重现测试（BRT）的协同生成策略，评估不同策略在120个Google真实bug上的效果，开发考虑测试变更信息的补丁选择器，并分析失败原因。


<details>
  <summary>Details</summary>
Motivation: 实际开发中，开发者提交补丁时通常会同时实现BRT测试。部署APR系统时发现开发者同样希望AI生成的补丁包含BRT以增加信心，但传统APR系统往往分开生成BRT和修复，或只关注最终修复。

Method: 研究APR代理在协同生成上下文中的表现，评估不同协同生成策略的效果，开发考虑测试变更信息的补丁选择器来筛选合理的修复和BRT，分析失败轨迹的根本原因。

Result: 协同生成使APR代理能够为至少与专用BRT代理相同数量的bug生成BRT，同时不降低合理修复的生成率，从而减少维护和协调独立修复与BRT生成管道的工程工作量。

Conclusion: 协同生成策略在自动程序修复中是有效的，能够同时生成修复和BRT测试，提高开发者对AI生成补丁的信心，并减少工程维护成本。

Abstract: Bug Reproduction Tests (BRTs) have been used in many agentic Automated Program Repair (APR) systems, primarily for validating promising fixes and aiding fix generation. In practice, when developers submit a patch, they often implement the BRT alongside the fix. Our experience deploying agentic APR reveals that developers similarly desire a BRT within AI-generated patches to increase their confidence. However, canonical APR systems tend to generate BRTs and fixes separately, or focus on producing only the fix in the final patch. In this paper, we study agentic APR in the context of cogeneration, where the APR agent is instructed to generate both a fix and a BRT in the same patch. We evaluate the effectiveness of different cogeneration strategies on 120 human-reported bugs at Google and characterize different cogeneration strategies by their influence on APR agent behavior. We develop and evaluate patch selectors that account for test change information to select patches with plausible fixes (and plausible BRTs). Finally, we analyze the root causes of failed cogeneration trajectories. Importantly, we show that cogeneration allows the APR agent to generate BRTs for at least as many bugs as a dedicated BRT agent, without compromising the generation rate of plausible fixes, thereby reducing engineering effort in maintaining and coordinating separate generation pipelines for fix and BRT at scale.

</details>


### [31] [HalluJudge: A Reference-Free Hallucination Detection for Context Misalignment in Code Review Automation](https://arxiv.org/abs/2601.19072)
*Kla Tantithamthavorn,Hong Yi Lin,Patanamon Thongtanunam,Wachiraphan Charoenwet,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: HalluJudge是一个检测LLM生成代码审查评论中幻觉问题的系统，通过上下文对齐评估，在真实企业项目中达到0.85 F1分数，成本仅0.009美元，67%评估与开发者偏好一致。


<details>
  <summary>Details</summary>
Motivation: LLM在代码审查自动化中表现出色，但存在幻觉问题——生成的审查评论与实际代码无关，这阻碍了LLM在代码审查工作流中的采用。需要有效且可扩展的方法来检测LLM生成的代码审查评论中的幻觉，且无需参考标准答案。

Method: 设计HalluJudge系统，通过上下文对齐评估生成审查评论的grounding程度。包含四种关键策略：从直接评估到结构化多分支推理（如Tree-of-Thoughts）。在Atlassian的企业级软件项目中进行全面评估。

Result: HalluJudge的幻觉检测成本效益高，F1分数达0.85，平均成本仅0.009美元。平均67%的HalluJudge评估与在线生产中实际LLM生成审查评论的开发者偏好一致。

Conclusion: HalluJudge可作为实用安全措施，减少开发者接触幻觉评论，促进对AI辅助代码审查的信任。

Abstract: Large Language models (LLMs) have shown strong capabilities in code review automation, such as review comment generation, yet they suffer from hallucinations -- where the generated review comments are ungrounded in the actual code -- poses a significant challenge to the adoption of LLMs in code review workflows. To address this, we explore effective and scalable methods for a hallucination detection in LLM-generated code review comments without the reference. In this work, we design HalluJudge that aims to assess the grounding of generated review comments based on the context alignment. HalluJudge includes four key strategies ranging from direct assessment to structured multi-branch reasoning (e.g., Tree-of-Thoughts). We conduct a comprehensive evaluation of these assessment strategies across Atlassian's enterprise-scale software projects to examine the effectiveness and cost-efficiency of HalluJudge. Furthermore, we analyze the alignment between HalluJudge's judgment and developer preference of the actual LLM-generated code review comments in the real-world production. Our results show that the hallucination assessment in HalluJudge is cost-effective with an F1 score of 0.85 and an average cost of $0.009. On average, 67% of the HalluJudge assessments are aligned with the developer preference of the actual LLM-generated review comments in the online production. Our results suggest that HalluJudge can serve as a practical safeguard to reduce developers' exposure to hallucinated comments, fostering trust in AI-assisted code reviews.

</details>


### [32] [Reward Engineering for Reinforcement Learning in Software Tasks](https://arxiv.org/abs/2601.19100)
*Md Rayhanul Masud,Azmine Toushik Wasi,Salman Rahman,Md Rizwan Parvez*

Main category: cs.SE

TL;DR: 本文首次系统性地综述了软件工程任务中强化学习的奖励设计方法，整理了现有文献，提出了三维分类框架，并总结了挑战与建议。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和自主代理的发展，强化学习在代码相关任务中的应用日益增多。然而，软件工程任务中的奖励设计面临独特挑战：软件目标通常不是单一数值，而是需要代理指标（如编译通过、测试通过、质量指标）。现有研究分散在不同领域和论文中，缺乏系统性综述来整合这些方法并展示完整的奖励设计图景。

Method: 本文采用系统性文献综述方法，聚焦现有奖励设计方法和技术。通过三个互补维度对文献进行结构化分类，总结每个维度内的奖励设计选择。具体维度未在摘要中明确说明，但暗示了多维分类框架。

Result: 提供了首个针对软件工程任务中强化学习奖励设计的系统性和全面性综述，整理了分散在不同领域的研究成果，建立了三维分类框架来组织现有方法，为研究者提供了完整的奖励设计图景。

Conclusion: 总结了软件工程任务中强化学习奖励设计面临的挑战，并提出了该领域的设计建议。强调了系统性综述的重要性，为未来研究提供了基础框架和方向指引。

Abstract: Reinforcement learning is increasingly used for code-centric tasks. These tasks include code generation, summarization, understanding, repair, testing, and optimization. This trend is growing faster with large language models and autonomous agents. A key challenge is how to design reward signals that make sense for software. In many RL problems, the reward is a clear number. In software, this is often not possible. The goal is rarely a single numeric objective. Instead, rewards are usually proxies. Common proxies check if the code compiles, passes tests, or satisfies quality metrics. Many reward designs have been proposed for code-related tasks. However, the work is scattered across areas and papers. There is no single survey that brings these approaches together and shows the full landscape of reward design for RL in software. In this survey, we provide the first systematic and comprehensive review of reward engineering for RL in software tasks. We focus on existing methods and techniques. We structure the literature along three complementary dimensions, summarizing the reward-design choices within each. We conclude with challenges and recommendations in the reward design space for SE tasks.

</details>


### [33] [Detecting and Correcting Hallucinations in LLM-Generated Code via Deterministic AST Analysis](https://arxiv.org/abs/2601.19106)
*Dipin Khati,Daniel Rodriguez-Cardenas,Paul Pantzer,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 提出一个确定性静态分析框架，通过AST解析和动态知识库验证，可靠检测并自动修正代码生成中的知识冲突幻觉错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码常包含知识冲突幻觉（KCHs）——如不存在的API参数等语义错误，这些错误难以被传统linter检测且导致运行时失败。现有基于约束解码或非确定性LLM修复的方法对这些错误不可靠。

Method: 提出后处理框架：将生成代码解析为抽象语法树（AST），通过库内省动态构建知识库（KB），使用确定性规则验证AST与KB的一致性，检测并自动修正API和标识符级别的冲突。

Result: 在200个手动整理的Python代码片段数据集上，框架检测KCHs达到100%精确率和87.6%召回率（F1分数0.934），成功自动修正了77.0%的已识别幻觉。

Conclusion: 确定性后处理方法为概率性修复提供了可行且可靠的替代方案，为实现可信代码生成提供了明确路径。

Abstract: Large Language Models (LLMs) for code generation boost productivity but frequently introduce Knowledge Conflicting Hallucinations (KCHs), subtle, semantic errors, such as non-existent API parameters, that evade linters and cause runtime failures. Existing mitigations like constrained decoding or non-deterministic LLM-in-the-loop repair are often unreliable for these errors. This paper investigates whether a deterministic, static-analysis framework can reliably detect \textit{and} auto-correct KCHs. We propose a post-processing framework that parses generated code into an Abstract Syntax Tree (AST) and validates it against a dynamically-generated Knowledge Base (KB) built via library introspection. This non-executing approach uses deterministic rules to find and fix both API and identifier-level conflicts. On a manually-curated dataset of 200 Python snippets, our framework detected KCHs with 100\% precision and 87.6\% recall (0.934 F1-score), and successfully auto-corrected 77.0\% of all identified hallucinations. Our findings demonstrate that this deterministic post-processing approach is a viable and reliable alternative to probabilistic repair, offering a clear path toward trustworthy code generation.

</details>


### [34] ["ENERGY STAR" LLM-Enabled Software Engineering Tools](https://arxiv.org/abs/2601.19260)
*Himon Thakur,Armin Moin*

Main category: cs.SE

TL;DR: 该论文研究了AI增强软件工程工具（如IDE）的能源效率问题，提出了结合RAG和提示工程技术的框架，以提升LLM代码生成的能效，并在多种模型架构上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 随着AI无缝集成到软件工程工具中并默认启用，软件开发生命周期的能耗模式面临重大影响，需要研究这些AI增强系统的能源效率。

Method: 提出结合检索增强生成（RAG）和提示工程技术（PETs）的方法，建立综合框架测量实时能耗和推理时间，在125M到7B参数的多种LLM架构上进行实验。

Result: 在GPT-2、CodeLlama、Qwen 2.5、DeepSeek Coder等模型上验证了核心思想，为更深入分析提供了概念验证。

Conclusion: AI增强的软件工程工具能耗问题日益重要，提出的RAG+PETs方法能同时提升代码生成质量和能源效率，为未来更全面的分析奠定了基础。

Abstract: The discussion around AI-Engineering, that is, Software Engineering (SE) for AI-enabled Systems, cannot ignore a crucial class of software systems that are increasingly becoming AI-enhanced: Those used to enable or support the SE process, such as Computer-Aided SE (CASE) tools and Integrated Development Environments (IDEs). In this paper, we study the energy efficiency of these systems. As AI becomes seamlessly available in these tools and, in many cases, is active by default, we are entering a new era with significant implications for energy consumption patterns throughout the Software Development Lifecycle (SDLC). We focus on advanced Machine Learning (ML) capabilities provided by Large Language Models (LLMs). Our proposed approach combines Retrieval-Augmented Generation (RAG) with Prompt Engineering Techniques (PETs) to enhance both the quality and energy efficiency of LLM-based code generation. We present a comprehensive framework that measures real-time energy consumption and inference time across diverse model architectures ranging from 125M to 7B parameters, including GPT-2, CodeLlama, Qwen 2.5, and DeepSeek Coder. These LLMs, chosen for practical reasons, are sufficient to validate the core ideas and provide a proof of concept for more in-depth future analysis.

</details>


### [35] [Understanding Dominant Themes in Reviewing Agentic AI-authored Code](https://arxiv.org/abs/2601.19287)
*Md. Asif Haider,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 该研究对AI代理生成的代码进行大规模实证分析，通过主题建模和LLM辅助标注，发现AI代码审查主要关注文档、重构、格式等问题，而非功能正确性，表明AI加速代码生产但仍需人工审查监督。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注AI代理的代码生成能力，但缺乏对AI生成代码在实际审查中如何被评审的实证研究。本研究旨在填补这一空白，探究评审者对AI生成代码的实际反应。

Method: 使用AIDev数据集的精选子集，分析19,450条内联评审评论，涵盖3,177个AI生成的PR。采用主题建模结合LLM辅助语义聚类和整合，构建12个评审评论主题分类法。评估零样本提示LLM标注评审评论的可靠性。

Result: 开源LLM在评审评论级别达到78.63%的精确匹配、0.78的宏观F1分数，与人工标注者具有实质性一致。在PR级别，LLM以78%的Top-1准确率识别主导评审主题，平均Jaccard相似度为0.76。大规模应用发现，AI代码审查主要关注文档缺口、重构需求、样式格式问题，而非功能正确性和逻辑变更。

Conclusion: AI代理能加速代码生产，但在文档、重构、格式等方面仍存在缺口，需要针对性的人工审查监督。研究为AI代码审查实践提供了实证基础。

Abstract: While prior work has examined the generation capabilities of Agentic AI systems, little is known about how reviewers respond to AI-authored code in practice. In this paper, we present a large-scale empirical study of code review dynamics in agent-generated PRs. Using a curated subset of the AIDev dataset, we analyze 19,450 inline review comments spanning 3,177 agent-authored PRs from real-world GitHub repositories. We first derive a taxonomy of 12 review comment themes using topic modeling combined with large language model (LLM)-assisted semantic clustering and consolidation. According to this taxonomy, we then investigate whether zero-shot prompts to LLM can reliably annotate review comments. Our evaluation against human annotations shows that open-source LLM achieves reasonably high exact match (78.63%), macro F1 score (0.78), and substantial agreement with human annotators at the review comment level. At the PR level, the LLM also correctly identifies the dominant review theme with 78% Top-1 accuracy and achieves an average Jaccard similarity of 0.76, indicating strong alignment with human judgments. Applying this annotation pipeline at scale, we find that apart from functional correctness and logical changes, reviews of agent-authored PRs predominantly focus on documentation gaps, refactoring needs, styling and formatting issues, with testing and security-related concerns. These findings suggest that while AI agents can accelerate code production, there remain gaps requiring targeted human review oversight.

</details>


### [36] [AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context](https://arxiv.org/abs/2601.19494)
*Lei Zhang,Yongda Yu,Minghui Yu,Xinxin Guo,Zhengqi Zhuang,Guoping Rong,Dong Shao,Haifeng Shen,Hongyu Kuang,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobing Xu*

Main category: cs.SE

TL;DR: AACR-Bench是一个用于自动化代码审查评估的新基准，支持多语言和跨文件上下文，通过AI辅助专家验证的标注流程显著提高了缺陷覆盖率，揭示了现有评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码审查基准存在两个关键限制：1) 缺乏多语言支持，限制了评估结果的泛化能力；2) 依赖原始PR评论中的噪声和不完整真实数据，限制了问题检测范围。

Method: 提出AACR-Bench基准，提供多编程语言的完整跨文件上下文，采用"AI辅助、专家验证"的标注流程，发现原始PR中常被忽略的潜在缺陷。

Result: AACR-Bench将缺陷覆盖率提高了285%，对主流LLM的评估显示，由于数据限制，之前的评估可能误判或仅部分捕捉了模型能力。研究发现上下文粒度/级别和检索方法选择显著影响ACR性能。

Conclusion: 该工作为ACR评估建立了更严格的标准，提供了关于LLM在ACR中应用的新见解，包括上下文粒度、检索方法选择等关键因素对性能的影响。

Abstract: High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an "AI-assisted, Expert-verified" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285\% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .

</details>


### [37] [Toward Architecture-Aware Evaluation Metrics for LLM Agents](https://arxiv.org/abs/2601.19583)
*Débora Souza,Patrícia Machado*

Main category: cs.SE

TL;DR: 提出了一种轻量级、基于架构的LLM智能体评估方法，将智能体组件与可观察行为及评估指标关联起来，实现更有针对性、透明和可操作的评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估存在碎片化且过于模型中心化的问题，现有研究忽视了规划器、记忆、工具路由等架构组件对智能体行为的影响，限制了诊断能力。

Method: 提出轻量级、基于架构的方法，将智能体组件与其可观察行为以及能够评估这些行为的指标联系起来，明确测量内容和原因，并通过实际智能体应用进行说明。

Result: 该方法能够实现更有针对性、透明和可操作的LLM智能体评估，澄清了测量内容和原因，并通过实际应用展示了其有效性。

Conclusion: 基于架构的评估方法能够解决当前LLM智能体评估的局限性，提供更深入的诊断能力，促进更有效的智能体开发和优化。

Abstract: LLM-based agents are becoming central to software engineering tasks, yet evaluating them remains fragmented and largely model-centric. Existing studies overlook how architectural components, such as planners, memory, and tool routers, shape agent behavior, limiting diagnostic power. We propose a lightweight, architecture-informed approach that links agent components to their observable behaviors and to the metrics capable of evaluating them. Our method clarifies what to measure and why, and we illustrate its application through real world agents, enabling more targeted, transparent, and actionable evaluation of LLM-based agents.

</details>


### [38] [Using LLMs to Evaluate Architecture Documents: Results from a Digital Marketplace Environment](https://arxiv.org/abs/2601.19693)
*Frank Elberzhager,Matthias Gerbershagen,Joshua Ginkel*

Main category: cs.SE

TL;DR: 研究评估LLM在软件架构文档质量分析中的表现，发现文档质量越高，LLM评估与人类专家评估的一致性越好，但存在不一致性需要进一步分析。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程中应用日益广泛，但LLM的实际效益尚不明确。本研究关注软件架构师，探索LLM支持的架构文档评估如何帮助改进这些工件。

Method: 在开发数字市场平台的研究项目中，使用不同LLM分析架构文档质量，并将结果与软件架构师的评估进行比较。

Result: 发现工件质量对LLM评估质量有显著影响：架构文档质量越好，LLM评估与人类专家评估的一致性越高。虽然LLM在该架构任务中表现有前景，但结果显示出需要进一步分析的不一致性。

Conclusion: LLM在架构文档评估任务中具有潜力，但评估结果存在不一致性，需要进一步分析才能推广使用。

Abstract: Generative AI plays an increasing role during software engineering activities to make them, e.g., more efficient or provide better quality. However, it is often unclear how much benefit LLMs really provide. We concentrate on software architects and investigated how an LLM-supported evaluation of architecture documents can support software architects to improve such artefacts. In the context of a research project where a digital marketplace is developed and digital solutions should be analyzed, we used different LLMs to analyze the quality of architecture documents and compared the results with evaluations from software architects. We found out that the quality of the artifact has a strong influence on the quality of the LLM, i.e., the better the quality of the architecture document was, the more consistent were the LLM-based evaluation and the human expert evaluation. While using LLMs in this architecture task is promising, our results showed inconsistencies that need further analyses before generalizing them.

</details>


### [39] [AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion](https://arxiv.org/abs/2601.19697)
*Tianyue Jiang,Yanli Wang,Yanlin Wang,Daya Guo,Ensheng Shi,Yuchi Ma,Jiachi Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: AlignCoder是一个仓库级代码补全框架，通过查询增强机制和基于强化学习的检索器训练方法，解决了现有检索增强生成方法中的查询-目标代码不对齐问题，在CrossCodeEval基准上实现了18.1%的EM分数提升。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型在仓库级代码补全任务中表现有限，主要因为对仓库特定上下文和领域知识的理解不足。虽然检索增强生成方法通过检索相关代码片段作为跨文件上下文显示了一定潜力，但存在两个基本问题：检索过程中查询与目标代码的不对齐，以及现有检索方法无法有效利用推理信息。

Method: 提出了AlignCoder框架，包含两个核心组件：1) 查询增强机制，通过生成多个候选补全来构建增强查询，弥合初始查询与目标代码之间的语义差距；2) 基于强化学习的检索器训练方法，训练一个AlignRetriever，学习利用增强查询中的推理信息进行更准确的检索。

Result: 在CrossCodeEval和RepoEval两个广泛使用的基准上，对五个骨干代码LLM进行评估，在CrossCodeEval基准上相比基线实现了18.1%的EM分数提升。结果显示该框架实现了优越性能，并在不同代码LLM和编程语言上表现出高泛化能力。

Conclusion: AlignCoder通过查询增强和强化学习训练的检索器，有效解决了仓库级代码补全中的查询-目标代码不对齐问题，显著提升了代码补全性能，并具有良好的泛化能力。

Abstract: Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [The Geometric Reasoner: Manifold-Informed Latent Foresight Search for Long-Context Reasoning](https://arxiv.org/abs/2601.18832)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.LG

TL;DR: TGR是一个无需训练的推理框架，通过几何感知的潜在前瞻搜索来提升长链思维推理，在严格内存限制下实现更好的轨迹覆盖。


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算成本与覆盖质量之间存在权衡：要么训练成本高，要么产生冗余轨迹。需要一种无需训练的方法来提升长链推理的轨迹覆盖。

Method: TGR采用流形感知的潜在前瞻搜索框架，在分块边界处通过轻量级前瞻估计和软几何正则化评分候选潜在锚点，鼓励平滑轨迹和多样化探索，并通过分块KV缓存重置保持内存线性增长。

Result: 在数学和代码基准测试中，TGR将Qwen3-8B模型的Pass@k曲线下面积（AUC）提升了最多13个点，计算开销仅为1.1-1.3倍。

Conclusion: TGR提供了一种高效的无训练方法，在严格内存限制下显著提升了长链推理的轨迹覆盖质量。

Abstract: Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times.

</details>


### [41] [Representational Homomorphism Predicts and Improves Compositional Generalization In Transformer Language Model](https://arxiv.org/abs/2601.18858)
*Zhiyu An,Wan Du*

Main category: cs.LG

TL;DR: 提出同态误差（HE）作为衡量神经网络组合泛化能力的结构指标，通过控制实验验证HE能预测OOD组合泛化性能，并证明HE正则化训练能显著提升OOD准确率。


<details>
  <summary>Details</summary>
Motivation: 组合泛化（理解熟悉组件的新组合）是神经网络的持续挑战。行为评估只能揭示模型何时失败，但无法解释失败在表示层面的原因。需要一种结构指标来量化模型隐藏状态空间与表达式代数之间的同态偏差。

Method: 提出同态误差（HE）指标，量化表达式代数与模型隐藏状态空间之间的近似同态偏差。针对SCAN风格任务实例化两种HE：一元组合的修饰符HE和二元组合的序列HE，通过学习表示级操作符来预测组合表示。在小型仅解码器Transformer上进行控制实验，测试HE对噪声注入下OOD组合泛化的预测能力，并进行消融研究和HE正则化训练实验。

Result: HE能有效预测OOD组合泛化，修饰符HE与OOD准确率相关性R^2=0.73。模型深度对HE和OOD准确率影响最小；训练数据覆盖存在阈值效应（不足的覆盖会急剧增加HE并降低OOD性能）；随机插入的噪声token会系统性增加HE。HE正则化训练显著降低修饰符HE（p=1.1×10⁻⁴）和序列HE（p=0.001），并显著提升OOD准确率（p=0.023）。

Conclusion: 同态误差（HE）具有作为诊断指标和可操作训练信号的潜力，能改善神经网络的组合泛化能力。代码已开源。

Abstract: Compositional generalization-the ability to interpret novel combinations of familiar components-remains a persistent challenge for neural networks. Behavioral evaluations reveal when models fail but offer limited insight into why failures arise at the representational level. We introduce Homomorphism Error (HE), a structural metric that quantifies deviations from approximate homomorphisms between the expression algebra and a model's hidden-state space. We instantiate HE for two compositional operators in SCAN-style tasks: modifier HE for unary composition and sequence HE for binary composition, measured by learning representation-level operators that predict composed representations from their parts. Across controlled experiments with small decoder-only Transformers, HE predicts out-of-distribution (OOD) compositional generalization under noise injection, achieving R^2 = 0.73 correlation between modifier HE and OOD accuracy. Ablations show that model depth has minimal effect on either HE or OOD accuracy, training data coverage exhibits threshold effects (insufficient coverage sharply increases HE and degrades OOD performance), and randomly inserted noise tokens systematically increase HE. Finally, we test if HE-regularized training improves OOD accuracy. Experiment shows that explicitly enforcing low modifier HE during training significantly reduces modifier HE (p = 1.1x10-4) and sequence HE (p = 0.001) and yields a statistically significant improvement in OOD accuracy (p = 0.023). Together, these results indicate the potential of HE to be both a diagnostic and an actionable training signal for improving compositional generalization. Code to reproduce our experiments is open-sourced.

</details>


### [42] [Toward Learning POMDPs Beyond Full-Rank Actions and State Observability](https://arxiv.org/abs/2601.18930)
*Seiji Shaw,Travis Manderson,Chad Kessens,Nicholas Roy*

Main category: cs.LG

TL;DR: 该论文提出了一种从动作-观察序列学习部分可观察马尔可夫决策过程(POMDP)参数的方法，通过结合谱方法和张量分解，能够估计观测和转移概率矩阵，支持下游推理任务。


<details>
  <summary>Details</summary>
Motivation: 研究如何让自主智能体学习和推理具有隐藏状态的系统（如带有隐藏锁定机制的家具）。现有方法要么无法直接估计转移和观测概率（如PSR），要么假设完全状态可观测性和满秩转移矩阵。需要一种能放松这些假设并估计显式概率模型的方法。

Method: 将问题建模为学习离散POMDP参数。结合预测状态表示(PSR)和张量分解方法，学习观测矩阵和转移矩阵（达到状态划分级别）。特别处理那些转移矩阵满秩的动作对应的状态划分，这些划分内的状态具有相同的观测分布。

Result: 实验表明，该方法学习到的划分级别转移模型在足够数据量下，与PSR作为标准基于采样的POMDP求解器模型的性能相当。显式的观测和转移概率可以在模型学习后用于指定规划器行为。

Conclusion: 该方法成功地从动作-观察序列中学习POMDP参数，放松了对完全状态可观测性和满秩转移矩阵的假设，提供了可用于下游推理任务的显式概率模型。

Abstract: We are interested in enabling autonomous agents to learn and reason about systems with hidden states, such as furniture with hidden locking mechanisms. We cast this problem as learning the parameters of a discrete Partially Observable Markov Decision Process (POMDP). The agent begins with knowledge of the POMDP's actions and observation spaces, but not its state space, transitions, or observation models. These properties must be constructed from action-observation sequences. Spectral approaches to learning models of partially observable domains, such as learning Predictive State Representations (PSRs), are known to directly estimate the number of hidden states. These methods cannot, however, yield direct estimates of transition and observation likelihoods, which are important for many downstream reasoning tasks. Other approaches leverage tensor decompositions to estimate transition and observation likelihoods but often assume full state observability and full-rank transition matrices for all actions. To relax these assumptions, we study how PSRs learn transition and observation matrices up to a similarity transform, which may be estimated via tensor methods. Our method learns observation matrices and transition matrices up to a partition of states, where the states in a single partition have the same observation distributions corresponding to actions whose transition matrices are full-rank. Our experiments suggest that these partition-level transition models learned by our method, with a sufficient amount of data, meets the performance of PSRs as models to be used by standard sampling-based POMDP solvers. Furthermore, the explicit observation and transition likelihoods can be leveraged to specify planner behavior after the model has been learned.

</details>


### [43] [Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning](https://arxiv.org/abs/2601.18984)
*Haolin Liu,Dian Yu,Sidi Lu,Yujun Zhou,Rui Liu,Zhenwen Liang,Haitao Mi,Chen-Yu Wei,Dong Yu*

Main category: cs.LG

TL;DR: VPPO提出了一种新的强化学习方法，利用过程奖励模型仅定位推理路径中的第一个错误，将轨迹划分为已验证的正确前缀和错误后缀，从而提供更稳定的学习信号和更好的信用分配。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要依赖稀疏的结果奖励，无法对部分成功解决方案中的正确中间步骤给予适当信用。过程奖励模型虽然提供细粒度的步骤级监督，但其评分通常存在噪声且难以评估。当前的PRM基准测试侧重于检测推理路径中的第一个错误步骤，这与PRM在强化学习中通常作为原始奖励最大化的使用方式存在不一致。

Method: 提出可验证前缀策略优化（VPPO），该方法仅使用PRM来定位强化学习过程中的第一个错误。对于不正确的rollout，VPPO基于第一个错误将轨迹划分为已验证的正确前缀和错误后缀，对前者给予奖励，仅在被检测到的错误之后应用有针对性的惩罚。

Result: 在多个推理基准测试中，VPPO在Pass@1和Pass@K指标上始终优于稀疏奖励强化学习和先前的PRM引导基线方法。

Conclusion: VPPO通过更精确地使用PRM来定位第一个错误，提供了稳定、可解释的学习信号，改善了信用分配，从而在推理任务中取得了更好的性能。

Abstract: Reinforcement learning (RL) has emerged as a powerful framework for improving the reasoning capabilities of large language models (LLMs). However, most existing RL approaches rely on sparse outcome rewards, which fail to credit correct intermediate steps in partially successful solutions. Process reward models (PRMs) offer fine-grained step-level supervision, but their scores are often noisy and difficult to evaluate. As a result, recent PRM benchmarks focus on a more objective capability: detecting the first incorrect step in a reasoning path. However, this evaluation target is misaligned with how PRMs are typically used in RL, where their step-wise scores are treated as raw rewards to maximize. To bridge this gap, we propose Verifiable Prefix Policy Optimization (VPPO), which uses PRMs only to localize the first error during RL. Given an incorrect rollout, VPPO partitions the trajectory into a verified correct prefix and an erroneous suffix based on the first error, rewarding the former while applying targeted penalties only after the detected mistake. This design yields stable, interpretable learning signals and improves credit assignment. Across multiple reasoning benchmarks, VPPO consistently outperforms sparse-reward RL and prior PRM-guided baselines on both Pass@1 and Pass@K.

</details>


### [44] [A Unifying View of Coverage in Linear Off-Policy Evaluation](https://arxiv.org/abs/2601.19030)
*Philip Amortila,Audrey Huang,Akshay Krishnamurthy,Nan Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种新的线性离策略评估（OPE）有限样本分析，引入特征动态覆盖这一新覆盖参数，为线性OPE中的覆盖概念提供了统一理解。


<details>
  <summary>Details</summary>
Motivation: 线性离策略评估中，传统有限样本保证依赖于覆盖参数C^π，但在仅目标值函数线性可实现的简约设置下，现有覆盖定义存在缺陷且与文献中的标准定义脱节，需要统一的覆盖概念。

Method: 采用工具变量视角，对经典算法LSTDQ进行新颖的有限样本分析，提出特征动态覆盖参数，该参数可解释为特征演化诱导动态系统中的线性覆盖。

Result: 开发了依赖于新覆盖参数的错误边界，在贝尔曼完备性等进一步假设下，该定义成功恢复了特定设置下的覆盖参数，实现了线性OPE中覆盖概念的统一。

Conclusion: 特征动态覆盖为线性离策略评估提供了统一的覆盖概念，填补了简约设置下的理论空白，连接了不同假设下的覆盖定义。

Abstract: Off-policy evaluation (OPE) is a fundamental task in reinforcement learning (RL). In the classic setting of linear OPE, finite-sample guarantees often take the form $$ \textrm{Evaluation error} \le \textrm{poly}(C^π, d, 1/n,\log(1/δ)), $$ where $d$ is the dimension of the features and $C^π$ is a coverage parameter that characterizes the degree to which the visited features lie in the span of the data distribution. While such guarantees are well-understood for several popular algorithms under stronger assumptions (e.g. Bellman completeness), the understanding is lacking and fragmented in the minimal setting where only the target value function is linearly realizable in the features. Despite recent interest in tight characterizations of the statistical rate in this setting, the right notion of coverage remains unclear, and candidate definitions from prior analyses have undesirable properties and are starkly disconnected from more standard definitions in the literature.
  We provide a novel finite-sample analysis of a canonical algorithm for this setting, LSTDQ. Inspired by an instrumental-variable view, we develop error bounds that depend on a novel coverage parameter, the feature-dynamics coverage, which can be interpreted as linear coverage in an induced dynamical system for feature evolution. With further assumptions -- such as Bellman-completeness -- our definition successfully recovers the coverage parameters specialized to those settings, finally yielding a unified understanding for coverage in linear OPE.

</details>


### [45] [Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward](https://arxiv.org/abs/2601.19055)
*Dipendra Misra,Aldo Pacchiano,Ta-Chung Chi,Ge Gao*

Main category: cs.LG

TL;DR: 该论文研究如何利用用户编辑部署数据（上下文、代理响应、用户编辑）来微调LLMs，提出了一种集成学习方法，能够统一处理偏好、监督标签和成本等不同类型的反馈。


<details>
  <summary>Details</summary>
Motivation: 用户编辑数据是LLM应用（如写作助手和代码代理）中自然产生的，可用于适应和个性化LLMs。不同反馈类型（偏好、监督标签、成本）在文献中通常分开研究，而用户编辑数据为统一这些反馈类型提供了机会。

Method: 首先推导了从不同反馈类型学习的算法边界，证明这些算法在用户、数据分布和模型类别方面有不同的权衡。然后提出一种简单的集成程序，联合学习这些反馈类型。

Result: 在基于Gao等人2024年工作的两个领域上，集成程序优于从单个反馈学习的方法。此外，该方法能够在测试时鲁棒地适应不同的用户编辑分布。

Conclusion: 用户编辑数据为LLM微调提供了有价值的自然反馈来源，提出的集成方法能够有效统一不同类型的反馈，并在不同用户编辑分布下保持鲁棒性。

Abstract: We study how to fine-tune LLMs using user-edit deployment data consisting of a set of context, an agent's response, and user edits. This deployment data is naturally generated by users in applications such as LLMs-based writing assistants and coding agents. The _natural_ origin of user edits makes it a desired source for adapting and personalizing LLMs. In this setup, there emerges a unification of various feedback types namely preferences, supervised labels, and cost that are typically studied separately in the literature. In this paper, we initiate the theoretical investigation of learning from user edits. We first derive bounds for learning algorithms that learn from each of these feedback types. We prove that these algorithms have different trade-offs depending upon the user, data distribution, and model class. We then propose a simple ensembling procedure to jointly learn from these feedback types. On two domains adapted from Gao et al. 2024, we show our ensembling procedure outperforms these methods that learn from individual feedback. Further, we show that our proposed procedure can robustly adapt to different user-edit distributions at test time.

</details>


### [46] [Native LLM and MLLM Inference at Scale on Apple Silicon](https://arxiv.org/abs/2601.19139)
*Wayner Barrios*

Main category: cs.LG

TL;DR: vllm-mlx：专为Apple Silicon优化的高效LLM和MLLM推理框架，基于MLX原生构建，提供连续批处理和内容感知前缀缓存，显著提升文本和多模态模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着Apple Silicon在机器学习开发中的普及，现有工具要么缺乏原生优化（如PyTorch MPS），要么仅专注于文本模型（如llama.cpp），无法满足多模态工作负载的需求，因此需要专门为Apple Silicon统一内存架构优化的高效推理解决方案。

Method: 基于MLX原生构建推理框架，针对文本模型实现连续批处理技术，支持16个并发请求；针对多模态模型引入基于内容哈希的内容感知前缀缓存，可识别相同图像内容而忽略输入格式差异，消除冗余视觉编码。

Result: 在Apple M4 Max上，文本模型推理吞吐量比llama.cpp提高21%-87%（Qwen3-0.6B到Nemotron-30B），16并发请求时聚合吞吐量提升4.3倍；多模态模型重复图像查询速度提升28倍，延迟从21.7秒降至1秒以下；64帧视频分析缓存加速24.7倍。

Conclusion: vllm-mlx为Apple Silicon提供了高效的LLM和MLLM推理解决方案，通过原生MLX优化、连续批处理和内容感知缓存技术，显著提升了消费级Apple硬件的推理性能，并已开源发布。

Abstract: The growing adoption of Apple Silicon for machine learning development has created demand for efficient inference solutions that leverage its unique unified memory architecture. However, existing tools either lack native optimization (PyTorch MPS) or focus solely on text models (llama.cpp), leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21% to 87% higher throughput than llama.cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate throughput at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates redundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our implementation as open source to support efficient inference on consumer Apple hardware.

</details>


### [47] [Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2601.19280)
*Kishan Panaganti,Zhenwen Liang,Wenhao Yu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 提出Multi-Adversary Group Distributionally Robust Optimization (GDRO)框架，通过动态难度分类和对抗性资源分配，优化LLM推理训练效率，相比GRPO基线在pass@8准确率上提升约10%。


<details>
  <summary>Details</summary>
Motivation: 标准RL方法（如GRPO）在异构推理数据上存在结构性低效：均匀采样和固定rollout数量导致对已解决模式浪费计算资源，而对困难问题的长尾分布训练不足。

Method: 提出GDRO框架，包含：1) Online Difficulty Classifier动态划分提示难度组；2) Prompt-GDRO使用EMA去偏乘性权重bandit采样器，针对困难边缘组；3) Rollout-GDRO使用影子价格控制器在组间重新分配rollout，在固定计算预算下最大化梯度方差减少。

Result: 在DAPO 14.1k数据集上使用Qwen3-Base模型验证，Prompt-GDRO和Rollout-GDRO分别在1.7B、4B和8B规模上相比GRPO基线平均提升pass@8准确率10.6%和10.1%。

Conclusion: GDRO框架通过动态难度适应和对抗性资源分配，有效解决了异构推理数据中的训练效率问题，实现了新兴课程学习效果，将资源转移到不断演化的推理前沿。

Abstract: Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.
  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance.

</details>


### [48] [Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model](https://arxiv.org/abs/2601.19232)
*Qi Si,Xuyang Liu,Penglei Wang,Xin Guo,Yuan Qi,Yuan Cheng*

Main category: cs.LG

TL;DR: SOLD：一种结合潜在扩散模型与强化学习的RNA逆折叠框架，通过单步噪声优化实现多结构目标的高效优化，显著提升结构准确性。


<details>
  <summary>Details</summary>
Motivation: 当前RNA逆折叠方法主要关注序列恢复，难以处理非可微的结构目标（如二级结构一致性、最小自由能、局部距离差异测试），导致结构准确性不足。需要一种能有效优化多种结构目标的方法。

Method: 提出SOLD框架：结合潜在扩散模型（LDM）与强化学习（RL）。LDM使用预训练的RNA-FM嵌入捕获共进化模式；RL通过策略驱动的奖励优化处理非可微结构目标，采用单步噪声优化而非完整扩散轨迹采样。

Result: SOLD在各项指标上超越了其LDM基线和现有最先进方法，显著提升了RNA逆折叠的结构准确性。

Conclusion: SOLD为RNA逆折叠提供了一个强大的框架，能够有效优化多种结构目标，对生物技术和治疗应用具有深远意义。

Abstract: RNA inverse folding, designing sequences to form specific 3D structures, is critical for therapeutics, gene regulation, and synthetic biology. Current methods, focused on sequence recovery, struggle to address structural objectives like secondary structure consistency (SS), minimum free energy (MFE), and local distance difference test (LDDT), leading to suboptimal structural accuracy. To tackle this, we propose a reinforcement learning (RL) framework integrated with a latent diffusion model (LDM). Drawing inspiration from the success of diffusion models in RNA inverse folding, which adeptly model complex sequence-structure interactions, we develop an LDM incorporating pre-trained RNA-FM embeddings from a large-scale RNA model. These embeddings capture co-evolutionary patterns, markedly improving sequence recovery accuracy. However, existing approaches, including diffusion-based methods, cannot effectively handle non-differentiable structural objectives. By contrast, RL excels in this task by using policy-driven reward optimization to navigate complex, non-gradient-based objectives, offering a significant advantage over traditional methods. In summary, we propose the Step-wise Optimization of Latent Diffusion Model (SOLD), a novel RL framework that optimizes single-step noise without sampling the full diffusion trajectory, achieving efficient refinement of multiple structural objectives. Experimental results demonstrate SOLD surpasses its LDM baseline and state-of-the-art methods across all metrics, establishing a robust framework for RNA inverse folding with profound implications for biotechnological and therapeutic applications.

</details>


### [49] [From Observations to Events: Event-Aware World Model for Reinforcement Learning](https://arxiv.org/abs/2601.19336)
*Zhao-Han Peng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.LG

TL;DR: EAWM提出事件感知世界模型框架，通过自动事件生成和分割来学习事件感知表征，提升模型强化学习的泛化能力和样本效率，在多个基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的强化学习方法在结构相似场景间的泛化能力不足，容易受到纹理、颜色等虚假变化的影响。受人类认知科学启发，人类将连续感官流分割为离散事件并依赖关键事件进行决策。

Method: 提出事件感知世界模型(EAWM)框架，包含自动事件生成器和通用事件分割器(GES)来从原始观测中提取事件并识别事件边界。通过事件预测来塑造表征空间以捕捉有意义的时空转换，并提供统一的世界模型架构表述。

Result: 在Atari 100K、Craftax 1M、DeepMind Control 500K和DMC-GB2 500K等基准测试中，EAWM将强基线方法的性能提升10%-45%，创造了新的最先进结果。

Conclusion: EAWM通过事件感知表征学习有效提升了基于模型强化学习的泛化能力和样本效率，为世界模型学习提供了新的认知科学启发框架。

Abstract: While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.

</details>


### [50] [Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection](https://arxiv.org/abs/2601.19375)
*Quy-Anh Dang,Chris Ngo*

Main category: cs.LG

TL;DR: 提出Selective Steering方法，通过数学严谨的范数保持旋转公式和判别性层选择，解决现有激活引导技术的局限性，实现更稳定可控的LLM行为修改。


<details>
  <summary>Details</summary>
Motivation: 尽管在对齐方面取得进展，大语言模型仍容易受到对抗攻击引发有害行为。现有激活引导技术存在局限性：激活加法需要精细系数调优且对层特定范数变化敏感，方向消融仅提供二元控制，而角度引导虽引入连续控制但实际实现违反范数保持，导致分布偏移和生成崩溃。

Method: 提出Selective Steering方法，包含两个关键创新：(1) 数学严谨的范数保持旋转公式，保持激活分布完整性；(2) 判别性层选择，仅在特征表示呈现相反符号类别对齐的层应用引导。

Result: 在九个模型上的实验表明，Selective Steering相比先前方法实现了5.5倍更高的攻击成功率，同时保持零困惑度违规和在标准基准上约100%的能力保留。

Conclusion: 该方法为可控和稳定的LLM行为修改提供了一个原则性、高效的框架。

Abstract: Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering

</details>


### [51] [APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition](https://arxiv.org/abs/2601.19452)
*Finn Rietz,Pedro Zuidberg dos Martires,Johannes Andreas Stork*

Main category: cs.LG

TL;DR: APC是一种分层模型，通过自适应组合多个数据驱动的归一化流先验来整合演示数据到强化学习中，能够处理次优、稀疏或不对齐的演示，避免严格遵循演示导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设演示数据是最优且与目标任务完全对齐的，但实际中演示经常是稀疏、次优或不对齐的，这会导致将演示整合到强化学习中时性能下降。

Method: 提出自适应策略组合（APC）分层模型，自适应组合多个数据驱动的归一化流先验。APC估计每个先验对目标任务的适用性，利用它们进行探索，同时根据需要精炼有用先验或避开不对齐的先验以优化下游奖励。

Result: 在多样化基准测试中，APC在演示对齐时加速学习，在严重不对齐情况下保持鲁棒性，利用次优演示引导探索，同时避免因过度严格遵循次优演示导致的性能下降。

Conclusion: APC能够有效处理实际中常见的次优、稀疏或不对齐演示数据，在加速学习的同时保持鲁棒性，解决了现有方法对演示质量要求过高的问题。

Abstract: Incorporating demonstration data into reinforcement learning (RL) can greatly accelerate learning, but existing approaches often assume demonstrations are optimal and fully aligned with the target task. In practice, demonstrations are frequently sparse, suboptimal, or misaligned, which can degrade performance when these demonstrations are integrated into RL. We propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow (NF) priors. Instead of enforcing strict adherence to the priors, APC estimates each prior's applicability to the target task while leveraging them for exploration. Moreover, APC either refines useful priors, or sidesteps misaligned ones when necessary to optimize downstream reward. Across diverse benchmarks, APC accelerates learning when demonstrations are aligned, remains robust under severe misalignment, and leverages suboptimal demonstrations to bootstrap exploration while avoiding performance degradation caused by overly strict adherence to suboptimal demonstrations.

</details>


### [52] [LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment](https://arxiv.org/abs/2601.19487)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Wenhai Wang*

Main category: cs.LG

TL;DR: LLM-VA通过将回答向量与安全判断向量对齐，解决LLM在安全对齐中的越狱和过度拒绝问题，无需微调即可实现更好的安全-效用平衡。


<details>
  <summary>Details</summary>
Motivation: 现有向量调控方法存在根本性权衡：减少越狱会增加过度拒绝，反之亦然。研究发现LLM将回答决策和安全判断编码为近乎正交的方向，导致这两个过程被当作独立处理。

Method: 提出LLM-VA方法，通过闭式权重更新将回答向量与安全判断向量对齐，使模型回答意愿因果依赖于安全评估。使用SVM识别各层向量，选择安全相关层，通过最小范数权重修改迭代对齐向量。

Result: 在12个LLM上的实验表明，LLM-VA比最佳基线F1分数提高11.45%，同时保持95.92%的效用，且能自动适应各模型的安全偏差而无需手动调优。

Conclusion: LLM-VA通过向量对齐解决了安全对齐LLM的根本问题，实现了更好的安全-效用平衡，且方法通用、无需微调或架构修改。

Abstract: Safety-aligned LLMs suffer from two failure modes: jailbreak (answering harmful inputs) and over-refusal (declining benign queries). Existing vector steering methods adjust the magnitude of answer vectors, but this creates a fundamental trade-off -- reducing jailbreak increases over-refusal and vice versa. We identify the root cause: LLMs encode the decision to answer (answer vector $v_a$) and the judgment of input safety (benign vector $v_b$) as nearly orthogonal directions, treating them as independent processes. We propose LLM-VA, which aligns $v_a$ with $v_b$ through closed-form weight updates, making the model's willingness to answer causally dependent on its safety assessment -- without fine-tuning or architectural changes. Our method identifies vectors at each layer using SVMs, selects safety-relevant layers, and iteratively aligns vectors via minimum-norm weight modifications. Experiments on 12 LLMs demonstrate that LLM-VA achieves 11.45% higher F1 than the best baseline while preserving 95.92% utility, and automatically adapts to each model's safety bias without manual tuning. Code and models are available at https://hotbento.github.io/LLM-VA-Web/.

</details>


### [53] [Safe Exploration via Policy Priors](https://arxiv.org/abs/2601.19612)
*Manuel Wendl,Yarden As,Manish Prajapat,Anton Pollak,Stelian Coros,Andreas Krause*

Main category: cs.LG

TL;DR: SOOPER是一种安全强化学习方法，利用次优但保守的策略作为先验，通过概率动力学模型进行乐观探索，同时在需要时悲观地回退到保守策略，保证学习过程中的安全性。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体在真实环境中需要安全探索，而不仅仅是受控的模拟环境。现有方法在安全性和学习效率之间存在权衡。

Method: 使用次优但保守的策略（如从离线数据或模拟器获得）作为先验，结合概率动力学模型进行乐观探索，同时在需要时悲观地回退到保守策略。

Result: SOOPER在关键的安全RL基准测试和真实硬件实验中表现出色，优于现有最先进方法，并验证了理论保证。

Conclusion: SOOPER提供了一种可扩展的安全强化学习方法，能够保证学习过程中的安全性，同时收敛到最优策略。

Abstract: Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.

</details>


### [54] [R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning](https://arxiv.org/abs/2601.19620)
*Zhizheng Jiang,Kang Zhao,Weikai Xu,Xinkui Lin,Wei Liu,Jian Luan,Shuo Shang,Peng Han*

Main category: cs.LG

TL;DR: R^3方法通过跨上下文重放、上下文自反思和结构熵排序奖励三个机制，解决大推理模型训练中组内优势崩溃的问题，在数学基准测试上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法依赖同一批次内高质量样本诱导的优势差距，但在挑战性任务下组内优势容易崩溃，导致训练过程脆弱且低效。

Method: 提出R^3强化学习机制：1)跨上下文重放策略，通过回忆同一查询的历史轨迹保持组内优势；2)上下文自反思机制，利用过去失败经验精炼输出；3)结构熵排序奖励，基于标记级熵模式对截断或失败样本分配相对奖励。

Result: 在Deepseek-R1-Distill-Qwen-1.5B上实现，使用DeepscaleR-40k数学数据集训练，在多个数学基准测试上达到SOTA性能，相比基础模型有显著改进且使用更少推理标记。

Conclusion: R^3方法通过三个创新机制有效解决了组内优势崩溃问题，提高了大推理模型训练的稳定性和效率，在数学推理任务上表现出色。

Abstract: Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.

</details>


### [55] [Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action](https://arxiv.org/abs/2601.19720)
*Gong Gao,Weidong Zhao,Xianhui Liu,Ning Jia*

Main category: cs.LG

TL;DR: 提出IRA算法，通过Q表示差异演化、贪婪动作引导和即时策略更新机制，解决基于价值的在线强化学习中探索效率低和策略更新延迟的问题，在MuJoCo连续控制任务上显著提升学习效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于价值的在线强化学习算法存在探索效率低和策略更新延迟的问题，导致策略利用缓慢。需要解决这些挑战以提高学习效率和最终性能。

Method: 提出IRA算法，包含三个核心组件：1) Q表示差异演化(RDE)促进Q网络表示学习；2) 贪婪动作引导(GAG)通过回溯历史动作实现显式策略约束；3) 即时策略更新(IPU)机制系统性地增加策略更新频率。

Result: 在八个MuJoCo连续控制任务上的实验结果表明，IRA能显著提高在线强化学习算法的学习效率和最终性能。同时发现IRA早期训练的保守性可以缓解基于价值RL中的高估偏差问题。

Conclusion: IRA算法通过改进表示学习、策略约束和更新机制，有效解决了基于价值在线强化学习中的探索和更新延迟问题，在连续控制任务上取得了显著性能提升。

Abstract: Existing value-based online reinforcement learning (RL) algorithms suffer from slow policy exploitation due to ineffective exploration and delayed policy updates. To address these challenges, we propose an algorithm called Instant Retrospect Action (IRA). Specifically, we propose Q-Representation Discrepancy Evolution (RDE) to facilitate Q-network representation learning, enabling discriminative representations for neighboring state-action pairs. In addition, we adopt an explicit method to policy constraints by enabling Greedy Action Guidance (GAG). This is achieved through backtracking historical actions, which effectively enhances the policy update process. Our proposed method relies on providing the learning algorithm with accurate $k$-nearest-neighbor action value estimates and learning to design a fast-adaptable policy through policy constraints. We further propose the Instant Policy Update (IPU) mechanism, which enhances policy exploitation by systematically increasing the frequency of policy updates. We further discover that the early-stage training conservatism of the IRA method can alleviate the overestimation bias problem in value-based RL. Experimental results show that IRA can significantly improve the learning efficiency and final performance of online RL algorithms on eight MuJoCo continuous control tasks.

</details>


### [56] [Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals](https://arxiv.org/abs/2601.19810)
*Octavio Pappalardo*

Main category: cs.LG

TL;DR: ULEE：一种结合上下文学习和对抗性目标生成的无监督元学习方法，在XLand-MiniGrid基准测试中展现出优越的探索和适应能力


<details>
  <summary>Details</summary>
Motivation: 解决强化学习智能体在广泛下游任务分布中有效学习的问题，特别是在目标任务超出预训练分布或任务身份未知的情况下，需要高效的探索和适应机制

Method: 提出ULEE方法，结合元学习框架优化多回合探索和适应，使用上下文学习器，并通过对抗性目标生成策略维持训练在智能体能力边界

Result: 在XLand-MiniGrid基准测试中，ULEE预训练展现出对新颖目标、环境动态和地图结构的泛化能力，获得改进的零样本和少样本性能，为长时微调提供强初始化

Conclusion: ULEE通过无监督元学习和对抗性目标生成，有效提升了强化学习智能体的探索和适应能力，优于从头学习、DIAYN预训练和其他课程学习方法

Abstract: Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent's post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent's capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.

</details>
