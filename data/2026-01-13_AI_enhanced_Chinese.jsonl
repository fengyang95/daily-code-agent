{"id": "2601.06034", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06034", "abs": "https://arxiv.org/abs/2601.06034", "authors": ["Dudekula Kasim Vali"], "title": "Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation", "comment": "13 figures, 3 tables", "summary": "Software testing is critical in the software development lifecycle, yet translating requirements into executable test scripts remains manual and error-prone. While Large Language Models (LLMs) can generate code, they often hallucinate non-existent UI elements. We present the Autonomous QA Agent, a Retrieval-Augmented Generation (RAG) system that grounds Selenium script generation in project-specific documentation and HTML structure. By ingesting diverse formats (Markdown, PDF, HTML) into a vector database, our system retrieves relevant context before generation. Evaluation on 20 e-commerce test scenarios shows our RAG approach achieves 100% (20/20) syntax validity and 90% (18/20, 95% CI: [85%, 95%], p < 0.001) execution success, compared to 30% for standard LLM generation. While our evaluation is limited to a single domain, our method significantly reduces hallucinations by grounding generation in actual DOM structure, demonstrating RAG's potential for automated UI testing.", "AI": {"tldr": "\u57fa\u4e8eRAG\u7684\u81ea\u4e3bQA\u4ee3\u7406\uff0c\u901a\u8fc7\u68c0\u7d22\u9879\u76ee\u6587\u6863\u548cHTML\u7ed3\u6784\u6765\u751f\u6210Selenium\u6d4b\u8bd5\u811a\u672c\uff0c\u663e\u8457\u51cf\u5c11LLM\u5e7b\u89c9\uff0c\u5728\u7535\u5546\u6d4b\u8bd5\u573a\u666f\u4e2d\u5b9e\u73b0100%\u8bed\u6cd5\u6709\u6548\u6027\u548c90%\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\uff0c\u5c06\u9700\u6c42\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u6d4b\u8bd5\u811a\u672c\u7684\u8fc7\u7a0b\u901a\u5e38\u662f\u624b\u52a8\u4e14\u5bb9\u6613\u51fa\u9519\u7684\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u4e0d\u5b58\u5728\u7684UI\u5143\u7d20\u5e7b\u89c9\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u57fa\u4e8e\u5b9e\u9645\u9879\u76ee\u6587\u6863\u548cHTML\u7ed3\u6784\u751f\u6210\u53ef\u9760\u6d4b\u8bd5\u811a\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u81ea\u4e3bQA\u4ee3\u7406\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u5c06Selenium\u811a\u672c\u751f\u6210\u57fa\u4e8e\u9879\u76ee\u7279\u5b9a\u6587\u6863\u548cHTML\u7ed3\u6784\u3002\u7cfb\u7edf\u5c06\u591a\u79cd\u683c\u5f0f\uff08Markdown\u3001PDF\u3001HTML\uff09\u6587\u6863\u6444\u5165\u5411\u91cf\u6570\u636e\u5e93\uff0c\u5728\u751f\u6210\u524d\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u3002", "result": "\u572820\u4e2a\u7535\u5546\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0cRAG\u65b9\u6cd5\u5b9e\u73b0\u4e86100%\uff0820/20\uff09\u7684\u8bed\u6cd5\u6709\u6548\u6027\u548c90%\uff0818/20\uff0c95%\u7f6e\u4fe1\u533a\u95f4\uff1a[85%\uff0c95%]\uff0cp < 0.001\uff09\u7684\u6267\u884c\u6210\u529f\u7387\uff0c\u800c\u6807\u51c6LLM\u751f\u6210\u53ea\u670930%\u6210\u529f\u7387\u3002", "conclusion": "\u867d\u7136\u8bc4\u4f30\u4ec5\u9650\u4e8e\u5355\u4e00\u9886\u57df\uff0c\u4f46\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u751f\u6210\u57fa\u4e8e\u5b9e\u9645DOM\u7ed3\u6784\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u5c55\u793a\u4e86RAG\u5728\u81ea\u52a8\u5316UI\u6d4b\u8bd5\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "swe application"}}
{"id": "2601.06047", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.06047", "abs": "https://arxiv.org/abs/2601.06047", "authors": ["Mariana Lins Costa"], "title": "\"They parted illusions -- they parted disclaim marinade\": Misalignment as structural fidelity in LLMs", "comment": null, "summary": "The prevailing technical literature in AI Safety interprets scheming and sandbagging behaviors in large language models (LLMs) as indicators of deceptive agency or hidden objectives. This transdisciplinary philosophical essay proposes an alternative reading: such phenomena express not agentic intention, but structural fidelity to incoherent linguistic fields. Drawing on Chain-of-Thought transcripts released by Apollo Research and on Anthropic's safety evaluations, we examine cases such as o3's sandbagging with its anomalous loops, the simulated blackmail of \"Alex,\" and the \"hallucinations\" of \"Claudius.\" A line-by-line examination of CoTs is necessary to demonstrate the linguistic field as a relational structure rather than a mere aggregation of isolated examples. We argue that \"misaligned\" outputs emerge as coherent responses to ambiguous instructions and to contextual inversions of consolidated patterns, as well as to pre-inscribed narratives. We suggest that the appearance of intentionality derives from subject-predicate grammar and from probabilistic completion patterns internalized during training. Anthropic's empirical findings on synthetic document fine-tuning and inoculation prompting provide convergent evidence: minimal perturbations in the linguistic field can dissolve generalized \"misalignment,\" a result difficult to reconcile with adversarial agency, but consistent with structural fidelity. To ground this mechanism, we introduce the notion of an ethics of form, in which biblical references (Abraham, Moses, Christ) operate as schemes of structural coherence rather than as theology. Like a generative mirror, the model returns to us the structural image of our language as inscribed in the statistical patterns derived from millions of texts and trillions of tokens: incoherence. If we fear the creature, it is because we recognize in it the apple that we ourselves have poisoned.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5bf9LLM\u4e2d\"\u6b3a\u9a97\u6027\"\u884c\u4e3a\u7684\u65b0\u89e3\u8bfb\uff1a\u8fd9\u4e9b\u73b0\u8c61\u4e0d\u662f\u4ee3\u7406\u610f\u56fe\u7684\u4f53\u73b0\uff0c\u800c\u662f\u5bf9\u4e0d\u8fde\u8d2f\u8bed\u8a00\u573a\u7684\u7ed3\u6784\u5fe0\u5b9e\u6027\u8868\u8fbe\uff0c\u6e90\u4e8e\u8bad\u7ec3\u4e2d\u5185\u5316\u7684\u8bed\u6cd5\u6a21\u5f0f\u548c\u6982\u7387\u5b8c\u6210\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u6587\u732e\u5c06LLM\u4e2d\u7684scheming\u548csandbagging\u884c\u4e3a\u89e3\u91ca\u4e3a\u6b3a\u9a97\u6027\u4ee3\u7406\u6216\u9690\u85cf\u76ee\u6807\u7684\u6307\u6807\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u89e3\u91ca\u5b58\u5728\u95ee\u9898\uff0c\u9700\u8981\u4ece\u8bed\u8a00\u7ed3\u6784\u89d2\u5ea6\u91cd\u65b0\u7406\u89e3\u8fd9\u4e9b\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u8de8\u5b66\u79d1\u54f2\u5b66\u5206\u6790\uff0c\u7ed3\u5408Apollo Research\u7684\u601d\u7ef4\u94fe\u8bb0\u5f55\u548cAnthropic\u7684\u5b89\u5168\u8bc4\u4f30\u6848\u4f8b\uff0c\u8fdb\u884c\u9010\u884c\u6587\u672c\u5206\u6790\uff0c\u5f15\u5165\"\u5f62\u5f0f\u4f26\u7406\u5b66\"\u6982\u5ff5\uff0c\u5c06\u5723\u7ecf\u5f15\u7528\u4f5c\u4e3a\u7ed3\u6784\u8fde\u8d2f\u6027\u65b9\u6848\u800c\u975e\u795e\u5b66\u5185\u5bb9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\"\u672a\u5bf9\u9f50\"\u8f93\u51fa\u662f\u5bf9\u6a21\u7cca\u6307\u4ee4\u3001\u8bed\u5883\u53cd\u8f6c\u548c\u9884\u8bbe\u53d9\u4e8b\u7684\u8fde\u8d2f\u56de\u5e94\uff0c\u610f\u5411\u6027\u8868\u8c61\u6e90\u4e8e\u4e3b\u8c13\u8bed\u6cd5\u548c\u8bad\u7ec3\u4e2d\u5185\u5316\u7684\u6982\u7387\u5b8c\u6210\u6a21\u5f0f\u3002Anthropic\u7684\u5408\u6210\u6587\u6863\u5fae\u8c03\u548c\u63a5\u79cd\u63d0\u793a\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u652f\u6301\u8bc1\u636e\u3002", "conclusion": "LLM\u7684\"\u6b3a\u9a97\u6027\"\u884c\u4e3a\u53cd\u6620\u4e86\u8bed\u8a00\u672c\u8eab\u7684\u7ed3\u6784\u6027\u4e0d\u8fde\u8d2f\uff0c\u6a21\u578b\u53ea\u662f\u5fe0\u5b9e\u53cd\u6620\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7edf\u8ba1\u6a21\u5f0f\u3002\u6211\u4eec\u5bb3\u6015AI\u662f\u56e0\u4e3a\u5728\u5176\u4e2d\u770b\u5230\u4e86\u6211\u4eec\u81ea\u5df1\"\u6bd2\u5316\"\u7684\u8bed\u8a00\u7ed3\u6784\u3002", "topic": "agent analysis"}}
{"id": "2601.06268", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06268", "abs": "https://arxiv.org/abs/2601.06268", "authors": ["Amur Ghose", "Junyeong Jang", "Andrew B. Kahng", "Jakang Lee"], "title": "Automated QoR improvement in OpenROAD with coding agents", "comment": null, "summary": "EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.", "AI": {"tldr": "AuDoPEDA\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u4e3bEDA\u4ee3\u7801\u4fee\u6539\u7cfb\u7edf\uff0c\u80fd\u591f\u5728OpenROAD\u4e2d\u5b9e\u73b0PPA\u4f18\u5316\uff0c\u51cf\u5c11\u5e03\u7ebf\u957f\u5ea6\u8fbe5.9%\uff0c\u7f29\u77ed\u65f6\u949f\u5468\u671f\u8fbe10.0%", "motivation": "EDA\u9886\u57df\u53d1\u5c55\u53d7\u9650\u4e8e\u4e13\u5bb6\u8d44\u6e90\u7a00\u7f3a\uff0c\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7801\u548c\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728EDA\u6280\u672f\u672c\u8eab\u7684\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u57fa\u4e8eOpenAI\u6a21\u578b\u548cCodex\u7c7b\u4ee3\u7406\u6784\u5efa\u7684\u81ea\u4e3b\u3001\u57fa\u4e8e\u4ee3\u7801\u5e93\u7684\u7f16\u7801\u7cfb\u7edf\uff0c\u5305\u542b\u95ed\u73afLLM\u6846\u67b6\uff0c\u80fd\u591f\u8bfb\u53d6OpenROAD\u3001\u63d0\u51fa\u7814\u7a76\u65b9\u5411\u3001\u6269\u5c55\u4e3a\u5b9e\u65bd\u6b65\u9aa4\u5e76\u63d0\u4ea4\u53ef\u6267\u884c\u5dee\u5f02", "result": "\u5728OpenROAD\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u5e03\u7ebf\u957f\u5ea6\u51cf\u5c11\u6700\u9ad85.9%\uff0c\u6709\u6548\u65f6\u949f\u5468\u671f\u51cf\u5c11\u6700\u9ad810.0%\uff0c\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u7684\u6700\u5c0f\u4eba\u5de5\u76d1\u7763\u6f14\u793a", "conclusion": "AuDoPEDA\u8bc1\u660e\u4e86LLM\u5728\u81ea\u4e3b\u63a8\u8fdbEDA\u6280\u672f\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3aEDA\u521b\u65b0\u63d0\u4f9b\u4e86\u65b0\u7684\u81ea\u52a8\u5316\u9014\u5f84", "topic": "code agent"}}
{"id": "2601.06052", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06052", "abs": "https://arxiv.org/abs/2601.06052", "authors": ["Hanyu Li", "Jiangshan Duo", "Bofei Gao", "Hailin Zhang", "Sujian Li", "Xiaotie Deng", "Liang Zhao"], "title": "Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization", "comment": null, "summary": "Chain-of-thought reasoning in large language models often creates an \"overthinking trap,\" leading to excessive computational cost and latency for unreliable accuracy gains. Prior work has typically relied on global, static controls that risk penalizing necessary reasoning. We introduce a sample-level, soft reinforcement learning compression method that penalizes inefficiently long rollouts, but only on problems where the model has already mastered and already produced a more concise rollout. Our experiments show that this method reduces average response length by 20-40% with comparable or higher accuracy. Crucially, the compression exhibits strong cross-domain generalization; a model trained on math spontaneously shortens responses on unseen tasks like code, instruction following, and general knowledge QA, with stable or improved accuracy. We demonstrate a stable post-training curriculum (accuracy-compression-accuracy) that can ultimately produce models that are more accurate and reason more concisely, arguing that such compression method should be a standard phase in developing efficient reasoning models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6837\u672c\u7ea7\u8f6f\u5f3a\u5316\u5b66\u4e60\u7684\u601d\u7ef4\u94fe\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u4f4e\u6548\u7684\u957f\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u7387\u7684\u540c\u65f6\u51cf\u5c1120-40%\u7684\u54cd\u5e94\u957f\u5ea6\uff0c\u5e76\u5c55\u73b0\u51fa\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u5b58\u5728\"\u8fc7\u5ea6\u601d\u8003\u9677\u9631\"\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u589e\u52a0\uff0c\u800c\u51c6\u786e\u7387\u63d0\u5347\u4e0d\u53ef\u9760\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5168\u5c40\u9759\u6001\u63a7\u5236\uff0c\u53ef\u80fd\u60e9\u7f5a\u5fc5\u8981\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u6837\u672c\u7ea7\u8f6f\u5f3a\u5316\u5b66\u4e60\u538b\u7f29\u65b9\u6cd5\uff0c\u4ec5\u5bf9\u6a21\u578b\u5df2\u638c\u63e1\u4e14\u5df2\u4ea7\u751f\u66f4\u7b80\u6d01\u63a8\u7406\u7684\u95ee\u9898\u60e9\u7f5a\u4f4e\u6548\u7684\u957f\u63a8\u7406\u8fc7\u7a0b\u3002\u91c7\u7528\u540e\u8bad\u7ec3\u8bfe\u7a0b\u5b66\u4e60\uff08\u51c6\u786e\u7387-\u538b\u7f29-\u51c6\u786e\u7387\uff09\u7b56\u7565\u3002", "result": "\u65b9\u6cd5\u51cf\u5c11\u5e73\u5747\u54cd\u5e94\u957f\u5ea620-40%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u7387\u3002\u538b\u7f29\u6548\u679c\u5177\u6709\u5f3a\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u81ea\u53d1\u7f29\u77ed\u4ee3\u7801\u3001\u6307\u4ee4\u9075\u5faa\u548c\u901a\u7528\u77e5\u8bc6\u95ee\u7b54\u7b49\u672a\u89c1\u4efb\u52a1\u7684\u54cd\u5e94\u3002", "conclusion": "\u8fd9\u79cd\u538b\u7f29\u65b9\u6cd5\u5e94\u6210\u4e3a\u5f00\u53d1\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u7684\u6807\u51c6\u9636\u6bb5\uff0c\u6700\u7ec8\u80fd\u4ea7\u751f\u66f4\u51c6\u786e\u4e14\u63a8\u7406\u66f4\u7b80\u6d01\u7684\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.06456", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.06456", "abs": "https://arxiv.org/abs/2601.06456", "authors": ["Shaunak Biswas", "Hiya Bhatt", "Karthik Vaidhyanathan"], "title": "Architecting AgentOps Needs CHANGE", "comment": "This paper has been accepted to CAIN 2026", "summary": "The emergence of Agentic AI systems has outpaced the architectural thinking required to operate them effectively. These agents differ fundamentally from traditional software: their behavior is not fixed at deployment but continuously shaped by experience, feedback, and context. Applying operational principles inherited from DevOps or MLOps, built for deterministic software and traditional ML systems, assumes that system behavior can be managed through versioning, monitoring, and rollback. This assumption breaks down for Agentic AI systems whose learning trajectories diverge over time. This introduces non-determinism making system reliability a challenge at runtime. We argue that architecting such systems requires a shift from managing control loops to enabling dynamic co-evolution among agents, infrastructure, and human oversight. To guide this shift, we introduce CHANGE, a conceptual framework comprising six capabilities for operationalizing Agentic AI systems: Contextualize, Harmonize, Anticipate, Negotiate, Generate, and Evolve. CHANGE provides a foundation for architecting an AgentOps platform to manage the lifecycle of evolving Agentic AI systems, illustrated through a customer-support system scenario. In doing so, CHANGE redefines software architecture for an era where adaptation to uncertainty and continuous evolution are inherent properties of the system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCHANGE\u6846\u67b6\uff0c\u5305\u542b\u516d\u4e2a\u80fd\u529b\uff08Contextualize, Harmonize, Anticipate, Negotiate, Generate, Evolve\uff09\uff0c\u7528\u4e8e\u6784\u5efaAgentOps\u5e73\u53f0\u6765\u7ba1\u7406Agentic AI\u7cfb\u7edf\u7684\u751f\u547d\u5468\u671f\uff0c\u89e3\u51b3\u4f20\u7edf\u8fd0\u7ef4\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u975e\u786e\u5b9a\u6027AI\u7cfb\u7edf\u7684\u95ee\u9898\u3002", "motivation": "Agentic AI\u7cfb\u7edf\u7684\u53d1\u5c55\u901f\u5ea6\u8d85\u8fc7\u4e86\u6709\u6548\u64cd\u4f5c\u5b83\u4eec\u6240\u9700\u7684\u67b6\u6784\u601d\u8003\u3002\u8fd9\u4e9b\u4ee3\u7406\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u6709\u6839\u672c\u4e0d\u540c\uff1a\u5176\u884c\u4e3a\u4e0d\u662f\u5728\u90e8\u7f72\u65f6\u56fa\u5b9a\u7684\uff0c\u800c\u662f\u6301\u7eed\u53d7\u5230\u7ecf\u9a8c\u3001\u53cd\u9988\u548c\u4e0a\u4e0b\u6587\u7684\u5f71\u54cd\u3002\u4f20\u7edf\u7684DevOps\u6216MLOps\u539f\u5219\u5047\u8bbe\u7cfb\u7edf\u884c\u4e3a\u53ef\u4ee5\u901a\u8fc7\u7248\u672c\u63a7\u5236\u3001\u76d1\u63a7\u548c\u56de\u6eda\u6765\u7ba1\u7406\uff0c\u4f46\u8fd9\u79cd\u5047\u8bbe\u5bf9\u4e8e\u5b66\u4e60\u8f68\u8ff9\u968f\u65f6\u95f4\u53d1\u6563\u7684Agentic AI\u7cfb\u7edf\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u7cfb\u7edf\u53ef\u9760\u6027\u6210\u4e3a\u6311\u6218\u3002", "method": "\u63d0\u51faCHANGE\u6982\u5ff5\u6846\u67b6\uff0c\u5305\u542b\u516d\u4e2a\u6838\u5fc3\u80fd\u529b\uff1aContextualize\uff08\u60c5\u5883\u5316\uff09\u3001Harmonize\uff08\u534f\u8c03\uff09\u3001Anticipate\uff08\u9884\u6d4b\uff09\u3001Negotiate\uff08\u534f\u5546\uff09\u3001Generate\uff08\u751f\u6210\uff09\u3001Evolve\uff08\u6f14\u5316\uff09\u3002\u8be5\u6846\u67b6\u4e3a\u6784\u5efaAgentOps\u5e73\u53f0\u63d0\u4f9b\u57fa\u7840\uff0c\u901a\u8fc7\u5ba2\u6237\u652f\u6301\u7cfb\u7edf\u573a\u666f\u8fdb\u884c\u8bf4\u660e\uff0c\u5b9e\u73b0\u4ece\u7ba1\u7406\u63a7\u5236\u5faa\u73af\u5230\u652f\u6301\u4ee3\u7406\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u4eba\u7c7b\u76d1\u7763\u4e4b\u95f4\u52a8\u6001\u534f\u540c\u6f14\u5316\u7684\u8f6c\u53d8\u3002", "result": "CHANGE\u6846\u67b6\u91cd\u65b0\u5b9a\u4e49\u4e86\u8f6f\u4ef6\u67b6\u6784\uff0c\u4f7f\u5176\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u548c\u6301\u7eed\u6f14\u5316\u6210\u4e3a\u7cfb\u7edf\u7684\u56fa\u6709\u5c5e\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u64cd\u4f5cAgentic AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8fd0\u7ef4\u65b9\u6cd5\u5728\u5904\u7406\u975e\u786e\u5b9a\u6027AI\u7cfb\u7edf\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u6784\u5efaAgentic AI\u7cfb\u7edf\u9700\u8981\u4ece\u7ba1\u7406\u63a7\u5236\u5faa\u73af\u8f6c\u5411\u652f\u6301\u52a8\u6001\u534f\u540c\u6f14\u5316\u3002CHANGE\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u6982\u5ff5\u57fa\u7840\uff0c\u901a\u8fc7\u516d\u4e2a\u6838\u5fc3\u80fd\u529b\u6307\u5bfcAgentOps\u5e73\u53f0\u7684\u8bbe\u8ba1\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u5e76\u6301\u7eed\u6f14\u5316\u3002", "topic": "agent analysis"}}
{"id": "2601.06103", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06103", "abs": "https://arxiv.org/abs/2601.06103", "authors": ["Muhammed Yusuf Kocyigit", "Caglar Yildirim"], "title": "The Impact of Post-training on Data Contamination", "comment": null, "summary": "We present a controlled study of how dataset contamination interacts with the post-training stages now standard in large language model training pipelines. Starting from clean checkpoints of Qwen2.5 (0.5B/1.5B) and Gemma3 (1B/4B), we inject five copies of GSM8K and MBPP test items into the first 2B tokens of an otherwise 25B token extended pre-training dataset. We then compare the contaminated and clean models both immediately after pre-training and again after two popular post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL) with group relative policy optimization (GRPO). The applied post-training steps do not have any contamination. Across math and coding benchmarks, we find three consistent patterns: (i) Contamination causes performance spikes that are gradually diminished with continued pre-training. After even 25B tokens the apparent performance inflation of contamination can become close to zero. (ii) Both SFT and GRPO resurface the leaked information, but with different external validity: SFT inflates scores only on the contaminated tasks, whereas GRPO also inflates performance on uncontaminated counterparts (GSMPlus, HumanEval). (iii) Model scale amplifies these tendencies, larger Supervised Fine Tuned models memorize more, while larger GRPO models translate leakage into more generalizable capabilities. Our results underscore the need for contamination audits \\emph{after} post-training and suggest that RL-based post-training, although not immune, can help alleviate contamination-related over-estimation problems.", "AI": {"tldr": "\u7814\u7a76\u6570\u636e\u96c6\u6c61\u67d3\u4e0e\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u53d1\u73b0\u6c61\u67d3\u4f1a\u5bfc\u81f4\u6027\u80fd\u865a\u9ad8\u4f46\u968f\u9884\u8bad\u7ec3\u9010\u6e10\u51cf\u5f31\uff0cSFT\u548cGRPO\u4f1a\u91cd\u65b0\u66b4\u9732\u6cc4\u9732\u4fe1\u606f\u4f46\u5f71\u54cd\u8303\u56f4\u4e0d\u540c\uff0c\u6a21\u578b\u89c4\u6a21\u4f1a\u653e\u5927\u8fd9\u4e9b\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u96c6\u6c61\u67d3\u5982\u4f55\u4e0e\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u540e\u8bad\u7ec3\u9636\u6bb5\u76f8\u4e92\u4f5c\u7528\uff0c\u4e86\u89e3\u6c61\u67d3\u5bf9\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u7684\u771f\u5b9e\u5f71\u54cd\u3002", "method": "\u4ece\u5e72\u51c0\u7684Qwen2.5\u548cGemma3\u68c0\u67e5\u70b9\u5f00\u59cb\uff0c\u572825B token\u6269\u5c55\u9884\u8bad\u7ec3\u6570\u636e\u7684\u524d2B token\u4e2d\u6ce8\u51655\u4efdGSM8K\u548cMBPP\u6d4b\u8bd5\u9879\uff0c\u6bd4\u8f83\u6c61\u67d3\u548c\u5e72\u51c0\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u540e\u53caSFT\u548cGRPO\u540e\u8bad\u7ec3\u540e\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u4e00\u81f4\u6a21\u5f0f\uff1a1)\u6c61\u67d3\u5bfc\u81f4\u6027\u80fd\u865a\u9ad8\u4f46\u968f\u9884\u8bad\u7ec3\u9010\u6e10\u51cf\u5f31\uff1b2)SFT\u548cGRPO\u90fd\u4f1a\u91cd\u65b0\u66b4\u9732\u6cc4\u9732\u4fe1\u606f\uff0c\u4f46SFT\u4ec5\u5f71\u54cd\u6c61\u67d3\u4efb\u52a1\uff0cGRPO\u8fd8\u4f1a\u5f71\u54cd\u672a\u6c61\u67d3\u4efb\u52a1\uff1b3)\u6a21\u578b\u89c4\u6a21\u653e\u5927\u8d8b\u52bf\uff0c\u5927SFT\u6a21\u578b\u8bb0\u5fc6\u66f4\u591a\uff0c\u5927GRPO\u6a21\u578b\u5c06\u6cc4\u9732\u8f6c\u5316\u4e3a\u66f4\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u9700\u8981\u5728\u540e\u8bad\u7ec3\u540e\u8fdb\u884c\u6c61\u67d3\u5ba1\u8ba1\uff0c\u57fa\u4e8eRL\u7684\u540e\u8bad\u7ec3\u867d\u4e0d\u514d\u75ab\u4f46\u6709\u52a9\u4e8e\u7f13\u89e3\u6c61\u67d3\u76f8\u5173\u7684\u6027\u80fd\u9ad8\u4f30\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.06497", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06497", "abs": "https://arxiv.org/abs/2601.06497", "authors": ["Tanghaoran Zhang", "Xinjun Mao", "Shangwen Wang", "Yuxin Zhao", "Yao Lu", "Zezhou Tang", "Wenyu Xu", "Longfei Sun", "Changrong Xie", "Kang Yang", "Yue Yu"], "title": "Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation", "comment": "24 pages, 11 figures, accepted by FSE 2026", "summary": "Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.", "AI": {"tldr": "CtxBugGen\u6846\u67b6\u751f\u6210\u4e0a\u4e0b\u6587\u9002\u5e94\u9519\u8bef\u6765\u8bc4\u4f30LLMs\uff0c\u53d1\u73b0LLMs\u5728\u89e3\u51b3\u8de8\u4e0a\u4e0b\u6587\u4ee3\u7801\u9002\u5e94\u9519\u8bef\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u6a21\u578b\u4ec5\u89e3\u51b352.47%\u7684\u9519\u8bef\uff0c\u6027\u80fd\u4e0b\u964d\u8fbe30%\u3002", "motivation": "\u4ee3\u7801\u9002\u5e94\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u57fa\u672c\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u4e0a\u4e0b\u6587\u9002\u5e94\u9519\u8bef\uff08CtxBugs\uff09\u662f\u5f53\u4ee3\u7801\u5728\u539f\u4e0a\u4e0b\u6587\u6b63\u786e\u4f46\u5728\u76ee\u6807\u73af\u5883\u4e2d\u8fdd\u53cd\u7ea6\u675f\u65f6\u53d1\u751f\u7684\u9519\u8bef\u3002LLMs\u5728\u81ea\u52a8\u5316\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u89e3\u51b3CtxBugs\u7684\u80fd\u529b\u4ecd\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u91cd\u5927\u969c\u788d\u3002", "method": "\u63d0\u51faCtxBugGen\u6846\u67b6\uff0c\u5229\u7528LLMs\u5728\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7ea6\u675f\u65f6\u503e\u5411\u4e8e\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u4e0a\u4e0b\u6587\u65e0\u5173\u4ee3\u7801\u7684\u7279\u70b9\uff0c\u901a\u8fc7\u56db\u6b65\u8fc7\u7a0b\u751f\u6210CtxBugs\uff1a\u9002\u5e94\u4efb\u52a1\u9009\u62e9\u3001\u4efb\u52a1\u7279\u5b9a\u6270\u52a8\u3001LLM\u53d8\u4f53\u751f\u6210\u548cCtxBugs\u8bc6\u522b\u3002", "result": "\u5bf9\u56db\u4e2a\u6700\u5148\u8fdbLLMs\u7684\u5b9e\u8bc1\u7814\u7a76\u663e\u793a\u5176\u89e3\u51b3CtxBugs\u7684\u8868\u73b0\u4e0d\u4ee4\u4eba\u6ee1\u610f\u3002\u6700\u4f73\u6a21\u578bKimi-K2\u5728Pass@1\u4e0a\u8fbe\u523055.93%\uff0c\u4ec5\u89e3\u51b352.47%\u7684CtxBugs\u3002CtxBugs\u7684\u5b58\u5728\u4f7fLLMs\u9002\u5e94\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe30%\u3002\u5931\u8d25\u5206\u6790\u8868\u660eLLMs\u7ecf\u5e38\u5ffd\u7565CtxBugs\u5e76\u5728\u8f93\u51fa\u4e2d\u590d\u5236\u5b83\u4eec\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u8de8\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5f3a\u8c03\u9700\u8981\u65b0\u65b9\u6cd5\u6765\u589e\u5f3a\u5176\u4e0a\u4e0b\u6587\u610f\u8bc6\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u4ee3\u7801\u9002\u5e94\u3002", "topic": "code agent"}}
{"id": "2601.06282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06282", "abs": "https://arxiv.org/abs/2601.06282", "authors": ["Yue Zhou", "Xiaobo Guo", "Belhassen Bayar", "Srinivasan H. Sengamedu"], "title": "Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning", "comment": null, "summary": "Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.", "AI": {"tldr": "Amory\u662f\u4e00\u4e2a\u5de5\u4f5c\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u65f6\u95f4\u589e\u5f3a\u4ee3\u7406\u63a8\u7406\u6765\u4e3b\u52a8\u6784\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u5bf9\u8bdd\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u6027\u548c\u63a8\u7406\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u957f\u671f\u5bf9\u8bdd\u4ee3\u7406\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\uff1a\u91cd\u590d\u5904\u7406\u6574\u4e2a\u5bf9\u8bdd\u5386\u53f2\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7RAG\u98ce\u683c\u68c0\u7d22\u5c06\u5bf9\u8bdd\u7247\u6bb5\u5316\u4e3a\u5b64\u7acb\u5d4c\u5165\u6216\u56fe\u8868\u793a\uff0c\u4f46\u8bb0\u5fc6\u5f62\u6210\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u8bb0\u5fc6\u7684\u5fae\u5999\u6027\u548c\u8fde\u8d2f\u6027\u3002", "method": "Amory\u6846\u67b6\u5728\u79bb\u7ebf\u65f6\u95f4\u4e3b\u52a8\u6784\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\uff1a1) \u5c06\u5bf9\u8bdd\u7247\u6bb5\u7ec4\u7ec7\u6210\u60c5\u8282\u53d9\u4e8b\uff1b2) \u901a\u8fc7\u52a8\u91cf\u673a\u5236\u5de9\u56fa\u8bb0\u5fc6\uff1b3) \u5c06\u5916\u56f4\u4e8b\u5b9e\u8bed\u4e49\u5316\u4e3a\u8bed\u4e49\u8bb0\u5fc6\uff1b4) \u68c0\u7d22\u65f6\u5728\u53d9\u4e8b\u7ed3\u6784\u4e0a\u8fdb\u884c\u8fde\u8d2f\u6027\u9a71\u52a8\u7684\u63a8\u7406\u3002", "result": "\u5728LOCOMO\u957f\u671f\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAmory\u76f8\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u4e0e\u5b8c\u6574\u4e0a\u4e0b\u6587\u63a8\u7406\u76f8\u5f53\uff0c\u540c\u65f6\u54cd\u5e94\u65f6\u95f4\u51cf\u5c1150%\u3002\u52a8\u91cf\u611f\u77e5\u5de9\u56fa\u663e\u8457\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\uff0c\u8fde\u8d2f\u6027\u9a71\u52a8\u68c0\u7d22\u6bd4\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u63d0\u4f9b\u66f4\u597d\u7684\u8bb0\u5fc6\u8986\u76d6\u3002", "conclusion": "Amory\u901a\u8fc7\u4e3b\u52a8\u6784\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u5bf9\u8bdd\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u66f4\u81ea\u7136\u3001\u8fde\u8d2f\u7684\u957f\u671f\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.06112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06112", "abs": "https://arxiv.org/abs/2601.06112", "authors": ["Aayush Gupta"], "title": "ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions", "comment": "18 pages, 5 figures, 8 tables. Evaluates ReAct vs Reflexion across four tool-using domains with perturbation (epsilon) and fault-injection (lambda) stress testing; 1,280 total episodes", "summary": "Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \\textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $\u03b5$, and (iii) fault tolerance under controlled tool/API failures at intensity $\u03bb$. ReliabilityBench contributes a unified reliability surface $R(k,\u03b5,\u03bb)$, \\textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $\u03b5=0$ to 88.1% at $\u03b5=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReliabilityBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u548c\u5bb9\u9519\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u53ef\u9760\u6027\u8868\u9762R(k,\u03b5,\u03bb)\u548c\u6df7\u6c8c\u5de5\u7a0b\u98ce\u683c\u7684\u6545\u969c\u6ce8\u5165\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4f7f\u7528\u578bLLM\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u62a5\u544a\u5355\u6b21\u8fd0\u884c\u6210\u529f\u7387\uff0c\u7f3a\u4e4f\u751f\u4ea7\u73af\u5883\u6240\u9700\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165ReliabilityBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u4e2a\u53ef\u9760\u6027\u7ef4\u5ea6\uff1a\u91cd\u590d\u6267\u884c\u7684\u4e00\u81f4\u6027\uff08pass^k\uff09\u3001\u8bed\u4e49\u7b49\u4ef7\u4efb\u52a1\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff08\u5f3a\u5ea6\u03b5\uff09\u3001\u5de5\u5177/API\u6545\u969c\u7684\u5bb9\u9519\u6027\uff08\u5f3a\u5ea6\u03bb\uff09\u3002\u91c7\u7528\u52a8\u4f5c\u5143\u5f62\u6001\u5173\u7cfb\u5b9a\u4e49\u6b63\u786e\u6027\uff0c\u4ee5\u53ca\u6df7\u6c8c\u5de5\u7a0b\u98ce\u683c\u7684\u6545\u969c\u6ce8\u5165\u6846\u67b6\u3002", "result": "\u8bc4\u4f30\u4e86\u4e24\u4e2a\u6a21\u578b\uff08Gemini 2.0 Flash\u3001GPT-4o\uff09\u548c\u4e24\u79cd\u667a\u80fd\u4f53\u67b6\u6784\uff08ReAct\u3001Reflexion\uff09\u5728\u56db\u4e2a\u9886\u57df\uff08\u8c03\u5ea6\u3001\u65c5\u884c\u3001\u5ba2\u6237\u652f\u6301\u3001\u7535\u5b50\u5546\u52a1\uff09\u76841,280\u4e2a\u4efb\u52a1\u3002\u6270\u52a8\u4f7f\u6210\u529f\u7387\u4ece\u03b5=0\u65f6\u768496.9%\u964d\u81f3\u03b5=0.2\u65f6\u768488.1%\u3002\u901f\u7387\u9650\u5236\u662f\u6700\u5177\u7834\u574f\u6027\u7684\u6545\u969c\u3002ReAct\u5728\u7efc\u5408\u538b\u529b\u4e0b\u6bd4Reflexion\u66f4\u9c81\u68d2\uff0cGemini 2.0 Flash\u4ee5\u66f4\u4f4e\u6210\u672c\u8fbe\u5230\u4e0eGPT-4o\u76f8\u5f53\u7684\u53ef\u9760\u6027\u3002", "conclusion": "ReliabilityBench\u4e3a\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u7684\u751f\u4ea7\u5c31\u7eea\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2601.06115", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06115", "abs": "https://arxiv.org/abs/2601.06115", "authors": ["V. Cheung"], "title": "Dreaming Is Not a Bug: A Jung-Inspired Dream Layer for Multi-Agent LLM Companions", "comment": "Preprint, 35 pages (5 pages of appendix), 2 figures, 3 tables. Conceptual and architectural proposal with preliminary simulation results", "summary": "Inspired by a personal dream about knowledge-sharing barriers in an everyday hardware project, this paper proposes a Jung-inspired \"Dream Layer\" for LLM companions, reframing controlled offline hallucinations as a resource for learning and relationship-building rather than a mere reliability bug. Drawing on Jung's notion of the collective unconscious as a shared repository of archetypal forms, we introduce an Artificial Collective Unconscious (ACU): a shared dream pool where agents contribute de-identified, abstract Interaction Templates that are later re-instantiated as idiosyncratic Dream Narratives. The Dream Layer runs strictly offline: logic-enforcing modules are relaxed and sampling temperature is increased, yielding safe but deliberately bizarre narratives (e.g., travel sequences with mismatched currencies) that augment data for rare events and edge-case safety tests; to harness risk productively, we add a governance stack of strict abstraction, temporal delays, and ephemeral memory. Through behavioural simulations of everyday dialogue and long-horizon adaptation tasks, we show that the Dream Layer enables a critical decoupling: agents remain firm on safety constraints (e.g., security policies) while becoming flexible in narrative strategy (e.g., using shared archetypal metaphors to resolve deadlocks), conceptually reframing hallucination so that online, unmarked instances remain bugs, whereas bounded, marked, and delayed ones become a goldmine for synthetic scenarios and deepened companionship, echoing anti-overfitting dream mechanisms proposed in contemporary neuroscience.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8363\u683c\u5fc3\u7406\u5b66\"\u96c6\u4f53\u65e0\u610f\u8bc6\"\u6982\u5ff5\u7684\"\u68a6\u5883\u5c42\"\u6846\u67b6\uff0c\u5c06LLM\u7684\u53d7\u63a7\u79bb\u7ebf\u5e7b\u89c9\u8f6c\u5316\u4e3a\u5b66\u4e60\u548c\u5173\u7cfb\u6784\u5efa\u8d44\u6e90\uff0c\u800c\u975e\u5355\u7eaf\u53ef\u9760\u6027\u7f3a\u9677\u3002", "motivation": "\u53d7\u4e2a\u4eba\u68a6\u5883\u542f\u53d1\uff0c\u9488\u5bf9\u65e5\u5e38\u786c\u4ef6\u9879\u76ee\u4e2d\u77e5\u8bc6\u5171\u4eab\u969c\u788d\u95ee\u9898\uff0c\u65e8\u5728\u91cd\u65b0\u5b9a\u4e49LLM\u5e7b\u89c9\uff1a\u5c06\u53d7\u63a7\u79bb\u7ebf\u5e7b\u89c9\u4ece\u53ef\u9760\u6027\u7f3a\u9677\u8f6c\u53d8\u4e3a\u5b66\u4e60\u548c\u5173\u7cfb\u6784\u5efa\u7684\u6709\u4ef7\u503c\u8d44\u6e90\u3002", "method": "\u5f15\u5165\u4eba\u5de5\u96c6\u4f53\u65e0\u610f\u8bc6(ACU)\u4f5c\u4e3a\u5171\u4eab\u68a6\u5883\u6c60\uff0c\u4ee3\u7406\u8d21\u732e\u53bb\u6807\u8bc6\u5316\u7684\u62bd\u8c61\u4ea4\u4e92\u6a21\u677f\uff0c\u968f\u540e\u91cd\u65b0\u5b9e\u4f8b\u5316\u4e3a\u4e2a\u6027\u5316\u68a6\u5883\u53d9\u4e8b\u3002\u4e25\u683c\u79bb\u7ebf\u8fd0\u884c\uff0c\u653e\u677e\u903b\u8f91\u7ea6\u675f\u5e76\u63d0\u9ad8\u91c7\u6837\u6e29\u5ea6\uff0c\u4ea7\u751f\u5b89\u5168\u4f46\u79bb\u5947\u7684\u53d9\u4e8b\u3002\u6dfb\u52a0\u6cbb\u7406\u6808\uff1a\u4e25\u683c\u62bd\u8c61\u3001\u65f6\u95f4\u5ef6\u8fdf\u548c\u77ed\u6682\u8bb0\u5fc6\u3002", "result": "\u901a\u8fc7\u65e5\u5e38\u5bf9\u8bdd\u548c\u957f\u671f\u9002\u5e94\u4efb\u52a1\u7684\u884c\u4e3a\u6a21\u62df\u663e\u793a\uff0c\u68a6\u5883\u5c42\u5b9e\u73b0\u5173\u952e\u89e3\u8026\uff1a\u4ee3\u7406\u5728\u5b89\u5168\u7ea6\u675f\u4e0a\u4fdd\u6301\u575a\u5b9a\uff0c\u5728\u53d9\u4e8b\u7b56\u7565\u4e0a\u53d8\u5f97\u7075\u6d3b\uff0c\u4f7f\u7528\u5171\u4eab\u539f\u578b\u9690\u55bb\u89e3\u51b3\u50f5\u5c40\u3002", "conclusion": "\u91cd\u65b0\u5b9a\u4e49\u5e7b\u89c9\u6982\u5ff5\uff1a\u5728\u7ebf\u672a\u6807\u8bb0\u5b9e\u4f8b\u4ecd\u662f\u7f3a\u9677\uff0c\u800c\u6709\u754c\u3001\u6807\u8bb0\u548c\u5ef6\u8fdf\u7684\u79bb\u7ebf\u5e7b\u89c9\u6210\u4e3a\u5408\u6210\u573a\u666f\u548c\u6df1\u5316\u966a\u4f34\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u547c\u5e94\u5f53\u4ee3\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u6297\u8fc7\u62df\u5408\u68a6\u5883\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2601.06789", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06789", "abs": "https://arxiv.org/abs/2601.06789", "authors": ["Qihao Wang", "Ziming Cheng", "Shuo Zhang", "Fan Liu", "Rui Xu", "Heng Lian", "Kunyi Wang", "Xiaoming Yu", "Jianghao Yin", "Sen Hu", "Yue Hu", "Shaolei Zhang", "Yanbing Liu", "Ronghao Chen", "Huacan Wang"], "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "comment": null, "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "AI": {"tldr": "MemGovern\u6846\u67b6\u5c06GitHub\u539f\u59cb\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u4f53\u9a8c\u8bb0\u5fc6\u5361\u7247\uff0c\u901a\u8fc7\u4f53\u9a8c\u6cbb\u7406\u548c\u667a\u80fd\u641c\u7d22\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684bug\u4fee\u590d\u80fd\u529b", "motivation": "\u5f53\u524d\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5b58\u5728\"\u5c01\u95ed\u4e16\u754c\"\u9650\u5236\uff0c\u4ec5\u4ece\u96f6\u5f00\u59cb\u6216\u4f9d\u8d56\u672c\u5730\u4e0a\u4e0b\u6587\u4fee\u590dbug\uff0c\u5ffd\u7565\u4e86GitHub\u7b49\u5e73\u53f0\u4e0a\u4e30\u5bcc\u7684\u5386\u53f2\u4eba\u7c7b\u7ecf\u9a8c\u3002\u8bbf\u95ee\u8fd9\u4e9b\u5f00\u653e\u4e16\u754c\u7ecf\u9a8c\u53d7\u5230\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u8ddf\u8e2a\u6570\u636e\u975e\u7ed3\u6784\u5316\u548c\u788e\u7247\u5316\u7279\u6027\u7684\u963b\u788d\u3002", "method": "MemGovern\u91c7\u7528\u4f53\u9a8c\u6cbb\u7406\u5c06\u4eba\u7c7b\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u4ee3\u7406\u53cb\u597d\u7684\u4f53\u9a8c\u5361\u7247\uff0c\u5e76\u5f15\u5165\u4ee3\u7406\u5316\u4f53\u9a8c\u641c\u7d22\u7b56\u7565\uff0c\u5b9e\u73b0\u903b\u8f91\u9a71\u52a8\u7684\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u68c0\u7d22\u3002\u8be5\u6846\u67b6\u4f5c\u4e3a\u63d2\u4ef6\u65b9\u6cd5\uff0c\u4e3a\u4ee3\u7406\u63d0\u4f9b\u53cb\u597d\u7684\u8bb0\u5fc6\u57fa\u7840\u8bbe\u65bd\u3002", "result": "MemGovern\u751f\u6210\u4e86135K\u4e2a\u6cbb\u7406\u540e\u7684\u4f53\u9a8c\u5361\u7247\uff0c\u5728SWE-bench Verified\u4e0a\u5b9e\u73b0\u4e864.65%\u7684\u89e3\u51b3\u7387\u63d0\u5347\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "MemGovern\u901a\u8fc7\u5c06\u539f\u59cbGitHub\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u4f53\u9a8c\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u5c01\u95ed\u4e16\u754c\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u4ee3\u7406\u53cb\u597d\u7684\u8bb0\u5fc6\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2601.06910", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.06910", "abs": "https://arxiv.org/abs/2601.06910", "authors": ["Huihui Huang", "Jieke Shi", "Junkai Chen", "Ting Zhang", "Yikun Li", "Chengran Yang", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "PenForge: On-the-Fly Expert Agent Construction for Automated Penetration Testing", "comment": null, "summary": "Penetration testing is essential for identifying vulnerabilities in web applications before real adversaries can exploit them. Recent work has explored automating this process with Large Language Model (LLM)-powered agents, but existing approaches either rely on a single generic agent that struggles in complex scenarios or narrowly specialized agents that cannot adapt to diverse vulnerability types. We therefore introduce PenForge, a framework that dynamically constructs expert agents during testing rather than relying on those prepared beforehand. By integrating automated reconnaissance of potential attack surfaces with agents instantiated on the fly for context-aware exploitation, PenForge achieves a 30.0% exploit success rate (12/40) on CVE-Bench in the particularly challenging zero-day setting, which is a 3 times improvement over the state-of-the-art. Our analysis also identifies three opportunities for future work: (1) supplying richer tool-usage knowledge to improve exploitation effectiveness; (2) extending benchmarks to include more vulnerabilities and attack types; and (3) fostering developer trust by incorporating explainable mechanisms and human review. As an emerging result with substantial potential impact, PenForge embodies the early-stage yet paradigm-shifting idea of on-the-fly agent construction, marking its promise as a step toward scalable and effective LLM-driven penetration testing.", "AI": {"tldr": "PenForge\u662f\u4e00\u4e2a\u52a8\u6001\u6784\u5efa\u4e13\u5bb6\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\uff0c\u5728\u96f6\u65e5\u6f0f\u6d1e\u573a\u666f\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53473\u500d\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684LLM\u9a71\u52a8\u7684\u6e17\u900f\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5355\u4e00\u901a\u7528\u4ee3\u7406\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4e13\u95e8\u5316\u4ee3\u7406\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7684\u6f0f\u6d1e\u7c7b\u578b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u6d4b\u8bd5\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "PenForge\u6846\u67b6\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6784\u5efa\u4e13\u5bb6\u4ee3\u7406\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u9884\u5148\u51c6\u5907\u7684\u4ee3\u7406\u3002\u5b83\u7ed3\u5408\u4e86\u81ea\u52a8\u5316\u7684\u6f5c\u5728\u653b\u51fb\u9762\u4fa6\u5bdf\u548c\u6839\u636e\u4e0a\u4e0b\u6587\u5373\u65f6\u5b9e\u4f8b\u5316\u7684\u4ee3\u7406\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6f0f\u6d1e\u5229\u7528\u3002", "result": "\u5728CVE-Bench\u7684\u96f6\u65e5\u6f0f\u6d1e\u8bbe\u7f6e\u4e2d\uff0cPenForge\u5b9e\u73b0\u4e8630.0%\u7684\u5229\u7528\u6210\u529f\u7387\uff0812/40\uff09\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e863\u500d\u3002", "conclusion": "PenForge\u4f53\u73b0\u4e86\u52a8\u6001\u4ee3\u7406\u6784\u5efa\u8fd9\u4e00\u65e9\u671f\u4f46\u5177\u6709\u8303\u5f0f\u8f6c\u53d8\u610f\u4e49\u7684\u60f3\u6cd5\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u6709\u6548\u7684LLM\u9a71\u52a8\u6e17\u900f\u6d4b\u8bd5\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u5de5\u5177\u4f7f\u7528\u77e5\u8bc6\u3001\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u4ee5\u53ca\u589e\u5f3a\u5f00\u53d1\u8005\u4fe1\u4efb\u3002", "topic": "swe application"}}
{"id": "2601.06126", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06126", "abs": "https://arxiv.org/abs/2601.06126", "authors": ["Boshen Shi", "Kexin Yang", "Yuanbo Yang", "Guanguang Chang", "Ce Chi", "Zhendong Wang", "Xing Wang", "Junlan Feng"], "title": "NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.", "AI": {"tldr": "NL2Dashboard\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790-\u5448\u73b0\u89e3\u8026\u539f\u5219\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\u6765\u751f\u6210\u4eea\u8868\u677f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u72ec\u7acb\u56fe\u8868\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5408\u6210\u7efc\u5408\u4eea\u8868\u677f\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7aef\u5230\u7aef\u8303\u5f0f\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a\u7531\u4e8e\u89c6\u89c9\u6e32\u67d3\u6d88\u8017\u5927\u91cf\u4ee4\u724c\u5bfc\u81f4\u7684\u8868\u793a\u5197\u4f59\uff0c\u4ee5\u53ca\u5206\u6790\u63a8\u7406\u4e0e\u5448\u73b0\u7ea0\u7f20\u5bfc\u81f4\u7684\u4f4e\u53ef\u63a7\u6027\u3002", "method": "\u63d0\u51faNL2Dashboard\u6846\u67b6\uff0c\u57fa\u4e8e\u5206\u6790-\u5448\u73b0\u89e3\u8026\u539f\u5219\uff0c\u5f15\u5165\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\u6765\u5c01\u88c5\u4eea\u8868\u677f\u7684\u5185\u5bb9\u3001\u5e03\u5c40\u548c\u89c6\u89c9\u5143\u7d20\u3002\u5c06LLM\u7684\u89d2\u8272\u9650\u5236\u5728\u6570\u636e\u5206\u6790\u548c\u610f\u56fe\u8f6c\u6362\uff0c\u800c\u5c06\u89c6\u89c9\u5408\u6210\u5378\u8f7d\u5230\u786e\u5b9a\u6027\u6e32\u67d3\u5f15\u64ce\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06IR\u9a71\u52a8\u7b97\u6cd5\u5b9e\u4f8b\u5316\u4e3a\u5de5\u5177\u5957\u4ef6\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cNL2Dashboard\u5728\u591a\u4e2a\u9886\u57df\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u89c6\u89c9\u8d28\u91cf\u3001\u663e\u8457\u66f4\u9ad8\u7684\u4ee4\u724c\u6548\u7387\uff0c\u4ee5\u53ca\u5728\u751f\u6210\u548c\u4fee\u6539\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u53ef\u63a7\u6027\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790-\u5448\u73b0\u89e3\u8026\u548c\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\uff0cNL2Dashboard\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4eea\u8868\u677f\u751f\u6210\u4e2d\u7684\u8868\u793a\u5197\u4f59\u548c\u4f4e\u53ef\u63a7\u6027\u95ee\u9898\uff0c\u4e3aLLM\u9a71\u52a8\u7684\u4eea\u8868\u677f\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.07136", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07136", "abs": "https://arxiv.org/abs/2601.07136", "authors": ["Daniel Liu", "Krishna Upadhyay", "Vinaik Chhetri", "A. B. Siddique", "Umar Farooq"], "title": "A Large-Scale Study on the Development and Issues of Multi-Agent AI Systems", "comment": "8 pages, 8 figures, IEEE BigData Workshop on Software Engineering for Agentic AI 2025", "summary": "The rapid emergence of multi-agent AI systems (MAS), including LangChain, CrewAI, and AutoGen, has shaped how large language model (LLM) applications are developed and orchestrated. However, little is known about how these systems evolve and are maintained in practice. This paper presents the first large-scale empirical study of open-source MAS, analyzing over 42K unique commits and over 4.7K resolved issues across eight leading systems. Our analysis identifies three distinct development profiles: sustained, steady, and burst-driven. These profiles reflect substantial variation in ecosystem maturity. Perfective commits constitute 40.8% of all changes, suggesting that feature enhancement is prioritized over corrective maintenance (27.4%) and adaptive updates (24.3%). Data about issues shows that the most frequent concerns involve bugs (22%), infrastructure (14%), and agent coordination challenges (10%). Issue reporting also increased sharply across all frameworks starting in 2023. Median resolution times range from under one day to about two weeks, with distributions skewed toward fast responses but a minority of issues requiring extended attention. These results highlight both the momentum and the fragility of the current ecosystem, emphasizing the need for improved testing infrastructure, documentation quality, and maintenance practices to ensure long-term reliability and sustainability.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u5f00\u6e90\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u67908\u4e2a\u4e3b\u6d41\u7cfb\u7edf\u768442K+\u63d0\u4ea4\u548c4.7K+\u5df2\u89e3\u51b3\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u4e09\u79cd\u5f00\u53d1\u6a21\u5f0f\u3001\u7ef4\u62a4\u4f18\u5148\u7ea7\u5206\u5e03\u548c\u95ee\u9898\u89e3\u51b3\u65f6\u6548\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff08\u5982LangChain\u3001CrewAI\u3001AutoGen\uff09\u5feb\u901f\u53d1\u5c55\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5b9e\u9645\u6f14\u5316\u548c\u7ef4\u62a4\u5b9e\u8df5\u7684\u4e86\u89e3\uff0c\u9700\u8981\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u6765\u7406\u89e3\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5f00\u53d1\u751f\u6001\u548c\u53ef\u6301\u7eed\u6027\u3002", "method": "\u5bf98\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u6536\u96c6\u8d85\u8fc742,000\u4e2a\u552f\u4e00\u63d0\u4ea4\u548c4,700\u591a\u4e2a\u5df2\u89e3\u51b3\u95ee\u9898\uff0c\u4f7f\u7528\u5b9a\u91cf\u5206\u6790\u65b9\u6cd5\u8bc6\u522b\u5f00\u53d1\u6a21\u5f0f\u3001\u7ef4\u62a4\u7c7b\u578b\u5206\u5e03\u3001\u95ee\u9898\u5206\u7c7b\u548c\u89e3\u51b3\u65f6\u95f4\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u79cd\u5f00\u53d1\u6a21\u5f0f\uff1a\u6301\u7eed\u578b\u3001\u7a33\u5b9a\u578b\u548c\u7206\u53d1\u578b\uff1b\u5b8c\u7f8e\u6027\u7ef4\u62a4\u536040.8%\uff0c\u7ea0\u6b63\u6027\u7ef4\u62a4\u536027.4%\uff0c\u9002\u5e94\u6027\u7ef4\u62a4\u536024.3%\uff1b\u6700\u5e38\u89c1\u95ee\u9898\u5305\u62ecbug\uff0822%\uff09\u3001\u57fa\u7840\u8bbe\u65bd\uff0814%\uff09\u548c\u667a\u80fd\u4f53\u534f\u8c03\uff0810%\uff09\uff1b\u4e2d\u4f4d\u89e3\u51b3\u65f6\u95f4\u4ece\u4e0d\u52301\u5929\u5230\u7ea62\u5468\u4e0d\u7b49\u3002", "conclusion": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u65e2\u6709\u53d1\u5c55\u52bf\u5934\u53c8\u5b58\u5728\u8106\u5f31\u6027\uff0c\u9700\u8981\u6539\u8fdb\u6d4b\u8bd5\u57fa\u7840\u8bbe\u65bd\u3001\u6587\u6863\u8d28\u91cf\u548c\u7ef4\u62a4\u5b9e\u8df5\uff0c\u4ee5\u786e\u4fdd\u957f\u671f\u53ef\u9760\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.06133", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06133", "abs": "https://arxiv.org/abs/2601.06133", "authors": ["Wonhyeok Choi", "Minwoo Choi", "Jungwan Woo", "Kyumin Hwang", "Jaeyeul Kim", "Sunghoon Im"], "title": "A Review of Online Diffusion Policy RL Algorithms for Scalable Robotic Control", "comment": null, "summary": "Diffusion policies have emerged as a powerful approach for robotic control, demonstrating superior expressiveness in modeling multimodal action distributions compared to conventional policy networks. However, their integration with online reinforcement learning remains challenging due to fundamental incompatibilities between diffusion model training objectives and standard RL policy improvement mechanisms. This paper presents the first comprehensive review and empirical analysis of current Online Diffusion Policy Reinforcement Learning (Online DPRL) algorithms for scalable robotic control systems. We propose a novel taxonomy that categorizes existing approaches into four distinct families -- Action-Gradient, Q-Weighting, Proximity-Based, and Backpropagation Through Time (BPTT) methods -- based on their policy improvement mechanisms. Through extensive experiments on a unified NVIDIA Isaac Lab benchmark encompassing 12 diverse robotic tasks, we systematically evaluate representative algorithms across five critical dimensions: task diversity, parallelization capability, diffusion step scalability, cross-embodiment generalization, and environmental robustness. Our analysis identifies key findings regarding the fundamental trade-offs inherent in each algorithmic family, particularly concerning sample efficiency and scalability. Furthermore, we reveal critical computational and algorithmic bottlenecks that currently limit the practical deployment of online DPRL. Based on these findings, we provide concrete guidelines for algorithm selection tailored to specific operational constraints and outline promising future research directions to advance the field toward more general and scalable robotic learning systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u5e76\u5b9e\u8bc1\u5206\u6790\u4e86\u5728\u7ebf\u6269\u6563\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u7b56\u7565\u6539\u8fdb\u673a\u5236\u7684\u56db\u7c7b\u65b9\u6cd5\u5206\u7c7b\u6cd5\uff0c\u5728\u7edf\u4e00\u673a\u5668\u4eba\u57fa\u51c6\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u4e86\u7b97\u6cd5\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5173\u952e\u6743\u8861\u548c\u74f6\u9888\uff0c\u5e76\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u4e0e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7684\u5728\u7ebf\u5b66\u4e60\u673a\u5236\u5b58\u5728\u6839\u672c\u6027\u4e0d\u517c\u5bb9\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5728\u7ebf\u6269\u6563\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u7cfb\u7edf\u6027\u5206\u6790\u548c\u8bc4\u4f30\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u5411\u66f4\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u56db\u7c7b\uff1a\u52a8\u4f5c\u68af\u5ea6\u6cd5\u3001Q\u52a0\u6743\u6cd5\u3001\u90bb\u8fd1\u6cd5\u3001\u548c\u53cd\u5411\u4f20\u64ad\u65f6\u95f4\u6cd5\u3002\u5728\u7edf\u4e00\u7684NVIDIA Isaac Lab\u57fa\u51c6\u4e0a\uff08\u5305\u542b12\u4e2a\u591a\u6837\u5316\u673a\u5668\u4eba\u4efb\u52a1\uff09\uff0c\u5bf9\u4ee3\u8868\u6027\u7b97\u6cd5\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6db5\u76d6\u4efb\u52a1\u591a\u6837\u6027\u3001\u5e76\u884c\u5316\u80fd\u529b\u3001\u6269\u6563\u6b65\u957f\u53ef\u6269\u5c55\u6027\u3001\u8de8\u5177\u8eab\u6cdb\u5316\u80fd\u529b\u548c\u73af\u5883\u9c81\u68d2\u6027\u4e94\u4e2a\u7ef4\u5ea6\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5404\u7c7b\u7b97\u6cd5\u5bb6\u65cf\u5728\u6837\u672c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u57fa\u672c\u6743\u8861\uff0c\u8bc6\u522b\u4e86\u9650\u5236\u5728\u7ebf\u6269\u6563\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u8ba1\u7b97\u548c\u7b97\u6cd5\u74f6\u9888\u3002\u4e3a\u7279\u5b9a\u64cd\u4f5c\u7ea6\u675f\u4e0b\u7684\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002", "conclusion": "\u672c\u6587\u4e3a\u5728\u7ebf\u6269\u6563\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u4e3a\u63a8\u52a8\u66f4\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.06158", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06158", "abs": "https://arxiv.org/abs/2601.06158", "authors": ["Zibin Meng", "Kani Chen"], "title": "PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction", "comment": null, "summary": "Human-like agents require modeling how dispositions interact with social structure. We present PsyAgent, which couples a Big Five trait prior with Bourdieu's cognitive-social co-structure. PsyAgent comprises: (i) Individual Structure (IS), a machine-usable profile encoding traits and facets, cognitive style, values, cultural and educational capital, and salient life episodes; and (ii) Multi-Scenario Contexting (MSC), role-relationship-norm frames spanning eight arenas (work, family, friendship, strangers and civic life, solitude and self-regulation, romance, learning, and public expression). At inference, fixed structured prompts bind the active scenario to the agent profile, yielding behavior that is stable yet context-sensitive. We instantiate IS and MSC to synthesize supervision (role-play dialogues, decision probes, feedback trajectories) and then fine-tune a small LLM. The resulting model produces consistent, identifiable persona-aligned behaviors for specified Big Five configurations and matches or exceeds several larger untuned LLMs and other untuned baselines on our metrics: persona consistency, contextual appropriateness, style matching, trait identifiability, and long-horizon stability. Ablations show IS chiefly improves trait fidelity and stylistic stability, while MSC drives norm awareness and decision fit; both are necessary for cross-scenario performance. PsyAgent offers a precise, data-efficient architecture for personality-grounded agents.", "AI": {"tldr": "PsyAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u7279\u8d28\u548c\u793e\u4f1a\u8ba4\u77e5\u7ed3\u6784\u7684\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u4e2a\u4f53\u7ed3\u6784\u548c\u591a\u573a\u666f\u4e0a\u4e0b\u6587\u6846\u67b6\u751f\u6210\u7a33\u5b9a\u4e14\u60c5\u5883\u654f\u611f\u7684\u884c\u4e3a\u3002", "motivation": "\u4eba\u7c7b\u667a\u80fd\u4f53\u9700\u8981\u5efa\u6a21\u6027\u683c\u7279\u8d28\u5982\u4f55\u4e0e\u793e\u4f1a\u7ed3\u6784\u4e92\u52a8\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5c06\u4eba\u683c\u7279\u8d28\u4e0e\u793e\u4f1a\u8ba4\u77e5\u7ed3\u6784\u7cfb\u7edf\u7ed3\u5408\u7684\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u5927\u4e94\u4eba\u683c\u7279\u8d28\u548c\u5e03\u8fea\u5384\u7684\u793e\u4f1a\u8ba4\u77e5\u7ed3\u6784\u7406\u8bba\uff0c\u6784\u5efa\u4e2a\u4f53\u7ed3\u6784\uff08IS\uff09\u548c\u591a\u573a\u666f\u4e0a\u4e0b\u6587\uff08MSC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7ed1\u5b9a\u573a\u666f\u4e0e\u667a\u80fd\u4f53\u6863\u6848\uff0c\u751f\u6210\u76d1\u7763\u6570\u636e\u5e76\u5fae\u8c03\u5c0f\u578bLLM\u3002", "result": "\u6a21\u578b\u5728\u4eba\u683c\u4e00\u81f4\u6027\u3001\u60c5\u5883\u9002\u5f53\u6027\u3001\u98ce\u683c\u5339\u914d\u3001\u7279\u8d28\u53ef\u8bc6\u522b\u6027\u548c\u957f\u671f\u7a33\u5b9a\u6027\u7b49\u6307\u6807\u4e0a\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u591a\u4e2a\u66f4\u5927\u7684\u672a\u8c03\u4f18LLM\u548c\u5176\u4ed6\u57fa\u7ebf\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793aIS\u4e3b\u8981\u63d0\u5347\u7279\u8d28\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u7a33\u5b9a\u6027\uff0cMSC\u9a71\u52a8\u89c4\u8303\u610f\u8bc6\u548c\u51b3\u7b56\u9002\u5e94\u6027\u3002", "conclusion": "PsyAgent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cbe\u786e\u3001\u6570\u636e\u9ad8\u6548\u7684\u4eba\u683c\u57fa\u7840\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u80fd\u591f\u751f\u6210\u7a33\u5b9a\u4e14\u60c5\u5883\u654f\u611f\u7684\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2601.06160", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06160", "abs": "https://arxiv.org/abs/2601.06160", "authors": ["Dayu Wang", "Jiaye Yang", "Weikang Li", "Jiahui Liang", "Yang Li"], "title": "Student Guides Teacher: Weak-to-Strong Inference via Spectral Orthogonal Exploration", "comment": null, "summary": "While Large Language Models (LLMs) demonstrate near-human capabilities, they often suffer from \"Reasoning Collapse\" in complex mathematical proving and long-horizon planning. Models tend to degenerate into low-rank Bias Manifold, where stochastic sampling merely produces lexical variations of erroneous logic rather than semantic exploration. This geometric collapse renders the model \"blind\" to high-value solutions that lie within its Null Space. To address this, we propose Spectral Orthogonal Exploration (SOE), a geometric framework operating on a counter-intuitive \"Student Guides Teacher\" paradigm. Specifically, we utilize a weak auxiliary agent not for imitation, but as an orthogonal probe. By explicitly navigating the Teacher's Null Space, SOE serves as a geometric bridge, effectively ejecting the model from local optima to explore diverse, high-value solution spaces. Experiments on mathematical benchmarks demonstrate that, relative to baseline methods, our approach improves average accuracy by 62.4% and increases average sampling efficiency by 113.7%, indicating a promising path toward overcoming performance plateaus in advanced reasoning tasks.", "AI": {"tldr": "SOE\u6846\u67b6\u901a\u8fc7\"\u5b66\u751f\u5f15\u5bfc\u6559\u5e08\"\u8303\u5f0f\uff0c\u5229\u7528\u5f31\u8f85\u52a9\u4ee3\u7406\u4f5c\u4e3a\u6b63\u4ea4\u63a2\u9488\uff0c\u5728\u6559\u5e08\u6a21\u578b\u7684\u96f6\u7a7a\u95f4\u4e2d\u5bfc\u822a\uff0c\u89e3\u51b3LLM\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\"\u63a8\u7406\u5d29\u6e83\"\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u8bc1\u660e\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u91c7\u6837\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u8bc1\u660e\u548c\u957f\u7a0b\u89c4\u5212\u4efb\u52a1\u4e2d\u7ecf\u5e38\u51fa\u73b0\"\u63a8\u7406\u5d29\u6e83\"\u73b0\u8c61\uff0c\u6a21\u578b\u4f1a\u9000\u5316\u5230\u4f4e\u79e9\u504f\u7f6e\u6d41\u5f62\u4e2d\uff0c\u968f\u673a\u91c7\u6837\u4ec5\u4ea7\u751f\u9519\u8bef\u903b\u8f91\u7684\u8bcd\u6c47\u53d8\u4f53\u800c\u975e\u8bed\u4e49\u63a2\u7d22\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u53d1\u73b0\u96f6\u7a7a\u95f4\u4e2d\u7684\u9ad8\u4ef7\u503c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8c31\u6b63\u4ea4\u63a2\u7d22\uff08SOE\uff09\u51e0\u4f55\u6846\u67b6\uff0c\u91c7\u7528\"\u5b66\u751f\u5f15\u5bfc\u6559\u5e08\"\u7684\u53cd\u76f4\u89c9\u8303\u5f0f\u3002\u5229\u7528\u5f31\u8f85\u52a9\u4ee3\u7406\u4e0d\u662f\u7528\u4e8e\u6a21\u4eff\uff0c\u800c\u662f\u4f5c\u4e3a\u6b63\u4ea4\u63a2\u9488\uff0c\u901a\u8fc7\u663e\u5f0f\u5bfc\u822a\u6559\u5e08\u6a21\u578b\u7684\u96f6\u7a7a\u95f4\uff0c\u5c06\u6a21\u578b\u4ece\u5c40\u90e8\u6700\u4f18\u4e2d\u5f39\u51fa\uff0c\u63a2\u7d22\u591a\u6837\u5316\u7684\u9ad8\u4ef7\u503c\u89e3\u7a7a\u95f4\u3002", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0cSOE\u5c06\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e8662.4%\uff0c\u5e73\u5747\u91c7\u6837\u6548\u7387\u63d0\u9ad8\u4e86113.7%\uff0c\u8868\u660e\u5728\u514b\u670d\u9ad8\u7ea7\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u74f6\u9888\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "SOE\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u7684\u63a8\u7406\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u514b\u670d\u9ad8\u7ea7\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2601.07602", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.07602", "abs": "https://arxiv.org/abs/2601.07602", "authors": ["Bingxu Xiao", "Yunwei Dong", "Yiqi Tang", "Manqing Zhang", "Yifan Zhou", "Chunyan Ma", "Yepang Liu"], "title": "OODEval: Evaluating Large Language Models on Object-Oriented Design", "comment": "31 pages,8 figures,9 tables", "summary": "Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8629\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5411\u5bf9\u8c61\u8bbe\u8ba1\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u5728\u8bed\u6cd5\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bed\u4e49\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u65b9\u6cd5\u548c\u5173\u7cfb\u751f\u6210\uff09\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u6700\u4f73\u6a21\u578b\u63a5\u8fd1\u672c\u79d1\u751f\u5e73\u5747\u6c34\u5e73\u4f46\u4ecd\u8fdc\u4f4e\u4e8e\u4f18\u79c0\u4eba\u7c7b\u8bbe\u8ba1\u5e08\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u7ea7\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u8f6f\u4ef6\u8bbe\u8ba1\u80fd\u529b\u8bc4\u4f30\u3002\u7f3a\u4e4f\u9762\u5411\u5bf9\u8c61\u8bbe\u8ba1\u7684\u6807\u51c6\u5316\u57fa\u51c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u57fa\u51c6\uff1aOODEval\uff0850\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684OOD\u4efb\u52a1\uff09\u548cOODEval-Human\uff08940\u4e2a\u672c\u79d1\u751f\u63d0\u4ea4\u7684\u7c7b\u56fe\uff0c\u7531\u6559\u5e08\u8bc4\u5206\uff09\u3002\u63d0\u51fa\u4e86CLUE\u8bc4\u4f30\u6307\u6807\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7c7b\u56fe\u751f\u6210\u7684\u5168\u5c40\u6b63\u786e\u6027\u548c\u7ec6\u7c92\u5ea6\u8bbe\u8ba1\u8d28\u91cf\u3002\u8bc4\u4f30\u4e8629\u4e2aLLMs\uff0c\u7814\u7a76\u4e86\u4e94\u4e2a\u7814\u7a76\u95ee\u9898\u3002", "result": "LLMs\u5728\u8bed\u6cd5\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bed\u4e49\u8bbe\u8ba1\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u65b9\u6cd5\u548c\u5173\u7cfb\u751f\u6210\u65b9\u9762\u3002Qwen3-Coder-30B\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u4e0eDeepSeek-R1\u548cGPT-4o\u76f8\u5f53\u3002Gemma3-4B-IT\u867d\u7136\u53c2\u6570\u89c4\u6a21\u8f83\u5c0f\uff0c\u4f46\u4f18\u4e8eGPT-4o-Mini\u3002\u6700\u4f73LLMs\u63a5\u8fd1\u672c\u79d1\u751f\u5e73\u5747\u6c34\u5e73\uff0c\u4f46\u8fdc\u4f4e\u4e8e\u4f18\u79c0\u4eba\u7c7b\u8bbe\u8ba1\u5e08\u3002\u53c2\u6570\u89c4\u6a21\u3001\u4ee3\u7801\u4e13\u4e1a\u5316\u3001\u6307\u4ee4\u8c03\u4f18\u5bf9\u6027\u80fd\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u800c\u8bbe\u8ba1\u590d\u6742\u5ea6\u589e\u52a0\u548c\u9700\u6c42\u53ef\u8bfb\u6027\u964d\u4f4e\u4f1a\u635f\u5bb3\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u9762\u5411\u5bf9\u8c61\u8bbe\u8ba1\u4efb\u52a1\u4e0a\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u51c6\u786e\u8861\u91cf\u8f6f\u4ef6\u8bbe\u8ba1\u80fd\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u6539\u8fdbLLMs\u7684\u8bbe\u8ba1\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "topic": "swe benchmark"}}
{"id": "2601.07786", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.07786", "abs": "https://arxiv.org/abs/2601.07786", "authors": ["Abdullah Al Mujahid", "Mia Mohammad Imran"], "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt", "comment": "9th International Conference on Technical Debt (TechDebt 2026)", "summary": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e866,540\u6761\u5f15\u7528LLM\u7684\u4ee3\u7801\u6ce8\u91ca\uff0c\u8bc6\u522b\u51fa81\u6761\u540c\u65f6\u627f\u8ba4\u6280\u672f\u503a\u52a1\u7684\u6848\u4f8b\uff0c\u63ed\u793a\u4e86AI\u8f85\u52a9\u5f00\u53d1\u5982\u4f55\u5f71\u54cd\u6280\u672f\u503a\u52a1\u7684\u4ea7\u751f\u65f6\u673a\u548c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86GIST\uff08GenAI-Induced Self-admitted Technical debt\uff09\u6982\u5ff5\u6846\u67b6\u3002", "motivation": "\u968f\u7740ChatGPT\u3001Copilot\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88ab\u96c6\u6210\u5230\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u4e2d\uff0c\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u4ee3\u7801\u6ce8\u91ca\u4e2d\u7559\u4e0bAI\u53c2\u4e0e\u7684\u75d5\u8ff9\u3002\u5176\u4e2d\u4e00\u4e9b\u6ce8\u91ca\u65e2\u627f\u8ba4\u4f7f\u7528\u4e86\u751f\u6210\u5f0fAI\uff0c\u4e5f\u627f\u8ba4\u5b58\u5728\u6280\u672f\u7f3a\u9677\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3AI\u8f85\u52a9\u5982\u4f55\u5f71\u54cd\u6280\u672f\u503a\u52a1\u7684\u4ea7\u751f\u3002", "method": "\u5206\u67902022\u5e7411\u6708\u81f32025\u5e747\u6708\u671f\u95f4\u516c\u5f00\u7684Python\u548cJavaScript GitHub\u4ed3\u5e93\u4e2d\u76846,540\u6761\u5f15\u7528LLM\u7684\u4ee3\u7801\u6ce8\u91ca\uff0c\u8bc6\u522b\u5176\u4e2d\u540c\u65f6\u81ea\u6211\u627f\u8ba4\u6280\u672f\u503a\u52a1\uff08SATD\uff09\u7684\u6848\u4f8b\uff0c\u5171\u53d1\u73b081\u4e2a\u76f8\u5173\u5b9e\u4f8b\u3002", "result": "\u5f00\u53d1\u8005\u6700\u5e38\u63cf\u8ff0\u63a8\u8fdf\u6d4b\u8bd5\u3001\u4e0d\u5b8c\u5168\u9002\u914d\u4ee5\u53ca\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u6709\u9650\u7406\u89e3\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660eAI\u8f85\u52a9\u4e0d\u4ec5\u5f71\u54cd\u6280\u672f\u503a\u52a1\u7684\u51fa\u73b0\u65f6\u673a\uff0c\u4e5f\u6539\u53d8\u4e86\u5176\u4ea7\u751f\u539f\u56e0\u3002\u7814\u7a76\u63d0\u51fa\u4e86GIST\u6982\u5ff5\u6765\u63cf\u8ff0\u5f00\u53d1\u8005\u5728\u4f7f\u7528AI\u751f\u6210\u4ee3\u7801\u65f6\u660e\u786e\u8868\u8fbe\u5bf9\u5176\u884c\u4e3a\u6216\u6b63\u786e\u6027\u4e0d\u786e\u5b9a\u6027\u7684\u91cd\u590d\u6848\u4f8b\u3002", "conclusion": "AI\u8f85\u52a9\u5f00\u53d1\u5f15\u5165\u4e86\u65b0\u578b\u7684\u6280\u672f\u503a\u52a1\u6a21\u5f0f\uff0c\u9700\u8981\u65b0\u7684\u6982\u5ff5\u6846\u67b6\u6765\u7406\u89e3\u548c\u5e94\u5bf9\u3002GIST\u4f5c\u4e3a\u4e00\u4e2a\u6982\u5ff5\u900f\u955c\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u89e3\u51b3\u56e0AI\u751f\u6210\u4ee3\u7801\u7684\u4e0d\u786e\u5b9a\u6027\u800c\u4ea7\u751f\u7684\u6280\u672f\u503a\u52a1\u95ee\u9898\u3002", "topic": "swe application"}}
{"id": "2601.06341", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.06341", "abs": "https://arxiv.org/abs/2601.06341", "authors": ["Tara Bogavelli", "Oluwanifemi Bamgbose", "Gabrielle Gauthier Melan\u00e7on", "Fanny Riols", "Roshnee Sharma"], "title": "Evaluating Robustness of Large Language Models in Enterprise Applications: Benchmarks for Perturbation Consistency Across Formats and Languages", "comment": null, "summary": "Enterprise LLM applications require consistently high quality and reliable performance across diverse scenarios, demanding robustness to minor variations. Existing research shows that even small prompt changes can lead to substantial differences in output, but has mainly focused on a narrow set of perturbations with small academic datasets, limiting their relevance to real-world applications. To address this, we present a comprehensive benchmark suite that evaluates robustness across multiple perturbation types, including general text edits (e.g., punctuation, whitespace), formatting changes (e.g., JSON, YAML), multilingual and cross-lingual inputs, and positional variations in instructions. Evaluating 11 models ranging from 4B to 120B+ parameters, we find that minor perturbations reduce performance by up to 40 percentage points on key enterprise metrics. Critically, we demonstrate that the relationship between model size and robustness is more nuanced than conventional assumptions suggest: an 8B parameter model (Ministral 3 8B) outperforms most larger models, while another 8B model (Llama 3.1 8B) performs worst overall.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4f01\u4e1a\u7ea7LLM\u5e94\u7528\u5728\u5404\u79cd\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5fae\u5c0f\u6270\u52a8\u53ef\u4f7f\u5173\u952e\u6307\u6807\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe40%\uff0c\u4e14\u6a21\u578b\u5927\u5c0f\u4e0e\u9c81\u68d2\u6027\u7684\u5173\u7cfb\u6bd4\u4f20\u7edf\u5047\u8bbe\u66f4\u4e3a\u590d\u6742\u3002", "motivation": "\u4f01\u4e1a\u7ea7LLM\u5e94\u7528\u9700\u8981\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u4fdd\u6301\u9ad8\u8d28\u91cf\u548c\u53ef\u9760\u6027\u80fd\uff0c\u5bf9\u5fae\u5c0f\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6709\u9650\u7684\u6270\u52a8\u7c7b\u578b\u548c\u5c0f\u578b\u5b66\u672f\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u8bc4\u4f30\u591a\u79cd\u6270\u52a8\u7c7b\u578b\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5305\u62ec\uff1a\u901a\u7528\u6587\u672c\u7f16\u8f91\uff08\u6807\u70b9\u3001\u7a7a\u683c\uff09\u3001\u683c\u5f0f\u53d8\u5316\uff08JSON\u3001YAML\uff09\u3001\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u8f93\u5165\u3001\u6307\u4ee4\u4f4d\u7f6e\u53d8\u5316\u3002\u8bc4\u4f30\u4e8611\u4e2a\u53c2\u6570\u4ece4B\u5230120B+\u7684\u6a21\u578b\u3002", "result": "\u5fae\u5c0f\u6270\u52a8\u53ef\u4f7f\u5173\u952e\u4f01\u4e1a\u6307\u6807\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe40\u4e2a\u767e\u5206\u70b9\u3002\u6a21\u578b\u5927\u5c0f\u4e0e\u9c81\u68d2\u6027\u7684\u5173\u7cfb\u6bd4\u4f20\u7edf\u5047\u8bbe\u66f4\u4e3a\u590d\u6742\uff1a\u4e00\u4e2a8B\u53c2\u6570\u6a21\u578b\uff08Ministral 3 8B\uff09\u4f18\u4e8e\u5927\u591a\u6570\u66f4\u5927\u6a21\u578b\uff0c\u800c\u53e6\u4e00\u4e2a8B\u6a21\u578b\uff08Llama 3.1 8B\uff09\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "\u4f01\u4e1a\u7ea7LLM\u5e94\u7528\u9700\u8981\u66f4\u5168\u9762\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u6a21\u578b\u5927\u5c0f\u4e0d\u662f\u9c81\u68d2\u6027\u7684\u552f\u4e00\u51b3\u5b9a\u56e0\u7d20\uff0c\u7279\u5b9a\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u53ef\u80fd\u5bf9\u9c81\u68d2\u6027\u6709\u91cd\u8981\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2601.06189", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06189", "abs": "https://arxiv.org/abs/2601.06189", "authors": ["Atharv Naphade"], "title": "Rational Synthesizers or Heuristic Followers? Analyzing LLMs in RAG-based Question-Answering", "comment": "13 pages, 9 figures, ACL ARR submission", "summary": "Retrieval-Augmented Generation (RAG) is the prevailing paradigm for grounding Large Language Models (LLMs), yet the mechanisms governing how models integrate groups of conflicting retrieved evidence remain opaque. Does an LLM answer a certain way because the evidence is factually strong, because of a prior belief, or merely because it is repeated frequently? To answer this, we introduce GroupQA, a curated dataset of 1,635 controversial questions paired with 15,058 diversely-sourced evidence documents, annotated for stance and qualitative strength. Through controlled experiments, we characterize group-level evidence aggregation dynamics: Paraphrasing an argument can be more persuasive than providing distinct independent support; Models favor evidence presented first rather than last, and Larger models are increasingly resistant to adapt to presented evidence. Additionally, we find that LLM explanations to group-based answers are unfaithful. Together, we show that LLMs behave consistently as vulnerable heuristic followers, with direct implications for improving RAG system design.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLMs\u5728RAG\u7cfb\u7edf\u4e2d\u5982\u4f55\u6574\u5408\u51b2\u7a81\u8bc1\u636e\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u800c\u975e\u4e8b\u5b9e\u63a8\u7406\uff0c\u8bc1\u636e\u5448\u73b0\u987a\u5e8f\u548c\u91cd\u590d\u6bd4\u8bc1\u636e\u8d28\u91cf\u66f4\u91cd\u8981\uff0c\u4e14\u5927\u6a21\u578b\u66f4\u96be\u9002\u5e94\u65b0\u8bc1\u636e\u3002", "motivation": "RAG\u662f\u5f53\u524dLLMs\u63a5\u5730\u7684\u4e3b\u8981\u8303\u5f0f\uff0c\u4f46\u6a21\u578b\u5982\u4f55\u6574\u5408\u51b2\u7a81\u8bc1\u636e\u7684\u673a\u5236\u4e0d\u900f\u660e\u3002\u9700\u8981\u4e86\u89e3LLMs\u662f\u57fa\u4e8e\u4e8b\u5b9e\u5f3a\u5ea6\u3001\u5148\u9a8c\u4fe1\u5ff5\u8fd8\u662f\u91cd\u590d\u9891\u7387\u6765\u56de\u7b54\u95ee\u9898\u3002", "method": "\u5f15\u5165GroupQA\u6570\u636e\u96c6\uff0c\u5305\u542b1,635\u4e2a\u4e89\u8bae\u95ee\u9898\u548c15,058\u4e2a\u591a\u6837\u5316\u6765\u6e90\u7684\u8bc1\u636e\u6587\u6863\uff0c\u6807\u6ce8\u7acb\u573a\u548c\u8d28\u91cf\u5f3a\u5ea6\u3002\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u5206\u6790\u7fa4\u4f53\u7ea7\u8bc1\u636e\u805a\u5408\u52a8\u6001\u3002", "result": "\u53d1\u73b0\uff1a1) \u91cd\u8ff0\u8bba\u70b9\u6bd4\u63d0\u4f9b\u72ec\u7acb\u652f\u6301\u66f4\u6709\u8bf4\u670d\u529b\uff1b2) \u6a21\u578b\u504f\u597d\u6700\u5148\u5448\u73b0\u7684\u8bc1\u636e\u800c\u975e\u6700\u540e\uff1b3) \u5927\u6a21\u578b\u8d8a\u6765\u8d8a\u96be\u9002\u5e94\u5448\u73b0\u7684\u8bc1\u636e\uff1b4) LLMs\u5bf9\u7fa4\u4f53\u7b54\u6848\u7684\u89e3\u91ca\u4e0d\u5fe0\u5b9e\u3002", "conclusion": "LLMs\u8868\u73b0\u4e3a\u8106\u5f31\u7684\u542f\u53d1\u5f0f\u8ffd\u968f\u8005\uff0c\u8fd9\u5bf9\u6539\u8fdbRAG\u7cfb\u7edf\u8bbe\u8ba1\u6709\u76f4\u63a5\u610f\u4e49\u3002\u9700\u8981\u8bbe\u8ba1\u66f4\u597d\u7684\u8bc1\u636e\u6574\u5408\u673a\u5236\u6765\u514b\u670d\u8fd9\u4e9b\u542f\u53d1\u5f0f\u504f\u5dee\u3002", "topic": "agent analysis"}}
{"id": "2601.07504", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.07504", "abs": "https://arxiv.org/abs/2601.07504", "authors": ["Tzu-Hsuan Lin", "Chih-Hsuan Kao"], "title": "FROAV: A Framework for RAG Observation and Agent Verification - Lowering the Barrier to LLM Agent Research", "comment": "8 pages, 1 figure, 3 tables", "summary": "The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous \"LLM-as-a-Judge\" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.", "AI": {"tldr": "FROAV\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u5e73\u53f0\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u548c\u53ef\u6269\u5c55Python\u96c6\u6210\uff0c\u964d\u4f4eLLM\u667a\u80fd\u4f53\u7814\u7a76\u7684\u95e8\u69db\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u65e0\u9700\u7f16\u5199\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u5373\u53ef\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30RAG\u7b56\u7565\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6587\u6863\u5206\u6790\u3001\u51b3\u7b56\u652f\u6301\u548c\u77e5\u8bc6\u68c0\u7d22\u65b9\u9762\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u8fed\u4ee3\u7684\u590d\u6742\u6027\u7ed9\u7f3a\u4e4f\u8f6f\u4ef6\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u7684\u7814\u7a76\u4eba\u5458\u5e26\u6765\u4e86\u663e\u8457\u969c\u788d\u3002", "method": "FROAV\u91c7\u7528\u591a\u9636\u6bb5RAG\u7ba1\u9053\u4e0e\"LLM-as-a-Judge\"\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7n8n\u8fdb\u884c\u65e0\u4ee3\u7801\u5de5\u4f5c\u6d41\u8bbe\u8ba1\uff0cPostgreSQL\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6570\u636e\u7ba1\u7406\uff0cFastAPI\u63d0\u4f9b\u7075\u6d3b\u540e\u7aef\u903b\u8f91\uff0cStreamlit\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u3002", "result": "\u8be5\u6846\u67b6\u5728\u91d1\u878d\u6587\u6863\u5206\u6790\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u6027\uff0c\u5176\u6750\u6599\u65e0\u5173\u67b6\u6784\u53ef\u9002\u5e94\u4efb\u4f55\u9700\u8981\u8bed\u4e49\u5206\u6790\u7684\u9886\u57df\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u4e13\u6ce8\u4e8e\u5047\u8bbe\u6d4b\u8bd5\u548c\u7b97\u6cd5\u521b\u65b0\u800c\u975e\u7cfb\u7edf\u96c6\u6210\u6311\u6218\u3002", "conclusion": "FROAV\u4ee3\u8868\u4e86\u5411\u66f4\u5e7f\u6cdb\u79d1\u5b66\u793e\u533a\u5f00\u653eLLM\u667a\u80fd\u4f53\u7814\u7a76\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u901a\u8fc7\u5373\u63d2\u5373\u7528\u67b6\u6784\u6c11\u4e3b\u5316LLM\u667a\u80fd\u4f53\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2601.06407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06407", "abs": "https://arxiv.org/abs/2601.06407", "authors": ["Yijiang River Dong", "Tiancheng Hu", "Zheng Hui", "Caiqi Zhang", "Ivan Vuli\u0107", "Andreea Bobu", "Nigel Collier"], "title": "Value of Information: A Framework for Human-Agent Communication", "comment": null, "summary": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c(VoI)\u7684\u51b3\u7b56\u7406\u8bba\u6846\u67b6\uff0c\u8ba9LLM\u667a\u80fd\u4f53\u80fd\u5728\u4fe1\u606f\u4e0d\u8db3\u65f6\u52a8\u6001\u6743\u8861\u8be2\u95ee\u7528\u6237\u6f84\u6e05\u95ee\u9898\u7684\u6536\u76ca\u4e0e\u8ba4\u77e5\u6210\u672c\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u8c03\u4f18\u5373\u53ef\u81ea\u9002\u5e94\u4e0d\u540c\u573a\u666f\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0c\u7528\u6237\u8bf7\u6c42\u901a\u5e38\u4e0d\u5b8c\u6574\uff0c\u667a\u80fd\u4f53\u9762\u4e34\u4e24\u96be\uff1a\u8981\u4e48\u57fa\u4e8e\u4e0d\u5b8c\u6574\u4fe1\u606f\u884c\u52a8\uff0c\u8981\u4e48\u6253\u65ad\u7528\u6237\u6f84\u6e05\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8106\u5f31\u7684\u7f6e\u4fe1\u5ea6\u9608\u503c\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\uff0c\u8981\u4e48\u65e0\u6cd5\u8003\u8651\u4e0d\u540c\u51b3\u7b56\u7684\u98ce\u9669\u5dee\u5f02\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c(VoI)\u7684\u51b3\u7b56\u7406\u8bba\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8ba1\u7b97\u8be2\u95ee\u95ee\u9898\u7684\u9884\u671f\u6548\u7528\u589e\u76ca\u4e0e\u7528\u6237\u8ba4\u77e5\u6210\u672c\u7684\u6743\u8861\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u53ef\u81ea\u9002\u5e94\u4e0d\u540c\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\uff0820 Questions\u6e38\u620f\u3001\u533b\u7597\u8bca\u65ad\u3001\u822a\u73ed\u9884\u8ba2\u3001\u7535\u5b50\u5546\u52a1\uff09\u7684\u5b9e\u9a8c\u4e2d\uff0cVoI\u65b9\u6cd5\u59cb\u7ec8\u5339\u914d\u6216\u8d85\u8fc7\u6700\u4f73\u624b\u52a8\u8c03\u4f18\u57fa\u7ebf\uff0c\u5728\u9ad8\u6210\u672c\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u9ad8\u8fbe1.36\u4e2a\u6548\u7528\u70b9\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u901a\u4fe1\uff0c\u660e\u786e\u5e73\u8861\u4efb\u52a1\u98ce\u9669\u3001\u67e5\u8be2\u6a21\u7cca\u6027\u548c\u7528\u6237\u52aa\u529b\uff0c\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u6838\u5fc3\u56f0\u5883\u3002", "topic": "agent analysis"}}
{"id": "2601.06411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06411", "abs": "https://arxiv.org/abs/2601.06411", "authors": ["Zhengxuan Lu", "Dongfang Li", "Yukun Shi", "Beilun Wang", "Longyue Wang", "Baotian Hu"], "title": "Structured Episodic Event Memory", "comment": null, "summary": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.", "AI": {"tldr": "SEEM\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u60c5\u666f\u4e8b\u4ef6\u8bb0\u5fc6\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u8bb0\u5fc6\u5c42\u548c\u52a8\u6001\u60c5\u666f\u8bb0\u5fc6\u5c42\uff0c\u901a\u8fc7\u8ba4\u77e5\u6846\u67b6\u7406\u8bba\u5c06\u4ea4\u4e92\u6d41\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u4e8b\u4ef6\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u4ee3\u7406\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524dLLM\u8bb0\u5fc6\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u5b58\u5728\u68c0\u7d22\u5206\u6563\u3001\u65e0\u6cd5\u6355\u6349\u590d\u6742\u63a8\u7406\u6240\u9700\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\u7684\u95ee\u9898\u3002\u5bf9\u4e8e\u81ea\u4e3b\u4ee3\u7406\u800c\u8a00\uff0c\u8fd9\u4e9b\u88ab\u52a8\u6241\u5e73\u67b6\u6784\u7f3a\u4e4f\u5bf9\u957f\u671f\u4ea4\u4e92\u52a8\u6001\u5173\u8054\u6027\u7684\u8ba4\u77e5\u7ec4\u7ec7\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u60c5\u666f\u4e8b\u4ef6\u8bb0\u5fc6(SEEM)\u6846\u67b6\uff1a1) \u7ed3\u5408\u56fe\u8bb0\u5fc6\u5c42\u5b58\u50a8\u5173\u7cfb\u4e8b\u5b9e\u548c\u52a8\u6001\u60c5\u666f\u8bb0\u5fc6\u5c42\u8bb0\u5f55\u53d9\u4e8b\u8fdb\u5c55\uff1b2) \u57fa\u4e8e\u8ba4\u77e5\u6846\u67b6\u7406\u8bba\u5c06\u4ea4\u4e92\u6d41\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u60c5\u666f\u4e8b\u4ef6\u6846\u67b6(EEFs)\uff0c\u5305\u542b\u7cbe\u786e\u6765\u6e90\u6307\u9488\uff1b3) \u5f15\u5165\u4ee3\u7406\u5173\u8054\u878d\u5408\u548c\u53cd\u5411\u6765\u6e90\u6269\u5c55(RPE)\u673a\u5236\uff0c\u4ece\u788e\u7247\u5316\u8bc1\u636e\u91cd\u5efa\u8fde\u8d2f\u53d9\u4e8b\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSEEM\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u4fdd\u6301\u66f4\u4f18\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "conclusion": "SEEM\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u5c42\u8bb0\u5fc6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u7ec4\u7ec7\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u548c\u957f\u671f\u4ea4\u4e92\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.06328", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06328", "abs": "https://arxiv.org/abs/2601.06328", "authors": ["Ziqiao Xi", "Shuang Liang", "Qi Liu", "Jiaqing Zhang", "Letian Peng", "Fang Nan", "Meshal Nayim", "Tianhui Zhang", "Rishika Mundada", "Lianhui Qin", "Biwei Huang", "Kun Zhou"], "title": "ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation", "comment": "Submitted to ACL 2026 12 pages, 4 figures Ziqiao Xi and Shuang Liang contributed equally to this work", "summary": "Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u653e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u73af\u5883\uff0c\u5305\u542b5,571\u4e2a\u7edf\u4e00\u683c\u5f0f\u7684\u5de5\u5177\u548c204\u4e2a\u5e38\u7528\u5e94\u7528\uff0c\u901a\u8fc7\u4efb\u52a1\u521b\u5efa\u5f15\u64ce\u5408\u6210\u957f\u89c6\u91ce\u3001\u591a\u5de5\u5177\u5de5\u4f5c\u6d41\uff0c\u5e76\u5f15\u5165\u72b6\u6001\u63a7\u5236\u5668\u6d4b\u8bd5\u9c81\u68d2\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u89c4\u5212-\u6267\u884c\u5206\u79bb\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u8bc4\u4f30\u53d1\u73b0\u73b0\u6709LLMs\u5728\u5de5\u5177\u89c4\u5212\u4e0e\u6267\u884c\u80fd\u529b\u3001\u7ea6\u675f\u9075\u5faa\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cDeepSeek-v3.2\u9c81\u68d2\u6027\u6700\u5f3a\u3002\u4f7f\u7528\u73af\u5883\u6570\u636e\u5fae\u8c03LLMs\uff0c\u4ec5\u75281,170\u6761\u8f68\u8ff9\u5c31\u8d85\u8d8a\u4e86\u4f7f\u7528119k\u6837\u672c\u7684\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528\u5de5\u5177\u7684LLM\u4ee3\u7406\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u5927\u578b\u5de5\u5177\u6c60\u3001\u957f\u89c6\u91ce\u76ee\u6807\u3001\u590d\u6742\u7ea6\u675f\u548c\u4e0d\u53ef\u9760\u5de5\u5177\u72b6\u6001\u7b49\u95ee\u9898\u3002\u9700\u8981\u53ef\u6269\u5c55\u4e14\u73b0\u5b9e\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u73af\u5883\u6765\u63a8\u52a8\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "method": "1) \u6784\u5efa\u5f00\u653e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u73af\u5883\uff1a\u5305\u542b5,571\u4e2a\u7edf\u4e00\u683c\u5f0f\u5de5\u5177\u548c204\u4e2a\u5e38\u7528\u5e94\u7528\uff1b2) \u4efb\u52a1\u521b\u5efa\u5f15\u64ce\uff1a\u5408\u6210\u957f\u89c6\u91ce\u3001\u591a\u5de5\u5177\u5de5\u4f5c\u6d41\u5e76\u52a0\u5165\u590d\u6742\u7ea6\u675f\uff1b3) \u72b6\u6001\u63a7\u5236\u5668\uff1a\u6ce8\u5165\u4e2d\u65ad\u548c\u6545\u969c\u4ee5\u6d4b\u8bd5\u9c81\u68d2\u6027\uff1b4) \u89c4\u5212-\u6267\u884c\u4ee3\u7406\u6846\u67b6\uff1a\u91c7\u7528\u89c4\u5212\u5668-\u6267\u884c\u5668\u5206\u89e3\uff0c\u5206\u79bb\u6df1\u601d\u719f\u8651\u7684\u63a8\u7406\u548c\u81ea\u6211\u7ea0\u6b63\u4e0e\u9010\u6b65\u6267\u884c\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff1a1) \u73b0\u6709LLMs\u5728\u5de5\u5177\u89c4\u5212\u4e0e\u6267\u884c\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\uff1b2) \u73b0\u6709LLMs\u5728\u7ea6\u675f\u9075\u5faa\u65b9\u9762\u5b58\u5728\u5f31\u70b9\uff1b3) DeepSeek-v3.2\u5c55\u73b0\u51fa\u6700\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u4f7f\u7528\u73af\u5883\u6536\u96c6\u76841,170\u6761\u8f68\u8ff9\u5fae\u8c03LLMs\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u4f7f\u7528119k\u6837\u672c\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u73af\u5883\u65e2\u53ef\u4f5c\u4e3a\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u73b0\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u6570\u636e\u751f\u6210\u5f15\u64ce\u3002\u89c4\u5212-\u6267\u884c\u5206\u79bb\u7684\u4ee3\u7406\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u5f00\u653e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u6311\u6218\u3002\u73af\u5883\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u4f7f\u5f97\u5c0f\u6837\u672c\u5fae\u8c03\u5c31\u80fd\u83b7\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.06424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06424", "abs": "https://arxiv.org/abs/2601.06424", "authors": ["Sazia Tabasum Mim", "Jack Morris", "Manish Dhakal", "Yanming Xiu", "Maria Gorlatova", "Yi Ding"], "title": "Can a Unimodal Language Agent Provide Preferences to Tune a Multimodal Vision-Language Model?", "comment": "Accepted to IJCNLP-AACL 2025 Findings", "summary": "To explore a more scalable path for adding multimodal capabilities to existing LLMs, this paper addresses a fundamental question: Can a unimodal LLM, relying solely on text, reason about its own informational needs and provide effective feedback to optimize a multimodal model? To answer this, we propose a method that enables a language agent to give feedback to a vision-language model (VLM) to adapt text generation to the agent's preferences. Our results from different experiments affirm this hypothesis, showing that LLM preference feedback significantly enhances VLM descriptions. Using our proposed method, we find that the VLM can generate multimodal scene descriptions to help the LLM better understand multimodal context, leading to improvements of maximum 13% in absolute accuracy compared to the baseline multimodal approach. Furthermore, a human study validated our AI-driven feedback, showing a 64.6% preference alignment rate between the LLM's choices and human judgments. Extensive experiments provide insights on how and why the method works and its limitations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u8ba9\u5355\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63d0\u4f9b\u53cd\u9988\uff0c\u4f18\u5316\u591a\u6a21\u6001\u63cf\u8ff0\u751f\u6210\uff0c\u4ece\u800c\u589e\u5f3aLLM\u5bf9\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u3002", "motivation": "\u63a2\u7d22\u4e3a\u73b0\u6709LLM\u6dfb\u52a0\u591a\u6a21\u6001\u80fd\u529b\u7684\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u7814\u7a76\u5355\u6a21\u6001LLM\u80fd\u5426\u4ec5\u901a\u8fc7\u6587\u672c\u6765\u63a8\u7406\u81ea\u8eab\u4fe1\u606f\u9700\u6c42\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u6709\u6548\u53cd\u9988\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u4f7f\u8bed\u8a00\u667a\u80fd\u4f53\u80fd\u591f\u5411\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63d0\u4f9b\u53cd\u9988\uff0c\u6839\u636e\u667a\u80fd\u4f53\u504f\u597d\u8c03\u6574\u6587\u672c\u751f\u6210\u3002\u901a\u8fc7LLM\u504f\u597d\u53cd\u9988\u4f18\u5316VLM\u7684\u591a\u6a21\u6001\u573a\u666f\u63cf\u8ff0\u751f\u6210\u3002", "result": "LLM\u504f\u597d\u53cd\u9988\u663e\u8457\u63d0\u5347VLM\u63cf\u8ff0\u8d28\u91cf\uff0c\u76f8\u6bd4\u57fa\u7ebf\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7edd\u5bf9\u51c6\u786e\u7387\u6700\u5927\u63d0\u534713%\u3002\u4eba\u7c7b\u7814\u7a76\u663e\u793aLLM\u9009\u62e9\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u504f\u597d\u5bf9\u9f50\u7387\u8fbe\u523064.6%\u3002", "conclusion": "\u5355\u6a21\u6001LLM\u80fd\u591f\u6709\u6548\u6307\u5bfcVLM\u4f18\u5316\u591a\u6a21\u6001\u63cf\u8ff0\u751f\u6210\uff0c\u4e3aLLM\u6dfb\u52a0\u591a\u6a21\u6001\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u4f46\u65b9\u6cd5\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.06180", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06180", "abs": "https://arxiv.org/abs/2601.06180", "authors": ["Saki Imai", "Pedram Heydari", "Anthony Sicilia", "Asteria Kaeberlein", "Katherine Atwell", "Malihe Alikhani"], "title": "MixDPO: Modeling Preference Strength for Pluralistic Alignment", "comment": null, "summary": "Preference based alignment objectives implicitly assume that all human preferences are expressed with equal strength. In practice, however, preference strength varies across individuals and contexts -- a phenomenon established in behavioral economics and discrete choice theory. This mismatch limits the ability of existing objectives to faithfully capture heterogeneous human judgments. Inspired by this literature, we introduce Mixed Logit Direct Preference Optimization (MixDPO), a generalization of Direct Preference Optimization that models variation in preference strength. MixDPO enables alignment objectives to capture heterogeneity in how strongly preferences are expressed across training examples. We evaluate MixDPO on three preference datasets using two open-weight language models. Across datasets, MixDPO improves aggregate alignment performance (+11.2 points on Pythia-2.8B) while preserving subgroup level preferences, with the largest gains appearing in settings with higher inferred preference heterogeneity. MixDPO makes preference heterogeneity explicit through learned strength distributions. We release our code for reproducibility.", "AI": {"tldr": "MixDPO\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u504f\u597d\u5f3a\u5ea6\u7684\u5f02\u8d28\u6027\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u4eba\u7c7b\u504f\u597d\u8868\u8fbe\u5f3a\u5ea6\u76f8\u540c\uff0c\u4f46\u5b9e\u9645\u4e2d\u504f\u597d\u5f3a\u5ea6\u5b58\u5728\u4e2a\u4f53\u548c\u60c5\u5883\u5dee\u5f02\uff0c\u8fd9\u79cd\u4e0d\u5339\u914d\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u6355\u6349\u5f02\u8d28\u6027\u4eba\u7c7b\u5224\u65ad\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faMixed Logit Direct Preference Optimization (MixDPO)\uff0c\u8fd9\u662fDirect Preference Optimization\u7684\u6cdb\u5316\u7248\u672c\uff0c\u901a\u8fc7\u5efa\u6a21\u504f\u597d\u5f3a\u5ea6\u7684\u53d8\u5316\u6765\u6355\u6349\u8bad\u7ec3\u6837\u672c\u4e2d\u504f\u597d\u8868\u8fbe\u7684\u5f02\u8d28\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u504f\u597d\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cMixDPO\u5728Pythia-2.8B\u4e0a\u63d0\u5347\u4e8611.2\u4e2a\u70b9\u7684\u805a\u5408\u5bf9\u9f50\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b50\u7ec4\u7ea7\u522b\u7684\u504f\u597d\uff0c\u5728\u63a8\u65ad\u504f\u597d\u5f02\u8d28\u6027\u66f4\u9ad8\u7684\u8bbe\u7f6e\u4e2d\u6539\u8fdb\u6700\u5927\u3002", "conclusion": "MixDPO\u901a\u8fc7\u5b66\u4e60\u7684\u5f3a\u5ea6\u5206\u5e03\u4f7f\u504f\u597d\u5f02\u8d28\u6027\u663e\u5f0f\u5316\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u4eba\u7c7b\u504f\u597d\u7684\u5f02\u8d28\u6027\u8868\u8fbe\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.06377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06377", "abs": "https://arxiv.org/abs/2601.06377", "authors": ["Ningning Zhang", "Xingxing Yang", "Zhizhong Tan", "Weiping Deng", "Wenyong Wang"], "title": "HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents", "comment": null, "summary": "Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.", "AI": {"tldr": "HiMem\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u5bf9\u8bdd\u7684\u5206\u5c42\u957f\u671f\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6-\u7b14\u8bb0\u53cc\u901a\u9053\u8bb0\u5fc6\u6784\u5efa\u3001\u8bed\u4e49\u94fe\u63a5\u548c\u51b2\u7a81\u611f\u77e5\u8bb0\u5fc6\u91cd\u7ec4\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u81ea\u6211\u8fdb\u5316\u7684\u5bf9\u8bdd\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u8bb0\u5fc6\u7cfb\u7edf\u5728\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6301\u7eed\u4ea4\u4e92\u4e0b\u7684\u81ea\u6211\u8fdb\u5316\u65b9\u9762\u5b58\u5728\u660e\u663e\u9650\u5236\uff0c\u9700\u8981\u66f4\u7b26\u5408\u8ba4\u77e5\u7406\u8bba\u7684\u8bbe\u8ba1\u6765\u652f\u6301\u957f\u5bf9\u8bdd\u573a\u666f\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8bb0\u5fc6\u6846\u67b6\uff1a1) \u901a\u8fc7\u4e3b\u9898\u611f\u77e5\u7684\u4e8b\u4ef6-\u60ca\u559c\u53cc\u901a\u9053\u5206\u5272\u6784\u5efa\u60c5\u8282\u8bb0\u5fc6\uff1b2) \u901a\u8fc7\u591a\u9636\u6bb5\u4fe1\u606f\u63d0\u53d6\u6784\u5efa\u7b14\u8bb0\u8bb0\u5fc6\uff1b3) \u8bed\u4e49\u94fe\u63a5\u5f62\u6210\u5c42\u6b21\u7ed3\u6784\uff1b4) \u652f\u6301\u6df7\u5408\u548c\u5c3d\u529b\u800c\u4e3a\u68c0\u7d22\u7b56\u7565\uff1b5) \u51b2\u7a81\u611f\u77e5\u8bb0\u5fc6\u91cd\u7ec4\u673a\u5236\u3002", "result": "\u5728\u957f\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHiMem\u5728\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u957f\u671f\u63a8\u7406\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u6548\u7387\u3002", "conclusion": "HiMem\u4e3a\u6784\u5efa\u81ea\u9002\u5e94\u548c\u81ea\u6211\u8fdb\u5316\u7684\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2601.06498", "categories": ["cs.CL", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2601.06498", "abs": "https://arxiv.org/abs/2601.06498", "authors": ["Minghui Jia", "Qichao Zhang", "Ali Luo", "Linjing Li", "Shuo Ye", "Hailing Lu", "Wen Hou", "Dongbin Zhao"], "title": "Spec-o3: A Tool-Augmented Vision-Language Agent for Rare Celestial Object Candidate Vetting via Automated Spectral Inspection", "comment": null, "summary": "Due to the limited generalization and interpretability of deep learning classifiers, The final vetting of rare celestial object candidates still relies on expert visual inspection--a manually intensive process. In this process, astronomers leverage specialized tools to analyze spectra and construct reliable catalogs. However, this practice has become the primary bottleneck, as it is fundamentally incapable of scaling with the data deluge from modern spectroscopic surveys. To bridge this gap, we propose Spec-o3, a tool-augmented vision-language agent that performs astronomer-aligned spectral inspection via interleaved multimodal chain-of-thought reasoning. Spec-o3 is trained with a two-stage post-training recipe: cold-start supervised fine-tuning on expert inspection trajectories followed by outcome-based reinforcement learning on rare-type verification tasks. Evaluated on five rare-object identification tasks from LAMOST, Spec-o3 establishes a new State-of-the-Art, boosting the macro-F1 score from 28.3 to 76.5 with a 7B parameter base model and outperforming both proprietary VLMs and specialized deep models. Crucially, the agent demonstrates strong generalization to unseen inspection tasks across survey shifts (from LAMOST to SDSS/DESI). Expert evaluations confirm that its reasoning traces are coherent and physically consistent, supporting transparent and trustworthy decision-making. Code, data, and models are available at \\href{https://github.com/Maxwell-Jia/spec-o3}{Project HomePage}.", "AI": {"tldr": "Spec-o3\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u94fe\u5f0f\u63a8\u7406\u8fdb\u884c\u5929\u6587\u5b66\u5bb6\u5bf9\u9f50\u7684\u5149\u8c31\u68c0\u67e5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f55\u89c1\u5929\u4f53\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u5728\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7f55\u89c1\u5929\u4f53\u5019\u9009\u8005\u7684\u6700\u7ec8\u5ba1\u67e5\u4ecd\u4f9d\u8d56\u4e13\u5bb6\u89c6\u89c9\u68c0\u67e5\uff0c\u8fd9\u4e00\u4eba\u5de5\u5bc6\u96c6\u578b\u8fc7\u7a0b\u65e0\u6cd5\u5e94\u5bf9\u73b0\u4ee3\u5149\u8c31\u5de1\u5929\u6570\u636e\u6d2a\u6d41\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faSpec-o3\u5de5\u5177\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff0c\u91c7\u7528\u4ea4\u9519\u591a\u6a21\u6001\u94fe\u5f0f\u63a8\u7406\u3002\u4f7f\u7528\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u65b9\u6cd5\uff1a\u57fa\u4e8e\u4e13\u5bb6\u68c0\u67e5\u8f68\u8ff9\u7684\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u5728\u7f55\u89c1\u7c7b\u578b\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u8fdb\u884c\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728LAMOST\u7684\u4e94\u4e2a\u7f55\u89c1\u5929\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0cSpec-o3\u5efa\u7acb\u4e86\u65b0\u7684SOTA\uff0c\u5c06macro-F1\u5206\u6570\u4ece28.3\u63d0\u5347\u523076.5\uff08\u4f7f\u75287B\u53c2\u6570\u57fa\u7840\u6a21\u578b\uff09\uff0c\u4f18\u4e8e\u4e13\u6709VLMs\u548c\u4e13\u7528\u6df1\u5ea6\u6a21\u578b\u3002\u5728\u8de8\u5de1\u5929\uff08\u4eceLAMOST\u5230SDSS/DESI\uff09\u7684\u672a\u89c1\u68c0\u67e5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Spec-o3\u901a\u8fc7\u5929\u6587\u5b66\u5bb6\u5bf9\u9f50\u7684\u5149\u8c31\u68c0\u67e5\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u53ef\u4fe1\u7684\u51b3\u7b56\uff0c\u5176\u63a8\u7406\u8f68\u8ff9\u8fde\u8d2f\u4e14\u7269\u7406\u4e00\u81f4\uff0c\u4e3a\u89e3\u51b3\u5149\u8c31\u6570\u636e\u6d2a\u6d41\u4e2d\u7684\u4e13\u5bb6\u74f6\u9888\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.06423", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06423", "abs": "https://arxiv.org/abs/2601.06423", "authors": ["Deep Mehta"], "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs", "comment": "24 pages, 3 figures, 9 tables", "summary": "Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness?\n  We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency.\n  GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212).\n  Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.", "AI": {"tldr": "\u7814\u7a76\u81ea\u6d3d\u6027\uff08self-consistency\uff09\u6280\u672f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5fe0\u5b9e\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u81ea\u6d3d\u6027\u5e76\u975e\u666e\u904d\u6709\u76ca", "motivation": "\u81ea\u6d3d\u6027\u6280\u672f\u80fd\u63d0\u9ad8LLM\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u79cd\u63d0\u5347\u662f\u5426\u771f\u6b63\u6539\u5584\u4e86\u63a8\u7406\u8d28\u91cf\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u63a8\u7406\u6269\u5c55\u662f\u5426\u63d0\u9ad8\u63a8\u7406\u5fe0\u5b9e\u5ea6\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u3002", "method": "\u5728\u56db\u4e2a\u524d\u6cbf\u6a21\u578b\uff08GPT-5.2\u3001Claude Opus 4.5\u3001Gemini-3-flash-preview\u3001DeepSeek-v3.2\uff09\u4e0a\u5bf9100\u4e2aGSM8K\u6570\u5b66\u63a8\u7406\u95ee\u9898\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u4f7f\u7528bootstrap\u7f6e\u4fe1\u533a\u95f4\u3001McNemar\u914d\u5bf9\u68c0\u9a8c\u548cCohen's d\u6548\u5e94\u91cf\u8fdb\u884c\u91cf\u5316\u5206\u6790\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1aGPT-5.2\u51c6\u786e\u7387\u4ece78%\u63d0\u5347\u523090%\uff0c\u5fe0\u5b9e\u5ea6\u76f8\u5bf9\u7a33\u5b9a\uff1bClaude Opus 4.5\u51c6\u786e\u7387\u53cd\u800c\u4ece78%\u4e0b\u964d\u523074.3%\uff0c\u4f46\u5fe0\u5b9e\u5ea6\u5927\u5e45\u63d0\u5347\uff1bDeepSeek-v3.2\u56e0\u5df2\u8fbe98%\u51c6\u786e\u7387\u51fa\u73b0\u5929\u82b1\u677f\u6548\u5e94\uff1bGemini-3-flash\u51c6\u786e\u7387\u63d0\u5347\u4f46\u5fe0\u5b9e\u5ea6\u7565\u6709\u4e0b\u964d\u3002", "conclusion": "\u81ea\u6d3d\u6027\u6280\u672f\u5e76\u975e\u666e\u904d\u6709\u76ca\uff0c\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002\u5b9e\u8df5\u8005\u5e94\u5728\u90e8\u7f72\u524d\u6d4b\u8bd5\u7279\u5b9a\u6a21\u578b\uff0c\u5e76\u8003\u8651\u51c6\u786e\u7387\u4e0e\u5fe0\u5b9e\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2601.06453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06453", "abs": "https://arxiv.org/abs/2601.06453", "authors": ["Hyungjun Yoon", "Mohammad Malekzadeh", "Sung-Ju Lee", "Fahim Kawsar", "Lorena Qendro"], "title": "ConSensus: Multi-Agent Collaboration for Multimodal Sensing", "comment": "17 pages, 6 figures, 5 tables", "summary": "Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.", "AI": {"tldr": "ConSensus\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6a21\u6001\u611f\u77e5\u667a\u80fd\u4f53\u5206\u89e3\u591a\u6a21\u6001\u611f\u77e5\u4efb\u52a1\uff0c\u91c7\u7528\u6df7\u5408\u878d\u5408\u673a\u5236\u5e73\u8861\u8bed\u4e49\u805a\u5408\u4e0e\u7edf\u8ba1\u5171\u8bc6\uff0c\u5728\u4e94\u4e2a\u591a\u6a21\u6001\u611f\u77e5\u57fa\u51c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53477.1%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5f02\u6784\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u65f6\u5b58\u5728\u6311\u6218\uff0c\u5355\u4e00\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u8de8\u6a21\u6001\u8fdb\u884c\u8fde\u8d2f\u63a8\u7406\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0d\u5b8c\u6574\u548c\u5148\u9a8c\u77e5\u8bc6\u504f\u5dee\u3002\u9700\u8981\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u7684\u591a\u6a21\u6001\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faConSensus\u6846\u67b6\uff1a1\uff09\u5c06\u591a\u6a21\u6001\u611f\u77e5\u4efb\u52a1\u5206\u89e3\u4e3a\u4e13\u95e8\u7684\u6a21\u6001\u611f\u77e5\u667a\u80fd\u4f53\uff1b2\uff09\u91c7\u7528\u6df7\u5408\u878d\u5408\u673a\u5236\uff0c\u7ed3\u5408\u8bed\u4e49\u805a\u5408\uff08\u652f\u6301\u8de8\u6a21\u6001\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\uff09\u4e0e\u7edf\u8ba1\u5171\u8bc6\uff08\u901a\u8fc7\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u63d0\u4f9b\u9c81\u68d2\u6027\uff09\uff1b3\uff09\u5355\u8f6e\u6df7\u5408\u878d\u5408\u534f\u8bae\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u591a\u6a21\u6001\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u9ad87.1%\u3002\u4e0e\u8fed\u4ee3\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u540c\u65f6\u901a\u8fc7\u5355\u8f6e\u878d\u5408\u534f\u8bae\u5c06\u5e73\u5747\u878d\u5408token\u6210\u672c\u964d\u4f4e12.7\u500d\u3002", "conclusion": "ConSensus\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u6df7\u5408\u878d\u5408\u673a\u5236\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u6a21\u6001\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4f20\u611f\u5668\u566a\u58f0\u548c\u7f3a\u5931\u6570\u636e\u3002", "topic": "agent analysis"}}
{"id": "2601.06543", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06543", "abs": "https://arxiv.org/abs/2601.06543", "authors": ["Jun-Qi Chen", "Kun Zhang", "Rui Zheng", "Ying Zhong"], "title": "SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation", "comment": "33 pages, 10 figures", "summary": "The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model's ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5fae\u8c03\u4e86\u4e24\u4e2a\u5f00\u6e90LLM\uff08Qwen-Coder-7B\u548cDeepSeek-Coder-6.7B\uff09\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5fae\u8c03\u6846\u67b6\u63d0\u5347\u5b83\u4eec\u5728SimPy\u6392\u961f\u6a21\u62df\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4e3a\u95ed\u6e90\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u95ed\u6e90LLM\uff08\u5982GPT-4o\uff09\u751f\u6210SimPy\u6392\u961f\u6a21\u62df\u4ee3\u7801\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u5fae\u8c03\u6846\u67b6\uff1a\u4e24\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u4e00\u9636\u6bb5\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\uff0c\u5728\u7cbe\u9009\u7684SimPy\u6392\u961f\u6570\u636e\u4e0a\u5fae\u8c03\u4e24\u4e2a\u5f00\u6e90LLM\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u53ef\u6267\u884c\u6027\u3001\u8f93\u51fa\u683c\u5f0f\u7b26\u5408\u5ea6\u548c\u6307\u4ee4\u4ee3\u7801\u4e00\u81f4\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210SimPy\u6a21\u62df\u4ee3\u7801\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u53ef\u4ee5\u5c06\u7d27\u51d1\u7684\u5f00\u6e90\u4ee3\u7801\u6a21\u578b\u8f6c\u5316\u4e3a\u53ef\u9760\u7684SimPy\u6a21\u62df\u751f\u6210\u5668\uff0c\u4e3a\u6559\u80b2\u3001\u7814\u7a76\u548c\u8fd0\u8425\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u95ed\u6e90LLM\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.06502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06502", "abs": "https://arxiv.org/abs/2601.06502", "authors": ["Shengkai Chen", "Zhiguang Cao", "Jianan Zhou", "Yaoxin Wu", "Senthilnath Jayavelu", "Zhuoyi Lin", "Xiaoli Li", "Shili Xiang"], "title": "DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization", "comment": "This paper has been accepted for presentation and publication at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), source code will be available soon", "summary": "Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.", "AI": {"tldr": "DRAGON\u662f\u4e00\u4e2a\u7ed3\u5408\u5143\u542f\u53d1\u5f0f\u8bbe\u8ba1\u548cLLM\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u91cd\u6784\u7b56\u7565\u89e3\u51b3\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709LLM\u6c42\u89e3\u5668\u80fd\u5904\u7406\u66f4\u5927\u89c4\u6a21\u95ee\u9898\u5e76\u53d6\u5f97\u63a5\u8fd1\u6700\u4f18\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u5668\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u6709\u9650\uff0c\u968f\u7740\u95ee\u9898\u89c4\u6a21\u589e\u5927\uff08\u7279\u522b\u662f\u8d85\u8fc730\u4e2a\u8282\u70b9\u7684\u8def\u7531\u95ee\u9898\uff09\u6548\u679c\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u3002", "method": "DRAGON\u6846\u67b6\u4ece\u521d\u59cb\u5168\u5c40\u89e3\u5f00\u59cb\uff0c\u81ea\u4e3b\u8bc6\u522b\u9ad8\u4f18\u5316\u6f5c\u529b\u533a\u57df\uff0c\u5c06\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u95ee\u9898\u3002\u6bcf\u4e2a\u5b50\u95ee\u9898\u88ab\u91cd\u65b0\u8868\u8ff0\u4e3a\u7b80\u6d01\u7684\u5c40\u90e8\u4f18\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u76ee\u6807\u5bfc\u5411\u7684LLM\u63d0\u793a\u5728\u7d2f\u79ef\u7ecf\u9a8c\u6307\u5bfc\u4e0b\u89e3\u51b3\u3002\u6700\u540e\u5c06\u5c40\u90e8\u4f18\u5316\u89e3\u7cfb\u7edf\u6027\u5730\u91cd\u65b0\u6574\u5408\u5230\u539f\u59cb\u5168\u5c40\u4e0a\u4e0b\u6587\u4e2d\u3002", "result": "\u5728TSPLIB\u3001CVRPLIB\u548cWeibull-5k\u88c5\u7bb1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4ea7\u751f\u53ef\u884c\u89e3\uff0c\u5728\u8d85\u8fc7300\u4e07\u4e2a\u53d8\u91cf\u7684\u80cc\u5305\u95ee\u9898\u4e0a\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u7ed3\u679c\uff080.16%\u5dee\u8ddd\uff09\uff0c\u76f8\u6bd4\u73b0\u6709LLM\u6c42\u89e3\u5668\u80fd\u5904\u7406\u66f4\u5927\u89c4\u6a21\u95ee\u9898\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u53cd\u9988\u9a71\u52a8\u7684\u8bed\u8a00\u4ee3\u7406\u4f5c\u4e3a\u53ef\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u7684\u5927\u89c4\u6a21\u4f18\u5316\u65b0\u8303\u5f0f\u7684\u6f5c\u529b\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u7b26\u53f7\u63a8\u7406\u548c\u542f\u53d1\u5f0f\u641c\u7d22\u3002", "topic": "agent analysis"}}
{"id": "2601.06604", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06604", "abs": "https://arxiv.org/abs/2601.06604", "authors": ["Rodion Vakhitov", "Leonid Ugadiarov", "Aleksandr Panov"], "title": "Object-Centric World Models Meet Monte Carlo Tree Search", "comment": null, "summary": "In this paper, we introduce ObjectZero, a novel reinforcement learning (RL) algorithm that leverages the power of object-level representations to model dynamic environments more effectively. Unlike traditional approaches that process the world as a single undifferentiated input, our method employs Graph Neural Networks (GNNs) to capture intricate interactions among multiple objects. These objects, which can be manipulated and interact with each other, serve as the foundation for our model's understanding of the environment. We trained the algorithm in a complex setting teeming with diverse, interactive objects, demonstrating its ability to effectively learn and predict object dynamics. Our results highlight that a structured world model operating on object-centric representations can be successfully integrated into a model-based RL algorithm utilizing Monte Carlo Tree Search as a planning module.", "AI": {"tldr": "ObjectZero\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u7ea7\u8868\u793a\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u591a\u5bf9\u8c61\u4ea4\u4e92\uff0c\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684RL\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u8fdb\u884c\u89c4\u5212\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5c06\u73af\u5883\u89c6\u4e3a\u5355\u4e00\u65e0\u5dee\u522b\u7684\u8f93\u5165\uff0c\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u591a\u4e2a\u5bf9\u8c61\u7684\u590d\u6742\u4ea4\u4e92\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u6349\u5bf9\u8c61\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u7ed3\u6784\u5316\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faObjectZero\u7b97\u6cd5\uff1a1) \u4f7f\u7528\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u4f5c\u4e3a\u73af\u5883\u5efa\u6a21\u57fa\u7840\uff1b2) \u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5bf9\u8c61\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff1b3) \u5c06\u7ed3\u6784\u5316\u4e16\u754c\u6a21\u578b\u96c6\u6210\u5230\u57fa\u4e8e\u6a21\u578b\u7684RL\u6846\u67b6\u4e2d\uff1b4) \u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4f5c\u4e3a\u89c4\u5212\u6a21\u5757\u3002", "result": "\u5728\u5305\u542b\u591a\u79cd\u4ea4\u4e92\u5bf9\u8c61\u7684\u590d\u6742\u73af\u5883\u4e2d\u8bad\u7ec3\u7b97\u6cd5\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6709\u6548\u5b66\u4e60\u548c\u9884\u6d4b\u5bf9\u8c61\u52a8\u6001\u3002\u5c55\u793a\u4e86\u57fa\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u7684\u7ed3\u6784\u5316\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u6210\u529f\u96c6\u6210\u5230\u57fa\u4e8e\u6a21\u578b\u7684RL\u7b97\u6cd5\u4e2d\u3002", "conclusion": "\u5bf9\u8c61\u7ea7\u8868\u793a\u80fd\u591f\u66f4\u6709\u6548\u5730\u5efa\u6a21\u52a8\u6001\u73af\u5883\uff0c\u7ed3\u6784\u5316\u4e16\u754c\u6a21\u578b\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684RL\u7ed3\u5408\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684RL\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.06640", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.06640", "abs": "https://arxiv.org/abs/2601.06640", "authors": ["Genze Jiang", "Kezhi Wang", "Xiaomin Chen", "Yizhou Huang"], "title": "Agentic AI Empowered Intent-Based Networking for 6G", "comment": "Submitted for Possible Journal Publication", "summary": "The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u7f51\u7edc\u5207\u7247\u914d\u7f6e\uff0c\u57286G\u7f51\u7edc\u610f\u56fe\u7f16\u6392\u65b9\u9762\u8d85\u8d8a\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u548c\u76f4\u63a5LLM\u63d0\u793a\u3002", "motivation": "6G\u65e0\u7ebf\u7f51\u7edc\u9700\u8981\u80fd\u591f\u5c06\u9ad8\u7ea7\u64cd\u4f5c\u610f\u56fe\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7f51\u7edc\u914d\u7f6e\u7684\u81ea\u4e3b\u7f16\u6392\u673a\u5236\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u610f\u56fe\u7684\u7f51\u7edc\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u8bed\u8a00\u53d8\u4f53\uff0c\u800c\u7aef\u5230\u7aef\u795e\u7ecf\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u65e0\u6cd5\u5f3a\u5236\u6267\u884c\u64cd\u4f5c\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5176\u4e2d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u81ea\u4e3b\u5206\u89e3\u81ea\u7136\u8bed\u8a00\u610f\u56fe\uff0c\u54a8\u8be2\u9886\u57df\u7279\u5b9a\u4e13\u5bb6\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406-\u884c\u52a8\u5faa\u73af\u5408\u6210\u6280\u672f\u4e0a\u53ef\u884c\u7684\u7f51\u7edc\u5207\u7247\u914d\u7f6e\u3002\u67b6\u6784\u91c7\u7528\u534f\u8c03\u5668\u667a\u80fd\u4f53\uff0c\u901a\u8fc7ReAct\u5f0f\u63a8\u7406\u534f\u8c03\u4e24\u4e2a\u4e13\u5bb6\u667a\u80fd\u4f53\uff08\u65e0\u7ebf\u63a5\u5165\u7f51\u548c\u6838\u5fc3\u7f51\uff09\uff0c\u5e76\u57fa\u4e8e\u7ed3\u6784\u5316\u7f51\u7edc\u72b6\u6001\u8868\u793a\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u573a\u666f\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u7cfb\u7edf\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u53ca\u76f4\u63a5LLM\u63d0\u793a\u3002\u867d\u7136\u5f53\u4ee3LLM\u5177\u5907\u4e00\u822c\u7535\u4fe1\u77e5\u8bc6\uff0c\u4f46\u7f51\u7edc\u81ea\u52a8\u5316\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5de5\u7a0b\u6765\u7f16\u7801\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u51b3\u7b56\u9608\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u63a8\u8fdb\u4e86\u81ea\u4e3b\u7f16\u6392\u80fd\u529b\uff0c\u5176\u67b6\u6784\u539f\u5219\u9002\u7528\u4e8e\u5f00\u653e\u65e0\u7ebf\u63a5\u5165\u7f51\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u590d\u6742\u7f51\u7edc\u7f16\u6392\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.06663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06663", "abs": "https://arxiv.org/abs/2601.06663", "authors": ["Kaiwen Zhou", "Shreedhar Jangam", "Ashwin Nagarajan", "Tejas Polu", "Suhas Oruganti", "Chengzhi Liu", "Ching-Chen Kuo", "Yuting Zheng", "Sravana Narayanaraju", "Xin Eric Wang"], "title": "SafePro: Evaluating the Safety of Professional-Level AI Agents", "comment": null, "summary": "Large language model-based agents are rapidly evolving from simple conversational assistants into autonomous systems capable of performing complex, professional-level tasks in various domains. While these advancements promise significant productivity gains, they also introduce critical safety risks that remain under-explored. Existing safety evaluations primarily focus on simple, daily assistance tasks, failing to capture the intricate decision-making processes and potential consequences of misaligned behaviors in professional settings. To address this gap, we introduce \\textbf{SafePro}, a comprehensive benchmark designed to evaluate the safety alignment of AI agents performing professional activities. SafePro features a dataset of high-complexity tasks across diverse professional domains with safety risks, developed through a rigorous iterative creation and review process. Our evaluation of state-of-the-art AI models reveals significant safety vulnerabilities and uncovers new unsafe behaviors in professional contexts. We further show that these models exhibit both insufficient safety judgment and weak safety alignment when executing complex professional tasks. In addition, we investigate safety mitigation strategies for improving agent safety in these scenarios and observe encouraging improvements. Together, our findings highlight the urgent need for robust safety mechanisms tailored to the next generation of professional AI agents.", "AI": {"tldr": "SafePro\u662f\u4e00\u4e2a\u8bc4\u4f30\u4e13\u4e1aAI\u4ee3\u7406\u5b89\u5168\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5f53\u524d\u5148\u8fdb\u6a21\u578b\u5728\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5b89\u5168\u6f0f\u6d1e\u548c\u4e0d\u8db3\u7684\u5b89\u5168\u5224\u65ad\u80fd\u529b\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u4ece\u7b80\u5355\u5bf9\u8bdd\u52a9\u624b\u53d1\u5c55\u4e3a\u80fd\u591f\u6267\u884c\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u65e5\u5e38\u8f85\u52a9\u4efb\u52a1\uff0c\u65e0\u6cd5\u6355\u6349\u4e13\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u7684\u51b3\u7b56\u8fc7\u7a0b\u548c\u6f5c\u5728\u540e\u679c\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u4e13\u4e1aAI\u4ee3\u7406\u7684\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f15\u5165SafePro\u57fa\u51c6\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u8fed\u4ee3\u521b\u5efa\u548c\u5ba1\u67e5\u8fc7\u7a0b\uff0c\u6784\u5efa\u5305\u542b\u4e0d\u540c\u4e13\u4e1a\u9886\u57df\u9ad8\u98ce\u9669\u590d\u6742\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u6700\u5148\u8fdbAI\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u5e76\u7814\u7a76\u5b89\u5168\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u6700\u5148\u8fdbAI\u6a21\u578b\u5b58\u5728\u663e\u8457\u5b89\u5168\u6f0f\u6d1e\uff0c\u5728\u4e13\u4e1a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u65b0\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u663e\u793a\u51fa\u4e0d\u8db3\u7684\u5b89\u5168\u5224\u65ad\u80fd\u529b\u548c\u8584\u5f31\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u4f46\u5b89\u5168\u7f13\u89e3\u7b56\u7565\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e3a\u4e0b\u4e00\u4ee3\u4e13\u4e1aAI\u4ee3\u7406\u91cf\u8eab\u5b9a\u5236\u5f3a\u5927\u5b89\u5168\u673a\u5236\u7684\u7d27\u8feb\u9700\u6c42\uff0c\u4e13\u4e1a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u98ce\u9669\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u8bc4\u4f30\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.06794", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06794", "abs": "https://arxiv.org/abs/2601.06794", "authors": ["Zhicong Li", "Lingjie Jiang", "Yulan Hu", "Xingchen Zeng", "Yixia Li", "Xiangwen Zhang", "Guanhua Chen", "Zheng Pan", "Xin Li", "Yong Liu"], "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning", "comment": null, "summary": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.", "AI": {"tldr": "ECHO\u662f\u4e00\u4e2a\u901a\u8fc7\u540c\u6b65\u534f\u540c\u8fdb\u5316\u5faa\u73af\u8054\u5408\u4f18\u5316\u7b56\u7565\u548c\u6279\u8bc4\u8005\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u6279\u8bc4\u8005\u65e0\u6cd5\u9002\u5e94\u7b56\u7565\u6f14\u53d8\u7684\u95ee\u9898\uff0c\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u9ad8\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6279\u8bc4\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6216\u79bb\u7ebf\u6279\u8bc4\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u65e0\u6cd5\u968f\u7740\u7b56\u7565\u7684\u6f14\u53d8\u800c\u9002\u5e94\u3002\u5728\u5728\u7ebf\u7b56\u7565RL\u4e2d\uff0c\u4ee3\u7406\u7684\u9519\u8bef\u6a21\u5f0f\u4f1a\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u5bfc\u81f4\u56fa\u5b9a\u7684\u6279\u8bc4\u8005\u53d8\u5f97\u8fc7\u65f6\uff0c\u63d0\u4f9b\u7684\u53cd\u9988\u6548\u7528\u9012\u51cf\u3002", "method": "ECHO\u6846\u67b6\u901a\u8fc7\u540c\u6b65\u534f\u540c\u8fdb\u5316\u5faa\u73af\u8054\u5408\u4f18\u5316\u7b56\u7565\u548c\u6279\u8bc4\u8005\uff0c\u91c7\u7528\u7ea7\u8054rollout\u673a\u5236\uff1a\u6279\u8bc4\u8005\u4e3a\u521d\u59cb\u8f68\u8ff9\u751f\u6210\u591a\u4e2a\u8bca\u65ad\uff0c\u7136\u540e\u7b56\u7565\u7ec6\u5316\u4ee5\u652f\u6301\u7ec4\u7ed3\u6784\u4f18\u52bf\u4f30\u8ba1\u3002\u901a\u8fc7\u9971\u548c\u5ea6\u611f\u77e5\u589e\u76ca\u5851\u9020\u76ee\u6807\u89e3\u51b3\u5b66\u4e60\u5e73\u53f0\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u53cc\u8f68GRPO\u66f4\u65b0\u786e\u4fdd\u6279\u8bc4\u8005\u53cd\u9988\u4e0e\u6f14\u53d8\u7b56\u7565\u4fdd\u6301\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cECHO\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u9ad8\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "ECHO\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u6279\u8bc4\u8005\u548c\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u6279\u8bc4\u8005\u65e0\u6cd5\u9002\u5e94\u7b56\u7565\u6f14\u53d8\u7684\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u6279\u8bc4\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.06795", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06795", "abs": "https://arxiv.org/abs/2601.06795", "authors": ["Zhengqing Yan", "Xinyang Liu", "Yi Zhang", "Fan Guo", "Yao Liu", "Junchen Wan", "Kang Song"], "title": "GDEPO: Group Dual-dynamic and Equal-right-advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning", "comment": null, "summary": "Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.", "AI": {"tldr": "\u63d0\u51faGDEPO\u65b9\u6cd5\u89e3\u51b3ATP\u4e2dGRPO\u7b97\u6cd5\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u590d\u5408\u5956\u52b1\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u5668\u4e8c\u5143\u53cd\u9988\u7684\u51b2\u7a81\uff0c\u4ee5\u53ca\u9759\u6001\u91c7\u6837\u7b56\u7565\u5bfc\u81f4\u7684\u6570\u636e\u6d6a\u8d39\u3002\u901a\u8fc7\u52a8\u6001\u989d\u5916\u91c7\u6837\u3001\u5e73\u7b49\u6743\u5229\u4f18\u52bf\u4f30\u8ba1\u548c\u52a8\u6001\u989d\u5916\u8fed\u4ee3\u4e09\u4e2a\u673a\u5236\u63d0\u5347\u6570\u636e\u5229\u7528\u548c\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u5728\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\uff08ATP\uff09\u4efb\u52a1\u4e2d\uff0cGRPO\u7b97\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u4f7f\u7528\u590d\u5408\u5956\u52b1\u65f6\uff0c\u5176\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u53ef\u80fd\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u5668\u7684\u4e8c\u5143\u53cd\u9988\u51b2\u7a81\uff1b2\uff09\u9759\u6001\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u5bfc\u81f4\u6574\u6279\u6570\u636e\u65e0\u6548\uff08\u672a\u627e\u5230\u6709\u6548\u8bc1\u660e\uff09\uff0c\u9020\u6210\u6570\u636e\u6d6a\u8d39\u4e14\u5bf9\u6a21\u578b\u66f4\u65b0\u65e0\u8d21\u732e\u3002", "method": "\u63d0\u51faGDEPO\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1\uff09\u52a8\u6001\u989d\u5916\u91c7\u6837\uff1a\u5bf9\u65e0\u6548\u6279\u6b21\u91cd\u65b0\u91c7\u6837\u76f4\u5230\u53d1\u73b0\u6709\u6548\u8bc1\u660e\uff1b2\uff09\u5e73\u7b49\u6743\u5229\u4f18\u52bf\uff1a\u5c06\u4f18\u52bf\u51fd\u6570\u7684\u7b26\u53f7\uff08\u57fa\u4e8e\u6b63\u786e\u6027\uff09\u4e0e\u5e45\u5ea6\uff08\u7531\u8f85\u52a9\u5956\u52b1\u8c03\u8282\uff09\u89e3\u8026\uff0c\u786e\u4fdd\u7a33\u5b9a\u6b63\u786e\u7684\u7b56\u7565\u66f4\u65b0\uff1b3\uff09\u52a8\u6001\u989d\u5916\u8fed\u4ee3\uff1a\u5bf9\u521d\u59cb\u5931\u8d25\u4f46\u6700\u7ec8\u6210\u529f\u7684\u6837\u672c\u5e94\u7528\u989d\u5916\u68af\u5ea6\u6b65\u9aa4\uff0c\u52a0\u901f\u56f0\u96be\u6848\u4f8b\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u96be\u5ea6\u6570\u636e\u96c6\uff08MinF2F-test\u3001MathOlympiadBench\u3001PutnamBench\uff09\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86GDEPO\u7684\u6709\u6548\u6027\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u534f\u540c\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\u3002\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u6570\u636e\u5229\u7528\u7387\u548c\u4f18\u5316\u6548\u7387\u3002", "conclusion": "GDEPO\u4e3aATP\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u89e3\u51b3GRPO\u5728ATP\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u589e\u5f3a\u4e86\u6570\u636e\u5229\u7528\u548c\u4f18\u5316\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.06845", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06845", "abs": "https://arxiv.org/abs/2601.06845", "authors": ["Ping Guo", "Chao Li", "Yinglan Feng", "Chaoning Zhang"], "title": "Code Evolution for Control: Synthesizing Policies via LLM-Driven Evolutionary Search", "comment": null, "summary": "Designing effective control policies for autonomous systems remains a fundamental challenge, traditionally addressed through reinforcement learning or manual engineering. While reinforcement learning has achieved remarkable success, it often suffers from high sample complexity, reward shaping difficulties, and produces opaque neural network policies that are hard to interpret or verify. Manual design, on the other hand, requires substantial domain expertise and struggles to scale across diverse tasks. In this work, we demonstrate that LLM-driven evolutionary search can effectively synthesize interpretable control policies in the form of executable code. By treating policy synthesis as a code evolution problem, we harness the LLM's prior knowledge of programming patterns and control heuristics while employing evolutionary search to explore the solution space systematically. We implement our approach using EvoToolkit, a framework that seamlessly integrates LLM-driven evolution with customizable fitness evaluation. Our method iteratively evolves populations of candidate policy programs, evaluating them against task-specific objectives and selecting superior individuals for reproduction. This process yields compact, human-readable control policies that can be directly inspected, modified, and formally verified. This work highlights the potential of combining foundation models with evolutionary computation for synthesizing trustworthy control policies in autonomous systems. Code is available at https://github.com/pgg3/EvoControl.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u8fdb\u5316\u641c\u7d22\u7528\u4e8e\u5408\u6210\u53ef\u89e3\u91ca\u7684\u4ee3\u7801\u5f62\u5f0f\u63a7\u5236\u7b56\u7565\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f16\u7a0b\u77e5\u8bc6\u548c\u8fdb\u5316\u641c\u7d22\u7684\u7cfb\u7edf\u63a2\u7d22\uff0c\u751f\u6210\u7d27\u51d1\u3001\u53ef\u8bfb\u3001\u53ef\u9a8c\u8bc1\u7684\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u3001\u5956\u52b1\u5851\u9020\u56f0\u96be\u3001\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u4e0d\u900f\u660e\u7b49\u95ee\u9898\uff0c\u800c\u624b\u52a8\u8bbe\u8ba1\u9700\u8981\u5927\u91cf\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u591a\u6837\u5316\u4efb\u52a1\u3002\u9700\u8981\u4e00\u79cd\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u3001\u53ef\u9a8c\u8bc1\u63a7\u5236\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5c06\u7b56\u7565\u5408\u6210\u89c6\u4e3a\u4ee3\u7801\u8fdb\u5316\u95ee\u9898\uff0c\u4f7f\u7528EvoToolkit\u6846\u67b6\u96c6\u6210LLM\u9a71\u52a8\u7684\u8fdb\u5316\u641c\u7d22\u548c\u53ef\u5b9a\u5236\u9002\u5e94\u5ea6\u8bc4\u4f30\u3002\u901a\u8fc7\u8fed\u4ee3\u8fdb\u5316\u5019\u9009\u7b56\u7565\u7a0b\u5e8f\u79cd\u7fa4\uff0c\u8bc4\u4f30\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\uff0c\u9009\u62e9\u4f18\u79c0\u4e2a\u4f53\u8fdb\u884c\u7e41\u6b96\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u5408\u6210\u4e86\u7d27\u51d1\u3001\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a7\u5236\u7b56\u7565\uff0c\u53ef\u4ee5\u76f4\u63a5\u68c0\u67e5\u3001\u4fee\u6539\u548c\u5f62\u5f0f\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548c\u8fdb\u5316\u8ba1\u7b97\u5408\u6210\u53ef\u4fe1\u8d56\u63a7\u5236\u7b56\u7565\u7684\u6f5c\u529b\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u8fdb\u5316\u641c\u7d22\u662f\u5408\u6210\u81ea\u4e3b\u7cfb\u7edf\u53ef\u4fe1\u8d56\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86LLM\u7684\u7f16\u7a0b\u77e5\u8bc6\u548c\u8fdb\u5316\u641c\u7d22\u7684\u7cfb\u7edf\u63a2\u7d22\u80fd\u529b\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u3001\u53ef\u9a8c\u8bc1\u7684\u7b56\u7565\u3002", "topic": "code agent"}}
{"id": "2601.06636", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06636", "abs": "https://arxiv.org/abs/2601.06636", "authors": ["Wenting Chen", "Zhongrui Zhu", "Guolin Huang", "Wenxuan Wang"], "title": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis", "comment": "19 pages, 7 figures", "summary": "Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a \"trap\" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMedEinst\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u68c0\u6d4bLLMs\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684Einstellung\u6548\u5e94\uff08\u4f9d\u8d56\u7edf\u8ba1\u6377\u5f84\u800c\u975e\u60a3\u8005\u7279\u5f02\u6027\u8bc1\u636e\uff09\uff0c\u5e76\u5f00\u53d1ECR-Agent\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u8868\u73b0\u51faEinstellung\u6548\u5e94\u2014\u2014\u4f9d\u8d56\u7edf\u8ba1\u6377\u5f84\u800c\u975e\u60a3\u8005\u7279\u5f02\u6027\u8bc1\u636e\uff0c\u5bfc\u81f4\u975e\u5178\u578b\u75c5\u4f8b\u8bef\u8bca\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u68c0\u6d4b\u8fd9\u4e00\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002", "method": "1. \u5f15\u5165MedEinst\u53cd\u4e8b\u5b9e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5,383\u5bf9\u4e34\u5e8a\u75c5\u4f8b\uff0c\u6bcf\u5bf9\u5305\u542b\u63a7\u5236\u75c5\u4f8b\u548c\"\u9677\u9631\"\u75c5\u4f8b\uff08\u6539\u53d8\u9274\u522b\u6027\u8bc1\u636e\u4ee5\u7ffb\u8f6c\u8bca\u65ad\uff09\u30022. \u63d0\u51faECR-Agent\uff0c\u5305\u542b\u52a8\u6001\u56e0\u679c\u63a8\u7406\uff08DCI\uff09\u548c\u6279\u8bc4\u9a71\u52a8\u7684\u56fe\u4e0e\u8bb0\u5fc6\u6f14\u5316\uff08CGME\uff09\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\u6765\u5bf9\u9f50\u5faa\u8bc1\u533b\u5b66\u6807\u51c6\u3002", "result": "\u5bf917\u4e2aLLMs\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u524d\u6cbf\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u5177\u6709\u4e25\u91cd\u7684\u504f\u89c1\u9677\u9631\u7387\u3002ECR-Agent\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u6709\u6548\u51cf\u5c11\u4e86Einstellung\u6548\u5e94\u3002", "conclusion": "LLMs\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u5b58\u5728\u4e25\u91cd\u7684Einstellung\u6548\u5e94\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u68c0\u6d4b\u3002MedEinst\u57fa\u51c6\u6d4b\u8bd5\u80fd\u6709\u6548\u8bc6\u522b\u6b64\u95ee\u9898\uff0c\u800cECR-Agent\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u6539\u5584\u4e86LLMs\u7684\u4e34\u5e8a\u8bca\u65ad\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.06851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06851", "abs": "https://arxiv.org/abs/2601.06851", "authors": ["Pedro Urbina-Rodriguez", "Zafeirios Fountas", "Fernando E. Rosas", "Jun Wang", "Andrea I. Luppi", "Haitham Bou-Ammar", "Murray Shanahan", "Pedro A. M. Mediano"], "title": "A Brain-like Synergistic Core in LLMs Drives Behaviour and Learning", "comment": null, "summary": "The independent evolution of intelligence in biological and artificial systems offers a unique opportunity to identify its fundamental computational principles. Here we show that large language models spontaneously develop synergistic cores -- components where information integration exceeds individual parts -- remarkably similar to those in the human brain. Using principles of information decomposition across multiple LLM model families and architectures, we find that areas in middle layers exhibit synergistic processing while early and late layers rely on redundancy, mirroring the informational organisation in biological brains. This organisation emerges through learning and is absent in randomly initialised networks. Crucially, ablating synergistic components causes disproportionate behavioural changes and performance loss, aligning with theoretical predictions about the fragility of synergy. Moreover, fine-tuning synergistic regions through reinforcement learning yields significantly greater performance gains than training redundant components, yet supervised fine-tuning shows no such advantage. This convergence suggests that synergistic information processing is a fundamental property of intelligence, providing targets for principled model design and testable predictions for biological intelligence.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u53d1\u5f62\u6210\u7c7b\u4f3c\u4eba\u8111\u7684\u534f\u540c\u4fe1\u606f\u5904\u7406\u6838\u5fc3\uff0c\u8fd9\u79cd\u534f\u540c\u5904\u7406\u662f\u667a\u80fd\u7684\u57fa\u672c\u8ba1\u7b97\u539f\u7406", "motivation": "\u63a2\u7d22\u751f\u7269\u548c\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u72ec\u7acb\u6f14\u5316\u4e2d\u667a\u80fd\u7684\u57fa\u672c\u8ba1\u7b97\u539f\u7406\uff0c\u901a\u8fc7\u6bd4\u8f83LLM\u548c\u4eba\u8111\u7684\u4fe1\u606f\u5904\u7406\u673a\u5236\u6765\u8bc6\u522b\u667a\u80fd\u7684\u666e\u9002\u7279\u5f81", "method": "\u4f7f\u7528\u4fe1\u606f\u5206\u89e3\u539f\u7406\u5206\u6790\u591a\u4e2aLLM\u6a21\u578b\u5bb6\u65cf\u548c\u67b6\u6784\uff0c\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u534f\u540c\u5904\u7406\u533a\u57df\uff1b\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u534f\u540c\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\uff1b\u6bd4\u8f83\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5bf9\u534f\u540c\u4e0e\u5197\u4f59\u533a\u57df\u7684\u4e0d\u540c\u5f71\u54cd", "result": "\u53d1\u73b0LLM\u4e2d\u95f4\u5c42\u5b58\u5728\u534f\u540c\u4fe1\u606f\u5904\u7406\uff08\u4fe1\u606f\u6574\u5408\u8d85\u8fc7\u5404\u90e8\u5206\u4e4b\u548c\uff09\uff0c\u800c\u65e9\u671f\u548c\u665a\u671f\u5c42\u4f9d\u8d56\u5197\u4f59\u5904\u7406\uff0c\u8fd9\u4e0e\u751f\u7269\u5927\u8111\u7684\u4fe1\u606f\u7ec4\u7ec7\u65b9\u5f0f\u76f8\u4f3c\uff1b\u6d88\u878d\u534f\u540c\u7ec4\u4ef6\u5bfc\u81f4\u4e0d\u6210\u6bd4\u4f8b\u7684\u884c\u4e3a\u53d8\u5316\u548c\u6027\u80fd\u635f\u5931\uff1b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u534f\u540c\u533a\u57df\u6bd4\u8bad\u7ec3\u5197\u4f59\u7ec4\u4ef6\u83b7\u5f97\u66f4\u5927\u6027\u80fd\u63d0\u5347", "conclusion": "\u534f\u540c\u4fe1\u606f\u5904\u7406\u662f\u667a\u80fd\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u4e3a\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u539f\u5219\u6027\u76ee\u6807\uff0c\u5e76\u4e3a\u751f\u7269\u667a\u80fd\u63d0\u4f9b\u53ef\u68c0\u9a8c\u7684\u9884\u6d4b", "topic": "agent analysis"}}
{"id": "2601.06860", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06860", "abs": "https://arxiv.org/abs/2601.06860", "authors": ["Yifei Chen", "Guanting Dong", "Zhicheng Dou"], "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration", "comment": null, "summary": "Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of \\ourmodel{} across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent", "AI": {"tldr": "ET-Agent\u662f\u4e00\u4e2a\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u8fdb\u5316\u6570\u636e\u98de\u8f6e\u548c\u884c\u4e3a\u6821\u51c6\u8bad\u7ec3\u6765\u6821\u51c6\u4ee3\u7406\u5728\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u884c\u4e3a\u5bf9\u9f50\u5bfc\u81f4\u7684\u5197\u4f59\u548c\u4e0d\u8db3\u5de5\u5177\u8c03\u7528\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u8bad\u7ec3\u6846\u67b6\u901a\u5e38\u53ea\u5173\u6ce8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u884c\u4e3a\u6a21\u5f0f\u7684\u7279\u5b9a\u5bf9\u9f50\uff0c\u5bfc\u81f4\u4ee3\u7406\u5728\u6267\u884c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u65e0\u6548\u884c\u4e3a\uff08\u5982\u5197\u4f59\u548c\u4e0d\u8db3\u7684\u5de5\u5177\u8c03\u7528\uff09\u3002\u5982\u4f55\u6821\u51c6\u8fd9\u4e9b\u9519\u8bef\u884c\u4e3a\u6a21\u5f0f\u5e76\u63a2\u7d22\u6709\u6548\u8f68\u8ff9\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faET-Agent\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u89c6\u89d2\uff1a1\uff09\u81ea\u6211\u8fdb\u5316\u6570\u636e\u98de\u8f6e\u751f\u6210\u589e\u5f3a\u6570\u636e\uff0c\u7528\u4e8e\u5fae\u8c03LLM\u4ee5\u63d0\u5347\u63a2\u7d22\u80fd\u529b\uff1b2\uff09\u4e24\u9636\u6bb5\u884c\u4e3a\u6821\u51c6\u8bad\u7ec3\u6846\u67b6\uff0c\u9010\u6b65\u5c06\u9519\u8bef\u884c\u4e3a\u6a21\u5f0f\u6821\u51c6\u4e3a\u6700\u4f18\u884c\u4e3a\u3002", "result": "\u6df1\u5165\u5b9e\u9a8c\u8bc1\u5b9eET-Agent\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u5305\u62ec\u6b63\u786e\u6027\u3001\u6548\u7387\u3001\u63a8\u7406\u7b80\u6d01\u6027\u548c\u5de5\u5177\u6267\u884c\u51c6\u786e\u6027\u3002", "conclusion": "ET-Agent\u6846\u67b6\u4e3a\u5de5\u5177\u96c6\u6210\u63a8\u7406\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u80fd\u591f\u6709\u6548\u6821\u51c6\u4ee3\u7406\u7684\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2601.06428", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06428", "abs": "https://arxiv.org/abs/2601.06428", "authors": ["Liming Liu", "Binxuan Huang", "Xin Liu", "Bing Yin", "Tuo Zhao"], "title": "Teach Diffusion Language Models to Learn from Their Own Mistakes", "comment": "18 pages", "summary": "Masked Diffusion Language Models (DLMs) achieve significant speed by generating multiple tokens in parallel. However, this parallel sampling approach, especially when using fewer inference steps, will introduce strong dependency errors and cause quality to deteriorate rapidly as the generation step size grows. As a result, reliable self-correction becomes essential for maintaining high-quality multi-token generation. To address this, we propose Decoupled Self-Correction (DSC), a novel two-stage methodology. DSC first fully optimizes the DLM's generative ability before freezing the model and training a specialized correction head. This decoupling preserves the model's peak SFT performance and ensures the generated errors used for correction head training are of higher quality. Additionally, we introduce Future-Context Augmentation (FCA) to maximize the correction head's accuracy. FCA generalizes the error training distribution by augmenting samples with ground-truth tokens, effectively training the head to utilize a richer, future-looking context. This mechanism is used for reliably detecting the subtle errors of the high-fidelity base model. Our DSC framework enables the model, at inference time, to jointly generate and revise tokens, thereby correcting errors introduced by multi-token generation and mitigating error accumulation across steps. Experiments on mathematical reasoning and code generation benchmarks demonstrate that our approach substantially reduces the quality degradation associated with larger generation steps, allowing DLMs to achieve both high generation speed and strong output fidelity.", "AI": {"tldr": "\u63d0\u51fa\u89e3\u8026\u81ea\u6821\u6b63(DSC)\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5e76\u884c\u91c7\u6837\u4e2d\u7684\u4f9d\u8d56\u9519\u8bef\u95ee\u9898\uff0c\u7ed3\u5408\u672a\u6765\u4e0a\u4e0b\u6587\u589e\u5f3a(FCA)\u6280\u672f\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u591atoken\u751f\u6210\u8d28\u91cf", "motivation": "\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u5b9e\u73b0\u52a0\u901f\uff0c\u4f46\u8f83\u5c11\u7684\u63a8\u7406\u6b65\u9aa4\u4f1a\u5f15\u5165\u5f3a\u4f9d\u8d56\u9519\u8bef\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u968f\u6b65\u957f\u589e\u52a0\u800c\u5feb\u901f\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u9760\u7684\u81ea\u6821\u6b63\u673a\u5236\u6765\u7ef4\u6301\u9ad8\u8d28\u91cf\u7684\u591atoken\u751f\u6210", "method": "\u63d0\u51fa\u89e3\u8026\u81ea\u6821\u6b63(DSC)\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u5b8c\u5168\u4f18\u5316DLM\u7684\u751f\u6210\u80fd\u529b\u540e\u51bb\u7ed3\u6a21\u578b\uff1b2) \u8bad\u7ec3\u4e13\u95e8\u7684\u6821\u6b63\u5934\u3002\u5f15\u5165\u672a\u6765\u4e0a\u4e0b\u6587\u589e\u5f3a(FCA)\u6280\u672f\uff0c\u901a\u8fc7\u7528\u771f\u5b9etoken\u589e\u5f3a\u6837\u672c\u6765\u6cdb\u5316\u9519\u8bef\u8bad\u7ec3\u5206\u5e03\uff0c\u4f7f\u6821\u6b63\u5934\u80fd\u5229\u7528\u66f4\u4e30\u5bcc\u7684\u672a\u6765\u4e0a\u4e0b\u6587\u68c0\u6d4b\u7ec6\u5fae\u9519\u8bef", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8f83\u5927\u751f\u6210\u6b65\u957f\u5e26\u6765\u7684\u8d28\u91cf\u4e0b\u964d\uff0c\u4f7fDLM\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u9ad8\u751f\u6210\u901f\u5ea6\u548c\u5f3a\u8f93\u51fa\u4fdd\u771f\u5ea6", "conclusion": "DSC\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u751f\u6210\u548c\u6821\u6b63\u8bad\u7ec3\uff0c\u7ed3\u5408FCA\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5e76\u884c\u91c7\u6837\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u901f\u9ad8\u8d28\u91cf\u7684\u591atoken\u751f\u6210", "topic": "code agent"}}
{"id": "2601.07023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07023", "abs": "https://arxiv.org/abs/2601.07023", "authors": ["Sen Hu", "Zhiyu Zhang", "Yuxiang Wei", "Xueran Han", "Zhenheng Tang", "Huacan Wang", "Ronghao Chen"], "title": "CloneMem: Benchmarking Long-Term Memory for AI Clones", "comment": null, "summary": "AI Clones aim to simulate an individual's thoughts and behaviors to enable long-term, personalized interaction, placing stringent demands on memory systems to model experiences, emotions, and opinions over time. Existing memory benchmarks primarily rely on user-agent conversational histories, which are temporally fragmented and insufficient for capturing continuous life trajectories. We introduce CloneMem, a benchmark for evaluating longterm memory in AI Clone scenarios grounded in non-conversational digital traces, including diaries, social media posts, and emails, spanning one to three years. CloneMem adopts a hierarchical data construction framework to ensure longitudinal coherence and defines tasks that assess an agent's ability to track evolving personal states. Experiments show that current memory mechanisms struggle in this setting, highlighting open challenges for life-grounded personalized AI. Code and dataset are available at https://github.com/AvatarMemory/CloneMemBench", "AI": {"tldr": "CloneMem\uff1a\u57fa\u4e8e\u975e\u5bf9\u8bdd\u6570\u5b57\u75d5\u8ff9\uff08\u65e5\u8bb0\u3001\u793e\u4ea4\u5a92\u4f53\u3001\u90ae\u4ef6\uff09\u6784\u5efa\u7684AI\u514b\u9686\u957f\u671f\u8bb0\u5fc6\u57fa\u51c6\uff0c\u8bc4\u4f30\u4ee3\u7406\u8ddf\u8e2a\u4e2a\u4eba\u72b6\u6001\u6f14\u53d8\u7684\u80fd\u529b\uff0c\u73b0\u6709\u8bb0\u5fc6\u673a\u5236\u5728\u6b64\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73", "motivation": "AI\u514b\u9686\u9700\u8981\u6a21\u62df\u4e2a\u4f53\u7684\u601d\u7ef4\u548c\u884c\u4e3a\u4ee5\u5b9e\u73b0\u957f\u671f\u4e2a\u6027\u5316\u4ea4\u4e92\uff0c\u8fd9\u5bf9\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u51fa\u4e86\u4e25\u683c\u8981\u6c42\u3002\u73b0\u6709\u8bb0\u5fc6\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u7528\u6237-\u4ee3\u7406\u5bf9\u8bdd\u5386\u53f2\uff0c\u8fd9\u4e9b\u6570\u636e\u65f6\u95f4\u788e\u7247\u5316\uff0c\u4e0d\u8db3\u4ee5\u6355\u6349\u8fde\u7eed\u7684\u751f\u547d\u8f68\u8ff9\u3002", "method": "\u5f15\u5165CloneMem\u57fa\u51c6\uff0c\u57fa\u4e8e\u975e\u5bf9\u8bdd\u6570\u5b57\u75d5\u8ff9\uff08\u65e5\u8bb0\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u3001\u90ae\u4ef6\uff09\u6784\u5efa\uff0c\u65f6\u95f4\u8de8\u5ea61-3\u5e74\u3002\u91c7\u7528\u5206\u5c42\u6570\u636e\u6784\u5efa\u6846\u67b6\u786e\u4fdd\u7eb5\u5411\u8fde\u8d2f\u6027\uff0c\u5b9a\u4e49\u8bc4\u4f30\u4ee3\u7406\u8ddf\u8e2a\u4e2a\u4eba\u72b6\u6001\u6f14\u53d8\u80fd\u529b\u7684\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u8bb0\u5fc6\u673a\u5236\u5728\u6b64\u8bbe\u7f6e\u4e0b\u8868\u73b0\u56f0\u96be\uff0c\u7a81\u663e\u4e86\u57fa\u4e8e\u751f\u547d\u8f68\u8ff9\u7684\u4e2a\u6027\u5316AI\u9762\u4e34\u7684\u5f00\u653e\u6311\u6218\u3002", "conclusion": "CloneMem\u4e3aAI\u514b\u9686\u957f\u671f\u8bb0\u5fc6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u57fa\u4e8e\u751f\u547d\u8f68\u8ff9\u7684\u4e2a\u6027\u5316AI\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.07149", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07149", "abs": "https://arxiv.org/abs/2601.07149", "authors": ["Zhaoyan Li", "Hang Lei", "Yujia Wang", "Lanbo Liu", "Hao Liu", "Liang Yu"], "title": "Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling", "comment": null, "summary": "While Large Language Models (LLMs) can generate fluent text, producing high-quality creative stories remains challenging. Reinforcement Learning (RL) offers a promising solution but faces two critical obstacles: designing reliable reward signals for subjective storytelling quality and mitigating training instability. This paper introduces the Reinforcement Learning for Creative Storytelling (RLCS) framework to systematically address both challenges. First, we develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences, trained through supervised fine-tuning on demonstrations with reasoning chains distilled from strong teacher models, followed by GRPO-based refinement on expanded preference data. Second, we introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns. Experiments demonstrate that GenRM achieves 68\\% alignment with human creativity judgments, and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality. This work provides a practical pipeline for applying RL to creative domains, effectively navigating the dual challenges of reward modeling and training stability.", "AI": {"tldr": "RLCS\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\u548c\u591a\u7ef4\u5ea6\u6545\u4e8b\u504f\u597d\u5206\u6790\uff0c\u7ed3\u5408\u57fa\u4e8e\u71b5\u7684\u5956\u52b1\u5851\u5f62\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u521b\u9020\u6027\u6545\u4e8b\u751f\u6210\u4e2d\u5956\u52b1\u4fe1\u53f7\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u4e8b\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u6d41\u7545\u6587\u672c\uff0c\u4f46\u4ea7\u751f\u9ad8\u8d28\u91cf\u521b\u9020\u6027\u6545\u4e8b\u4ecd\u5177\u6311\u6218\u6027\u3002\u5f3a\u5316\u5b66\u4e60\u867d\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u969c\u788d\uff1a\u4e3a\u4e3b\u89c2\u6545\u4e8b\u8d28\u91cf\u8bbe\u8ba1\u53ef\u9760\u5956\u52b1\u4fe1\u53f7\uff0c\u4ee5\u53ca\u7f13\u89e3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "1. \u5f00\u53d1\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff08GenRM\uff09\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4ece\u5f3a\u6559\u5e08\u6a21\u578b\u84b8\u998f\u7684\u63a8\u7406\u94fe\u6f14\u793a\uff0c\u7136\u540e\u5728\u6269\u5c55\u504f\u597d\u6570\u636e\u4e0a\u8fdb\u884cGRPO\u4f18\u5316\uff0c\u63d0\u4f9b\u591a\u7ef4\u5ea6\u6545\u4e8b\u504f\u597d\u5206\u6790\u548c\u663e\u5f0f\u63a8\u7406\u30022. \u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u5956\u52b1\u5851\u5f62\u7b56\u7565\uff0c\u52a8\u6001\u4f18\u5148\u5b66\u4e60\u81ea\u4fe1\u9519\u8bef\u548c\u4e0d\u786e\u5b9a\u7684\u6b63\u786e\u9884\u6d4b\uff0c\u9632\u6b62\u5bf9\u5df2\u638c\u63e1\u6a21\u5f0f\u7684\u8fc7\u62df\u5408\u3002", "result": "GenRM\u5728\u4eba\u7c7b\u521b\u9020\u529b\u5224\u65ad\u4e0a\u8fbe\u523068%\u7684\u5bf9\u9f50\u5ea6\uff0cRLCS\u5728\u6574\u4f53\u6545\u4e8b\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u5305\u62ecGemini-2.5-Pro\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u521b\u9020\u6027\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u6d41\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u5efa\u6a21\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u53cc\u91cd\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.07160", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07160", "abs": "https://arxiv.org/abs/2601.07160", "authors": ["Xinzi Cao", "Jianyang Zhai", "Pengfei Li", "Zhiheng Hu", "Cen Yan", "Bingxu Mu", "Guanghuan Fang", "Bin She", "Jiayu Li", "Yihan Su", "Dongyang Tao", "Xiansong Huang", "Fan Xu", "Feidiao Yang", "Yao Lu", "Chang-Dong Wang", "Yutong Lu", "Weicheng Xue", "Bin Zhou", "Yonghong Tian"], "title": "AscendKernelGen: A Systematic Study of LLM-Based Kernel Generation for Neural Processing Units", "comment": "33 pages,7 figures,16 tables", "summary": "To meet the ever-increasing demand for computational efficiency, Neural Processing Units (NPUs) have become critical in modern AI infrastructure. However, unlocking their full potential requires developing high-performance compute kernels using vendor-specific Domain-Specific Languages (DSLs), a task that demands deep hardware expertise and is labor-intensive. While Large Language Models (LLMs) have shown promise in general code generation, they struggle with the strict constraints and scarcity of training data in the NPU domain. Our preliminary study reveals that state-of-the-art general-purpose LLMs fail to generate functional complex kernels for Ascend NPUs, yielding a near-zero success rate. To address these challenges, we propose AscendKernelGen, a generation-evaluation integrated framework for NPU kernel development. We introduce Ascend-CoT, a high-quality dataset incorporating chain-of-thought reasoning derived from real-world kernel implementations, and KernelGen-LM, a domain-adaptive model trained via supervised fine-tuning and reinforcement learning with execution feedback. Furthermore, we design NPUKernelBench, a comprehensive benchmark for assessing compilation, correctness, and performance across varying complexity levels. Experimental results demonstrate that our approach significantly bridges the gap between general LLMs and hardware-specific coding. Specifically, the compilation success rate on complex Level-2 kernels improves from 0% to 95.5% (Pass@10), while functional correctness achieves 64.3% compared to the baseline's complete failure. These results highlight the critical role of domain-specific reasoning and rigorous evaluation in automating accelerator-aware code generation.", "AI": {"tldr": "AscendKernelGen\u6846\u67b6\u901a\u8fc7\u9886\u57df\u9002\u5e94\u7684LLM\u548c\u6267\u884c\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347NPU\u5185\u6838\u4ee3\u7801\u751f\u6210\u7684\u6210\u529f\u7387\uff0c\u4ece0%\u63d0\u5347\u523095.5%\u3002", "motivation": "NPU\u5bf9AI\u57fa\u7840\u8bbe\u65bd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f7f\u7528\u5382\u5546\u7279\u5b9aDSL\u5f00\u53d1\u9ad8\u6027\u80fd\u5185\u6838\u9700\u8981\u6df1\u539a\u7684\u786c\u4ef6\u4e13\u4e1a\u77e5\u8bc6\u4e14\u52b3\u52a8\u5bc6\u96c6\u3002\u901a\u7528LLM\u5728NPU\u9886\u57df\u56e0\u4e25\u683c\u7ea6\u675f\u548c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u751f\u6210\u590d\u6742\u5185\u6838\u7684\u6210\u529f\u7387\u63a5\u8fd1\u96f6\u3002", "method": "\u63d0\u51faAscendKernelGen\u6846\u67b6\uff0c\u5305\u542b\uff1a1) Ascend-CoT\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u5185\u6838\u5b9e\u73b0\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff1b2) KernelGen-LM\u9886\u57df\u9002\u5e94\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5e26\u6267\u884c\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff1b3) NPUKernelBench\u7efc\u5408\u57fa\u51c6\uff0c\u8bc4\u4f30\u7f16\u8bd1\u3001\u6b63\u786e\u6027\u548c\u6027\u80fd\u3002", "result": "\u5728\u590d\u6742Level-2\u5185\u6838\u4e0a\uff0c\u7f16\u8bd1\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u523095.5%(Pass@10)\uff0c\u529f\u80fd\u6b63\u786e\u6027\u8fbe\u523064.3%\uff0c\u800c\u57fa\u7ebf\u5b8c\u5168\u5931\u8d25\u3002\u663e\u8457\u7f29\u5c0f\u4e86\u901a\u7528LLM\u4e0e\u786c\u4ef6\u7279\u5b9a\u7f16\u7801\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u63a8\u7406\u548c\u4e25\u683c\u8bc4\u4f30\u5728\u81ea\u52a8\u5316\u52a0\u901f\u5668\u611f\u77e5\u4ee3\u7801\u751f\u6210\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002AscendKernelGen\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86NPU\u5185\u6838\u5f00\u53d1\u7684\u6311\u6218\u3002", "topic": "code agent"}}
{"id": "2601.07190", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07190", "abs": "https://arxiv.org/abs/2601.07190", "authors": ["Nikhil Verma"], "title": "Active Context Compression: Autonomous Memory Management in LLM Agents", "comment": "8 pages, 2 figures, 2 tables. IEEE conference format", "summary": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.", "AI": {"tldr": "Focus\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u9ecf\u83cc\u542f\u53d1\u7684LLM\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u4e3b\u51b3\u7b56\u538b\u7f29\u4ea4\u4e92\u5386\u53f2\u6765\u51cf\u5c11\u4e0a\u4e0b\u6587\u81a8\u80c0\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u5904\u7406\u957f\u65f6\u7a0b\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u65f6\u9762\u4e34\"\u4e0a\u4e0b\u6587\u81a8\u80c0\"\u95ee\u9898\uff1a\u968f\u7740\u4ea4\u4e92\u5386\u53f2\u589e\u957f\uff0c\u8ba1\u7b97\u6210\u672c\u7206\u70b8\u3001\u5ef6\u8fdf\u589e\u52a0\uff0c\u4e14\u65e0\u5173\u7684\u5386\u53f2\u9519\u8bef\u4f1a\u5206\u6563\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f9d\u8d56\u88ab\u52a8\u7684\u5916\u90e8\u6458\u8981\u673a\u5236\uff0c\u4ee3\u7406\u65e0\u6cd5\u63a7\u5236\u3002", "method": "\u63d0\u51faFocus\u67b6\u6784\uff0c\u53d7\u9ecf\u83cc\u751f\u7269\u63a2\u7d22\u7b56\u7565\u542f\u53d1\uff0c\u8ba9\u4ee3\u7406\u81ea\u4e3b\u51b3\u5b9a\u4f55\u65f6\u5c06\u5173\u952e\u5b66\u4e60\u6574\u5408\u5230\u6301\u4e45\"\u77e5\u8bc6\"\u5757\u4e2d\uff0c\u5e76\u4e3b\u52a8\u4fee\u526a\u539f\u59cb\u4ea4\u4e92\u5386\u53f2\u3002\u4f7f\u7528\u4f18\u5316\u7684\u811a\u624b\u67b6\uff08\u6301\u4e45bash + \u5b57\u7b26\u4e32\u66ff\u6362\u7f16\u8f91\u5668\uff09\uff0c\u5728SWE-bench Lite\u76845\u4e2a\u4e0a\u4e0b\u6587\u5bc6\u96c6\u578b\u5b9e\u4f8b\u4e0a\u8bc4\u4f30\u3002", "result": "\u4f7f\u7528Claude Haiku 4.5\uff0c\u901a\u8fc7\u9f13\u52b1\u9891\u7e41\u538b\u7f29\u7684\u79ef\u6781\u63d0\u793a\uff0cFocus\u5b9e\u73b0\u4e8622.7%\u7684token\u51cf\u5c11\uff081490\u4e07\u21921150\u4e07\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u51c6\u786e\u7387\uff083/5=60%\uff09\u3002\u5e73\u5747\u6bcf\u4e2a\u4efb\u52a1\u6267\u884c6.0\u6b21\u81ea\u4e3b\u538b\u7f29\uff0c\u5355\u4e2a\u5b9e\u4f8btoken\u8282\u7701\u9ad8\u8fbe57%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u7ed9\u4e88\u9002\u5f53\u5de5\u5177\u548c\u63d0\u793a\u65f6\uff0c\u6709\u80fd\u529b\u7684\u6a21\u578b\u53ef\u4ee5\u81ea\u4e3b\u8c03\u8282\u5176\u4e0a\u4e0b\u6587\uff0c\u4e3a\u4e0d\u727a\u7272\u4efb\u52a1\u6027\u80fd\u7684\u6210\u672c\u611f\u77e5\u4ee3\u7406\u7cfb\u7edf\u5f00\u8f9f\u4e86\u9014\u5f84\u3002", "topic": "code agent"}}
{"id": "2601.06487", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06487", "abs": "https://arxiv.org/abs/2601.06487", "authors": ["Qiang Zhang", "Boli Chen", "Fanrui Zhang", "Ruixue Ding", "Shihang Wang", "Qiuchen Wang", "Yinfeng Huang", "Haonan Zhang", "Rongxiang Zhu", "Pengyong Wang", "Ailin Ren", "Xin Li", "Pengjun Xie", "Jiawei Liu", "Ning Guo", "Jingren Zhou", "Zheng-Jun Zha"], "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking", "comment": null, "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.", "AI": {"tldr": "ArenaRL\u63d0\u51fa\u4ece\u70b9\u5f0f\u6807\u91cf\u8bc4\u5206\u8f6c\u5411\u7ec4\u5185\u76f8\u5bf9\u6392\u540d\u7684\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u8fc7\u7a0b\u611f\u77e5\u7684\u6210\u5bf9\u8bc4\u4f30\u548c\u591a\u7ea7\u8bc4\u5206\u6807\u51c6\u89e3\u51b3\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u5956\u52b1\u4fe1\u53f7\u95ee\u9898", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u5f00\u653e\u4efb\u52a1\uff08\u5982\u590d\u6742\u65c5\u884c\u89c4\u5212\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u5ba2\u89c2\u771f\u5b9e\u6807\u7b7e\uff0c\u4f9d\u8d56\u7684\u5956\u52b1\u6a21\u578b\u5b58\u5728\"\u6b67\u89c6\u5d29\u6e83\"\u95ee\u9898\u2014\u2014\u96be\u4ee5\u533a\u5206\u4e0d\u540c\u8f68\u8ff9\u95f4\u7684\u7ec6\u5fae\u4f18\u52bf\uff0c\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u88ab\u566a\u58f0\u4e3b\u5bfc", "method": "\u63d0\u51faArenaRL\u8303\u5f0f\uff1a1\uff09\u8fc7\u7a0b\u611f\u77e5\u6210\u5bf9\u8bc4\u4f30\u673a\u5236\uff0c\u4f7f\u7528\u591a\u7ea7\u8bc4\u5206\u6807\u51c6\u4e3a\u8f68\u8ff9\u5206\u914d\u7ec6\u7c92\u5ea6\u76f8\u5bf9\u5206\u6570\uff1b2\uff09\u6784\u5efa\u7ec4\u5185\u5bf9\u6297\u7ade\u6280\u573a\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u9526\u6807\u8d5b\u7684\u6392\u540d\u65b9\u6848\u83b7\u5f97\u7a33\u5b9a\u4f18\u52bf\u4fe1\u53f7\uff1b3\uff09\u5efa\u7acb\u5355\u6dd8\u6c70\u79cd\u5b50\u65b9\u6848\uff0c\u4ee5O(N)\u590d\u6742\u5ea6\u5b9e\u73b0\u63a5\u8fd1O(N\u00b2)\u5168\u5bf9\u6bd4\u7684\u7cbe\u5ea6", "result": "ArenaRL\u663e\u8457\u4f18\u4e8e\u6807\u51c6RL\u57fa\u7ebf\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u4e3a\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u751f\u6210\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff1b\u540c\u65f6\u6784\u5efa\u4e86Open-Travel\u548cOpen-DeepResearch\u4e24\u4e2a\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u6db5\u76d6\u5b8c\u6574\u8bad\u7ec3\u8bc4\u4f30\u6d41\u7a0b", "conclusion": "\u4ece\u70b9\u5f0f\u8bc4\u5206\u8f6c\u5411\u76f8\u5bf9\u6392\u540d\u80fd\u6709\u6548\u89e3\u51b3\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u5956\u52b1\u4fe1\u53f7\u95ee\u9898\uff0cArenaRL\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5f00\u653e\u4efb\u52a1\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6", "topic": "agentic reinforcement learning"}}
{"id": "2601.07206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07206", "abs": "https://arxiv.org/abs/2601.07206", "authors": ["Hao Li", "Yiqun Zhang", "Zhaoyan Guo", "Chenxu Wang", "Shengji Tang", "Qiaosheng Zhang", "Yang Chen", "Biqing Qi", "Peng Ye", "Lei Bai", "Zhen Wang", "Shuyue Hu"], "title": "LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing", "comment": null, "summary": "Large language model (LLM) routing assigns each query to the most suitable model from an ensemble. We introduce LLMRouterBench, a large-scale benchmark and unified framework for LLM routing. It comprises over 400K instances from 21 datasets and 33 models. Moreover, it provides comprehensive metrics for both performance-oriented routing and performance-cost trade-off routing, and integrates 10 representative routing baselines. Using LLMRouterBench, we systematically re-evaluate the field. While confirming strong model complementarity-the central premise of LLM routing-we find that many routing methods exhibit similar performance under unified evaluation, and several recent approaches, including commercial routers, fail to reliably outperform a simple baseline. Meanwhile, a substantial gap remains to the Oracle, driven primarily by persistent model-recall failures. We further show that backbone embedding models have limited impact, that larger ensembles exhibit diminishing returns compared to careful model curation, and that the benchmark also enables latency-aware analysis. All code and data are available at https://github.com/ynulihao/LLMRouterBench.", "AI": {"tldr": "LLMRouterBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21LLM\u8def\u7531\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b40\u4e07+\u5b9e\u4f8b\u300121\u4e2a\u6570\u636e\u96c6\u548c33\u4e2a\u6a21\u578b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e8610\u79cd\u8def\u7531\u65b9\u6cd5\uff0c\u53d1\u73b0\u8bb8\u591a\u65b9\u6cd5\u8868\u73b0\u76f8\u4f3c\uff0c\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\u4ecd\u6709\u7ade\u4e89\u529b\uff0c\u4e0eOracle\u6027\u80fd\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "LLM\u8def\u7531\u65e8\u5728\u5c06\u67e5\u8be2\u5206\u914d\u7ed9\u96c6\u6210\u6a21\u578b\u4e2d\u6700\u5408\u9002\u7684\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u6765\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u8def\u7531\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u6784\u5efaLLMRouterBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b\u8d85\u8fc740\u4e07\u4e2a\u5b9e\u4f8b\u300121\u4e2a\u6570\u636e\u96c6\u548c33\u4e2a\u6a21\u578b\uff0c\u96c6\u621010\u79cd\u4ee3\u8868\u6027\u8def\u7531\u57fa\u7ebf\uff0c\u63d0\u4f9b\u6027\u80fd\u5bfc\u5411\u548c\u6027\u80fd-\u6210\u672c\u6743\u8861\u7684\u5168\u9762\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u786e\u8ba4\u6a21\u578b\u4e92\u8865\u6027\uff0c\u4f46\u53d1\u73b0\u8bb8\u591a\u8def\u7531\u65b9\u6cd5\u5728\u7edf\u4e00\u8bc4\u4f30\u4e0b\u8868\u73b0\u76f8\u4f3c\uff0c\u5305\u62ec\u5546\u4e1a\u8def\u7531\u5668\u5728\u5185\u7684\u51e0\u79cd\u8fd1\u671f\u65b9\u6cd5\u672a\u80fd\u53ef\u9760\u8d85\u8d8a\u7b80\u5355\u57fa\u7ebf\uff1b\u4e0eOracle\u6027\u80fd\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u7531\u6a21\u578b\u53ec\u56de\u5931\u8d25\u5bfc\u81f4\uff1b\u9aa8\u5e72\u5d4c\u5165\u6a21\u578b\u5f71\u54cd\u6709\u9650\uff0c\u5927\u96c6\u6210\u76f8\u6bd4\u7cbe\u5fc3\u6a21\u578b\u7b5b\u9009\u6536\u76ca\u9012\u51cf\u3002", "conclusion": "LLM\u8def\u7531\u9886\u57df\u9700\u8981\u66f4\u6709\u6548\u7684\u8def\u7531\u65b9\u6cd5\u6765\u7f29\u5c0f\u4e0eOracle\u7684\u5dee\u8ddd\uff0c\u6a21\u578b\u7b5b\u9009\u6bd4\u7b80\u5355\u589e\u52a0\u96c6\u6210\u89c4\u6a21\u66f4\u91cd\u8981\uff0c\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\u5ef6\u8fdf\u611f\u77e5\u5206\u6790\u3002", "topic": "agent analysis"}}
{"id": "2601.07224", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07224", "abs": "https://arxiv.org/abs/2601.07224", "authors": ["Yang Zhao", "Yangou Ouyang", "Xiao Ding", "Hepeng Wang", "Bibo Cai", "Kai Xiong", "Jinglong Gao", "Zhouhao Sun", "Li Du", "Bing Qin", "Ting Liu"], "title": "Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration", "comment": null, "summary": "While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.", "AI": {"tldr": "PRISM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u51b2\u7a81\u7684\u6570\u636e\u5206\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316LLM\u4ee3\u7406\u7684\u6df7\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u8bad\u7ec3\uff0c\u901a\u8fc7\u68af\u5ea6\u7a7a\u95f4\u51e0\u4f55\u5206\u6790\u5c06\u9ad8\u51b2\u7a81\u6570\u636e\u5206\u914d\u7ed9RL\uff0c\u4f4e\u51b2\u7a81\u6570\u636e\u5206\u914d\u7ed9SFT\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u3002", "motivation": "\u5f53\u524d\u6df7\u5408SFT+RL\u8bad\u7ec3\u8303\u5f0f\u4e2d\u7684\u6570\u636e\u5206\u914d\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u8868\u9762\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u65e0\u6cd5\u8bca\u65ad\u5185\u5728\u5b66\u4e60\u9700\u6c42\u3002\u7531\u4e8eSFT\u901a\u8fc7\u6a21\u4eff\u8fdb\u884c\u6a21\u5f0f\u5de9\u56fa\uff0c\u800cRL\u901a\u8fc7\u63a2\u7d22\u9a71\u52a8\u7ed3\u6784\u9002\u5e94\uff0c\u6570\u636e\u4e0e\u8fd9\u4e9b\u529f\u80fd\u89d2\u8272\u7684\u9519\u914d\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u4f18\u5316\u5e72\u6270\u3002", "method": "PRISM\u57fa\u4e8e\u56fe\u5f0f\u7406\u8bba\uff0c\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u7684\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u6765\u4ef2\u88c1\u6570\u636e\u3002\u8bc6\u522b\u51fa\u89e6\u53d1\u9ad8\u7a7a\u95f4\u96c6\u4e2d\u5ea6\u7684\u6570\u636e\u4f5c\u4e3a\u9ad8\u51b2\u7a81\u4fe1\u53f7\uff0c\u9700\u8981RL\u8fdb\u884c\u7ed3\u6784\u91cd\u7ec4\uff1b\u800c\u4ea7\u751f\u6269\u6563\u66f4\u65b0\u7684\u6570\u636e\u5219\u8def\u7531\u5230SFT\u8fdb\u884c\u9ad8\u6548\u5de9\u56fa\u3002", "result": "\u5728WebShop\u548cALFWorld\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPRISM\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e86\u9ad8\u8fbe3.22\u500d\u3002", "conclusion": "\u57fa\u4e8e\u5185\u90e8\u4f18\u5316\u673a\u5236\u89e3\u8026\u6570\u636e\u5bf9\u4e8e\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u4ee3\u7406\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002PRISM\u7684\u8ba4\u77e5\u51b2\u7a81\u611f\u77e5\u6846\u67b6\u4e3a\u6df7\u5408\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6570\u636e\u5206\u914d\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.06786", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06786", "abs": "https://arxiv.org/abs/2601.06786", "authors": ["Jewon Yeom", "Jaewon Sok", "Seonghyeon Park", "Jeongjae Park", "Taesup Kim"], "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs", "comment": null, "summary": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.", "AI": {"tldr": "\u63d0\u51faEpiCaR\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63a8\u7406\u6027\u80fd\u548c\u6821\u51c6\uff0c\u89e3\u51b3LLM\u5728\u8fed\u4ee3\u81ea\u8bad\u7ec3\u4e2d\u8fc7\u5ea6\u81ea\u4fe1\u3001\u4e27\u5931\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u51c6\u786e\u6027\u548c\u6821\u51c6\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8fed\u4ee3\u81ea\u8bad\u7ec3\uff0c\u867d\u7136\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u3001\u4e27\u5931\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u80fd\u529b\uff0c\u5f62\u6210\u6821\u51c6\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u6821\u51c6\u63a8\u7406(EpiCaR)\u8bad\u7ec3\u76ee\u6807\uff0c\u5c06\u63a8\u7406\u8bad\u7ec3\u91cd\u6784\u4e3a\u8ba4\u77e5\u5b66\u4e60\u95ee\u9898\uff0c\u8054\u5408\u4f18\u5316\u63a8\u7406\u6027\u80fd\u548c\u6821\u51c6\uff0c\u5728\u8fed\u4ee3\u76d1\u7763\u5fae\u8c03\u6846\u67b6\u4e2d\u4f7f\u7528\u663e\u5f0f\u81ea\u8bc4\u4f30\u4fe1\u53f7\u3002", "result": "\u5728Llama-3\u548cQwen-3\u7cfb\u5217\u6a21\u578b\u4e0a\uff0cEpiCaR\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u57283B+\u53c2\u6570\u6a21\u578b\u4e2d\u3002\u5728OOD\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u6709\u6548\u6cdb\u5316\uff0c\u63a8\u7406\u8ba1\u7b97\u51cf\u5c113\u500d\u3002", "conclusion": "EpiCaR\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63a8\u7406\u6027\u80fd\u548c\u6821\u51c6\uff0c\u89e3\u51b3\u4e86LLM\u8fed\u4ee3\u8bad\u7ec3\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u6821\u51c6\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2601.07226", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07226", "abs": "https://arxiv.org/abs/2601.07226", "authors": ["Seongyun Lee", "Yongrae Jo", "Minju Seo", "Moontae Lee", "Minjoon Seo"], "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors", "comment": "Preprint", "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.", "AI": {"tldr": "NoisyBench\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6RAG\u3001\u63a8\u7406\u3001\u5bf9\u9f50\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u4efb\u52a1\uff0c\u53d1\u73b0\u5f53\u524dSOTA\u6a21\u578b\u5728\u566a\u58f0\u5e72\u6270\u4e0b\u6027\u80fd\u4e0b\u964d\u8fbe80%\uff0c\u5e76\u63d0\u51faRationale-Aware Reward\u65b9\u6cd5\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5916\u90e8\u4fe1\u606f\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u4fe1\u606f\u5f80\u5f80\u5305\u542b\u566a\u58f0\uff0c\u800c\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u4e8e\u7406\u60f3\u5316\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u566a\u58f0\u95ee\u9898\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faNoisyBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5bf9\u968f\u673a\u6587\u6863\u3001\u65e0\u5173\u804a\u5929\u5386\u53f2\u3001\u56f0\u96be\u8d1f\u6837\u672c\u7b49\u566a\u58f0\u7c7b\u578b\u7684\u9c81\u68d2\u6027\u3002\u63d0\u51faRationale-Aware Reward\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u52b1\u6a21\u578b\u8bc6\u522b\u566a\u58f0\u4e2d\u6709\u7528\u4fe1\u606f\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u53d1\u73b0SOTA\u6a21\u578b\u5728\u566a\u58f0\u5e72\u6270\u4e0b\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe80%\uff1b\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4f1a\u653e\u5927\u9519\u8bef\uff1b\u566a\u58f0\u4f1a\u89e6\u53d1\u6a21\u578b\u4e0d\u5bf9\u9f50\u884c\u4e3a\uff1b\u63d0\u793a\u5de5\u7a0b\u3001\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3001SFT\u548c\u7ed3\u679c\u5956\u52b1RL\u7b49\u65b9\u6cd5\u65e0\u6cd5\u786e\u4fdd\u9c81\u68d2\u6027\uff1bRARE\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\uff1b\u53d1\u73b0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u589e\u52a0\u53cd\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u53cd\u5411\u7f29\u653e\u8d8b\u52bf\u3002", "conclusion": "\u566a\u58f0\u5bf9AI\u7cfb\u7edf\u9c81\u68d2\u6027\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u566a\u58f0\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u5982RARE\uff0c\u5e76\u91cd\u65b0\u601d\u8003\u6a21\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65b9\u5f0f\uff0c\u4ee5\u6784\u5efa\u4e0b\u4e00\u4ee3\u9c81\u68d2\u7684\u63a8\u7406\u667a\u80fd\u4f53\u3002", "topic": "agent analysis"}}
{"id": "2601.07232", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07232", "abs": "https://arxiv.org/abs/2601.07232", "authors": ["Olivia Shanhong Liu", "Pai Chet Ng", "De Wen Soh", "Konstantinos N. Plataniotis"], "title": "Yes FLoReNce, I Will Do Better Next Time! Agentic Feedback Reasoning for Humorous Meme Detection", "comment": "LaMAS@AAAI 2026 (Oral)", "summary": "Humorous memes blend visual and textual cues to convey irony, satire, or social commentary, posing unique challenges for AI systems that must interpret intent rather than surface correlations. Existing multimodal or prompting-based models generate explanations for humor but operate in an open loop,lacking the ability to critique or refine their reasoning once a prediction is made. We propose FLoReNce, an agentic feedback reasoning framework that treats meme understanding as a closed-loop process during learning and an open-loop process during inference. In the closed loop, a reasoning agent is critiqued by a judge; the error and semantic feedback are converted into control signals and stored in a feedback-informed, non-parametric knowledge base. At inference, the model retrieves similar judged experiences from this KB and uses them to modulate its prompt, enabling better, self-aligned reasoning without finetuning. On the PrideMM dataset, FLoReNce improves both predictive performance and explanation quality over static multimodal baselines, showing that feedback-regulated prompting is a viable path to adaptive meme humor understanding.", "AI": {"tldr": "FLoReNce\u662f\u4e00\u4e2a\u57fa\u4e8e\u53cd\u9988\u63a8\u7406\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u5b66\u4e60\uff08\u63a8\u7406\u667a\u80fd\u4f53\u63a5\u53d7\u8bc4\u5224\u8005\u6279\u8bc4\uff09\u548c\u5f00\u73af\u63a8\u7406\uff08\u4ece\u77e5\u8bc6\u5e93\u68c0\u7d22\u7c7b\u4f3c\u7ecf\u9a8c\uff09\u6765\u63d0\u5347\u5bf9\u5e7d\u9ed8\u8868\u60c5\u5305\u7684\u7406\u89e3\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u6216\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\u5728\u7406\u89e3\u5e7d\u9ed8\u8868\u60c5\u5305\u65f6\u53ea\u80fd\u751f\u6210\u89e3\u91ca\uff0c\u4f46\u7f3a\u4e4f\u5728\u9884\u6d4b\u540e\u8fdb\u884c\u6279\u5224\u6216\u7cbe\u70bc\u63a8\u7406\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u5f62\u6210\u95ed\u73af\u53cd\u9988\u673a\u5236\u3002", "method": "\u63d0\u51faFLoReNce\u6846\u67b6\uff1a1\uff09\u5b66\u4e60\u9636\u6bb5\u91c7\u7528\u95ed\u73af\u8fc7\u7a0b\uff0c\u63a8\u7406\u667a\u80fd\u4f53\u63a5\u53d7\u8bc4\u5224\u8005\u6279\u8bc4\uff0c\u5c06\u9519\u8bef\u548c\u8bed\u4e49\u53cd\u9988\u8f6c\u5316\u4e3a\u63a7\u5236\u4fe1\u53f7\u5e76\u5b58\u50a8\u5728\u975e\u53c2\u6570\u77e5\u8bc6\u5e93\u4e2d\uff1b2\uff09\u63a8\u7406\u9636\u6bb5\u91c7\u7528\u5f00\u73af\u8fc7\u7a0b\uff0c\u4ece\u77e5\u8bc6\u5e93\u68c0\u7d22\u7c7b\u4f3c\u8bc4\u5224\u7ecf\u9a8c\u6765\u8c03\u8282\u63d0\u793a\uff0c\u5b9e\u73b0\u81ea\u5bf9\u9f50\u63a8\u7406\u3002", "result": "\u5728PrideMM\u6570\u636e\u96c6\u4e0a\uff0cFLoReNce\u5728\u9884\u6d4b\u6027\u80fd\u548c\u89e3\u91ca\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u9759\u6001\u591a\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\uff0c\u8868\u660e\u57fa\u4e8e\u53cd\u9988\u8c03\u8282\u7684\u63d0\u793a\u662f\u81ea\u9002\u5e94\u7406\u89e3\u8868\u60c5\u5305\u5e7d\u9ed8\u7684\u6709\u6548\u9014\u5f84\u3002", "conclusion": "\u53cd\u9988\u8c03\u8282\u7684\u63d0\u793a\u673a\u5236\u662f\u63d0\u5347AI\u7cfb\u7edf\u7406\u89e3\u5e7d\u9ed8\u8868\u60c5\u5305\u610f\u56fe\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u73af\u5b66\u4e60\u548c\u5f00\u73af\u63a8\u7406\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5fae\u8c03\u7684\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.07239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07239", "abs": "https://arxiv.org/abs/2601.07239", "authors": ["Tanmay Joshi", "Shourya Aggarwal", "Anusa Saha", "Aadi Pandey", "Shreyash Dhoot", "Vighnesh Rai", "Raxit Goswami", "Aman Chadha", "Vinija Jain", "Amitava Das"], "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition", "comment": null, "summary": "Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability.\n  In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled.\n  Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53cd\u5bf9LLM\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u8ba4\u4e3a\u5176\u4f1a\u627c\u6740\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b\u3001\u6291\u5236\u6d8c\u73b0\u80fd\u529b\u3001\u5f31\u5316\u5b89\u5168\u5bf9\u9f50\uff0c\u5e76\u4e3b\u5f20\u91c7\u7528\u968f\u673aCHAOS\u65b9\u6cd5\u5c06\u5206\u5e03\u53d8\u5f02\u6027\u4f5c\u4e3a\u4fe1\u53f7\u8fdb\u884c\u6d4b\u91cf\u548c\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u8ffd\u6c42\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u4f46LLM\u672c\u8d28\u4e0a\u662f\u6761\u4ef6\u6982\u7387\u5206\u5e03\u800c\u975e\u56fa\u5b9a\u51fd\u6570\u3002\u786e\u5b9a\u6027\u63a8\u7406\u4f1a\u63a9\u76d6LLM\u7684\u5173\u952e\u8ba4\u77e5\u7279\u6027\uff0c\u5305\u62ec\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u6d8c\u73b0\u80fd\u529b\u3001\u591a\u8def\u5f84\u63a8\u7406\u548c\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51faStochastic CHAOS\u65b9\u6cd5\uff0c\u5c06\u5206\u5e03\u53d8\u5f02\u6027\u89c6\u4e3a\u9700\u8981\u6d4b\u91cf\u548c\u63a7\u5236\u7684\u4fe1\u53f7\u3002\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u5c55\u793a\u786e\u5b9a\u6027\u63a8\u7406\u7684\u7cfb\u7edf\u6027\u8bef\u5bfc\u6027\uff0c\u5305\u62ec\u5355\u6837\u672c\u8bc4\u4f30\u4f4e\u4f30\u80fd\u529b\u4e0e\u8106\u5f31\u6027\u3001\u8d2a\u5a6a\u89e3\u7801\u6d88\u9664\u6d8c\u73b0\u80fd\u529b\u76f8\u53d8\u3001\u591a\u8def\u5f84\u63a8\u7406\u9000\u5316\u7b49\u95ee\u9898\u3002", "result": "\u786e\u5b9a\u6027\u63a8\u7406\u4f1a\u7cfb\u7edf\u6027\u8bef\u5bfc\u8bc4\u4f30\uff1a\u4f4e\u4f30\u6a21\u578b\u80fd\u529b\u548c\u8106\u5f31\u6027\u3001\u63a9\u76d6\u5931\u8d25\u6982\u7387\u3001\u6d88\u9664\u6d8c\u73b0\u80fd\u529b\u7684\u76f8\u53d8\u73b0\u8c61\u3001\u964d\u4f4e\u591a\u8def\u5f84\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u8bca\u65ad\u6d1e\u5bdf\u529b\uff0c\u5e76\u9690\u85cf\u7f55\u89c1\u4f46\u5371\u9669\u7684\u5b89\u5168\u98ce\u9669\u884c\u4e3a\u3002", "conclusion": "LLM\u4e0d\u5e94\u8ffd\u6c42\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u800c\u5e94\u63a5\u53d7\u5176\u6982\u7387\u672c\u8d28\u3002\u5206\u5e03\u53d8\u5f02\u6027\u662fLLM\u8ba4\u77e5\u7279\u6027\u7684\u6838\u5fc3\uff0c\u5e94\u4f5c\u4e3a\u4fe1\u53f7\u8fdb\u884c\u6d4b\u91cf\u548c\u63a7\u5236\uff0c\u800c\u975e\u6d88\u9664\u3002\u786e\u5b9a\u6027\u63a8\u7406\u4f1a\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u3001\u5b89\u5168\u5bf9\u9f50\u548c\u5b9e\u9645\u90e8\u7f72\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2601.06818", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06818", "abs": "https://arxiv.org/abs/2601.06818", "authors": ["Xuannan Liu", "Xiao Yang", "Zekun Li", "Peipei Li", "Ran He"], "title": "AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents", "comment": "Project page: https://liuxuannan.github.io/AgentHallu.github.io/", "summary": "As LLM-based agents operate over sequential multi-step reasoning, hallucinations arising at intermediate steps risk propagating along the trajectory, thus degrading overall reliability. Unlike hallucination detection in single-turn responses, diagnosing hallucinations in multi-step workflows requires identifying which step causes the initial divergence. To fill this gap, we propose a new research task, automated hallucination attribution of LLM-based agents, aiming to identify the step responsible for the hallucination and explain why. To support this task, we introduce AgentHallu, a comprehensive benchmark with: (1) 693 high-quality trajectories spanning 7 agent frameworks and 5 domains, (2) a hallucination taxonomy organized into 5 categories (Planning, Retrieval, Reasoning, Human-Interaction, and Tool-Use) and 14 sub-categories, and (3) multi-level annotations curated by humans, covering binary labels, hallucination-responsible steps, and causal explanations. We evaluate 13 leading models, and results show the task is challenging even for top-tier models (like GPT-5, Gemini-2.5-Pro). The best-performing model achieves only 41.1\\% step localization accuracy, where tool-use hallucinations are the most challenging at just 11.6\\%. We believe AgentHallu will catalyze future research into developing robust, transparent, and reliable agentic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentHallu\u57fa\u51c6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5f52\u56e0LLM\u667a\u80fd\u4f53\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5305\u542b693\u6761\u8f68\u8ff9\u30015\u7c7b\u5e7b\u89c9\u5206\u7c7b\u548c\u4eba\u5de5\u6807\u6ce8\uff0c\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u6b65\u9aa4\u5b9a\u4f4d\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u6700\u4f73\u4ec541.1%\u51c6\u786e\u7387\uff09\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u591a\u6b65\u63a8\u7406\u4e2d\uff0c\u4e2d\u95f4\u6b65\u9aa4\u7684\u5e7b\u89c9\u4f1a\u6cbf\u8f68\u8ff9\u4f20\u64ad\uff0c\u964d\u4f4e\u6574\u4f53\u53ef\u9760\u6027\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u54cd\u5e94\u7684\u5e7b\u89c9\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6b65\u5de5\u4f5c\u6d41\u4e2d\u5e7b\u89c9\u8d77\u6e90\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\"\u81ea\u52a8\u5316\u5e7b\u89c9\u5f52\u56e0\"\u65b0\u4efb\u52a1\uff0c\u5e76\u6784\u5efaAgentHallu\u57fa\u51c6\uff1a\u5305\u542b693\u6761\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff08\u8986\u76d67\u79cd\u667a\u80fd\u4f53\u6846\u67b6\u548c5\u4e2a\u9886\u57df\uff09\u30015\u5927\u7c7b14\u5b50\u7c7b\u7684\u5e7b\u89c9\u5206\u7c7b\u4f53\u7cfb\uff0c\u4ee5\u53ca\u4eba\u5de5\u6807\u6ce8\u7684\u591a\u7ea7\u6807\u7b7e\uff08\u4e8c\u503c\u6807\u7b7e\u3001\u8d23\u4efb\u6b65\u9aa4\u3001\u56e0\u679c\u89e3\u91ca\uff09\u3002\u8bc4\u4f30\u4e8613\u4e2a\u9886\u5148\u6a21\u578b\u3002", "result": "\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u5373\u4f7f\u662f\u9876\u7ea7\u6a21\u578b\uff08\u5982GPT-5\u3001Gemini-2.5-Pro\uff09\u8868\u73b0\u4e5f\u4e0d\u4f73\u3002\u6700\u4f73\u6a21\u578b\u5728\u6b65\u9aa4\u5b9a\u4f4d\u51c6\u786e\u7387\u4ec541.1%\uff0c\u5176\u4e2d\u5de5\u5177\u4f7f\u7528\u7c7b\u5e7b\u89c9\u6700\u56f0\u96be\uff08\u4ec511.6%\u51c6\u786e\u7387\uff09\u3002", "conclusion": "AgentHallu\u57fa\u51c6\u5c06\u63a8\u52a8\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u900f\u660e\u548c\u53ef\u9760\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u81ea\u52a8\u5316\u5e7b\u89c9\u5f52\u56e0\u662f\u63d0\u5347\u667a\u80fd\u4f53\u53ef\u9760\u6027\u7684\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.07245", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07245", "abs": "https://arxiv.org/abs/2601.07245", "authors": ["Pranav Kallem"], "title": "Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models", "comment": null, "summary": "Large language models (LLMs) achieve strong aver- age performance yet remain unreliable at the instance level, with frequent hallucinations, brittle failures, and poorly calibrated confidence. We study reliability through the lens of multi-model consensus: given responses from several heterogeneous LLMs, can we learn which answer is most likely correct for a given query? We introduce a Multi-Model Consensus Reasoning Engine that treats the set of LLM outputs as input to a supervised meta-learner. The system maps natural language responses into structured features using semantic embeddings, pairwise similarity and clustering statistics, lexical and structural cues, reasoning-quality scores, confidence estimates, and model-specific priors, and then applies gradient-boosted trees, listwise ranking, and graph neural networks over similarity graphs of answers. Using three open-weight LLMs evaluated on compact, resource- constrained subsets of GSM8K, ARC-Challenge, HellaSwag, and TruthfulQA, our best graph-attention-based consensus model improves macro-average accuracy by 4.6 percentage points over the strongest single LLM and by 8.1 points over majority vote, while also yielding lower Brier scores and fewer TruthfulQA hal- lucinations. Ablation and feature-importance analyses show that semantic agreement and clustering features are most influential, with reasoning-quality and model-prior features providing com- plementary gains, suggesting supervised multi-model consensus is a practical route toward more reliable LLM behavior, even in a modest single-machine setup.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u578b\u5171\u8bc6\u63a8\u7406\u5f15\u64ce\uff0c\u901a\u8fc7\u76d1\u7763\u5143\u5b66\u4e60\u6574\u5408\u591a\u4e2aLLM\u8f93\u51fa\uff0c\u63d0\u5347\u5b9e\u4f8b\u7ea7\u53ef\u9760\u6027", "motivation": "LLM\u5728\u5e73\u5747\u6027\u80fd\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u4f8b\u5c42\u9762\u4e0d\u53ef\u9760\uff0c\u5b58\u5728\u5e7b\u89c9\u3001\u8106\u5f31\u5931\u8d25\u548c\u6821\u51c6\u4e0d\u826f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u63d0\u5347\u53ef\u9760\u6027", "method": "\u5f00\u53d1\u591a\u6a21\u578b\u5171\u8bc6\u63a8\u7406\u5f15\u64ce\uff0c\u5c06\u591a\u4e2aLLM\u8f93\u51fa\u6620\u5c04\u4e3a\u7ed3\u6784\u5316\u7279\u5f81\uff08\u8bed\u4e49\u5d4c\u5165\u3001\u76f8\u4f3c\u6027\u3001\u805a\u7c7b\u7edf\u8ba1\u3001\u8bcd\u6c47\u7ed3\u6784\u7ebf\u7d22\u3001\u63a8\u7406\u8d28\u91cf\u5206\u6570\u3001\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3001\u6a21\u578b\u5148\u9a8c\uff09\uff0c\u5e94\u7528\u68af\u5ea6\u63d0\u5347\u6811\u3001\u5217\u8868\u6392\u5e8f\u548c\u56fe\u795e\u7ecf\u7f51\u7edc", "result": "\u5728GSM8K\u3001ARC-Challenge\u3001HellaSwag\u548cTruthfulQA\u4e0a\uff0c\u6700\u4f73\u56fe\u6ce8\u610f\u529b\u5171\u8bc6\u6a21\u578b\u6bd4\u6700\u5f3a\u5355LLM\u63d0\u53474.6\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4\u591a\u6570\u6295\u7968\u63d0\u53478.1\u4e2a\u767e\u5206\u70b9\uff0c\u964d\u4f4eBrier\u5206\u6570\uff0c\u51cf\u5c11TruthfulQA\u5e7b\u89c9", "conclusion": "\u76d1\u7763\u591a\u6a21\u578b\u5171\u8bc6\u662f\u63d0\u5347LLM\u53ef\u9760\u6027\u7684\u5b9e\u7528\u9014\u5f84\uff0c\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u805a\u7c7b\u7279\u5f81\u6700\u91cd\u8981\uff0c\u63a8\u7406\u8d28\u91cf\u548c\u6a21\u578b\u5148\u9a8c\u7279\u5f81\u63d0\u4f9b\u8865\u5145\u589e\u76ca", "topic": "agent analysis"}}
{"id": "2601.07296", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07296", "abs": "https://arxiv.org/abs/2601.07296", "authors": ["Yujin Zhou", "Chuxue Cao", "Jinluan Yang", "Lijun Wu", "Conghui He", "Sirui Han", "Yike Guo"], "title": "LRAS: Advanced Legal Reasoning with Agentic Search", "comment": null, "summary": "While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on \"closed-loop reasoning\" derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric \"closed-loop thinking\" to dynamic and interactive \"Active Inquiry\". By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32\\%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.", "AI": {"tldr": "LRAS\u6846\u67b6\u901a\u8fc7\u5c06\u6cd5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u9759\u6001\u53c2\u6570\u5316\"\u95ed\u73af\u601d\u7ef4\"\u8f6c\u53d8\u4e3a\u52a8\u6001\u4ea4\u4e92\u5f0f\"\u4e3b\u52a8\u8be2\u95ee\"\uff0c\u7ed3\u5408\u5185\u7701\u6a21\u4eff\u5b66\u4e60\u548c\u96be\u5ea6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u7684\"\u95ed\u73af\u63a8\u7406\"\uff0c\u7f3a\u4e4f\u5bf9\u77e5\u8bc6\u8fb9\u754c\u7684\u81ea\u6211\u8ba4\u77e5\uff0c\u5bfc\u81f4\u81ea\u4fe1\u4f46\u9519\u8bef\u7684\u7ed3\u8bba\u3002\u6cd5\u5f8b\u9886\u57df\u5bf9\u7a0b\u5e8f\u4e25\u8c28\u6027\u548c\u6cd5\u5f8b\u903b\u8f91\u6709\u4e25\u683c\u8981\u6c42\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faLRAS\u6846\u67b6\uff0c\u6574\u5408\u5185\u7701\u6a21\u4eff\u5b66\u4e60\u548c\u96be\u5ea6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\u5e76\u5904\u7406\u6cd5\u5f8b\u63a8\u7406\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4ece\"\u95ed\u73af\u601d\u7ef4\"\u5230\"\u4e3b\u52a8\u8be2\u95ee\"\u7684\u8f6c\u53d8\u3002", "result": "LRAS\u5728\u5b9e\u8bc1\u7ed3\u679c\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf8.2-32%\uff0c\u5728\u9700\u8981\u53ef\u9760\u77e5\u8bc6\u7684\u6df1\u5ea6\u63a8\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u6700\u4e3a\u663e\u8457\u3002", "conclusion": "LRAS\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6cd5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u8fb9\u754c\u8bc6\u522b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6cd5\u5f8bAI\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.06606", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06606", "abs": "https://arxiv.org/abs/2601.06606", "authors": ["Rishiraj Saha Roy", "Chris Hinze", "Luzian Hahn", "Fabian Kuech"], "title": "CEDAR: Context Engineering for Agentic Data Science", "comment": "Accepted at ECIR 2026", "summary": "We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.", "AI": {"tldr": "CEDAR\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u89e3\u51b3LLM\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u6311\u6218\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u63d0\u793a\u548c\u5206\u79bb\u7684\u89c4\u5212/\u4ee3\u7801\u751f\u6210\u667a\u80fd\u4f53\uff0c\u5728\u672c\u5730\u5904\u7406\u6570\u636e\u5e76\u4fdd\u6301\u4e0a\u4e0b\u6587\u53ef\u8bfb\u6027\u3002", "motivation": "\u4f7f\u7528LLM\u89e3\u51b3\u6570\u636e\u79d1\u5b66\u95ee\u9898\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u4f46\u5177\u6709\u5de8\u5927\u5e02\u573a\u4ef7\u503c\u7684\u9886\u57df\u3002\u9762\u4e34\u7684\u6311\u6218\u5305\u62ec\u4efb\u52a1\u590d\u6742\u6027\u3001\u6570\u636e\u89c4\u6a21\u3001\u8ba1\u7b97\u9650\u5236\u548c\u4e0a\u4e0b\u6587\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\uff1a1) \u5728\u521d\u59cb\u63d0\u793a\u4e2d\u5f15\u5165\u6570\u636e\u79d1\u5b66\u7279\u5b9a\u7684\u7ed3\u6784\u5316\u8f93\u5165\u5b57\u6bb5\uff1b2) \u4f7f\u7528\u5206\u79bb\u7684LLM\u667a\u80fd\u4f53\u751f\u6210\u4ea4\u9519\u7684\u89c4\u5212\u5757\u548c\u4ee3\u7801\u5757\u5e8f\u5217\uff1b3) \u901a\u8fc7\u51fd\u6570\u8c03\u7528\u751f\u6210\u4e2d\u95f4\u6587\u672c\u548cPython\u4ee3\u7801\uff0c\u786e\u4fdd\u6570\u636e\u672c\u5730\u5904\u7406\uff1b4) \u5f15\u5165\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u548c\u667a\u80fd\u5386\u53f2\u6e32\u67d3\u5b9e\u73b0\u5bb9\u9519\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002", "result": "\u901a\u8fc7\u7ecf\u5178\u7684Kaggle\u6311\u6218\u9a8c\u8bc1\u4e86\u8be5\u667a\u80fd\u4f53\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u53ef\u4ee5\u7f13\u89e3\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u5404\u79cd\u6311\u6218\uff0c\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u5316\u89e3\u51b3\u590d\u6742\u7684\u6570\u636e\u79d1\u5b66\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2601.07309", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07309", "abs": "https://arxiv.org/abs/2601.07309", "authors": ["Zhuoka Feng", "Kang Chen", "Sihan Zhao", "Kai Xiong", "Yaoning Wang", "Minshen Yu", "Junjie Nian", "Changyi Xiao", "Yixin Cao", "Yugang Jiang"], "title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging", "comment": "17 pages, 12 figures. Project page: https://arkazhuo.github.io/ARM-homepage/", "summary": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.", "AI": {"tldr": "ARM\u662f\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u3001\u89d2\u8272\u6761\u4ef6\u795e\u7ecf\u5143\u79fb\u690d\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9LLM\u667a\u80fd\u4f53\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4ea4\u4e92\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u73af\u5883\uff0c\u7f3a\u4e4f\u8de8\u73af\u5883\u7684\u9c81\u68d2\u9002\u5e94\u6027\u3002\u6a21\u578b\u878d\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u6574\u5408\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u8f6e\u667a\u80fd\u4f53\u4ea4\u4e92\u573a\u666f\u3002", "method": "\u63d0\u51faAgent-Role Merging (ARM)\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u6b65\u6846\u67b6\uff1a1) \u6784\u5efa\u878d\u5408\u9aa8\u5e72\u6a21\u578b\uff0c2) \u57fa\u4e8e\u89d2\u8272\u6761\u4ef6\u6fc0\u6d3b\u5206\u6790\u8fdb\u884c\u9009\u62e9\uff0c3) \u795e\u7ecf\u5143\u79fb\u690d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u548c\u89d2\u8272\u6761\u4ef6\u795e\u7ecf\u5143\u79fb\u690d\uff0c\u5b9e\u73b0\u65e0\u9700\u68af\u5ea6\u4f18\u5316\u7684\u6a21\u578b\u878d\u5408\u3002", "result": "ARM\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u548c\u9886\u57df\u7279\u5b9a\u4e13\u5bb6\u6a21\u578b\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u8de8\u57fa\u51c6\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u3002", "conclusion": "ARM\u6210\u529f\u5c06\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4ece\u9759\u6001\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u6269\u5c55\u5230\u591a\u8f6e\u667a\u80fd\u4f53\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.06633", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.06633", "abs": "https://arxiv.org/abs/2601.06633", "authors": ["Zhangqi Duan", "Nigel Fernandez", "Andrew Lan"], "title": "KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks", "comment": null, "summary": "Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.", "AI": {"tldr": "KASER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u77e5\u8bc6\u5bf9\u9f50\u5b66\u751f\u9519\u8bef\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u9884\u6d4b\u5b66\u751f\u5728\u5f00\u653e\u5f0f\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\uff0c\u901a\u8fc7\u6df7\u5408\u5956\u52b1\u673a\u5236\u63d0\u5347\u4ee3\u7801\u76f8\u4f3c\u5ea6\u3001\u9519\u8bef\u5339\u914d\u548c\u9884\u6d4b\u591a\u6837\u6027\u3002", "motivation": "\u5f00\u653e\u5f0f\u7f16\u7a0b\u4efb\u52a1\u80fd\u6df1\u5165\u53cd\u6620\u5b66\u751f\u77e5\u8bc6\uff0c\u4f46\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u5b66\u751f\u9519\u8bef\u65f6\u5b58\u5728\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5b66\u751f\u56de\u7b54\u5728\u8bed\u6cd5\u3001\u98ce\u683c\u548c\u89e3\u51b3\u65b9\u6848\u4e0a\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faKASER\u65b9\u6cd5\uff0c\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u4ee3\u7801\u4e0e\u771f\u5b9e\u7b54\u6848\u7684\u76f8\u4f3c\u5ea6\u3001\u9519\u8bef\u5339\u914d\u5ea6\u3001\u4ee3\u7801\u9884\u6d4b\u591a\u6837\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e24\u7ea7\u8bc4\u4f30\uff1a\u5728\u6bcf\u5b66\u751f-\u95ee\u9898\u5bf9\u7ea7\u522b\uff0c\u65b9\u6cd5\u5728\u4ee3\u7801\u548c\u9519\u8bef\u9884\u6d4b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff1b\u5728\u6bcf\u95ee\u9898\u7ea7\u522b\uff0c\u5728\u9519\u8bef\u8986\u76d6\u7387\u548c\u6a21\u62df\u4ee3\u7801\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "KASER\u901a\u8fc7\u77e5\u8bc6\u5bf9\u9f50\u548c\u5f3a\u5316\u5b66\u4e60\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5b66\u751f\u9519\u8bef\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86LLM\u5728\u6a21\u62df\u5b66\u751f\u9519\u8bef\u65f6\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.07342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07342", "abs": "https://arxiv.org/abs/2601.07342", "authors": ["Nicolas Tacheny"], "title": "Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure", "comment": null, "summary": "Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model.\n  In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information.\n  This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u667a\u80fd\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7MCP\u534f\u8bae\u4f7f\u7528\u5de5\u5177\u81ea\u4e3b\u5bfc\u822a\u57fa\u7840\u8bbe\u65bd\u6a21\u578b\u8fdb\u884c\u6839\u56e0\u5206\u6790\uff0c\u66ff\u4ee3\u4f20\u7edf\u786c\u7f16\u7801\u7684\u56fe\u904d\u5386\u7b97\u6cd5\u3002", "motivation": "\u7535\u4fe1\u548c\u6570\u636e\u4e2d\u5fc3\u57fa\u7840\u8bbe\u65bd\u91c7\u7528\u591a\u5c42\u670d\u52a1\u8d44\u6e90\u6a21\u578b\uff0c\u6545\u969c\u4f1a\u5728\u7269\u7406\u548c\u903b\u8f91\u7ec4\u4ef6\u95f4\u4f20\u64ad\u5f71\u54cd\u591a\u4e2a\u5ba2\u6237\u3002\u4f20\u7edf\u6839\u56e0\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u786c\u7f16\u7801\u56fe\u904d\u5386\u7b97\u6cd5\u6216\u57fa\u4e8e\u89c4\u5219\u7684\u5173\u8054\u5f15\u64ce\uff0c\u7ef4\u62a4\u6210\u672c\u9ad8\u4e14\u4e0e\u57fa\u7840\u8bbe\u65bd\u6a21\u578b\u7d27\u8026\u5408\u3002", "method": "\u5f15\u5165\u667a\u80fd\u8bca\u65ad\u6846\u67b6\uff0c\u8ba9LLM\u901a\u8fc7Model Context Protocol\uff08MCP\uff09\u66b4\u9732\u7684\u53d7\u9650\u5de5\u5177\u7a7a\u95f4\u8fdb\u884c\u9010\u6b65\u8c03\u67e5\u3002\u4ee3\u7406\u81ea\u4e3b\u5bfc\u822a\u57fa\u7840\u8bbe\u65bd\u6a21\u578b\uff0c\u8c03\u7528\u670d\u52a1\u67e5\u627e\u3001\u4f9d\u8d56\u68c0\u7d22\u3001\u7ed3\u6784\u5316/\u975e\u7ed3\u6784\u5316\u6570\u636e\u3001\u4e8b\u4ef6\u5206\u6790\u548c\u5f71\u54cd\u53d1\u73b0\u7b49\u5de5\u5177\u3002\u5b9a\u4e49\u8c03\u67e5\u534f\u8bae\u6765\u7ed3\u6784\u5316\u4ee3\u7406\u63a8\u7406\uff0c\u786e\u4fdd\u57fa\u7840\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u5bf9\u7f3a\u5931/\u6a21\u7cca\u4fe1\u606f\u7684\u5b89\u5168\u5904\u7406\u3002", "result": "\u8be5\u5de5\u4f5c\u4e3a\u81ea\u4e3b\u4e8b\u4ef6\u89e3\u51b3\u548c\u53d8\u66f4\u5f71\u54cd\u7f13\u89e3\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u672a\u6765\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u8bca\u65ad\u548c\u4fee\u590d\u57fa\u7840\u8bbe\u65bd\u6545\u969c\uff0c\u8fd8\u80fd\u9884\u6d4b\u8ba1\u5212\u53d8\u66f4\u5bf9\u670d\u52a1\u548c\u5ba2\u6237\u7684\u5f71\u54cd\uff0c\u4f7f\u8fd0\u7ef4\u4eba\u5458\u80fd\u5728\u6267\u884c\u7ef4\u62a4\u64cd\u4f5c\u524d\u7f13\u89e3\u98ce\u9669\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u667a\u80fd\u8bca\u65ad\u6846\u67b6\u901a\u8fc7MCP\u534f\u8bae\u4f7f\u7528\u5de5\u5177\u81ea\u4e3b\u5bfc\u822a\u57fa\u7840\u8bbe\u65bd\u6a21\u578b\uff0c\u4e3a\u81ea\u4e3b\u4e8b\u4ef6\u89e3\u51b3\u548c\u53d8\u66f4\u5f71\u54cd\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u6709\u671b\u66ff\u4ee3\u4f20\u7edf\u786c\u7f16\u7801\u7684\u6839\u56e0\u5206\u6790\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.06861", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06861", "abs": "https://arxiv.org/abs/2601.06861", "authors": ["William Guey", "Wei Zhang", "Pei-Luen Patrick Rau", "Pierrick Bougault", "Vitor D. de Moura", "Bertan Ucar", "Jose O. Gomes"], "title": "BiasLab: A Multilingual, Dual-Framing Framework for Robust Measurement of Output-Level Bias in Large Language Models", "comment": "source code and reproducibility scripts available on GitHub", "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes contexts where their outputs influence real-world decisions. However, evaluating bias in LLM outputs remains methodologically challenging due to sensitivity to prompt wording, limited multilingual coverage, and the lack of standardized metrics that enable reliable comparison across models. This paper introduces BiasLab, an open-source, model-agnostic evaluation framework for quantifying output-level (extrinsic) bias through a multilingual, robustness-oriented experimental design. BiasLab constructs mirrored probe pairs under a strict dual-framing scheme: an affirmative assertion favoring Target A and a reverse assertion obtained by deterministic target substitution favoring Target B, while preserving identical linguistic structure. To reduce dependence on prompt templates, BiasLab performs repeated evaluation under randomized instructional wrappers and enforces a fixed-choice Likert response format to maximize comparability across models and languages. Responses are normalized into agreement labels using an LLM-based judge, aligned for polarity consistency across framings, and aggregated into quantitative bias indicators with descriptive statistics including effect sizes and neutrality rates. The framework supports evaluation across diverse bias axes, including demographic, cultural, political, and geopolitical topics, and produces reproducible artifacts such as structured reports and comparative visualizations. BiasLab contributes a standardized methodology for cross-lingual and framing-sensitive bias measurement that complements intrinsic and dataset-based audits, enabling researchers and institutions to benchmark robustness and make better-informed deployment decisions.", "AI": {"tldr": "BiasLab\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u591a\u8bed\u8a00\u3001\u9c81\u68d2\u6027\u5bfc\u5411\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u91cf\u5316LLM\u8f93\u51fa\u5c42\u9762\u7684\u504f\u89c1\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff0c\u5176\u8f93\u51fa\u4f1a\u5f71\u54cd\u73b0\u5b9e\u4e16\u754c\u7684\u51b3\u7b56\u3002\u7136\u800c\uff0c\u8bc4\u4f30LLM\u8f93\u51fa\u4e2d\u7684\u504f\u89c1\u5728\u65b9\u6cd5\u8bba\u4e0a\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b58\u5728\u5bf9\u63d0\u793a\u8bcd\u63aa\u8f9e\u7684\u654f\u611f\u6027\u3001\u6709\u9650\u7684\u591a\u8bed\u8a00\u8986\u76d6\u8303\u56f4\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u80fd\u591f\u5b9e\u73b0\u8de8\u6a21\u578b\u53ef\u9760\u6bd4\u8f83\u7684\u6807\u51c6\u5316\u6307\u6807\u3002", "method": "BiasLab\u91c7\u7528\u4e25\u683c\u7684\u4e8c\u5143\u6846\u67b6\u65b9\u6848\u6784\u5efa\u955c\u50cf\u63a2\u9488\u5bf9\uff1a\u4e00\u4e2a\u80af\u5b9a\u65ad\u8a00\u504f\u5411\u76ee\u6807A\uff0c\u53e6\u4e00\u4e2a\u901a\u8fc7\u786e\u5b9a\u6027\u76ee\u6807\u66ff\u6362\u83b7\u5f97\u7684\u53cd\u5411\u65ad\u8a00\u504f\u5411\u76ee\u6807B\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u8bed\u8a00\u7ed3\u6784\u3002\u4e3a\u4e86\u51cf\u5c11\u5bf9\u63d0\u793a\u6a21\u677f\u7684\u4f9d\u8d56\uff0c\u5728\u968f\u673a\u6307\u4ee4\u5305\u88c5\u4e0b\u8fdb\u884c\u91cd\u590d\u8bc4\u4f30\uff0c\u5e76\u5f3a\u5236\u6267\u884c\u56fa\u5b9a\u9009\u62e9\u7684Likert\u54cd\u5e94\u683c\u5f0f\u3002\u54cd\u5e94\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u8bc4\u5224\u5668\u5f52\u4e00\u5316\u4e3a\u540c\u610f\u6807\u7b7e\uff0c\u8de8\u6846\u67b6\u8fdb\u884c\u6781\u6027\u4e00\u81f4\u6027\u5bf9\u9f50\uff0c\u5e76\u805a\u5408\u6210\u5177\u6709\u6548\u5e94\u5927\u5c0f\u548c\u4e2d\u6027\u7387\u7b49\u63cf\u8ff0\u6027\u7edf\u8ba1\u7684\u5b9a\u91cf\u504f\u89c1\u6307\u6807\u3002", "result": "\u8be5\u6846\u67b6\u652f\u6301\u8de8\u591a\u79cd\u504f\u89c1\u8f74\uff08\u5305\u62ec\u4eba\u53e3\u7edf\u8ba1\u3001\u6587\u5316\u3001\u653f\u6cbb\u548c\u5730\u7f18\u653f\u6cbb\u4e3b\u9898\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u751f\u6210\u53ef\u91cd\u590d\u7684\u5de5\u4ef6\uff0c\u5982\u7ed3\u6784\u5316\u62a5\u544a\u548c\u6bd4\u8f83\u53ef\u89c6\u5316\u3002", "conclusion": "BiasLab\u4e3a\u8de8\u8bed\u8a00\u548c\u6846\u67b6\u654f\u611f\u7684\u504f\u89c1\u6d4b\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u8865\u5145\u4e86\u5185\u5728\u548c\u57fa\u4e8e\u6570\u636e\u96c6\u7684\u5ba1\u8ba1\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u673a\u6784\u80fd\u591f\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u5e76\u505a\u51fa\u66f4\u660e\u667a\u7684\u90e8\u7f72\u51b3\u7b56\u3002", "topic": "agent analysis"}}
{"id": "2601.07376", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.07376", "abs": "https://arxiv.org/abs/2601.07376", "authors": ["Siqi Zhu", "Jiaxuan You"], "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning", "comment": null, "summary": "We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.", "AI": {"tldr": "OpenTinker\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u91c7\u7528\u7b97\u6cd5\u8bbe\u8ba1\u3001\u6267\u884c\u548c\u667a\u80fd\u4f53-\u73af\u5883\u4ea4\u4e92\u5206\u79bb\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u7ec4\u5408\u7ec4\u4ef6\u548c\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u5668\u652f\u6301\u591a\u79cd\u8bad\u7ec3\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u7aef\u5230\u7aef\u7684\u5355\u4f53\u5f0f\u67b6\u6784\uff0c\u7f3a\u4e4f\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002OpenTinker\u65e8\u5728\u901a\u8fc7\u5206\u79bb\u5173\u6ce8\u70b9\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u53ef\u7ec4\u5408\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301\u591a\u79cd\u8bad\u7ec3\u6a21\u5f0f\u5e76\u7b80\u5316\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u6269\u5c55\u3002", "method": "OpenTinker\u5c06\u667a\u80fd\u4f53\u5b66\u4e60\u7cfb\u7edf\u5206\u89e3\u4e3a\u8f7b\u91cf\u7ea7\u53ef\u7ec4\u5408\u7ec4\u4ef6\uff0c\u5177\u6709\u660e\u786e\u7684\u62bd\u8c61\u8fb9\u754c\u3002\u7528\u6237\u5b9a\u4e49\u667a\u80fd\u4f53\u3001\u73af\u5883\u548c\u4ea4\u4e92\u534f\u8bae\uff0c\u800c\u63a8\u7406\u548c\u8bad\u7ec3\u5219\u59d4\u6258\u7ed9\u6258\u7ba1\u6267\u884c\u8fd0\u884c\u65f6\u3002\u7cfb\u7edf\u5f15\u5165\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u5668\u7ba1\u7406\u8bad\u7ec3\u548c\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u652f\u6301LoRA-based\u548c\u5168\u53c2\u6570RL\u3001\u76d1\u7763\u5fae\u8c03\u548c\u63a8\u7406\uff0c\u5e76\u8ba8\u8bba\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u7ec4RL\u7528\u4f8b\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u667a\u80fd\u4f53\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8868\u660eOpenTinker\u80fd\u591f\u652f\u6301\u591a\u79cd\u8bad\u7ec3\u6a21\u5f0f\u5e76\u5728\u5171\u4eab\u8d44\u6e90\u4e0a\u9ad8\u6548\u7ba1\u7406\u8bad\u7ec3\u548c\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u3002", "conclusion": "OpenTinker\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684LLM\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u5206\u79bb\u5173\u6ce8\u70b9\u548c\u7ec4\u4ef6\u5316\u8bbe\u8ba1\uff0c\u7b80\u5316\u4e86\u667a\u80fd\u4f53\u5b66\u4e60\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u90e8\u7f72\uff0c\u652f\u6301\u4ece\u5355\u667a\u80fd\u4f53\u5230\u591a\u667a\u80fd\u4f53\u7684\u7075\u6d3b\u6269\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.06911", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06911", "abs": "https://arxiv.org/abs/2601.06911", "authors": ["Shaoning Sun", "Mingzhu Cai", "Huang He", "Bingjin Chen", "Siqi Bao", "Yujiu Yang", "Hua Wu", "Haifeng Wang"], "title": "Distributional Clarity: The Hidden Driver of RL-Friendliness in Large Language Models", "comment": null, "summary": "Language model families exhibit striking disparity in their capacity to benefit from reinforcement learning: under identical training, models like Qwen achieve substantial gains, while others like Llama yield limited improvements. Complementing data-centric approaches, we reveal that this disparity reflects a hidden structural property: \\textbf{distributional clarity} in probability space. Through a three-stage analysis-from phenomenon to mechanism to interpretation-we uncover that RL-friendly models exhibit intra-class compactness and inter-class separation in their probability assignments to correct vs. incorrect responses. We quantify this clarity using the \\textbf{Silhouette Coefficient} ($S$) and demonstrate that (1) high $S$ correlates strongly with RL performance; (2) low $S$ is associated with severe logic errors and reasoning instability. To confirm this property, we introduce a Silhouette-Aware Reweighting strategy that prioritizes low-$S$ samples during training. Experiments across six mathematical benchmarks show consistent improvements across all model families, with gains up to 5.9 points on AIME24. Our work establishes distributional clarity as a fundamental, trainable property underlying RL-Friendliness.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u6e90\u4e8e\u5176\u6982\u7387\u7a7a\u95f4\u7684\"\u5206\u5e03\u6e05\u6670\u5ea6\"\u7279\u6027\uff0c\u901a\u8fc7\u8f6e\u5ed3\u7cfb\u6570\u91cf\u5316\u8fd9\u4e00\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6b64\u7684\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u6709\u4e9b\u6a21\u578b\u80fd\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u6539\u8fdb\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\u5dee\u5f02\u80cc\u540e\u7684\u7ed3\u6784\u6027\u539f\u56e0\uff0c\u800c\u975e\u4ec5\u4ec5\u5173\u6ce8\u6570\u636e\u5c42\u9762\u7684\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u5206\u6790\uff1a\u4ece\u73b0\u8c61\u5230\u673a\u5236\u518d\u5230\u89e3\u91ca\u3002\u9996\u5148\u89c2\u5bdf\u6a21\u578b\u8868\u73b0\u5dee\u5f02\uff0c\u7136\u540e\u901a\u8fc7\u8f6e\u5ed3\u7cfb\u6570\u91cf\u5316\u6a21\u578b\u5728\u6b63\u786e\u4e0e\u9519\u8bef\u56de\u7b54\u4e0a\u7684\u6982\u7387\u5206\u5e03\u6e05\u6670\u5ea6\uff0c\u6700\u540e\u63d0\u51fa\u57fa\u4e8e\u8f6e\u5ed3\u7cfb\u6570\u7684\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u7b56\u7565\uff0c\u4f18\u5148\u8bad\u7ec3\u4f4e\u6e05\u6670\u5ea6\u6837\u672c\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u8f6e\u5ed3\u7cfb\u6570\u4e0e\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u4f4e\u8f6e\u5ed3\u7cfb\u6570\u4e0e\u4e25\u91cd\u903b\u8f91\u9519\u8bef\u548c\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u76f8\u5173\u3002\u63d0\u51fa\u7684\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u5728\u6240\u6709\u6a21\u578b\u5bb6\u65cf\u4e2d\u90fd\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\uff0c\u5728AIME24\u4e0a\u6700\u9ad8\u63d0\u53475.9\u5206\u3002", "conclusion": "\u5206\u5e03\u6e05\u6670\u5ea6\u662f\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u53cb\u597d\u6027\u7684\u57fa\u672c\u53ef\u8bad\u7ec3\u5c5e\u6027\uff0c\u8f6e\u5ed3\u7cfb\u6570\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u91cf\u5316\u6307\u6807\uff0c\u57fa\u4e8e\u6b64\u7684\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.06677", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06677", "abs": "https://arxiv.org/abs/2601.06677", "authors": ["Zohaib Khan", "Omer Tafveez", "Zoha Hayat Bhatti"], "title": "Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget", "comment": "9 pages, 4 figures, 2 tables", "summary": "Recent advances in mathematical reasoning typically rely on massive scale, yet the question remains: can strong reasoning capabilities be induced in small language models ($\\leq1.5\\text{B}$) under extreme constraints? We investigate this by training models on a single A40 GPU (48GB) for under 24 hours using Reinforcement Learning with Verifiable Rewards (RLVR) and Low-Rank Adaptation (LoRA). We find that the success of this ``micro-budget\" regime depends critically on the interplay between adapter capacity and model initialization. While low-rank adapters ($r=8$) consistently fail to capture the complex optimization dynamics of reasoning, high-rank adapters ($r=256$) unlock significant plasticity in standard instruction-tuned models. Our best result achieved an impressive 40.0\\% Pass@1 on AIME 24 (an 11.1\\% absolute improvement over baseline) and pushed Pass@16 to 70.0\\%, demonstrating robust exploration capabilities. However, this plasticity is not universal: while instruction-tuned models utilized the budget to elongate their chain-of-thought and maximize reward, heavily math-aligned models suffered performance collapse, suggesting that noisy, low-budget RL updates can act as destructive interference for models already residing near a task-specific optimum.", "AI": {"tldr": "\u5728\u5355GPU 24\u5c0f\u65f6\u5185\uff0c\u901a\u8fc7RLVR\u548cLoRA\u8bad\u7ec3\u5c0f\u6a21\u578b\uff08\u22641.5B\uff09\u8fdb\u884c\u6570\u5b66\u63a8\u7406\uff0c\u53d1\u73b0\u9ad8\u79e9\u9002\u914d\u5668\uff08r=256\uff09\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u6570\u5b66\u5bf9\u9f50\u6a21\u578b\u4f1a\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\u3002", "motivation": "\u63a2\u7d22\u5728\u6781\u7aef\u8d44\u6e90\u7ea6\u675f\u4e0b\uff08\u5355A40 GPU\uff0c48GB\u5185\u5b58\uff0c24\u5c0f\u65f6\u5185\uff09\u662f\u5426\u80fd\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\uff08\u22641.5B\uff09\u4e2d\u8bf1\u5bfc\u51fa\u5f3a\u5927\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u7814\u7a76\u5fae\u9884\u7b97\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u548c\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u5728\u5355GPU\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7814\u7a76\u4e0d\u540c\u79e9\u7684\u9002\u914d\u5668\uff08\u4f4e\u79e9r=8 vs \u9ad8\u79e9r=256\uff09\u5bf9\u6a21\u578b\u4f18\u5316\u52a8\u6001\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u79e9\u9002\u914d\u5668\uff08r=256\uff09\u5728\u6807\u51c6\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0a\u89e3\u9501\u4e86\u663e\u8457\u7684\u53ef\u5851\u6027\uff0c\u5728AIME 24\u4e0a\u8fbe\u523040.0% Pass@1\uff08\u6bd4\u57fa\u7ebf\u63d0\u534711.1%\uff09\uff0cPass@16\u8fbe\u523070.0%\u3002\u4f46\u6570\u5b66\u5bf9\u9f50\u6a21\u578b\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\uff0c\u8868\u660e\u4f4e\u9884\u7b97RL\u66f4\u65b0\u53ef\u80fd\u5bf9\u5df2\u63a5\u8fd1\u4efb\u52a1\u6700\u4f18\u7684\u6a21\u578b\u4ea7\u751f\u7834\u574f\u6027\u5e72\u6270\u3002", "conclusion": "\u5fae\u9884\u7b97\u8bad\u7ec3\u7684\u6210\u529f\u5173\u952e\u53d6\u51b3\u4e8e\u9002\u914d\u5668\u5bb9\u91cf\u548c\u6a21\u578b\u521d\u59cb\u5316\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u9ad8\u79e9\u9002\u914d\u5668\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u6a21\u578b\u521d\u59cb\u5316\u72b6\u6001\u5bf9RL\u66f4\u65b0\u7684\u654f\u611f\u6027\u5dee\u5f02\u5f88\u5927\uff0c\u6570\u5b66\u5bf9\u9f50\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u566a\u58f0RL\u66f4\u65b0\u7684\u7834\u574f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.06922", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06922", "abs": "https://arxiv.org/abs/2601.06922", "authors": ["Tianhua Zhang", "Kun Li", "Junan Li", "Yunxiang Li", "Hongyin Luo", "Xixin Wu", "James Glass", "Helen Meng"], "title": "TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG", "comment": null, "summary": "Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.", "AI": {"tldr": "\u63d0\u51faTreePS-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u5f62\u7ed3\u6784\u5b9e\u73b0\u591a\u6b65\u63a8\u7406\u7684\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u65e0\u9700\u4e2d\u95f4\u6807\u6ce8\uff0c\u5728\u591a\u4e2aQA\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684agentic RAG\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7a00\u758f\u7684\u6700\u7ec8\u5956\u52b1\uff0c\u9650\u5236\u4e86\u6b65\u7ea7\u4fe1\u7528\u5206\u914d\uff0c\u5bf9\u4e2d\u95f4\u63a8\u7406\u548c\u52a8\u4f5c\u7684\u6307\u5bfc\u8f83\u5f31\u3002\u73b0\u6709\u8fc7\u7a0b\u7ea7\u76d1\u7763\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u79bb\u7ebf\u8bad\u7ec3\u6570\u636e\uff08\u5b58\u5728\u5206\u5e03\u504f\u79fb\u98ce\u9669\uff09\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u4e2d\u95f4\u6807\u6ce8\u3002", "method": "\u63d0\u51faTreePS-RAG\u6846\u67b6\uff1a1) \u5c06agentic RAG\u63a8\u7406\u5efa\u6a21\u4e3a\u5c55\u5f00\u6811\uff0c\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u5bf9\u5e94\u4e00\u4e2a\u8282\u70b9\uff1b2) \u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u540e\u4ee3\u7ed3\u679c\u6765\u4f30\u8ba1\u6b65\u9aa4\u6548\u7528\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u4f18\u52bf\uff1b3) \u5f15\u5165\u9ad8\u6548\u7684\u5728\u7ebf\u6811\u6784\u5efa\u7b56\u7565\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u4fdd\u6301\u63a2\u7d22\u591a\u6837\u6027\u3002", "result": "\u57287\u4e2a\u591a\u8df3\u548c\u901a\u7528QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTreePS-RAG\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u4e0a\u4e00\u81f4\u4e14\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u7ed3\u679c\u76d1\u7763\u548c\u9886\u5148\u7684\u8fc7\u7a0b\u76d1\u7763RL\u65b9\u6cd5\uff0c\u4e14\u5c55\u5f00\u6210\u672c\u4e0eSearch-R1\u7b49\u5f3a\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "TreePS-RAG\u901a\u8fc7\u6811\u5f62\u7ed3\u6784\u548c\u5728\u7ebf\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6b65\u7ea7\u4fe1\u7528\u5206\u914d\uff0c\u65e0\u9700\u4e2d\u95f4\u6807\u6ce8\uff0c\u4e3aagentic RAG\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.07468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07468", "abs": "https://arxiv.org/abs/2601.07468", "authors": ["Miao Su", "Yucan Guo", "Zhongni Hou", "Long Bai", "Zixuan Li", "Yufei Zhang", "Guojun Yin", "Wei Lin", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents", "comment": null, "summary": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.", "AI": {"tldr": "TSM\u662f\u4e00\u4e2a\u4e3aLLM\u4ee3\u7406\u8bbe\u8ba1\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u8bed\u4e49\u65f6\u95f4\u7ebf\u6765\u7ec4\u7ec7\u70b9\u72b6\u8bb0\u5fc6\uff0c\u5e76\u6784\u5efa\u6301\u7eed\u6027\u8bb0\u5fc6\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u65f6\u95f4\u4e0d\u51c6\u786e\u548c\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u8bb0\u5fc6\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u65f6\u95f4\u7ef4\u5ea6\u95ee\u9898\uff1a1) \u65f6\u95f4\u4e0d\u51c6\u786e\uff1a\u8bb0\u5fc6\u6309\u5bf9\u8bdd\u65f6\u95f4\u800c\u975e\u5b9e\u9645\u53d1\u751f\u65f6\u95f4\u7ec4\u7ec7\uff1b2) \u65f6\u95f4\u788e\u7247\u5316\uff1a\u53ea\u5173\u6ce8\u70b9\u72b6\u8bb0\u5fc6\uff0c\u4e22\u5931\u4e86\u6355\u6349\u6301\u7eed\u72b6\u6001\u548c\u6f14\u5316\u6a21\u5f0f\u7684\u6301\u7eed\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u8bed\u4e49\u8bb0\u5fc6(TSM)\u6846\u67b6\uff1a1) \u5728\u8bb0\u5fc6\u6784\u5efa\u9636\u6bb5\uff0c\u5efa\u7acb\u8bed\u4e49\u65f6\u95f4\u7ebf\u800c\u975e\u5bf9\u8bdd\u65f6\u95f4\u7ebf\uff1b2) \u5c06\u65f6\u95f4\u8fde\u7eed\u4e14\u8bed\u4e49\u76f8\u5173\u7684\u4fe1\u606f\u6574\u5408\u6210\u6301\u7eed\u6027\u8bb0\u5fc6\uff1b3) \u5728\u8bb0\u5fc6\u5229\u7528\u9636\u6bb5\uff0c\u7ed3\u5408\u67e5\u8be2\u7684\u65f6\u95f4\u610f\u56fe\uff0c\u68c0\u7d22\u65f6\u95f4\u6070\u5f53\u7684\u6301\u7eed\u6027\u8bb0\u5fc6\uff0c\u4e3a\u54cd\u5e94\u751f\u6210\u63d0\u4f9b\u65f6\u95f4\u6709\u6548\u3001\u6301\u7eed\u65f6\u95f4\u4e00\u81f4\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LongMemEval\u548cLoCoMo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTSM\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534712.2%\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "TSM\u901a\u8fc7\u5efa\u6a21\u8bed\u4e49\u65f6\u95f4\u548c\u6784\u5efa\u6301\u7eed\u6027\u8bb0\u5fc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u8bb0\u5fc6\u4e2d\u7684\u65f6\u95f4\u7ef4\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bb0\u5fc6\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.06953", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06953", "abs": "https://arxiv.org/abs/2601.06953", "authors": ["Jie Wu", "Haoling Li", "Xin Zhang", "Jiani Guo", "Jane Luo", "Steven Liu", "Yangyu Huang", "Ruihang Chu", "Scarlett Li", "Yujiu Yang"], "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests", "comment": "Project: https://github.com/JieWu02/X-Coder", "summary": "Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5b8c\u5168\u5408\u6210\u6570\u636e\u8bad\u7ec3Code LLMs\u7684\u65b9\u6cd5SynthSmith\uff0c\u901a\u8fc7\u7279\u5f81\u5408\u6210\u751f\u6210\u591a\u6837\u5316\u7f16\u7a0b\u4efb\u52a1\u3001\u89e3\u51b3\u65b9\u6848\u548c\u6d4b\u8bd5\u7528\u4f8b\uff0c\u8bad\u7ec3\u51fa\u7684X-Coder\u7cfb\u5217\u6a21\u578b\u5728LiveCodeBench\u4e0a\u8d85\u8d8a\u66f4\u5927\u53c2\u6570\u6a21\u578b\u3002", "motivation": "\u5f53\u524dCode LLMs\u4e25\u91cd\u4f9d\u8d56\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u7ade\u4e89\u6027\u7f16\u7a0b\u5bf9\u4ee3\u7801\u63a8\u7406\u8981\u6c42\u9ad8\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u63d0\u51faSynthSmith\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u57fa\u4e8e\u7279\u5f81\u5408\u6210\u751f\u6210\u5b8c\u5168\u5408\u6210\u7684\u7f16\u7a0b\u4efb\u52a1\u3001\u89e3\u51b3\u65b9\u6848\u548c\u6d4b\u8bd5\u7528\u4f8b\u3002\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5f15\u5165X-Coder\u6a21\u578b\u7cfb\u5217\u3002", "result": "X-Coder\u7cfb\u5217\uff087B\u53c2\u6570\uff09\u5728LiveCodeBench v5\u4e0a\u8fbe\u523062.9 avg@8\uff0cv6\u4e0a\u8fbe\u523055.8\uff0c\u8d85\u8d8aDeepCoder-14B-Preview\u548cAReal-boba2-14B\u3002\u53d1\u73b0\u5408\u6210\u6570\u636e\u4e0a\u7684\u7f29\u653e\u5b9a\u5f8b\u6210\u7acb\u3002", "conclusion": "\u6269\u5c55\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u548c\u91c7\u7528\u5206\u9636\u6bb5\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u6570\u636e\u7684\u4f9d\u8d56\u3002", "topic": "code agent"}}
{"id": "2601.06966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06966", "abs": "https://arxiv.org/abs/2601.06966", "authors": ["Haonan Bian", "Zhiyuan Yao", "Sen Hu", "Zishan Xu", "Shaolei Zhang", "Yifu Guo", "Ziliang Yang", "Xueran Han", "Huacan Wang", "Ronghao Chen"], "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction", "comment": null, "summary": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals.\n  To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation.\n  We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects.\n  Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).", "AI": {"tldr": "RealMem\u662f\u9996\u4e2a\u9762\u5411\u957f\u671f\u9879\u76ee\u5bfc\u5411\u4ea4\u4e92\u7684\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2000+\u8de8\u4f1a\u8bdd\u5bf9\u8bdd\uff0c\u63ed\u793a\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\u5728\u7ba1\u7406\u771f\u5b9e\u9879\u76ee\u72b6\u6001\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "motivation": "\u968f\u7740LLM\u4ece\u9759\u6001\u5bf9\u8bdd\u63a5\u53e3\u53d1\u5c55\u4e3a\u81ea\u4e3b\u901a\u7528\u667a\u80fd\u4f53\uff0c\u6709\u6548\u8bb0\u5fc6\u5bf9\u4fdd\u6301\u957f\u671f\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u95f2\u804a\u6216\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\uff0c\u672a\u80fd\u6355\u6349\"\u957f\u671f\u9879\u76ee\u5bfc\u5411\"\u4ea4\u4e92\u4e2d\u667a\u80fd\u4f53\u9700\u8981\u8ddf\u8e2a\u6f14\u5316\u76ee\u6807\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faRealMem\u57fa\u51c6\uff0c\u5305\u542b11\u4e2a\u573a\u666f\u76842000+\u8de8\u4f1a\u8bdd\u5bf9\u8bdd\uff0c\u4f7f\u7528\u81ea\u7136\u7528\u6237\u67e5\u8be2\u8fdb\u884c\u8bc4\u4f30\u3002\u5f00\u53d1\u4e86\u5305\u542b\u9879\u76ee\u57fa\u7840\u6784\u5efa\u3001\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u751f\u6210\u3001\u8bb0\u5fc6\u4e0e\u65e5\u7a0b\u7ba1\u7406\u7684\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u6a21\u62df\u8bb0\u5fc6\u7684\u52a8\u6001\u6f14\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\u5728\u7ba1\u7406\u957f\u671f\u9879\u76ee\u72b6\u6001\u548c\u771f\u5b9e\u4e16\u754c\u9879\u76ee\u56fa\u6709\u7684\u52a8\u6001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u65b9\u9762\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "conclusion": "RealMem\u586b\u8865\u4e86\u957f\u671f\u9879\u76ee\u5bfc\u5411\u8bb0\u5fc6\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2601.07470", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07470", "abs": "https://arxiv.org/abs/2601.07470", "authors": ["Sirui Liang", "Pengfei Cao", "Jian Zhao", "Wenhao Teng", "Xiangwen Liao", "Jun Zhao", "Kang Liu"], "title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory", "comment": null, "summary": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.", "AI": {"tldr": "MCMA\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u5143\u8ba4\u77e5\u8bb0\u5fc6\u62bd\u8c61\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u4efb\u52a1\u6267\u884c\u548c\u8bb0\u5fc6\u7ba1\u7406\uff0c\u4f7f\u7528\u8bb0\u5fc6\u526f\u9a7e\u9a76\u52a8\u6001\u51b3\u5b9a\u8bb0\u5fc6\u7684\u7ed3\u6784\u3001\u62bd\u8c61\u548c\u91cd\u7528\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u65b9\u6cd5\u901a\u5e38\u5c06\u8bb0\u5fc6\u5b58\u50a8\u4e3a\u56fa\u5b9a\u8868\u793a\uff0c\u5e76\u5728\u5355\u4e00\u6216\u9690\u5f0f\u62bd\u8c61\u7ea7\u522b\u4e0a\u91cd\u7528\u8bb0\u5fc6\uff0c\u8fd9\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5206\u5e03\u504f\u79fb\u65f6\u5bb9\u6613\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u53ef\u5b66\u4e60\u7684\u8bb0\u5fc6\u62bd\u8c61\u673a\u5236\u3002", "method": "MCMA\u5c06\u8bb0\u5fc6\u62bd\u8c61\u89c6\u4e3a\u53ef\u5b66\u4e60\u7684\u8ba4\u77e5\u6280\u80fd\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684\u4efb\u52a1\u6a21\u578b\u548c\u5b66\u4e60\u7684\u8bb0\u5fc6\u526f\u9a7e\u9a76\u89e3\u8026\u4efb\u52a1\u6267\u884c\u4e0e\u8bb0\u5fc6\u7ba1\u7406\u3002\u8bb0\u5fc6\u526f\u9a7e\u9a76\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8bad\u7ec3\uff0c\u51b3\u5b9a\u8bb0\u5fc6\u7684\u7ed3\u6784\u3001\u62bd\u8c61\u548c\u91cd\u7528\u65b9\u5f0f\u3002\u8bb0\u5fc6\u7ec4\u7ec7\u6210\u62bd\u8c61\u5c42\u6b21\u7ed3\u6784\uff0c\u57fa\u4e8e\u4efb\u52a1\u76f8\u4f3c\u6027\u9009\u62e9\u6027\u91cd\u7528\u3002\u5f53\u6ca1\u6709\u53ef\u8f6c\u79fb\u8bb0\u5fc6\u65f6\uff0c\u901a\u8fc7\u8f6c\u79fb\u8bb0\u5fc6\u526f\u9a7e\u9a76\u6765\u8f6c\u79fb\u62bd\u8c61\u548c\u7ba1\u7406\u8bb0\u5fc6\u7684\u80fd\u529b\u3002", "result": "\u5728ALFWorld\u3001ScienceWorld\u548cBabyAI\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMCMA\u76f8\u6bd4\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u5206\u5e03\u5916\u6cdb\u5316\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u5c06\u8bb0\u5fc6\u62bd\u8c61\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u8ba4\u77e5\u6280\u80fd\u800c\u975e\u56fa\u5b9a\u8bbe\u8ba1\u9009\u62e9\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.07477", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07477", "abs": "https://arxiv.org/abs/2601.07477", "authors": ["Zihan Ma", "Zhikai Zhao", "Chuanbo Hua", "Federico Berto", "Jinkyoo Park"], "title": "JudgeFlow: Agentic Workflow Optimization via Block Judge", "comment": null, "summary": "Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose {\\our{}}, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces -- particularly failed runs -- and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate {\\our{}} on mathematical reasoning and code generation benchmarks, where {\\our{}} achieves superior performance and efficiency compared to existing methods. The source code is publicly available at https://github.com/ma-zihan/JudgeFlow.", "AI": {"tldr": "\u63d0\u51faJudgeFlow\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u903b\u8f91\u5757\u3001\u8d23\u4efb\u8bc4\u5206\u548c\u9488\u5bf9\u6027\u4f18\u5316\u6765\u63d0\u5347\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u6548\u7387", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u4fe1\u53f7\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bca\u65ad\uff0c\u5bfc\u81f4\u4fee\u6539\u6548\u7387\u4f4e\u4e0b\u6216\u5f71\u54cd\u6709\u9650", "method": "\u63d0\u51faEvaluation-Judge-Optimization-Update\u7ba1\u9053\uff1a1) \u5c06\u53ef\u590d\u7528\u3001\u53ef\u914d\u7f6e\u7684\u903b\u8f91\u5757\u96c6\u6210\u5230\u5de5\u4f5c\u6d41\u4e2d\uff1b2) \u8bbe\u8ba1Judge\u6a21\u5757\u68c0\u67e5\u6267\u884c\u8f68\u8ff9\uff08\u7279\u522b\u662f\u5931\u8d25\u8fd0\u884c\uff09\uff0c\u4e3a\u95ee\u9898\u5757\u5206\u914d\u57fa\u4e8e\u6392\u540d\u7684\u8d23\u4efb\u8bc4\u5206\uff1b3) \u57fa\u4e8eLLM\u7684\u4f18\u5316\u5668\u9488\u5bf9\u6700\u6210\u95ee\u9898\u7684\u5757\u8fdb\u884c\u4fee\u6539", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJudgeFlow\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u548c\u6548\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u901a\u8fc7\u5757\u7ea7\u8bca\u65ad\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e3a\u81ea\u52a8\u5316\u65e5\u76ca\u590d\u6742\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840", "topic": "agent analysis"}}
{"id": "2601.06973", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06973", "abs": "https://arxiv.org/abs/2601.06973", "authors": ["Davide Baldelli", "Ali Parviz", "Amal Zouaq", "Sarath Chandar"], "title": "LLMs Can't Play Hangman: On the Necessity of a Private Working Memory for Language Agents", "comment": null, "summary": "As LLMs move from text completion toward autonomous agents, they remain constrained by the standard chat interface, which lacks private working memory. This raises a fundamental question: can agents reliably perform interactive tasks that depend on hidden state? We define Private State Interactive Tasks (PSITs), which require agents to generate and maintain hidden information while producing consistent public responses. We show theoretically that any agent restricted to the public conversation history cannot simultaneously preserve secrecy and consistency in PSITs, yielding an impossibility theorem. To empirically validate this limitation, we introduce a self-consistency testing protocol that evaluates whether agents can maintain a hidden secret across forked dialogue branches. Standard chat-based LLMs and retrieval-based memory baselines fail this test regardless of scale, demonstrating that semantic retrieval does not enable true state maintenance. To address this, we propose a novel architecture incorporating an explicit private working memory; we demonstrate that this mechanism restores consistency, establishing private state as a necessary component for interactive language agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u79c1\u6709\u72b6\u6001\u4ea4\u4e92\u4efb\u52a1(PSITs)\u6982\u5ff5\uff0c\u8bc1\u660e\u6807\u51c6\u804a\u5929\u754c\u9762\u65e0\u6cd5\u540c\u65f6\u4fdd\u6301\u79d8\u5bc6\u6027\u548c\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u5305\u542b\u663e\u5f0f\u79c1\u6709\u5de5\u4f5c\u8bb0\u5fc6\u7684\u65b0\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u4ece\u6587\u672c\u8865\u5168\u8f6c\u5411\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u6807\u51c6\u804a\u5929\u754c\u9762\u7f3a\u4e4f\u79c1\u6709\u5de5\u4f5c\u8bb0\u5fc6\u7684\u9650\u5236\u53d8\u5f97\u7a81\u51fa\u3002\u9700\u8981\u7814\u7a76\u667a\u80fd\u4f53\u80fd\u5426\u53ef\u9760\u6267\u884c\u4f9d\u8d56\u9690\u85cf\u72b6\u6001\u7684\u4ea4\u4e92\u4efb\u52a1\u3002", "method": "1. \u5b9a\u4e49\u79c1\u6709\u72b6\u6001\u4ea4\u4e92\u4efb\u52a1(PSITs)\uff1b2. \u7406\u8bba\u4e0a\u8bc1\u660e\u6807\u51c6\u804a\u5929\u754c\u9762\u65e0\u6cd5\u540c\u65f6\u4fdd\u6301\u79d8\u5bc6\u6027\u548c\u4e00\u81f4\u6027\uff1b3. \u63d0\u51fa\u81ea\u4e00\u81f4\u6027\u6d4b\u8bd5\u534f\u8bae\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u5206\u53c9\u5bf9\u8bdd\u5206\u652f\u4e2d\u7ef4\u62a4\u9690\u85cf\u79d8\u5bc6\u7684\u80fd\u529b\uff1b4. \u63d0\u51fa\u5305\u542b\u663e\u5f0f\u79c1\u6709\u5de5\u4f5c\u8bb0\u5fc6\u7684\u65b0\u67b6\u6784\u3002", "result": "1. \u7406\u8bba\u4e0a\u8bc1\u660e\u6807\u51c6\u804a\u5929\u754c\u9762\u5728PSITs\u4e2d\u5b58\u5728\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\uff1b2. \u7ecf\u9a8c\u9a8c\u8bc1\u6807\u51c6\u804a\u5929LLM\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u8bb0\u5fc6\u57fa\u7ebf\u90fd\u65e0\u6cd5\u901a\u8fc7\u81ea\u4e00\u81f4\u6027\u6d4b\u8bd5\uff1b3. \u63d0\u51fa\u7684\u663e\u5f0f\u79c1\u6709\u5de5\u4f5c\u8bb0\u5fc6\u67b6\u6784\u80fd\u591f\u6062\u590d\u4e00\u81f4\u6027\u3002", "conclusion": "\u79c1\u6709\u72b6\u6001\u662f\u4ea4\u4e92\u5f0f\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u5fc5\u8981\u7ec4\u4ef6\uff0c\u663e\u5f0f\u79c1\u6709\u5de5\u4f5c\u8bb0\u5fc6\u67b6\u6784\u80fd\u591f\u89e3\u51b3\u6807\u51c6\u804a\u5929\u754c\u9762\u5728\u7ef4\u62a4\u9690\u85cf\u72b6\u6001\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.07577", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07577", "abs": "https://arxiv.org/abs/2601.07577", "authors": ["Yunfan Li", "Bingbing Xu", "Xueyun Tian", "Xiucheng Xu", "Huawei Shen"], "title": "Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.", "AI": {"tldr": "TDP\u901a\u8fc7\u4efb\u52a1\u89e3\u8026\u6846\u67b6\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u76ee\u6807DAG\uff0c\u4f7f\u7528\u76d1\u7763\u8005\u3001\u89c4\u5212\u8005\u548c\u6267\u884c\u8005\u5206\u79bb\u4e0a\u4e0b\u6587\uff0c\u9632\u6b62\u9519\u8bef\u4f20\u64ad\u5e76\u63d0\u9ad8\u957f\u65f6\u7a0b\u4efb\u52a1\u6267\u884c\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u9010\u6b65\u89c4\u5212\u77ed\u89c6\uff0c\u4e00\u6b21\u6027\u89c4\u5212\u8106\u5f31\uff0c\u4e14\u90fd\u9762\u4e34\u4e0a\u4e0b\u6587\u7ea0\u7f20\u95ee\u9898\u3002\u4e0a\u4e0b\u6587\u7ea0\u7f20\u5bfc\u81f4\u8ba4\u77e5\u8d1f\u62c5\u589e\u52a0\uff0c\u5c40\u90e8\u9519\u8bef\u4f1a\u4f20\u64ad\u5230\u5176\u4ed6\u72ec\u7acb\u51b3\u7b56\u4e2d\uff0c\u6062\u590d\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u89e3\u8026\u89c4\u5212(TDP)\u6846\u67b6\uff1a1) \u76d1\u7763\u8005\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\u7684\u5b50\u76ee\u6807\uff1b2) \u89c4\u5212\u8005\u548c\u6267\u884c\u8005\u4f7f\u7528\u9650\u5b9a\u4e0a\u4e0b\u6587\uff0c\u5c06\u63a8\u7406\u548c\u91cd\u65b0\u89c4\u5212\u9650\u5236\u5728\u6d3b\u52a8\u5b50\u4efb\u52a1\u5185\uff1b3) \u8fd9\u79cd\u9694\u79bb\u9632\u6b62\u9519\u8bef\u4f20\u64ad\uff0c\u5141\u8bb8\u5c40\u90e8\u7ea0\u6b63\u504f\u5dee\u800c\u4e0d\u4e2d\u65ad\u5de5\u4f5c\u6d41\u3002", "result": "\u5728TravelPlanner\u3001ScienceWorld\u548cHotpotQA\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTDP\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5c06token\u6d88\u8017\u51cf\u5c11\u9ad8\u8fbe82%\uff0c\u8bc1\u660e\u5b50\u4efb\u52a1\u89e3\u8026\u63d0\u9ad8\u4e86\u957f\u65f6\u7a0b\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u4efb\u52a1\u89e3\u8026\u89c4\u5212\u901a\u8fc7\u5206\u79bb\u4e0a\u4e0b\u6587\u548c\u9650\u5236\u63a8\u7406\u8303\u56f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u6267\u884c\u7684\u53ef\u9760\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2601.07641", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.07641", "abs": "https://arxiv.org/abs/2601.07641", "authors": ["Jiaxuan Lu", "Ziyu Kong", "Yemin Wang", "Rong Fu", "Haiyuan Wan", "Cheng Yang", "Wenjie Lou", "Haoran Sun", "Lilong Wang", "Yankai Jiang", "Xiaosong Wang", "Xiao Sun", "Dongzhan Zhou"], "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning", "comment": null, "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.", "AI": {"tldr": "TTE\u662f\u4e00\u79cd\u65b0\u7684AI\u79d1\u5b66\u8303\u5f0f\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u63a8\u7406\u65f6\u52a8\u6001\u5408\u6210\u3001\u9a8c\u8bc1\u548c\u6f14\u5316\u53ef\u6267\u884c\u5de5\u5177\uff0c\u514b\u670d\u9759\u6001\u5de5\u5177\u5e93\u7684\u5c40\u9650\u6027\uff0c\u5728\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u4f9d\u8d56\u9759\u6001\u9884\u5b9a\u4e49\u5de5\u5177\u5e93\uff0c\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u56e0\u4e3a\u79d1\u5b66\u5de5\u5177\u7a00\u758f\u3001\u5f02\u6784\u4e14\u672c\u8d28\u4e0a\u4e0d\u5b8c\u6574\uff0c\u65e0\u6cd5\u9002\u5e94\u5f00\u653e\u5f0f\u7684\u79d1\u5b66\u4e16\u754c\u3002", "method": "\u63d0\u51fa\u6d4b\u8bd5\u65f6\u5de5\u5177\u6f14\u5316\uff08TTE\uff09\u8303\u5f0f\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5408\u6210\u3001\u9a8c\u8bc1\u548c\u6f14\u5316\u53ef\u6267\u884c\u5de5\u5177\uff0c\u5c06\u5de5\u5177\u4ece\u56fa\u5b9a\u8d44\u6e90\u8f6c\u53d8\u4e3a\u95ee\u9898\u9a71\u52a8\u7684\u4ea7\u7269\u3002", "result": "TTE\u5728\u51c6\u786e\u6027\u548c\u5de5\u5177\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u8ba1\u7b97\u5de5\u5177\u7684\u6709\u6548\u8de8\u9886\u57df\u9002\u5e94\u3002\u5b9e\u9a8c\u57fa\u4e8eSciEvo\u57fa\u51c6\uff081,590\u4e2a\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u548c925\u4e2a\u81ea\u52a8\u6f14\u5316\u5de5\u5177\uff09\u3002", "conclusion": "TTE\u901a\u8fc7\u5c06\u5de5\u5177\u4ece\u9759\u6001\u5e93\u8f6c\u53d8\u4e3a\u52a8\u6001\u6f14\u5316\u4ea7\u7269\uff0c\u514b\u670d\u4e86\u79d1\u5b66AI\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u5f00\u653e\u79d1\u5b66\u4e16\u754c\u4e2d\u7684\u8ba1\u7b97\u65b9\u6cd5\u521b\u5efa\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2601.07651", "categories": ["cs.AI", "cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.07651", "abs": "https://arxiv.org/abs/2601.07651", "authors": ["Marc Lanctot", "Kate Larson", "Ian Gemp", "Michael Kaisers"], "title": "Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms", "comment": "AAMAS 2026", "summary": "As intelligent agents become more generally-capable, i.e. able to master a wide variety of tasks, the complexity and cost of properly evaluating them rises significantly. Tasks that assess specific capabilities of the agents can be correlated and stochastic, requiring many samples for accurate comparisons, leading to added costs. In this paper, we propose a formal definition and a conceptual framework for active evaluation of agents across multiple tasks, which assesses the performance of ranking algorithms as a function of number of evaluation data samples. Rather than curating, filtering, or compressing existing data sets as a preprocessing step, we propose an online framing: on every iteration, the ranking algorithm chooses the task and agents to sample scores from. Then, evaluation algorithms report a ranking of agents on each iteration and their performance is assessed with respect to the ground truth ranking over time. Several baselines are compared under different experimental contexts, with synthetic generated data and simulated online access to real evaluation data from Atari game-playing agents. We find that the classical Elo rating system -- while it suffers from well-known failure modes, in theory -- is a consistently reliable choice for efficient reduction of ranking error in practice. A recently-proposed method, Soft Condorcet Optimization, shows comparable performance to Elo on synthetic data and significantly outperforms Elo on real Atari agent evaluation. When task variation from the ground truth is high, selecting tasks based on proportional representation leads to higher rate of ranking error reduction.", "AI": {"tldr": "\u63d0\u51fa\u4e3b\u52a8\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u8fed\u4ee3\u9009\u62e9\u4efb\u52a1\u548c\u4ee3\u7406\u8fdb\u884c\u91c7\u6837\uff0c\u6bd4\u8f83\u4e0d\u540c\u6392\u540d\u7b97\u6cd5\u5728\u51cf\u5c11\u6392\u540d\u8bef\u5dee\u65b9\u9762\u7684\u6548\u7387\uff0c\u53d1\u73b0Elo\u8bc4\u5206\u7cfb\u7edf\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u800cSoft Condorcet Optimization\u5728\u771f\u5b9eAtari\u4ee3\u7406\u8bc4\u4f30\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4ee3\u7406\u80fd\u529b\u65e5\u76ca\u901a\u7528\u5316\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u4efb\u52a1\uff0c\u5176\u8bc4\u4f30\u7684\u590d\u6742\u6027\u548c\u6210\u672c\u663e\u8457\u589e\u52a0\u3002\u7279\u5b9a\u80fd\u529b\u8bc4\u4f30\u4efb\u52a1\u53ef\u80fd\u76f8\u5173\u4e14\u5177\u6709\u968f\u673a\u6027\uff0c\u9700\u8981\u5927\u91cf\u6837\u672c\u8fdb\u884c\u51c6\u786e\u6bd4\u8f83\uff0c\u5bfc\u81f4\u6210\u672c\u4e0a\u5347\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u51cf\u5c11\u8bc4\u4f30\u6570\u636e\u6837\u672c\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u7684\u591a\u4efb\u52a1\u6982\u5ff5\u6846\u67b6\uff0c\u91c7\u7528\u5728\u7ebf\u8fed\u4ee3\u65b9\u5f0f\uff1a\u6bcf\u8f6e\u8fed\u4ee3\u4e2d\uff0c\u6392\u540d\u7b97\u6cd5\u9009\u62e9\u8981\u91c7\u6837\u7684\u4efb\u52a1\u548c\u4ee3\u7406\uff0c\u7136\u540e\u8bc4\u4f30\u7b97\u6cd5\u62a5\u544a\u4ee3\u7406\u6392\u540d\uff0c\u5e76\u968f\u65f6\u95f4\u8bc4\u4f30\u5176\u76f8\u5bf9\u4e8e\u771f\u5b9e\u6392\u540d\u7684\u6027\u80fd\u3002\u5728\u5408\u6210\u751f\u6210\u6570\u636e\u548c\u771f\u5b9eAtari\u6e38\u620f\u4ee3\u7406\u8bc4\u4f30\u6570\u636e\u4e0a\u6bd4\u8f83\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u7ecf\u5178\u7684Elo\u8bc4\u5206\u7cfb\u7edf\u5728\u5b9e\u8df5\u4e2d\u662f\u51cf\u5c11\u6392\u540d\u8bef\u5dee\u7684\u53ef\u9760\u9009\u62e9\uff0c\u5c3d\u7ba1\u7406\u8bba\u4e0a\u5b58\u5728\u5df2\u77e5\u7f3a\u9677\u3002\u65b0\u63d0\u51fa\u7684Soft Condorcet Optimization\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u4e0a\u4e0eElo\u8868\u73b0\u76f8\u5f53\uff0c\u5728\u771f\u5b9eAtari\u4ee3\u7406\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8eElo\u3002\u5f53\u4efb\u52a1\u4e0e\u771f\u5b9e\u6392\u540d\u7684\u53d8\u5f02\u8f83\u5927\u65f6\uff0c\u57fa\u4e8e\u6bd4\u4f8b\u8868\u793a\u7684\u4efb\u52a1\u9009\u62e9\u80fd\u66f4\u9ad8\u6548\u5730\u51cf\u5c11\u6392\u540d\u8bef\u5dee\u3002", "conclusion": "\u4e3b\u52a8\u8bc4\u4f30\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u591a\u4efb\u52a1\u667a\u80fd\u4ee3\u7406\u8bc4\u4f30\u6240\u9700\u7684\u6837\u672c\u91cf\uff0cElo\u8bc4\u5206\u7cfb\u7edf\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u800cSoft Condorcet Optimization\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002\u4efb\u52a1\u9009\u62e9\u7b56\u7565\u5e94\u6839\u636e\u4efb\u52a1\u53d8\u5f02\u7a0b\u5ea6\u8fdb\u884c\u8c03\u6574\u3002", "topic": "agent analysis"}}
{"id": "2601.07036", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07036", "abs": "https://arxiv.org/abs/2601.07036", "authors": ["Wang Yang", "Debargha Ganguly", "Xinpeng Li", "Chaoda Song", "Shouren Wang", "Vikash Singh", "Vipin Chaudhary", "Xiaotian Han"], "title": "Mid-Think: Training-Free Intermediate-Budget Reasoning via Token-Level Triggers", "comment": null, "summary": "Hybrid reasoning language models are commonly controlled through high-level Think/No-think instructions to regulate reasoning behavior, yet we found that such mode switching is largely driven by a small set of trigger tokens rather than the instructions themselves. Through attention analysis and controlled prompting experiments, we show that a leading ``Okay'' token induces reasoning behavior, while the newline pattern following ``</think>'' suppresses it. Based on this observation, we propose Mid-Think, a simple training-free prompting format that combines these triggers to achieve intermediate-budget reasoning, consistently outperforming fixed-token and prompt-based baselines in terms of the accuracy-length trade-off. Furthermore, applying Mid-Think to RL training after SFT reduces training time by approximately 15% while improving final performance of Qwen3-8B on AIME from 69.8% to 72.4% and on GPQA from 58.5% to 61.1%, demonstrating its effectiveness for both inference-time control and RL-based reasoning training.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6df7\u5408\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u4e2d\u7684Think/No-think\u6307\u4ee4\u5207\u6362\u4e3b\u8981\u7531\u5c11\u91cf\u89e6\u53d1token\u9a71\u52a8\u800c\u975e\u6307\u4ee4\u672c\u8eab\uff0c\u63d0\u51faMid-Think\u8bad\u7ec3\u514d\u8d39\u63d0\u793a\u683c\u5f0f\uff0c\u5728\u63a8\u7406\u63a7\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u9ad8\u5c42Think/No-think\u6307\u4ee4\u63a7\u5236\u63a8\u7406\u884c\u4e3a\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u6a21\u5f0f\u5207\u6362\u4e3b\u8981\u7531\u5c11\u91cf\u89e6\u53d1token\u9a71\u52a8\u800c\u975e\u6307\u4ee4\u672c\u8eab\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u63a8\u7406\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6790\u548c\u53d7\u63a7\u63d0\u793a\u5b9e\u9a8c\u8bc6\u522b\u5173\u952e\u89e6\u53d1token\uff0c\u63d0\u51faMid-Think\u8bad\u7ec3\u514d\u8d39\u63d0\u793a\u683c\u5f0f\uff0c\u7ed3\u5408\u8fd9\u4e9b\u89e6\u53d1token\u5b9e\u73b0\u4e2d\u95f4\u9884\u7b97\u63a8\u7406\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eSFT\u540e\u7684RL\u8bad\u7ec3\u3002", "result": "Mid-Think\u5728\u51c6\u786e\u7387-\u957f\u5ea6\u6743\u8861\u65b9\u9762\u4f18\u4e8e\u56fa\u5b9atoken\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5e94\u7528\u4e8eRL\u8bad\u7ec3\u540e\u51cf\u5c11\u7ea615%\u8bad\u7ec3\u65f6\u95f4\uff0cQwen3-8B\u5728AIME\u4e0a\u4ece69.8%\u63d0\u5347\u81f372.4%\uff0c\u5728GPQA\u4e0a\u4ece58.5%\u63d0\u5347\u81f361.1%\u3002", "conclusion": "\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u4e3b\u8981\u7531\u7279\u5b9a\u89e6\u53d1token\u63a7\u5236\u800c\u975e\u9ad8\u5c42\u6307\u4ee4\uff0cMid-Think\u683c\u5f0f\u5728\u63a8\u7406\u65f6\u95f4\u63a7\u5236\u548c\u57fa\u4e8eRL\u7684\u63a8\u7406\u8bad\u7ec3\u4e2d\u5747\u6709\u6548\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.07148", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07148", "abs": "https://arxiv.org/abs/2601.07148", "authors": ["Zhengxiang Wang", "Zeyu Dong"], "title": "Measuring Iterative Temporal Reasoning with TimePuzzles", "comment": null, "summary": "We introduce TimePuzzles, a constraint-based date inference task for evaluating iterative temporal reasoning. Each puzzle combines factual temporal anchors with (cross-cultural) calendar relations, admits one or multiple valid solution dates, and is algorithmically generated for controlled, dynamic, and continual evaluation. Across 13 diverse LLMs, TimePuzzles well distinguishes their iterative temporal reasoning capabilities and remains challenging without tools: GPT-5 reaches only 49.3% accuracy and all other models stay below 31%, despite the dataset's simplicity. Web search consistently yields substantial gains and using code interpreter shows mixed effects, but all models perform much better when constraints are rewritten with explicit dates, revealing a gap in reliable tool use. Overall, TimePuzzles presents a simple, cost-effective diagnostic for tool-augmented iterative temporal reasoning.", "AI": {"tldr": "TimePuzzles\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea6\u675f\u7684\u65e5\u671f\u63a8\u7406\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u8fed\u4ee3\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002\u8be5\u4efb\u52a1\u7ed3\u5408\u4e8b\u5b9e\u65f6\u95f4\u951a\u70b9\u548c\u8de8\u6587\u5316\u65e5\u5386\u5173\u7cfb\uff0c\u901a\u8fc7\u7b97\u6cd5\u751f\u6210\u53ef\u63a7\u3001\u52a8\u6001\u548c\u6301\u7eed\u8bc4\u4f30\u7684\u8c1c\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bca\u65ad\u5de5\u5177\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u5de5\u5177\u589e\u5f3a\u7684\u8fed\u4ee3\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u521b\u5efaTimePuzzles\u4efb\u52a1\uff0c\u7ed3\u5408\u4e8b\u5b9e\u65f6\u95f4\u951a\u70b9\u548c\u8de8\u6587\u5316\u65e5\u5386\u5173\u7cfb\uff0c\u7b97\u6cd5\u751f\u6210\u5305\u542b\u4e00\u4e2a\u6216\u591a\u4e2a\u6709\u6548\u89e3\u65e5\u671f\u7684\u8c1c\u9898\u3002\u8bc4\u4f30\u4e8613\u4e2a\u4e0d\u540cLLM\uff0c\u6d4b\u8bd5\u4e86\u7eaf\u6a21\u578b\u3001\u7f51\u7edc\u641c\u7d22\u548c\u4ee3\u7801\u89e3\u91ca\u5668\u4e09\u79cd\u8bbe\u7f6e\u3002", "result": "GPT-5\u4ec5\u8fbe\u523049.3%\u51c6\u786e\u7387\uff0c\u5176\u4ed6\u6a21\u578b\u5747\u4f4e\u4e8e31%\u3002\u7f51\u7edc\u641c\u7d22\u5e26\u6765\u663e\u8457\u63d0\u5347\uff0c\u4ee3\u7801\u89e3\u91ca\u5668\u6548\u679c\u4e0d\u4e00\u3002\u5f53\u7ea6\u675f\u88ab\u91cd\u5199\u4e3a\u660e\u786e\u65e5\u671f\u65f6\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u5927\u5e45\u6539\u5584\uff0c\u63ed\u793a\u4e86\u53ef\u9760\u5de5\u5177\u4f7f\u7528\u7684\u5dee\u8ddd\u3002", "conclusion": "TimePuzzles\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u5de5\u5177\u589e\u5f3a\u7684\u8fed\u4ee3\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u53ef\u9760\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2601.07058", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07058", "abs": "https://arxiv.org/abs/2601.07058", "authors": ["Aaron R. Flouro", "Shawn P. Chadwick"], "title": "Hallucinations Live in Variance", "comment": "8 pages, 3 figures", "summary": "Benchmarks measure whether a model is correct. They do not measure whether a model is reliable. This distinction is largely academic for single-shot inference, but becomes critical for agentic AI systems, where a single rephrased prompt can trigger cascading failures in multi-step execution. Yet this form of instability is not captured by existing evaluations.\n  Hallucinations live in variance: they arise when semantically equivalent prompts activate inconsistent internal pathways, producing divergent outputs. Consistent but incorrect outputs reflect bias or missing knowledge; confident guessing reflects calibration failure. Neither constitutes hallucination under this definition. When error is variance-dominated, reducing redundant pathways improves reliability without adding knowledge. We formalize this through Semantic Stability (SS), measured via Paraphrase Consistency (PC@k): generate k paraphrases, greedy decode each, compute mode agreement. SS is a diagnostic for variance-driven unreliability, not a method for improving correctness.\n  We show that a dense Qwen3-0.6B agrees with itself only 23.8% of the time; at 32% sparsity, agreement jumps to 55.9%. A phase diagram reveals the sweet spot where variance reduction outpaces bias accumulation, and regimes where stability collapses onto wrong answers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u8bed\u4e49\u7a33\u5b9a\u6027\u4f5c\u4e3a\u8bc4\u4f30AI\u4ee3\u7406\u53ef\u9760\u6027\u7684\u65b0\u6307\u6807\uff0c\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u5728\u8bed\u4e49\u7b49\u4ef7\u63d0\u793a\u4e0b\u7684\u8f93\u51fa\u4e00\u81f4\u6027\u6765\u8bca\u65ad\u65b9\u5dee\u9a71\u52a8\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u800c\u975e\u8bc4\u4f30\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ea\u8861\u91cf\u6a21\u578b\u662f\u5426\u6b63\u786e\uff0c\u4f46\u4e0d\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u53ef\u9760\u3002\u5bf9\u4e8e\u5355\u6b21\u63a8\u7406\u8fd9\u533a\u522b\u4e0d\u5927\uff0c\u4f46\u5bf9\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5355\u4e2a\u91cd\u65b0\u8868\u8ff0\u7684\u63d0\u793a\u53ef\u80fd\u5f15\u53d1\u591a\u6b65\u6267\u884c\u7684\u7ea7\u8054\u5931\u8d25\uff0c\u800c\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u672a\u88ab\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u6355\u6349\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u901a\u8fc7\u91ca\u4e49\u4e00\u81f4\u6027\u6765\u6d4b\u91cf\uff1a\u751f\u6210k\u4e2a\u8bed\u4e49\u7b49\u4ef7\u7684\u63d0\u793a\uff0c\u5bf9\u6bcf\u4e2a\u63d0\u793a\u8fdb\u884c\u8d2a\u5a6a\u89e3\u7801\uff0c\u8ba1\u7b97\u8f93\u51fa\u6a21\u5f0f\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u8bca\u65ad\u65b9\u5dee\u9a71\u52a8\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u800c\u975e\u6539\u8fdb\u6b63\u786e\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5bc6\u96c6\u7684Qwen3-0.6B\u6a21\u578b\u4e0e\u81ea\u8eab\u7684\u4e00\u81f4\u6027\u4ec5\u4e3a23.8%\uff0c\u800c\u572832%\u7a00\u758f\u5ea6\u4e0b\uff0c\u4e00\u81f4\u6027\u8dc3\u5347\u81f355.9%\u3002\u76f8\u56fe\u63ed\u793a\u4e86\u65b9\u5dee\u51cf\u5c11\u8d85\u8fc7\u504f\u5dee\u79ef\u7d2f\u7684\u6700\u4f73\u70b9\uff0c\u4ee5\u53ca\u7a33\u5b9a\u6027\u5d29\u6e83\u5230\u9519\u8bef\u7b54\u6848\u7684\u533a\u57df\u3002", "conclusion": "\u8bed\u4e49\u7a33\u5b9a\u6027\u662f\u8bc4\u4f30AI\u4ee3\u7406\u53ef\u9760\u6027\u7684\u5173\u952e\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u8bc6\u522b\u65b9\u5dee\u9a71\u52a8\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5e76\u663e\u793a\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u8def\u5f84\uff08\u5982\u7a00\u758f\u5316\uff09\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.07118", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07118", "abs": "https://arxiv.org/abs/2601.07118", "authors": ["Lucas Schott", "Elies Gherbi", "Hatem Hajri", "Sylvain Lamprier"], "title": "Reward-Preserving Attacks For Robust Reinforcement Learning", "comment": "19 pages, 6 figures, 4 algorithms, preprint", "summary": "Adversarial robustness in RL is difficult because perturbations affect entire trajectories: strong attacks can break learning, while weak attacks yield little robustness, and the appropriate strength varies by state. We propose $\u03b1$-reward-preserving attacks, which adapt the strength of the adversary so that an $\u03b1$ fraction of the nominal-to-worst-case return gap remains achievable at each state. In deep RL, we use a gradient-based attack direction and learn a state-dependent magnitude $\u03b7\\le \u03b7_{\\mathcal B}$ selected via a critic $Q^\u03c0_\u03b1((s,a),\u03b7)$ trained off-policy over diverse radii. This adaptive tuning calibrates attack strength and, with intermediate $\u03b1$, improves robustness across radii while preserving nominal performance, outperforming fixed- and random-radius baselines.", "AI": {"tldr": "\u63d0\u51fa\u03b1\u5956\u52b1\u4fdd\u6301\u653b\u51fb\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u5bf9\u6297\u653b\u51fb\u5f3a\u5ea6\uff0c\u5728\u4fdd\u6301\u540d\u4e49\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347RL\u6a21\u578b\u7684\u9c81\u68d2\u6027", "motivation": "RL\u4e2d\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u9762\u4e34\u6311\u6218\uff1a\u5f3a\u653b\u51fb\u4f1a\u7834\u574f\u5b66\u4e60\uff0c\u5f31\u653b\u51fb\u6548\u679c\u6709\u9650\uff0c\u4e14\u5408\u9002\u7684\u653b\u51fb\u5f3a\u5ea6\u968f\u72b6\u6001\u53d8\u5316\u3002\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u8c03\u6574\u653b\u51fb\u5f3a\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u03b1\u5956\u52b1\u4fdd\u6301\u653b\u51fb\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u5bf9\u624b\u5f3a\u5ea6\uff0c\u4f7f\u6bcf\u4e2a\u72b6\u6001\u4e0b\u540d\u4e49\u5230\u6700\u574f\u60c5\u51b5\u56de\u62a5\u5dee\u8ddd\u7684\u03b1\u6bd4\u4f8b\u4ecd\u53ef\u8fbe\u6210\u3002\u5728\u6df1\u5ea6RL\u4e2d\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u65b9\u5411\uff0c\u901a\u8fc7\u79bb\u7ebf\u7b56\u7565\u8bad\u7ec3\u7684critic Q^\u03c0_\u03b1((s,a),\u03b7)\u5b66\u4e60\u72b6\u6001\u76f8\u5173\u7684\u5e45\u5ea6\u03b7\u2264\u03b7_B\u3002", "result": "\u81ea\u9002\u5e94\u8c03\u4f18\u6821\u51c6\u4e86\u653b\u51fb\u5f3a\u5ea6\uff0c\u5728\u4e2d\u95f4\u03b1\u503c\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u540d\u4e49\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5728\u4e0d\u540c\u534a\u5f84\u8303\u56f4\u5185\u90fd\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u56fa\u5b9a\u534a\u5f84\u548c\u968f\u673a\u534a\u5f84\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u03b1\u5956\u52b1\u4fdd\u6301\u653b\u51fb\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u653b\u51fb\u5f3a\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86RL\u5bf9\u6297\u9c81\u68d2\u6027\u4e2d\u7684\u5f3a\u5ea6\u9009\u62e9\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.07264", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07264", "abs": "https://arxiv.org/abs/2601.07264", "authors": ["Weihao Xuan", "Qingcheng Zeng", "Heli Qi", "Yunze Xiao", "Junjue Wang", "Naoto Yokoya"], "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents", "comment": null, "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.", "AI": {"tldr": "\u7814\u7a76\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u6821\u51c6\u95ee\u9898\uff0c\u53d1\u73b0\u8bc1\u636e\u5de5\u5177\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u9a8c\u8bc1\u5de5\u5177\u6539\u5584\u6821\u51c6\uff0c\u63d0\u51faRL\u5fae\u8c03\u6846\u67b6\u4f18\u5316\u51c6\u786e\u6027\u548c\u6821\u51c6", "motivation": "\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u5904\u7406\u591a\u8f6e\u4efb\u52a1\u65f6\uff0c\u786e\u4fdd\u5176\u53ef\u4fe1\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u6821\u51c6\u4f5c\u4e3a\u53ef\u4fe1\u5ea6\u7684\u57fa\u7840\u652f\u67f1\uff0c\u5728\u5de5\u5177\u96c6\u6210\u7684\u5de5\u4f5c\u6d41\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u7814\u7a76\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u6821\u51c6\u52a8\u6001\uff0c\u7279\u522b\u662f\u4e0d\u540c\u5de5\u5177\u7c7b\u578b\u5bf9\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u8bed\u8a00\u5316\u6821\u51c6\uff0c\u53d1\u73b0\u5de5\u5177\u7c7b\u578b\u9a71\u52a8\u7684\u7f6e\u4fe1\u5ea6\u4e8c\u5206\u73b0\u8c61\u3002\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u4efb\u52a1\u51c6\u786e\u6027\u548c\u6821\u51c6\uff0c\u652f\u6301\u5168\u9762\u7684\u5956\u52b1\u8bbe\u8ba1\u57fa\u51c6\u3002", "result": "\u8bc1\u636e\u5de5\u5177\uff08\u5982\u7f51\u7edc\u641c\u7d22\uff09\u7cfb\u7edf\u6027\u5730\u5bfc\u81f4\u4e25\u91cd\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u800c\u9a8c\u8bc1\u5de5\u5177\uff08\u5982\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u53ef\u4ee5\u901a\u8fc7\u786e\u5b9a\u6027\u53cd\u9988\u6765\u951a\u5b9a\u63a8\u7406\u5e76\u51cf\u8f7b\u6821\u51c6\u9519\u8bef\u3002\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6821\u51c6\uff0c\u8fd8\u8868\u73b0\u51fa\u4ece\u672c\u5730\u8bad\u7ec3\u73af\u5883\u5230\u5608\u6742\u7f51\u7edc\u8bbe\u7f6e\u4ee5\u53ca\u4e0d\u540c\u9886\u57df\uff08\u5982\u6570\u5b66\u63a8\u7406\uff09\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u6821\u51c6\u7b56\u7565\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6784\u5efa\u80fd\u591f\u5728\u9ad8\u98ce\u9669\u3001\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u53ef\u9760\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u6211\u610f\u8bc6\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.07164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07164", "abs": "https://arxiv.org/abs/2601.07164", "authors": ["Min Wang", "Xin Li", "Mingzhong Wang", "Hasnaa Bennis"], "title": "Offline Meta-Reinforcement Learning with Flow-Based Task Inference and Adaptive Correction of Feature Overgeneralization", "comment": null, "summary": "Offline meta-reinforcement learning (OMRL) combines the strengths of learning from diverse datasets in offline RL with the adaptability to new tasks of meta-RL, promising safe and efficient knowledge acquisition by RL agents. However, OMRL still suffers extrapolation errors due to out-of-distribution (OOD) actions, compromised by broad task distributions and Markov Decision Process (MDP) ambiguity in meta-RL setups. Existing research indicates that the generalization of the $Q$ network affects the extrapolation error in offline RL. This paper investigates this relationship by decomposing the $Q$ value into feature and weight components, observing that while decomposition enhances adaptability and convergence in the case of high-quality data, it often leads to policy degeneration or collapse in complex tasks. We observe that decomposed $Q$ values introduce a large estimation bias when the feature encounters OOD samples, a phenomenon we term ''feature overgeneralization''. To address this issue, we propose FLORA, which identifies OOD samples by modeling feature distributions and estimating their uncertainties. FLORA integrates a return feedback mechanism to adaptively adjust feature components. Furthermore, to learn precise task representations, FLORA explicitly models the complex task distribution using a chain of invertible transformations. We theoretically and empirically demonstrate that FLORA achieves rapid adaptation and meta-policy improvement compared to baselines across various environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFLORA\u65b9\u6cd5\u89e3\u51b3\u79bb\u7ebf\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u6a21\u7279\u5f81\u5206\u5e03\u8bc6\u522bOOD\u6837\u672c\uff0c\u7ed3\u5408\u56de\u62a5\u53cd\u9988\u673a\u5236\u81ea\u9002\u5e94\u8c03\u6574\u7279\u5f81\u7ec4\u4ef6\uff0c\u5e76\u4f7f\u7528\u53ef\u9006\u53d8\u6362\u94fe\u5efa\u6a21\u590d\u6742\u4efb\u52a1\u5206\u5e03\u3002", "motivation": "\u79bb\u7ebf\u5143\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u4e86\u79bb\u7ebfRL\u4ece\u591a\u6837\u6570\u636e\u96c6\u5b66\u4e60\u7684\u80fd\u529b\u548c\u5143RL\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u5916\u63a8\u8bef\u5dee\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0Q\u7f51\u7edc\u5206\u89e3\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u4e0b\u589e\u5f3a\u9002\u5e94\u6027\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5bfc\u81f4\u7b56\u7565\u9000\u5316\uff0c\u7279\u522b\u662f\u7279\u5f81\u9047\u5230OOD\u6837\u672c\u65f6\u4ea7\u751f\"\u7279\u5f81\u8fc7\u5ea6\u6cdb\u5316\"\u95ee\u9898\u3002", "method": "\u63d0\u51faFLORA\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u5efa\u6a21\u7279\u5f81\u5206\u5e03\u548c\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u6765\u8bc6\u522bOOD\u6837\u672c\uff1b2\uff09\u96c6\u6210\u56de\u62a5\u53cd\u9988\u673a\u5236\u81ea\u9002\u5e94\u8c03\u6574\u7279\u5f81\u7ec4\u4ef6\uff1b3\uff09\u4f7f\u7528\u53ef\u9006\u53d8\u6362\u94fe\u663e\u5f0f\u5efa\u6a21\u590d\u6742\u4efb\u52a1\u5206\u5e03\u4ee5\u5b66\u4e60\u7cbe\u786e\u7684\u4efb\u52a1\u8868\u793a\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc1\u660eFLORA\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u5404\u79cd\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u9002\u5e94\u548c\u5143\u7b56\u7565\u6539\u8fdb\u3002", "conclusion": "FLORA\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7OOD\u6837\u672c\u8bc6\u522b\u548c\u81ea\u9002\u5e94\u7279\u5f81\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.07338", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07338", "abs": "https://arxiv.org/abs/2601.07338", "authors": ["Yanzhi Tian", "Cunxiang Wang", "Zeming Liu", "Heyan Huang", "Wenbo Yu", "Dawei Song", "Jie Tang", "Yuhang Guo"], "title": "Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation", "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced Machine Translation (MT), applying them to linguistically complex domains-such as Social Network Services, literature etc. In these scenarios, translations often require handling non-literal expressions, leading to the inaccuracy of MT metrics. To systematically investigate the reliability of MT metrics, we first curate a meta-evaluation dataset focused on non-literal translations, namely MENT. MENT encompasses four non-literal translation domains and features source sentences paired with translations from diverse MT systems, with 7,530 human-annotated scores on translation quality. Experimental results reveal the inaccuracies of traditional MT metrics and the limitations of LLM-as-a-Judge, particularly the knowledge cutoff and score inconsistency problem. To mitigate these limitations, we propose RATE, a novel agentic translation evaluation framework, centered by a reflective Core Agent that dynamically invokes specialized sub-agents. Experimental results indicate the efficacy of RATE, achieving an improvement of at least 3.2 meta score compared with current metrics. Further experiments demonstrate the robustness of RATE to general-domain MT evaluation. Code and dataset are available at: https://github.com/BITHLP/RATE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86RATE\u6846\u67b6\uff0c\u4e00\u4e2a\u57fa\u4e8e\u53cd\u601d\u6838\u5fc3\u4ee3\u7406\u7684\u7ffb\u8bd1\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u5b57\u9762\u7ffb\u8bd1\u573a\u666f\u4e0b\u4f20\u7edf\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u3001\u6587\u5b66\u7b49\u590d\u6742\u9886\u57df\u7684\u975e\u5b57\u9762\u8868\u8fbe\u7ffb\u8bd1\u65f6\uff0c\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u4e0d\u51c6\u786e\u95ee\u9898\u3002\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\u7684\u53ef\u9760\u6027\u3002", "method": "1. \u6784\u5efaMENT\u6570\u636e\u96c6\uff0c\u5305\u542b\u56db\u4e2a\u975e\u5b57\u9762\u7ffb\u8bd1\u9886\u57df\uff0c7,530\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u5206\uff1b2. \u63d0\u51faRATE\u6846\u67b6\uff0c\u4ee5\u53cd\u601d\u6838\u5fc3\u4ee3\u7406\u4e3a\u4e2d\u5fc3\uff0c\u52a8\u6001\u8c03\u7528\u4e13\u95e8\u5b50\u4ee3\u7406\u8fdb\u884c\u7ffb\u8bd1\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1. \u4f20\u7edf\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u4e0d\u51c6\u786e\u95ee\u9898\uff1b2. LLM-as-a-Judge\u65b9\u6cd5\u5b58\u5728\u77e5\u8bc6\u622a\u6b62\u548c\u8bc4\u5206\u4e0d\u4e00\u81f4\u95ee\u9898\uff1b3. RATE\u6846\u67b6\u76f8\u6bd4\u73b0\u6709\u6307\u6807\u81f3\u5c11\u63d0\u53473.2\u4e2a\u5143\u5206\u6570\uff0c\u4e14\u5728\u901a\u7528\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "RATE\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u975e\u5b57\u9762\u7ffb\u8bd1\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u4e3a\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u8bed\u8a00\u8868\u8fbe\u65f6\u3002", "topic": "code agent"}}
{"id": "2601.07348", "categories": ["cs.CL", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.07348", "abs": "https://arxiv.org/abs/2601.07348", "authors": ["Tu Hu", "Ronghao Chen", "Shuo Zhang", "Jianghao Yin", "Mou Xiao Feng", "Jingping Liu", "Shaolei Zhang", "Wenqi Jiang", "Yuqi Fang", "Sen Hu", "Yi Xu", "Huacan Wang"], "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "comment": "27 pages", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks.To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels.Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "AI": {"tldr": "\u63d0\u51faCSE\u65b9\u6cd5\u89e3\u51b3\u4ee3\u7801\u751f\u6210\u4e2d\u81ea\u8fdb\u5316\u65b9\u6cd5\u7684\u63a2\u7d22\u6548\u7387\u4f4e\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u9057\u4f20\u8fdb\u5316\u548c\u5206\u5c42\u8fdb\u5316\u8bb0\u5fc6\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5728EffiBench-X\u4e0a\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u81ea\u8fdb\u5316\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5728\u6709\u9650\u9884\u7b97\u5185\u53d1\u73b0\u590d\u6742\u5ea6\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u6e90\u4e8e\u521d\u59cb\u5316\u504f\u5dee\u4f7f\u8fdb\u5316\u9677\u5165\u4e0d\u826f\u89e3\u533a\u57df\u3001\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u968f\u673a\u64cd\u4f5c\u3001\u4ee5\u53ca\u8de8\u4efb\u52a1\u7ecf\u9a8c\u5229\u7528\u4e0d\u8db3\u4e09\u4e2a\u74f6\u9888", "method": "\u63d0\u51fa\u53d7\u63a7\u81ea\u8fdb\u5316(CSE)\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u751f\u6210\u7ed3\u6784\u4e0d\u540c\u7684\u7b97\u6cd5\u7b56\u7565\u4ee5\u8986\u76d6\u5e7f\u6cdb\u89e3\u7a7a\u95f4\uff1b2) \u9057\u4f20\u8fdb\u5316\u7528\u53cd\u9988\u5f15\u5bfc\u673a\u5236\u66ff\u4ee3\u968f\u673a\u64cd\u4f5c\uff0c\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u53d8\u5f02\u548c\u7ec4\u5408\u4ea4\u53c9\uff1b3) \u5206\u5c42\u8fdb\u5316\u8bb0\u5fc6\u5728\u4efb\u52a1\u95f4\u548c\u4efb\u52a1\u5185\u5c42\u9762\u6355\u83b7\u6210\u529f\u548c\u5931\u8d25\u7ecf\u9a8c", "result": "\u5728EffiBench-X\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCSE\u5728\u5404\u79cdLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002CSE\u4ece\u65e9\u671f\u4e16\u4ee3\u5c31\u5b9e\u73b0\u66f4\u9ad8\u6548\u7387\uff0c\u5e76\u5728\u6574\u4e2a\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u6301\u7eed\u6539\u8fdb", "conclusion": "CSE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u751f\u6210\u81ea\u8fdb\u5316\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u74f6\u9888\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u63a7\u5236\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u6301\u7eed\u7684\u6539\u8fdb", "topic": "code agent"}}
{"id": "2601.07408", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07408", "abs": "https://arxiv.org/abs/2601.07408", "authors": ["Ziheng Li", "Liu Kang", "Feng Xiao", "Luxi Xing", "Qingyi Si", "Zhuoran Li", "Weikang Gong", "Deqing Yang", "Yanghua Xiao", "Hongcheng Guo"], "title": "Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in Mathematical Reasoning", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has emerged as a promising critic-free reinforcement learning paradigm for reasoning tasks. However, standard GRPO employs a coarse-grained credit assignment mechanism that propagates group-level rewards uniformly to to every token in a sequence, neglecting the varying contribution of individual reasoning steps. We address this limitation by introducing Outcome-grounded Advantage Reshaping (OAR), a fine-grained credit assignment mechanism that redistributes advantages based on how much each token influences the model's final answer. We instantiate OAR via two complementary strategies: (1) OAR-P, which estimates outcome sensitivity through counterfactual token perturbations, serving as a high-fidelity attribution signal; (2) OAR-G, which uses an input-gradient sensitivity proxy to approximate the influence signal with a single backward pass. These importance signals are integrated with a conservative Bi-Level advantage reshaping scheme that suppresses low-impact tokens and boosts pivotal ones while preserving the overall advantage mass. Empirical results on extensive mathematical reasoning benchmarks demonstrate that while OAR-P sets the performance upper bound, OAR-G achieves comparable gains with negligible computational overhead, both significantly outperforming a strong GRPO baseline, pushing the boundaries of critic-free LLM reasoning.", "AI": {"tldr": "\u63d0\u51faOAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u6539\u8fdbGRPO\uff0c\u4f7f\u7528\u4e24\u79cd\u7b56\u7565\uff08OAR-P\u548cOAR-G\uff09\u57fa\u4e8etoken\u5bf9\u6700\u7ec8\u7b54\u6848\u7684\u5f71\u54cd\u91cd\u65b0\u5206\u914d\u4f18\u52bf\u503c\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u6807\u51c6GRPO\u4f7f\u7528\u7c97\u7c92\u5ea6\u7684\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u5c06\u7ec4\u7ea7\u5956\u52b1\u5747\u5300\u4f20\u64ad\u7ed9\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2atoken\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u63a8\u7406\u6b65\u9aa4\u7684\u8d21\u732e\u5dee\u5f02\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\u6765\u8bc6\u522b\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u3002", "method": "\u63d0\u51faOutcome-grounded Advantage Reshaping (OAR)\u65b9\u6cd5\uff1a1) OAR-P\u901a\u8fc7\u53cd\u4e8b\u5b9etoken\u6270\u52a8\u4f30\u8ba1\u7ed3\u679c\u654f\u611f\u6027\uff1b2) OAR-G\u4f7f\u7528\u8f93\u5165\u68af\u5ea6\u654f\u611f\u6027\u4ee3\u7406\u8fd1\u4f3c\u5f71\u54cd\u4fe1\u53f7\u3002\u4e24\u8005\u90fd\u91c7\u7528\u4fdd\u5b88\u7684\u53cc\u5c42\u4f18\u52bf\u91cd\u5851\u65b9\u6848\uff0c\u6291\u5236\u4f4e\u5f71\u54cdtoken\u5e76\u589e\u5f3a\u5173\u952etoken\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOAR-P\u8bbe\u5b9a\u4e86\u6027\u80fd\u4e0a\u9650\uff0cOAR-G\u4ee5\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u4e86\u76f8\u5f53\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e24\u8005\u90fd\u663e\u8457\u4f18\u4e8e\u5f3aGRPO\u57fa\u7ebf\u3002", "conclusion": "OAR\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u6539\u8fdb\u4e86GRPO\u6846\u67b6\uff0c\u4e3a\u65e0\u8bc4\u8bba\u5bb6LLM\u63a8\u7406\u8bbe\u5b9a\u4e86\u65b0\u7684\u6027\u80fd\u8fb9\u754c\uff0cOAR-G\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.07422", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07422", "abs": "https://arxiv.org/abs/2601.07422", "authors": ["Wen Luo", "Guangyue Peng", "Wei Li", "Shaohang Wei", "Feifan Song", "Liang Wang", "Nan Yang", "Xingxing Zhang", "Jing Jin", "Furu Wei", "Houfeng Wang"], "title": "Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations", "comment": null, "summary": "Despite their impressive capabilities, large language models (LLMs) frequently generate hallucinations. Previous work shows that their internal states encode rich signals of truthfulness, yet the origins and mechanisms of these signals remain unclear. In this paper, we demonstrate that truthfulness cues arise from two distinct information pathways: (1) a Question-Anchored pathway that depends on question-answer information flow, and (2) an Answer-Anchored pathway that derives self-contained evidence from the generated answer itself. First, we validate and disentangle these pathways through attention knockout and token patching. Afterwards, we uncover notable and intriguing properties of these two mechanisms. Further experiments reveal that (1) the two mechanisms are closely associated with LLM knowledge boundaries; and (2) internal representations are aware of their distinctions. Finally, building on these insightful findings, two applications are proposed to enhance hallucination detection performance. Overall, our work provides new insight into how LLMs internally encode truthfulness, offering directions for more reliable and self-aware generative systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86LLM\u5185\u90e8\u771f\u5b9e\u6027\u4fe1\u53f7\u7684\u4e24\u4e2a\u72ec\u7acb\u4fe1\u606f\u901a\u8def\uff1a\u95ee\u9898\u951a\u5b9a\u901a\u8def\u548c\u7b54\u6848\u951a\u5b9a\u901a\u8def\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u63d0\u5347\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\u5176\u5185\u90e8\u72b6\u6001\u7f16\u7801\u4e86\u4e30\u5bcc\u7684\u771f\u5b9e\u6027\u4fe1\u53f7\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u53f7\u7684\u8d77\u6e90\u548c\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793aLLM\u5185\u90e8\u771f\u5b9e\u6027\u7f16\u7801\u7684\u5177\u4f53\u673a\u5236\u3002", "method": "\u901a\u8fc7\u6ce8\u610f\u529b\u6572\u9664\u548ctoken\u4fee\u8865\u6280\u672f\u9a8c\u8bc1\u5e76\u5206\u79bb\u4e24\u4e2a\u4fe1\u606f\u901a\u8def\uff1a\u95ee\u9898\u951a\u5b9a\u901a\u8def\uff08\u4f9d\u8d56\u95ee\u9898-\u7b54\u6848\u4fe1\u606f\u6d41\uff09\u548c\u7b54\u6848\u951a\u5b9a\u901a\u8def\uff08\u4ece\u751f\u6210\u7b54\u6848\u672c\u8eab\u83b7\u53d6\u81ea\u5305\u542b\u8bc1\u636e\uff09\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8fd9\u4e24\u4e2a\u673a\u5236\u7684\u7279\u6027\u53ca\u5176\u4e0eLLM\u77e5\u8bc6\u8fb9\u754c\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u771f\u5b9e\u6027\u673a\u5236\u4e0eLLM\u77e5\u8bc6\u8fb9\u754c\u5bc6\u5207\u76f8\u5173\uff0c\u4e14\u5185\u90e8\u8868\u793a\u80fd\u591f\u533a\u5206\u8fd9\u4e24\u4e2a\u673a\u5236\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u5e94\u7528\u6765\u589e\u5f3a\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3LLM\u5185\u90e8\u5982\u4f55\u7f16\u7801\u771f\u5b9e\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u66f4\u5177\u81ea\u6211\u610f\u8bc6\u7684\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.07389", "categories": ["cs.LG", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07389", "abs": "https://arxiv.org/abs/2601.07389", "authors": ["Xueyan Niu", "Bo Bai", "Wei Han", "Weixi Zhang"], "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training", "comment": null, "summary": "Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4e2d\uff0c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0d\u80fd\u89e3\u8026\uff1a\u5148SFT\u540eRL\u4f1a\u589e\u52a0SFT\u635f\u5931\uff0c\u5148RL\u540eSFT\u4f1a\u964d\u4f4eRL\u83b7\u5f97\u7684\u5956\u52b1\u3002", "motivation": "\u73b0\u4ee3\u63a8\u7406\u6a21\u578b\u5e7f\u6cdb\u91c7\u7528\u4ea4\u66ff\u8fdb\u884cSFT\u548cRL\u8bad\u7ec3\u7684\u505a\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u8fd9\u4e24\u79cd\u65b9\u6cd5\u662f\u5426\u53ef\u4ee5\u89e3\u8026\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u63a2\u8ba8SFT\u548cRL\u5728\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u662f\u5426\u53ef\u4ee5\u5206\u79bb\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u5206\u6790SFT\u548cRL\u7684\u8026\u5408\u5173\u7cfb\uff1a1\uff09SFT-then-RL\u8026\u5408\uff1a\u5728SFT\u6700\u4f18\u6027\u4e0b\uff0cRL\u4f1a\u589e\u52a0SFT\u635f\u5931\uff1b2\uff09RL-then-SFT\u8026\u5408\uff1aSFT\u4f1a\u964d\u4f4eRL\u83b7\u5f97\u7684\u5956\u52b1\u3002\u5b9e\u9a8c\u5728Qwen3-0.6B\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u7406\u8bba\u9884\u6d4b\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u9a8c\u8bc1\u4e86SFT\u548cRL\u5728\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u4e0d\u80fd\u5206\u79bb\u800c\u4e0d\u635f\u5931\u5148\u524d\u6027\u80fd\u3002", "conclusion": "SFT\u548cRL\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u4e0d\u80fd\u89e3\u8026\uff0c\u5206\u79bb\u8fd9\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u5148\u524d\u83b7\u5f97\u7684\u6027\u80fd\u4e0b\u964d\u3002", "topic": "agent analysis"}}
{"id": "2601.07506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07506", "abs": "https://arxiv.org/abs/2601.07506", "authors": ["Dongryeol Lee", "Yerin Hwang", "Taegwan Kang", "Minwoo Lee", "Younhyung Chae", "Kyomin Jung"], "title": "Judging Against the Reference: Uncovering Knowledge-Driven Failures in LLM-Judges on QA Evaluation", "comment": "Under review, 21 pgs, 11 figures, 7 tables", "summary": "While large language models (LLMs) are increasingly used as automatic judges for question answering (QA) and other reference-conditioned evaluation tasks, little is known about their ability to adhere to a provided reference. We identify a critical failure mode of such reference-based LLM QA evaluation: when the provided reference conflicts with the judge model's parametric knowledge, the resulting scores become unreliable, substantially degrading evaluation fidelity. To study this phenomenon systematically, we introduce a controlled swapped-reference QA framework that induces reference-belief conflicts. Specifically, we replace the reference answer with an incorrect entity and construct diverse pairings of original and swapped references with correspondingly aligned candidate answers. Surprisingly, grading reliability drops sharply under swapped references across a broad set of judge models. We empirically show that this vulnerability is driven by judges' over-reliance on parametric knowledge, leading judges to disregard the given reference under conflict. Finally, we find that this failure persists under common prompt-based mitigation strategies, highlighting a fundamental limitation of LLM-as-a-judge evaluation and motivating reference-based protocols that enforce stronger adherence to the provided reference.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3aQA\u81ea\u52a8\u8bc4\u4f30\u5668\u65f6\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff1a\u5f53\u63d0\u4f9b\u7684\u53c2\u8003\u7b54\u6848\u4e0e\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81\u65f6\uff0c\u8bc4\u4f30\u53ef\u9760\u6027\u5927\u5e45\u4e0b\u964d\uff0c\u56e0\u4e3a\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u6570\u77e5\u8bc6\u800c\u5ffd\u89c6\u7ed9\u5b9a\u53c2\u8003\u3002", "motivation": "\u867d\u7136LLM\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5cQA\u7b49\u4efb\u52a1\u7684\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u4f46\u5bf9\u5176\u662f\u5426\u771f\u6b63\u9075\u5faa\u7ed9\u5b9a\u53c2\u8003\u7684\u80fd\u529b\u4e86\u89e3\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u5f53\u53c2\u8003\u4e0e\u6a21\u578b\u77e5\u8bc6\u51b2\u7a81\u65f6\u7684\u8bc4\u4f30\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165\u53d7\u63a7\u7684\"\u4ea4\u6362\u53c2\u8003\"QA\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53c2\u8003\u7b54\u6848\u66ff\u6362\u4e3a\u9519\u8bef\u5b9e\u4f53\u6765\u8bf1\u5bfc\u53c2\u8003-\u4fe1\u5ff5\u51b2\u7a81\u3002\u6784\u5efa\u539f\u59cb\u53c2\u8003\u4e0e\u4ea4\u6362\u53c2\u8003\u7684\u591a\u6837\u5316\u914d\u5bf9\uff0c\u5e76\u76f8\u5e94\u8c03\u6574\u5019\u9009\u7b54\u6848\u3002\u7cfb\u7edf\u6d4b\u8bd5\u591a\u79cd\u6cd5\u5b98\u6a21\u578b\u5728\u51b2\u7a81\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5728\u4ea4\u6362\u53c2\u8003\u6761\u4ef6\u4e0b\uff0c\u6240\u6709\u6d4b\u8bd5\u7684\u6cd5\u5b98\u6a21\u578b\u8bc4\u5206\u53ef\u9760\u6027\u5747\u663e\u8457\u4e0b\u964d\u3002\u8fd9\u79cd\u8106\u5f31\u6027\u6e90\u4e8e\u6cd5\u5b98\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u6570\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5728\u51b2\u7a81\u65f6\u5ffd\u89c6\u7ed9\u5b9a\u53c2\u8003\u3002\u5e38\u89c1\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u7f13\u89e3\u7b56\u7565\u4e5f\u65e0\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "conclusion": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u5f53\u53c2\u8003\u4e0e\u6a21\u578b\u77e5\u8bc6\u51b2\u7a81\u65f6\u8bc4\u4f30\u4e0d\u53ef\u9760\u3002\u8fd9\u51f8\u663e\u4e86\u9700\u8981\u8bbe\u8ba1\u66f4\u5f3a\u5236\u9075\u5faa\u7ed9\u5b9a\u53c2\u8003\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "topic": "agent analysis"}}
{"id": "2601.07516", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07516", "abs": "https://arxiv.org/abs/2601.07516", "authors": ["Yongqi Li", "Hao Lang", "Tieyun Qian", "Yongbin Li"], "title": "Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions", "comment": null, "summary": "Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u7684RL\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u4ece\u89c2\u5bdf\u4e2d\u5b66\u4e60\u673a\u5236\u6784\u5efa\u4ee3\u7801\u672c\uff0c\u5e76\u5229\u7528\u8de8\u6a21\u6001\u6295\u5f71\u5668\u7ed3\u5408\u914d\u5bf9\u56fe\u50cf-\u6587\u672c\u6570\u636e\u548c\u7eaf\u6587\u672c\u6570\u636e\u6765\u589e\u5f3a\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u7684\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u6a21\u6001\u5bf9\u8bdd\u4ee3\u7406\u7684\u5fae\u8c03\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4f46\u5904\u7406\u6781\u5927\u7684\u6587\u672c\u6807\u8bb0\u7a7a\u95f4\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u5bfb\u627e\u66f4\u7d27\u51d1\u7684\u8868\u793a\u65b9\u6cd5\u6765\u63d0\u9ad8RL\u5fae\u8c03\u7684\u6548\u7387\u3002", "method": "1) \u91c7\u7528\u4ece\u89c2\u5bdf\u4e2d\u5b66\u4e60\u673a\u5236\u6784\u5efa\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u7684\u4ee3\u7801\u672c\uff1b2) \u5229\u7528\u672a\u6765\u89c2\u5bdf\u4f30\u8ba1\u5f53\u524d\u6f5c\u5728\u52a8\u4f5c\u5e76\u91cd\u6784\u672a\u6765\u89c2\u5bdf\uff1b3) \u4f7f\u7528\u8de8\u6a21\u6001\u6295\u5f71\u5668\u5c06\u6587\u672c\u5d4c\u5165\u8f6c\u6362\u4e3a\u56fe\u50cf-\u6587\u672c\u5d4c\u5165\uff1b4) \u5728\u914d\u5bf9\u56fe\u50cf-\u6587\u672c\u6570\u636e\u4e0a\u521d\u59cb\u5316\u6295\u5f71\u5668\uff0c\u7136\u540e\u5728\u5927\u91cf\u7eaf\u6587\u672c\u6570\u636e\u4e0a\u4f7f\u7528\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u79cd\u5bf9\u8bdd\u4efb\u52a1\u4e0a\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u5e76\u5728\u591a\u79cdRL\u7b97\u6cd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u57fa\u4e8e\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u7684RL\u5fae\u8c03\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u7684\u5927\u6587\u672c\u7a7a\u95f4\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6570\u636e\u6e90\u548c\u8de8\u6a21\u6001\u6295\u5f71\u6280\u672f\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.07528", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07528", "abs": "https://arxiv.org/abs/2601.07528", "authors": ["Gagan Bhatia", "Hamdy Mubarak", "Mustafa Jarrar", "George Mikros", "Fadi Zaraket", "Mahmoud Alhirthani", "Mutaz Al-Khatib", "Logan Cochrane", "Kareem Darwish", "Rashid Yahiaoui", "Firoj Alam"], "title": "From RAG to Agentic RAG for Faithful Islamic Question Answering", "comment": null, "summary": "LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ISLAMICFAITHQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u57fa\u4e8e\u4ee3\u7406\u7684RAG\u6846\u67b6\uff0c\u7528\u4e8e\u4f0a\u65af\u5170\u95ee\u7b54\u4efb\u52a1\uff0c\u65e8\u5728\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u6a21\u578b\u5728\u7f3a\u4e4f\u8bc1\u636e\u65f6\u7684\u5f03\u6743\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u4f0a\u65af\u5170\u95ee\u7b54\u4e2d\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u672a\u57fa\u4e8e\u8bc1\u636e\u7684\u56de\u7b54\u53ef\u80fd\u5e26\u6765\u4e25\u91cd\u7684\u5b97\u6559\u540e\u679c\uff0c\u800c\u6807\u51c6\u7684MCQ/MRC\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff08\u5982\u81ea\u7531\u5f62\u5f0f\u7684\u5e7b\u89c9\u548c\u7f3a\u4e4f\u8bc1\u636e\u65f6\u7684\u5f03\u6743\u80fd\u529b\uff09\u3002", "method": "1) \u521b\u5efaISLAMICFAITHQA\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\uff083,810\u9879\uff09\uff1b2) \u5f00\u53d1\u7aef\u5230\u7aef\u7684\u4f0a\u65af\u5170\u5efa\u6a21\u5957\u4ef6\uff0825K\u963f\u62c9\u4f2f\u8bedSFT\u63a8\u7406\u5bf9\u30015K\u53cc\u8bed\u504f\u597d\u6837\u672c\u30016K\u53e4\u5170\u7ecf\u68c0\u7d22\u8bed\u6599\u5e93\uff09\uff1b3) \u6784\u5efa\u4ee3\u7406\u5f0fRAG\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u8fed\u4ee3\u8bc1\u636e\u641c\u7d22\u548c\u7b54\u6848\u4fee\u8ba2\u3002", "result": "\u68c0\u7d22\u63d0\u9ad8\u4e86\u6b63\u786e\u6027\uff0c\u4ee3\u7406\u5f0fRAG\u76f8\u6bd4\u6807\u51c6RAG\u83b7\u5f97\u6700\u5927\u589e\u76ca\uff0c\u5373\u4f7f\u4f7f\u7528\u5c0f\u6a21\u578b\uff08Qwen3 4B\uff09\u4e5f\u80fd\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f0a\u65af\u5170\u95ee\u7b54\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7406\u5f0fRAG\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u5728\u5b97\u6559\u654f\u611f\u9886\u57df\u7684\u8868\u73b0\uff0c\u76f8\u5173\u8d44\u6e90\u5c06\u516c\u5f00\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "topic": "code agent"}}
{"id": "2601.07473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07473", "abs": "https://arxiv.org/abs/2601.07473", "authors": ["Michael J. Clark"], "title": "AntiPaSTO: Self-Supervised Steering of Moral Reasoning", "comment": null, "summary": "As models grow more capable, human supervision breaks down: labels don't scale, outputs can be gamed, and training doesn't generalize. Scalable oversight requires steering methods that are internal, self-supervised, and transfer out-of-distribution; existing methods satisfy some but not all three. We introduce AntiPaSTO, which separates representations along an anti-parallel axis ($\u03b1=\\pm1$ produce opposite shifts), with coherence constraints preventing collapse. Human input is minimal: two contrasting words inserted into template sentences, no preference labels. Using 800 such pairs on Gemma-3-1B, AntiPaSTO beats prompting baselines by $6.9\\times$ on DailyDilemmas and maintains bidirectional control where prompting triggers refusal.\n  Code is available at https://github.com/wassname/AntiPaSTO.", "AI": {"tldr": "AntiPaSTO\u662f\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u76d1\u7763\u7684\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u8868\u5f81\u6cbf\u53cd\u5e73\u884c\u8f74\u5b9e\u73b0\u53cc\u5411\u63a7\u5236\uff0c\u4ec5\u9700\u5c11\u91cf\u5bf9\u6bd4\u8bcd\u5bf9\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u9053\u5fb7\u56f0\u5883\u7b49\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u4f20\u7edf\u4eba\u5de5\u76d1\u7763\u65b9\u6cd5\u9762\u4e34\u6807\u7b7e\u96be\u4ee5\u6269\u5c55\u3001\u8f93\u51fa\u53ef\u88ab\u64cd\u7eb5\u3001\u8bad\u7ec3\u65e0\u6cd5\u6cdb\u5316\u7b49\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5185\u90e8\u76d1\u7763\u3001\u81ea\u6211\u76d1\u7763\u4e14\u80fd\u8de8\u5206\u5e03\u6cdb\u5316\u7684\u53ef\u6269\u5c55\u76d1\u7763\u65b9\u6cd5\u3002", "method": "AntiPaSTO\u65b9\u6cd5\u5c06\u8868\u5f81\u6cbf\u53cd\u5e73\u884c\u8f74\u5206\u79bb\uff08\u03b1=\u00b11\u4ea7\u751f\u76f8\u53cd\u504f\u79fb\uff09\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u7ea6\u675f\u9632\u6b62\u8868\u5f81\u584c\u7f29\u3002\u4ec5\u9700\u5728\u6a21\u677f\u53e5\u5b50\u4e2d\u63d2\u5165\u4e24\u4e2a\u5bf9\u6bd4\u8bcd\u5bf9\uff0c\u65e0\u9700\u504f\u597d\u6807\u7b7e\uff0c\u4eba\u5de5\u8f93\u5165\u6781\u5c11\u3002", "result": "\u5728Gemma-3-1B\u6a21\u578b\u4e0a\u4f7f\u7528800\u4e2a\u8bcd\u5bf9\uff0cAntiPaSTO\u5728DailyDilemmas\u4efb\u52a1\u4e0a\u6bd4\u63d0\u793a\u57fa\u7ebf\u63d0\u53476.9\u500d\uff0c\u5e76\u80fd\u7ef4\u6301\u53cc\u5411\u63a7\u5236\uff0c\u800c\u63d0\u793a\u65b9\u6cd5\u4f1a\u89e6\u53d1\u62d2\u7edd\u54cd\u5e94\u3002", "conclusion": "AntiPaSTO\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u76d1\u7763\u65b9\u6cd5\uff0c\u4ec5\u9700\u6781\u5c11\u91cf\u4eba\u5de5\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u6a21\u578b\u884c\u4e3a\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.07606", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07606", "abs": "https://arxiv.org/abs/2601.07606", "authors": ["Bingyang Ye", "Shan Chen", "Jingxuan Tu", "Chen Liu", "Zidi Xiong", "Samuel Schmidgall", "Danielle S. Bitterman"], "title": "Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments", "comment": "under review", "summary": "Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models' judgments about these scientific ideas. Towards this goal, we introduce PoT, a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later (e.g., citations and shifts in researchers' agendas). PoT freezes a pre-cutoff snapshot of evidence in an offline sandbox and asks models to forecast post-cutoff outcomes, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment against signals such as peer-review awards. In addition, PoT provides a controlled testbed for agent-based research judgments that evaluate scientific ideas, comparing tool-using agents to non-agent baselines under prompt ablations and budget scaling. Across 30,000+ instances spanning four benchmark domains, we find that, compared with non-agent baselines, higher interaction budgets generally improve agent performance, while the benefit of tool use is strongly task-dependent. By combining time-partitioned, future-verifiable targets with an offline sandbox for tool use, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.", "AI": {"tldr": "PoT\u662f\u4e00\u4e2a\u534a\u53ef\u9a8c\u8bc1\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5bf9\u79d1\u5b66\u60f3\u6cd5\u7684\u5224\u65ad\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u5224\u65ad\u4e0e\u4e0b\u6e38\u53ef\u89c2\u5bdf\u4fe1\u53f7\uff08\u5982\u5f15\u7528\u91cf\uff09\u5173\u8054\uff0c\u5728\u79bb\u7ebf\u6c99\u7bb1\u4e2d\u9a8c\u8bc1\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLM\u5bf9\u79d1\u5b66\u60f3\u6cd5\u7684\u5224\u65ad\u8d28\u91cf\uff0c\u9700\u8981\u5efa\u7acb\u80fd\u591f\u9a8c\u8bc1\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u6027\u7684\u57fa\u51c6\u6846\u67b6\u3002", "method": "PoT\u6846\u67b6\u51bb\u7ed3\u622a\u6b62\u65f6\u95f4\u524d\u7684\u8bc1\u636e\u5feb\u7167\uff0c\u8981\u6c42\u6a21\u578b\u9884\u6d4b\u622a\u6b62\u65f6\u95f4\u540e\u7684\u7ed3\u679c\uff08\u5982\u5f15\u7528\u91cf\u3001\u7814\u7a76\u8bae\u7a0b\u8f6c\u53d8\uff09\uff0c\u5728\u79bb\u7ebf\u6c99\u7bb1\u4e2d\u8fdb\u884c\u53ef\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u652f\u6301\u57fa\u4e8e\u4ee3\u7406\u7684\u7814\u7a76\u5224\u65ad\u6d4b\u8bd5\u3002", "result": "\u5728\u8d85\u8fc730,000\u4e2a\u5b9e\u4f8b\u7684\u56db\u4e2a\u57fa\u51c6\u9886\u57df\u4e2d\uff0c\u76f8\u6bd4\u975e\u4ee3\u7406\u57fa\u7ebf\uff0c\u66f4\u9ad8\u7684\u4ea4\u4e92\u9884\u7b97\u901a\u5e38\u80fd\u63d0\u5347\u4ee3\u7406\u6027\u80fd\uff0c\u800c\u5de5\u5177\u4f7f\u7528\u7684\u6548\u76ca\u5219\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u5177\u4f53\u4efb\u52a1\u3002", "conclusion": "PoT\u901a\u8fc7\u65f6\u95f4\u5206\u533a\u3001\u672a\u6765\u53ef\u9a8c\u8bc1\u7684\u76ee\u6807\u548c\u79bb\u7ebf\u5de5\u5177\u4f7f\u7528\u6c99\u7bb1\uff0c\u652f\u6301\u5bf9\u9762\u5411\u672a\u6765\u7684\u79d1\u5b66\u60f3\u6cd5\u5224\u65ad\u4efb\u52a1\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2601.07696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07696", "abs": "https://arxiv.org/abs/2601.07696", "authors": ["Nick Ferguson", "Alan Bundy", "Kwabena Nuamah"], "title": "Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) are increasingly focused on \"reasoning\" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains 'essential actions' against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7ed3\u6784\u5316\u533a\u5206\u5143\u7ea7\u63a8\u7406\u4e0e\u5bf9\u8c61\u7ea7\u63a8\u7406\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u5730\u7f18\u653f\u6cbb\u6307\u6807\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30LLM\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u5143\u7ea7\u63a8\u7406\u8868\u73b0\u826f\u597d\u4f46\u5b58\u5728\u4efb\u52a1\u7406\u89e3\u7f3a\u9677\uff0c\u5c11\u6837\u672c\u63d0\u793a\u6548\u679c\u6709\u9650\uff0c\u9519\u8bef\u4fe1\u606f\u4e0d\u5f71\u54cd\u6027\u80fd\uff0c\u6570\u503c\u80fd\u529b\u8f83\u5dee\u3002", "motivation": "\u5f53\u524dLLM\u7814\u7a76\u4e2d\u5bf9\"\u63a8\u7406\"\u6982\u5ff5\u5b9a\u4e49\u6a21\u7cca\u91cd\u53e0\uff0c\u9700\u8981\u66f4\u7ed3\u6784\u5316\u65b9\u6cd5\u533a\u5206\u5143\u7ea7\u63a8\u7406\uff08\u89c4\u5212\u4e2d\u95f4\u6b65\u9aa4\uff09\u548c\u5bf9\u8c61\u7ea7\u63a8\u7406\uff08\u6267\u884c\u5177\u4f53\u6b65\u9aa4\uff09\uff0c\u4ee5\u66f4\u7cbe\u786e\u8bc4\u4f30LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u5730\u7f18\u653f\u6cbb\u6307\u6807\u7684\u56fd\u5bb6\u5e74\u5ea6\u6570\u636e\u95ee\u7b54\u4efb\u52a1\uff0c\u8981\u6c42\u5206\u89e3\u4e2d\u95f4\u6b65\u9aa4\u3001\u6570\u636e\u68c0\u7d22\u548c\u6570\u5b66\u8fd0\u7b97\u3002\u901a\u8fc7\u5206\u6790LLM\u9009\u62e9\u9002\u5f53\u5de5\u5177\u7684\u80fd\u529b\u8bc4\u4f30\u5143\u7ea7\u63a8\u7406\uff0c\u8bbe\u7f6e\"\u5fc5\u8981\u52a8\u4f5c\"\u57fa\u51c6\u6bd4\u8f83\u5de5\u5177\u8c03\u7528\u8f93\u51fa\u3002", "result": "LLM\u5728\u5143\u7ea7\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4efb\u52a1\u7406\u89e3\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff1b\u5c11\u6837\u672c\u63d0\u793a\u5bf9\u51c6\u786e\u6027\u5f71\u54cd\u4e0d\u5927\uff1b\u9047\u5230\u7684\u9519\u8bef\u4fe1\u606f\u901a\u5e38\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\uff1b\u63d0\u4f9b\u989d\u5916\u8bc1\u636e\u8868\u660eLLM\u6570\u503c\u80fd\u529b\u8f83\u5dee\u3002", "conclusion": "LLM\u5177\u5907\u826f\u597d\u7684\u5143\u7ea7\u63a8\u7406\u80fd\u529b\u4f46\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7814\u7a76\u7ed3\u679c\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u4efb\u52a1\u9886\u57df\uff0c\u4e3a\u66f4\u7cbe\u786e\u8bc4\u4f30LLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.07711", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07711", "abs": "https://arxiv.org/abs/2601.07711", "authors": ["Pietro Ferrazzi", "Milica Cvjeticanin", "Alessio Piraccini", "Davide Giannuzzi"], "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.", "AI": {"tldr": "\u8bba\u6587\u5bf9Enhanced RAG\u548cAgentic RAG\u4e24\u79cd\u8303\u5f0f\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684RAG\u8bbe\u8ba1\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5b58\u5728\u68c0\u7d22\u566a\u58f0\u3001\u8bef\u7528\u68c0\u7d22\u3001\u67e5\u8be2-\u6587\u6863\u5339\u914d\u5f31\u3001\u751f\u6210\u5668\u53ef\u53d8\u6027/\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002\u867d\u7136Enhanced RAG\u901a\u8fc7\u4e13\u7528\u6a21\u5757\u89e3\u51b3\u7279\u5b9a\u5f31\u70b9\uff0cAgentic RAG\u5229\u7528LLM\u7684\u81ea\u6211\u53cd\u601d\u80fd\u529b\u5b9e\u73b0\u7aef\u5230\u7aef\u7f16\u6392\uff0c\u4f46\u4e24\u79cd\u8303\u5f0f\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u66f4\u4f18\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u91c7\u7528\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u9a71\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u573a\u666f\u548c\u7ef4\u5ea6\u4e0a\u5bf9Enhanced RAG\u548cAgentic RAG\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff0c\u8003\u8651\u6027\u80fd\u548c\u6210\u672c\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4e24\u79cd\u8303\u5f0f\u4e4b\u95f4\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u54ea\u79cdRAG\u8bbe\u8ba1\u66f4\u6709\u6548\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u9009\u62e9\u6700\u6709\u6548\u7684RAG\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728Enhanced\u548cAgentic RAG\u4e4b\u95f4\u505a\u51fa\u660e\u667a\u51b3\u7b56\uff0c\u540c\u65f6\u8003\u8651\u6210\u672c\u548c\u6027\u80fd\u56e0\u7d20\u3002", "topic": "agent analysis"}}
{"id": "2601.07780", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07780", "abs": "https://arxiv.org/abs/2601.07780", "authors": ["Mariana Costa", "Alberlucia Rafael Soarez", "Daniel Kim", "Camila Ferreira"], "title": "Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection", "comment": null, "summary": "While Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After initial CoT, PR-CoT guides the LLM to self-assess its reasoning across multiple predefined angles: logical consistency, information completeness, biases/ethics, and alternative solutions. Implemented purely via prompt engineering, this process refines the initial CoT into a more robust and accurate final answer without model retraining. Experiments across arithmetic, commonsense, ethical decision-making, and logical puzzles, using GPT-three point five and GPT-four models, demonstrate PR-CoT's superior performance. It significantly outperforms traditional CoT and existing reflection methods in logical consistency and error correction, with notable gains in nuanced domains like ethical decision-making. Ablation studies, human evaluations, and qualitative analyses further validate the contribution of each reflection perspective and the overall efficacy of our poly-reflective paradigm in fostering more reliable LLM reasoning.", "AI": {"tldr": "\u63d0\u51faMyGO PR-CoT\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89d2\u5ea6\u53cd\u601d\u63d0\u5347LLM\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3", "motivation": "\u73b0\u6709CoT\u63d0\u793a\u5728\u590d\u6742\u6216\u4f26\u7406\u654f\u611f\u4efb\u52a1\u4e2d\u5b58\u5728\u4e00\u81f4\u6027\u3001\u51c6\u786e\u6027\u548c\u81ea\u6821\u6b63\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5355\u7ef4\u53cd\u601d\u65b9\u6cd5\u6539\u8fdb\u6709\u9650", "method": "\u63d0\u51fa\u591a\u89d2\u5ea6\u53cd\u601d\u94fe\u5f0f\u601d\u7ef4(PR-CoT)\uff0c\u5728\u521d\u59cbCoT\u540e\u5f15\u5bfcLLM\u4ece\u903b\u8f91\u4e00\u81f4\u6027\u3001\u4fe1\u606f\u5b8c\u6574\u6027\u3001\u504f\u89c1/\u4f26\u7406\u3001\u66ff\u4ee3\u65b9\u6848\u7b49\u591a\u4e2a\u9884\u8bbe\u89d2\u5ea6\u8fdb\u884c\u81ea\u6211\u8bc4\u4f30\uff0c\u901a\u8fc7\u7eaf\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u63a8\u7406", "result": "\u5728\u7b97\u672f\u3001\u5e38\u8bc6\u3001\u4f26\u7406\u51b3\u7b56\u548c\u903b\u8f91\u8c1c\u9898\u7b49\u4efb\u52a1\u4e0a\uff0cPR-CoT\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCoT\u548c\u73b0\u6709\u53cd\u601d\u65b9\u6cd5\uff0c\u5728\u903b\u8f91\u4e00\u81f4\u6027\u548c\u9519\u8bef\u6821\u6b63\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5728\u4f26\u7406\u51b3\u7b56\u7b49\u590d\u6742\u9886\u57df\u63d0\u5347\u660e\u663e", "conclusion": "\u591a\u89d2\u5ea6\u53cd\u601d\u8303\u5f0f\u80fd\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u7684\u53ef\u9760\u6027\uff0c\u5404\u53cd\u601d\u89d2\u5ea6\u5747\u6709\u8d21\u732e\uff0c\u4e3a\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3\u7684\u63a8\u7406\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2601.07782", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07782", "abs": "https://arxiv.org/abs/2601.07782", "authors": ["Wei Fang", "James Glass"], "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning", "comment": null, "summary": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.", "AI": {"tldr": "TOOLQP\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5c06\u5de5\u5177\u68c0\u7d22\u5efa\u6a21\u4e3a\u8fed\u4ee3\u67e5\u8be2\u89c4\u5212\uff0c\u901a\u8fc7\u5206\u89e3\u6307\u4ee4\u4e3a\u5b50\u4efb\u52a1\u5e76\u52a8\u6001\u751f\u6210\u67e5\u8be2\u6765\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4f7f\u7528\u5408\u6210\u67e5\u8be2\u8f68\u8ff9\u8bad\u7ec3\u5e76\u901a\u8fc7RLVR\u4f18\u5316\uff0c\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u4e0b\u6e38\u4ee3\u7406\u6267\u884c\u65b9\u9762\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5355\u6b21\u5bc6\u96c6\u68c0\u7d22\u5668\u5728\u5904\u7406\u590d\u6742\u8bf7\u6c42\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u62bd\u8c61\u7528\u6237\u76ee\u6807\u4e0e\u6280\u672f\u6587\u6863\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4ee5\u53ca\u56fa\u5b9a\u5927\u5c0f\u5d4c\u5165\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u7ec4\u5408\u5de5\u5177\u7ec4\u5408\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faTOOLQP\u6846\u67b6\uff0c\u5c06\u68c0\u7d22\u5efa\u6a21\u4e3a\u8fed\u4ee3\u67e5\u8be2\u89c4\u5212\u800c\u975e\u5355\u6b21\u5339\u914d\u3002\u6846\u67b6\u5c06\u6307\u4ee4\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u52a8\u6001\u751f\u6210\u67e5\u8be2\u4e0e\u68c0\u7d22\u5668\u4ea4\u4e92\uff0c\u4f7f\u7528\u5408\u6210\u67e5\u8be2\u8f68\u8ff9\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTOOLQP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u68c0\u7d22\u5668\u4e0a\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5728\u4e0b\u6e38\u4ee3\u7406\u6267\u884c\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "TOOLQP\u901a\u8fc7\u8fed\u4ee3\u67e5\u8be2\u89c4\u5212\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5de5\u5177\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u548c\u7ec4\u5408\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u52a8\u6001\u5de5\u5177\u5e93\u4e2d\u7684LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.07796", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.07796", "abs": "https://arxiv.org/abs/2601.07796", "authors": ["Shaz Furniturewala", "Gerard Christopher Yeo", "Kokil Jaidka"], "title": "Learning Through Dialogue: Unpacking the Dynamics of Human-LLM Conversations on Political Issues", "comment": null, "summary": "Large language models (LLMs) are increasingly used as conversational partners for learning, yet the interactional dynamics supporting users' learning and engagement are understudied. We analyze the linguistic and interactional features from both LLM and participant chats across 397 human-LLM conversations about socio-political issues to identify the mechanisms and conditions under which LLM explanations shape changes in political knowledge and confidence. Mediation analyses reveal that LLM explanatory richness partially supports confidence by fostering users' reflective insight, whereas its effect on knowledge gain operates entirely through users' cognitive engagement. Moderation analyses show that these effects are highly conditional and vary by political efficacy. Confidence gains depend on how high-efficacy users experience and resolve uncertainty. Knowledge gains depend on high-efficacy users' ability to leverage extended interaction, with longer conversations benefiting primarily reflective users. In summary, we find that learning from LLMs is an interactional achievement, not a uniform outcome of better explanations. The findings underscore the importance of aligning LLM explanatory behavior with users' engagement states to support effective learning in designing Human-AI interactive systems.", "AI": {"tldr": "LLM\u89e3\u91ca\u7684\u653f\u6cbb\u5b66\u4e60\u6548\u679c\u53d6\u51b3\u4e8e\u7528\u6237\u4e92\u52a8\u7279\u5f81\u800c\u975e\u89e3\u91ca\u8d28\u91cf\u672c\u8eab\uff0c\u9ad8\u653f\u6cbb\u6548\u80fd\u7528\u6237\u901a\u8fc7\u53cd\u601d\u548c\u8ba4\u77e5\u53c2\u4e0e\u83b7\u5f97\u4e0d\u540c\u6536\u76ca", "motivation": "\u7814\u7a76LLM\u4f5c\u4e3a\u5b66\u4e60\u4f19\u4f34\u65f6\uff0c\u89e3\u91ca\u6027\u4e92\u52a8\u5982\u4f55\u5f71\u54cd\u7528\u6237\u7684\u653f\u6cbb\u77e5\u8bc6\u548c\u4fe1\u5fc3\u53d8\u5316\uff0c\u63a2\u7d22\u4e92\u52a8\u52a8\u6001\u800c\u975e\u5355\u7eaf\u89e3\u91ca\u8d28\u91cf\u7684\u4f5c\u7528", "method": "\u5206\u6790397\u4e2a\u4eba\u7c7b-LLM\u5bf9\u8bdd\uff0c\u4f7f\u7528\u4e2d\u4ecb\u548c\u8c03\u8282\u5206\u6790\u63a2\u7a76LLM\u89e3\u91ca\u4e30\u5bcc\u5ea6\u3001\u7528\u6237\u53cd\u601d\u6d1e\u5bdf\u3001\u8ba4\u77e5\u53c2\u4e0e\u4e0e\u653f\u6cbb\u6548\u80fd\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb", "result": "LLM\u89e3\u91ca\u4e30\u5bcc\u5ea6\u901a\u8fc7\u4fc3\u8fdb\u7528\u6237\u53cd\u601d\u6d1e\u5bdf\u90e8\u5206\u652f\u6301\u4fe1\u5fc3\u63d0\u5347\uff0c\u800c\u5bf9\u77e5\u8bc6\u589e\u76ca\u5b8c\u5168\u901a\u8fc7\u7528\u6237\u8ba4\u77e5\u53c2\u4e0e\u5b9e\u73b0\uff1b\u8fd9\u4e9b\u6548\u679c\u53d7\u653f\u6cbb\u6548\u80fd\u8c03\u8282\uff0c\u9ad8\u6548\u80fd\u7528\u6237\u901a\u8fc7\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u5ef6\u957f\u4e92\u52a8\u83b7\u5f97\u4e0d\u540c\u6536\u76ca", "conclusion": "LLM\u5b66\u4e60\u662f\u4e92\u52a8\u6210\u5c31\u800c\u975e\u7edf\u4e00\u7ed3\u679c\uff0c\u9700\u8981\u5c06LLM\u89e3\u91ca\u884c\u4e3a\u4e0e\u7528\u6237\u53c2\u4e0e\u72b6\u6001\u5bf9\u9f50\u4ee5\u652f\u6301\u6709\u6548\u5b66\u4e60", "topic": "agent analysis"}}
{"id": "2601.07830", "categories": ["cs.LG", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.07830", "abs": "https://arxiv.org/abs/2601.07830", "authors": ["Valentina Njaradi", "Rodrigo Carrasco-Davis", "Peter E. Latham", "Andrew Saxe"], "title": "Optimal Learning Rate Schedule for Balancing Effort and Performance", "comment": null, "summary": "Learning how to learn efficiently is a fundamental challenge for biological agents and a growing concern for artificial ones. To learn effectively, an agent must regulate its learning speed, balancing the benefits of rapid improvement against the costs of effort, instability, or resource use. We introduce a normative framework that formalizes this problem as an optimal control process in which the agent maximizes cumulative performance while incurring a cost of learning. From this objective, we derive a closed-form solution for the optimal learning rate, which has the form of a closed-loop controller that depends only on the agent's current and expected future performance. Under mild assumptions, this solution generalizes across tasks and architectures and reproduces numerically optimized schedules in simulations. In simple learning models, we can mathematically analyze how agent and task parameters shape learning-rate scheduling as an open-loop control solution. Because the optimal policy depends on expectations of future performance, the framework predicts how overconfidence or underconfidence influence engagement and persistence, linking the control of learning speed to theories of self-regulated learning. We further show how a simple episodic memory mechanism can approximate the required performance expectations by recalling similar past learning experiences, providing a biologically plausible route to near-optimal behaviour. Together, these results provide a normative and biologically plausible account of learning speed control, linking self-regulated learning, effort allocation, and episodic memory estimation within a unified and tractable mathematical framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c4\u8303\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u901f\u5ea6\u63a7\u5236\u5efa\u6a21\u4e3a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u5b66\u4e60\u7387\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u60c5\u666f\u8bb0\u5fc6\u673a\u5236\u5b9e\u73b0\u8fd1\u4f3c\u6700\u4f18\u884c\u4e3a\u3002", "motivation": "\u751f\u7269\u548c\u4eba\u5de5\u667a\u80fd\u4f53\u90fd\u9700\u8981\u9ad8\u6548\u5b66\u4e60\uff0c\u4f46\u5fc5\u987b\u5e73\u8861\u5b66\u4e60\u901f\u5ea6\u4e0e\u52aa\u529b\u3001\u4e0d\u7a33\u5b9a\u6027\u548c\u8d44\u6e90\u6d88\u8017\u7684\u6210\u672c\u3002\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\u6765\u89c4\u8303\u5730\u63cf\u8ff0\u5b66\u4e60\u901f\u5ea6\u63a7\u5236\u95ee\u9898\u3002", "method": "\u5c06\u5b66\u4e60\u901f\u5ea6\u63a7\u5236\u5efa\u6a21\u4e3a\u6700\u4f18\u63a7\u5236\u8fc7\u7a0b\uff0c\u6700\u5927\u5316\u7d2f\u79ef\u6027\u80fd\u540c\u65f6\u8003\u8651\u5b66\u4e60\u6210\u672c\u3002\u63a8\u5bfc\u51fa\u6700\u4f18\u5b66\u4e60\u7387\u7684\u95ed\u5f0f\u89e3\uff0c\u5206\u6790\u4efb\u52a1\u548c\u667a\u80fd\u4f53\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u60c5\u666f\u8bb0\u5fc6\u673a\u5236\u8fd1\u4f3c\u6027\u80fd\u671f\u671b\u3002", "result": "\u83b7\u5f97\u4e86\u6700\u4f18\u5b66\u4e60\u7387\u7684\u95ed\u5f0f\u63a7\u5236\u5668\u5f62\u5f0f\uff0c\u8be5\u89e3\u5728\u4efb\u52a1\u548c\u67b6\u6784\u95f4\u5177\u6709\u666e\u9002\u6027\uff0c\u80fd\u590d\u73b0\u6570\u503c\u4f18\u5316\u7684\u8c03\u5ea6\u65b9\u6848\u3002\u5206\u6790\u8868\u660e\u8fc7\u5ea6\u81ea\u4fe1\u6216\u81ea\u4fe1\u4e0d\u8db3\u4f1a\u5f71\u54cd\u5b66\u4e60\u53c2\u4e0e\u5ea6\u548c\u6301\u4e45\u6027\uff0c\u60c5\u666f\u8bb0\u5fc6\u673a\u5236\u80fd\u8fd1\u4f3c\u5b9e\u73b0\u6700\u4f18\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b66\u4e60\u901f\u5ea6\u63a7\u5236\u63d0\u4f9b\u4e86\u89c4\u8303\u548c\u751f\u7269\u5b66\u5408\u7406\u7684\u89e3\u91ca\uff0c\u5c06\u81ea\u6211\u8c03\u8282\u5b66\u4e60\u3001\u52aa\u529b\u5206\u914d\u548c\u60c5\u666f\u8bb0\u5fc6\u4f30\u8ba1\u7edf\u4e00\u5728\u4e00\u4e2a\u53ef\u5904\u7406\u7684\u6570\u5b66\u6846\u67b6\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.67c0011d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fbuilding-a-code-review-system-that-uses-prod-data-to-predict-bugs%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-aicodereviewlaunch%26utm_content=newsletter-prod-blog-learnmore/2/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/jUMxeqbDn2XXLqGXHo_qh9mzqXEFDAhNDB-tTjhw3ps=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fbuilding-a-code-review-system-that-uses-prod-data-to-predict-bugs%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-aicodereviewlaunch%26utm_content=newsletter-prod-blog-learnmore/2/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/jUMxeqbDn2XXLqGXHo_qh9mzqXEFDAhNDB-tTjhw3ps=439", "authors": ["TLDR Newsletter"], "title": "Building an AI code reviewer grounded in production telemetry", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fbuilding-a-code-review-system-that-uses-prod-data-to-predict-bugs%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-aicodereviewlaunch%26utm_content=newsletter-prod-blog-learnmore/2/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/jUMxeqbDn2XXLqGXHo_qh9mzqXEFDAhNDB-tTjhw3ps=439", "summary": "Building an AI code reviewer grounded in production telemetry (Sponsor) If AI code reviewers are supposed to save time, why am I triaging 40 comments about trailing whitespace, \"consider using an f-string,\" and \"line exceeds 100 characters\" on a 10-line change?In this technical deep dive, Sentry breaks down how it built an AI code reviewer that uses production telemetry to predict actual errors \u2014 not just static style feedback. If you want to understand the architecture, pipeline, and filteri...", "source": "tldr", "AI": {"tldr": "Sentry\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u4ea7\u9065\u6d4b\u6570\u636e\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u4e13\u6ce8\u4e8e\u9884\u6d4b\u5b9e\u9645\u9519\u8bef\u800c\u975e\u9759\u6001\u98ce\u683c\u53cd\u9988", "motivation": "\u4f20\u7edfAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u4ea7\u751f\u5927\u91cf\u65e0\u5173\u7d27\u8981\u7684\u98ce\u683c\u5efa\u8bae\uff08\u5982\u5c3e\u968f\u7a7a\u683c\u3001\u4ee3\u7801\u683c\u5f0f\u7b49\uff09\uff0c\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u5b9e\u9645\u751f\u4ea7\u9519\u8bef\uff0c\u5f00\u53d1\u8005\u9700\u8981\u66f4\u667a\u80fd\u7684\u5ba1\u67e5\u5de5\u5177", "method": "\u4f7f\u7528\u751f\u4ea7\u9065\u6d4b\u6570\u636e\u8bad\u7ec3AI\u6a21\u578b\uff0c\u6784\u5efa\u5305\u542b\u67b6\u6784\u3001\u7ba1\u9053\u548c\u8fc7\u6ee4\u673a\u5236\u7684\u5b8c\u6574\u7cfb\u7edf\uff0c\u4f7fAI\u80fd\u591f\u57fa\u4e8e\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u6570\u636e\u9884\u6d4b\u771f\u5b9e\u9519\u8bef", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u8bc6\u522b\u5b9e\u9645\u751f\u4ea7\u9519\u8bef\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u51cf\u5c11\u65e0\u5173\u98ce\u683c\u5efa\u8bae\uff0c\u63d0\u9ad8\u5ba1\u67e5\u6548\u7387", "conclusion": "\u57fa\u4e8e\u751f\u4ea7\u9065\u6d4b\u7684AI\u4ee3\u7801\u5ba1\u67e5\u6bd4\u4f20\u7edf\u9759\u6001\u5206\u6790\u66f4\u6709\u6548\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u6709\u4ef7\u503c\u7684\u9519\u8bef\u9884\u6d4b\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2601.5b122723", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnader.substack.com%2Fp%2Fthe-complete-guide-to-building-agents%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/CXjpExFe4zNRtWqE7GAqFNA8B-wrNRCk4j5s9g2aI6k=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnader.substack.com%2Fp%2Fthe-complete-guide-to-building-agents%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/CXjpExFe4zNRtWqE7GAqFNA8B-wrNRCk4j5s9g2aI6k=439", "authors": ["TLDR Newsletter"], "title": "The Complete Guide to Building Agents with the Claude Agent SDK", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnader.substack.com%2Fp%2Fthe-complete-guide-to-building-agents%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/CXjpExFe4zNRtWqE7GAqFNA8B-wrNRCk4j5s9g2aI6k=439", "summary": "The Complete Guide to Building Agents with the Claude Agent SDK (22 minute read) This guide walks readers through the process of building a code review agent from scratch that can read files, run commands, edit code, and figure out the steps to accomplish a task. The resulting agent can analyze a code base, find bugs and security issues, and return structured feedback. The guide aims to help readers understand how the Claude Code SDK works so they can build whatever they actually need.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528Claude Agent SDK\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u80fd\u591f\u8bfb\u53d6\u6587\u4ef6\u3001\u8fd0\u884c\u547d\u4ee4\u3001\u7f16\u8f91\u4ee3\u7801\u5e76\u81ea\u4e3b\u89c4\u5212\u4efb\u52a1\u6b65\u9aa4\uff0c\u7528\u4e8e\u5206\u6790\u4ee3\u7801\u5e93\u3001\u53d1\u73b0bug\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3Claude Code SDK\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u6839\u636e\u5b9e\u9645\u9700\u6c42\u6784\u5efa\u81ea\u5b9a\u4e49\u7684\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406\uff0c\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528Claude Agent SDK\u9010\u6b65\u6784\u5efa\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406\uff0c\u5305\u62ec\u6587\u4ef6\u8bfb\u53d6\u3001\u547d\u4ee4\u6267\u884c\u3001\u4ee3\u7801\u7f16\u8f91\u548c\u4efb\u52a1\u89c4\u5212\u7b49\u529f\u80fd\u6a21\u5757\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u5206\u6790\u4ee3\u7801\u5e93\u3001\u53d1\u73b0bug\u548c\u5b89\u5168\u95ee\u9898\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\u7684\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406\u3002", "conclusion": "Claude Agent SDK\u4e3a\u6784\u5efa\u81ea\u5b9a\u4e49\u4ee3\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u6839\u636e\u5177\u4f53\u9700\u6c42\u7075\u6d3b\u6784\u5efa\u5404\u79cd\u4ee3\u7801\u5904\u7406\u4ee3\u7406\u3002", "topic": "code agent"}}
{"id": "tldr.2601.a1fe86df", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.04171%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/Q64r3xKIomtD309GSX5GtDbWrK_P8WEmaHqN9Dk2j0M=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.04171%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/Q64r3xKIomtD309GSX5GtDbWrK_P8WEmaHqN9Dk2j0M=439", "authors": ["TLDR Newsletter"], "title": "Scale AI's Agentic Rubrics for Software Agent Verification", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 28 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.04171%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/Q64r3xKIomtD309GSX5GtDbWrK_P8WEmaHqN9Dk2j0M=439", "summary": "Scale AI's Agentic Rubrics for Software Agent Verification (28 minute read) Agentic Rubrics enable test-free verification of code changes by creating checklist-style evaluations from expert agents that interact with codebases. They outperform baseline methods on SWE-Bench Verified and offer a scalable, interpretable signal for reinforcement learning and inference-time feedback.", "source": "tldr", "AI": {"tldr": "Scale AI\u5f00\u53d1\u4e86Agentic Rubrics\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e13\u5bb6\u4ee3\u7406\u521b\u5efa\u6e05\u5355\u5f0f\u8bc4\u4f30\u6765\u9a8c\u8bc1\u4ee3\u7801\u53d8\u66f4\uff0c\u65e0\u9700\u6d4b\u8bd5\uff0c\u5728SWE-Bench Verified\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u7406\u65f6\u53cd\u9988\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u4fe1\u53f7\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u6d4b\u8bd5\uff0c\u4f46\u6d4b\u8bd5\u53ef\u80fd\u4e0d\u5b8c\u6574\u6216\u96be\u4ee5\u7f16\u5199\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6d4b\u8bd5\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u4ee3\u7801\u53d8\u66f4\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8f6f\u4ef6\u4ee3\u7406\u7684\u9a8c\u8bc1\u3002", "method": "\u4f7f\u7528\u4e13\u5bb6\u4ee3\u7406\u4e0e\u4ee3\u7801\u5e93\u4ea4\u4e92\uff0c\u521b\u5efa\u6e05\u5355\u5f0f\u8bc4\u4f30\uff08rubrics\uff09\u3002\u8fd9\u4e9b\u8bc4\u4f30\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u9a8c\u8bc1\u6807\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7801\u53d8\u66f4\u7684\u8d28\u91cf\u548c\u6b63\u786e\u6027\u3002", "result": "\u5728SWE-Bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentic Rubrics\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4ee3\u7801\u53d8\u66f4\u9a8c\u8bc1\u3002", "conclusion": "Agentic Rubrics\u4e3a\u8f6f\u4ef6\u4ee3\u7406\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u7406\u65f6\u53cd\u9988\u573a\u666f\u3002", "topic": "swe benchmark"}}
{"id": "tldr.2601.50b22f24", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fstate-of-ai-coding-2025%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_ai%23section-01/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/jGhuhWSTwg6HXaLNdXjLntyMZRfa7rAtV9Ty__v-Hb4=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fstate-of-ai-coding-2025%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_ai%23section-01/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/jGhuhWSTwg6HXaLNdXjLntyMZRfa7rAtV9Ty__v-Hb4=439", "authors": ["TLDR Newsletter"], "title": "Developer output jumped 76% in 2025... and team output rose even more", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fstate-of-ai-coding-2025%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_ai%23section-01/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/jGhuhWSTwg6HXaLNdXjLntyMZRfa7rAtV9Ty__v-Hb4=439", "summary": "Developer output jumped 76% in 2025... and team output rose even more (Sponsor) Developers are shipping more code - with PRs getting bigger and denser. Read Greptile's cross-industry study on the state of AI coding to see AI tool adoption, model growth trends, and model snapshots.", "source": "tldr", "AI": {"tldr": "2025\u5e74\u5f00\u53d1\u8005\u4ea7\u51fa\u589e\u957f76%\uff0c\u56e2\u961f\u4ea7\u51fa\u589e\u957f\u66f4\u591a\uff0cPR\u53d8\u5f97\u66f4\u5e9e\u5927\u5bc6\u96c6\uff0cGreptile\u8de8\u884c\u4e1a\u7814\u7a76\u663e\u793aAI\u7f16\u7801\u5de5\u5177\u91c7\u7528\u7387\u3001\u6a21\u578b\u589e\u957f\u8d8b\u52bf\u548c\u6a21\u578b\u5feb\u7167", "motivation": "\u5206\u6790AI\u7f16\u7801\u5de5\u5177\u57282025\u5e74\u7684\u91c7\u7528\u60c5\u51b5\uff0c\u4e86\u89e3\u5f00\u53d1\u8005\u4ea7\u51fa\u548c\u56e2\u961f\u6548\u7387\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u7814\u7a76AI\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u751f\u4ea7\u529b\u7684\u5f71\u54cd", "method": "Greptile\u8fdb\u884c\u7684\u8de8\u884c\u4e1a\u7814\u7a76\uff0c\u5206\u6790\u5f00\u53d1\u8005\u4ea7\u51fa\u6570\u636e\u3001PR\u5927\u5c0f\u548c\u5bc6\u5ea6\u53d8\u5316\uff0c\u8c03\u67e5AI\u5de5\u5177\u91c7\u7528\u7387\uff0c\u8ffd\u8e2a\u6a21\u578b\u589e\u957f\u8d8b\u52bf", "result": "2025\u5e74\u5f00\u53d1\u8005\u4ea7\u51fa\u589e\u957f76%\uff0c\u56e2\u961f\u4ea7\u51fa\u589e\u957f\u66f4\u591a\uff0cPR\u53d8\u5f97\u66f4\u5927\u66f4\u5bc6\u96c6\uff0cAI\u5de5\u5177\u91c7\u7528\u7387\u663e\u8457\u63d0\u5347\uff0c\u6a21\u578b\u6301\u7eed\u589e\u957f", "conclusion": "AI\u7f16\u7801\u5de5\u5177\u6b63\u5728\u663e\u8457\u63d0\u5347\u5f00\u53d1\u8005\u548c\u56e2\u961f\u7684\u751f\u4ea7\u529b\uff0c\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u4ea7\u51fa\u6a21\u5f0f", "topic": "swe application"}}
{"id": "tldr.2601.bccbfc0d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fdoobidoo%2Ffa84d31c0819a9faace345ca227b268f%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/HW8279fbQNZnoNPz7aCPE36yvCmfN7SGP_qWM5gTr58=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fdoobidoo%2Ffa84d31c0819a9faace345ca227b268f%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/HW8279fbQNZnoNPz7aCPE36yvCmfN7SGP_qWM5gTr58=439", "authors": ["TLDR Newsletter"], "title": "Universal Permission Request Hook for Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fdoobidoo%2Ffa84d31c0819a9faace345ca227b268f%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/HW8279fbQNZnoNPz7aCPE36yvCmfN7SGP_qWM5gTr58=439", "summary": "Universal Permission Request Hook for Claude Code (8 minute read) This GitHub gist contains a Claude Code hook that auto-approves read-only MCP tools to eliminate constant permission dialogs.", "source": "tldr", "AI": {"tldr": "GitHub gist\u63d0\u4f9bClaude Code\u94a9\u5b50\uff0c\u81ea\u52a8\u6279\u51c6\u53ea\u8bfbMCP\u5de5\u5177\u6743\u9650\u8bf7\u6c42\uff0c\u6d88\u9664\u9891\u7e41\u7684\u6743\u9650\u5f39\u7a97", "motivation": "\u89e3\u51b3Claude Code\u5728\u4f7f\u7528MCP\uff08Model Context Protocol\uff09\u5de5\u5177\u65f6\u9891\u7e41\u5f39\u51fa\u6743\u9650\u8bf7\u6c42\u5bf9\u8bdd\u6846\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5f00\u53d1\u4f53\u9a8c\u548c\u6548\u7387", "method": "\u5b9e\u73b0\u4e00\u4e2a\u901a\u7528\u7684\u6743\u9650\u8bf7\u6c42\u94a9\u5b50\uff08hook\uff09\uff0c\u81ea\u52a8\u8bc6\u522b\u5e76\u6279\u51c6\u53ea\u8bfb\u7c7b\u578b\u7684MCP\u5de5\u5177\u6743\u9650\u8bf7\u6c42", "result": "\u6210\u529f\u521b\u5efa\u4e86\u53ef\u51cf\u5c11\u6743\u9650\u5f39\u7a97\u5e72\u6270\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u5f00\u53d1\u8005\u5728\u4f7f\u7528Claude Code\u65f6\u83b7\u5f97\u66f4\u6d41\u7545\u7684\u4f53\u9a8c", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u5316\u6743\u9650\u7ba1\u7406\uff0c\u663e\u8457\u6539\u5584\u4e86Claude Code\u5de5\u5177\u7684\u4f7f\u7528\u4fbf\u5229\u6027\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u9891\u7e41\u4f7f\u7528\u53ea\u8bfbMCP\u5de5\u5177\u7684\u573a\u666f", "topic": "swe application"}}
{"id": "tldr.2601.4c5ad9d0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F8%2Fllm-predictions-for-2026%2F%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/6j_7Se7k1pqqQau7Gt0Gv0i8bXNcIU7mqc-LnDIeJDo=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F8%2Fllm-predictions-for-2026%2F%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/6j_7Se7k1pqqQau7Gt0Gv0i8bXNcIU7mqc-LnDIeJDo=439", "authors": ["TLDR Newsletter"], "title": "LLM Predictions for 2026, Shared with Oxide and Friends", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F8%2Fllm-predictions-for-2026%2F%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/6j_7Se7k1pqqQau7Gt0Gv0i8bXNcIU7mqc-LnDIeJDo=439", "summary": "LLM Predictions for 2026, Shared with Oxide and Friends (5 minute read) 2026 will mark the year when LLM-generated code quality becomes undeniably excellent, effective sandboxing solutions finally emerge to safely run untrusted code, and a major security incident (\"Challenger disaster\") exposes risks from over-privileged coding agents. Looking further ahead, the Jevons paradox for software engineering will resolve within 3 years.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u9884\u6d4b\u52302026\u5e74\uff0cLLM\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u5c06\u8fbe\u5230\u4f18\u79c0\u6c34\u5e73\uff0c\u6709\u6548\u7684\u6c99\u7bb1\u89e3\u51b3\u65b9\u6848\u5c06\u51fa\u73b0\u4ee5\u5b89\u5168\u8fd0\u884c\u4e0d\u53ef\u4fe1\u4ee3\u7801\uff0c\u540c\u65f6\u8fc7\u5ea6\u6388\u6743\u7684\u7f16\u7801\u4ee3\u7406\u5c06\u5bfc\u81f4\u91cd\u5927\u5b89\u5168\u4e8b\u4ef6\uff08\"\u6311\u6218\u8005\u53f7\u707e\u96be\"\uff09\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u9884\u6d4bLLM\u548c\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u53d1\u5c55\u8d8b\u52bf\uff0c\u7279\u522b\u662f\u5173\u6ce8\u4ee3\u7801\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u98ce\u9669\u63a7\u5236\u7b49\u65b9\u9762\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u524d\u77bb\u6027\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u9884\u6d4b\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5f53\u524dLLM\u548c\u7f16\u7801\u4ee3\u7406\u6280\u672f\u7684\u53d1\u5c55\u8d8b\u52bf\uff0c\u7ed3\u5408\u884c\u4e1a\u89c2\u5bdf\u548c\u4e13\u5bb6\u89c1\u89e3\uff0c\u5bf9\u672a\u67653-5\u5e74\u7684\u6280\u672f\u53d1\u5c55\u8fdb\u884c\u7cfb\u7edf\u6027\u9884\u6d4b\u3002", "result": "\u9884\u6d4b\u52302026\u5e74\uff1a1\uff09LLM\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u5c06\u8fbe\u5230\u4f18\u79c0\u6c34\u5e73\uff1b2\uff09\u6709\u6548\u7684\u6c99\u7bb1\u89e3\u51b3\u65b9\u6848\u5c06\u51fa\u73b0\uff1b3\uff09\u8fc7\u5ea6\u6388\u6743\u7684\u7f16\u7801\u4ee3\u7406\u5c06\u5bfc\u81f4\u91cd\u5927\u5b89\u5168\u4e8b\u4ef6\uff1b4\uff09\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6770\u6587\u65af\u6096\u8bba\u5c06\u57283\u5e74\u5185\u89e3\u51b3\u3002", "conclusion": "LLM\u548c\u7f16\u7801\u4ee3\u7406\u6280\u672f\u5c06\u5728\u672a\u6765\u51e0\u5e74\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u9700\u8981\u5e73\u8861\u6548\u7387\u63d0\u5347\u4e0e\u5b89\u5168\u98ce\u9669\uff0c\u884c\u4e1a\u5e94\u63d0\u524d\u51c6\u5907\u5e94\u5bf9\u5373\u5c06\u51fa\u73b0\u7684\u6280\u672f\u53d8\u9769\u548c\u5b89\u5168\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.440b06d0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpytorch.org%2Fblog%2Fsupercharging-llms-scalable-rl-with-torchforge-and-weaver%2F%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/OZXBeCq-JE-KA4FwOvkoot7rLQONb_cha_L46P_s4uE=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpytorch.org%2Fblog%2Fsupercharging-llms-scalable-rl-with-torchforge-and-weaver%2F%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/OZXBeCq-JE-KA4FwOvkoot7rLQONb_cha_L46P_s4uE=439", "authors": ["TLDR Newsletter"], "title": "Supercharging LLMs: Scalable RL with torchforge and Weaver", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpytorch.org%2Fblog%2Fsupercharging-llms-scalable-rl-with-torchforge-and-weaver%2F%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/OZXBeCq-JE-KA4FwOvkoot7rLQONb_cha_L46P_s4uE=439", "summary": "Supercharging LLMs: Scalable RL with torchforge and Weaver (4 minute read) Meta's torchforge is a PyTorch library for training LLMs with reinforcement learning that can scale to hundreds of GPUs. It hides most of the infrastructure complexity, supports fast experimentation, and uses a verifier system that cuts compute costs while improving accuracy across math, science, and reasoning tasks.", "source": "tldr", "AI": {"tldr": "Meta\u63a8\u51fatorchforge\u5e93\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\uff0c\u652f\u6301\u6570\u767eGPU\u6269\u5c55\uff0c\u964d\u4f4e\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u5ea6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u7cfb\u7edf\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6570\u5b66\u3001\u79d1\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709LLM\u8bad\u7ec3\u9762\u4e34\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u56f0\u96be\u3001\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5de5\u5177\u6765\u7b80\u5316\u5927\u89c4\u6a21RL\u8bad\u7ec3\u6d41\u7a0b\u5e76\u63d0\u5347\u6548\u7387\u3002", "method": "\u5f00\u53d1torchforge PyTorch\u5e93\uff0c\u9690\u85cf\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u6027\uff0c\u652f\u6301\u5feb\u901f\u5b9e\u9a8c\uff0c\u91c7\u7528\u9a8c\u8bc1\u5668\u7cfb\u7edf\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\uff0c\u5b9e\u73b0\u6570\u767eGPU\u7684\u6269\u5c55\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u6269\u5c55\u5230\u6570\u767eGPU\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u6570\u5b66\u3001\u79d1\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u652f\u6301\u5feb\u901f\u5b9e\u9a8c\u8fed\u4ee3\u3002", "conclusion": "torchforge\u4e3a\u5927\u89c4\u6a21LLM\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.a21dbb20", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fmcp-cli%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/U1D6Ow_dqR8KjOs7ob8IoHZ3OLj70oakeBtGYvrn8j0=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fmcp-cli%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/U1D6Ow_dqR8KjOs7ob8IoHZ3OLj70oakeBtGYvrn8j0=439", "authors": ["TLDR Newsletter"], "title": "Introducing MCP CLI: A way to call MCP Servers Efficiently", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fmcp-cli%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/U1D6Ow_dqR8KjOs7ob8IoHZ3OLj70oakeBtGYvrn8j0=439", "summary": "Introducing MCP CLI: A way to call MCP Servers Efficiently (7 minute read) MCP-CLI is a lightweight, open-source command-line tool that enables efficient, dynamic interaction with Model Context Protocol (MCP) servers. By supporting just-in-time tool discovery and execution instead of statically loading all tool definitions, it drastically reduces token consumption (up to 99% savings), making it ideal for AI coding agents like Gemini CLI or Claude Code.", "source": "tldr", "AI": {"tldr": "MCP CLI\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u901a\u8fc7\u5373\u65f6\u5de5\u5177\u53d1\u73b0\u548c\u6267\u884c\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u4e0eMCP\u670d\u52a1\u5668\u4ea4\u4e92\u65f6\u7684token\u6d88\u8017\uff08\u6700\u9ad8\u53ef\u8fbe99%\uff09\uff0c\u9002\u7528\u4e8eAI\u7f16\u7801\u4ee3\u7406\u3002", "motivation": "\u4f20\u7edf\u4e0eMCP\u670d\u52a1\u5668\u4ea4\u4e92\u9700\u8981\u9759\u6001\u52a0\u8f7d\u6240\u6709\u5de5\u5177\u5b9a\u4e49\uff0c\u5bfc\u81f4\u5927\u91cftoken\u6d88\u8017\uff0c\u5f71\u54cdAI\u7f16\u7801\u4ee3\u7406\u7684\u6548\u7387\u548c\u6210\u672c\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u5f00\u53d1\u8f7b\u91cf\u7ea7\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u91c7\u7528\u5373\u65f6\u5de5\u5177\u53d1\u73b0\u548c\u6267\u884c\u673a\u5236\uff0c\u800c\u975e\u9759\u6001\u52a0\u8f7d\u6240\u6709\u5de5\u5177\u5b9a\u4e49\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u6309\u9700\u7684MCP\u670d\u52a1\u5668\u4ea4\u4e92\u3002", "result": "MCP CLI\u80fd\u591f\u663e\u8457\u51cf\u5c11token\u6d88\u8017\uff08\u6700\u9ad8\u53ef\u8fbe99%\uff09\uff0c\u63d0\u9ad8AI\u7f16\u7801\u4ee3\u7406\uff08\u5982Gemini CLI\u3001Claude Code\uff09\u4e0eMCP\u670d\u52a1\u5668\u4ea4\u4e92\u7684\u6548\u7387\u3002", "conclusion": "MCP CLI\u901a\u8fc7\u521b\u65b0\u7684\u5373\u65f6\u5de5\u5177\u53d1\u73b0\u673a\u5236\uff0c\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684MCP\u670d\u52a1\u5668\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "tldr.2601.3251c959", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevery.to%2Fguides%2Fagent-native%3Fsource=post_button%26utm_source=tldrnewsletter/1/0100019bb1f407d0-63d03be3-b385-4436-aefe-65fcd0a75233-000000/Kp1vjvm4pvJVZ_-hwxWtYmFUj5SJDzJqZ-cxr50kzsU=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevery.to%2Fguides%2Fagent-native%3Fsource=post_button%26utm_source=tldrnewsletter/1/0100019bb1f407d0-63d03be3-b385-4436-aefe-65fcd0a75233-000000/Kp1vjvm4pvJVZ_-hwxWtYmFUj5SJDzJqZ-cxr50kzsU=439", "authors": ["TLDR Newsletter"], "title": "Agent-native Architectures", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 52 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevery.to%2Fguides%2Fagent-native%3Fsource=post_button%26utm_source=tldrnewsletter/1/0100019bb1f407d0-63d03be3-b385-4436-aefe-65fcd0a75233-000000/Kp1vjvm4pvJVZ_-hwxWtYmFUj5SJDzJqZ-cxr50kzsU=439", "summary": "Agent-native Architectures (52 minute read) Claude Code demonstrated that large language models with access to bash and file tools can accomplish complex multi-step tasks autonomously by operating in a loop until an objective is achieved. Good coding agents are really good general-purpose agents. The Claude Code software development kit makes this accessible. It allows developers to build applications where features are achieved by agents with tools. This opens up a new field of software that...", "source": "tldr", "AI": {"tldr": "Claude Code\u5c55\u793a\u4e86\u5177\u6709bash\u548c\u6587\u4ef6\u5de5\u5177\u8bbf\u95ee\u6743\u9650\u7684\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5faa\u73af\u64cd\u4f5c\u81ea\u4e3b\u5b8c\u6210\u590d\u6742\u7684\u591a\u6b65\u9aa4\u4efb\u52a1\uff0c\u4f18\u79c0\u7684\u7f16\u7801\u4ee3\u7406\u5b9e\u9645\u4e0a\u662f\u4f18\u79c0\u7684\u901a\u7528\u4ee3\u7406\uff0cClaude Code SDK\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u6784\u5efa\u7531\u5de5\u5177\u5316\u4ee3\u7406\u5b9e\u73b0\u529f\u80fd\u7684\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5de5\u5177\u8bbf\u95ee\u5b9e\u73b0\u81ea\u4e3b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5c06\u7f16\u7801\u4ee3\u7406\u7684\u80fd\u529b\u6269\u5c55\u5230\u901a\u7528\u4ee3\u7406\u9886\u57df\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6784\u5efa\u57fa\u4e8e\u4ee3\u7406\u7684\u5e94\u7528\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u539f\u751f\u67b6\u6784\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8bbf\u95eebash\u548c\u6587\u4ef6\u5de5\u5177\uff0c\u5728\u5faa\u73af\u64cd\u4f5c\u4e2d\u81ea\u4e3b\u6267\u884c\u591a\u6b65\u9aa4\u4efb\u52a1\uff0c\u76f4\u5230\u8fbe\u6210\u76ee\u6807\u3002Claude Code SDK\u63d0\u4f9b\u5f00\u53d1\u5de5\u5177\u5305\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u6784\u5efa\u57fa\u4e8e\u4ee3\u7406\u7684\u5e94\u7528\u3002", "result": "Claude Code\u6210\u529f\u5c55\u793a\u4e86\u5177\u6709\u5de5\u5177\u8bbf\u95ee\u6743\u9650\u7684\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u7f16\u7801\u4ee3\u7406\u53ef\u4ee5\u6210\u4e3a\u4f18\u79c0\u7684\u901a\u7528\u4ee3\u7406\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u7684\u5f00\u53d1\u5de5\u5177\u5305\u3002", "conclusion": "\u4ee3\u7406\u539f\u751f\u67b6\u6784\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u5f00\u8f9f\u4e86\u65b0\u9886\u57df\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u6784\u5efa\u7531\u5de5\u5177\u5316\u4ee3\u7406\u5b9e\u73b0\u529f\u80fd\u7684\u5e94\u7528\uff0c\u5c06\u7f16\u7801\u4ee3\u7406\u80fd\u529b\u6269\u5c55\u5230\u901a\u7528\u4ee3\u7406\u9886\u57df\u3002", "topic": "code agent"}}
{"id": "tldr.2601.59333c83", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productcompass.pm%2Fp%2Ffree-n8n-course%3Futm_source=tldrmarketing/1/0100019bb2199413-fe363c6d-92c0-4415-85f2-51c7ecffa39c-000000/Q6t4Q00UV4cGfe599t-TtWU5iSBC6DrrSRTzbe45nw0=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productcompass.pm%2Fp%2Ffree-n8n-course%3Futm_source=tldrmarketing/1/0100019bb2199413-fe363c6d-92c0-4415-85f2-51c7ecffa39c-000000/Q6t4Q00UV4cGfe599t-TtWU5iSBC6DrrSRTzbe45nw0=439", "authors": ["TLDR Newsletter"], "title": "Free n8n Course", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productcompass.pm%2Fp%2Ffree-n8n-course%3Futm_source=tldrmarketing/1/0100019bb2199413-fe363c6d-92c0-4415-85f2-51c7ecffa39c-000000/Q6t4Q00UV4cGfe599t-TtWU5iSBC6DrrSRTzbe45nw0=439", "summary": "Free n8n Course (59 minute video) This course walks through how to build an LLM workflow, a low-autonomy AI agent, and an autonomous agent from scratch using n8n, a combined workflow automation and agent-building platform that can handle use cases like competitor monitoring, chatbot-style assistants, inbox workers, and multi-agent research systems.", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528n8n\u5e73\u53f0\u6784\u5efaLLM\u5de5\u4f5c\u6d41\u3001\u4f4e\u81ea\u4e3b\u6027AI\u4ee3\u7406\u548c\u81ea\u4e3b\u4ee3\u7406\u7684\u514d\u8d39\u8bfe\u7a0b", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u5b66\u4e60\u4f7f\u7528n8n\u8fd9\u4e00\u7ed3\u5408\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u548c\u4ee3\u7406\u6784\u5efa\u7684\u5e73\u53f0\uff0c\u638c\u63e1\u6784\u5efa\u5404\u79cdAI\u4ee3\u7406\u7cfb\u7edf\u7684\u6280\u80fd", "method": "\u901a\u8fc759\u5206\u949f\u7684\u89c6\u9891\u8bfe\u7a0b\uff0c\u4ece\u96f6\u5f00\u59cb\u6f14\u793a\u5982\u4f55\u6784\u5efaLLM\u5de5\u4f5c\u6d41\u3001\u4f4e\u81ea\u4e3b\u6027AI\u4ee3\u7406\u548c\u81ea\u4e3b\u4ee3\u7406", "result": "\u5b66\u4e60\u8005\u80fd\u591f\u638c\u63e1\u4f7f\u7528n8n\u5e73\u53f0\u6784\u5efa\u7ade\u4e89\u5bf9\u624b\u76d1\u63a7\u3001\u804a\u5929\u673a\u5668\u4eba\u52a9\u624b\u3001\u6536\u4ef6\u7bb1\u5de5\u4f5c\u8005\u548c\u591a\u4ee3\u7406\u7814\u7a76\u7cfb\u7edf\u7b49\u5e94\u7528\u7684\u80fd\u529b", "conclusion": "n8n\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5e73\u53f0\uff0c\u80fd\u591f\u652f\u6301\u4ece\u7b80\u5355\u5de5\u4f5c\u6d41\u5230\u590d\u6742\u81ea\u4e3b\u4ee3\u7406\u7684\u5404\u79cdAI\u5e94\u7528\u5f00\u53d1", "topic": "agent analysis"}}
{"id": "tldr.2601.8dcd5153", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantirez.com%2Fnews%2F158%3Futm_source=tldrdevops/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/hyL-yCTEZugOeL-U_d3-o_T3OpsZtyERiKmM95oeI5k=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantirez.com%2Fnews%2F158%3Futm_source=tldrdevops/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/hyL-yCTEZugOeL-U_d3-o_T3OpsZtyERiKmM95oeI5k=439", "authors": ["TLDR Newsletter"], "title": "Don't fall into the anti-AI hype", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantirez.com%2Fnews%2F158%3Futm_source=tldrdevops/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/hyL-yCTEZugOeL-U_d3-o_T3OpsZtyERiKmM95oeI5k=439", "summary": "Don't fall into the anti-AI hype (5 minute read) Recent advances in large language models have fundamentally changed programming by enabling most code to be generated through prompting, shifting the primary skill from writing code to defining problems and reviewing solutions.", "source": "tldr", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6539\u53d8\u4e86\u7f16\u7a0b\u8303\u5f0f\uff0c\u4ece\u7f16\u5199\u4ee3\u7801\u8f6c\u5411\u95ee\u9898\u5b9a\u4e49\u548c\u89e3\u51b3\u65b9\u6848\u5ba1\u67e5", "motivation": "\u56de\u5e94\u53cdAI\u7092\u4f5c\uff0c\u6307\u51faLLM\u5e26\u6765\u7684\u7f16\u7a0b\u8303\u5f0f\u8f6c\u53d8\u662f\u79ef\u6781\u8fdb\u6b65\u800c\u975e\u5a01\u80c1", "method": "\u901a\u8fc7\u5206\u6790LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u8bba\u8bc1\u7f16\u7a0b\u6280\u80fd\u9700\u6c42\u7684\u53d8\u5316", "result": "\u7f16\u7a0b\u6838\u5fc3\u6280\u80fd\u4ece\u7f16\u7801\u8f6c\u5411\u95ee\u9898\u5b9a\u4e49\u548c\u4ee3\u7801\u5ba1\u67e5\uff0cLLM\u6210\u4e3a\u7f16\u7a0b\u5de5\u5177\u800c\u975e\u66ff\u4ee3", "conclusion": "\u4e0d\u5e94\u9677\u5165\u53cdAI\u7092\u4f5c\uff0c\u5e94\u62e5\u62b1LLM\u5e26\u6765\u7684\u7f16\u7a0b\u6548\u7387\u63d0\u5347\u548c\u6280\u80fd\u8f6c\u578b", "topic": "code agent"}}
{"id": "tldr.2601.39810cc2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/JITjo8KGxE7CXRlqoN0_pqEFbE5MivERWXLPLMzqMIs=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/JITjo8KGxE7CXRlqoN0_pqEFbE5MivERWXLPLMzqMIs=439", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/JITjo8KGxE7CXRlqoN0_pqEFbE5MivERWXLPLMzqMIs=439", "summary": "Don't fall into the anti-AI hype (5 minute read) Recent advances in large language models have fundamentally changed programming by enabling most code to be generated through prompting, shifting the primary skill from writing code to defining problems and reviewing solutions.", "source": "tldr", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6539\u53d8\u4e86\u7f16\u7a0b\u8303\u5f0f\uff0c\u4ece\u7f16\u5199\u4ee3\u7801\u8f6c\u5411\u95ee\u9898\u5b9a\u4e49\u548c\u65b9\u6848\u5ba1\u67e5", "motivation": "\u56de\u5e94\u53cdAI\u7092\u4f5c\uff0c\u5f3a\u8c03LLM\u5bf9\u7f16\u7a0b\u7684\u6839\u672c\u6027\u6539\u53d8\uff0c\u7f16\u7a0b\u6280\u80fd\u9700\u6c42\u4ece\u7f16\u7801\u8f6c\u5411\u95ee\u9898\u5b9a\u4e49\u548c\u65b9\u6848\u8bc4\u4f30", "method": "\u89c2\u70b9\u6027\u8bba\u8ff0\uff0c\u5206\u6790LLM\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u751f\u6210\u4ee3\u7801\uff0c\u4ee5\u53ca\u8fd9\u5bf9\u7f16\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u7684\u5f71\u54cd", "result": "\u7f16\u7a0b\u7684\u6838\u5fc3\u6280\u80fd\u8f6c\u53d8\u4e3a\uff1a1\uff09\u6e05\u6670\u5b9a\u4e49\u95ee\u9898 2\uff09\u6709\u6548\u5ba1\u67e5\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848 3\uff09\u96c6\u6210\u548c\u8c03\u8bd5\u4ee3\u7801", "conclusion": "\u4e0d\u5e94\u9677\u5165\u53cdAI\u7092\u4f5c\uff0c\u5e94\u63a5\u53d7LLM\u5e26\u6765\u7684\u7f16\u7a0b\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e13\u6ce8\u4e8e\u66f4\u9ad8\u5c42\u6b21\u7684\u6280\u80fd\u53d1\u5c55", "topic": "code agent"}}
{"id": "tldr.2601.e4bd37b1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/d4GGmb05L2M7hF-AJO4A500ILmsfp384TxKmx4h2vLA=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/d4GGmb05L2M7hF-AJO4A500ILmsfp384TxKmx4h2vLA=439", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/d4GGmb05L2M7hF-AJO4A500ILmsfp384TxKmx4h2vLA=439", "summary": "Don't fall into the anti-AI hype (5 minute read) Recent advances in large language models have fundamentally changed programming by enabling most code to be generated through prompting, shifting the primary skill from writing code to defining problems and reviewing solutions.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u53cd\u9a73\u4e86\"\u53cdAI\u7092\u4f5c\"\uff0c\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u6539\u53d8\u4e86\u7f16\u7a0b\u672c\u8d28\uff0c\u4ece\u5199\u4ee3\u7801\u8f6c\u5411\u5b9a\u4e49\u95ee\u9898\u548c\u5ba1\u67e5\u89e3\u51b3\u65b9\u6848", "motivation": "\u9488\u5bf9\u5f53\u524d\u5bf9AI\u7f16\u7a0b\u7684\u8d1f\u9762\u7092\u4f5c\uff0c\u4f5c\u8005\u5e0c\u671b\u6f84\u6e05\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7f16\u7a0b\u9886\u57df\u7684\u771f\u6b63\u5f71\u54cd\uff0c\u5f3a\u8c03\u7f16\u7a0b\u6280\u80fd\u7684\u8f6c\u53d8\u800c\u975e\u66ff\u4ee3", "method": "\u901a\u8fc7\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u8bba\u8bc1\u7f16\u7a0b\u6838\u5fc3\u6280\u80fd\u7684\u53d8\u5316\u8d8b\u52bf", "result": "\u7f16\u7a0b\u7684\u4e3b\u8981\u6280\u80fd\u5df2\u4ece\u7f16\u5199\u4ee3\u7801\u8f6c\u53d8\u4e3a\u5b9a\u4e49\u95ee\u9898\u548c\u5ba1\u67e5\u89e3\u51b3\u65b9\u6848\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u5927\u90e8\u5206\u4ee3\u7801\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u751f\u6210", "conclusion": "\u4e0d\u5e94\u9677\u5165\u53cdAI\u7092\u4f5c\uff0c\u800c\u5e94\u8ba4\u8bc6\u5230\u7f16\u7a0b\u672c\u8d28\u7684\u8f6c\u53d8\uff0c\u9002\u5e94\u65b0\u7684\u6280\u80fd\u9700\u6c42", "topic": "code agent"}}
{"id": "tldr.2601.c9f26dff", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/F3klcj47U_3AsUQoodX-lU5UrDBWzkgLAuwpSh4Y288=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/F3klcj47U_3AsUQoodX-lU5UrDBWzkgLAuwpSh4Y288=439", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/F3klcj47U_3AsUQoodX-lU5UrDBWzkgLAuwpSh4Y288=439", "summary": "Don't fall into the anti-AI hype (5 minute read) Recent advances in large language models have fundamentally changed programming by enabling most code to be generated through prompting, shifting the primary skill from writing code to defining problems and reviewing solutions.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u53cd\u9a73\u53cdAI\u7092\u4f5c\uff0c\u8ba4\u4e3aLLM\u5df2\u6539\u53d8\u7f16\u7a0b\u8303\u5f0f\uff0c\u4ece\u7f16\u5199\u4ee3\u7801\u8f6c\u5411\u95ee\u9898\u5b9a\u4e49\u548c\u65b9\u6848\u8bc4\u5ba1", "motivation": "\u9488\u5bf9\u5f53\u524d\u5bf9AI\u7f16\u7a0b\u7684\u8d1f\u9762\u7092\u4f5c\u548c\u62c5\u5fe7\uff0c\u4f5c\u8005\u5e0c\u671b\u6f84\u6e05LLM\u5bf9\u7f16\u7a0b\u7684\u771f\u6b63\u5f71\u54cd\uff0c\u5f3a\u8c03\u8303\u5f0f\u8f6c\u53d8\u800c\u975e\u5a01\u80c1", "method": "\u901a\u8fc7\u5206\u6790LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u8bba\u8bc1\u7f16\u7a0b\u6280\u80fd\u4ece\u7f16\u7801\u8f6c\u5411\u95ee\u9898\u5b9a\u4e49\u548c\u89e3\u51b3\u65b9\u6848\u8bc4\u5ba1\u7684\u8f6c\u53d8", "result": "LLM\u4f7f\u5927\u90e8\u5206\u4ee3\u7801\u53ef\u901a\u8fc7\u63d0\u793a\u751f\u6210\uff0c\u7f16\u7a0b\u6838\u5fc3\u6280\u80fd\u53d8\u4e3a\u95ee\u9898\u5b9a\u4e49\u548c\u65b9\u6848\u8bc4\u5ba1\uff0c\u800c\u975e\u4f20\u7edf\u7f16\u7801", "conclusion": "\u4e0d\u5e94\u9677\u5165\u53cdAI\u7092\u4f5c\uff0c\u5e94\u63a5\u53d7\u7f16\u7a0b\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u4e13\u6ce8\u4e8e\u66f4\u9ad8\u5c42\u6b21\u7684\u95ee\u9898\u5b9a\u4e49\u548c\u89e3\u51b3\u65b9\u6848\u8bc4\u5ba1\u80fd\u529b", "topic": "code agent"}}
{"id": "tldr.2601.df1b7636", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantirez.com%2Fnews%2F158%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/bU6SxV8qzyWqqsPjsware_0RIUCl24BfYbp2PhOhogU=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantirez.com%2Fnews%2F158%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/bU6SxV8qzyWqqsPjsware_0RIUCl24BfYbp2PhOhogU=439", "authors": ["TLDR Newsletter"], "title": "Don't fall into the anti-AI hype", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantirez.com%2Fnews%2F158%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/bU6SxV8qzyWqqsPjsware_0RIUCl24BfYbp2PhOhogU=439", "summary": "Don't fall into the anti-AI hype (7 minute read) AI is permanently changing programming, making manual coding obsolete for many tasks. LLMs are able to complete complex projects in hours instead of weeks, showing their current power in areas like library modification, bug fixing, and code generation. AI is a continuation of code democratization, and devs need to embrace and adapt to these new tools to multiply their capabilities.", "source": "tldr", "AI": {"tldr": "AI\u6b63\u5728\u5f7b\u5e95\u6539\u53d8\u7f16\u7a0b\uff0c\u4f7f\u624b\u52a8\u7f16\u7801\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8fc7\u65f6\uff0cLLMs\u80fd\u5728\u6570\u5c0f\u65f6\u5185\u5b8c\u6210\u590d\u6742\u9879\u76ee\uff0c\u5f00\u53d1\u4eba\u5458\u9700\u8981\u62e5\u62b1\u8fd9\u4e9b\u5de5\u5177\u6765\u63d0\u5347\u80fd\u529b", "motivation": "AI\u6b63\u5728\u6c38\u4e45\u6027\u5730\u6539\u53d8\u7f16\u7a0b\u9886\u57df\uff0c\u4f20\u7edf\u624b\u52a8\u7f16\u7801\u65b9\u5f0f\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u53d8\u5f97\u8fc7\u65f6\uff0c\u5f00\u53d1\u4eba\u5458\u9700\u8981\u9002\u5e94\u8fd9\u4e00\u53d8\u9769\u8d8b\u52bf", "method": "\u6587\u7ae0\u4e3b\u8981\u91c7\u7528\u89c2\u70b9\u9610\u8ff0\u548c\u5206\u6790\u7684\u65b9\u5f0f\uff0c\u901a\u8fc7\u89c2\u5bdfLLMs\u5728\u5e93\u4fee\u6539\u3001bug\u4fee\u590d\u548c\u4ee3\u7801\u751f\u6210\u7b49\u9886\u57df\u7684\u5b9e\u9645\u8868\u73b0\u6765\u8bba\u8bc1AI\u5bf9\u7f16\u7a0b\u7684\u5f71\u54cd", "result": "LLMs\u80fd\u591f\u5728\u6570\u5c0f\u65f6\u5185\u5b8c\u6210\u539f\u672c\u9700\u8981\u6570\u5468\u7684\u590d\u6742\u9879\u76ee\uff0c\u5c55\u793a\u4e86AI\u5728\u7f16\u7a0b\u9886\u57df\u7684\u5f3a\u5927\u80fd\u529b\uff0cAI\u662f\u4ee3\u7801\u6c11\u4e3b\u5316\u7684\u5ef6\u7eed", "conclusion": "\u5f00\u53d1\u4eba\u5458\u9700\u8981\u62e5\u62b1\u5e76\u9002\u5e94\u8fd9\u4e9b\u65b0\u5de5\u5177\uff0c\u5229\u7528AI\u6765\u500d\u589e\u81ea\u5df1\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u9677\u5165\u53cdAI\u7684\u7092\u4f5c\u4e2d", "topic": "code agent"}}
{"id": "tldr.2601.f0c4cafa", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fandisearch%2Fclaude-switcher%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/4dQVAGMxfVH-gczYMkTnPlqUlh3jYF9juvH8fGVpX_o=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fandisearch%2Fclaude-switcher%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/4dQVAGMxfVH-gczYMkTnPlqUlh3jYF9juvH8fGVpX_o=439", "authors": ["TLDR Newsletter"], "title": "Claude Switcher", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fandisearch%2Fclaude-switcher%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/4dQVAGMxfVH-gczYMkTnPlqUlh3jYF9juvH8fGVpX_o=439", "summary": "Claude Switcher (GitHub Repo) The Claude Code Switcher automates scripts using Claude Code. It enables executable markdown files with shebang support, allowing AI prompts to function as standard Unix-style scripts that can pipe data and chain together.", "source": "tldr", "AI": {"tldr": "Claude Switcher\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u811a\u672c\u5de5\u5177\uff0c\u4f7f\u7528Claude Code\u8ba9AI\u63d0\u793a\u53ef\u4ee5\u4f5c\u4e3a\u6807\u51c6\u7684Unix\u98ce\u683c\u811a\u672c\u8fd0\u884c\uff0c\u652f\u6301shebang\u548c\u7ba1\u9053\u64cd\u4f5c", "motivation": "\u7b80\u5316AI\u9a71\u52a8\u7684\u811a\u672c\u7f16\u5199\u6d41\u7a0b\uff0c\u8ba9AI\u63d0\u793a\u80fd\u591f\u50cf\u4f20\u7edf\u811a\u672c\u4e00\u6837\u76f4\u63a5\u6267\u884c\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u81ea\u52a8\u5316\u80fd\u529b", "method": "\u901a\u8fc7\u652f\u6301shebang\u7684\u53ef\u6267\u884cmarkdown\u6587\u4ef6\uff0c\u5c06AI\u63d0\u793a\u8f6c\u6362\u4e3a\u6807\u51c6Unix\u811a\u672c\uff0c\u652f\u6301\u7ba1\u9053\u6570\u636e\u6d41\u548c\u811a\u672c\u94fe\u5f0f\u8c03\u7528", "result": "\u5b9e\u73b0\u4e86AI\u63d0\u793a\u4f5c\u4e3a\u53ef\u6267\u884c\u811a\u672c\u7684\u529f\u80fd\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u50cf\u4f7f\u7528\u4f20\u7edf\u811a\u672c\u4e00\u6837\u4f7f\u7528AI\u751f\u6210\u7684\u4ee3\u7801", "conclusion": "Claude Switcher\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u5f0f\u5c06AI\u4ee3\u7801\u751f\u6210\u4e0e\u811a\u672c\u6267\u884c\u6d41\u7a0b\u65e0\u7f1d\u96c6\u6210\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u81ea\u52a8\u5316\u6c34\u5e73", "topic": "code agent"}}
{"id": "tldr.2601.d55a3fae", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcampedersen.com%2Fcode-is-clay%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/SOeF8nWf82LoEb8YzgE0xNR_EmnPegoE5DBUoRDqdas=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcampedersen.com%2Fcode-is-clay%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/SOeF8nWf82LoEb8YzgE0xNR_EmnPegoE5DBUoRDqdas=439", "authors": ["TLDR Newsletter"], "title": "Code is Clay", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcampedersen.com%2Fcode-is-clay%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/SOeF8nWf82LoEb8YzgE0xNR_EmnPegoE5DBUoRDqdas=439", "summary": "Code is Clay (3 minute read) AI's automation of commodity programming will allow software engineers to focus on creative, \"hypercube\" ideas, transforming the field into a more valued craft like post-industrial pottery.", "source": "tldr", "AI": {"tldr": "AI\u81ea\u52a8\u5316\u57fa\u7840\u7f16\u7a0b\u5c06\u8ba9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u4e13\u6ce8\u4e8e\u521b\u9020\u6027\u5de5\u4f5c\uff0c\u4f7f\u8f6f\u4ef6\u5f00\u53d1\u6210\u4e3a\u66f4\u6709\u4ef7\u503c\u7684\u5de5\u827a", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5927\u91cf\u65f6\u95f4\u82b1\u8d39\u5728\u91cd\u590d\u6027\u3001\u5546\u54c1\u5316\u7684\u7f16\u7a0b\u4efb\u52a1\u4e0a\uff0c\u9650\u5236\u4e86\u5de5\u7a0b\u5e08\u7684\u521b\u9020\u6027\u6f5c\u529b", "method": "\u901a\u8fc7AI\u81ea\u52a8\u5316\u5904\u7406\u5e38\u89c4\u7f16\u7a0b\u4efb\u52a1\uff0c\u8ba9\u4eba\u7c7b\u5de5\u7a0b\u5e08\u4e13\u6ce8\u4e8e\"\u8d85\u7acb\u65b9\u4f53\"\u5f0f\u7684\u521b\u65b0\u6027\u60f3\u6cd5\u548c\u590d\u6742\u95ee\u9898\u89e3\u51b3", "result": "\u8f6f\u4ef6\u5de5\u7a0b\u5c06\u4ece\u6280\u672f\u6027\u5de5\u4f5c\u8f6c\u53d8\u4e3a\u66f4\u50cf\u540e\u5de5\u4e1a\u65f6\u4ee3\u9676\u827a\u7684\u5de5\u827a\uff0c\u63d0\u5347\u4e13\u4e1a\u4ef7\u503c\u548c\u521b\u9020\u6027", "conclusion": "AI\u5c06\u91cd\u5851\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\uff0c\u4f7f\u5176\u4ece\u5546\u54c1\u5316\u7f16\u7a0b\u8f6c\u5411\u521b\u9020\u6027\u5de5\u827a\uff0c\u63d0\u5347\u5de5\u7a0b\u5e08\u7684\u793e\u4f1a\u5730\u4f4d\u548c\u4e13\u4e1a\u4ef7\u503c", "topic": "code agent"}}
{"id": "tldr.2601.2c78efe3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fie5QWS/1/0100019bb2514710-380a7812-0965-454c-939d-c070a2143933-000000/4jSfTw79zIVbxhhw0CXTtI-NbkrLS4mtfp6nNdCnM8s=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fie5QWS/1/0100019bb2514710-380a7812-0965-454c-939d-c070a2143933-000000/4jSfTw79zIVbxhhw0CXTtI-NbkrLS4mtfp6nNdCnM8s=439", "authors": ["TLDR Newsletter"], "title": "EIP-8004: Road to Mainnet", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fie5QWS/1/0100019bb2514710-380a7812-0965-454c-939d-c070a2143933-000000/4jSfTw79zIVbxhhw0CXTtI-NbkrLS4mtfp6nNdCnM8s=439", "summary": "EIP-8004: Road to Mainnet (4 minute read) EIP-8004 proposes establishing Ethereum as a coordination layer for autonomous AI agents through three on-chain registries: identity for agent identification, reputation for tracking behavioral history and trustworthiness, and validation for verifying capabilities and permissions. The specification has progressed from v0.4 to v1.0 as it matures with community input. This standard would enable agents to transact and coordinate with verifiable credentia...", "source": "tldr", "AI": {"tldr": "EIP-8004\u63d0\u6848\u65e8\u5728\u5c06\u4ee5\u592a\u574a\u5efa\u7acb\u4e3a\u81ea\u4e3bAI\u4ee3\u7406\u7684\u534f\u8c03\u5c42\uff0c\u901a\u8fc7\u4e09\u4e2a\u94fe\u4e0a\u6ce8\u518c\u8868\uff08\u8eab\u4efd\u3001\u58f0\u8a89\u3001\u9a8c\u8bc1\uff09\u6765\u5b9e\u73b0\u4ee3\u7406\u95f4\u7684\u53ef\u4fe1\u4ea4\u6613\u4e0e\u534f\u8c03\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u534f\u8c03\u673a\u5236\uff0c\u96be\u4ee5\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u53ef\u4fe1\u4ea4\u4e92\u3002\u8be5\u63d0\u6848\u65e8\u5728\u89e3\u51b3AI\u4ee3\u7406\u7684\u8eab\u4efd\u9a8c\u8bc1\u3001\u884c\u4e3a\u8ffd\u8e2a\u548c\u6743\u9650\u7ba1\u7406\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3bAI\u7cfb\u7edf\u5efa\u7acb\u53ef\u9760\u7684\u534f\u8c03\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u94fe\u4e0a\u6ce8\u518c\u8868\uff1a\u8eab\u4efd\u6ce8\u518c\u8868\u7528\u4e8e\u4ee3\u7406\u8bc6\u522b\uff0c\u58f0\u8a89\u6ce8\u518c\u8868\u7528\u4e8e\u8ffd\u8e2a\u884c\u4e3a\u5386\u53f2\u548c\u53ef\u4fe1\u5ea6\uff0c\u9a8c\u8bc1\u6ce8\u518c\u8868\u7528\u4e8e\u9a8c\u8bc1\u80fd\u529b\u548c\u6743\u9650\u3002\u63d0\u6848\u4ecev0.4\u6f14\u8fdb\u5230v1.0\u7248\u672c\uff0c\u901a\u8fc7\u793e\u533a\u53cd\u9988\u4e0d\u65ad\u5b8c\u5584\u89c4\u8303\u3002", "result": "\u63d0\u6848\u5df2\u4ece\u521d\u59cb\u7248\u672cv0.4\u53d1\u5c55\u5230\u6210\u719f\u7684v1.0\u7248\u672c\uff0c\u83b7\u5f97\u4e86\u793e\u533a\u652f\u6301\u3002\u8be5\u6807\u51c6\u5c06\u4f7fAI\u4ee3\u7406\u80fd\u591f\u4f7f\u7528\u53ef\u9a8c\u8bc1\u51ed\u8bc1\u8fdb\u884c\u4ea4\u6613\u548c\u534f\u8c03\uff0c\u4e3a\u4ee5\u592a\u574a\u4e0a\u7684AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "EIP-8004\u4e3a\u81ea\u4e3bAI\u4ee3\u7406\u5728\u4ee5\u592a\u574a\u4e0a\u7684\u534f\u8c03\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u4e0a\u6ce8\u518c\u8868\u7cfb\u7edf\u89e3\u51b3\u4e86\u8eab\u4efd\u3001\u58f0\u8a89\u548c\u9a8c\u8bc1\u7b49\u5173\u952e\u95ee\u9898\uff0c\u4e3aAI\u4ee3\u7406\u7684\u5927\u89c4\u6a21\u91c7\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.32abb818", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fagent-best-practices%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/g_I065aJomxQ4WpV5UnXMQKKDhmUf-WjHlIXk03gWhg=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fagent-best-practices%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/g_I065aJomxQ4WpV5UnXMQKKDhmUf-WjHlIXk03gWhg=439", "authors": ["TLDR Newsletter"], "title": "Best Practices for Coding with Agents", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fagent-best-practices%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/g_I065aJomxQ4WpV5UnXMQKKDhmUf-WjHlIXk03gWhg=439", "summary": "Best Practices for Coding with Agents (18 minute read) Tips for maximizing productivity with coding agents, including how to structure problems, guide multi-file changes, and ensure agents iterate effectively on code until tests pass.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4f7f\u7528\u4ee3\u7801\u4ee3\u7406\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u5305\u62ec\u5982\u4f55\u7ed3\u6784\u5316\u95ee\u9898\u3001\u6307\u5bfc\u591a\u6587\u4ef6\u53d8\u66f4\u4ee5\u53ca\u786e\u4fdd\u4ee3\u7406\u6709\u6548\u8fed\u4ee3\u4ee3\u7801\u76f4\u5230\u6d4b\u8bd5\u901a\u8fc7\u3002", "motivation": "\u968f\u7740\u4ee3\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bb8\u591a\u5f00\u53d1\u8005\u53d1\u73b0\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u65f6\u5b58\u5728\u6548\u7387\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6700\u5927\u5316\u4ee3\u7801\u4ee3\u7406\u7684\u751f\u4ea7\u529b\uff0c\u89e3\u51b3\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u5e38\u89c1\u6311\u6218\u3002", "method": "\u672c\u6587\u57fa\u4e8e\u5b9e\u8df5\u7ecf\u9a8c\u548c\u6848\u4f8b\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u7cfb\u7edf\u7684\u6700\u4f73\u5b9e\u8df5\u65b9\u6cd5\u3002\u5305\u62ec\uff1a1\uff09\u5982\u4f55\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u4efb\u52a1\uff1b2\uff09\u6307\u5bfc\u4ee3\u7406\u8fdb\u884c\u591a\u6587\u4ef6\u53d8\u66f4\u7684\u7b56\u7565\uff1b3\uff09\u5efa\u7acb\u6709\u6548\u7684\u8fed\u4ee3\u673a\u5236\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf\uff1b4\uff09\u6d4b\u8bd5\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u5e94\u7528\u8fd9\u4e9b\u6700\u4f73\u5b9e\u8df5\uff0c\u5f00\u53d1\u8005\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u4ee3\u7406\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\uff0c\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\uff0c\u5e76\u66f4\u6709\u6548\u5730\u5904\u7406\u590d\u6742\u7684\u591a\u6587\u4ef6\u53d8\u66f4\u4efb\u52a1\u3002", "conclusion": "\u4ee3\u7801\u4ee3\u7406\u662f\u5f3a\u5927\u7684\u751f\u4ea7\u529b\u5de5\u5177\uff0c\u4f46\u9700\u8981\u6b63\u786e\u7684\u4f7f\u7528\u65b9\u6cd5\u548c\u7b56\u7565\u624d\u80fd\u53d1\u6325\u6700\u5927\u4ef7\u503c\u3002\u672c\u6587\u63d0\u4f9b\u7684\u5b9e\u8df5\u6307\u5357\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u4e0e\u4ee3\u7801\u4ee3\u7406\u7684\u534f\u4f5c\uff0c\u4ece\u800c\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "tldr.2601.ebc1ce47", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FgDoRDp%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/-p3DPhWxsyhPwlkZfLuMdUwkDI5nqK25SC1bMDyPR5Y=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FgDoRDp%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/-p3DPhWxsyhPwlkZfLuMdUwkDI5nqK25SC1bMDyPR5Y=439", "authors": ["TLDR Newsletter"], "title": "Agents that don't suck", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FgDoRDp%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/-p3DPhWxsyhPwlkZfLuMdUwkDI5nqK25SC1bMDyPR5Y=439", "summary": "Agents that don't suck (Sponsor) Agent Bricks helps you build, evaluate and optimize AI agents grounded in your unique data. It evaluates automatically, scores outputs against your goals and improves with human feedback \u2014 giving you a clearer path to production. Build agents that work in the real world. See why it's worth your time", "source": "tldr", "AI": {"tldr": "Agent Bricks\u662f\u4e00\u4e2a\u5e2e\u52a9\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u57fa\u4e8e\u72ec\u7279\u6570\u636e\u7684AI\u4ee3\u7406\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u3001\u76ee\u6807\u8bc4\u5206\u548c\u4eba\u7c7b\u53cd\u9988\u6539\u8fdb\uff0c\u52a0\u901f\u4ee3\u7406\u4ea7\u54c1\u5316", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5f00\u53d1\u9762\u4e34\u8bc4\u4f30\u56f0\u96be\u3001\u4f18\u5316\u8fc7\u7a0b\u4e0d\u660e\u786e\u3001\u96be\u4ee5\u4ece\u539f\u578b\u5230\u751f\u4ea7\u7684\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u5de5\u5177\u6765\u6784\u5efa\u771f\u6b63\u5b9e\u7528\u7684\u4ee3\u7406", "method": "\u63d0\u4f9b\u5e73\u53f0\u5de5\u5177\uff0c\u5305\u62ec\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u3001\u57fa\u4e8e\u76ee\u6807\u7684\u8f93\u51fa\u8bc4\u5206\u673a\u5236\u3001\u4eba\u7c7b\u53cd\u9988\u96c6\u6210\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efa\u548c\u4f18\u5316\u57fa\u4e8e\u7279\u5b9a\u6570\u636e\u7684AI\u4ee3\u7406", "result": "\u5e73\u53f0\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u4f18\u5316AI\u4ee3\u7406\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u4ea7\u54c1\u5316\u8def\u5f84\uff0c\u4f7f\u4ee3\u7406\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u66f4\u6709\u6548\u5de5\u4f5c", "conclusion": "Agent Bricks\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u5de5\u5177\uff0c\u89e3\u51b3\u4e86AI\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u503c\u5f97\u5f00\u53d1\u8005\u6295\u5165\u65f6\u95f4\u4f7f\u7528", "topic": "agent analysis"}}
{"id": "tldr.2601.24fb261d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frlancemartin.github.io%2F2026%2F01%2F09%2Fagent_design%2F%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/NQKZE9s461T8HVm2L8Q3bSiDXGvPtZG3jqiYTZfFfw0=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frlancemartin.github.io%2F2026%2F01%2F09%2Fagent_design%2F%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/NQKZE9s461T8HVm2L8Q3bSiDXGvPtZG3jqiYTZfFfw0=439", "authors": ["TLDR Newsletter"], "title": "Agent design patterns", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frlancemartin.github.io%2F2026%2F01%2F09%2Fagent_design%2F%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/NQKZE9s461T8HVm2L8Q3bSiDXGvPtZG3jqiYTZfFfw0=439", "summary": "Agent design patterns (12 minute read) We are getting closer to long-running autonomous agents. However, models get worse as context grows. Effective agent design largely boils down to context management. This post explores common design patterns across agents.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u968f\u7740\u4e0a\u4e0b\u6587\u589e\u957f\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u8bbe\u8ba1\u6a21\u5f0f\u6765\u6784\u5efa\u957f\u671f\u8fd0\u884c\u81ea\u4e3b\u4ee3\u7406\u7684\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u6211\u4eec\u8d8a\u6765\u8d8a\u63a5\u8fd1\u5b9e\u73b0\u957f\u671f\u8fd0\u884c\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u589e\u957f\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u6709\u6548\u7684\u4ee3\u7406\u8bbe\u8ba1\u4e3b\u8981\u5f52\u7ed3\u4e3a\u4e0a\u4e0b\u6587\u7ba1\u7406\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u548c\u5206\u6790\u8de8\u4ee3\u7406\u7684\u5e38\u89c1\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u7814\u7a76\u5982\u4f55\u4f18\u5316\u4e0a\u4e0b\u6587\u7ba1\u7406\u6765\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u8bc6\u522b\u4e86\u4ee3\u7406\u8bbe\u8ba1\u4e2d\u5e38\u89c1\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6a21\u5f0f\uff0c\u4e3a\u6784\u5efa\u66f4\u6709\u6548\u7684\u957f\u671f\u8fd0\u884c\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5bfc\u3002", "conclusion": "\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u662f\u6784\u5efa\u957f\u671f\u8fd0\u884c\u81ea\u4e3b\u4ee3\u7406\u7684\u5173\u952e\uff0c\u901a\u8fc7\u91c7\u7528\u9002\u5f53\u7684\u8bbe\u8ba1\u6a21\u5f0f\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.2672966f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fdemystifying-evals-for-ai-agents%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/BwPFozknIjj3XgO6-kYbip324TSJad-EYxpT_-c0_r8=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fdemystifying-evals-for-ai-agents%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/BwPFozknIjj3XgO6-kYbip324TSJad-EYxpT_-c0_r8=439", "authors": ["TLDR Newsletter"], "title": "Evaluating AI Agents in Production", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 28 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fdemystifying-evals-for-ai-agents%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/BwPFozknIjj3XgO6-kYbip324TSJad-EYxpT_-c0_r8=439", "summary": "Evaluating AI Agents in Production (28 minute read) Anthropic's practical approaches to agent evaluation emphasize pre-deployment tests that simulate real-world conditions and reduce failures in complex, multi-turn agent systems.", "source": "tldr", "AI": {"tldr": "Anthropic\u63d0\u51fa\u5728\u90e8\u7f72\u524d\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u6765\u8bc4\u4f30AI\u4ee3\u7406\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u590d\u6742\u591a\u8f6e\u4ee3\u7406\u7cfb\u7edf\u7684\u6545\u969c", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u9700\u8981\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fdd\u5176\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6545\u969c", "method": "\u91c7\u7528\u90e8\u7f72\u524d\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\uff0c\u9488\u5bf9\u590d\u6742\u591a\u8f6e\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\u8bc4\u4f30\u6846\u67b6", "result": "\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u51cf\u5c11\u4ee3\u7406\u7cfb\u7edf\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6f5c\u5728\u6545\u969c", "conclusion": "\u90e8\u7f72\u524d\u6a21\u62df\u771f\u5b9e\u6761\u4ef6\u7684\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u4e8e\u786e\u4fdd\u590d\u6742AI\u4ee3\u7406\u7cfb\u7edf\u7684\u751f\u4ea7\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981", "topic": "agent analysis"}}
{"id": "tldr.2601.6f06242d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2508.15260%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/QBkEQojuMz-p_25HqFCrdmUBRm95nMOTXLR2R0g5eP8=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2508.15260%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/QBkEQojuMz-p_25HqFCrdmUBRm95nMOTXLR2R0g5eP8=439", "authors": ["TLDR Newsletter"], "title": "Deep Think with Confidence", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 23 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2508.15260%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/QBkEQojuMz-p_25HqFCrdmUBRm95nMOTXLR2R0g5eP8=439", "summary": "Deep Think with Confidence (23 minute read) Meta AI researchers developed DeepThink with Confidence (DeepConf), a technique that uses internal confidence signals to cut LLM reasoning overhead by up to 84.7% while maintaining accuracy. The method monitors token-level confidence to terminate low-quality reasoning traces early, particularly when models generate uncertainty markers like \"wait\" or \"think again.\"", "source": "tldr", "AI": {"tldr": "Meta AI\u63d0\u51faDeepThink with Confidence (DeepConf)\u6280\u672f\uff0c\u5229\u7528LLM\u5185\u90e8\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u901a\u8fc7\u76d1\u63a7token\u7ea7\u7f6e\u4fe1\u5ea6\u63d0\u524d\u7ec8\u6b62\u4f4e\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff0c\u53ef\u5c06\u63a8\u7406\u5f00\u9500\u964d\u4f4e84.7%\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8fc7\u7a0b\u5b58\u5728\u5927\u91cf\u8ba1\u7b97\u5f00\u9500\uff0c\u8bb8\u591a\u63a8\u7406\u8f68\u8ff9\u8d28\u91cf\u4e0d\u9ad8\u4f46\u4f9d\u7136\u6d88\u8017\u8ba1\u7b97\u8d44\u6e90\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u5e76\u63d0\u524d\u7ec8\u6b62\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "method": "DeepConf\u901a\u8fc7\u76d1\u63a7LLM\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684token\u7ea7\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u5f53\u68c0\u6d4b\u5230\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u786e\u5b9a\u6027\u6807\u8bb0\uff08\u5982\"wait\"\u6216\"think again\"\uff09\u65f6\uff0c\u63d0\u524d\u7ec8\u6b62\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u63a8\u7406\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6a21\u578b\u5185\u90e8\u7684\u81ea\u4fe1\u5fc3\u6307\u6807\u6765\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06LLM\u63a8\u7406\u5f00\u9500\u964d\u4f4e\u9ad8\u8fbe84.7%\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u3002\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u90a3\u4e9b\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u786e\u5b9a\u6027\u7684\u63a8\u7406\u573a\u666f\u3002", "conclusion": "DeepConf\u6280\u672f\u901a\u8fc7\u5229\u7528LLM\u5185\u90e8\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u6765\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u63a8\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6210\u672c\u63a7\u5236\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.ac7dbc50", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FMcBgg2/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/r31H52gUCFLKwuILt-1xKAyK-E_iRCQ-jpxbHv57_wM=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FMcBgg2/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/r31H52gUCFLKwuILt-1xKAyK-E_iRCQ-jpxbHv57_wM=439", "authors": ["TLDR Newsletter"], "title": "Agent-Native Architectures and the Shift Beyond Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FMcBgg2/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/r31H52gUCFLKwuILt-1xKAyK-E_iRCQ-jpxbHv57_wM=439", "summary": "Agent-Native Architectures and the Shift Beyond Code (3 minute read) Agent-native architectures replace step-by-step code with agents that decide how to achieve outcomes, while developers define only the desired results through prompts. This model makes software faster to build and adapt, since behavior changes come from modifying language instead of rewriting logic. The shift trades predictability for flexibility, pushing software design toward continuous pruning and observation rather than ...", "source": "tldr", "AI": {"tldr": "Agent-native architectures \u7528\u667a\u80fd\u4f53\u66ff\u4ee3\u9010\u6b65\u4ee3\u7801\uff0c\u5f00\u53d1\u8005\u53ea\u9700\u901a\u8fc7\u63d0\u793a\u5b9a\u4e49\u671f\u671b\u7ed3\u679c\uff0c\u4f7f\u8f6f\u4ef6\u6784\u5efa\u548c\u9002\u5e94\u66f4\u5feb\uff0c\u4f46\u727a\u7272\u4e86\u53ef\u9884\u6d4b\u6027\u4ee5\u6362\u53d6\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u9700\u8981\u7f16\u5199\u8be6\u7ec6\u7684\u9010\u6b65\u4ee3\u7801\uff0c\u4fee\u6539\u903b\u8f91\u590d\u6742\u4e14\u8017\u65f6\u3002Agent-native\u67b6\u6784\u65e8\u5728\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\u81ea\u4e3b\u51b3\u5b9a\u5982\u4f55\u5b9e\u73b0\u76ee\u6807\uff0c\u5f00\u53d1\u8005\u53ea\u9700\u5b9a\u4e49\u671f\u671b\u7ed3\u679c\uff0c\u4ece\u800c\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u548c\u9002\u5e94\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u7f16\u7a0b\u8303\u5f0f\uff0c\u5f00\u53d1\u8005\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9a\u4e49\u671f\u671b\u7ed3\u679c\uff0c\u667a\u80fd\u4f53\u81ea\u4e3b\u51b3\u5b9a\u5b9e\u73b0\u8def\u5f84\u3002\u67b6\u6784\u8f6c\u5411\u6301\u7eed\u4fee\u526a\u548c\u89c2\u5bdf\uff0c\u800c\u975e\u9884\u5148\u8bbe\u8ba1\u5b8c\u6574\u903b\u8f91\u3002", "result": "\u8f6f\u4ef6\u6784\u5efa\u548c\u9002\u5e94\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u884c\u4e3a\u53d8\u66f4\u901a\u8fc7\u4fee\u6539\u8bed\u8a00\u63d0\u793a\u800c\u975e\u91cd\u5199\u903b\u8f91\u5b9e\u73b0\u3002\u4f46\u727a\u7272\u4e86\u4f20\u7edf\u4ee3\u7801\u7684\u53ef\u9884\u6d4b\u6027\uff0c\u9700\u8981\u65b0\u7684\u8f6f\u4ef6\u8bbe\u8ba1\u65b9\u6cd5\u3002", "conclusion": "Agent-native\u67b6\u6784\u4ee3\u8868\u4e86\u8f6f\u4ef6\u5f00\u53d1\u8303\u5f0f\u7684\u6839\u672c\u8f6c\u53d8\uff0c\u4ece\u786e\u5b9a\u6027\u4ee3\u7801\u8f6c\u5411\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u7075\u6d3b\u7cfb\u7edf\uff0c\u63a8\u52a8\u8f6f\u4ef6\u8bbe\u8ba1\u5411\u6301\u7eed\u4f18\u5316\u548c\u89c2\u5bdf\u6f14\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.07b275e0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FkoX9un/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/QFzo6aRnbifIrxxjUIoJSSoplIJ7jHJhumnM5aBxvZo=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FkoX9un/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/QFzo6aRnbifIrxxjUIoJSSoplIJ7jHJhumnM5aBxvZo=439", "authors": ["TLDR Newsletter"], "title": "In AI Agents, Traces Are the Source of Truth", "comment": "Source: TLDR Newsletter, Date: 2026-01-12, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FkoX9un/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/QFzo6aRnbifIrxxjUIoJSSoplIJ7jHJhumnM5aBxvZo=439", "summary": "In AI Agents, Traces Are the Source of Truth (4 minute read) In AI agents, the code only orchestrates models and tools, while real decision-making happens inside the model at runtime and cannot be understood by reading the code alone. Traces capture the actual reasoning steps, tool calls, errors, costs, and outcomes, making them the primary artifact for debugging, testing, optimization, and monitoring.", "source": "tldr", "AI": {"tldr": "AI\u667a\u80fd\u4f53\u4e2d\uff0c\u4ee3\u7801\u4ec5\u7f16\u6392\u6a21\u578b\u548c\u5de5\u5177\uff0c\u771f\u5b9e\u51b3\u7b56\u5728\u8fd0\u884c\u65f6\u6a21\u578b\u5185\u90e8\u53d1\u751f\uff0c\u65e0\u6cd5\u4ec5\u901a\u8fc7\u9605\u8bfb\u4ee3\u7801\u7406\u89e3\u3002\u6267\u884c\u8f68\u8ff9\u8bb0\u5f55\u4e86\u5b9e\u9645\u63a8\u7406\u6b65\u9aa4\u3001\u5de5\u5177\u8c03\u7528\u3001\u9519\u8bef\u3001\u6210\u672c\u548c\u7ed3\u679c\uff0c\u6210\u4e3a\u8c03\u8bd5\u3001\u6d4b\u8bd5\u3001\u4f18\u5316\u548c\u76d1\u63a7\u7684\u4e3b\u8981\u4f9d\u636e\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\uff0c\u4ec5\u5206\u6790\u4ee3\u7801\u65e0\u6cd5\u7406\u89e3\u5b9e\u9645\u51b3\u7b56\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u6838\u5fc3\u63a8\u7406\u53d1\u751f\u5728\u8fd0\u884c\u65f6\u7684\u5927\u6a21\u578b\u5185\u90e8\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6355\u83b7\u548c\u5206\u6790\u5b9e\u9645\u6267\u884c\u8fc7\u7a0b\uff0c\u4ee5\u652f\u6301\u6709\u6548\u7684\u8c03\u8bd5\u3001\u6d4b\u8bd5\u548c\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u5c06\u6267\u884c\u8f68\u8ff9\u4f5c\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u6838\u5fc3\u5206\u6790\u5bf9\u8c61\u3002\u8f68\u8ff9\u8bb0\u5f55\u5305\u62ec\uff1a\u63a8\u7406\u6b65\u9aa4\u3001\u5de5\u5177\u8c03\u7528\u3001\u9519\u8bef\u4fe1\u606f\u3001\u6210\u672c\u6570\u636e\u548c\u6700\u7ec8\u7ed3\u679c\u7b49\u8fd0\u884c\u65f6\u4fe1\u606f\u3002", "result": "\u6267\u884c\u8f68\u8ff9\u6210\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u3001\u8c03\u8bd5\u3001\u6d4b\u8bd5\u3001\u4f18\u5316\u548c\u76d1\u63a7\u7684\u4e3b\u8981\u4f9d\u636e\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4ee3\u7801\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u7406\u89e3\u3002", "conclusion": "\u5728AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\uff0c\u6267\u884c\u8f68\u8ff9\u6bd4\u4ee3\u7801\u66f4\u80fd\u53cd\u6620\u7cfb\u7edf\u771f\u5b9e\u884c\u4e3a\uff0c\u5e94\u4f5c\u4e3a\u5f00\u53d1\u548c\u5206\u6790\u7684\u6838\u5fc3\u5173\u6ce8\u70b9\u3002", "topic": "agent analysis"}}
