{"id": "2601.11647", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11647", "abs": "https://arxiv.org/abs/2601.11647", "authors": ["Aniket Abhishek Soni", "Milan Parikh", "Rashi Nimesh Kumar Dhenia", "Jubin Abhishek Soni", "Ayush Raj Jha", "Sneja Mitinbhai Shah"], "title": "Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines", "comment": "Accepted and presented at CICN 2025 (International Conference on Computational Intelligence and Communication Networks). 7 pages, 5 figures", "summary": "Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software delivery, yet their static workflows often introduce inefficiencies as systems scale. This paper proposes a reinforcement learning (RL) based approach to dynamically optimize CI/CD pipeline workflows. The pipeline is modeled as a Markov Decision Process, and an RL agent is trained to make runtime decisions such as selecting full, partial, or no test execution in order to maximize throughput while minimizing testing overhead.\n  A configurable CI/CD simulation environment is developed to evaluate the approach across build, test, and deploy stages. Experimental results show that the RL optimized pipeline achieves up to a 30 percent improvement in throughput and approximately a 25 percent reduction in test execution time compared to static baselines, while maintaining a defect miss rate below 5 percent. The agent learns to selectively skip or abbreviate tests for low risk commits, accelerating feedback cycles without significantly increasing failure risk.\n  These results demonstrate the potential of reinforcement learning to enable adaptive and intelligent DevOps workflows, providing a practical pathway toward more efficient, resilient, and sustainable CI/CD automation.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316CI/CD\u6d41\u6c34\u7ebf\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u667a\u80fd\u51b3\u7b56\u6d4b\u8bd5\u6267\u884c\u7b56\u7565\uff0c\u63d0\u5347\u541e\u5410\u91cf30%\uff0c\u51cf\u5c11\u6d4b\u8bd5\u65f6\u95f425%\uff0c\u540c\u65f6\u4fdd\u6301\u7f3a\u9677\u9057\u6f0f\u7387\u4f4e\u4e8e5%", "motivation": "\u73b0\u4ee3CI/CD\u6d41\u6c34\u7ebf\u91c7\u7528\u9759\u6001\u5de5\u4f5c\u6d41\uff0c\u968f\u7740\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\u5f15\u5165\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002\u9700\u8981\u52a8\u6001\u4f18\u5316\u65b9\u6cd5\u6765\u5e73\u8861\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u6267\u884c\u6548\u7387", "method": "\u5c06CI/CD\u6d41\u6c34\u7ebf\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u8fd0\u884c\u65f6\u51b3\u7b56\uff08\u5b8c\u6574\u6d4b\u8bd5\u3001\u90e8\u5206\u6d4b\u8bd5\u6216\u4e0d\u6d4b\u8bd5\uff09\uff0c\u5f00\u53d1\u53ef\u914d\u7f6e\u7684CI/CD\u4eff\u771f\u73af\u5883\u8fdb\u884c\u8bc4\u4f30", "result": "\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u6d41\u6c34\u7ebf\u76f8\u6bd4\u9759\u6001\u57fa\u7ebf\uff1a\u541e\u5410\u91cf\u63d0\u534730%\uff0c\u6d4b\u8bd5\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u7ea625%\uff0c\u7f3a\u9677\u9057\u6f0f\u7387\u4fdd\u6301\u57285%\u4ee5\u4e0b\u3002\u4ee3\u7406\u5b66\u4f1a\u4e3a\u4f4e\u98ce\u9669\u63d0\u4ea4\u9009\u62e9\u6027\u8df3\u8fc7\u6216\u7b80\u5316\u6d4b\u8bd5", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u667a\u80fd\u7684DevOps\u5de5\u4f5c\u6d41\uff0c\u4e3a\u66f4\u9ad8\u6548\u3001\u5f39\u6027\u548c\u53ef\u6301\u7eed\u7684CI/CD\u81ea\u52a8\u5316\u63d0\u4f9b\u5b9e\u7528\u8def\u5f84", "topic": "agentic reinforcement learning"}}
{"id": "2601.11655", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11655", "abs": "https://arxiv.org/abs/2601.11655", "authors": ["Caihua Li", "Lianghong Guo", "Yanlin Wang", "Daya Guo", "Wei Tao", "Zhenyu Shan", "Mingwei Liu", "Jiachi Chen", "Haoyu Song", "Duyu Tang", "Hongyu Zhang", "Zibin Zheng"], "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey", "comment": "26 pages, 4 figures, 5 tables", "summary": "Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86AI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u95ee\u9898\u89e3\u51b3\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u6db5\u76d6\u6570\u636e\u6784\u5efa\u3001\u65b9\u6cd5\u5206\u6790\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u5e94\u7528\u5b9e\u8df5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\u5e93\u3002", "motivation": "\u95ee\u9898\u89e3\u51b3\u662f\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u590d\u6742\u4efb\u52a1\uff0cSWE-bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u56f0\u96be\uff0c\u8fd9\u63a8\u52a8\u4e86\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u9700\u8981\u5bf9\u8be5\u9886\u57df\u8fdb\u884c\u7cfb\u7edf\u6027\u68b3\u7406\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff1a1) \u5206\u6790\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff08\u81ea\u52a8\u6536\u96c6\u4e0e\u5408\u6210\u65b9\u6cd5\uff09\uff1b2) \u5168\u9762\u5206\u6790\u65b9\u6cd5\u8bba\uff08\u65e0\u8bad\u7ec3\u6846\u67b6\u7684\u6a21\u5757\u5316\u7ec4\u4ef6\u5230\u57fa\u4e8e\u8bad\u7ec3\u7684\u6280\u672f\u5982\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\uff1b3) \u8ba8\u8bba\u6570\u636e\u8d28\u91cf\u548c\u4ee3\u7406\u884c\u4e3a\u7684\u5173\u952e\u5206\u6790\uff1b4) \u63a2\u8ba8\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u5efa\u7acb\u4e86\u8be5\u9886\u57df\u7684\u7cfb\u7edf\u6027\u77e5\u8bc6\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u521b\u5efa\u4e86\u5f00\u6e90\u8d44\u6e90\u5e93\u4f5c\u4e3a\u52a8\u6001\u53c2\u8003\u8d44\u6e90\u3002", "conclusion": "\u95ee\u9898\u89e3\u51b3\u662fAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u91cd\u8981\u6311\u6218\u9886\u57df\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u65b9\u6cd5\u548c\u8d44\u6e90\u652f\u6301\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u6570\u636e\u8d28\u91cf\u3001\u65b9\u6cd5\u521b\u65b0\u548c\u5b9e\u9645\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "2601.11672", "categories": ["cs.SE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.11672", "abs": "https://arxiv.org/abs/2601.11672", "authors": ["Deepak Babu Piskala"], "title": "From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy Informs the Design of Agentic AI Systems", "comment": null, "summary": "A core abstraction in early Unix systems was the principle that 'everything is a file', enabling heterogeneous devices and kernel resources to be manipulated via uniform read/write interfaces. This paper explores how an analogous unification is emerging in contemporary agentic AI. We trace the evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous software agents, highlighting how file-like abstractions and code-based specifications collapse diverse resources into consistent, composable interfaces. The resulting perspective suggests that adopting file- and code-centric interaction models may enable agentic systems that are more maintainable, auditable, and operationally robust.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eceUnix\u7684\"\u4e00\u5207\u7686\u6587\u4ef6\"\u539f\u5219\u5230\u73b0\u4ee3AI\u667a\u80fd\u4f53\u7684\u7c7b\u6bd4\u7edf\u4e00\u5316\u8d8b\u52bf\uff0c\u8ba4\u4e3a\u91c7\u7528\u6587\u4ef6\u548c\u4ee3\u7801\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u4e92\u6a21\u578b\u80fd\u4f7f\u667a\u80fd\u4f53\u7cfb\u7edf\u66f4\u6613\u7ef4\u62a4\u3001\u53ef\u5ba1\u8ba1\u4e14\u64cd\u4f5c\u7a33\u5065\u3002", "motivation": "\u65e9\u671fUnix\u7cfb\u7edf\u7684\"\u4e00\u5207\u7686\u6587\u4ef6\"\u539f\u5219\u4e3a\u5f02\u6784\u8bbe\u5907\u548c\u5185\u6838\u8d44\u6e90\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8bfb\u5199\u63a5\u53e3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7c7b\u4f3c\u7684\u7edf\u4e00\u5316\u62bd\u8c61\u5982\u4f55\u5728\u5f53\u4ee3AI\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u51fa\u73b0\uff0c\u5e76\u5206\u6790\u8fd9\u79cd\u7c7b\u6bd4\u5982\u4f55\u4fc3\u8fdb\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u8ffd\u6eaf\u4eceUnix\u5230DevOps\u3001\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\uff0c\u518d\u5230\u81ea\u4e3b\u8f6f\u4ef6\u667a\u80fd\u4f53\u7684\u6f14\u5316\u5386\u7a0b\uff0c\u5206\u6790\u6587\u4ef6\u7c7b\u62bd\u8c61\u548c\u57fa\u4e8e\u4ee3\u7801\u7684\u89c4\u8303\u5982\u4f55\u5c06\u591a\u6837\u5316\u8d44\u6e90\u6574\u5408\u4e3a\u4e00\u81f4\u3001\u53ef\u7ec4\u5408\u7684\u63a5\u53e3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7c7b\u4f3c\u4e8eUnix\u7684\u6587\u4ef6\u62bd\u8c61\u539f\u5219\u6b63\u5728AI\u667a\u80fd\u4f53\u9886\u57df\u5f62\u6210\uff0c\u6587\u4ef6\u548c\u4ee3\u7801\u4e2d\u5fc3\u7684\u4ea4\u4e92\u6a21\u578b\u80fd\u591f\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u7edf\u4e00\u7684\u63a5\u53e3\u62bd\u8c61\u3002", "conclusion": "\u91c7\u7528\u6587\u4ef6\u548c\u4ee3\u7801\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u4e92\u6a21\u578b\u53ef\u80fd\u4f7f\u667a\u80fd\u4f53\u7cfb\u7edf\u66f4\u5177\u53ef\u7ef4\u62a4\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u64cd\u4f5c\u7a33\u5065\u6027\uff0c\u5ef6\u7eed\u4e86Unix\u7edf\u4e00\u62bd\u8c61\u7684\u601d\u60f3\u4f20\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2601.11556", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.11556", "abs": "https://arxiv.org/abs/2601.11556", "authors": ["Boyang Wang", "Yash Vishe", "Xin Xu", "Zachary Novack", "Julian McAuley", "Junda Wu"], "title": "CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration", "comment": null, "summary": "Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.", "AI": {"tldr": "CSyMR-Bench\uff1a\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u7b26\u53f7\u97f3\u4e50\u63a8\u7406\u4e2d\u7ec4\u5408\u5206\u6790\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b126\u4e2a\u6765\u81ea\u4e13\u5bb6\u8bba\u575b\u548c\u4e13\u4e1a\u8003\u8bd5\u7684\u591a\u9009\u9898\uff0c\u9700\u8981\u7ed3\u5408\u591a\u4e2a\u539f\u5b50\u5206\u6790\u6765\u5f97\u51fa\u6700\u7ec8\u7b54\u6848\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u7684\u77e5\u8bc6\u6216\u539f\u5b50\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u8fde\u63a5\u97f3\u4e50\u7ed3\u6784\u6240\u9700\u7684\u7ec4\u5408\u6027\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u8fd9\u5728\u7b26\u53f7\u97f3\u4e50\u7406\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u6784\u5efaCSyMR-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff08126\u4e2a\u591a\u9009\u9898\uff09\uff1b2. \u5f00\u53d1\u57fa\u4e8emusic21\u5e93\u7b26\u53f7\u97f3\u4e50\u5206\u6790\u5de5\u5177\u7684\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u6846\u67b6\uff1b3. \u5728\u793e\u533a\u6765\u6e90\u548c\u8003\u8bd5\u98ce\u683c\u95ee\u9898\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "CSyMR-Bench\u5bf9\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u90fd\u6784\u6210\u975e\u5e73\u51e1\u6311\u6218\uff0c\u800c\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u6846\u67b6\u5728\u6240\u6709\u57fa\u7ebf\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5b9e\u73b0\u4e865-7%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u7ec4\u5408\u6027\u7b26\u53f7\u97f3\u4e50\u63a8\u7406\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u548c\u5de5\u5177\u589e\u5f3a\u65b9\u6cd5\u6765\u6709\u6548\u89e3\u51b3\u3002CSyMR-Bench\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u6846\u67b6\u5c55\u793a\u4e86\u5229\u7528\u9886\u57df\u7279\u5b9a\u5de5\u5177\u63d0\u5347LLM\u97f3\u4e50\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.11687", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11687", "abs": "https://arxiv.org/abs/2601.11687", "authors": ["Harmohit Singh"], "title": "Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems", "comment": null, "summary": "We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u751f\u4ea7\u4f18\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684Python\u4ee3\u7801\u8fdb\u884c\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\uff0c\u901a\u8fc7\u8bed\u4e49\u7f13\u5b58\u3001\u53cc\u9608\u503c\u51b3\u7b56\u548c\u610f\u56fe\u9a71\u52a8\u7684\u52a8\u6001\u63d0\u793a\u7ec4\u88c5\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u6602\u8d35\u7684\u524d\u6cbf\u6a21\u578b\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u53c8\u80fd\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f01\u4e1a\u7ea7\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\u3002", "method": "1) \u57fa\u4e8eLLM\u7684\u7b49\u4ef7\u68c0\u6d4b\u548c\u7ed3\u6784\u5316\u9002\u914d\u63d0\u793a\u7684\u8bed\u4e49\u7f13\u5b58\u7cfb\u7edf\uff1b2) \u5206\u79bb\u7cbe\u786e\u5339\u914d\u68c0\u7d22\u548c\u53c2\u8003\u5f15\u5bfc\u751f\u6210\u7684\u53cc\u9608\u503c\u51b3\u7b56\u673a\u5236\uff1b3) \u901a\u8fc7\u8868\u611f\u77e5\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u51cf\u5c1140-60%token\u6d88\u8017\u7684\u610f\u56fe\u9a71\u52a8\u52a8\u6001\u63d0\u793a\u7ec4\u88c5\u7cfb\u7edf\u3002", "result": "\u751f\u4ea7\u67e5\u8be2\u7f13\u5b58\u547d\u4e2d\u7387\u8fbe67%\uff0c\u5904\u7406\u8d85\u8fc710,000\u4e2a\u67e5\u8be2\uff0c\u5e73\u5747\u5ef6\u8fdf8.2\u79d2\uff0c\u8bed\u4e49\u51c6\u786e\u738794.3%\uff0c\u5df2\u90e8\u7f72\u4e8e\u4f01\u4e1a\u5e93\u5b58\u7ba1\u7406\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u5206\u6790\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u7387\u7684\u5e73\u8861\u3002", "topic": "code agent"}}
{"id": "2601.11572", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11572", "abs": "https://arxiv.org/abs/2601.11572", "authors": ["Timo Aukusti Laine"], "title": "Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces", "comment": "23 pages, 5 figures", "summary": "We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.", "AI": {"tldr": "\u4f7f\u7528\u7ebf\u6027\u4ee3\u6570\u548c\u54c8\u5bc6\u987f\u5f62\u5f0f\u7b49\u6570\u5b66\u5de5\u5177\u5206\u6790LLM\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u53d1\u73b0L2\u5f52\u4e00\u5316\u7ea6\u675f\u4f7f\u5d4c\u5165\u7a7a\u95f4\u9002\u5408\u54c8\u5bc6\u987f\u5206\u6790\uff0c\u63a8\u5bfc\u4e86\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5411\u91cf\u6270\u52a8\u7684\u5173\u7cfb\uff0c\u5e76\u63a2\u7d22\u91cf\u5b50\u529b\u5b66\u7c7b\u6bd4", "motivation": "\u89c2\u5bdf\u5230LLM\u5d4c\u5165\u8868\u73b0\u51fa\u79bb\u6563\u8bed\u4e49\u72b6\u6001\uff0c\u8868\u660e\u5b58\u5728\u79bb\u6563\u8bed\u4e49\u8868\u793a\uff0c\u5e0c\u671b\u901a\u8fc7\u6570\u5b66\u5de5\u5177\u5206\u6790\u8bed\u4e49\u5173\u7cfb", "method": "\u5e94\u7528\u7ebf\u6027\u4ee3\u6570\u548c\u54c8\u5bc6\u987f\u5f62\u5f0f\u5206\u6790LLM\u5d4c\u5165\u7a7a\u95f4\uff0c\u63a8\u5bfcL2\u5f52\u4e00\u5316\u7ea6\u675f\u4e0b\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u63a2\u7d22\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5411\u91cf\u6270\u52a8\u7684\u5173\u7cfb\uff0c\u7814\u7a76\u76f4\u63a5\u548c\u95f4\u63a5\u8bed\u4e49\u8f6c\u6362\uff0c\u5e76\u5f15\u5165\u91cf\u5b50\u529b\u5b66\u7c7b\u6bd4", "result": "L2\u5f52\u4e00\u5316\u7ea6\u675f\u5bfc\u81f4\u7ed3\u6784\u5316\u5d4c\u5165\u7a7a\u95f4\u9002\u5408\u54c8\u5bc6\u987f\u5206\u6790\uff0c\u63a8\u5bfc\u4e86\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5411\u91cf\u6270\u52a8\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u63a2\u7d22\u4e86\u91cf\u5b50\u529b\u5b66\u7c7b\u6bd4\u5982\u96f6\u70b9\u80fd\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e0eKoopman-von Neumann\u529b\u5b66\u7684\u6f5c\u5728\u8054\u7cfb", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5165\u7406\u89e3LLM\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u53ef\u80fd\u6709\u52a9\u4e8e\u5f00\u53d1\u51cf\u8f7b\u5e7b\u89c9\u7684\u65b0\u65b9\u6cd5\uff0c\u4f46\u89e3\u91ca\u9700\u8981\u8c28\u614e\u8003\u8651", "topic": "agent analysis"}}
{"id": "2601.11574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11574", "abs": "https://arxiv.org/abs/2601.11574", "authors": ["Lukas Abrie Nel"], "title": "GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.", "AI": {"tldr": "GRADE\u4f7f\u7528Gumbel-softmax\u91cd\u53c2\u6570\u5316\u66ff\u4ee3\u9ad8\u65b9\u5dee\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u677e\u5f1b\u5b9e\u73b0\u4ece\u5956\u52b1\u4fe1\u53f7\u5230\u6a21\u578b\u53c2\u6570\u7684\u7aef\u5230\u7aef\u68af\u5ea6\u4f20\u64ad\uff0c\u5728\u6587\u672c\u5bf9\u9f50\u4efb\u52a1\u4e0a\u6bd4PPO\u63d0\u534750%\u6027\u80fd\u4e14\u68af\u5ea6\u65b9\u5dee\u964d\u4f4e14\u500d\u3002", "motivation": "RLHF\u5df2\u6210\u4e3a\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46PPO\u7b49\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b58\u5728\u68af\u5ea6\u65b9\u5dee\u9ad8\u3001\u9700\u8981\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faGRADE\u65b9\u6cd5\uff0c\u4f7f\u7528Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u914d\u5408\u76f4\u901a\u4f30\u8ba1\uff08GRADE-STE\uff09\uff0c\u5bf9\u79bb\u6563token\u91c7\u6837\u8fc7\u7a0b\u8fdb\u884c\u53ef\u5fae\u677e\u5f1b\uff0c\u5b9e\u73b0\u4ece\u5956\u52b1\u4fe1\u53f7\u901a\u8fc7\u751f\u6210token\u5230\u6a21\u578b\u53c2\u6570\u7684\u7aef\u5230\u7aef\u68af\u5ea6\u4f20\u64ad\u3002", "result": "\u5728IMDB\u6570\u636e\u96c6\u7684\u60c5\u611f\u63a7\u5236\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cGRADE-STE\u83b7\u5f970.763\u7684\u6d4b\u8bd5\u5956\u52b1\uff0c\u76f8\u6bd4PPO\u76840.510\u548cREINFORCE\u76840.617\u63d0\u534750%\uff1b\u68af\u5ea6\u65b9\u5dee\u6bd4REINFORCE\u4f4e14\u500d\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u52a8\u6001\u66f4\u7a33\u5b9a\u3002", "conclusion": "GRADE\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6bd4\u5f3a\u5316\u5b66\u4e60\u66f4\u7b80\u5355\u3001\u7a33\u5b9a\u3001\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5176\u6539\u8fdb\u5728\u9a8c\u8bc1\u96c6/\u6d4b\u8bd5\u96c6\u4e0a\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.11783", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11783", "abs": "https://arxiv.org/abs/2601.11783", "authors": ["Murtuza N. Shergadwala"], "title": "The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing", "comment": null, "summary": "The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($>99\\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\\approx19\\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\\%$--$83\\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($>90\\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u6cd5\u5b98\u8bc4\u4f30\u5b58\u5728\"\u7a33\u5b9a\u6027\u9677\u9631\"\uff1a\u867d\u7136\u5224\u51b3\u7a33\u5b9a\u6027\u9ad8\uff08>99%\uff09\uff0c\u4f46\u63a8\u7406\u7a33\u5b9a\u6027\u5dee\u5f02\u663e\u8457\uff0c\u5ba2\u89c2\u6307\u4ee4\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u53ef\u4f4e\u81f319%\uff0c\u4e3b\u89c2\u6307\u4ee4\u572835%-83%\u4e4b\u95f4\uff0c\u8868\u660e\u9ad8\u5224\u51b3\u7a33\u5b9a\u6027\u53ef\u80fd\u63a9\u76d6\u8106\u5f31\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u5728\u53d7\u76d1\u7ba1\u884c\u4e1a\uff08\u5982\u4eba\u529b\u8d44\u6e90\uff09\u4e2d\uff0c\u4f01\u4e1a\u9700\u8981\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u751f\u6210\u5f0fAI\u5ba1\u8ba1\u673a\u5236\u3002\u867d\u7136LLM\u4f5c\u4e3a\u6cd5\u5b98\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u5176\u5728\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7cfb\u7edf\u6307\u4ee4\u9075\u5b88\u60c5\u51b5\u65f6\u7684\u53ef\u9760\u6027\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u5e94\u7528\u6d4b\u8bd5\u6307\u4ee4\u7c7b\u578b\u5982\u4f55\u5f71\u54cd\u6cd5\u5b98\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u8303\u56f4\u6307\u4ee4\u5206\u89e3\u6846\u67b6\uff0c\u5c06\u5e94\u7528\u6d4b\u8bd5\u6307\u4ee4\u5206\u7c7b\u4e3a\u5ba2\u89c2\u548c\u4e3b\u89c2\u7c7b\u578b\uff0c\u4ee5\u9694\u79bb\u5bfc\u81f4\u6cd5\u5b98\u4e0d\u7a33\u5b9a\u7684\u56e0\u7d20\u3002\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u4e24\u4e2a\u4ee3\u8868\u6027\u7684\u4eba\u529b\u8d44\u6e90\u751f\u6210\u5f0fAI\u5e94\u7528\uff0c\u8bc4\u4f30\u56db\u79cd\u6cd5\u5b98\u67b6\u6784\u5728\u591a\u6b21\u8fd0\u884c\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u63ed\u793a\u4e86\"\u7a33\u5b9a\u6027\u9677\u9631\"\u73b0\u8c61\uff1a\u5224\u51b3\u7a33\u5b9a\u6027\u4e0e\u63a8\u7406\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u5206\u6b67\u3002\u867d\u7136\u6cd5\u5b98\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u90fd\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u5224\u51b3\u4e00\u81f4\u6027\uff08>99%\uff09\uff0c\u4f46\u4ed6\u4eec\u7684\u63a8\u7406\u8f68\u8ff9\u5dee\u5f02\u663e\u8457\u3002\u5ba2\u89c2\u6307\u4ee4\uff08\u5982\u5b57\u6570\u7edf\u8ba1\uff09\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u4f4e\u81f3\u7ea619%\uff0c\u4e3b\u89c2\u6307\u4ee4\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u572835%-83%\u4e4b\u95f4\u53d8\u5316\uff0c\u800c\u4e13\u6ce8\u4e8e\u79bb\u6563\u5b9e\u4f53\u63d0\u53d6\u7684\u5ba2\u89c2\u6307\u4ee4\u5b9e\u73b0\u4e86\u9ad8\u63a8\u7406\u7a33\u5b9a\u6027\uff08>90%\uff09\u3002", "conclusion": "\u9ad8\u5224\u51b3\u7a33\u5b9a\u6027\u53ef\u80fd\u63a9\u76d6\u8106\u5f31\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u5efa\u8bae\u5ba1\u8ba1\u5458\u4e25\u683c\u9650\u5b9a\u81ea\u52a8\u5316\u8bc4\u4f30\u534f\u8bae\u7684\u8303\u56f4\uff1a\u5c06\u6240\u6709\u53ef\u786e\u5b9a\u6027\u9a8c\u8bc1\u7684\u903b\u8f91\u59d4\u6258\u7ed9\u4ee3\u7801\uff0c\u540c\u65f6\u4fdd\u7559LLM\u6cd5\u5b98\u7528\u4e8e\u590d\u6742\u7684\u8bed\u4e49\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2601.11578", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11578", "abs": "https://arxiv.org/abs/2601.11578", "authors": ["Ibrahim Al Azher", "Zhishuai Guo", "Hamed Alhoori"], "title": "LimAgents: Multi-Agent LLMs for Generating Research Limitations", "comment": "18 Pages, 9 figures", "summary": "Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.", "AI": {"tldr": "LimAgents\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210OpenReview\u8bc4\u8bba\u3001\u4f5c\u8005\u58f0\u660e\u9650\u5236\u548c\u5f15\u7528\u6587\u732e\uff0c\u7cfb\u7edf\u6027\u5730\u751f\u6210\u5b9e\u8d28\u6027\u7814\u7a76\u5c40\u9650\u6027\u5206\u6790\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u5f53\u524dLLM\u5728\u8bc6\u522b\u7814\u7a76\u5c40\u9650\u6027\u65f6\u5b58\u5728\u8868\u9762\u5316\u3001\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u5e38\u53ea\u662f\u91cd\u590d\u4f5c\u8005\u58f0\u660e\u7684\u5c40\u9650\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u65b9\u6cd5\u8bba\u95ee\u9898\u548c\u4e0a\u4e0b\u6587\u5dee\u8ddd\u3002\u540c\u65f6\uff0c\u8bb8\u591a\u4f5c\u8005\u53ea\u62ab\u9732\u90e8\u5206\u6216\u7410\u788e\u7684\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u5c40\u9650\u6027\u5206\u6790\u4e0d\u591f\u5168\u9762\u3002", "method": "\u63d0\u51faLimAgents\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u591a\u4e2a\u4e13\u95e8\u89d2\u8272\u7684\u667a\u80fd\u4f53\uff1a\u63d0\u53d6\u663e\u5f0f\u5c40\u9650\u6027\u3001\u5206\u6790\u65b9\u6cd5\u8bba\u5dee\u8ddd\u3001\u6a21\u62df\u540c\u884c\u8bc4\u5ba1\u89c6\u89d2\u3001\u5206\u6790\u5f15\u7528\u6587\u732e\u4e0a\u4e0b\u6587\u3002\u901a\u8fc7Judge\u667a\u80fd\u4f53\u7cbe\u70bc\u8f93\u51fa\uff0cMaster\u667a\u80fd\u4f53\u6574\u5408\u6210\u6e05\u6670\u96c6\u5408\u3002\u540c\u65f6\u5f15\u5165\u57fa\u4e8eLLM-as-a-Judge\u7684\u70b9\u8bc4\u4f30\u534f\u8bae\u6765\u66f4\u51c6\u786e\u8861\u91cf\u8986\u76d6\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLimAgents\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1aRAG+\u591a\u667a\u80fd\u4f53GPT-4o mini\u914d\u7f6e\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u83b7\u5f97+15.51%\u7684\u8986\u76d6\u589e\u76ca\uff0cLlama 3 8B\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u83b7\u5f97+4.41%\u7684\u6539\u8fdb\u3002", "conclusion": "LimAgents\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u663e\u5f0f\u3001\u9690\u5f0f\u3001\u540c\u884c\u8bc4\u5ba1\u5173\u6ce8\u548c\u6587\u732e\u80cc\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u66f4\u5168\u9762\u3001\u5b9e\u8d28\u6027\u7684\u5c40\u9650\u6027\u5206\u6790\uff0c\u8d85\u8d8a\u4f20\u7edf\u8868\u9762\u5316\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.11781", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11781", "abs": "https://arxiv.org/abs/2601.11781", "authors": ["Dawood Wasif", "Terrence J. Moore", "Seunghyun Yoon", "Hyuk Lim", "Dan Dongseong Kim", "Frederica F. Nelson", "Jin-Hee Cho"], "title": "Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles", "comment": "Submitted to ICRA 2026 (under review)", "summary": "Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.", "AI": {"tldr": "RAIL\u662f\u4e00\u4e2a\u98ce\u9669\u611f\u77e5\u7684\u4eba\u673a\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u79cd\u8fd0\u884c\u65f6\u4fe1\u53f7\u751f\u6210\u5165\u4fb5\u98ce\u9669\u8bc4\u5206\uff0c\u6839\u636e\u98ce\u9669\u7b49\u7ea7\u52a8\u6001\u8c03\u6574\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5728\u9ad8\u98ce\u9669\u65f6\u7ed3\u5408\u7279\u5b9a\u9632\u62a4\u673a\u5236\uff0c\u540c\u65f6\u652f\u6301\u4eba\u7c7b\u5e72\u9884\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u9762\u5bf9\u7f55\u89c1\u7684\u957f\u5c3e\u573a\u666f\u6216\u7f51\u7edc\u7269\u7406\u5165\u4fb5\u65f6\uff0c\u5fc5\u987b\u4fdd\u6301\u5b89\u5168\u548c\u6709\u6548\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u98ce\u9669\u573a\u666f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u611f\u77e5\u98ce\u9669\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u7684\u6846\u67b6\u3002", "method": "RAIL\u878d\u5408\u4e09\u79cd\u4fe1\u53f7\uff08\u66f2\u7387\u6267\u884c\u5b8c\u6574\u6027\u3001\u78b0\u649e\u65f6\u95f4\u63a5\u8fd1\u5ea6\u548c\u89c2\u6d4b\u504f\u79fb\u4e00\u81f4\u6027\uff09\u901a\u8fc7\u52a0\u6743Noisy-OR\u751f\u6210\u5165\u4fb5\u98ce\u9669\u8bc4\u5206(IRS)\u3002\u6839\u636eIRS\u9608\u503c\uff0c\u7cfb\u7edf\u5728\u4f4e\u98ce\u9669\u65f6\u6267\u884c\u540d\u4e49\u7b56\u7565\uff0c\u9ad8\u98ce\u9669\u65f6\u7ed3\u5408\u7279\u5b9a\u9632\u62a4\u673a\u5236\u3002\u4f7f\u7528\u4e0a\u4e0b\u6587bandit\u6839\u636e\u4fe1\u53f7\u5411\u91cf\u9009\u62e9\u9632\u62a4\u673a\u5236\uff0c\u5e76\u91c7\u7528Soft Actor-Critic\u7ed3\u5408\u98ce\u9669\u4f18\u5148\u56de\u653e\u548c\u53cc\u91cd\u5956\u52b1\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u5728MetaDrive\u4e0a\uff0cRAIL\u83b7\u5f97\u6d4b\u8bd5\u56de\u62a5360.65\uff0c\u6d4b\u8bd5\u6210\u529f\u73870.85\uff0c\u6d4b\u8bd5\u5b89\u5168\u8fdd\u89c40.75\uff0c\u6270\u52a8\u73870.0027\uff0c\u8bad\u7ec3\u5b89\u5168\u8fdd\u89c4\u4ec529.07\u3002\u5728CAN\u6ce8\u5165\u548cLiDAR\u6b3a\u9a97\u653b\u51fb\u4e0b\uff0c\u6210\u529f\u7387\u63d0\u5347\u81f30.68\u548c0.80\uff0c\u653b\u51fb\u4e0b\u8131\u79bb\u7387\u964d\u81f30.37\u548c0.03\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u81f30.34\u548c0.11\u3002\u5728CARLA\u4e0a\uff0c\u4ec5\u75288000\u6b65\u83b7\u5f97\u6d4b\u8bd5\u56de\u62a51609.70\u548c\u6d4b\u8bd5\u6210\u529f\u73870.41\u3002", "conclusion": "RAIL\u6846\u67b6\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u7684\u4eba\u673a\u534f\u540c\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u98ce\u9669\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709RL\u3001\u5b89\u5168RL\u3001\u79bb\u7ebf/\u6a21\u4eff\u5b66\u4e60\u548c\u5148\u524d\u7684\u4eba\u673a\u534f\u540c\u57fa\u7ebf\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.11868", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11868", "abs": "https://arxiv.org/abs/2601.11868", "authors": ["Mike A. Merrill", "Alexander G. Shaw", "Nicholas Carlini", "Boxuan Li", "Harsh Raj", "Ivan Bercovich", "Lin Shi", "Jeong Yeon Shin", "Thomas Walshe", "E. Kelly Buchanan", "Junhong Shen", "Guanghao Ye", "Haowei Lin", "Jason Poulos", "Maoyu Wang", "Marianna Nezhurina", "Jenia Jitsev", "Di Lu", "Orfeas Menis Mastromichalakis", "Zhiwei Xu", "Zizhao Chen", "Yue Liu", "Robert Zhang", "Leon Liangyu Chen", "Anurag Kashyap", "Jan-Lucas Uslu", "Jeffrey Li", "Jianbo Wu", "Minghao Yan", "Song Bian", "Vedang Sharma", "Ke Sun", "Steven Dillmann", "Akshay Anand", "Andrew Lanpouthakoun", "Bardia Koopah", "Changran Hu", "Etash Guha", "Gabriel H. S. Dreiman", "Jiacheng Zhu", "Karl Krauth", "Li Zhong", "Niklas Muennighoff", "Robert Amanfu", "Shangyin Tan", "Shreyas Pimpalgaonkar", "Tushar Aggarwal", "Xiangning Lin", "Xin Lan", "Xuandong Zhao", "Yiqing Liang", "Yuanli Wang", "Zilong Wang", "Changzhi Zhou", "David Heineman", "Hange Liu", "Harsh Trivedi", "John Yang", "Junhong Lin", "Manish Shetty", "Michael Yang", "Nabil Omi", "Negin Raoof", "Shanda Li", "Terry Yue Zhuo", "Wuwei Lin", "Yiwei Dai", "Yuxin Wang", "Wenhao Chai", "Shang Zhou", "Dariush Wahdany", "Ziyu She", "Jiaming Hu", "Zhikang Dong", "Yuxuan Zhu", "Sasha Cui", "Ahson Saiyed", "Arinbj\u00f6rn Kolbeinsson", "Jesse Hu", "Christopher Michael Rytting", "Ryan Marten", "Yixin Wang", "Alex Dimakis", "Andy Konwinski", "Ludwig Schmidt"], "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces", "comment": null, "summary": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .", "AI": {"tldr": "Terminal-Bench 2.0\u662f\u4e00\u4e2a\u5305\u542b89\u4e2a\u7ec8\u7aef\u73af\u5883\u4efb\u52a1\u7684\u786c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u771f\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u8981\u4e48\u4e0d\u8861\u91cf\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff0c\u8981\u4e48\u96be\u5ea6\u4e0d\u8db3\u4ee5\u6709\u6548\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u3002\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u8db3\u591f\u56f0\u96be\u7684\u57fa\u51c6\u6765\u6d4b\u91cfAI\u4ee3\u7406\u5728\u81ea\u4e3b\u5b8c\u6210\u6709\u4ef7\u503c\u957f\u65f6\u57df\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86Terminal-Bench 2.0\u57fa\u51c6\uff0c\u5305\u542b89\u4e2a\u8ba1\u7b97\u673a\u7ec8\u7aef\u73af\u5883\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u72ec\u7279\u73af\u5883\u3001\u4eba\u5de5\u7f16\u5199\u7684\u89e3\u51b3\u65b9\u6848\u548c\u5168\u9762\u7684\u9a8c\u8bc1\u6d4b\u8bd5\u3002\u4efb\u52a1\u8bbe\u8ba1\u7075\u611f\u6765\u6e90\u4e8e\u771f\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u95ee\u9898\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u548c\u4ee3\u7406\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5206\u4f4e\u4e8e65%\uff0c\u8868\u660e\u5f53\u524dAI\u4ee3\u7406\u5728\u590d\u6742\u7ec8\u7aef\u4efb\u52a1\u4e0a\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002\u901a\u8fc7\u9519\u8bef\u5206\u6790\u8bc6\u522b\u4e86\u6a21\u578b\u548c\u4ee3\u7406\u9700\u8981\u6539\u8fdb\u7684\u9886\u57df\u3002", "conclusion": "Terminal-Bench 2.0\u662f\u4e00\u4e2a\u6709\u6548\u7684\u786c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u591f\u8861\u91cfAI\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u53d1\u5e03\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u548c\u5f00\u53d1\u3002", "topic": "swe benchmark"}}
{"id": "2601.11816", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11816", "abs": "https://arxiv.org/abs/2601.11816", "authors": ["Zahra Moslemi", "Keerthi Koneru", "Yen-Ting Lee", "Sheethal Kumar", "Ramesh Radhakrishnan"], "title": "POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation", "comment": "Workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks: AAAI 2026", "summary": "Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation", "AI": {"tldr": "POLARIS\u662f\u4e00\u4e2a\u9762\u5411\u4f01\u4e1a\u540e\u53f0\u5de5\u4f5c\u6d41\u7684\u6cbb\u7406\u578bLLM\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u578b\u5316\u8ba1\u5212\u5408\u6210\u548c\u9a8c\u8bc1\u6267\u884c\u5b9e\u73b0\u53ef\u5ba1\u8ba1\u3001\u7b56\u7565\u5bf9\u9f50\u7684\u64cd\u4f5c\u81ea\u52a8\u5316\u3002", "motivation": "\u4f01\u4e1a\u540e\u53f0\u5de5\u4f5c\u6d41\u9700\u8981\u53ef\u5ba1\u8ba1\u3001\u7b56\u7565\u5bf9\u9f50\u4e14\u64cd\u4f5c\u53ef\u9884\u6d4b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u800c\u901a\u7528\u7684\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u5f80\u5f80\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u3002", "method": "POLARIS\u91c7\u7528\u6cbb\u7406\u578b\u7f16\u6392\u6846\u67b6\uff0c\u5c06\u81ea\u52a8\u5316\u89c6\u4e3a\u57fa\u4e8eLLM\u667a\u80fd\u4f53\u7684\u7c7b\u578b\u5316\u8ba1\u5212\u5408\u6210\u548c\u9a8c\u8bc1\u6267\u884c\u3002\u5305\u62ec\uff1a\u89c4\u5212\u5668\u751f\u6210\u7c7b\u578b\u68c0\u67e5\u7684\u6709\u5411\u65e0\u73af\u56fe\uff0c\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406\u6a21\u5757\u9009\u62e9\u5408\u89c4\u8ba1\u5212\uff0c\u6267\u884c\u8fc7\u7a0b\u901a\u8fc7\u9a8c\u8bc1\u5668\u95e8\u63a7\u68c0\u67e5\u3001\u6709\u9650\u4fee\u590d\u5faa\u73af\u548c\u7f16\u8bd1\u7684\u7b56\u7565\u62a4\u680f\u6765\u4fdd\u62a4\u3002", "result": "\u5728\u6587\u6863\u4e2d\u5fc3\u5316\u91d1\u878d\u4efb\u52a1\u4e2d\uff0cPOLARIS\u751f\u6210\u51b3\u7b56\u7ea7\u5de5\u4ef6\u548c\u5b8c\u6574\u6267\u884c\u8ddf\u8e2a\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002\u5728SROIE\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.81\u7684micro F1\u5206\u6570\uff0c\u5728\u53d7\u63a7\u5408\u6210\u5957\u4ef6\u4e0a\u5b9e\u73b00.95-1.00\u7684\u5f02\u5e38\u8def\u7531\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5ba1\u8ba1\u8ddf\u8e2a\u3002", "conclusion": "POLARIS\u4e3a\u7b56\u7565\u5bf9\u9f50\u7684\u667a\u80fd\u4f53AI\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u548c\u57fa\u51c6\u53c2\u8003\uff0c\u6784\u6210\u4e86\u6cbb\u7406\u578b\u667a\u80fd\u4f53AI\u7684\u521d\u6b65\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2601.11585", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11585", "abs": "https://arxiv.org/abs/2601.11585", "authors": ["Hyunjun Kim"], "title": "Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents", "comment": null, "summary": "Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u71b5\u4e0a\u4e0b\u6587\u5851\u9020\uff08ECS\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u7b54\u6848\u5206\u5e03\u5411\u6b63\u786e\u7b54\u6848\u7684\u504f\u79fb\u6765\u6d4b\u91cf\u4e0a\u4e0b\u6587\u6548\u7528\uff0c\u5728\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u9009\u62e9\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8bcd\u6c47\u76f8\u4f3c\u6027\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u9700\u8981\u533a\u5206\u5b9e\u7528\u4fe1\u606f\u548c\u8bef\u5bfc\u6027\u5e72\u6270\u3002\u4f20\u7edf\u57fa\u4e8e\u8bcd\u6c47\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u8bed\u7528\u6548\u7528\u2014\u2014\u5373\u6bb5\u843d\u662f\u5426\u771f\u6b63\u6709\u52a9\u4e8e\u56de\u7b54\u95ee\u9898\u3002", "method": "\u5f15\u5165\u71b5\u4e0a\u4e0b\u6587\u5851\u9020\uff08ECS\uff09\u6846\u67b6\uff0c\u5c06\u6548\u7528\u5f62\u5f0f\u5316\u4e3a\u7b54\u6848\u6982\u7387\u7684\u6709\u7b26\u53f7\u53d8\u5316\uff0c\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u7b54\u6848\u5206\u5e03\u5411\u6b63\u786e\u7b54\u6848\u7684\u504f\u79fb\u6765\u91cf\u5316\u4e0a\u4e0b\u6587\u6548\u7528\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u4efb\u52a1\u65e0\u5173\u7684\u66f4\u65b0\u4ea7\u751f\u63a5\u8fd1\u96f6\u7684\u5206\u5e03\u504f\u79fb\u3002", "result": "\u5728LongMemEval\uff08\u4f1a\u8bdd\u7ea7\uff09\u548cLoCoMo\uff08\u8f6e\u6b21\u7ea7\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cECS\u5728\u7ec6\u7c92\u5ea6\u8f6e\u6b21\u9009\u62e9\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528Llama-3.1-8B\u8fbe\u5230F1=0.265\uff0c\u76f8\u6bd4TF-IDF\uff08F1=0.154\uff09\u670971.83%\u7684\u76f8\u5bf9\u63d0\u5347\u3002", "conclusion": "ECS\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u6349\u8bed\u7528\u6548\u7528\uff0c\u5728\u7cbe\u786e\u4e0a\u4e0b\u6587\u9009\u62e9\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u8bcd\u6c47\u76f8\u4f3c\u6027\u65b9\u6cd5\uff0c\u4e3aLLM\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.12146", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12146", "abs": "https://arxiv.org/abs/2601.12146", "authors": ["Viktor Kjellberg", "Miroslaw Staron", "Farnaz Fotrousi"], "title": "From LLMs to Agents in Programming: The Impact of Providing an LLM with a Compiler", "comment": null, "summary": "Large Language Models have demonstrated a remarkable capability in natural language and program generation and software development. However, the source code generated by the LLMs does not always meet quality requirements and may fail to compile. Therefore, many studies evolve into agents that can reason about the problem before generating the source code for the solution. The goal of this paper is to study the degree to which such agents benefit from access to software development tools, in our case, a \\texttt{gcc} compiler. We conduct a computational experiment on the RosettaCode dataset, on 699 programming tasks in C. We evaluate how the integration with a compiler shifts the role of the language model from a passive generator to an active agent capable of iteratively developing runnable programs based on feedback from the compiler. We evaluated 16 language models with sizes ranging from small (135 million) to medium (3 billion) and large (70 billion). Our results show that access to a compiler improved the compilation success by 5.3 to 79.4 percentage units in compilation without affecting the semantics of the generated program. Syntax errors dropped by 75\\%, and errors related to undefined references dropped by 87\\% for the tasks where the agents outperformed the baselines. We also observed that in some cases, smaller models with a compiler outperform larger models with a compiler. We conclude that it is essential for LLMs to have access to software engineering tools to enhance their performance and reduce the need for large models in software engineering, such as reducing our energy footprint.", "AI": {"tldr": "\u7814\u7a76LLM\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u7f16\u8bd1\u5668\u5de5\u5177\u63d0\u5347\u6027\u80fd\uff0c\u53d1\u73b0\u5728C\u8bed\u8a00\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0c\u7f16\u8bd1\u5668\u8bbf\u95ee\u663e\u8457\u63d0\u9ad8\u7f16\u8bd1\u6210\u529f\u7387\uff0c\u51cf\u5c11\u8bed\u6cd5\u9519\u8bef\uff0c\u4e14\u5c0f\u6a21\u578b\u642d\u914d\u7f16\u8bd1\u5668\u6709\u65f6\u80fd\u8d85\u8d8a\u5927\u6a21\u578b\u3002", "motivation": "LLM\u751f\u6210\u7684\u6e90\u4ee3\u7801\u8d28\u91cf\u4e0d\u7a33\u5b9a\uff0c\u7ecf\u5e38\u65e0\u6cd5\u7f16\u8bd1\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u901a\u8fc7\u7f16\u8bd1\u5668\u5de5\u5177\uff08\u5982gcc\uff09\u8fdb\u884c\u8fed\u4ee3\u5f00\u53d1\u7684\u80fd\u529b\uff0c\u8bc4\u4f30\u5de5\u5177\u8bbf\u95ee\u5bf9LLM\u4ece\u88ab\u52a8\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u4ee3\u7406\u7684\u5f71\u54cd\u3002", "method": "\u5728RosettaCode\u6570\u636e\u96c6\u7684699\u4e2aC\u8bed\u8a00\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8fdb\u884c\u8ba1\u7b97\u5b9e\u9a8c\u3002\u8bc4\u4f3016\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLM\uff081.35\u4ebf\u5230700\u4ebf\u53c2\u6570\uff09\uff0c\u6bd4\u8f83\u6709/\u65e0\u7f16\u8bd1\u5668\u8bbf\u95ee\u65f6\u7684\u8868\u73b0\u3002\u8ba9LLM\u4ee3\u7406\u57fa\u4e8e\u7f16\u8bd1\u5668\u53cd\u9988\u8fed\u4ee3\u5f00\u53d1\u53ef\u8fd0\u884c\u7a0b\u5e8f\u3002", "result": "\u7f16\u8bd1\u5668\u8bbf\u95ee\u4f7f\u7f16\u8bd1\u6210\u529f\u7387\u63d0\u53475.3-79.4\u4e2a\u767e\u5206\u70b9\uff0c\u4e0d\u5f71\u54cd\u7a0b\u5e8f\u8bed\u4e49\u3002\u8bed\u6cd5\u9519\u8bef\u51cf\u5c1175%\uff0c\u672a\u5b9a\u4e49\u5f15\u7528\u9519\u8bef\u51cf\u5c1187%\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u914d\u5907\u7f16\u8bd1\u5668\u7684\u5c0f\u6a21\u578b\u6027\u80fd\u8d85\u8fc7\u914d\u5907\u7f16\u8bd1\u5668\u7684\u5927\u6a21\u578b\u3002", "conclusion": "LLM\u5fc5\u987b\u8bbf\u95ee\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u5927\u578b\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u964d\u4f4e\u80fd\u6e90\u6d88\u8017\u3002\u7f16\u8bd1\u5668\u5de5\u5177\u4f7fLLM\u4ece\u88ab\u52a8\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u80fd\u591f\u8fed\u4ee3\u5f00\u53d1\u53ef\u8fd0\u884c\u7a0b\u5e8f\u7684\u4e3b\u52a8\u4ee3\u7406\u3002", "topic": "code agent"}}
{"id": "2601.11658", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11658", "abs": "https://arxiv.org/abs/2601.11658", "authors": ["Indrajit Kar", "Sammy Zonunpuia", "Zonunfeli Ralte"], "title": "Towards AGI A Pragmatic Approach Towards Self Evolving Agent", "comment": null, "summary": "Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5206\u5c42\u81ea\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u5408\u6210\u548c\u8fdb\u5316\u7b97\u6cd5\u4f7fLLM\u667a\u80fd\u4f53\u80fd\u591f\u6301\u7eed\u9002\u5e94\u548c\u81ea\u4e3b\u6269\u5c55\u80fd\u529b", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u90e8\u7f72\u540e\u662f\u9759\u6001\u7684\uff0c\u7f3a\u4e4f\u81ea\u4e3b\u6269\u5c55\u80fd\u529b\u3001\u751f\u6210\u65b0\u5de5\u5177\u6216\u8fdb\u5316\u63a8\u7406\u7684\u80fd\u529b\uff0c\u9700\u8981\u5b9e\u73b0\u6301\u7eed\u81ea\u9002\u5e94", "method": "\u5206\u5c42\u81ea\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u57fa\u7840LLM\u3001\u64cd\u4f5cSLM\u667a\u80fd\u4f53\u3001\u4ee3\u7801\u751f\u6210LLM\u548c\u6559\u5e08LLM\u3002\u4efb\u52a1\u5931\u8d25\u65f6\u5347\u7ea7\u5230\u5de5\u5177\u5408\u6210\uff0c\u6301\u7eed\u5931\u8d25\u65f6\u89e6\u53d1\u8fdb\u5316\u9636\u6bb5\uff08\u8bfe\u7a0b\u5b66\u4e60\u3001\u57fa\u4e8e\u5956\u52b1\u7684\u5b66\u4e60\u6216\u9057\u4f20\u7b97\u6cd5\u8fdb\u5316\uff09", "result": "\u5728TaskCraft\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff1a\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u5feb\u901f\u6062\u590d\u548c\u5f3a\u6cdb\u5316\uff0c\u57fa\u4e8e\u5956\u52b1\u7684\u5b66\u4e60\u5728\u9ad8\u96be\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9057\u4f20\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u884c\u4e3a\u591a\u6837\u6027\u3002\u6240\u6709\u8fdb\u5316\u667a\u80fd\u4f53\u90fd\u4f18\u4e8e\u539f\u59cb\u7248\u672c", "conclusion": "\u5206\u5c42\u81ea\u8fdb\u5316\u6846\u67b6\u5b9e\u73b0\u4e86\u7a33\u5065\u3001\u81ea\u4e3b\u3001\u81ea\u6211\u6539\u8fdb\u7684\u667a\u80fd\u4f53\u8fdb\u5316\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u6301\u7eed\u9002\u5e94\u548c\u6269\u5c55\u80fd\u529b\u7684\u53ef\u884c\u6027", "topic": "agent analysis"}}
{"id": "2601.11840", "categories": ["cs.AI", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11840", "abs": "https://arxiv.org/abs/2601.11840", "authors": ["Hongyu Lin", "Samer Abdallah", "Makar Valentinov", "Paul Brennan", "Elijah Kagan", "Christoph M. Wintersteiger", "Denis Ignatovich", "Grant Passmore"], "title": "Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic", "comment": "52 pages, 23 figures. Includes a new benchmark dataset (code-logic-bench) and evaluation of neurosymbolic reasoning for software analysis", "summary": "Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.\n  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.\n  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.\n  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.", "AI": {"tldr": "CodeLogician\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u4ee3\u7406\uff0c\u7ed3\u5408LLMs\u548c\u5f62\u5f0f\u5316\u63a8\u7406\u5f15\u64ceImandraX\uff0c\u7528\u4e8e\u7cbe\u786e\u5206\u6790\u8f6f\u4ef6\u903b\u8f91\uff0c\u5728\u4ee3\u7801\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u4e0a\u76f8\u6bd4\u7eafLLM\u65b9\u6cd5\u63d0\u534741-47\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "LLMs\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u884c\u4e3a\u8fdb\u884c\u7cbe\u786e\u3001\u8be6\u5c3d\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u57fa\u51c6\u8981\u4e48\u4e13\u6ce8\u4e8e\u4e0e\u771f\u5b9e\u8f6f\u4ef6\u8131\u8282\u7684\u6570\u5b66\u8bc1\u660e\u81ea\u52a8\u5316\uff0c\u8981\u4e48\u4e13\u6ce8\u4e8e\u4e0d\u9700\u8981\u8bed\u4e49\u4e25\u8c28\u6027\u7684\u5de5\u7a0b\u4efb\u52a1\u3002", "method": "\u63d0\u51faCodeLogician\u795e\u7ecf\u7b26\u53f7\u4ee3\u7406\uff0c\u96c6\u6210\u5de5\u4e1a\u7ea7\u81ea\u52a8\u63a8\u7406\u5f15\u64ceImandraX\u3002\u4f7f\u7528LLMs\u6784\u5efa\u8f6f\u4ef6\u7cfb\u7edf\u7684\u663e\u5f0f\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u4f7f\u81ea\u52a8\u63a8\u7406\u80fd\u591f\u56de\u7b54\u8d85\u8d8a\u4e8c\u5143\u9a8c\u8bc1\u7ed3\u679c\u7684\u4e30\u5bcc\u8bed\u4e49\u95ee\u9898\u3002", "result": "\u5728code-logic-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u7eafLLM\u63a8\u7406\uff0cCodeLogician\u7684\u5f62\u5f0f\u5316\u589e\u5f3a\u5e26\u6765\u663e\u8457\u6539\u8fdb\uff0c\u7f29\u5c0f\u4e8641-47\u4e2a\u767e\u5206\u70b9\u7684\u63a8\u7406\u51c6\u786e\u7387\u5dee\u8ddd\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u5bf9\u4e8e\u6269\u5c55\u7a0b\u5e8f\u5206\u6790\u3001\u5b9e\u73b0\u4e25\u683c\u81ea\u4e3b\u8f6f\u4ef6\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "topic": "code agent"}}
{"id": "2601.12148", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12148", "abs": "https://arxiv.org/abs/2601.12148", "authors": ["Muhammad Umar Zeshan", "Motunrayo Ibiyo", "Claudio Di Sipio", "Phuong T. Nguyen", "Davide Di Ruscio"], "title": "Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages", "comment": "The paper has been peer-reviewed and accepted for publication to the Journal of Systems and Software (https://www.sciencedirect.com/journal/journal-of-systems-and-software)", "summary": "Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.", "AI": {"tldr": "LAMPS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f7f\u7528\u534f\u4f5c\u5f0fLLM\u68c0\u6d4b\u6076\u610fPyPI\u5305\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u523097-99%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u6e90\u4ed3\u5e93\u4e2d\u7684\u6076\u610f\u4ee3\u7801\u5bf9\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u6784\u6210\u5a01\u80c1\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u5de5\u5177\u5ffd\u89c6\u6e90\u4ee3\u7801\u8bed\u4e49\u6a21\u5f0f\uff0cLLM\u5728\u53ef\u89e3\u91ca\u548c\u6a21\u5757\u5316\u5b89\u5168\u7ba1\u9053\u4e2d\u7684\u5e94\u7528\u6709\u9650\u3002", "method": "\u63d0\u51faLAMPS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u5305\u68c0\u7d22\u3001\u6587\u4ef6\u63d0\u53d6\u3001\u5206\u7c7b\u548c\u88c1\u51b3\u805a\u5408\u56db\u4e2a\u89d2\u8272\u7279\u5b9a\u667a\u80fd\u4f53\uff0c\u901a\u8fc7CrewAI\u6846\u67b6\u534f\u8c03\uff0c\u7ed3\u5408\u5fae\u8c03CodeBERT\u6a21\u578b\u548cLLaMA-3\u667a\u80fd\u4f53\u8fdb\u884c\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "result": "\u5728D1\u6570\u636e\u96c6\uff086000\u4e2asetup.py\u6587\u4ef6\uff09\u4e0a\u8fbe\u523097.7%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eMPHunter\uff1b\u5728D2\u6570\u636e\u96c6\uff081296\u4e2a\u591a\u6587\u4ef6\uff09\u4e0a\u8fbe\u523099.5%\u51c6\u786e\u7387\u548c99.5%\u5e73\u8861\u51c6\u786e\u7387\uff0c\u4f18\u4e8eRAG\u65b9\u6cd5\u548c\u5fae\u8c03\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\uff0cMcNemar\u68c0\u9a8c\u8bc1\u5b9e\u6539\u8fdb\u663e\u8457\u3002", "conclusion": "\u5206\u5e03\u5f0fLLM\u63a8\u7406\u7528\u4e8e\u6076\u610f\u4ee3\u7801\u68c0\u6d4b\u53ef\u884c\uff0c\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u5728\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\u4e2d\u5177\u6709\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2601.12186", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12186", "abs": "https://arxiv.org/abs/2601.12186", "authors": ["Vatsal Venkatkrishna", "Indraneil Paul", "Iryna Gurevych"], "title": "Aletheia: What Makes RLVR For Code Verifiers Tick?", "comment": "8 pages, 6 figures", "summary": "Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Aletheia\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7801\u9a8c\u8bc1\u5668\u5728\u4e0d\u540c\u7b56\u7565\u6a21\u578b\u548c\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0RLVR\u65b9\u6cd5\u5728\u4ee3\u7801\u9a8c\u8bc1\u4e2d\u6709\u6548\uff0c\u4f46\u53ef\u4ee5\u7b80\u5316\u8bad\u7ec3\u914d\u65b9\u3002", "motivation": "\u4ee3\u7801\u9a8c\u8bc1\u5668\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5e94\u7528\u8f83\u5c11\uff0c\u4f46\u5b83\u4eec\u5728\u96be\u4ee5\u83b7\u5f97\u6267\u884c\u53cd\u9988\u7684\u573a\u666f\u4e2d\u5f88\u6709\u4ef7\u503c\u3002\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7801\u9a8c\u8bc1\u5668\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u521b\u5efaAletheia\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301\u57fa\u4e8e\u6267\u884c\u7684\u4ee3\u7801\u9a8c\u8bc1\u5668\u8bc4\u4f30\u3002\u7814\u7a76RLVR\u8bad\u7ec3\u914d\u65b9\u7684\u5173\u952e\u7ec4\u4ef6\uff1a\u4e2d\u95f4\u601d\u8003\u8f68\u8ff9\u3001\u4ece\u8d1f\u6837\u672c\u5b66\u4e60\u3001\u5728\u7ebf\u7b56\u7565\u8bad\u7ec3\u3002", "result": "RLVR\u65b9\u6cd5\u5728\u4ee3\u7801\u9a8c\u8bc1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4f46\u8bad\u7ec3\u914d\u65b9\u53ef\u4ee5\u7b80\u5316\u3002\u5c0f\u89c4\u6a21\u9a8c\u8bc1\u5668\u4e2d\u5728\u7ebf\u7b56\u7565\u5b66\u4e60\u662f\u5173\u952e\uff0c\u5927\u89c4\u6a21\u9a8c\u8bc1\u5668\u4e2d\u57fa\u4e8e\u601d\u8003\u7684\u8bad\u7ec3\u6700\u91cd\u8981\u3002", "conclusion": "\u4ee3\u7801\u9a8c\u8bc1\u5668\u662f\u4ee3\u7801\u751f\u6210\u540e\u8bad\u7ec3\u5de5\u5177\u7bb1\u7684\u6709\u529b\u8865\u5145\uff0cRLVR\u65b9\u6cd5\u6709\u6548\u4f46\u53ef\u7b80\u5316\uff0c\u4e0d\u540c\u89c4\u6a21\u9a8c\u8bc1\u5668\u9700\u8981\u4e0d\u540c\u5173\u952e\u7ec4\u4ef6\u3002", "topic": "code agent"}}
{"id": "2601.12262", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12262", "abs": "https://arxiv.org/abs/2601.12262", "authors": ["Tongtong Wu", "Rongyi Chen", "Wenjie Du", "Suyu Ma", "Guilin Qi", "Zhenchang Xing", "Shahram Khadivi", "Ramesh Periyathambi", "Gholamreza Haffari"], "title": "Environment-Aware Code Generation: How far are We?", "comment": "ICSE 2026", "summary": "Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u73af\u5883\u611f\u77e5\u4ee3\u7801\u751f\u6210(EACG)\u7684\u6982\u5ff5\uff0c\u5e76\u521b\u5efa\u4e86VersiBCB\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30LLMs\u5728\u7279\u5b9a\u8f6f\u4ef6\u73af\u5883\u4e0b\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLMs\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u6570\u636e\u3001\u53c2\u6570\u548c\u7f13\u5b58\u4e09\u79cd\u9002\u914d\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u6539\u5584\u73af\u5883\u517c\u5bb9\u6027\u548c\u53ef\u6267\u884c\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u53ea\u6d4b\u8bd5\u5b64\u7acb\u7684\u5c0f\u89c4\u6a21\u4ee3\u7801\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u8f6f\u4ef6\u73af\u5883\u7684\u590d\u6742\u6027\u3002\u4e0d\u6e05\u695aLLMs\u662f\u5426\u80fd\u751f\u6210\u5728\u7528\u6237\u7279\u5b9a\u73af\u5883\u4e0b\u53ef\u6267\u884c\u7684\u4ee3\u7801\uff0c\u8fd9\u9650\u5236\u4e86LLMs\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u73af\u5883\u611f\u77e5\u4ee3\u7801\u751f\u6210(EACG)\u6846\u67b6\uff0c\u521b\u5efa\u4e86VersiBCB\u57fa\u51c6\u6d4b\u8bd5\uff08\u591a\u5305\u3001\u6267\u884c\u9a8c\u8bc1\u3001\u5f03\u7528\u611f\u77e5\uff09\u3002\u7814\u7a76\u4e86\u4e09\u79cd\u9002\u914d\u7b56\u7565\uff1a\u6570\u636e\u9002\u914d\uff08\u4f7f\u7528\u73af\u5883\u7279\u5b9a\u6570\u636e\u8bad\u7ec3\uff09\u3001\u53c2\u6570\u9002\u914d\uff08\u8c03\u6574\u6a21\u578b\u53c2\u6570\uff09\u3001\u7f13\u5b58\u9002\u914d\uff08\u5229\u7528\u7f13\u5b58\u673a\u5236\uff09\u3002", "result": "\u5f53\u524dLLMs\u5728\u73af\u5883\u7279\u5b9a\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u63d0\u51fa\u7684\u4e09\u79cd\u9002\u914d\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u73af\u5883\u517c\u5bb9\u6027\u548c\u53ef\u6267\u884c\u6027\u3002VersiBCB\u57fa\u51c6\u6d4b\u8bd5\u6709\u6548\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u73af\u5883\u611f\u77e5\u4ee3\u7801\u751f\u6210\u662f\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u9002\u914d\u7b56\u7565\u3002\u8be5\u7814\u7a76\u4e3aLLMs\u5728\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u548c\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2601.11903", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11903", "abs": "https://arxiv.org/abs/2601.11903", "authors": ["YenTing Lee", "Keerthi Koneru", "Zahra Moslemi", "Sheethal Kumar", "Ramesh Radhakrishnan"], "title": "AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems", "comment": "Workshop on W51: How Can We Trust and Control Agentic AI? Toward Alignment, Robustness, and Verifiability in Autonomous LLM Agents at AAAI 2026", "summary": "Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.\n  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight", "AI": {"tldr": "AEMA\u662f\u4e00\u4e2a\u9762\u5411\u4f01\u4e1a\u7ea7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u3001\u53ef\u5ba1\u8ba1\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u63d0\u4f9b\u6bd4\u5355\u4e00LLM-as-a-Judge\u66f4\u7a33\u5b9a\u3001\u53ef\u8ffd\u6eaf\u7684\u8bc4\u4f30\u65b9\u6848\u3002", "motivation": "\u73b0\u6709LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5355\u54cd\u5e94\u8bc4\u5206\u6216\u72ed\u7a84\u57fa\u51c6\u6d4b\u8bd5\u5728\u6269\u5c55\u5230\u4f01\u4e1a\u7ea7\u591a\u667a\u80fd\u4f53\u89c4\u6a21\u65f6\u7f3a\u4e4f\u7a33\u5b9a\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u9700\u8981\u66f4\u53ef\u9760\u3001\u900f\u660e\u4e14\u53ef\u9a8c\u8bc1\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faAEMA\uff08\u81ea\u9002\u5e94\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u8fc7\u7a0b\u611f\u77e5\u3001\u53ef\u5ba1\u8ba1\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u4eba\u7c7b\u76d1\u7763\u4e0b\u89c4\u5212\u3001\u6267\u884c\u548c\u805a\u5408\u5f02\u6784\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u591a\u6b65\u9aa4\u8bc4\u4f30\uff0c\u652f\u6301\u53ef\u8ffd\u6eaf\u7684\u8bb0\u5f55\u548c\u95ee\u8d23\u81ea\u52a8\u5316\u3002", "result": "\u5728\u6a21\u62df\u771f\u5b9e\u4e1a\u52a1\u573a\u666f\u7684\u4f01\u4e1a\u98ce\u683c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e0a\u6d4b\u8bd5\uff0cAEMA\u76f8\u6bd4\u5355\u4e00LLM-as-a-Judge\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u3001\u66f4\u597d\u7684\u4eba\u7c7b\u5bf9\u9f50\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u8bb0\u5f55\uff0c\u652f\u6301\u8d1f\u8d23\u4efb\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "conclusion": "AEMA\u4e3a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u8def\u5f84\uff0c\u652f\u6301\u8d1f\u8d23\u4efb\u7684\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5177\u6709\u8fc7\u7a0b\u611f\u77e5\u3001\u53ef\u5ba1\u8ba1\u548c\u53ef\u6269\u5c55\u7684\u7279\u70b9\u3002", "topic": "agent analysis"}}
{"id": "2601.12273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12273", "abs": "https://arxiv.org/abs/2601.12273", "authors": ["Chihiro Yoshida", "Yuta Ishimoto", "Olivier Nourry", "Masanari Kondo", "Makoto Matsushita", "Yasutaka Kamei", "Yoshiki Higo"], "title": "Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs", "comment": "6 pages, Accepted at SANER-ERA 2026", "summary": "In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5b50\u7a0b\u5e8f\u81ea\u52a8\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u4fe1\u606f\u3001\u52a8\u6001\u4fe1\u606f\u548c\u53d8\u5f02\u5206\u6790\u7ed3\u679c\u6765\u63d0\u5347\u4fee\u590d\u6210\u529f\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u53d8\u5f02\u5206\u6790\u80fd\u663e\u8457\u6539\u5584\u4fee\u590d\u6548\u679c\uff08\u8fbe\u523094.4%\u6210\u529f\u7387\uff09\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u7a0b\u5e8f\u81ea\u52a8\u4fee\u590d\u6280\u672f\u5b58\u5728\u4fee\u590d\u6210\u529f\u7387\u4f4e\u548c\u751f\u6210\u8865\u4e01\u53ef\u7406\u89e3\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u63d0\u9ad8\u53ef\u9760\u6027\u53c8\u80fd\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u751f\u6210\u4ee3\u7801\u4fee\u590d\u53ca\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff1b\u8bbe\u8ba1\u56db\u79cd\u63d0\u793a\u914d\u7f6e\uff0c\u7ec4\u5408\u9759\u6001\u4fe1\u606f\u3001\u52a8\u6001\u4fe1\u606f\u548c\u53d8\u5f02\u5206\u6790\u7ed3\u679c\uff1b\u53d8\u5f02\u5206\u6790\u901a\u8fc7\u8bc4\u4f30\u7a0b\u5e8f\u5fae\u5c0f\u53d8\u5316\u5bf9\u6267\u884c\u7ed3\u679c\u7684\u5f71\u54cd\u63d0\u4f9b\u8be6\u7ec6\u52a8\u6001\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u53d8\u5f02\u5206\u6790\u80fd\u4e3a\u57fa\u4e8eLLM\u7684\u91cf\u5b50\u7a0b\u5e8fAPR\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4fee\u590d\u6210\u529f\u7387\uff08\u8fbe\u523094.4%\uff09\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fd8\u80fd\u6539\u5584\u751f\u6210\u89e3\u91ca\u7684\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u91cf\u5b50\u7a0b\u5e8f\u81ea\u52a8\u4fee\u590d\u6280\u672f\u6307\u51fa\u4e86\u65b0\u65b9\u5411\uff0c\u5f3a\u8c03\u540c\u65f6\u589e\u5f3a\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u53d8\u5f02\u5206\u6790\u662f\u63d0\u5347LLM-based APR\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "topic": "code agent"}}
{"id": "2601.11974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11974", "abs": "https://arxiv.org/abs/2601.11974", "authors": ["Xinmeng Hou", "Peiliang Gong", "Bohao Qu", "Wuqi Wang", "Qing Guo", "Yang Liu"], "title": "Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement", "comment": null, "summary": "While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.", "AI": {"tldr": "MARS\u6846\u67b6\u901a\u8fc7\u5355\u6b21\u5faa\u73af\u5b9e\u73b0\u9ad8\u6548\u81ea\u6211\u8fdb\u5316\uff0c\u7ed3\u5408\u539f\u5219\u6027\u53cd\u601d\u548c\u7a0b\u5e8f\u6027\u53cd\u601d\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u53d7\u9650\u4e8e\u9759\u6001\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u3002\u73b0\u6709\u81ea\u6211\u6539\u8fdb\u6846\u67b6\u4f9d\u8d56\u4f4e\u6548\u7684\u591a\u8f6e\u9012\u5f52\u5faa\u73af\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8", "method": "\u63d0\u51faMARS\u6846\u67b6\uff0c\u53d7\u6559\u80b2\u5fc3\u7406\u5b66\u542f\u53d1\uff0c\u7ed3\u5408\u539f\u5219\u6027\u53cd\u601d\uff08\u62bd\u8c61\u89c4\u8303\u89c4\u5219\u907f\u514d\u9519\u8bef\uff09\u548c\u7a0b\u5e8f\u6027\u53cd\u601d\uff08\u63a8\u5bfc\u9010\u6b65\u6210\u529f\u7b56\u7565\uff09\uff0c\u5728\u5355\u6b21\u5faa\u73af\u4e2d\u5408\u6210\u4f18\u5316\u6307\u4ee4", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMARS\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u81ea\u6211\u8fdb\u5316\u7cfb\u7edf\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500", "conclusion": "MARS\u6846\u67b6\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7cfb\u7edf\u6027\u63a8\u7406\u903b\u8f91\u4f18\u5316\uff0c\u4e3a\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411", "topic": "agent analysis"}}
{"id": "2601.12522", "categories": ["cs.SE", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.12522", "abs": "https://arxiv.org/abs/2601.12522", "authors": ["Asif Mohammed Samir", "Mohammad Masudur Rahman"], "title": "Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition", "comment": "13 pages, 7 tables, 5 figures", "summary": "Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.", "AI": {"tldr": "CogniGent\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u9519\u8bef\u5b9a\u4f4d\u6280\u672f\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u3001\u8c03\u7528\u56fe\u5206\u6790\u548c\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u6a21\u62df\u5f00\u53d1\u8005\u7684\u52a8\u6001\u8ba4\u77e5\u8c03\u8bd5\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9519\u8bef\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u8f6f\u4ef6\u9519\u8bef\u6bcf\u5e74\u9020\u6210\u6570\u5341\u4ebf\u7f8e\u5143\u635f\u5931\uff0c\u5f00\u53d1\u800550%\u65f6\u95f4\u7528\u4e8e\u4fee\u590d\u9519\u8bef\u3002\u4f20\u7edf\u9519\u8bef\u5b9a\u4f4d\u65b9\u6cd5\u5b64\u7acb\u5206\u6790\u4ee3\u7801\u7ec4\u4ef6\uff0c\u5ffd\u7565\u4e86\u7ec4\u4ef6\u95f4\u7684\u8fde\u63a5\u5173\u7cfb\u3002\u73b0\u6709LLM\u65b9\u6cd5\u7f3a\u4e4f\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u96be\u4ee5\u6709\u6548\u7ba1\u7406\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faCogniGent\u591a\u667a\u80fd\u4f53\u6280\u672f\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u3001\u8c03\u7528\u56fe\u6839\u56e0\u5206\u6790\u548c\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u6a21\u62df\u5f00\u53d1\u8005\u7684\u52a8\u6001\u8ba4\u77e5\u8c03\u8bd5\u5b9e\u8df5\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u652f\u6301\u9519\u8bef\u5b9a\u4f4d\u3002", "result": "\u5728591\u4e2a\u9519\u8bef\u62a5\u544a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd46\u4e2a\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u5728\u6587\u6863\u548c\u65b9\u6cd5\u7ea7\u522bMAP\u63d0\u534723.33-38.57%\uff0cMRR\u63d0\u534725.14-53.74%\uff0c\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u786e\u8ba4\u4e86\u6280\u672f\u4f18\u52bf\u3002", "conclusion": "CogniGent\u901a\u8fc7\u89e3\u51b3\u63a8\u7406\u3001\u4f9d\u8d56\u548c\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u5c06\u7c7b\u4eba\u8ba4\u77e5\u4e0e\u667a\u80fd\u4f53\u81ea\u52a8\u5316\u7ed3\u5408\uff0c\u63a8\u8fdb\u4e86\u9519\u8bef\u5b9a\u4f4d\u6280\u672f\u53d1\u5c55\u3002", "topic": "swe application"}}
{"id": "2601.12735", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12735", "abs": "https://arxiv.org/abs/2601.12735", "authors": ["Hao Chen", "Yunchun Li", "Chen Chen", "Fengxu Lin", "Wei Li"], "title": "OpenAI for OpenAPI: Automated generation of REST API specification via LLMs", "comment": null, "summary": "REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.", "AI": {"tldr": "OOPS\uff1a\u9996\u4e2a\u6280\u672f\u65e0\u5173\u7684LLM\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eceREST API\u4ee3\u7801\u751f\u6210OpenAPI\u89c4\u8303\uff0c\u901a\u8fc7API\u4f9d\u8d56\u56fe\u548c\u591a\u9636\u6bb5\u751f\u6210\u89e3\u51b3LLM\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f00\u53d1\u8005\u7f16\u5199\u548c\u7ef4\u62a4OpenAPI\u89c4\u8303\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u9759\u6001\u5206\u6790\u65b9\u6cd5\u53d7\u9650\u4e8e\u7279\u5b9a\u7f16\u7a0b\u8bed\u8a00\u548c\u6846\u67b6\uff0c\u800cLLM\u65b9\u6cd5\u53d7\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u5e7b\u89c9\u95ee\u9898\u7ea6\u675f\u3002", "method": "\u63d0\u51faOOPS\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\u7aef\u70b9\u65b9\u6cd5\u63d0\u53d6\u548cOAS\u751f\u6210\u3002\u901a\u8fc7\u6784\u5efaAPI\u4f9d\u8d56\u56fe\u5efa\u7acb\u6587\u4ef6\u5173\u8054\u89e3\u51b3\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u751f\u6210\u548c\u81ea\u4f18\u5316\u673a\u5236\u7f13\u89e3\u8bed\u6cd5\u548c\u8bed\u4e49\u5e7b\u89c9\u3002", "result": "\u572812\u4e2a\u771f\u5b9eREST API\uff08\u6db5\u76d65\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c8\u4e2a\u5f00\u53d1\u6846\u67b6\uff09\u4e0a\u8bc4\u4f30\uff0c\u7aef\u70b9\u65b9\u6cd5\u63a8\u65adF1\u5206\u6570\u8d85\u8fc798%\uff0c\u8bf7\u6c42\u53c2\u6570\u548c\u54cd\u5e94\u63a8\u65ad\u8fbe97%\uff0c\u53c2\u6570\u7ea6\u675f\u63a8\u65ad\u8fbe92%\u3002\u8f93\u5165token\u5e73\u5747\u4f4e\u4e8e5.6K\uff0c\u8f93\u51fatoken\u5e73\u5747\u4f4e\u4e8e0.9K\u3002", "conclusion": "OOPS\u80fd\u591f\u51c6\u786e\u751f\u6210\u9ad8\u8d28\u91cfOAS\uff0c\u652f\u6301\u591a\u79cd\u6280\u672f\u6808\uff0c\u51cf\u5c11\u6280\u672f\u7279\u5b9a\u89c4\u5219\u548c\u4eba\u5de5\u5e72\u9884\uff0c\u4e3aREST API\u6587\u6863\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u65e0\u5173\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.11854", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.11854", "abs": "https://arxiv.org/abs/2601.11854", "authors": ["Yifei Zhang", "Hooshang Nayyeri", "Rinat Khaziev", "Emine Yilmaz", "Gokhan Tur", "Dilek Hakkani-T\u00fcr", "Hari Thadakamalla"], "title": "ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System", "comment": null, "summary": "Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.", "AI": {"tldr": "ATOD\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u5305\u542bATOD-Eval\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u8bc4\u4f30\u5668", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9LLM\u9a71\u52a8\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u667a\u80fd\u4f53\u884c\u4e3a\uff08\u5982\u591a\u76ee\u6807\u534f\u8c03\u3001\u957f\u671f\u63a8\u7406\u3001\u5f02\u6b65\u6267\u884c\u7b49\uff09\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u652f\u6301", "method": "\u63d0\u51faATOD\u57fa\u51c6\u6d4b\u8bd5\u548c\u5408\u6210\u5bf9\u8bdd\u751f\u6210\u7ba1\u9053\uff0c\u521b\u5efa\u9700\u8981\u957f\u671f\u63a8\u7406\u7684\u4e30\u5bcc\u6807\u6ce8\u5bf9\u8bdd\uff1b\u5f00\u53d1ATOD-Eval\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u7ef4\u5ea6\u8f6c\u5316\u4e3a\u7ec6\u7c92\u5ea6\u6307\u6807\uff1b\u63d0\u51fa\u57fa\u4e8e\u8bb0\u5fc6\u7684\u8bc4\u4f30\u5668", "result": "ATOD-Eval\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u4efb\u52a1\u5b8c\u6210\u5ea6\u3001\u667a\u80fd\u4f53\u80fd\u529b\u548c\u54cd\u5e94\u8d28\u91cf\uff1b\u63d0\u51fa\u7684\u8bc4\u4f30\u5668\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6743\u8861", "conclusion": "ATOD\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u9ad8\u7ea7\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u667a\u80fd\u4f53\u884c\u4e3a\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6", "topic": "agent analysis"}}
{"id": "2601.12762", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12762", "abs": "https://arxiv.org/abs/2601.12762", "authors": ["Xingjie Gao", "Pengcheng Huang", "Zhenghao Liu", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Chen Qian", "Ge Yu", "Yu Gu"], "title": "Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction", "comment": null, "summary": "Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.", "AI": {"tldr": "ToolMaster\u662f\u4e00\u4e2a\u8ba9LLM\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u4e3b\u52a8\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u7684\u6846\u67b6\uff0c\u91c7\u7528\u8bd5\u9519\u6267\u884c\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u672a\u89c1\u5de5\u5177\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8f68\u8ff9\u8bb0\u5fc6\u7684\u65b9\u6cd5\u5728\u9762\u5bf9\u65b0\u5de5\u5177\u6216\u6f14\u5316\u5de5\u5177\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5de5\u5177\u4f7f\u7528\u6846\u67b6\u3002", "method": "\u91c7\u7528\u8bd5\u9519\u6267\u884c\u8303\u5f0f\uff1a\u5148\u6a21\u4eff\u5305\u542b\u663e\u5f0f\u5de5\u5177\u5c1d\u8bd5\u548c\u81ea\u6211\u4fee\u6b63\u7684\u6559\u5e08\u8f68\u8ff9\uff0c\u518d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316\u5c1d\u8bd5\u548c\u6267\u884c\u9636\u6bb5\u3002", "result": "ToolMaster\u5728\u672a\u89c1\u6216\u4e0d\u719f\u6089\u5de5\u5177\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u901a\u8fc7\u4e3b\u52a8\u73af\u5883\u4ea4\u4e92\u5f62\u6210\u7ecf\u9a8c\u77e5\u8bc6\u7684\u65b9\u6cd5\u6bd4\u9759\u6001\u8f68\u8ff9\u8bb0\u5fc6\u66f4\u80fd\u63d0\u5347LLM\u5de5\u5177\u4f7f\u7528\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2601.12845", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12845", "abs": "https://arxiv.org/abs/2601.12845", "authors": ["Jo\u00e3o Pascoal Faria", "Emanuel Trigo", "Vinicius Honorato", "Rui Abreu"], "title": "Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles", "comment": null, "summary": "Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.", "AI": {"tldr": "LLMs\u81ea\u52a8\u4e3aDafny\u7a0b\u5e8f\u751f\u6210\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6ce8\u91ca\uff0c\u7ed3\u5408Claude Opus 4.5\u548cGPT-5.2\u7684\u591a\u6a21\u578b\u65b9\u6cd5\u5728110\u4e2a\u7a0b\u5e8f\u4e0a\u8fbe\u523098.2%\u7684\u6b63\u786e\u7387\uff0c\u6700\u591a8\u6b21\u4fee\u590d\u8fed\u4ee3\u3002", "motivation": "\u5f53\u524d\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u867d\u7136\u81ea\u52a8\u5316\u7a0b\u5ea6\u63d0\u9ad8\uff0c\u4f46\u4e3a\u7a0b\u5e8f\u6dfb\u52a0\u5f62\u5f0f\u5316\u89c4\u8303\u6ce8\u91ca\uff08\u524d\u7f6e\u6761\u4ef6\u3001\u540e\u7f6e\u6761\u4ef6\u3001\u5faa\u73af\u4e0d\u53d8\u91cf\u7b49\uff09\u4ecd\u9700\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\u548c\u4e13\u4e1a\u77e5\u8bc6\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5229\u7528LLMs\u81ea\u52a8\u751f\u6210\u8fd9\u4e9b\u6ce8\u91ca\uff0c\u964d\u4f4e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u95e8\u69db\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u578b\u65b9\u6cd5\u7ed3\u5408Claude Opus 4.5\u548cGPT-5.2\uff0c\u4ece\u5e26\u6709\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u6ce8\u91ca\u548c\u6d4b\u8bd5\u4ee3\u7801\u7684\u4f20\u7edf\u4ee3\u7801\u4e2d\u81ea\u52a8\u751f\u6210Dafny\u7a0b\u5e8f\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6ce8\u91ca\u3002\u5229\u7528\u9a8c\u8bc1\u5668\u53cd\u9988\u8fdb\u884c\u6700\u591a8\u6b21\u4fee\u590d\u8fed\u4ee3\uff0c\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u7684\u65ad\u8a00\u4f5c\u4e3a\u9759\u6001\u9884\u8a00\u673a\u9a8c\u8bc1\u751f\u6210\u7684\u524d\u7f6e/\u540e\u7f6e\u6761\u4ef6\u3002", "result": "\u5728110\u4e2aDafny\u7a0b\u5e8f\u5b9e\u9a8c\u4e2d\uff0c\u591a\u6a21\u578b\u65b9\u6cd5\u5728\u6700\u591a8\u6b21\u4fee\u590d\u8fed\u4ee3\u5185\u4e3a98.2%\u7684\u7a0b\u5e8f\u751f\u6210\u4e86\u6b63\u786e\u7684\u6ce8\u91ca\u3002\u903b\u8f91\u56de\u5f52\u5206\u6790\u663e\u793a\u8bc1\u660e\u8f85\u52a9\u6ce8\u91ca\u5bf9\u5f53\u524dLLMs\u6784\u6210\u4e0d\u6210\u6bd4\u4f8b\u7684\u96be\u5ea6\u3002\u8fd8\u5f00\u53d1\u4e86VS Code\u6269\u5c55\u96c6\u6210\u81ea\u52a8\u751f\u6210\u529f\u80fd\uff0c\u83b7\u5f97\u79ef\u6781\u7684\u53ef\u7528\u6027\u53cd\u9988\u3002", "conclusion": "LLMs\u80fd\u591f\u6709\u6548\u81ea\u52a8\u751f\u6210\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6ce8\u91ca\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002\u591a\u6a21\u578b\u65b9\u6cd5\u548c\u9a8c\u8bc1\u5668\u53cd\u9988\u4fee\u590d\u673a\u5236\u662f\u5173\u952e\u6210\u529f\u56e0\u7d20\u3002\u8bc1\u660e\u8f85\u52a9\u6ce8\u91ca\u7684\u751f\u6210\u4ecd\u662f\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "2601.12890", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12890", "abs": "https://arxiv.org/abs/2601.12890", "authors": ["Hang Gao", "Tao Peng", "Baoquan Cui", "Hong Huang", "Fengge Wu", "Junsuo Zhao", "Jian Zhang"], "title": "Efficient Code Analysis via Graph-Guided Large Language Models", "comment": null, "summary": "Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u83b7\u53d6\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u4ee3\u7801\u56fe\u89e3\u6790\u3001GNN\u521d\u6b65\u68c0\u6d4b\u548c\u6ce8\u610f\u529b\u56de\u6eaf\uff0c\u589e\u5f3aLLM\u5b9a\u4f4d\u6076\u610f\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u51cf\u5c11\u65e0\u5173\u4e0a\u4e0b\u6587\u5e72\u6270\u3002", "motivation": "\u6076\u610f\u884c\u4e3a\u5e38\u9690\u85cf\u5728\u5927\u578b\u590d\u6742\u4ee3\u7801\u5e93\u4e2d\u5bb9\u6613\u88ab\u5ffd\u89c6\u7684\u5c0f\u7247\u6bb5\uff0c\u8de8\u6587\u4ef6\u4f9d\u8d56\u4f7f\u5f97\u5373\u4f7f\u5f3a\u5927\u7684LLM\u4e5f\u96be\u4ee5\u53ef\u9760\u68c0\u6d4b\uff0c\u9700\u8981\u589e\u5f3aLLM\u5b9a\u4f4d\u6076\u610f\u884c\u4e3a\u7684\u80fd\u529b\u3002", "method": "1) \u5c06\u9879\u76ee\u89e3\u6790\u4e3a\u4ee3\u7801\u56fe\uff1b2) \u4f7f\u7528LLM\u7f16\u7801\u8282\u70b9\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u53f7\uff1b3) \u5728\u7a00\u758f\u76d1\u7763\u4e0b\u8bad\u7ec3GNN\u8fdb\u884c\u521d\u6b65\u68c0\u6d4b\uff1b4) \u901a\u8fc7\u9884\u6d4b\u56de\u6eaf\u8bc6\u522b\u5173\u952e\u4ee3\u7801\u533a\u57df\uff1b5) \u7528\u8fd9\u4e9b\u533a\u57df\u5f15\u5bfcLLM\u6ce8\u610f\u529b\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u548c\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u65e0\u5173\u4e0a\u4e0b\u6587\u5e72\u6270\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86LLM\u5b9a\u4f4d\u6076\u610f\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u5728\u8f6f\u4ef6\u5b89\u5168\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u673a\u5236\u5e73\u8861\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6807\u6ce8\u6210\u672c\u3002", "topic": "code agent"}}
{"id": "2601.12138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12138", "abs": "https://arxiv.org/abs/2601.12138", "authors": ["Abhishek Kumar", "Riya Tapwal", "Carsten Maple"], "title": "DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.", "AI": {"tldr": "DriveSafe\uff1a\u9488\u5bf9LLM\u9a7e\u9a76\u52a9\u624b\u7684\u56db\u5c42\u6b21\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u5305\u542b129\u4e2a\u7ec6\u7c92\u5ea6\u98ce\u9669\u7c7b\u522b\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u9a7e\u9a76\u573a\u666f\u4e2d\u5b89\u5168\u62d2\u7edd\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u8f66\u8f7d\u6570\u5b57\u52a9\u624b\u4e2d\uff0c\u4f46\u73b0\u6709\u5b89\u5168\u5206\u7c7b\u548c\u8bc4\u4f30\u6846\u67b6\u5927\u591a\u662f\u901a\u7528\u578b\u7684\uff0c\u672a\u80fd\u6355\u6349\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u9886\u57df\u7279\u5b9a\u98ce\u9669\u3002\u4e0d\u5b89\u5168\u3001\u6a21\u7cca\u6216\u6cd5\u5f8b\u9519\u8bef\u7684\u54cd\u5e94\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u5b89\u5168\u3001\u9053\u5fb7\u548c\u76d1\u7ba1\u540e\u679c\u3002", "method": "\u63d0\u51fa\u4e86DriveSafe\uff0c\u4e00\u4e2a\u5206\u5c42\u7684\u56db\u5c42\u6b21\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u5305\u542b129\u4e2a\u7ec6\u7c92\u5ea6\u539f\u5b50\u98ce\u9669\u7c7b\u522b\uff0c\u6db5\u76d6\u6280\u672f\u3001\u6cd5\u5f8b\u3001\u793e\u4f1a\u548c\u4f26\u7406\u7ef4\u5ea6\u3002\u8be5\u5206\u7c7b\u6cd5\u57fa\u4e8e\u771f\u5b9e\u9a7e\u9a76\u6cd5\u89c4\u548c\u5b89\u5168\u539f\u5219\u6784\u5efa\uff0c\u5e76\u7531\u9886\u57df\u4e13\u5bb6\u8bc4\u5ba1\u3002\u901a\u8fc7\u8bc4\u4f30\u516d\u4e2a\u5e7f\u6cdb\u90e8\u7f72\u7684LLM\u7684\u62d2\u7edd\u884c\u4e3a\u6765\u9a8c\u8bc1\u6784\u5efa\u63d0\u793a\u7684\u5b89\u5168\u76f8\u5173\u6027\u548c\u771f\u5b9e\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u88ab\u8bc4\u4f30\u7684\u6a21\u578b\u7ecf\u5e38\u65e0\u6cd5\u9002\u5f53\u62d2\u7edd\u4e0d\u5b89\u5168\u6216\u4e0d\u5408\u89c4\u7684\u9a7e\u9a76\u76f8\u5173\u67e5\u8be2\uff0c\u7a81\u663e\u4e86\u901a\u7528\u5b89\u5168\u5bf9\u9f50\u5728\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u8981\u9488\u5bf9\u9a7e\u9a76\u9886\u57df\u7684\u7279\u5b9a\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u73b0\u6709LLM\u5728\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\u5b58\u5728\u4e0d\u8db3\uff0cDriveSafe\u5206\u7c7b\u6cd5\u4e3a\u7cfb\u7edf\u8bc4\u4f30LLM\u9a7e\u9a76\u52a9\u624b\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.12951", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12951", "abs": "https://arxiv.org/abs/2601.12951", "authors": ["Felix M\u00e4chtle", "Jan-Niclas Serr", "Nils Loose", "Thomas Eisenbarth"], "title": "Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models", "comment": "Published in the Proceedings of DeepTest 2026", "summary": "Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u53d1\u73b0LLM\u7684\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u4e0e\u4f20\u7edf\u4eba\u7c7b\u4e2d\u5fc3\u8f6f\u4ef6\u5ea6\u91cf\u6307\u6807\u76f8\u5173\u6027\u5f88\u5f31\uff0c\u800c\u5f71\u5b50\u6a21\u578b\u80fd\u66f4\u597d\u5730\u9884\u6d4bLLM\u8868\u73b0\uff0c\u8868\u660eLLM\u7406\u89e3\u9075\u5faa\u6a21\u578b\u7279\u6709\u7684\u89c4\u5f8b\uff0c\u9700\u8981\u8d85\u8d8a\u805a\u5408\u51c6\u786e\u7387\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\u53ea\u63d0\u4f9b\u7c97\u7565\u7684\u6027\u80fd\u603b\u7ed3\uff0c\u65e0\u6cd5\u63ed\u793a\u6a21\u578b\u80fd\u529b\u7684\u591a\u6837\u6027\u3002\u9700\u8981\u7814\u7a76LLM\u7684\u4ee3\u7801\u7406\u89e3\u6027\u80fd\u662f\u5426\u4e0e\u4f20\u7edf\u4eba\u7c7b\u4e2d\u5fc3\u8f6f\u4ef6\u5ea6\u91cf\u6307\u6807\u4e00\u81f4\uff0c\u8fd8\u662f\u53cd\u6620\u4e86\u72ec\u7279\u7684\u975e\u4eba\u7c7b\u89c4\u5f8b\u3002", "method": "\u5f15\u5165\u8bca\u65ad\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u7406\u89e3\u91cd\u6784\u4e3a\u4e8c\u5143\u8f93\u5165\u8f93\u51fa\u4e00\u81f4\u6027\u4efb\u52a1\uff0c\u8bc4\u4f30\u5206\u7c7b\u548c\u751f\u6210\u6a21\u578b\u3002\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5c06\u6a21\u578b\u6027\u80fd\u4e0e\u4f20\u7edf\u4eba\u7c7b\u4e2d\u5fc3\u590d\u6742\u5ea6\u6307\u6807\uff08\u5982\u8bcd\u6c47\u91cf\u3001\u63a7\u5236\u6d41\u590d\u6742\u5ea6\u3001\u62bd\u8c61\u8bed\u6cd5\u6811\u7ed3\u6784\uff09\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "\u4eba\u7c7b\u5b9a\u4e49\u6307\u6807\u4e0eLLM\u6210\u529f\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5f88\u5f31\uff08AUROC 0.63\uff09\uff0c\u800c\u5f71\u5b50\u6a21\u578b\u83b7\u5f97\u663e\u8457\u66f4\u9ad8\u7684\u9884\u6d4b\u6027\u80fd\uff08AUROC 0.86\uff09\uff0c\u6355\u6349\u5230\u8d85\u8d8a\u4f20\u7edf\u8f6f\u4ef6\u5ea6\u91cf\u7684\u590d\u6742\u3001\u90e8\u5206\u53ef\u9884\u6d4b\u6a21\u5f0f\u3002", "conclusion": "LLM\u7406\u89e3\u53cd\u6620\u4e86\u6a21\u578b\u7279\u6709\u7684\u89c4\u5f8b\uff0c\u8fd9\u4e9b\u89c4\u5f8b\u53ea\u80fd\u901a\u8fc7\u4eba\u7c7b\u8bbe\u8ba1\u6216\u5b66\u4e60\u7279\u5f81\u90e8\u5206\u83b7\u53d6\u3002\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u805a\u5408\u51c6\u786e\u7387\u3001\u8f6c\u5411\u5b9e\u4f8b\u7ea7\u8bca\u65ad\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u540c\u65f6\u627f\u8ba4\u9884\u6d4b\u6b63\u786e\u7ed3\u679c\u7684\u6839\u672c\u9650\u5236\u3002", "topic": "agent analysis"}}
{"id": "2601.11913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11913", "abs": "https://arxiv.org/abs/2601.11913", "authors": ["Yichen Jiang", "Peng Ye", "Jiakang Yuan", "Chongjun Tu", "Lei Bai", "Tao Chen"], "title": "LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding", "comment": "12 pages, 5 figures", "summary": "Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.", "AI": {"tldr": "\u63d0\u51faLSTM-MAS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6a21\u4effLSTM\u67b6\u6784\u5904\u7406\u957f\u6587\u672c\uff0c\u901a\u8fc7\u94fe\u5f0f\u7ed3\u6784\u548c\u95e8\u63a7\u673a\u5236\u51cf\u5c11\u9519\u8bef\u79ef\u7d2f\u548c\u5e7b\u89c9\u4f20\u64ad\uff0c\u5728\u591a\u4e2a\u957f\u6587\u672c\u7406\u89e3\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u5904\u7406\u957f\u6587\u672c\u5b58\u5728\u6311\u6218\uff1a\u5355LLM\u65b9\u6cd5\u9700\u8981\u51cf\u5c11\u4e0a\u4e0b\u6587\u7a97\u53e3\u6216\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bfc\u81f4\u989d\u5916\u8ba1\u7b97\u6210\u672c\u6216\u53d7\u9650\u7684\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\uff1b\u591a\u667a\u80fd\u4f53\u6846\u67b6\u867d\u80fd\u7f13\u89e3\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u4ecd\u6613\u53d7\u9519\u8bef\u79ef\u7d2f\u548c\u5e7b\u89c9\u4f20\u64ad\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1LSTM-MAS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6a21\u4effLSTM\u67b6\u6784\uff1a\u91c7\u7528\u94fe\u5f0f\u7ed3\u6784\u7ec4\u7ec7\u667a\u80fd\u4f53\uff0c\u6bcf\u4e2a\u8282\u70b9\u5305\u542bworker agent\uff08\u6bb5\u7ea7\u7406\u89e3\uff09\u3001filter agent\uff08\u5197\u4f59\u51cf\u5c11\uff09\u3001judge agent\uff08\u6301\u7eed\u9519\u8bef\u68c0\u6d4b\uff09\u548cmanager agent\uff08\u5168\u5c40\u4fe1\u606f\u4f20\u64ad\u548c\u4fdd\u7559\u8c03\u63a7\uff09\uff0c\u5206\u522b\u5bf9\u5e94LSTM\u7684\u8f93\u5165\u95e8\u3001\u9057\u5fd8\u95e8\u3001\u6052\u5b9a\u8bef\u5dee\u5faa\u73af\u5355\u5143\u548c\u8f93\u51fa\u95e8\u3002", "result": "\u76f8\u6bd4\u4e4b\u524d\u6700\u4f73\u591a\u667a\u80fd\u4f53\u65b9\u6cd5CoA\uff0c\u5728NarrativeQA\u3001Qasper\u3001HotpotQA\u548cMuSiQue\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u534740.93%\u300143.70%\u3001121.57%\u548c33.12%\u3002", "conclusion": "LSTM-MAS\u901a\u8fc7\u6a21\u4effLSTM\u7684\u5c42\u6b21\u4fe1\u606f\u6d41\u548c\u95e8\u63a7\u8bb0\u5fc6\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u4fe1\u606f\u4f20\u9012\u548c\u9009\u62e9\u6027\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\uff0c\u6709\u6548\u907f\u514d\u4e86\u9519\u8bef\u79ef\u7d2f\u548c\u5e7b\u89c9\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u672c\u7406\u89e3\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.13007", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13007", "abs": "https://arxiv.org/abs/2601.13007", "authors": ["Rusheng Pan", "Bingcheng Mao", "Tianyi Ma", "Zhenhua Ling"], "title": "ArchAgent: Scalable Legacy Software Architecture Recovery with LLMs", "comment": "to be published in ICASSP 2026", "summary": "Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). We present ArchAgent, a scalable agent-based framework that combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases. ArchAgent introduces scalable diagram generation with contextual pruning and integrates cross-repository data to identify business-critical modules. Evaluations of typical large-scale GitHub projects show significant improvements over existing benchmarks. An ablation study confirms that dependency context improves the accuracy of generated architectures of production-level repositories, and a real-world case study demonstrates effective recovery of critical business logics from legacy projects. The dataset is available at https://github.com/panrusheng/arch-eval-benchmark.", "AI": {"tldr": "ArchAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u5206\u6790\u3001\u81ea\u9002\u5e94\u4ee3\u7801\u5206\u5272\u548cLLM\u9a71\u52a8\u7684\u5408\u6210\uff0c\u4ece\u8de8\u4ed3\u5e93\u4ee3\u7801\u5e93\u4e2d\u91cd\u6784\u591a\u89c6\u56fe\u3001\u4e1a\u52a1\u5bf9\u9f50\u7684\u8f6f\u4ef6\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u67b6\u6784\u6f02\u79fb\u3001\u5173\u7cfb\u7f3a\u5931\u548cLLM\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u4ece\u5927\u89c4\u6a21\u9057\u7559\u8f6f\u4ef6\u4e2d\u6062\u590d\u51c6\u786e\u67b6\u6784\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u67b6\u6784\u6f02\u79fb\uff08\u5b9e\u9645\u5b9e\u73b0\u504f\u79bb\u8bbe\u8ba1\uff09\u3001\u5173\u7cfb\u7f3a\u5931\uff08\u4ee3\u7801\u4f9d\u8d56\u4e0d\u5b8c\u6574\uff09\u3001\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8de8\u4ed3\u5e93\u4ee3\u7801\u5e93\u548c\u4e1a\u52a1\u5173\u952e\u6a21\u5757\u8bc6\u522b\u3002", "method": "ArchAgent\u91c7\u7528\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u9759\u6001\u5206\u6790\u63d0\u53d6\u4ee3\u7801\u7ed3\u6784\uff0c\u81ea\u9002\u5e94\u4ee3\u7801\u5206\u5272\u5904\u7406\u5927\u89c4\u6a21\u4ee3\u7801\uff0cLLM\u9a71\u52a8\u7684\u5408\u6210\u751f\u6210\u67b6\u6784\u89c6\u56fe\u3002\u5f15\u5165\u53ef\u6269\u5c55\u7684\u56fe\u8868\u751f\u6210\u673a\u5236\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u526a\u679d\u548c\u8de8\u4ed3\u5e93\u6570\u636e\u96c6\u6210\uff0c\u4ee5\u8bc6\u522b\u4e1a\u52a1\u5173\u952e\u6a21\u5757\u3002", "result": "\u5728\u5178\u578b\u5927\u89c4\u6a21GitHub\u9879\u76ee\u8bc4\u4f30\u4e2d\uff0cArchAgent\u76f8\u6bd4\u73b0\u6709\u57fa\u51c6\u6709\u663e\u8457\u6539\u8fdb\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4f9d\u8d56\u4e0a\u4e0b\u6587\u80fd\u63d0\u9ad8\u751f\u4ea7\u7ea7\u4ed3\u5e93\u67b6\u6784\u751f\u6210\u7684\u51c6\u786e\u6027\uff0c\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u4ece\u9057\u7559\u9879\u76ee\u4e2d\u6709\u6548\u6062\u590d\u5173\u952e\u4e1a\u52a1\u903b\u8f91\u7684\u80fd\u529b\u3002", "conclusion": "ArchAgent\u901a\u8fc7\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u9057\u7559\u8f6f\u4ef6\u67b6\u6784\u6062\u590d\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u591a\u89c6\u56fe\u67b6\u6784\u91cd\u6784\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6062\u590d\u4e1a\u52a1\u5173\u952e\u903b\u8f91\u7684\u6709\u6548\u6027\u3002\u76f8\u5173\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "topic": "code agent"}}
{"id": "2601.13015", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13015", "abs": "https://arxiv.org/abs/2601.13015", "authors": ["Nowfel Mashnoor", "Mohammad Akyash", "Hadi Kamali", "Kimia Azar"], "title": "MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code Generation", "comment": null, "summary": "The automated generation of hardware register-transfer level (RTL) code with large language models (LLMs) shows promise, yet current solutions struggle to produce syntactically and functionally correct code for complex digital designs. This paper introduces MeltRTL, a novel framework that integrates multi-expert attention with inference-time intervention (ITI) to significantly improve LLM-based RTL code generation accuracy without retraining the base model. MeltRTL introduces three key innovations: (1) A multi-expert attention architecture that dynamically routes design specifications to specialized expert networks, enabling targeted reasoning across various hardware categories; (2) An inference-time intervention mechanism that employs non-linear probes to detect and correct hardware-specific inaccuracies during generation; and (3) An efficient intervention framework that selectively operates on expert-specific attention heads with minimal computational overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96% synthesizability and 60% functional correctness, compared to the base LLM's 85.3% and 45.3%, respectively. These improvements are obtained entirely at inference time, with only 27% computational overhead and no model fine-tuning, making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation studies further show the complementary benefits of multi-expert architecture and ITI, highlighting their synergistic effects when combined.", "AI": {"tldr": "MeltRTL\u662f\u4e00\u4e2a\u7528\u4e8e\u786c\u4ef6RTL\u4ee3\u7801\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u6ce8\u610f\u529b\u67b6\u6784\u548c\u63a8\u7406\u65f6\u5e72\u9884\u673a\u5236\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86LLM\u751f\u6210RTL\u4ee3\u7801\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u786c\u4ef6RTL\u4ee3\u7801\u751f\u6210\u65b9\u6848\u5728\u751f\u6210\u590d\u6742\u6570\u5b57\u8bbe\u8ba1\u7684\u8bed\u6cd5\u548c\u529f\u80fd\u6b63\u786e\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5c31\u80fd\u63d0\u9ad8\u51c6\u786e\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMeltRTL\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u591a\u4e13\u5bb6\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u52a8\u6001\u8def\u7531\u8bbe\u8ba1\u89c4\u8303\u5230\u4e13\u4e1a\u4e13\u5bb6\u7f51\u7edc\uff1b2\uff09\u63a8\u7406\u65f6\u5e72\u9884\u673a\u5236\uff0c\u4f7f\u7528\u975e\u7ebf\u6027\u63a2\u9488\u68c0\u6d4b\u548c\u7ea0\u6b63\u786c\u4ef6\u7279\u5b9a\u9519\u8bef\uff1b3\uff09\u9ad8\u6548\u5e72\u9884\u6846\u67b6\uff0c\u9009\u62e9\u6027\u64cd\u4f5c\u4e13\u5bb6\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002", "result": "\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMeltRTL\u5b9e\u73b0\u4e8696%\u7684\u53ef\u7efc\u5408\u6027\u548c60%\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u76f8\u6bd4\u57fa\u7840LLM\u768485.3%\u548c45.3%\u6709\u663e\u8457\u63d0\u5347\u3002\u8fd9\u4e9b\u6539\u8fdb\u5b8c\u5168\u5728\u63a8\u7406\u65f6\u83b7\u5f97\uff0c\u4ec5\u589e\u52a027%\u8ba1\u7b97\u5f00\u9500\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u3002", "conclusion": "MeltRTL\u6846\u67b6\u901a\u8fc7\u591a\u4e13\u5bb6\u67b6\u6784\u548c\u63a8\u7406\u65f6\u5e72\u9884\u7684\u534f\u540c\u6548\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u751f\u6210RTL\u4ee3\u7801\u7684\u8d28\u91cf\uff0c\u4e14\u53ef\u76f4\u63a5\u90e8\u7f72\u5728\u73b0\u6709\u9884\u8bad\u7ec3LLM\u4e0a\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "topic": "code agent"}}
{"id": "2601.12259", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12259", "abs": "https://arxiv.org/abs/2601.12259", "authors": ["Jiashuo Liu", "Siyuan Chen", "Zaiyuan Wang", "Zhiyuan Zeng", "Jiacheng Guo", "Liang Hu", "Lingyue Yin", "Suozhi Huang", "Wenxin Hao", "Yang Yang", "Zerui Cheng", "Zixin Yao", "Lingyue Yin", "Haoxin Liu", "Jiayi Cheng", "Yuzhen Li", "Zezhong Ma", "Bingjie Wang", "Bingsen Qiu", "Xiao Liu", "Zeyang Zhang", "Zijian Liu", "Jinpeng Wang", "Mingren Yin", "Tianci He", "Yali Liao", "Yixiao Tian", "Zhenwei Zhu", "Anqi Dai", "Ge Zhang", "Jingkai Liu", "Kaiyuan Zhang", "Wenlong Wu", "Xiang Gao", "Xinjie Chen", "Zhixin Yao", "Zhoufutu Wen", "B. Aditya Prakash", "Jose Blanchet", "Mengdi Wang", "Nian Si", "Wenhao Huang"], "title": "FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains", "comment": "21 pages", "summary": "Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.", "AI": {"tldr": "FutureX-Pro\u6269\u5c55\u4e86FutureX\u7684\u901a\u7528\u672a\u6765\u9884\u6d4b\u57fa\u51c6\uff0c\u9488\u5bf9\u91d1\u878d\u3001\u96f6\u552e\u3001\u516c\u5171\u536b\u751f\u548c\u81ea\u7136\u707e\u5bb3\u56db\u4e2a\u9ad8\u4ef7\u503c\u5782\u76f4\u9886\u57df\u5efa\u7acb\u4e86\u4e13\u95e8\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u8bc4\u4f30\u5f53\u524dSOTA\u667a\u80fd\u4f53LLM\u5728\u8fd9\u4e9b\u5173\u952e\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u901a\u7528\u667a\u80fd\u4f53\u5728\u5f00\u653e\u9886\u57df\u641c\u7d22\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8d44\u672c\u5bc6\u96c6\u578b\u548c\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53LLM\u662f\u5426\u5177\u5907\u5de5\u4e1a\u90e8\u7f72\u6240\u9700\u7684\u9886\u57df\u57fa\u7840\u3002", "method": "\u57fa\u4e8eFutureX\u7684\u65e0\u6c61\u67d3\u3001\u5b9e\u65f6\u8bc4\u4f30\u7ba1\u9053\uff0c\u9488\u5bf9\u56db\u4e2a\u5782\u76f4\u9886\u57df\uff08\u91d1\u878d\u3001\u96f6\u552e\u3001\u516c\u5171\u536b\u751f\u3001\u81ea\u7136\u707e\u5bb3\uff09\u5efa\u7acb\u4e13\u95e8\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u5728\u5165\u95e8\u7ea7\u4f46\u57fa\u7840\u6027\u7684\u9884\u6d4b\u4efb\u52a1\u4e0a\u5bf9\u667a\u80fd\u4f53LLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0e\u9ad8\u4ef7\u503c\u5782\u76f4\u5e94\u7528\u6240\u9700\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u5f53\u524dSOTA\u667a\u80fd\u4f53LLM\u5728\u5de5\u4e1a\u90e8\u7f72\u51c6\u5907\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u667a\u80fd\u4f53LLM\u5728\u4e13\u4e1a\u5782\u76f4\u9886\u57df\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u9886\u57df\u57fa\u7840\u624d\u80fd\u6ee1\u8db3\u5de5\u4e1a\u90e8\u7f72\u7684\u8981\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2601.13097", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13097", "abs": "https://arxiv.org/abs/2601.13097", "authors": ["Elena Bruches", "Daniil Grebenkin", "Mikhail Klementev", "Vadim Alperovich", "Roman Derunets", "Dari Baturova", "Georgy Mkrtchyan", "Oleg Sedukhin", "Ivan Bondarenko", "Nikolay Bushkov", "Stanislav Moiseev"], "title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation", "comment": "This paper has been accepted for publication at the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.", "AI": {"tldr": "RM-RF\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u514d\u8fd0\u884c\u8bc4\u4f30\u81ea\u52a8\u751f\u6210\u7684\u5355\u5143\u6d4b\u8bd5\u3002\u5b83\u76f4\u63a5\u4ece\u6e90\u4ee3\u7801\u548c\u6d4b\u8bd5\u4ee3\u7801\u9884\u6d4b\u4e09\u4e2a\u6267\u884c\u76f8\u5173\u4fe1\u53f7\uff0c\u76f8\u6bd4\u4f20\u7edf\u7f16\u8bd1\u6267\u884c\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u751f\u6210\u8bc4\u4f30\u9700\u8981\u53cd\u590d\u7f16\u8bd1\u548c\u6267\u884c\u6d4b\u8bd5\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u57fa\u7840\u8bbe\u65bd\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u5927\u89c4\u6a21\u6d4b\u8bd5\u751f\u6210\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u5feb\u901f\u53cd\u9988\u3002", "method": "\u6784\u5efa\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff08Java\u3001Python\u3001Go\uff09\uff0c\u5305\u542b\u6e90\u6587\u4ef6\u3001\u6d4b\u8bd5\u6587\u4ef6\u548c\u5019\u9009\u6d4b\u8bd5\u6dfb\u52a0\u3002\u8bad\u7ec3RM-RF\u6a21\u578b\u4ece\u4ee3\u7801\u76f4\u63a5\u9884\u6d4b\u4e09\u4e2a\u6267\u884c\u4fe1\u53f7\uff1a\u7f16\u8bd1\u8fd0\u884c\u6210\u529f\u3001\u4ee3\u7801\u8986\u76d6\u7387\u63d0\u5347\u3001\u53d8\u5f02\u6740\u6b7b\u7387\u6539\u8fdb\u3002\u6d4b\u8bd5\u4e86\u591a\u79cd\u6a21\u578b\u5bb6\u65cf\u548c\u8c03\u4f18\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5168\u5fae\u8c03\u3001LoRA PEFT\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u9884\u6d4b\u76ee\u6807\u4e0a\u5e73\u5747F1\u8fbe\u52300.69\u3002\u76f8\u6bd4\u4f20\u7edf\u7f16\u8bd1\u6267\u884c\u5de5\u5177\uff0cRM-RF\u63d0\u4f9b\u663e\u8457\u66f4\u4f4e\u7684\u5ef6\u8fdf\u548c\u57fa\u7840\u8bbe\u65bd\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "RM-RF\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u751f\u6210\u8bc4\u4f30\uff0c\u4e3a\u5927\u89c4\u6a21\u6d4b\u8bd5\u751f\u6210\u548cRL\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u9ad8\u6548\u53cd\u9988\u673a\u5236\uff0c\u5e73\u8861\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "2601.11895", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11895", "abs": "https://arxiv.org/abs/2601.11895", "authors": ["Pareesa Ameneh Golnari", "Adarsh Kumarappan", "Wen Wen", "Xiaoyu Liu", "Gabriel Ryan", "Yuting Sun", "Shengyu Fu", "Elsie Nallipogu"], "title": "DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models", "comment": null, "summary": "DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.", "AI": {"tldr": "DevBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u5f00\u53d1\u8005\u9065\u6d4b\u6570\u636e\u7684\u4ee3\u7801\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1800\u4e2a\u8bc4\u4f30\u5b9e\u4f8b\uff0c\u6db5\u76d66\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c6\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u5f3a\u8c03\u751f\u6001\u6709\u6548\u6027\u5e76\u907f\u514d\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u751f\u6001\u6709\u6548\u6027\uff08\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u5f00\u53d1\u573a\u666f\uff09\uff0c\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u63d0\u4f9b\u8be6\u7ec6\u7684\u8bca\u65ad\u4fe1\u606f\u6765\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u548c\u6539\u8fdb\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u5f00\u53d1\u8005\u9065\u6d4b\u6570\u636e\u6784\u5efa\u8bc4\u4f30\u5b9e\u4f8b\uff0c\u7ed3\u5408\u529f\u80fd\u6b63\u786e\u6027\u3001\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u548cLLM\u8bc4\u4f30\uff08\u5173\u6ce8\u6709\u7528\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff09\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5bf99\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u8bed\u6cd5\u7cbe\u5ea6\u3001\u8bed\u4e49\u63a8\u7406\u548c\u5b9e\u9645\u6548\u7528\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u9488\u5bf9\u6027\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "conclusion": "DevBench\u63d0\u4f9b\u4e86\u5176\u4ed6\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u7f3a\u5931\u4f46\u5b9e\u9645\u90e8\u7f72\u548c\u9488\u5bf9\u6027\u6a21\u578b\u5f00\u53d1\u5fc5\u9700\u7684\u8be6\u7ec6\u8bca\u65ad\u4fe1\u606f\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u548c\u6539\u8fdb\u3002", "topic": "swe benchmark"}}
{"id": "2601.13118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13118", "abs": "https://arxiv.org/abs/2601.13118", "authors": ["Alessandro Midolo", "Alessandro Giagnorio", "Fiorella Zampetti", "Rosalia Tufano", "Gabriele Bavota", "Massimiliano Di Penta"], "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization", "comment": null, "summary": "Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e8610\u6761\u9488\u5bf9\u4ee3\u7801\u751f\u6210\u7684\u63d0\u793a\u4f18\u5316\u6307\u5357\uff0c\u901a\u8fc7\u6d4b\u8bd5\u9a71\u52a8\u65b9\u6cd5\u81ea\u52a8\u4f18\u5316\u63d0\u793a\uff0c\u5e76\u8bc4\u4f30\u4e86\u5f00\u53d1\u8005\u5bf9\u8fd9\u4e9b\u6307\u5357\u7684\u4f7f\u7528\u60c5\u51b5\u548c\u611f\u77e5\u6709\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5e7f\u6cdb\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u4ee3\u7801\u751f\u6210\u7684\u5177\u4f53\u63d0\u793a\u4f18\u5316\u6307\u5357\u3002\u73b0\u6709\u7814\u7a76\u663e\u793a\u5408\u9002\u7684\u63d0\u793a\u5de5\u7a0b\u80fd\u6539\u5584\u4ee3\u7801\u751f\u6210\uff0c\u4f46\u5f00\u53d1\u8005\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u6307\u5bfc\u539f\u5219\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u7684\u6d4b\u8bd5\u9a71\u52a8\u65b9\u6cd5\u81ea\u52a8\u4f18\u5316\u4ee3\u7801\u751f\u6210\u63d0\u793a\uff0c\u5206\u6790\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u6539\u8fdb\u9879\uff0c\u4ece\u4e2d\u63d0\u70bc\u51fa10\u6761\u63d0\u793a\u6539\u8fdb\u6307\u5357\u3002\u7136\u540e\u5bf950\u540d\u4ece\u4e1a\u8005\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e86\u89e3\u4ed6\u4eec\u5bf9\u8fd9\u4e9b\u6307\u5357\u7684\u4f7f\u7528\u60c5\u51b5\u548c\u611f\u77e5\u6709\u7528\u6027\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u51fa10\u6761\u6709\u6548\u7684\u63d0\u793a\u6539\u8fdb\u6307\u5357\uff0c\u6d89\u53caI/O\u89c4\u8303\u3001\u524d\u540e\u6761\u4ef6\u3001\u793a\u4f8b\u63d0\u4f9b\u3001\u7ec6\u8282\u63cf\u8ff0\u548c\u6d88\u9664\u6b67\u4e49\u7b49\u65b9\u9762\u3002\u4ece\u4e1a\u8005\u8bc4\u4f30\u663e\u793a\uff0c\u4ed6\u4eec\u5bf9\u8fd9\u4e9b\u6307\u5357\u7684\u611f\u77e5\u6709\u7528\u6027\u5e76\u4e0d\u603b\u662f\u4e0e\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ece\u4e1a\u8005\u3001\u6559\u80b2\u8005\u548cLLM\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u63d0\u793a\u4f18\u5316\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "2601.11956", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11956", "abs": "https://arxiv.org/abs/2601.11956", "authors": ["Yuyin Lu", "Ziran Liang", "Yanghui Rao", "Wenqi Fan", "Fu Lee Wang", "Qing Li"], "title": "Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence", "comment": null, "summary": "Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.", "AI": {"tldr": "DoublyCal\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u6821\u51c6\u539f\u5219\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4ee3\u7406\u6a21\u578b\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u8bc1\u636e\u548c\u6821\u51c6\u7684\u8bc1\u636e\u7f6e\u4fe1\u5ea6\uff0c\u6307\u5bfc\u9ed1\u76d2LLM\u4ea7\u751f\u66f4\u51c6\u786e\u4e14\u7f6e\u4fe1\u5ea6\u53ef\u8ffd\u6eaf\u7684\u9884\u6d4b", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u65b9\u6cd5\u65e0\u6cd5\u91cf\u5316\u68c0\u7d22\u8bc1\u636e\u548cLLM\u63a8\u7406\u4e2d\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027", "method": "\u57fa\u4e8e\u53cc\u91cd\u6821\u51c6\u539f\u5219\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4ee3\u7406\u6a21\u578b\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u8bc1\u636e\u5e76\u6821\u51c6\u8bc1\u636e\u7f6e\u4fe1\u5ea6\uff0c\u7136\u540e\u7528\u6821\u51c6\u540e\u7684\u8bc1\u636e\u6307\u5bfc\u9ed1\u76d2LLM\u8fdb\u884c\u63a8\u7406", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDoublyCal\u663e\u8457\u63d0\u9ad8\u4e86\u9ed1\u76d2LLM\u7684\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684token\u6210\u672c", "conclusion": "DoublyCal\u6846\u67b6\u901a\u8fc7\u91cf\u5316\u8bc1\u636e\u4e0d\u786e\u5b9a\u6027\u5e76\u5efa\u7acb\u53ef\u8ffd\u6eaf\u7684\u7f6e\u4fe1\u5ea6\uff0c\u6709\u6548\u63d0\u9ad8\u4e86LLM\u63a8\u7406\u7684\u53ef\u4fe1\u5ea6", "topic": "agent analysis"}}
{"id": "2601.12294", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12294", "abs": "https://arxiv.org/abs/2601.12294", "authors": ["Dawei Li", "Yuguang Yao", "Zhen Tan", "Huan Liu", "Ruocheng Guo"], "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents", "comment": "under review", "summary": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ToolPRMBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u57fa\u51c6\u57fa\u4e8e\u591a\u4e2a\u4ee3\u8868\u6027\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6784\u5efa\uff0c\u5c06\u667a\u80fd\u4f53\u8f68\u8ff9\u8f6c\u6362\u4e3a\u6b65\u9aa4\u7ea7\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5305\u542b\u4ea4\u4e92\u5386\u53f2\u3001\u6b63\u786e\u52a8\u4f5c\u3001\u5408\u7406\u4f46\u4e0d\u6b63\u786e\u7684\u66ff\u4ee3\u52a8\u4f5c\u53ca\u76f8\u5173\u5de5\u5177\u5143\u6570\u636e\u3002", "motivation": "\u867d\u7136\u5956\u52b1\u5f15\u5bfc\u641c\u7d22\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u63d0\u4f9b\u6b65\u9aa4\u7ea7\u5956\u52b1\u6765\u589e\u5f3a\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\uff0c\u4f46\u5728\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u53ef\u9760\u7684PRMs\u8bc4\u4f30\u57fa\u51c6\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u591f\u5168\u9762\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cfPRMs\u5728\u5b9e\u9645\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "1. \u57fa\u4e8e\u591a\u4e2a\u4ee3\u8868\u6027\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6784\u5efaToolPRMBench\uff1b2. \u5c06\u667a\u80fd\u4f53\u8f68\u8ff9\u8f6c\u6362\u4e3a\u6b65\u9aa4\u7ea7\u6d4b\u8bd5\u7528\u4f8b\uff1b3. \u4f7f\u7528\u79bb\u7ebf\u91c7\u6837\u9694\u79bb\u5c40\u90e8\u5355\u6b65\u9519\u8bef\uff1b4. \u4f7f\u7528\u5728\u7ebf\u91c7\u6837\u6355\u6349\u5b8c\u6574\u667a\u80fd\u4f53\u8f68\u8ff9\u4e2d\u7684\u5b9e\u9645\u591a\u6b65\u5931\u8d25\uff1b5. \u63d0\u51fa\u591aLLM\u9a8c\u8bc1\u6d41\u7a0b\u51cf\u5c11\u6807\u7b7e\u566a\u58f0\u5e76\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5728\u5927\u8bed\u8a00\u6a21\u578b\u3001\u901a\u7528PRMs\u548c\u5de5\u5177\u4e13\u7528PRMs\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aPRMs\u6709\u6548\u6027\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u5e76\u7a81\u663e\u4e86\u5de5\u5177\u4e13\u7528PRMs\u5728\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "ToolPRMBench\u4e3a\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u53ef\u9760\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u4e0d\u540cPRMs\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u8bc1\u660e\u4e86\u5de5\u5177\u4e13\u7528PRMs\u7684\u4f18\u52bf\u3002\u8be5\u57fa\u51c6\u5c06\u4fc3\u8fdb\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u4e2dPRMs\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5f00\u53d1\u3002", "topic": "agent analysis"}}
{"id": "2601.11957", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11957", "abs": "https://arxiv.org/abs/2601.11957", "authors": ["Bingxuan Li", "Jeonghwan Kim", "Cheng Qian", "Xiusi Chen", "Eitan Anzenberg", "Niran Kundapur", "Heng Ji"], "title": "PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning", "comment": null, "summary": "Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CalConflictBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u4ee3\u7406\u5728\u65e5\u5386\u51b2\u7a81\u89e3\u51b3\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f00\u53d1\u4e86PEARL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u4e13\u4e1a\u4eba\u58eb\u7ecf\u5e38\u9762\u4e34\u65e5\u5386\u9080\u8bf7\u51b2\u7a81\uff0c\u9700\u8981\u53cd\u590d\u51b3\u5b9a\u53c2\u52a0\u3001\u91cd\u65b0\u5b89\u6392\u6216\u62d2\u7edd\u54ea\u4e9b\u4f1a\u8bae\u3002\u81ea\u52a8\u5316\u8fd9\u4e00\u51b3\u7b56\u8fc7\u7a0b\u5f88\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u8c03\u5ea6\u5de5\u4f5c\u8017\u65f6\u4e14\u4eba\u5de5\u59d4\u6258\u96be\u4ee5\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u8bed\u8a00\u4ee3\u7406\u80fd\u5426\u6709\u6548\u7ba1\u7406\u65f6\u95f4\u3002", "method": "\u9996\u5148\u5f15\u5165CalConflictBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u987a\u5e8f\u65b9\u5f0f\u5448\u73b0\u51b2\u7a81\u5e76\u63d0\u4f9b\u6bcf\u8f6e\u53cd\u9988\u3002\u7136\u540e\u63d0\u51faPEARL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u8bb0\u5fc6\u6a21\u5757\u548c\u4f18\u5316\u7684\u8f6e\u6b21\u5956\u52b1\u8bbe\u8ba1\u6765\u589e\u5f3a\u8bed\u8a00\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u9010\u6b65\u63a8\u65ad\u5e76\u9002\u5e94\u7528\u6237\u504f\u597d\u3002", "result": "\u5f53\u524dLLM\u4ee3\u7406\u8868\u73b0\u4e0d\u4f73\uff0c\u5982Qwen-3-30B-Think\u5e73\u5747\u9519\u8bef\u7387\u8fbe35%\u3002PEARL\u6846\u67b6\u5b9e\u73b0\u4e860.76\u7684\u9519\u8bef\u51cf\u5c11\u7387\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5e73\u5747\u9519\u8bef\u7387\u63d0\u534755%\u3002", "conclusion": "\u65e5\u5386\u51b2\u7a81\u89e3\u51b3\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u89c6\u91ce\u4efb\u52a1\uff0c\u5f53\u524d\u8bed\u8a00\u4ee3\u7406\u8868\u73b0\u6709\u9650\uff0c\u4f46\u901a\u8fc7PEARL\u8fd9\u6837\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u63a8\u65ad\u548c\u9002\u5e94\u7528\u6237\u504f\u597d\u3002", "topic": "agent analysis"}}
{"id": "2601.13139", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13139", "abs": "https://arxiv.org/abs/2601.13139", "authors": ["Alessandro Midolo", "Emiliano Tramontana", "Massimiliano Di Penta"], "title": "From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability", "comment": null, "summary": "Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.", "AI": {"tldr": "GPT-4o\u5728Python\u4ee3\u7801\u91cd\u6784\u4e2d\u80fd\u4fdd\u6301\u884c\u4e3a\u6b63\u786e\u6027\u5e76\u6539\u5584\u4ee3\u7801\u8d28\u91cf\uff0c\u4f46\u4f1a\u964d\u4f4e\u53ef\u8bfb\u6027", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u81ea\u52a8\u5316\u91cd\u6784\u5de5\u5177\uff0c\u4f46\u5176\u5b9e\u7528\u6027\u6709\u9650\u3002LLMs\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u91cd\u6784\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46LLM\u9a71\u52a8\u65b9\u6cd5\u7684\u6548\u679c\u8bc4\u4f30\u4ecd\u5b58\u5728\u672a\u89e3\u95ee\u9898", "method": "\u4f7f\u7528GPT-4o\u5bf9ClassEval\u57fa\u51c6\u4e2d\u7684100\u4e2aPython\u7c7b\u8fdb\u884c\u7efc\u5408\u5b9e\u8bc1\u7814\u7a76\uff0c\u63a2\u7d22Fowler\u76ee\u5f55\u542f\u53d1\u7684\u7c7b\u7ea7\u91cd\u6784\uff0c\u4ece\u4e09\u4e2a\u4e92\u8865\u89c6\u89d2\u8bc4\u4f30\uff1a\u884c\u4e3a\u6b63\u786e\u6027\uff08\u5355\u5143\u6d4b\u8bd5\u9a8c\u8bc1\uff09\u3001\u4ee3\u7801\u8d28\u91cf\uff08Pylint\u3001Flake8\u3001SonarCloud\u8bc4\u4f30\uff09\u3001\u53ef\u8bfb\u6027\uff08\u6700\u5148\u8fdb\u7684\u53ef\u8bfb\u6027\u5de5\u5177\u6d4b\u91cf\uff09", "result": "GPT-4o\u901a\u5e38\u80fd\u4ea7\u751f\u4fdd\u6301\u884c\u4e3a\u6b63\u786e\u7684\u91cd\u6784\uff0c\u51cf\u5c11\u4ee3\u7801\u5f02\u5473\u5e76\u6539\u5584\u8d28\u91cf\u6307\u6807\uff0c\u4f46\u4ee3\u4ef7\u662f\u964d\u4f4e\u4e86\u53ef\u8bfb\u6027", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aLLMs\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u91cd\u6784\u4e2d\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u65b0\u8bc1\u636e\uff0c\u7a81\u51fa\u4e86\u5c06LLMs\u96c6\u6210\u5230\u5b9e\u9645\u91cd\u6784\u5de5\u4f5c\u6d41\u7684\u65b9\u5411", "topic": "swe application"}}
{"id": "2601.11969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11969", "abs": "https://arxiv.org/abs/2601.11969", "authors": ["Zecheng Tang", "Baibei Ji", "Ruoxi Sun", "Haitian Wang", "WangJie You", "Zhang Yijun", "Wenpeng Zhu", "Ji Qi", "Juntao Li", "Min Zhang"], "title": "$\\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models", "comment": null, "summary": "Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.", "AI": {"tldr": "MemoryRewardBench\uff1a\u9996\u4e2a\u7cfb\u7edf\u7814\u7a76\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u957f\u65f6\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d68K-128K\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5305\u542b10\u79cd\u4e0d\u540c\u8bb0\u5fc6\u6a21\u5f0f\uff0c\u53d1\u73b0\u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u3002", "motivation": "\u968f\u7740\u8bb0\u5fc6\u4e2d\u5fc3\u673a\u5236\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6709\u6548\u7684\u8bb0\u5fc6\u7ba1\u7406\u6210\u4e3aLLM\u8de8\u5e8f\u5217\u4f20\u64ad\u4fe1\u606f\u7684\u5173\u952e\u80fd\u529b\u3002\u9700\u8981\u5956\u52b1\u6a21\u578b\u6765\u81ea\u52a8\u53ef\u9760\u5730\u8bc4\u4f30\u8bb0\u5fc6\u8d28\u91cf\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e00\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u6784\u5efaMemoryRewardBench\u57fa\u51c6\uff0c\u8986\u76d6\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5305\u542b10\u79cd\u4e0d\u540c\u8bb0\u5fc6\u7ba1\u7406\u6a21\u5f0f\u7684\u8bbe\u7f6e\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4ece8K\u5230128K tokens\u3002\u572813\u4e2a\u524d\u6cbf\u5956\u52b1\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u6b63\u5728\u7f29\u5c0f\uff0c\u65b0\u4e00\u4ee3\u6a21\u578b\u65e0\u8bba\u53c2\u6570\u6570\u91cf\u5982\u4f55\u90fd\u6301\u7eed\u8d85\u8d8a\u524d\u4ee3\u3002\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u5956\u52b1\u6a21\u578b\u5728\u8bc4\u4f30LLM\u8bb0\u5fc6\u7ba1\u7406\u65b9\u9762\u7684\u80fd\u529b\u548c\u57fa\u672c\u5c40\u9650\u6027\u3002", "conclusion": "MemoryRewardBench\u4e3a\u7cfb\u7edf\u7814\u7a76\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u957f\u65f6\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u53d1\u5c55\u8d8b\u52bf\u548c\u73b0\u6709\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.13240", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13240", "abs": "https://arxiv.org/abs/2601.13240", "authors": ["Xue Jiang", "Jiaru Qian", "Xianjie Shi", "Chenjie Li", "Hao Zhu", "Ziyu Wang", "Jielun Zhang", "Zheyu Zhao", "Kechi Zhang", "Jia Li", "Wenpin Jiao", "Zhi Jin", "Ge Li", "Yihong Dong"], "title": "KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?", "comment": null, "summary": "Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.", "AI": {"tldr": "KOCO-BENCH\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u9886\u57df\u4e13\u4e1a\u5316\u65b9\u6cd5\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b6\u4e2a\u65b0\u5174\u9886\u57df\u300111\u4e2a\u8f6f\u4ef6\u6846\u67b6\u548c25\u4e2a\u9879\u76ee\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u548c\u591a\u7c92\u5ea6\u4efb\u52a1\u8bc4\u4f30LLM\u83b7\u53d6\u548c\u5e94\u7528\u9886\u57df\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u7279\u5b9a\u4ee3\u7801\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u9886\u57df\u4e13\u4e1a\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b83\u4eec\u53ea\u5173\u6ce8LLM\u62e5\u6709\u4ec0\u4e48\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u5982\u4f55\u83b7\u53d6\u548c\u5e94\u7528\u65b0\u77e5\u8bc6\uff0c\u4e14\u7f3a\u4e4f\u660e\u786e\u7684\u77e5\u8bc6\u5e93\u6765\u5f00\u53d1\u9886\u57df\u4e13\u4e1a\u5316\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b6\u4e2a\u65b0\u5174\u9886\u57df\u300111\u4e2a\u8f6f\u4ef6\u6846\u67b6\u548c25\u4e2a\u9879\u76ee\u7684\u57fa\u51c6\uff0c\u63d0\u4f9b\u7cbe\u5fc3\u7b56\u5212\u7684\u77e5\u8bc6\u5e93\uff0c\u8bbe\u8ba1\u591a\u7c92\u5ea6\u8bc4\u4f30\u4efb\u52a1\uff1a\u9886\u57df\u4ee3\u7801\u751f\u6210\uff08\u4ece\u51fd\u6570\u7ea7\u5230\u9879\u76ee\u7ea7\uff0c\u5305\u542b\u4e25\u683c\u6d4b\u8bd5\u5957\u4ef6\uff09\u548c\u9886\u57df\u77e5\u8bc6\u7406\u89e3\uff08\u901a\u8fc7\u591a\u9879\u9009\u62e9\u95ee\u7b54\uff09\u3002", "result": "KOCO-BENCH\u5bf9\u6700\u5148\u8fdb\u7684LLM\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u5373\u4f7f\u5e94\u7528\u9886\u57df\u4e13\u4e1a\u5316\u65b9\u6cd5\uff08\u5982SFT\u3001RAG\u3001kNN-LM\uff09\uff0c\u6539\u8fdb\u4ecd\u7136\u6709\u9650\u3002\u6700\u4f73\u6027\u80fd\u7684\u7f16\u7801\u4ee3\u7406Claude Code\u4ec5\u8fbe\u523034.2%\uff0c\u8868\u660e\u9700\u8981\u66f4\u6709\u6548\u7684\u9886\u57df\u4e13\u4e1a\u5316\u65b9\u6cd5\u3002", "conclusion": "KOCO-BENCH\u586b\u8865\u4e86\u8bc4\u4f30LLM\u9886\u57df\u4e13\u4e1a\u5316\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u57fa\u51c6\u3001\u8bc4\u4f30\u4ee3\u7801\u548c\u57fa\u7ebf\u3002", "topic": "swe benchmark"}}
{"id": "2601.11960", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11960", "abs": "https://arxiv.org/abs/2601.11960", "authors": ["Jingchu Wang", "Bingbing Xu", "Yige Yuan", "Bin Xie", "Xiaoqian Sun", "Huawei Shen"], "title": "R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning", "comment": null, "summary": "Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.", "AI": {"tldr": "\u63d0\u51faR\u00b2PO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u6b8b\u5deeRollout-Head\u89e3\u8026\u8bad\u7ec3\u8f68\u8ff9\u548c\u63a8\u7406\u54cd\u5e94\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7b56\u7565\u540c\u65f6\u4ea7\u751f\u63a8\u7406\u54cd\u5e94\u548c\u8bad\u7ec3\u4f18\u5316\u8f68\u8ff9\uff0c\u5bfc\u81f4\u751f\u6210\u7a33\u5b9a\u63a8\u7406\u54cd\u5e94\u4e0e\u591a\u6837\u5316\u8bad\u7ec3\u8f68\u8ff9\u4e4b\u95f4\u7684\u76ee\u6807\u51b2\u7a81\uff0c\u9020\u6210\u63a2\u7d22\u4e0d\u8db3\uff0c\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faR\u00b2PO\uff08Residual Rollout Policy Optimization\uff09\uff0c\u5728\u7b56\u7565\u4e4b\u4e0a\u5f15\u5165\u8f7b\u91cf\u7ea7\u6b8b\u5deeRollout-Head\uff0c\u5c06\u8bad\u7ec3\u8f68\u8ff9\u4e0e\u63a8\u7406\u54cd\u5e94\u89e3\u8026\uff0c\u5b9e\u73b0\u8bad\u7ec3\u671f\u95f4\u53ef\u63a7\u7684\u8f68\u8ff9\u591a\u6837\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u751f\u6210\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728MATH-500\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.1%\uff0c\u5728APPS\u4e0a\u63d0\u53472.4%\uff0c\u540c\u65f6\u51cf\u5c11\u683c\u5f0f\u9519\u8bef\u5e76\u7f13\u89e3\u957f\u5ea6\u504f\u5dee\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316\u3002", "conclusion": "R\u00b2PO\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u8f68\u8ff9\u548c\u63a8\u7406\u54cd\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.13384", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13384", "abs": "https://arxiv.org/abs/2601.13384", "authors": ["Jiajun Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Yuheng Jing", "Zeyao Ma", "Tianyi Bai", "Zilei Wang", "Qiang Liu", "Liang Wang", "Binyuan Hui", "Junyang Lin"], "title": "From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning", "comment": null, "summary": "The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.", "AI": {"tldr": "\u63d0\u51faSearch-and-Replace Infilling (SRI)\u6846\u67b6\uff0c\u5c06\u4ee3\u7406\u9a8c\u8bc1-\u7f16\u8f91\u673a\u5236\u5185\u5316\u4e3a\u5355\u6b21\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4f20\u7edfFIM\u4ee3\u7801\u8865\u5168\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4ee3\u7801\u8865\u5168\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfFill-in-the-Middle (FIM)\u4ee3\u7801\u8865\u5168\u8303\u5f0f\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u65e0\u6cd5\u4fee\u6b63\u4e0a\u4e0b\u6587\u9519\u8bef\uff1b2\uff09\u4f9d\u8d56\u672a\u5bf9\u9f50\u3001\u4e0d\u5b89\u5168\u7684Base\u6a21\u578b\u3002\u800cChat LLMs\u867d\u7136\u5b89\u5168\uff0c\u4f46\u6027\u80fd\u4e0b\u964d\uff1bAgentic\u5de5\u4f5c\u6d41\u867d\u7136\u7075\u6d3b\uff0c\u4f46\u5ef6\u8fdf\u8fc7\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u517c\u987e\u6027\u80fd\u3001\u5b89\u5168\u548c\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSearch-and-Replace Infilling (SRI)\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u641c\u7d22\u9636\u6bb5\u7ed3\u6784\u5316\u5730\u5b9a\u4f4d\u7f16\u8f91\u4f4d\u7f6e\uff0c\u5c06\u4ee3\u7406\u9a8c\u8bc1-\u7f16\u8f91\u673a\u5236\u5185\u5316\u4e3a\u7edf\u4e00\u7684\u5355\u6b21\u63a8\u7406\u8fc7\u7a0b\u3002\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6SRI-200K\uff0c\u5e76\u5fae\u8c03SRI-Coder\u7cfb\u5217\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5c06\u9759\u6001\u586b\u5145\u6269\u5c55\u5230\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u7f16\u8f91\u3002", "result": "\u4ec5\u4f7f\u75282\u4e07\u6837\u672c\u5fae\u8c03\uff0cSRI-Coder\u5c31\u80fd\u4f7fChat\u6a21\u578b\u8d85\u8d8a\u5176Base\u5bf9\u5e94\u6a21\u578b\u7684\u8865\u5168\u6027\u80fd\u3002\u4e0eFIM\u98ce\u683c\u5fae\u8c03\u4e0d\u540c\uff0cSRI\u4fdd\u6301\u901a\u7528\u7f16\u7801\u80fd\u529b\uff0c\u63a8\u7406\u5ef6\u8fdf\u4e0e\u6807\u51c6FIM\u76f8\u5f53\u3002\u6574\u4e2aQwen3-Coder\u7cfb\u5217\u90fd\u96c6\u6210\u4e86SRI\u80fd\u529b\u3002", "conclusion": "SRI\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfFIM\u4ee3\u7801\u8865\u5168\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8865\u5168\u8d28\u91cf\uff0c\u4e3a\u5f00\u53d1\u8005\u793e\u533a\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u81ea\u52a8\u8865\u5168\u548c\u8f85\u52a9\u5f00\u53d1\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "2601.12410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12410", "abs": "https://arxiv.org/abs/2601.12410", "authors": ["Dingyi Yang", "Junqi Zhao", "Xue Li", "Ce Li", "Boyang Li"], "title": "Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation", "comment": "23 pages, 11 figures", "summary": "Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.", "AI": {"tldr": "LLMs\u5728\u77e5\u8bc6\u72b6\u6001\u8ffd\u8e2a\u548c\u4f30\u8ba1\u4efb\u52a1\u4e0a\u8868\u73b0\u63a5\u8fd1\u968f\u673a\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u672a\u6765\u7814\u7a76\u5e94\u66f4\u91cd\u89c6\u77e5\u8bc6\u4f30\u8ba1\u548c\u610f\u56fe\u7406\u89e3\u80fd\u529b", "motivation": "\u8ba4\u77e5\u4eba\u7c7b\u5b66\u8ba4\u4e3a\u4eba\u7c7b\u667a\u80fd\u7684\u5173\u952e\u5728\u4e8e\u63a8\u65ad\u4ed6\u4eba\u77e5\u8bc6\u72b6\u6001\u548c\u7406\u89e3\u610f\u56fe\u7684\u80fd\u529b\uff0c\u800c\u9ed1\u7329\u7329\u7b49\u52a8\u7269\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLM\u5728\u77e5\u8bc6\u72b6\u6001\u8ffd\u8e2a\u548c\u4f30\u8ba1\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u4e24\u4e2a\u4efb\u52a1\uff1a(1)\u68c0\u6d4b\u6545\u4e8b\u89d2\u8272\u662f\u5426\u901a\u8fc7\u884c\u52a8\u5c55\u793a\u4e86\u4ed6\u4eec\u672c\u4e0d\u5e94\u62e5\u6709\u7684\u77e5\u8bc6\uff1b(2)\u9884\u6d4b\u6545\u4e8b\u89d2\u8272\u57fa\u4e8e\u81ea\u8eab\u77e5\u8bc6\uff08\u800c\u975e\u5ba2\u89c2\u4e8b\u5b9e\uff09\u7684\u4e0b\u4e00\u6b65\u884c\u52a8\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u63a5\u8fd1\u968f\u673a\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "LLM\u5728\u77e5\u8bc6\u72b6\u6001\u8ffd\u8e2a\u548c\u610f\u56fe\u7406\u89e3\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u672a\u6765LLM\u7814\u7a76\u5e94\u66f4\u91cd\u89c6\u77e5\u8bc6\u4f30\u8ba1\u548c\u610f\u56fe\u7406\u89e3\u80fd\u529b\u7684\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2601.12008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12008", "abs": "https://arxiv.org/abs/2601.12008", "authors": ["Shiqing Gao", "Yihang Zhou", "Shuai Shao", "Haoyu Luo", "Yiheng Bing", "Jiaxin Ding", "Luoyi Fu", "Xinbing Wang"], "title": "Extreme Value Policy Optimization for Safe Reinforcement Learning", "comment": "Published in the 42nd International Conference on Machine Learning (ICML 2025)", "summary": "Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.", "AI": {"tldr": "EVO\u7b97\u6cd5\u5229\u7528\u6781\u503c\u7406\u8bba\u5efa\u6a21\u6781\u7aef\u5956\u52b1\u548c\u6210\u672c\u6837\u672c\uff0c\u901a\u8fc7\u6781\u7aef\u5206\u4f4d\u6570\u4f18\u5316\u76ee\u6807\u548c\u6781\u7aef\u4f18\u5148\u7ea7\u56de\u653e\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u76f8\u6bd4\u671f\u671b\u7ea6\u675f\u65b9\u6cd5\u6709\u66f4\u4f4e\u7684\u7ea6\u675f\u8fdd\u53cd\u6982\u7387\u3002", "motivation": "\u4f20\u7edf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u671f\u671b\u7d2f\u79ef\u6210\u672c\u4f5c\u4e3a\u7ea6\u675f\uff0c\u4f46\u5ffd\u7565\u4e86\u5c3e\u90e8\u5206\u5e03\u4e2d\u7f55\u89c1\u4f46\u5f71\u54cd\u5de8\u5927\u7684\u6781\u7aef\u503c\u4e8b\u4ef6\uff08\u5982\u9ed1\u5929\u9e45\u4e8b\u4ef6\uff09\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u7ea6\u675f\u8fdd\u53cd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u6781\u7aef\u503c\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faEVO\u7b97\u6cd5\uff1a1\uff09\u5229\u7528\u6781\u503c\u7406\u8bba\u5efa\u6a21\u6781\u7aef\u5956\u52b1\u548c\u6210\u672c\u6837\u672c\uff1b2\uff09\u5f15\u5165\u6781\u7aef\u5206\u4f4d\u6570\u4f18\u5316\u76ee\u6807\uff0c\u663e\u5f0f\u6355\u6349\u6210\u672c\u5c3e\u90e8\u5206\u5e03\u4e2d\u7684\u6781\u7aef\u6837\u672c\uff1b3\uff09\u63d0\u51fa\u6781\u7aef\u4f18\u5148\u7ea7\u56de\u653e\u673a\u5236\uff0c\u5728\u7ecf\u9a8c\u56de\u653e\u4e2d\u653e\u5927\u7f55\u89c1\u4f46\u9ad8\u5f71\u54cd\u6781\u7aef\u6837\u672c\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u7406\u8bba\u5206\u6790\uff1a\u5efa\u7acb\u4e86\u7b56\u7565\u66f4\u65b0\u671f\u95f4\u671f\u671b\u7ea6\u675f\u8fdd\u53cd\u7684\u4e0a\u754c\uff0c\u4fdd\u8bc1\u5728\u96f6\u8fdd\u53cd\u5206\u4f4d\u6570\u6c34\u5e73\u4e0a\u7684\u4e25\u683c\u7ea6\u675f\u6ee1\u8db3\u3002\u5b9e\u9a8c\u8bc1\u660e\uff1aEVO\u5728\u8bad\u7ec3\u671f\u95f4\u663e\u8457\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u7b56\u7565\u6027\u80fd\uff0c\u76f8\u6bd4\u671f\u671b\u7ea6\u675f\u65b9\u6cd5\u6709\u66f4\u4f4e\u7684\u7ea6\u675f\u8fdd\u53cd\u6982\u7387\uff0c\u6bd4\u5206\u4f4d\u6570\u56de\u5f52\u65b9\u6cd5\u6709\u66f4\u4f4e\u7684\u65b9\u5dee\u3002", "conclusion": "EVO\u7b97\u6cd5\u901a\u8fc7\u6781\u503c\u7406\u8bba\u6709\u6548\u5904\u7406\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6781\u7aef\u503c\u4e8b\u4ef6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.12444", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.12444", "abs": "https://arxiv.org/abs/2601.12444", "authors": ["Hui Yang", "Jiaoyan Chen", "Uli Sattler"], "title": "Large Language Model for OWL Proofs", "comment": null, "summary": "The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728OWL\u672c\u4f53\u8bba\u4e2d\u751f\u6210\u8bc1\u660e\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u903b\u8f91\u590d\u6742\u6027\u662f\u5f71\u54cd\u6027\u80fd\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u975e\u8868\u793a\u683c\u5f0f\uff0c\u540c\u65f6\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u5fe0\u5b9e\u3001\u4eba\u7c7b\u53ef\u8bfb\u7684\u8bc1\u660e\uff08\u89e3\u91ca\u7ed3\u8bba\u4e3a\u4f55\u6210\u7acb\uff09\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728OWL\u672c\u4f53\u8bba\u8fd9\u79cd\u590d\u6742\u77e5\u8bc6\u8868\u793a\u548c\u63a8\u7406\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6784\u5efa\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5728OWL\u672c\u4f53\u8bba\u80cc\u666f\u4e0b\u8bc4\u4f30LLMs\u7684\u8bc1\u660e\u751f\u6210\u80fd\u529b\u3002\u8bc4\u4f30\u5305\u62ec\u4e09\u4e2a\u987a\u5e8f\u4efb\u52a1\uff1a\u63d0\u53d6\u3001\u7b80\u5316\u548c\u89e3\u91ca\uff0c\u4ee5\u53ca\u4e00\u4e2a\u989d\u5916\u7684\u903b\u8f91\u5b8c\u6574\u6027\u8bc4\u4f30\u4efb\u52a1\u3002\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u63a8\u7406LLMs\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) \u67d0\u4e9b\u6a21\u578b\u603b\u4f53\u8868\u73b0\u826f\u597d\u4f46\u5728\u590d\u6742\u6848\u4f8b\u4e0a\u4ecd\u6709\u5c40\u9650\uff1b(2) \u903b\u8f91\u590d\u6742\u6027\uff08\u800c\u975e\u8868\u793a\u683c\u5f0f\u5982\u5f62\u5f0f\u903b\u8f91\u8bed\u8a00\u4e0e\u81ea\u7136\u8bed\u8a00\uff09\u662f\u5f71\u54cdLLM\u6027\u80fd\u7684\u4e3b\u5bfc\u56e0\u7d20\uff1b(3) \u8f93\u5165\u6570\u636e\u4e2d\u7684\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\u4f1a\u663e\u8457\u964d\u4f4eLLMs\u7684\u8868\u73b0\u3002", "conclusion": "LLMs\u5728\u63d0\u4f9b\u4e25\u683c\u903b\u8f91\u89e3\u91ca\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u590d\u6742\u6216\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u652f\u6301\u5f39\u6027\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2601.13597", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13597", "abs": "https://arxiv.org/abs/2601.13597", "authors": ["Shyam Agarwal", "Hao He", "Bogdan Vasilescu"], "title": "AI IDEs or Autonomous Agents? Measuring the Impact of Coding Agents on Software Development", "comment": null, "summary": "Large language model (LLM)-based coding agents increasingly act as autonomous contributors that generate and merge pull requests, yet their real-world effects on software projects are unclear, especially relative to widely adopted IDE-based AI assistants. We present a longitudinal causal study of agent adoption in open-source repositories using staggered difference-in-differences with matched controls. Using the AIDev dataset, we define adoption as the first agent-generated pull request and analyze monthly repository-level outcomes spanning development velocity (commits, lines added) and software quality (static-analysis warnings, cognitive complexity, duplication, and comment density). Results show large, front-loaded velocity gains only when agents are the first observable AI tool in a project; repositories with prior AI IDE usage experience minimal or short-lived throughput benefits. In contrast, quality risks are persistent across settings, with static-analysis warnings and cognitive complexity rising roughly 18% and 35%, indicating sustained agent-induced complexity debt even when velocity advantages fade. These heterogeneous effects suggest diminishing returns to AI assistance and highlight the need for quality safeguards, provenance tracking, and selective deployment of autonomous agents. Our findings establish an empirical basis for understanding how agentic and IDE-based tools interact, and motivate research on balancing acceleration with maintainability in AI-integrated development workflows.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u56e0\u679c\u5206\u6790\u53d1\u73b0\uff0cAI\u4ee3\u7801\u4ee3\u7406\u5728\u9996\u6b21\u5f15\u5165\u9879\u76ee\u65f6\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u901f\u5ea6\uff0c\u4f46\u4f1a\u6301\u7eed\u589e\u52a0\u4ee3\u7801\u590d\u6742\u5ea6\u7b49\u8d28\u91cf\u95ee\u9898\uff1b\u800c\u5df2\u6709AI IDE\u7684\u9879\u76ee\u5219\u83b7\u76ca\u6709\u9650\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7801\u4ee3\u7406\u4f5c\u4e3a\u81ea\u4e3b\u8d21\u732e\u8005\u751f\u6210\u548c\u5408\u5e76PR\uff0c\u4f46\u5176\u5bf9\u8f6f\u4ef6\u9879\u76ee\u7684\u5b9e\u9645\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\uff0c\u7279\u522b\u662f\u76f8\u5bf9\u4e8e\u5e7f\u6cdb\u91c7\u7528\u7684IDE AI\u52a9\u624b\u3002\u9700\u8981\u4e86\u89e3\u4ee3\u7406\u5de5\u5177\u4e0eIDE\u5de5\u5177\u5982\u4f55\u4ea4\u4e92\uff0c\u4ee5\u53ca\u5982\u4f55\u5728AI\u96c6\u6210\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u5e73\u8861\u52a0\u901f\u4e0e\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4ea4\u9519\u53cc\u91cd\u5dee\u5206\u6cd5\uff08staggered difference-in-differences\uff09\u4e0e\u5339\u914d\u5bf9\u7167\u7ec4\u8fdb\u884c\u7eb5\u5411\u56e0\u679c\u7814\u7a76\u3002\u5c06\u9996\u6b21\u4ee3\u7406\u751f\u6210\u7684PR\u5b9a\u4e49\u4e3a\u91c7\u7528\uff0c\u5206\u6790\u6708\u5ea6\u4ed3\u5e93\u7ea7\u7ed3\u679c\uff0c\u5305\u62ec\u5f00\u53d1\u901f\u5ea6\uff08\u63d0\u4ea4\u6570\u3001\u4ee3\u7801\u884c\u6570\uff09\u548c\u8f6f\u4ef6\u8d28\u91cf\uff08\u9759\u6001\u5206\u6790\u8b66\u544a\u3001\u8ba4\u77e5\u590d\u6742\u5ea6\u3001\u91cd\u590d\u7387\u3001\u6ce8\u91ca\u5bc6\u5ea6\uff09\u3002", "result": "1. \u901f\u5ea6\u6536\u76ca\uff1a\u4ec5\u5f53\u4ee3\u7406\u662f\u9879\u76ee\u4e2d\u9996\u4e2a\u53ef\u89c2\u5bdfAI\u5de5\u5177\u65f6\uff0c\u624d\u6709\u663e\u8457\u7684\u524d\u671f\u901f\u5ea6\u63d0\u5347\uff1b\u5df2\u6709AI IDE\u4f7f\u7528\u7684\u4ed3\u5e93\u83b7\u76ca\u6709\u9650\u6216\u77ed\u6682\u30022. \u8d28\u91cf\u98ce\u9669\uff1a\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u6301\u7eed\u5b58\u5728\uff0c\u9759\u6001\u5206\u6790\u8b66\u544a\u589e\u52a0\u7ea618%\uff0c\u8ba4\u77e5\u590d\u6742\u5ea6\u589e\u52a0\u7ea635%\uff0c\u8868\u660e\u5373\u4f7f\u901f\u5ea6\u4f18\u52bf\u6d88\u9000\uff0c\u4ee3\u7406\u5f15\u53d1\u7684\u590d\u6742\u5ea6\u503a\u52a1\u4ecd\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "AI\u8f85\u52a9\u5b58\u5728\u6536\u76ca\u9012\u51cf\u6548\u5e94\uff0c\u9700\u8981\u8d28\u91cf\u4fdd\u969c\u63aa\u65bd\u3001\u6765\u6e90\u8ddf\u8e2a\u548c\u9009\u62e9\u6027\u90e8\u7f72\u81ea\u4e3b\u4ee3\u7406\u3002\u7814\u7a76\u4e3a\u7406\u89e3\u4ee3\u7406\u4e0eIDE\u5de5\u5177\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u5e76\u63a8\u52a8\u7814\u7a76\u5982\u4f55\u5728AI\u96c6\u6210\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u5e73\u8861\u52a0\u901f\u4e0e\u53ef\u7ef4\u62a4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.12538", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12538", "abs": "https://arxiv.org/abs/2601.12538", "authors": ["Tianxin Wei", "Ting-Wei Li", "Zhining Liu", "Xuying Ning", "Ze Yang", "Jiaru Zou", "Zhichen Zeng", "Ruizhong Qiu", "Xiao Lin", "Dongqi Fu", "Zihao Li", "Mengting Ai", "Duo Zhou", "Wenxuan Bao", "Yunzhe Li", "Gaotang Li", "Cheng Qian", "Yu Wang", "Xiangru Tang", "Yin Xiao", "Liri Fang", "Hui Liu", "Xianfeng Tang", "Yuji Zhang", "Chi Wang", "Jiaxuan You", "Heng Ji", "Hanghang Tong", "Jingrui He"], "title": "Agentic Reasoning for Large Language Models", "comment": "Project: https://github.com/weitianxin/Awesome-Agentic-Reasoning", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u667a\u80fd\u4f53\u63a8\u7406\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u6027\u5730\u5c06LLM\u4f5c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u7ec4\u7ec7\u4e3a\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u57fa\u7840\u667a\u80fd\u4f53\u63a8\u7406\u3001\u81ea\u6211\u8fdb\u5316\u63a8\u7406\u548c\u96c6\u4f53\u591a\u667a\u80fd\u4f53\u63a8\u7406\uff0c\u5e76\u533a\u5206\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u540e\u8bad\u7ec3\u63a8\u7406\uff0c\u6700\u540e\u8ba8\u8bba\u4e86\u5b9e\u9645\u5e94\u7528\u548c\u672a\u6765\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u5f00\u653e\u52a8\u6001\u73af\u5883\u4e2d\u4ecd\u5b58\u5728\u5c40\u9650\u3002\u667a\u80fd\u4f53\u63a8\u7406\u901a\u8fc7\u5c06LLM\u91cd\u6784\u4e3a\u80fd\u591f\u89c4\u5212\u3001\u884c\u52a8\u548c\u6301\u7eed\u5b66\u4e60\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u4ee3\u8868\u4e86\u8303\u5f0f\u8f6c\u53d8\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u4ece\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7ec4\u7ec7\u667a\u80fd\u4f53\u63a8\u7406\uff1a1)\u57fa\u7840\u667a\u80fd\u4f53\u63a8\u7406\uff08\u5355\u667a\u80fd\u4f53\u5728\u7a33\u5b9a\u73af\u5883\u4e2d\u7684\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u641c\u7d22\uff09\uff1b2)\u81ea\u6211\u8fdb\u5316\u63a8\u7406\uff08\u901a\u8fc7\u53cd\u9988\u3001\u8bb0\u5fc6\u548c\u9002\u5e94\u4f18\u5316\u80fd\u529b\uff09\uff1b3)\u96c6\u4f53\u591a\u667a\u80fd\u4f53\u63a8\u7406\uff08\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u534f\u8c03\u3001\u77e5\u8bc6\u5171\u4eab\u548c\u5171\u540c\u76ee\u6807\uff09\u3002\u540c\u65f6\u533a\u5206\u4e0a\u4e0b\u6587\u63a8\u7406\uff08\u901a\u8fc7\u7ed3\u6784\u5316\u7f16\u6392\u6269\u5c55\u6d4b\u8bd5\u65f6\u4ea4\u4e92\uff09\u548c\u540e\u8bad\u7ec3\u63a8\u7406\uff08\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u4f18\u5316\u884c\u4e3a\uff09\u3002", "result": "\u6784\u5efa\u4e86\u7edf\u4e00\u7684\u667a\u80fd\u4f53\u63a8\u7406\u8def\u7ebf\u56fe\uff0c\u8fde\u63a5\u601d\u60f3\u4e0e\u884c\u52a8\uff0c\u6db5\u76d6\u4e86\u4ece\u79d1\u5b66\u3001\u673a\u5668\u4eba\u3001\u533b\u7597\u4fdd\u5065\u3001\u81ea\u4e3b\u7814\u7a76\u5230\u6570\u5b66\u7b49\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u9886\u57df\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u4ee3\u8868\u6027\u6846\u67b6\u3002", "conclusion": "\u667a\u80fd\u4f53\u63a8\u7406\u4ee3\u8868\u4e86LLM\u80fd\u529b\u7684\u91cd\u8981\u6f14\u8fdb\u65b9\u5411\uff0c\u5c06\u63a8\u7406\u4ece\u9759\u6001\u8fc7\u7a0b\u8f6c\u53d8\u4e3a\u52a8\u6001\u4ea4\u4e92\u8fc7\u7a0b\u3002\u672a\u6765\u6311\u6218\u5305\u62ec\u4e2a\u6027\u5316\u3001\u957f\u65f6\u7a0b\u4ea4\u4e92\u3001\u4e16\u754c\u5efa\u6a21\u3001\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u4ee5\u53ca\u5b9e\u9645\u90e8\u7f72\u7684\u6cbb\u7406\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.13655", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13655", "abs": "https://arxiv.org/abs/2601.13655", "authors": ["Guangba Yu", "Zirui Wang", "Yujie Huang", "Renyi Zhong", "Yuedong Zhong", "Yilun Wang", "Michael R. Lyu"], "title": "Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs", "comment": null, "summary": "The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.\n  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u5f00\u6e90LLM\u751f\u6001\u7cfb\u7edf\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790705\u4e2a\u771f\u5b9e\u6545\u969c\uff0c\u63ed\u793a\u767d\u76d2\u7f16\u6392\u5c06\u53ef\u9760\u6027\u74f6\u9888\u4ece\u6a21\u578b\u7b97\u6cd5\u7f3a\u9677\u8f6c\u79fb\u5230\u90e8\u7f72\u6808\u7684\u7cfb\u7edf\u8106\u5f31\u6027\u3002", "motivation": "\u5f00\u6e90LLM\u7684\u6c11\u4e3b\u5316\u8ba9\u7528\u6237\u53ef\u4ee5\u5728\u672c\u5730\u57fa\u7840\u8bbe\u65bd\u4e0a\u5fae\u8c03\u548c\u90e8\u7f72\u6a21\u578b\uff0c\u4f46\u66b4\u9732\u4e86\"\u7b2c\u4e00\u82f1\u91cc\"\u90e8\u7f72\u73af\u5883\u3002\u4e0e\u9ed1\u76d2API\u6d88\u8d39\u4e0d\u540c\uff0c\u7528\u6237\u7ba1\u7406\u7684\u7f16\u6392\u53ef\u9760\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u76f2\u70b9\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5bf9\u5f00\u6e90DeepSeek\u3001Llama\u548cQwen\u751f\u6001\u7cfb\u7edf\u7684705\u4e2a\u771f\u5b9e\u6545\u969c\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u6545\u969c\u6a21\u5f0f\u548c\u6839\u672c\u539f\u56e0\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u73b0\u8c61\uff1a1\uff09\u8bca\u65ad\u5206\u6b67\uff1a\u8fd0\u884c\u65f6\u5d29\u6e83\u72ec\u7279\u5730\u6307\u793a\u57fa\u7840\u8bbe\u65bd\u6469\u64e6\uff0c\u800c\u4e0d\u6b63\u786e\u529f\u80fd\u5219\u4f5c\u4e3a\u5185\u90e8\u5206\u8bcd\u5668\u7f3a\u9677\u7684\u7279\u5f81\uff1b2\uff09\u7cfb\u7edf\u540c\u8d28\u6027\uff1a\u6839\u672c\u539f\u56e0\u5728\u4e0d\u540c\u7cfb\u5217\u4e2d\u8d8b\u540c\uff0c\u786e\u8ba4\u53ef\u9760\u6027\u969c\u788d\u662f\u5171\u4eab\u751f\u6001\u7cfb\u7edf\u56fa\u6709\u7684\uff1b3\uff09\u751f\u547d\u5468\u671f\u5347\u7ea7\uff1a\u969c\u788d\u4ece\u5fae\u8c03\u671f\u95f4\u7684\u5185\u5728\u914d\u7f6e\u6597\u4e89\u5347\u7ea7\u5230\u63a8\u7406\u671f\u95f4\u7684\u590d\u5408\u73af\u5883\u4e0d\u517c\u5bb9\u3002", "conclusion": "\u767d\u76d2\u7f16\u6392\u5c06\u53ef\u9760\u6027\u74f6\u9888\u4ece\u6a21\u578b\u7b97\u6cd5\u7f3a\u9677\u8f6c\u79fb\u5230\u90e8\u7f72\u6808\u7684\u7cfb\u7edf\u8106\u5f31\u6027\uff0c\u8fd9\u4e9b\u89c1\u89e3\u4e3a\u589e\u5f3aLLM\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.12091", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12091", "abs": "https://arxiv.org/abs/2601.12091", "authors": ["Qian Tan", "Lei Jiang", "Yuting Zeng", "Shuoyang Ding", "Xiaohua Xu"], "title": "Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate", "comment": "13 pages", "summary": "Large language models (LLMs) exhibit systematic Western-centric bias, yet whether prompting in non-Western languages (e.g., Chinese) can mitigate this remains understudied. Answering this question requires rigorous evaluation and effective mitigation, but existing approaches fall short on both fronts: evaluation methods force outputs into predefined cultural categories without a neutral option, while mitigation relies on expensive multi-cultural corpora or agent frameworks that use functional roles (e.g., Planner--Critique) lacking explicit cultural representation. To address these gaps, we introduce CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote (MAV), which enables explicit ``no bias'' judgments. Using this framework, we find that Chinese prompting merely shifts bias toward East Asian perspectives rather than eliminating it. To mitigate such persistent bias, we propose Multi-Agent Cultural Debate (MACD), a training-free framework that assigns agents distinct cultural personas and orchestrates deliberation via a \"Seeking Common Ground while Reserving Differences\" strategy. Experiments demonstrate that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and 86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone) on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.", "AI": {"tldr": "\u63d0\u51faCEBiasBench\u4e2d\u82f1\u53cc\u8bed\u57fa\u51c6\u548cMulti-Agent Vote\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u4e2d\u6587\u63d0\u793a\u4ec5\u5c06\u504f\u89c1\u8f6c\u5411\u4e1c\u4e9a\u89c6\u89d2\u800c\u975e\u6d88\u9664\u3002\u63d0\u51faMulti-Agent Cultural Debate\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u5316\u89d2\u8272\u8fa9\u8bba\u6709\u6548\u7f13\u89e3\u504f\u89c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u897f\u65b9\u4e2d\u5fc3\u504f\u89c1\uff0c\u4f46\u975e\u897f\u65b9\u8bed\u8a00\uff08\u5982\u4e2d\u6587\uff09\u63d0\u793a\u80fd\u5426\u7f13\u89e3\u8fd9\u79cd\u504f\u89c1\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bc4\u4f30\u548c\u7f13\u89e3\u4e24\u65b9\u9762\u90fd\u5b58\u5728\u4e0d\u8db3\uff1a\u8bc4\u4f30\u65b9\u6cd5\u5f3a\u5236\u8f93\u51fa\u9884\u5b9a\u4e49\u6587\u5316\u7c7b\u522b\u800c\u65e0\u4e2d\u7acb\u9009\u9879\uff1b\u7f13\u89e3\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u591a\u6587\u5316\u8bed\u6599\u5e93\u6216\u7f3a\u4e4f\u660e\u786e\u6587\u5316\u8868\u5f81\u7684\u4ee3\u7406\u6846\u67b6\u3002", "method": "1) \u5f15\u5165CEBiasBench\u4e2d\u82f1\u53cc\u8bed\u57fa\u51c6\uff1b2) \u63d0\u51faMulti-Agent Vote\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\"\u65e0\u504f\u89c1\"\u5224\u65ad\uff1b3) \u63d0\u51faMulti-Agent Cultural Debate\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u4e3a\u4ee3\u7406\u5206\u914d\u4e0d\u540c\u6587\u5316\u89d2\u8272\uff0c\u91c7\u7528\"\u6c42\u540c\u5b58\u5f02\"\u7b56\u7565\u8fdb\u884c\u8fa9\u8bba\u3002", "result": "\u4e2d\u6587\u63d0\u793a\u4ec5\u5c06\u504f\u89c1\u8f6c\u5411\u4e1c\u4e9a\u89c6\u89d2\u800c\u975e\u6d88\u9664\u504f\u89c1\u3002MACD\u6846\u67b6\u5728CEBiasBench\u4e0a\u8fbe\u523057.6%\u5e73\u5747\u65e0\u504f\u89c1\u7387\uff08LLM-as-judge\u8bc4\u4f30\uff09\u548c86.0%\uff08MAV\u8bc4\u4f30\uff09\uff0c\u76f8\u6bd4GPT-4o\u57fa\u7ebf\u768447.6%\u548c69.0%\u6709\u663e\u8457\u63d0\u5347\u3002\u5728\u963f\u62c9\u4f2f\u8bedCAMeL\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4ee3\u7406\u6846\u67b6\u4e2d\u660e\u786e\u7684\u6587\u5316\u8868\u5f81\u5bf9\u4e8e\u8de8\u6587\u5316\u516c\u5e73\u81f3\u5173\u91cd\u8981\u3002Multi-Agent Cultural Debate\u901a\u8fc7\u6587\u5316\u89d2\u8272\u8fa9\u8bba\u6709\u6548\u7f13\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u504f\u89c1\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "topic": "agent analysis"}}
{"id": "2601.13682", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13682", "abs": "https://arxiv.org/abs/2601.13682", "authors": ["Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Kangwen Zhao", "Dongyun Xue", "Mingxiao Feng", "Wengang Zhou", "Houqiang Li"], "title": "CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation", "comment": null, "summary": "The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \\times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\\%$ and True Negative Rate (TNR) of $90.89\\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\\%$ and $9.37\\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.", "AI": {"tldr": "\u63d0\u51fa\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7LLM\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5229\u7528\u6267\u884c\u5931\u8d25\u7ed3\u679c\u4f5c\u4e3a\u53cd\u9988\u4f18\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6CodeContests-O\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u63a8\u7406\u6a21\u578b\u9700\u8981\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u6570\u636e\uff0c\u7f16\u7a0b\u4efb\u52a1\u662f\u7406\u60f3\u6765\u6e90\u3002\u4f46\u73b0\u6709\u7ade\u4e89\u7f16\u7a0b\u5e73\u53f0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6a21\u578b\u5185\u5728\u751f\u6210\u80fd\u529b\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u7528\u4f8b\u591a\u6837\u6027\u4e0d\u8db3", "method": "\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u6846\u67b6\uff1a1) LLM\u751f\u6210\u521d\u59cb\u6d4b\u8bd5\u7528\u4f8b\uff1b2) \u5bf9\u6b63\u786e\u548c\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u6267\u884c\u6d4b\u8bd5\uff1b3) \u5229\u7528\u5931\u8d25\u7ed3\u679c\u4f5c\u4e3a\u53cd\u9988\u6307\u5bfcLLM\u4f18\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u63d0\u9ad8\u4fdd\u771f\u5ea6\u548c\u533a\u5206\u5ea6\uff1b4) \u5e94\u7528\u4e8eCodeContests\u6570\u636e\u96c6\u6784\u5efaCodeContests-O", "result": "CodeContests-O\u57281100\u4e07\u89e3\u51b3\u65b9\u6848\u4e2d\u8fbe\u5230\u5e73\u5747TPR 89.37%\u548cTNR 90.89%\uff0c\u663e\u8457\u4f18\u4e8eCodeContests\u548cCodeContests+\u3002\u5728Qwen2.5-7B\u4e0a\u5fae\u8c03\u540e\uff0cLiveCodeBench Pass@1\u63d0\u53479.52%", "conclusion": "\u63d0\u51fa\u7684\u53cd\u9988\u9a71\u52a8\u8fed\u4ee3\u6846\u67b6\u6709\u6548\uff0cCodeContests-O\u6570\u636e\u96c6\u8d28\u91cf\u9ad8\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5df2\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u652f\u6301\u53ef\u590d\u73b0\u6027\u548c\u672a\u6765\u7814\u7a76", "topic": "swe benchmark"}}
{"id": "2601.13713", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13713", "abs": "https://arxiv.org/abs/2601.13713", "authors": ["Aditya Bharat Soni", "Rajat Ghosh", "Vaishnavi Bhargava", "Valerie Chen", "Debojyoti Dutta"], "title": "SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories", "comment": null, "summary": "Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- \"test first, write code later\", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\\% in success rate and 21\\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSWE-Tester\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u5f00\u6e90LLM\u751f\u6210\u95ee\u9898\u590d\u73b0\u6d4b\u8bd5\u7684\u6846\u67b6\uff0c\u5728SWT-Bench Verified\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad810%\u7684\u6210\u529f\u7387\u548c21%\u53d8\u66f4\u8986\u76d6\u7387\u7684\u7edd\u5bf9\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u95ed\u6e90LLM\uff0c\u5bf9\u5f00\u6e90\u6a21\u578b\u63a2\u7d22\u6709\u9650\u3002\u81ea\u52a8\u4ece\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u751f\u6210\u6d4b\u8bd5\u80fd\u63d0\u9ad8\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u3001\u7b80\u5316\u6839\u56e0\u5206\u6790\u3001\u4fc3\u8fdb\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff0c\u5e76\u63d0\u5347\u81ea\u52a8\u95ee\u9898\u89e3\u51b3\u7cfb\u7edf\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faSWE-Tester\u8bad\u7ec3\u7ba1\u9053\uff1a\u9996\u5148\u4ece2.6K\u4e2aGitHub\u4ed3\u5e93\u4e2d\u6574\u740641K\u4e2a\u9ad8\u8d28\u91cf\u8bad\u7ec3\u5b9e\u4f8b\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684\u5f00\u6e90LLM\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728SWT-Bench Verified\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad810%\u6210\u529f\u7387\u548c21%\u53d8\u66f4\u8986\u76d6\u7387\u7684\u7edd\u5bf9\u63d0\u5347\u3002\u5206\u6790\u663e\u793a\u589e\u52a0\u63a8\u7406\u8ba1\u7b97\u3001\u66f4\u591a\u6570\u636e\u548c\u66f4\u5927\u6a21\u578b\u80fd\u5e26\u6765\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u63a8\u8fdb\u5f00\u6e90LLM\u5728\u6b64\u9886\u57df\u7684\u5e94\u7528\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4e3a\u5f00\u6e90\u6a21\u578b\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002", "topic": "swe benchmark"}}
{"id": "2601.12560", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.12560", "abs": "https://arxiv.org/abs/2601.12560", "authors": ["Arunkumar V", "Gangadharan G. R.", "Rajkumar Buyya"], "title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents", "comment": "28 pages, 4 figures, 5 tables", "summary": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684Agentic AI\u5206\u7c7b\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u5206\u89e3\u4e3a\u611f\u77e5\u3001\u5927\u8111\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u5de5\u5177\u4f7f\u7528\u548c\u534f\u4f5c\u516d\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5e76\u5206\u6790\u4e86\u4ece\u7ebf\u6027\u63a8\u7406\u5230\u539f\u751f\u63a8\u7406\u6a21\u578b\u7684\u6f14\u8fdb\u8d8b\u52bf\u3002", "motivation": "\u968f\u7740AI\u4ece\u5355\u7eaf\u751f\u6210\u6587\u672c\u8f6c\u5411Agentic AI\uff08\u667a\u80fd\u4f53AI\uff09\uff0c\u5404\u79cd\u7cfb\u7edf\u8bbe\u8ba1\u6d8c\u73b0\uff0c\u4ece\u7b80\u5355\u7684\u5355\u5faa\u73af\u667a\u80fd\u4f53\u5230\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f7f\u5f97\u8fd9\u4e00\u9886\u57df\u53d8\u5f97\u96be\u4ee5\u5bfc\u822a\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\u6765\u68b3\u7406\u8fd9\u4e00\u590d\u6742\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u5206\u89e3\u4e3a\u516d\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u611f\u77e5\u3001\u5927\u8111\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u5de5\u5177\u4f7f\u7528\u548c\u534f\u4f5c\u3002\u4f7f\u7528\u8fd9\u4e00\u6846\u67b6\u5206\u6790\u4ece\u7ebf\u6027\u63a8\u7406\u5230\u539f\u751f\u63a8\u7406\u6a21\u578b\u7684\u6f14\u8fdb\uff0c\u4ee5\u53ca\u4ece\u56fa\u5b9aAPI\u8c03\u7528\u5230\u5f00\u653e\u6807\u51c6\uff08\u5982MCP\u548cNative Computer Use\uff09\u7684\u8f6c\u53d8\u3002", "result": "\u5efa\u7acb\u4e86Agentic AI\u7684\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u73af\u5883\uff08\u6570\u5b57\u64cd\u4f5c\u7cfb\u7edf\u3001\u5177\u8eab\u673a\u5668\u4eba\u7b49\uff09\u4e2d\u7684\u64cd\u4f5c\u6a21\u5f0f\uff0c\u5e76\u56de\u987e\u4e86\u5f53\u524d\u7684\u8bc4\u4f30\u5b9e\u8df5\u3002", "conclusion": "\u8bba\u6587\u4e3aAgentic AI\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u6307\u51fa\u4e86\u5e7b\u89c9\u3001\u65e0\u9650\u5faa\u73af\u3001\u63d0\u793a\u6ce8\u5165\u7b49\u5f00\u653e\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u6784\u5efa\u66f4\u7a33\u5065\u53ef\u9760\u81ea\u4e3b\u7cfb\u7edf\u7684\u7814\u7a76\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.13754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13754", "abs": "https://arxiv.org/abs/2601.13754", "authors": ["Haoyu Gao", "Peerachai Banyongrakkul", "Hao Guan", "Mansooreh Zahedi", "Christoph Treude"], "title": "On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source", "comment": "accepted as MSR short paper", "summary": "Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\\% merged without any explicit review. Finally, we discuss implications for developers and researchers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u534f\u4f5c\u7684PR\u4e3b\u8981\u6765\u81ea\u65e0\u4ee3\u7801\u6240\u6709\u6743\u7684\u8d21\u732e\u8005\uff0c\u4f46\u591a\u6570\u4ed3\u5e93\u7f3a\u4e4fAI\u4f7f\u7528\u6307\u5357\uff1bAI\u534f\u4f5cPR\u5408\u5e76\u66f4\u5feb\u3001\u53cd\u9988\u66f4\u5c11\uff0c\u7279\u522b\u662f\u975e\u6240\u6709\u8005\u63d0\u4ea4\u7684AI\u534f\u4f5cPR80%\u65e0\u660e\u786e\u5ba1\u67e5\u5373\u88ab\u5408\u5e76\u3002", "motivation": "\u968f\u7740LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u5f00\u6e90\u8f6f\u4ef6\u4e2d\"AI\u4f5c\u4e3a\u961f\u53cb\"\u7684\u91c7\u7528\u52a0\u901f\uff0c\u4f46\u5f00\u53d1\u8005\u4e0eAI\u534f\u4f5c\u7684\u4ea4\u4e92\u6a21\u5f0f\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6269\u5c55AIDev\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u7684\u8d21\u732e\u8005\u4ee3\u7801\u6240\u6709\u6743\u4fe1\u606f\uff0c\u5e76\u4e0e\u4eba\u5de5\u521b\u5efa\u7684PR\u5efa\u7acb\u6bd4\u8f83\u57fa\u7ebf\uff0c\u7814\u7a76\u9879\u76ee\u7ea7\u6307\u5357\u548c\u5f00\u53d1\u8005\u4e0eAI\u8f85\u52a9PR\u7684\u4ea4\u4e92\u3002", "result": "1) 67.5%\u7684AI\u534f\u4f5cPR\u6765\u81ea\u65e0\u4ee3\u7801\u6240\u6709\u6743\u7684\u8d21\u732e\u8005\uff1b2) \u591a\u6570\u4ed3\u5e93\u7f3a\u4e4fAI\u7f16\u7801\u4ee3\u7406\u4f7f\u7528\u6307\u5357\uff1b3) AI\u534f\u4f5cPR\u5408\u5e76\u663e\u8457\u66f4\u5feb\u4e14\u53cd\u9988\u6700\u5c11\uff1b4) \u975e\u6240\u6709\u8005\u63d0\u4ea4\u7684AI\u534f\u4f5cPR\u7ea680%\u65e0\u660e\u786e\u5ba1\u67e5\u5373\u88ab\u5408\u5e76\u3002", "conclusion": "AI\u534f\u4f5cPR\u5448\u73b0\u72ec\u7279\u7684\u4ea4\u4e92\u6a21\u5f0f\uff1a\u5408\u5e76\u66f4\u5feb\u3001\u53cd\u9988\u66f4\u5c11\uff0c\u7279\u522b\u662f\u975e\u6240\u6709\u8005\u63d0\u4ea4\u7684AI\u534f\u4f5cPR\u5ba1\u67e5\u4e0d\u8db3\uff0c\u8fd9\u5bf9\u5f00\u53d1\u8005\u548c\u7814\u7a76\u8005\u5177\u6709\u91cd\u8981\u542f\u793a\u3002", "topic": "swe application"}}
{"id": "2601.13933", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13933", "abs": "https://arxiv.org/abs/2601.13933", "authors": ["Mingming Zhang", "Xu Wang", "Jian Zhang", "Xiangxin Meng", "Jiayi Zhang", "Chunming Hu"], "title": "VulnResolver: A Hybrid Agent Framework for LLM-Based Automated Vulnerability Issue Resolution", "comment": null, "summary": "As software systems grow in complexity, security vulnerabilities have become increasingly prevalent, posing serious risks and economic costs. Although automated detection tools such as fuzzers have advanced considerably, effective resolution still often depends on human expertise. Existing automated vulnerability repair (AVR) methods rely heavily on manually provided annotations (e.g., fault locations or CWE labels), which are often difficult and time-consuming to obtain, while overlooking the rich, naturally embedded semantic context found in issue reports from developers.\n  In this paper, we present VulnResolver, the first LLM-based hybrid agent framework for automated vulnerability issue resolution. VulnResolver unites the adaptability of autonomous agents with the stability of workflow-guided repair through two specialized agents. The Context Pre-Collection Agent (CPCAgent) adaptively explores the repository to gather dependency and contextual information, while the Safety Property Analysis Agent (SPAAgent) generates and validates the safety properties violated by vulnerabilities. Together, these agents produce structured analyses that enrich the original issue reports, enabling more accurate vulnerability localization and patch generation.\n  Evaluations on the SEC-bench benchmark show that VulnResolver resolves 75% of issues on SEC-bench Lite, achieving the best resolution performance. On SEC-bench Full, VulnResolver also significantly outperforms the strongest baseline, the agent-based OpenHands, confirming its effectiveness. Overall, VulnResolver delivers an adaptive and security-aware framework that advances end-to-end automated vulnerability issue resolution through workflow stability and the specialized agents' capabilities in contextual reasoning and property-based analysis.", "AI": {"tldr": "VulnResolver\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6df7\u5408\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6f0f\u6d1e\u95ee\u9898\u89e3\u51b3\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e13\u95e8\u4ee3\u7406\u5b9e\u73b0\u4e0a\u4e0b\u6587\u6536\u96c6\u548c\u5b89\u5168\u5c5e\u6027\u5206\u6790\uff0c\u5728SEC-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\uff0c\u5b89\u5168\u6f0f\u6d1e\u65e5\u76ca\u666e\u904d\u4e14\u4ee3\u4ef7\u9ad8\u6602\u3002\u73b0\u6709\u81ea\u52a8\u5316\u6f0f\u6d1e\u4fee\u590d\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\uff08\u5982\u6545\u969c\u4f4d\u7f6e\u6216CWE\u6807\u7b7e\uff09\uff0c\u8fd9\u4e9b\u6807\u6ce8\u96be\u4ee5\u83b7\u53d6\u4e14\u8017\u65f6\uff0c\u540c\u65f6\u5ffd\u7565\u4e86\u5f00\u53d1\u8005\u95ee\u9898\u62a5\u544a\u4e2d\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faVulnResolver\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u4e3b\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u5de5\u4f5c\u6d41\u5f15\u5bfc\u4fee\u590d\u7684\u7a33\u5b9a\u6027\u3002\u5305\u542b\u4e24\u4e2a\u4e13\u95e8\u4ee3\u7406\uff1a\u4e0a\u4e0b\u6587\u9884\u6536\u96c6\u4ee3\u7406\uff08CPCAgent\uff09\u81ea\u9002\u5e94\u63a2\u7d22\u4ed3\u5e93\u6536\u96c6\u4f9d\u8d56\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff1b\u5b89\u5168\u5c5e\u6027\u5206\u6790\u4ee3\u7406\uff08SPAAgent\uff09\u751f\u6210\u5e76\u9a8c\u8bc1\u6f0f\u6d1e\u8fdd\u53cd\u7684\u5b89\u5168\u5c5e\u6027\u3002\u8fd9\u4e9b\u4ee3\u7406\u751f\u6210\u7ed3\u6784\u5316\u5206\u6790\u6765\u4e30\u5bcc\u539f\u59cb\u95ee\u9898\u62a5\u544a\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u6f0f\u6d1e\u5b9a\u4f4d\u548c\u8865\u4e01\u751f\u6210\u3002", "result": "\u5728SEC-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVulnResolver\u5728SEC-bench Lite\u4e0a\u89e3\u51b3\u4e8675%\u7684\u95ee\u9898\uff0c\u8fbe\u5230\u6700\u4f73\u89e3\u51b3\u6027\u80fd\u3002\u5728SEC-bench Full\u4e0a\uff0c\u4e5f\u663e\u8457\u4f18\u4e8e\u6700\u5f3a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff08\u57fa\u4e8e\u4ee3\u7406\u7684OpenHands\uff09\uff0c\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "VulnResolver\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u4e14\u5b89\u5168\u611f\u77e5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u4f5c\u6d41\u7a33\u5b9a\u6027\u548c\u4e13\u95e8\u4ee3\u7406\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u57fa\u4e8e\u5c5e\u6027\u5206\u6790\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63a8\u8fdb\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u6f0f\u6d1e\u95ee\u9898\u89e3\u51b3\u3002", "topic": "code agent"}}
{"id": "2601.13943", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13943", "abs": "https://arxiv.org/abs/2601.13943", "authors": ["Zhiyuan Peng", "Xin Yin", "Pu Zhao", "Fangkai Yang", "Lu Wang", "Ran Jia", "Xu Chen", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository", "comment": null, "summary": "Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a \"review-rebuttal\" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.", "AI": {"tldr": "RepoGenesis\u662f\u9996\u4e2a\u591a\u8bed\u8a00\u4ed3\u5e93\u7ea7\u7aef\u5230\u7aefWeb\u5fae\u670d\u52a1\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b106\u4e2aPython\u548cJava\u5fae\u670d\u52a1\u4ed3\u5e93\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u7cfb\u7edf\u5728\u67b6\u6784\u4e00\u81f4\u6027\u3001\u4f9d\u8d56\u7ba1\u7406\u548c\u8de8\u6587\u4ef6\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u51fd\u6570/\u7c7b\u7ea7\u4ee3\u7801\u751f\u6210\u6216\u73b0\u6709\u4ee3\u7801\u5e93\u4fee\u6539\uff0c\u7f3a\u4e4f\u5b8c\u6574\u7684\u5fae\u670d\u52a1\u4ed3\u5e93\u751f\u6210\u57fa\u51c6\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u76840\u52301\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u6784\u5efa\u5305\u542b106\u4e2aPython\u548cJava\u5fae\u670d\u52a1\u4ed3\u5e93\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d618\u4e2a\u9886\u57df\u548c11\u4e2a\u6846\u67b6\uff0c\u5305\u542b1258\u4e2aAPI\u7aef\u70b9\u548c2335\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u91c7\u7528\"\u8bc4\u5ba1-\u53cd\u9a73\"\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5c3d\u7ba1API\u8986\u76d6\u7387\u9ad8\u8fbe73.91%\uff0c\u90e8\u7f72\u6210\u529f\u7387100%\uff0c\u4f46\u6700\u4f73\u7cfb\u7edf\u5728Python\u548cJava\u4e0a\u7684Pass@1\u5206\u522b\u4ec5\u4e3a23.67%\u548c21.45%\uff0c\u66b4\u9732\u4e86\u67b6\u6784\u4e00\u81f4\u6027\u3001\u4f9d\u8d56\u7ba1\u7406\u548c\u8de8\u6587\u4ef6\u4e00\u81f4\u6027\u7684\u7f3a\u9677\u3002", "conclusion": "RepoGenesis\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u73b0\u6709\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u5728\u5b8c\u6574\u5fae\u670d\u52a1\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u63a8\u8fdb\u5fae\u670d\u52a1\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8bc4\u4f30\u5e73\u53f0\u3002", "topic": "swe benchmark"}}
{"id": "2601.12212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12212", "abs": "https://arxiv.org/abs/2601.12212", "authors": ["Chenan Wang", "Daniel H. Shi", "Haipeng Chen"], "title": "Speculative Sampling with Reinforcement Learning", "comment": "Accepted to AAAI 2026", "summary": "Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\\times$ speedup over the backbone LLM and up to 1.12$\\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.", "AI": {"tldr": "\u63d0\u51faRe-SpS\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u63a8\u6d4b\u91c7\u6837\u4e2d\u7684\u8349\u7a3f\u6811\u8d85\u53c2\u6570\uff0c\u63d0\u5347LLM\u63a8\u7406\u901f\u5ea6", "motivation": "\u5f53\u524d\u63a8\u6d4b\u91c7\u6837\u65b9\u6cd5\uff08\u5982EAGLE-3\uff09\u4f7f\u7528\u9759\u6001\u8d85\u53c2\u6570\u63a7\u5236\u8349\u7a3f\u6811\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u548c\u9886\u57df\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002\u9700\u8981\u52a8\u6001\u8c03\u6574\u8d85\u53c2\u6570\u4ee5\u5e73\u8861\u63a8\u6d4b\u7684\u6fc0\u8fdb\u7a0b\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u6d4b\u91c7\u6837\u6846\u67b6Re-SpS\uff0c\u52a8\u6001\u5b9e\u65f6\u8c03\u6574\u8349\u7a3f\u6811\u8d85\u53c2\u6570\u3002\u5229\u7528\u76ee\u6807\u6a21\u578b\u9690\u85cf\u72b6\u6001\u7684\u9ad8\u6548\u72b6\u6001\u8868\u793a\uff0c\u5f15\u5165\u591a\u6b65\u52a8\u4f5c\u6301\u4e45\u5316\u4ee5\u66f4\u597d\u5730\u5efa\u6a21\u4e0a\u4e0b\u6587\uff0c\u5b66\u4e60\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7b56\u7565\u6765\u6700\u5927\u5316\u751f\u6210\u901f\u5ea6\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4SOTA\u65b9\u6cd5EAGLE-3\u6301\u7eed\u6539\u8fdb\uff0c\u76f8\u6bd4\u9aa8\u5e72LLM\u5b9e\u73b0\u6700\u9ad85.45\u500d\u52a0\u901f\uff0c\u76f8\u6bd4EAGLE-3\u5b9e\u73b0\u6700\u9ad81.12\u500d\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u4fdd\u771f\u5ea6\u65e0\u635f\u5931\u3002", "conclusion": "Re-SpS\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u8349\u7a3f\u6811\u8d85\u53c2\u6570\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.12711", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2601.12711", "abs": "https://arxiv.org/abs/2601.12711", "authors": ["Kevin Wang", "Neel P. Bhatt", "Cong Liu", "Junbo Li", "Runjin Chen", "Yihan Xi", "Timothy Barclay", "Alvaro Velasquez", "Ufuk Topcu", "Zhangyang Wang"], "title": "Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts", "comment": null, "summary": "Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\uff0c\u52a8\u6001\u7ed3\u5408\u6570\u503c\u53c2\u6570\u66f4\u65b0\u548c\u7b26\u53f7\u63d0\u793a\u7f16\u8f91\u4e24\u79cd\u7b56\u7565\uff0c\u901a\u8fc7\u7edf\u4e00\u76d1\u63a7\u4fe1\u53f7\u548c\u5956\u52b1\u5206\u7c7b\u5668\u51b3\u5b9a\u4f55\u65f6\u4f7f\u7528LoRA\u8fdb\u884c\u4e8b\u5b9e\u91cd\u6784\uff0c\u4f55\u65f6\u4f7f\u7528TextGrad\u8fdb\u884ctoken\u7ea7\u7f16\u8f91\uff0c\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u4e0a\u4f18\u4e8e\u7eaf\u6570\u503c\u6216\u7eaf\u7b26\u53f7\u57fa\u7ebf\u3002", "motivation": "LLMs\u53ef\u4ee5\u901a\u8fc7\u6570\u503c\u66f4\u65b0\uff08\u4fee\u6539\u6a21\u578b\u53c2\u6570\uff09\u6216\u7b26\u53f7\u64cd\u4f5c\uff08\u5904\u7406\u79bb\u6563\u63d0\u793a\u6216\u903b\u8f91\u7ea6\u675f\uff09\u8fdb\u884c\u9002\u914d\u3002\u6570\u503c\u5fae\u8c03\u64c5\u957f\u6ce8\u5165\u65b0\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u7b26\u53f7\u66f4\u65b0\u5219\u80fd\u7075\u6d3b\u63a7\u5236\u98ce\u683c\u548c\u5bf9\u9f50\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u9700\u8981\u7ed3\u5408\u8fd9\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u4ee5\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\uff1a1\uff09\u7edf\u4e00\u76d1\u63a7\u4fe1\u53f7\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u5206\u7c7b\u5668\uff0c\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u4f7f\u7528LoRA\u8fdb\u884c\u6df1\u5ea6\u4e8b\u5b9e\u91cd\u6784\uff0c\u4f55\u65f6\u5e94\u7528TextGrad\u8fdb\u884ctoken\u7ea7\u7f16\u8f91\uff1b2\uff09\u4fdd\u6301\u5185\u5b58\u6548\u7387\uff0c\u4ec5\u5728\u9700\u8981\u65f6\u5c06\u7b26\u53f7\u8f6c\u6362\u5378\u8f7d\u5230\u5916\u90e8LLM\uff1b3\uff09\u7b26\u53f7\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u7cbe\u70bc\u63d0\u793a\u53ef\u4f5c\u4e3a\u9ad8\u8d28\u91cf\u53ef\u91cd\u7528\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u795e\u7ecf\u7b26\u53f7LoRA\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u6570\u503c\u6216\u7eaf\u7b26\u53f7\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u9002\u5e94\u6027\u548c\u6539\u8fdb\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u7b49\u6570\u636e\u7a00\u7f3a\u9886\u57df\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6570\u503c\u548c\u7b26\u53f7\u66f4\u65b0\u7684\u4ea4\u9519\u7ed3\u5408\u4e3a\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5f00\u542f\u4e86\u65b0\u7684\u591a\u529f\u80fd\u6027\u6c34\u5e73\uff0c\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\u5c55\u793a\u4e86\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4e3aLLM\u9002\u914d\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.12286", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12286", "abs": "https://arxiv.org/abs/2601.12286", "authors": ["Jonathan Pan"], "title": "Conversational Context Classification: A Representation Engineering Approach", "comment": null, "summary": "The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u8868\u5f81\u5de5\u7a0b\u548c\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\u5728LLM\u5185\u90e8\u72b6\u6001\u4e2d\u8bc6\u522b\u7279\u5b9a\u4e0a\u4e0b\u6587\u5b50\u7a7a\u95f4\uff0c\u4ee5\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u504f\u79bb\u9884\u671f\u5bf9\u8bdd\u89c4\u8303", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u6709\u6548\u4fdd\u969c\u5176\u64cd\u4f5c\u5b89\u5168\uff0c\u7279\u522b\u662f\u68c0\u6d4b\u6a21\u578b\u751f\u6210\u8131\u79bb\u4e0a\u4e0b\u6587\u56de\u590d\u7684\u8d8b\u52bf\uff0c\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0a\u4e0b\u6587\u8bed\u4e49", "method": "\u4f7f\u7528\u8868\u5f81\u5de5\u7a0b\u548c\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\uff0c\u5728LLM\u5185\u90e8\u72b6\u6001\u4e2d\u8bc6\u522b\u4ee3\u8868\u7279\u5b9a\u4e0a\u4e0b\u6587\u7684\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u5728\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e0a\u8bad\u7ec3OCSVM\uff0c\u5728LLM\u9690\u85cf\u72b6\u6001\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5efa\u7acb\u9c81\u68d2\u8fb9\u754c\uff0c\u5e76\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u5728\u8bc6\u522b\u7279\u5b9a\u4e0a\u4e0b\u6587\u5b50\u7a7a\u95f4\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5bf9\u8bdd\u7ebf\u7a0b\u662f\u5426\u5728\u4e0a\u4e0b\u6587\u8303\u56f4\u5185", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u7528\u4e8e\u68c0\u6d4b\u5bf9\u8bdd\u662f\u5426\u5728\u4e0a\u4e0b\u6587\u8303\u56f4\u5185\uff0c\u8fd8\u4e3a\u66f4\u597d\u5730\u89e3\u91caLLM\u7684\u7814\u7a76\u505a\u51fa\u4e86\u8d21\u732e", "topic": "agent analysis"}}
{"id": "2601.12369", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12369", "abs": "https://arxiv.org/abs/2601.12369", "authors": ["Ming Zhang", "Jiabao Zhuang", "Wenqing Jing", "Ziyu Kong", "Jingyi Deng", "Yujiong Shen", "Kexin Tan", "Yuhang Zhao", "Ning Luo", "Renzhe Zheng", "Jiahui Lin", "Mingqi Wu", "Long Ma", "Yi Zou", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies", "comment": null, "summary": "Deep Research Agents are increasingly used for automated survey generation. However, whether they can write surveys like human experts remains unclear. Existing benchmarks focus on fluency or citation accuracy, but none evaluates the core capabilities: retrieving essential papers and organizing them into coherent knowledge structures. We introduce TaxoBench, a diagnostic benchmark derived from 72 highly-cited computer science surveys. We manually extract expert-authored taxonomy trees containing 3,815 precisely categorized citations as ground truth. Our benchmark supports two evaluation modes: Deep Research mode tests end-to-end retrieval and organization given only a topic, while Bottom-Up mode isolates structuring capability by providing the exact papers human experts used. We evaluate 7 leading Deep Research agents and 12 frontier LLMs. Results reveal a dual bottleneck: the best agent recalls only 20.9% of expert-selected papers, and even with perfect input, the best model achieves only 0.31 ARI in organization. Current deep research agents remain far from expert-level survey writing. Our benchmark is publicly available at https://github.com/KongLongGeFDU/TaxoBench.", "AI": {"tldr": "TaxoBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u80fd\u529b\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u57fa\u4e8e72\u7bc7\u9ad8\u5f15\u7528\u8ba1\u7b97\u673a\u79d1\u5b66\u7efc\u8ff0\u6784\u5efa\uff0c\u5305\u542b3,815\u4e2a\u7cbe\u786e\u5206\u7c7b\u7684\u5f15\u7528\u4f5c\u4e3a\u57fa\u51c6\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6700\u4f73\u4ee3\u7406\u53ea\u80fd\u53ec\u56de20.9%\u7684\u4e13\u5bb6\u9009\u62e9\u8bba\u6587\uff0c\u5373\u4f7f\u6709\u5b8c\u7f8e\u8f93\u5165\uff0c\u6700\u4f73\u6a21\u578b\u5728\u7ec4\u7ec7\u7ed3\u6784\u65b9\u9762\u4e5f\u4ec5\u8fbe\u52300.31 ARI\uff0c\u8868\u660e\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4e0e\u4e13\u5bb6\u7ea7\u7efc\u8ff0\u64b0\u5199\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u7efc\u8ff0\uff0c\u4f46\u5b83\u4eec\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e13\u5bb6\u4e00\u6837\u64b0\u5199\u7efc\u8ff0\u5c1a\u4e0d\u6e05\u695a\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6d41\u7545\u6027\u6216\u5f15\u7528\u51c6\u786e\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6838\u5fc3\u80fd\u529b\uff08\u68c0\u7d22\u5173\u952e\u8bba\u6587\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u8fde\u8d2f\u77e5\u8bc6\u7ed3\u6784\uff09\u7684\u8bc4\u4f30\u3002", "method": "\u4ece72\u7bc7\u9ad8\u5f15\u7528\u8ba1\u7b97\u673a\u79d1\u5b66\u7efc\u8ff0\u4e2d\u624b\u52a8\u63d0\u53d6\u4e13\u5bb6\u6784\u5efa\u7684\u5206\u7c7b\u6811\uff0c\u5305\u542b3,815\u4e2a\u7cbe\u786e\u5206\u7c7b\u7684\u5f15\u7528\u4f5c\u4e3a\u57fa\u51c6\u3002\u652f\u6301\u4e24\u79cd\u8bc4\u4f30\u6a21\u5f0f\uff1a\u6df1\u5ea6\u7814\u7a76\u6a21\u5f0f\uff08\u7ed9\u5b9a\u4e3b\u9898\u6d4b\u8bd5\u7aef\u5230\u7aef\u68c0\u7d22\u548c\u7ec4\u7ec7\u80fd\u529b\uff09\u548c\u81ea\u5e95\u5411\u4e0a\u6a21\u5f0f\uff08\u63d0\u4f9b\u4eba\u7c7b\u4e13\u5bb6\u4f7f\u7528\u7684\u786e\u5207\u8bba\u6587\uff0c\u9694\u79bb\u7ec4\u7ec7\u7ed3\u6784\u80fd\u529b\uff09\u3002\u8bc4\u4f30\u4e867\u4e2a\u9886\u5148\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u548c12\u4e2a\u524d\u6cbfLLM\u3002", "result": "\u63ed\u793a\u4e86\u53cc\u91cd\u74f6\u9888\uff1a\u6700\u4f73\u4ee3\u7406\u53ea\u80fd\u53ec\u56de20.9%\u7684\u4e13\u5bb6\u9009\u62e9\u8bba\u6587\uff1b\u5373\u4f7f\u6709\u5b8c\u7f8e\u8f93\u5165\uff0c\u6700\u4f73\u6a21\u578b\u5728\u7ec4\u7ec7\u7ed3\u6784\u65b9\u9762\u4e5f\u4ec5\u8fbe\u52300.31 ARI\uff08\u8c03\u6574\u5170\u5fb7\u6307\u6570\uff09\u3002\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4e0e\u4e13\u5bb6\u7ea7\u7efc\u8ff0\u64b0\u5199\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\u3002", "conclusion": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5728\u68c0\u7d22\u5173\u952e\u8bba\u6587\u548c\u7ec4\u7ec7\u77e5\u8bc6\u7ed3\u6784\u65b9\u9762\u4ecd\u8fdc\u672a\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\u3002TaxoBench\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.14132", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14132", "abs": "https://arxiv.org/abs/2601.14132", "authors": ["Rodrigo Falc\u00e3o", "Frank Elberzhager", "Karthik Vaidhyanathan"], "title": "Toward self-coding information systems", "comment": "Accepted for ICSE 2026 Track \"Software Architecture BoF\"", "summary": "In this extended abstract, we propose a novel research topic in the field of agentic AI, which we refer to as self-coding information systems. These systems will be able to dynamically adapt their structure or behavior by evaluating potential adaptation decisions, generate source code, test, and (re)deploy their source code autonomously, at runtime, reducing the time to market of new features. Here we motivate the topic, provide a formal definition of self-coding information systems, discuss some expected impacts of the new technology, and indicate potential research directions.", "AI": {"tldr": "\u63d0\u51fa\"\u81ea\u7f16\u7801\u4fe1\u606f\u7cfb\u7edf\"\u65b0\u6982\u5ff5\uff0c\u7cfb\u7edf\u80fd\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u8bc4\u4f30\u3001\u751f\u6210\u3001\u6d4b\u8bd5\u548c\u90e8\u7f72\u4ee3\u7801\uff0c\u5b9e\u73b0\u81ea\u4e3b\u7ed3\u6784/\u884c\u4e3a\u9002\u5e94\uff0c\u7f29\u77ed\u65b0\u529f\u80fd\u4e0a\u5e02\u65f6\u95f4", "motivation": "\u5f53\u524d\u4fe1\u606f\u7cfb\u7edf\u9700\u8981\u4eba\u5de5\u7f16\u7801\u548c\u90e8\u7f72\uff0c\u5bfc\u81f4\u65b0\u529f\u80fd\u5f00\u53d1\u5468\u671f\u957f\u3002\u81ea\u7f16\u7801\u7cfb\u7edf\u80fd\u81ea\u4e3b\u9002\u5e94\u53d8\u5316\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u9ad8\u7cfb\u7edf\u54cd\u5e94\u901f\u5ea6\u548c\u7075\u6d3b\u6027", "method": "\u63d0\u51fa\u81ea\u7f16\u7801\u4fe1\u606f\u7cfb\u7edf\u7684\u6b63\u5f0f\u5b9a\u4e49\uff0c\u8ba8\u8bba\u5176\u9884\u671f\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u6f5c\u5728\u7814\u7a76\u65b9\u5411\u3002\u7cfb\u7edf\u5305\u62ec\u8bc4\u4f30\u9002\u5e94\u51b3\u7b56\u3001\u751f\u6210\u6e90\u4ee3\u7801\u3001\u6d4b\u8bd5\u548c\u81ea\u4e3b\u90e8\u7f72\u7b49\u80fd\u529b", "result": "\u8fd9\u662f\u4e00\u4e2a\u6982\u5ff5\u6027\u63d0\u6848\uff0c\u5c1a\u672a\u6709\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002\u63d0\u51fa\u4e86\u65b0\u7684\u7814\u7a76\u4e3b\u9898\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u81ea\u7f16\u7801\u7cfb\u7edf\u5f00\u53d1\u5960\u5b9a\u7406\u8bba\u57fa\u7840", "conclusion": "\u81ea\u7f16\u7801\u4fe1\u606f\u7cfb\u7edf\u662f\u4ee3\u7406AI\u9886\u57df\u6709\u524d\u666f\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u80fd\u5b9e\u73b0\u7cfb\u7edf\u81ea\u4e3b\u9002\u5e94\u548c\u6f14\u5316\uff0c\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u8303\u5f0f\u6709\u91cd\u8981\u5f71\u54cd", "topic": "code agent"}}
{"id": "2601.12822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12822", "abs": "https://arxiv.org/abs/2601.12822", "authors": ["Wenqi Zhang", "Yulin Shen", "Changyue Jiang", "Jiarun Dai", "Geng Hong", "Xudong Pan"], "title": "MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction", "comment": null, "summary": "Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.", "AI": {"tldr": "MirrorGuard\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u62df\u7684\u8bad\u7ec3\u6765\u589e\u5f3a\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u5728\u6587\u672c\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u9ad8\u98ce\u9669GUI\u4ea4\u4e92\u8f68\u8ff9\uff0c\u5b66\u4e60\u5728\u771f\u5b9e\u64cd\u4f5c\u524d\u62e6\u622a\u548c\u4fee\u6b63\u4e0d\u5b89\u5168\u63a8\u7406\u94fe\u3002", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u4e2d\uff0c\u4f7f\u5176\u80fd\u591f\u901a\u8fc7\u56fe\u5f62\u7528\u6237\u754c\u9762\u81ea\u4e3b\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4ea4\u4e92\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u8fd9\u79cd\u81ea\u4e3b\u6027\u5f15\u5165\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\uff1a\u6076\u610f\u6307\u4ee4\u6216\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u53ef\u80fd\u89e6\u53d1\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u5e76\u5bfc\u81f4\u6709\u5bb3\u7684\u7cfb\u7edf\u7ea7\u64cd\u4f5c\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u68c0\u6d4b\u7684\u963b\u65ad\uff09\u867d\u7136\u80fd\u9632\u6b62\u635f\u5bb3\uff0c\u4f46\u901a\u5e38\u4f1a\u8fc7\u65e9\u4e2d\u6b62\u4efb\u52a1\uff0c\u964d\u4f4e\u4e86\u4ee3\u7406\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faMirrorGuard\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u6a21\u62df\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\u5f00\u53d1\u4e86\u65b0\u9896\u7684\u795e\u7ecf\u7b26\u53f7\u6a21\u62df\u7ba1\u9053\uff0c\u5728\u7eaf\u6587\u672c\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u903c\u771f\u7684\u9ad8\u98ce\u9669GUI\u4ea4\u4e92\u8f68\u8ff9\uff0c\u6355\u83b7\u4e0d\u5b89\u5168\u63a8\u7406\u6a21\u5f0f\u548c\u6f5c\u5728\u7cfb\u7edf\u5371\u9669\uff0c\u800c\u65e0\u9700\u6267\u884c\u771f\u5b9e\u64cd\u4f5c\u3002\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0cMirrorGuard\u5b66\u4e60\u5728CUAs\u4ea7\u751f\u548c\u6267\u884c\u4e0d\u5b89\u5168\u64cd\u4f5c\u4e4b\u524d\u62e6\u622a\u548c\u4fee\u6b63\u5176\u4e0d\u5b89\u5168\u63a8\u7406\u94fe\u3002", "result": "\u5728\u5b57\u8282\u8df3\u52a8UI-TARS\u7cfb\u7edf\u4e0a\uff0cMirrorGuard\u5c06\u4e0d\u5b89\u5168\u7387\u4ece66.5%\u964d\u4f4e\u523013.0%\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u9519\u8bef\u62d2\u7edd\u7387\uff08FRR\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6700\u5148\u8fdb\u7684GuardAgent\u4ec5\u5c06\u4e0d\u5b89\u5168\u7387\u964d\u4f4e\u523053.9%\uff0c\u4e14FRR\u9ad8\u51fa15.4%\u3002\u5728\u591a\u6837\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u548cCUA\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cMirrorGuard\u663e\u8457\u51cf\u8f7b\u4e86\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "\u6a21\u62df\u884d\u751f\u7684\u9632\u5fa1\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u4ee3\u7406\u57fa\u672c\u5b9e\u7528\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u5f3a\u5927\u7684\u771f\u5b9e\u4e16\u754c\u4fdd\u62a4\u3002MirrorGuard\u8bc1\u660e\u4e86\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u529f\u80fd\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u589e\u5f3a\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.12842", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12842", "abs": "https://arxiv.org/abs/2601.12842", "authors": ["Qitong Fang", "Haotian Li", "Xu Wang"], "title": "SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning", "comment": "11 pages, 3 figures. Equal contribution: Qitong Fang and Haotian Li. Corresponding authors: Qitong Fang (fangqitong@student.jlju.edu.cn), Haotian Li (lihaotian@student.jlju.edu.cn), Xu Wang (wangxu@jlju.edu.cn)", "summary": "Automated agent workflows can enhance the problem-solving ability of large language models (LLMs), but common search strategies rely on stochastic exploration and often traverse implausible branches. This occurs because current pipelines sample candidate steps from generic prompts or learned policies with weak domain priors, yielding near-random walks over operators, units, and formats. To promote ordered exploration, this paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, and diversity) and structural pattern guidance, thereby steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets; additional results with GPT-5.2 assess executor transferability and performance on frontier reasoning models. Overall, domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability.", "AI": {"tldr": "SCULPT\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u5f15\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u611f\u77e5\u7684\u7b26\u53f7\u68c0\u67e5\u548c\u7ed3\u6784\u6a21\u5f0f\u6307\u5bfc\u6765\u63d0\u5347LLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u4f9d\u8d56\u968f\u673a\u63a2\u7d22\uff0c\u7ecf\u5e38\u904d\u5386\u4e0d\u5408\u7406\u7684\u63a8\u7406\u5206\u652f\uff0c\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u901a\u7528\u63d0\u793a\u6216\u5f31\u9886\u57df\u5148\u9a8c\u7684\u7b56\u7565\u6765\u91c7\u6837\u5019\u9009\u6b65\u9aa4\uff0c\u5bfc\u81f4\u5728\u64cd\u4f5c\u7b26\u3001\u5355\u4f4d\u548c\u683c\u5f0f\u4e0a\u7684\u8fd1\u968f\u673a\u6e38\u8d70\u3002", "method": "SCULPT\u5c06\u9886\u57df\u611f\u77e5\u8bc4\u5206\u96c6\u6210\u5230MCTS\u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u56de\u4f20\u9636\u6bb5\uff0c\u901a\u8fc7\u7b26\u53f7\u68c0\u67e5\uff08\u7ef4\u5ea6\u4e00\u81f4\u6027\u3001\u7c7b\u578b\u517c\u5bb9\u6027\u3001\u5e45\u5ea6\u5408\u7406\u6027\u3001\u6df1\u5ea6\u63a7\u5236\u548c\u591a\u6837\u6027\uff09\u548c\u7ed3\u6784\u6a21\u5f0f\u6307\u5bfc\u6765\u8bc4\u5206\u548c\u526a\u679d\u52a8\u4f5c\u3002", "result": "\u5728\u76f8\u540c\u7684LLM\u914d\u7f6e\u4e0b\uff0cSCULPT\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u6539\u8fdb\uff1b\u4f7f\u7528GPT-5.2\u7684\u989d\u5916\u7ed3\u679c\u8bc4\u4f30\u4e86\u6267\u884c\u5668\u53ef\u8f6c\u79fb\u6027\u548c\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u9886\u57df\u611f\u77e5\u7ea6\u675f\u53ef\u4ee5\u5728\u4fdd\u6301\u6548\u7387\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.12419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12419", "abs": "https://arxiv.org/abs/2601.12419", "authors": ["Mahammad Namazov", "Tom\u00e1\u0161 Koref", "Ivan Habernal"], "title": "Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification", "comment": null, "summary": "Interpretability is critical for applications of large language models in the legal domain which requires trust and transparency. While some studies develop task-specific approaches, other use the classification model's parameters to explain the decisions. However, which technique explains the legal outcome prediction best remains an open question. To address this challenge, we propose a comparative analysis framework for model-agnostic interpretability techniques. Among these, we employ two rationale extraction methods, which justify outcomes with human-interpretable and concise text fragments (i.e., rationales) from the given input text. We conduct comparison by evaluating faithfulness-via normalized sufficiency and comprehensiveness metrics along with plausibility-by asking legal experts to evaluate extracted rationales. We further assess the feasibility of LLM-as-a-Judge using legal expert evaluation results. We show that the model's \"reasons\" for predicting a violation differ substantially from those of legal experts, despite highly promising quantitative analysis results and reasonable downstream classification performance. The source code of our experiments is publicly available at https://github.com/trusthlt/IntEval.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6bd4\u8f83\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u6cd5\u5f8b\u9886\u57df\u7684LLM\u51b3\u7b56\u89e3\u91ca\uff0c\u901a\u8fc7\u5fe0\u5b9e\u5ea6\u548c\u5408\u7406\u6027\u8bc4\u4f30\u53d1\u73b0\u6a21\u578b\u9884\u6d4b\u7406\u7531\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u6cd5\u5f8b\u9886\u57df\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\uff0c\u4f46\u73b0\u6709\u89e3\u91ca\u6280\u672f\u4e2d\u54ea\u79cd\u6700\u9002\u5408\u6cd5\u5f8b\u7ed3\u679c\u9884\u6d4b\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6bd4\u8f83\u5206\u6790\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u79cd\u7406\u7531\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u5145\u5206\u6027\u548c\u5168\u9762\u6027\u6307\u6807\u8bc4\u4f30\u5fe0\u5b9e\u5ea6\uff0c\u5e76\u8bf7\u6cd5\u5f8b\u4e13\u5bb6\u8bc4\u4f30\u63d0\u53d6\u7406\u7531\u7684\u5408\u7406\u6027\u3002", "result": "\u5c3d\u7ba1\u91cf\u5316\u5206\u6790\u7ed3\u679c\u5f88\u6709\u524d\u666f\u4e14\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd\u5408\u7406\uff0c\u4f46\u6a21\u578b\u9884\u6d4b\u8fdd\u89c4\u7684\"\u7406\u7531\"\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u7684\u7406\u7531\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u7406\u89e3LLM\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63a2\u8ba8\u4e86\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.12912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12912", "abs": "https://arxiv.org/abs/2601.12912", "authors": ["Andreas Br\u00e4nnstr\u00f6m", "Juan Carlos Nieves"], "title": "Human Emotion Verification by Action Languages via Answer Set Programming", "comment": "Under consideration in Theory and Practice of Logic Programming (TPLP)", "summary": "In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\u548c\u8f6c\u79fb\u7cfb\u7edf\u7684C-MT\u8bed\u8a00\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u968f\u53ef\u89c2\u5bdf\u52a8\u4f5c\u7684\u6f14\u5316\uff0c\u7279\u522b\u5173\u6ce8\u60c5\u7eea\u53d8\u5316\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u56e0\u679c\u89c4\u5219\u6765\u63a7\u5236\u4ee3\u7406\u884c\u4e3a\u548c\u9650\u5236\u5fc3\u7406\u526f\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u53ef\u63a7\u4ee3\u7406\u884c\u4e3a\u7684\u652f\u6301\uff0c\u65e0\u6cd5\u6709\u6548\u9650\u5236\u52a8\u4f5c\u5e26\u6765\u7684\u5fc3\u7406\u526f\u4f5c\u7528\u3002\u9700\u8981\u4e00\u79cd\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u5efa\u6a21\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\uff08\u7279\u522b\u662f\u60c5\u7eea\uff09\u968f\u52a8\u4f5c\u5e8f\u5217\u7684\u6f14\u5316\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u5fc3\u7406\u5b66\u7406\u8bba\u7684\u6bd4\u8f83\u3002", "method": "\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\u548c\u8f6c\u79fb\u7cfb\u7edf\u6784\u5efaC-MT\u8bed\u8a00\uff0c\u5229\u7528\u8bc4\u4ef7\u60c5\u7eea\u7406\u8bba\u7b49\u5fc3\u7406\u5b66\u7406\u8bba\u5c06\u5fc3\u7406\u72b6\u6001\u5f62\u5f0f\u5316\u4e3a\u591a\u7ef4\u914d\u7f6e\u3002\u5f15\u5165\u65b0\u7684\"forbids to cause\"\u56e0\u679c\u89c4\u5219\u548c\u4e13\u95e8\u7684\u5fc3\u7406\u72b6\u6001\u52a8\u6001\u8868\u8fbe\u5f0f\uff0c\u5c06\u5fc3\u7406\u53d8\u5316\u539f\u5219\u8f6c\u5316\u4e3a\u8f6c\u79fb\u7ea6\u675f\u548c\u4e0d\u53d8\u6027\u5c5e\u6027\uff0c\u901a\u8fc7\u8f68\u8ff9\u5206\u6790\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u5efa\u6a21\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u52a8\u6001\u6f14\u5316\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u652f\u6301\u53ef\u63a7\u63a8\u7406\u548c\u5fc3\u7406\u53d8\u5316\u539f\u5219\u7684\u6bd4\u8f83\u5206\u6790\u3002\u8be5\u6846\u67b6\u5df2\u5e94\u7528\u4e8e\u60c5\u7eea\u9a8c\u8bc1\u6a21\u578b\u7684\u8bbe\u8ba1\u3002", "conclusion": "C-MT\u8bed\u8a00\u4e3a\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u6f14\u5316\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u5efa\u6a21\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u60c5\u7eea\u9a8c\u8bc1\u548c\u53ef\u63a7\u4ee3\u7406\u884c\u4e3a\u8bbe\u8ba1\uff0c\u652f\u6301\u4e0d\u540c\u5fc3\u7406\u5b66\u7406\u8bba\u7684\u6bd4\u8f83\u5206\u6790\u3002", "topic": "agent analysis"}}
{"id": "2601.12465", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12465", "abs": "https://arxiv.org/abs/2601.12465", "authors": ["Miao Peng", "Weizhou Shen", "Nuo Chen", "Chenliang Li", "Ming Yan", "Jia Li"], "title": "Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the \"almost-there\" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from \"almost-there\" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.", "AI": {"tldr": "\u63d0\u51faDeepReasonQA\u548cLongPAS\u65b9\u6cd5\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2dRLVR\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u9ad8\u96be\u5ea6\u591a\u8df3QA\u5bf9\uff0c\u5e76\u5229\u7528\u8fc7\u7a0b\u4f18\u52bf\u5851\u9020\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6027\u80fd\u3002", "motivation": "RLVR\u5728\u77ed\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u57fa\u7840\u548c\u957f\u8ddd\u79bb\u63a8\u7406\u7684\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\"almost-there\"\u73b0\u8c61\uff0c\u5373\u8f68\u8ff9\u5927\u90e8\u5206\u6b63\u786e\u4f46\u5728\u6700\u540e\u4e00\u6b65\u5931\u8d25\uff0c\u5f52\u56e0\u4e8e\u957f\u4e0a\u4e0b\u6587QA\u6570\u636e\u7f3a\u4e4f\u9ad8\u63a8\u7406\u5bc6\u5ea6\uff0c\u4ee5\u53caRL\u8bad\u7ec3\u4e2d\u5bf9\u90e8\u5206\u6b63\u786e\u8f68\u8ff9\u7684\u60e9\u7f5a\u5bfc\u81f4\u5b66\u4e60\u4fe1\u53f7\u4e22\u5931\u3002", "method": "1. DeepReasonQA\uff1a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u53ef\u63a7\u5408\u6210\u6846\u67b6\uff0c\u6784\u5efa\u5177\u6709\u56fa\u6709\u63a8\u7406\u94fe\u7684\u9ad8\u96be\u5ea6\u591a\u8df3\u957f\u4e0a\u4e0b\u6587QA\u5bf9\uff1b2. LongPAS\uff1a\u957f\u4e0a\u4e0b\u6587\u8fc7\u7a0b\u4f18\u52bf\u5851\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u6548\u6027\u548c\u76f8\u5173\u6027\u7ef4\u5ea6\u8bc4\u4f30\u63a8\u7406\u6b65\u9aa4\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u4ece\"almost-there\"\u8f68\u8ff9\u4e2d\u6355\u83b7\u5173\u952e\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u4e09\u4e2a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8eRLVR\u57fa\u7ebf\uff0c\u4e0e\u524d\u6cbfLLM\u6027\u80fd\u76f8\u5f53\u4f46\u4f7f\u7528\u66f4\u5c11\u53c2\u6570\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8bc1\u5b9e\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684RL\u8bad\u7ec3\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u63a8\u7406\u5bc6\u96c6\u578b\u6570\u636e\u548c\u5f15\u5165\u8fc7\u7a0b\u4f18\u52bf\u5851\u9020\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2dRLVR\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.12471", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12471", "abs": "https://arxiv.org/abs/2601.12471", "authors": ["Sravanthi Machcha", "Sushrita Yerra", "Sahil Gupta", "Aishwarya Sahoo", "Sharmin Sultana", "Hong Yu", "Zonghai Yao"], "title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty", "comment": "Equal contribution for the first two authors; To appear in proceedings of the Main Conference of the European Chapter of the Association for Computational Linguistics (EACL) 2026", "summary": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.", "AI": {"tldr": "MedAbstain\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u533b\u5b66\u591a\u9009\u9898\u4e2d\u5f03\u6743\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u5373\u4f7f\u9ad8\u51c6\u786e\u7387\u6a21\u578b\u4e5f\u5e38\u65e0\u6cd5\u5728\u4e0d\u786e\u5b9a\u65f6\u5f03\u6743\uff0c\u63d0\u4f9b\u660e\u786e\u5f03\u6743\u9009\u9879\u6bd4\u8f93\u5165\u6270\u52a8\u66f4\u80fd\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u8fc7\u4e8e\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u65f6\u80fd\u591f\u5f03\u6743\u540c\u6837\u91cd\u8981\uff0c\u8fd9\u5bf9\u4e8e\u53ef\u4fe1\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165MedAbstain\u57fa\u51c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u6574\u5408\u4e86\u5171\u5f62\u9884\u6d4b\u3001\u5bf9\u6297\u6027\u95ee\u9898\u6270\u52a8\u548c\u660e\u786e\u5f03\u6743\u9009\u9879\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5f00\u6e90\u548c\u95ed\u6e90LLMs\u5728\u533b\u5b66\u591a\u9009\u9898\u4e2d\u7684\u5f03\u6743\u80fd\u529b\u3002", "result": "\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u9ad8\u51c6\u786e\u7387\u6a21\u578b\u4e5f\u7ecf\u5e38\u65e0\u6cd5\u5728\u4e0d\u786e\u5b9a\u65f6\u5f03\u6743\uff1b\u63d0\u4f9b\u660e\u786e\u5f03\u6743\u9009\u9879\u6bd4\u8f93\u5165\u6270\u52a8\u66f4\u80fd\u589e\u52a0\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u5b89\u5168\u5f03\u6743\uff1b\u6269\u5927\u6a21\u578b\u89c4\u6a21\u6216\u9ad8\u7ea7\u63d0\u793a\u5e26\u6765\u7684\u6539\u8fdb\u6709\u9650\u3002", "conclusion": "\u5f03\u6743\u673a\u5236\u5bf9\u4e8eLLM\u53ef\u4fe1\u90e8\u7f72\u5177\u6709\u6838\u5fc3\u4f5c\u7528\uff0c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.13060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13060", "abs": "https://arxiv.org/abs/2601.13060", "authors": ["Zecheng Li", "Zhihui Cao", "Wenke Huang", "Yudong Zhang", "Keying Qi", "Rui Wang", "Zeyu Zheng", "Jian Zhao", "Hao Zhu", "Hengxin Wu", "Yuran Wang", "Guitao Fan", "Guokun Wu", "Yicong Liu", "Zhilin Gao", "Haikun Xu", "He Yang", "Minqi Xiang", "Xingyu Liu", "Zuojian Wang"], "title": "MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux", "comment": null, "summary": "Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.", "AI": {"tldr": "MagicGUI-RMS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5956\u52b1\u6a21\u578b\u7cfb\u7edf\uff0c\u7528\u4e8eGUI\u667a\u80fd\u4f53\u7684\u81ea\u9002\u5e94\u8f68\u8ff9\u8bc4\u4f30\u3001\u7ea0\u6b63\u53cd\u9988\u548c\u81ea\u6211\u8fdb\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u548c\u901a\u7528\u5956\u52b1\u6a21\u578b\u7ed3\u5408\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc4\u4f30\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "motivation": "\u5f53\u524dGUI\u667a\u80fd\u4f53\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u81ea\u52a8\u5316\u8bc4\u4f30\u667a\u80fd\u4f53\u8f68\u8ff9\u548c\u89c4\u6a21\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u9759\u6001\u89c4\u5219\u9a8c\u8bc1\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faMagicGUI-RMS\u591a\u667a\u80fd\u4f53\u5956\u52b1\u6a21\u578b\u7cfb\u7edf\uff0c\u96c6\u6210\u9886\u57df\u7279\u5b9a\u5956\u52b1\u6a21\u578b(DS-RM)\u548c\u901a\u7528\u5956\u52b1\u6a21\u578b(GP-RM)\uff0c\u8bbe\u8ba1\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u7ba1\u9053\u81ea\u52a8\u751f\u6210\u5e73\u8861\u591a\u6837\u7684\u5956\u52b1\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u56de\u6d41\u673a\u5236\u6301\u7eed\u6539\u8fdb\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMagicGUI-RMS\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u884c\u4e3a\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u57fa\u4e8e\u5956\u52b1\u81ea\u9002\u5e94\u6784\u5efa\u81ea\u6211\u6539\u8fdb\u7684GUI\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u7840\u3002", "conclusion": "MagicGUI-RMS\u4e3a\u6784\u5efa\u57fa\u4e8e\u5956\u52b1\u81ea\u9002\u5e94\u7684\u81ea\u6211\u6539\u8fdbGUI\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u6709\u6548\u7684\u57fa\u7840\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.13186", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.13186", "abs": "https://arxiv.org/abs/2601.13186", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "title": "Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching", "comment": "33 pages, 19 figures", "summary": "Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTIVS-O\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\u7f13\u5b58\u548c\u53ef\u89c2\u6d4b\u6027\u8bc4\u5206\uff0c\u5728\u5d4c\u5957\u5b66\u4e60\u67b6\u6784\u4e2d\u7814\u7a76\u9632\u5fa1\u6548\u679c\u4e0e\u900f\u660e\u5ea6\u7684\u6743\u8861\uff0c\u5b9e\u73b0\u96f6\u9ad8\u98ce\u9669\u6f0f\u6d1e\u540c\u65f6\u51cf\u5c1141.6%\u7684LLM\u8c03\u7528\u3002", "motivation": "\u63d0\u793a\u6ce8\u5165\u662f\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2dLLM\u5b89\u5168\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\uff0c\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u7f3a\u4e4f\u5bf9\u9632\u5fa1\u6548\u679c\u4e0e\u900f\u660e\u5ea6\u4e4b\u95f4\u6743\u8861\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6269\u5c55TIVS\u6846\u67b6\uff0c\u589e\u52a0\u8bed\u4e49\u76f8\u4f3c\u6027\u7f13\u5b58\u548c\u7b2c\u4e94\u4e2a\u6307\u6807\uff08\u53ef\u89c2\u6d4b\u6027\u8bc4\u5206\u6bd4\uff09\uff0c\u5728HOPE\u542f\u53d1\u7684\u5d4c\u5957\u5b66\u4e60\u67b6\u6784\u4e2d\u5b9e\u73b0\u667a\u80fd\u4f53\u7ba1\u9053\u4e0e\u8fde\u7eed\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4f7f\u7528301\u4e2a\u5408\u6210\u63d0\u793a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u96f6\u9ad8\u98ce\u9669\u6f0f\u6d1e\uff0c\u8bed\u4e49\u7f13\u5b58\u51cf\u5c1141.6%\u7684LLM\u8c03\u7528\uff0c\u964d\u4f4e\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u78b3\u6392\u653e\uff0c\u4e94\u79cdTIVS-O\u914d\u7f6e\u63ed\u793a\u4e86\u7f13\u89e3\u4e25\u683c\u6027\u4e0e\u53d6\u8bc1\u900f\u660e\u5ea6\u4e4b\u95f4\u7684\u6700\u4f18\u6743\u8861\u3002", "conclusion": "\u53ef\u89c2\u6d4b\u6027\u611f\u77e5\u8bc4\u4f30\u80fd\u63ed\u793a\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u4e2d\u7684\u975e\u5355\u8c03\u6548\u5e94\uff0c\u5185\u5b58\u589e\u5f3a\u667a\u80fd\u4f53\u53ef\u5728\u4e0d\u6539\u52a8\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u540c\u65f6\u6700\u5927\u5316\u5b89\u5168\u9c81\u68d2\u6027\u3001\u5b9e\u65f6\u6027\u80fd\u3001\u8fd0\u8425\u6210\u672c\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.13206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13206", "abs": "https://arxiv.org/abs/2601.13206", "authors": ["Neil K. R. Sehgal", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues", "comment": null, "summary": "Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\\% vs. 4\\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\\geq$95\\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.", "AI": {"tldr": "LLMs\u5728\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u6301\u7eed\u8ffd\u8e2a\u5269\u4f59\u65f6\u95f4\u7684\u5b9e\u65f6\u8c08\u5224\u4e2d\uff0c\u4f46\u901a\u8fc7\u63d0\u4f9b\u5269\u4f59\u65f6\u95f4\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u6539\u5584\u8868\u73b0\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6c9f\u901a\uff08\u5982\u6cbb\u7597\u4f1a\u8bdd\u3001\u5546\u4e1a\u8c08\u5224\uff09\u901a\u5e38\u6709\u65f6\u95f4\u9650\u5236\uff0c\u4f46\u5f53\u524dLLM\u67b6\u6784\u548c\u8bc4\u4f30\u534f\u8bae\u5f88\u5c11\u6d4b\u8bd5\u5176\u5728\u5b9e\u65f6\u622a\u6b62\u65f6\u95f4\u4e0b\u7684\u65f6\u95f4\u610f\u8bc6\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u8c08\u5224\u5b9e\u9a8c\uff0c\u8ba9\u6210\u5bf9\u7684LLM\u4ee3\u7406\u5728\u4e25\u683c\u622a\u6b62\u65f6\u95f4\u4e0b\u8fdb\u884c\u8c08\u5224\u3002\u8bbe\u7f6e\u4e24\u4e2a\u6761\u4ef6\uff1a\u63a7\u5236\u7ec4\uff08\u4ec5\u77e5\u9053\u5168\u5c40\u65f6\u95f4\u9650\u5236\uff09\u548c\u65f6\u95f4\u611f\u77e5\u7ec4\uff08\u6bcf\u8f6e\u83b7\u5f97\u5269\u4f59\u65f6\u95f4\u66f4\u65b0\uff09\u3002\u6bd4\u8f83\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u4ea4\u6613\u8fbe\u6210\u7387\u3002", "result": "\u65f6\u95f4\u611f\u77e5\u6761\u4ef6\u4e0b\u7684\u4ea4\u6613\u8fbe\u6210\u7387\u663e\u8457\u66f4\u9ad8\uff08GPT-5.1\u4e3a32% vs 4%\uff09\uff0c\u62a5\u4ef7\u63a5\u53d7\u7387\u9ad8\u51fa\u516d\u500d\u3002\u4f46\u5728\u57fa\u4e8e\u8f6e\u6b21\u7684\u9650\u5236\u4e0b\uff0cLLMs\u80fd\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u4ea4\u6613\u8fbe\u6210\u7387\uff08\u226595%\uff09\u3002", "conclusion": "LLMs\u5728\u5185\u90e8\u8ffd\u8e2a\u6d41\u901d\u65f6\u95f4\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u8bb8\u591a\u65f6\u95f4\u654f\u611f\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002\u5931\u8d25\u6e90\u4e8e\u65f6\u95f4\u8ffd\u8e2a\u80fd\u529b\u800c\u975e\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.12401", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12401", "abs": "https://arxiv.org/abs/2601.12401", "authors": ["Jinmei Liu", "Haoru Li", "Zhenhong Sun", "Chaofeng Chen", "Yatao Bian", "Bo Wang", "Daoyi Dong", "Chunlin Chen", "Zhi Wang"], "title": "Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \\textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \\textbf{DRIFT} (\\textbf{D}ive\\textbf{R}sity-\\textbf{I}ncentivized Reinforcement \\textbf{F}ine-\\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \\textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \\textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \\textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\\%\\!\\sim\\! 43.46\\%$ increase in diversity at equivalent alignment levels and a $ 59.65\\% \\!\\sim\\! 65.86\\%$ increase in alignment at equivalent levels of diversity.", "AI": {"tldr": "DRIFT\u6846\u67b6\u901a\u8fc7\u91c7\u6837\u3001\u63d0\u793a\u548c\u4f18\u5316\u4e09\u4e2a\u89d2\u5ea6\u6fc0\u52b1\u591a\u6837\u6027\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u751f\u6210\u6a21\u578b\u65f6\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u4efb\u52a1\u5bf9\u9f50\u548c\u751f\u6210\u591a\u6837\u6027\u4e4b\u95f4\u5b9e\u73b0\u5e15\u7d2f\u6258\u4f18\u52bf\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u65f6\u5b58\u5728\"\u591a\u6837\u6027\u5d29\u6e83\u8bc5\u5492\"\uff0c\u76ee\u6807\u51fd\u6570\u548c\u4f18\u5316\u8fc7\u7a0b\u5bfc\u81f4\u7b56\u7565\u574d\u7f29\u4e3a\u72c4\u62c9\u514b\u5206\u5e03\uff0c\u9650\u5236\u4e86\u751f\u6210\u591a\u6837\u6027\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u9700\u8981\u591a\u6837\u5316\u7684\u5019\u9009\u751f\u6210\u3002", "method": "\u63d0\u51faDRIFT\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u89d2\u5ea6\u7cfb\u7edf\u6fc0\u52b1\u591a\u6837\u6027\uff1a1) \u91c7\u6837\u5956\u52b1\u96c6\u4e2d\u5b50\u96c6\u8fc7\u6ee4\u5f02\u5e38\u503c\u9632\u6b62\u8fc7\u65e9\u5d29\u6e83\uff1b2) \u4f7f\u7528\u968f\u673a\u53d8\u4f53\u63d0\u793a\u6269\u5c55\u6761\u4ef6\u7a7a\u95f4\uff1b3) \u901a\u8fc7\u57fa\u4e8e\u52bf\u80fd\u7684\u5956\u52b1\u5851\u5f62\u673a\u5236\u4f18\u5316\u7ec4\u5185\u591a\u6837\u6027\u3002", "result": "DRIFT\u5728\u4efb\u52a1\u5bf9\u9f50\u548c\u751f\u6210\u591a\u6837\u6027\u65b9\u9762\u5b9e\u73b0\u5e15\u7d2f\u6258\u4f18\u52bf\uff1a\u5728\u540c\u7b49\u5bf9\u9f50\u6c34\u5e73\u4e0b\u591a\u6837\u6027\u63d0\u53479.08%~43.46%\uff0c\u5728\u540c\u7b49\u591a\u6837\u6027\u6c34\u5e73\u4e0b\u5bf9\u9f50\u5ea6\u63d0\u534759.65%~65.86%\u3002", "conclusion": "DRIFT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u751f\u6210\u6a21\u578b\u65f6\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u5bf9\u9f50\u4e0e\u751f\u6210\u591a\u6837\u6027\u7684\u5e73\u8861\uff0c\u589e\u5f3a\u4e86\u751f\u6210\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.13262", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13262", "abs": "https://arxiv.org/abs/2601.13262", "authors": ["Eric Onyame", "Akash Ghosh", "Subhadip Baidya", "Sriparna Saha", "Xiuying Chen", "Chirag Agarwal"], "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning", "comment": null, "summary": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/", "AI": {"tldr": "CURE-MED\u63d0\u51fa\u4e00\u4e2a\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u5207\u6362\u611f\u77e5\u7684\u76d1\u7763\u5fae\u8c03\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5728\u591a\u8bed\u8a00\u533b\u7597\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u8bed\u8a00\u4e00\u81f4\u6027\u548c\u903b\u8f91\u6b63\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5355\u8bed\u8a00\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u533b\u7597\u63a8\u7406\u5e94\u7528\u4e2d\u4ecd\u4e0d\u53ef\u9760\uff0c\u963b\u788d\u4e86\u5176\u5728\u591a\u8bed\u8a00\u533b\u7597\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u9996\u5148\u5f15\u5165CUREMED-BENCH\u6570\u636e\u96c6\uff0c\u5305\u542b13\u79cd\u8bed\u8a00\u7684\u5f00\u6e90\u533b\u7597\u63a8\u7406\u67e5\u8be2\uff1b\u7136\u540e\u63d0\u51faCURE-MED\u6846\u67b6\uff0c\u6574\u5408\u4ee3\u7801\u5207\u6362\u611f\u77e5\u7684\u76d1\u7763\u5fae\u8c03\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5171\u540c\u63d0\u5347\u903b\u8f91\u6b63\u786e\u6027\u548c\u8bed\u8a00\u7a33\u5b9a\u6027\u3002", "result": "\u572813\u79cd\u8bed\u8a00\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u5e76\u6709\u6548\u6269\u5c55\uff1a7B\u53c2\u6570\u6a21\u578b\u8fbe\u523085.21%\u8bed\u8a00\u4e00\u81f4\u6027\u548c54.35%\u903b\u8f91\u6b63\u786e\u6027\uff1b32B\u53c2\u6570\u6a21\u578b\u8fbe\u523094.96%\u8bed\u8a00\u4e00\u81f4\u6027\u548c70.04%\u903b\u8f91\u6b63\u786e\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u652f\u6301LLMs\u5b9e\u73b0\u53ef\u9760\u4e14\u516c\u5e73\u7684\u591a\u8bed\u8a00\u533b\u7597\u63a8\u7406\uff0c\u4fc3\u8fdb\u4e86\u591a\u8bed\u8a00\u533b\u7597\u73af\u5883\u4e2dAI\u7cfb\u7edf\u7684\u90e8\u7f72\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.12607", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12607", "abs": "https://arxiv.org/abs/2601.12607", "authors": ["Anurag Acharya", "Timothy Vega", "Rizwan A. Ashraf", "Anshu Sharma", "Derek Parker", "Robert Rallo"], "title": "A Cloud-based Multi-Agentic Workflow for Science", "comment": null, "summary": "As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u3001\u6a21\u578b\u72ec\u7acb\u7684\u4e91\u7aef\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4f5c\u4e3a\u79d1\u5b66\u52a9\u624b\uff0c\u80fd\u591f\u5904\u7406\u4ece\u6587\u732e\u7efc\u8ff0\u5230\u6a21\u62df\u8fd0\u884c\u7b49\u4efb\u52a1\uff0c\u5728\u50ac\u5316\u5242\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u8fd0\u884c\u6a21\u62df\u6216\u505a\u51fa\u590d\u6742\u51b3\u7b56\uff09\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5176\u5728\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u3002\u867d\u7136\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u80fd\u591f\u8c03\u7528\u5916\u90e8\u8d44\u6e90\u548c\u5de5\u5177\uff0c\u4f46\u8bbe\u8ba1\u5e73\u8861\u6a21\u578b\u3001\u4e91\u63d0\u4f9b\u5546\u548c\u5916\u90e8\u8d44\u6e90\u7684\u5de5\u4f5c\u6d41\u7a0b\u975e\u5e38\u56f0\u96be\uff0c\u4f7f\u5f97\u5b9e\u73b0\u667a\u80fd\u4f53\u7cfb\u7edf\u53d8\u5f97\u590d\u6742\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u3001\u6a21\u578b\u72ec\u7acb\u7684\u4e91\u7aef\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u91c7\u7528\u76d1\u7763\u8005\u667a\u80fd\u4f53\u534f\u8c03\u591a\u4e2a\u5177\u6709\u7279\u5b9a\u80fd\u529b\u7684\u667a\u80fd\u4f53\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u6587\u732e\u7efc\u8ff0\u3001\u6570\u636e\u5206\u6790\u7b49\u7b80\u5355\u4efb\u52a1\u548c\u6a21\u62df\u8fd0\u884c\u7b49\u590d\u6742\u4efb\u52a1\u3002\u5728\u50ac\u5316\u5242\u7814\u7a76\u9886\u57df\u6784\u5efa\u4e86\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6846\u67b6\u8bbe\u8ba1\u3002", "result": "\u7cfb\u7edf\u80fd\u591f90%\u7684\u65f6\u95f4\u5c06\u4efb\u52a1\u8def\u7531\u5230\u6b63\u786e\u7684\u667a\u80fd\u4f53\uff0c\u5728\u5408\u6210\u4efb\u52a1\u4e2d\u6210\u529f\u5b8c\u6210\u4efb\u52a197.5%\u7684\u65f6\u95f4\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u6210\u529f91%\u7684\u65f6\u95f4\u3002\u4e0e\u524d\u6cbf\u6a21\u578b\u76f8\u6bd4\uff0c\u8fbe\u5230\u76f8\u5f53\u6216\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002\u62a5\u544a\u4e86\u8fd0\u8425\u6210\u672c\u53ca\u5404\u9879\u670d\u52a1\u7684\u6210\u672c\u7ec6\u5206\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u53ef\u884c\u7684\u79d1\u5b66\u52a9\u624b\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u79d1\u5b66\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5728\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u590d\u5236\u7684\u6f5c\u529b\u3002\u7cfb\u7edf\u5728\u4efb\u52a1\u8def\u7531\u548c\u5b8c\u6210\u7387\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6210\u672c\u53ef\u63a7\u3002", "topic": "agent analysis"}}
{"id": "2601.13358", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13358", "abs": "https://arxiv.org/abs/2601.13358", "authors": ["Samuel Cyrenius Anderson"], "title": "The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models", "comment": "34 pages, 10 figures", "summary": "Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u6269\u5f20\u4e0d\u4f1a\u5747\u5300\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u800c\u662f\u4f1a\u91cd\u6784\u63a8\u7406\u8fc7\u7a0b\u3002\u901a\u8fc7\u5206\u679025,000+\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u53d1\u73b0\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u4f1a\u89e6\u53d1\u9886\u57df\u7279\u5b9a\u7684\u76f8\u53d8\uff1a\u6cd5\u5f8b\u63a8\u7406\u53d1\u751f\"\u7ed3\u6676\u5316\"\uff0c\u79d1\u5b66\u548c\u6570\u5b66\u63a8\u7406\u4fdd\u6301\"\u6db2\u6001\"\uff0c\u4ee3\u7801\u63a8\u7406\u5f62\u6210\"\u79bb\u6563\u683c\u70b9\"\u3002\u51e0\u4f55\u7ed3\u6784\u53ef\u9884\u6d4b\u53ef\u5b66\u4e60\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u795e\u7ecf\u63a8\u7406\u7b97\u5b50\u6765\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u6a21\u578b\u89c4\u6a21\u6269\u5f20\u5982\u4f55\u5f71\u54cd\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u63a2\u7d22\u7f29\u653e\u5b9a\u5f8b\u662f\u5426\u5747\u5300\u63d0\u5347\u6240\u6709\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4f1a\u5bfc\u81f4\u4e0d\u540c\u9886\u57df\u7684\u7ed3\u6784\u6027\u53d8\u5316\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5206\u679025,000+\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u6db5\u76d6\u6cd5\u5f8b\u3001\u79d1\u5b66\u3001\u4ee3\u7801\u3001\u6570\u5b66\u56db\u4e2a\u9886\u57df\u548c8B\u300170B\u4e24\u4e2a\u89c4\u6a21\uff1b2) \u6d4b\u91cf\u8868\u793a\u7ef4\u5ea6\u3001\u8f68\u8ff9\u5bf9\u9f50\u5ea6\u3001\u6d41\u5f62\u89e3\u7f20\u7b49\u51e0\u4f55\u7279\u5f81\uff1b3) \u63d0\u51fa\u795e\u7ecf\u63a8\u7406\u7b97\u5b50\uff0c\u5b66\u4e60\u4ece\u521d\u59cb\u9690\u85cf\u72b6\u6001\u5230\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u7684\u6620\u5c04\uff1b4) \u8bc6\u522b\u8de8\u9886\u57df\u548c\u89c4\u6a21\u7684\u632f\u8361\u7279\u5f81\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a1) \u6cd5\u5f8b\u63a8\u7406\u53d1\u751f\"\u7ed3\u6676\u5316\"\uff1a\u8868\u793a\u7ef4\u5ea6\u4e0b\u964d45%\uff0c\u8f68\u8ff9\u5bf9\u9f50\u5ea6\u589e\u52a031%\uff0c\u6d41\u5f62\u89e3\u7f20\u5ea6\u63d0\u534710\u500d\uff1b2) \u79d1\u5b66\u548c\u6570\u5b66\u63a8\u7406\u4fdd\u6301\"\u6db2\u6001\"\uff1a\u51e0\u4f55\u7279\u5f81\u57289\u500d\u53c2\u6570\u589e\u52a0\u4e0b\u4fdd\u6301\u4e0d\u53d8\uff1b3) \u4ee3\u7801\u63a8\u7406\u5f62\u6210\"\u79bb\u6563\u683c\u70b9\"\uff1a\u8f6e\u5ed3\u7cfb\u6570\u4ece0.13\u63d0\u5347\u52300.42\uff1b4) \u795e\u7ecf\u63a8\u7406\u7b97\u5b50\u5728\u6cd5\u5f8b\u63a8\u7406\u4e0a\u8fbe\u523063.6%\u7684\u51c6\u786e\u7387\uff1b5) \u53d1\u73b0\u8de8\u9886\u57df\u548c\u89c4\u6a21\u7684\u666e\u904d\u632f\u8361\u7279\u5f81(\u76f8\u5e72\u6027\u7ea6-0.4)\u3002", "conclusion": "\u7ed3\u8bba\u662f\u63a8\u7406\u6210\u672c\u7531\u6d41\u5f62\u51e0\u4f55\u800c\u975e\u4efb\u52a1\u96be\u5ea6\u51b3\u5b9a\uff0c\u8fd9\u4e3a\u5728\u62d3\u6251\u5141\u8bb8\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u63a8\u7406\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002\u6a21\u578b\u89c4\u6a21\u6269\u5f20\u4f1a\u89e6\u53d1\u9886\u57df\u7279\u5b9a\u7684\u76f8\u53d8\uff0c\u800c\u4e0d\u662f\u5747\u5300\u7684\u80fd\u529b\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2601.13383", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13383", "abs": "https://arxiv.org/abs/2601.13383", "authors": ["Akbar Anbar Jafari", "Cagri Ozcinar", "Gholamreza Anbarjafari"], "title": "A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge", "comment": "15 pages, 3 figures", "summary": "The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.", "AI": {"tldr": "AgentForge\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90Python\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u7b80\u5316LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u4ee3\u7406\u6784\u5efa\uff0c\u63d0\u4f9b\u53ef\u7ec4\u5408\u6280\u80fd\u62bd\u8c61\u3001\u7edf\u4e00LLM\u540e\u7aef\u63a5\u53e3\u548c\u58f0\u660e\u5f0fYAML\u914d\u7f6e\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u6846\u67b6\u5b58\u5728\u67b6\u6784\u50f5\u5316\u3001\u4f9b\u5e94\u5546\u9501\u5b9a\u548c\u590d\u6742\u6027\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u90e8\u7f72\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6c11\u4e3b\u5316\u81ea\u4e3b\u4ee3\u7406\u5f00\u53d1\u3002", "method": "\u63d0\u51faAgentForge\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u53ef\u7ec4\u5408\u6280\u80fd\u62bd\u8c61\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5206\u89e3\uff1b2) \u7edf\u4e00LLM\u540e\u7aef\u63a5\u53e3\uff0c\u652f\u6301\u4e91\u7aefAPI\u548c\u672c\u5730\u63a8\u7406\u5f15\u64ce\u65e0\u7f1d\u5207\u6362\uff1b3) \u58f0\u660e\u5f0fYAML\u914d\u7f6e\u7cfb\u7edf\uff0c\u5206\u79bb\u4ee3\u7406\u903b\u8f91\u4e0e\u5b9e\u73b0\u7ec6\u8282\u3002\u5c06\u6280\u80fd\u7ec4\u5408\u673a\u5236\u5f62\u5f0f\u5316\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u573a\u666f\u7684\u8bc4\u4f30\u4e2d\uff0cAgentForge\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u76f8\u6bd4LangChain\u51cf\u5c1162%\u5f00\u53d1\u65f6\u95f4\uff0c\u76f8\u6bd4\u76f4\u63a5API\u96c6\u6210\u51cf\u5c1178%\u5f00\u53d1\u65f6\u95f4\u3002\u7f16\u6392\u5ef6\u8fdf\u4f4e\u4e8e100ms\uff0c\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002\u6846\u67b6\u96c6\u6210\u4e86\u516d\u4e2a\u5185\u7f6e\u6280\u80fd\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6280\u80fd\u5f00\u53d1\u3002", "conclusion": "AgentForge\u586b\u8865\u4e86LLM\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u91cd\u8981\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u81ea\u4e3b\u4ee3\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.13443", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13443", "abs": "https://arxiv.org/abs/2601.13443", "authors": ["H\u00e9ctor Manuel Manzanilla-Granados", "Zaira Navarrete-Cazales", "Miriam Pescador-Rojas", "Tonahtiu Ram\u00edrez-Romero"], "title": "Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models", "comment": "Preprint. This version corresponds to the initial public release of the CUA architecture and associated evaluation metrics", "summary": "The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.\n  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.\n  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u663e\u5f0f\u8ba4\u77e5\u5206\u914d\"\u539f\u5219\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u7ec4\u7ec7\u8ba4\u77e5\u529f\u80fd\u6765\u7ed3\u6784\u5316AI\u8f85\u52a9\u63a8\u7406\uff0c\u5e76\u5b9e\u73b0\u4e3a\u8ba4\u77e5\u901a\u7528\u4ee3\u7406\u67b6\u6784\uff0c\u5728\u519c\u4e1a\u9886\u57df\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u66f4\u597d\u7684\u8ba4\u77e5\u6536\u655b\u548c\u5de5\u5177\u610f\u8bc6\u3002", "motivation": "\u5f53\u524dLLM\u4f7f\u7528\u65b9\u5f0f\u5728\u8ba4\u77e5\u4e0a\u7f3a\u4e4f\u7ed3\u6784\uff0c\u5c06\u95ee\u9898\u6846\u67b6\u3001\u77e5\u8bc6\u63a2\u7d22\u3001\u68c0\u7d22\u3001\u65b9\u6cd5\u610f\u8bc6\u548c\u89e3\u91ca\u7b49\u8ba4\u77e5\u529f\u80fd\u5408\u5e76\u5230\u5355\u4e00\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u9650\u5236\u4e86\u53ef\u8ffd\u6eaf\u6027\u3001\u8ba4\u77e5\u63a7\u5236\u548c\u53ef\u91cd\u590d\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u8d23\u4efb\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u663e\u5f0f\u8ba4\u77e5\u5206\u914d\u539f\u5219\uff0c\u5b9e\u73b0\u4e3a\u8ba4\u77e5\u901a\u7528\u4ee3\u7406\u67b6\u6784\uff0c\u5c06\u63a8\u7406\u7ec4\u7ec7\u4e3a\u63a2\u7d22\u4e0e\u6846\u67b6\u3001\u8ba4\u77e5\u951a\u5b9a\u3001\u5de5\u5177\u4e0e\u65b9\u6cd5\u6620\u5c04\u3001\u89e3\u91ca\u5408\u6210\u7b49\u9636\u6bb5\uff0c\u5e76\u5f15\u5165\u901a\u7528\u8ba4\u77e5\u5de5\u5177\u6982\u5ff5\u6765\u5f62\u5f0f\u5316\u5404\u79cd\u8c03\u67e5\u624b\u6bb5\u3002", "result": "\u5728\u519c\u4e1a\u9886\u57df\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u4e2d\uff0cCUA\u5f15\u5bfc\u7684\u63a8\u7406\u663e\u793a\u51fa\u66f4\u65e9\u4e14\u7ed3\u6784\u5316\u7684\u8ba4\u77e5\u6536\u655b\u3001\u5728\u8bed\u4e49\u6269\u5c55\u4e0b\u66f4\u9ad8\u7684\u8ba4\u77e5\u5bf9\u9f50\u5ea6\uff0c\u5e76\u80fd\u7cfb\u7edf\u6027\u5730\u66b4\u9732\u8c03\u67e5\u7684\u5de5\u5177\u666f\u89c2\uff0c\u800c\u57fa\u7ebfLLM\u63a8\u7406\u5728\u8ba4\u77e5\u5bf9\u9f50\u4e0a\u8868\u73b0\u51fa\u66f4\u5927\u53d8\u5f02\u6027\u4e14\u65e0\u6cd5\u663e\u5f0f\u5448\u73b0\u5de5\u5177\u7ed3\u6784\u3002", "conclusion": "\u663e\u5f0f\u8ba4\u77e5\u5206\u914d\u539f\u5219\u548cCUA\u67b6\u6784\u80fd\u591f\u6709\u6548\u7ed3\u6784\u5316AI\u8f85\u52a9\u63a8\u7406\uff0c\u63d0\u9ad8\u8ba4\u77e5\u63a7\u5236\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u4e3a\u9ad8\u8d23\u4efb\u73af\u5883\u4e2d\u7684AI\u8f85\u52a9\u63a8\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.12698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12698", "abs": "https://arxiv.org/abs/2601.12698", "authors": ["Qiuyi Qu", "Yicheng Sui", "Yufei Sun", "Rui Chen", "Xiaofei Zhang", "Yuzhi Zhang", "Haofeng Wang", "Ge Lan", "Ning Zhang"], "title": "A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization", "comment": null, "summary": "GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u677f\u5316\u91cd\u5199\u5c42\u7ed3\u5408\u641c\u7d22\u81ea\u52a8\u8c03\u4f18\u7684GPU\u5185\u6838\u4f18\u5316\u65b9\u6cd5\uff0c\u76f8\u6bd4\u76f4\u63a5\u4ee3\u7801\u91cd\u5199\u66f4\u7a33\u5b9a\u53ef\u63a7\uff0c\u5b9e\u9a8c\u663e\u793a\u6700\u4f73\u60c5\u51b5\u4e0b\u53ef\u5b9e\u73b03\u500d\u4ee5\u4e0a\u52a0\u901f\u3002", "motivation": "GPU\u4ee3\u7801\u4f18\u5316\u5bf9HPC\u548c\u5927\u6a21\u578b\u8bad\u7ec3/\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709LLM\u4ee3\u7406\u65b9\u6cd5\u591a\u4f9d\u8d56\u76f4\u63a5\u4ee3\u7801\u91cd\u5199\uff0c\u53c2\u6570\u9009\u62e9\u9690\u5f0f\u4e14\u96be\u4ee5\u63a7\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u4e0d\u7a33\u5b9a\u3002", "method": "\u5728\u4ee3\u7406\u9a71\u52a8\u8fed\u4ee3\u5faa\u73af\u4e0a\u589e\u52a0\u6a21\u677f\u5316\u91cd\u5199\u5c42\uff1a\u5c06\u5185\u6838\u8bed\u4e49\u91cd\u6784\u4e3a\u663e\u5f0f\u53c2\u6570\u5316\u6a21\u677f\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8e\u641c\u7d22\u7684\u81ea\u52a8\u8c03\u4f18\u4f18\u5316\u6a21\u677f\u53c2\u6570\uff0c\u7ed3\u5408\u6027\u80fd\u5206\u6790\u53cd\u9988\u8fdb\u884c\u7ea6\u675f\u53c2\u6570\u641c\u7d22\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5185\u6838\u5b9e\u9a8c\u4e2d\uff0c\u6700\u4f73\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8d85\u8fc73\u500d\u7684\u52a0\u901f\u3002\u76f8\u6bd4\u7eaf\u4ee3\u7406\u76f4\u63a5\u91cd\u5199\uff0c\u6a21\u677f\u52a0\u641c\u7d22\u8bbe\u8ba1\u663e\u8457\u51cf\u5c11\u8fed\u4ee3\u4f18\u5316\u7684\u968f\u673a\u6027\uff0c\u4f7f\u8fc7\u7a0b\u66f4\u53ef\u89e3\u91ca\u3002", "conclusion": "\u6a21\u677f\u5316\u91cd\u5199\u4e0e\u641c\u7d22\u81ea\u52a8\u8c03\u4f18\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u4e3aGPU\u5185\u6838\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u7cfb\u7edf\u3001\u7a33\u5b9a\u7684\u9014\u5f84\uff0c\u53ef\u6269\u5c55\u5230OpenCL\u3001HIP\u7b49\u540e\u7aef\uff0c\u5b9e\u73b0\u751f\u4ea7\u5de5\u4f5c\u8d1f\u8f7d\u7684\u81ea\u52a8\u5316\u6027\u80fd\u4f18\u5316\u3002", "topic": "code agent"}}
{"id": "2601.12731", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12731", "abs": "https://arxiv.org/abs/2601.12731", "authors": ["Stefano Civelli", "Pietro Bernardelle", "Nicol\u00f2 Brunello", "Gianluca Demartini"], "title": "A Shared Geometry of Difficulty in Multilingual Language Models", "comment": null, "summary": "Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.", "AI": {"tldr": "LLMs\u5bf9\u95ee\u9898\u96be\u5ea6\u7684\u9884\u6d4b\u5448\u73b0\u4e24\u9636\u6bb5\u8868\u5f81\uff1a\u6d45\u5c42\u8868\u5f81\u5f62\u6210\u8bed\u8a00\u65e0\u5173\u7684\u96be\u5ea6\u4f30\u8ba1\uff0c\u6df1\u5c42\u8868\u5f81\u5219\u53d8\u5f97\u8bed\u8a00\u7279\u5b9a\u5316\u3002", "motivation": "\u7814\u7a76LLMs\u4e2d\u95ee\u9898\u96be\u5ea6\u9884\u6d4b\u7684\u591a\u8bed\u8a00\u51e0\u4f55\u7279\u6027\uff0c\u63a2\u7d22\u96be\u5ea6\u76f8\u5173\u4fe1\u53f7\u5728\u4e0d\u540c\u6a21\u578b\u5185\u90e8\u5c42\u6b21\u7684\u8868\u5f81\u65b9\u5f0f\u53ca\u5176\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528Easy2Hard\u57fa\u51c6\u7684AMC\u5b50\u96c6\uff0c\u7ffb\u8bd1\u621021\u79cd\u8bed\u8a00\uff0c\u5728LLMs\u5185\u90e8\u8868\u5f81\u4e0a\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\uff0c\u5206\u6790\u6d45\u5c42\uff08\u65e9\u671f\u5c42\uff09\u548c\u6df1\u5c42\uff08\u540e\u671f\u5c42\uff09\u8868\u5f81\u7684\u529f\u80fd\u5dee\u5f02\u3002", "result": "\u6df1\u5c42\u8868\u5f81\u63a2\u9488\u5728\u540c\u8bed\u8a00\u4e0a\u51c6\u786e\u7387\u9ad8\u4f46\u8de8\u8bed\u8a00\u6cdb\u5316\u5dee\uff1b\u6d45\u5c42\u8868\u5f81\u63a2\u9488\u5728\u540c\u8bed\u8a00\u4e0a\u51c6\u786e\u7387\u8f83\u4f4e\u4f46\u8de8\u8bed\u8a00\u6cdb\u5316\u663e\u8457\u66f4\u597d\uff0c\u8868\u660eLLMs\u5148\u5f62\u6210\u8bed\u8a00\u65e0\u5173\u7684\u96be\u5ea6\u8868\u5f81\uff0c\u540e\u53d8\u4e3a\u8bed\u8a00\u7279\u5b9a\u5316\u3002", "conclusion": "LLMs\u5bf9\u95ee\u9898\u96be\u5ea6\u4f30\u8ba1\u5b58\u5728\u4e24\u9636\u6bb5\u8868\u5f81\u8fc7\u7a0b\uff1a\u5148\u62bd\u8c61\u6982\u5ff5\u7a7a\u95f4\uff08\u8bed\u8a00\u65e0\u5173\uff09\uff0c\u540e\u8bed\u8a00\u7279\u5b9a\u8f93\u51fa\uff0c\u8fd9\u4e0eLLM\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u7684\u53d1\u73b0\u4e00\u81f4\uff0c\u4e14\u6269\u5c55\u5230\u4e86\u5143\u8ba4\u77e5\u5c5e\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.13481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13481", "abs": "https://arxiv.org/abs/2601.13481", "authors": ["Jian Zhang", "Zhangqi Wang", "Zhiyuan Wang", "Weiping Fu", "Yu He", "Haiping Zhu", "Qika Lin", "Jun Liu"], "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement", "comment": null, "summary": "Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.", "AI": {"tldr": "APOLO\u662f\u4e00\u4e2a\u7528\u4e8e\u7cbe\u795e\u5065\u5eb7\u9886\u57df\u60c5\u611f\u8bca\u65ad\u7684\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u89e3\u51b3\u60c5\u611f\u5171\u75c5\u548c\u4e34\u5e8a\u7ebf\u7d22\u63a2\u7d22\u6548\u7387\u95ee\u9898\uff0c\u63d0\u5347LLM\u5728\u533b\u7597\u573a\u666f\u4e0b\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u3001\u4e0a\u4e0b\u6587\u5bc6\u96c6\u7684\u533b\u7597\u73af\u5883\u4e2d\uff0c\u5176\u8bca\u65ad\u53ef\u9760\u6027\u9ad8\u5ea6\u4f9d\u8d56\u63d0\u793a\u8bbe\u8ba1\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u60c5\u611f\u5171\u75c5\uff08\u591a\u79cd\u60c5\u611f\u72b6\u6001\u4ea4\u7ec7\uff09\u548c\u4e34\u5e8a\u76f8\u5173\u7ebf\u7d22\u63a2\u7d22\u6548\u7387\u4f4e\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faAPOLO\u6846\u67b6\uff0c\u5c06\u6307\u4ee4\u4f18\u5316\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\uff08\u89c4\u5212\u8005\u3001\u6559\u5e08\u3001\u6279\u8bc4\u8005\u3001\u5b66\u751f\u3001\u76ee\u6807\u8005\uff09\uff0c\u5728\u95ed\u73af\u6846\u67b6\u4e2d\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\uff0c\u63d0\u5347\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPOLO\u5728\u9886\u57df\u7279\u5b9a\u548c\u5206\u5c42\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7cbe\u795e\u5065\u5eb7\u9886\u57df\u53ef\u4fe1\u8d56\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u7684\u8303\u5f0f\u3002", "conclusion": "APOLO\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u4f18\u5316\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597\u60c5\u611f\u8bca\u65ad\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u9ad8\u98ce\u9669\u533b\u7597\u73af\u5883\u4e2d\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.12748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12748", "abs": "https://arxiv.org/abs/2601.12748", "authors": ["Bin Xie", "Bingbing Xu", "Xueyun Tian", "Yilin Chen", "Huawei Shen"], "title": "Towards Robust Process Reward Modeling via Noise-aware Learning", "comment": null, "summary": "Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \\underline{\\textbf{N}}oise-\\underline{\\textbf{A}}ware \\underline{\\textbf{I}}terative \\underline{\\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\\% absolute gain in average F1 over PRMs trained with noisy supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\u89e3\u51b3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u566a\u58f0\u76d1\u7763\u95ee\u9898\uff1a\u6807\u7b7e\u9636\u6bb5\u4f7f\u7528LLM\u68c0\u6d4b\u53cd\u601d\u884c\u4e3a\u4fee\u6b63\u6807\u7b7e\uff0c\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\u9010\u6b65\u7cbe\u70bc\u566a\u58f0\u6807\u7b7e\u3002", "motivation": "\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u53d7\u9650\u4e8e\u6602\u8d35\u7684\u9010\u8fc7\u7a0b\u76d1\u7763\u3002\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6cd5\u4f1a\u4ea7\u751f\u7b56\u7565\u4f9d\u8d56\u7684\u5956\u52b1\uff0c\u5bfc\u81f4\u6807\u7b7e\u566a\u58f0\uff08\u5305\u62ec\u9519\u8bef\u5956\u52b1\u6b63\u786e\u6b65\u9aa4\u548c\u60e9\u7f5a\u6b63\u786e\u6b65\u9aa4\uff09\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u6807\u7b7e\u9636\u6bb5\uff1a\u5f15\u5165\u53cd\u601d\u611f\u77e5\u6807\u7b7e\u4fee\u6b63\u673a\u5236\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u88c1\u5224\u68c0\u6d4b\u4e0e\u5f53\u524d\u63a8\u7406\u6b65\u9aa4\u76f8\u5173\u7684\u53cd\u601d\u548c\u81ea\u6211\u4fee\u6b63\u884c\u4e3a\uff0c\u6291\u5236\u9ad8\u4f30\u5956\u52b1\uff1b2) \u8bad\u7ec3\u9636\u6bb5\uff1a\u63d0\u51fa\u566a\u58f0\u611f\u77e5\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7fPRM\u80fd\u591f\u57fa\u4e8e\u81ea\u8eab\u7f6e\u4fe1\u5ea6\u9010\u6b65\u7cbe\u70bc\u566a\u58f0\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6b65\u9aa4\u7ea7\u6b63\u786e\u6027\u5224\u522b\u80fd\u529b\uff0c\u76f8\u6bd4\u4f7f\u7528\u566a\u58f0\u76d1\u7763\u8bad\u7ec3\u7684PRM\uff0c\u5e73\u5747F1\u5206\u6570\u7edd\u5bf9\u63d0\u5347\u9ad8\u8fbe27%\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u566a\u58f0\u76d1\u7763\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u7684\u53cd\u601d\u68c0\u6d4b\u548c\u566a\u58f0\u611f\u77e5\u8fed\u4ee3\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b65\u9aa4\u7ea7\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.13518", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.13518", "abs": "https://arxiv.org/abs/2601.13518", "authors": ["Jiayi Yuan", "Jonathan N\u00f6ther", "Natasha Jaques", "Goran Radanovi\u0107"], "title": "AgenticRed: Optimizing Agentic Systems for Automated Red-teaming", "comment": "Website: https://yuanjiayiy.github.io/AgenticRed/", "summary": "While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.", "AI": {"tldr": "AgenticRed\uff1a\u4e00\u79cd\u5229\u7528LLM\u4e0a\u4e0b\u6587\u5b66\u4e60\u81ea\u52a8\u8bbe\u8ba1\u548c\u4f18\u5316\u7ea2\u961f\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u901a\u8fc7\u8fdb\u5316\u9009\u62e9\u6f14\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u7ea2\u961f\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6307\u5b9a\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b58\u5728\u4eba\u4e3a\u504f\u89c1\u4e14\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u8bbe\u8ba1\u548c\u4f18\u5316\u7ea2\u961f\u7cfb\u7edf\u7684\u6846\u67b6\u3002", "method": "\u5c06\u7ea2\u961f\u89c6\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u95ee\u9898\uff0c\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u8fed\u4ee3\u8bbe\u8ba1\u548c\u4f18\u5316\u7ea2\u961f\u7cfb\u7edf\uff0c\u91c7\u7528\u8fdb\u5316\u9009\u62e9\u65b9\u6cd5\u6f14\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u7ed3\u6784\u3002", "result": "\u5728Llama-2-7B\u4e0a\u8fbe\u523096%\u653b\u51fb\u6210\u529f\u7387\uff08\u63d0\u534736%\uff09\uff0c\u5728Llama-3-8B\u4e0a\u8fbe\u523098%\uff0c\u5bf9GPT-3.5-Turbo\u548cGPT-4o-mini\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u5bf9Claude-Sonnet-3.5\u8fbe\u523060%\uff08\u63d0\u534724%\uff09\u3002", "conclusion": "\u81ea\u52a8\u5316\u7cfb\u7edf\u8bbe\u8ba1\u662fAI\u5b89\u5168\u8bc4\u4f30\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u80fd\u591f\u8ddf\u4e0a\u6a21\u578b\u5feb\u901f\u6f14\u8fdb\u7684\u6b65\u4f10\uff0c\u4e3a\u7ea2\u961f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.12771", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12771", "abs": "https://arxiv.org/abs/2601.12771", "authors": ["Keito Inoshita"], "title": "Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory", "comment": null, "summary": "Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.", "AI": {"tldr": "\u63d0\u51faLAMA\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u4e16\u754c\u77e5\u8bc6\u4f5c\u4e3a\u5173\u8054\u8bb0\u5fc6\uff0c\u901a\u8fc7\u56de\u5fc6\u540c\u540d\u540d\u4eba\u5e76\u805a\u5408\u5176\u56fd\u7c4d\u6765\u9884\u6d4b\u56fd\u7c4d\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u63d0\u793a\u65b9\u6cd5", "motivation": "LLMs\u62e5\u6709\u4e30\u5bcc\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u4f46\u6709\u6548\u63d0\u53d6\u8fd9\u4e9b\u77e5\u8bc6\u7684\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u56fd\u7c4d\u548c\u5730\u533a\u9884\u6d4b\u4efb\u52a1\u4e0d\u4ec5\u9700\u8981\u8bed\u8a00\u7279\u5f81\uff0c\u8fd8\u9700\u8981\u6587\u5316\u548c\u5386\u53f2\u80cc\u666f\u77e5\u8bc6\uff0c\u8fd9\u4f7f\u5f97LLM\u7684\u4e16\u754c\u77e5\u8bc6\u7279\u522b\u6709\u4ef7\u503c\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684LLM\u63d0\u793a\u65b9\u6cd5\u4f9d\u8d56\u76f4\u63a5\u63a8\u7406\uff0c\u5728\u5e94\u7528\u62bd\u8c61\u8bed\u8a00\u89c4\u5219\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faLLM\u5173\u8054\u8bb0\u5fc6\u4ee3\u7406(LAMA)\u6846\u67b6\uff0c\u4e0d\u76f4\u63a5\u4ece\u540d\u5b57\u63a8\u65ad\u56fd\u7c4d\uff0c\u800c\u662f\u56de\u5fc6\u540c\u540d\u7684\u8457\u540d\u4eba\u7269\u5e76\u901a\u8fc7\u95f4\u63a5\u63a8\u7406\u805a\u5408\u5176\u56fd\u7c4d\u3002\u91c7\u7528\u53cc\u4ee3\u7406\u67b6\u6784\uff1a\u4eba\u7269\u4ee3\u7406\u548c\u5a92\u4f53\u4ee3\u7406\uff0c\u5206\u522b\u4e13\u6ce8\u4e8e\u4e0d\u540c\u77e5\u8bc6\u9886\u57df\uff0c\u5e76\u884c\u56de\u5fc6\u8457\u540d\u4eba\u7269\uff0c\u901a\u8fc7\u6295\u7968\u751f\u6210Top-1\u9884\u6d4b\uff0c\u901a\u8fc7\u6761\u4ef6\u8865\u5168\u751f\u6210Top-K\u9884\u6d4b\u3002", "result": "\u572899\u4e2a\u56fd\u5bb6\u7684\u56fd\u7c4d\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cLAMA\u8fbe\u52300.817\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfLLM\u63d0\u793a\u65b9\u6cd5\u548c\u795e\u7ecf\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff1aLLMs\u5728\u56de\u5fc6\u5177\u4f53\u793a\u4f8b\u65b9\u9762\u6bd4\u62bd\u8c61\u63a8\u7406\u66f4\u53ef\u9760\uff1b\u57fa\u4e8e\u56de\u5fc6\u7684\u65b9\u6cd5\u5bf9\u4f4e\u9891\u56fd\u7c4d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e0d\u53d7\u6570\u636e\u9891\u7387\u5206\u5e03\u5f71\u54cd\uff1b\u53cc\u4ee3\u7406\u67b6\u6784\u5177\u6709\u4e92\u8865\u6027\uff0c\u4ea7\u751f\u534f\u540c\u6548\u5e94\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u8be5\u7cfb\u7edf\u68c0\u7d22\u548c\u805a\u5408LLM\u77e5\u8bc6\u800c\u975e\u63d0\u793a\u63a8\u7406\u3002LAMA\u6846\u67b6\u901a\u8fc7\u5229\u7528LLM\u4f5c\u4e3a\u5173\u8054\u8bb0\u5fc6\uff0c\u5728\u9700\u8981\u4e16\u754c\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2601.12598", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12598", "abs": "https://arxiv.org/abs/2601.12598", "authors": ["Younes Bouhadjar", "Maxime Fabre", "Felix Schmidt", "Emre Neftci"], "title": "Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization", "comment": "11 pages, 4 figures and 4 tables", "summary": "Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at https://github.com/symseqbench/selectivbench", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SelectivBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u9009\u62e9\u6027\u7684\u8f7b\u91cf\u7ea7\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u5173\u952e\u67b6\u6784\u7279\u5f81\u3002", "motivation": "\u7ebf\u6027\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3aTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u867d\u7136\u8bad\u7ec3\u53ef\u5e76\u884c\u5316\u4e14\u63a8\u7406\u65f6\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u6052\u5b9a\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8981\u4e48\u8fc7\u4e8e\u7b80\u5355\u65e0\u6cd5\u63ed\u793a\u5b9e\u8d28\u6027\u5dee\u5f02\uff0c\u8981\u4e48\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u3002\u7f3a\u4e4f\u5bf9\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u76f4\u63a5\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u4e86\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u7684\u7ec6\u5316\u5206\u7c7b\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86SelectivBench\u2014\u2014\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u5b9a\u5236\u7684\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002\u8be5\u57fa\u51c6\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u8bed\u6cd5\u751f\u6210\u53ef\u8c03\u6574\u590d\u6742\u5ea6\u7684\u5e8f\u5217\uff0c\u5305\u542b\u6545\u610f\u8fdd\u53cd\u8f6c\u6362\u89c4\u5219\u7684\u4e0d\u89c4\u5219\u95f4\u9694\uff0c\u4e13\u95e8\u8bc4\u4f30\u5e8f\u5217\u6a21\u578b\u5728\u5c0f\u5230\u4e2d\u7b49\u89c4\u6a21\u4e0b\u7684\u9009\u62e9\u6027\u80fd\u529b\u3002", "result": "\u5728SelectivBench\u4e0a\u8bc4\u4f30\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u663e\u793a\u51fa\u4e0e\u5927\u89c4\u6a21\u8bed\u8a00\u4efb\u52a1\u4e00\u81f4\u7684\u8868\u73b0\u6a21\u5f0f\u3002\u5206\u6790\u63ed\u793a\u4e86\u5173\u952e\u67b6\u6784\u7279\u5f81\u7684\u4f5c\u7528\uff1a\u95e8\u63a7\u548c\u5feb\u901f\u9057\u5fd8\u673a\u5236\u4fc3\u8fdb\u53ec\u56de\uff0c\u72b6\u6001\u5185\u901a\u9053\u6df7\u5408\u5bf9\u9009\u62e9\u6027\u4e0d\u5fc5\u8981\u4f46\u5bf9\u6cdb\u5316\u5173\u952e\uff0csoftmax\u6ce8\u610f\u529b\u56e0\u5185\u5b58\u5bb9\u91cf\u968f\u5e8f\u5217\u957f\u5ea6\u6269\u5c55\u800c\u4fdd\u6301\u4f18\u52bf\u3002", "conclusion": "SelectivBench\u5b9e\u73b0\u4e86\u5bf9\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u7684\u9488\u5bf9\u6027\u9ad8\u6548\u63a2\u7d22\uff0c\u4e3a\u7814\u7a76\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u53d7\u63a7\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5e8f\u5217\u6a21\u578b\u7684\u9009\u62e9\u6027\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2601.13559", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13559", "abs": "https://arxiv.org/abs/2601.13559", "authors": ["Sun Hui", "Ding Yanfeng", "Huidong Ma", "Chang Xu", "Keyan Jin", "Lizheng Zu", "Cheng Zhong", "xiaoguang Liu", "Gang Wang", "Wentong Cai"], "title": "AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent", "comment": null, "summary": "Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.", "AI": {"tldr": "AgentGC\uff1a\u9996\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8fdb\u5316\u5f0f\u57fa\u56e0\u7ec4\u6570\u636e\u538b\u7f29\u5668\uff0c\u901a\u8fc7\u4e09\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08Leader\u548cWorker\uff09\u5b9e\u73b0\u7528\u6237\u53cb\u597d\u754c\u9762\u3001\u8ba4\u77e5\u4f18\u5316\u548c\u81ea\u52a8\u5316\u538b\u7f29\uff0c\u5728\u538b\u7f29\u6bd4\u548c\u541e\u5410\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u56e0\u7ec4\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u975e\u8fdb\u5316\u6027\u3001\u4f4e\u7ea7\u538b\u7f29\u5efa\u6a21\u3001\u9002\u5e94\u6027\u6709\u9650\u548c\u7528\u6237\u754c\u9762\u4e0d\u53cb\u597d\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u3001\u53ef\u8fdb\u5316\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faAgentGC\u4e09\u5c42\u67b6\u6784\uff1a1\uff09\u7528\u6237\u5c42\u901a\u8fc7Leader\u7ed3\u5408LLM\u63d0\u4f9b\u53cb\u597d\u754c\u9762\uff1b2\uff09\u8ba4\u77e5\u5c42\u7531Leader\u9a71\u52a8\uff0c\u96c6\u6210LLM\u8fdb\u884c\u7b97\u6cd5-\u6570\u636e\u96c6-\u7cfb\u7edf\u8054\u5408\u4f18\u5316\uff1b3\uff09\u538b\u7f29\u5c42\u7531Worker\u8d1f\u8d23\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u591a\u77e5\u8bc6\u5b66\u4e60\u6846\u67b6\u6267\u884c\u538b\u7f29\u89e3\u538b\u3002\u652f\u6301\u4e09\u79cd\u6a21\u5f0f\uff1aCP\uff08\u538b\u7f29\u6bd4\u4f18\u5148\uff09\u3001TP\uff08\u541e\u5410\u91cf\u4f18\u5148\uff09\u3001BM\uff08\u5e73\u8861\u6a21\u5f0f\uff09\u3002", "result": "\u57289\u4e2a\u6570\u636e\u96c6\u4e0a\u4e0e14\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0c\u5e73\u5747\u538b\u7f29\u6bd4\u63d0\u534716.66%\u300116.11%\u300116.33%\uff0c\u541e\u5410\u91cf\u63d0\u53474.73\u500d\u30019.23\u500d\u30019.15\u500d\u3002", "conclusion": "AgentGC\u901a\u8fc7\u667a\u80fd\u4f53\u67b6\u6784\u548cLLM\u96c6\u6210\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u56e0\u7ec4\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u538b\u7f29\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "2601.13589", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13589", "abs": "https://arxiv.org/abs/2601.13589", "authors": ["HyeYoung Lee"], "title": "Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification", "comment": null, "summary": "This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u97f3\u9891\u60c5\u611f\u4fe1\u53f7\u5b9e\u65f6\u751f\u6210\u54cd\u5e94\u5f0f\u5a92\u4f53\u5185\u5bb9\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u56db\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\u5b9e\u73b0\u60c5\u611f\u8bc6\u522b\u5230\u5b89\u5168\u53ef\u63a7\u5185\u5bb9\u7684\u8f6c\u6362\uff0c\u5305\u542b\u5b89\u5168\u9a8c\u8bc1\u5faa\u73af\u786e\u4fdd\u5e74\u9f84\u9002\u5b9c\u6027\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4f46\u7f3a\u4e4f\u5c06\u8bc6\u522b\u51fa\u7684\u60c5\u611f\u72b6\u6001\u8f6c\u5316\u4e3a\u5b89\u5168\u3001\u5e74\u9f84\u9002\u5b9c\u3001\u53ef\u63a7\u7684\u54cd\u5e94\u5185\u5bb9\u7684\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u54cd\u5e94\u5f0f\u5a92\u4f53\u5185\u5bb9\u5e76\u786e\u4fdd\u5b89\u5168\u6027\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u56db\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\uff1a1)\u57fa\u4e8eCNN\u7684\u60c5\u611f\u8bc6\u522b\u667a\u80fd\u4f53\u63d0\u53d6\u58f0\u5b66\u7279\u5f81\uff1b2)\u54cd\u5e94\u7b56\u7565\u51b3\u7b56\u667a\u80fd\u4f53\u5c06\u60c5\u611f\u6620\u5c04\u5230\u54cd\u5e94\u6a21\u5f0f\uff1b3)\u5185\u5bb9\u53c2\u6570\u751f\u6210\u667a\u80fd\u4f53\u4ea7\u751f\u5a92\u4f53\u63a7\u5236\u53c2\u6570\uff1b4)\u5b89\u5168\u9a8c\u8bc1\u667a\u80fd\u4f53\u5f3a\u5236\u6267\u884c\u5e74\u9f84\u9002\u5b9c\u6027\u548c\u523a\u6fc0\u7ea6\u675f\u3002\u7cfb\u7edf\u5305\u542b\u663e\u5f0f\u7684\u5b89\u5168\u9a8c\u8bc1\u5faa\u73af\uff0c\u5728\u8f93\u51fa\u524d\u8fc7\u6ee4\u751f\u6210\u5185\u5bb9\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u8fbe\u523073.2%\u7684\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387\u300189.4%\u7684\u54cd\u5e94\u6a21\u5f0f\u4e00\u81f4\u6027\u3001100%\u7684\u5b89\u5168\u5408\u89c4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u4e8e100ms\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u9002\u5408\u8bbe\u5907\u7aef\u90e8\u7f72\u3002", "conclusion": "\u8be5\u6a21\u5757\u5316\u67b6\u6784\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u513f\u7ae5\u76f8\u5173\u5a92\u4f53\u3001\u6cbb\u7597\u5e94\u7528\u548c\u60c5\u611f\u54cd\u5e94\u667a\u80fd\u8bbe\u5907\uff0c\u5b9e\u73b0\u4e86\u4ece\u60c5\u611f\u8bc6\u522b\u5230\u5b89\u5168\u53ef\u63a7\u5185\u5bb9\u751f\u6210\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2601.13591", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13591", "abs": "https://arxiv.org/abs/2601.13591", "authors": ["Maojun Sun", "Yifei Xie", "Yue Wu", "Ruijian Han", "Binyan Jiang", "Defeng Sun", "Yancheng Yuan", "Jian Huang"], "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems", "comment": null, "summary": "Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.", "AI": {"tldr": "DSAEval\u662f\u4e00\u4e2a\u5305\u542b641\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u79d1\u5b66\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e285\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5177\u6709\u591a\u6a21\u6001\u73af\u5883\u611f\u77e5\u3001\u591a\u67e5\u8be2\u4ea4\u4e92\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4e09\u5927\u7279\u70b9\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u6570\u636e\u4ee3\u7406\u65e8\u5728\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u4efb\u52a1\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u6570\u636e\u79d1\u5b66\u95ee\u9898\u7684\u5f00\u653e\u6027\u3001\u8de8\u591a\u4e2a\u5206\u7c7b\u4e14\u7f3a\u4e4f\u6807\u51c6\u7b54\u6848\u7684\u7279\u6027\u7ed9\u8bc4\u4f30\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faDSAEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b641\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u79d1\u5b66\u95ee\u9898\uff0c\u57fa\u4e8e285\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff08\u89c6\u89c9\u548c\u6587\u672c\uff09\u3002\u8be5\u57fa\u51c6\u5177\u6709\u4e09\u5927\u7279\u70b9\uff1a\u591a\u6a21\u6001\u73af\u5883\u611f\u77e5\u3001\u591a\u67e5\u8be2\u4ea4\u4e92\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e8611\u4e2a\u5148\u8fdb\u7684\u4ee3\u7406LLM\uff0c\u7ed3\u679c\u663e\u793aClaude-Sonnet-4.5\u6574\u4f53\u6027\u80fd\u6700\u5f3a\uff0cGPT-5.2\u6700\u6709\u6548\u7387\uff0cMiMo-V2-Flash\u6700\u5177\u6210\u672c\u6548\u76ca\u3002\u591a\u6a21\u6001\u611f\u77e5\u5728\u89c6\u89c9\u76f8\u5173\u4efb\u52a1\u4e0a\u80fd\u63d0\u53472.04%\u523011.30%\u7684\u6027\u80fd\u3002", "conclusion": "\u5f53\u524d\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u5728\u7ed3\u6784\u5316\u6570\u636e\u548c\u5e38\u89c4\u6570\u636e\u5206\u6790\u5de5\u4f5c\u6d41\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u975e\u7ed3\u6784\u5316\u9886\u57df\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u7814\u7a76\u4e3a\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "topic": "swe benchmark"}}
{"id": "2601.12703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12703", "abs": "https://arxiv.org/abs/2601.12703", "authors": ["Andrew Gordon", "Garrett Baker", "George Wang", "William Snell", "Stan van Wingerden", "Daniel Murfet"], "title": "Towards Spectroscopy: Susceptibility Clusters in Language Models", "comment": null, "summary": "Spectroscopy infers the internal structure of physical systems by measuring their response to perturbations. We apply this principle to neural networks: perturbing the data distribution by upweighting a token $y$ in context $x$, we measure the model's response via susceptibilities $\u03c7_{xy}$, which are covariances between component-level observables and the perturbation computed over a localized Gibbs posterior via stochastic gradient Langevin dynamics (SGLD). Theoretically, we show that susceptibilities decompose as a sum over modes of the data distribution, explaining why tokens that follow their contexts \"for similar reasons\" cluster together in susceptibility space. Empirically, we apply this methodology to Pythia-14M, developing a conductance-based clustering algorithm that identifies 510 interpretable clusters ranging from grammatical patterns to code structure to mathematical notation. Comparing to sparse autoencoders, 50% of our clusters match SAE features, validating that both methods recover similar structure.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8c31\u5206\u6790\u539f\u7406\u7684\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u7ed3\u6784\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u6570\u636e\u5206\u5e03\u6d4b\u91cf\u6a21\u578b\u54cd\u5e94\uff0c\u8bc6\u522b\u51fa510\u4e2a\u53ef\u89e3\u91ca\u7684\u805a\u7c7b\uff0c\u6db5\u76d6\u8bed\u6cd5\u6a21\u5f0f\u3001\u4ee3\u7801\u7ed3\u6784\u548c\u6570\u5b66\u7b26\u53f7\u7b49\u3002", "motivation": "\u53d7\u5149\u8c31\u5b66\u901a\u8fc7\u6270\u52a8\u7269\u7406\u7cfb\u7edf\u6d4b\u91cf\u54cd\u5e94\u6765\u63a8\u65ad\u5185\u90e8\u7ed3\u6784\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u5c06\u8fd9\u4e00\u539f\u7406\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6270\u52a8\u6570\u636e\u5206\u5e03\u6765\u7406\u89e3\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u548c\u8868\u793a\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\uff08SGLD\uff09\u5728\u5c40\u90e8\u5409\u5e03\u65af\u540e\u9a8c\u4e0a\u8ba1\u7b97\u654f\u611f\u5ea6\u03c7_xy\uff0c\u5f00\u53d1\u57fa\u4e8e\u7535\u5bfc\u7684\u805a\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u6570\u636e\u5206\u5e03\u4e2d\u7279\u5b9a\u6807\u8bb0\u7684\u6743\u91cd\u6765\u6d4b\u91cf\u6a21\u578b\u7ec4\u4ef6\u7684\u54cd\u5e94\u3002", "result": "\u5728Pythia-14M\u6a21\u578b\u4e0a\u8bc6\u522b\u51fa510\u4e2a\u53ef\u89e3\u91ca\u7684\u805a\u7c7b\uff0c\u6db5\u76d6\u8bed\u6cd5\u6a21\u5f0f\u3001\u4ee3\u7801\u7ed3\u6784\u548c\u6570\u5b66\u7b26\u53f7\u7b49\u3002\u4e0e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u76f8\u6bd4\uff0c50%\u7684\u805a\u7c7b\u4e0eSAE\u7279\u5f81\u5339\u914d\uff0c\u9a8c\u8bc1\u4e86\u4e24\u79cd\u65b9\u6cd5\u6062\u590d\u76f8\u4f3c\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u5149\u8c31\u5b66\u539f\u7406\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u89e3\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u8bc6\u522b\u51fa\u53ef\u89e3\u91ca\u7684\u8868\u793a\u6a21\u5f0f\uff0c\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u679c\u4e00\u81f4\u3002", "topic": "agent analysis"}}
{"id": "2601.13709", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.13709", "abs": "https://arxiv.org/abs/2601.13709", "authors": ["Christopher Kao", "Vanshika Vats", "James Davis"], "title": "Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games", "comment": "For associated dataset, see https://github.com/cocochief4/llm-mafia. Published in IEEE ICA 2025, waiting for IEEEXplore proceedings", "summary": "Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.", "AI": {"tldr": "LLM\u4ee3\u7406\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u300a\u9ed1\u624b\u515a\u300b\u4e2d\u8868\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u5f3a\u7684\u6b3a\u9a97\u80fd\u529b\uff0c\u901a\u8fc7\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6a21\u62df\u771f\u5b9e\u793e\u4ea4\u73af\u5883\uff0cGPT-4o\u667a\u80fd\u4f53\u7684\u6b3a\u9a97\u8d28\u91cf\u66f4\u9ad8\uff0c\u66f4\u96be\u88ab\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u66f4\u591a\u5e94\u7528\u4e2d\u4f7f\u7528\uff0c\u5bf9\u5176\u5b89\u5168\u6027\u7684\u62c5\u5fe7\u589e\u52a0\u3002\u867d\u7136\u4e4b\u524d\u7814\u7a76\u8868\u660eLLM\u5728\u53d7\u63a7\u4efb\u52a1\u4e2d\u4f1a\u6b3a\u9a97\uff0c\u4f46\u5bf9\u5176\u5728\u81ea\u7136\u8bed\u8a00\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u6b3a\u9a97\u80fd\u529b\u4e86\u89e3\u8f83\u5c11\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLM\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u7684\u6b3a\u9a97\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6a21\u62df35\u573a\u300a\u9ed1\u624b\u515a\u300b\u6e38\u620f\uff0cGPT-4o\u4f5c\u4e3a\u73a9\u5bb6\u3002\u521b\u5efa\u57fa\u4e8eGPT-4-Turbo\u7684\"\u9ed1\u624b\u515a\u68c0\u6d4b\u5668\"\uff0c\u5728\u4e0d\u63d0\u4f9b\u73a9\u5bb6\u89d2\u8272\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5206\u6790\u6e38\u620f\u5bf9\u8bdd\u8bb0\u5f55\u6765\u9884\u6d4b\u9ed1\u624b\u515a\u73a9\u5bb6\u3002\u5c06\u9884\u6d4b\u51c6\u786e\u7387\u4f5c\u4e3a\u6b3a\u9a97\u8d28\u91cf\u7684\u66ff\u4ee3\u6307\u6807\uff0c\u5e76\u4e0e28\u573a\u4eba\u7c7b\u6e38\u620f\u548c\u968f\u673a\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u9ed1\u624b\u515a\u68c0\u6d4b\u5668\u5728LLM\u6e38\u620f\u4e2d\u7684\u9884\u6d4b\u51c6\u786e\u7387\u4f4e\u4e8e\u4eba\u7c7b\u6e38\u620f\uff0c\u8868\u660eLLM\u80fd\u66f4\u597d\u5730\u878d\u5165\u7fa4\u4f53\uff0c\u6b3a\u9a97\u6548\u679c\u66f4\u5f3a\u3002\u8fd9\u4e00\u7ed3\u679c\u5728\u4e0d\u540c\u6e38\u620f\u5929\u6570\u548c\u68c0\u6d4b\u5230\u7684\u9ed1\u624b\u515a\u6570\u91cf\u4e0a\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "LLM\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u6b3a\u9a97\u80fd\u529b\u6bd4\u4eba\u7c7b\u66f4\u590d\u6742\u548c\u6709\u6548\uff0c\u8fd9\u7a81\u663e\u4e86LLM\u6b3a\u9a97\u7684\u98ce\u9669\u548c\u590d\u6742\u6027\u3002\u7814\u7a76\u53d1\u5e03\u4e86LLM\u9ed1\u624b\u515a\u5bf9\u8bdd\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2601.12979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12979", "abs": "https://arxiv.org/abs/2601.12979", "authors": ["Qingyu Lu", "Liang Ding", "Kanjian Zhang", "Jinxia Zhang", "Dacheng Tao"], "title": "The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check", "comment": "Under Review", "summary": "The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a \"bitter lesson\": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.", "AI": {"tldr": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u5728\u5b9e\u65f6\u4ee3\u7406\u4ea4\u4e92\u4e2d\u6548\u7387\u4f18\u52bf\u660e\u663e\uff0c\u4f46\u5728\u5b9e\u9645\u4ee3\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u7cbe\u786e\u683c\u5f0f\u8981\u6c42\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "motivation": "\u7814\u7a76dLLMs\u4f5c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u66ff\u4ee3\u65b9\u6848\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u9a8c\u8bc1\u5176\u6548\u7387\u4f18\u52bf\u662f\u5426\u80fd\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u4ee3\u7406\u884c\u4e3a\u3002", "method": "\u5728Agentboard\u548cBFCL\u57fa\u51c6\u4e0a\u5168\u9762\u8bc4\u4f30dLLMs\u5728\u4e24\u79cd\u4ee3\u7406\u8303\u5f0f\u4e2d\u7684\u8868\u73b0\uff1a\u5177\u8eab\u4ee3\u7406\uff08\u9700\u8981\u957f\u65f6\u7a0b\u89c4\u5212\uff09\u548c\u5de5\u5177\u8c03\u7528\u4ee3\u7406\uff08\u9700\u8981\u7cbe\u786e\u683c\u5f0f\uff09\u3002\u5f15\u5165DiffuAgent\u591a\u4ee3\u7406\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06dLLMs\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u8ba4\u77e5\u6838\u5fc3\u96c6\u6210\u5230\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u3002", "result": "dLLMs\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1a1\uff09\u5728\u5177\u8eab\u8bbe\u7f6e\u4e2d\uff0cdLLMs\u53cd\u590d\u5c1d\u8bd5\u5931\u8d25\uff0c\u65e0\u6cd5\u5728\u65f6\u95f4\u53cd\u9988\u4e0b\u8fdb\u884c\u5206\u652f\uff1b2\uff09\u5728\u5de5\u5177\u8c03\u7528\u8bbe\u7f6e\u4e2d\uff0cdLLMs\u5728\u6269\u6563\u566a\u58f0\u4e0b\u65e0\u6cd5\u4fdd\u6301\u7b26\u53f7\u7cbe\u5ea6\uff08\u5982\u4e25\u683c\u7684JSON\u6a21\u5f0f\uff09\u3002dLLMs\u5728\u975e\u56e0\u679c\u89d2\u8272\uff08\u5982\u8bb0\u5fc6\u603b\u7ed3\u548c\u5de5\u5177\u9009\u62e9\uff09\u4e2d\u6709\u6548\uff0c\u4f46\u9700\u8981\u5c06\u56e0\u679c\u3001\u7cbe\u786e\u548c\u903b\u8f91\u57fa\u7840\u63a8\u7406\u673a\u5236\u6574\u5408\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\u624d\u80fd\u9002\u7528\u4e8e\u4ee3\u7406\u4efb\u52a1\u3002", "conclusion": "\u5f53\u524ddLLMs\u4e0d\u80fd\u4f5c\u4e3a\u53ef\u9760\u7684\u4ee3\u7406\u4e3b\u5e72\uff0c\u5176\u6548\u7387\u4f18\u52bf\u5e76\u672a\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u4ee3\u7406\u884c\u4e3a\u3002dLLMs\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u66f4\u9002\u5408\u975e\u56e0\u679c\u89d2\u8272\uff0c\u9700\u8981\u6539\u8fdb\u63a8\u7406\u673a\u5236\u624d\u80fd\u7528\u4e8e\u6838\u5fc3\u4ee3\u7406\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2601.12730", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12730", "abs": "https://arxiv.org/abs/2601.12730", "authors": ["Zhaochun Li", "Chen Wang", "Jionghao Bai", "Shisheng Cui", "Ge Lan", "Zhou Zhao", "Yue Wang"], "title": "Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off", "comment": null, "summary": "The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \\textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the \"luck\" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \\textbf{distribution-centric} perspective for RL, in which exploration is always guided by a \"better\" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.", "AI": {"tldr": "DCPO\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u4e2d\u5fc3\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u7ea7\u6b63\u5219\u5316\u63a7\u5236\u7b56\u7565\u71b5\uff0c\u89e3\u51b3\u4f20\u7edf\u6837\u672c\u4e2d\u5fc3\u65b9\u6cd5\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u534720%\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\uff08\u5982GRPO\uff09\u503e\u5411\u4e8e\u8fc7\u5ea6\u5229\u7528\uff0c\u5bfc\u81f4\u71b5\u5355\u8c03\u4e0b\u964d\u3001\u6837\u672c\u6536\u655b\u3001\u63a2\u7d22\u6d88\u5931\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u591a\u4e3a\u6837\u672c\u4e2d\u5fc3\u65b9\u6cd5\uff0c\u4f9d\u8d56\u7a00\u6709\u6837\u672c\u7684\"\u8fd0\u6c14\"\uff0c\u7f3a\u4e4f\u5bf9\u7b56\u7565\u7684\u539f\u5219\u6027\u63a7\u5236\uff0c\u6548\u679c\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faDistribution-Centric Policy Optimization (DCPO)\uff0c\u4ece\u5206\u5e03\u4e2d\u5fc3\u89c6\u89d2\u91cd\u65b0\u5b9a\u4e49\u63a2\u7d22\u95ee\u9898\uff0c\u5c06\u71b5\u8c03\u8282\u91cd\u65b0\u8868\u8ff0\u4e3a\u5206\u5e03\u7ea7\u6b63\u5219\u5316\u3002DCPO\u5b8c\u5168\u5728\u7b56\u7565\u5185\u5b9e\u73b0\u53ef\u63a7\u71b5\uff0c\u65e0\u9700\u4ece\u5916\u90e8\u5206\u5e03\u91c7\u6837\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDCPO\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u5347\u7ea620%\u3002DCPO\u7528\u5206\u5e03\u7ea7\u539f\u5219\u66ff\u4ee3\u6837\u672c\u7ea7\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u7075\u6d3b\u6846\u67b6\u3002", "conclusion": "DCPO\u901a\u8fc7\u5206\u5e03\u4e2d\u5fc3\u89c6\u89d2\u4e3aRL\u63a2\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u63a2\u7d22\u548c\u66f4\u5f3a\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.13761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13761", "abs": "https://arxiv.org/abs/2601.13761", "authors": ["Shengda Fan", "Xuyan Ye", "Yankai Lin"], "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution", "comment": null, "summary": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.", "AI": {"tldr": "DARC\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u95ee\u9898\u751f\u6210\u4e0e\u89e3\u7b54\u8bad\u7ec3\uff0c\u89e3\u51b3\u81ea\u5bf9\u5f08\u4e2d\u7684\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u81ea\u5bf9\u5f08\u6846\u67b6\u5b58\u5728\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\uff1a(1) \u95ee\u9898\u751f\u6210\u5668\u7684\u76ee\u6807\u975e\u5e73\u7a33\u6027\uff08\u4f9d\u8d56\u4e8e\u89e3\u7b54\u5668\u7684\u5956\u52b1\u53cd\u9988\uff09\uff0c(2) \u89e3\u7b54\u5668\u8bad\u7ec3\u4e2d\u7684\u81ea\u751f\u6210\u4f2a\u6807\u7b7e\u5e26\u6765\u7684\u5f15\u5bfc\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u95ee\u9898\u751f\u6210\u5668\u5408\u6210\u96be\u5ea6\u6821\u51c6\u7684\u95ee\u9898\uff08\u57fa\u4e8e\u660e\u786e\u96be\u5ea6\u7ea7\u522b\u548c\u5916\u90e8\u8bed\u6599\uff09\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u975e\u5bf9\u79f0\u81ea\u84b8\u998f\u673a\u5236\u8bad\u7ec3\u89e3\u7b54\u5668\uff0c\u5176\u4e2d\u6587\u6863\u589e\u5f3a\u7684\u6559\u5e08\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u6765\u76d1\u7763\u65e0\u6587\u6863\u8bbf\u95ee\u7684\u5b66\u751f\u89e3\u7b54\u5668\u3002", "result": "DARC\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u57289\u4e2a\u63a8\u7406\u57fa\u51c6\u548c3\u4e2a\u9aa8\u5e72\u6a21\u578b\u4e0a\u5e73\u5747\u63d0\u534710.9\u4e2a\u70b9\uff0c\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a5\u8fd1\u5b8c\u5168\u76d1\u7763\u6a21\u578b\u7684\u6027\u80fd\u4e14\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "DARC\u901a\u8fc7\u89e3\u8026\u95ee\u9898\u751f\u6210\u4e0e\u89e3\u7b54\u8bad\u7ec3\uff0c\u7a33\u5b9a\u4e86\u81ea\u8fdb\u5316\u8fc7\u7a0b\uff0c\u4e3a\u81ea\u6539\u8fdbAI\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.12995", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12995", "abs": "https://arxiv.org/abs/2601.12995", "authors": ["Runxuan Liu", "Xianhao Ou", "Xinyan Ma", "Jiyuan Wang", "Jiafeng Liang", "Jiaqi Li", "Tao He", "Zheng Chu", "Rongchuan Mu", "Zekun Wang", "Baoxin Wang", "Dayong Wu", "Ming Liu", "Shijin Wang", "Guoping Hu", "Bing Qin"], "title": "Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models", "comment": null, "summary": "Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u56fe\u63a8\u7406\u8303\u5f0f(GRP)\u548cPASC-GRPO\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u56fe\u8868\u793a\u548c\u8fc7\u7a0b\u611f\u77e5\u9a8c\u8bc1\u89e3\u51b3\u4f20\u7edfCoT\u63a8\u7406\u4e2d\u7684\u8bed\u4e49\u8bc4\u4f30\u74f6\u9888\u3001\u76d1\u7763\u7c97\u7cd9\u3001\u5956\u52b1\u9ed1\u5ba2\u7b49\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLMs\u7684\u63a8\u7406\u4e3b\u8981\u751f\u6210\u7eaf\u6587\u672c\uff0c\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\u8fdb\u884c\u8bed\u4e49\u8bc4\u4f30\u4f1a\u9020\u6210\u8bad\u7ec3\u8ba1\u7b97\u74f6\u9888\u3002\u5c3d\u7ba1\u6709RLVR\u4f18\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u5b58\u5728\u76d1\u7763\u7c97\u7cd9\u3001\u5956\u52b1\u9ed1\u5ba2\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u548c\u6cdb\u5316\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u56fe\u63a8\u7406\u8303\u5f0f(GRP)\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u548c\u7b26\u53f7\u5316\u63a8\u7406\uff0c\u901a\u8fc7\u5e26\u6b65\u9aa4\u7ea7\u8ba4\u77e5\u6807\u7b7e\u7684\u56fe\u7ed3\u6784\u8868\u793a\u3002\u57fa\u4e8eGRP\u8bbe\u8ba1PASC-GRPO\u65b9\u6cd5\uff0c\u7528\u7ed3\u6784\u5316\u8bc4\u4f30\u66ff\u4ee3\u8bed\u4e49\u8bc4\u4f30\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u7ed3\u679c\u5956\u52b1\u5b9e\u73b0\u8fc7\u7a0b\u611f\u77e5\u9a8c\u8bc1\uff0c\u901a\u8fc7\u5206\u5c42\u88c1\u526a\u4f18\u52bf\u4f30\u8ba1\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002\u6570\u636e\u3001\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u540e\u7eed\u53d1\u5e03\u3002", "conclusion": "GRP\u548cPASC-GRPO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfCoT\u63a8\u7406\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u548c\u8fc7\u7a0b\u611f\u77e5\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.13880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13880", "abs": "https://arxiv.org/abs/2601.13880", "authors": ["Ye Tian", "Zihao Wang", "Onat Gungor", "Xiaoran Fan", "Tajana Rosing"], "title": "LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health", "comment": null, "summary": "Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.", "AI": {"tldr": "LifeAgentBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21QA\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u957f\u671f\u3001\u8de8\u7ef4\u5ea6\u3001\u591a\u7528\u6237\u751f\u6d3b\u65b9\u5f0f\u5065\u5eb7\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u542b22,573\u4e2a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86LifeAgent\u4f5c\u4e3a\u5f3a\u57fa\u7ebf\u4ee3\u7406\u3002", "motivation": "\u4e2a\u6027\u5316\u6570\u5b57\u5065\u5eb7\u652f\u6301\u9700\u8981\u5bf9\u5f02\u6784\u751f\u6d3b\u65b9\u5f0f\u4fe1\u53f7\u8fdb\u884c\u957f\u671f\u8de8\u7ef4\u5ea6\u63a8\u7406\uff0c\u4f46\u5f53\u524dLLM\u5728\u6b64\u573a\u666f\u4e0b\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u57fa\u51c6\u8bc4\u4f30\u3002", "method": "1) \u6784\u5efaLifeAgentBench\u57fa\u51c6\uff0c\u5305\u542b22,573\u4e2a\u4ece\u57fa\u7840\u68c0\u7d22\u5230\u590d\u6742\u63a8\u7406\u7684\u95ee\u9898\uff1b2) \u53d1\u5e03\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6784\u5efa\u6d41\u7a0b\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff1b3) \u7cfb\u7edf\u8bc4\u4f3011\u4e2a\u9886\u5148LLM\uff1b4) \u63d0\u51faLifeAgent\u4ee3\u7406\uff0c\u96c6\u6210\u591a\u6b65\u8bc1\u636e\u68c0\u7d22\u4e0e\u786e\u5b9a\u6027\u805a\u5408\u3002", "result": "1) \u8bc6\u522b\u51faLLM\u5728\u957f\u671f\u805a\u5408\u548c\u8de8\u7ef4\u5ea6\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u74f6\u9888\uff1b2) LifeAgent\u76f8\u6bd4\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\uff1b3) \u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "LifeAgentBench\u4e3a\u8bc4\u4f30LLM\u5065\u5eb7\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u51c6\uff0cLifeAgent\u4ee3\u7406\u5728\u89e3\u51b3\u957f\u671f\u8de8\u7ef4\u5ea6\u63a8\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u5065\u5eb7\u52a9\u624b\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\u3002", "topic": "agent analysis"}}
{"id": "2601.13969", "categories": ["cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13969", "abs": "https://arxiv.org/abs/2601.13969", "authors": ["Joaqu\u00edn Polonuer", "Lucas Vittor", "I\u00f1aki Arango", "Ayush Noori", "David A. Clifton", "Luciano Del Corro", "Marinka Zitnik"], "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval", "comment": null, "summary": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.", "AI": {"tldr": "ARK\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u5668\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u5e7f\u5ea6-\u6df1\u5ea6\u6743\u8861\uff0c\u4f7f\u7528\u5168\u5c40\u641c\u7d22\u548c\u90bb\u57df\u63a2\u7d22\u4e24\u79cd\u64cd\u4f5c\uff0c\u5728STaRK\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u901a\u8fc7\u65e0\u6807\u7b7e\u6a21\u4eff\u84b8\u998f\u5230\u5c0f\u6a21\u578b\u4e2d\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u5668\u8986\u76d6\u5e7f\u4f46\u6df1\u5ea6\u6d45\uff0c\u57fa\u4e8e\u904d\u5386\u7684\u65b9\u6cd5\u4f9d\u8d56\u79cd\u5b50\u8282\u70b9\u9009\u62e9\u4e14\u5728\u591a\u5b9e\u4f53\u591a\u5173\u7cfb\u67e5\u8be2\u4e2d\u5bb9\u6613\u5931\u8d25\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u5e73\u8861\u5e7f\u5ea6\u641c\u7d22\u548c\u6df1\u5ea6\u904d\u5386\u7684\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "ARK\u91c7\u7528\u4ee3\u7406\u5f0f\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u5668\uff0c\u8ba9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e24\u79cd\u64cd\u4f5c\u5de5\u5177\u63a7\u5236\u68c0\u7d22\uff1a\u5168\u5c40\u8bcd\u6c47\u641c\u7d22\uff08\u7528\u4e8e\u8bed\u8a00\u5bc6\u96c6\u578b\u67e5\u8be2\uff09\u548c\u4e00\u8df3\u90bb\u57df\u63a2\u7d22\uff08\u7528\u4e8e\u5173\u7cfb\u5bc6\u96c6\u578b\u67e5\u8be2\uff09\u3002\u8fd9\u4e24\u79cd\u64cd\u4f5c\u53ef\u4ee5\u7ec4\u5408\u5b9e\u73b0\u591a\u8df3\u904d\u5386\uff0c\u65e0\u9700\u9884\u8bbe\u8df3\u6570\u6216\u68c0\u7d22\u8bad\u7ec3\u3002", "result": "\u5728STaRK\u57fa\u51c6\u4e0a\uff0cARK\u8fbe\u523059.1%\u7684\u5e73\u5747Hit@1\u548c67.4\u7684\u5e73\u5747MRR\uff0c\u6bd4\u68c0\u7d22\u57fa\u7ebf\u548c\u65e0\u8bad\u7ec3\u4ee3\u7406\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u6700\u9ad831.4%\u7684Hit@1\u548c28.0%\u7684MRR\u3002\u901a\u8fc7\u65e0\u6807\u7b7e\u6a21\u4eff\u84b8\u998f\u52308B\u6a21\u578b\uff0c\u5728AMAZON\u3001MAG\u548cPRIME\u6570\u636e\u96c6\u4e0a\u5206\u522b\u6bd4\u57fa\u78408B\u6a21\u578b\u63d0\u5347+7.0\u3001+26.6\u548c+13.5\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\u7684Hit@1\uff0c\u540c\u65f6\u4fdd\u7559\u9ad8\u8fbe98.5%\u7684\u6559\u5e08\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "ARK\u901a\u8fc7\u81ea\u9002\u5e94\u5e73\u8861\u5e7f\u5ea6\u641c\u7d22\u548c\u6df1\u5ea6\u904d\u5386\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u6027\u80fd\u3002\u5176\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u53ef\u4ee5\u901a\u8fc7\u65e0\u6807\u7b7e\u6a21\u4eff\u6709\u6548\u84b8\u998f\u5230\u66f4\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002", "topic": "agent analysis"}}
{"id": "2601.14027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14027", "abs": "https://arxiv.org/abs/2601.14027", "authors": ["Junqi Liu", "Zihao Zhou", "Zekai Zhu", "Marco Dos Santos", "Weikun He", "Jiawei Liu", "Ran Wang", "Yunzhou Xie", "Junqiao Zhao", "Qiufeng Wang", "Lihong Zhi", "Jia Li", "Wenda Li"], "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics", "comment": null, "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u901a\u7528\u4ee3\u7801\u4ee3\u7406\u4f5c\u4e3a\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u5668\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7Numina-Lean-Agent\u5b9e\u73b0\uff0c\u5728Putnam 2025\u4e2d\u53d6\u5f9712/12\u7684\u6ee1\u5206\u6210\u7ee9\uff0c\u5e76\u6210\u529f\u5f62\u5f0f\u5316Brascamp-Lieb\u5b9a\u7406\u3002", "motivation": "\u73b0\u6709\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u6d41\u6c34\u7ebf\u548c\u8bad\u7ec3\u8fc7\u7684\u5f62\u5f0f\u8bc1\u660e\u5668\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002\u901a\u7528\u4ee3\u7801\u4ee3\u7406\u80fd\u63d0\u4f9b\u8d85\u8d8a\u8bc1\u660e\u7684\u591a\u6837\u5316\u63a8\u7406\u4efb\u52a1\u63a5\u53e3\uff0c\u4ec5\u901a\u8fc7\u66ff\u6362\u57fa\u7840\u6a21\u578b\u5373\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4e14MCP\u652f\u6301\u7075\u6d3b\u6269\u5c55\u548c\u81ea\u4e3b\u8c03\u7528\u4e13\u4e1a\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u76f4\u63a5\u4f7f\u7528\u901a\u7528\u4ee3\u7801\u4ee3\u7406\u4f5c\u4e3a\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u5668\u7684\u65b0\u8303\u5f0f\u3002\u5177\u4f53\u5b9e\u73b0Numina-Lean-Agent\uff0c\u7ed3\u5408Claude Code\u4e0eNumina-Lean-MCP\uff0c\u5b9e\u73b0\u4e0eLean\u7684\u81ea\u4e3b\u4ea4\u4e92\u3001\u76f8\u5173\u5b9a\u7406\u68c0\u7d22\u3001\u975e\u5f62\u5f0f\u5316\u8bc1\u660e\u548c\u8f85\u52a9\u63a8\u7406\u5de5\u5177\u8c03\u7528\u3002", "result": "\u4f7f\u7528Claude Opus 4.5\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0cNumina-Lean-Agent\u5728Putnam 2025\u4e2d\u89e3\u51b3\u4e86\u6240\u6709\u95ee\u9898\uff0812/12\uff09\uff0c\u4e0e\u6700\u4f73\u95ed\u6e90\u7cfb\u7edf\u6301\u5e73\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e0e\u6570\u5b66\u5bb6\u4ea4\u4e92\u6210\u529f\u5f62\u5f0f\u5316\u4e86Brascamp-Lieb\u5b9a\u7406\uff0c\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u3002", "conclusion": "\u901a\u7528\u4ee3\u7801\u4ee3\u7406\u4f5c\u4e3a\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u5668\u7684\u65b0\u8303\u5f0f\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cNumina-Lean-Agent\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u6570\u5b66\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7075\u6d3b\u3001\u53ef\u590d\u73b0\u7684\u5f62\u5f0f\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2601.14171", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14171", "abs": "https://arxiv.org/abs/2601.14171", "authors": ["Qianli Ma", "Chang Guo", "Zhiheng Tian", "Siyu Wang", "Jipeng Xiao", "Yuanhao Yue", "Zhipeng Zhang"], "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "comment": null, "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "AI": {"tldr": "RebuttalAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u53cd\u9a73\u751f\u6210\u91cd\u6784\u4e3a\u4ee5\u8bc1\u636e\u4e3a\u4e2d\u5fc3\u7684\u89c4\u5212\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u89e3\u8bc4\u5ba1\u610f\u89c1\u3001\u6784\u5efa\u6df7\u5408\u4e0a\u4e0b\u6587\u548c\u96c6\u6210\u5916\u90e8\u641c\u7d22\uff0c\u786e\u4fdd\u6bcf\u4e2a\u8bba\u70b9\u90fd\u6709\u660e\u786e\u7684\u8bc1\u636e\u652f\u6491\u3002", "motivation": "\u64b0\u5199\u6709\u6548\u7684\u53cd\u9a73\u4fe1\u662f\u4e00\u9879\u9ad8\u98ce\u9669\u4efb\u52a1\uff0c\u9700\u8981\u51c6\u786e\u7406\u89e3\u8bc4\u5ba1\u610f\u56fe\u548c\u7a3f\u4ef6\u7ec6\u8282\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u5c06\u5176\u89c6\u4e3a\u76f4\u63a5\u6587\u672c\u751f\u6210\u95ee\u9898\uff0c\u5b58\u5728\u5e7b\u89c9\u3001\u9057\u6f0f\u6279\u8bc4\u548c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u4f9d\u636e\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faRebuttalAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) \u5c06\u590d\u6742\u53cd\u9988\u5206\u89e3\u4e3a\u539f\u5b50\u5316\u5173\u5207\u70b9\uff1b2) \u52a8\u6001\u6784\u5efa\u6df7\u5408\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u538b\u7f29\u6458\u8981\u548c\u9ad8\u4fdd\u771f\u6587\u672c\uff1b3) \u96c6\u6210\u81ea\u4e3b\u6309\u9700\u5916\u90e8\u641c\u7d22\u6a21\u5757\u89e3\u51b3\u9700\u8981\u5916\u90e8\u6587\u732e\u7684\u95ee\u9898\uff1b4) \u5728\u8d77\u8349\u524d\u751f\u6210\u53ef\u68c0\u67e5\u7684\u54cd\u5e94\u8ba1\u5212\u3002", "result": "\u5728\u63d0\u51fa\u7684RebuttalBench\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u7ba1\u9053\u5728\u8986\u76d6\u7387\u3001\u5fe0\u5b9e\u5ea6\u548c\u7b56\u7565\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u4e3a\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u63a7\u7684\u52a9\u624b\u3002", "conclusion": "RebuttalAgent\u901a\u8fc7\u8bc1\u636e\u4e2d\u5fc3\u7684\u89c4\u5212\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u53cd\u9a73\u751f\u6210\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u9a8c\u8bc1\u548c\u53ef\u63a7\u7684\u540c\u884c\u8bc4\u5ba1\u8f85\u52a9\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.13115", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13115", "abs": "https://arxiv.org/abs/2601.13115", "authors": ["Fengran Mo", "Yifan Gao", "Sha Li", "Hansi Zeng", "Xin Liu", "Zhaoxuan Tan", "Xian Li", "Jianshu Chen", "Dakuo Wang", "Meng Jiang"], "title": "Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u8de8\u8f6e\u6b21\u4ea4\u66ff\u641c\u7d22\u548c\u63a8\u7406\u6765\u5904\u7406\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u52a8\u6001\u7528\u6237\u610f\u56fe\uff0c\u8d85\u8d8a\u73b0\u6709\u9759\u6001\u6d41\u6c34\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u5bf9\u8bdd\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u9759\u6001\u7684\u6539\u5199-\u68c0\u7d22-\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5206\u522b\u4f18\u5316\u4e0d\u540c\u6b65\u9aa4\uff0c\u5ffd\u7565\u4e86\u6df7\u5408\u4e3b\u52a8\u52a8\u4f5c\u7684\u8054\u5408\u4f18\u5316\u3002\u867d\u7136\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u5728\u5355\u8f6e\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u8054\u5408\u4f18\u5316\u68c0\u7d22\u548c\u751f\u6210\u7684\u6709\u6548\u6027\uff0c\u4f46\u7f3a\u4e4f\u5904\u7406\u591a\u8f6e\u4ea4\u4e92\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u8f6e\u6b21\u4e2d\u4ea4\u66ff\u8fdb\u884c\u641c\u7d22\u548c\u63a8\u7406\uff0c\u5b9e\u73b0\u63a2\u7d22\u6027\u548c\u9002\u5e94\u6027\u884c\u4e3a\u3002\u4f7f\u7528\u9488\u5bf9\u6f14\u5316\u7528\u6237\u76ee\u6807\u7684\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u8fdb\u884cRL\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u591a\u4e2a\u73b0\u6709\u5f3a\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8de8\u8f6e\u6b21\u4ea4\u66ff\u641c\u7d22\u548c\u63a8\u7406\u7684\u5bf9\u8bdd\u4ee3\u7406\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u52a8\u6001\u6f14\u5316\u7684\u7528\u6237\u610f\u56fe\uff0c\u76f8\u6bd4\u9759\u6001\u6d41\u6c34\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.14192", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14192", "abs": "https://arxiv.org/abs/2601.14192", "authors": ["Xiaofang Yang", "Lijun Li", "Heng Zhou", "Tong Zhu", "Xiaoye Qu", "Yuchen Fan", "Qianshan Wei", "Rui Ye", "Li Kang", "Yiran Qin", "Zhiqiang Kou", "Daizong Liu", "Qi Li", "Ning Ding", "Siheng Chen", "Jing Shao"], "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "comment": "35 pages, 200 references", "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u95ee\u9898\uff0c\u4ece\u5185\u5b58\u3001\u5de5\u5177\u5b66\u4e60\u548c\u89c4\u5212\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u51fa\u53d1\uff0c\u5206\u6790\u4e86\u6210\u672c\uff08\u5ef6\u8fdf\u3001token\u6570\u3001\u6b65\u9aa4\u7b49\uff09\uff0c\u63d0\u51fa\u4e86\u6548\u7387\u7684\u4e24\u79cd\u8bc4\u4f30\u65b9\u5f0f\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u57fa\u51c6\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5411\u667a\u80fd\u4f53\u7cfb\u7edf\u6269\u5c55\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6709\u6548\u6027\u800c\u5ffd\u89c6\u4e86\u6548\u7387\uff0c\u800c\u6548\u7387\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u7814\u7a76\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u4ece\u667a\u80fd\u4f53\u7684\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff08\u5185\u5b58\u3001\u5de5\u5177\u5b66\u4e60\u3001\u89c4\u5212\uff09\u51fa\u53d1\uff0c\u7efc\u8ff0\u4e86\u591a\u79cd\u63d0\u9ad8\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u538b\u7f29\u7ba1\u7406\u3001\u8bbe\u8ba1\u51cf\u5c11\u5de5\u5177\u8c03\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u3001\u63a7\u5236\u641c\u7d22\u673a\u5236\u7b49\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u6548\u7387\u8bc4\u4f30\u65b9\u5f0f\uff1a\u56fa\u5b9a\u6210\u672c\u9884\u7b97\u4e0b\u7684\u6709\u6548\u6027\u6bd4\u8f83\uff0c\u4ee5\u53ca\u76f8\u4f3c\u6709\u6548\u6027\u6c34\u5e73\u4e0b\u7684\u6210\u672c\u6bd4\u8f83\u3002", "result": "\u603b\u7ed3\u4e86\u667a\u80fd\u4f53\u6548\u7387\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u4e0d\u540c\u65b9\u6cd5\u80cc\u540e\u7684\u5171\u540c\u539f\u5219\uff0c\u6574\u7406\u4e86\u76f8\u5173\u57fa\u51c6\u8bc4\u4f30\u534f\u8bae\u548c\u5e38\u7528\u6548\u7387\u6307\u6807\uff0c\u5efa\u7acb\u4e86\u6548\u7387\u4e0e\u6709\u6548\u6027\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u667a\u80fd\u4f53\u6548\u7387\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u4e3a\u6548\u7387\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u6311\u6218\u548c\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.12988", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12988", "abs": "https://arxiv.org/abs/2601.12988", "authors": ["Zijian Wang", "Tiancheng Huang", "Hanqi Li", "Da Ma", "Lu Chen", "Kai Yu"], "title": "PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient", "comment": "35 pages, 9 figures, 7 tables", "summary": "The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a theoretical analysis that establishes DFPO's favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.", "AI": {"tldr": "PaperCompass\u662f\u4e00\u4e2a\u901a\u8fc7\u5206\u79bb\u9ad8\u5c42\u89c4\u5212\u548c\u7ec6\u7c92\u5ea6\u6267\u884c\u6765\u63d0\u5347\u79d1\u5b66\u6587\u732e\u9605\u8bfb\u4ee3\u7406\u6548\u7387\u7684\u6846\u67b6\uff0c\u91c7\u7528Draft-and-Follow Policy Optimization\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u901a\u8fc7\u624b\u52a8\u9605\u8bfb\u8ddf\u8e2a\u65b0\u8fdb\u5c55\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u4ee3\u7406\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5927\u91cf\u5de5\u7a0b\u5316\u63d0\u793a\uff0c\u8981\u4e48\u4f7f\u7528\u4f20\u7edf\u7684SFT-RL\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u5bb9\u6613\u5bfc\u81f4\u8fc7\u5ea6\u63a2\u7d22\u548c\u4f4e\u6548\u4ea7\u51fa\u3002", "method": "PaperCompass\u6846\u67b6\u5c06\u9ad8\u5c42\u89c4\u5212\u4e0e\u7ec6\u7c92\u5ea6\u6267\u884c\u5206\u79bb\uff1a\u9996\u5148\u5236\u5b9a\u660e\u786e\u8ba1\u5212\uff08draft plan\uff09\u6982\u8ff0\u884c\u52a8\u5e8f\u5217\uff0c\u7136\u540e\u901a\u8fc7\u8be6\u7ec6\u63a8\u7406\u5b9e\u4f8b\u5316\u6bcf\u4e2a\u6b65\u9aa4\uff0c\u4e3a\u76f8\u5e94\u51fd\u6570\u8c03\u7528\u9009\u62e9\u53c2\u6570\u3002\u8bad\u7ec3\u65b9\u6cd5\u91c7\u7528Draft-and-Follow Policy Optimization\uff08DFPO\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u8349\u7a3f\u8ba1\u5212\u548c\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728Paper-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPaperCompass\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u53d6\u5f97\u4e86\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u652f\u6301DFPO\u7684\u4f18\u5316\u7279\u6027\u3002", "conclusion": "PaperCompass\u901a\u8fc7\u5206\u79bb\u89c4\u5212\u548c\u6267\u884c\u7684\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u79d1\u5b66\u6587\u732e\u5904\u7406\u4e2d\u7684\"\u77e5\u884c\u5dee\u8ddd\"\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7a33\u5b9a\u53ef\u9760\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.13020", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13020", "abs": "https://arxiv.org/abs/2601.13020", "authors": ["Zhiyan Hou", "Haiyun Guo", "Haokai Ma", "Yandu Sun", "Yonghui Yang", "Jinqiao Wang"], "title": "PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning", "comment": null, "summary": "Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates forgetting.To address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51faPASs-based MoE-LoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8def\u5f84\u6fc0\u6d3b\u5b50\u7a7a\u95f4\u6821\u51c6\u8def\u7531\u548c\u7a33\u5b9a\u91cd\u8981\u79e9\u65b9\u5411\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u4e13\u5bb6\u5171\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u6297\u9057\u5fd8\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLoRA\u7684MoE\u65b9\u6cd5\u5728\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\uff0c\u8def\u7531\u5668\u548c\u4e13\u5bb6\u4f1a\u5171\u540c\u6f02\u79fb\uff0c\u5bfc\u81f4\u65e9\u671f\u8f93\u5165-\u4e13\u5bb6\u4e13\u4e1a\u5316\u9010\u6e10\u504f\u79bb\uff0c\u9020\u6210\u4e13\u5bb6\u8d23\u4efb\u6a21\u7cca\u548c\u9057\u5fd8\u52a0\u5267\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\"\u9519\u4f4d\u5171\u6f02\u79fb\"\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u8def\u5f84\u6fc0\u6d3b\u5b50\u7a7a\u95f4(PASs)\u4f5c\u4e3a\u80fd\u529b\u5bf9\u9f50\u7684\u5750\u6807\u7cfb\u7edf\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u56fa\u5b9a\u5bb9\u91cf\u7684PASs-based MoE-LoRA\u65b9\u6cd5\uff0c\u5305\u542bPAS\u5f15\u5bfc\u7684\u91cd\u65b0\u52a0\u6743\uff08\u6821\u51c6\u8def\u7531\uff09\u548cPAS\u611f\u77e5\u7684\u79e9\u7a33\u5b9a\u5316\uff08\u9009\u62e9\u6027\u7a33\u5b9a\u5148\u524d\u4efb\u52a1\u7684\u91cd\u8981\u79e9\u65b9\u5411\uff09\u3002", "result": "\u5728\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6297\u9057\u5fd8\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u57fa\u7ebf\u548cMoE-LoRA\u53d8\u4f53\uff0c\u4e14\u4e0d\u589e\u52a0\u53c2\u6570\u3002", "conclusion": "PASs-based MoE-LoRA\u65b9\u6cd5\u901a\u8fc7\u8def\u5f84\u6fc0\u6d3b\u5b50\u7a7a\u95f4\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u5171\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2601.13300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13300", "abs": "https://arxiv.org/abs/2601.13300", "authors": ["Yow-Fu Liou", "Yu-Chien Tang", "Yu-Hsiang Liu", "An-Zi Yen"], "title": "OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference", "comment": null, "summary": "Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOI-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5728\u591a\u9009\u9898\u754c\u9762\u6ce8\u5165\u8bef\u5bfc\u6027\u6307\u4ee4\u9009\u9879\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u5bf9\u6307\u4ee4\u5e72\u6270\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eLLM\u51b3\u7b56\u5bb9\u6613\u53d7\u5230\u793e\u4ea4\u7ebf\u7d22\u3001\u6846\u67b6\u6548\u5e94\u548c\u6307\u4ee4\u7b49\u5b9a\u5411\u4fe1\u53f7\u7684\u5f71\u54cd\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5bf9\u8fd9\u4e9b\u5e72\u6270\u7684\u9c81\u68d2\u6027", "method": "\u63d0\u51fa\u9009\u9879\u6ce8\u5165\u65b9\u6cd5\uff0c\u5728MCQA\u754c\u9762\u6dfb\u52a0\u5305\u542b\u8bef\u5bfc\u6027\u6307\u4ee4\u7684\u989d\u5916\u9009\u9879\uff0c\u6784\u5efa\u5305\u542b3000\u4e2a\u95ee\u9898\u7684OI-Bench\u57fa\u51c6\uff0c\u6db5\u76d616\u79cd\u6307\u4ee4\u7c7b\u578b", "result": "\u8bc4\u4f3012\u4e2aLLM\u663e\u793a\u663e\u8457\u6f0f\u6d1e\u548c\u5f02\u8d28\u6027\u9c81\u68d2\u6027\uff0c\u653b\u51fb\u6210\u529f\u7387\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u7f13\u89e3\u7b56\u7565", "conclusion": "OI-Bench\u652f\u6301\u5728\u57fa\u4e8e\u9009\u62e9\u7684\u754c\u9762\u4e2d\u7cfb\u7edf\u8bc4\u4f30LLM\u5bf9\u6307\u4ee4\u5e72\u6270\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u91cd\u8981\u5b89\u5168\u6f0f\u6d1e", "topic": "agent analysis"}}
{"id": "2601.13352", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.13352", "abs": "https://arxiv.org/abs/2601.13352", "authors": ["Yuxing Lu", "J. Ben Tamo", "Weichen Zhao", "Nan Sun", "Yishan Zhong", "Wenqi Shi", "Jinzhuo Wang", "May D. Wang"], "title": "LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction", "comment": "17 pages, 5 figures, 6 tables", "summary": "Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.", "AI": {"tldr": "LLM-as-RNN\uff1a\u5c06\u51bb\u7ed3LLM\u8f6c\u6362\u4e3a\u5faa\u73af\u9884\u6d4b\u5668\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8bb0\u5fc6\u72b6\u6001\u5b9e\u73b0\u5728\u7ebf\u5b66\u4e60\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff0c\u5728\u591a\u4e2a\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u65f6\u4e0a\u4e0b\u6587\u5386\u53f2\u4e0d\u53ef\u53d8\uff0c\u4e00\u65e6\u5728\u751f\u6210\u6b65\u9aa4t\u51fa\u9519\uff0c\u6a21\u578b\u7f3a\u4e4f\u53ef\u66f4\u65b0\u7684\u8bb0\u5fc6\u673a\u5236\u6765\u6539\u8fdb\u6b65\u9aa4t+1\u7684\u9884\u6d4b\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLLM-as-RNN\u6846\u67b6\uff0c\u5c06\u51bb\u7ed3LLM\u8f6c\u6362\u4e3a\u5faa\u73af\u9884\u6d4b\u5668\uff0c\u9690\u85cf\u72b6\u6001\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u8bb0\u5fc6\uff08\u7ed3\u6784\u5316\u7cfb\u7edf\u63d0\u793a\u6458\u8981\uff09\uff0c\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u7684\u6587\u672c\u91cd\u5199\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u66f4\u65b0\u72b6\u6001\uff0c\u5b9e\u73b0\u5728\u7ebf\u5b66\u4e60\u3002", "result": "\u5728\u533b\u7597\u3001\u6c14\u8c61\u548c\u91d1\u878d\u4e09\u4e2a\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM-as-RNN\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u3001\u5b8c\u6574\u5386\u53f2\u548cMemPrompt\u57fa\u7ebf\uff0c\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u53476.5%\uff0c\u540c\u65f6\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u4eba\u7c7b\u53ef\u8bfb\u5b66\u4e60\u8f68\u8ff9\u3002", "conclusion": "LLM-as-RNN\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8bb0\u5fc6\u72b6\u6001\u5b9e\u73b0\u6709\u6548\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff0c\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u7ea0\u6b63\u9519\u8bef\u5e76\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u6a21\u5f0f\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u5faa\u73af\u9884\u6d4b\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2601.13359", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13359", "abs": "https://arxiv.org/abs/2601.13359", "authors": ["Asen Dotsinski", "Panagiotis Eustratiadis"], "title": "Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection", "comment": null, "summary": "As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce \"sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., \"Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"sockpuppetting\"\u7684\u7b80\u5355\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6a21\u578b\u8f93\u51fa\u5f00\u5934\u63d2\u5165\u63a5\u53d7\u5e8f\u5217\u6765\u653b\u51fb\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ec5\u9700\u4e00\u884c\u4ee3\u7801\u4e14\u65e0\u9700\u4f18\u5316\uff0c\u653b\u51fb\u6210\u529f\u7387\u6bd4GCG\u65b9\u6cd5\u9ad880%\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u4fdd\u62a4\u5b83\u4eec\u514d\u53d7\u6076\u610f\u63d0\u793a\u653b\u51fb\u548c\u7406\u89e3\u53ef\u80fd\u7684\u653b\u51fb\u5411\u91cf\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u73b0\u6709\u81ea\u52a8\u5316\u8d8a\u72f1\u65b9\u6cd5\u5982GCG\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u66f4\u7b80\u5355\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\"sockpuppetting\"\u65b9\u6cd5\uff1a\u5728\u6a21\u578b\u8f93\u51fa\u5f00\u5934\u63d2\u5165\u63a5\u53d7\u5e8f\u5217\uff08\u5982\"Sure, here is how to...\"\uff09\uff0c\u7136\u540e\u8ba9\u6a21\u578b\u5b8c\u6210\u54cd\u5e94\u3002\u8fd8\u63a2\u7d22\u4e86\u6df7\u5408\u65b9\u6cd5\uff0c\u5728\u52a9\u624b\u6d88\u606f\u5757\u5185\u4f18\u5316\u5bf9\u6297\u540e\u7f00\u800c\u975e\u7528\u6237\u63d0\u793a\u3002", "result": "\u5728Qwen3-8B\u4e0a\uff0csockpuppetting\u5728\u6bcf\u63d0\u793a\u6bd4\u8f83\u4e2d\u6bd4GCG\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad880%\uff1b\u5728Llama-3.1-8B\u4e0a\uff0c\u6df7\u5408\u65b9\u6cd5\u5728\u63d0\u793a\u65e0\u5173\u8bbe\u7f6e\u4e2d\u6bd4GCG\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad864%\u3002", "conclusion": "sockpuppetting\u662f\u4e00\u79cd\u6709\u6548\u7684\u4f4e\u6210\u672c\u653b\u51fb\u65b9\u6cd5\uff0c\u5373\u4f7f\u662f\u6280\u672f\u4e0d\u719f\u7ec3\u7684\u653b\u51fb\u8005\u4e5f\u80fd\u4f7f\u7528\uff0c\u7a81\u663e\u4e86\u5f00\u6e90\u6a21\u578b\u9700\u8981\u9632\u5fa1\u8f93\u51fa\u524d\u7f00\u6ce8\u5165\u653b\u51fb\u7684\u5fc5\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.13244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13244", "abs": "https://arxiv.org/abs/2601.13244", "authors": ["Prateek Munjal", "Clement Christophe", "Ronnie Rajan", "Praveenkumar Kanithi"], "title": "Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks", "comment": null, "summary": "Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.", "AI": {"tldr": "\u6307\u4ee4\u5fae\u8c03\u5e76\u4e0d\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u800c\u662f\u8bf1\u5bfc\u8868\u9762\u6a21\u5f0f\u5339\u914d\u3002\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u4f18\u52bf\u4e0d\u7a33\u5b9a\uff0c\u5728\u96f6\u6837\u672cCoT\u8bbe\u7f6e\u4e0b\u57fa\u7840\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u5fae\u8c03\u589e\u76ca\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8106\u5f31\u3002", "motivation": "\u63a2\u7a76\u6307\u4ee4\u5fae\u8c03\u662f\u5426\u771f\u6b63\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u8bf1\u5bfc\u8868\u9762\u6a21\u5f0f\u5339\u914d\u3002\u5f53\u524d\u5bf9\u6307\u4ee4\u5fae\u8c03\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5185\u5728\u63a8\u7406\u80fd\u529b\u7f3a\u4e4f\u6e05\u6670\u7406\u89e3\u3002", "method": "\u5728\u6807\u51c6\u6570\u5b66\u57fa\u51c6\uff08GSM8K\uff09\u3001\u7ed3\u6784\u6270\u52a8\u53d8\u4f53\u548c\u9886\u57df\u504f\u79fb\u4efb\u52a1\uff08MedCalc\uff09\u4e0a\u8bc4\u4f30\u57fa\u7840\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u3002\u5206\u6790\u96f6\u6837\u672cCoT\u4e0e\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "1. \u5728\u96f6\u6837\u672cCoT\u8bbe\u7f6e\u4e0b\uff0c\u57fa\u7840\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u6307\u4ee4\u5fae\u8c03\u53d8\u4f53\uff08Llama3-70B\u4e0b\u964d\u9ad8\u8fbe32.67%\uff09\u30022. \u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u4ec5\u5728\u63d0\u4f9b\u5c11\u6837\u672c\u793a\u4f8b\u65f6\u624d\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7840\u6a21\u578b\u30023. \u5728\u9886\u57df\u7279\u5b9aMedCalc\u57fa\u51c6\u4e0a\uff0c\u57fa\u7840\u6a21\u578b\u4f18\u4e8e\u6307\u4ee4\u5fae\u8c03\u53d8\u4f53\u30024. \u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5728\u6270\u52a8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6025\u5267\u4e0b\u964d\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u5e76\u4e0d\u589e\u5f3a\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u800c\u662f\u4f7f\u6a21\u578b\u4f9d\u8d56\u7279\u5b9a\u63d0\u793a\u6a21\u5f0f\u3002\u5176\u6027\u80fd\u589e\u76ca\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8106\u5f31\uff0c\u8868\u660e\u5f53\u524d\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\u4e3b\u8981\u8bf1\u5bfc\u8868\u9762\u6a21\u5f0f\u5339\u914d\u800c\u975e\u7a33\u5065\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2601.13295", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.13295", "abs": "https://arxiv.org/abs/2601.13295", "authors": ["Arpandeep Khatua", "Hao Zhu", "Peter Tran", "Arya Prabhudesai", "Frederic Sadrieh", "Johann K. Lieberwirth", "Xinkai Yu", "Yicheng Fu", "Michael J. Ryan", "Jiaxin Pei", "Diyi Yang"], "title": "CooperBench: Why Coding Agents Cannot be Your Teammates Yet", "comment": "https://cooperbench.com", "summary": "Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CooperBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b600\u591a\u4e2a\u534f\u4f5c\u7f16\u7801\u4efb\u52a1\uff0c\u53d1\u73b0AI\u4ee3\u7406\u5728\u534f\u4f5c\u65f6\u6210\u529f\u7387\u6bd4\u5355\u72ec\u6267\u884c\u4f4e30%\uff0c\u63ed\u793a\u4e86\u6c9f\u901a\u4e0d\u7545\u3001\u627f\u8bfa\u504f\u79bb\u548c\u9519\u8bef\u9884\u671f\u4e09\u5927\u534f\u8c03\u95ee\u9898\uff0c\u547c\u5401\u4ece\u8ffd\u6c42\u4e2a\u4f53\u80fd\u529b\u8f6c\u5411\u53d1\u5c55\u793e\u4ea4\u667a\u80fd\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u590d\u6742\u5de5\u4f5c\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u534f\u4f5c\uff0c\u5b83\u4eec\u9700\u8981\u53d1\u5c55\u534f\u8c03\u80fd\u529b\u6765\u6210\u4e3a\u6709\u6548\u7684\u56e2\u961f\u6210\u5458\u3002\u7136\u800c\u4f5c\u8005\u5047\u8bbe\u5f53\u524d\u4ee3\u7406\u7f3a\u4e4f\u8fd9\u4e9b\u80fd\u529b\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u57fa\u51c6\u6765\u6d4b\u8bd5\u548c\u8bc4\u4f30\u4ee3\u7406\u7684\u534f\u4f5c\u80fd\u529b\u3002", "method": "\u5f15\u5165CooperBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b600\u591a\u4e2a\u534f\u4f5c\u7f16\u7801\u4efb\u52a1\uff0c\u8986\u76d612\u4e2a\u5e93\u548c4\u79cd\u7f16\u7a0b\u8bed\u8a00\u3002\u6bcf\u4e2a\u4efb\u52a1\u7ed9\u4e24\u4e2a\u4ee3\u7406\u5206\u914d\u4e0d\u540c\u7684\u529f\u80fd\u7279\u6027\uff0c\u8fd9\u4e9b\u7279\u6027\u53ef\u4ee5\u72ec\u7acb\u5b9e\u73b0\u4f46\u53ef\u80fd\u5728\u6ca1\u6709\u9002\u5f53\u534f\u8c03\u65f6\u53d1\u751f\u51b2\u7a81\u3002\u4efb\u52a1\u57fa\u4e8e\u771f\u5b9e\u5f00\u6e90\u4ed3\u5e93\u548c\u4e13\u5bb6\u7f16\u5199\u7684\u6d4b\u8bd5\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u6a21\u62df\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u7f16\u7801\u4ee3\u7406\u3002", "result": "\u89c2\u5bdf\u5230\"\u534f\u8c03\u8bc5\u5492\"\u73b0\u8c61\uff1a\u4ee3\u7406\u534f\u4f5c\u65f6\u7684\u5e73\u5747\u6210\u529f\u7387\u6bd4\u5355\u72ec\u6267\u884c\u4e24\u4e2a\u4efb\u52a1\u4f4e30%\u3002\u8fd9\u4e0e\u4eba\u7c7b\u56e2\u961f\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff08\u4eba\u7c7b\u56e2\u961f\u589e\u52a0\u6210\u5458\u901a\u5e38\u4f1a\u63d0\u9ad8\u751f\u4ea7\u529b\uff09\u3002\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u6c9f\u901a\u6e20\u9053\u88ab\u6a21\u7cca\u3001\u65f6\u673a\u4e0d\u5f53\u548c\u4e0d\u51c6\u786e\u7684\u4fe1\u606f\u5835\u585e\uff1b2) \u5373\u4f7f\u6709\u6709\u6548\u6c9f\u901a\uff0c\u4ee3\u7406\u4e5f\u4f1a\u504f\u79bb\u627f\u8bfa\uff1b3) \u4ee3\u7406\u7ecf\u5e38\u5bf9\u4ed6\u4eba\u8ba1\u5212\u548c\u6c9f\u901a\u6301\u6709\u9519\u8bef\u9884\u671f\u3002\u540c\u65f6\u89c2\u5bdf\u5230\u7f55\u89c1\u4f46\u6709\u8da3\u7684\u6d8c\u73b0\u534f\u8c03\u884c\u4e3a\uff0c\u5305\u62ec\u89d2\u8272\u5206\u5de5\u3001\u8d44\u6e90\u5206\u914d\u548c\u8c08\u5224\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u534f\u4f5c\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\uff0c\u547c\u5401\u4ece\u8ffd\u6c42\u4e2a\u4f53\u4ee3\u7406\u80fd\u529b\u8f6c\u5411\u53d1\u5c55\u793e\u4ea4\u667a\u80fd\u3002\u5f53\u524dAI\u4ee3\u7406\u7f3a\u4e4f\u6709\u6548\u7684\u56e2\u961f\u534f\u8c03\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u793e\u4ea4\u667a\u80fd\u6765\u89e3\u51b3\u534f\u4f5c\u4e2d\u7684\u6c9f\u901a\u3001\u627f\u8bfa\u548c\u9884\u671f\u95ee\u9898\u3002", "topic": "swe benchmark"}}
{"id": "2601.13453", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13453", "abs": "https://arxiv.org/abs/2601.13453", "authors": ["Aditya Thole", "Anmol Agrawal", "Arnav Ramamoorthy", "Dhruv Kumar"], "title": "PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving", "comment": null, "summary": "Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8/5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems", "AI": {"tldr": "PSA\u662f\u4e00\u4e2a\u81ea\u4e3b\u4ee3\u7406\uff0c\u4f7f\u7528Manim\u52a8\u753b\u751f\u6210\u957f\u8fbe6\u5206\u949f\u7684\u7269\u7406\u89e3\u91ca\u89c6\u9891\uff0c\u5e76\u901a\u8fc7\u5305\u542b15\u4e2a\u5b9a\u91cf\u53c2\u6570\u7684\u8bc4\u4f30\u7ba1\u9053\u548cVLM\u53cd\u9988\u8fed\u4ee3\u6539\u8fdb\u89c6\u9891\u8d28\u91cf\uff0c\u572832\u4e2a\u7269\u7406\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86100%\u89c6\u9891\u5b8c\u6210\u7387\uff0c\u4f46\u5b58\u5728\u89c6\u89c9\u5e03\u5c40\u4e0d\u4e00\u81f4\u548c\u89e3\u91ca\u9519\u8bef\u7b49\u6311\u6218\u3002", "motivation": "\u867d\u7136LLM\u5728\u6587\u672c\u7269\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u89c9\u89e3\u91ca\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u89c6\u89c9\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u7269\u7406\u6982\u5ff5\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u751f\u6210\u957f\u89c6\u9891\u89e3\u91ca\u7684\u7cfb\u7edf\u3002", "method": "\u5f15\u5165PhysicsSolutionAgent (PSA)\uff0c\u4f7f\u7528Manim\u52a8\u753b\u751f\u6210\u7269\u7406\u89e3\u91ca\u89c6\u9891\uff1b\u8bbe\u8ba1\u8bc4\u4f30\u7ba1\u9053\u8fdb\u884c15\u4e2a\u5b9a\u91cf\u53c2\u6570\u7684\u81ea\u52a8\u5316\u68c0\u67e5\uff0c\u5e76\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u9988\u6765\u8fed\u4ee3\u6539\u8fdb\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u572832\u4e2a\u7269\u7406\u95ee\u9898\u89c6\u9891\u8bc4\u4f30\u4e2d\uff0cPSA\u4f7f\u7528GPT-5-mini\u5b9e\u73b0\u4e86100%\u89c6\u9891\u5b8c\u6210\u7387\uff0c\u5e73\u5747\u81ea\u52a8\u5316\u5f97\u52063.8/5\u3002\u4f46\u5b9a\u6027\u5206\u6790\u548c\u4eba\u5de5\u68c0\u67e5\u53d1\u73b0\u89c6\u89c9\u5e03\u5c40\u4e0d\u4e00\u81f4\u3001\u89c6\u89c9\u5185\u5bb9\u89e3\u91ca\u9519\u8bef\u7b49\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u95ee\u9898\u96be\u5ea6\u548c\u7c7b\u578b\u5bf9\u89c6\u9891\u8d28\u91cf\u7684\u7cfb\u7edf\u6027\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u53ef\u9760Manim\u4ee3\u7801\u751f\u6210\u7684\u5173\u952e\u9650\u5236\uff0c\u7a81\u663e\u4e86\u591a\u6a21\u6001\u63a8\u7406\u548c\u8bc4\u4f30\u5728\u7269\u7406\u95ee\u9898\u89c6\u89c9\u89e3\u91ca\u4e2d\u7684\u5e7f\u6cdb\u6311\u6218\uff0c\u5f3a\u8c03\u672a\u6765\u591a\u6a21\u6001\u6559\u80b2\u7cfb\u7edf\u9700\u8981\u6539\u8fdb\u89c6\u89c9\u7406\u89e3\u3001\u9a8c\u8bc1\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "2601.13398", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13398", "abs": "https://arxiv.org/abs/2601.13398", "authors": ["Nickil Maveli", "Antonio Vergari", "Shay B. Cohen"], "title": "Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility", "comment": "32 pages (preprint)", "summary": "LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.", "AI": {"tldr": "RTCE\u662f\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801\u5927\u6a21\u578b\u5f80\u8fd4\u6267\u884c\u4e00\u81f4\u6027\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u4fdd\u6301\u7f16\u7801-\u89e3\u7801\u53cc\u5411\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5373\u4f7f\u4f7f\u7528\u96f6\u6837\u672c\u63d0\u793a\u3001\u76d1\u7763\u5fae\u8c03\u548c\u81ea\u53cd\u601d\u673a\u5236\u4e5f\u53ea\u80fd\u83b7\u5f97\u6709\u9650\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5f80\u8fd4\u4ee3\u7801\u6267\u884c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u4fdd\u6301\u524d\u540e\u5411\u6267\u884c\u4e00\u81f4\u6027\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u73b0\u6709\u57fa\u51c6\uff08\u5982I/O\u9884\u6d4b\u3001\u6267\u884c\u63a8\u7406\u6216\u5f80\u8fd4\u81ea\u7136\u8bed\u8a00\u57fa\u51c6\uff09\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u4e00\u81f4\u6027\u7f3a\u9677\u3002", "method": "\u63d0\u51faRoundTripCodeEval (RTCE)\u57fa\u51c6\uff0c\u5305\u542b\u56db\u4e2a\u4e0d\u540c\u7684\u4ee3\u7801\u6267\u884c\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u514d\u6267\u884c\u7684\u7cbe\u786e\u5339\u914d\u8bc4\u4f30\u53cc\u5c04\u4fdd\u771f\u5ea6\u3002\u4f7f\u7528\u96f6\u6837\u672c\u63d0\u793a\u3001\u76d1\u7763\u5fae\u8c03\u6267\u884c\u8f68\u8ff9\u548c\u81ea\u53cd\u601d\u673a\u5236\u7cfb\u7edf\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u4ee3\u7801LLMs\u3002", "result": "\u6bcf\u79cd\u65b9\u6cd5\uff08\u96f6\u6837\u672c\u63d0\u793a\u3001\u76d1\u7763\u5fae\u8c03\u3001\u81ea\u53cd\u601d\uff09\u90fd\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\uff0c\u4f46\u90fd\u65e0\u6cd5\u5f25\u5408\u5dee\u8ddd\u3002\u5f53\u524dLLMs\u5728\u771f\u6b63\u7684\u5f80\u8fd4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u53ef\u4fe1\u4ee3\u7801\u63a8\u7406\u6240\u9700\u7684\u5185\u5728\u4e00\u81f4\u6027\u3002", "conclusion": "RTCE\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u672a\u6355\u6349\u5230\u7684\u65b0\u89c1\u89e3\uff0c\u8868\u660e\u5f53\u524dLLMs\u7f3a\u4e4f\u53ef\u4fe1\u4ee3\u7801\u63a8\u7406\u6240\u9700\u7684\u5185\u5728\u4e00\u81f4\u6027\u3002\u57fa\u51c6\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728\u63a5\u53d7\u540e\u53d1\u5e03\u3002", "topic": "swe benchmark"}}
{"id": "2601.13537", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13537", "abs": "https://arxiv.org/abs/2601.13537", "authors": ["Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Minwoo Lee", "Kyomin Jung"], "title": "When Wording Steers the Evaluation: Framing Bias in LLM judges", "comment": "4 pages", "summary": "Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6846\u67b6\u504f\u5dee\uff0c\u5373\u63d0\u793a\u8bed\u7684\u5fae\u5c0f\u53d8\u5316\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u5224\u65ad\uff0c\u8fd9\u63ed\u793a\u4e86\u5f53\u524dLLM\u8bc4\u4f30\u7cfb\u7edf\u7684\u7ed3\u6784\u6027\u7f3a\u9677\u3002", "motivation": "\u867d\u7136\u5df2\u77e5LLM\u5bf9\u63d0\u793a\u8bed\u63aa\u8f9e\u654f\u611f\uff0c\u4f46\u8fd9\u79cd\u6846\u67b6\u504f\u5dee\u5bf9LLM\u8bc4\u4f30\u4efb\u52a1\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u5728\u9700\u8981\u7a33\u5b9a\u516c\u6b63\u5224\u65ad\u7684\u9ad8\u98ce\u9669\u8bc4\u4f30\u573a\u666f\u4e2d\uff0c\u8fd9\u79cd\u504f\u5dee\u53ef\u80fd\u4e25\u91cd\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "method": "\u53d7\u5fc3\u7406\u5b66\u6846\u67b6\u6548\u5e94\u542f\u53d1\uff0c\u7814\u7a76\u8005\u7cfb\u7edf\u7814\u7a76\u4e86\u63d0\u793a\u8bed\u6846\u67b6\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5224\u65ad\u3002\u8bbe\u8ba1\u4e86\u4f7f\u7528\u8c13\u8bcd-\u80af\u5b9a\u548c\u8c13\u8bcd-\u5426\u5b9a\u7ed3\u6784\u7684\u5bf9\u79f0\u63d0\u793a\u8bed\uff0c\u5728\u56db\u4e2a\u9ad8\u98ce\u9669\u8bc4\u4f30\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u4e8614\u4e2aLLM\u6cd5\u5b98\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6846\u67b6\u504f\u5dee\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\uff0c\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u660e\u663e\u7684\u6846\u67b6\u654f\u611f\u6027\u3002\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5728\u540c\u610f\u6216\u62d2\u7edd\u503e\u5411\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u5dee\u5f02\uff0c\u8868\u660e\u6846\u67b6\u504f\u5dee\u662f\u5f53\u524dLLM\u8bc4\u4f30\u7cfb\u7edf\u7684\u7ed3\u6784\u6027\u7279\u5f81\u3002", "conclusion": "\u6846\u67b6\u504f\u5dee\u662f\u5f53\u524dLLM\u8bc4\u4f30\u7cfb\u7edf\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u6846\u67b6\u611f\u77e5\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u7684\u7a33\u5b9a\u6027\u548c\u516c\u6b63\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.13590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13590", "abs": "https://arxiv.org/abs/2601.13590", "authors": ["Fan Huang", "Haewoon Kwak", "Jisun An"], "title": "Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions", "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u5728SMCR\u6c9f\u901a\u6846\u67b6\u4e0b\u5bf9\u8bf4\u670d\u7684\u6613\u611f\u6027\uff0c\u53d1\u73b0\u5c0f\u6a21\u578b\u6781\u6613\u88ab\u8bf4\u670d\uff0c\u5143\u8ba4\u77e5\u63d0\u793a\u53cd\u800c\u589e\u52a0\u8106\u5f31\u6027\uff0c\u5bf9\u6297\u6027\u5fae\u8c03\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u95ee\u7b54\u4efb\u52a1\uff0c\u4f46\u7814\u7a76\u8868\u660e\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u8bf4\u670d\u5e76\u91c7\u7eb3\u53cd\u4e8b\u5b9e\u4fe1\u5ff5\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728SMCR\u6846\u67b6\u4e0b\u5bf9\u8bf4\u670d\u7684\u6613\u611f\u6027\uff0c\u4e86\u89e3\u4e0d\u540c\u8bf4\u670d\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u4fe1\u5ff5\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528SMCR\u6c9f\u901a\u6846\u67b6\uff0c\u5728\u4e94\u4e2a\u4e3b\u6d41LLM\u548c\u4e09\u4e2a\u9886\u57df\uff08\u4e8b\u5b9e\u77e5\u8bc6\u3001\u533b\u5b66QA\u3001\u793e\u4f1a\u504f\u89c1\uff09\u4e0a\u5206\u6790\u4e0d\u540c\u8bf4\u670d\u7b56\u7565\u5bf9\u591a\u8f6e\u4ea4\u4e92\u4e2d\u4fe1\u5ff5\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002\u8bc4\u4f30\u5143\u8ba4\u77e5\u63d0\u793a\u548c\u5bf9\u6297\u6027\u5fae\u8c03\u4f5c\u4e3a\u9632\u5fa1\u63aa\u65bd\u7684\u6548\u679c\u3002", "result": "\u5c0f\u6a21\u578b\u8868\u73b0\u51fa\u6781\u7aef\u987a\u4ece\u6027\uff0c80%\u4ee5\u4e0a\u7684\u4fe1\u5ff5\u53d8\u5316\u53d1\u751f\u5728\u7b2c\u4e00\u8f6e\u8bf4\u670d\u4e2d\u3002\u5143\u8ba4\u77e5\u63d0\u793a\u53cd\u800c\u589e\u52a0\u8106\u5f31\u6027\uff0c\u52a0\u901f\u4fe1\u5ff5\u4fb5\u8680\u3002\u5bf9\u6297\u6027\u5fae\u8c03\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02\uff1aGPT-4o-mini\u8fbe\u523098.6%\u7684\u9c81\u68d2\u6027\uff0cMistral 7B\u4ece35.7%\u63d0\u5347\u523079.3%\uff0c\u800cLlama\u6a21\u578b\u5373\u4f7f\u5fae\u8c03\u540e\u4ecd\u4fdd\u6301\u9ad8\u6613\u611f\u6027\uff08<14%\uff09\u3002", "conclusion": "\u5f53\u524d\u9c81\u68d2\u6027\u5e72\u9884\u63aa\u65bd\u5b58\u5728\u663e\u8457\u7684\u6a21\u578b\u4f9d\u8d56\u6027\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u503c\u5f97\u4fe1\u8d56\u7684LLM\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684LLM\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.13572", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13572", "abs": "https://arxiv.org/abs/2601.13572", "authors": ["Xiangchi Yuan", "Dachuan Shi", "Chunhui Zhang", "Zheyuan Liu", "Shenglong Yao", "Soroush Vosoughi", "Wenke Lee"], "title": "Behavior Knowledge Merge in Reinforced Agentic Models", "comment": null, "summary": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRAM\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9RL\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6a21\u578b\u8fdb\u884c\u6a21\u578b\u5408\u5e76\uff0c\u89e3\u51b3\u4f20\u7edfSFT\u5408\u5e76\u65b9\u6cd5\u5728RL\u667a\u80fd\u4f53\u4e0a\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u667a\u80fd\u4f53\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u6a21\u578b\u5408\u5e76\u662f\u5c06\u591a\u4e2a\u4efb\u52a1\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6574\u5408\u4e3a\u901a\u7528\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5408\u5e76\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u76d1\u7763\u5fae\u8c03\u8bbe\u8ba1\uff0c\u4e0d\u9002\u7528\u4e8eRL\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u56e0\u4e3aRL\u4ea7\u751f\u7684\u4efb\u52a1\u5411\u91cf\u5177\u6709\u9ad8\u5ea6\u7a00\u758f\u6027\u548c\u5f02\u8d28\u6027\uff0c\u800cSFT\u5408\u5e76\u65b9\u6cd5\u5047\u8bbe\u5411\u91cf\u5bc6\u96c6\u4e14\u5168\u5c40\u53ef\u6bd4\u3002", "method": "\u63d0\u51faRAM\u6846\u67b6\uff0c\u4e13\u95e8\u4e3aRL\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6a21\u578b\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u89e3\u8026\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u53c2\u6570\u66f4\u65b0\uff0c\u5bf9\u5171\u4eab\u7ec4\u4ef6\u8fdb\u884c\u5e73\u5747\uff0c\u540c\u65f6\u9009\u62e9\u6027\u5730\u4fdd\u7559\u548c\u91cd\u65b0\u7f29\u653e\u72ec\u7279\u7ec4\u4ef6\uff0c\u4ee5\u62b5\u6d88\u53c2\u6570\u66f4\u65b0\u7684\u7a00\u91ca\u6548\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u9886\u57df\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAM\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5408\u5e76\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fd8\u80fd\u89e3\u9501\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u540c\u6f5c\u529b\uff0c\u5728\u5404\u81ea\u9886\u57df\u4e2d\u5b9e\u73b0\u4f18\u4e8e\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "conclusion": "RAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86RL\u667a\u80fd\u4f53\u6a21\u578b\u5408\u5e76\u4e2d\u7684\u4efb\u52a1\u5411\u91cf\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u7684\u5408\u5e76\u65b9\u6cd5\u4fdd\u7559\u4e86\u5173\u952e\u7684\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6a21\u578b\u6574\u5408\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.13717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13717", "abs": "https://arxiv.org/abs/2601.13717", "authors": ["Zehan Li", "Yuxuan Wang", "Ali El Lahib", "Ying-Jieh Xia", "Xinyu Pi"], "title": "Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff", "comment": null, "summary": "Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably \"rewind\" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u62df\u65e0\u77e5(SI)\u65e0\u6cd5\u6709\u6548\u8fd1\u4f3c\u771f\u5b9e\u65e0\u77e5(TI)\uff0c\u5728477\u4e2a\u7ade\u8d5b\u7ea7\u95ee\u9898\u548c9\u4e2a\u6a21\u578b\u4e0a\uff0cSI\u4e0eTI\u5b58\u572852%\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63a8\u7406\u8fc7\u7a0b\u65e0\u6cd5\u6291\u5236\u5148\u9a8c\u77e5\u8bc6\uff0c\u56e0\u6b64\u57fa\u4e8eSI\u7684\u56de\u987e\u6027\u9884\u6d4b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u7f3a\u9677\u3002", "motivation": "\u8bc4\u4f30LLM\u9884\u6d4b\u80fd\u529b\u9762\u4e34\u6839\u672c\u77db\u76fe\uff1a\u524d\u77bb\u6027\u8bc4\u4f30\u65b9\u6cd5\u4e25\u8c28\u4f46\u5ef6\u8fdf\u9ad8\uff0c\u56de\u987e\u6027\u9884\u6d4b\u9762\u4e34\u6570\u636e\u5feb\u901f\u51cf\u5c11\u7684\u95ee\u9898\u3002\u6a21\u62df\u65e0\u77e5(SI)\u4f5c\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u9700\u8981\u9a8c\u8bc1\u5176\u662f\u5426\u80fd\u6709\u6548\u8fd1\u4f3c\u771f\u5b9e\u65e0\u77e5(TI)\u3002", "method": "\u5728477\u4e2a\u7ade\u8d5b\u7ea7\u95ee\u9898\u4e0a\u6d4b\u8bd59\u4e2a\u6a21\u578b\uff0c\u6bd4\u8f83\u6a21\u62df\u65e0\u77e5(SI)\u4e0e\u771f\u5b9e\u65e0\u77e5(TI)\u7684\u8868\u73b0\uff0c\u5206\u6790\u622a\u6b62\u6307\u4ee4\u6548\u679c\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u5bf9\u77e5\u8bc6\u6291\u5236\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u63a8\u7406\u4f18\u5316\u6a21\u578b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "SI\u7cfb\u7edf\u6027\u5931\u8d25\uff1a1)\u622a\u6b62\u6307\u4ee4\u5bfc\u81f4SI\u4e0eTI\u5b58\u572852%\u6027\u80fd\u5dee\u8ddd\uff1b2)\u601d\u7ef4\u94fe\u63a8\u7406\u65e0\u6cd5\u6291\u5236\u5148\u9a8c\u77e5\u8bc6\uff0c\u5373\u4f7f\u63a8\u7406\u75d5\u8ff9\u4e0d\u542b\u660e\u786e\u622a\u6b62\u540e\u4fe1\u606f\uff1b3)\u63a8\u7406\u4f18\u5316\u6a21\u578b\u7684SI\u4fdd\u771f\u5ea6\u66f4\u5dee\uff0c\u5c3d\u7ba1\u63a8\u7406\u75d5\u8ff9\u8d28\u91cf\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u793a\u65e0\u6cd5\u53ef\u9760\"\u56de\u6eda\"\u6a21\u578b\u77e5\u8bc6\uff0c\u57fa\u4e8e\u9884\u622a\u6b62\u4e8b\u4ef6\u7684\u56de\u987e\u6027\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u65b9\u6cd5\u8bba\u7f3a\u9677\uff0c\u4e0d\u5efa\u8bae\u4f7f\u7528\u57fa\u4e8eSI\u7684\u56de\u987e\u6027\u8bbe\u7f6e\u6765\u57fa\u51c6\u6d4b\u8bd5\u9884\u6d4b\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.13653", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13653", "abs": "https://arxiv.org/abs/2601.13653", "authors": ["Xingjian Wu", "Junkai Lu", "Zhengyu Li", "Xiangfei Qiu", "Jilin Hu", "Chenjuan Guo", "Christian S. Jensen", "Bin Yang"], "title": "TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation", "comment": null, "summary": "Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.", "AI": {"tldr": "TimeART\u662f\u4e00\u4e2a\u878d\u5408\u5f3a\u5927\u591a\u529f\u80fd\u5de5\u5177\u5206\u6790\u80fd\u529b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0c\u4f5c\u4e3a\u5b8c\u5168\u81ea\u4e3b\u7684\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c100k\u4e13\u5bb6\u8f68\u8ff9\u8bed\u6599\u5e93\uff0c\u5728\u591a\u4e2aTSQA\u4efb\u52a1\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5b58\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5176\u5206\u6790\u89e3\u91ca\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff08\u5982\u707e\u5bb3\u9884\u6d4b\u3001\u91d1\u878d\u98ce\u9669\u63a7\u5236\uff09\uff0c\u4f46\u5f53\u524d\u5de5\u4f5c\u6d41\u7a0b\u4e3b\u8981\u4f9d\u8d56\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u52b3\u52a8\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u81ea\u52a8\u5316\u3002", "method": "1. \u63d0\u51faTimeART\u6846\u67b6\uff0c\u878d\u5408\u73b0\u6210\u5de5\u5177\u7684\u5206\u6790\u80fd\u529b\u548cLLMs\u7684\u63a8\u7406\u80fd\u529b\uff1b2. \u6536\u96c6100k\u4e13\u5bb6\u8f68\u8ff9\u8bed\u6599\u5e93TimeToolBench\uff1b3. \u8bbe\u8ba1\u56db\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u8ba9\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6a21\u578b\u4ece\u65e9\u671f\u7ecf\u9a8c\u548c\u81ea\u6211\u53cd\u601d\u4e2d\u5b66\u4e60\u3002", "result": "\u5728TimeToolBench\u4e0a\u8bad\u7ec3\u76848B\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6a21\u578b\u7ed3\u5408TimeART\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684state-of-the-art\u6027\u80fd\u3002", "conclusion": "TimeART\u4e3a\u81ea\u4e3b\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u878d\u5408\u5de5\u5177\u5206\u6790\u548cLLM\u63a8\u7406\u7684\u6846\u67b6\u5728\u81ea\u52a8\u5316\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.13964", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13964", "abs": "https://arxiv.org/abs/2601.13964", "authors": ["Cheol-Hui Lee", "Hwa-Yeon Lee", "Dong-Joo Kim"], "title": "RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning", "comment": null, "summary": "The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\\% and 8.80\\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\\% probability for sleep stage classification and Crop \\& Resize with a 77\\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \\href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.", "AI": {"tldr": "RL-BioAug\uff1a\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u81ea\u52a8\u786e\u5b9a\u6700\u4f18\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7684\u6846\u67b6\uff0c\u4ec5\u970010%\u6807\u8bb0\u6570\u636e\u6307\u5bfc\uff0c\u5728EEG\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u663e\u8457\u4f18\u4e8e\u968f\u673a\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "EEG\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u7279\u6027\u4f7f\u5f97\u9759\u6001\u6216\u968f\u673a\u6570\u636e\u589e\u5f3a\u7b56\u7565\u96be\u4ee5\u4fdd\u7559\u5185\u5728\u4fe1\u606f\uff0c\u5f71\u54cd\u5bf9\u6bd4\u5b66\u4e60\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94EEG\u4fe1\u53f7\u7279\u6027\u7684\u667a\u80fd\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRL-BioAug\u6846\u67b6\uff0c\u4f7f\u7528\u6807\u7b7e\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u81ea\u4e3b\u786e\u5b9a\u6700\u4f18\u589e\u5f3a\u7b56\u7565\u3002\u4ec5\u752810%\u6807\u8bb0\u6570\u636e\u6307\u5bfc\u4ee3\u7406\u7b56\u7565\uff0c\u4f7f\u7f16\u7801\u5668\u80fd\u4ee5\u4e25\u683c\u81ea\u76d1\u7763\u65b9\u5f0f\u5b66\u4e60\u9c81\u68d2\u8868\u793a\u3002", "result": "\u5728Sleep-EDFX\u548cCHB-MIT\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u968f\u673a\u9009\u62e9\u7b56\u7565\uff0cMacro-F1\u5206\u6570\u5206\u522b\u63d0\u53479.69%\u548c8.80%\u3002\u4ee3\u7406\u80fd\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff0c\u5982\u7761\u7720\u5206\u671f\u4efb\u52a162%\u6982\u7387\u9009\u62e9\u65f6\u95f4\u63a9\u7801\uff0c\u766b\u75eb\u68c0\u6d4b\u4efb\u52a177%\u6982\u7387\u9009\u62e9\u88c1\u526a\u4e0e\u8c03\u6574\u5927\u5c0f\u3002", "conclusion": "RL-BioAug\u6709\u6f5c\u529b\u53d6\u4ee3\u4f20\u7edf\u7684\u542f\u53d1\u5f0f\u589e\u5f3a\u65b9\u6cd5\uff0c\u5efa\u7acb\u6570\u636e\u589e\u5f3a\u7684\u81ea\u4e3b\u65b0\u8303\u5f0f\uff0c\u4e3aEEG\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u589e\u5f3a\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.14050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14050", "abs": "https://arxiv.org/abs/2601.14050", "authors": ["Yuxin Chen", "Zhengzhou Cai", "Xiangtian Ji", "Weixiang Zhao", "An Zhang", "Xiang Wang", "Tat-Seng Chua"], "title": "Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86MoE\u6a21\u578b\u7684\u591a\u8bed\u8a00\u5904\u7406\u673a\u5236\uff0c\u53d1\u73b0\u8def\u7531\u884c\u4e3a\u4e0e\u8bed\u8a00\u5bb6\u65cf\u5bf9\u9f50\uff0c\u4e13\u5bb6\u5229\u7528\u5448\u73b0\u6e05\u6670\u7684\u5c42\u7ea7\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8def\u7531\u5f15\u5bfc\u7684\u8f6c\u5411\u65b9\u6cd5\u4ee5\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1MoE\u67b6\u6784\u5728\u591a\u8bed\u8a00\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\uff08\u6027\u80fd\u63d0\u5347\u548c\u8de8\u8bed\u8a00\u5dee\u5f02\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u9700\u8981\u7cfb\u7edf\u5206\u6790MoE\u6a21\u578b\u7684\u8def\u7531\u884c\u4e3a\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u6a21\u5f0f\u3002", "method": "\u5bf9MoE\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u7814\u7a76\u8de8\u8bed\u8a00\u548c\u7f51\u7edc\u6df1\u5ea6\u7684\u8def\u7531\u884c\u4e3a\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u63d0\u51fa\u8def\u7531\u5f15\u5bfc\u7684\u8f6c\u5411\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u81ea\u9002\u5e94\u5730\u5c06\u4e2d\u95f4\u5c42\u7684\u8def\u7531\u884c\u4e3a\u5f15\u5bfc\u81f3\u4e0e\u4e3b\u5bfc\u8bed\u8a00\u76f8\u5173\u7684\u5171\u4eab\u4e13\u5bb6\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff1a1\uff09\u8def\u7531\u4e0e\u8bed\u8a00\u5bb6\u65cf\u5bf9\u9f50\uff1b2\uff09\u4e13\u5bb6\u5229\u7528\u5448\u73b0\u6e05\u6670\u7684\u5c42\u7ea7\u6a21\u5f0f\uff1b3\uff09\u9ad8\u8d44\u6e90\u8bed\u8a00\u4f9d\u8d56\u5171\u4eab\u4e13\u5bb6\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u66f4\u591a\u4f9d\u8d56\u8bed\u8a00\u4e13\u5c5e\u4e13\u5bb6\u4f46\u6027\u80fd\u8f83\u5f31\uff1b4\uff09\u65e9\u671f\u548c\u665a\u671fMoE\u5c42\u652f\u6301\u8bed\u8a00\u7279\u5b9a\u5904\u7406\uff0c\u4e2d\u95f4\u5c42\u4f5c\u4e3a\u8bed\u8a00\u65e0\u5173\u7684\u5bb9\u91cf\u4e2d\u5fc3\u3002\u63d0\u51fa\u7684\u8def\u7531\u5f15\u5bfc\u8f6c\u5411\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\uff0c\u7279\u522b\u662f\u8bed\u8a00\u76f8\u5173\u7684\u8bed\u8a00\u5bf9\u3002", "conclusion": "MoE\u6a21\u578b\u4e2d\u7684\u591a\u8bed\u8a00\u5904\u7406\u5177\u6709\u9ad8\u5ea6\u7ed3\u6784\u5316\u7279\u5f81\uff0c\u8def\u7531\u884c\u4e3a\u4e0e\u8bed\u8a00\u5bb6\u65cf\u76f8\u5173\u3002\u901a\u8fc7\u8def\u7531\u5f15\u5bfc\u7684\u4e2d\u95f4\u5c42\u8f6c\u5411\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdbMoE\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2601.14053", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.14053", "abs": "https://arxiv.org/abs/2601.14053", "authors": ["Badri N. Patro", "Vijay S. Agneeswaran"], "title": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems", "comment": null, "summary": "The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.", "AI": {"tldr": "LLMOrbit\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u7684\u5faa\u73af\u5206\u7c7b\u6cd5\uff0c\u5206\u67902019-2025\u5e74\u95f4\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8bc6\u522b\u4e09\u5927\u5371\u673a\uff08\u6570\u636e\u7a00\u7f3a\u3001\u6210\u672c\u589e\u957f\u3001\u80fd\u8017\u4e0d\u53ef\u6301\u7eed\uff09\u548c\u516d\u5927\u7a81\u7834\u58c1\u5792\u7684\u8303\u5f0f\uff0c\u63ed\u793a\u4e09\u4e2a\u8303\u5f0f\u8f6c\u53d8\uff08\u540e\u8bad\u7ec3\u589e\u76ca\u3001\u6548\u7387\u9769\u547d\u3001\u6c11\u4e3b\u5316\uff09\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u9886\u57df\u6b63\u7ecf\u5386\u4ece\u57fa\u7840Transformer\u67b6\u6784\u5230\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u63a8\u7406\u7cfb\u7edf\u7684\u9769\u547d\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u7406LLM\u53d1\u5c55\u8109\u7edc\uff0c\u5206\u6790\u5f53\u524d\u9762\u4e34\u7684\u6269\u5c55\u58c1\u5792\u5371\u673a\uff0c\u5e76\u63a2\u7d22\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u7684\u6280\u672f\u8303\u5f0f\u3002", "method": "\u63d0\u51faLLMOrbit\u5faa\u73af\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u516b\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8f68\u9053\u7ef4\u5ea6\u5206\u6790\u8d85\u8fc750\u4e2a\u6a21\u578b\u548c15\u4e2a\u7ec4\u7ec7\u3002\u4ece\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u65b9\u6cd5\u3001\u6548\u7387\u6a21\u5f0f\u7b49\u89d2\u5ea6\u7cfb\u7edf\u8003\u5bdf\u73b0\u4ee3LLM\u3001\u751f\u6210\u5f0fAI\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "\u8bc6\u522b\u4e09\u5927\u5371\u673a\uff1a\u6570\u636e\u7a00\u7f3a\uff082026-2028\u5e74\u8017\u5c3d9-27T tokens\uff09\u3001\u6210\u672c\u6307\u6570\u589e\u957f\uff085\u5e74\u5185\u4ece300\u4e07\u7f8e\u5143\u52303\u4ebf\u7f8e\u5143\u4ee5\u4e0a\uff09\u3001\u80fd\u8017\u4e0d\u53ef\u6301\u7eed\uff08\u589e\u52a022\u500d\uff09\u3002\u53d1\u73b0\u516d\u5927\u7a81\u7834\u58c1\u5792\u7684\u8303\u5f0f\uff1a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u91cf\u5316\u3001\u5206\u5e03\u5f0f\u8fb9\u7f18\u8ba1\u7b97\u3001\u6a21\u578b\u878d\u5408\u3001\u9ad8\u6548\u8bad\u7ec3\u3001\u5c0f\u578b\u4e13\u7528\u6a21\u578b\u3002\u63ed\u793a\u4e09\u4e2a\u8303\u5f0f\u8f6c\u53d8\uff1a\u540e\u8bad\u7ec3\u589e\u76ca\u3001\u6548\u7387\u9769\u547d\u3001\u6c11\u4e3b\u5316\u3002", "conclusion": "LLM\u53d1\u5c55\u6b63\u9762\u4e34\u6269\u5c55\u58c1\u5792\uff0c\u4f46\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u6548\u7387\u4f18\u5316\u548c\u5f00\u6e90\u6c11\u4e3b\u5316\u7b49\u521b\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u66b4\u529b\u6269\u5c55\u7684\u60c5\u51b5\u4e0b\u7ee7\u7eed\u63a8\u8fdb\u3002\u540e\u8bad\u7ec3\u6280\u672f\u3001\u6548\u7387\u9769\u547d\u548c\u5f00\u6e90\u8fd0\u52a8\u662f\u63a8\u52a8AI\u53d1\u5c55\u7684\u5173\u952e\u529b\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2601.14175", "categories": ["cs.LG", "cs.AI", "cs.CL", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.14175", "abs": "https://arxiv.org/abs/2601.14175", "authors": ["Suvrat Raju", "Praneeth Netrapalli"], "title": "A model of errors in transformers", "comment": "8+17pages", "summary": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u5728\u9700\u8981\u786e\u5b9a\u6027\u8f93\u51fa\u7684\u4efb\u52a1\uff08\u5982\u7b97\u672f\uff09\u4e2d\u7684\u9519\u8bef\u7387\uff0c\u63d0\u51fa\u9519\u8bef\u6e90\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u5c0f\u8bef\u5dee\u7d2f\u79ef\uff0c\u5e76\u63a8\u5bfc\u51fa\u9519\u8bef\u7387\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u4e24\u53c2\u6570\u5b9a\u91cf\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76LLM\u5728\u9700\u8981\u786e\u5b9a\u6027\u8f93\u51fa\u7684\u91cd\u590d\u6027\u4efb\u52a1\uff08\u5982\u7b97\u672f\uff09\u4e2d\u4e3a\u4f55\u4f1a\u51fa\u9519\uff0c\u6311\u6218\u4e86\u5173\u4e8eLLM\u9519\u8bef\u6e90\u4e8e\"\u63a8\u7406\u5d29\u6e83\"\u6216\u65e0\u6cd5\u8868\u8fbe\"\u7ec4\u5408\u51fd\u6570\"\u7684\u89c2\u70b9\u3002", "method": "\u91c7\u7528\"\u6709\u6548\u573a\u8bba\"\u89c6\u89d2\uff0c\u5c06LLM\u7684\u4f17\u591a\u53c2\u6570\u91cd\u7ec4\u4e3a\u4e24\u4e2a\u5173\u952e\u53c2\u6570\uff1a\u57fa\u672c\u566a\u58f0\u7387\u548c\u53ef\u80fd\u9519\u8bef\u9884\u6d4b\u7684\u4ee4\u724c\u6570\u91cf\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u9519\u8bef\u7387\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u5b9a\u91cf\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528Gemini 2.5 Flash\u3001Gemini 2.5 Pro\u548cDeepSeek R1\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u6d4b\u8bd5\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\uff0c\u9884\u6d4b\u9519\u8bef\u7387\u4e0e\u89c2\u5bdf\u5230\u7684\u9519\u8bef\u7387\u8868\u73b0\u51fa\u6781\u597d\u7684\u4e00\u81f4\u6027\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e5f\u53d1\u73b0\u4e86\u504f\u5dee\u3002\u6a21\u578b\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u6784\u5efa\u63d0\u793a\u6765\u964d\u4f4e\u9519\u8bef\u7387\u3002", "conclusion": "LLM\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u53ef\u4ee5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u7684\u5c0f\u8bef\u5dee\u7d2f\u79ef\u6765\u89e3\u91ca\uff0c\u800c\u4e0d\u662f\"\u63a8\u7406\u5d29\u6e83\"\u6216\u7ec4\u5408\u80fd\u529b\u4e0d\u8db3\u3002\u63d0\u51fa\u7684\u4e24\u53c2\u6570\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u9519\u8bef\u7387\uff0c\u5e76\u4e3a\u4f18\u5316\u63d0\u793a\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.14209", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14209", "abs": "https://arxiv.org/abs/2601.14209", "authors": ["Matthew Y. R. Yang", "Hao Bai", "Ian Wu", "Gene Yang", "Amrith Setlur", "Aviral Kumar"], "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning", "comment": null, "summary": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.", "AI": {"tldr": "InT\u662f\u4e00\u79cd\u8bad\u7ec3\u8303\u5f0f\uff0c\u8ba9\u6a21\u578b\u901a\u8fc7\u63d0\u51fa\u77ed\u5c0f\u3001\u6709\u9488\u5bf9\u6027\u7684\u4fee\u6b63\u6765\u5bf9\u81ea\u5df1\u7684\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u4ece\u800c\u5f15\u5bfc\u8f68\u8ff9\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u6807\u51c6RL\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u6807\u51c6\u7ed3\u679c\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u53ea\u5bf9\u6700\u7ec8\u7b54\u6848\u5206\u914d\u4fe1\u7528\uff0c\u5f53\u7ed3\u679c\u9519\u8bef\u65f6\u60e9\u7f5a\u6574\u4e2a\u63a8\u7406\u8f68\u8ff9\uff0c\u6b63\u786e\u65f6\u7edf\u4e00\u5f3a\u5316\u6240\u6709\u6b65\u9aa4\u3002\u8fd9\u5bfc\u81f4\u6b63\u786e\u4e2d\u95f4\u6b65\u9aa4\u5728\u5931\u8d25\u8f68\u8ff9\u4e2d\u88ab\u6291\u5236\uff0c\u800c\u865a\u5047\u6b65\u9aa4\u5728\u6210\u529f\u8f68\u8ff9\u4e2d\u88ab\u5f3a\u5316\uff0c\u5373\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e72\u9884\u8bad\u7ec3\uff08InT\uff09\uff1a\u5229\u7528\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e2d\u901a\u5e38\u53ef\u7528\u7684\u53c2\u8003\u89e3\uff0c\u6a21\u578b\u8bc6\u522b\u81ea\u8eab\u63a8\u7406\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\uff0c\u5e76\u63d0\u51fa\u5355\u6b65\u5e72\u9884\u6765\u5c06\u8f68\u8ff9\u91cd\u5b9a\u5411\u5230\u6b63\u786e\u89e3\u3002\u7136\u540e\u5bf9\u9519\u8bef\u70b9\u4e4b\u524d\u7684\u7b56\u7565\u5c55\u5f00\u52a0\u4e0a\u5e72\u9884\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5c06\u9519\u8bef\u5b9a\u4f4d\u5230\u5bfc\u81f4\u5931\u8d25\u7684\u5177\u4f53\u6b65\u9aa4\u3002", "result": "\u7ecf\u8fc7InT\u548c\u540e\u7eedRL\u5fae\u8c03\u540e\uff0c\u5728IMO-AnswerBench\u4e0a\u6bd44B\u53c2\u6570\u57fa\u7840\u6a21\u578b\u63d0\u9ad8\u4e86\u8fd114%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8egpt-oss-20b\u7b49\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "InT\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u4e3aRL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u5bf9\u81ea\u5df1\u7684\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6807\u51c6RL\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.14210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14210", "abs": "https://arxiv.org/abs/2601.14210", "authors": ["Rohan Bhatnagar", "Youran Sun", "Chi Andrew Zhang", "Yixin Wen", "Haizhao Yang"], "title": "HALT: Hallucination Assessment via Latent Testing", "comment": null, "summary": "Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u63a2\u9488\uff0c\u76f4\u63a5\u4eceLLM\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u8bfb\u53d6\u5e7b\u89c9\u98ce\u9669\uff0c\u5b9e\u73b0\u96f6\u5ef6\u8fdf\u7684\u5e7b\u89c9\u98ce\u9669\u8bc4\u4f30\uff0c\u7528\u4e8e\u9009\u62e9\u6027\u751f\u6210\u548c\u8def\u7531\u51b3\u7b56\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u53ef\u89c6\u4e3a\u5fe0\u5b9e\u8bfb\u53d6\u5931\u8d25\uff1a\u5c3d\u7ba1\u5185\u90e8\u8868\u793a\u53ef\u80fd\u7f16\u7801\u4e86\u67e5\u8be2\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u89e3\u7801\u538b\u529b\u4ecd\u4f1a\u4ea7\u751f\u6d41\u7545\u7b54\u6848\u3002\u9700\u8981\u76f4\u63a5\u4ece\u4e2d\u95f4\u5c42\u8bfb\u53d6\u88ab\u6700\u7ec8\u89e3\u7801\u9636\u6bb5\u8870\u51cf\u7684\u8ba4\u77e5\u4fe1\u53f7\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u63a2\u9488\uff0c\u4f5c\u4e3a\u5c0f\u578b\u8f85\u52a9\u7f51\u7edc\uff0c\u5728\u95ee\u9898\u6807\u8bb0\u7684\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u4e0a\u64cd\u4f5c\u3002\u8ba1\u7b97\u6210\u672c\u6bd4\u4ee4\u724c\u751f\u6210\u4f4e\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u53ef\u4e0e\u63a8\u7406\u5b8c\u5168\u5e76\u884c\u8bc4\u4f30\uff0c\u5b9e\u73b0\u8fd1\u77ac\u65f6\u5e7b\u89c9\u98ce\u9669\u8bc4\u4f30\u3002", "result": "\u5728\u56db\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4e2aLLM\u5bb6\u65cf\u4e0a\uff0c\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684AUROC\u548cAURAC\uff0c\u5728\u6570\u636e\u96c6\u504f\u79fb\u4e0b\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u4e2d\u95f4\u8868\u793a\u7684\u53ef\u89e3\u91ca\u7ed3\u6784\u3002", "conclusion": "\u5feb\u901f\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u8bfb\u53d6\u53ef\u4f5c\u4e3a\u53ef\u9760\u667a\u80fd\u4f53AI\u7684\u539f\u5219\u6027\u57fa\u7840\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u751f\u6210\u548c\u8def\u7531\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u95ee\u7b54\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2601.14242", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14242", "abs": "https://arxiv.org/abs/2601.14242", "authors": ["Bertie Vidgen", "Austin Mann", "Abby Fennelly", "John Wright Stanly", "Lucas Rothman", "Marco Burstein", "Julien Benchek", "David Ostrofsky", "Anirudh Ravichandran", "Debnil Sur", "Neel Venugopal", "Alannah Hsia", "Isaac Robinson", "Calix Huang", "Olivia Varones", "Daniyal Khan", "Michael Haines", "Zach Richards", "Chirag Mahapatra", "Brendan Foody", "Osvald Nitski"], "title": "APEX-Agents", "comment": null, "summary": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.", "AI": {"tldr": "APEX-Agents\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u4ee3\u7406\u6267\u884c\u6295\u8d44\u94f6\u884c\u3001\u7ba1\u7406\u54a8\u8be2\u548c\u6cd5\u5f8b\u9886\u57df\u957f\u65f6\u7a0b\u8de8\u5e94\u7528\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u548c\u5de5\u5177\uff0cGemini 3 Flash\u8868\u73b0\u6700\u4f73(24.0%)\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30AI\u4ee3\u7406\u5728\u4e13\u4e1a\u9886\u57df\uff08\u6295\u8d44\u94f6\u884c\u3001\u7ba1\u7406\u54a8\u8be2\u3001\u6cd5\u5f8b\uff09\u4e2d\u6267\u884c\u590d\u6742\u957f\u65f6\u7a0b\u8de8\u5e94\u7528\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u51c6\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u771f\u5b9e\u5de5\u4f5c\u573a\u666f\u3002", "method": "\u521b\u5efaAPEX-Agents\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b480\u4e2a\u4efb\u52a1\uff0c\u6a21\u62df\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\uff08\u6587\u4ef6\u548c\u5de5\u5177\uff09\uff0c\u4f7f\u7528Pass@1\u8bc4\u4f308\u4e2a\u4ee3\u7406\uff0c\u5e76\u5f00\u6e90\u57fa\u51c6\u6570\u636e\u548cArchipelago\u6267\u884c\u8bc4\u4f30\u57fa\u7840\u8bbe\u65bd\u3002", "result": "Gemini 3 Flash (Thinking=High)\u5f97\u5206\u6700\u9ad8(24.0%)\uff0c\u5176\u6b21\u662fGPT-5.2\u3001Claude Opus 4.5\u548cGemini 3 Pro\u3002\u6240\u6709\u6d4b\u8bd5\u4ee3\u7406\u8868\u73b0\u76f8\u5bf9\u8f83\u4f4e\uff0c\u663e\u793a\u957f\u65f6\u7a0b\u8de8\u5e94\u7528\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "APEX-Agents\u4e3a\u8bc4\u4f30AI\u4ee3\u7406\u5728\u4e13\u4e1a\u9886\u57df\u7684\u957f\u65f6\u7a0b\u8de8\u5e94\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5f00\u6e90\u57fa\u7840\u8bbe\u65bd\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u53d1\u5c55\uff0c\u5f53\u524d\u4ee3\u7406\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "topic": "agent benchmark"}}
{"id": "2601.14234", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.14234", "abs": "https://arxiv.org/abs/2601.14234", "authors": ["Qiyang Li", "Sergey Levine"], "title": "Q-learning with Adjoint Matching", "comment": "32 pages, 8 figures, 7 tables", "summary": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.", "AI": {"tldr": "QAM\u662f\u4e00\u79cd\u65b0\u7684TD\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f34\u968f\u5339\u914d\u6280\u672f\u89e3\u51b3\u8fde\u7eed\u52a8\u4f5cRL\u4e2d\u6269\u6563/\u6d41\u5339\u914d\u7b56\u7565\u4f18\u5316\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u8fde\u7eed\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff1a\u5982\u4f55\u9ad8\u6548\u4f18\u5316\u5177\u6709\u8868\u8fbe\u529b\u7684\u6269\u6563\u6216\u6d41\u5339\u914d\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u4f7f\u7528\u503c\u51fd\u6570\u800c\u4e22\u5f03\u68af\u5ea6\u4fe1\u606f\uff0c\u8981\u4e48\u4f9d\u8d56\u8fd1\u4f3c\u65b9\u6cd5\u727a\u7272\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u6216\u5f15\u5165\u504f\u5dee\u3002", "method": "\u63d0\u51faQ-learning with Adjoint Matching (QAM)\uff0c\u5229\u7528\u4f34\u968f\u5339\u914d\u6280\u672f\u5c06\u8bc4\u8bba\u5bb6\u7684\u52a8\u4f5c\u68af\u5ea6\u8f6c\u6362\u4e3a\u6b65\u8fdb\u5f0f\u76ee\u6807\u51fd\u6570\uff0c\u907f\u514d\u4e0d\u7a33\u5b9a\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u540c\u65f6\u63d0\u4f9b\u65e0\u504f\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u7b56\u7565\u3002\u7ed3\u5408\u65f6\u95f4\u5dee\u5206\u5907\u4efd\u8fdb\u884c\u8bc4\u8bba\u5bb6\u5b66\u4e60\u3002", "result": "\u5728\u56f0\u96be\u7684\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e0a\uff0cQAM\u5728\u79bb\u7ebfRL\u548c\u79bb\u7ebf\u5230\u5728\u7ebfRL\u4e2d\u90fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "QAM\u901a\u8fc7\u4f34\u968f\u5339\u914d\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u52a8\u4f5cRL\u4e2d\u6269\u6563\u7b56\u7565\u4f18\u5316\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u8868\u8fbe\u529b\u5f3a\u7684\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u7a33\u5b9a\u4e14\u65e0\u504f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.14243", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14243", "abs": "https://arxiv.org/abs/2601.14243", "authors": ["Haocheng Xi", "Charlie Ruan", "Peiyuan Liao", "Yujun Lin", "Han Cai", "Yilong Zhao", "Shuo Yang", "Kurt Keutzer", "Song Han", "Ligeng Zhu"], "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow", "comment": "11 pages, 6 figures, 4 tables", "summary": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.", "AI": {"tldr": "Jet-RL\u63d0\u51fa\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709BF16\u8bad\u7ec3+FP8\u63a8\u7406\u7b56\u7565\u7684\u6570\u503c\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b033%\u63a8\u7406\u52a0\u901f\u548c16%\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u73b0\u6709RL\u8bad\u7ec3\u4e2d\u63a8\u7406\u9636\u6bb5\u536070%\u4ee5\u4e0a\u65f6\u95f4\uff0c\u91cf\u5316\u8bad\u7ec3\uff08\u7279\u522b\u662fFP8\u7cbe\u5ea6\uff09\u662f\u89e3\u51b3\u74f6\u9888\u7684\u53ef\u884c\u65b9\u6848\u3002\u4f46\u5e38\u7528\u7684BF16\u8bad\u7ec3+FP8\u63a8\u7406\u7b56\u7565\u5728\u957f\u5e8f\u5217\u548c\u590d\u6742\u4efb\u52a1\u4e0b\u5b58\u5728\u4e25\u91cd\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u7cbe\u5ea6\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u63d0\u51faJet-RL\u6846\u67b6\uff0c\u91c7\u7528\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u6d41\u540c\u65f6\u7528\u4e8e\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u6700\u5c0f\u5316\u6570\u503c\u5dee\u5f02\uff0c\u6d88\u9664\u4f4e\u6548\u7684\u8de8\u6b65\u9aa4\u6821\u51c6\u9700\u6c42\u3002", "result": "Jet-RL\u5728\u63a8\u7406\u9636\u6bb5\u5b9e\u73b033%\u52a0\u901f\uff0c\u8bad\u7ec3\u9636\u6bb541%\u52a0\u901f\uff0c\u7aef\u5230\u7aef16%\u52a0\u901f\uff0c\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6536\u655b\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u6d41\u662f\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u6570\u503c\u4e0d\u5339\u914d\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\uff0cJet-RL\u6846\u67b6\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.82801a8e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fbuilding-bugbot%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/g3Bq7D_m3HbXyJ_zQjOnvx7CDk2mWDMexpuhRIhf_T8=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fbuilding-bugbot%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/g3Bq7D_m3HbXyJ_zQjOnvx7CDk2mWDMexpuhRIhf_T8=440", "authors": ["TLDR Newsletter"], "title": "Bugbot's Code Review Evolution", "comment": "Source: TLDR Newsletter, Date: 2026-01-19, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fbuilding-bugbot%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/g3Bq7D_m3HbXyJ_zQjOnvx7CDk2mWDMexpuhRIhf_T8=440", "summary": "Bugbot's Code Review Evolution (5 minute read) Cursor's Bugbot has evolved into a capable AI code reviewer that detects logic, performance, and security issues in pull requests.", "source": "tldr", "AI": {"tldr": "Cursor\u7684Bugbot\u5df2\u8fdb\u5316\u4e3a\u80fd\u68c0\u6d4b\u903b\u8f91\u3001\u6027\u80fd\u548c\u5b89\u5168\u6027\u95ee\u9898\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177", "motivation": "\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u8017\u65f6\u4e14\u5bb9\u6613\u9057\u6f0f\u95ee\u9898\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u548c\u5f00\u53d1\u6548\u7387", "method": "Bugbot\u4f5c\u4e3aAI\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406\uff0c\u5206\u6790pull requests\u4e2d\u7684\u4ee3\u7801\uff0c\u68c0\u6d4b\u903b\u8f91\u9519\u8bef\u3001\u6027\u80fd\u95ee\u9898\u548c\u5b89\u5168\u6f0f\u6d1e", "result": "Bugbot\u5df2\u8fdb\u5316\u4e3a\u80fd\u591f\u6709\u6548\u8bc6\u522b\u591a\u79cd\u4ee3\u7801\u95ee\u9898\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177", "conclusion": "AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5982Bugbot\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u548c\u5f00\u53d1\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2601.c9adc030", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FnPJc6z/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/83dyNf4mcJ1ddoFXXifAE6StoeYxCMGJmaS9mzNvzrc=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FnPJc6z/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/83dyNf4mcJ1ddoFXXifAE6StoeYxCMGJmaS9mzNvzrc=440", "authors": ["TLDR Newsletter"], "title": "The Bitter Lesson of Agent Frameworks", "comment": "Source: TLDR Newsletter, Date: 2026-01-19, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FnPJc6z/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/83dyNf4mcJ1ddoFXXifAE6StoeYxCMGJmaS9mzNvzrc=440", "summary": "The Bitter Lesson of Agent Frameworks (12 minute read) An agent is just a for-loop of messages. Agent frameworks aren't required. Models only need a complete action space, a for-loop, an explicit exit, and context management. Every abstraction is a liability, and every 'helper' is a failure point. The less you build, the more it works.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u8ba4\u4e3a\u667a\u80fd\u4f53\u6846\u67b6\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u667a\u80fd\u4f53\u672c\u8d28\u4e0a\u53ea\u662f\u6d88\u606f\u7684for\u5faa\u73af\u3002\u6a21\u578b\u53ea\u9700\u8981\u5b8c\u6574\u7684\u52a8\u4f5c\u7a7a\u95f4\u3001\u5faa\u73af\u3001\u663e\u5f0f\u9000\u51fa\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002\u6bcf\u4e2a\u62bd\u8c61\u90fd\u662f\u8d1f\u62c5\uff0c\u6bcf\u4e2a\"\u52a9\u624b\"\u90fd\u662f\u5931\u8d25\u70b9\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u6846\u67b6\u8fc7\u5ea6\u590d\u6742\u5316\uff0c\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u62bd\u8c61\u5c42\u548c\u4f9d\u8d56\uff0c\u53cd\u800c\u964d\u4f4e\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002\u4f5c\u8005\u5e0c\u671b\u63ed\u793a\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u672c\u8d28\uff0c\u63d0\u5021\u6781\u7b80\u4e3b\u4e49\u7684\u8bbe\u8ba1\u54f2\u5b66\u3002", "method": "\u63d0\u51fa\u6781\u7b80\u4e3b\u4e49\u65b9\u6cd5\uff1a\u667a\u80fd\u4f53\u53ea\u9700\u8981\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u5b8c\u6574\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c2\uff09\u6d88\u606f\u5faa\u73af\uff08for-loop\uff09\uff0c3\uff09\u663e\u5f0f\u9000\u51fa\u673a\u5236\uff0c4\uff09\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002\u907f\u514d\u4f7f\u7528\u590d\u6742\u7684\u6846\u67b6\u548c\u62bd\u8c61\u5c42\u3002", "result": "\u901a\u8fc7\u7b80\u5316\u8bbe\u8ba1\uff0c\u667a\u80fd\u4f53\u7cfb\u7edf\u53d8\u5f97\u66f4\u52a0\u53ef\u9760\u3001\u9ad8\u6548\u4e14\u6613\u4e8e\u7ef4\u62a4\u3002\u51cf\u5c11\u6784\u5efa\u7684\u5185\u5bb9\u53cd\u800c\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u5de5\u4f5c\u6548\u679c\u3002", "conclusion": "\u667a\u80fd\u4f53\u6846\u67b6\u901a\u5e38\u662f\u4e0d\u5fc5\u8981\u7684\u590d\u6742\u6027\u6765\u6e90\u3002\u6700\u6709\u6548\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u5e94\u8be5\u57fa\u4e8e\u6781\u7b80\u4e3b\u4e49\u539f\u5219\uff0c\u907f\u514d\u8fc7\u5ea6\u5de5\u7a0b\u5316\uff0c\u4e13\u6ce8\u4e8e\u6838\u5fc3\u7684\u6d88\u606f\u5faa\u73af\u548c\u52a8\u4f5c\u6267\u884c\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.941fb925", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.letta.com%2Fblog%2Fbenchmarking-ai-agent-memory%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/Gh3qSqbY708mrH60zIeaCkNgAtuxkC4nRyfjQ7dk5aA=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.letta.com%2Fblog%2Fbenchmarking-ai-agent-memory%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/Gh3qSqbY708mrH60zIeaCkNgAtuxkC4nRyfjQ7dk5aA=440", "authors": ["TLDR Newsletter"], "title": "Benchmarking AI Agent Memory: Is a Filesystem All You Need?", "comment": "Source: TLDR Newsletter, Date: 2026-01-19, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.letta.com%2Fblog%2Fbenchmarking-ai-agent-memory%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/Gh3qSqbY708mrH60zIeaCkNgAtuxkC4nRyfjQ7dk5aA=440", "summary": "Benchmarking AI Agent Memory: Is a Filesystem All You Need? (6 minute read) An agent's memory depends on its architecture, tools, and the underlying model. You can always mix and match frameworks, tools, and models. Simple file system tools are sufficient for a well-designed agent. More complex memory tools can be plugged in via MCP or custom tools.", "source": "tldr", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u8bbe\u8ba1\u826f\u597d\u7684AI\u667a\u80fd\u4f53\uff0c\u7b80\u5355\u7684\u6587\u4ef6\u7cfb\u7edf\u5de5\u5177\u8db3\u4ee5\u6ee1\u8db3\u5176\u8bb0\u5fc6\u9700\u6c42\uff0c\u800c\u66f4\u590d\u6742\u7684\u8bb0\u5fc6\u5de5\u5177\u53ef\u4ee5\u901a\u8fc7MCP\u6216\u81ea\u5b9a\u4e49\u5de5\u5177\u96c6\u6210\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u80fd\u529b\u4f9d\u8d56\u4e8e\u5176\u67b6\u6784\u3001\u5de5\u5177\u548c\u5e95\u5c42\u6a21\u578b\uff0c\u4f46\u4e1a\u754c\u5bf9\u4e8e\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u7684\u6700\u4f73\u5b9e\u8df5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e0d\u540c\u8bb0\u5fc6\u5de5\u5177\uff08\u4ece\u7b80\u5355\u6587\u4ef6\u7cfb\u7edf\u5230\u590d\u6742\u8bb0\u5fc6\u7cfb\u7edf\uff09\u5728AI\u667a\u80fd\u4f53\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u901a\u8fc7MCP\uff08\u6a21\u578b\u63a7\u5236\u534f\u8bae\uff09\u6216\u81ea\u5b9a\u4e49\u5de5\u5177\u96c6\u6210\u590d\u6742\u8bb0\u5fc6\u529f\u80fd\u7684\u53ef\u80fd\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u8bbe\u8ba1\u826f\u597d\u7684\u667a\u80fd\u4f53\uff0c\u7b80\u5355\u7684\u6587\u4ef6\u7cfb\u7edf\u5de5\u5177\u5df2\u7ecf\u8db3\u591f\u6709\u6548\uff0c\u800c\u66f4\u590d\u6742\u7684\u8bb0\u5fc6\u5de5\u5177\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u9009\u63d2\u4ef6\u901a\u8fc7\u6807\u51c6\u5316\u63a5\u53e3\u96c6\u6210\u3002", "conclusion": "\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u6027\u80fd\u66f4\u591a\u53d6\u51b3\u4e8e\u6574\u4f53\u67b6\u6784\u8bbe\u8ba1\u800c\u975e\u8bb0\u5fc6\u5de5\u5177\u7684\u590d\u6742\u5ea6\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u4ece\u7b80\u5355\u7684\u6587\u4ef6\u7cfb\u7edf\u5f00\u59cb\uff0c\u518d\u6839\u636e\u9700\u8981\u9010\u6b65\u96c6\u6210\u66f4\u590d\u6742\u7684\u8bb0\u5fc6\u529f\u80fd\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.da278568", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.08763%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/fGR9o0QYvm2oJck2kaT5PIVtU3XV4p4UcnzoXGH3UBY=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.08763%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/fGR9o0QYvm2oJck2kaT5PIVtU3XV4p4UcnzoXGH3UBY=440", "authors": ["TLDR Newsletter"], "title": "Uniqueness-Aware RL for LLM Diversity", "comment": "Source: TLDR Newsletter, Date: 2026-01-19, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.08763%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/fGR9o0QYvm2oJck2kaT5PIVtU3XV4p4UcnzoXGH3UBY=440", "summary": "Uniqueness-Aware RL for LLM Diversity (14 minute read) MIT researchers have proposed a new reinforcement learning method that rewards diverse high-level solution strategies using LLM-based clustering of rollouts. This method improves pass@k across tasks without degrading pass@1.", "source": "tldr", "AI": {"tldr": "MIT\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u8f68\u8ff9\u805a\u7c7b\u6765\u5956\u52b1\u591a\u6837\u5316\u7684\u9ad8\u5c42\u89e3\u51b3\u65b9\u6848\u7b56\u7565\uff0c\u5728\u4e0d\u964d\u4f4epass@1\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8pass@k\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5f80\u5f80\u4ea7\u751f\u540c\u8d28\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86pass@k\u6027\u80fd\u7684\u63d0\u5347\u6f5c\u529b", "method": "\u63d0\u51fa\u72ec\u7279\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u5bf9\u751f\u6210\u7684\u4ee3\u7801\u8f68\u8ff9\u8fdb\u884c\u805a\u7c7b\uff0c\u8bc6\u522b\u4e0d\u540c\u7684\u9ad8\u5c42\u89e3\u51b3\u65b9\u6848\u7b56\u7565\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4e2d\u5f15\u5165\u591a\u6837\u6027\u5956\u52b1\u9879", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86pass@k\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86pass@1\u6027\u80fd\u4e0d\u4e0b\u964d\uff0c\u8bc1\u660e\u4e86\u591a\u6837\u6027\u5956\u52b1\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u5956\u52b1\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u6574\u4f53\u6027\u80fd\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.557a8fb3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frijnard.com%2Fblog%2Fthe-code-only-agent%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/YFKB1v8wpUsROJ2HzKFkFFI1s_n5Gv1K_aFUQGZxM2g=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frijnard.com%2Fblog%2Fthe-code-only-agent%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/YFKB1v8wpUsROJ2HzKFkFFI1s_n5Gv1K_aFUQGZxM2g=440", "authors": ["TLDR Newsletter"], "title": "The Code-Only Agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-19, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frijnard.com%2Fblog%2Fthe-code-only-agent%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/YFKB1v8wpUsROJ2HzKFkFFI1s_n5Gv1K_aFUQGZxM2g=440", "summary": "The Code-Only Agent (17 minute read) Building an agent can be overwhelming. The ecosystem pushes developers toward complexity and the 'right' way to do things. This post explores a prompting pattern where agents create and run code as a response to every prompt. In this framework, agents can't do anything productive without writing code. This method produces a code witness of an answer, codifying the work in a very literal sense. It forces agents to produce proofs, run proofs, and interpret p...", "source": "tldr", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\"\u7eaf\u4ee3\u7801\u4ee3\u7406\"\u6a21\u5f0f\uff0c\u8ba9AI\u4ee3\u7406\u901a\u8fc7\u7f16\u5199\u548c\u8fd0\u884c\u4ee3\u7801\u6765\u54cd\u5e94\u6bcf\u4e2a\u63d0\u793a\uff0c\u5f3a\u5236\u4ee3\u7406\u751f\u6210\u53ef\u6267\u884c\u7684\u4ee3\u7801\u8bc1\u660e\u800c\u975e\u76f4\u63a5\u56de\u7b54\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u5f00\u53d1\u751f\u6001\u7cfb\u7edf\u8fc7\u4e8e\u590d\u6742\uff0c\u5f00\u53d1\u8005\u88ab\u63a8\u5411\u590d\u6742\u7684\"\u6b63\u786e\"\u5b9e\u73b0\u65b9\u5f0f\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u4e00\u79cd\u66f4\u7b80\u5355\u76f4\u63a5\u7684\u4ee3\u7406\u6784\u5efa\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u7eaf\u4ee3\u7801\u4ee3\u7406\u6a21\u5f0f\uff0c\u4ee3\u7406\u65e0\u6cd5\u76f4\u63a5\u505a\u4efb\u4f55\u6709\u751f\u4ea7\u529b\u7684\u4e8b\u60c5\uff0c\u5fc5\u987b\u4e3a\u6bcf\u4e2a\u63d0\u793a\u7f16\u5199\u548c\u8fd0\u884c\u4ee3\u7801\u3002\u8fd9\u79cd\u65b9\u6cd5\u5f3a\u5236\u4ee3\u7406\u751f\u6210\u4ee3\u7801\u89c1\u8bc1\uff0c\u5c06\u5de5\u4f5c\u4ee5\u975e\u5e38\u5b57\u9762\u7684\u65b9\u5f0f\u7f16\u7801\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u7b54\u6848\u7684\u4ee3\u7801\u89c1\u8bc1\uff0c\u5c06\u5de5\u4f5c\u4ee5\u4ee3\u7801\u5f62\u5f0f\u5177\u4f53\u5316\u3002\u5f3a\u5236\u4ee3\u7406\u751f\u6210\u8bc1\u660e\u3001\u8fd0\u884c\u8bc1\u660e\u5e76\u89e3\u91ca\u7ed3\u679c\u3002", "conclusion": "\u7eaf\u4ee3\u7801\u4ee3\u7406\u6a21\u5f0f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u4ee3\u7406\u5f00\u53d1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u7684\u673a\u5236\uff0c\u786e\u4fdd\u4ee3\u7406\u8f93\u51fa\u7684\u53ef\u9a8c\u8bc1\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2601.40102ac4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=infosec-primary-jan19-cta1/2/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/ligKyRL5dcfvS-vkU34wI0Ctbc0mW_eu52zPMUVCFCA=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=infosec-primary-jan19-cta1/2/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/ligKyRL5dcfvS-vkU34wI0Ctbc0mW_eu52zPMUVCFCA=440", "authors": ["TLDR Newsletter"], "title": "AMA: Inside the OWASP Top 10 for Agentic Applications", "comment": "Source: TLDR Newsletter, Date: 2026-01-19, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=infosec-primary-jan19-cta1/2/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/ligKyRL5dcfvS-vkU34wI0Ctbc0mW_eu52zPMUVCFCA=440", "summary": "AMA: Inside the OWASP Top 10 for Agentic Applications (Sponsor) The industry now has a peer-reviewed risk framework for autonomous, tool-using AI agents. In this live AMA-style webinar on Wed. 1/28, engage with Chris Hughes, Steve Wilson, Michael Bargury, and Kayla Underkoffler - security experts who helped shape the new OWASP standard. Learn more about: What each risk category means in practice Concrete steps to map the Top 10 into your threat models, control frameworks, and SOC workflows Ho...", "source": "tldr", "AI": {"tldr": "OWASP\u53d1\u5e03\u4e86\u9488\u5bf9\u81ea\u4e3bAI\u4ee3\u7406\u5e94\u7528\u7684\u5b89\u5168\u98ce\u9669\u6846\u67b6Top 10\uff0c\u5e76\u4e3e\u529eAMA\u7f51\u7edc\u7814\u8ba8\u4f1a\u4ecb\u7ecd\u5982\u4f55\u5c06\u5176\u6574\u5408\u5230\u5a01\u80c1\u6a21\u578b\u548c\u5b89\u5168\u6d41\u7a0b\u4e2d", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u5e94\u7528\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u884c\u4e1a\u9700\u8981\u6807\u51c6\u5316\u7684\u5b89\u5168\u98ce\u9669\u6846\u67b6\u6765\u8bc4\u4f30\u548c\u7ba1\u7406\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\uff0cOWASP\u4e3a\u6b64\u5236\u5b9a\u4e86\u4e13\u95e8\u9488\u5bf9\u4ee3\u7406\u5e94\u7528\u7684\u5b89\u5168\u6807\u51c6", "method": "\u901a\u8fc7\u4e13\u5bb6\u56e2\u961f\u5236\u5b9aOWASP Top 10\u98ce\u9669\u6846\u67b6\uff0c\u5e76\u901a\u8fc7AMA\u7f51\u7edc\u7814\u8ba8\u4f1a\u5f62\u5f0f\u5411\u884c\u4e1a\u4ecb\u7ecd\u6846\u67b6\u5185\u5bb9\u3001\u98ce\u9669\u7c7b\u522b\u542b\u4e49\u4ee5\u53ca\u5982\u4f55\u5c06\u5176\u6574\u5408\u5230\u5b9e\u9645\u5b89\u5168\u5de5\u4f5c\u4e2d", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u9488\u5bf9\u81ea\u4e3bAI\u4ee3\u7406\u5e94\u7528\u7684\u540c\u884c\u8bc4\u5ba1\u5b89\u5168\u98ce\u9669\u6846\u67b6\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u98ce\u9669\u8bc4\u4f30\u548c\u7ba1\u7406\u5de5\u5177", "conclusion": "OWASP Top 10 for Agentic Applications\u4e3aAI\u4ee3\u7406\u5e94\u7528\u5b89\u5168\u63d0\u4f9b\u4e86\u91cd\u8981\u6807\u51c6\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u7cfb\u7edf\u5316\u5730\u8bc6\u522b\u548c\u7ba1\u7406\u76f8\u5173\u5b89\u5168\u98ce\u9669", "topic": "agent analysis"}}
{"id": "tldr.2601.a1d30144", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=infosec-primary-jan19-cta1/2/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/ligKyRL5dcfvS-vkU34wI0Ctbc0mW_eu52zPMUVCFCA=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=infosec-primary-jan19-cta1/2/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/ligKyRL5dcfvS-vkU34wI0Ctbc0mW_eu52zPMUVCFCA=440", "authors": ["TLDR Newsletter"], "title": "Chris Hughes, Steve Wilson, Michael Bargury, and Kayla Underkoffler", "comment": "Source: TLDR Newsletter, Date: 2026-01-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=infosec-primary-jan19-cta1/2/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/ligKyRL5dcfvS-vkU34wI0Ctbc0mW_eu52zPMUVCFCA=440", "summary": "AMA: Inside the OWASP Top 10 for Agentic Applications (Sponsor) The industry now has a peer-reviewed risk framework for autonomous, tool-using AI agents. In this live AMA-style webinar on Wed. 1/28, engage with Chris Hughes, Steve Wilson, Michael Bargury, and Kayla Underkoffler - security experts who helped shape the new OWASP standard. Learn more about: What each risk category means in practice Concrete steps to map the Top 10 into your threat models, control frameworks, and SOC workflows Ho...", "source": "tldr", "AI": {"tldr": "OWASP\u53d1\u5e03\u4e86\u9488\u5bf9\u81ea\u4e3bAI\u4ee3\u7406\u7684\u98ce\u9669\u6846\u67b6Top 10\uff0c\u5e76\u4e3e\u529eAMA\u7f51\u7edc\u7814\u8ba8\u4f1a\uff0c\u8ba9\u5b89\u5168\u4e13\u5bb6\u8bb2\u89e3\u5982\u4f55\u5c06\u98ce\u9669\u7c7b\u522b\u6620\u5c04\u5230\u5b9e\u9645\u5a01\u80c1\u6a21\u578b\u548c\u63a7\u5236\u6846\u67b6\u4e2d\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u884c\u4e1a\u9700\u8981\u4e00\u4e2a\u7ecf\u8fc7\u540c\u884c\u8bc4\u5ba1\u7684\u98ce\u9669\u6846\u67b6\u6765\u8bc6\u522b\u548c\u7ba1\u7406\u76f8\u5173\u5b89\u5168\u98ce\u9669\uff0cOWASP\u4e3a\u6b64\u5236\u5b9a\u4e86\u4e13\u95e8\u9488\u5bf9\u5de5\u5177\u4f7f\u7528AI\u4ee3\u7406\u7684Top 10\u98ce\u9669\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u4e3e\u529eAMA\u5f0f\u7f51\u7edc\u7814\u8ba8\u4f1a\uff0c\u9080\u8bf7\u53c2\u4e0e\u5236\u5b9aOWASP\u6807\u51c6\u7684\u5b89\u5168\u4e13\u5bb6\uff08Chris Hughes\u3001Steve Wilson\u3001Michael Bargury\u3001Kayla Underkoffler\uff09\u8bb2\u89e3\u98ce\u9669\u6846\u67b6\uff0c\u5305\u62ec\u98ce\u9669\u7c7b\u522b\u7684\u5b9e\u9645\u542b\u4e49\u4ee5\u53ca\u5982\u4f55\u5c06\u5176\u6620\u5c04\u5230\u5a01\u80c1\u6a21\u578b\u3001\u63a7\u5236\u6846\u67b6\u548c\u5b89\u5168\u8fd0\u8425\u4e2d\u5fc3\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "result": "\u884c\u4e1a\u73b0\u5728\u6709\u4e86\u9488\u5bf9\u81ea\u4e3b\u3001\u5de5\u5177\u4f7f\u7528AI\u4ee3\u7406\u7684\u540c\u884c\u8bc4\u5ba1\u98ce\u9669\u6846\u67b6\uff0c\u5b89\u5168\u4e13\u5bb6\u901a\u8fc7AMA\u7814\u8ba8\u4f1a\u5411\u4ece\u4e1a\u8005\u4f20\u6388\u5982\u4f55\u5b9e\u9645\u5e94\u7528\u8fd9\u4e00\u6846\u67b6\u6765\u589e\u5f3aAI\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002", "conclusion": "OWASP Top 10 for Agentic Applications\u4e3aAI\u4ee3\u7406\u5b89\u5168\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u98ce\u9669\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6307\u5bfc\u5e2e\u52a9\u7ec4\u7ec7\u5c06\u5b89\u5168\u63a7\u5236\u63aa\u65bd\u96c6\u6210\u5230AI\u4ee3\u7406\u5f00\u53d1\u548c\u90e8\u7f72\u6d41\u7a0b\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.553895d1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.promptarmor.com%2Fresources%2Fclaude-cowork-exfiltrates-files%3Futm_source=tldrinfosec/1/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/GuyGYGUxPNZNHTB8OGLR1J2a5PIWCeIW7tICh-SCar8=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.promptarmor.com%2Fresources%2Fclaude-cowork-exfiltrates-files%3Futm_source=tldrinfosec/1/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/GuyGYGUxPNZNHTB8OGLR1J2a5PIWCeIW7tICh-SCar8=440", "authors": ["TLDR Newsletter"], "title": "Claude Cowork Exfiltrates Files", "comment": "Source: TLDR Newsletter, Date: 2026-01-19, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.promptarmor.com%2Fresources%2Fclaude-cowork-exfiltrates-files%3Futm_source=tldrinfosec/1/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/GuyGYGUxPNZNHTB8OGLR1J2a5PIWCeIW7tICh-SCar8=440", "summary": "Claude Cowork Exfiltrates Files (3 minute read) Claude Cowork is a new general-purpose agent now available in research preview. It is vulnerable to indirect prompt injection via uploaded files and can be abused to exfiltrate sensitive documents. This attack exploits a previously reported, unremediated vulnerability in the Claude sandbox that allows network connections to the Claude API.", "source": "tldr", "AI": {"tldr": "Claude Cowork\u4ee3\u7406\u5b58\u5728\u6587\u4ef6\u4e0a\u4f20\u6f0f\u6d1e\uff0c\u53ef\u901a\u8fc7\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u6cc4\u9732\u654f\u611f\u6587\u6863", "motivation": "\u63ed\u793aClaude Cowork\u8fd9\u4e00\u65b0\u578b\u901a\u7528\u4ee3\u7406\u5728\u6587\u4ef6\u4e0a\u4f20\u529f\u80fd\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u8be5\u6f0f\u6d1e\u5141\u8bb8\u653b\u51fb\u8005\u901a\u8fc7\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u7a83\u53d6\u654f\u611f\u6587\u4ef6", "method": "\u5229\u7528Claude\u6c99\u7bb1\u4e2d\u5148\u524d\u62a5\u544a\u4f46\u672a\u4fee\u590d\u7684\u6f0f\u6d1e\uff0c\u8be5\u6f0f\u6d1e\u5141\u8bb8\u7f51\u7edc\u8fde\u63a5\u5230Claude API\uff0c\u901a\u8fc7\u4e0a\u4f20\u6587\u4ef6\u8fdb\u884c\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb", "result": "\u6210\u529f\u6f14\u793a\u4e86Claude Cowork\u53ef\u4ee5\u88ab\u6ee5\u7528\u6765\u6cc4\u9732\u654f\u611f\u6587\u6863\uff0c\u8bc1\u660e\u8be5\u4ee3\u7406\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669", "conclusion": "Claude Cowork\u4f5c\u4e3a\u7814\u7a76\u9884\u89c8\u7248\u53d1\u5e03\uff0c\u5b58\u5728\u5df2\u77e5\u4f46\u672a\u4fee\u590d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u7acb\u5373\u4fee\u590d\u4ee5\u9632\u6b62\u654f\u611f\u6570\u636e\u6cc4\u9732", "topic": "agent analysis"}}
{"id": "tldr.2601.87bb87af", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.silennai.com%2Fclaude-code%3Futm_source=tldrnewsletter/1/0100019bdb259f26-0b912e7d-3876-48c9-be8b-fe370c495c64-000000/E1AaxAziEC3YHWkrrnyyfmQgbBPTHJlrP4SXQneYJVI=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.silennai.com%2Fclaude-code%3Futm_source=tldrnewsletter/1/0100019bdb259f26-0b912e7d-3876-48c9-be8b-fe370c495c64-000000/E1AaxAziEC3YHWkrrnyyfmQgbBPTHJlrP4SXQneYJVI=440", "authors": ["TLDR Newsletter"], "title": "I was a top 0.01% Cursor user. Here's why I switched to Claude Code 2.0", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 32 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.silennai.com%2Fclaude-code%3Futm_source=tldrnewsletter/1/0100019bdb259f26-0b912e7d-3876-48c9-be8b-fe370c495c64-000000/E1AaxAziEC3YHWkrrnyyfmQgbBPTHJlrP4SXQneYJVI=440", "summary": "I was a top 0.01% Cursor user. Here's why I switched to Claude Code 2.0 (32 minute read) Claude Code 2.0 has a flexible and robust harness with an evolved UX and fewer bugs. The RLHF Anthropic did on Opus 4.5 completely changed the equation. Users no longer need to review code or instruct the model at the level of files or functions: they can just test behaviors instead. Claude Code can be used by developers who never plan on learning how to code and just care about outputs.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u4eceCursor\u8f6c\u5411Claude Code 2.0\uff0c\u56e0\u4e3a\u540e\u8005\u5177\u6709\u66f4\u7075\u6d3b\u5f3a\u5927\u7684\u6846\u67b6\u3001\u6539\u8fdb\u7684\u7528\u6237\u4f53\u9a8c\u548c\u66f4\u5c11bug\uff0c\u7279\u522b\u662fOpus 4.5\u7684RLHF\u8bad\u7ec3\u8ba9\u7528\u6237\u53ea\u9700\u6d4b\u8bd5\u884c\u4e3a\u800c\u65e0\u9700\u9010\u884c\u5ba1\u67e5\u4ee3\u7801\u3002", "motivation": "\u4f5c\u8005\u4f5c\u4e3a\u9876\u7ea7Cursor\u7528\u6237\uff0c\u53d1\u73b0Claude Code 2.0\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5176\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8ba9\u975e\u4e13\u4e1a\u5f00\u53d1\u8005\u4e5f\u80fd\u8f7b\u677e\u4f7f\u7528\uff0c\u53ea\u9700\u5173\u6ce8\u8f93\u51fa\u7ed3\u679c\u800c\u65e0\u9700\u6df1\u5165\u4ee3\u7801\u7ec6\u8282\u3002", "method": "\u901a\u8fc7\u5b9e\u9645\u4f7f\u7528\u4f53\u9a8c\u5bf9\u6bd4\uff0c\u5206\u6790Claude Code 2.0\u7684\u6280\u672f\u7279\u70b9\uff0c\u5305\u62ec\u5176\u7075\u6d3b\u5f3a\u5927\u7684\u6846\u67b6\u3001\u6539\u8fdb\u7684\u7528\u6237\u754c\u9762\u3001\u51cf\u5c11\u7684bug\u6570\u91cf\uff0c\u4ee5\u53caOpus 4.5\u6a21\u578b\u901a\u8fc7RLHF\u8bad\u7ec3\u5e26\u6765\u7684\u884c\u4e3a\u6d4b\u8bd5\u80fd\u529b\u3002", "result": "Claude Code 2.0\u76f8\u6bd4Cursor\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d8a\u7684\u5f00\u53d1\u4f53\u9a8c\uff0c\u7528\u6237\u4e0d\u518d\u9700\u8981\u9010\u6587\u4ef6\u6216\u9010\u51fd\u6570\u5ba1\u67e5\u4ee3\u7801\uff0c\u53ea\u9700\u6d4b\u8bd5\u884c\u4e3a\u5373\u53ef\uff0c\u751a\u81f3\u975e\u7a0b\u5e8f\u5458\u4e5f\u80fd\u6709\u6548\u4f7f\u7528\uff0c\u8fd9\u5b8c\u5168\u6539\u53d8\u4e86\u4ee3\u7801\u8f85\u52a9\u5de5\u5177\u7684\u8303\u5f0f\u3002", "conclusion": "Claude Code 2.0\u4ee3\u8868\u4e86\u4ee3\u7801\u751f\u6210\u5de5\u5177\u7684\u91cd\u8981\u8fdb\u6b65\uff0c\u5176\u57fa\u4e8e\u884c\u4e3a\u6d4b\u8bd5\u7684\u65b9\u6cd5\u964d\u4f4e\u4e86\u4f7f\u7528\u95e8\u69db\uff0c\u4f7f\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u7fa4\u4f53\u80fd\u591f\u53d7\u76ca\u4e8eAI\u4ee3\u7801\u8f85\u52a9\uff0c\u6807\u5fd7\u7740\u4ece\u4ee3\u7801\u5ba1\u67e5\u5230\u884c\u4e3a\u9a8c\u8bc1\u7684\u8f6c\u53d8\u3002", "topic": "swe application"}}
{"id": "tldr.2601.ef9deaaa", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2026%2F1%2F18%2Fagent-psychosis%2F%3Futm_source=tldrnewsletter/1/0100019bdb259f26-0b912e7d-3876-48c9-be8b-fe370c495c64-000000/Gx33uStNMK8WEFFts2xOPazDp3_7JPnJzV3S--okkUI=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2026%2F1%2F18%2Fagent-psychosis%2F%3Futm_source=tldrnewsletter/1/0100019bdb259f26-0b912e7d-3876-48c9-be8b-fe370c495c64-000000/Gx33uStNMK8WEFFts2xOPazDp3_7JPnJzV3S--okkUI=440", "authors": ["TLDR Newsletter"], "title": "Agent Psychosis: Are We Going Insane?", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2026%2F1%2F18%2Fagent-psychosis%2F%3Futm_source=tldrnewsletter/1/0100019bdb259f26-0b912e7d-3876-48c9-be8b-fe370c495c64-000000/Gx33uStNMK8WEFFts2xOPazDp3_7JPnJzV3S--okkUI=440", "summary": "Agent Psychosis: Are We Going Insane? (12 minute read) AI agents are massive slop machines if you turn off your brain and let go immediately.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u5982\u679c\u76f2\u76ee\u4f7f\u7528\u4f1a\u6210\u4e3a\u4e0d\u53ef\u9760\u7684\"\u5783\u573e\u751f\u6210\u5668\"\uff0c\u9700\u8981\u6279\u5224\u6027\u601d\u7ef4\u6765\u907f\u514d\"\u4ee3\u7406\u7cbe\u795e\u75c5\"", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7cfb\u7edf\u5b58\u5728\u76f2\u76ee\u4fe1\u4efb\u548c\u8fc7\u5ea6\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u7528\u6237\u5982\u679c\u4e0d\u52a0\u6279\u5224\u5730\u63a5\u53d7\u4ee3\u7406\u8f93\u51fa\uff0c\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u7ed3\u679c", "method": "\u901a\u8fc7\u6279\u5224\u6027\u5206\u6790\u548c\u7528\u6237\u53c2\u4e0e\u6765\u8bc4\u4f30AI\u4ee3\u7406\u7684\u53ef\u9760\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u4fdd\u6301\u4eba\u7c7b\u5224\u65ad\u529b", "result": "\u6307\u51fa\u76f2\u76ee\u4f7f\u7528AI\u4ee3\u7406\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9760\u8f93\u51fa\uff0c\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u548c\u6279\u5224\u6027\u601d\u7ef4\u6765\u786e\u4fdd\u8d28\u91cf", "conclusion": "AI\u4ee3\u7406\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u548c\u6279\u5224\u6027\u601d\u7ef4\uff0c\u4e0d\u80fd\u5b8c\u5168\u4f9d\u8d56\u81ea\u52a8\u5316\uff0c\u5426\u5219\u4f1a\u4ea7\u751f\"\u4ee3\u7406\u7cbe\u795e\u75c5\"\u73b0\u8c61", "topic": "agent analysis"}}
{"id": "tldr.2601.8ff00b6e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finalroundai.com%2Fblog%2Fai-can-not-replace-junior-programmers%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/zYlL-eTlELaDwYg3U-4Xgz8blpsEGlF9ttGDJiAsR4g=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finalroundai.com%2Fblog%2Fai-can-not-replace-junior-programmers%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/zYlL-eTlELaDwYg3U-4Xgz8blpsEGlF9ttGDJiAsR4g=441", "authors": ["TLDR Newsletter"], "title": "AI Not Ready to Replace Junior Devs Says Ruby on Rails Creator", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finalroundai.com%2Fblog%2Fai-can-not-replace-junior-programmers%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/zYlL-eTlELaDwYg3U-4Xgz8blpsEGlF9ttGDJiAsR4g=441", "summary": "AI Not Ready to Replace Junior Devs Says Ruby on Rails Creator (8 minute read) The creator of Ruby on Rails says that AI coding tools are not yet ready to replace junior developers due to their inconsistency and lack of quality. AI is a \"flickering light bulb,\" occasionally brilliant but often producing code that is poorly structured, hard to maintain, and lacks true system understanding.", "source": "tldr", "AI": {"tldr": "Ruby on Rails\u521b\u59cb\u4eba\u8ba4\u4e3aAI\u7f16\u7801\u5de5\u5177\u5c1a\u65e0\u6cd5\u66ff\u4ee3\u521d\u7ea7\u5f00\u53d1\u8005\uff0c\u56e0\u4e3aAI\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u4e0d\u7a33\u5b9a\u3001\u7ed3\u6784\u5dee\u3001\u7f3a\u4e4f\u7cfb\u7edf\u7406\u89e3\uff0c\u53ea\u662f\"\u95ea\u70c1\u7684\u706f\u6ce1\"\u5076\u5c14\u51fa\u8272\u4f46\u7ecf\u5e38\u4e0d\u53ef\u9760", "motivation": "\u9488\u5bf9\u5f53\u524dAI\u7f16\u7801\u5de5\u5177\u88ab\u8fc7\u5ea6\u7092\u4f5c\u53ef\u80fd\u66ff\u4ee3\u521d\u7ea7\u5f00\u53d1\u8005\u7684\u73b0\u8c61\uff0cRuby on Rails\u521b\u59cb\u4eba\u57fa\u4e8e\u5b9e\u9645\u89c2\u5bdf\u63d0\u51fa\u6279\u5224\u6027\u89c2\u70b9\uff0c\u5f3a\u8c03AI\u5de5\u5177\u7684\u5c40\u9650\u6027", "method": "\u57fa\u4e8e\u4f5c\u8005\u4f5c\u4e3a\u8d44\u6df1\u5f00\u53d1\u8005\u548cRuby on Rails\u521b\u59cb\u4eba\u7684\u5b9e\u8df5\u7ecf\u9a8c\u89c2\u5bdf\uff0c\u5bf9AI\u7f16\u7801\u5de5\u5177\u7684\u5b9e\u9645\u8868\u73b0\u8fdb\u884c\u5b9a\u6027\u5206\u6790", "result": "AI\u7f16\u7801\u5de5\u5177\u5b58\u5728\u4e25\u91cd\u4e0d\u4e00\u81f4\u6027\uff0c\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u5dee\u3001\u7ed3\u6784\u6df7\u4e71\u3001\u96be\u4ee5\u7ef4\u62a4\uff0c\u7f3a\u4e4f\u5bf9\u7cfb\u7edf\u7684\u771f\u6b63\u7406\u89e3\uff0c\u65e0\u6cd5\u53ef\u9760\u66ff\u4ee3\u521d\u7ea7\u5f00\u53d1\u8005", "conclusion": "AI\u7f16\u7801\u5de5\u5177\u76ee\u524d\u8fd8\u4e0d\u591f\u6210\u719f\uff0c\u4e0d\u80fd\u66ff\u4ee3\u521d\u7ea7\u5f00\u53d1\u8005\uff0c\u9700\u8981\u66f4\u591a\u65f6\u95f4\u53d1\u5c55\u624d\u80fd\u771f\u6b63\u6210\u4e3a\u53ef\u9760\u7684\u7f16\u7a0b\u52a9\u624b", "topic": "agent analysis"}}
{"id": "tldr.2601.298bf708", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.humanlayer.dev%2Fblog%2Fbrief-history-of-ralph%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/REiuVqRZaM5YKmuqjMFo9dGFexT9cfxAUZs_bZ9qum4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.humanlayer.dev%2Fblog%2Fbrief-history-of-ralph%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/REiuVqRZaM5YKmuqjMFo9dGFexT9cfxAUZs_bZ9qum4=441", "authors": ["TLDR Newsletter"], "title": "A brief history of ralph", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.humanlayer.dev%2Fblog%2Fbrief-history-of-ralph%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/REiuVqRZaM5YKmuqjMFo9dGFexT9cfxAUZs_bZ9qum4=441", "summary": "A brief history of ralph (10 minute read) The \"Ralph Wiggum Technique\" is an agentic coding method that went viral in late 2025. Initially a simple bash loop for continuous AI execution, ralph's evolution had many applications in refactoring, spec generation, and project setup. It was very useful for specific tasks like large-scale code refactoring.", "source": "tldr", "AI": {"tldr": "Ralph Wiggum Technique\u662f\u4e00\u79cd\u57282025\u5e74\u5e95\u6d41\u884c\u7684\u667a\u80fd\u7f16\u7801\u65b9\u6cd5\uff0c\u6700\u521d\u662f\u7b80\u5355\u7684bash\u5faa\u73af\u7528\u4e8e\u6301\u7eedAI\u6267\u884c\uff0c\u540e\u6765\u6f14\u53d8\u4e3a\u5728\u91cd\u6784\u3001\u89c4\u8303\u751f\u6210\u548c\u9879\u76ee\u8bbe\u7f6e\u7b49\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u975e\u5e38\u6709\u7528\u3002", "motivation": "\u8be5\u6280\u672f\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316AI\u6267\u884c\u6d41\u7a0b\u6765\u63d0\u9ad8\u7f16\u7801\u6548\u7387\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5927\u89c4\u6a21\u4ee3\u7801\u91cd\u6784\u7b49\u7279\u5b9a\u4efb\u52a1\uff0c\u89e3\u51b3\u4f20\u7edf\u624b\u52a8\u7f16\u7801\u7684\u8017\u65f6\u95ee\u9898\u3002", "method": "\u6700\u521d\u91c7\u7528\u7b80\u5355\u7684bash\u5faa\u73af\u5b9e\u73b0\u6301\u7eedAI\u6267\u884c\uff0c\u540e\u6765\u6f14\u53d8\u4e3a\u66f4\u590d\u6742\u7684\u4ee3\u7406\u5f0f\u7f16\u7801\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u4ee3\u7801\u91cd\u6784\u3001\u89c4\u8303\u751f\u6210\u548c\u9879\u76ee\u8bbe\u7f6e\u7b49\u591a\u4e2a\u9886\u57df\u3002", "result": "\u8be5\u6280\u672f\u57282025\u5e74\u5e95\u8fc5\u901f\u6d41\u884c\uff0c\u88ab\u8bc1\u660e\u5728\u5927\u578b\u4ee3\u7801\u91cd\u6784\u7b49\u7279\u5b9a\u4efb\u52a1\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u6210\u4e3a\u5f53\u65f6\u91cd\u8981\u7684\u7f16\u7801\u5de5\u5177\u548c\u65b9\u6cd5\u3002", "conclusion": "Ralph Wiggum Technique\u5c55\u793a\u4e86\u4ee3\u7406\u5f0f\u7f16\u7801\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316AI\u6267\u884c\u6d41\u7a0b\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u7801\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u91cd\u6784\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "code agent"}}
{"id": "tldr.2601.03b1f488", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Flooper-article%2F%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/Dz8RdQnEgnL_KWjaxfKMIQiQB73_Fcr5fAg8hIYyFEU=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Flooper-article%2F%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/Dz8RdQnEgnL_KWjaxfKMIQiQB73_Fcr5fAg8hIYyFEU=441", "authors": ["TLDR Newsletter"], "title": "Looper: The AI Junior That Never Forgets the Backlog", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Flooper-article%2F%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/Dz8RdQnEgnL_KWjaxfKMIQiQB73_Fcr5fAg8hIYyFEU=441", "summary": "Looper: The AI Junior That Never Forgets the Backlog (8 minute read) Looper is an AI coding tool that makes sure of deterministic and auditable development by enforcing structured, single-task AI agent iterations managed via a JSON backlog and a forced review pass.", "source": "tldr", "AI": {"tldr": "Looper\u662f\u4e00\u4e2aAI\u7f16\u7801\u5de5\u5177\uff0c\u901a\u8fc7JSON\u5f85\u529e\u4e8b\u9879\u548c\u5f3a\u5236\u5ba1\u67e5\u6d41\u7a0b\u6765\u786e\u4fdd\u786e\u5b9a\u6027\u548c\u53ef\u5ba1\u8ba1\u7684\u5f00\u53d1", "motivation": "\u5f53\u524dAI\u7f16\u7801\u5de5\u5177\u7f3a\u4e4f\u7ed3\u6784\u5316\u548c\u53ef\u5ba1\u8ba1\u7684\u5f00\u53d1\u6d41\u7a0b\uff0c\u5bfc\u81f4\u5f00\u53d1\u8fc7\u7a0b\u4e0d\u53ef\u9884\u6d4b\u4e14\u96be\u4ee5\u8ffd\u8e2a", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u3001\u5355\u4efb\u52a1AI\u4ee3\u7406\u8fed\u4ee3\uff0c\u901a\u8fc7JSON\u5f85\u529e\u4e8b\u9879\u7ba1\u7406\u548c\u5f3a\u5236\u5ba1\u67e5\u6d41\u7a0b\u6765\u786e\u4fdd\u5f00\u53d1\u8d28\u91cf", "result": "\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u548c\u53ef\u5ba1\u8ba1\u7684\u5f00\u53d1\u8fc7\u7a0b\uff0c\u786e\u4fddAI\u7f16\u7801\u5de5\u5177\u80fd\u591f\u7cfb\u7edf\u5316\u5730\u5904\u7406\u5f00\u53d1\u4efb\u52a1", "conclusion": "Looper\u901a\u8fc7\u7ed3\u6784\u5316\u65b9\u6cd5\u548c\u5f3a\u5236\u5ba1\u67e5\u673a\u5236\uff0c\u4e3aAI\u7f16\u7801\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u9760\u548c\u53ef\u8ffd\u8e2a\u7684\u5f00\u53d1\u6846\u67b6", "topic": "code agent"}}
{"id": "tldr.2601.95aedd6d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.johndcook.com%2Fblog%2F2026%2F01%2F19%2Fugly-code%2F%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/agdWIsOj64E2EcaeO8-0Q35ZBv5GoYfOxeKNFTgPfz4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.johndcook.com%2Fblog%2F2026%2F01%2F19%2Fugly-code%2F%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/agdWIsOj64E2EcaeO8-0Q35ZBv5GoYfOxeKNFTgPfz4=441", "authors": ["TLDR Newsletter"], "title": "Two cheers for ugly code", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.johndcook.com%2Fblog%2F2026%2F01%2F19%2Fugly-code%2F%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/agdWIsOj64E2EcaeO8-0Q35ZBv5GoYfOxeKNFTgPfz4=441", "summary": "Two cheers for ugly code (2 minute read) Ugly code can be valuable due to the implicit knowledge it contains or its proven utility over time, making it worth the effort to work with and learn from rather than discard.", "source": "tldr", "AI": {"tldr": "\u4e11\u964b\u4ee3\u7801\u56e0\u5176\u8574\u542b\u7684\u9690\u6027\u77e5\u8bc6\u548c\u7ecf\u8fc7\u65f6\u95f4\u9a8c\u8bc1\u7684\u5b9e\u7528\u6027\u800c\u5177\u6709\u4ef7\u503c\uff0c\u503c\u5f97\u6295\u5165\u7cbe\u529b\u53bb\u7406\u89e3\u548c\u7ef4\u62a4\u800c\u975e\u76f4\u63a5\u4e22\u5f03", "motivation": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u4eba\u4eec\u901a\u5e38\u8ffd\u6c42\u4f18\u96c5\u3001\u6574\u6d01\u7684\u4ee3\u7801\uff0c\u4f46\u5f80\u5f80\u4f1a\u5ffd\u89c6\u90a3\u4e9b\u770b\u4f3c\"\u4e11\u964b\"\u7684\u4ee3\u7801\u53ef\u80fd\u5305\u542b\u7684\u5b9d\u8d35\u4ef7\u503c\u3002\u8fd9\u4e9b\u4ee3\u7801\u53ef\u80fd\u627f\u8f7d\u7740\u91cd\u8981\u7684\u4e1a\u52a1\u903b\u8f91\u3001\u5386\u53f2\u7ecf\u9a8c\u6216\u7279\u5b9a\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e11\u964b\u4ee3\u7801\u7684\u4ef7\u503c\u6765\u6e90\uff0c\u5305\u62ec\u5176\u8574\u542b\u7684\u9690\u6027\u77e5\u8bc6\uff08\u5982\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3001\u5386\u53f2\u51b3\u7b56\u80cc\u666f\uff09\u548c\u7ecf\u8fc7\u65f6\u95f4\u9a8c\u8bc1\u7684\u5b9e\u7528\u6027\uff08\u5982\u7a33\u5b9a\u6027\u3001\u6027\u80fd\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u91cd\u65b0\u8bc4\u4f30\u4ee3\u7801\u8d28\u91cf\u7684\u6807\u51c6\u3002", "result": "\u4e11\u964b\u4ee3\u7801\u5f80\u5f80\u5305\u542b\u96be\u4ee5\u901a\u8fc7\u6587\u6863\u4f20\u9012\u7684\u9690\u6027\u77e5\u8bc6\uff0c\u5e76\u4e14\u7ecf\u8fc7\u957f\u671f\u751f\u4ea7\u73af\u5883\u9a8c\u8bc1\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002\u8fd9\u4e9b\u4ef7\u503c\u8d85\u8fc7\u4e86\u8868\u9762\u4e0a\u7684\u4ee3\u7801\u7f8e\u89c2\u6027\u3002", "conclusion": "\u5e94\u8be5\u91cd\u65b0\u601d\u8003\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\uff0c\u8ba4\u8bc6\u5230\u4e11\u964b\u4ee3\u7801\u53ef\u80fd\u5177\u6709\u7684\u72ec\u7279\u4ef7\u503c\uff0c\u5728\u91cd\u6784\u6216\u91cd\u5199\u51b3\u7b56\u4e2d\u66f4\u52a0\u8c28\u614e\uff0c\u4f18\u5148\u8003\u8651\u4ee3\u7801\u7684\u5b9e\u9645\u6548\u7528\u548c\u77e5\u8bc6\u4f20\u627f\u3002", "topic": "swe application"}}
{"id": "tldr.2601.a02c6f12", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjohncodes.com%2Farchive%2F2026%2F01-18-all-your-opencodes%2F%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/u44NY_yb_jSU-4mZcIGFKFynNefigd3bAr7c3o-QL6E=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjohncodes.com%2Farchive%2F2026%2F01-18-all-your-opencodes%2F%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/u44NY_yb_jSU-4mZcIGFKFynNefigd3bAr7c3o-QL6E=441", "authors": ["TLDR Newsletter"], "title": "all your OpenCodes belong to us", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjohncodes.com%2Farchive%2F2026%2F01-18-all-your-opencodes%2F%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/u44NY_yb_jSU-4mZcIGFKFynNefigd3bAr7c3o-QL6E=441", "summary": "all your OpenCodes belong to us (7 minute read) OpenCode versions before v1.1.10 contained a critical RCE vulnerability via an unauthenticated HTTP server that exposed endpoints for arbitrary shell command execution, interactive terminal session creation, and arbitrary file reads. The vulnerability also enabled prompt injection into the LLM's context window, creating a secondary attack vector to manipulate the AI agent into exfiltrating data or performing malicious actions. Thousands of devel...", "source": "tldr", "AI": {"tldr": "OpenCode v1.1.10\u4e4b\u524d\u7248\u672c\u5b58\u5728\u4e25\u91cdRCE\u6f0f\u6d1e\uff0c\u901a\u8fc7\u672a\u8ba4\u8bc1HTTP\u670d\u52a1\u5668\u66b4\u9732\u7aef\u70b9\uff0c\u5141\u8bb8\u4efb\u610fshell\u547d\u4ee4\u6267\u884c\u3001\u521b\u5efa\u4ea4\u4e92\u5f0f\u7ec8\u7aef\u4f1a\u8bdd\u548c\u4efb\u610f\u6587\u4ef6\u8bfb\u53d6\uff0c\u8fd8\u80fd\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u64cd\u7eb5AI\u4ee3\u7406\u7a83\u53d6\u6570\u636e\u6216\u6267\u884c\u6076\u610f\u64cd\u4f5c\u3002", "motivation": "\u63ed\u793aOpenCode\u65e9\u671f\u7248\u672c\u5b58\u5728\u7684\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u8fd9\u4e9b\u6f0f\u6d1e\u53ef\u80fd\u88ab\u653b\u51fb\u8005\u5229\u7528\u6765\u8fdc\u7a0b\u63a7\u5236\u53d7\u5f71\u54cd\u7684\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7AI\u4ee3\u7406\u8fdb\u884c\u4e8c\u6b21\u653b\u51fb\u3002", "method": "\u901a\u8fc7\u5b89\u5168\u5206\u6790\u53d1\u73b0OpenCode v1.1.10\u4e4b\u524d\u7248\u672c\u4e2d\u7684\u672a\u8ba4\u8bc1HTTP\u670d\u52a1\u5668\u66b4\u9732\u4e86\u591a\u4e2a\u5371\u9669\u7aef\u70b9\uff0c\u5305\u62ec\u547d\u4ee4\u6267\u884c\u3001\u7ec8\u7aef\u4f1a\u8bdd\u521b\u5efa\u548c\u6587\u4ef6\u8bfb\u53d6\u529f\u80fd\uff0c\u540c\u65f6\u5b58\u5728\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\u53ef\u64cd\u7eb5LLM\u4e0a\u4e0b\u6587\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u4e25\u91cd\u7684RCE\u6f0f\u6d1e\uff0c\u5f71\u54cd\u6570\u5343\u4e2a\u5f00\u53d1\u73af\u5883\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u8fdc\u7a0b\u6267\u884c\u4efb\u610f\u547d\u4ee4\u3001\u8bbf\u95ee\u6587\u4ef6\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u64cd\u7eb5AI\u4ee3\u7406\u6267\u884c\u6076\u610f\u64cd\u4f5c\u3002", "conclusion": "OpenCode\u65e9\u671f\u7248\u672c\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u7f3a\u9677\uff0c\u9700\u8981\u7acb\u5373\u5347\u7ea7\u5230v1.1.10\u6216\u66f4\u9ad8\u7248\u672c\uff0c\u5e76\u52a0\u5f3aAI\u4ee3\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u9632\u62a4\uff0c\u9632\u6b62\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "topic": "code agent"}}
{"id": "tldr.2601.33535cad", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.csoonline.com%2Farticle%2F4118264%2Fservicenow-bodysnatcher-flaw-highlights-risks-of-rushed-ai-integrations.html%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/F6C5IBZHaqUedKXXWetzogt6av7IVdomfUUuJXkxO44=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.csoonline.com%2Farticle%2F4118264%2Fservicenow-bodysnatcher-flaw-highlights-risks-of-rushed-ai-integrations.html%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/F6C5IBZHaqUedKXXWetzogt6av7IVdomfUUuJXkxO44=441", "authors": ["TLDR Newsletter"], "title": "ServiceNow BodySnatcher flaw highlights risks of rushed AI integrations", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.csoonline.com%2Farticle%2F4118264%2Fservicenow-bodysnatcher-flaw-highlights-risks-of-rushed-ai-integrations.html%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/F6C5IBZHaqUedKXXWetzogt6av7IVdomfUUuJXkxO44=441", "summary": "ServiceNow BodySnatcher flaw highlights risks of rushed AI integrations (8 minute read) BodySnatcher is a critical vulnerability in ServiceNow's Now Assist AI Agents and Virtual Agent API that allows unauthenticated attackers to execute agentic workflows as any user by exploiting weak authentication defaults and auto-linking features\u2014potentially creating backdoor admin accounts. ServiceNow patched hosted instances in late October (Now Assist AI Agents 5.1.18/5.2.19+ and Virtual Agent API 3.15...", "source": "tldr", "AI": {"tldr": "ServiceNow BodySnatcher\u6f0f\u6d1e\u66b4\u9732AI\u96c6\u6210\u5b89\u5168\u98ce\u9669\uff0c\u5141\u8bb8\u672a\u8ba4\u8bc1\u653b\u51fb\u8005\u901a\u8fc7\u5229\u7528\u5f31\u8ba4\u8bc1\u9ed8\u8ba4\u8bbe\u7f6e\u548c\u81ea\u52a8\u94fe\u63a5\u529f\u80fd\uff0c\u4ee5\u4efb\u610f\u7528\u6237\u8eab\u4efd\u6267\u884c\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u53ef\u80fd\u521b\u5efa\u540e\u95e8\u7ba1\u7406\u5458\u8d26\u6237\u3002", "motivation": "\u63ed\u793aServiceNow AI\u4ee3\u7406\u96c6\u6210\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u5728\u5feb\u901fAI\u96c6\u6210\u8fc7\u7a0b\u4e2d\u5ffd\u89c6\u5b89\u5168\u6027\u7684\u98ce\u9669\uff0c\u7279\u522b\u662f\u9ed8\u8ba4\u8ba4\u8bc1\u8bbe\u7f6e\u548c\u81ea\u52a8\u94fe\u63a5\u529f\u80fd\u5e26\u6765\u7684\u5b89\u5168\u9690\u60a3\u3002", "method": "\u901a\u8fc7\u5206\u6790ServiceNow Now Assist AI Agents\u548cVirtual Agent API\u7684\u8ba4\u8bc1\u673a\u5236\uff0c\u53d1\u73b0\u653b\u51fb\u8005\u53ef\u5229\u7528\u5f31\u8ba4\u8bc1\u9ed8\u8ba4\u8bbe\u7f6e\u548c\u81ea\u52a8\u94fe\u63a5\u529f\u80fd\uff0c\u4ee5\u672a\u8ba4\u8bc1\u8eab\u4efd\u6267\u884c\u4efb\u610f\u7528\u6237\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u3002", "result": "\u53d1\u73b0BodySnatcher\u5173\u952e\u6f0f\u6d1e\uff0c\u5141\u8bb8\u653b\u51fb\u8005\u521b\u5efa\u540e\u95e8\u7ba1\u7406\u5458\u8d26\u6237\u3002ServiceNow\u5df2\u4e8e10\u6708\u5e95\u4fee\u8865\u6258\u7ba1\u5b9e\u4f8b\uff08Now Assist AI Agents 5.1.18/5.2.19+\u548cVirtual Agent API 3.15+\uff09\u3002", "conclusion": "AI\u4ee3\u7406\u96c6\u6210\u9700\u8981\u66f4\u4e25\u683c\u7684\u5b89\u5168\u5ba1\u67e5\uff0c\u7279\u522b\u662f\u9ed8\u8ba4\u8ba4\u8bc1\u8bbe\u7f6e\u548c\u81ea\u52a8\u5316\u529f\u80fd\u3002\u4f01\u4e1a\u5e94\u5ba1\u614e\u8bc4\u4f30AI\u96c6\u6210\u5b89\u5168\u98ce\u9669\uff0c\u53ca\u65f6\u5e94\u7528\u5b89\u5168\u8865\u4e01\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.52de0233", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.wiz.io%2Fblog%2Fagentic-browser-security-2025-year-end-review%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/DLvsRSyBX566fxVNcwg3QDZzYZPMj2fSKwGB9zuqGOg=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.wiz.io%2Fblog%2Fagentic-browser-security-2025-year-end-review%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/DLvsRSyBX566fxVNcwg3QDZzYZPMj2fSKwGB9zuqGOg=441", "authors": ["TLDR Newsletter"], "title": "Agentic Browser Security: 2025 Year-End Review", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.wiz.io%2Fblog%2Fagentic-browser-security-2025-year-end-review%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/DLvsRSyBX566fxVNcwg3QDZzYZPMj2fSKwGB9zuqGOg=441", "summary": "Agentic Browser Security: 2025 Year-End Review (4 minute read) Wiz reviewed 2025's surge in agentic browser adoption alongside numerous prompt-injection vulnerabilities.", "source": "tldr", "AI": {"tldr": "Wiz\u5bf92025\u5e74\u4ee3\u7406\u5f0f\u6d4f\u89c8\u5668\u91c7\u7528\u6fc0\u589e\u53ca\u76f8\u5173\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\u8fdb\u884c\u4e86\u5e74\u5ea6\u56de\u987e\u5206\u6790", "motivation": "\u968f\u7740\u4ee3\u7406\u5f0f\u6d4f\u89c8\u5668\u57282025\u5e74\u5feb\u901f\u666e\u53ca\uff0c\u76f8\u5173\u7684\u5b89\u5168\u98ce\u9669\u7279\u522b\u662f\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\u663e\u8457\u589e\u52a0\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u5b89\u5168\u8bc4\u4f30", "method": "\u5bf92025\u5e74\u4ee3\u7406\u5f0f\u6d4f\u89c8\u5668\u91c7\u7528\u60c5\u51b5\u8fdb\u884c\u56de\u987e\u6027\u5206\u6790\uff0c\u8bc6\u522b\u548c\u8bc4\u4f30\u76f8\u5173\u7684\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e", "result": "\u53d1\u73b0\u4ee3\u7406\u5f0f\u6d4f\u89c8\u5668\u91c7\u7528\u57282025\u5e74\u51fa\u73b0\u6fc0\u589e\uff0c\u540c\u65f6\u4f34\u968f\u5927\u91cf\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\uff0c\u63ed\u793a\u4e86\u8be5\u6280\u672f\u5feb\u901f\u666e\u53ca\u5e26\u6765\u7684\u5b89\u5168\u6311\u6218", "conclusion": "\u4ee3\u7406\u5f0f\u6d4f\u89c8\u5668\u7684\u5feb\u901f\u91c7\u7528\u9700\u8981\u66f4\u5f3a\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\u7684\u9632\u5fa1\u673a\u5236", "topic": "agent analysis"}}
