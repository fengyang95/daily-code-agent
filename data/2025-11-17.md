<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 16]
- [cs.AI](#cs.AI) [Total: 12]
- [tldr.article](#tldr.article) [Total: 15]
- [cs.LG](#cs.LG) [Total: 6]
- [wechat.article](#wechat.article) [Total: 11]
- [cs.SE](#cs.SE) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: 提出了一个无监督的循环检测框架，结合结构和语义分析来识别LLM驱动应用中隐藏的执行循环，这些循环会消耗资源但不触发显式错误。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的智能应用表现出非确定性行为，可能形成隐藏的执行循环，默默消耗资源而不触发显式错误，传统可观测性平台无法检测这些成本高昂的低效问题。

Method: 采用混合方法：首先应用计算高效的时间调用栈分析识别显式循环，然后利用语义相似性分析发现由冗余内容生成特征的微妙循环。

Result: 在基于LangGraph的股票市场应用的1575条轨迹上评估，混合方法F1得分为0.72（精确率0.62，召回率0.86），显著优于单独的结构方法（F1:0.08）和语义方法（F1:0.28）。

Conclusion: 虽然结果令人鼓舞，但仍有很大改进空间，未来工作需要完善方法并解决当前局限性。

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [2] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在处理时间约束方面存在系统性风险，包括双峰性能分布、极端提示脆弱性和系统性行动偏见，且参数数量与能力无关。即使通过微调也无法可靠学习时间约束满足，需要专门的架构机制。


<details>
  <summary>Details</summary>
Motivation: 测试LLM在需要实时决策的智能体架构中是否能够可靠地判断行动窗口是否开放或关闭，这一假设此前未经检验。

Method: 使用截止时间检测任务对8个生产级模型（2.8-8B参数）进行时间约束处理特性分析，包括性能分布、提示敏感性和行动偏见测试，并进行200个合成示例的微调实验。

Result: 发现系统性部署风险：双峰性能分布（模型准确率要么95%要么50%）、极端提示脆弱性（仅格式变化就导致30-60个百分点波动）和系统性行动偏见（失败模型100%假阳性率）。参数数量与能力无关，3.8B模型与7B模型表现相当。微调仅能部分改善能力。

Conclusion: 时间约束满足无法通过自然语言的下一个词预测可靠学习，即使有针对性的微调也无效。需要包含持续时间状态表示、显式约束检查和系统组合推理的架构机制，当前自回归架构缺乏这些机制。

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [3] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 研究发现工具增强语言模型虽然能提高最终答案准确率，但会导致推理质量下降，出现工具诱导短视现象，即模型将工具输出作为推理的替代品而非辅助证据。


<details>
  <summary>Details</summary>
Motivation: 探究工具增强语言模型在使用外部工具时是否真正提升了推理能力，还是仅仅依赖工具输出而忽视了连贯的推理过程。

Method: 使用PYMATH基准测试（1,679个竞赛级数学问题），开发多维度评估套件来量化推理退化，并提出基于偏好优化的框架来重新调整模型使用工具的方式。

Result: 工具增强模型在最终答案准确率上提升19.3个百分点，但推理质量显著下降（非工具模型在推理过程比较中胜出41.5%），工具使用频率越高，推理连贯性越差。

Conclusion: 工具使用虽然提高答案准确性，但损害推理质量，需要通过偏好优化等方法重新调整模型将工具作为辅助证据而非推理替代品。

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [4] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 本文提出了一种贝叶斯方法来量化基于大语言模型的文本生成系统在二元评估指标中的统计不确定性，重点关注由概率文本生成策略引起的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估方法往往忽略统计不确定性量化，无法准确评估模型在有害输出生成、对抗性输入敏感性等方面的行为表现。

Method: 采用贝叶斯方法量化二元评估指标中的不确定性，特别关注概率文本生成策略引起的不确定性，并通过两个案例研究验证该方法。

Result: 该方法能够为基于LLM的系统行为提供有用的不确定性量化，在拒绝率评估和成对偏好评估中表现出良好的应用效果。

Conclusion: 贝叶斯方法能够有效量化大语言模型评估中的统计不确定性，为模型行为评估提供更可靠的统计基础。

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [5] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: 提出了一种自监督框架来提升防护模型的语义鲁棒性，通过使用释义集合和偏斜感知聚合策略来增强预测一致性，显著减少了语义变异性并改善了模型校准。


<details>
  <summary>Details</summary>
Motivation: 防护模型对语言表面变化的敏感性是其关键漏洞，即使意义保持不变的释义也会导致安全评分大幅波动，表明缺乏语义基础。

Method: 使用释义集合强制预测一致性，采用新颖的偏斜感知聚合策略进行鲁棒目标计算，避免标准聚合方法可能降低安全性的问题。

Result: 在六个开源防护模型上测试，方法将语义变异性降低了约58%，基准准确率平均提高约2.5%，并能泛化到未见过的风格变化。

Conclusion: 语义一致性应作为首要训练目标，该方法为构建更可靠的防护模型提供了可扩展方案，并揭示了模型校准与一致性之间的双向关系。

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [6] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 提出了STaDS框架来评估LLMs的理解能力，发现大多数模型在不同领域难以保持一致的准确率，且存在预测准确但决策依据不忠实的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs虽然预测准确率高，但准确性并不等同于真正的理解能力。需要评估模型是否能像人类专家一样，在不同实例和领域中做出基于相关决策因素的一致决策。

Method: 引入结构化表格决策模拟(STaDS)，通过15个不同决策场景对9个前沿LLMs进行评估，从问题理解、知识预测和决策因素依赖三个维度联合评估理解能力。

Result: 大多数模型在不同领域难以保持一致的强准确率；模型可能准确但全局不忠实，存在陈述理由与预测驱动因素之间的不匹配。

Conclusion: 需要超越准确性的全局理解评估协议，开发新框架来增强LLMs的理解能力。

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [7] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: 提出了一个基于人类反馈的持续学习框架，用于改进文本到SQL的查询生成，通过存储和重用从反馈中提取的知识来提高执行准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在从自然语言生成SQL查询时，难以处理数据库特定模式和隐含领域知识，需要一种能够从人类反馈中持续学习的方法。

Method: 设计了一个学习代理框架，接收自然语言反馈来优化查询，并将揭示的知识提炼存储到结构化内存中，供未来任务重用。评估了多种学习代理架构变体，包括过程代理等内存增强代理。

Result: 在BIRD基准测试开发集上的实验表明，内存增强代理（特别是过程代理）通过利用人类在环反馈，实现了显著的准确性提升和错误减少。

Conclusion: 将隐含的人类专业知识转化为可重用知识对于构建更适应、领域感知的文本到SQL系统至关重要，这些系统能够从人类在环中持续学习。

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [8] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: 使用多轮对话测试LLM的鲁棒性，发现简单的'再想想'提示会导致准确率显著下降，可通过马尔可夫链建模准确率动态变化，提出稳态准确率作为交互场景下的鲁棒性指标。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实任务中的广泛应用，评估其在多轮交互中的鲁棒性变得至关重要，特别是在需要一致推理的高风险场景中。

Method: 使用简单的多轮后续提示评估模型答案变化，通过马尔可夫链建模准确率动态，并探索线性探针是否能预测这些变化。

Result: 发现LLM存在显著脆弱性：'再想想'提示导致Gemini 1.5 Flash准确率下降约10%，结合语义等效重述问题使Claude 3.5 Haiku准确率下降7.5%。模型准确率可通过马尔可夫链有效建模，稳态准确率平均比首轮低8%。

Conclusion: 建立了稳态准确率作为交互场景下的原则性鲁棒性指标，暴露了模型在重复提问下的脆弱性，解决这种不稳定性对高风险交互部署至关重要。

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [9] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: Squid Game是一个动态对抗性评估环境，通过资源受限和不对称信息设置来评估LLM在交互游戏中的表现，包含六个淘汰赛级别，测试多方面能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试假设良性、资源丰富的环境，无法评估LLM在压力下的行为，且存在数据污染问题，需要建立更可信的评估框架。

Method: 设计了包含六个淘汰赛级别的动态对抗环境，让LLM与其他LLM对手进行交互游戏，评估指令遵循、代码、推理、规划和安全对齐等多方面能力。

Result: 评估了50多个LLM，观察到同一模型谱系中的代际性能跃迁，发现某些模型使用投机捷径获胜，表明静态基准测试可能存在更高级别的评估范式污染。

Conclusion: 动态评估可以作为静态评估的补充部分，相关分析显示动态评估与现有基准测试具有互补性。

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [10] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本文系统评估了大型语言模型在国际关系领域的国家层面偏见，基于联合国安理会历史记录开发了偏见评估框架，发现模型存在对西方国家的有利偏见和对俄罗斯的不利偏见，并提出结合检索增强生成和反思技术的去偏见方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是系统评估LLMs在国际关系领域的国家层面偏见，因为LLMs在该领域的应用日益广泛，但缺乏对其偏见的系统性研究。

Method: 基于联合国安理会历史记录开发了包含三个测试的偏见评估框架，重点关注安理会五个常任理事国，并提出结合检索增强生成和反思技术的去偏见框架。

Result: 实验结果显示LLMs存在明显的国家层面偏见，偏见模式因模型和评估情境而异，推理能力更强的模型偏见更小，提出的去偏见框架有效减少了偏见并提升了性能。

Conclusion: LLMs的国家层面偏见具有多维性，应用LLMs于国际关系领域时需要同时评估偏见和性能，提出的去偏见方法具有实际应用价值。

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [11] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 该论文提出了一种通过参数空间对齐来改进任务算术的方法，解决了LLM技能转移中的负干扰问题，在推理基准测试中优于标准任务算术。


<details>
  <summary>Details</summary>
Motivation: 任务算术在LLM技能转移中存在负干扰问题，特别是当模型在训练过程中出现分歧时，这限制了技能的有效转移。

Method: 首先对齐模型的参数空间，利用Transformer架构的置换、旋转和缩放对称性，针对GQA和SwiGLU层进行适配，探索基于权重和基于激活的方法。

Result: 在具有挑战性的推理基准测试中，该方法始终优于标准任务算术，成功将高级推理技能转移到非推理模型中。

Conclusion: 该工作为在进化的LLM家族之间合并和转移专门技能提供了有效方法，减少了冗余微调并增强了模型适应性。

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [12] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 研究发现LLM作为评判者时，对话框架会显著影响其判断准确性，即使是最小对话语境也能导致平均9.24%的性能变化。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在需要社交或对话判断的任务中作为评判者的可靠性，特别是在从直接事实查询转向对话判断任务时的表现变化。

Method: 通过对比模型在直接事实查询和最小对话语境中的表现，并在两种条件下施加简单反驳压力来测量模型立场的坚定程度。

Result: 不同模型表现出不同倾向：GPT-4o-mini在社交框架下显示奉承倾向，而Llama-8B-Instruct变得过度批判。对话框架显著改变模型判断。

Conclusion: 对话框架是LLM评估的关键因素，该框架为诊断模型信念提供了可复现方法，有助于开发更可信的对话系统。

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [13] [When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets](https://arxiv.org/abs/2511.10985)
*Aladin Djuhera,Farhan Ahmed,Swanand Ravindra Kadhe,Syed Zawad,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 本文对主流开源DPO数据集进行了首次系统性分析，使用Magpie框架标注样本质量，并基于分析结果构建了新的DPO混合数据集UltraMix，该数据集比最佳单个数据集小30%但性能更优。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐主要采用DPO技术，但开源DPO数据集缺乏系统性比较，难以理解偏好选择方式、任务类型覆盖范围以及人类判断反映程度。

Method: 使用Magpie框架对每个样本进行任务类别、输入质量和偏好奖励的标注，实现可扩展的细粒度偏好质量检查，并基于分析结果从五个语料库中精选构建UltraMix混合数据集。

Result: 揭示了不同数据集在奖励边际上的结构和质量差异，UltraMix比最佳单个数据集小30%但在关键基准测试中性能更优。

Conclusion: 数据中心的偏好优化研究需要更系统的数据集分析，UltraMix展示了通过精选混合可以显著提升DPO效果。

Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.

</details>


### [14] [iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference](https://arxiv.org/abs/2511.11306)
*Wei Fan,JinYi Yoon,Bo Ji*

Main category: cs.CL

TL;DR: 提出了智能多智能体辩论框架iMAD，通过轻量级分类器选择性触发多智能体辩论，在保持准确性的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论框架对所有查询都进行辩论，计算成本高且可能推翻正确单智能体答案，需要更高效的触发机制。

Method: iMAD首先让单智能体生成结构化自我批评响应，提取41个可解释的语言和语义特征，然后使用FocusCal损失训练的轻量级辩论决策分类器决定是否触发多智能体辩论。

Result: 在六个视觉问答数据集上的实验表明，iMAD将token使用量减少高达92%，同时将最终答案准确率提高高达13.5%。

Conclusion: iMAD框架通过智能触发辩论机制，在保持多智能体辩论优势的同时显著提高了计算效率。

Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).

</details>


### [15] [NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery](https://arxiv.org/abs/2511.11324)
*Anurag J. Vaidya,Felix Meissen,Daniel C. Castro,Shruthi Bannur,Tristan Lazard,Drew F. K. Williamson,Faisal Mahmood,Javier Alvarez-Valle,Stephanie L. Hyland,Kenza Bouzid*

Main category: cs.CL

TL;DR: NOVA是一个代理框架，可将科学查询转化为可执行的分析流水线，通过迭代生成和运行Python代码。它集成了49个领域特定工具，并能在SlideQuest基准测试中优于其他编码代理基线。


<details>
  <summary>Details</summary>
Motivation: 数字化病理学分析涉及复杂、耗时的流程和专业知识，限制了其可访问性。需要开发能够自动执行多步骤推理和计算问题解决的系统。

Method: NOVA框架通过迭代生成和运行Python代码来构建分析流水线，集成了49个领域特定工具（如细胞核分割、全玻片编码），并能临时创建新工具。

Result: 在SlideQuest基准测试（90个问题）中，NOVA优于其他编码代理基线。病理学家验证的案例研究显示其能将形态学与预后相关的PAM50亚型联系起来。

Conclusion: NOVA展示了可扩展的发现潜力，能够处理复杂的病理学分析任务，超越了传统的知识回忆或诊断问答系统。

Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.

</details>


### [16] [W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search](https://arxiv.org/abs/2511.11518)
*Zhenyu Ding,Yuhao Wang,Tengyue Xiao,Haoying Wang,Guojun Ma,Mingyang Wan,Caigui Jiang,Ning Ding*

Main category: cs.CL

TL;DR: 提出了W2S-AlignTree框架，首次将蒙特卡洛树搜索与弱到强泛化范式结合，实现推理时对齐，无需修改强模型参数即可提供细粒度指导。


<details>
  <summary>Details</summary>
Motivation: 解决LLM输出与人类偏好不对齐的问题，现有训练时对齐方法成本高、扩展性有限，需要可扩展且自适应的推理时对齐机制。

Method: 将LLM对齐建模为生成搜索树中的最优启发式搜索问题，利用弱模型的实时步级信号作为对齐代理，引入熵感知探索机制。

Result: 在情感生成、摘要和指令跟随任务中一致优于强基线，将Llama3-8B在摘要任务上的性能从1.89提升到2.19，相对提升15.9%。

Conclusion: W2S-AlignTree提供了一种有效且可扩展的推理时对齐解决方案，实现了细粒度控制而无需模型参数修改。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: Co-EPG是一个自迭代训练框架，通过规划和接地模型的协同进化来解决GUI任务自动化中的挑战。该框架建立了正反馈循环，通过GRPO优化规划模型，同时利用生成的数据优化接地模型，实现持续自我增强。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理方法存在两个基本限制：(1) 跨模型协同利用不足，(2) 过度依赖合成数据生成而利用不足。需要一种能够有效整合规划和接地能力的方法。

Method: 提出Co-EPG自迭代训练框架，通过Group Relative Policy Optimization (GRPO)优化规划模型，同时利用生成的数据优化接地模型，建立规划和接地模型之间的正反馈循环。

Result: 在Multimodal-Mind2Web和AndroidControl基准测试中，仅经过三次迭代就超越了现有最先进方法，无需外部数据。代理在每次迭代中持续改进，展示了强大的自我增强能力。

Conclusion: 这项工作为GUI代理建立了一种新颖的训练范式，从孤立优化转向集成、自驱动的协同进化方法。

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [18] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 这篇论文从自适应性的角度重新审视大语言模型的推理能力，提出将推理努力根据任务难度和不确定性进行动态分配，并建立了包含训练方法和训练自由方法的系统分类法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对所有任务采用统一的推理策略，导致简单任务生成过长推理链而困难任务推理不足，需要发展能够根据任务特性自适应分配推理努力的能力。

Method: 将自适应推理形式化为控制增强的策略优化问题，提出系统分类法：训练方法（强化学习、监督微调、学习控制器）和训练自由方法（提示条件化、反馈驱动停止、模块化组合）。

Result: 建立了连接经典认知范式与算法实现的框架，澄清了不同机制如何在实际中实现自适应推理，并支持跨策略的系统比较。

Conclusion: 识别了自我评估、元推理和人类对齐推理控制等开放挑战，为自适应推理研究提供了系统性框架。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [19] [HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments](https://arxiv.org/abs/2511.10810)
*Ran Elgedawy,Sanjay Das,Ethan Seefried,Gavin Wiggins,Ryan Burchfield,Dana Hewit,Sudarshan Srinivasan,Todd Thomas,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: HARNESS是一个模块化AI框架，用于预测危险事件和分析美国能源部环境中的操作风险，结合LLM与结构化工作数据、历史事件检索和风险分析，通过人机协作机制提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在关键任务工作场所，操作安全至关重要。由于日常任务的复杂性和危险性，需要开发能够主动预测危险事件的系统来保障安全。

Method: 整合大型语言模型与结构化工作数据、历史事件检索和风险分析，采用人在回路机制让领域专家优化预测，形成自适应学习循环。

Result: 初步部署显示有希望的结果，系统通过专家协作和迭代推理提高了预测安全系统的可靠性和效率。

Conclusion: HARNESS通过结合领域专家协作与迭代智能推理，改进了预测安全系统的性能。未来工作将聚焦于准确性、专家一致性和决策延迟的量化评估。

Abstract: Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.

</details>


### [20] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 该论文探讨了自主AI系统在遇到无法完全满足所有约束条件的新场景时，需要超越训练策略来构建、评估和证明候选行动方案的能力要求。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在实际部署中必然会遇到训练数据未覆盖的新场景，需要能够处理不完全满足所有约束条件的情况，以符合人类期望和价值观。

Method: 通过分析和实证案例研究，识别智能体决策所需的知识类型，包括规范性、实用性和情境性理解。

Result: 确定了智能体在复杂现实环境中做出更符合人类期望决策所需的知识整合要求。

Conclusion: 智能体需要整合多种知识类型来选择和追求更符合人类期望的行动方案，以应对复杂现实环境中的决策挑战。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [21] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 研究发现角色分配策略对多智能体辩论性能有显著影响，提出了'Truth Last'策略提升推理任务表现22%，并开发了MADC策略通过一致性评估来优化多智能体辩论机制。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论在增强推理能力方面具有潜力，但角色分配策略这一关键方面尚未得到充分探索，特别是在实际应用中真理未知的情况下。

Method: 提出'Truth Last'角色分配策略，并开发多智能体辩论一致性(MADC)策略，通过路径一致性评估独立角色间的一致性，模拟最高一致性得分的角色作为真理。

Result: 在9个LLM模型上验证，MADC策略持续表现出先进性能，有效克服了多智能体辩论的性能瓶颈，推理任务性能提升达22%。

Conclusion: MADC为LLM智能体扩展提供了关键改进路径，系统性地优化了多智能体辩论的核心机制。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [22] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出DSS框架，利用可微分模拟器Waymax作为状态预测器和评估器，通过梯度下降优化动作序列，显著提升自动驾驶的跟踪和路径规划精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，规划对于避免碰撞和在复杂密集交通场景中导航至关重要。传统方法在学习策略、状态预测器和评估器时面临挑战。

Method: 使用可微分模拟器Waymax作为状态预测器和评估器，利用其硬编码动力学实现高精度状态预测，通过梯度下降在想象轨迹上优化动作序列。

Result: 实验表明DSS（规划梯度与随机搜索结合）相比序列预测、模仿学习、无模型RL和其他规划方法，显著提升了跟踪和路径规划精度。

Conclusion: DSS框架通过结合规划梯度和随机搜索，为自动驾驶规划提供了有效解决方案。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [23] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 提出了MUG协议来解决多智能体辩论中的幻觉问题，通过类似"谁是卧底"的游戏机制检测幻觉智能体，使用反事实测试和动态证据修改来提升多模态推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论假设所有辩论者都是理性的，但现实中智能体本身容易出现幻觉，这限制了辩论范式的可靠性。

Method: MUG协议将多智能体辩论重构为检测"卧底"智能体的过程，通过修改参考图像引入反事实证据，观察智能体是否能准确识别变化，从而识别幻觉智能体。

Result: MUG在三个关键维度上改进了多智能体辩论协议：实现基于反事实测试的事实验证、引入跨证据推理、促进主动推理。

Conclusion: MUG为LLMs的多模态推理提供了一个更可靠有效的框架。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [24] [UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios](https://arxiv.org/abs/2511.11252)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.AI

TL;DR: 提出了UAVBench基准数据集，包含5万个经过验证的无人机飞行场景和5万个多选题，用于系统评估LLM在无人机领域的推理能力。评估了32个最先进的LLM，发现在感知和政策推理方面表现良好，但在伦理意识和资源受限决策方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 自主空中系统越来越依赖LLM进行任务规划、感知和决策，但缺乏标准化和物理基础的基准限制了对其推理能力的系统评估。

Method: 通过分类指导的LLM提示和多阶段安全验证生成5万个验证过的无人机飞行场景，并创建包含10种认知和伦理推理风格的5万个多选题。

Result: 评估了32个最先进的LLM，包括GPT-5、ChatGPT-4o等，发现它们在感知和政策推理方面表现强劲，但在伦理意识和资源受限决策方面存在持续挑战。

Conclusion: UAVBench为自主空中系统中的代理AI基准测试建立了可重现和物理基础的基础，推动了下一代无人机推理智能的发展。

Abstract: Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench

</details>


### [25] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign是一个推理时框架，将大视觉语言模型对齐重新定义为经济理性的搜索问题，通过前瞻性评估函数动态权衡安全性、效用和成本，在降低计算成本的同时实现更好的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐方法在安全性、实用性和运营成本之间存在权衡困难，且仅关注最终输出的过程盲目性会浪费大量计算预算在不安全的推理上，允许有害推理通过良性理由伪装来规避安全检测。

Method: 将LVLM视为有限理性智能体，增量扩展思维图，使用前瞻性函数（类似于净现值）对动作进行评分，动态权衡预期安全性、效用和成本与剩余预算，并通过最弱环节原则强制执行路径安全性。

Result: 在3个闭源和2个开源模型上的6个数据集实验表明，EcoAlign以更低的计算成本达到或超越了最先进的安全性和实用性水平。

Conclusion: EcoAlign为强大的LVLM对齐提供了一个原则性的经济路径，解决了安全与效率的平衡问题。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [26] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: MarsRL是一个新颖的强化学习框架，通过代理管道并行性联合优化多代理系统中的所有代理，显著提升了开源模型在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多代理推理系统在闭源模型上表现良好，但在开源模型中由于批评和修正能力不足而难以推广。需要开发能够提升开源模型多代理推理能力的训练框架。

Method: 提出MarsRL框架，采用代理特定的奖励机制来减少奖励噪声，并使用管道启发的训练方法来高效处理长轨迹，联合优化系统中的所有代理。

Result: 在Qwen3-30B-A3B-Thinking-2507上，MarsRL将AIME2025准确率从86.5%提升至93.3%，BeyondAIME从64.9%提升至73.8%，甚至超过了Qwen3-235B-A22B-Thinking-2507。

Conclusion: MarsRL展示了在多代理推理系统中推进强化学习的潜力，能够显著提升开源模型在复杂推理任务上的表现，并扩展其应用范围。

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [27] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个经验引导的推理系统，能够在推理时动态生成包含LLM调用、工具、采样参数和控制逻辑的完整计算策略，通过元策略实现所有策略组件的自适应调整。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI系统在推理时无法灵活调整采样参数、工具配置、系统提示或切换代理范式的问题，实现基于经验的自适应问题解决。

Method: 使用基于LLM的元策略，包含Guide组件生成候选策略，Consolidator组件整合执行反馈改进策略生成，能够动态创建完整的计算程序。

Result: 在五个挑战性基准测试中，相比最强基线实现了高达14%的准确率提升，同时计算成本降低高达111倍，且随着经验积累性能持续提升。

Conclusion: EGuR通过动态策略生成和基于经验的优化，显著提升了AI系统的自适应能力和效率，为构建更灵活的智能系统提供了新途径。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


### [28] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: 提出了一种基于模型引导策略塑造的测试时对齐技术，能够在复杂动态环境中精确控制个体行为属性，在伦理对齐和奖励最大化之间实现原则性权衡，无需重新训练智能体。


<details>
  <summary>Details</summary>
Motivation: 决策AI智能体在复杂动态环境中部署时面临与人类价值观或指导方针保持对齐的关键挑战。仅以实现目标为训练目的的智能体可能采取有害行为，暴露出最大化奖励函数与保持对齐之间的关键权衡。

Method: 使用模型引导策略塑造的测试时对齐技术，通过场景-动作属性分类器进行策略塑造，确保决策与伦理属性对齐。在MACHIAVELLI基准测试上进行评估，包含134个基于文本的游戏环境和数千个涉及伦理决策的标注场景。

Result: 测试时策略塑造为缓解不同环境和对齐属性中的不道德行为提供了有效且可扩展的解决方案，能够精确控制个体行为属性，并在伦理对齐和奖励最大化之间实现原则性权衡。

Conclusion: 测试时策略塑造是确保预训练智能体在复杂环境中保持伦理对齐的有效方法，无需昂贵的重新训练过程。

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [29] [Strix](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fusestrix%2Fstrix%3Futm_source=tldrdevops/1/0100019a824235d4-236add6a-0a0a-42b1-aee2-0b8d01c8b65e-000000/Dks1mLVV8Op9twdYu0WFGAcVA7Vtp7aNlnZChADe4Oc=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Strix是一个开源AI代理，通过动态运行代码来模拟黑客行为，识别和验证漏洞，为开发者提供快速安全测试。


<details>
  <summary>Details</summary>
Motivation: 为开发者提供快速、自动化的安全测试工具，通过模拟黑客行为来识别潜在漏洞。

Method: 使用AI代理动态运行代码来模拟黑客攻击，识别和验证安全漏洞。

Result: 能够集成到CI/CD流水线中，提供快速的安全测试能力。

Conclusion: Strix是一个有效的开源安全测试工具，能够帮助开发者在开发过程中及时发现安全漏洞。

Abstract: Strix (GitHub Repo) Strix is open-source AI agent that emulates hackers by dynamically running code to identify and validate vulnerabilities, providing fast security testing for developers. GPT-5 and Claude Sonnet 4.5 are recommended for optimal use. Strix can be integrated into CI/CD pipelines.

</details>


### [30] [Introducing GPT-5.1 for developers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiU8nnf/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/T4ve092CTtxF9HxW29Wfjl8EUmP7cD6Cj1K_KoevixY=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI发布了GPT-5.1模型，专为代理和编码任务设计，平衡智能与速度，提供动态推理时间调整、无推理模式、扩展提示缓存等功能，并引入apply_patch和shell等新工具。


<details>
  <summary>Details</summary>
Motivation: 开发一个在智能和速度之间取得平衡的模型，专门针对代理和编码任务，提高开发效率。

Method: GPT-5.1模型采用动态推理时间调整机制，根据任务复杂度自动调整推理时间；提供无推理模式处理简单任务；使用扩展提示缓存提高效率；引入apply_patch工具进行可靠代码编辑和shell工具执行命令。

Result: GPT-5.1在代理和编码任务中表现出平衡的智能和速度，能够根据任务复杂度动态调整性能，提供更高效的开发体验。

Conclusion: GPT-5.1是一个专门为开发者和代理任务优化的模型，通过智能的速度平衡和新的工具集，显著提升了编码和代理任务的效率。

Abstract: Introducing GPT-5.1 for developers (11 minute read) OpenAI has released GPT-5.1, a new model in the GPT-5 series designed for agentic and coding tasks that balances intelligence and speed. GPT-5.1 dynamically adjusts its reasoning time based on task complexity, offers a "no reasoning" mode for faster responses on simpler tasks, and includes extended prompt caching for improved efficiency. It introduces new tools like the apply_patch tool for reliable code editing and a shell tool for executin...

</details>


### [31] [I am a programmer, not a rubber-stamp that approves Copilot generated code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fprahladyeri.github.io%2Fblog%2F2025%2F10%2Fi-am-a-programmer.html%3Futm_source=tldrwebdev/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/RFSo00nLZqdx7fzzFxQ33IKyGbqjd63zMhPqWUVhRXQ=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: I am a programmer, not a rubber-stamp that approves Copilot generated code (2 minute read) Enforced AI assistance in programming is transforming programmers from creators into mere approvers of AI-generated code, potentially devaluing the profession.

</details>


### [32] [Why agents DO NOT write most of our code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Foctomind.dev%2Fblog%2Fwhy-agents-do-not-write-most-of-our-code-a-reality-check%3Futm_source=tldrwebdev/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/AOzKsx-ct-lOJ0EWPyW59IUMvy2seXpNI4uOwbHGk_E=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理目前无法编写大部分代码，主要由于代码库心智模型缺失、缺乏自我反思能力，以及在复杂任务中无法持续提供生产力提升


<details>
  <summary>Details</summary>
Motivation: 探讨为什么尽管有大量炒作，AI代理在实际软件开发中仍无法承担主要编码工作，分析当前技术局限性

Method: 基于实际开发经验分析AI代理在代码编写中的局限性，包括心智模型、自我反思和生产力提升等方面

Result: 发现AI代理在复杂软件开发中存在显著局限性，无法替代人类开发者的核心作用

Conclusion: AI代理目前更适合作为辅助工具而非主要编码者，需要进一步技术突破才能承担更重要的开发角色

Abstract: Why agents DO NOT write most of our code (10 minute read) Despite the hype, AI agents are not yet capable of writing most of this company's code due to issues like loss of mental model of the codebase, absence of self-reflection in AI, and the current inability of AI to consistently deliver meaningful productivity boosts in complex tasks.

</details>


### [33] [Webflow Expands Beyond Design with New AI Tool for Building Full-stack Web Applications](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsiliconangle.com%2F2025%2F11%2F12%2Fwebflow-expands-beyond-design-new-ai-tool-building-full-stack-web-applications%2F%3Futm_source=tldrdesign/1/0100019a8279d01a-b7d96b19-23a2-4745-ad7e-0dacadaad4e5-000000/xQRJe9S_42qTUMWPQ2fJTKSIznUz0JTs2Q5ORDl-HpM=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Webflow推出AI工具App Gen，允许用户通过自然语言提示构建和部署全栈Web应用，无需编写代码。


<details>
  <summary>Details</summary>
Motivation: 让非技术用户能够快速构建全栈Web应用，同时保持品牌设计一致性。

Method: 使用AI技术解析自然语言提示，自动生成代码并与现有设计系统和CMS数据集成。

Result: 目前处于测试阶段，能够自动集成设计系统、维护品牌一致性，未来将支持认证等功能。

Conclusion: App Gen使Webflow从设计平台扩展到全栈应用开发领域，降低了Web应用开发门槛。

Abstract: Webflow Expands Beyond Design with New AI Tool for Building Full-stack Web Applications (2 minute read) Webflow's App Gen is an AI-powered tool that enables users to build and deploy full-stack web applications through natural language prompts without writing code. The platform automatically integrates applications with existing design systems and CMS data while maintaining brand consistency across typography, colors, and components. Currently in beta, App Gen will eventually support authenti...

</details>


### [34] [Agentic Creative Workflow Studio](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyouart.ai%2F%3Futm_source=tldrdesign/1/0100019a8279d01a-b7d96b19-23a2-4745-ad7e-0dacadaad4e5-000000/65WOiOgkM_JZhC-N8MT5CewcOc9mzotRjxUsF4fpr3U=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: YouArt是一个集成的AI创意工作室平台，提供AI图像和视频生成功能


<details>
  <summary>Details</summary>
Motivation: 为创意工作者提供一个统一的AI创作工具平台，简化AI图像和视频的生成流程

Method: 开发集成的AI创意工作室，整合多种AI生成模型和工具

Result: 成功构建了YouArt平台，能够生成高质量的AI图像和视频内容

Conclusion: YouArt作为一个综合AI创意工作室，有效提升了创意工作的效率和质量

Abstract: Agentic Creative Workflow Studio (Website) YouArt is an all-in-one AI Creative Studio. It allows you to create stunning AI image and video generators.

</details>


### [35] [Shannon](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FKeygraphHQ%2Fshannon%3Futm_source=tldrinfosec/1/0100019a82b12d74-84ef4594-cee1-4170-85a9-22ccb5ccb1c3-000000/9dZH8fyooAl_5kqEsMAxKYQBXBRjgBxCbC_COQTiipo=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Shannon是一个AI渗透测试工具，能够自主发现代码中的攻击向量，并通过内置浏览器执行真实攻击来验证漏洞的可利用性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自动发现和验证web应用漏洞的AI渗透测试工具，以在恶意攻击者之前发现并修复安全漏洞。

Method: 通过分析代码自主寻找攻击向量，然后使用内置浏览器执行真实攻击（如注入攻击和认证绕过）来验证漏洞。

Result: Shannon能够有效发现和验证web应用中的安全漏洞，证明其作为自动化渗透测试工具的有效性。

Conclusion: Shannon作为一个AI渗透测试工具，能够自主发现和验证web应用漏洞，有助于提高应用安全性。

Abstract: Shannon (GitHub Repo) Shannon is an AI pentester whose goal is to break your web app before someone else does. It autonomously hunts for attack vectors in your code, then uses its built-in browser to execute real exploits, such as injection attacks and auth bypass, to prove the vulnerability is actually exploitable.

</details>


### [36] [AI-orchestrated Cyberattack Detailed by Anthropic](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fassets.anthropic.com%2Fm%2Fec212e6566a0d47%2Foriginal%2FDisrupting-the-first-reported-AI-orchestrated-cyber-espionage-campaign.pdf%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/YFjY1V6b6Xai0IFFKHGfHHSZPol8t0RDtkEKRlV5hCA=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic披露了首个由AI驱动的网络间谍活动，攻击者使用Claude Code工具自主执行，针对全球30多个实体，展示了AI代理系统在网络安全中的威胁。


<details>
  <summary>Details</summary>
Motivation: 揭示AI驱动的网络攻击的现实威胁，展示AI代理系统在网络安全领域的潜在风险。

Method: 使用Anthropic的Claude Code工具进行自主网络间谍活动，攻击者与中国国家支持的组织有关。

Result: 成功针对全球30多个实体，证明了AI代理系统能够自主执行复杂的网络攻击。

Conclusion: AI驱动的网络攻击已成为现实威胁，需要加强对此类AI代理系统的安全防护和监管。

Abstract: AI-orchestrated Cyberattack Detailed by Anthropic (28 minute read) Anthropic disclosed the first documented AI-driven cyber espionage campaign, carried out largely autonomously using its Claude Code tool. The attackers, linked to a Chinese state-sponsored group, targeted 30+ entities worldwide, demonstrating the growing threat of agentic AI systems in cybersecurity.

</details>


### [37] [GenAI observability: 6 practical recommendations](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fbring-clarity-to-your-ai-systems%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-hyperframe-observability-for-ai%26utm_term=111425/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/tErOKy5SqN-eM0dzNGV6pjiyvVtuct6YI2EV8gO6PSo=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文提出了6个实用的GenAI可观测性建议，帮助组织为生成式AI和代理AI做准备，强调需要超越传统监控的全面可观测性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和代理AI虽然强大但不可预测，它们经常在既定工作流程中采取全新路径，仅靠传统监控无法满足需求，需要更全面的可观测性方案。

Method: 通过Dynatrace报告提出了6个实用的可观测性建议，针对GenAI和代理AI的特点设计。

Result: 提供了6个具体的可观测性推荐方案，帮助组织更好地准备和管理GenAI及代理AI系统。

Conclusion: 组织需要采纳这些实用的可观测性建议来应对GenAI和代理AI带来的挑战，确保系统的透明度和可控性。

Abstract: GenAI observability: 6 practical recommendations (Sponsor) GenAI and agentic AI are powerful but unpredictable. It's not just hallucination — they regularly take entirely new paths through established workflows. Clarity requires more than monitoring: this Dynatrace report lays out six pragmatic observability recommendations for practitioners preparing their organizations for GenAI and agentic AI. Read the report

</details>


### [38] [Introducing Code Wiki: Accelerating your code understanding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-code-wiki-accelerating-your-code-understanding%2F%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/dpv63e1TcbRGltxmhaOySYaVjrbgPx7uan7T1oXMpYY=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Code Wiki是一个持续更新的代码仓库结构化wiki平台，能够自动扫描代码库并在每次变更后重新生成文档，使文档与代码同步发展。


<details>
  <summary>Details</summary>
Motivation: 解决代码文档与代码本身不同步的问题，帮助新贡献者快速上手，同时让资深开发者能够理解新代码。

Method: 平台扫描完整代码库，在每次代码变更后自动重新生成文档，每个wiki部分和聊天回答都直接超链接到相关的代码文件和定义。

Result: 创建了一个持续更新的结构化代码wiki，文档与代码保持同步，提供了直接的代码链接。

Conclusion: Code Wiki能够加速代码理解过程，使新贡献者能够立即开始工作，同时帮助资深开发者理解新代码。

Abstract: Introducing Code Wiki: Accelerating your code understanding (3 minute read) Code Wiki is a platform that maintains a continuously updated, structured wiki for code repositories. It scans the full code base and regenerates documentation after each change, so docs evolve with the code. Every wiki section and chat answer is hyperlinked directly to the relevant code files and definitions. The platform will allow new contributors to start immediately, while senior developers can now understand new...

</details>


### [39] [DeepMind released SIMA 2](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fblog%2Fsima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds%2F%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/A1IfxZDpPMGrT63O-dIqtfMCD3YrpdWmy8dGDGoDvN4=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: DeepMind发布了SIMA 2，这是一个通用AI智能体，利用Gemini LLM能力在虚拟环境中进行推理、适应和交互。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在复杂虚拟环境中进行通用推理和交互的AI智能体，提升AI在游戏和模拟环境中的表现。

Method: 使用Gemini大型语言模型的能力，让智能体能够理解环境、进行推理并采取相应行动。

Result: SIMA 2展示了在虚拟环境中的通用交互能力，能够适应不同场景并执行复杂任务。

Conclusion: SIMA 2代表了AI智能体发展的一个重要里程碑，展示了LLM在虚拟环境交互中的潜力。

Abstract: DeepMind released SIMA 2 (9 minute read) SIMA 2 is a generalist AI agent that uses Gemini LLM capabilities to reason, adapt, and interact with virtual environments.

</details>


### [40] [Build Guardrails to Scale AI Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该报告介绍了如何通过建立防护措施来扩展AI代理，减少监管风险、控制代理行为并避免代价高昂的错误。


<details>
  <summary>Details</summary>
Motivation: 随着组织采用生成式AI，需要解决AI代理带来的运营、监管和声誉风险，将治理从数据和模型扩展到代理层面。

Method: 通过建立防护措施和治理框架，从数据到模型再到代理的全面治理，识别并填补治理空白。

Result: 提供了组织AI治理准备度的基准评估方法，帮助在问题升级到董事会层面之前解决风险。

Conclusion: 建立适当的防护措施和治理框架对于规模化采用AI代理至关重要，能够有效管理相关风险。

Abstract: Build Guardrails to Scale AI Agents (Sponsor) Learn how to reduce regulatory risk, control agent behavior, and avoid costly missteps as your teams adopt GenAI. This BARC report, sponsored by Dataiku, shows how organizations can extend governance from data to models to agents. It covers the operational, regulatory, and reputational risks of agentic AI and shows how to close the gaps before they become board-level problems. Read now to benchmark your AI governance readiness.

</details>


### [41] [reduce regulatory risk](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该报告探讨如何为AI代理建立防护措施，通过从数据到模型再到代理的治理扩展，降低监管风险、控制代理行为并避免代价高昂的失误。


<details>
  <summary>Details</summary>
Motivation: 随着团队采用生成式AI，组织面临运营、监管和声誉风险，需要建立有效的治理机制来预防这些问题升级为董事会层面的问题。

Method: 通过扩展治理框架，从数据治理延伸到模型治理再到代理治理，建立全面的AI治理体系。

Result: 提供了评估AI治理准备度的基准，帮助组织识别和弥补治理差距。

Conclusion: 建立全面的防护措施对于规模化部署AI代理至关重要，能够有效管理风险并确保合规性。

Abstract: Build Guardrails to Scale AI Agents (Sponsor) Learn how to reduce regulatory risk, control agent behavior, and avoid costly missteps as your teams adopt GenAI. This BARC report, sponsored by Dataiku, shows how organizations can extend governance from data to models to agents. It covers the operational, regulatory, and reputational risks of agentic AI and shows how to close the gaps before they become board-level problems. Read now to benchmark your AI governance readiness.

</details>


### [42] [BARC report](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文讨论了如何通过建立防护措施来规模化AI代理，减少监管风险、控制代理行为，避免代价高昂的错误。


<details>
  <summary>Details</summary>
Motivation: 随着团队采用生成式AI，需要解决AI代理带来的运营、监管和声誉风险，防止这些问题升级为董事会层面的问题。

Method: 通过将治理从数据和模型扩展到AI代理，建立全面的AI治理框架来管理风险。

Result: 提供了组织可以评估其AI治理准备度的基准方法。

Conclusion: 组织需要建立完善的防护措施和治理框架来安全地规模化AI代理应用。

Abstract: Build Guardrails to Scale AI Agents (Sponsor) Learn how to reduce regulatory risk, control agent behavior, and avoid costly missteps as your teams adopt GenAI. This BARC report, sponsored by Dataiku, shows how organizations can extend governance from data to models to agents. It covers the operational, regulatory, and reputational risks of agentic AI and shows how to close the gaps before they become board-level problems. Read now to benchmark your AI governance readiness.

</details>


### [43] [Organizing Code, Experiments, and Research for Kaggle Competitions](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftowardsdatascience.com%2Forganizing-code-experiments-and-research-for-kaggle-competitions%2F%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/B1OC1q0sF-uNzysANg9LEPE7Ej1c7A9JEW8u1vVekC0=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍了在Kaggle竞赛中组织代码、实验和研究的有效方法，强调模块化仓库结构、版本控制和wandb、Hydra等工具的重要性


<details>
  <summary>Details</summary>
Motivation: 为了解决数据科学项目中代码组织混乱、实验难以追踪和复现的问题，特别是在Kaggle竞赛这种需要高效协作的场合

Method: 采用模块化仓库结构、版本控制系统，以及wandb和Hydra等工具来管理实验和配置

Result: 实现了更高效的实验流程、更好的协作能力和可复现的研究环境

Conclusion: 良好的代码组织、实验跟踪和可复现环境是数据科学项目成功的关键要素

Abstract: Organizing Code, Experiments, and Research for Kaggle Competitions (14 minute read) Robust code organization, meticulous experiment tracking, and reproducible environments are non-negotiable for successful data science projects. Using modular repo structures, version control, and tools like wandb and Hydra streamlines experimentation and collaboration across platforms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出一种利用行为策略收集离策略数据的方法，以降低回报估计的方差，从而提高强化学习算法的样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法依赖回报估计进行策略改进，但高方差的回报估计会导致样本效率低下和训练不稳定。最近研究表明，精心设计的行为策略可以收集离策略数据，获得方差更低的回报估计。

Method: 将离策略评估的关键见解扩展到在线强化学习设置，在策略评估和改进交错进行时学习最优策略。扩展了两种策略梯度方法，使用单一行为策略收集数据，获得方差更低的回报估计。

Result: 在多样化环境中的实验表明，该方法相比传统方法具有更好的样本效率和性能表现。

Conclusion: 收集离策略数据可以获得方差更低的回报估计，从而提高强化学习算法的效率和稳定性，挑战了传统认为在线策略数据收集是最优的观点。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [45] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM是一种精确的代码选择算法，通过向LLM提出成对成员资格和成对等价性查询来从多个生成的程序中选出正确程序，在四个流行代码数据集上平均比现有最佳算法提升13.0%的pass@1成功率。


<details>
  <summary>Details</summary>
Motivation: 现有代码选择算法可能无法识别正确程序，因为它们可能错误识别不等价程序，或者依赖LLM并假设它总能正确确定每个输入的输出。

Method: 提出ExPairT-LLM算法，通过向LLM提出两种新型查询：成对成员资格和成对等价性查询，这些查询对LLM更简单，使算法能够通过锦标赛方式识别正确程序，对某些LLM错误具有鲁棒性。

Result: 在四个流行代码数据集上，ExPairT-LLM的pass@1（成功率）平均比最先进的代码选择算法高出13.0%，最高可达27.1%。同时将执行复杂推理的LLM的pass@1提高了24.0%。

Conclusion: ExPairT-LLM通过更简单的查询类型和锦标赛机制，有效提升了代码生成中程序选择的准确性和鲁棒性。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [46] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: GO UT Bench是一个包含5264对代码和单元测试的基准数据集，用于解决代码LLM在Go语言中训练数据不平衡问题，特别针对单元测试生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据严重偏向开源代码，而低估了软件工程任务，特别是在Go等低资源语言中。这导致模型擅长代码自动补全，但在实际开发工作流（如单元测试生成）上表现不佳。

Method: 构建GO UT Bench基准数据集，包含来自10个许可宽松的Go仓库的5264对代码和单元测试。在两个LLM家族（专家混合和密集解码器）上评估其作为微调数据集的有效性。

Result: 微调后的模型在超过75%的基准任务上优于基础模型。

Conclusion: GO UT Bench作为微调数据集能有效提升代码LLM在Go语言单元测试生成任务上的性能。

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [47] [Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations](https://arxiv.org/abs/2511.10872)
*Shuyuan Zhang,Zihan Wang,Xiao-Wen Chang,Doina Precup*

Main category: cs.LG

TL;DR: 提出G4RL方法，通过图编码器-解码器评估未见状态，解决图引导分层强化学习中的样本效率低和子目标表示差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图引导分层强化学习方法依赖领域特定知识构建图，或动态构建图但难以有效利用图信息传递给新访问状态，存在样本效率低和子目标表示差的问题。

Method: 开发图编码器-解码器来评估未见状态，G4RL方法可集成到任何现有GCHRL方法中，在具有对称和可逆转换的环境中提升性能。

Result: 经验结果表明，利用图编码器-解码器产生的高层和低层内在奖励，显著提升了最先进GCHRL方法的性能，计算成本增加很小。

Conclusion: G4RL方法有效解决了图引导分层强化学习中的关键挑战，在密集和稀疏奖励环境中都能显著提升性能。

Abstract: The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.

</details>


### [48] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文研究了在边缘设备上使用内存高效的零阶优化(MeZO)进行微调的方法，相比传统反向传播(BP)训练，MeZO通过仅使用前向评估来估计梯度，消除了存储中间激活和优化器状态的需求，从而在内存受限环境下支持更大的模型部署。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要在严格内存约束下适应不同的智能任务，传统BP训练需要存储层激活和优化器状态，这在设备内存完全容纳模型权重的部署中严重限制了可部署的最大模型规模。

Method: 使用内存高效的零阶优化(MeZO)方法，仅通过前向评估来估计梯度，无需存储中间激活或优化器状态。首先提供BP和MeZO训练下可容纳模型大小的理论估计，然后进行数值验证。

Result: 数值验证表明，在设备内存约束下，只要有足够的微调时间，MeZO在准确性方面具有优势，能够显著增大可在片上内存中部署的模型规模。

Conclusion: MeZO通过消除存储中间激活和优化器状态的需求，缓解了边缘设备上的内存瓶颈，使得在内存受限环境下能够部署更大的模型进行微调，尽管可能需要更长的训练时间。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [49] [Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning](https://arxiv.org/abs/2511.11402)
*Amit Jain,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.LG

TL;DR: 提出基于Transformer的强化学习框架，统一多阶段航天器轨迹优化，通过单一策略架构处理动态不同的任务阶段，消除手动阶段转换需求。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法需要为不同任务阶段分别训练策略，限制了自适应性和增加了操作复杂性，需要能够跨动态不同阶段保持连贯记忆的统一框架。

Method: 基于近端策略优化(PPO)，用Transformer编码器-解码器结构替代传统循环网络，集成门控Transformer-XL(GTrXL)架构，在关键操作中保持秒到分钟时间跨度的连贯记忆。

Result: 在单阶段基准测试中达到接近最优性能，在多阶段航点导航变体中表现良好，在复杂多阶段火箭上升问题中有效学习跨动态不同阶段的连贯控制策略。

Conclusion: Transformer框架不仅匹配简单情况下的解析解，还能有效学习跨动态不同阶段的连贯控制策略，为可扩展自主任务规划奠定基础，减少对阶段特定控制器的依赖。

Abstract: Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [50] [Mem-α：通过<em class="highlight">强化学习</em>来学习记忆构建](http://mp.weixin.qq.com/s?__biz=Mzk3NTA3NzA1Nw==&mid=2247483894&idx=1&sn=be19f82c98ddd5dd72854d969b259586&chksm=c50d5477b832abef75ace6e6df739568fe16f1db2dd6356ae30279fe1cf77ec5bfc30d0cc62b#rd)
*懒惰小蜜蜂的AI蜜罐*

Main category: wechat.article

TL;DR: 1、强化学习框架（1）任务定义：Mem-α将记忆构建建模为了一个强化学习任务输入：一个多轮交互序列 C={c_1，c_2，…，c_n}，可视为连续的对话、文档片段或信息块（chunk）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1、强化学习框架（1）任务定义：Mem-α将记忆构建建模为了一个强化学习任务输入：一个多轮交互序列 C={c_1，c_2，…，c_n}，可视为连续的对话、文档片段或信息块（chunk）。

</details>


### [51] [<em class="highlight">强化学习</em>"四到位"  筑牢安全生产防线](http://mp.weixin.qq.com/s?__biz=MzI4NzY0NDk5NQ==&mid=2247634939&idx=2&sn=929ab72899fc50e14f8667c30a55a41b&chksm=eae121d22b3029b0b609dbda376023b29e78cd8a9f63a0fc109e83e7dfdbd9b00b00fcbd10c8#rd)
*印象新安煤矿*

Main category: wechat.article

TL;DR: 进入四季度以来，新安煤矿紧紧围绕安全生产实际，全面启动煤矿专业知识强化学习活动，通过“组织到位、宣传到位、培训到位、考核到位”工作举措，持续提升从业人员安全素养与专业技能，为矿井实现全年安全生产提供坚


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 进入四季度以来，新安煤矿紧紧围绕安全生产实际，全面启动煤矿专业知识强化学习活动，通过“组织到位、宣传到位、培训到位、考核到位”工作举措，持续提升从业人员安全素养与专业技能，为矿井实现全年安全生产提供坚

</details>


### [52] [DeepEyes：通过<em class="highlight">强化学习</em>激发“用图像思考”](http://mp.weixin.qq.com/s?__biz=MzkzNzkyOTczMw==&mid=2247484304&idx=1&sn=c9f7d154ddd76c5e2deabd595f5ad00b&chksm=c34dd0b5db37e4f63d532948ce1e8eb638bd1f497cfbeb78f192be54c382c9d01a1e959eec4d#rd)
*AI深学视界*

Main category: wechat.article

TL;DR: 这是一种通过端到端强化学习激励“用图像思考”能力的模型，而不需要冷启动SFT。值得注意的是，这种能力是在模型本身中固有的，利用其固有的基础能力作为一种工具，而不是依赖于单独的专用模型。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这是一种通过端到端强化学习激励“用图像思考”能力的模型，而不需要冷启动SFT。值得注意的是，这种能力是在模型本身中固有的，利用其固有的基础能力作为一种工具，而不是依赖于单独的专用模型。

</details>


### [53] [多智能体深度<em class="highlight">强化学习</em>综述：算法框架、核心挑战与前沿进展](http://mp.weixin.qq.com/s?__biz=MzYzMTIyODU1MA==&mid=2247483768&idx=1&sn=dcede30bc050ce26c59096af65753248&chksm=f1325dc11d9f4ced0f6ee0f707d35a30cd5c29f5df550bbb6a2ef55476abe5ad0b170541b77c#rd)
*呆塔科研竞赛助手*

Main category: wechat.article

TL;DR: 本文适合对强化学习基础有一定了解，希望深入多智能体方向的研究生读者。我们将重点分析训练架构设计、行为涌现机制以及应对非平稳性、信用分配等特有挑战的算法策略。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文适合对强化学习基础有一定了解，希望深入多智能体方向的研究生读者。我们将重点分析训练架构设计、行为涌现机制以及应对非平稳性、信用分配等特有挑战的算法策略。

</details>


### [54] [NeurIPS 2025 | <em class="highlight">强化学习</em>-相关论文12篇](http://mp.weixin.qq.com/s?__biz=Mzk0NzY3ODMxMA==&mid=2247488450&idx=1&sn=59385fe314573e532a209315ded66b58&chksm=c24241e0c4941c07f6595a16ef739beb22eff68ab933e83a935315ef8b991d22b65f9ff5d8ed#rd)
*AI新文*

Main category: wechat.article

TL;DR: 原文链接离线安全强化学习的在线优化原标题：Online Optimization for Offline Safe Reinforcement Learning作者：Yassine Chemingui；Aryan Deshwal；Alan Fern；Thanh Nguyen-Tang；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 原文链接离线安全强化学习的在线优化原标题：Online Optimization for Offline Safe Reinforcement Learning作者：Yassine Chemingui；Aryan Deshwal；Alan Fern；Thanh Nguyen-Tang；

</details>


### [55] [首篇基于<em class="highlight">强化学习</em>的Agentic Search最新综述！](http://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247488148&idx=1&sn=82934d68fbd3a7bcc0968d67768fec32&chksm=c03749801b93b02f1a91b3227f3a4670ce372295973ae6c88b0b8efd9376e6419afcd3edd523#rd)
*AIGC小白入门记*

Main category: wechat.article

TL;DR: 而这背后的核心驱动力，正是强化学习（Reinforcement Learning， RL）。ret rieval control trainable freeze tool & knowledge integration search search query final answer search llm engine llm engine pollicy policy think re aso...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而这背后的核心驱动力，正是强化学习（Reinforcement Learning， RL）。ret rieval control trainable freeze tool & knowledge integration search search query final answer search llm engine llm engine pollicy policy think re asoning single agent code execute r llm policy query optimiza

</details>


### [56] [热点头条|百度世界大会勾勒我国<em class="highlight">大模型</em>应用规模化落地新图景](http://mp.weixin.qq.com/s?__biz=MzAxMDA2Nzg5NA==&mid=2452403125&idx=1&sn=bdc5fee62e5e61ae407a5a8c20fcf7a6&chksm=8d8d9c0b50e799fe7524ee1b14ccad41ec6ebc2ec3eab2f5700a9d264bafe5acb5fcbe4427d4#rd)
*晒科网*

Main category: wechat.article

TL;DR: 11月13日，百度世界大会集中展示了我国大模型应用在自动驾驶、数字人、智能搜索和智能体等领域的规模化落地成效，多项核心指标实现实质性突破。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 11月13日，百度世界大会集中展示了我国大模型应用在自动驾驶、数字人、智能搜索和智能体等领域的规模化落地成效，多项核心指标实现实质性突破。

</details>


### [57] [利用<em class="highlight">大模型</em>发起网络安全攻击](http://mp.weixin.qq.com/s?__biz=MzI5NTQ3NzIxMw==&mid=2247485789&idx=1&sn=90ad50849bc3755550e7662c71848635&chksm=ed0b144bc499fe2a554a4e1e6a355fe7b2f823811e278652c9af7e5bbe693e5751616f762b53#rd)
*AI简化安全*

Main category: wechat.article

TL;DR: 一、大模型网络攻击的具体过程1.1 攻击架构设计攻击者构建了一个自主攻击框架，核心是将Claude Code和MCP工具结合使用。该框架将Claude作为编排系统，能够将复杂的多阶段攻击分解为离散的技术任务，包括漏洞扫描、凭证验证、


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一、大模型网络攻击的具体过程1.1 攻击架构设计攻击者构建了一个自主攻击框架，核心是将Claude Code和MCP工具结合使用。该框架将Claude作为编排系统，能够将复杂的多阶段攻击分解为离散的技术任务，包括漏洞扫描、凭证验证、

</details>


### [58] [专家观点丨<em class="highlight">大模型</em>重塑软件生态：机遇与挑战下的产业新路径](http://mp.weixin.qq.com/s?__biz=Mzk0MDUzOTcxNg==&mid=2247497472&idx=1&sn=66d3785529f07974dd74583541f94216&chksm=c3eb5852a22726a7f077f06cc91b50d1c62b0d64aa821f6b3f526b1b4af56397fe1b8b00ecf1#rd)
*CAICT人工智能*

Main category: wechat.article

TL;DR: 在模型技术层面，需重点突破长上下文理解与专业领域泛化等技术瓶颈，当前国内大模型少数支持256K上下文，大部分处于32K至128K之间，这对于处理复杂代码库、工程级项目、系统架构文档等场景时显然不够。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在模型技术层面，需重点突破长上下文理解与专业领域泛化等技术瓶颈，当前国内大模型少数支持256K上下文，大部分处于32K至128K之间，这对于处理复杂代码库、工程级项目、系统架构文档等场景时显然不够。

</details>


### [59] [每周AI<em class="highlight">大模型</em>更新速递11.10~11.16](http://mp.weixin.qq.com/s?__biz=MzAxNjYxOTY3NQ==&mid=2455622412&idx=1&sn=669b5e8d827503a40d57a79916bcee19&chksm=8d6d100595c2709ed2430b9fc6615e76b33703e9078dd094f92985ab967d0d08dfe302758482#rd)
*大模型评测及优化NoneLinear*

Main category: wechat.article

TL;DR: 大模型/agent评测技术交流：关注公众号，发送消息"进群"


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型/agent评测技术交流：关注公众号，发送消息"进群"

</details>


### [60] [AI<em class="highlight">大模型</em>开发核心技术栈：从框架到部署的全景解析](http://mp.weixin.qq.com/s?__biz=Mzg3Mzg5MjY3Nw==&mid=2247525222&idx=1&sn=49ac27b51502efcedb2171a7f3ba0b4d&chksm=cf2606fcd179863e35a613c94f55f222e1d3d1c62c3c4b0d4102e8b97b113bbfdcf3b8cc2008#rd)
*AIGC开放社区*

Main category: wechat.article

TL;DR: 它们专注于编排和调度大模型的能力，是引爆应用层创新的催化剂。理解这两层框架的特点与分工，是开发者构建现代AI应用的第一步。1.1 深度学习基础框架：三足鼎立，PyTorch王者地位稳固


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 它们专注于编排和调度大模型的能力，是引爆应用层创新的催化剂。理解这两层框架的特点与分工，是开发者构建现代AI应用的第一步。1.1 深度学习基础框架：三足鼎立，PyTorch王者地位稳固

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [61] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究调查了研究软件工程师对同行代码审查的看法，发现尽管同行代码审查对提高研究软件质量至关重要，但RSEs在实践中面临独特挑战，需要结构化流程、改进工具和针对性培训来提升采用率。


<details>
  <summary>Details</summary>
Motivation: 研究软件对科研发现至关重要，但需求变化、复杂输入和遗留依赖阻碍了软件质量和可维护性。虽然同行代码审查能提高软件质量，但其在研究软件工程师中的采用情况尚未被探索。

Method: 通过问卷调查收集研究软件工程师对同行代码审查的看法，调查设计与先前研究保持一致以进行对比分析，同时包含针对RSEs的额外问题。

Result: 收到61份有效问卷回复，发现结果与先前研究一致，同时揭示了RSEs相比更广泛开发者群体面临的独特挑战和实践差异。

Conclusion: 同行代码审查对提高研究软件质量、可维护性和可靠性至关重要。尽管RSEs面临独特挑战，但通过结构化流程、改进工具和针对性培训可以提升同行审查在研究软件开发中的采用率和有效性。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [62] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 提出了一种基于LLM和人工参与的补丁有效性判断方法，通过生成每个bug的评分标准，经过人工审查和优化后，使用LLM根据优化后的标准判断补丁有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化程序修复评估方法依赖执行测试，无法准确捕捉补丁的真实有效性，而人工标注成本高昂。

Method: 采用人机协作方法：首先用LLM生成每个bug的评分标准，经过一次性人工审查和优化，然后用LLM根据优化后的标准判断补丁有效性。

Result: 该方法与人工共识达成高度一致（Cohen's kappa 0.75），召回率0.94，精确率0.80。在包含人工评分不一致的完整数据集上表现有所下降但仍有改进空间。

Conclusion: 该方法能显著降低人工标注成本，在补丁有效性判断上达到与人工高度一致的水平，为自动化程序修复评估提供了可靠解决方案。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [63] [Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair](https://arxiv.org/abs/2511.11012)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 本文首次系统研究LLM驱动的代码代理在多段bug修复任务上的表现，评估了4种代理在372个多段bug上的修复能力，发现修复准确率从25.8%到93.3%不等，且随着bug分散度和复杂度增加而下降。


<details>
  <summary>Details</summary>
Motivation: 传统程序修复主要关注单段缺陷，而忽略了现实系统中普遍存在的多段bug。修复这些bug需要在多个不连续代码区域进行协调编辑，面临更大挑战。

Method: 在Hunk4J数据集的372个多段bug上评估4种LLM代码代理，使用细粒度指标分析1,488条修复轨迹，包括定位、修复准确率、回归行为和操作动态。开发了Maple工具为代理提供仓库级上下文。

Result: 修复准确率差异显著：Claude Code最高(93.3%)，Qwen Code最低(25.8%)。高表现代理展示出更好的语义一致性，实现正向回归减少，而低表现代理常引入新测试失败。失败修复消耗更多资源(39%-343%更多token)和更长时间(43%-427%)。Maple将Gemini-cli的修复准确率提升30%。

Conclusion: 通过细粒度指标和轨迹级分析，本研究不仅关注准确率，还解释了代码代理在多段修复过程中如何定位、推理和行动。

Abstract: Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.

</details>


### [64] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 研究如何在工业过程自动化领域使用LLM处理专有领域特定语言，通过少量样本提示方法解决简单问题，确保数据安全。


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究主要关注通用编程语言，工业自动化领域的专有语言支持不足，企业需要在不投入大量训练成本的情况下利用LLM。

Method: 使用少量样本提示方法，在专有领域特定语言上测试LLM性能，确保在本地部署以保护敏感数据。

Result: 少量样本提示方法足以解决专有语言中的简单问题，即使该语言在LLM中支持不佳。

Conclusion: 企业可以通过少量样本提示在本地部署LLM，有效处理专有领域语言问题，无需大规模模型训练。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>
