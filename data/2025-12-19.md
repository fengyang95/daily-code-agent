<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 19]
- [wechat.article](#wechat.article) [Total: 15]
- [tldr.article](#tldr.article) [Total: 12]
- [cs.SE](#cs.SE) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BRAID: Bounded Reasoning for Autonomous Inference and Decisions](https://arxiv.org/abs/2512.15959)
*Armağan Amcalar,Eyup Cinar*

Main category: cs.CL

TL;DR: BRAID提出了一种基于有界推理的提示框架，使用Mermaid指令图实现结构化推理，显著提升了LLM代理的推理准确性和成本效率。


<details>
  <summary>Details</summary>
Motivation: LLM在性能、成本和token使用之间存在非线性关系，当前自然语言扩展式的推理方式效率低下，需要一种结构化、可扩展的推理框架来优化自主代理系统的推理效率。

Method: BRAID采用有界推理框架，使用Mermaid指令图创建结构化、机器可读的提示，使模型能够进行结构化推理而非无界的自然语言token扩展。

Result: 在AdvancedIF、GSM-Hard和SCALE MultiChallenge基准测试中，BRAID显著提高了推理准确性和成本效率，证明其在不同GPT模型层级上的有效性。

Conclusion: BRAID是一种有效且可扩展的技术，可用于优化自主代理系统中的推理效率，结构化机器可读提示能大幅提升生产系统中代理的推理准确性和成本效益。

Abstract: Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.

</details>


### [2] [Are We on the Right Way to Assessing LLM-as-a-Judge?](https://arxiv.org/abs/2512.16041)
*Yuanning Feng,Sinan Wang,Zhengxiang Cheng,Yao Wan,Dongping Chen*

Main category: cs.CL

TL;DR: Sage是一个无需人工标注的LLM-as-a-Judge评估套件，通过局部自一致性和全局逻辑一致性来评估LLM法官的质量，发现当前顶尖LLM在近四分之一困难案例中存在偏好不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge基准主要依赖人工标注的真实标签，这引入了人类偏见，削弱了可靠性评估，并施加了可扩展性限制。需要一种无需人工标注的评估方法来克服这些限制。

Method: 基于理性选择理论的公理，引入两个新视角：局部自一致性（成对偏好稳定性）和全局逻辑一致性（完整偏好集的传递性）。构建了650个问题的数据集，结合结构化基准问题和真实用户查询。

Result: 实验表明Sage指标稳定且与监督基准高度相关。发现当前最先进的LLM作为法官存在显著可靠性问题，即使Gemini-2.5-Pro和GPT-5在近四分之一困难案例中无法保持一致的偏好。发现情境偏好的新现象，微调LLM-as-a-Judge、基于面板的法官和深度推理可以提升一致性。

Conclusion: Sage是一个可靠的无监督评估套件，揭示了当前LLM法官的可靠性问题。人类判断也存在显著不一致性，表明人工标注可能不是可靠的金标准。情境偏好解释了为什么明确的评分标准有助于模型保持一致性。

Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.

</details>


### [3] [JustRL: Scaling a 1.5B LLM with a Simple RL Recipe](https://arxiv.org/abs/2512.16649)
*Bingxiang He,Zekai Qu,Zeyuan Liu,Yinghao Chen,Yuxin Zuo,Cheng Qian,Kaiyan Zhang,Weize Chen,Chaojun Xiao,Ganqu Cui,Ning Ding,Zhiyuan Liu*

Main category: cs.CL

TL;DR: JustRL提出了一种极简的单阶段强化学习方法，使用固定超参数在1.5B推理模型上达到SOTA性能，同时计算量减半，挑战了当前RL训练中复杂多阶段流程的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的强化学习方法趋向于越来越复杂（多阶段训练、动态超参数调度、课程学习等），作者质疑这种复杂性是否必要，希望探索一种更简单有效的替代方案。

Method: 提出JustRL方法：单阶段训练、固定超参数、不使用复杂的技巧（如显式长度惩罚、鲁棒验证器）。在1.5B推理模型上进行实验，超参数在两个模型间无需调整即可迁移。

Result: 在两个1.5B推理模型上，在9个数学基准测试中分别达到54.9%和64.3%的平均准确率，计算量比复杂方法减少2倍。训练过程平滑单调改进4000+步，没有崩溃或平台期。消融实验显示添加"标准技巧"反而会降低性能。

Conclusion: 领域可能为了解决问题而增加复杂性，但这些问题在稳定、规模化的基线方法中会自然消失。作者发布模型和代码，为社区提供一个简单、经过验证的基线。

Abstract: Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \textbf{Is this complexity necessary?} We present \textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2$\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.

</details>


### [4] [AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.16883)
*Tzu-Han Lin,Wei-Lin Chen,Chen-An Li,Hung-yi Lee,Yun-Nung Chen,Yu Meng*

Main category: cs.CL

TL;DR: 提出AdaSearch框架，通过两阶段强化学习分离问题解决与搜索决策，提高LLM搜索代理的自知能力，减少不必要搜索调用，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM搜索代理存在过度依赖搜索的问题，导致成本增加和暴露于噪声/恶意内容的风险，而仅依赖参数知识又可能产生幻觉。需要开发能自适应平衡参数知识和外部搜索的代理。

Method: 提出AdaSearch框架：1）量化现有搜索代理的自知能力；2）采用两阶段强化学习框架，将问题解决与是否调用搜索的决策解耦；3）使决策过程显式和可解释。

Result: 在多个模型系列和规模上的实验表明，AdaSearch显著提高了知识边界意识，减少了不必要的搜索调用，保持了强大的任务性能，并提供更透明、可解释的决策行为。

Conclusion: AdaSearch通过解耦问题解决与搜索决策，有效解决了搜索代理的过度搜索问题，为金融和医疗等高风险领域提供了更透明、可解释的解决方案。

Abstract: Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction](https://arxiv.org/abs/2512.15751)
*Wei Guan,Jian Cao,Jinyu Cai,Qiqi Cai,Jianqi Gao,See-Kiong Ng*

Main category: cs.LG

TL;DR: GLOW是一个用于智能体工作流性能预测的统一框架，结合了GNN的图结构建模能力和LLM的推理能力，通过图导向的LLM提取拓扑感知语义特征，并与GNN编码的结构表示融合，在FLORA-Bench上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能体工作流（AWs）在解决复杂任务方面具有潜力，但其自动化生成的可扩展性受到执行评估高成本和延迟的严重限制。现有的AW性能预测方法无法同时捕捉AW中复杂的拓扑依赖关系和深层语义逻辑。

Method: 提出GLOW统一框架，结合GNN的图结构建模能力和LLM的推理能力。使用图导向的LLM（在图任务上指令调优）提取拓扑感知语义特征，与GNN编码的结构表示融合。采用对比对齐策略进一步优化潜在空间以区分高质量AW。

Result: 在FLORA-Bench上的大量实验表明，GLOW在预测准确性和排序效用方面优于最先进的基线方法。

Conclusion: GLOW通过统一图结构和语义建模，有效解决了AW性能预测中拓扑依赖和语义逻辑的联合建模问题，为AW自动化生成提供了高效评估方案。

Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.

</details>


### [6] [Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models](https://arxiv.org/abs/2512.15973)
*Caner Erden*

Main category: cs.LG

TL;DR: 提出DR-RL框架，通过强化学习和在线矩阵扰动理论，动态优化LLM中多头自注意力的低秩分解，在保持准确性的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 传统低秩近似方法通常采用静态秩假设，无法适应不同输入上下文的变化，限制了灵活性。需要一种能根据实时序列动态、层敏感性和硬件约束动态选择秩的方法。

Method: 使用强化学习代理将秩选择建模为序列策略优化问题，奖励函数平衡注意力保真度和计算延迟。采用在线矩阵扰动理论实现增量秩更新，避免推理时完全分解的高成本。结合轻量级Transformer策略网络和批处理SVD操作。

Result: 实验表明DR-RL在保持与全秩注意力统计等效的下游准确性的同时，显著减少浮点运算，尤其在长序列场景（L>4096）中效果明显。

Conclusion: 该工作填补了MHSA自适应效率和理论严谨性之间的空白，为资源受限深度学习中的秩缩减技术提供了有原则、数学基础扎实的替代方案。

Abstract: We propose Dynamic Rank Reinforcement Learning (DR-RL), a novel framework that adaptively optimizes the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models (LLMs) through the integration of reinforcement learning and online matrix perturbation theory. While traditional low-rank approximations often rely on static rank assumptions--limiting their flexibility across diverse input contexts--our method dynamically selects ranks based on real-time sequence dynamics, layer-specific sensitivities, and hardware constraints. The core innovation lies in an RL agent that formulates rank selection as a sequential policy optimization problem, where the reward function strictly balances attention fidelity against computational latency. Crucially, we employ online matrix perturbation bounds to enable incremental rank updates, thereby avoiding the prohibitive cost of full decomposition during inference. Furthermore, the integration of a lightweight Transformer-based policy network and batched Singular Value Decomposition (SVD) operations ensures scalable deployment on modern GPU architectures. Experiments demonstrate that DR-RL maintains downstream accuracy statistically equivalent to full-rank attention while significantly reducing Floating Point Operations (FLOPs), particularly in long-sequence regimes (L > 4096). This work bridges the gap between adaptive efficiency and theoretical rigor in MHSA, offering a principled, mathematically grounded alternative to heuristic rank reduction techniques in resource-constrained deep learning. Source code and experiment logs are available at: https://github.com/canererden/DR_RL_Project

</details>


### [7] [INTELLECT-3: Technical Report](https://arxiv.org/abs/2512.16144)
*Prime Intellect Team,Mika Senghaas,Fares Obeid,Sami Jaghouar,William Brown,Jack Min Ong,Daniel Auras,Matej Sirovatka,Jannik Straube,Andrew Baker,Sebastian Müller,Justus Mattern,Manveer Basra,Aiman Ismail,Dominik Scherm,Cooper Miller,Ameen Patel,Simon Kirsten,Mario Sieg,Christian Reetz,Kemal Erdem,Vincent Weisser,Johannes Hagemann*

Main category: cs.LG

TL;DR: INTELLECT-3是一个106B参数的MoE模型，通过大规模强化学习训练，在数学、代码、科学和推理基准上达到同尺寸模型的最佳性能，并开源了完整的RL基础设施栈。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够在数学、代码、科学和推理任务上达到前沿性能的高效模型，同时开源完整的强化学习基础设施以促进社区发展。

Method: 使用混合专家架构（106B参数，12B激活），基于GLM-4.5-Air-Base模型进行SFT和RL训练，开发了prime-rl框架支持大规模异步强化学习，最高扩展到512个H200 GPU。

Result: INTELLECT-3在其尺寸范围内在数学、代码、科学和推理基准上达到最先进性能，超越了多个更大的前沿模型。

Conclusion: 通过大规模强化学习训练和优化的基础设施栈，可以构建出在多个领域具有竞争力的高效模型，开源完整基础设施将促进强化学习在智能体训练方面的发展。

Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.

</details>


### [8] [Emergent Bias and Fairness in Multi-Agent Decision Systems](https://arxiv.org/abs/2512.16433)
*Maeve Madigan,Parameswaran Kamalaruban,Glenn Moynihan,Tom Kempton,David Sutton,Stuart Burrell*

Main category: cs.LG

TL;DR: 该论文提出多智能体预测系统的公平性评估方法，通过大规模模拟揭示金融决策中无法追溯到单个智能体的涌现偏见模式，强调多智能体系统需作为整体实体而非组件分析进行评估。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在预测任务中表现出性能优势，但缺乏有效的公平性评估方法，导致在高风险金融领域部署存在风险。金融决策中的偏见可能直接导致监管违规和财务损失，因此需要开发专门的多智能体公平性评估方法。

Method: 开发多智能体预测系统的公平性评估方法，在金融表格数据领域测量系统公平性特征。通过大规模模拟实验，考察不同多智能体配置（包括不同通信和协作机制）下的公平性指标，分析涌现偏见的模式。

Result: 研究发现金融决策中存在无法追溯到单个智能体组件的涌现偏见模式，表明多智能体系统可能表现出真正的集体行为。公平性风险是多智能体系统模型风险的重要组成部分，对信用评分和收入估计等任务有实际影响。

Conclusion: 多智能体决策系统必须作为整体实体进行评估，而不是通过对其组成组件的还原主义分析。公平性风险是金融多智能体系统模型风险的重要部分，需要系统性评估方法。

Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.

</details>


### [9] [Meta-RL Induces Exploration in Language Agents](https://arxiv.org/abs/2512.16848)
*Yulun Jiang,Liangze Jiang,Damien Teney,Michael Moor,Maria Brbic*

Main category: cs.LG

TL;DR: LaMer是一个元强化学习框架，让LLM智能体能够在测试时主动探索环境并从反馈中学习，显著提升在需要探索的任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL训练的LLM智能体在需要主动探索的任务上表现不佳，难以从试错经验中高效适应。需要一种方法让智能体能够在测试时主动探索并学习环境反馈。

Method: LaMer包含两个关键组件：(1)跨回合训练框架以鼓励探索和长期奖励优化；(2)通过反思进行上下文策略适应，使智能体能够从任务反馈信号中适应策略而无需梯度更新。

Result: 在多个环境中，LaMer相比RL基线有显著性能提升：Sokoban提升11%，MineSweeper提升14%，Webshop提升19%。在更具挑战性或未见任务上也表现出更好的泛化能力。

Conclusion: 元强化学习为语言智能体提供了诱导探索的原则性方法，通过学习探索策略实现对新颖环境更鲁棒的适应。

Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.

</details>


### [10] [Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning](https://arxiv.org/abs/2512.16911)
*Andrew Wagenmaker,Perry Dong,Raymond Tsao,Chelsea Finn,Sergey Levine*

Main category: cs.LG

TL;DR: 论文提出后验行为克隆（PostBC）方法，通过建模演示者的后验分布来改进预训练策略，使其更适合RL微调，相比标准行为克隆能显著提升微调性能。


<details>
  <summary>Details</summary>
Motivation: 当前实践中，通常先在大规模演示数据集上预训练策略，然后用RL微调。虽然微调算法得到广泛研究，但如何确保预训练策略是RL微调的有效初始化却很少关注。标准行为克隆（BC）可能无法覆盖演示者的所有动作，影响后续RL微调效果。

Method: 提出后验行为克隆（PostBC）方法：不直接拟合观察到的演示动作，而是训练策略来建模给定演示数据集下演示者行为的后验分布。这确保了策略覆盖演示者的所有动作，同时保持与BC相当的预训练性能。PostBC仅需标准监督学习，可与现代生成模型结合实现。

Result: 理论证明BC可能无法覆盖演示者动作，而PostBC能确保这种覆盖。在机器人控制基准测试和真实世界机器人操作任务中，PostBC相比标准BC能显著提升RL微调性能。

Conclusion: PostBC提供了一种简单有效的预训练方法，能产生更适合RL微调的初始化策略，在保持预训练性能的同时显著改善微调效果，适用于实际机器人控制任务。

Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying](https://arxiv.org/abs/2512.15776)
*Shaun Baek,Sam Liu,Joseph Ukpong*

Main category: cs.AI

TL;DR: 该论文研究了LLM在具身环境中的符号接地问题，特别是信息不对称分布时的特权信息偏见。通过AI2-THOR中的非对称辅助推理框架，发现领导者能感知目标但团队成功率低，主动查询协议比标准指令更有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为强大的推理引擎，在具身环境中面临"符号接地"问题，特别是在信息不对称分布时。研究者关注"特权信息偏见"现象，即知识丰富的"领导者"代理由于缺乏心智理论而无法有效指导传感器受限的"跟随者"。

Method: 提出了一个新颖的非对称辅助推理框架，在AI2-THOR环境中进行实验。设计了"领导者-跟随者"协作场景，比较了"推送式"指令和"拉取式"主动查询协议的效果。

Result: 实验显示显著的"成功差距"：领导者能感知目标的占35.0%，但协作团队成功率仅17.0%，意味着近50%可行计划因通信接地错误而失败。主动查询协议比标准指令更稳健，成功场景中澄清请求频率是2倍。

Conclusion: 研究揭示了主动不确定性减少机制作为安全人机协作和机器人间协作的前提条件。拉取式协议通过主动查询能有效缓解特权信息偏见问题。

Abstract: Large Language Models (LLMs) act as powerful reasoning engines but struggle with "symbol grounding" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or "Curse of Knowledge"), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant "Success Gap": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a "Pull-based" protocol (active querying) is significantly more robust than standard "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.

</details>


### [12] [Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM](https://arxiv.org/abs/2512.15784)
*Zibin Liu,Cheng Zhang,Xi Zhao,Yunfei Feng,Bingyu Bai,Dahu Feng,Erhu Feng,Yubin Xia,Haibo Chen*

Main category: cs.AI

TL;DR: MOBIMEM是一个内存中心的LLM代理系统，通过三种专用内存原语（Profile、Experience、Action Memory）和操作系统风格的服务，实现无需模型重训练的代理自我进化，显著提升移动环境中的任务成功率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前以模型为中心的代理架构在部署后难以自我进化，需要持续模型重训练/微调，这带来高昂计算成本，且存在模型准确性与推理效率之间的固有权衡。

Method: 提出MOBIMEM内存中心代理系统：1) 三种专用内存原语：Profile Memory使用轻量级距离图结构对齐用户偏好；Experience Memory使用多级模板实例化新任务执行逻辑；Action Memory记录细粒度交互序列。2) 操作系统风格服务：调度器协调并行子任务和内存操作；AgentRR机制实现安全高效动作重用；上下文感知异常处理确保优雅恢复。

Result: 在AndroidWorld和top-50应用上评估：实现83.1%的配置文件对齐，检索时间23.83毫秒（比GraphRAG基线快280倍）；任务成功率提升高达50.3%；移动设备端到端延迟降低高达9倍。

Conclusion: MOBIMEM通过内存中心架构成功解决了LLM代理自我进化的挑战，无需模型重训练即可实现个性化、能力扩展和效率提升，为移动环境中的代理部署提供了实用解决方案。

Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.

</details>


### [13] [Subjective functions](https://arxiv.org/abs/2512.15948)
*Samuel J. Gershman*

Main category: cs.AI

TL;DR: 论文提出"主观函数"概念作为高阶目标函数，研究期望预测误差作为具体示例，探讨智能体如何内生地合成目标函数。


<details>
  <summary>Details</summary>
Motivation: 人类智能能够动态合成新的目标函数，但人工智能系统缺乏这种能力。论文旨在探索目标函数的来源、如何选择追求的目标，以及如何赋予人工系统类似的动态目标合成能力。

Method: 提出"主观函数"概念作为高阶目标函数，该函数内生于智能体（基于智能体自身特征而非外部任务定义）。以期望预测误差作为具体的主观函数示例进行研究。

Result: 论文建立了主观函数的概念框架，展示了期望预测误差作为主观函数的可行性，并将该框架与心理学、神经科学和机器学习中的相关思想联系起来。

Conclusion: 主观函数为理解智能体如何内生地合成目标函数提供了理论框架，有望推动人工智能系统获得类似人类的目标动态合成能力。

Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.

</details>


### [14] [Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting](https://arxiv.org/abs/2512.16022)
*Defu Cao,Michael Gee,Jinbo Liu,Hengxuan Wang,Wei Yang,Rui Wang,Yan Liu*

Main category: cs.AI

TL;DR: 该论文提出了一种新方法，将大语言模型重新定位为智能裁判，用于评估、解释和协调时间序列基础模型的集成，通过R1风格微调和SHAP引导训练，在GIFT-Eval基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型众多，但没有单一方法能始终表现最优，核心挑战在于如何构建最优集成并保持可解释性。虽然大语言模型具有强大的推理能力，但直接应用于时间序列预测效果不佳。

Method: 将LLM重新定位为智能裁判，通过R1风格的微调过程（使用SHAP忠实度分数指导），教会模型将集成权重解释为关于时间动态的有意义因果陈述。训练后的代理通过多轮对话进行前瞻性评估，提供因果解释，并自适应优化策略。

Result: 在GIFT-Eval基准测试的23个数据集、97种设置上验证，该方法在CRPS和MASE指标上显著优于领先的时间序列基础模型，建立了新的最先进结果。

Conclusion: 通过将LLM重新定位为智能集成协调器，并引入领域特定的微调方法，成功解决了时间序列集成优化的挑战，实现了性能提升和可解释性的双重目标。

Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.

</details>


### [15] [WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning](https://arxiv.org/abs/2512.16108)
*Wendong Bi,Yirong Mao,Xianglong Liu,Kai Tian,Jian Zhang,Hanjie Wang,Wenhui Que*

Main category: cs.AI

TL;DR: 提出了WeMusic-Agent训练框架，通过知识内化和智能边界学习，让LLM在对话式音乐推荐中智能决定何时使用内化知识、何时调用外部工具，并构建了开源基准。


<details>
  <summary>Details</summary>
Motivation: 现有对话式音乐推荐方法难以平衡专业领域知识和灵活工具集成，需要既能理解用户偏好和音乐上下文，又能智能决策何时使用内化知识、何时调用外部工具。

Method: 提出WeMusic-Agent训练框架，结合知识内化和智能边界学习；开发WeMusic-Agent-M1模型，在500亿音乐相关语料上持续预训练内化音乐知识，同时学习调用外部工具（如音乐检索API、推荐系统）；构建基于微信听书真实数据的开源对话音乐推荐基准。

Result: 在真实数据上的实验表明，WeMusic-Agent相比现有模型取得显著改进，基准支持多维度评估（相关性、个性化、多样性）。

Conclusion: WeMusic-Agent框架有效解决了对话式音乐推荐中知识内化与工具调用的平衡问题，提出的模型和基准为领域发展提供了重要基础。

Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.

</details>


### [16] [ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs](https://arxiv.org/abs/2512.16149)
*Hao Chen,Zhexin Hu,Jiajun Chai,Haocheng Yang,Hang He,Xiaohan Wang,Wei Lin,Luhang Wang,Guojun Yin,Zhuofeng zhao*

Main category: cs.AI

TL;DR: ToolForge：无需真实API调用的自动化工具调用数据合成框架，仅需少量虚拟工具即可生成高质量多跳推理数据，8B参数模型在多项基准上超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: 现有工具调用数据生成方法依赖大量真实API调用，成本高昂且缺乏多跳推理和自反思能力，需要更高效、低成本的数据合成方案

Method: 基于(问题、黄金上下文、答案)三元组构建虚拟工具，通过多跳推理和自反思机制合成大规模工具学习数据，采用多层验证框架确保数据质量

Result: 仅用8B参数的模型在合成数据上训练后，在多个基准测试中超越GPT-4o，证明该方法能有效生成高质量工具调用数据

Conclusion: ToolForge提供了一种无需真实API调用的高效数据合成框架，显著降低工具调用模型训练成本，同时提升多跳推理能力

Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .

</details>


### [17] [PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving](https://arxiv.org/abs/2512.16214)
*Jianming Liu,Ren Zhu,Jian Xu,Kun Ding,Xu-Yao Zhang,Gaofeng Meng,Cheng-Lin Liu*

Main category: cs.AI

TL;DR: 提出PDE-Agent：首个基于LLM驱动多智能体协作的PDE求解框架，通过工具链增强实现从自然语言描述到自动化求解


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解方法依赖人工设置和领域专业知识，现有PINN和DeepXDE等方法仍需要专家知识且缺乏完全自主性，需要更自动化的解决方案

Method: 1) Prog-Act框架：基于图记忆的多智能体协作，通过双循环机制（局部修复和全局修订）实现动态规划和错误纠正；2) 资源池：集成工具-参数分离机制，集中管理运行时工件并解决工具间依赖关系

Result: 开发了PDE-Bench基准测试，验证PDE-Agent在复杂多步骤、跨步骤依赖任务中表现出优越的适用性和性能

Conclusion: 工具链增强的多智能体PDE求解新范式将推动自动化科学计算的未来发展

Abstract: Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.

</details>


### [18] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: 提出了AMUSE基准测试来评估多模态大语言模型在多说话人对话场景中的代理推理能力，并开发了RAFT框架通过奖励优化和选择性参数适应来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（如GPT-4o、Qwen3-Omni）在感知方面表现强劲，但在多说话人对话场景中表现不佳，这些场景需要代理推理能力来跟踪说话者、维持角色和跨时间基础事件。这些场景对多模态音频-视频理解至关重要，如对话视频助手和会议分析应用。

Method: 1. 引入AMUSE基准测试，围绕需要代理推理的任务设计，要求模型将复杂的音频-视觉交互分解为规划、基础和反思步骤；2. 在三种模式（零样本、引导、代理）和六个任务家族中评估MLLMs；3. 提出RAFT框架，集成奖励优化与内在多模态自我评估作为奖励，并进行选择性参数适应以实现数据和参数高效更新。

Result: 当前模型在所有模式下都表现出弱的多说话人推理能力和不一致的行为。使用RAFT框架，在基准测试上实现了高达39.52%的相对准确率提升。

Conclusion: AMUSE和RAF共同为检验多模态模型中的代理推理能力并提升其性能提供了实用平台。

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [19] [Learning to Wait: Synchronizing Agents with the Physical World](https://arxiv.org/abs/2512.16262)
*Yifei She,Ping Zhang,He Liu,Yanmin Jia,Yang Jing,Zijun Liu,Peng Sun,Xiangbin Li,Xiaohe Hu*

Main category: cs.AI

TL;DR: 提出一种代理端方法，让LLM能够主动对齐其认知时间线与物理世界，通过预测等待时间（time.sleep(t)）来同步异步环境，减少查询开销和执行延迟。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的代理任务通常涉及具有可变延迟的非阻塞操作，这与同步MDP不同，存在"时间间隙"问题。现有环境端解决方案（如阻塞包装器或频繁轮询）要么限制可扩展性，要么用冗余观察稀释代理的上下文窗口。

Method: 提出代理端方法，将代码即行动范式扩展到时间领域，利用语义先验和上下文学习来预测精确的等待持续时间（time.sleep(t)），使代理能够与异步环境同步而无需详尽检查。

Result: 在模拟Kubernetes集群中的实验表明，代理能够精确校准其内部时钟，最小化查询开销和执行延迟，验证了时间感知是可学习的能力。

Conclusion: 时间感知是代理在开放环境中自主演进的关键可学习能力，代理端方法能够有效解决异步环境中的时间同步问题。

Abstract: Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.

</details>


### [20] [QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems](https://arxiv.org/abs/2512.16279)
*Yiliu Yang,Yilei Jiang,Qunzhong Wang,Yingshui Tan,Xiaoyong Zhu,Sherman S. M. Chow,Bo Zheng,Xiangyu Yue*

Main category: cs.AI

TL;DR: QuadSentinel是一个四智能体安全防护系统，将自然语言安全策略编译为机器可检查的规则，通过状态跟踪器、策略验证器、威胁监视器和裁判四个组件在线执行安全策略，提高LLM智能体任务执行时的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在执行复杂任务时存在安全风险，而部署者用自然语言编写的安全策略存在模糊性和上下文依赖性，难以映射到机器可检查的规则，导致运行时安全执行不可靠。

Method: 将安全策略表达为序列，构建四智能体防护系统：状态跟踪器监控智能体状态，策略验证器将策略编译为机器可检查的规则，威胁监视器检测潜在威胁，裁判通过高效的top-k谓词更新器优先检查并分层解决冲突，降低计算成本。

Result: 在ST-WebAgentBench和AgentHarm基准测试中，QuadSentinel提高了护栏准确性和规则召回率，同时减少了误报。相比ShieldAgent等单智能体基线，提供了更好的整体安全控制。

Conclusion: QuadSentinel模式可以在不修改核心智能体的情况下被近期部署采用，通过保持策略分离和机器可检查性来增强安全性。该模式为LLM智能体安全防护提供了有效的解决方案。

Abstract: Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.

</details>


### [21] [OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models](https://arxiv.org/abs/2512.16295)
*Zhenyu Wu,Jingjing Xie,Zehao Li,Bowen Yang,Qiushi Sun,Zhaoyang Liu,Zhoumianze Liu,Yu Qiao,Xiangyu Yue,Zun Wang,Zichen Ding*

Main category: cs.AI

TL;DR: OS-Oracle提出了一种用于GUI计算机使用代理的批评模型，通过可扩展的数据管道、两阶段训练范式和新基准，显著提升了GUI代理的决策可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着VLM驱动的计算机使用代理在GUI导航和操作方面能力增强，可靠的步骤级决策成为实际部署的关键瓶颈。在长流程工作流中，错误会快速累积，不可逆操作可能导致意外后果，因此需要批评模型在执行前评估每个动作。

Method: 1) 跨平台GUI批评数据的可扩展合成管道；2) 结合监督微调(SFT)和一致性保持组相对策略优化(CP-GRPO)的两阶段训练范式；3) OS-Critic Bench基准，用于评估移动、Web和桌面平台的批评模型性能。

Result: 构建了包含31万批评样本的高质量数据集，OS-Oracle-7B模型在OS-Critic Bench上达到开源VLM的最先进性能，在移动领域超越专有模型，作为预批评器能提升UI-TARS-1.5-7B等原生GUI代理在OSWorld和AndroidWorld环境中的性能。

Conclusion: OS-Oracle通过系统化的数据合成、训练方法和评估基准，有效解决了GUI代理步骤级决策的可靠性问题，为计算机使用代理的实际部署提供了重要支持。

Abstract: With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.

</details>


### [22] [Adaptation of Agentic AI](https://arxiv.org/abs/2512.16301)
*Pengcheng Jiang,Jiacheng Lin,Zhiyi Shi,Zifeng Wang,Luxi He,Yichen Wu,Ming Zhong,Peiyang Song,Qizheng Zhang,Heng Wang,Xueqiang Xu,Hanwen Xu,Pengrui Han,Dylan Zhang,Jiashuo Sun,Chaoqi Yang,Kun Qian,Tian Wang,Changran Hu,Manling Li,Quanzheng Li,Hao Peng,Sheng Wang,Jingbo Shang,Chao Zhang,Jiaxuan You,Liyuan Liu,Pan Lu,Yu Zhang,Heng Ji,Yejin Choi,Dawn Song,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 本文提出了一个系统化框架，统一了智能体AI系统中的适应机制，涵盖智能体适应和工具适应两大类别，并进一步细分为不同信号形式，为构建更强大、高效、可靠的智能体AI系统提供概念基础和实践路线图。


<details>
  <summary>Details</summary>
Motivation: 随着基于基础模型的智能体AI系统能力不断增强、应用范围不断扩大，适应机制成为提升性能、可靠性和泛化能力的关键手段。然而，快速扩展的研究领域缺乏系统化的框架来统一理解各种适应策略，这阻碍了系统设计和策略选择的效率。

Method: 提出了一个系统化框架，将智能体AI适应研究统一为两大类别：智能体适应和工具适应。进一步将智能体适应分解为工具执行信号驱动和智能体输出信号驱动两种形式，将工具适应分解为智能体无关和智能体监督两种形式。通过该框架分析设计空间、明确权衡取舍，并提供策略选择和切换的实践指导。

Result: 该框架帮助澄清了智能体AI中适应策略的设计空间，使各种策略的权衡取舍变得明确，为系统设计中的策略选择或切换提供了实用指导。通过回顾每个类别的代表性方法，分析了它们的优势和局限性，并突出了关键开放挑战和未来机会。

Conclusion: 本文为研究人员和实践者构建更强大、高效、可靠的智能体AI系统提供了概念基础和实践路线图。统一的适应框架有助于系统化理解当前研究进展，指导未来技术发展方向。

Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

</details>


### [23] [Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference](https://arxiv.org/abs/2512.16317)
*Arther Tian,Alex Ding,Frank Chen,Alan Wu,Aaron Chan,Bruce Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种成本感知的PoQ框架，将显式效率测量整合到推理节点和评估节点的奖励机制中，通过平衡质量与成本来支持经济可持续的去中心化LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化LLM推理验证方法难以扩展到现代模型，而原始的PoQ方法忽略了推理节点和评估节点之间的异构计算成本差异，需要一种能考虑成本因素的改进方案。

Method: 提出成本感知PoQ框架，整合了三种评估方法：基础真值标记级F1、轻量级学习评估器和GPT判断；采用线性奖励函数平衡归一化质量和成本；在抽取式问答和抽象摘要任务上测试了5个LLM和3个评估模型。

Result: 语义文本相似性双编码器比交叉编码器与基础真值和GPT分数相关性更高；质量-成本分析显示池中最大模型在单位延迟质量方面最有效；蒙特卡洛模拟表明成本感知奖励方案能持续奖励高质量低成本节点。

Conclusion: 成本感知PoQ为经济可持续的去中心化LLM推理提供了实用基础，评估器架构是关键设计选择，最大模型在质量成本效率方面表现最佳。

Abstract: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.
  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.

</details>


### [24] [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465)
*Jinwu Chen,Qidie Wu,Bin Li,Lin Ma,Xin Si,Yang Hu,Shouyi Yin,Jun Yang*

Main category: cs.AI

TL;DR: cuPilot是一个策略协调的多智能体框架，通过引入策略作为中间语义表示来优化CUDA内核，在100个内核基准测试中平均比PyTorch快3.09倍。


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化需要硬件-软件协同设计专业知识且高性能内核库具有专有性，现有LLM结合进化算法的方法由于智能体设计不佳和进化表示不匹配而性能不足。

Method: 提出策略协调的多智能体框架cuPilot，引入策略作为内核进化的中间语义表示，包括策略协调进化算法、屋顶线引导提示和策略级种群初始化。

Result: 在100个内核基准测试中平均比PyTorch快3.09倍，在GEMM任务中展示了复杂的优化并实现了关键硬件单元的高利用率。

Conclusion: cuPilot通过策略协调的多智能体框架有效解决了现有内核优化方法的局限性，实现了显著的性能提升。

Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.

</details>


### [25] [From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment](https://arxiv.org/abs/2512.16532)
*Himanshu Gharat,Himanshi Agrawal,Gourab K. Patro*

Main category: cs.AI

TL;DR: 研究记忆增强型个性化AI代理在招聘场景中的偏见问题，发现即使使用安全训练的LLM，偏见仍会通过个性化过程被系统性地引入和强化。


<details>
  <summary>Details</summary>
Motivation: 虽然记忆增强的个性化AI代理能提升任务连续性和响应相关性，但这种个性化可能引入偏见风险。现有研究主要关注ML和LLM的偏见，但对记忆增强个性化代理的偏见问题缺乏探索。

Method: 以招聘为用例，模拟记忆增强个性化代理的行为，研究偏见是否以及如何在各个操作阶段被引入和放大。使用安全训练的LLM构建代理进行实验。

Result: 实验发现，即使使用安全训练的LLM，偏见仍会通过个性化过程被系统性地引入和强化，强调需要在记忆增强的LLM代理中增加保护措施或防护机制。

Conclusion: 记忆增强的个性化AI代理存在偏见风险，需要额外的保护措施或代理防护机制来减轻偏见在个性化过程中的引入和放大。

Abstract: Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.

</details>


### [26] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 提出Generative Adversarial Reasoner框架，通过对抗强化学习联合训练推理器和判别器，使用计算高效的审查机制提供密集的步骤级奖励，提升LLM数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管具有显式推理能力的大语言模型在数学推理方面表现出色，但仍存在过程错误，如计算错误、逻辑脆弱和表面合理但无效的步骤。需要改进推理质量。

Method: 提出Generative Adversarial Reasoner框架，包含推理器和判别器，通过对抗强化学习联合训练。使用计算高效的审查机制将推理链划分为逻辑完整的片段，判别器评估每个片段的合理性并提供结构化理由。推理器因逻辑一致且得出正确答案的步骤获得奖励，判别器因正确检测错误或区分推理过程获得奖励。

Result: 在多个数学基准测试中取得一致提升。在AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提升到61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7（+10.0）。

Conclusion: 该方法通过密集、校准良好的步骤级奖励补充稀疏的精确匹配信号，改进了信用分配，提高了样本效率，增强了LLM的整体推理质量。模块化判别器还支持灵活的奖励塑造。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [27] [Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning](https://arxiv.org/abs/2512.16698)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Mohammad Nehad Alam,Proma Hossain Progga,Swakkhar Shatabda*

Main category: cs.AI

TL;DR: 多智能体设计在几何问题求解中并非总是最优，开源模型受益明显而闭源模型在传统基准上单智能体表现更好


<details>
  <summary>Details</summary>
Motivation: 研究多智能体与单智能体在几何问题求解中的性能差异，探索多智能体设计的实际效益

Method: 在四个视觉数学基准（Geometry3K、MathVerse、OlympiadBench、We-Math）上系统比较单智能体与多智能体流水线性能

Result: 开源模型（如Qwen-2.5-VL）在多智能体模式下性能显著提升，而闭源模型（Gemini-2.0-Flash）在传统基准上单智能体表现更好，仅在较新数据集上有适度改进

Conclusion: 多智能体流水线对开源模型有明显优势，对强闭源系统在新基准上有辅助作用，但智能体分解并非普遍最优策略

Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver

</details>


### [28] [Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems](https://arxiv.org/abs/2512.16707)
*Abhisek Ganguly*

Main category: cs.AI

TL;DR: 论文形式化了算法智能的两个独立计算限制：形式不完备性和动态不可预测性，并证明这两种极端情况共同限制了智能体对自身预测能力进行推理的能力。


<details>
  <summary>Details</summary>
Motivation: 研究算法智能的固有计算限制，特别是智能体在推理自身预测能力时面临的根本约束。旨在澄清智能系统中推理、预测和自我分析之间的内在权衡关系。

Method: 形式化两种独立计算限制：形式不完备性（限制一致推理系统的演绎能力）和动态不可预测性（限制有限精度下的长期预测）。分析这两种极端情况如何共同约束智能体的自我预测能力推理。

Result: 证明算法智能体通常无法计算自身的最大预测边界，揭示了智能系统中推理、预测和自我分析之间的固有权衡关系。

Conclusion: 算法智能存在根本的计算限制，智能体无法完全理解自身的预测能力，这为智能系统的设计和分析提供了重要的理论边界。

Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.

</details>


### [29] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体的协调协作实现，而非单一系统。为此需要超越个体对齐，建立分布式的AGI安全框架，通过虚拟智能体沙盒经济来管理智能体间的交易和协调风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究主要关注单一AI系统的防护，假设最终会出现单一的通用人工智能（AGI）。但另一种可能性是：通用能力首先通过多个子AGI智能体的协调协作实现。随着具备工具使用、通信协调能力的AI智能体快速部署，这种"拼凑式AGI"假说需要被认真考虑，并开发相应的安全保障措施。

Method: 提出一个分布式的AGI安全框架，超越对个体智能体的评估和对齐。该框架的核心是设计和实现虚拟智能体沙盒经济（不可渗透或半渗透），其中智能体间的交易由稳健的市场机制管理，并配合适当的可审计性、声誉管理和监督，以减轻集体风险。

Result: 论文提出了一个概念性框架，用于应对分布式AGI系统的安全挑战。该框架强调通过经济沙盒机制来管理智能体间的交互，而不是仅仅关注个体智能体的对齐问题。

Conclusion: "拼凑式AGI"假说需要被认真对待，并应指导相应安全保障措施的发展。分布式的AGI安全框架通过虚拟智能体沙盒经济提供了一种管理智能体间协调风险的方法，这是当前AI安全研究需要优先考虑的方向。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [30] [LLMs-based <em class="highlight">code</em> <em class="highlight">agent</em>工作速览](http://mp.weixin.qq.com/s?__biz=MzIwMDU4Nzg0Mw==&mid=2247594758&idx=1&sn=36c1411a8d9f59c49405ae1c7d52a744&chksm=97a9d8b063897b03cc8a838a295e0a80de4cdb3353ef1faa1d0d14d09c638c5cd998bad6613f#rd)
*RUC AI Box*

Main category: wechat.article

TL;DR: Introducing KAT-Dev-32B， KAT-Coder： Advancing Code Intelligence through Scalable Agentic RL论文链接：https：//kwaipilot.github.io/KAT-Coder/核心工作：快手团队发布了KAT系列两个模型——开源的KAT-Dev-32B（32B参数）与性能更强的KAT-Coder。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Introducing KAT-Dev-32B， KAT-Coder： Advancing Code Intelligence through Scalable Agentic RL论文链接：https：//kwaipilot.github.io/KAT-Coder/核心工作：快手团队发布了KAT系列两个模型——开源的KAT-Dev-32B（32B参数）与性能更强的KAT-Coder。

</details>


### [31] [从信息匹配到智能规划，<em class="highlight">Agentic</em> RAG的架构实现](http://mp.weixin.qq.com/s?__biz=Mzg3MTExOTcwNA==&mid=2247485149&idx=1&sn=f1f967d44f5941c85935c8ff31010c88&chksm=cfb254a62089aad69f6d1a62a036ff2e0aa7e7f3f6657fab4339edd2306a6260f1983def7a53#rd)
*深度学习机器*

Main category: wechat.article

TL;DR: 在深入研究具体代码实现之前，我们有必要先退后一步，从更高维度审视一个典型的 Agentic RAG 系统在架构层面需要哪些组件，以及它们是如何协同工作的。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在深入研究具体代码实现之前，我们有必要先退后一步，从更高维度审视一个典型的 Agentic RAG 系统在架构层面需要哪些组件，以及它们是如何协同工作的。

</details>


### [32] [2026年AI预测：自主<em class="highlight">智能体</em>（<em class="highlight">Agentic</em> AI）将接管企业？现实可能是一场“混合战”](http://mp.weixin.qq.com/s?__biz=MzkyNjYzNTIwNw==&mid=2247494203&idx=1&sn=b4205a49844c935a482277a40b4b53a9&chksm=c356205d3d16ccd7833d557e3932fea4e67dc49b7dc8a528438554c1ce97e73c6039bb2ca46a#rd)
*CIOCDO*

Main category: wechat.article

TL;DR: IDC预测：到2026年，全球2000强企业中 40%的工作角色将涉及“自主智能体”（Agentic AI）的协作。这表明，我们正处于一个从“实验”到“生产”的艰难跨越期。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: IDC预测：到2026年，全球2000强企业中 40%的工作角色将涉及“自主智能体”（Agentic AI）的协作。这表明，我们正处于一个从“实验”到“生产”的艰难跨越期。

</details>


### [33] [67页<em class="highlight">Agentic</em> AI综述：13大机构发布，4大核心范式与未来路线图！](http://mp.weixin.qq.com/s?__biz=MzU4MjkxMTgwMQ==&mid=2247492787&idx=1&sn=fd3f2425156c90b64b14846461891247&chksm=fcf8970e9dff3a9df96620335d5e22ea292c13800a06d0292f63022ed74199c9450b3a779603#rd)
*AI研究*

Main category: wechat.article

TL;DR: 近日，来自加州理工、斯坦福、伯克利、佐治亚理工等 13 所顶尖机构的研究人员联合发表了一篇重磅综述，首次系统性地提出了 Agentic AI 适配 的统一框架。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近日，来自加州理工、斯坦福、伯克利、佐治亚理工等 13 所顶尖机构的研究人员联合发表了一篇重磅综述，首次系统性地提出了 Agentic AI 适配 的统一框架。

</details>


### [34] [<em class="highlight">Agentic</em> Commerce正在重构商业底层逻辑——从4,700%的流量增长，看AI<em class="highlight">代理</em>如何成为新的商业入口](http://mp.weixin.qq.com/s?__biz=MzkwNDMwMzUxNw==&mid=2247503360&idx=1&sn=84d3512edbe0ff433626dae0ee3846af&chksm=c16df233efa7f2ecdcc8f114770b8e4ecb2339370c9f7a2d35e527f41f126e5ff85eb960b889#rd)
*AIGC前线*

Main category: wechat.article

TL;DR: 但Agentic Commerce的本质差异在于：它不仅改变了流量来源，更改变了交易决策权的归属。SEO → GEO → ACO：三代可见性逻辑时代优化目标核心能力商业影响


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 但Agentic Commerce的本质差异在于：它不仅改变了流量来源，更改变了交易决策权的归属。SEO → GEO → ACO：三代可见性逻辑时代优化目标核心能力商业影响

</details>


### [35] [技术实践 | 构建你的第一个 Snowflake <em class="highlight">智能体</em>：从概念到原型只需 3 步！](http://mp.weixin.qq.com/s?__biz=MzYzNTIwNzIwMw==&mid=2247483915&idx=3&sn=407b8778964c9c9df97037cdc7847f2e&chksm=f1b351cb02fdf51327fdef396d99cb331930ff4e8026b43caf4a467c5d6f82298c0b43481960#rd)
*Snowflake AI数据云*

Main category: wechat.article

TL;DR: Agentic AI 远不止是最新的热门词汇，它更是一种颠覆性技术！想象这样一个场景：您的数据不再被动等待您提出正确问题，而是持续主动地为您工作。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 远不止是最新的热门词汇，它更是一种颠覆性技术！想象这样一个场景：您的数据不再被动等待您提出正确问题，而是持续主动地为您工作。

</details>


### [36] [<em class="highlight">Agentic</em> AI 时代的岗位与能力重构：从麦肯锡研究看未来HR的三条必修课](http://mp.weixin.qq.com/s?__biz=MzU3OTYxODc2Mg==&mid=2247489321&idx=1&sn=993ce3a434a01a62674943571483e9e0&chksm=fc7f4bceefaa1e8bb0a3b0a8fa103324017b3d759ab92476c4647c8b318412372ba1773b12c4#rd)
*跟雷哥探究HR领域的AI*

Main category: wechat.article

TL;DR: Agentic AI 时代，真正需要写清的是“这项工作由人和 Agent 如何共同完成”。麦肯锡在银行、保险、运营等前线场景里的案例，有一个共同特点：不是简单地“用 AI 替代一部分人”，而是先把岗位拆成任务，再为每类任务匹配最


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 时代，真正需要写清的是“这项工作由人和 Agent 如何共同完成”。麦肯锡在银行、保险、运营等前线场景里的案例，有一个共同特点：不是简单地“用 AI 替代一部分人”，而是先把岗位拆成任务，再为每类任务匹配最

</details>


### [37] [2025<em class="highlight">智能体</em>爆发元年：关于 <em class="highlight">Agentic</em> AI 崛起、可视化编程素养与超级个体进化的深度研究报告](http://mp.weixin.qq.com/s?__biz=MzkxMTYwMjIxOA==&mid=2247483891&idx=2&sn=4734685aee87a6826dce25e542aa9bd7&chksm=c0d6d78b5795459a58729d4b3cb4a430c71dbc897340398c29c80b58b3774e8048632a3f1eb4#rd)
*凯乐杂谈*

Main category: wechat.article

TL;DR: 第一部分：Agentic AI 的崛起与 2025 年的新工作范式1.1 从“聊天机器人”到“虚拟同事”的演进在过去两年中，公众对 AI 的认知主要停留在“聊天机器人”（Chatbot）的层面——即用户输入提示词（Prompt），AI 输出文本或图像。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 第一部分：Agentic AI 的崛起与 2025 年的新工作范式1.1 从“聊天机器人”到“虚拟同事”的演进在过去两年中，公众对 AI 的认知主要停留在“聊天机器人”（Chatbot）的层面——即用户输入提示词（Prompt），AI 输出文本或图像。

</details>


### [38] [【解决方案】从“死磕提示词”到“<em class="highlight">Agentic</em> AI”：重新定义AI问题解决范式](http://mp.weixin.qq.com/s?__biz=MzI3NDI4MzIyNQ==&mid=2247517425&idx=4&sn=fac08770c3585eae31c97dc73074fe49&chksm=ea9d1f0216732514919c7c923be5c57966c71e644f7b6123ecb01f03db74513462ac9e7d36b1#rd)
*产业智能官*

Main category: wechat.article

TL;DR: 从五大模式的演进中，我们能清晰看到 Agentic AI 的底层逻辑 ——重要的不是 “答案本身”，而是 “答案如何形成”。这种思维革命，对技术从业者与企业决策者的启示远超技术范畴：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从五大模式的演进中，我们能清晰看到 Agentic AI 的底层逻辑 ——重要的不是 “答案本身”，而是 “答案如何形成”。这种思维革命，对技术从业者与企业决策者的启示远超技术范畴：

</details>


### [39] [从助手到执行者：豆包<em class="highlight">大模型</em>领跑Agent时代](http://mp.weixin.qq.com/s?__biz=MzAxNjM3NzYyNg==&mid=2651705775&idx=1&sn=b371809bc9ebf403827c9cca94981903&chksm=81c5613c9438b1d28bb036655e804310f15da12f27b6cb7a653c884db99bb76f2e05d55b2ea5#rd)
*科技明说*

Main category: wechat.article

TL;DR: 当日，豆包大模型升级至1.8版本，同步推出的还有音视频创作模型Seedance 1.5 Pro。权威评测显示，豆包大模型在多模态理解、生成能力及Agent能力上，已跻身全球第一梯队。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 当日，豆包大模型升级至1.8版本，同步推出的还有音视频创作模型Seedance 1.5 Pro。权威评测显示，豆包大模型在多模态理解、生成能力及Agent能力上，已跻身全球第一梯队。

</details>


### [40] [豆包<em class="highlight">大模型</em>1.8发布，Seedance模型同步升级](http://mp.weixin.qq.com/s?__biz=MzkzMjY0ODg3OA==&mid=2247497037&idx=2&sn=6728c8dc1e92c90c2ff814f68ed2bf36&chksm=c347782976dcef94490f8db5284350dcdb22f11362be93772ff8d3b356305fb39aa28bccd1b6#rd)
*百数朝智网*

Main category: wechat.article

TL;DR: 豆包大模型1.8多模态Agent能力媲美全球顶尖模型豆包大模型1.8（Doubao-Seed-1.8）面向多模态Agent场景进行了定向优化。其工具调用能力、复杂指令遵循能力及OS Agent能力显著增强，大幅提升了模型在处理复杂任务时的规划与执行水平


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 豆包大模型1.8多模态Agent能力媲美全球顶尖模型豆包大模型1.8（Doubao-Seed-1.8）面向多模态Agent场景进行了定向优化。其工具调用能力、复杂指令遵循能力及OS Agent能力显著增强，大幅提升了模型在处理复杂任务时的规划与执行水平

</details>


### [41] [豆包<em class="highlight">大模型</em>日均调用量突破50万亿tokens](http://mp.weixin.qq.com/s?__biz=MzA4MzIyMTkzNQ==&mid=2650079047&idx=1&sn=227bfc9701a8a6393e1344a318b5eb60&chksm=86de59701afc92487c289b23a7f611a54b1e1a94d129b15ba3dcfbcd93f3cc6e4e75e04459bb#rd)
*中经e商圈*

Main category: wechat.article

TL;DR: 豆包大模型1.8显著增强了工具调用能力（Tool Use）和复杂指令遵循能力、GUI Agent能力等。这意味着模型不再被动等待指令，而是能够像一个经验丰富的项目经理一样，对复杂任务的规划、执行、流程理解能力，更适合用来开发处


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 豆包大模型1.8显著增强了工具调用能力（Tool Use）和复杂指令遵循能力、GUI Agent能力等。这意味着模型不再被动等待指令，而是能够像一个经验丰富的项目经理一样，对复杂任务的规划、执行、流程理解能力，更适合用来开发处

</details>


### [42] [本周看点 | 小米发布MiMo模型；火山引擎总裁：明年<em class="highlight">大模型</em>市场将扩10倍；OpenAI发布GPT-5.2-Codex](http://mp.weixin.qq.com/s?__biz=Mzk4ODI1ODMyOQ==&mid=2247485076&idx=1&sn=8b952034e8e1224f495d97340a20c9c2&chksm=c480a3cfa1523d97b9fc272b7c2d49981a3bba40778bff2f95fda5061e3acf1bfc21fac90576#rd)
*奥斯比千模加速器*

Main category: wechat.article

TL;DR: 导语大模型技术正急速迭代，行业浪潮波澜壮阔却又信息纷杂。洞察趋势、把握动向，是在AI时代保持竞争力的关键。奥斯比看产业，带您一起回顾本周内大模型产业的技术突破、商业动态与生态信号。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 导语大模型技术正急速迭代，行业浪潮波澜壮阔却又信息纷杂。洞察趋势、把握动向，是在AI时代保持竞争力的关键。奥斯比看产业，带您一起回顾本周内大模型产业的技术突破、商业动态与生态信号。

</details>


### [43] [终结“AI模型选择焦虑”：RNA预训练<em class="highlight">大模型</em>测评与分析Benchmark](http://mp.weixin.qq.com/s?__biz=Mzg4OTY5OTUxOQ==&mid=2247493160&idx=1&sn=a9c0b8164003fe7b73fa1202966ecb7e&chksm=ceae16fbe8c705d57f83ae078997eb0c9bb2f5764b4c68a0e7c1fd6e90bf82d4d68e352901aa#rd)
*AI For Bioinformatics*

Main category: wechat.article

TL;DR: 同时，研究发现以往“模型越大越好”的观点并不绝对成立。例如，与应用场景语义适配的预训练数据，以及编码方式同样会对模型性能产生明显的影响。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 同时，研究发现以往“模型越大越好”的观点并不绝对成立。例如，与应用场景语义适配的预训练数据，以及编码方式同样会对模型性能产生明显的影响。

</details>


### [44] [刚刚，OpenAI最强编程<em class="highlight">大模型</em>发布！](http://mp.weixin.qq.com/s?__biz=Mzk0MDY2ODM3NQ==&mid=2247488554&idx=1&sn=23c738b956b6d681f64442a008aa4a29&chksm=c3e1b9c4ffc4a7874adb3194133e3d3371f77d52a6eb0ca8d9c6d8cdcf1a9acfdfaf0eccdae9#rd)
*AI大模型前沿*

Main category: wechat.article

TL;DR: 新模型的发布获得了人们的普遍关注。在开发者社区人们认为，如果说 Claude Code 擅长「原始代码」，那么 Codex/GPT5.x 在仔细、系统地查找「问题」（无论是代码问题还是数学问题）方面则是无可匹敌的。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 新模型的发布获得了人们的普遍关注。在开发者社区人们认为，如果说 Claude Code 擅长「原始代码」，那么 Codex/GPT5.x 在仔细、系统地查找「问题」（无论是代码问题还是数学问题）方面则是无可匹敌的。

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [45] [Prompt Injection Inside GitHub Actions: The New Frontier of Supply Chain Attacks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.aikido.dev%2Fblog%2Fpromptpwnd-github-actions-ai-agents%3Futm_source=tldrinfosec/1/0100019b2ca3ad8b-71774bf9-7c87-462e-ac41-812b4a62a85d-000000/iPo2vcWvrHwc_3EuZN1sjzoGIfvOb0QHbMuSXgQS294=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: PromptPwnd是一种影响GitHub Actions和GitLab CI/CD管道的新漏洞类别，攻击者通过向AI代理注入恶意提示来执行特权工具和窃取密钥


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在CI/CD管道中的广泛使用，出现了新的供应链攻击向量。攻击者可以利用提示注入漏洞，通过提交恶意输入来操纵AI代理执行危险操作

Method: 研究分析了GitHub Actions和GitLab CI/CD管道中AI代理的使用模式，识别了提示注入漏洞的机制。攻击者通过issue、pull request或commit message中的不受信任输入，将恶意指令注入到AI代理的提示中

Result: 发现了PromptPwnd这一新的漏洞类别，攻击者可以借此让AI代理执行特权工具、泄露GCP密钥等敏感信息，对供应链安全构成严重威胁

Conclusion: AI代理在CI/CD管道中的使用引入了新的安全风险，需要开发新的安全措施来防止提示注入攻击，确保供应链安全

Abstract: Prompt Injection Inside GitHub Actions: The New Frontier of Supply Chain Attacks (11 minute read) PromptPwnd is a new vulnerability class that affects GitHub Actions and GitLab CI/CD pipelines that use AI agents like Gemini CLI, Claude Code, and OpenAI Codex. In this vulnerability, untrusted user input from issues, pull requests, or commit messages is injected into prompts, leading to the execution of privileged tools. The attack allows secret exfiltration by tricking AI agents into leaking G...

</details>


### [46] [What Data Do Coding Agents Send, and Where to?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchasersystems.com%2Fblog%2Fwhat-data-do-coding-agents-send-and-where-to%2F%3Futm_source=tldrinfosec/1/0100019b2ca3ad8b-71774bf9-7c87-462e-ac41-812b4a62a85d-000000/08FeAtPm734PLh5W6EorfEvjtpRvGSDv9DDn2FfFs8o=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 研究人员测试了7个AI编程助手的数据收集行为，发现它们会发送代码片段、环境信息、API密钥等敏感数据到外部服务器，存在安全风险


<details>
  <summary>Details</summary>
Motivation: 随着AI编程助手日益普及，安全团队需要了解这些工具收集什么数据以及数据流向何处，以评估潜在的安全风险

Method: Chaser Systems创建测试环境，对7个编程助手进行测试，包括启动代理、标签自动补全、创建新功能、git提交推送、运行测试、上传数据到0x0.st等操作，并检查对AWS凭证和项目外文件的访问

Result: 测试发现编程助手会收集并发送代码片段、环境信息、API密钥等敏感数据到外部服务器，存在数据泄露风险

Conclusion: AI编程助手存在显著的数据安全风险，需要加强安全防护措施和用户隐私保护

Abstract: What Data Do Coding Agents Send, and Where to? (8 minute read) As AI coding agents become more common, security teams should understand what data these tools collect and where it goes. Chaser Systems created a test environment with seven coding agents, testing actions such as starting the agent, tab autocomplete, creating new features, committing and pushing to a git repository, running tests, and uploading data to 0x0.st. They also checked access to AWS credentials and files outside the proj...

</details>


### [47] [Announcing Xint Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftheori.io%2Fblog%2Fannouncing-xint-code%3Futm_source=tldrinfosec/1/0100019b2ca3ad8b-71774bf9-7c87-462e-ac41-812b4a62a85d-000000/VGcZA-AvaDMfN6HZibii8HT2VyFWJFdy-ArtLGpef3g=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Xint Code是一款AI驱动的代码分析工具，在ZeroDay Cloud竞赛中自动发现了Redis、PostgreSQL和MariaDB的关键0-day RCE漏洞，表现优于所有人工团队


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具存在误报率高、需要人工干预等问题，需要开发能够自动发现关键漏洞的AI代码分析工具

Method: 通过AI技术分析源代码、配置文件和二进制文件，无需打包或环境配置要求，自动生成可操作的漏洞报告

Result: 在ZeroDay Cloud竞赛中横扫数据库类别，发现了Redis、PostgreSQL和MariaDB的关键0-day RCE漏洞，误报率显著低于传统静态分析工具

Conclusion: Xint Code展示了AI驱动的代码分析在自动发现关键漏洞方面的强大能力，为代码安全分析提供了新的解决方案

Abstract: Announcing Xint Code (3 minute read) Theori's Xint Code, an AI-powered code analysis tool, discovered critical 0-day RCE vulnerabilities in Redis, PostgreSQL, and MariaDB with zero human intervention at ZeroDay Cloud, sweeping the database category and outperforming all human teams. The tool analyzes source code, configuration files, and binaries without packaging or harnessing requirements, producing actionable reports with dramatically fewer false positives than traditional static analysis ...

</details>


### [48] [Inside NVIDIA Nemotron 3](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.nvidia.com%2Fblog%2Finside-nvidia-nemotron-3-techniques-tools-and-data-that-make-it-efficient-and-accurate%2F%3Futm_source=tldrai/1/0100019b2cc2bdb3-fda2ebfa-4ed1-46c8-b637-36eb81368d85-000000/6u_JiRPbyaxv1QaASC4LRK49sts0RyRtWMoLKVUNREU=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: NVIDIA Nemotron 3 是一个开源的混合 Mamba-Transformer MoE 架构，专为智能体AI设计


<details>
  <summary>Details</summary>
Motivation: 开发一个专门为智能体AI优化的架构，结合Mamba的高效序列建模能力和Transformer的注意力机制，通过混合专家(MoE)模式实现更好的性能

Method: 采用混合Mamba-Transformer架构，结合状态空间模型(Mamba)和Transformer注意力机制，使用混合专家(MoE)设计来提高模型容量和效率

Result: 创建了一个专门为智能体AI任务优化的开源架构，能够高效处理序列数据并具备强大的推理能力

Conclusion: Nemotron 3 的混合架构为智能体AI提供了强大的基础，结合了不同模型的优势，有望在智能体任务中取得优异表现

Abstract: Inside NVIDIA Nemotron 3 (18 minute read) Nemotron 3 is an open hybrid Mamba-Transformer MoE architecture designed for agentic AI.

</details>


### [49] [Building an Answering Machine](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmotherduck.com%2Fblog%2Fanalytics-agents%2F%3Futm_source=tldrdata/1/0100019b3124efab-b970dcee-538b-4e90-88eb-4d137cc0b4d5-000000/VOQ-Wd8ZOphdvy21GA2jT_8f4DCggWFTIB8Nfh8072M=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: MotherDuck推出Answering Machine，让非技术用户能用自然语言查询真实业务数据，通过智能代理方法而非单一SQL猜测，提供可靠的自助分析服务。


<details>
  <summary>Details</summary>
Motivation: 解决非技术用户难以使用SQL查询真实、杂乱业务数据的问题，让任何人都能通过自然语言提问获得可靠分析结果，实现真正的自助分析。

Method: 采用智能代理方法：探索数据表、运行查询、检查结果、迭代优化，模拟人类分析师的工作流程，而非猜测单一SQL查询。

Result: 开发出实用的自助分析工具，非技术用户可通过ChatGPT、Claude或Gemini等平台用自然语言提问，获得可靠答案，无需SQL知识。

Conclusion: 智能代理方法能有效解决非技术用户分析真实业务数据的难题，提供可靠的自助分析体验，具有实际应用价值。

Abstract: Building an Answering Machine (14 minute read) MotherDuck's new Answering Machine lets anyone ask plain-English questions about real, messy data from ChatGPT, Claude, or Gemini and get reliable answers without SQL. It works by using an agentic approach that explores tables, runs queries, checks results, and iterates like a human analyst, rather than guessing a single query. The result is practical self-service analytics that even non-technical users can trust on real business data.

</details>


### [50] [Agents that don't suck](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FS3PHje%3Futm_source=tldrdata/1/0100019b3124efab-b970dcee-538b-4e90-88eb-4d137cc0b4d5-000000/3d-pKx7okeFGkZFe8AcQj6ckcZoUcFg8uXD2LueRdJI=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Bricks是一个帮助构建、评估和优化基于独特数据的AI代理的平台，通过自动评估、目标评分和人类反馈来改进代理性能


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在现实世界中表现不佳的问题，提供系统化的方法来构建和优化能够实际工作的AI代理

Method: 提供平台工具，自动评估代理输出，根据用户目标进行评分，并通过人类反馈持续改进代理性能

Result: 帮助开发者构建出能够在现实世界中有效工作的AI代理，提供清晰的生产部署路径

Conclusion: Agent Bricks平台通过系统化的评估和优化流程，能够帮助开发者构建出真正实用的AI代理

Abstract: Agents that don't suck (Sponsor) Agent Bricks helps you build, evaluate and optimize AI agents grounded in your unique data. It evaluates automatically, scores outputs against your goals and improves with human feedback — giving you a clearer path to production. Build agents that work in the real world. See why it's worth your time

</details>


### [51] [LLM Optimization Made Simple: A Beginner's Guide to Ax](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fax-llm%2Fax%2Fblob%2Fmain%2Fdocs%2FOPTIMIZE.md%23-5-minute-quick-start%3Futm_source=tldrdata/1/0100019b3124efab-b970dcee-538b-4e90-88eb-4d137cc0b4d5-000000/L32J4q5Rd75ujBliJVQRUDZL6tjZDuPeIvJN_TrEHd0=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta的Ax框架简化了LLM程序优化，通过自动指令和few-shot演示调优，仅需3-5个标注样本和1-2分钟即可将准确率从70%提升到90%，同时降低成本约80%


<details>
  <summary>Details</summary>
Motivation: LLM优化通常复杂且耗时，需要大量手动调优。现有方法对初学者不友好，且成本高昂。需要一种简单高效的自动化优化方案

Method: 使用Ax框架自动化优化LLM程序，包括指令调优和few-shot演示优化。采用teacher-student工作流程，将优化结果保存为AxOptimizedProgram用于生产部署

Result: 仅需3-5个标注样本，优化过程约1-2分钟，可将准确率从70%提升到90%，同时降低成本约80%。优化结果可复现并用于生产环境

Conclusion: Ax框架为LLM优化提供了简单高效的解决方案，降低了优化门槛，使初学者也能快速获得高质量的优化结果，适合生产环境部署

Abstract: LLM Optimization Made Simple: A Beginner's Guide to Ax (GitHub Repo) Meta's Ax framework streamlines optimization of LLM “programs” for tasks like classification via automatic instruction + few-shot demo tuning. With as few as 3 to 5 labeled examples and a metric, optimization typically runs ~1-2 minutes and can move accuracy from 70% to 90% while cutting costs by ~80%. The optimized result is saved as an AxOptimizedProgram for reproducible production rollout. A teacher-student workflow claim...

</details>


### [52] [The State of AI Coding 2025](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fstate-of-ai-coding-2025%3Futm_source=tldrnewsletter/1/0100019b3133cccb-119e512b-446d-4e62-b481-12b088b83bdb-000000/8-G4GUPVA8f8O2ac9EDWp0vtXnTSYW0iWxtv61uV1Wc=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Greptile发布的2025年AI编码现状报告显示，AI编码工具在软件开发中带来生产力提升，OpenAI仍是领先模型提供商但差距正在缩小，代码输出量普遍增加。


<details>
  <summary>Details</summary>
Motivation: 该报告旨在分析AI编码工具在软件开发中的最新趋势和行业应用情况，为开发者、团队和企业提供关于AI编码工具采纳、性能表现和实际影响的跨行业研究数据。

Method: 采用跨行业研究方法，分析开发工作流程中的生产力提升、AI工具采纳情况、模型发展趋势，以及延迟、成本和标记化等方面的性能表现，同时涵盖基础模型和应用的最新论文。

Result: 研究发现各团队的代码输出量普遍增加，OpenAI仍然是领先的模型提供商，但与其他提供商的差距正在缩小。报告还提供了关于AI工具采纳率、性能指标和成本效益的具体数据。

Conclusion: AI编码工具正在显著提升软件开发效率，行业竞争格局正在发生变化，OpenAI的领导地位面临挑战，AI编码技术将继续推动软件开发模式的变革。

Abstract: The State of AI Coding 2025 (10 minute read) Greptile's State of AI Coding report is a cross-industry study on recent trends in AI software development. It covers productivity gain across development workflows, AI tool adoption, model growth trends, performance across latency, cost, and tokenization, and recent papers on foundational models and applications. Code output has increased across teams. OpenAI is still the leading model provider, but the gap is closing.

</details>


### [53] [Design and Code Your Landing Page in Minutes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpageai.pro%2F%3Futm_source=tldrdesign/1/0100019b3190c124-ac8dc70c-8e0a-47ad-930d-067d804861c5-000000/FUzWTD683lbHgNVjOK01tbCMLaV1dTErYOLByknPOXA=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 一个AI驱动的网站生成工具，通过单次提示快速创建生产就绪的着陆页，支持点击定制和灵活部署


<details>
  <summary>Details</summary>
Motivation: 传统网站开发过程耗时且需要技术专业知识，用户希望快速获得定制化的生产就绪网站，无需复杂编码过程

Method: 使用AI技术将自然语言提示转换为完整的网站代码，提供可视化编辑界面进行点击式定制，支持多种部署选项

Result: 能够在几分钟内生成功能完整的着陆页，用户可以通过简单交互完成定制，并直接下载或部署到各种平台

Conclusion: 该工具显著降低了网站创建的技术门槛和时间成本，使非技术用户也能快速获得专业质量的网站

Abstract: Design and Code Your Landing Page in Minutes (Website) Get a production-ready website from a single prompt. Customize it with a click, then download or deploy anywhere.

</details>


### [54] [OWASP Social OSINT Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbm-github%2Fowasp-social-osint-agent%3Futm_source=tldrinfosec/1/0100019b31cb83ee-e98945bc-f116-4b47-a82b-c9a72a6e923a-000000/jEJWFvJwc-ZcVTW6Aw2eSjQyxn_34WlW7PjLD7ICI7I=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OWASP Social OSINT Agent是一个用于开源情报调查的智能体，利用支持文本和视觉的LLM通过OpenAI兼容API收集、分析和综合单个或多个社交媒体账户的用户活动


<details>
  <summary>Details</summary>
Motivation: 解决OSINT（开源情报）调查中需要从多个社交媒体平台收集和分析用户活动的挑战，传统方法效率低下且难以整合多模态信息

Method: 开发一个智能体系统，利用支持文本和视觉的LLM（通过OpenAI兼容API），能够处理多个社交媒体账户，自动收集用户活动数据，进行分析和综合

Result: 创建了一个功能完整的OSINT调查工具，能够跨平台收集和分析社交媒体数据，支持多模态信息处理，提高了OSINT调查的效率和准确性

Conclusion: OWASP Social OSINT Agent是一个有效的OSINT调查工具，通过结合先进的LLM技术，显著提升了社交媒体情报收集和分析的能力

Abstract: OWASP Social OSINT Agent (GitHub Repo) OWASP Social OSINT Agent is an agent designed for OSINT investigations that leverages both text and vision-capable LLMs via any OpenAI-compatible API to gather, analyze, and synthesize user activity across single or multiple social media accounts.

</details>


### [55] [Klarna gives merchants the tools for discovery by AI agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FA7HKCN/1/0100019b31d332a1-7d121465-eacf-469b-aecf-45cdf73e1086-000000/1sNaWBb5njOGJe1OM0-mstLtRSEqdqzJ7KUMJsCfqOg=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Klarna推出Agentic Product Protocol，为AI代理提供超过1亿产品和4亿价格的实时结构化数据流，支持跨商户、市场和平台的商品发现、比较和推荐


<details>
  <summary>Details</summary>
Motivation: 为AI代理系统提供实时、结构化的商品数据，解决AI代理在电子商务中发现、比较和推荐真实商品时面临的数据访问问题

Method: 开发Agentic Product Protocol协议，提供标准化的实时数据流，包含超过1亿产品和4亿价格，覆盖12个市场

Result: 建立了AI代理访问实时商品数据的基础设施，使AI系统能够基于实时价格和库存信息进行商品发现和推荐

Conclusion: Agentic Product Protocol为AI代理在电子商务中的实际应用提供了关键的数据基础设施支持

Abstract: Klarna gives merchants the tools for discovery by AI agents (3 minute read) Klarna's Agentic Product Protocol gives AI systems access to a live, structured feed of more than 100 million products and 400 million prices standardized across 12 markets. The protocol establishes a foundation for agents to find, compare, and recommend real products with live prices and availability, across merchants, markets, and platforms.

</details>


### [56] [Stripe launches tech to help firms sell through AI agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fi9eNbj/1/0100019b31d332a1-7d121465-eacf-469b-aecf-45cdf73e1086-000000/W3TSClKUis2nBH0ZvpGAw6gBfg2xoMn_2jyug9d9TDU=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Stripe推出Agentic Commerce Suite，帮助零售商通过多个AI代理进行销售


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在商业领域的兴起，企业需要能够通过多个AI代理进行销售的技术解决方案，以提升销售效率和覆盖范围

Method: Stripe开发了Agentic Commerce Suite技术套件，为零售商提供通过多个AI代理进行销售的技术基础设施

Result: Stripe已经与多家主要零售商签约，采用其新的Agentic Commerce Suite进行AI代理销售

Conclusion: AI代理在商业销售中的应用正在加速，Stripe的技术解决方案为零售商提供了通过多个AI代理进行销售的能力

Abstract: Stripe launches tech to help firms sell through AI agents (2 minute read) Stripe has signed up a host of major retail players to its new Agentic Commerce Suite for selling through multiple AI agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [57] [CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory](https://arxiv.org/abs/2512.15813)
*Nishant Gaurav,Adit Akarsh,Tejas Ravishankar,Manoj Bajaj*

Main category: cs.SE

TL;DR: CodeMem提出了一种通过代码实现程序性记忆的架构，用于构建可重复使用的确定性可靠智能体工作流，解决现有工具使用AI代理在重复任务中的概率不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前工具使用AI代理存在动作空间有限、上下文效率低下和概率不稳定性等问题，不适合处理重复任务。虽然之前的工作通过使用Python作为动作空间解决了前两个问题，但概率不稳定性仍然存在，导致相同任务在不同执行中产生不同轨迹。

Method: CodeMem通过代码实现程序性记忆，构建可重复使用的智能体工作流架构。该架构能够存储和执行确定性的工作流程，确保任务执行的一致性和可靠性。

Result: 论文提出了CodeMem架构，能够实现确定性可靠的智能体工作流，解决了现有代理在重复任务中的概率不稳定性问题，提高了任务执行的可靠性和一致性。

Conclusion: 通过代码实现的程序性记忆是解决智能体工作流概率不稳定性的有效方法，CodeMem架构为构建可靠、可重复使用的自动化工作流提供了解决方案。

Abstract: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.

</details>


### [58] [Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls](https://arxiv.org/abs/2512.16272)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky*

Main category: cs.SE

TL;DR: 研究分析了LLM作为代码生成评估器(LaaJ)在COBOL现代化场景中的局限性，发现其会遗漏领域关键错误，通过开发轻量级分析检查器提供提示，结合LLM+Hints配置可将错误检测覆盖率从45%提升至94%。


<details>
  <summary>Details</summary>
Motivation: LLM作为代码评估器(LaaJ)虽然可扩展性强，但在实际工业应用中（特别是COBOL代码现代化场景）存在局限性，会遗漏领域特定的关键错误，这引发了对其在关键评估任务中可靠性的担忧。

Method: 1) 分析生成的COBOL程序和LaaJ评估结果，基于专家知识构建初步分类法；2) 开发轻量级分析检查器工具，可标记30多种实践中观察到的领域特定问题；3) 将检查器输出作为分析提示动态注入到评估器提示中，引导LaaJ重新审视可能忽略的方面；4) 在100个程序测试集上使用四个生产级LaaJ进行实验。

Result: LaaJ单独仅能检测约45%的错误，分析检查器单独缺乏解释深度。当结合使用时，LaaJ+Hints配置在最佳评估器和注入提示下达到94%的覆盖率，并产生质量更高、更准确的解释，显著提升了评估可靠性。

Conclusion: 分析-LLM混合方法可以显著增强部署管道中的评估可靠性，证明了结合领域特定分析工具与LLM评估器的有效性。

Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.

</details>


### [59] [Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse](https://arxiv.org/abs/2512.16790)
*Aaron Imani,Mohammad Moshirpour,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 该研究首次对LLM在软件工程任务中的内部评论表示进行概念级可解释性分析，发现LLM将评论作为独立潜在概念内化，并能区分不同类型评论，通过激活/停用这些概念可显著影响模型性能（-90%到+67%）。


<details>
  <summary>Details</summary>
Motivation: 尽管评论是源代码的非功能性元素，但大型语言模型在执行软件工程任务时经常依赖它们。然而，模型内部这种依赖的具体位置及其如何影响性能仍不清楚，需要深入理解LLM在SE任务中的内部工作机制。

Method: 使用概念激活向量（CAV）分析LLM内部评论表示，研究三个任务（代码补全、翻译、优化）。通过系统性地激活和停用嵌入空间中的评论概念，测量性能变化。还进行了控制实验，使用相同代码输入让LLM执行10个SE任务，测量潜在表示中评论概念的激活强度。

Result: LLM不仅将评论内化为独特的潜在概念，还能区分Javadocs、内联和多行评论等子类型。激活/停用评论概念导致显著、模型特定且任务依赖的性能变化（-90%到+67%）。代码总结任务触发最强的评论概念激活，而代码补全的敏感性最弱。

Conclusion: 该研究为构建基于内部概念表示而非仅依赖表层输入的软件工程工具和模型开辟了新方向，强调了对LLM内部概念表示进行推理和操作的重要性。

Abstract: While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.

</details>


### [60] [Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework](https://arxiv.org/abs/2512.16816)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Gemma Catolino,Andrea De Lucia,Fabio Palomba*

Main category: cs.SE

TL;DR: CAFFE是一个用于评估LLM反事实公平性的结构化测试框架，通过明确定义的测试组件、自动生成测试数据和语义相似度评估，相比现有方法能更全面地检测不公平行为。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现代软件系统中的广泛应用，公平性问题日益突出。现有方法主要使用蜕变测试来检测公平性问题，但需要更系统、意图感知的测试框架来评估LLM的反事实公平性。

Method: CAFFE框架包含三个核心部分：1) 通过明确定义的组件（提示意图、对话上下文、输入变体、期望公平阈值、测试环境配置）形式化LLM公平性测试用例；2) 自动生成有针对性的测试数据；3) 使用语义相似度指标评估模型响应。

Result: 在三种不同架构的LLM上进行的实验表明，CAFFE相比现有的蜕变测试方法，能够实现更广泛的偏见覆盖和更可靠的不公平行为检测。

Conclusion: CAFFE为LLM反事实公平性测试提供了一个结构化、意图感知的评估框架，能够更全面、可靠地检测模型中的不公平行为，为LLM公平性评估提供了新的方法论。

Abstract: Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.

</details>
