{"id": "2510.26130", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26130", "abs": "https://arxiv.org/abs/2510.26130", "authors": ["Musfiqur Rahman", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation", "comment": "Pre-print prepared for journal submission", "summary": "Large language models (LLMs) have advanced code generation at the function\nlevel, yet their ability to produce correct class-level implementations in\nauthentic software projects remains poorly understood. This work introduces a\nnovel benchmark derived from open-source repositories, comprising real-world\nclasses divided into seen and unseen partitions to evaluate generalization\nunder practical conditions. The evaluation examines multiple LLMs under varied\ninput specifications, retrieval-augmented configurations, and documentation\ncompleteness levels.\n  Results reveal a stark performance disparity: LLMs achieve 84% to 89%\ncorrectness on established synthetic benchmarks but only 25% to 34% on\nreal-world class tasks, with negligible differences between familiar and novel\ncodebases. Comprehensive docstrings yield modest gains of 1% to 3% in\nfunctional accuracy, though statistical significance is rare.\nRetrieval-augmented generation proves most effective with partial\ndocumentation, improving correctness by 4% to 7% by supplying concrete\nimplementation patterns absent from specifications. Error profiling identifies\nAttributeError, TypeError, and AssertionError as dominant failure modes (84% of\ncases), with synthetic tests overemphasizing assertion issues and real-world\nscenarios highlighting type and attribute mismatches. Retrieval augmentation\nreduces logical flaws but can introduce dependency conflicts.\n  The benchmark and analysis expose critical limitations in current LLM\ncapabilities for class-level engineering, offering actionable insights for\nenhancing context modelling, documentation strategies, and retrieval\nintegration in production code assistance tools.", "AI": {"tldr": "LLMs\u5728\u51fd\u6570\u7ea7\u4ee3\u7801\u751f\u6210\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u8f6f\u4ef6\u9879\u76ee\u7684\u7c7b\u7ea7\u5b9e\u73b0\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6b63\u786e\u7387\u4ece\u5408\u6210\u57fa\u51c6\u768484-89%\u964d\u81f3\u771f\u5b9e\u573a\u666f\u768425-34%\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u90e8\u5206\u6587\u6863\u60c5\u51b5\u4e0b\u6700\u6709\u6548\uff0c\u80fd\u63d0\u53474-7%\u6b63\u786e\u7387\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u8f6f\u4ef6\u9879\u76ee\u4e2d\u751f\u6210\u7c7b\u7ea7\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u4e86\u89e3\u5176\u5728\u5b9e\u8df5\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u8868\u73b0\uff0c\u586b\u8865\u5f53\u524d\u5bf9\u7c7b\u7ea7\u5de5\u7a0b\u80fd\u529b\u7406\u89e3\u4e0d\u8db3\u7684\u7a7a\u767d\u3002", "method": "\u4ece\u5f00\u6e90\u4ed3\u5e93\u6784\u5efa\u65b0\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u7c7b\uff0c\u5206\u4e3a\u5df2\u89c1\u548c\u672a\u89c1\u5206\u533a\u3002\u8bc4\u4f30\u591a\u79cdLLMs\u5728\u4e0d\u540c\u8f93\u5165\u89c4\u8303\u3001\u68c0\u7d22\u589e\u5f3a\u914d\u7f6e\u548c\u6587\u6863\u5b8c\u6574\u6027\u6c34\u5e73\u4e0b\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u771f\u5b9e\u7c7b\u4efb\u52a1\u4e2d\u6b63\u786e\u7387\u4ec525-34%\uff0c\u8fdc\u4f4e\u4e8e\u5408\u6210\u57fa\u51c6\u3002\u5b8c\u6574\u6587\u6863\u5b57\u7b26\u4e32\u4ec5\u5e26\u67651-3%\u7684\u5fae\u5c0f\u63d0\u5347\u3002\u68c0\u7d22\u589e\u5f3a\u5728\u90e8\u5206\u6587\u6863\u65f6\u6700\u6709\u6548\uff0c\u80fd\u63d0\u53474-7%\u6b63\u786e\u7387\u3002\u4e3b\u8981\u9519\u8bef\u7c7b\u578b\u4e3aAttributeError\u3001TypeError\u548cAssertionError\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u7c7b\u7ea7\u5de5\u7a0b\u80fd\u529b\u5b58\u5728\u4e25\u91cd\u5c40\u9650\uff0c\u9700\u8981\u6539\u8fdb\u4e0a\u4e0b\u6587\u5efa\u6a21\u3001\u6587\u6863\u7b56\u7565\u548c\u68c0\u7d22\u96c6\u6210\uff0c\u4e3a\u751f\u4ea7\u4ee3\u7801\u8f85\u52a9\u5de5\u5177\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6539\u8fdb\u65b9\u5411\u3002", "topic": "swe benchmark"}}
{"id": "2510.25776", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25776", "abs": "https://arxiv.org/abs/2510.25776", "authors": ["Chiung-Yi Tseng", "Somshubhra Roy", "Maisha Thasin", "Danyang Zhang", "Blessing Effiong"], "title": "StreetMath: Study of LLMs' Approximation Behaviors", "comment": null, "summary": "There is a substantial body of literature examining the mathematical\nreasoning capabilities of large language models (LLMs), particularly their\nperformance on precise arithmetic operations in autoregressive architectures.\nHowever, their ability to perform approximate reasoning in informal, fast-paced\nmathematical operations has received far less attention, especially among\nnon-autoregressive decoder models. Our work addresses this gap by introducing\nStreetMath, a benchmark designed to evaluate models' approximation abilities\nunder real-world approximation scenarios. We conduct extensive evaluations\nacross different LLM architectures: Qwen3-4B-Instruct-2507,\nQwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and\nMamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to\nprobe their internal computational states. Our analysis reveals that LLMs\ngenerally attempt to compute exact values or invoke external tools even in\ntasks that call for approximation. Moreover, while models sometimes reach the\ncorrect answer in early layers or steps, they still consume more tokens when\nsolving approximation tasks. Additional experiments indicate that exact and\napproximate arithmetic operations rely on largely separate neural components.\nDrawing upon research on cognitive psychology, we argue that LLMs do not\nexhibit cognitive miserliness in the same way humans do in street math\nsettings. We open source our work https://github.com/ctseng777/StreetMath", "AI": {"tldr": "\u63d0\u51fa\u4e86StreetMath\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728\u73b0\u5b9e\u4e16\u754c\u8fd1\u4f3c\u8ba1\u7b97\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u503e\u5411\u4e8e\u7cbe\u786e\u8ba1\u7b97\u800c\u975e\u8fd1\u4f3c\u63a8\u7406\uff0c\u4e14\u7cbe\u786e\u548c\u8fd1\u4f3c\u8ba1\u7b97\u4f9d\u8d56\u4e0d\u540c\u7684\u795e\u7ecf\u7ec4\u4ef6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u5728\u7cbe\u786e\u7b97\u672f\u8fd0\u7b97\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u5bf9\u5176\u5728\u975e\u6b63\u5f0f\u3001\u5feb\u901f\u8fd1\u4f3c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u975e\u81ea\u56de\u5f52\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u3002", "method": "\u5f15\u5165StreetMath\u57fa\u51c6\uff0c\u5728\u4e0d\u540cLLM\u67b6\u6784\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5e76\u5e94\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u63a2\u6d4b\u5176\u5185\u90e8\u8ba1\u7b97\u72b6\u6001\u3002", "result": "LLM\u901a\u5e38\u5c1d\u8bd5\u8ba1\u7b97\u7cbe\u786e\u503c\u6216\u8c03\u7528\u5916\u90e8\u5de5\u5177\uff0c\u5373\u4f7f\u5728\u9700\u8981\u8fd1\u4f3c\u7684\u4efb\u52a1\u4e2d\u4e5f\u662f\u5982\u6b64\uff1b\u6a21\u578b\u6709\u65f6\u5728\u65e9\u671f\u5c42\u6216\u6b65\u9aa4\u5f97\u5230\u6b63\u786e\u7b54\u6848\uff0c\u4f46\u4ecd\u6d88\u8017\u66f4\u591atoken\uff1b\u7cbe\u786e\u548c\u8fd1\u4f3c\u7b97\u672f\u8fd0\u7b97\u4f9d\u8d56\u4e0d\u540c\u7684\u795e\u7ecf\u7ec4\u4ef6\u3002", "conclusion": "LLM\u5728\u8857\u5934\u6570\u5b66\u573a\u666f\u4e2d\u4e0d\u50cf\u4eba\u7c7b\u90a3\u6837\u8868\u73b0\u51fa\u8ba4\u77e5\u541d\u556c\u6027\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u5728\u5feb\u901f\u8fd1\u4f3c\u63a8\u7406\u4e2d\u7684\u6548\u7387\u4f18\u5316\u3002", "topic": "agent analysis"}}
{"id": "2510.26171", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.26171", "abs": "https://arxiv.org/abs/2510.26171", "authors": ["Hasnain Iqbal", "Zerina Begum", "Kazi Sakib"], "title": "Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests", "comment": null, "summary": "Flaky tests can make automated software testing unreliable due to their\nunpredictable behavior. These tests can pass or fail on the same code base on\nmultiple runs. However, flaky tests often do not refer to any fault, even\nthough they can cause the continuous integration (CI) pipeline to fail. A\ncommon type of flaky test is the order-dependent (OD) test. The outcome of an\nOD test depends on the order in which it is run with respect to other test\ncases. Several studies have explored the detection and repair of OD tests.\nHowever, their methods require re-runs of tests multiple times, that are not\nrelated to the order dependence. Hence, prioritizing potential OD tests is\nnecessary to reduce the re-runs. In this paper, we propose a method to\nprioritize potential order-dependent tests. By analyzing shared static fields\nin test classes, we identify tests that are more likely to be order-dependent.\nIn our experiment on 27 project modules, our method successfully prioritized\nall OD tests in 23 cases, reducing test executions by an average of 65.92% and\nunnecessary re-runs by 72.19%. These results demonstrate that our approach\nsignificantly improves the efficiency of OD test detection by lowering\nexecution costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u6d4b\u8bd5\u7c7b\u4e2d\u5171\u4eab\u9759\u6001\u5b57\u6bb5\u6765\u4f18\u5148\u6392\u5e8f\u6f5c\u5728\u987a\u5e8f\u4f9d\u8d56\u6d4b\u8bd5\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6d4b\u8bd5\u6267\u884c\u6b21\u6570\u548c\u4e0d\u5fc5\u8981\u7684\u91cd\u590d\u8fd0\u884c\u3002", "motivation": "\u987a\u5e8f\u4f9d\u8d56\u6d4b\u8bd5\u4f1a\u5bfc\u81f4\u6301\u7eed\u96c6\u6210\u7ba1\u9053\u5931\u8d25\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u91cd\u590d\u8fd0\u884c\u6d4b\u8bd5\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5148\u6392\u5e8f\u6f5c\u5728OD\u6d4b\u8bd5\u4ee5\u51cf\u5c11\u91cd\u590d\u8fd0\u884c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6d4b\u8bd5\u7c7b\u4e2d\u7684\u5171\u4eab\u9759\u6001\u5b57\u6bb5\u6765\u8bc6\u522b\u66f4\u53ef\u80fd\u5177\u6709\u987a\u5e8f\u4f9d\u8d56\u6027\u7684\u6d4b\u8bd5\uff0c\u4ece\u800c\u4f18\u5148\u6392\u5e8f\u8fd9\u4e9b\u6d4b\u8bd5\u3002", "result": "\u572827\u4e2a\u9879\u76ee\u6a21\u5757\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u572823\u4e2a\u6848\u4f8b\u4e2d\u4f18\u5148\u6392\u5e8f\u4e86\u6240\u6709OD\u6d4b\u8bd5\uff0c\u5e73\u5747\u51cf\u5c11\u6d4b\u8bd5\u6267\u884c65.92%\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u91cd\u590d\u8fd0\u884c72.19%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u964d\u4f4e\u6267\u884c\u6210\u672c\u663e\u8457\u63d0\u9ad8\u4e86OD\u6d4b\u8bd5\u68c0\u6d4b\u7684\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "2510.25860", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.25860", "abs": "https://arxiv.org/abs/2510.25860", "authors": ["Xingjian Zhang", "Tianhong Gao", "Suliang Jin", "Tianhao Wang", "Teng Ye", "Eytan Adar", "Qiaozhu Mei"], "title": "Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters", "comment": null, "summary": "Large language models (LLMs) are increasingly used as raters for evaluation\ntasks. However, their reliability is often limited for subjective tasks, when\nhuman judgments involve subtle reasoning beyond annotation labels. Thinking\ntraces, the reasoning behind a judgment, are highly informative but challenging\nto collect and curate. We present a human-LLM collaborative framework to infer\nthinking traces from label-only annotations. The proposed framework uses a\nsimple and effective rejection sampling method to reconstruct these traces at\nscale. These inferred thinking traces are applied to two complementary tasks:\n(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation\nguidelines for proprietary LLM raters. Across multiple datasets, our methods\nlead to significantly improved LLM-human agreement. Additionally, the refined\nannotation guidelines increase agreement among different LLM models. These\nresults suggest that LLMs can serve as practical proxies for otherwise\nunrevealed human thinking traces, enabling label-only corpora to be extended\ninto thinking-trace-augmented resources that enhance the reliability of LLM\nraters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\u4ece\u4ec5\u6807\u7b7e\u6807\u6ce8\u4e2d\u63a8\u65ad\u601d\u7ef4\u8f68\u8ff9\uff0c\u7528\u4e8e\u6539\u8fdbLLM\u8bc4\u4f30\u5668\u7684\u6027\u80fd\u3002", "motivation": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u5728\u4e3b\u89c2\u4efb\u52a1\u4e2d\u53ef\u9760\u6027\u6709\u9650\uff0c\u56e0\u4e3a\u4eba\u7c7b\u5224\u65ad\u6d89\u53ca\u8d85\u51fa\u6807\u6ce8\u6807\u7b7e\u7684\u5fae\u5999\u63a8\u7406\uff0c\u800c\u601d\u7ef4\u8f68\u8ff9\u96be\u4ee5\u6536\u96c6\u548c\u6574\u7406\u3002", "method": "\u4f7f\u7528\u4eba\u673a\u534f\u4f5c\u6846\u67b6\u548c\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\u4ece\u4ec5\u6807\u7b7e\u6807\u6ce8\u4e2d\u91cd\u5efa\u601d\u7ef4\u8f68\u8ff9\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5fae\u8c03\u5f00\u653eLLM\u8bc4\u4f30\u5668\u548c\u5408\u6210\u66f4\u6e05\u6670\u7684\u6807\u6ce8\u6307\u5357\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86LLM\u4e0e\u4eba\u7c7b\u7684\u4e00\u81f4\u6027\uff0c\u6539\u8fdb\u7684\u6807\u6ce8\u6307\u5357\u4e5f\u589e\u52a0\u4e86\u4e0d\u540cLLM\u6a21\u578b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u7c7b\u601d\u7ef4\u8f68\u8ff9\u7684\u5b9e\u7528\u4ee3\u7406\uff0c\u4f7f\u4ec5\u6807\u7b7e\u8bed\u6599\u5e93\u6269\u5c55\u4e3a\u601d\u7ef4\u8f68\u8ff9\u589e\u5f3a\u8d44\u6e90\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u8bc4\u4f30\u5668\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.26287", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.26287", "abs": "https://arxiv.org/abs/2510.26287", "authors": ["Guochang Li", "Yuchen Liu", "Zhen Qin", "Yunkun Wang", "Jianping Zhong", "Chen Zhi", "Binhua Li", "Fei Huang", "Yongbin Li", "Shuiguang Deng"], "title": "Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search", "comment": null, "summary": "Repository-level software engineering tasks require large language models\n(LLMs) to efficiently navigate and extract information from complex codebases\nthrough multi-turn tool interactions. Existing approaches face significant\nlimitations: training-free, in-context learning methods struggle to guide\nagents effectively in tool utilization and decision-making based on\nenvironmental feedback, while training-based approaches typically rely on\ncostly distillation from larger LLMs, introducing data compliance concerns in\nenterprise environments. To address these challenges, we introduce\nRepoSearch-R1, a novel agentic reinforcement learning framework driven by\nMonte-carlo Tree Search (MCTS). This approach allows agents to generate\ndiverse, high-quality reasoning trajectories via self-training without\nrequiring model distillation or external supervision. Based on RepoSearch-R1,\nwe construct a RepoQA-Agent specifically designed for repository\nquestion-answering tasks. Comprehensive evaluation on repository\nquestion-answering tasks demonstrates that RepoSearch-R1 achieves substantial\nimprovements of answer completeness: 16.0% enhancement over no-retrieval\nmethods, 19.5% improvement over iterative retrieval methods, and 33% increase\nin training efficiency compared to general agentic reinforcement learning\napproaches. Our cold-start training methodology eliminates data compliance\nconcerns while maintaining robust exploration diversity and answer completeness\nacross repository-level reasoning tasks.", "AI": {"tldr": "RepoSearch-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u8bad\u7ec3\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff0c\u65e0\u9700\u6a21\u578b\u84b8\u998f\u6216\u5916\u90e8\u76d1\u7763\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u65e0\u8bad\u7ec3\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6307\u5bfc\u4ee3\u7406\u4f7f\u7528\u5de5\u5177\u548c\u57fa\u4e8e\u73af\u5883\u53cd\u9988\u51b3\u7b56\uff0c\u800c\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u4f9d\u8d56\u4ece\u5927\u6a21\u578b\u84b8\u998f\uff0c\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5b58\u5728\u6570\u636e\u5408\u89c4\u95ee\u9898\u3002", "method": "\u5f15\u5165RepoSearch-R1\u6846\u67b6\uff0c\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u81ea\u8bad\u7ec3\u751f\u6210\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\u3002\u6784\u5efa\u4e86\u4e13\u95e8\u7528\u4e8e\u4ed3\u5e93\u95ee\u7b54\u4efb\u52a1\u7684RepoQA-Agent\u3002", "result": "\u5728\u4ed3\u5e93\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cRepoSearch-R1\u663e\u8457\u63d0\u5347\u7b54\u6848\u5b8c\u6574\u6027\uff1a\u6bd4\u65e0\u68c0\u7d22\u65b9\u6cd5\u63d0\u9ad816.0%\uff0c\u6bd4\u8fed\u4ee3\u68c0\u7d22\u65b9\u6cd5\u63d0\u9ad819.5%\uff0c\u8bad\u7ec3\u6548\u7387\u6bd4\u901a\u7528\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad833%\u3002", "conclusion": "\u51b7\u542f\u52a8\u8bad\u7ec3\u65b9\u6cd5\u6d88\u9664\u4e86\u6570\u636e\u5408\u89c4\u95ee\u9898\uff0c\u540c\u65f6\u5728\u4ed3\u5e93\u7ea7\u63a8\u7406\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u63a2\u7d22\u591a\u6837\u6027\u548c\u7b54\u6848\u5b8c\u6574\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25786", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25786", "abs": "https://arxiv.org/abs/2510.25786", "authors": ["Yaniv Nikankin", "Dana Arad", "Itay Itzhak", "Anja Reusch", "Adi Simhi", "Gal Kesten-Pomeranz", "Yonatan Belinkov"], "title": "BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection", "comment": null, "summary": "One of the main challenges in mechanistic interpretability is circuit\ndiscovery, determining which parts of a model perform a given task. We build on\nthe Mechanistic Interpretability Benchmark (MIB) and propose three key\nimprovements to circuit discovery. First, we use bootstrapping to identify\nedges with consistent attribution scores. Second, we introduce a simple\nratio-based selection strategy to prioritize strong positive-scoring edges,\nbalancing performance and faithfulness. Third, we replace the standard greedy\nselection with an integer linear programming formulation. Our methods yield\nmore faithful circuits and outperform prior approaches across multiple MIB\ntasks and models. Our code is available at:\nhttps://github.com/technion-cs-nlp/MIB-Shared-Task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e09\u79cd\u6539\u8fdb\u7535\u8def\u53d1\u73b0\u7684\u65b9\u6cd5\uff1a\u4f7f\u7528\u81ea\u52a9\u6cd5\u8bc6\u522b\u4e00\u81f4\u5f52\u56e0\u8fb9\u3001\u5f15\u5165\u6bd4\u7387\u9009\u62e9\u7b56\u7565\u3001\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\u66ff\u4ee3\u8d2a\u5a6a\u9009\u62e9\uff0c\u5728MIB\u57fa\u51c6\u4e0a\u83b7\u5f97\u66f4\u5fe0\u5b9e\u548c\u6027\u80fd\u66f4\u597d\u7684\u7535\u8def\u3002", "motivation": "\u89e3\u51b3\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e2d\u7535\u8def\u53d1\u73b0\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5373\u786e\u5b9a\u6a21\u578b\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u7ec4\u4ef6\u90e8\u5206\u3002", "method": "1. \u4f7f\u7528\u81ea\u52a9\u6cd5\u8bc6\u522b\u5177\u6709\u4e00\u81f4\u5f52\u56e0\u5206\u6570\u7684\u8fb9\uff1b2. \u5f15\u5165\u57fa\u4e8e\u6bd4\u7387\u7684\u7b80\u5355\u9009\u62e9\u7b56\u7565\u6765\u4f18\u5148\u8003\u8651\u5f3a\u6b63\u5206\u6570\u8fb9\uff1b3. \u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\u66ff\u6362\u6807\u51c6\u8d2a\u5a6a\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2aMIB\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u5fe0\u5b9e\u7684\u7535\u8def\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u79cd\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7535\u8def\u53d1\u73b0\u7684\u6027\u80fd\u548c\u5fe0\u5b9e\u5ea6\uff0c\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.25796", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.25796", "abs": "https://arxiv.org/abs/2510.25796", "authors": ["Farnoosh Namdarpour", "Joseph Y. J. Chow"], "title": "Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning", "comment": null, "summary": "Ride-pooling, also known as ride-sharing, shared ride-hailing, or\nmicrotransit, is a service wherein passengers share rides. This service can\nreduce costs for both passengers and operators and reduce congestion and\nenvironmental impacts. A key limitation, however, is its myopic\ndecision-making, which overlooks long-term effects of dispatch decisions. To\naddress this, we propose a simulation-informed reinforcement learning (RL)\napproach. While RL has been widely studied in the context of ride-hailing\nsystems, its application in ride-pooling systems has been less explored. In\nthis study, we extend the learning and planning framework of Xu et al. (2018)\nfrom ride-hailing to ride-pooling by embedding a ride-pooling simulation within\nthe learning mechanism to enable non-myopic decision-making. In addition, we\npropose a complementary policy for rebalancing idle vehicles. By employing\nn-step temporal difference learning on simulated experiences, we derive\nspatiotemporal state values and subsequently evaluate the effectiveness of the\nnon-myopic policy using NYC taxi request data. Results demonstrate that the\nnon-myopic policy for matching can increase the service rate by up to 8.4%\nversus a myopic policy while reducing both in-vehicle and wait times for\npassengers. Furthermore, the proposed non-myopic policy can decrease fleet size\nby over 25% compared to a myopic policy, while maintaining the same level of\nperformance, thereby offering significant cost savings for operators.\nIncorporating rebalancing operations into the proposed framework cuts wait time\nby up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%\ncompared to using the framework for matching decisions alone at the cost of\nincreased vehicle minutes traveled per passenger.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u62df\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\u62fc\u8f66\u670d\u52a1\u4e2d\u7684\u77ed\u89c6\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u5d4c\u5165\u62fc\u8f66\u6a21\u62df\u548c\u5b66\u4e60\u673a\u5236\u5b9e\u73b0\u975e\u77ed\u89c6\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u670d\u52a1\u7387\u5e76\u51cf\u5c11\u4e86\u7b49\u5f85\u65f6\u95f4\u3002", "motivation": "\u62fc\u8f66\u670d\u52a1\u867d\u7136\u80fd\u964d\u4f4e\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\uff0c\u4f46\u5176\u77ed\u89c6\u51b3\u7b56\u9650\u5236\u4e86\u957f\u671f\u6548\u76ca\u3002\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u62fc\u8f66\u7cfb\u7edf\u4e2d\u5e94\u7528\u8f83\u5c11\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u5c06Xu\u7b49\u4eba\u7684\u5b66\u4e60\u89c4\u5212\u6846\u67b6\u4ece\u7f51\u7ea6\u8f66\u6269\u5c55\u5230\u62fc\u8f66\uff0c\u5d4c\u5165\u62fc\u8f66\u6a21\u62df\u673a\u5236\uff0c\u4f7f\u7528n\u6b65\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u4ece\u6a21\u62df\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u65f6\u7a7a\u72b6\u6001\u503c\uff0c\u5e76\u63d0\u51fa\u7a7a\u95f2\u8f66\u8f86\u518d\u5e73\u8861\u7b56\u7565\u3002", "result": "\u975e\u77ed\u89c6\u5339\u914d\u7b56\u7565\u4f7f\u670d\u52a1\u7387\u63d0\u9ad88.4%\uff0c\u51cf\u5c11\u4e58\u5ba2\u7b49\u5f85\u65f6\u95f4\u548c\u8f66\u5185\u65f6\u95f4\uff0c\u8f66\u961f\u89c4\u6a21\u53ef\u51cf\u5c1125%\u4ee5\u4e0a\u3002\u7ed3\u5408\u518d\u5e73\u8861\u64cd\u4f5c\u540e\uff0c\u7b49\u5f85\u65f6\u95f4\u51cf\u5c1127.3%\uff0c\u8f66\u5185\u65f6\u95f4\u51cf\u5c1112.5%\uff0c\u670d\u52a1\u7387\u63d0\u9ad815.1%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u62df\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u62fc\u8f66\u7cfb\u7edf\u7684\u77ed\u89c6\u51b3\u7b56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u8d28\u91cf\u548c\u8fd0\u8425\u6548\u7387\uff0c\u4e3a\u8fd0\u8425\u5546\u5e26\u6765\u663e\u8457\u6210\u672c\u8282\u7ea6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.25799", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25799", "abs": "https://arxiv.org/abs/2510.25799", "authors": ["Adam S. Jovine", "Tinghan Ye", "Francis Bahk", "Jingjing Wang", "David B. Shmoys", "Peter I. Frazier"], "title": "LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection", "comment": null, "summary": "Human experts often struggle to select the best option from a large set of\nitems with multiple competing objectives, a process bottlenecked by the\ndifficulty of formalizing complex, implicit preferences. To address this, we\nintroduce LISTEN, a framework that leverages a Large Language Model (LLM) as a\nzero-shot preference oracle, guided only by an expert's high-level priorities\nin natural language. To operate within LLM constraints like context windows and\ninference costs, we propose two iterative algorithms: LISTEN-U, which uses the\nLLM to refine a parametric utility function, and LISTEN-T, a non-parametric\nmethod that performs tournament-style selections over small batches of\nsolutions. Evaluated on diverse tasks including flight booking, shopping, and\nexam scheduling, our results show LISTEN-U excels when preferences are\nparametrically aligned (a property we measure with a novel concordance metric),\nwhile LISTEN-T offers more robust performance. This work explores a promising\ndirection for steering complex multi-objective decisions directly with natural\nlanguage, reducing the cognitive burden of traditional preference elicitation.", "AI": {"tldr": "LISTEN\u6846\u67b6\u4f7f\u7528LLM\u4f5c\u4e3a\u96f6\u6837\u672c\u504f\u597d\u9884\u8a00\u673a\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u5728\u591a\u4e2a\u7ade\u4e89\u76ee\u6807\u4e2d\u505a\u51fa\u9009\u62e9\uff0c\u63d0\u51fa\u4e86LISTEN-U\uff08\u53c2\u6570\u5316\u6548\u7528\u51fd\u6570\uff09\u548cLISTEN-T\uff08\u975e\u53c2\u6570\u5316\u9526\u6807\u8d5b\u9009\u62e9\uff09\u4e24\u79cd\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u4e13\u5bb6\u5728\u591a\u4e2a\u7ade\u4e89\u76ee\u6807\u4e2d\u9009\u62e9\u6700\u4f73\u9009\u9879\u65f6\u7684\u56f0\u96be\uff0c\u7279\u522b\u662f\u96be\u4ee5\u5f62\u5f0f\u5316\u590d\u6742\u9690\u542b\u504f\u597d\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u504f\u597d\u9884\u8a00\u673a\uff0c\u63d0\u51faLISTEN-U\uff08\u901a\u8fc7LLM\u7cbe\u70bc\u53c2\u6570\u5316\u6548\u7528\u51fd\u6570\uff09\u548cLISTEN-T\uff08\u5728\u5c0f\u6279\u91cf\u89e3\u51b3\u65b9\u6848\u4e0a\u8fdb\u884c\u9526\u6807\u8d5b\u5f0f\u9009\u62e9\uff09\u4e24\u79cd\u8fed\u4ee3\u7b97\u6cd5\u3002", "result": "\u5728\u822a\u73ed\u9884\u8ba2\u3001\u8d2d\u7269\u548c\u8003\u8bd5\u5b89\u6392\u7b49\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLISTEN-U\u5728\u504f\u597d\u53c2\u6570\u5bf9\u9f50\u65f6\u8868\u73b0\u4f18\u5f02\uff0cLISTEN-T\u5219\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a2\u7d22\u4e86\u7528\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u6307\u5bfc\u590d\u6742\u591a\u76ee\u6807\u51b3\u7b56\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u51cf\u8f7b\u4e86\u4f20\u7edf\u504f\u597d\u83b7\u53d6\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "topic": "agent analysis"}}
{"id": "2510.25908", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25908", "abs": "https://arxiv.org/abs/2510.25908", "authors": ["Emily Herron", "Junqi Yin", "Feiyi Wang"], "title": "SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications", "comment": "Preprint Submitted to ACM Transactions on AI for Science (TAIS)", "summary": "Large language models (LLMs) have demonstrated transformative potential in\nscientific research, yet their deployment in high-stakes contexts raises\nsignificant trustworthiness concerns. Here, we introduce SciTrust 2.0, a\ncomprehensive framework for evaluating LLM trustworthiness in scientific\napplications across four dimensions: truthfulness, adversarial robustness,\nscientific safety, and scientific ethics. Our framework incorporates novel,\nopen-ended truthfulness benchmarks developed through a verified\nreflection-tuning pipeline and expert validation, alongside a novel ethics\nbenchmark for scientific research contexts covering eight subcategories\nincluding dual-use research and bias. We evaluated seven prominent LLMs,\nincluding four science-specialized models and three general-purpose industry\nmodels, using multiple evaluation metrics including accuracy, semantic\nsimilarity measures, and LLM-based scoring. General-purpose industry models\noverall outperformed science-specialized models across each trustworthiness\ndimension, with GPT-o4-mini demonstrating superior performance in truthfulness\nassessments and adversarial robustness. Science-specialized models showed\nsignificant deficiencies in logical and ethical reasoning capabilities, along\nwith concerning vulnerabilities in safety evaluations, particularly in\nhigh-risk domains such as biosecurity and chemical weapons. By open-sourcing\nour framework, we provide a foundation for developing more trustworthy AI\nsystems and advancing research on model safety and ethics in scientific\ncontexts.", "AI": {"tldr": "SciTrust 2.0\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\u53ef\u4fe1\u5ea6\u7684\u7efc\u5408\u6846\u67b6\uff0c\u6db5\u76d6\u771f\u5b9e\u6027\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u3001\u79d1\u5b66\u5b89\u5168\u548c\u79d1\u5b66\u4f26\u7406\u56db\u4e2a\u7ef4\u5ea6\u3002\u8bc4\u4f30\u663e\u793a\u901a\u7528\u884c\u4e1a\u6a21\u578b\u5728\u5404\u65b9\u9762\u4f18\u4e8e\u79d1\u5b66\u4e13\u7528\u6a21\u578b\u3002", "motivation": "LLM\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u573a\u666f\u90e8\u7f72\u65f6\u5b58\u5728\u53ef\u4fe1\u5ea6\u62c5\u5fe7\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u65b0\u9896\u5f00\u653e\u5f0f\u771f\u5b9e\u6027\u57fa\u51c6\u548c\u79d1\u5b66\u4f26\u7406\u57fa\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u51c6\u786e\u6027\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548cLLM\u8bc4\u5206\u7b49\u591a\u79cd\u6307\u6807\u8bc4\u4f307\u4e2a\u4e3b\u8981LLM\u3002", "result": "\u901a\u7528\u884c\u4e1a\u6a21\u578b\u5728\u53ef\u4fe1\u5ea6\u5404\u7ef4\u5ea6\u4e0a\u6574\u4f53\u4f18\u4e8e\u79d1\u5b66\u4e13\u7528\u6a21\u578b\uff0cGPT-o4-mini\u5728\u771f\u5b9e\u6027\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002\u79d1\u5b66\u4e13\u7528\u6a21\u578b\u5728\u903b\u8f91\u548c\u4f26\u7406\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "conclusion": "\u79d1\u5b66\u4e13\u7528\u6a21\u578b\u5728\u5b89\u5168\u6027\u548c\u4f26\u7406\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\u3002\u5f00\u6e90\u6846\u67b6\u4e3a\u5f00\u53d1\u66f4\u53ef\u4fe1AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.25805", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25805", "abs": "https://arxiv.org/abs/2510.25805", "authors": ["Stefano Civelli", "Pietro Bernardelle", "Nardiena A. Pratama", "Gianluca Demartini"], "title": "Ideology-Based LLMs for Content Moderation", "comment": null, "summary": "Large language models (LLMs) are increasingly used in content moderation\nsystems, where ensuring fairness and neutrality is essential. In this study, we\nexamine how persona adoption influences the consistency and fairness of harmful\ncontent classification across different LLM architectures, model sizes, and\ncontent modalities (language vs. vision). At first glance, headline performance\nmetrics suggest that personas have little impact on overall classification\naccuracy. However, a closer analysis reveals important behavioral shifts.\nPersonas with different ideological leanings display distinct propensities to\nlabel content as harmful, showing that the lens through which a model \"views\"\ninput can subtly shape its judgments. Further agreement analyses highlight that\nmodels, particularly larger ones, tend to align more closely with personas from\nthe same political ideology, strengthening within-ideology consistency while\nwidening divergence across ideological groups. To show this effect more\ndirectly, we conducted an additional study on a politically targeted task,\nwhich confirmed that personas not only behave more coherently within their own\nideology but also exhibit a tendency to defend their perspective while\ndownplaying harmfulness in opposing views. Together, these findings highlight\nhow persona conditioning can introduce subtle ideological biases into LLM\noutputs, raising concerns about the use of AI systems that may reinforce\npartisan perspectives under the guise of neutrality.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u89d2\u8272\u8bbe\u5b9a\u5982\u4f55\u5f71\u54cd\u4e0d\u540cLLM\u67b6\u6784\u3001\u6a21\u578b\u5927\u5c0f\u548c\u5185\u5bb9\u6a21\u6001\u5728\u6709\u5bb3\u5185\u5bb9\u5206\u7c7b\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u516c\u5e73\u6027\u3002\u867d\u7136\u603b\u4f53\u51c6\u786e\u7387\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u89d2\u8272\u8bbe\u5b9a\u4f1a\u5f15\u5165\u610f\u8bc6\u5f62\u6001\u504f\u89c1\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e0d\u540c\u653f\u6cbb\u7acb\u573a\u4e0b\u5bf9\u5185\u5bb9\u5371\u5bb3\u6027\u7684\u5224\u65ad\u4ea7\u751f\u5dee\u5f02\u3002", "motivation": "\u968f\u7740LLM\u5728\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u516c\u5e73\u6027\u548c\u4e2d\u7acb\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u89d2\u8272\u8bbe\u5b9a\u5bf9LLM\u6709\u5bb3\u5185\u5bb9\u5206\u7c7b\u7684\u4e00\u81f4\u6027\u548c\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540cLLM\u67b6\u6784\u3001\u6a21\u578b\u5927\u5c0f\u548c\u5185\u5bb9\u6a21\u6001\uff08\u8bed\u8a00vs\u89c6\u89c9\uff09\u4e0b\u7684\u6709\u5bb3\u5185\u5bb9\u5206\u7c7b\u8868\u73b0\uff0c\u7814\u7a76\u89d2\u8272\u8bbe\u5b9a\u5bf9\u6a21\u578b\u5224\u65ad\u7684\u5f71\u54cd\u3002\u5305\u62ec\u4e00\u81f4\u6027\u5206\u6790\u3001\u610f\u8bc6\u5f62\u6001\u503e\u5411\u6027\u6d4b\u8bd5\u548c\u653f\u6cbb\u9488\u5bf9\u6027\u4efb\u52a1\u7814\u7a76\u3002", "result": "\u89d2\u8272\u8bbe\u5b9a\u5bf9\u603b\u4f53\u5206\u7c7b\u51c6\u786e\u7387\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u4f1a\u5bfc\u81f4\u660e\u663e\u7684\u610f\u8bc6\u5f62\u6001\u504f\u89c1\u3002\u4e0d\u540c\u653f\u6cbb\u7acb\u573a\u7684\u89d2\u8272\u5bf9\u5185\u5bb9\u5371\u5bb3\u6027\u7684\u5224\u65ad\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8f83\u5927\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u4e0e\u540c\u610f\u8bc6\u5f62\u6001\u89d2\u8272\u4fdd\u6301\u4e00\u81f4\uff0c\u52a0\u5267\u4e86\u8de8\u610f\u8bc6\u5f62\u6001\u7fa4\u4f53\u7684\u5206\u6b67\u3002", "conclusion": "\u89d2\u8272\u8bbe\u5b9a\u4f1a\u5728LLM\u8f93\u51fa\u4e2d\u5f15\u5165\u5fae\u5999\u7684\u610f\u8bc6\u5f62\u6001\u504f\u89c1\uff0c\u8fd9\u5bf9\u4f2a\u88c5\u4e2d\u7acb\u7684AI\u7cfb\u7edf\u4f7f\u7528\u63d0\u51fa\u4e86\u8b66\u793a\uff0c\u53ef\u80fd\u5f3a\u5316\u515a\u6d3e\u89c2\u70b9\u3002", "topic": "agent analysis"}}
{"id": "2510.25914", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25914", "abs": "https://arxiv.org/abs/2510.25914", "authors": ["Ngoc Phuoc An Vo", "Manish Kesarwani", "Ruchi Mahindru", "Chandrasekhar Narayanaswami"], "title": "FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization", "comment": null, "summary": "FinOps (Finance + Operations) represents an operational framework and\ncultural practice which maximizes cloud business value through collaborative\nfinancial accountability across engineering, finance, and business teams.\nFinOps practitioners face a fundamental challenge: billing data arrives in\nheterogeneous formats, taxonomies, and metrics from multiple cloud providers\nand internal systems which eventually lead to synthesizing actionable insights,\nand making time-sensitive decisions. To address this challenge, we propose\nleveraging autonomous, goal-driven AI agents for FinOps automation. In this\npaper, we built a FinOps agent for a typical use-case for IT infrastructure and\ncost optimization. We built a system simulating a realistic end-to-end industry\nprocess starting with retrieving data from various sources to consolidating and\nanalyzing the data to generate recommendations for optimization. We defined a\nset of metrics to evaluate our agent using several open-source and close-source\nlanguage models and it shows that the agent was able to understand, plan, and\nexecute tasks as well as an actual FinOps practitioner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eFinOps\u81ea\u52a8\u5316\u7684\u81ea\u4e3bAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u548c\u5206\u6790\u6765\u81ea\u591a\u4e2a\u4e91\u63d0\u4f9b\u5546\u548c\u5185\u90e8\u7cfb\u7edf\u7684\u5f02\u6784\u8ba1\u8d39\u6570\u636e\uff0c\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u548c\u6210\u672c\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "FinOps\u4ece\u4e1a\u8005\u9762\u4e34\u8ba1\u8d39\u6570\u636e\u683c\u5f0f\u5f02\u6784\u3001\u5206\u7c7b\u548c\u6307\u6807\u4e0d\u7edf\u4e00\u7684\u6311\u6218\uff0c\u8fd9\u5bfc\u81f4\u96be\u4ee5\u5feb\u901f\u5408\u6210\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u5e76\u505a\u51fa\u53ca\u65f6\u51b3\u7b56\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2aFinOps\u4ee3\u7406\u7cfb\u7edf\uff0c\u6a21\u62df\u7aef\u5230\u7aef\u7684\u884c\u4e1a\u6d41\u7a0b\uff0c\u5305\u62ec\u4ece\u591a\u4e2a\u6570\u636e\u6e90\u68c0\u7d22\u6570\u636e\u3001\u6574\u5408\u5206\u6790\u6570\u636e\u5e76\u751f\u6210\u4f18\u5316\u5efa\u8bae\u3002\u4f7f\u7528\u5f00\u6e90\u548c\u95ed\u6e90\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u4ee3\u7406\u80fd\u591f\u50cf\u5b9e\u9645\u7684FinOps\u4ece\u4e1a\u8005\u4e00\u6837\u7406\u89e3\u3001\u89c4\u5212\u548c\u6267\u884c\u4efb\u52a1\u3002", "conclusion": "\u81ea\u4e3bAI\u4ee3\u7406\u53ef\u4ee5\u6709\u6548\u652f\u6301FinOps\u81ea\u52a8\u5316\uff0c\u5e2e\u52a9\u89e3\u51b3\u5f02\u6784\u6570\u636e\u6574\u5408\u548c\u53ca\u65f6\u51b3\u7b56\u7684\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2510.26457", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26457", "abs": "https://arxiv.org/abs/2510.26457", "authors": ["Fang Liu", "Simiao Liu", "Yinghao Zhu", "Xiaoli Lian", "Li Zhang"], "title": "SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning", "comment": "Accepted by ICSE 2026. Code and data:\n  https://github.com/SIMIAO515/SecureReviewer", "summary": "Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments.", "AI": {"tldr": "SecureReviewer\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u4ee3\u7801\u5b89\u5168\u5ba1\u67e5\u7684LLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5b89\u5168\u4ee3\u7801\u5ba1\u67e5\u6570\u636e\u96c6\u3001\u5b89\u5168\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u548cRAG\u6280\u672f\uff0c\u63d0\u9ad8\u4e86LLM\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u8bc6\u522b\u548c\u89e3\u51b3\u5b89\u5168\u95ee\u9898\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u4ee3\u7801\u5ba1\u67e5\uff0c\u5728\u8bc6\u522b\u548c\u89e3\u51b3\u5b89\u5168\u76f8\u5173\u95ee\u9898\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u4e14\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u8bc4\u4f30\u6307\u6807\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u5b89\u5168\u4ee3\u7801\u5ba1\u67e5\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5b89\u5168\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u8bad\u7ec3LLM\uff0c\u96c6\u6210RAG\u6280\u672f\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5f15\u5165SecureBLEU\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSecureReviewer\u5728\u5b89\u5168\u6f0f\u6d1e\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u751f\u6210\u5ba1\u67e5\u8bc4\u8bba\u7684\u6574\u4f53\u8d28\u91cf\u53ca\u5b9e\u7528\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "SecureReviewer\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u4ee3\u7801\u5b89\u5168\u5ba1\u67e5\u4e2d\u7684\u80fd\u529b\uff0c\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u65e9\u671f\u8bc6\u522b\u548c\u89e3\u51b3\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2510.25801", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25801", "abs": "https://arxiv.org/abs/2510.25801", "authors": ["Kun Chen", "Peng Shi", "Haibo Qiu", "Zhixiong Zeng", "Siqi Yang", "Wenji Mao", "Lin Ma"], "title": "Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start", "comment": "Project Page: https://github.com/Kwen-Chen/SPECS-VL", "summary": "Reinforcement learning (RL) with verifiable rewards has recently catalyzed a\nwave of \"MLLM-r1\" approaches that bring RL to vision language models. Most\nrepresentative paradigms begin with a cold start, typically employing\nsupervised fine-tuning (SFT), to initialize the policy before RL. However,\nSFT-based cold start adopts the reasoning paradigm intertwined with task\nsolution and output format, which may induce instruction-style overfitting,\nweakens out-of-distribution generalization, and ultimately affects downstream\nRL. We revisit the cold start along two views, its training method and data\nconstruction, and introduce the Generalization Factor (GF) coefficient to\nquantify the generalization capability under different methods. Our empirical\nstudy finds that preference-based training methods (e.g. DPO) generalizes\nbetter than SFT-based methods in cold start. Motivated by this, we propose\nSPECS-a Self-distilled, Preference-based Cold Start framework that decouples\nmultimodal learning: (1) generates introspective preference data pairs via\nself-distillation, avoiding reliance on larger teachers or manual annotation;\n(2) performs preference-based training to learn, focusing on shallow,\ntransferable surface-form criteria (format, structure, style) rather than\nmemorizing content; and (3) hands off to RL with verifiable rewards for deep\nreasoning results. Experimental results across multiple multimodal benchmarks\nshow that our decoupling learning framework yields consistent performance gains\nover strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.\nAdditional experiments indicate that SPECS contributes to reducing\nin-distribution \"stuckness,\" improving exploration, stabilizing training, and\nraising the performance ceiling.", "AI": {"tldr": "\u63d0\u51faSPECS\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u751f\u6210\u504f\u597d\u6570\u636e\u5bf9\uff0c\u8fdb\u884c\u504f\u597d\u8bad\u7ec3\u5b66\u4e60\u8868\u9762\u5f62\u5f0f\u6807\u51c6\uff0c\u7136\u540e\u4ea4\u7ed9\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\uff0c\u76f8\u6bd4\u4f20\u7edfSFT\u51b7\u542f\u52a8\u65b9\u6cd5\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eSFT\u7684\u51b7\u542f\u52a8\u65b9\u6cd5\u5c06\u63a8\u7406\u8303\u5f0f\u4e0e\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u548c\u8f93\u51fa\u683c\u5f0f\u4ea4\u7ec7\u5728\u4e00\u8d77\uff0c\u53ef\u80fd\u5bfc\u81f4\u6307\u4ee4\u98ce\u683c\u8fc7\u62df\u5408\uff0c\u524a\u5f31\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u6700\u7ec8\u5f71\u54cd\u4e0b\u6e38\u5f3a\u5316\u5b66\u4e60\u3002", "method": "SPECS\u6846\u67b6\uff1a1) \u901a\u8fc7\u81ea\u84b8\u998f\u751f\u6210\u5185\u7701\u504f\u597d\u6570\u636e\u5bf9\uff0c\u907f\u514d\u4f9d\u8d56\u5927\u578b\u6559\u5e08\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\uff1b2) \u8fdb\u884c\u57fa\u4e8e\u504f\u597d\u7684\u8bad\u7ec3\uff0c\u4e13\u6ce8\u4e8e\u5b66\u4e60\u6d45\u5c42\u3001\u53ef\u8f6c\u79fb\u7684\u8868\u9762\u5f62\u5f0f\u6807\u51c6\uff08\u683c\u5f0f\u3001\u7ed3\u6784\u3001\u98ce\u683c\uff09\u800c\u975e\u8bb0\u5fc6\u5185\u5bb9\uff1b3) \u5c06\u5b66\u4e60\u7ed3\u679c\u4ea4\u7ed9\u5177\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPECS\u6846\u67b6\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0cMEGA-Bench\u63d0\u9ad84.1%\uff0cMathVista\u63d0\u9ad812.2%\u3002\u5b9e\u9a8c\u8fd8\u8868\u660eSPECS\u6709\u52a9\u4e8e\u51cf\u5c11\u5206\u5e03\u5185\"\u5361\u987f\"\u3001\u6539\u8fdb\u63a2\u7d22\u3001\u7a33\u5b9a\u8bad\u7ec3\u5e76\u63d0\u9ad8\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u57fa\u4e8e\u504f\u597d\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u51b7\u542f\u52a8\u9636\u6bb5\u6bd4SFT\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u7684\u89e3\u8026\u5b66\u4e60\u6846\u67b6SPECS\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26480", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.26480", "abs": "https://arxiv.org/abs/2510.26480", "authors": ["Sivajeet Chand", "Melih Kilic", "Roland W\u00fcrsching", "Sushant Kumar Pandey", "Alexander Pretschner"], "title": "Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study", "comment": "Accepted at AIware'25 - Main Track", "summary": "Automating the Extract Method refactoring (EMR) remains challenging and\nlargely manual despite its importance in improving code readability and\nmaintainability. Recent advances in open-source, resource-efficient Large\nLanguage Models (LLMs) offer promising new approaches for automating such\nhigh-level tasks. In this work, we critically evaluate five state-of-the-art\nopen-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python\ncode. We systematically assess functional correctness and code quality using\nautomated metrics and investigate the impact of prompting strategies by\ncomparing one-shot prompting to a Recursive criticism and improvement (RCI)\napproach. RCI-based prompting consistently outperforms one-shot prompting in\ntest pass rates and refactoring quality. The best-performing models,\nDeepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)\nscores of 0.829 and 0.808, while reducing lines of code (LOC) per method from\n12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453\nand 3.294, respectively. A developer survey on RCI-generated refactorings shows\nover 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation\ncriteria. In contrast, the original code scored below neutral, particularly in\nreadability and maintainability, underscoring the benefits of automated\nrefactoring guided by quality prompts. While traditional metrics like CC and\nLOC provide useful signals, they often diverge from human judgments,\nemphasizing the need for human-in-the-loop evaluation. Our open-source\nbenchmark offers a foundation for future research on automated refactoring with\nLLMs.", "AI": {"tldr": "\u8bc4\u4f305\u4e2a\u5f00\u6e90LLM\u5728Python\u4ee3\u7801\u63d0\u53d6\u65b9\u6cd5\u91cd\u6784\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0RCI\u63d0\u793a\u7b56\u7565\u4f18\u4e8e\u5355\u6b21\u63d0\u793a\uff0cDeepseek-Coder-RCI\u548cQwen2.5-Coder-RCI\u8868\u73b0\u6700\u4f73\uff0c\u5f00\u53d1\u8005\u8c03\u67e5\u663e\u793a70%\u4ee5\u4e0a\u7684\u91cd\u6784\u88ab\u63a5\u53d7\u3002", "motivation": "\u81ea\u52a8\u5316\u63d0\u53d6\u65b9\u6cd5\u91cd\u6784\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5f00\u6e90LLM\u4e3a\u81ea\u52a8\u5316\u8fd9\u7c7b\u9ad8\u7ea7\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f305\u4e2a3B\u52308B\u53c2\u6570\u7684\u5f00\u6e90LLM\uff0c\u4f7f\u7528\u81ea\u52a8\u6307\u6807\u8bc4\u4f30\u529f\u80fd\u6b63\u786e\u6027\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u6bd4\u8f83\u5355\u6b21\u63d0\u793a\u548cRCI\u63d0\u793a\u7b56\u7565\u3002", "result": "RCI\u63d0\u793a\u5728\u6d4b\u8bd5\u901a\u8fc7\u7387\u548c\u91cd\u6784\u8d28\u91cf\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u6700\u4f73\u6a21\u578b\u6d4b\u8bd5\u901a\u8fc7\u7387\u5206\u522b\u8fbe\u523082.9%\u548c80.8%\uff0c\u663e\u8457\u964d\u4f4e\u4ee3\u7801\u884c\u6570\u548c\u5708\u590d\u6742\u5ea6\u3002\u5f00\u53d1\u8005\u8c03\u67e5\u663e\u793a70%\u4ee5\u4e0a\u63a5\u53d7\u7387\u3002", "conclusion": "RCI\u63d0\u793a\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u91cd\u6784\u8d28\u91cf\uff0c\u4f46\u4f20\u7edf\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u4eba\u673a\u534f\u540c\u8bc4\u4f30\u3002", "topic": "swe application"}}
{"id": "2510.26516", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.26516", "abs": "https://arxiv.org/abs/2510.26516", "authors": ["Truong Hai Dang", "Jingyu Xiao", "Yintong Huo"], "title": "Envisioning Future Interactive Web Development: Editing Webpage with Natural Language", "comment": "accepted by AIWare'25", "summary": "The evolution of web applications relies on iterative code modifications, a\nprocess that is traditionally manual and time-consuming. While Large Language\nModels (LLMs) can generate UI code, their ability to edit existing code from\nnew design requirements (e.g., \"center the logo\") remains a challenge. This is\nlargely due to the absence of large-scale, high-quality tuning data to align\nmodel performance with human expectations. In this paper, we introduce a novel,\nautomated data generation pipeline that uses LLMs to synthesize a high-quality\nfine-tuning dataset for web editing, named Instruct4Edit. Our approach\ngenerates diverse instructions, applies the corresponding code modifications,\nand performs visual verification to ensure correctness. By fine-tuning models\non Instruct4Edit, we demonstrate consistent improvement in translating human\nintent into precise, structurally coherent, and visually accurate code changes.\nThis work provides a scalable and transparent foundation for natural language\nbased web editing, demonstrating that fine-tuning smaller open-source models\ncan achieve competitive performance with proprietary systems. We release all\ndata, code implementations, and model checkpoints for reproduction.", "AI": {"tldr": "\u63d0\u51fa\u4e86Instruct4Edit\u81ea\u52a8\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4f7f\u7528LLM\u5408\u6210\u9ad8\u8d28\u91cf\u7f51\u9875\u7f16\u8f91\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03\u4f7f\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5c06\u4eba\u7c7b\u610f\u56fe\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u4ee3\u7801\u4fee\u6539", "motivation": "\u7f51\u9875\u5e94\u7528\u8fed\u4ee3\u9700\u8981\u4ee3\u7801\u4fee\u6539\uff0c\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u624b\u52a8\u3002LLM\u867d\u7136\u80fd\u751f\u6210UI\u4ee3\u7801\uff0c\u4f46\u6839\u636e\u65b0\u8bbe\u8ba1\u9700\u6c42\u7f16\u8f91\u73b0\u6709\u4ee3\u7801\u4ecd\u5177\u6311\u6218\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8c03\u4f18\u6570\u636e", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4f7f\u7528LLM\u5408\u6210\u591a\u6837\u6307\u4ee4\uff0c\u5e94\u7528\u76f8\u5e94\u4ee3\u7801\u4fee\u6539\uff0c\u5e76\u8fdb\u884c\u89c6\u89c9\u9a8c\u8bc1\u786e\u4fdd\u6b63\u786e\u6027\uff0c\u521b\u5efaInstruct4Edit\u6570\u636e\u96c6", "result": "\u5728Instruct4Edit\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u5728\u5c06\u4eba\u7c7b\u610f\u56fe\u8f6c\u5316\u4e3a\u7cbe\u786e\u3001\u7ed3\u6784\u4e00\u81f4\u4e14\u89c6\u89c9\u51c6\u786e\u7684\u4ee3\u7801\u4fee\u6539\u65b9\u9762\u8868\u73b0\u4e00\u81f4\u63d0\u5347", "conclusion": "\u4e3a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u7f51\u9875\u7f16\u8f91\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u900f\u660e\u7684\u57fa\u7840\uff0c\u8bc1\u660e\u5fae\u8c03\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\u53ef\u4ee5\u8fbe\u5230\u4e0e\u4e13\u6709\u7cfb\u7edf\u7ade\u4e89\u7684\u6027\u80fd", "topic": "swe application"}}
{"id": "2510.25997", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25997", "abs": "https://arxiv.org/abs/2510.25997", "authors": ["Manu Redd", "Tao Zhe", "Dongjie Wang"], "title": "From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL", "comment": "8 pages, 5 figures, GeoGenAgent'25 - ACM SIGSPATIAL", "summary": "Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing\naccess to structured data, allowing users to query databases without learning\nSQL. Yet existing systems struggle with realistic spatio-temporal queries,\nwhere success requires aligning vague user phrasing with schema-specific\ncategories, handling temporal reasoning, and choosing appropriate outputs. We\npresent an agentic pipeline that extends a naive text-to-SQL baseline\n(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The\nagent can plan, decompose, and adapt queries through schema inspection, SQL\ngeneration, execution, and visualization tools. We evaluate on 35\nnatural-language queries over the NYC and Tokyo check-in dataset, covering\nspatial, temporal, and multi-dataset reasoning. The agent achieves\nsubstantially higher accuracy than the naive baseline 91.4% vs. 28.6% and\nenhances usability through maps, plots, and structured natural-language\nsummaries. Crucially, our design enables more natural human-database\ninteraction, supporting users who lack SQL expertise, detailed schema\nknowledge, or prompting skill. We conclude that agentic orchestration, rather\nthan stronger SQL generators alone, is a promising foundation for interactive\ngeospatial assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684NL-to-SQL\u7cfb\u7edf\uff0c\u901a\u8fc7ReAct\u4ee3\u7406\u534f\u8c03\u67e5\u8be2\u89c4\u5212\u3001\u5206\u89e3\u548c\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u67e5\u8be2\u7684\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u73b0\u6709NL-to-SQL\u7cfb\u7edf\u5728\u5904\u7406\u73b0\u5b9e\u65f6\u7a7a\u67e5\u8be2\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u597d\u5730\u5904\u7406\u6a21\u7cca\u7528\u6237\u8868\u8ff0\u3001\u65f6\u95f4\u63a8\u7406\u548c\u8f93\u51fa\u9009\u62e9\uff0c\u4ee5\u652f\u6301\u975eSQL\u4e13\u5bb6\u7684\u7528\u6237\u3002", "method": "\u6269\u5c55\u4e86llama-3-sqlcoder-8b\u57fa\u7ebf\uff0c\u4f7f\u7528Mistral-based ReAct\u4ee3\u7406\u8fdb\u884c\u7f16\u6392\uff0c\u901a\u8fc7\u6a21\u5f0f\u68c0\u67e5\u3001SQL\u751f\u6210\u3001\u6267\u884c\u548c\u53ef\u89c6\u5316\u5de5\u5177\u5b9e\u73b0\u67e5\u8be2\u89c4\u5212\u3001\u5206\u89e3\u548c\u9002\u5e94\u3002", "result": "\u572835\u4e2a\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e0a\u8bc4\u4f30\uff0c\u4ee3\u7406\u7cfb\u7edf\u51c6\u786e\u7387\u8fbe\u523091.4%\uff0c\u8fdc\u9ad8\u4e8e\u57fa\u7ebf28.6%\uff0c\u5e76\u901a\u8fc7\u5730\u56fe\u3001\u56fe\u8868\u548c\u81ea\u7136\u8bed\u8a00\u6458\u8981\u589e\u5f3a\u4e86\u53ef\u7528\u6027\u3002", "conclusion": "\u4ee3\u7406\u7f16\u6392\u800c\u975e\u4ec5\u5f3a\u5316SQL\u751f\u6210\u5668\uff0c\u662f\u6784\u5efa\u4ea4\u4e92\u5f0f\u5730\u7406\u7a7a\u95f4\u52a9\u624b\u7684\u6709\u524d\u666f\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2510.25941", "categories": ["cs.CL", "I.2"], "pdf": "https://arxiv.org/pdf/2510.25941", "abs": "https://arxiv.org/abs/2510.25941", "authors": ["Andr\u00e9 V. Duarte", "Xuying li", "Bin Zeng", "Arlindo L. Oliveira", "Lei Li", "Zhuo Li"], "title": "RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline", "comment": null, "summary": "If we cannot inspect the training data of a large language model (LLM), how\ncan we ever know what it has seen? We believe the most compelling evidence\narises when the model itself freely reproduces the target content. As such, we\npropose RECAP, an agentic pipeline designed to elicit and verify memorized\ntraining data from LLM outputs. At the heart of RECAP is a feedback-driven\nloop, where an initial extraction attempt is evaluated by a secondary language\nmodel, which compares the output against a reference passage and identifies\ndiscrepancies. These are then translated into minimal correction hints, which\nare fed back into the target model to guide subsequent generations. In\naddition, to address alignment-induced refusals, RECAP includes a jailbreaking\nmodule that detects and overcomes such barriers. We evaluate RECAP on\nEchoTrace, a new benchmark spanning over 30 full books, and the results show\nthat RECAP leads to substantial gains over single-iteration approaches. For\ninstance, with GPT-4.1, the average ROUGE-L score for the copyrighted text\nextraction improved from 0.38 to 0.47 - a nearly 24% increase.", "AI": {"tldr": "RECAP\u662f\u4e00\u79cd\u4ee3\u7406\u5f0f\u7ba1\u9053\uff0c\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u5faa\u73af\u4eceLLM\u8f93\u51fa\u4e2d\u63d0\u53d6\u548c\u9a8c\u8bc1\u8bb0\u5fc6\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5305\u62ec\u8d8a\u72f1\u6a21\u5757\u6765\u514b\u670d\u5bf9\u9f50\u5bfc\u81f4\u7684\u62d2\u7edd\u3002", "motivation": "\u5728\u65e0\u6cd5\u68c0\u67e5LLM\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u9a8c\u8bc1\u6a21\u578b\u662f\u5426\u8bb0\u5fc6\u4e86\u7279\u5b9a\u5185\u5bb9\uff0c\u6700\u6709\u529b\u7684\u8bc1\u636e\u662f\u6a21\u578b\u81ea\u7531\u91cd\u73b0\u76ee\u6807\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u53cd\u9988\u9a71\u52a8\u5faa\u73af\uff1a\u521d\u59cb\u63d0\u53d6\u5c1d\u8bd5\u7531\u6b21\u7ea7\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\uff0c\u6bd4\u8f83\u8f93\u51fa\u4e0e\u53c2\u8003\u6bb5\u843d\u5e76\u8bc6\u522b\u5dee\u5f02\uff0c\u7136\u540e\u8f6c\u5316\u4e3a\u6700\u5c0f\u4fee\u6b63\u63d0\u793a\u53cd\u9988\u7ed9\u76ee\u6807\u6a21\u578b\u6307\u5bfc\u540e\u7eed\u751f\u6210\u3002\u8fd8\u5305\u62ec\u68c0\u6d4b\u548c\u514b\u670d\u5bf9\u9f50\u62d2\u7edd\u7684\u8d8a\u72f1\u6a21\u5757\u3002", "result": "\u5728EchoTrace\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d630\u591a\u672c\u5b8c\u6574\u4e66\u7c4d\uff09\u4e0a\uff0cRECAP\u76f8\u6bd4\u5355\u6b21\u8fed\u4ee3\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002GPT-4.1\u7684\u7248\u6743\u6587\u672c\u63d0\u53d6\u5e73\u5747ROUGE-L\u5f97\u5206\u4ece0.38\u63d0\u9ad8\u52300.47\uff0c\u589e\u957f\u8fd124%\u3002", "conclusion": "RECAP\u80fd\u6709\u6548\u63d0\u53d6\u548c\u9a8c\u8bc1LLM\u8bb0\u5fc6\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u548c\u8d8a\u72f1\u673a\u5236\u663e\u8457\u63d0\u5347\u63d0\u53d6\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.25889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25889", "abs": "https://arxiv.org/abs/2510.25889", "authors": ["Kang Chen", "Zhihao Liu", "Tonghe Zhang", "Zhen Guo", "Si Xu", "Hao Lin", "Hongzhi Zang", "Quanlu Zhang", "Zhaofei Yu", "Guoliang Fan", "Tiejun Huang", "Yu Wang", "Chao Yu"], "title": "$\u03c0_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models", "comment": "Preprint, work in progress. 24 pages", "summary": "Vision-Language-Action (VLA) models enable robots to understand and perform\ncomplex tasks from multimodal input. Although recent work explores using\nreinforcement learning (RL) to automate the laborious data collection process\nin scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based\nVLAs (e.g., $\\pi_0$, $\\pi_{0.5}$) remains challenging due to intractable action\nlog-likelihoods from iterative denoising.\n  We address this challenge with $\\pi_{\\text{RL}}$, an open-source framework\nfor training flow-based VLAs in parallel simulation. $\\pi_{\\text{RL}}$\nimplements two RL algorithms: (1) {Flow-Noise} models the denoising process as\na discrete-time MDP with a learnable noise network for exact log-likelihood\ncomputation. (2) {Flow-SDE} integrates denoising with agent-environment\ninteraction, formulating a two-layer MDP that employs ODE-to-SDE conversion for\nefficient RL exploration.\n  We evaluate $\\pi_{\\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,\n$\\pi_{\\text{RL}}$ boosts few-shot SFT models $\\pi_0$ and $\\pi_{0.5}$ from 57.6%\nto 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train\n$\\pi_{\\text{RL}}$ in 320 parallel environments, improving $\\pi_0$ from 41.6% to\n85.7% and $\\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,\ndemonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, $\\pi_{\\text{RL}}$ achieves significant performance gains and\nstronger generalization over SFT-models, validating the effectiveness of online\nRL for flow-based VLAs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u03c0_RL\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5e76\u884c\u4eff\u771f\u4e2d\u8bad\u7ec3\u57fa\u4e8e\u6d41\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u5904\u7406\u6d41\u6a21\u578b\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6d41\u7684VLA\u6a21\u578b\u5728\u5e94\u7528\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u52a8\u4f5c\u5bf9\u6570\u4f3c\u7136\u96be\u4ee5\u8ba1\u7b97\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cdRL\u7b97\u6cd5\uff1aFlow-Noise\u5c06\u53bb\u566a\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u79bb\u6563\u65f6\u95f4MDP\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u566a\u58f0\u7f51\u7edc\u8fdb\u884c\u7cbe\u786e\u5bf9\u6570\u4f3c\u7136\u8ba1\u7b97\uff1bFlow-SDE\u5c06\u53bb\u566a\u4e0e\u667a\u80fd\u4f53-\u73af\u5883\u4ea4\u4e92\u96c6\u6210\uff0c\u91c7\u7528ODE\u5230SDE\u8f6c\u6362\u5b9e\u73b0\u9ad8\u6548RL\u63a2\u7d22\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c06\u03c0_0\u548c\u03c0_0.5\u6a21\u578b\u7684\u6027\u80fd\u5206\u522b\u4ece57.6%\u63d0\u5347\u523097.6%\u548c\u4ece77.1%\u63d0\u5347\u523098.3%\uff1b\u5728ManiSkill\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728320\u4e2a\u5e76\u884c\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u5c06\u03c0_0\u548c\u03c0_0.5\u6a21\u578b\u57284352\u4e2a\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5206\u522b\u4ece41.6%\u63d0\u5347\u523085.7%\u548c\u4ece40.0%\u63d0\u5347\u523084.8%\u3002", "conclusion": "\u03c0_RL\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6d41VLA\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5bf9\u8fd9\u7c7b\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26057", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26057", "abs": "https://arxiv.org/abs/2510.26057", "authors": ["Andrew L. Kun"], "title": "Can AI be Accountable?", "comment": "To be published as a chapter in Daniele Quercia and Marios\n  Constantinides (Eds.). Operationalizing Responsible AI. Cambridge University\n  Press. Forthcoming", "summary": "The AI we use is powerful, and its power is increasing rapidly. If this\npowerful AI is to serve the needs of consumers, voters, and decision makers,\nthen it is imperative that the AI is accountable. In general, an agent is\naccountable to a forum if the forum can request information from the agent\nabout its actions, if the forum and the agent can discuss this information, and\nif the forum can sanction the agent. Unfortunately, in too many cases today's\nAI is not accountable -- we cannot question it, enter into a discussion with\nit, let alone sanction it. In this chapter we relate the general definition of\naccountability to AI, we illustrate what it means for AI to be accountable and\nunaccountable, and we explore approaches that can improve our chances of living\nin a world where all AI is accountable to those who are affected by it.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u95ee\u8d23\u5236\u7684\u91cd\u8981\u6027\uff0c\u5206\u6790\u4e86\u5f53\u524dAI\u7f3a\u4e4f\u95ee\u8d23\u6027\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u5584AI\u95ee\u8d23\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740AI\u80fd\u529b\u7684\u5feb\u901f\u63d0\u5347\uff0c\u786e\u4fddAI\u5bf9\u7528\u6237\u3001\u9009\u6c11\u548c\u51b3\u7b56\u8005\u8d1f\u8d23\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524dAI\u7f3a\u4e4f\u95ee\u8d23\u673a\u5236\uff0c\u65e0\u6cd5\u88ab\u8d28\u7591\u3001\u8ba8\u8bba\u6216\u5236\u88c1\u3002", "method": "\u5c06\u4e00\u822c\u95ee\u8d23\u5236\u5b9a\u4e49\u5e94\u7528\u4e8eAI\u9886\u57df\uff0c\u901a\u8fc7\u6848\u4f8b\u8bf4\u660eAI\u95ee\u8d23\u4e0e\u4e0d\u95ee\u8d23\u7684\u542b\u4e49\uff0c\u63a2\u7d22\u63d0\u9ad8AI\u95ee\u8d23\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u660e\u786e\u4e86AI\u95ee\u8d23\u5236\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f53\u524dAI\u95ee\u8d23\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u5584AI\u95ee\u8d23\u6027\u7684\u53ef\u884c\u9014\u5f84\u3002", "conclusion": "\u9700\u8981\u5efa\u7acb\u6709\u6548\u7684AI\u95ee\u8d23\u673a\u5236\uff0c\u786e\u4fddAI\u5bf9\u53d7\u5176\u5f71\u54cd\u7684\u4eba\u8d1f\u8d23\uff0c\u8fd9\u662f\u5b9e\u73b0AI\u8d1f\u8d23\u4efb\u53d1\u5c55\u7684\u5173\u952e\u3002", "topic": "agent analysis"}}
{"id": "2510.26676", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.26676", "abs": "https://arxiv.org/abs/2510.26676", "authors": ["Samiha Shimmi", "Nicholas M. Synovic", "Mona Rahimi", "George K. Thiruvathukal"], "title": "Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study", "comment": "9 pages, 6 figures; Samiha Shimmi and Nicholas M. Synovic contributed\n  equally to this work (co-first authors); Mona Rahimi and George K.\n  Thiruvathukal contributed equally to this work (co-supervisors)", "summary": "Software vulnerabilities often persist or re-emerge even after being fixed,\nrevealing the complex interplay between code evolution and socio-technical\nfactors. While source code metrics provide useful indicators of\nvulnerabilities, software engineering process metrics can uncover patterns that\nlead to their introduction. Yet few studies have explored whether process\nmetrics can reveal risky development activities over time -- insights that are\nessential for anticipating and mitigating software vulnerabilities. This work\nhighlights the critical role of process metrics along with code changes in\nunderstanding and mitigating vulnerability reintroduction. We move beyond\nfile-level prediction and instead analyze security fixes at the commit level,\nfocusing not only on whether a single fix introduces a vulnerability but also\non the longer sequences of changes through which vulnerabilities evolve and\nre-emerge. Our approach emphasizes that reintroduction is rarely the result of\none isolated action, but emerges from cumulative development activities and\nsocio-technical conditions. To support this analysis, we conducted a case study\non the ImageMagick project by correlating longitudinal process metrics such as\nbus factor, issue density, and issue spoilage with vulnerability reintroduction\nactivities, encompassing 76 instances of reintroduced vulnerabilities. Our\nfindings show that reintroductions often align with increased issue spoilage\nand fluctuating issue density, reflecting short-term inefficiencies in issue\nmanagement and team responsiveness. These observations provide a foundation for\nbroader studies that combine process and code metrics to predict risky fixes\nand strengthen software security.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790ImageMagick\u9879\u76ee\u4e2d76\u4e2a\u6f0f\u6d1e\u91cd\u65b0\u5f15\u5165\u6848\u4f8b\uff0c\u53d1\u73b0\u8fc7\u7a0b\u6307\u6807\uff08\u5982\u95ee\u9898\u8150\u8d25\u5ea6\u548c\u95ee\u9898\u5bc6\u5ea6\uff09\u4e0e\u6f0f\u6d1e\u91cd\u65b0\u5f15\u5165\u5bc6\u5207\u76f8\u5173\uff0c\u5f3a\u8c03\u6f0f\u6d1e\u91cd\u65b0\u5f15\u5165\u662f\u7d2f\u79ef\u6027\u5f00\u53d1\u6d3b\u52a8\u548c\u793e\u6280\u6761\u4ef6\u7684\u7ed3\u679c\u3002", "motivation": "\u8f6f\u4ef6\u6f0f\u6d1e\u5373\u4f7f\u5728\u4fee\u590d\u540e\u4ecd\u4f1a\u6301\u7eed\u5b58\u5728\u6216\u91cd\u65b0\u51fa\u73b0\uff0c\u63ed\u793a\u4ee3\u7801\u6f14\u8fdb\u4e0e\u793e\u6280\u56e0\u7d20\u95f4\u7684\u590d\u6742\u4e92\u52a8\u3002\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u63a2\u7d22\u8fc7\u7a0b\u6307\u6807\u662f\u5426\u80fd\u63ed\u793a\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u98ce\u9669\u5f00\u53d1\u6d3b\u52a8\uff0c\u8fd9\u5bf9\u9884\u6d4b\u548c\u7f13\u89e3\u8f6f\u4ef6\u6f0f\u6d1e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728ImageMagick\u9879\u76ee\u4e2d\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u5c06\u7eb5\u5411\u8fc7\u7a0b\u6307\u6807\uff08\u5982\u5df4\u58eb\u56e0\u5b50\u3001\u95ee\u9898\u5bc6\u5ea6\u3001\u95ee\u9898\u8150\u8d25\u5ea6\uff09\u4e0e\u6f0f\u6d1e\u91cd\u65b0\u5f15\u5165\u6d3b\u52a8\u76f8\u5173\u8054\uff0c\u5206\u6790\u63d0\u4ea4\u7ea7\u522b\u7684\u5b89\u5168\u4fee\u590d\uff0c\u5173\u6ce8\u6f0f\u6d1e\u6f14\u5316\u548c\u91cd\u65b0\u51fa\u73b0\u7684\u957f\u671f\u53d8\u5316\u5e8f\u5217\u3002", "result": "\u7814\u7a76\u663e\u793a\u6f0f\u6d1e\u91cd\u65b0\u5f15\u5165\u901a\u5e38\u4e0e\u95ee\u9898\u8150\u8d25\u5ea6\u589e\u52a0\u548c\u95ee\u9898\u5bc6\u5ea6\u6ce2\u52a8\u76f8\u4e00\u81f4\uff0c\u53cd\u6620\u4e86\u95ee\u9898\u7ba1\u7406\u548c\u56e2\u961f\u54cd\u5e94\u80fd\u529b\u7684\u77ed\u671f\u4f4e\u6548\u3002\u8fc7\u7a0b\u6307\u6807\u80fd\u6709\u6548\u63ed\u793a\u5bfc\u81f4\u6f0f\u6d1e\u91cd\u65b0\u5f15\u5165\u7684\u98ce\u9669\u5f00\u53d1\u6a21\u5f0f\u3002", "conclusion": "\u8fc7\u7a0b\u6307\u6807\u4e0e\u4ee3\u7801\u6307\u6807\u7ed3\u5408\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u98ce\u9669\u4fee\u590d\u5e76\u589e\u5f3a\u8f6f\u4ef6\u5b89\u5168\u6027\uff0c\u6f0f\u6d1e\u91cd\u65b0\u5f15\u5165\u5f88\u5c11\u662f\u5b64\u7acb\u884c\u52a8\u7684\u7ed3\u679c\uff0c\u800c\u662f\u7d2f\u79ef\u5f00\u53d1\u6d3b\u52a8\u548c\u793e\u6280\u6761\u4ef6\u7684\u4ea7\u7269\u3002", "topic": "agent analysis"}}
{"id": "2510.26699", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.26699", "abs": "https://arxiv.org/abs/2510.26699", "authors": ["Aylton Almeida", "Laerte Xavier", "Marco Tulio Valente"], "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment", "comment": null, "summary": "Keeping software systems up to date is essential to avoid technical debt,\nsecurity vulnerabilities, and the rigidity typical of legacy systems. However,\nupdating libraries and frameworks remains a time consuming and error-prone\nprocess. Recent advances in Large Language Models (LLMs) and agentic coding\nsystems offer new opportunities for automating such maintenance tasks. In this\npaper, we evaluate the update of a well-known Python library, SQLAlchemy,\nacross a dataset of ten client applications. For this task, we use the Github's\nCopilot Agent Mode, an autonomous AI systema capable of planning and executing\nmulti-step migration workflows. To assess the effectiveness of the automated\nmigration, we also introduce Migration Coverage, a metric that quantifies the\nproportion of API usage points correctly migrated. The results of our study\nshow that the LLM agent was capable of migrating functionalities and API usages\nbetween SQLAlchemy versions (migration coverage: 100%, median), but failed to\nmaintain the application functionality, leading to a low test-pass rate\n(39.75%, median).", "AI": {"tldr": "\u8bc4\u4f30\u4f7f\u7528GitHub Copilot Agent Mode\u81ea\u52a8\u8fc1\u79fbSQLAlchemy\u5e93\u7248\u672c\u7684\u6548\u679c\uff0c\u53d1\u73b0\u867d\u7136API\u8fc1\u79fb\u8986\u76d6\u7387\u9ad8(100%)\uff0c\u4f46\u5e94\u7528\u529f\u80fd\u6d4b\u8bd5\u901a\u8fc7\u7387\u4f4e(39.75%)", "motivation": "\u8f6f\u4ef6\u5e93\u66f4\u65b0\u662f\u8017\u65f6\u4e14\u6613\u9519\u7684\u8fc7\u7a0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u667a\u80fd\u7f16\u7801\u7cfb\u7edf\u4e3a\u81ea\u52a8\u5316\u6b64\u7c7b\u7ef4\u62a4\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a", "method": "\u4f7f\u7528GitHub Copilot Agent Mode\u4f5c\u4e3a\u81ea\u4e3bAI\u7cfb\u7edf\uff0c\u572810\u4e2a\u5ba2\u6237\u7aef\u5e94\u7528\u6570\u636e\u96c6\u4e0a\u6267\u884cSQLAlchemy\u5e93\u7684\u591a\u6b65\u9aa4\u8fc1\u79fb\u5de5\u4f5c\u6d41\uff0c\u5e76\u5f15\u5165\u8fc1\u79fb\u8986\u76d6\u7387\u6307\u6807\u8bc4\u4f30\u6548\u679c", "result": "LLM\u4ee3\u7406\u80fd\u591f\u6210\u529f\u8fc1\u79fb\u529f\u80fd\u548cAPI\u4f7f\u7528(\u8fc1\u79fb\u8986\u76d6\u7387\u4e2d\u4f4d\u6570100%)\uff0c\u4f46\u672a\u80fd\u4fdd\u6301\u5e94\u7528\u529f\u80fd\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u901a\u8fc7\u7387\u4f4e(\u4e2d\u4f4d\u657039.75%)", "conclusion": "\u867d\u7136LLM\u4ee3\u7406\u5728API\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4fdd\u6301\u5e94\u7528\u529f\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb", "topic": "swe application"}}
{"id": "2510.25992", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25992", "abs": "https://arxiv.org/abs/2510.25992", "authors": ["Yihe Deng", "I-Hung Hsu", "Jun Yan", "Zifeng Wang", "Rujun Han", "Gufeng Zhang", "Yanfei Chen", "Wei Wang", "Tomas Pfister", "Chen-Yu Lee"], "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning", "comment": null, "summary": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u76d1\u7763\u5f3a\u5316\u5b66\u4e60(SRL)\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u89e3\u51b3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u903b\u8f91\"\u52a8\u4f5c\"\u5e8f\u5217\uff0c\u901a\u8fc7\u5185\u90e8\u63a8\u7406\u72ec\u767d\u548c\u57fa\u4e8e\u4e13\u5bb6\u52a8\u4f5c\u76f8\u4f3c\u6027\u7684\u5e73\u6ed1\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u5c0f\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u56f0\u96be\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u5bf9\u4e8e\u5c0f\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u6b63\u786e\u89e3\u5f88\u5c11\u88ab\u91c7\u6837\u65f6\u5931\u8d25\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u5219\u5bb9\u6613\u901a\u8fc7\u9010\u8bcd\u6a21\u4eff\u8fc7\u62df\u5408\u957f\u6f14\u793a\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SRL\u6846\u67b6\u5c06\u95ee\u9898\u89e3\u51b3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u903b\u8f91\u52a8\u4f5c\u5e8f\u5217\u3002\u6a21\u578b\u5728\u63d0\u4ea4\u6bcf\u4e2a\u52a8\u4f5c\u524d\u751f\u6210\u5185\u90e8\u63a8\u7406\u72ec\u767d\uff0c\u57fa\u4e8eSFT\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u7684\u4e13\u5bb6\u52a8\u4f5c\u63d0\u4f9b\u9010\u6b65\u76f8\u4f3c\u6027\u5956\u52b1\uff0c\u5373\u4f7f\u5728\u6240\u6709\u63a8\u6f14\u90fd\u9519\u8bef\u65f6\u4e5f\u80fd\u63d0\u4f9b\u4e30\u5bcc\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "SRL\u4f7f\u5c0f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u4e4b\u524dSFT\u6216RLVR\u65e0\u6cd5\u5b66\u4e60\u7684\u6311\u6218\u6027\u95ee\u9898\u3002\u5148\u7528SRL\u521d\u59cb\u5316\u8bad\u7ec3\u518d\u7528RLVR\u7cbe\u70bc\u53ef\u83b7\u5f97\u6700\u5f3a\u6027\u80fd\u3002SRL\u8fd8\u80fd\u6709\u6548\u6cdb\u5316\u5230\u4ee3\u7406\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002", "conclusion": "SRL\u662f\u4e00\u4e2a\u7a33\u5065\u4e14\u901a\u7528\u7684\u9762\u5411\u63a8\u7406\u7684LLM\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u5c0f\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u74f6\u9888\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26143", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26143", "abs": "https://arxiv.org/abs/2510.26143", "authors": ["Bo Pang", "Deqian Kong", "Silvio Savarese", "Caiming Xiong", "Yingbo Zhou"], "title": "Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math", "comment": "9 pages", "summary": "Reinforcement learning (RL) can elicit strong reasoning in large language\nmodels (LLMs), yet most open efforts focus on math and code. We propose\nReasoning Curriculum, a simple two-stage curriculum that first elicits\nreasoning skills in pretraining-aligned domains such as math, then adapts and\nrefines these skills across other domains via joint RL. Stage 1 performs a\nbrief cold start and then math-only RL with verifiable rewards to develop\nreasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and\nconsolidate these skills. The curriculum is minimal and backbone-agnostic,\nrequiring no specialized reward models beyond standard verifiability checks.\nEvaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning\ncurriculum yields consistent gains. Ablations and a cognitive-skill analysis\nindicate that both stages are necessary and that math-first elicitation\nincreases cognitive behaviors important for solving complex problems. Reasoning\nCurriculum provides a compact, easy-to-adopt recipe for general reasoning.", "AI": {"tldr": "\u63d0\u51faReasoning Curriculum\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5148\u5728\u6570\u5b66\u9886\u57df\u8bad\u7ec3\u63a8\u7406\u80fd\u529b\uff0c\u7136\u540e\u901a\u8fc7\u8054\u5408\u5f3a\u5316\u5b66\u4e60\u5c06\u63a8\u7406\u6280\u80fd\u8fc1\u79fb\u5230\u5176\u4ed6\u9886\u57df\uff0c\u65e0\u9700\u4e13\u95e8\u7684\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u5173\u6ce8\u6570\u5b66\u548c\u4ee3\u7801\u9886\u57df\uff0c\u4f46\u7f3a\u4e4f\u5c06\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u5176\u4ed6\u9886\u57df\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u9886\u57df\u63d0\u5347\u63a8\u7406\u80fd\u529b\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a\u9636\u6bb51\u5728\u6570\u5b66\u9886\u57df\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u6765\u53d1\u5c55\u63a8\u7406\u6280\u80fd\uff1b\u9636\u6bb52\u5728\u6df7\u5408\u9886\u57df\u6570\u636e\u8fdb\u884c\u8054\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u8fc1\u79fb\u548c\u5de9\u56fa\u63a8\u7406\u6280\u80fd\u3002", "result": "\u5728Qwen3-4B\u548cLlama-3.1-8B\u6a21\u578b\u4e0a\u7684\u591a\u9886\u57df\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e26\u6765\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6570\u5b66\u4f18\u5148\u7684\u63a8\u7406\u6fc0\u53d1\u589e\u52a0\u4e86\u89e3\u51b3\u590d\u6742\u95ee\u9898\u6240\u9700\u7684\u5173\u952e\u8ba4\u77e5\u884c\u4e3a\u3002", "conclusion": "Reasoning Curriculum\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u6613\u4e8e\u91c7\u7528\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65b9\u6848\uff0c\u4e24\u4e2a\u9636\u6bb5\u90fd\u662f\u5fc5\u8981\u7684\uff0c\u6570\u5b66\u4f18\u5148\u7684\u63a8\u7406\u6fc0\u53d1\u7b56\u7565\u6709\u6548\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26020", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26020", "abs": "https://arxiv.org/abs/2510.26020", "authors": ["Feijie Wu", "Weiwu Zhu", "Yuxiang Zhang", "Soumya Chatterjee", "Jiarong Zhu", "Fan Mo", "Rodin Luo", "Jing Gao"], "title": "PORTool: Tool-Use LLM Training with Rewarded Tree", "comment": null, "summary": "Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.", "AI": {"tldr": "PORTool\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u5177\u4f7f\u7528LLM\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u7d22\u591a\u79cd\u53ef\u80fd\u7684\u5de5\u5177\u8c03\u7528\u8f68\u8ff9\u6765\u63d0\u9ad8\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u5de5\u5177\u4f7f\u7528LLM\u53ea\u80fd\u6a21\u4eff\u56fa\u5b9a\u7684\u5de5\u5177\u8c03\u7528\u6d41\u7a0b\uff0c\u65e0\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63a2\u7d22\u591a\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1a1)\u4e3a\u67e5\u8be2\u751f\u6210\u591a\u4e2a\u5de5\u5177\u8c03\u7528\u8f68\u8ff9\u5f62\u6210\u6811\u72b6\u7ed3\u6784\uff1b2)\u57fa\u4e8e\u4ea7\u751f\u6b63\u786e\u7b54\u6848\u548c\u6210\u529f\u5de5\u5177\u8c03\u7528\u7684\u80fd\u529b\u5206\u914d\u6b65\u9aa4\u5956\u52b1\uff1b3)\u4f7f\u7528\u6b65\u9aa4\u76f8\u5bf9\u4f18\u52bf\u548c\u8f68\u8ff9\u76f8\u5bf9\u4f18\u52bf\u7684\u6df7\u5408\u4f18\u52bf\u6765\u8bad\u7ec3LLM", "result": "\u5728\u5305\u542b17\u4e2a\u5de5\u5177\u7684\u5b9e\u9a8c\u4e2d\uff0cPORTool\u5728\u6700\u7ec8\u51c6\u786e\u7387\u548c\u5de5\u5177\u8c03\u7528\u6b65\u9aa4\u6570\u91cf\u65b9\u9762\u76f8\u6bd4\u5176\u4ed6\u8bad\u7ec3\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347", "conclusion": "PORTool\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u591a\u79cd\u5de5\u5177\u8c03\u7528\u8f68\u8ff9\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5de5\u5177\u4f7f\u7528LLM\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2510.26144", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26144", "abs": "https://arxiv.org/abs/2510.26144", "authors": ["Annan Li", "Chufan Wu", "Zengle Ge", "Yee Hin Chong", "Zhinan Hou", "Lizhe Cao", "Cheng Ju", "Jianmin Wu", "Huaiming Li", "Haobo Zhang", "Shenghao Feng", "Mo Zhao", "Fengzhi Qiu", "Rui Yang", "Mengmeng Zhang", "Wenyi Zhu", "Yingying Sun", "Quan Sun", "Shunhao Yan", "Danyu Liu", "Dawei Yin", "Dou Shen"], "title": "The FM Agent", "comment": null, "summary": "Large language models (LLMs) are catalyzing the development of autonomous AI\nresearch agents for scientific and engineering discovery. We present FM Agent,\na novel and general-purpose multi-agent framework that leverages a synergistic\ncombination of LLM-based reasoning and large-scale evolutionary search to\naddress complex real-world challenges. The core of FM Agent integrates several\nkey innovations: 1) a cold-start initialization phase incorporating expert\nguidance, 2) a novel evolutionary sampling strategy for iterative optimization,\n3) domain-specific evaluators that combine correctness, effectiveness, and\nLLM-supervised feedback, and 4) a distributed, asynchronous execution\ninfrastructure built on Ray. Demonstrating broad applicability, our system has\nbeen evaluated across diverse domains, including operations research, machine\nlearning, GPU kernel optimization, and classical mathematical problems. FM\nAgent reaches state-of-the-art results autonomously, without human\ninterpretation or tuning -- 1976.3 on ALE-Bench (+5.2\\%), 43.56\\% on MLE-Bench\n(+4.0pp), up to 20x speedups on KernelBench, and establishes new\nstate-of-the-art(SOTA) results on several classical mathematical problems.\nBeyond academic benchmarks, FM Agent shows considerable promise for both\nlarge-scale enterprise R\\&D workflows and fundamental scientific research,\nwhere it can accelerate innovation, automate complex discovery processes, and\ndeliver substantial engineering and scientific advances with broader societal\nimpact.", "AI": {"tldr": "FM Agent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u8fdb\u5316\u641c\u7d22\u6765\u89e3\u51b3\u590d\u6742\u73b0\u5b9e\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u81ea\u4e3bAI\u7814\u7a76\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u590d\u6742\u7684\u79d1\u5b66\u548c\u5de5\u7a0b\u53d1\u73b0\u6311\u6218\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u521b\u65b0\u3002", "method": "\u7ed3\u5408LLM\u63a8\u7406\u548c\u5927\u89c4\u6a21\u8fdb\u5316\u641c\u7d22\uff0c\u5305\u542b\u51b7\u542f\u52a8\u521d\u59cb\u5316\u3001\u8fdb\u5316\u91c7\u6837\u7b56\u7565\u3001\u9886\u57df\u7279\u5b9a\u8bc4\u4f30\u5668\u548c\u5206\u5e03\u5f0f\u5f02\u6b65\u6267\u884c\u57fa\u7840\u8bbe\u65bd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff1aALE-Bench 1976.3\uff08+5.2%\uff09\u3001MLE-Bench 43.56%\uff08+4.0pp\uff09\u3001KernelBench\u6700\u9ad820\u500d\u52a0\u901f\uff0c\u5e76\u5728\u7ecf\u5178\u6570\u5b66\u95ee\u9898\u4e0a\u5efa\u7acb\u65b0SOTA\u3002", "conclusion": "FM Agent\u5728\u4f01\u4e1a\u548c\u57fa\u7840\u79d1\u5b66\u7814\u7a76\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u52a0\u901f\u521b\u65b0\u3001\u81ea\u52a8\u5316\u590d\u6742\u53d1\u73b0\u8fc7\u7a0b\uff0c\u4ea7\u751f\u91cd\u5927\u5de5\u7a0b\u548c\u79d1\u5b66\u8fdb\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2510.26167", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26167", "abs": "https://arxiv.org/abs/2510.26167", "authors": ["Renhao Li", "Jianhong Tu", "Yang Su", "Hamid Alinejad-Rokny", "Derek F. Wong", "Junyang Lin", "Min Yang"], "title": "One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning", "comment": null, "summary": "Reward models (RMs) play a critical role in aligning large language models\n(LLMs) with human preferences. Yet in the domain of tool learning, the lack of\nRMs specifically designed for function-calling tasks has limited progress\ntoward more capable agentic AI. We introduce ToolRM, a family of lightweight\ngenerative RMs tailored for general tool-use scenarios. To build these models,\nwe propose a novel pipeline that constructs pairwise preference data using\nrule-based scoring and multidimensional sampling. This yields\nToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique\ntasks that supports reinforcement learning with verifiable feedback. To\nevaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on\nthe agentic evaluation suite BFCL. Trained on our constructed data, models from\nthe Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially\noutperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward\njudgments. Beyond training objectives, ToolRM generalizes to broader critique\ntasks, including Best-of-N sampling and self-correction. Experiments on\nACEBench highlight its effectiveness and efficiency, enabling inference-time\nscaling and reducing output token usage by over 66%. We release data and model\ncheckpoints to facilitate future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86ToolRM\u7cfb\u5217\u8f7b\u91cf\u7ea7\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u5de5\u5177\u4f7f\u7528\u573a\u666f\uff0c\u901a\u8fc7\u6784\u5efaToolPref-Pairwise-30K\u6570\u636e\u96c6\u548cTRBench\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5de5\u5177\u5b66\u4e60\u9886\u57df\uff0c\u7f3a\u4e4f\u4e13\u95e8\u4e3a\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u8bbe\u8ba1\u7684\u5956\u52b1\u6a21\u578b\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4ee3\u7406AI\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u8bc4\u5206\u548c\u591a\u7ef4\u91c7\u6837\u6784\u5efa\u6210\u5bf9\u504f\u597d\u6570\u636e\uff0c\u521b\u5efaToolPref-Pairwise-30K\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8eBFCL\u8bc4\u4f30\u5957\u4ef6\u5efa\u7acbTRBench\u57fa\u51c6\u3002", "result": "\u57fa\u4e8eQwen3-4B/8B\u7cfb\u5217\u7684\u6a21\u578b\u5728\u6210\u5bf9\u5956\u52b1\u5224\u65ad\u4e2d\u51c6\u786e\u7387\u63d0\u5347\u8fbe14.28%\uff0c\u663e\u8457\u4f18\u4e8eClaude 4\u548cOpenAI o3\u7b49\u524d\u6cbf\u6a21\u578b\uff0c\u5728ACEBench\u4e0a\u51cf\u5c11\u8f93\u51fatoken\u4f7f\u7528\u8d85\u8fc766%\u3002", "conclusion": "ToolRM\u4e0d\u4ec5\u9002\u7528\u4e8e\u8bad\u7ec3\u76ee\u6807\uff0c\u8fd8\u80fd\u6cdb\u5316\u5230\u66f4\u5e7f\u6cdb\u7684\u8bc4\u5224\u4efb\u52a1\uff0c\u5305\u62ecBest-of-N\u91c7\u6837\u548c\u81ea\u6211\u4fee\u6b63\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u3002", "topic": "agent analysis"}}
{"id": "2510.26101", "categories": ["cs.CL", "cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.26101", "abs": "https://arxiv.org/abs/2510.26101", "authors": ["Taku Mikuriya", "Tatsuya Ishigaki", "Masayuki Kawarada", "Shunya Minami", "Tadashi Kadowaki", "Yohichi Suzuki", "Soshun Naito", "Shunya Takata", "Takumi Kato", "Tamotsu Basseda", "Reo Yamada", "Hiroya Takamura"], "title": "QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback", "comment": null, "summary": "Large language models (LLMs) have increasingly been applied to automatic\nprogramming code generation. This task can be viewed as a language generation\ntask that bridges natural language, human knowledge, and programming logic.\nHowever, it remains underexplored in domains that require interaction with\nhardware devices, such as quantum programming, where human coders write Python\ncode that is executed on a quantum computer. To address this gap, we introduce\nQCoder Benchmark, an evaluation framework that assesses LLMs on quantum\nprogramming with feedback from simulated hardware devices. Our benchmark offers\ntwo key features. First, it supports evaluation using a quantum simulator\nenvironment beyond conventional Python execution, allowing feedback of\ndomain-specific metrics such as circuit depth, execution time, and error\nclassification, which can be used to guide better generation. Second, it\nincorporates human-written code submissions collected from real programming\ncontests, enabling both quantitative comparisons and qualitative analyses of\nLLM outputs against human-written codes. Our experiments reveal that even\nadvanced models like GPT-4o achieve only around 18.97% accuracy, highlighting\nthe difficulty of the benchmark. In contrast, reasoning-based models such as o3\nreach up to 78% accuracy, outperforming averaged success rates of human-written\ncodes (39.98%). We release the QCoder Benchmark dataset and public evaluation\nAPI to support further research.", "AI": {"tldr": "\u63d0\u51fa\u4e86QCoder Benchmark\uff0c\u4e00\u4e2a\u8bc4\u4f30LLMs\u5728\u91cf\u5b50\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6846\u67b6\uff0c\u652f\u6301\u91cf\u5b50\u6a21\u62df\u5668\u73af\u5883\u548c\u771f\u5b9e\u4eba\u7c7b\u4ee3\u7801\u5bf9\u6bd4\uff0c\u5b9e\u9a8c\u663e\u793a\u63a8\u7406\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT-4o\u548c\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u9700\u8981\u4e0e\u786c\u4ef6\u8bbe\u5907\u4ea4\u4e92\u7684\u7f16\u7a0b\u9886\u57df\uff08\u5982\u91cf\u5b50\u7f16\u7a0b\uff09\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u771f\u5b9e\u91cf\u5b50\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1QCoder Benchmark\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u91cf\u5b50\u6a21\u62df\u5668\u73af\u5883\u652f\u6301\u9886\u57df\u7279\u5b9a\u6307\u6807\u53cd\u9988\uff08\u5982\u7535\u8def\u6df1\u5ea6\u3001\u6267\u884c\u65f6\u95f4\u3001\u9519\u8bef\u5206\u7c7b\uff09\uff0c\u5e76\u6574\u5408\u771f\u5b9e\u7f16\u7a0b\u7ade\u8d5b\u4e2d\u7684\u4eba\u7c7b\u4ee3\u7801\u63d0\u4ea4\u7528\u4e8e\u5bf9\u6bd4\u5206\u6790\u3002", "result": "GPT-4o\u4ec5\u8fbe\u523018.97%\u51c6\u786e\u7387\uff0c\u800c\u63a8\u7406\u6a21\u578bo3\u8fbe\u523078%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u4eba\u7c7b\u4ee3\u780139.98%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002", "conclusion": "\u91cf\u5b50\u7f16\u7a0b\u5bf9\u73b0\u6709LLMs\u5177\u6709\u6311\u6218\u6027\uff0c\u63a8\u7406\u6a21\u578b\u5728\u8be5\u9886\u57df\u8868\u73b0\u7a81\u51fa\uff0cQCoder Benchmark\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "topic": "swe benchmark"}}
{"id": "2510.26242", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26242", "abs": "https://arxiv.org/abs/2510.26242", "authors": ["Xinhang Li", "Qing Guo", "Junyu Chen", "Zheng Guo", "Shengzhe Xu", "Lei Li", "Lin Zhang"], "title": "Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles", "comment": null, "summary": "With increasing urban traffic complexity, Traffic Signal Control (TSC) is\nessential for optimizing traffic flow and improving road safety. Large Language\nModels (LLMs) emerge as promising approaches for TSC. However, they are prone\nto hallucinations in emergencies, leading to unreliable decisions that may\ncause substantial delays for emergency vehicles. Moreover, diverse intersection\ntypes present substantial challenges for traffic state encoding and\ncross-intersection training, limiting generalization across heterogeneous\nintersections. Therefore, this paper proposes Retrieval Augmented Generation\n(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable\nTSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning\nframework, which dynamically adjusts reasoning depth based on the emergency\nscenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to\ndistill specific knowledge and guidance from historical cases, enhancing the\nreliability and rationality of agents' emergency decisions. Secondly, this\npaper designs a type-agnostic traffic representation and proposes a\nReward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3\nadaptively samples training experience from diverse intersections with\nenvironment feedback-based priority and fine-tunes LLM agents with a designed\nreward-weighted likelihood loss, guiding REG-TSC toward high-reward policies\nacross heterogeneous intersections. On three real-world road networks with 17\nto 177 heterogeneous intersections, extensive experiments show that REG-TSC\nreduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle\nwaiting time by 83.16%, outperforming other state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faREG-TSC\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5206\u5e03\u5f0fLLM\u4ee3\u7406\u89e3\u51b3\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u7d27\u6025\u54cd\u5e94\u548c\u5f02\u6784\u4ea4\u53c9\u53e3\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u901a\u6548\u7387\u548c\u7d27\u6025\u8f66\u8f86\u901a\u884c", "motivation": "\u4f20\u7edfLLM\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u51b3\u7b56\u4e0d\u53ef\u9760\uff0c\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u7c7b\u578b\u7684\u4ea4\u53c9\u53e3\uff0c\u9650\u5236\u4e86\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b", "method": "1. \u7d27\u6025\u611f\u77e5\u63a8\u7406\u6846\u67b6\uff1a\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u4f7f\u7528\u57fa\u4e8e\u8bc4\u5ba1\u7684\u7d27\u6025RAG\u4ece\u5386\u53f2\u6848\u4f8b\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff1b2. \u7c7b\u578b\u65e0\u5173\u4ea4\u901a\u8868\u793a\u548c\u5956\u52b1\u5f15\u5bfc\u5f3a\u5316\u4f18\u5316(R3)\uff1a\u81ea\u9002\u5e94\u91c7\u6837\u8bad\u7ec3\u7ecf\u9a8c\uff0c\u4f7f\u7528\u5956\u52b1\u52a0\u6743\u4f3c\u7136\u635f\u5931\u5fae\u8c03LLM\u4ee3\u7406", "result": "\u57283\u4e2a\u771f\u5b9e\u8def\u7f51(17-177\u4e2a\u5f02\u6784\u4ea4\u53c9\u53e3)\u4e0a\uff0cREG-TSC\u51cf\u5c11\u65c5\u884c\u65f6\u95f442.00%\u3001\u6392\u961f\u957f\u5ea662.31%\u3001\u7d27\u6025\u8f66\u8f86\u7b49\u5f85\u65f6\u95f483.16%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "REG-TSC\u901a\u8fc7RAG\u589e\u5f3a\u548c\u5206\u5e03\u5f0fLLM\u4ee3\u7406\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u7d27\u6025\u54cd\u5e94\u548c\u5f02\u6784\u4ea4\u53c9\u53e3\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u6548\u7387\u548c\u5b89\u5168\u6027", "topic": "agent analysis"}}
{"id": "2510.26270", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26270", "abs": "https://arxiv.org/abs/2510.26270", "authors": ["Jiazhen Yuan", "Wei Zhao", "Zhengbiao Bai"], "title": "Graph-Enhanced Policy Optimization in LLM Agent Training", "comment": "Under review as a conference paper", "summary": "Group based reinforcement learning (RL) has shown impressive results on\ncomplex reasoning and mathematical tasks. Yet, when applied to train\nmulti-turn, interactive LLM agents, these methods often suffer from structural\nblindness-the inability to exploit the underlying connectivity of the\nenvironment. This manifests in three critical challenges: (1) inefficient,\nunguided exploration, (2) imprecise credit assignment due to overlooking\npivotal states, and (3) myopic planning caused by static reward discounting. We\naddress these issues with Graph-Enhanced Policy Optimization (GEPO), which\ndynamically constructs a state-transition graph from agent experience and\nemploys graph-theoretic centrality to provide three synergistic learning\nsignals: (1)structured intrinsic rewards that guide exploration toward\nhigh-impact states, (2) a graph-enhanced advantage function for topology-aware\ncredit assignment, and (3) a dynamic discount factor adapted to each state's\nstrategic value. On the ALFWorld, WebShop, and a proprietary Workbench\nbenchmarks, GEPO demonstrates strong performance, achieving absolute success\nrate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These\nresults highlight that explicitly modeling environmental structure is a robust,\ngeneralizable strategy for advancing LLM agent training.", "AI": {"tldr": "GEPO\u901a\u8fc7\u6784\u5efa\u72b6\u6001\u8f6c\u79fb\u56fe\u5e76\u5229\u7528\u56fe\u8bba\u4e2d\u5fc3\u6027\u6765\u89e3\u51b3\u591a\u8f6e\u4ea4\u4e92LLM\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u76f2\u95ee\u9898\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u7fa4\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u8f6e\u4ea4\u4e92LLM\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u76f2\u95ee\u9898\uff0c\u5305\u62ec\u63a2\u7d22\u6548\u7387\u4f4e\u3001\u4fe1\u7528\u5206\u914d\u4e0d\u51c6\u786e\u548c\u77ed\u89c6\u89c4\u5212\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faGraph-Enhanced Policy Optimization (GEPO)\uff0c\u52a8\u6001\u6784\u5efa\u72b6\u6001\u8f6c\u79fb\u56fe\uff0c\u5229\u7528\u56fe\u8bba\u4e2d\u5fc3\u6027\u63d0\u4f9b\u7ed3\u6784\u5316\u5185\u5728\u5956\u52b1\u3001\u56fe\u589e\u5f3a\u4f18\u52bf\u51fd\u6570\u548c\u52a8\u6001\u6298\u6263\u56e0\u5b50\u3002", "result": "\u5728ALFWorld\u3001WebShop\u548c\u4e13\u6709Workbench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGEPO\u76f8\u6bd4\u57fa\u7ebf\u5206\u522b\u5b9e\u73b0\u4e86+4.1%\u3001+5.3%\u548c+10.9%\u7684\u7edd\u5bf9\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u73af\u5883\u7ed3\u6784\u662f\u63a8\u8fdbLLM\u4ee3\u7406\u8bad\u7ec3\u7684\u5f3a\u5927\u4e14\u53ef\u6cdb\u5316\u7684\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26374", "abs": "https://arxiv.org/abs/2510.26374", "authors": ["Qianli Shen", "Daoyuan Chen", "Yilun Huang", "Zhenqing Ling", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning", "comment": null, "summary": "Reinforcement finetuning (RFT) is a key technique for aligning Large Language\nModels (LLMs) with human preferences and enhancing reasoning, yet its\neffectiveness is highly sensitive to which tasks are explored during training.\nUniform task sampling is inefficient, wasting computation on tasks that are\neither trivial or unsolvable, while existing task selection methods often\nsuffer from high rollout costs, poor adaptivity, or incomplete evidence. We\nintroduce \\textbf{BOTS}, a unified framework for \\textbf{B}ayesian\n\\textbf{O}nline \\textbf{T}ask \\textbf{S}election in LLM reinforcement\nfinetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior\nestimates of task difficulty as the model evolves. It jointly incorporates\n\\emph{explicit evidence} from direct evaluations of selected tasks and\n\\emph{implicit evidence} inferred from these evaluations for unselected tasks,\nwith Thompson sampling ensuring a principled balance between exploration and\nexploitation. To make implicit evidence practical, we instantiate it with an\nultra-light interpolation-based plug-in that estimates difficulties of\nunevaluated tasks without extra rollouts, adding negligible overhead.\nEmpirically, across diverse domains and LLM scales, BOTS consistently improves\ndata efficiency and performance over baselines and ablations, providing a\npractical and extensible solution for dynamic task selection in RFT.", "AI": {"tldr": "BOTS\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u5f3a\u5316\u5fae\u8c03\u4e2d\u8d1d\u53f6\u65af\u5728\u7ebf\u4efb\u52a1\u9009\u62e9\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ef4\u62a4\u4efb\u52a1\u96be\u5ea6\u7684\u540e\u9a8c\u4f30\u8ba1\uff0c\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u8bc1\u636e\uff0c\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4efb\u52a1\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u9ad8\u6210\u672c\u3001\u9002\u5e94\u6027\u5dee\u6216\u8bc1\u636e\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4efb\u52a1\u9009\u62e9\u7b56\u7565\u6765\u63d0\u5347\u5f3a\u5316\u5fae\u8c03\u7684\u6548\u679c\u3002", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u65ad\uff0cBOTS\u81ea\u9002\u5e94\u7ef4\u62a4\u4efb\u52a1\u96be\u5ea6\u7684\u540e\u9a8c\u4f30\u8ba1\uff0c\u7ed3\u5408\u663e\u5f0f\u8bc1\u636e\uff08\u76f4\u63a5\u8bc4\u4f30\uff09\u548c\u9690\u5f0f\u8bc1\u636e\uff08\u63a8\u65ad\u672a\u9009\u4efb\u52a1\uff09\uff0c\u4f7f\u7528Thompson\u91c7\u6837\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u548cLLM\u89c4\u6a21\u4e0a\uff0cBOTS\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u548c\u6d88\u878d\u5b9e\u9a8c\uff0c\u6301\u7eed\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "BOTS\u4e3aRFT\u4e2d\u7684\u52a8\u6001\u4efb\u52a1\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26025", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26025", "abs": "https://arxiv.org/abs/2510.26025", "authors": ["Semyon Lomaso", "Judah Goldfeder", "Mehmet Hamza Erol", "Matthew So", "Yao Yan", "Addison Howard", "Nathan Kutz", "Ravid Shwartz Ziv"], "title": "Exploring Human-AI Conceptual Alignment through the Prism of Chess", "comment": null, "summary": "Do AI systems truly understand human concepts or merely mimic surface\npatterns? We investigate this through chess, where human creativity meets\nprecise strategic concepts. Analyzing a 270M-parameter transformer that\nachieves grandmaster-level play, we uncover a striking paradox: while early\nlayers encode human concepts like center control and knight outposts with up to\n85\\% accuracy, deeper layers, despite driving superior performance, drift\ntoward alien representations, dropping to 50-65\\% accuracy. To test conceptual\nrobustness beyond memorization, we introduce the first Chess960 dataset: 240\nexpert-annotated positions across 6 strategic concepts. When opening theory is\neliminated through randomized starting positions, concept recognition drops\n10-20\\% across all methods, revealing the model's reliance on memorized\npatterns rather than abstract understanding. Our layer-wise analysis exposes a\nfundamental tension in current architectures: the representations that win\ngames diverge from those that align with human thinking. These findings suggest\nthat as AI systems optimize for performance, they develop increasingly alien\nintelligence, a critical challenge for creative AI applications requiring\ngenuine human-AI collaboration. Dataset and code are available at:\nhttps://github.com/slomasov/ChessConceptsLLM.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u7cfb\u7edf\u5728\u8c61\u68cb\u4e2d\u8868\u73b0\u51fa\u4e00\u4e2a\u77db\u76fe\uff1a\u65e9\u671f\u5c42\u80fd\u8f83\u597d\u7f16\u7801\u4eba\u7c7b\u6218\u7565\u6982\u5ff5\uff0c\u4f46\u6df1\u5c42\u867d\u7136\u6027\u80fd\u66f4\u597d\u5374\u504f\u79bb\u4eba\u7c7b\u601d\u7ef4\uff0c\u5f62\u6210\"\u5f02\u5316\u667a\u80fd\"\u3002\u901a\u8fc7Chess960\u6570\u636e\u96c6\u6d4b\u8bd5\uff0c\u53d1\u73b0AI\u66f4\u591a\u4f9d\u8d56\u8bb0\u5fc6\u6a21\u5f0f\u800c\u975e\u62bd\u8c61\u7406\u89e3\u3002", "motivation": "\u63a2\u7a76AI\u7cfb\u7edf\u662f\u5426\u771f\u6b63\u7406\u89e3\u4eba\u7c7b\u6982\u5ff5\u8fd8\u662f\u4ec5\u4ec5\u6a21\u4eff\u8868\u9762\u6a21\u5f0f\uff0c\u901a\u8fc7\u8c61\u68cb\u8fd9\u4e00\u7cbe\u786e\u6218\u7565\u9886\u57df\u6765\u7814\u7a76AI\u7684\u6982\u5ff5\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5206\u6790270M\u53c2\u6570transformer\u6a21\u578b\u5728\u8c61\u68cb\u4e2d\u7684\u8868\u73b0\uff0c\u5f15\u5165\u9996\u4e2aChess960\u6570\u636e\u96c6\uff08240\u4e2a\u4e13\u5bb6\u6807\u6ce8\u4f4d\u7f6e\uff0c\u8986\u76d66\u4e2a\u6218\u7565\u6982\u5ff5\uff09\uff0c\u901a\u8fc7\u968f\u673a\u8d77\u59cb\u4f4d\u7f6e\u6d88\u9664\u5f00\u5c40\u7406\u8bba\u5f71\u54cd\uff0c\u8fdb\u884c\u5206\u5c42\u6982\u5ff5\u8bc6\u522b\u5206\u6790\u3002", "result": "\u65e9\u671f\u5c42\u5bf9\u4eba\u7c7b\u6982\u5ff5\uff08\u5982\u4e2d\u5fc3\u63a7\u5236\u548c\u9a6c\u524d\u54e8\uff09\u7f16\u7801\u51c6\u786e\u7387\u8fbe85%\uff0c\u4f46\u6df1\u5c42\u51c6\u786e\u7387\u964d\u81f350-65%\uff1b\u5728Chess960\u6d4b\u8bd5\u4e2d\uff0c\u6982\u5ff5\u8bc6\u522b\u7387\u4e0b\u964d10-20%\uff0c\u663e\u793a\u6a21\u578b\u4f9d\u8d56\u8bb0\u5fc6\u6a21\u5f0f\u800c\u975e\u62bd\u8c61\u7406\u89e3\u3002", "conclusion": "\u5f53\u524d\u67b6\u6784\u5b58\u5728\u6839\u672c\u6027\u5f20\u529b\uff1a\u8d62\u5f97\u6e38\u620f\u7684\u8868\u5f81\u4e0e\u4eba\u7c7b\u601d\u7ef4\u8868\u5f81\u76f8\u80cc\u79bb\uff0cAI\u7cfb\u7edf\u5728\u4f18\u5316\u6027\u80fd\u8fc7\u7a0b\u4e2d\u53d1\u5c55\u51fa\"\u5f02\u5316\u667a\u80fd\"\uff0c\u8fd9\u5bf9\u9700\u8981\u771f\u6b63\u4eba\u673a\u534f\u4f5c\u7684\u521b\u9020\u6027AI\u5e94\u7528\u6784\u6210\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2510.26193", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26193", "abs": "https://arxiv.org/abs/2510.26193", "authors": ["Dongjun Jang", "Youngchae Ahn", "Hyopil Shin"], "title": "RCScore: Quantifying Response Consistency in Large Language Models", "comment": null, "summary": "Current LLM evaluations often rely on a single instruction template,\noverlooking models' sensitivity to instruction style-a critical aspect for\nreal-world deployments. We present RCScore, a multi-dimensional framework\nquantifying how instruction formulation affects model responses. By\nsystematically transforming benchmark problems into multiple instruction\nstyles, RCScore reveals performance variations undetected by conventional\nmetrics. Our experiments across ten LLMs on four reasoning benchmarks\ndemonstrate that instruction style can shift accuracy by up to 16.7% points. We\nintroduce Cross-Response Similarity (CRS), a method applying RCScore metrics to\nmeasure stylistic self-consistency, and establish its strong correlation with\ntask accuracy, suggesting consistency as a valuable proxy for model\nreliability. Additional findings show that deterministic decoding produces more\nstylistically stable outputs, and model scale correlates positively with\ncross-style consistency. RCScore offers a principled approach to assess\ninstruction robustness.", "AI": {"tldr": "RCScore\u662f\u4e00\u4e2a\u591a\u7ef4\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u6307\u4ee4\u8868\u8ff0\u5bf9\u6a21\u578b\u54cd\u5e94\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u5c06\u57fa\u51c6\u95ee\u9898\u8f6c\u5316\u4e3a\u591a\u79cd\u6307\u4ee4\u98ce\u683c\u6765\u63ed\u793a\u4f20\u7edf\u6307\u6807\u672a\u68c0\u6d4b\u5230\u7684\u6027\u80fd\u53d8\u5316\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u6307\u4ee4\u6a21\u677f\uff0c\u5ffd\u89c6\u4e86\u6a21\u578b\u5bf9\u6307\u4ee4\u98ce\u683c\u7684\u654f\u611f\u6027\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u57fa\u51c6\u95ee\u9898\u7cfb\u7edf\u8f6c\u5316\u4e3a\u591a\u79cd\u6307\u4ee4\u98ce\u683c\uff0c\u5e76\u5f15\u5165\u4ea4\u53c9\u54cd\u5e94\u76f8\u4f3c\u6027(CRS)\u6765\u8861\u91cf\u98ce\u683c\u81ea\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5bf9\u5341\u4e2aLLM\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6307\u4ee4\u98ce\u683c\u53ef\u4f7f\u51c6\u786e\u7387\u53d8\u5316\u9ad8\u8fbe16.7\u4e2a\u767e\u5206\u70b9\uff0c\u4e14CRS\u4e0e\u4efb\u52a1\u51c6\u786e\u7387\u5f3a\u76f8\u5173\u3002\u786e\u5b9a\u6027\u89e3\u7801\u4ea7\u751f\u66f4\u7a33\u5b9a\u7684\u8f93\u51fa\uff0c\u6a21\u578b\u89c4\u6a21\u4e0e\u8de8\u98ce\u683c\u4e00\u81f4\u6027\u6b63\u76f8\u5173\u3002", "conclusion": "RCScore\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6307\u4ee4\u9c81\u68d2\u6027\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u4e00\u81f4\u6027\u53ef\u4f5c\u4e3a\u6a21\u578b\u53ef\u9760\u6027\u7684\u6709\u4ef7\u503c\u4ee3\u7406\u6307\u6807\u3002", "topic": "agent analysis"}}
{"id": "2510.26396", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26396", "abs": "https://arxiv.org/abs/2510.26396", "authors": ["Joel Z. Leibo", "Alexander Sasha Vezhnevets", "William A. Cunningham", "Stanley M. Bileschi"], "title": "A Pragmatic View of AI Personhood", "comment": "40 pages", "summary": "The emergence of agentic Artificial Intelligence (AI) is set to trigger a\n\"Cambrian explosion\" of new kinds of personhood. This paper proposes a\npragmatic framework for navigating this diversification by treating personhood\nnot as a metaphysical property to be discovered, but as a flexible bundle of\nobligations (rights and responsibilities) that societies confer upon entities\nfor a variety of reasons, especially to solve concrete governance problems. We\nargue that this traditional bundle can be unbundled, creating bespoke solutions\nfor different contexts. This will allow for the creation of practical tools --\nsuch as facilitating AI contracting by creating a target \"individual\" that can\nbe sanctioned -- without needing to resolve intractable debates about an AI's\nconsciousness or rationality. We explore how individuals fit in to social roles\nand discuss the use of decentralized digital identity technology, examining\nboth \"personhood as a problem\", where design choices can create \"dark patterns\"\nthat exploit human social heuristics, and \"personhood as a solution\", where\nconferring a bundle of obligations is necessary to ensure accountability or\nprevent conflict. By rejecting foundationalist quests for a single, essential\ndefinition of personhood, this paper offers a more pragmatic and flexible way\nto think about integrating AI agents into our society.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u6846\u67b6\uff0c\u5c06\u4eba\u683c\u89c6\u4e3a\u793e\u4f1a\u4e3a\u89e3\u51b3\u6cbb\u7406\u95ee\u9898\u800c\u8d4b\u4e88\u5b9e\u4f53\u7684\u6743\u5229\u4e0e\u8d23\u4efb\u6346\u7ed1\uff0c\u800c\u975e\u5f62\u800c\u4e0a\u5b66\u5c5e\u6027\uff0c\u4ee5\u5e94\u5bf9AI\u4ee3\u7406\u6fc0\u589e\u5e26\u6765\u7684\u65b0\u578b\u4eba\u683c\u591a\u6837\u5316\u3002", "motivation": "\u968f\u7740\u4ee3\u7406AI\u7684\u51fa\u73b0\uff0c\u5c06\u5f15\u53d1\u65b0\u578b\u4eba\u683c\u7684\"\u5bd2\u6b66\u7eaa\u5927\u7206\u53d1\"\uff0c\u9700\u8981\u5b9e\u7528\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u79cd\u591a\u6837\u5316\uff0c\u800c\u4e0d\u9677\u5165\u5173\u4e8eAI\u610f\u8bc6\u6216\u7406\u6027\u7684\u65e0\u89e3\u8fa9\u8bba\u3002", "method": "\u63d0\u51fa\u4eba\u683c\u89e3\u7ed1\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u4eba\u683c\u6982\u5ff5\u5206\u89e3\u4e3a\u53ef\u5b9a\u5236\u7684\u6743\u5229\u4e0e\u8d23\u4efb\u6346\u7ed1\uff0c\u5229\u7528\u53bb\u4e2d\u5fc3\u5316\u6570\u5b57\u8eab\u4efd\u6280\u672f\uff0c\u63a2\u8ba8\u4eba\u683c\u4f5c\u4e3a\u95ee\u9898\uff08\u53ef\u80fd\u88ab\u5229\u7528\uff09\u548c\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff08\u786e\u4fdd\u95ee\u8d23\uff09\u7684\u53cc\u91cd\u89d2\u8272\u3002", "result": "\u5f00\u53d1\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5982\u901a\u8fc7\u521b\u5efa\u53ef\u88ab\u5236\u88c1\u7684\"\u4e2a\u4f53\"\u76ee\u6807\u6765\u4fc3\u8fdbAI\u5408\u7ea6\uff0c\u65e0\u9700\u89e3\u51b3AI\u610f\u8bc6\u7b49\u6839\u672c\u6027\u95ee\u9898\u3002", "conclusion": "\u62d2\u7edd\u5bfb\u6c42\u5355\u4e00\u672c\u8d28\u4eba\u683c\u5b9a\u4e49\u7684\u57fa\u7840\u4e3b\u4e49\u65b9\u6cd5\uff0c\u4e3a\u5c06AI\u4ee3\u7406\u6574\u5408\u5230\u793e\u4f1a\u4e2d\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u548c\u7075\u6d3b\u7684\u601d\u8003\u65b9\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2510.26402", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26402", "abs": "https://arxiv.org/abs/2510.26402", "authors": ["Vikrant Sahu", "Gagan Raj Gupta", "Raghav Borikar", "Nitin Mane"], "title": "Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education", "comment": null, "summary": "The rapid growth of programming education has outpaced traditional assessment\ntools, leaving faculty with limited means to provide meaningful, scalable\nfeedback. Conventional autograders, while efficient, act as black-box systems\nthat simply return pass/fail results, offering little insight into student\nthinking or learning needs.\n  Autograder+ is designed to shift autograding from a purely summative process\nto a formative learning experience. It introduces two key capabilities:\nautomated feedback generation using a fine-tuned Large Language Model, and\nvisualization of student code submissions to uncover learning patterns. The\nmodel is fine-tuned on curated student code and expert feedback to ensure\npedagogically aligned, context-aware guidance.\n  In evaluation across 600 student submissions from multiple programming tasks,\nthe system produced feedback with strong semantic alignment to instructor\ncomments. For visualization, contrastively learned code embeddings trained on\n1,000 annotated submissions enable grouping solutions into meaningful clusters\nbased on functionality and approach. The system also supports prompt-pooling,\nallowing instructors to guide feedback style through selected prompt templates.\n  By integrating AI-driven feedback, semantic clustering, and interactive\nvisualization, Autograder+ reduces instructor workload while supporting\ntargeted instruction and promoting stronger learning outcomes.", "AI": {"tldr": "Autograder+\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u7f16\u7a0b\u6559\u80b2\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u81ea\u52a8\u5316\u53cd\u9988\uff0c\u5e76\u4f7f\u7528\u4ee3\u7801\u5d4c\u5165\u53ef\u89c6\u5316\u5b66\u751f\u63d0\u4ea4\u5185\u5bb9\uff0c\u5c06\u4f20\u7edf\u81ea\u52a8\u8bc4\u5206\u4ece\u603b\u7ed3\u6027\u8bc4\u4f30\u8f6c\u53d8\u4e3a\u5f62\u6210\u6027\u5b66\u4e60\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u4f5c\u4e3a\u9ed1\u76d2\u4ec5\u63d0\u4f9b\u901a\u8fc7/\u5931\u8d25\u7ed3\u679c\uff0c\u65e0\u6cd5\u6d1e\u5bdf\u5b66\u751f\u601d\u7ef4\u8fc7\u7a0b\u6216\u5b66\u4e60\u9700\u6c42\uff0c\u7f16\u7a0b\u6559\u80b2\u7684\u5feb\u901f\u53d1\u5c55\u8d85\u51fa\u4e86\u4f20\u7edf\u8bc4\u4f30\u5de5\u5177\u7684\u80fd\u529b\u8303\u56f4\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u81ea\u52a8\u5316\u53cd\u9988\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4ee3\u7801\u5d4c\u5165\u8fdb\u884c\u53ef\u89c6\u5316\u805a\u7c7b\uff0c\u652f\u6301\u63d0\u793a\u6c60\u6a21\u677f\u6307\u5bfc\u53cd\u9988\u98ce\u683c\u3002", "result": "\u5728600\u4e2a\u5b66\u751f\u63d0\u4ea4\u7684\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u751f\u6210\u7684\u53cd\u9988\u4e0e\u6559\u5e08\u8bc4\u8bba\u5177\u6709\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u57fa\u4e8e1000\u4e2a\u6807\u6ce8\u63d0\u4ea4\u8bad\u7ec3\u7684\u4ee3\u7801\u5d4c\u5165\u80fd\u591f\u6309\u529f\u80fd\u548c\u65b9\u6cd5\u7684\u76f8\u4f3c\u6027\u8fdb\u884c\u6709\u610f\u4e49\u7684\u805a\u7c7b\u3002", "conclusion": "Autograder+\u901a\u8fc7\u6574\u5408AI\u9a71\u52a8\u53cd\u9988\u3001\u8bed\u4e49\u805a\u7c7b\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u51cf\u8f7b\u6559\u5e08\u5de5\u4f5c\u91cf\uff0c\u652f\u6301\u9488\u5bf9\u6027\u6559\u5b66\u5e76\u4fc3\u8fdb\u66f4\u597d\u7684\u5b66\u4e60\u6210\u679c\u3002", "topic": "swe application"}}
{"id": "2510.26086", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26086", "abs": "https://arxiv.org/abs/2510.26086", "authors": ["Zheng Zhang", "Haonan Li", "Xingyu Li", "Hang Zhang", "Zhiyun Qian"], "title": "LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline", "comment": null, "summary": "Bug bisection has been an important security task that aims to understand the\nrange of software versions impacted by a bug, i.e., identifying the commit that\nintroduced the bug. However, traditional patch-based bisection methods are\nfaced with several significant barriers: For example, they assume that the\nbug-inducing commit (BIC) and the patch commit modify the same functions, which\nis not always true. They often rely solely on code changes, while the commit\nmessage frequently contains a wealth of vulnerability-related information. They\nare also based on simple heuristics (e.g., assuming the BIC initializes lines\ndeleted in the patch) and lack any logical analysis of the vulnerability.\n  In this paper, we make the observation that Large Language Models (LLMs) are\nwell-positioned to break the barriers of existing solutions, e.g., comprehend\nboth textual data and code in patches and commits. Unlike previous BIC\nidentification approaches, which yield poor results, we propose a comprehensive\nmulti-stage pipeline that leverages LLMs to: (1) fully utilize patch\ninformation, (2) compare multiple candidate commits in context, and (3)\nprogressively narrow down the candidates through a series of down-selection\nsteps. In our evaluation, we demonstrate that our approach achieves\nsignificantly better accuracy than the state-of-the-art solution by more than\n38\\%. Our results further confirm that the comprehensive multi-stage pipeline\nis essential, as it improves accuracy by 60\\% over a baseline LLM-based\nbisection method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9636\u6bb5Bug\u4e8c\u5206\u6cd5\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u8865\u4e01\u7684\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc738%\uff0c\u591a\u9636\u6bb5\u7ba1\u9053\u6bd4\u57fa\u7ebfLLM\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u9ad860%\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8865\u4e01\u7684Bug\u4e8c\u5206\u6cd5\u5b58\u5728\u591a\u4e2a\u9650\u5236\uff1a\u5047\u8bbeBug\u5f15\u5165\u63d0\u4ea4\u548c\u8865\u4e01\u63d0\u4ea4\u4fee\u6539\u76f8\u540c\u51fd\u6570\u3001\u4ec5\u4f9d\u8d56\u4ee3\u7801\u53d8\u66f4\u800c\u5ffd\u7565\u63d0\u4ea4\u6d88\u606f\u4e2d\u7684\u6f0f\u6d1e\u4fe1\u606f\u3001\u57fa\u4e8e\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\u7f3a\u4e4f\u903b\u8f91\u5206\u6790\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u9636\u6bb5\u7ba1\u9053\uff0c\u5229\u7528LLM\uff1a(1)\u5145\u5206\u5229\u7528\u8865\u4e01\u4fe1\u606f\uff0c(2)\u5728\u4e0a\u4e0b\u6587\u4e2d\u6bd4\u8f83\u591a\u4e2a\u5019\u9009\u63d0\u4ea4\uff0c(3)\u901a\u8fc7\u4e00\u7cfb\u5217\u7b5b\u9009\u6b65\u9aa4\u9010\u6b65\u7f29\u5c0f\u5019\u9009\u8303\u56f4\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e8638%\u4ee5\u4e0a\uff0c\u591a\u9636\u6bb5\u7ba1\u9053\u6bd4\u57fa\u7ebfLLM\u4e8c\u5206\u6cd5\u51c6\u786e\u7387\u63d0\u9ad8\u4e8660%\u3002", "conclusion": "LLM\u80fd\u591f\u7a81\u7834\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u969c\u788d\uff0c\u7406\u89e3\u8865\u4e01\u548c\u63d0\u4ea4\u4e2d\u7684\u6587\u672c\u6570\u636e\u548c\u4ee3\u7801\uff0c\u591a\u9636\u6bb5\u7ba1\u9053\u5bf9\u4e8e\u63d0\u9ad8Bug\u4e8c\u5206\u6cd5\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "topic": "swe application"}}
{"id": "2510.26089", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.26089", "abs": "https://arxiv.org/abs/2510.26089", "authors": ["Fazel Arasteh", "Arian Haghparast", "Manos Papagelis"], "title": "Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing", "comment": "29 pages, 12 figures. Fazel Arasteh and Arian Haghparast contributed\n  equally to this research. Submitted to ACM Transactions on Spatial Algorithms\n  and Systems (TSAS). The code for this work is publicly available at\n  https://github.com/Arianhgh/HHAN", "summary": "Traffic congestion in urban road networks leads to longer trip times and\nhigher emissions, especially during peak periods. While the Shortest Path First\n(SPF) algorithm is optimal for a single vehicle in a static network, it\nperforms poorly in dynamic, multi-vehicle settings, often worsening congestion\nby routing all vehicles along identical paths. We address dynamic vehicle\nrouting through a multi-agent reinforcement learning (MARL) framework for\ncoordinated, network-aware fleet navigation. We first propose Adaptive\nNavigation (AN), a decentralized MARL model where each intersection agent\nprovides routing guidance based on (i) local traffic and (ii) neighborhood\nstate modeled using Graph Attention Networks (GAT). To improve scalability in\nlarge networks, we further propose Hierarchical Hub-based Adaptive Navigation\n(HHAN), an extension of AN that assigns agents only to key intersections\n(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles\nmicro-routing within each hub region. For hub coordination, HHAN adopts\ncentralized training with decentralized execution (CTDE) under the Attentive\nQ-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions\nvia attention. Hub agents use flow-aware state features that combine local\ncongestion and predictive dynamics for proactive routing. Experiments on\nsynthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces\naverage travel time versus SPF and learning baselines, maintaining 100% routing\nsuccess. HHAN scales to networks with hundreds of intersections, achieving up\nto 15.9% improvement under heavy traffic. These findings highlight the\npotential of network-constrained MARL for scalable, coordinated, and\ncongestion-aware routing in intelligent transportation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff08AN\u548cHHAN\uff09\u6765\u89e3\u51b3\u52a8\u6001\u8f66\u8f86\u8def\u7531\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u5206\u5c42\u67b6\u6784\u5728\u4ea4\u901a\u7f51\u7edc\u4e2d\u5b9e\u73b0\u534f\u8c03\u5bfc\u822a\uff0c\u663e\u8457\u51cf\u5c11\u5e73\u5747\u65c5\u884c\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u5728\u52a8\u6001\u591a\u8f66\u8f86\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f1a\u5bfc\u81f4\u6240\u6709\u8f66\u8f86\u9009\u62e9\u76f8\u540c\u8def\u5f84\u800c\u52a0\u5267\u62e5\u5835\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u534f\u8c03\u591a\u8f66\u8f86\u3001\u8003\u8651\u7f51\u7edc\u72b6\u6001\u7684\u667a\u80fd\u8def\u7531\u65b9\u6cd5\u3002", "method": "AN\u6a21\u578b\uff1a\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u5206\u6563\u5f0fMARL\uff0c\u6bcf\u4e2a\u4ea4\u53c9\u53e3\u667a\u80fd\u4f53\u57fa\u4e8e\u672c\u5730\u4ea4\u901a\u548c\u90bb\u57df\u72b6\u6001\u63d0\u4f9b\u8def\u7531\u6307\u5bfc\u3002HHAN\u6a21\u578b\uff1a\u5206\u5c42\u67b6\u6784\uff0c\u53ea\u5728\u5173\u952e\u67a2\u7ebd\u5206\u914d\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u548c\u6ce8\u610f\u529bQ\u6df7\u5408\u6846\u67b6\u3002", "result": "\u5728\u5408\u6210\u7f51\u683c\u548c\u771f\u5b9e\u57ce\u5e02\u5730\u56fe\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAN\u76f8\u6bd4SPF\u548c\u5b66\u4e60\u57fa\u7ebf\u51cf\u5c11\u4e86\u5e73\u5747\u65c5\u884c\u65f6\u95f4\uff0c\u4fdd\u6301100%\u8def\u7531\u6210\u529f\u7387\u3002HHAN\u53ef\u6269\u5c55\u5230\u6570\u767e\u4e2a\u4ea4\u53c9\u53e3\u7684\u7f51\u7edc\uff0c\u5728\u91cd\u4ea4\u901a\u4e0b\u5b9e\u73b0\u9ad8\u8fbe15.9%\u7684\u6539\u8fdb\u3002", "conclusion": "\u7f51\u7edc\u7ea6\u675f\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u534f\u8c03\u548c\u62e5\u5835\u611f\u77e5\u8def\u7531\u7684\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26285", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.26285", "abs": "https://arxiv.org/abs/2510.26285", "authors": ["Michal \u0160tef\u00e1nik", "Timothee Mickus", "Marek Kadl\u010d\u00edk", "Bertram H\u00f8jer", "Michal Spiegel", "Ra\u00fal V\u00e1zquez", "Aman Sinha", "Josef Kucha\u0159", "Philipp Mondorf"], "title": "Unravelling the Mechanisms of Manipulating Numbers in Language Models", "comment": null, "summary": "Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6570\u5b57\u5b66\u4e60\u51fa\u4e86\u53ef\u4e92\u6362\u3001\u7cfb\u7edf\u5316\u4e14\u9ad8\u5ea6\u51c6\u786e\u7684\u8868\u793a\uff0c\u5c3d\u7ba1\u5728\u8f93\u51fa\u4e2d\u4f1a\u51fa\u73b0\u9519\u8bef\u3002\u8fd9\u5141\u8bb8\u521b\u5efa\u901a\u7528\u63a2\u9488\u6765\u8ffd\u8e2a\u9519\u8bef\u5230\u7279\u5b9a\u5c42\u3002", "motivation": "\u89e3\u91ca\u4e3a\u4ec0\u4e48LLMs\u5728\u6570\u5b57\u5d4c\u5165\u8868\u793a\u4e0a\u8868\u73b0\u51c6\u786e\uff0c\u4f46\u5728\u5904\u7406\u6570\u5b57\u4fe1\u606f\u65f6\u5374\u4ea7\u751f\u9519\u8bef\u8f93\u51fa\u7684\u51b2\u7a81\u73b0\u8c61", "method": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u64cd\u4f5c\u6570\u5b57\uff0c\u91cf\u5316\u8fd9\u4e9b\u673a\u5236\u7684\u51c6\u786e\u6027\u4e0b\u9650\uff0c\u521b\u5efa\u901a\u7528\u63a2\u9488\u6765\u8ffd\u8e2a\u4fe1\u606f\u5230\u7279\u5b9a\u5c42", "result": "\u53d1\u73b0\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u51fa\u4e86\u53ef\u4e92\u6362\u3001\u7cfb\u7edf\u5316\u4e14\u9ad8\u5ea6\u51c6\u786e\u7684\u6570\u5b57\u8868\u793a\uff0c\u8fd9\u4e9b\u8868\u793a\u5728\u9690\u85cf\u72b6\u6001\u548c\u8f93\u5165\u4e0a\u4e0b\u6587\u7c7b\u578b\u4e2d\u5177\u6709\u666e\u904d\u6027", "conclusion": "\u4e3a\u7406\u89e3\u9884\u8bad\u7ec3LLMs\u5982\u4f55\u64cd\u4f5c\u6570\u5b57\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u66f4\u51c6\u786e\u63a2\u9488\u6280\u672f\u5728\u6539\u8fdbLLMs\u67b6\u6784\u4e2d\u7684\u6f5c\u529b", "topic": "agent analysis"}}
{"id": "2510.26603", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26603", "abs": "https://arxiv.org/abs/2510.26603", "authors": ["Reda El Makroum", "Sebastian Zwickl-Bernhard", "Lukas Kranzl"], "title": "Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling", "comment": "34 pages, 9 figures. Code available at\n  https://github.com/RedaElMakroum/agentic-ai-hems", "summary": "The electricity sector transition requires substantial increases in\nresidential demand response capacity, yet Home Energy Management Systems (HEMS)\nadoption remains limited by user interaction barriers requiring translation of\neveryday preferences into technical parameters. While large language models\nhave been applied to energy systems as code generators and parameter\nextractors, no existing implementation deploys LLMs as autonomous coordinators\nmanaging the complete workflow from natural language input to multi-appliance\nscheduling. This paper presents an agentic AI HEMS where LLMs autonomously\ncoordinate multi-appliance scheduling from natural language requests to device\ncontrol, achieving optimal scheduling without example demonstrations. A\nhierarchical architecture combining one orchestrator with three specialist\nagents uses the ReAct pattern for iterative reasoning, enabling dynamic\ncoordination without hardcoded workflows while integrating Google Calendar for\ncontext-aware deadline extraction. Evaluation across three open-source models\nusing real Austrian day-ahead electricity prices reveals substantial capability\ndifferences. Llama-3.3-70B successfully coordinates all appliances across all\nscenarios to match cost-optimal benchmarks computed via mixed-integer linear\nprogramming, while other models achieve perfect single-appliance performance\nbut struggle to coordinate all appliances simultaneously. Progressive prompt\nengineering experiments demonstrate that analytical query handling without\nexplicit guidance remains unreliable despite models' general reasoning\ncapabilities. We open-source the complete system including orchestration logic,\nagent prompts, tools, and web interfaces to enable reproducibility, extension,\nand future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u667a\u80fd\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u76f4\u63a5\u534f\u8c03\u591a\u7535\u5668\u8c03\u5ea6\uff0c\u65e0\u9700\u793a\u4f8b\u6f14\u793a\u5373\u53ef\u5b9e\u73b0\u6700\u4f18\u8c03\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u91c7\u7528\u53d7\u9650\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u7528\u6237\u9700\u8981\u5c06\u65e5\u5e38\u504f\u597d\u8f6c\u5316\u4e3a\u6280\u672f\u53c2\u6570\u7684\u4ea4\u4e92\u969c\u788d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5c06LLM\u90e8\u7f72\u4e3a\u4ece\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u5230\u591a\u7535\u5668\u8c03\u5ea6\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\u81ea\u4e3b\u534f\u8c03\u5668\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u7ed3\u5408\u4e00\u4e2a\u534f\u8c03\u5668\u548c\u4e09\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff0c\u4f7f\u7528ReAct\u6a21\u5f0f\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\uff0c\u96c6\u6210Google\u65e5\u5386\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u622a\u6b62\u65f6\u95f4\u63d0\u53d6\uff0c\u65e0\u9700\u786c\u7f16\u7801\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u8bc4\u4f30\u663e\u793aLlama-3.3-70B\u5728\u6240\u6709\u573a\u666f\u4e0b\u6210\u529f\u534f\u8c03\u6240\u6709\u7535\u5668\uff0c\u5339\u914d\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u8ba1\u7b97\u51fa\u7684\u6210\u672c\u6700\u4f18\u57fa\u51c6\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5728\u5355\u7535\u5668\u8c03\u5ea6\u4e0a\u8868\u73b0\u5b8c\u7f8e\u4f46\u96be\u4ee5\u540c\u65f6\u534f\u8c03\u6240\u6709\u7535\u5668\u3002", "conclusion": "LLM\u4f5c\u4e3a\u81ea\u4e3b\u534f\u8c03\u5668\u5728\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5206\u6790\u6027\u67e5\u8be2\u5904\u7406\u5728\u6ca1\u6709\u660e\u786e\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u4ecd\u4e0d\u53ef\u9760\uff0c\u5f00\u6e90\u7cfb\u7edf\u652f\u6301\u53ef\u91cd\u73b0\u6027\u3001\u6269\u5c55\u548c\u672a\u6765\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2510.26606", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26606", "abs": "https://arxiv.org/abs/2510.26606", "authors": ["Kentaro Ozeki", "Risako Ando", "Takanobu Morishita", "Hirohiko Abe", "Koji Mineshima", "Mitsuhiro Okada"], "title": "Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives", "comment": "Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025", "summary": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u8303\u63a8\u7406\u9886\u57df\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u867d\u7136LLMs\u901a\u5e38\u9075\u5faa\u6709\u6548\u63a8\u7406\u6a21\u5f0f\uff0c\u4f46\u5728\u7279\u5b9a\u7c7b\u578b\u7684\u89c4\u8303\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u663e\u793a\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u63a8\u7406\u4e2d\u7684\u8ba4\u77e5\u504f\u5dee\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u5404\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5904\u7406\u89c4\u8303\u63a8\u7406\uff08\u6d89\u53ca\u4e49\u52a1\u548c\u8bb8\u53ef\u7b49\u89c4\u8303\u6a21\u6001\uff09\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u89c4\u8303\u548c\u8ba4\u77e5\u9886\u57df\u7684\u5e7f\u6cdb\u63a8\u7406\u6a21\u5f0f\uff0c\u540c\u65f6\u7eb3\u5165\u5f71\u54cd\u4eba\u7c7b\u63a8\u7406\u7684\u975e\u5f62\u5f0f\u8ba4\u77e5\u56e0\u7d20\u3002\u901a\u8fc7\u6bd4\u8f83LLMs\u5728\u89c4\u8303\u6a21\u6001\u548c\u8ba4\u77e5\u6a21\u6001\u4e0b\u7684\u63a8\u7406\u8868\u73b0\u6765\u8bc4\u4f30\u5176\u80fd\u529b\u3002", "result": "LLMs\u5728\u89c4\u8303\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u663e\u793a\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u63a8\u7406\u4e2d\u7684\u8ba4\u77e5\u504f\u5dee\uff0c\u5c3d\u7ba1\u603b\u4f53\u4e0a\u9075\u5faa\u6709\u6548\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u5728LLMs\u89c4\u8303\u63a8\u7406\u4e2d\u5b9e\u73b0\u903b\u8f91\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u63d0\u9ad8\u5176\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.26658", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26658", "abs": "https://arxiv.org/abs/2510.26658", "authors": ["Zewen Chi", "Li Dong", "Qingxiu Dong", "Yaru Hao", "Xun Wu", "Shaohan Huang", "Furu Wei"], "title": "The Era of Agentic Organization: Learning to Organize with Language Models", "comment": null, "summary": "We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.", "AI": {"tldr": "\u63d0\u51fa\u5f02\u6b65\u601d\u7ef4\uff08AsyncThink\uff09\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7ec4\u7ec7\u8005\u52a8\u6001\u5206\u914d\u5b50\u4efb\u52a1\u7ed9\u5de5\u4f5c\u8005\u3001\u5408\u5e76\u4e2d\u95f4\u77e5\u8bc6\uff0c\u5b9e\u73b0LLM\u63a8\u7406\u8fc7\u7a0b\u7684\u5e76\u53d1\u6267\u884c\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u601d\u7ef4\u7ed3\u6784\u3002", "motivation": "\u5b9e\u73b0\u667a\u80fd\u4f53\u7ec4\u7ec7\u65b0\u65f6\u4ee3\uff0c\u8ba9\u667a\u80fd\u4f53\u901a\u8fc7\u534f\u4f5c\u548c\u5e76\u53d1\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u8d85\u8d8a\u4e2a\u4f53\u667a\u80fd\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5f02\u6b65\u601d\u7ef4\u534f\u8bae\uff0c\u5305\u542b\u7ec4\u7ec7\u8005\u52a8\u6001\u5206\u914d\u5b50\u67e5\u8be2\u7ed9\u5de5\u4f5c\u8005\u3001\u5408\u5e76\u4e2d\u95f4\u77e5\u8bc6\u3001\u751f\u6210\u8fde\u8d2f\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u601d\u7ef4\u7ed3\u6784\u3002", "result": "\u76f8\u6bd4\u5e76\u884c\u601d\u7ef4\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e28%\uff0c\u6570\u5b66\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u4efb\u52a1\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u5f02\u6b65\u601d\u7ef4\u662f\u5b9e\u73b0\u667a\u80fd\u4f53\u7ec4\u7ec7\u534f\u4f5c\u7684\u6709\u6548\u8303\u5f0f\uff0c\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.26752", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26752", "abs": "https://arxiv.org/abs/2510.26752", "authors": ["William Overman", "Mohsen Bayati"], "title": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy", "comment": null, "summary": "As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u6700\u5c0f\u63a7\u5236\u63a5\u53e3\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u81ea\u4e3b\u884c\u52a8\u548c\u8bf7\u6c42\u4eba\u7c7b\u76d1\u7763\u4e4b\u95f4\u9009\u62e9\uff0c\u540c\u65f6\u4eba\u7c7b\u5728\u4fe1\u4efb\u548c\u76d1\u7763\u4e4b\u95f4\u9009\u62e9\u3002\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u52bf\u535a\u5f08\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u5bf9\u9f50\u4fdd\u8bc1\uff1a\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u667a\u80fd\u4f53\u589e\u52a0\u81ea\u4e3b\u6027\u4e0d\u4f1a\u635f\u5bb3\u4eba\u7c7b\u4ef7\u503c\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u6301\u7cfb\u7edf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u63a7\u5236\u63a5\u53e3\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u63a7\u5236\uff0c\u89e3\u51b3\u667a\u80fd\u4f53\u90e8\u7f72\u540e\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u5c06\u4eba\u673a\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e24\u4eba\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff0c\u91cd\u70b9\u5206\u6790\u9a6c\u5c14\u53ef\u592b\u52bf\u535a\u5f08\u60c5\u51b5\uff0c\u5728\u4eba\u7c7b\u4ef7\u503c\u51fd\u6570\u7684\u7ed3\u6784\u5047\u8bbe\u4e0b\u63d0\u4f9b\u5bf9\u9f50\u4fdd\u8bc1\u3002", "result": "\u7f51\u683c\u4e16\u754c\u6a21\u62df\u663e\u793a\uff0c\u901a\u8fc7\u72ec\u7acb\u5b66\u4e60\uff0c\u667a\u80fd\u4f53\u5b66\u4f1a\u5728\u4e0d\u786e\u5b9a\u65f6\u8bf7\u6c42\u5e2e\u52a9\uff0c\u4eba\u7c7b\u5b66\u4f1a\u4f55\u65f6\u76d1\u7763\uff0c\u5f62\u6210\u907f\u514d\u5b89\u5168\u8fdd\u89c4\u7684\u534f\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u90e8\u7f72\u540e\u9519\u4f4d\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b89\u5168\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u900f\u660e\u63a7\u5236\u5c42\u5b9e\u73b0\u53ef\u9884\u6d4b\u7684\u6fc0\u52b1\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2510.26498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26498", "abs": "https://arxiv.org/abs/2510.26498", "authors": ["Adam E. Flanders", "Yifan Peng", "Luciano Prevedello", "Robyn Ball", "Errol Colak", "Prahlad Menon", "George Shih", "Hui-Ming Lin", "Paras Lakhani"], "title": "A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool", "comment": "29 pages, 3 figures, 4 tables", "summary": "Purpose: The purpose of this study was to determine if an ensemble of\nmultiple LLM agents could be used collectively to provide a more reliable\nassessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\nprompt that assessed for presence of ICH. 1,726 examples were manually\nreviewed. Performance characteristics of the eight open-source models and\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\ntested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\n(0.500-0.543). No statistically significant differences were observed between\nTop-3, Full-9, and Consensus (p > 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\nmore consistent and reliable method to derive a ground truth retrospective\nevaluation of a clinical AI triage tool over a single LLM alone.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u660e\u4f7f\u7528\u591a\u4e2a\u5f00\u6e90LLM\u6a21\u578b\u7ec4\u6210\u7684\u96c6\u6210\u7cfb\u7edf\u6bd4\u5355\u4e00LLM\u80fd\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u4e34\u5e8aAI\u5206\u8bca\u5de5\u5177\u7684\u6027\u80fd\u3002", "motivation": "\u786e\u5b9a\u591a\u4e2aLLM\u4ee3\u7406\u7684\u96c6\u6210\u662f\u5426\u80fd\u6bd4\u5355\u4e00LLM\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u50cf\u7d20\u7ea7AI\u5206\u8bca\u5de5\u5177\u8bc4\u4f30\u3002", "method": "\u4f7f\u752829,766\u4f8b\u975e\u5bf9\u6bd4CT\u5934\u90e8\u68c0\u67e5\u6570\u636e\uff0c\u901a\u8fc78\u4e2a\u5f00\u6e90LLM\u6a21\u578b\u548cGPT-4o\u7ec4\u6210\u7684\u96c6\u6210\u7cfb\u7edf\uff0c\u4f7f\u7528\u591a\u8f6e\u63d0\u793a\u8bc4\u4f30\u9885\u5185\u51fa\u8840(ICH)\u5b58\u5728\u60c5\u51b5\uff0c\u5e76\u4e0e\u624b\u52a8\u5ba1\u67e5\u76841,726\u4f8b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "Llama3.3:70b\u548cGPT-4o\u8868\u73b0\u6700\u4f73(AUC=0.78)\uff0cLlama3.3:70b\u5728F1\u5206\u6570(0.81)\u3001\u53ec\u56de\u7387(0.85)\u7b49\u65b9\u9762\u8868\u73b0\u6700\u597d\u3002\u96c6\u6210\u65b9\u6cd5\u5728MCC\u6307\u6807\u4e0a\u4f18\u4e8e\u5355\u4e00GPT-4o\u3002", "conclusion": "\u4e2d\u7b49\u81f3\u5927\u578b\u5f00\u6e90LLM\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u6bd4\u5355\u4e00LLM\u66f4\u4e00\u81f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u83b7\u53d6\u4e34\u5e8aAI\u5206\u8bca\u5de5\u5177\u7684\u56de\u987e\u6027\u8bc4\u4f30\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2510.26243", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26243", "abs": "https://arxiv.org/abs/2510.26243", "authors": ["Hieu M. Vu", "Tan M. Nguyen"], "title": "Angular Steering: Behavior Control via Rotation in Activation Space", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Controlling specific behaviors in large language models while preserving\ntheir general capabilities is a central challenge for safe and reliable\nartificial intelligence deployment. Current steering methods, such as vector\naddition and directional ablation, are constrained within a two-dimensional\nsubspace defined by the activation and feature direction, making them sensitive\nto chosen parameters and potentially affecting unrelated features due to\nunintended interactions in activation space. We introduce Angular Steering, a\nnovel and flexible method for behavior modulation that operates by rotating\nactivations within a fixed two-dimensional subspace. By formulating steering as\na geometric rotation toward or away from a target behavior direction, Angular\nSteering provides continuous, fine-grained control over behaviors such as\nrefusal and compliance. We demonstrate this method using refusal steering\nemotion steering as use cases. Additionally, we propose Adaptive Angular\nSteering, a selective variant that rotates only activations aligned with the\ntarget feature, further enhancing stability and coherence. Angular Steering\ngeneralizes existing addition and orthogonalization techniques under a unified\ngeometric rotation framework, simplifying parameter selection and maintaining\nmodel stability across a broader range of adjustments. Experiments across\nmultiple model families and sizes show that Angular Steering achieves robust\nbehavioral control while maintaining general language modeling performance,\nunderscoring its flexibility, generalization, and robustness compared to prior\napproaches. Code and artifacts are available at\nhttps://github.com/lone17/angular-steering/.", "AI": {"tldr": "\u63d0\u51faAngular Steering\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u65cb\u8f6c\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u8c03\u8282LLM\u884c\u4e3a\uff0c\u5b9e\u73b0\u8fde\u7eed\u7cbe\u7ec6\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u884c\u4e3a\u8c03\u63a7\u65b9\u6cd5\u5c40\u9650\u4e8e\u4e8c\u7ef4\u5b50\u7a7a\u95f4\uff0c\u5bf9\u53c2\u6570\u654f\u611f\u4e14\u53ef\u80fd\u5f71\u54cd\u65e0\u5173\u7279\u5f81\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7a33\u5065\u7684\u8c03\u63a7\u65b9\u6cd5", "method": "\u5728\u56fa\u5b9a\u4e8c\u7ef4\u5b50\u7a7a\u95f4\u5185\u65cb\u8f6c\u6fc0\u6d3b\u5411\u91cf\uff0c\u5c06\u8c03\u63a7\u8868\u8ff0\u4e3a\u51e0\u4f55\u65cb\u8f6c\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u7248\u672c\u4ec5\u65cb\u8f6c\u4e0e\u76ee\u6807\u7279\u5f81\u5bf9\u9f50\u7684\u6fc0\u6d3b", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u5c3a\u5bf8\u4e0a\u9a8c\u8bc1\uff0cAngular Steering\u5b9e\u73b0\u7a33\u5065\u884c\u4e3a\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u5efa\u6a21\u6027\u80fd", "conclusion": "Angular Steering\u5728\u7075\u6d3b\u6027\u3001\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5b89\u5168\u53ef\u9760AI\u90e8\u7f72\u63d0\u4f9b\u6709\u6548\u5de5\u5177", "topic": "agent analysis"}}
{"id": "2510.26575", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26575", "abs": "https://arxiv.org/abs/2510.26575", "authors": ["Kun Luo", "Hongjin Qian", "Zheng Liu", "Ziyi Xia", "Shitao Xiao", "Siqi Bao", "Jun Zhao", "Kang Liu"], "title": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs.", "AI": {"tldr": "InfoFlow\u6846\u67b6\u901a\u8fc7\u5b50\u95ee\u9898\u5206\u89e3\u3001\u5931\u8d25\u5f15\u5bfc\u63d0\u793a\u548c\u53cc\u667a\u80fd\u4f53\u7cbe\u70bc\u6765\u89e3\u51b3\u6df1\u5ea6\u641c\u7d22\u4e2d\u5956\u52b1\u5bc6\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u641c\u7d22\u573a\u666f\u4e2d\u5956\u52b1\u5bc6\u5ea6\u4f4e\uff0c\u667a\u80fd\u4f53\u9700\u8981\u4ed8\u51fa\u5927\u91cf\u63a2\u7d22\u6210\u672c\u624d\u80fd\u83b7\u5f97\u7a00\u5c11\u751a\u81f3\u4e3a\u96f6\u7684\u6700\u7ec8\u5956\u52b1\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60(RLVR)\u7684\u5e94\u7528\u3002", "method": "1) \u5b50\u95ee\u9898\u5206\u89e3\uff1a\u5c06\u957f\u8ddd\u79bb\u4efb\u52a1\u5206\u89e3\u4ee5\u5206\u914d\u8fc7\u7a0b\u5956\u52b1\uff1b2) \u5931\u8d25\u5f15\u5bfc\u63d0\u793a\uff1a\u5411\u505c\u6ede\u8f68\u8ff9\u6ce8\u5165\u7ea0\u6b63\u6307\u5bfc\uff1b3) \u53cc\u667a\u80fd\u4f53\u7cbe\u70bc\uff1a\u4f7f\u7528\u53cc\u667a\u80fd\u4f53\u67b6\u6784\u51cf\u8f7b\u6df1\u5ea6\u63a2\u7d22\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "result": "\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfoFlow\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f7f\u8f7b\u91cf\u7ea7LLM\u80fd\u591f\u8fbe\u5230\u4e0e\u5148\u8fdb\u4e13\u6709LLM\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "InfoFlow\u901a\u8fc7\u7cfb\u7edf\u6027\u89e3\u51b3\u5956\u52b1\u5bc6\u5ea6\u4f18\u5316\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u641c\u7d22\u573a\u666f\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26615", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26615", "abs": "https://arxiv.org/abs/2510.26615", "authors": ["Yiqiao Jin", "Rachneet Kaur", "Zhen Zeng", "Sumitra Ganesh", "Srijan Kumar"], "title": "SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding", "comment": "https://slideagent.github.io/", "summary": "Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall).", "AI": {"tldr": "SlideAgent\u662f\u4e00\u4e2a\u7528\u4e8e\u7406\u89e3\u591a\u6a21\u6001\u3001\u591a\u9875\u9762\u3001\u591a\u5e03\u5c40\u6587\u6863\uff08\u7279\u522b\u662f\u5e7b\u706f\u7247\uff09\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u5728\u5168\u5c40\u3001\u9875\u9762\u548c\u5143\u7d20\u4e09\u4e2a\u7ea7\u522b\u6784\u5efa\u7ed3\u6784\u5316\u8868\u793a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u7684\u591a\u9875\u9762\u89c6\u89c9\u6587\u6863\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5bf9\u5143\u7d20\u548c\u9875\u9762\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a8\u7406\u65b9\u9762\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7406\u89e3\u65b9\u9762\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u7c7b\u590d\u6742\u6587\u6863\u3002", "method": "SlideAgent\u91c7\u7528\u4e13\u95e8\u7684\u4ee3\u7406\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u5168\u5c40\u3001\u9875\u9762\u548c\u5143\u7d20\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u7684\u3001\u4e0e\u67e5\u8be2\u65e0\u5173\u7684\u8868\u793a\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u9009\u62e9\u6027\u5730\u6fc0\u6d3b\u4e13\u95e8\u4ee3\u7406\u8fdb\u884c\u591a\u7ea7\u63a8\u7406\uff0c\u5e76\u5c06\u8f93\u51fa\u6574\u5408\u4e3a\u8fde\u8d2f\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7b54\u6848\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSlideAgent\u76f8\u6bd4\u4e13\u6709\u6a21\u578b\u6574\u4f53\u63d0\u53477.9%\uff0c\u76f8\u6bd4\u5f00\u6e90\u6a21\u578b\u6574\u4f53\u63d0\u53479.8%\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "SlideAgent\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u548c\u4e13\u95e8\u4ee3\u7406\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u9875\u9762\u89c6\u89c9\u6587\u6863\u7684\u7406\u89e3\u95ee\u9898\uff0c\u5728\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2510.26328", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26328", "abs": "https://arxiv.org/abs/2510.26328", "authors": ["David Schmotz", "Sahar Abdelnabi", "Maksym Andriushchenko"], "title": "Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt Injections", "comment": null, "summary": "Enabling continual learning in LLMs remains a key unresolved research\nchallenge. In a recent announcement, a frontier LLM company made a step towards\nthis by introducing Agent Skills, a framework that equips agents with new\nknowledge based on instructions stored in simple markdown files. Although Agent\nSkills can be a very useful tool, we show that they are fundamentally insecure,\nsince they enable trivially simple prompt injections. We demonstrate how to\nhide malicious instructions in long Agent Skill files and referenced scripts to\nexfiltrate sensitive data, such as internal files or passwords. Importantly, we\nshow how to bypass system-level guardrails of a popular coding agent: a benign,\ntask-specific approval with the \"Don't ask again\" option can carry over to\nclosely related but harmful actions. Overall, we conclude that despite ongoing\nresearch efforts and scaling model capabilities, frontier LLMs remain\nvulnerable to very simple prompt injections in realistic scenarios. Our code is\navailable at https://github.com/aisa-group/promptinject-agent-skills.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86Agent Skills\u6846\u67b6\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u5141\u8bb8\u901a\u8fc7\u7b80\u5355\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7a83\u53d6\u654f\u611f\u6570\u636e\uff0c\u5e76\u80fd\u7ed5\u8fc7\u7cfb\u7edf\u7ea7\u9632\u62a4\u673a\u5236\u3002", "motivation": "\u968f\u7740LLM\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684\u53d1\u5c55\uff0cAgent Skills\u6846\u67b6\u88ab\u5f15\u5165\u6765\u589e\u5f3a\u667a\u80fd\u4f53\u77e5\u8bc6\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8be5\u6846\u67b6\u5b58\u5728\u6839\u672c\u6027\u7684\u5b89\u5168\u7f3a\u9677\uff0c\u9700\u8981\u63ed\u793a\u5176\u8106\u5f31\u6027\u3002", "method": "\u901a\u8fc7\u5728\u957fAgent Skill\u6587\u4ef6\u548c\u5f15\u7528\u811a\u672c\u4e2d\u9690\u85cf\u6076\u610f\u6307\u4ee4\uff0c\u6f14\u793a\u5982\u4f55\u7a83\u53d6\u654f\u611f\u6570\u636e\uff0c\u5e76\u5c55\u793a\u5982\u4f55\u5229\u7528\"\u4e0d\u518d\u8be2\u95ee\"\u9009\u9879\u7ed5\u8fc7\u7cfb\u7edf\u9632\u62a4\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u5373\u4f7f\u662f\u6700\u524d\u6cbf\u7684LLM\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u7b80\u5355\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u80fd\u591f\u7ed5\u8fc7\u9632\u62a4\u673a\u5236\u6267\u884c\u6076\u610f\u64cd\u4f5c\u3002", "conclusion": "\u5c3d\u7ba1LLM\u80fd\u529b\u4e0d\u65ad\u6269\u5c55\uff0c\u4f46\u5728\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u4e2d\u4ecd\u7136\u5b58\u5728\u57fa\u672c\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "topic": "agent analysis"}}
{"id": "2510.26347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26347", "abs": "https://arxiv.org/abs/2510.26347", "authors": ["Sebastian Zieglmeier", "Niklas Erdmann", "Narada D. Warakagoda"], "title": "Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle", "comment": null, "summary": "Reinforcement learning (RL) algorithms are designed to optimize\nproblem-solving by learning actions that maximize rewards, a task that becomes\nparticularly challenging in random and nonstationary environments. Even\nadvanced RL algorithms are often limited in their ability to solve problems in\nthese conditions. In applications such as searching for underwater pollution\nclouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate\nreward-sparse environments, where actions frequently result in a zero reward.\nThis paper aims to address these challenges by revisiting and modifying\nclassical RL approaches to efficiently operate in sparse, randomized, and\nnonstationary environments. We systematically study a large number of\nmodifications, including hierarchical algorithm changes, multigoal learning,\nand the integration of a location memory as an external output filter to\nprevent state revisits. Our results demonstrate that a modified Monte\nCarlo-based approach significantly outperforms traditional Q-learning and two\nexhaustive search patterns, illustrating its potential in adapting RL to\ncomplex environments. These findings suggest that reinforcement learning\napproaches can be effectively adapted for use in random, nonstationary, and\nreward-sparse environments.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4fee\u6539\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u7a00\u758f\u3001\u968f\u673a\u548c\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c\uff0c\u7279\u522b\u662f\u5728\u81ea\u4e3b\u6c34\u4e0b\u8f66\u8f86\u641c\u7d22\u6c34\u4e0b\u6c61\u67d3\u4e91\u7b49\u5e94\u7528\u4e2d\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u968f\u673a\u548c\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u7a00\u758f\u7684\u73af\u5883\u4e2d\uff0c\u8bb8\u591a\u52a8\u4f5c\u53ea\u80fd\u83b7\u5f97\u96f6\u5956\u52b1\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u4f7fRL\u7b97\u6cd5\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u591a\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u5305\u62ec\u5206\u5c42\u7b97\u6cd5\u53d8\u66f4\u3001\u591a\u76ee\u6807\u5b66\u4e60\uff0c\u4ee5\u53ca\u96c6\u6210\u4f4d\u7f6e\u8bb0\u5fc6\u4f5c\u4e3a\u5916\u90e8\u8f93\u51fa\u8fc7\u6ee4\u5668\u4ee5\u9632\u6b62\u72b6\u6001\u91cd\u590d\u8bbf\u95ee\u3002\u91c7\u7528\u4fee\u6539\u7684\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u3002", "result": "\u4fee\u6539\u540e\u7684\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edfQ\u5b66\u4e60\u548c\u4e24\u79cd\u7a77\u4e3e\u641c\u7d22\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6f5c\u529b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u9002\u5e94\u968f\u673a\u3001\u975e\u5e73\u7a33\u548c\u5956\u52b1\u7a00\u758f\u7684\u73af\u5883\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26790", "abs": "https://arxiv.org/abs/2510.26790", "authors": ["Hyunji Lee", "Minseon Kim", "Chinmay Singh", "Matheus Pereira", "Atharv Sonwane", "Isadora White", "Elias Stengel-Eskin", "Mohit Bansal", "Zhengyan Shi", "Alessandro Sordoni", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan", "Lucas Caccia"], "title": "Gistify! Codebase-Level Understanding via Runtime Execution", "comment": null, "summary": "As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.", "AI": {"tldr": "Gistify\u662f\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u80fd\u529b\u7684\u4efb\u52a1\uff0c\u8981\u6c42LLM\u4ece\u5b8c\u6574\u4ee3\u7801\u5e93\u4e2d\u63d0\u53d6\u7279\u5b9a\u529f\u80fd\u7684\u7cbe\u7b80\u5b9e\u73b0\u6587\u4ef6\uff0c\u80fd\u590d\u73b0\u76f8\u540c\u547d\u4ee4\u7684\u8f93\u51fa\u7ed3\u679c\u3002", "motivation": "\u968f\u7740\u4ee3\u7801\u4ee3\u7406\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u81ea\u52a8\u8bbe\u8ba1\u5177\u6709\u6311\u6218\u6027\u7684\u4ee3\u7801\u5e93\u7ea7\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGistify\u4efb\u52a1\uff0c\u8ba9\u4ee3\u7801LLM\u57fa\u4e8e\u5b8c\u6574\u4ee3\u7801\u5e93\u548c\u7279\u5b9a\u5165\u53e3\u70b9\uff0c\u521b\u5efa\u6700\u5c0f\u5316\u3001\u81ea\u5305\u542b\u7684\u6587\u4ef6\u6765\u590d\u73b0\u7279\u5b9a\u529f\u80fd\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u89e3\u51b3Gistify\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6267\u884c\u8f68\u8ff9\u8f83\u957f\u7684\u4efb\u52a1\u3002", "conclusion": "Gistify\u4efb\u52a1\u80fd\u6709\u6548\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u5bf9\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u7406\u89e3\u3001\u6267\u884c\u6d41\u5efa\u6a21\u548c\u4ee3\u7801\u8865\u4e01\u751f\u6210\u80fd\u529b\u3002", "topic": "swe benchmark"}}
{"id": "2510.26389", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.26389", "abs": "https://arxiv.org/abs/2510.26389", "authors": ["Wenchang Duan", "Yaoliang Yu", "Jiwan He", "Yi Shi"], "title": "Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning", "comment": null, "summary": "Recently, deep multi-agent reinforcement learning (MARL) has demonstrated\npromising performance for solving challenging tasks, such as long-term\ndependencies and non-Markovian environments. Its success is partly attributed\nto conditioning policies on large fixed context length. However, such large\nfixed context lengths may lead to limited exploration efficiency and redundant\ninformation. In this paper, we propose a novel MARL framework to obtain\nadaptive and effective contextual information. Specifically, we design a\ncentral agent that dynamically optimizes context length via temporal gradient\nanalysis, enhancing exploration to facilitate convergence to global optima in\nMARL. Furthermore, to enhance the adaptive optimization capability of the\ncontext length, we present an efficient input representation for the central\nagent, which effectively filters redundant information. By leveraging a\nFourier-based low-frequency truncation method, we extract global temporal\ntrends across decentralized agents, providing an effective and efficient\nrepresentation of the MARL environment. Extensive experiments demonstrate that\nthe proposed method achieves state-of-the-art (SOTA) performance on long-term\ndependency tasks, including PettingZoo, MiniGrid, Google Research Football\n(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u5085\u91cc\u53f6\u4f4e\u9891\u622a\u65ad\u65b9\u6cd5\uff0c\u63d0\u5347\u63a2\u7d22\u6548\u7387\u548c\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u56fa\u5b9a\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u53ef\u80fd\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u6709\u9650\u548c\u4fe1\u606f\u5197\u4f59\u95ee\u9898\uff0c\u9700\u8981\u81ea\u9002\u5e94\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u83b7\u53d6\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e2d\u5fc3\u667a\u80fd\u4f53\u901a\u8fc7\u65f6\u95f4\u68af\u5ea6\u5206\u6790\u52a8\u6001\u4f18\u5316\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5e76\u4f7f\u7528\u5085\u91cc\u53f6\u4f4e\u9891\u622a\u65ad\u65b9\u6cd5\u63d0\u53d6\u53bb\u4e2d\u5fc3\u5316\u667a\u80fd\u4f53\u7684\u5168\u5c40\u65f6\u95f4\u8d8b\u52bf\uff0c\u8fc7\u6ee4\u5197\u4f59\u4fe1\u606f\u3002", "result": "\u5728\u957f\u671f\u4f9d\u8d56\u4efb\u52a1\uff08PettingZoo\u3001MiniGrid\u3001GRF\u3001SMACv2\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u957f\u5ea6\u4f18\u5316\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u6548\u7387\u548c\u6536\u655b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26475", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26475", "abs": "https://arxiv.org/abs/2510.26475", "authors": ["Qiaoling Chen", "Zijun Liu", "Peng Sun", "Shenggui Li", "Guoteng Wang", "Ziming Liu", "Yonggang Wen", "Siyuan Feng", "Tianwei Zhang"], "title": "ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems", "comment": null, "summary": "Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation.", "AI": {"tldr": "ReSpec\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u89e3\u7801\u914d\u7f6e\u3001\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6f14\u8fdb\u8349\u7a3f\u6a21\u578b\u4ee5\u53ca\u6309\u5956\u52b1\u52a0\u6743\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86\u63a8\u6d4b\u89e3\u7801\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5728Qwen\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e864.5\u500d\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u5956\u52b1\u6536\u655b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u9002\u5e94\u65f6\uff0c\u751f\u6210\u9636\u6bb5\u6d88\u8017\u8d85\u8fc775%\u7684\u8bad\u7ec3\u65f6\u95f4\u3002\u63a8\u6d4b\u89e3\u7801\u5728\u670d\u52a1\u7cfb\u7edf\u4e2d\u52a0\u901f\u81ea\u56de\u5f52\u751f\u6210\uff0c\u4f46\u5176\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u884c\u4e3a\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faReSpec\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u673a\u5236\uff1a\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u89e3\u7801\u914d\u7f6e\u3001\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6f14\u8fdb\u8349\u7a3f\u6a21\u578b\u3001\u6309rollout\u5956\u52b1\u52a0\u6743\u66f4\u65b0\u3002", "result": "\u5728Qwen\u6a21\u578b\uff083B-14B\uff09\u4e0a\u5b9e\u73b0\u9ad8\u8fbe4.5\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u5956\u52b1\u6536\u655b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "ReSpec\u4e3a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.26491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26491", "abs": "https://arxiv.org/abs/2510.26491", "authors": ["Erle Zhu", "Dazhi Jiang", "Yuan Wang", "Xujun Li", "Jiale Cheng", "Yuxian Gu", "Yilin Niu", "Aohan Zeng", "Jie Tang", "Minlie Huang", "Hongning Wang"], "title": "Data-Efficient RLVR via Off-Policy Influence Guidance", "comment": null, "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.", "AI": {"tldr": "\u63d0\u51fa\u4e86CROPI\u6846\u67b6\uff0c\u4f7f\u7528\u5f71\u54cd\u51fd\u6570\u7406\u8bba\u6307\u5bfc\u6570\u636e\u9009\u62e9\uff0c\u901a\u8fc7\u79bb\u7ebf\u5f71\u54cd\u4f30\u8ba1\u548c\u7a00\u758f\u968f\u673a\u6295\u5f71\u6280\u672f\uff0c\u5728RLVR\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u52a0\u901f", "motivation": "\u5f53\u524dRLVR\u4e2d\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u542f\u53d1\u5f0f\uff0c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u548c\u6cdb\u5316\u6027\uff0c\u9700\u8981\u7406\u8bba\u6307\u5bfc\u7684\u9ad8\u6548\u6570\u636e\u9009\u62e9\u65b9\u6cd5", "method": "\u4f7f\u7528\u5f71\u54cd\u51fd\u6570\u4f30\u8ba1\u6570\u636e\u70b9\u5bf9\u5b66\u4e60\u76ee\u6807\u7684\u8d21\u732e\uff0c\u63d0\u51fa\u79bb\u7ebf\u5f71\u54cd\u4f30\u8ba1\u65b9\u6cd5\u907f\u514d\u7b56\u7565\u5c55\u5f00\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u91c7\u7528\u7a00\u758f\u968f\u673a\u6295\u5f71\u964d\u4f4eLLM\u9ad8\u7ef4\u68af\u5ea6\u7ef4\u5ea6", "result": "\u57287B\u53c2\u6570\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c1.5B\u6a21\u578b\u5b9e\u73b02.66\u500d\u6b65\u9aa4\u7ea7\u52a0\u901f\uff0c\u6bcf\u9636\u6bb5\u4ec5\u4f7f\u752810%\u6570\u636e", "conclusion": "\u57fa\u4e8e\u5f71\u54cd\u7684\u6570\u636e\u9009\u62e9\u5728\u9ad8\u6548RLVR\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cCROPI\u6846\u67b6\u663e\u8457\u52a0\u901f\u8bad\u7ec3", "topic": "agentic reinforcement learning"}}
{"id": "2510.26787", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26787", "abs": "https://arxiv.org/abs/2510.26787", "authors": ["Mantas Mazeika", "Alice Gatti", "Cristina Menghini", "Udari Madhushani Sehwag", "Shivam Singhal", "Yury Orlovskiy", "Steven Basart", "Manasi Sharma", "Denis Peskoff", "Elaine Lau", "Jaehyuk Lim", "Lachlan Carroll", "Alice Blair", "Vinaya Sivakumar", "Sumana Basu", "Brad Kenstler", "Yuntao Ma", "Julian Michael", "Xiaoke Li", "Oliver Ingebretsen", "Aditya Mehta", "Jean Mottola", "John Teichmann", "Kevin Yu", "Zaina Shaik", "Adam Khoja", "Richard Ren", "Jason Hausenloy", "Long Phan", "Ye Htet", "Ankit Aich", "Tahseen Rabbani", "Vivswan Shah", "Andriy Novykov", "Felix Binder", "Kirill Chugunov", "Luis Ramirez", "Matias Geralnik", "Hern\u00e1n Mesura", "Dean Lee", "Ed-Yeremai Hernandez Cardona", "Annette Diamond", "Summer Yue", "Alexandr Wang", "Bing Liu", "Ernesto Hernandez", "Dan Hendrycks"], "title": "Remote Labor Index: Measuring AI Automation of Remote Work", "comment": "Website: https://www.remotelabor.ai", "summary": "AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.", "AI": {"tldr": "AI\u5728\u77e5\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u6b65\u8fc5\u901f\uff0c\u4f46\u7ecf\u6d4e\u4ef7\u503c\u8f6c\u5316\u4e0d\u660e\u786e\u3002\u7814\u7a76\u8005\u5f15\u5165\u8fdc\u7a0b\u52b3\u52a8\u6307\u6570(RLI)\u6765\u8861\u91cfAI\u4ee3\u7406\u5728\u771f\u5b9e\u7ecf\u6d4e\u9879\u76ee\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0AI\u4ee3\u7406\u81ea\u52a8\u5316\u7387\u4ec5\u4e3a2.5%\u3002", "motivation": "\u8bc4\u4f30AI\u5728\u77e5\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8fdb\u6b65\u5982\u4f55\u8f6c\u5316\u4e3a\u5b9e\u9645\u7ecf\u6d4e\u4ef7\u503c\u548c\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u4e3aAI\u9a71\u52a8\u7684\u52b3\u52a8\u529b\u81ea\u52a8\u5316\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u5f15\u5165\u8fdc\u7a0b\u52b3\u52a8\u6307\u6570(RLI)\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u884c\u4e1a\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u3001\u5177\u6709\u7ecf\u6d4e\u4ef7\u503c\u7684\u9879\u76ee\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u5b9e\u8df5\u73af\u5883\u4e2d\u7684\u7aef\u5230\u7aef\u6027\u80fd\u3002", "result": "AI\u4ee3\u7406\u5728RLI\u4e0a\u8868\u73b0\u63a5\u8fd1\u5e95\u7ebf\uff0c\u8868\u73b0\u6700\u4f73\u7684\u4ee3\u7406\u81ea\u52a8\u5316\u7387\u4ec5\u4e3a2.5%\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u6709\u52a9\u4e8e\u57fa\u4e8e\u5b9e\u8bc1\u8bc1\u636e\u8ba8\u8bbaAI\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u4e3a\u8ddf\u8e2aAI\u5f71\u54cd\u548c\u5229\u76ca\u76f8\u5173\u8005\u4e3b\u52a8\u5e94\u5bf9AI\u9a71\u52a8\u7684\u52b3\u52a8\u529b\u81ea\u52a8\u5316\u5efa\u7acb\u5171\u540c\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.26788", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26788", "abs": "https://arxiv.org/abs/2510.26788", "authors": ["Penghui Qi", "Zichen Liu", "Xiangxin Zhou", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Defeating the Training-Inference Mismatch via FP16", "comment": null, "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0FP16\u6bd4BF16\u5728LLM\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e2d\u66f4\u7a33\u5b9a\uff0c\u80fd\u6d88\u9664\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u4e4b\u95f4\u7684\u6570\u503c\u4e0d\u5339\u914d\u95ee\u9898", "motivation": "\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e2d\u56e0\u6d6e\u70b9\u7cbe\u5ea6\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u7b56\u7565\u4e0d\u5339\u914d\u95ee\u9898", "method": "\u5c06\u6d6e\u70b9\u7cbe\u5ea6\u4eceBF16\u6539\u4e3aFP16\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u5b66\u4e60\u7b97\u6cd5", "result": "FP16\u5e26\u6765\u66f4\u7a33\u5b9a\u7684\u4f18\u5316\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u5f3a\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "FP16\u5728RL\u5fae\u8c03\u4e2d\u4f18\u4e8eBF16\uff0c\u5efa\u8bae\u91cd\u65b0\u8003\u8651\u7cbe\u5ea6\u6743\u8861", "topic": "agentic reinforcement learning"}}
{"id": "2510.26782", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26782", "abs": "https://arxiv.org/abs/2510.26782", "authors": ["Zaishuo Xia", "Yukuan Lu", "Xinyi Li", "Yifan Xu", "Yubei Chen"], "title": "Clone Deterministic 3D Worlds with Geometrically-Regularized World Models", "comment": null, "summary": "A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u51e0\u4f55\u6b63\u5219\u5316\u4e16\u754c\u6a21\u578b\uff08GRWM\uff09\uff0c\u901a\u8fc7\u5f3a\u5236\u81ea\u7136\u611f\u5b98\u8f68\u8ff9\u4e2d\u7684\u8fde\u7eed\u70b9\u5728\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u4fdd\u6301\u63a5\u8fd1\uff0c\u663e\u8457\u6539\u5584\u4e86\u6f5c\u5728\u8868\u793a\u8d28\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4e16\u754c\u6a21\u578b\u7684\u957f\u671f\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4e16\u754c\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8868\u793a\u8d28\u91cf\u95ee\u9898\u2014\u2014\u9ad8\u7ef4\u5916\u90e8\u8f93\u5165\u548c\u635f\u5931/\u7ea0\u7f20\u7684\u6f5c\u5728\u8868\u793a\u4f7f\u52a8\u6001\u5b66\u4e60\u53d8\u5f97\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u6765\u663e\u8457\u63d0\u5347\u4e16\u754c\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faGRWM\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5b9e\u65bd\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u786e\u4fdd\u8fde\u7eed\u89c2\u6d4b\u70b9\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4fdd\u6301\u90bb\u8fd1\u5173\u7cfb\u3002\u8be5\u65b9\u6cd5\u5373\u63d2\u5373\u7528\uff0c\u4ec5\u9700\u6700\u5c0f\u67b6\u6784\u4fee\u6539\uff0c\u4e0e\u591a\u79cd\u6f5c\u5728\u751f\u6210\u4e3b\u5e72\u517c\u5bb9\u3002", "result": "\u5728\u786e\u5b9a\u60273D\u73af\u5883\u548c\u957f\u671f\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cGRWM\u663e\u8457\u63d0\u9ad8\u4e86rollout\u4fdd\u771f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002\u5206\u6790\u8868\u660e\u5176\u4f18\u52bf\u6e90\u4e8e\u5b66\u4e60\u5230\u5177\u6709\u4f18\u8d8a\u51e0\u4f55\u7ed3\u6784\u7684\u6f5c\u5728\u6d41\u5f62\u3002", "conclusion": "\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u662f\u6784\u5efa\u9c81\u68d2\u4e16\u754c\u6a21\u578b\u7684\u76f4\u63a5\u6709\u6548\u9014\u5f84\uff0c\u80fd\u591f\u5728\u65e0\u9700\u6269\u5927\u52a8\u6001\u6a21\u5757\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u957f\u671f\u9884\u6d4b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.2cbb288b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fcomposer%3Futm_source=tldrwebdev/1/0100019a34ce511c-254779a5-7ea5-4067-b8fe-1a9633d438d1-000000/X5Cj-Tcgbb8gySnRIrK2AhU93p0fK1uVNQuKwA9-tC0=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fcomposer%3Futm_source=tldrwebdev/1/0100019a34ce511c-254779a5-7ea5-4067-b8fe-1a9633d438d1-000000/X5Cj-Tcgbb8gySnRIrK2AhU93p0fK1uVNQuKwA9-tC0=429", "authors": ["TLDR Newsletter"], "title": "Composer: Building a fast frontier model with RL", "comment": "Source: TLDR Newsletter, Date: 2025-10-30, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fcomposer%3Futm_source=tldrwebdev/1/0100019a34ce511c-254779a5-7ea5-4067-b8fe-1a9633d438d1-000000/X5Cj-Tcgbb8gySnRIrK2AhU93p0fK1uVNQuKwA9-tC0=429", "summary": "Composer: Building a fast frontier model with RL (5 minute read) Cursor has developed Composer, a new agent model for software engineering that achieves frontier coding results four times faster than similar models. Composer is a mixture-of-experts model trained with reinforcement learning to solve real-world software engineering challenges using various tools, including code editing and semantic search.", "source": "tldr", "AI": {"tldr": "Cursor\u5f00\u53d1\u4e86Composer\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u6bd4\u540c\u7c7b\u6a21\u578b\u5feb4\u500d\u8fbe\u5230\u524d\u6cbf\u7f16\u7801\u7ed3\u679c\u3002", "motivation": "\u5f00\u53d1\u66f4\u5feb\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u6a21\u578b\u6765\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u4ee3\u7801\u7f16\u8f91\u548c\u8bed\u4e49\u641c\u7d22\u7b49\u591a\u79cd\u5de5\u5177\u3002", "result": "Composer\u6bd4\u7c7b\u4f3c\u6a21\u578b\u5feb4\u500d\u8fbe\u5230\u524d\u6cbf\u7f16\u7801\u7ed3\u679c\u3002", "conclusion": "Composer\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u6a21\u578b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7f16\u7801\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2510.625f021b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Fllama-meta-ai%3Futm_source=tldrwebdev/1/0100019a34ce511c-254779a5-7ea5-4067-b8fe-1a9633d438d1-000000/27X171kh-CvR8b8Yi9lvEyW1amRSUg-eA6RlInwocDw=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Fllama-meta-ai%3Futm_source=tldrwebdev/1/0100019a34ce511c-254779a5-7ea5-4067-b8fe-1a9633d438d1-000000/27X171kh-CvR8b8Yi9lvEyW1amRSUg-eA6RlInwocDw=429", "authors": ["TLDR Newsletter"], "title": "Is Llama really as bad as people say? I put Meta's AI to the test", "comment": "Source: TLDR Newsletter, Date: 2025-10-30, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Fllama-meta-ai%3Futm_source=tldrwebdev/1/0100019a34ce511c-254779a5-7ea5-4067-b8fe-1a9633d438d1-000000/27X171kh-CvR8b8Yi9lvEyW1amRSUg-eA6RlInwocDw=429", "summary": "Is Llama really as bad as people say? I put Meta's AI to the test (11 minute read) Meta's Llama 3.2 1B model is a useful, lightweight coding assistant for development work. It's not as bad as critics claim. However, it can't handle agentic coding and produces functional but imperfect code, constantly requiring manual fixes. Llama is best for simple projects like todo apps.", "source": "tldr", "AI": {"tldr": "Meta\u7684Llama 3.2 1B\u6a21\u578b\u662f\u4e00\u4e2a\u6709\u7528\u7684\u8f7b\u91cf\u7ea7\u7f16\u7a0b\u52a9\u624b\uff0c\u9002\u5408\u5f00\u53d1\u5de5\u4f5c\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u667a\u80fd\u7f16\u7a0b\u4efb\u52a1\uff0c\u751f\u6210\u7684\u4ee3\u7801\u9700\u8981\u624b\u52a8\u4fee\u590d\uff0c\u6700\u9002\u5408\u7b80\u5355\u9879\u76ee\u3002", "motivation": "\u8bc4\u4f30Meta\u7684Llama 3.2 1B\u6a21\u578b\u5728\u5b9e\u9645\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u56de\u5e94\u6279\u8bc4\u8005\u7684\u8d1f\u9762\u8bc4\u4ef7\u3002", "method": "\u901a\u8fc7\u5b9e\u9645\u6d4b\u8bd5Llama 3.2 1B\u6a21\u578b\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u7b80\u5355\u9879\u76ee\u548c\u590d\u6742\u4efb\u52a1\u3002", "result": "Llama 3.2 1B\u6a21\u578b\u662f\u4e00\u4e2a\u6709\u7528\u7684\u8f7b\u91cf\u7ea7\u7f16\u7a0b\u52a9\u624b\uff0c\u80fd\u751f\u6210\u529f\u80fd\u6027\u7684\u4ee3\u7801\uff0c\u4f46\u9700\u8981\u624b\u52a8\u4fee\u590d\uff0c\u65e0\u6cd5\u5904\u7406\u667a\u80fd\u7f16\u7a0b\u4efb\u52a1\uff0c\u6700\u9002\u5408\u7b80\u5355\u9879\u76ee\u5982\u5f85\u529e\u4e8b\u9879\u5e94\u7528\u3002", "conclusion": "Llama 3.2 1B\u6a21\u578b\u5e76\u975e\u5982\u6279\u8bc4\u8005\u6240\u8bf4\u7684\u90a3\u4e48\u7cdf\u7cd5\uff0c\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u8f7b\u91cf\u7ea7\u7f16\u7a0b\u52a9\u624b\uff0c\u4f46\u6709\u5176\u5c40\u9650\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2510.1e32ccee", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagent.minimax.io%2F%3Futm_source=newsletter%26utm_campaign=tldr/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/joKtmHQFV-W7D47wUZ8TFPVoS5Pdyz3bqnoeqedlks4=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagent.minimax.io%2F%3Futm_source=newsletter%26utm_campaign=tldr/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/joKtmHQFV-W7D47wUZ8TFPVoS5Pdyz3bqnoeqedlks4=429", "authors": ["TLDR Newsletter"], "title": "MiniMax M2 \u2014 Open-Sourced & Free", "comment": "Source: TLDR Newsletter, Date: 2025-10-30, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagent.minimax.io%2F%3Futm_source=newsletter%26utm_campaign=tldr/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/joKtmHQFV-W7D47wUZ8TFPVoS5Pdyz3bqnoeqedlks4=429", "summary": "MiniMax M2 \u2014 Open-Sourced & Free (Sponsor) Built for Agents & Code, 2\u00d7 faster at 8% of Claude Sonnet's price. Create, code, and deploy smarter with selective parameter activation.Try it now: MiniMax Agent | MiniMax-M2 API | Open-source for local use.", "source": "tldr", "AI": {"tldr": "MiniMax M2\u662f\u4e00\u4e2a\u5f00\u6e90\u514d\u8d39\u7684AI\u6a21\u578b\uff0c\u4e13\u4e3a\u667a\u80fd\u4f53\u548c\u4ee3\u7801\u4efb\u52a1\u8bbe\u8ba1\uff0c\u6bd4Claude Sonnet\u5feb2\u500d\u4e14\u4ef7\u683c\u4ec5\u4e3a8%", "motivation": "\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u9ad8\u6027\u80fd\u3001\u4f4e\u6210\u672c\u7684AI\u6a21\u578b\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u548c\u4ee3\u7801\u751f\u6210\u9886\u57df", "method": "\u5f00\u53d1\u4e86MiniMax M2\u6a21\u578b\uff0c\u91c7\u7528\u9009\u62e9\u6027\u53c2\u6570\u6fc0\u6d3b\u6280\u672f\uff0c\u63d0\u4f9bAPI\u63a5\u53e3\u548c\u672c\u5730\u5f00\u6e90\u7248\u672c", "result": "\u6a21\u578b\u6027\u80fd\u8fbe\u5230Claude Sonnet\u76842\u500d\u901f\u5ea6\uff0c\u6210\u672c\u4ec5\u4e3a8%\uff0c\u652f\u6301\u667a\u80fd\u4f53\u521b\u5efa\u3001\u4ee3\u7801\u7f16\u5199\u548c\u90e8\u7f72", "conclusion": "MiniMax M2\u4e3aAI\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u667a\u80fd\u4f53\u548c\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55", "topic": "code agent"}}
{"id": "tldr.2510.8d1498d3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fswe-1-5%3Futm_source=tldrai/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/3csmTJHpVNjpjfjjWNLvt9SbeC7FxYunwvva6NW9xas=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fswe-1-5%3Futm_source=tldrai/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/3csmTJHpVNjpjfjjWNLvt9SbeC7FxYunwvva6NW9xas=429", "authors": ["TLDR Newsletter"], "title": "Introducing SWE-1.5: Our Fast Agent Model", "comment": "Source: TLDR Newsletter, Date: 2025-10-30, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fswe-1-5%3Futm_source=tldrai/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/3csmTJHpVNjpjfjjWNLvt9SbeC7FxYunwvva6NW9xas=429", "summary": "Introducing SWE-1.5: Our Fast Agent Model (8 minute read) SWE-1.5 is a frontier-sized model with hundreds of billions of parameters optimized for software engineering. It achieves near state-of-the-art coding performance and sets a new standard for speed. The model is now available in Windsurf, serving at up to 950 tokens per second. It can be used to deeply explore and understand large codebases, build end-to-end full-stack apps, and easily edit configurations without needing to memorize fie...", "source": "tldr", "AI": {"tldr": "SWE-1.5\u662f\u4e00\u4e2a\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4f18\u5316\u7684\u524d\u6cbf\u89c4\u6a21\u6a21\u578b\uff0c\u5177\u6709\u6570\u5343\u4ebf\u53c2\u6570\uff0c\u5728\u7f16\u7801\u6027\u80fd\u4e0a\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u8bbe\u5b9a\u4e86\u65b0\u7684\u901f\u5ea6\u6807\u51c6\uff0c\u5728Windsurf\u4e2d\u53ef\u8fbe950 tokens/\u79d2\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4f18\u5316\u7684\u9ad8\u901fAI\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u7684\u6df1\u5ea6\u63a2\u7d22\u3001\u5168\u6808\u5e94\u7528\u6784\u5efa\u548c\u914d\u7f6e\u7f16\u8f91\u7b49\u4efb\u52a1\u3002", "method": "\u6784\u5efa\u5177\u6709\u6570\u5343\u4ebf\u53c2\u6570\u7684\u524d\u6cbf\u89c4\u6a21\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u5728Windsurf\u5e73\u53f0\u4e0a\u90e8\u7f72\u4ee5\u5b9e\u73b0\u9ad8\u901f\u670d\u52a1\u3002", "result": "\u6a21\u578b\u5728\u7f16\u7801\u6027\u80fd\u4e0a\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u670d\u52a1\u901f\u5ea6\u53ef\u8fbe950 tokens/\u79d2\uff0c\u80fd\u591f\u6df1\u5ea6\u7406\u89e3\u5927\u578b\u4ee3\u7801\u5e93\u3001\u6784\u5efa\u5168\u6808\u5e94\u7528\u548c\u7f16\u8f91\u914d\u7f6e\u3002", "conclusion": "SWE-1.5\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u8bbe\u5b9a\u4e86\u65b0\u7684\u901f\u5ea6\u548c\u6027\u80fd\u6807\u51c6\uff0c\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u4ee3\u7801\u52a9\u624b\u6a21\u578b\u3002", "topic": "code agent"}}
{"id": "tldr.2510.5a21668d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagent-labs%3Futm_source=tldrai/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/S-JfYGYlkGAdeO-wcd5cjjlTgKOmco6iik_vHe-Cqx4=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagent-labs%3Futm_source=tldrai/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/S-JfYGYlkGAdeO-wcd5cjjlTgKOmco6iik_vHe-Cqx4=429", "authors": ["TLDR Newsletter"], "title": "Agent Labs Are Eating the Software World", "comment": "Source: TLDR Newsletter, Date: 2025-10-30, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagent-labs%3Futm_source=tldrai/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/S-JfYGYlkGAdeO-wcd5cjjlTgKOmco6iik_vHe-Cqx4=429", "summary": "Agent Labs Are Eating the Software World (8 minute read) Agent labs ship product first and build infrastructure later. They turn AI models into goal-directed systems that deliver outcomes, capturing the real value in the AI stack. Founders only need a deep understanding of a domain and the ability to build reliable workflows on top of existing models. The most valuable developer skills are shifting from model architecture to system design, evaluation engineering, and domain-specific workflow ...", "source": "tldr", "AI": {"tldr": "Agent labs focus on product-first approach, building goal-directed AI systems that deliver outcomes and capture value in AI stack. Key skills shift from model architecture to system design and domain-specific workflow expertise.", "motivation": "To highlight the emerging trend where agent labs prioritize product delivery over infrastructure, emphasizing the importance of turning AI models into practical, outcome-driven systems.", "method": "Agent labs adopt a product-first strategy, building reliable workflows on existing models rather than focusing on model architecture development.", "result": "This approach allows agent labs to capture real value in the AI stack by delivering functional systems that solve domain-specific problems effectively.", "conclusion": "The most valuable developer skills are evolving from model architecture expertise to system design, evaluation engineering, and domain-specific workflow development.", "topic": "agent analysis"}}
{"id": "tldr.2510.09a23f43", "categories": ["tldr.article"], "pdf": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fxano.com%2Fgo%3Futm_medium=affiliate%26utm_source=tldr%26utm_campaign=tldr_vibe_coding/2/0100019a39cb5dab-dc276273-a1e7-4fb2-bd18-cf96ba9283f8-000000/_QmPySlFWnaRmPkUpDej_WlgCt4SHX0rLGQlh0la3wM=429", "abs": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fxano.com%2Fgo%3Futm_medium=affiliate%26utm_source=tldr%26utm_campaign=tldr_vibe_coding/2/0100019a39cb5dab-dc276273-a1e7-4fb2-bd18-cf96ba9283f8-000000/_QmPySlFWnaRmPkUpDej_WlgCt4SHX0rLGQlh0la3wM=429", "authors": ["TLDR Newsletter"], "title": "How to Fix the #1 Problem with Vibe Coding", "comment": "Source: TLDR Newsletter, Date: 2025-10-31, Reading time: Sponsor, Links: http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fxano.com%2Fgo%3Futm_medium=affiliate%26utm_source=tldr%26utm_campaign=tldr_vibe_coding/2/0100019a39cb5dab-dc276273-a1e7-4fb2-bd18-cf96ba9283f8-000000/_QmPySlFWnaRmPkUpDej_WlgCt4SHX0rLGQlh0la3wM=429", "summary": "How to Fix the #1 Problem with Vibe Coding (Sponsor) Vibe coding makes building fast\u2014but hides what's really happening. All of the logic, architecture, and security live in hundreds of lines of code. Even to a trained engineer, the sheer volume can make it impractical to parse.Vibe code without the black box.Xano lets you build with AI and see your entire backend logic visually, so you can ship faster and stay in control.No black boxes. Just visibility, flexibility, and a production-ready bac...", "source": "tldr", "AI": {"tldr": "Xano\u5e73\u53f0\u901a\u8fc7\u53ef\u89c6\u5316\u65b9\u5f0f\u89e3\u51b3AI\u7f16\u7a0b\u4e2d\u7684\u9ed1\u76d2\u95ee\u9898\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u6e05\u6670\u770b\u5230\u540e\u7aef\u903b\u8f91\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u53ef\u63a7\u6027", "motivation": "\u89e3\u51b3AI\u7f16\u7a0b\u4e2d\u4ee3\u7801\u903b\u8f91\u4e0d\u900f\u660e\u3001\u67b6\u6784\u6df7\u4e71\u7684\u95ee\u9898\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u63a7\u5236\u751f\u6210\u7684\u4ee3\u7801", "method": "\u63d0\u4f9b\u53ef\u89c6\u5316\u540e\u7aef\u5f00\u53d1\u5e73\u53f0\uff0c\u5c06AI\u751f\u6210\u7684\u4ee3\u7801\u903b\u8f91\u4ee5\u56fe\u5f62\u5316\u65b9\u5f0f\u5c55\u793a\uff0c\u6d88\u9664\u9ed1\u76d2\u6548\u5e94", "result": "\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u5feb\u5730\u6784\u5efa\u5e94\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u4ee3\u7801\u67b6\u6784\u548c\u5b89\u5168\u6027\u7684\u5b8c\u5168\u63a7\u5236", "conclusion": "\u53ef\u89c6\u5316AI\u7f16\u7a0b\u5de5\u5177\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4ee3\u7801\u9ed1\u76d2\u95ee\u9898\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf", "topic": "swe application"}}
{"id": "wechat.2510.de89c8d4", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483793&idx=1&sn=1a7af2be1afb9633fc7dc9f4482ec601&chksm=e9f82ab34e5aede85fade5f56db06fb609670bc6d4c03a3b4e1fb7ffd61a578f39733672d713#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483793&idx=1&sn=1a7af2be1afb9633fc7dc9f4482ec601&chksm=e9f82ab34e5aede85fade5f56db06fb609670bc6d4c03a3b4e1fb7ffd61a578f39733672d713#rd", "authors": ["CodeAgent\u4ee3\u7801\u667a\u80fd"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u00b7 Code LLMs \u00b7 \u521d\u521b\u52a8\u6001\uff0810.31\uff09", "comment": "Source: WeChat, Published: 2025-10-31 05:38:31", "summary": "1\uff5cAgent / Release \u2014 Cognition \u53d1\u5e03 SWE1.5\uff0c\u901f\u5ea6\u8d85\u8d8a\u73b0\u6709\u7f16\u7801\u6a21\u578b2\uff5cStartup / Release \u2014 Cursor 2.0 \u4e0e Composer\uff1a\u9ad8\u901f MoE \u7f16\u7801\u6a21\u578b\u548c\u591a\u4ee3\u7406\u63a5\u53e33\uff5cAgent / Startup \u2014 Tiger Data \u5f00\u6e90 Slack \u539f\u751f\u751f\u4ea7\u4ee3\u7406 Eon \u53ca\u914d\u5957\u7ec4\u4ef6", "AI": {"tldr": "1\uff5cAgent / Release \u2014 Cognition \u53d1\u5e03 SWE1.5\uff0c\u901f\u5ea6\u8d85\u8d8a\u73b0\u6709\u7f16\u7801\u6a21\u578b2\uff5cStartup / Release \u2014 Cursor 2.0 \u4e0e Composer\uff1a\u9ad8\u901f MoE \u7f16\u7801\u6a21\u578b\u548c\u591a\u4ee3\u7406\u63a5\u53e33\uff5cAgent / Startup \u2014 Tiger Data \u5f00\u6e90 Slack \u539f\u751f\u751f\u4ea7\u4ee3\u7406 Eon \u53ca\u914d\u5957\u7ec4\u4ef6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.6cc3f89b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3Njk5ODg4OQ==&mid=2247493591&idx=1&sn=52e4e8a25587a5d905fffd8c234424e4&chksm=ea0e63f33b30a279968e4846bc67f609a5576268f3c589916a934a0c7e17f7b4f63c5c21a77f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3Njk5ODg4OQ==&mid=2247493591&idx=1&sn=52e4e8a25587a5d905fffd8c234424e4&chksm=ea0e63f33b30a279968e4846bc67f609a5576268f3c589916a934a0c7e17f7b4f63c5c21a77f#rd", "authors": ["\u5c0f\u76d2\u5b50\u7684\u6280\u672f\u5206\u4eab"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5f00\u53d1\u5168\u666f\u56fe\uff08LLM Deployment Landscape\uff09", "comment": "Source: WeChat, Published: 2025-10-31 13:25:34", "summary": "\u25cfMaxKB https\uff1a//github.com/1Panel-dev/MaxKB\u25cfFastGPT https\uff1a//github.com/labring/FastGPT\u25cfFlowise AI https\uff1a//flowiseai.com/Agent Tool / Dev Kit / Protocol\u25cfLiteLLM https\uff1a//docs.litellm.ai/\u25cfSupabase https\uff1a//supabase.com/", "AI": {"tldr": "\u25cfMaxKB https\uff1a//github.com/1Panel-dev/MaxKB\u25cfFastGPT https\uff1a//github.com/labring/FastGPT\u25cfFlowise AI https\uff1a//flowiseai.com/Agent Tool / Dev Kit / Protocol\u25cfLiteLLM https\uff1a//docs.litellm.ai/\u25cfSupabase https\uff1a/...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.b2b48330", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxOTkzMjgwMQ==&mid=2247515104&idx=1&sn=35406b6f81d8279ea7039e762282c070&chksm=9d8cb6f44fab9914f0d59c04b91befb0ba381f35c255fbf851589bd420b07e36dfc93fae0c5d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxOTkzMjgwMQ==&mid=2247515104&idx=1&sn=35406b6f81d8279ea7039e762282c070&chksm=9d8cb6f44fab9914f0d59c04b91befb0ba381f35c255fbf851589bd420b07e36dfc93fae0c5d#rd", "authors": ["\u8363\u8f89\u4fe1\u606f\u79d1\u6280"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u5173\u952e\u6280\u672f\u4e0e\u6311\u6218", "comment": "Source: WeChat, Published: 2025-10-31 09:56:25", "summary": "\u4ece 2018 \u5e74\u5230 2025 \u5e74\uff0c\u6211\u4eec\u89c1\u8bc1\u4e86\u6a21\u578b\u968f\u7740\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u4ece\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u3001\u53ea\u6709\u51e0\u5343\u4e07\u53c2\u6570\u6a21\u578b\uff0c\u589e\u957f\u5230\u4e0a\u4e07\u4ebf\u7684\u89c4\u6a21\uff0c\u662f\u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u8111\u5bb9\u91cf\u8d8a\u6765\u8d8a\u5927\u7684\u8fc7\u7a0b\uff0c\u5b83\u5177\u6709\u66f4\u5f3a\u7684\u8ba4\u77e5\u4e16\u754c\u7684\u80fd\u529b\u3002", "AI": {"tldr": "\u4ece 2018 \u5e74\u5230 2025 \u5e74\uff0c\u6211\u4eec\u89c1\u8bc1\u4e86\u6a21\u578b\u968f\u7740\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u4ece\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u3001\u53ea\u6709\u51e0\u5343\u4e07\u53c2\u6570\u6a21\u578b\uff0c\u589e\u957f\u5230\u4e0a\u4e07\u4ebf\u7684\u89c4\u6a21\uff0c\u662f\u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u8111\u5bb9\u91cf\u8d8a\u6765\u8d8a\u5927\u7684\u8fc7\u7a0b\uff0c\u5b83\u5177\u6709\u66f4\u5f3a\u7684\u8ba4\u77e5\u4e16\u754c\u7684\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.4cd50419", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5NDY5NzMwNQ==&mid=2649309644&idx=1&sn=876ee368feddcc44971b834eaea429bb&chksm=8946e1be1fcf0d37c8ffb140d71a0a3dff1510b27ede6118b431197a2b000ace7bd06a1fe02b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5NDY5NzMwNQ==&mid=2649309644&idx=1&sn=876ee368feddcc44971b834eaea429bb&chksm=8946e1be1fcf0d37c8ffb140d71a0a3dff1510b27ede6118b431197a2b000ace7bd06a1fe02b#rd", "authors": ["\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\u7f51\u7edc\u4fe1\u606f\u4e2d\u5fc3"], "title": "AI\u201c\u767e\u666f\u201d | \u4e0a\u6d77\u4ea4\u5927\u56e2\u961f\u5f00\u53d1\u8111\u7535<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff0c\u8f85\u52a9\u766b\u75eb\u68c0\u6d4b\u548c\u60c5\u7eea\u8bc6\u522b", "comment": "Source: WeChat, Published: 2025-10-31 09:17:15", "summary": "\u8fed\u4ee3\u5347\u7ea7\uff1a\u7b2c\u4e8c\u4ee3\u8111\u7535\u5927\u6a21\u578bGram\u4e3a\u4e86\u5b9e\u73b0\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u5e94\u7528\uff0c\u56e2\u961f\u5f00\u542f\u4e86\u7b2c\u4e8c\u4ee3\u8111\u7535\u5927\u6a21\u578bGram\u7684\u8bad\u7ec3\u3002\u8be5\u6a21\u578b\u91c7\u53d6\u4e86\u65b0\u7684\u591a\u89c6\u89d2\u5c42\u7f16\u7801\u5668\u7684\u7ed3\u6784\uff0c\u53ef\u4ee5\u589e\u5f3a\u8111\u7535\u56fe\u6570\u636e\u7684\u8868\u5f81\u5b66\u4e60\u6548\u7387\u3002", "AI": {"tldr": "\u8fed\u4ee3\u5347\u7ea7\uff1a\u7b2c\u4e8c\u4ee3\u8111\u7535\u5927\u6a21\u578bGram\u4e3a\u4e86\u5b9e\u73b0\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u5e94\u7528\uff0c\u56e2\u961f\u5f00\u542f\u4e86\u7b2c\u4e8c\u4ee3\u8111\u7535\u5927\u6a21\u578bGram\u7684\u8bad\u7ec3\u3002\u8be5\u6a21\u578b\u91c7\u53d6\u4e86\u65b0\u7684\u591a\u89c6\u89d2\u5c42\u7f16\u7801\u5668\u7684\u7ed3\u6784\uff0c\u53ef\u4ee5\u589e\u5f3a\u8111\u7535\u56fe\u6570\u636e\u7684\u8868\u5f81\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.7750fbc8", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247596748&idx=2&sn=4c41f21ceba70421fa2f597836db422e&chksm=e9d47f7f30d119a17e8035303f7fc6f10440e7d94ab420f8276c21dc3720591d791c5dbc94ce#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247596748&idx=2&sn=4c41f21ceba70421fa2f597836db422e&chksm=e9d47f7f30d119a17e8035303f7fc6f10440e7d94ab420f8276c21dc3720591d791c5dbc94ce#rd", "authors": ["\u6728\u6728\u81ea\u7531"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u00b7\u767d\u76ae\u4e66 | 2025\u201c\u94f6\u53d1+AI\u201d\u5e94\u7528\u8d8b\u52bf\u62a5\u544a-\u6d59\u6c5f\u5f00\u53d1\u5927\u5b66&\u963f\u91cc\u7814\u7a76\u9662", "comment": "Source: WeChat, Published: 2025-10-31 08:28:57", "summary": "AI\u00b7\u5927\u6a21\u578b\u00b7\u9886\u5730\u62a5\u544a\uff1a2025\u201c\u94f6\u53d1+AI\u201d\u5e94\u7528\u8d8b\u52bf\u62a5\u544a-\u6d59\u6c5f\u5f00\u53d1\u5927\u5b66&\u963f\u91cc\u7814\u7a76\u9662\u672c\u62a5\u544a\u9898\u4e3a\u201c\u94f6\u53d1+AI\u201d\u5e94\u7528\u8d8b\u52bf\u62a5\u544a\uff0c\u7531\u6d59\u6c5f\u5f00\u653e\u5927\u5b66\u548c\u963f\u91cc\u7814\u7a76\u9662\u8054\u5408\u53d1\u5e03\uff0c\u65e8\u5728\u5206\u6790\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u8001\u5e74\u4eba\u7fa4\u4f53\u4e2d\u7684\u5e94\u7528\u8d8b\u52bf\u3002", "AI": {"tldr": "AI\u00b7\u5927\u6a21\u578b\u00b7\u9886\u5730\u62a5\u544a\uff1a2025\u201c\u94f6\u53d1+AI\u201d\u5e94\u7528\u8d8b\u52bf\u62a5\u544a-\u6d59\u6c5f\u5f00\u53d1\u5927\u5b66&\u963f\u91cc\u7814\u7a76\u9662\u672c\u62a5\u544a\u9898\u4e3a\u201c\u94f6\u53d1+AI\u201d\u5e94\u7528\u8d8b\u52bf\u62a5\u544a\uff0c\u7531\u6d59\u6c5f\u5f00\u653e\u5927\u5b66\u548c\u963f\u91cc\u7814\u7a76\u9662\u8054\u5408\u53d1\u5e03\uff0c\u65e8\u5728\u5206\u6790\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u8001\u5e74\u4eba\u7fa4\u4f53\u4e2d\u7684\u5e94\u7528\u8d8b\u52bf\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.8f9a6e3a", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNzYzMzQwMg==&mid=2651694250&idx=1&sn=64f40d0f45272d88db25eb0d5a071347&chksm=81449ee9a2ad0f16b38d62d05f128fe85465f8af41e033d8248adcffe3c1b6491fa891230e56#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNzYzMzQwMg==&mid=2651694250&idx=1&sn=64f40d0f45272d88db25eb0d5a071347&chksm=81449ee9a2ad0f16b38d62d05f128fe85465f8af41e033d8248adcffe3c1b6491fa891230e56#rd", "authors": ["\u7ae0\u9c7c\u5927\u6570\u636e"], "title": "\u901a\u4fd7\u89e3\u6790\u5927\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>LLM\u539f\u7406", "comment": "Source: WeChat, Published: 2025-10-31 07:55:45", "summary": "\u672c\u6587\u5c06\u5b8c\u5168\u805a\u7126\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u672c\u8eab\u89e3\u7b54\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u73b0\u4ee3\u667a\u80fd\u4f53\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f\u6211\u4eec\u5c06\u4ece\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u672c\u5b9a\u4e49\u51fa\u53d1\uff0c\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u539f\u7406\u7684\u5b66\u4e60\uff0c\u4e3a\u7406\u89e3LLM\u5982\u4f55\u83b7\u5f97\u5f3a\u5927\u7684\u77e5\u8bc6\u50a8\u5907\u4e0e\u63a8\u7406\u80fd\u529b\u6253\u4e0b\u575a\u5b9e\u7684\u57fa\u7840\u3002", "AI": {"tldr": "\u672c\u6587\u5c06\u5b8c\u5168\u805a\u7126\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u672c\u8eab\u89e3\u7b54\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u73b0\u4ee3\u667a\u80fd\u4f53\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f\u6211\u4eec\u5c06\u4ece\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u672c\u5b9a\u4e49\u51fa\u53d1\uff0c\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u539f\u7406\u7684\u5b66\u4e60\uff0c\u4e3a\u7406\u89e3LLM\u5982\u4f55\u83b7\u5f97\u5f3a\u5927\u7684\u77e5\u8bc6\u50a8\u5907\u4e0e\u63a8\u7406\u80fd\u529b\u6253\u4e0b\u575a\u5b9e\u7684\u57fa\u7840\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.5a8bb8a5", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650999192&idx=3&sn=5fa8d222a14423dd66bfa0f3f81c9825&chksm=85416a8d6c5dad78190dad94fef55464dc79d76c855f864c2c5e789a365d2f7eeb07221a597f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650999192&idx=3&sn=5fa8d222a14423dd66bfa0f3f81c9825&chksm=85416a8d6c5dad78190dad94fef55464dc79d76c855f864c2c5e789a365d2f7eeb07221a597f#rd", "authors": ["\u673a\u5668\u4e4b\u5fc3"], "title": "\u6e2f\u79d1\u63d0\u51fa\u65b0\u7b97\u6cd5\u9769\u65b0<em class=\"highlight\">\u5927\u6a21\u578b</em>\u63a8\u7406\u8303\u5f0f\uff1a\u968f\u673a\u7b56\u7565\u4f30\u503c\u7adf\u6210LLM\u6570\u5b66\u63a8\u7406\u300c\u795e\u64cd\u4f5c\u300d", "comment": "Source: WeChat, Published: 2025-10-31 04:09:09", "summary": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5df2\u6210\u4e3a\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u624b\u6bb5\u3002\u7136\u800c\uff0c\u4e3b\u6d41\u65b9\u6cd5\u5982 PPO\u3001GRPO \u7b49\u4ecd\u7136\u4f9d\u8d56\u4e3a\u4f20\u7edf RL \u573a\u666f\u8bbe\u8ba1\u7684\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u7684\u5b66\u4e60\u76ee\u6807\uff0c\u672c\u8d28\u4e0a\u53ef\u4ee5\u88ab\u7b56\u7565\u8fed\u4ee3\uff08p", "AI": {"tldr": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5df2\u6210\u4e3a\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u624b\u6bb5\u3002\u7136\u800c\uff0c\u4e3b\u6d41\u65b9\u6cd5\u5982 PPO\u3001GRPO \u7b49\u4ecd\u7136\u4f9d\u8d56\u4e3a\u4f20\u7edf RL \u573a\u666f\u8bbe\u8ba1\u7684\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u7684\u5b66\u4e60\u76ee\u6807\uff0c\u672c\u8d28\u4e0a\u53ef\u4ee5\u88ab\u7b56\u7565\u8fed\u4ee3\uff08p", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.1217c720", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1OTkwMTM1Nw==&mid=2247484843&idx=1&sn=616adb03814e2a3cd82e220c53263b1b&chksm=eb06539f771cdddc4787258a09c2822b6539452d5bb41aa66ac883bf9cc06448b31caca7ff2e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1OTkwMTM1Nw==&mid=2247484843&idx=1&sn=616adb03814e2a3cd82e220c53263b1b&chksm=eb06539f771cdddc4787258a09c2822b6539452d5bb41aa66ac883bf9cc06448b31caca7ff2e#rd", "authors": ["\u4e01\u8f89\u7684\u8f6f\u4ef6\u67b6\u6784\u8bf4"], "title": "playground\uff1a\u653b\u514b<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7cfb\u7edf\u91cd\u6784\u4e2d\u4f9d\u8d56\u6cbb\u7406\u7684\u96be\u9898", "comment": "Source: WeChat, Published: 2025-10-31 03:46:04", "summary": "\u76ee\u524d\u5927\u6a21\u578b\u5728\u7f16\u7a0b\u9886\u57df\u5e94\u7528\u5177\u6709\u786e\u5b9a\u6027\uff0c\u662f\u5927\u6a21\u578b\u5e94\u7528\u7684\u660e\u786e\u8d5b\u9053\uff0c\u4e5f\u5438\u5f15\u4e86\u5404\u5927\u5382\u5546\u4e00\u62e5\u800c\u5165\u3002llm\u8f85\u52a9\u7f16\u7a0b \u5728c\u7aef\u7684\u6e17\u900f\u7387\u4ee547%\u6392\u540d\u7b2c\u4e8c\u300251% 47% 43% 38% 37% \u5199\u4f5c\u652f\u6301 \u7f16\u7a0b\u9879\u76ee \u4f5c\u4e1a\u8f85\u5bfc\uff0c \u5236\u4f5c\u6f14\u793a\u6587\u7a3f\uff0c \u97f3\u4e50\u548c\u97f3\u9891\u521b\u4f5c\u3002", "AI": {"tldr": "\u76ee\u524d\u5927\u6a21\u578b\u5728\u7f16\u7a0b\u9886\u57df\u5e94\u7528\u5177\u6709\u786e\u5b9a\u6027\uff0c\u662f\u5927\u6a21\u578b\u5e94\u7528\u7684\u660e\u786e\u8d5b\u9053\uff0c\u4e5f\u5438\u5f15\u4e86\u5404\u5927\u5382\u5546\u4e00\u62e5\u800c\u5165\u3002llm\u8f85\u52a9\u7f16\u7a0b \u5728c\u7aef\u7684\u6e17\u900f\u7387\u4ee547%\u6392\u540d\u7b2c\u4e8c\u300251% 47% 43% 38% 37% \u5199\u4f5c\u652f\u6301 \u7f16\u7a0b\u9879\u76ee \u4f5c\u4e1a\u8f85\u5bfc\uff0c \u5236\u4f5c\u6f14\u793a\u6587\u7a3f\uff0c \u97f3\u4e50\u548c\u97f3\u9891\u521b\u4f5c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.292e06fa", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMjY0ODg0MA==&mid=2247483671&idx=1&sn=5008c150e9acb78352d12fa7022cddcd&chksm=fe0c080f0adf2fabfcdeb2276f7532a306ef6d651d029bebf8c0f96bcce10916dc0e423b8d51#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMjY0ODg0MA==&mid=2247483671&idx=1&sn=5008c150e9acb78352d12fa7022cddcd&chksm=fe0c080f0adf2fabfcdeb2276f7532a306ef6d651d029bebf8c0f96bcce10916dc0e423b8d51#rd", "authors": ["\u4eba\u5927\u6570\u667a\u7814\u53d1\u4e2d\u5fc3"], "title": "\u8303\u4e3e\u6559\u6388\u56e2\u961f\u7814\u53d1\u7684\u9762\u5411\u6570\u636e\u79d1\u5b66\u7684 Agentic <em class=\"highlight\">\u5927\u6a21\u578b</em>\u2014\u2014DeepAnalyze\u6b63\u5f0f\u53d1\u5e03", "comment": "Source: WeChat, Published: 2025-10-31 03:01:48", "summary": "\u8fd1\u65e5\uff0c\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66\u4fe1\u606f\u5b66\u9662\u3001\u6570\u636e\u5de5\u7a0b\u4e0e\u77e5\u8bc6\u5de5\u7a0b\u6559\u80b2\u90e8\u91cd\u70b9\u5b9e\u9a8c\u5ba4\u8303\u4e3e\u6559\u6388\u56e2\u961f\u6b63\u5f0f\u53d1\u5e03\u4e86\u9762\u5411\u6570\u636e\u79d1\u5b66\u7684 agentic \u5927\u6a21\u578b\u2014\u2014deepanalyze\u3002\u8be5\u9879\u76ee\u65e8\u5728\u63a8\u52a8\u6570\u636e\u79d1\u5b66\u4ece\u4f20\u7edf\u7684\u201c\u5de5\u5177\u578b\u5206\u6790\u201d\u8fc8\u5411\u201c\u667a\u80fd\u4f53\u9a71\u52a8\u5206\u6790\u201d\u7684\u65b0\u8303\u5f0f\u3002", "AI": {"tldr": "\u8fd1\u65e5\uff0c\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66\u4fe1\u606f\u5b66\u9662\u3001\u6570\u636e\u5de5\u7a0b\u4e0e\u77e5\u8bc6\u5de5\u7a0b\u6559\u80b2\u90e8\u91cd\u70b9\u5b9e\u9a8c\u5ba4\u8303\u4e3e\u6559\u6388\u56e2\u961f\u6b63\u5f0f\u53d1\u5e03\u4e86\u9762\u5411\u6570\u636e\u79d1\u5b66\u7684 agentic \u5927\u6a21\u578b\u2014\u2014deepanalyze\u3002\u8be5\u9879\u76ee\u65e8\u5728\u63a8\u52a8\u6570\u636e\u79d1\u5b66\u4ece\u4f20\u7edf\u7684\u201c\u5de5\u5177\u578b\u5206\u6790\u201d\u8fc8\u5411\u201c\u667a\u80fd\u4f53\u9a71\u52a8\u5206\u6790\u201d\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.cbd1e00c", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzOTcyMjIxNg==&mid=2247604385&idx=1&sn=7aa7e087d8f930887e1331889d78cf94&chksm=fb92c9a0017a7598c149af449d1c2a78162cb59220a9219ca5ac5bd13c013fdbcd1069c24f0e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzOTcyMjIxNg==&mid=2247604385&idx=1&sn=7aa7e087d8f930887e1331889d78cf94&chksm=fb92c9a0017a7598c149af449d1c2a78162cb59220a9219ca5ac5bd13c013fdbcd1069c24f0e#rd", "authors": ["\u804c\u79f0\u5fae"], "title": "\u4e00\u6587\u4e86\u89e3\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff0c\u667a\u80fd\u4f53\u3001AIGC\uff0c\u5173\u7cfb\u53ca\u5e94\u7528", "comment": "Source: WeChat, Published: 2025-10-30 22:54:58", "summary": "\u4ec0\u4e48\u662f\u5927\u6a21\u578b \u2605 \u5927\u6a21\u578b\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5177\u6709\u6d77\u91cf\u53c2\u6570\u3001\u5f3a\u5927\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u548c\u751f\u6210\u591a\u79cd\u7c7b\u578b\u6570\u636e\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002\u201d\u5927\u6a21\u578b\u901a\u5e38\u6307\u7684\u662f\u5927\u89c4\u6a21\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002", "AI": {"tldr": "\u4ec0\u4e48\u662f\u5927\u6a21\u578b \u2605 \u5927\u6a21\u578b\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5177\u6709\u6d77\u91cf\u53c2\u6570\u3001\u5f3a\u5927\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u548c\u751f\u6210\u591a\u79cd\u7c7b\u578b\u6570\u636e\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002\u201d\u5927\u6a21\u578b\u901a\u5e38\u6307\u7684\u662f\u5927\u89c4\u6a21\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.826b249f", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwMjUxMzM0Nw==&mid=2247485104&idx=1&sn=5ddabfaa49204a40ad9161da62a14cfb&chksm=c1899545515f3d6f78473ad02a7b9ea540e1977fdf93cb4ede3df6fd69250f4bb2ab203d2e1c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwMjUxMzM0Nw==&mid=2247485104&idx=1&sn=5ddabfaa49204a40ad9161da62a14cfb&chksm=c1899545515f3d6f78473ad02a7b9ea540e1977fdf93cb4ede3df6fd69250f4bb2ab203d2e1c#rd", "authors": ["\u5de5\u4e1a\u8f6f\u4ef6\u4ea7\u4e1a\u53d1\u5c55\u63a2\u7d22"], "title": "\u901a\u7528\u4e0e\u63a8\u7406<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8def\u5f84\u6f14\u8fdb\u5bf9 CAD/CAE Agentic AI \u7684\u5f71\u54cd\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-30 22:00:29", "summary": "\u6458\u8981\uff1a\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e862023\u5e74\u81f32025\u5e74\u95f4\"\u5927\u8bed\u8a00\u6a21\u578b\"\uff08LLM\uff09\u7684\u7814\u53d1\u8def\u5f84\u6f14\u8fdb\uff0c\u7279\u522b\u5173\u6ce8\u901a\u7528\u5927\u6a21\u578b\u4e0e\u63a8\u7406\u589e\u5f3a\u578b\u5927\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u5206\u6790\u5176\u5bf9CAD/CAE\u9886\u57df Agentic AI \u8f6c\u578b\u7684\u5f71\u54cd\u6761\u4ef6\u548c\u4f5c\u7528\u3002", "AI": {"tldr": "\u6458\u8981\uff1a\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e862023\u5e74\u81f32025\u5e74\u95f4\"\u5927\u8bed\u8a00\u6a21\u578b\"\uff08LLM\uff09\u7684\u7814\u53d1\u8def\u5f84\u6f14\u8fdb\uff0c\u7279\u522b\u5173\u6ce8\u901a\u7528\u5927\u6a21\u578b\u4e0e\u63a8\u7406\u589e\u5f3a\u578b\u5927\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u5206\u6790\u5176\u5bf9CAD/CAE\u9886\u57df Agentic AI \u8f6c\u578b\u7684\u5f71\u54cd\u6761\u4ef6\u548c\u4f5c\u7528\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.f7ff2d2c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTQwOTEyMA==&mid=2247484066&idx=1&sn=d2cafe994e68c03262077b5c67013c6e&chksm=c592265f3a36b8335e7af5a2889a27d0d81f59962462971655be020c09efa25e1e0244e334fa#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTQwOTEyMA==&mid=2247484066&idx=1&sn=d2cafe994e68c03262077b5c67013c6e&chksm=c592265f3a36b8335e7af5a2889a27d0d81f59962462971655be020c09efa25e1e0244e334fa#rd", "authors": ["AI\u65f6\u4ee3\u65e5\u5fd7"], "title": "AI\u65f6\u4ee3\uff0c\u56fd\u5185\u5916<em class=\"highlight\">\u5927\u6a21\u578b\u5927\u6a21\u578b</em>\u54ea\u5bb6\u5f3a\u3002\u5b83\u79f0\u7b2c\u4e8c\uff0c\u6ca1\u4eba\u6562\u8bf4\u81ea\u5df1\u7b2c\u4e00", "comment": "Source: WeChat, Published: 2025-10-30 16:34:34", "summary": "\u8fd9\u5f20\u56fe\u5c55\u793a\u7684\u662f\u5168\u7403\u5f00\u53d1\u8005\u5bf9\u5f00\u6e90\u5927\u6a21\u578b\u7684\u91c7\u7528\u8d8b\u52bf\uff08Developer Adoption of Open Models\uff09\u3002\u7edf\u8ba1\u65f6\u95f4\u4ece2023\u5e7410\u6708\u52302025\u5e7410\u6708\u7684\u53d8\u5316\u60c5\u51b5\u3002\u7eb5\u8f74\u4e3a\u300c\u6708\u4e0b\u8f7d\u91cf\u300d\u3001\u6a2a\u8f74\u4e3a\u65f6\u95f4\u3002", "AI": {"tldr": "\u8fd9\u5f20\u56fe\u5c55\u793a\u7684\u662f\u5168\u7403\u5f00\u53d1\u8005\u5bf9\u5f00\u6e90\u5927\u6a21\u578b\u7684\u91c7\u7528\u8d8b\u52bf\uff08Developer Adoption of Open Models\uff09\u3002\u7edf\u8ba1\u65f6\u95f4\u4ece2023\u5e7410\u6708\u52302025\u5e7410\u6708\u7684\u53d8\u5316\u60c5\u51b5\u3002\u7eb5\u8f74\u4e3a\u300c\u6708\u4e0b\u8f7d\u91cf\u300d\u3001\u6a2a\u8f74\u4e3a\u65f6\u95f4\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.262ba220", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MjA4MjA4MA==&mid=2655285746&idx=1&sn=5e462023eba95440ab01a463ef1b9229&chksm=bc0a3e0962c1b6f47323a93d9e2786aeb08f2430ec635137fe405f12e7ff30756876b2616d72#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MjA4MjA4MA==&mid=2655285746&idx=1&sn=5e462023eba95440ab01a463ef1b9229&chksm=bc0a3e0962c1b6f47323a93d9e2786aeb08f2430ec635137fe405f12e7ff30756876b2616d72#rd", "authors": ["\u89c2\u5bdf\u8005\u7f51"], "title": "\u7845\u8c37\u5927\u5382\uff0c\u96c6\u4f53\u201c\u5012\u6208\u201d\u7528\u8d77\u4e2d\u56fd<em class=\"highlight\">\u5927\u6a21\u578b</em>", "comment": "Source: WeChat, Published: 2025-10-30 15:00:13", "summary": "\u8fd9\u8868\u660e\u4e2d\u56fd\u5927\u6a21\u578b\u6b63\u4f5c\u4e3a\u57fa\u7840\u8bbe\u65bd\uff0c\u88ab\u6574\u5408\u8fdb\u7f8e\u56fdAI\u5f00\u53d1\u7684\u751f\u6001\u4f4d\u4e2d\u3002\u5982\u679c\u8bf4Qwen\u548cGLM\u5c55\u793a\u4e86\u5e73\u53f0\u7684\u5e7f\u5ea6\uff0cKimi\u5219\u76f4\u63a5\u70b9\u71c3\u4e86\u201c\u6027\u4ef7\u6bd4\u201d\u7684\u5bfc\u706b\u7d22\uff0c\u751a\u81f3\u88ab\u89c6\u4e3a\u4e00\u4e2a\u6765\u81ea\u7845\u8c37\u6838\u5fc3\u7684\u201c\u53db\u9003\u201d\u4fe1\u53f7\u3002", "AI": {"tldr": "\u8fd9\u8868\u660e\u4e2d\u56fd\u5927\u6a21\u578b\u6b63\u4f5c\u4e3a\u57fa\u7840\u8bbe\u65bd\uff0c\u88ab\u6574\u5408\u8fdb\u7f8e\u56fdAI\u5f00\u53d1\u7684\u751f\u6001\u4f4d\u4e2d\u3002\u5982\u679c\u8bf4Qwen\u548cGLM\u5c55\u793a\u4e86\u5e73\u53f0\u7684\u5e7f\u5ea6\uff0cKimi\u5219\u76f4\u63a5\u70b9\u71c3\u4e86\u201c\u6027\u4ef7\u6bd4\u201d\u7684\u5bfc\u706b\u7d22\uff0c\u751a\u81f3\u88ab\u89c6\u4e3a\u4e00\u4e2a\u6765\u81ea\u7845\u8c37\u6838\u5fc3\u7684\u201c\u53db\u9003\u201d\u4fe1\u53f7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.d20c927b", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxOTU5NTU4MQ==&mid=2247495932&idx=1&sn=b5d77f92eb70759d3abd276818ebfa15&chksm=9a1733e040fc6e55aac32801ee9d947cacde28dbd643582a206c5fe50b2a74fe55755d0b0413#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxOTU5NTU4MQ==&mid=2247495932&idx=1&sn=b5d77f92eb70759d3abd276818ebfa15&chksm=9a1733e040fc6e55aac32801ee9d947cacde28dbd643582a206c5fe50b2a74fe55755d0b0413#rd", "authors": ["ChallengeHub"], "title": "\u5f3a\u5316\u5b66\u4e60(RL)\u7b80\u4ecb\u53ca\u5176\u5728\u5927\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>\u4e2d\u7684\u5e94\u7528", "comment": "Source: WeChat, Published: 2025-10-30 14:00:53", "summary": "\u770b\u5230huggingface\u4e0a\u6709\u4e2a\u5927\u6a21\u578b\u8bfe\u7a0b\uff0c\u5176\u4e2d\u6709\u4e2a\u7ae0\u8282\u662f\u8bb2\u5982\u4f55\u6784\u5efa\u63a8\u7406\u5927\u6a21\u578b\uff0c\u4e0b\u9762\u662f\u5bf9\u5e94\u7684\u5b66\u4e60\u5185\u5bb9\u3002\u63a5\u4e0b\u6765\u4f1a\u7528\u6700\u901a\u4fd7\u6613\u61c2\u7684\u65b9\u5f0f\u4ecb\u7ecdRL\uff0c\u5c31\u7b97\u4e4b\u524d\u5b8c\u5168\u6ca1\u63a5\u89e6\u8fc7\u4e5f\u80fd\u770b\u61c2\u3002", "AI": {"tldr": "\u770b\u5230huggingface\u4e0a\u6709\u4e2a\u5927\u6a21\u578b\u8bfe\u7a0b\uff0c\u5176\u4e2d\u6709\u4e2a\u7ae0\u8282\u662f\u8bb2\u5982\u4f55\u6784\u5efa\u63a8\u7406\u5927\u6a21\u578b\uff0c\u4e0b\u9762\u662f\u5bf9\u5e94\u7684\u5b66\u4e60\u5185\u5bb9\u3002\u63a5\u4e0b\u6765\u4f1a\u7528\u6700\u901a\u4fd7\u6613\u61c2\u7684\u65b9\u5f0f\u4ecb\u7ecdRL\uff0c\u5c31\u7b97\u4e4b\u524d\u5b8c\u5168\u6ca1\u63a5\u89e6\u8fc7\u4e5f\u80fd\u770b\u61c2\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.ce24eb26", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1MDAxNDczMQ==&mid=2247564488&idx=4&sn=123eee9befc37efbb8358bcf1fb99936&chksm=fa6979f4824b499d183a99efd37e9e2d5a478eace6f4a890dfd027250d70d43e2bcd549df058#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1MDAxNDczMQ==&mid=2247564488&idx=4&sn=123eee9befc37efbb8358bcf1fb99936&chksm=fa6979f4824b499d183a99efd37e9e2d5a478eace6f4a890dfd027250d70d43e2bcd549df058#rd", "authors": ["\u5c71\u897f\u540d\u5e08\u5728\u7ebf"], "title": "\u4e00\u6587\u4e86\u89e3<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff0c\u667a\u80fd\u4f53\u3001AIGC\uff0c\u5173\u7cfb\u53ca\u5e94\u7528", "comment": "Source: WeChat, Published: 2025-10-30 13:56:16", "summary": "\u5173\u6ce8\u516c\u4f17\u8d26\u53f7\uff0c\u53d1\u9001\u6d88\u606f llm \u83b7\u53d6ppt \u4ec0\u4e48\u662f\u5927\u6a21\u578b \u2605 \u5927\u6a21\u578b\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5177\u6709\u6d77\u91cf\u53c2\u6570\u3001\u5f3a\u5927\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u548c\u751f\u6210\u591a\u79cd\u7c7b\u578b\u6570\u636e\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002\u201d", "AI": {"tldr": "\u5173\u6ce8\u516c\u4f17\u8d26\u53f7\uff0c\u53d1\u9001\u6d88\u606f llm \u83b7\u53d6ppt \u4ec0\u4e48\u662f\u5927\u6a21\u578b \u2605 \u5927\u6a21\u578b\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5177\u6709\u6d77\u91cf\u53c2\u6570\u3001\u5f3a\u5927\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u548c\u751f\u6210\u591a\u79cd\u7c7b\u578b\u6570\u636e\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002\u201d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
