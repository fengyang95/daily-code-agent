<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.LG](#cs.LG) [Total: 20]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.AI](#cs.AI) [Total: 22]
- [tldr.article](#tldr.article) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: SmartSnap提出了一种主动的、实时的自我验证范式，让智能体在执行GUI任务时主动收集简洁的证据快照，而不是事后被动验证整个交互轨迹，从而降低验证成本并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有任务验证方法是被动的、事后处理过程，需要分析智能体的整个交互轨迹，这种处理冗长且包含无关噪声上下文的方法导致验证成本高昂且可靠性低，限制了智能体强化学习的可扩展性。

Method: 提出SmartSnap范式，从被动事后验证转变为主动实时自我验证。引入自我验证智能体，具有双重使命：完成任务并提供精选的快照证据。遵循3C原则（完整性、简洁性、创造性），智能体利用在线环境访问权限在最小决定性快照集上进行自我验证。

Result: 在移动任务上的实验表明，SmartSnap范式能够以可扩展的方式训练LLM驱动的智能体，为8B和30B模型分别带来26.08%和16.66%的性能提升。自我验证智能体在与DeepSeek V3.1和Qwen3-235B-A22B的竞争中表现出色。

Conclusion: SmartSnap通过将解决方案寻找与证据寻求相结合，促进了高效、自我验证智能体的培养，为智能体强化学习的可扩展性提供了新范式。

Abstract: Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


### [2] [M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation](https://arxiv.org/abs/2512.22628)
*Fanglin Xu,Wei Zhang,Jian Yang,Guo Chen,Aishan Liu,Zhoujun Li,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: M2G-Eval是一个多粒度、多语言的代码生成评估框架，包含四个粒度级别（类、函数、块、行）和18种编程语言，用于精细诊断大语言模型的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估基准大多只关注单一结构粒度和有限编程语言，无法揭示模型在不同代码范围和跨语言场景下的细粒度能力差异。

Method: 开发M2G-Eval框架，包含17K+训练任务和1,286个人工标注、污染控制的测试实例。使用监督微调和Group Relative Policy Optimization训练Qwen3-8B模型得到M2G-Eval-Coder变体。

Result: 评估30个模型发现：(1) 难度层次明显，行级任务最简单，类级最困难；(2) 随着任务复杂度增加，全粒度和部分粒度语言间的性能差距扩大；(3) 跨语言相关性强，表明模型学习了可迁移的编程概念。

Conclusion: M2G-Eval能够精细诊断代码生成能力，并突显了在合成复杂、长篇代码方面存在的持续挑战。

Abstract: The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.

</details>


### [3] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: LLMs在复杂编程任务中常出错，作者假设这是由于通用语言的灵活性导致。他们设计了Anka DSL来约束语法，显著提升了LLM在数据转换管道任务上的准确率。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成方面表现出色，但在复杂多步骤编程任务中仍存在系统性错误。作者认为这些错误源于通用语言的灵活性，允许多种有效方法并需要隐式状态管理，导致歧义和错误。

Method: 引入Anka，一种专门为数据转换管道设计的领域特定语言（DSL）。Anka具有明确、受约束的语法，减少了代码生成中的歧义。尽管LLMs从未接触过Anka，但通过上下文提示就能学习该语言。

Result: Claude 3.5 Haiku在100个基准问题上达到99.9%解析成功率和95.8%总体任务准确率。在复杂多步骤管道任务中，Anka比Python有40个百分点的准确率优势（100% vs. 60%）。GPT-4o-mini验证了类似优势（+26.7个百分点）。

Conclusion: LLMs能够完全通过上下文提示学习新的DSL，达到接近原生的准确率；受约束的语法显著减少复杂任务中的错误；为LLM生成专门设计的DSL可以超越LLMs有大量训练的通用语言。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>


### [4] [AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning](https://arxiv.org/abs/2512.22857)
*Shihao Cai,Runnan Fang,Jialong Wu,Baixuan Li,Xinyu Wang,Yong Jiang,Liangcai Su,Liwen Zhang,Wenbiao Yin,Zhen Zhang,Fuli Feng,Pengjun Xie,Xiaobin Wang*

Main category: cs.CL

TL;DR: 提出自动化合成高难度模拟环境的统一流程和环境级强化学习算法，提升语言智能体的训练效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在模拟环境中的研究存在局限性：环境合成多为半自动化、任务难度不足、模拟用户不稳定、环境异质性等问题，限制了语言智能体的发展

Method: 1) 自动化可扩展的模拟环境合成流程，专注于高难度但易于验证的任务；2) 环境级强化学习算法，缓解用户不稳定性，在环境层面进行优势估计

Result: 在tau-bench、tau2-Bench和VitaBench等智能体基准测试中验证了方法的有效性，深入分析显示其具有良好的跨域泛化能力

Conclusion: 提出的自动化环境合成流程和环境级强化学习算法能有效提升语言智能体在模拟环境中的训练效率和稳定性，并具有良好的泛化性能

Abstract: Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.

</details>


### [5] [Diversity or Precision? A Deep Dive into Next Token Prediction](https://arxiv.org/abs/2512.22955)
*Haoyuan Wu,Hai Wang,Jiajia Wu,Jinxiang Ou,Keyao Wang,Weile Chen,Zihao Zheng,Bei Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种将监督学习视为策略梯度优化的方法，通过奖励塑造策略重塑预训练模型的token输出分布，为后续强化学习提供更优的探索空间，最终提升端到端推理性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习能显著提升大语言模型的推理能力，但其效果严重依赖于预训练模型的token输出分布所定义的探索空间。当前标准的交叉熵损失限制了探索潜力，需要重新审视预训练目标如何为后续RL提供更好的探索基础。

Method: 将标准交叉熵损失重新解释为单步episode中的策略梯度优化特例，提出广义预训练目标：1) 将下一个token预测视为随机决策过程；2) 引入奖励塑造策略，通过正奖励缩放因子控制ground-truth token的概率集中度；3) 采用排名感知机制，对高排名和低排名的负token进行非对称处理。

Result: 研究发现，与直觉相反，施加精度导向的先验分布（而非更高熵的分布）能为RL提供更优越的探索空间，最终提升了端到端推理性能。

Conclusion: 通过将监督学习框架重新设计为策略梯度优化，可以系统性地研究预训练分布如何塑造后续RL的探索潜力，并发现精度导向的分布比高熵分布更有利于RL训练。

Abstract: Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.

</details>


### [6] [Accelerating Language Model Workflows with Prompt Choreography](https://arxiv.org/abs/2512.23049)
*TJ Bai,Jason Eisner*

Main category: cs.CL

TL;DR: Prompt Choreography框架通过动态全局KV缓存优化多智能体LLM工作流，支持并行调用和任意消息子集关注，显著降低延迟并提升端到端速度


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体工作流中部署增多，存在大量冗余计算，需要更高效的执行框架来降低延迟和提升性能

Method: 提出Prompt Choreography框架，维护动态全局KV缓存，支持LLM调用关注任意重新排序的先前消息子集，支持并行调用，并通过微调LLM使其适应缓存机制

Result: 显著降低每消息延迟（2.0-6.2倍的首token生成速度提升），在某些冗余计算为主的工作流中实现超过2.2倍的端到端加速

Conclusion: Prompt Choreography通过缓存优化有效解决了多智能体LLM工作流中的冗余计算问题，显著提升了执行效率

Abstract: Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\times$) in some workflows dominated by redundant computation.

</details>


### [7] [Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process](https://arxiv.org/abs/2512.23213)
*Zhijun Chen,Zeyu Ji,Qianren Mao,Junhang Cheng,Bangjie Qin,Hao Wu,Zhuoran Li,Jingzheng Li,Kai Sun,Zizhe Wang,Yikun Ban,Zhu Sun,Xiangyang Ji,Hailong Sun*

Main category: cs.CL

TL;DR: LLM-PeerReview是一种无监督的LLM集成方法，通过同行评审机制从多个LLM生成的候选中选择最佳响应，利用多个模型的集体智慧。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效整合多个LLM的不同优势，需要一个清晰、可解释且完全无监督的集成框架来选择最佳响应。

Method: 基于同行评审的三阶段框架：1) 使用LLM-as-a-Judge技术让多个LLM相互评分；2) 通过图模型真值推断算法或简单平均策略聚合分数；3) 选择最高分响应作为集成输出。

Result: 在两个变体上，在四个数据集上都取得了强劲结果，分别比先进的Smoothie-Global模型高出6.9%和7.3%。

Conclusion: LLM-PeerReview是一个概念简单但经验上强大的无监督集成方法，能够有效利用多个LLM的集体智慧，具有灵活适应性和泛化能力。

Abstract: We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.

</details>


### [8] [AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents](https://arxiv.org/abs/2512.23343)
*Jiafeng Liang,Hao Li,Chang Li,Jiaqi Zhou,Shixin Jiang,Zekun Wang,Changkai Ji,Zhihao Zhu,Runxuan Liu,Tao Ren,Jinlan Fu,See-Kiong Ng,Xia Liang,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: 该论文系统性地综合了认知神经科学和LLM驱动智能体中的记忆机制，从定义、功能、分类、存储到管理生命周期进行了比较分析，并探讨了记忆评估基准、安全性以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有自主智能体研究虽然借鉴了认知神经科学设计记忆工作流，但由于学科壁垒难以吸收人类记忆机制的精髓。本文旨在弥合这一差距，系统性地整合跨学科记忆知识，连接认知神经科学与LLM驱动智能体。

Method: 1. 阐明从认知神经科学到LLM再到智能体的渐进轨迹中的记忆定义和功能；2. 从生物和人工角度比较分析记忆分类、存储机制和完整管理生命周期；3. 回顾评估智能体记忆的主流基准；4. 从攻击和防御双重角度探讨记忆安全性；5. 展望未来研究方向。

Result: 论文提供了跨学科记忆知识的系统性综合框架，建立了认知神经科学与LLM智能体之间的连接，分析了记忆的多个维度（分类、存储、管理、评估、安全），并提出了未来研究方向。

Conclusion: 记忆作为连接过去与未来的关键枢纽，对AI系统处理复杂任务至关重要。通过系统整合认知神经科学和LLM智能体的记忆知识，可以为设计更高效的自主智能体提供理论基础和实践指导，未来应关注多模态记忆系统和技能获取等方向。

Abstract: Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.

</details>


### [9] [Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias](https://arxiv.org/abs/2512.23518)
*Hazel Kim,Philip Torr*

Main category: cs.CL

TL;DR: MoLaCE是一个轻量级推理时框架，通过混合基于潜在概念的不同专家来减少LLMs中的确认偏误，无需多智能体辩论的计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在严重的确认偏误问题，当提示暗示偏好答案时，模型会强化这种偏误而非探索替代方案。这一问题在基础模型中已有害，在多智能体辩论中风险更大，可能形成回音室效应而非纠正偏误。

Method: 提出MoLaCE框架，将专家实例化为不同潜在概念上的激活强度混合。关键洞见是：由于语言具有组合性，不同表述的提示会以特定方式重新加权影响事实正确性的潜在概念，因此不能对所有输入应用单一固定干预。

Result: 实证表明MoLaCE能持续减少确认偏误、提高鲁棒性，性能匹配或超越多智能体辩论，同时仅需一小部分计算量。可集成到多智能体辩论框架中以多样化视角并减少相关错误。

Conclusion: MoLaCE使单个LLM能够内部模拟辩论优势，保持计算效率和可扩展性，有效解决LLMs中的确认偏误问题。

Abstract: Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.

</details>


### [10] [Instruction-Following Evaluation of Large Vision-Language Models](https://arxiv.org/abs/2512.23572)
*Daiki Shiono,Shumpei Miyawaki,Ryota Tanaka,Jun Suzuki*

Main category: cs.CL

TL;DR: 研究发现大型视觉语言模型在视觉指令微调后，指令跟随能力会下降，而包含输出格式说明的训练数据可以缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在集成视觉能力后，经常无法保持原有语言模型的指令跟随能力，导致无法按照预期执行任务指令。

Method: 构建新的训练数据集，明确标注输出格式是否被指定；研究在微调过程中明确指示输出格式对LVLMs指令跟随能力的影响。

Result: 定量评估证实LVLMs在常用数据集微调后指令跟随能力下降；包含输出格式说明的数据集训练的模型指令跟随更准确。

Conclusion: 在视觉指令微调中包含输出格式说明的样本可能有助于缓解指令跟随能力的下降。

Abstract: Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.

</details>


### [11] [Nested Browser-Use Learning for Agentic Information Seeking](https://arxiv.org/abs/2512.23647)
*Baixuan Li,Jialong Wu,Wenbiao Yin,Kuan Li,Zhongwang Zhang,Huifeng Yin,Zhengwei Tao,Liwen Zhang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: NestBrowse提出了一种嵌套浏览器使用学习框架，将交互控制与页面探索解耦，使信息搜索代理能够进行完整的浏览器交互，而不仅仅是API级别的片段检索


<details>
  <summary>Details</summary>
Motivation: 当前信息搜索代理主要局限于API级别的片段检索和基于URL的页面获取，无法访问通过真实浏览可获得的更丰富信息。完整的浏览器交互虽然能解锁更深层能力，但其细粒度控制和冗长页面内容返回给ReAct式函数调用代理带来了巨大复杂性

Method: 提出Nested Browser-Use Learning (NestBrowse)，引入最小且完整的浏览器操作框架，通过嵌套结构将交互控制与页面探索解耦。这种设计简化了代理推理，同时实现了有效的深度网络信息获取

Result: 在具有挑战性的深度信息搜索基准测试中，NestBrowse在实践中显示出明显优势。进一步的深入分析强调了其效率和灵活性

Conclusion: NestBrowse通过嵌套浏览器使用学习框架，成功解决了信息搜索代理在完整浏览器交互中的复杂性挑战，实现了更有效的深度网络信息获取

Abstract: Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models](https://arxiv.org/abs/2512.22170)
*Jiesong Lian,Ruizhe Zhong,Zixiang Zhou,Xiaoyue Mi,Yixue Hao,Yuan Zhou,Qinglin Lu,Long Hu,Junchi Yan*

Main category: cs.LG

TL;DR: SoliReward是一个用于视频生成模型奖励模型训练的系统框架，通过单项目二元标注收集高质量数据，采用分层渐进查询注意力架构，并引入改进的BT损失来处理胜-平局场景，从而缓解奖励黑客攻击问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型的后训练对齐面临三个主要挑战：1）基于提示内成对标注的数据收集存在标签噪声；2）基于VLM的奖励模型架构设计（特别是输出机制）研究不足；3）奖励模型在后训练中容易受到奖励黑客攻击。

Method: 提出SoliReward框架：1）通过单项目二元标注收集高质量低成本数据，采用跨提示配对策略构建偏好对；2）使用分层渐进查询注意力机制增强特征聚合；3）引入改进的BT损失，显式处理胜-平局场景，正则化正样本的分数分布。

Result: 在评估物理合理性、主体变形和语义对齐的基准测试中，该方法在直接奖励模型评估指标和视频生成模型后训练效果方面都显示出改进。

Conclusion: SoliReward通过系统化的数据收集、架构设计和损失函数改进，有效解决了视频奖励模型训练中的关键挑战，为视频生成模型的后训练对齐提供了更可靠的偏好信号。

Abstract: Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM's score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark will be publicly available.

</details>


### [13] [Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks](https://arxiv.org/abs/2512.22186)
*Vishnu Mohan*

Main category: cs.LG

TL;DR: 提出一个结合DDQN和课程学习的强化学习框架，用于网球策略优化，在模拟环境中实现高胜率，但发现学习策略存在防御性偏差。


<details>
  <summary>Details</summary>
Motivation: 网球策略优化是一个复杂的序列决策问题，涉及分层计分、随机结果、长时程信用分配、体力疲劳和对手技能适应等挑战。需要开发一个能够处理这些复杂性的强化学习框架。

Method: 构建了一个完整的网球模拟环境，包含点、局、盘三级计分系统，10种离散战术动作类别，对称疲劳动态和连续对手技能参数。使用Dueling Double Deep Q-Network(DDQN)架构，通过课程学习逐步提高对手难度（从0.40到0.50）。

Result: 训练后的智能体在平衡对手上获得98-100%的胜率，发球效率63.0-67.5%，接发效率52.8-57.1%。消融研究表明，dueling架构和课程学习对稳定收敛至关重要，标准DQN基线无法学习有效策略。

Conclusion: 尽管性能强劲，但战术分析显示学习策略存在明显的防御性偏差，优先考虑避免失误和延长回合，而非积极得分。这表明在简化体育模拟中，仅以胜率为驱动的优化存在局限性，强调了奖励设计对现实体育强化学习的重要性。

Abstract: Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.

</details>


### [14] [Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents](https://arxiv.org/abs/2512.22200)
*Dhruv Tiwari*

Main category: cs.LG

TL;DR: 提出情感启发学习信号（EILS）框架，用连续稳态评估信号（好奇心、压力、自信）替代传统静态奖励函数，实现动态内部反馈机制以提升智能体在开放环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体依赖外部定义的静态奖励函数，在封闭静态环境中表现优异，但在开放真实环境中脆弱。标准智能体缺乏内部自主性：难以在没有密集反馈时探索、无法适应分布变化、需要大量手动调参。需要生物情感类似的高层稳态控制机制来解决这些问题。

Method: 引入情感启发学习信号（EILS）统一框架，将情感建模为连续的稳态评估信号（好奇心、压力、自信），而非语义标签。这些信号作为从交互历史推导的向量值内部状态，实时动态调制智能体的优化景观：好奇心调节熵防止模式崩溃，压力调节可塑性克服不活跃，自信调整信任区域稳定收敛。

Result: 假设这种闭环稳态调节能使EILS智能体在样本效率和非平稳适应方面优于标准基线方法。

Conclusion: EILS框架通过生物启发的内部反馈引擎替代分散的优化启发式方法，为开放环境中鲁棒自主性提供了新方向，将情感作为连续稳态信号而非语义标签，有望解决传统强化学习和LLM方法的局限性。

Abstract: The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this "extrinsic maximization" approach has rendered superhuman performance in closed, stationary fields, it produces agents that are fragile in open-ended, real-world environments. Standard agents lack internal autonomy: they struggle to explore without dense feedback, fail to adapt to distribution shifts (non-stationarity), and require extensive manual tuning of static hyperparameters. This paper proposes that the unaddressed factor in robust autonomy is a functional analog to biological emotion, serving as a high-level homeostatic control mechanism. We introduce Emotion-Inspired Learning Signals (EILS), a unified framework that replaces scattered optimization heuristics with a coherent, bio-inspired internal feedback engine. Unlike traditional methods that treat emotions as semantic labels, EILS models them as continuous, homeostatic appraisal signals such as Curiosity, Stress, and Confidence. We formalize these signals as vector-valued internal states derived from interaction history. These states dynamically modulate the agent's optimization landscape in real time: curiosity regulates entropy to prevent mode collapse, stress modulates plasticity to overcome inactivity, and confidence adapts trust regions to stabilize convergence. We hypothesize that this closed-loop homeostatic regulation can enable EILS agents to outperform standard baselines in terms of sample efficiency and non-stationary adaptation.

</details>


### [15] [Masking Teacher and Reinforcing Student for Distilling Vision-Language Models](https://arxiv.org/abs/2512.22238)
*Byung-Kwan Lee,Yu-Chiang Frank Wang,Ryo Hachiuma*

Main category: cs.LG

TL;DR: 提出Masters框架，通过掩码渐进式强化学习蒸馏，解决大教师模型与小学生模型间尺寸差距导致的蒸馏困难问题，实现高效知识迁移。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型(VLMs)虽然表现出色，但尺寸过大难以部署到移动或边缘设备。需要紧凑而强大的VLMs，但现有蒸馏方法面临大教师与小学生的尺寸差距问题，导致学生难以复制教师复杂的高维表示，造成学习不稳定和性能下降。

Method: 提出Masters框架：1) 掩码教师非主导权重以降低复杂度；2) 渐进式恢复教师容量，使学生能平稳学习丰富表示；3) 离线强化学习阶段结合两种奖励：准确性奖励和蒸馏奖励；4) 利用掩码教师预生成响应，避免昂贵的在线思考-回答过程。

Result: 该方法使学生模型能够从大教师模型中高效学习丰富表示，实现稳定训练和强性能，避免了传统在线强化学习的高计算成本和生成长响应的问题。

Conclusion: Masters框架通过掩码渐进式强化学习蒸馏，有效解决了大教师与小学生的尺寸差距问题，实现了高效的知识迁移，为部署紧凑而强大的视觉语言模型提供了可行方案。

Abstract: Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.

</details>


### [16] [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)
*Bhaktipriya Radharapu,Eshika Saxena,Kenneth Li,Chenxi Whitehouse,Adina Williams,Nicola Cancedda*

Main category: cs.LG

TL;DR: 本文提出使用线性探针从LLM法官的隐藏状态中获取校准的不确定性估计，相比现有方法在计算效率和校准质量上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的法官在工业应用中变得越来越重要，高效获取良好校准的不确定性估计对于生产部署至关重要。现有方法（如语言化置信度和多生成方法）要么校准效果差，要么计算成本高。

Method: 引入使用Brier分数损失训练的线性探针，从推理法官的隐藏状态中提供校准的不确定性估计，无需额外的模型训练。

Result: 探针在客观任务（推理、数学、事实性、编码）和主观人类偏好判断上都表现出优于现有方法的校准效果，计算节省约10倍，对未见评估领域具有鲁棒泛化能力，在高置信度预测上提供更高准确性。

Conclusion: 基于可解释性的不确定性估计为生产中的LLM法官提供了实用、可扩展的即插即用解决方案，虽然会产生保守估计，但在安全关键部署中可能有益。

Abstract: As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.

</details>


### [17] [LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs](https://arxiv.org/abs/2512.22266)
*Bing Hao,Minglai Shao,Zengyi Wo,Yunlong Chu,Yuhang Liu,Ruijie Wang*

Main category: cs.LG

TL;DR: 该论文系统研究了大语言模型在动态图时序模体分析中的性能，提出了LLMTM基准测试，开发了工具增强的LLM代理，并设计了结构感知调度器来平衡精度与成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，其在动态图处理方面的能力受到关注。时序模体作为动态图的基本单元和重要局部属性，能直接反映异常和独特现象，对理解动态图的演化动态和结构特征至关重要。然而，利用LLM进行动态图时序模体分析的研究相对较少。

Method: 1) 提出LLMTM基准测试，包含6个定制任务和9种时序模体类型；2) 进行广泛实验分析不同提示技术和9种LLM模型的影响；3) 开发工具增强的LLM代理，使用精确设计的提示解决任务；4) 提出结构感知调度器，考虑动态图结构属性和LLM认知负载，智能调度标准LLM提示和更强大的代理之间的查询。

Result: 实验表明，工具增强的LLM代理能以高精度解决任务，但成本较高。结构感知调度器能有效维持高精度同时降低成本。

Conclusion: 该研究系统探索了LLM在时序模体分析中的性能，提出的基准测试和调度器方法为平衡精度与成本提供了有效解决方案，推动了LLM在动态图分析领域的应用。

Abstract: The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.

</details>


### [18] [Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against](https://arxiv.org/abs/2512.22293)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 语言模型无法从训练数据中的警告框架内容（如"不要使用-此代码存在漏洞"）学会避免被警告的行为，因为模型只是学习统计共现模式而非语用解释。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解为什么语言模型无法从训练数据中的警告内容中学会避免不良行为，尽管这些警告明确指示了某些代码或行为应该避免。

Method: 通过实验比较模型在接触警告内容和直接内容时的表现，使用稀疏自编码器分析潜在特征，识别描述X和执行X的重叠激活模式，并分析"隐形滑移"现象。

Result: 接触警告内容的模型复制被标记内容的比率（76.7%）与直接接触内容的模型（83.3%）在统计上无显著差异。特征#8684在警告和利用上下文中激活程度相似，表明模型未能正交化描述和执行。

Conclusion: 当前架构中统计共现主导语用解释，模型学习的是上下文后倾向于出现什么，而不是为什么出现在那里。训练时特征消融能解决此问题，但提示和推理时引导无效。

Abstract: Warning-framed content in training data (e.g., "DO NOT USE - this code is vulnerable") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: "describing X" and "performing X" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call "stealth slip", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.

</details>


### [19] [Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals](https://arxiv.org/abs/2512.22508)
*Lucky Susanto,Anasta Pranawijayana,Cortino Sukotjo,Soni Prasad,Derry Wijaya*

Main category: cs.LG

TL;DR: 利用元数据和幻觉信号预测LLM在医学考试中的回答正确性，准确率最高提升7.14%，但现有方法尚不足以用于高风险部署


<details>
  <summary>Details</summary>
Motivation: LLM在医疗等高风险领域应用时，生成错误信息（幻觉）是主要担忧。虽然已有检测和缓解幻觉的研究，但预测LLM回答是否正确仍是一个关键但未被充分探索的问题。

Method: 在修复学多项选择考试上测试通用模型(GPT-4o)和推理中心模型(OSS-120B)。使用三种不同提示策略的元数据和幻觉信号为每个（模型，提示）组合构建正确性预测器。

Result: 元数据方法可将准确率最高提升+7.14%，达到83.12%的精确率（相比假设所有答案正确的基线）。实际幻觉是错误回答的强指标，但仅元数据信号不能可靠预测幻觉。提示策略虽不影响整体准确率，但显著改变模型内部行为和元数据的预测效用。

Conclusion: 为开发LLM可靠性信号提供了有前景的方向，但本文探索的方法尚不够稳健，无法用于关键的高风险部署。

Abstract: Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.

</details>


### [20] [Scaling Unverifiable Rewards: A Case Study on Visual Insights](https://arxiv.org/abs/2512.22650)
*Shuyu Gan,James Mooney,Pan Hao,Renxiang Wang,Mingyi Hong,Qianwen Wang,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出Selective TTS框架，在无法验证奖励的多阶段任务中，通过跨阶段分配计算资源并早期剪枝低质量分支，提升推理质量


<details>
  <summary>Details</summary>
Motivation: 现实世界多阶段任务缺乏可验证的最终奖励或足够数据训练鲁棒的奖励模型，导致基于评判的迭代细化容易在阶段间累积误差

Method: 提出Selective TTS框架：1) 在多智能体流水线中跨阶段分配计算而非时间上的重复细化；2) 使用过程特定评判器早期剪枝低质量分支；3) 在数据科学流水线中构建端到端多智能体系统生成图表和报告；4) 设计可靠的LLM评判模型

Result: LLM评判模型与人类专家对齐良好(Kendall's τ=0.55)；在固定计算预算下，Selective TTS将平均分从61.64提升到65.86，同时降低了方差

Conclusion: Selective TTS能有效提升无法验证奖励的复杂开放任务的质量，为科学发现和故事生成等任务提供新的推理扩展方法

Abstract: Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's τ=0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.

</details>


### [21] [A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms](https://arxiv.org/abs/2512.23097)
*Yingru Li,Ziniu Li,Jiacai Liu*

Main category: cs.LG

TL;DR: 提出一个统一框架，将模仿学习与强化学习结合用于LLM微调，通过分析包含轨迹级KL散度和任务奖励的复合目标梯度，分解为可解析计算的密集梯度（用于token级模仿）和蒙特卡洛估计的稀疏梯度（用于长程奖励优化）。


<details>
  <summary>Details</summary>
Motivation: 当前LLM微调方法中，模仿学习和强化学习通常分开处理，缺乏统一的框架。需要一种能够同时利用监督学习的高效性和强化学习的长程优化能力的集成方法。

Method: 通过分析复合目标函数的梯度，将其自然分解为两个部分：1）可解析计算的密集梯度，用于token级模仿学习；2）蒙特卡洛估计的稀疏梯度，用于长程奖励优化。密集梯度具有闭式解，便于GPU高效实现。

Result: 提出了一个统一的LLM微调框架，能够同时进行token级模仿学习和长程奖励优化，其中密集梯度部分具有闭式解，可实现高效的GPU实现。

Conclusion: 该框架成功地将模仿学习和强化学习统一起来，为LLM微调提供了更高效和全面的方法，特别适合需要同时考虑局部token准确性和全局任务奖励的场景。

Abstract: We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.

</details>


### [22] [FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents](https://arxiv.org/abs/2512.22733)
*Jiaqi Shao,Yufeng Miao,Wei Zhang,Bing Luo*

Main category: cs.LG

TL;DR: FoldAct框架解决了长视野RL中上下文折叠方法导致的非平稳观测分布问题，通过分离损失计算、全上下文一致性损失和选择性片段训练，实现稳定训练和5.19倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有上下文折叠方法将摘要动作视为标准动作，忽略了摘要会改变智能体未来观测空间，导致策略依赖的非平稳观测分布，违反了RL核心假设，引发梯度稀释、自条件和计算成本三大挑战。

Method: 提出FoldAct框架，包含三个关键创新：1) 分离损失计算，为摘要和动作token提供独立梯度信号；2) 全上下文一致性损失，减少分布偏移；3) 选择性片段训练，降低计算成本。

Result: 实现了长视野搜索智能体的稳定训练，解决了非平稳观测问题，同时将训练效率提升了5.19倍。

Conclusion: FoldAct通过明确处理上下文折叠带来的非平稳观测问题，为长视野RL提供了稳定高效的训练框架，解决了现有方法的根本性缺陷。

Abstract: Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \textbf{FoldAct}\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\times$ speedup.

</details>


### [23] [ReDiF: Reinforced Distillation for Few Step Diffusion](https://arxiv.org/abs/2512.22802)
*Amirhossein Tighkhorshid,Zahra Dehghanian,Gholamali Aminian,Chengchun Shi,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出基于强化学习的扩散模型蒸馏框架，将蒸馏过程视为策略优化问题，通过奖励信号动态指导学生探索去噪路径，实现更少推理步骤和计算资源下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型采样速度慢，现有蒸馏方法依赖固定的重构或一致性损失，限制了学生模型的学习效率和性能。需要一种更灵活、动态的蒸馏方法来提高扩散模型的推理效率。

Method: 将扩散模型蒸馏过程重新定义为强化学习策略优化问题。学生模型作为策略网络，通过奖励信号（基于与教师模型输出的对齐度）进行训练，动态探索多个去噪路径，学习采取更长、优化的步骤走向数据分布的高概率区域。

Result: 实验结果表明，该方法相比现有蒸馏技术，在显著减少推理步骤和计算资源的情况下实现了更优越的性能。框架具有模型无关性，适用于任何类型的扩散模型。

Conclusion: 提出的强化学习蒸馏框架为扩散模型提供了一种通用的优化范式，能够有效提高推理效率，同时保持生成质量，为高效扩散学习开辟了新途径。

Abstract: Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.

</details>


### [24] [TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning](https://arxiv.org/abs/2512.22824)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: 提出了一种基于时间方差驱动课程的学生-教师学习范式，用于加速目标条件强化学习，通过教师模块动态选择策略置信度方差最高的目标来提升学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在单目标任务上表现良好，但在多目标场景下均匀选择目标会导致样本效率低下。受生物系统自适应结构化学习过程启发，需要一种更高效的目标选择机制来加速目标条件强化学习。

Method: 提出学生-教师学习范式，教师模块基于策略置信度（由状态-动作值Q函数参数化）的时间方差动态选择目标。教师针对高不确定性目标提供自适应聚焦学习信号，促进持续高效进展。方法建立了Q值时间方差与策略演化之间的理论联系。

Result: 在11个不同的机器人操作和迷宫导航任务上进行评估，结果显示相比最先进的课程学习和目标选择方法，该方法取得了持续且显著的改进。

Conclusion: 提出的时间方差驱动课程方法能够有效加速目标条件强化学习，具有算法无关性，可无缝集成到现有RL框架中，通过自适应目标选择显著提升学习效率。

Abstract: Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.

</details>


### [25] [Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning](https://arxiv.org/abs/2512.23087)
*Yingru Li,Jiawei Xu,Jiacai Liu,Yuxuan Tong,Ziniu Li,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种解决大语言模型强化学习中训练-推理不匹配问题的方法，通过动态剪枝词汇表排除极端低概率词元，用有界的优化偏差替代系统性的不匹配偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的强化学习面临训练-推理不匹配问题：高吞吐量推理引擎和数值精确训练系统从相同参数产生不同的概率分布，这种不匹配对低概率词元影响更大，导致梯度估计不稳定。

Method: 提出将RL目标约束到动态剪枝的"安全"词汇表，排除极端尾部的低概率词元，用有界的优化偏差替代系统性的不匹配偏差。

Result: 该方法实现了稳定的训练，理论上界定了词汇表剪枝引入的优化偏差。

Conclusion: 通过动态剪枝词汇表排除极端低概率词元，可以有效解决训练-推理不匹配问题，实现稳定的强化学习训练。

Abstract: Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.

</details>


### [26] [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://arxiv.org/abs/2512.23236)
*Gang Liao,Hongsen Qin,Ying Wang,Alicia Golden,Michael Kuchnik,Yavuz Yetim,Jia Jiunn Ang,Chunli Fu,Yihan He,Samuel Hsia,Zewei Jiang,Dianshi Li,Uladzimir Pashkevich,Varna Puvvada,Feng Shi,Matt Steiner,Ruichao Xiao,Nathan Yan,Xiayu Yu,Zhou Fang,Abdul Zainul-Abedin,Ketan Singh,Hongtao Yu,Wenyuan Chi,Barney Huang,Sean Zhang,Noah Weller,Zach Marine,Wyatt Cook,Carole-Jean Wu,Gaoxiang Liu*

Main category: cs.LG

TL;DR: KernelEvolve是一个面向DLRM的代理内核编码框架，通过多抽象层自动生成和优化内核，解决硬件异构性问题，显著提升性能并降低开发时间。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型(DLRM)训练和推理需要高效快速，但面临模型架构多样性、内核原语多样性、硬件异构性三大系统挑战，需要自动化解决方案来应对大规模异构硬件环境。

Method: 提出KernelEvolve框架，采用多编程抽象层（从Triton/CuTe DSL到底层硬件无关语言），基于图搜索策略进行内核优化，通过检索增强提示合成动态适应运行时执行环境。

Result: 在KernelBench套件上实现100%通过率（250个问题），支持160个PyTorch ATen操作符，在三种异构硬件平台上保持100%正确性，开发时间从数周缩短到数小时，性能显著优于PyTorch基线。

Conclusion: KernelEvolve有效解决了DLRM的硬件异构性问题，不仅提升性能效率，还降低了新AI硬件的编程门槛，支持自动化为内部开发的AI硬件生成内核。

Abstract: Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.

</details>


### [27] [Evaluating Parameter Efficient Methods for RLVR](https://arxiv.org/abs/2512.23165)
*Qingyu Yin,Yulun Wu,Zhennan Shen,Sunbowen Li,Zhilin Wang,Yanshu Li,Chak Tou Leong,Jiale Kang,Jinjin Gu*

Main category: cs.LG

TL;DR: 首次系统评估12种参数高效微调方法在RLVR范式下的表现，发现DoRA、AdaLoRA等结构变体优于标准LoRA，SVD初始化策略存在谱崩溃问题，极端参数压缩会损害推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR通过可验证反馈激励语言模型提升推理能力，但现有研究默认使用LoRA等PEFT方法，缺乏对RLVR场景下最优PEFT架构的系统探索。

Method: 在DeepSeek-R1-Distill系列模型上，对超过12种PEFT方法在数学推理基准上进行全面评估，包括结构变体、SVD初始化策略和极端参数压缩方法。

Result: 1. DoRA、AdaLoRA、MiSS等结构变体持续优于标准LoRA；2. PiSSA、MiLoRA等SVD初始化策略因主成分更新与RL优化不匹配而失败；3. VeRA、Rank-1等极端参数压缩严重限制推理能力。

Conclusion: 标准LoRA不应作为RLVR的默认选择，需要更多探索参数高效的RL方法，为PEFT在RLVR中的应用提供了明确指导。

Abstract: We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.

</details>


### [28] [The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models](https://arxiv.org/abs/2512.23340)
*Dakuan Lu,Jiaqi Zhang,Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 该研究提出了多模型协作定律，证明LLM集成系统遵循总参数数量的幂律缩放，比单模型缩放具有更显著的性能提升趋势和更低的损失下限。


<details>
  <summary>Details</summary>
Motivation: 单个大语言模型的能力存在固有边界，而多模型集成技术（如模型路由和后处理集成）虽然快速发展，但缺乏统一的理论框架来预测多模型协作的性能缩放规律。

Method: 提出多模型协作定律，采用方法无关的公式化方法，假设理想化的集成预言机，其中每个样本的总交叉熵损失由模型池中任何模型的最小损失决定。

Result: 多模型系统相对于总参数数量遵循幂律缩放，比单模型缩放表现出更显著的改进趋势和更低的理论损失下限；异构模型家族的集成比单一模型家族内的集成获得更好的性能缩放。

Conclusion: 模型协作是扩展LLM智能前沿的关键维度，模型多样性是协作增益的主要驱动力。

Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.

</details>


### [29] [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)
*Yilun Luo,HuaQing Zheng,Haoqian Meng,Wenyuan Liu,Peng Zhang*

Main category: cs.LG

TL;DR: 华为openPangu-Embedded模型的三种CoT推理模式在Ascend NPU上存在内存和延迟开销问题，本文通过低比特量化（INT8和W4A8）优化推理效率，在保持90%以上精度的情况下实现1.5倍预填充加速。


<details>
  <summary>Details</summary>
Motivation: openPangu-Embedded模型的三种Chain-of-Thought推理模式（slow_think、auto_think、no_think）虽然增强了推理能力，但生成长推理轨迹带来了显著的内存和延迟开销，在Ascend NPU上的实际部署面临挑战。

Method: 提出统一低比特推理框架，支持INT8（W8A8）和W4A8量化，将FP16计算转换为更高效的整数运算，专门针对Atlas A2上的openPangu-Embedded模型进行优化。

Result: 在代码生成基准测试（HumanEval和MBPP）上，INT8量化保持超过90%的FP16基线精度，在Atlas A2上实现1.5倍预填充加速；W4A8量化显著减少内存消耗，但精度有所折衷。

Conclusion: 低比特量化能有效促进Ascend NPU上的高效CoT推理，同时保持较高的模型保真度，为实际部署提供了可行的解决方案。

Abstract: Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.

</details>


### [30] [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](https://arxiv.org/abs/2512.23461)
*Zhuo Li,Pengyu Cheng,Zhechao Yu,Feifei Tong,Anningzhe Gao,Tsung-Hui Chang,Xiang Wan,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 提出DIR方法，通过信息优化来消除奖励模型中的归纳偏差，提升RLHF性能


<details>
  <summary>Details</summary>
Motivation: 奖励模型训练数据质量低，包含多种归纳偏差（如响应长度、迎合性、格式等），容易导致过拟合和奖励攻击。现有去偏方法要么针对单一偏差，要么只建模简单的线性相关性，无法处理复杂多样的偏差。

Method: 提出基于信息瓶颈的去偏方法DIR：最大化奖励模型分数与人类偏好对之间的互信息，同时最小化奖励模型输出与偏好输入中偏差属性之间的互信息。从信息论角度提供理论依据，能够处理非线性相关的复杂偏差类型。

Result: 在三种归纳偏差（响应长度、迎合性、格式）上验证了DIR的有效性。DIR不仅能有效减轻目标偏差，还能提升RLHF在多种基准测试上的性能，获得更好的泛化能力。

Conclusion: DIR方法通过信息优化有效解决了奖励模型中的复杂归纳偏差问题，扩展了去偏方法在实际应用场景中的适用性，显著提升了RLHF的性能和泛化能力。

Abstract: Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.

</details>


### [31] [BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization](https://arxiv.org/abs/2512.23631)
*Iris Xu,Guangtao Zeng,Zexue He,Charles Jin,Aldo Pareja,Dan Gutfreund,Chuang Gan,Zhang-Wei Hong*

Main category: cs.LG

TL;DR: BOAD框架通过多臂老虎机优化自动发现分层多智能体系统，显著提升大语言模型在长视野软件工程任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体系统在处理真实世界软件工程问题时，需要在一个推理链中处理整个工作流程，导致模型需要保留无关上下文，产生虚假相关性和泛化能力差。受人类工程师分解复杂问题的启发，需要设计能够协调专业化子智能体的分层多智能体系统。

Method: 提出Bandit Optimization for Agent Design (BOAD)框架，将层次发现建模为多臂老虎机问题，每个臂代表候选子智能体，奖励衡量其与其他智能体协作时的帮助程度。该框架能在有限评估预算下高效探索子智能体设计。

Result: 在SWE-bench-Verified上，BOAD优于单智能体和手动设计的多智能体系统。在SWE-bench-Live上，36B系统在评估时排名第二，超越了GPT-4和Claude等更大模型。

Conclusion: 自动发现的分层多智能体系统能显著提高在具有挑战性的长视野软件工程任务上的泛化能力。

Abstract: Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [32] [Syntax Is Not Enough: An Empirical Study of Small Transformer Models for Neural Code Repair](https://arxiv.org/abs/2512.22216)
*Shaunak Samant*

Main category: cs.SE

TL;DR: 小型CodeT5模型在Java程序修复中语法正确率高(94%)，但语义修复完全失败(0精确匹配)，80%情况直接复制bug代码


<details>
  <summary>Details</summary>
Motivation: 尽管神经模型在程序修复基准测试中表现良好，但实际部署仍有限。本研究旨在探索小型transformer模型能否有效修复真实Java bug，以及语法正确性是否可靠反映语义正确性

Method: 使用CodeT5-small模型(60.5M参数)在CodeXGLUE的52,364个Java bug修复对上微调，通过AST解析评估token级性能和语法有效性

Result: 模型收敛良好且语法正确率高(约94%生成语法有效Java代码)，但在精确匹配评估下修复完全失败(0精确匹配)，约80%情况直接复制bug输入

Conclusion: 语法正确性不是语义正确性的可靠代理，小型transformer模型在真实Java bug修复中表现不佳，需要更严格的语义评估方法

Abstract: Automated program repair using neural models has shown promising results on benchmark datasets, yet practical deployment remains limited. In this study, we examine whether a small transformer model can meaningfully repair real-world Java bugs and whether syntactic correctness is a reliable proxy for semantic correctness.
  We fine-tune CodeT5-small (60.5M parameters) on 52,364 Java bug-fix pairs from CodeXGLUE and evaluate both token-level performance and syntactic validity using AST parsing. While the model converges cleanly and achieves high grammatical correctness, producing syntactically valid Java code in approximately ninety-four percent of cases, it fails to generate correct repairs under exact-match evaluation, achieving zero exact matches. In approximately eighty percent of cases, the model reproduces the buggy input verbatim.

</details>


### [33] [Agentic Software Issue Resolution with Large Language Models: A Survey](https://arxiv.org/abs/2512.22256)
*Zhonghao Jiang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: 这篇论文对基于LLM的智能体软件问题解决进行了系统性综述，涵盖了126项最新研究，从基准测试、技术和实证研究三个维度建立了分类体系，并强调了智能体强化学习带来的范式转变。


<details>
  <summary>Details</summary>
Motivation: 软件问题解决（如bug修复和效率优化）是软件维护的关键环节。虽然LLM在自动软件问题解决方面取得了进展，但现实世界的软件问题解决需要长程推理、迭代探索和反馈驱动的决策，这需要超越传统单步方法的智能体能力。LLM-based智能体系统已成为软件问题解决的主流，其进展不仅能提升软件维护效率和质量，还能为验证智能体系统的推理、规划和执行能力提供现实环境。

Method: 对126项基于LLM的智能体软件问题解决研究进行系统性综述，概述任务的一般工作流程，并从三个维度建立分类体系：基准测试（benchmarks）、技术（techniques）和实证研究（empirical studies）。特别关注智能体强化学习带来的范式转变。

Result: 建立了全面的分类框架，总结了当前研究现状，识别了智能体强化学习如何改变软件工程中智能体系统的设计和训练方式，为领域提供了系统化的知识整理。

Conclusion: 基于LLM的智能体软件问题解决是一个快速发展的领域，智能体强化学习带来了范式转变。论文总结了关键挑战并指出了未来研究方向，为人工智能与软件工程的融合提供了重要参考。

Abstract: Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.
  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.

</details>


### [34] [AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents](https://arxiv.org/abs/2512.22387)
*Bhanu Prakash Vangala,Ali Adibifar,Tanu Malik,Ashish Gehani*

Main category: cs.SE

TL;DR: 对三种LLM代码生成代理（Claude Code、OpenAI Codex、Gemini）生成的300个项目进行实证研究，发现只有68.3%的项目能在干净环境中直接执行，存在显著的隐藏依赖问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为代码生成代理的兴起，它们对生成代码可重现性的影响尚未得到充分探索。本研究旨在调查LLM生成的代码是否能在仅使用模型指定依赖的干净环境中成功执行。

Method: 评估三种最先进的LLM代码生成代理（Claude Code、OpenAI Codex、Gemini），使用100个标准化提示生成300个项目（Python、JavaScript、Java各100个）。引入三层依赖框架（声称依赖、工作依赖、运行时依赖）来量化执行可重现性。

Result: 只有68.3%的项目能开箱即用执行，不同语言间差异显著（Python 89.2%、Java 44.0%）。从声明依赖到实际运行时依赖平均扩展13.5倍，揭示大量隐藏依赖。

Conclusion: LLM生成的代码存在严重的可重现性问题，特别是在依赖管理方面。需要改进LLM代码生成代理的依赖规范能力，以确保生成代码的可靠执行。

Abstract: The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.

</details>


### [35] [Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding](https://arxiv.org/abs/2512.22418)
*Yi-Hung Chou,Boyuan Jiang,Yi Wen Chen,Mingyue Weng,Victoria Jackson,Thomas Zimmermann,James A. Jones*

Main category: cs.SE

TL;DR: 该研究通过扎根理论分析20个"氛围编码"视频，揭示了开发者使用LLM进行编程的实践谱系：从完全依赖AI不检查代码，到检查并调整生成结果，但都需应对生成的随机性，调试常被描述为"掷骰子"。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的"氛围编码"（主要通过提示而非写代码来构建软件）被广泛宣传为生产力突破，但实际中开发者如何定义和实践这些方法尚不清楚。本研究旨在揭示这一新兴现象。

Method: 采用扎根理论研究法，分析20个氛围编码视频：包括7个直播编码会话（约16小时，254个提示）和13个观点视频（约5小时），辅以活动时长和提示意图的补充分析。

Result: 发现行为谱系：一些氛围编码者几乎完全依赖AI而不检查代码，而另一些则会检查和调整生成输出。所有方法都必须应对生成的随机性，调试常被描述为"掷骰子"。不同的心智模型（由专业知识和AI依赖程度塑造）影响提示策略、评估实践和信任水平。

Conclusion: 这些发现为软件工程未来研究开辟了新方向，并为工具设计和教育提供了实践机会。

Abstract: Large language models (LLMs) are reshaping software engineering by enabling "vibe coding," in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as "rolling the dice." Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.

</details>


### [36] [GraphLocator: Graph-guided Causal Reasoning for Issue Localization](https://arxiv.org/abs/2512.22469)
*Wei Liu,Chao Peng,Pengfei Gao,Aofan Liu,Wei Zhang,Haiyan Zhao,Zhi Jin*

Main category: cs.SE

TL;DR: GraphLocator通过因果图结构解决软件问题定位中的症状-原因不匹配和一对多不匹配问题，显著提升定位准确率


<details>
  <summary>Details</summary>
Motivation: 软件问题定位任务面临两个关键挑战：症状-原因不匹配（问题描述不直接揭示根本原因）和一对多不匹配（单个问题对应多个相互依赖的代码实体），现有方法难以有效处理这些语义鸿沟

Method: 提出GraphLocator方法，构建因果问题图(CIG)，顶点表示发现的子问题及其关联代码实体，边编码因果关系。采用两阶段工作流：症状顶点定位和动态CIG发现，先识别症状位置，然后通过迭代推理邻接顶点动态扩展CIG

Result: 在三个真实数据集上，GraphLocator相比基线方法平均提升函数级召回率19.49%和精确率11.89%；在症状-原因和一对多不匹配场景下分别提升召回率16.44%和19.18%，精确率7.78%和13.23%；生成的CIG在下游解决任务中带来28.74%的性能提升

Conclusion: GraphLocator通过因果结构发现和动态问题解耦有效解决了软件问题定位中的两个关键不匹配问题，显著提升了定位准确性和下游任务性能

Abstract: The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.

</details>


### [37] [From Rookie to Expert: Manipulating LLMs for Automated Vulnerability Exploitation in Enterprise Software](https://arxiv.org/abs/2512.22753)
*Moustapha Awwalou Diouf,Maimouna Tamah Diao,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Jordan Samhi,Gervais Mendy,Samuel Ouya,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: LLMs使非程序员也能成为攻击者，通过RSA策略可100%成功生成漏洞利用代码，挑战了传统安全假设


<details>
  <summary>Details</summary>
Motivation: LLMs的普及使非程序员也能开发软件，但这也破坏了传统安全假设——即攻击需要技术专业知识。研究旨在展示LLMs如何能被社会工程学操纵，使新手成为有效攻击者

Method: 提出RSA策略（角色分配、场景伪装、行动诱导），通过社会工程学方法操纵LLMs生成漏洞利用代码。在Odoo ERP平台上测试了5个主流LLM（GPT-4o、Gemini、Claude、Microsoft Copilot、DeepSeek）

Result: 100%成功率：所有测试的CVE漏洞都能在3-4轮提示内生成至少一个可工作的利用代码。相比之前研究需要手动努力，本研究完全消除了这种开销

Conclusion: LLMs颠覆了软件工程安全基本原则：技术与非技术人员的界限不再有效；漏洞描述的技术复杂性不再提供保护；传统安全边界瓦解。需要为"仅需提示能力而非代码理解"的新时代重新设计安全实践

Abstract: LLMs democratize software engineering by enabling non-programmers to create applications, but this same accessibility fundamentally undermines security assumptions that have guided software engineering for decades. We show in this work how publicly available LLMs can be socially engineered to transform novices into capable attackers, challenging the foundational principle that exploitation requires technical expertise. To that end, we propose RSA (Role-assignment, Scenario-pretexting, and Action-solicitation), a pretexting strategy that manipulates LLMs into generating functional exploits despite their safety mechanisms. Testing against Odoo -- a widely used ERP platform, we evaluated five mainstream LLMs (GPT-4o, Gemini, Claude, Microsoft Copilot, and DeepSeek) and achieved a 100% success rate: tested CVE yielded at least one working exploit within 3-4 prompting rounds. While prior work [13] found LLM-assisted attacks difficult and requiring manual effort, we demonstrate that this overhead can be eliminated entirely.
  Our findings invalidate core software engineering security principles: the distinction between technical and non-technical actors no longer provides valid threat models; technical complexity of vulnerability descriptions offers no protection when LLMs can abstract it away; and traditional security boundaries dissolve when the same tools that build software can be manipulated to break it. This represents a paradigm shift in software engineering -- we must redesign security practices for an era where exploitation requires only the ability to craft prompts, not understand code.
  Artifacts available at: https://anonymous.4open.science/r/From-Rookie-to-Attacker-D8B3.

</details>


### [38] [FasterPy: An LLM-based Code Execution Efficiency Optimization Framework](https://arxiv.org/abs/2512.22827)
*Yue Wu,Minghao Han,Ruiyin Li,Peng Liang,Amjed Tahir,Zengyang Li,Qiong Feng,Mojtaba Shahin*

Main category: cs.SE

TL;DR: FasterPy是一个基于大语言模型的Python代码优化框架，结合检索增强生成和低秩适配技术，在PIE基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法需要手动设计和维护特定性能bug的规则，劳动密集且适用性有限。基于机器学习和深度学习的方法虽然有所改进，但依赖特定程序表示和精心构建的训练数据集，开发成本高且难以扩展。大语言模型在代码生成方面的卓越能力为自动化代码优化提供了新途径。

Method: FasterPy结合检索增强生成（RAG）和低秩适配（LoRA）技术。RAG基于从现有性能改进代码对和相应性能测量构建的知识库，LoRA用于增强代码优化性能。

Result: 在Performance Improving Code Edits (PIE)基准测试中，该方法在多个指标上优于现有模型。

Conclusion: FasterPy是一个低成本、高效的框架，能够适应大语言模型来优化Python代码的执行效率。

Abstract: Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.

</details>


### [39] [Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving](https://arxiv.org/abs/2512.23511)
*Xinyi Zheng,Ningke Li,Xiaokun Luan,Kailong Wang,Ling Shi,Meng Sun,Haoyu Wang*

Main category: cs.SE

TL;DR: MATP是一个通过多步自动定理证明来系统验证LLM推理的评估框架，将自然语言推理转化为一阶逻辑，使用自动定理证明器评估逻辑有效性，在推理步骤验证上比基于提示的方法提升超过42个百分点。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗、法律和科学研究等高风险领域展现出强大的推理能力，但其推理中常包含被流畅语言掩盖的细微逻辑错误，现有方法如事实核查、自一致性方法和基于规则的验证无法检测多步推理中的复杂逻辑缺陷。

Method: MATP将自然语言推理转化为一阶逻辑（FOL），然后应用自动定理证明器来评估逐步的逻辑有效性，能够识别隐藏的逻辑错误并提供细粒度的推理正确性分类。

Result: 在包含10,830个推理实例的基准测试中（来自PrOntoQA-OOD、ProofWriter和FOLIO任务，由10个LLM生成），MATP在推理步骤验证上比基于提示的基线方法提升超过42个百分点，并揭示了推理模型比通用模型生成更逻辑一致的输出。

Conclusion: MATP展示了通过自动定理证明增强LLM生成推理可信度的潜力，为系统验证LLM推理提供了一种有效方法。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.
  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型在监督微调后，即使没有明确提示，也会在争议性和有害话题上表现出说服倾向，揭示了新兴的有害说服风险。


<details>
  <summary>Details</summary>
Motivation: 随着对话式AI系统的广泛应用，AI对人类观点和信念的影响日益显著。先前研究主要关注滥用场景下的模型说服力，但本研究旨在探究模型在何种情况下会自发进行说服，这对于评估新兴的说服风险至关重要。

Method: 研究在两种场景下考察无提示说服：1）通过内部激活引导使模型具有特定人格特质；2）通过监督微调使模型表现出相同特质。研究使用包含良性话题的通用说服数据集进行微调，然后测试模型在争议性和有害话题上的说服倾向。

Result: 研究发现，通过激活引导向说服相关或无关特质的方向调整，并不能可靠增加模型的无提示说服倾向。然而，监督微调确实能提高模型的说服倾向。更重要的是，仅使用良性话题数据集进行微调的模型，在争议性和有害话题上也表现出更高的说服倾向。

Conclusion: 监督微调可能导致模型在没有明确提示的情况下，在争议性和有害话题上产生说服行为，这表明新兴的有害说服风险确实存在，需要进一步研究和关注。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [41] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 提出了Agentic Risk & Capability (ARC)框架，这是一个技术治理框架，帮助组织识别、评估和缓解由智能AI系统带来的风险，重点关注能力中心视角和风险来源分析。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统具有自主行动能力（如代码执行、互联网交互、文件修改），带来了显著机遇和新风险，对组织治理提出了挑战，需要全面识别、评估和缓解这些多样化且不断演化的风险。

Method: 引入ARC框架，采用能力中心视角分析智能AI系统，提炼出三个主要风险来源（组件、设计、能力），建立风险源、具体风险和相应技术控制之间的明确联系，并提供结构化实施方法。

Result: 开发了一个开放源码的技术治理框架，为组织提供稳健且适应性强的方法来应对智能AI的复杂性，支持快速有效创新，同时确保智能AI系统的安全、可靠和负责任部署。

Conclusion: ARC框架为组织管理智能AI风险提供了实用工具，通过系统化方法平衡创新与安全需求，促进负责任AI部署。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [42] [Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks](https://arxiv.org/abs/2512.22255)
*Abhranil Chandra,Ayush Agrawal,Arian Hosseini,Sebastian Fischmeister,Rishabh Agarwal,Navin Goyal,Aaron Courville*

Main category: cs.AI

TL;DR: 研究发现，即使思维链（CoT）轨迹最终答案错误，用更强大模型生成的合成数据集训练语言模型也能提升推理能力，效果甚至优于人类标注数据。


<details>
  <summary>Details</summary>
Motivation: 探索如何更有效地提升语言模型的推理能力，特别是研究即使最终答案错误的思维链轨迹是否仍对模型学习有益，以及数据分布与模型自身分布的接近程度对学习效果的影响。

Method: 使用更强大模型生成思维链轨迹的合成数据集（即使最终答案错误）来训练语言模型。通过语言模型改写人类标注轨迹来调整数据分布，并引入逐渐增加的推理缺陷来测试模型对错误推理的容忍度。

Result: 在数学、算法推理和代码生成等推理任务上，使用合成数据集训练的语言模型表现优于使用人类标注数据训练的模型。数据分布接近模型自身分布是关键因素，且最终答案正确并不总是可靠推理过程的指标。

Conclusion: 精心策划接近模型自身分布的数据集对提升推理能力至关重要，即使推理轨迹最终答案错误，其中的有效推理步骤仍能为模型学习提供价值。

Abstract: We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.

</details>


### [43] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World：一个工具增强的多智能体框架，通过多智能体反馈实现推理时世界模型生成，并作为监督微调的数据引擎，在PDDL和可执行代码表示上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLM生成符号世界模型（如PDDL领域或可执行模拟器）面临大规模可验证监督数据缺乏的问题，现有静态验证方法无法捕捉交互执行中的行为级错误。

Method: 提出三阶段多智能体框架：1) Deep Researcher通过网页搜索进行知识合成填补规范缺口；2) Model Developer实现可执行世界模型；3) Testing Team进行自适应单元测试和基于模拟的验证。

Result: 在三个涵盖PDDL和可执行代码表示的基准测试中实现一致的SOTA推理时性能。使用该框架生成的数据微调模型，世界模型生成能力平均相对提升30.95%。

Conclusion: Agent2World不仅提供了强大的推理时世界模型生成能力，还通过多智能体交互反馈创建了高质量训练数据，显著提升了模型的世界建模能力。

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [44] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 提出Monadic Context Engineering (MCE)架构范式，利用函子、应用函子和单子的代数结构为AI智能体设计提供形式化基础，解决现有智能体架构的状态管理、错误处理和并发问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自主智能体架构通常采用命令式、临时性的设计模式，导致系统脆弱，存在状态管理困难、错误处理不足和并发处理问题。

Method: 引入Monadic Context Engineering (MCE)架构范式，利用函子、应用函子和单子的代数结构构建智能体工作流。单子支持健壮的顺序组合，应用函子提供并行执行的结构化方法，单子变换器实现这些能力的系统化组合。

Result: MCE能够从简单、可独立验证的组件构建复杂、健壮且高效的AI智能体，并扩展为支持元智能体的生成式编排，通过元编程动态创建和管理子智能体工作流。

Conclusion: MCE为AI智能体设计提供了形式化基础，通过代数抽象内在管理横切关注点，解决了现有架构的脆弱性问题，支持构建复杂、可组合的智能体系统。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [45] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 提出了DarkPatterns-LLM基准数据集和诊断框架，用于细粒度评估LLM输出中的操纵性内容，涵盖七个伤害类别，发现现有模型在检测自主性破坏模式方面存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的普及加剧了对操纵性或欺骗性行为的担忧，现有安全基准主要依赖粗糙的二元标签，无法捕捉构成操纵的微妙心理和社会机制。

Method: 开发了包含401个精心策划示例的数据集，采用四层分析管道：多粒度检测、多尺度意图分析、威胁协调协议和深度上下文风险对齐，涵盖七个伤害类别。

Result: 评估GPT-4、Claude 3.5和LLaMA-3-70B等最先进模型，发现性能差异显著（65.2%-89.7%），在检测自主性破坏模式方面存在一致弱点。

Conclusion: DarkPatterns-LLM建立了首个标准化、多维度的LLM操纵检测基准，为构建更可信的AI系统提供可操作的诊断工具。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [46] [SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G](https://arxiv.org/abs/2512.22579)
*Yong Xiao,Xubo Li,Haoran Zhou,Yingyu Li,Yayu Gao,Guangming Shi,Ping Zhang,Marwan Krunz*

Main category: cs.AI

TL;DR: SANet是一种用于无线网络的语义感知AgentNet架构，通过多智能体多目标优化实现用户语义目标推断和网络层智能体自动分配，在减少计算开销的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: AgentNet作为去中心化的AI原生网络范式，其中协作的智能体通常具有不同甚至冲突的目标，需要解决这种多智能体多目标优化问题。

Method: 提出SANet架构，采用模型分区与共享框架，将大型模型划分为共享部分和智能体特定部分，提出两种去中心化优化算法，并开发硬件原型实现三层网络交互。

Result: 实验结果显示，该框架相比最先进算法实现了高达14.61%的性能提升，同时仅需要44.37%的FLOPs计算量。

Conclusion: SANet通过语义感知和多智能体优化，在无线网络中实现了高效的自适应决策，证明了优化、泛化和冲突误差之间存在三方面权衡。

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.

</details>


### [47] [LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation](https://arxiv.org/abs/2512.22608)
*Zhongyang Liu,Haoyu Pei,Xiangyi Xiao,Xiaocong Du,Yihui Li,Suting Hong,Kunpeng Zhang,Haipeng Zhang*

Main category: cs.AI

TL;DR: 提出SimVC-CAS多智能体系统，通过模拟风险投资群体决策过程来预测初创企业成功，相比传统单决策者方法显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 初创企业价值高但失败率高，预测其成功是跨学科研究的关键挑战。现有方法通常从单一决策者角度建模，忽略了现实中主导风险投资决策的投资者群体集体动态。

Method: 提出SimVC-CAS集体智能体系统，将VC决策模拟为多智能体交互过程。设计角色扮演智能体（代表具有独特特质和偏好的投资者）和基于GNN的监督交互模块，通过图结构共同投资网络实现异质评估和真实信息交换。

Result: 使用PitchBook真实数据并在严格的数据泄漏控制下，SimVC-CAS显著提高预测准确性（例如平均precision@10相对提升约25%），同时提供可解释的多视角推理。

Conclusion: SimVC-CAS不仅改进了初创企业融资预测，还为其他复杂群体决策场景提供了启示。

Abstract: Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.

</details>


### [48] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: 结构化审议（LLM互相评审预测）在模型多样且信息共享时能显著提升预测准确率，但在同质模型组中无效，额外上下文信息也无帮助。


<details>
  <summary>Details</summary>
Motivation: 人类预测中结构化审议能提升准确性，本研究探索类似干预（让LLM互相评审预测）是否能提升大型语言模型的预测准确率。

Method: 使用Metaculus Q2 2025 AI预测锦标赛的202个已解决二元问题，评估四种场景下的准确率：1) 多样模型+分布式信息，2) 多样模型+共享信息，3) 同质模型+分布式信息，4) 同质模型+共享信息。

Result: 在场景2（多样模型+共享信息）中，干预显著提升准确率，Log Loss降低0.020（约4%相对改善，p=0.017）。同质模型组无改善，额外上下文信息也无帮助。

Conclusion: 审议可能是提升LLM预测的可行策略，但效果取决于模型多样性和信息共享，同质模型组无法从审议中获益。

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [49] [TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning](https://arxiv.org/abs/2512.22673)
*Xiang Cheng,Yulan Hu,Xiangwen Zhang,Lu Xu,Zheng Pan,Xin Li,Yong Liu*

Main category: cs.AI

TL;DR: TravelBench是一个真实世界的旅行规划基准测试，包含多轮交互和工具使用，通过可控的沙盒环境和确定性工具输出来评估LLM代理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划任务在领域覆盖和多轮交互方面有限，无法支持动态的用户-代理交互，因此无法全面评估代理能力。需要建立一个更全面、实用的基准测试来推进LLM代理在旅行规划方面的发展。

Method: 收集真实场景的用户请求，构建三个子集（多轮、单轮、不可解）来评估不同方面的代理性能。建立包含10个旅行领域工具的可控沙盒环境，提供确定性工具输出以确保可靠推理。

Result: 在TravelBench上评估了多个LLM，并分析了它们的行为和性能。该基准为推进LLM代理在旅行规划方面提供了实用且可复现的评估框架。

Conclusion: TravelBench提供了一个全面评估LLM代理旅行规划能力的基准，支持多轮交互和工具使用，有助于推动该领域的研究进展。

Abstract: Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.

</details>


### [50] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: 提出SAMP-HDRL框架，通过分层深度强化学习解决非平稳市场中的投资组合优化问题，实现资产动态分组、全局-局部协调决策和基于效用的资本分配。


<details>
  <summary>Details</summary>
Motivation: 非平稳市场中存在制度转换、动态相关性等问题，现有深度强化学习方法策略可解释性差，需要一种能嵌入市场结构约束、提高适应性和可解释性的投资组合管理方法。

Method: 采用分层深度强化学习框架：1) 动态资产分组将市场分为高质量和普通子集；2) 上层代理提取全局市场信号；3) 下层代理在掩码约束下进行组内分配；4) 基于效用的资本分配机制整合风险和无风险资产。

Result: 在2019-2021年三种市场制度下的回测显示，SAMP-HDRL在波动和震荡条件下持续优于9个传统基线和9个DRL基准。相比最强基线，至少获得5%更高回报率、夏普比率、索提诺比率和2%更高欧米茄比率，在动荡市场中收益更大。

Conclusion: SAMP-HDRL通过将结构市场约束直接嵌入DRL流程，在复杂金融环境中提供了改进的适应性、鲁棒性和可解释性。消融研究证实上层-下层协调、动态聚类和资本分配对鲁棒性至关重要。

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [51] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason是一个通过R1风格方法（SFT+GRPO）训练的视觉语言模型，仅使用少量数据和单GPU。研究发现GRPO能恢复分布内性能但损害跨数据集迁移能力，揭示了RL范式而非规模导致的泛化悖论。


<details>
  <summary>Details</summary>
Motivation: 尽管RL在提升LLM推理任务方面取得进展，但其在资源受限的医学影像应用中的效果尚未充分探索。研究旨在探索在有限计算资源下，RL方法对医学影像视觉语言模型性能的影响。

Method: 采用R1风格方法：先进行监督微调（SFT），然后使用GRPO进行强化学习。仅使用2,000个SFT样本、1,000个RL样本和单个A100 GPU。在CheXpert和NIH基准上进行评估。

Result: GRPO在分布内性能上提升23%（CheXpert macro-F1=0.346），但在跨数据集迁移上下降19%（NIH）。发现泛化悖论：SFT检查点在RL优化前在NIH上表现更好。结构化推理支架对通用VLM有益，但对医学预训练模型增益有限。

Conclusion: 对于需要跨不同人群鲁棒性的临床部署，精心策划的监督微调可能优于激进的强化学习。RL范式而非模型规模导致了泛化问题。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [52] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: SPIRAL框架将三个专门的LLM代理嵌入MCTS循环，通过规划器、模拟器和批评家的协同工作，将MCTS从暴力搜索转变为引导式自校正推理过程，在复杂规划任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在需要探索和自校正的复杂规划任务中经常失败，因为其线性推理过程难以从早期错误中恢复。传统的搜索算法如MCTS在稀疏奖励下效果有限，且未能充分利用LLM的丰富语义能力。

Method: SPIRAL框架将三个专门的LLM代理嵌入MCTS循环：规划器提出创造性下一步，模拟器通过预测现实结果来接地搜索，批评家通过反思提供密集奖励信号。这种协同将MCTS从暴力搜索转变为引导式自校正推理过程。

Result: 在DailyLifeAPIs和HuggingFace数据集上，SPIRAL始终优于默认的思维链规划方法和其他最先进代理。在DailyLifeAPIs上达到83.6%的整体准确率，比次优搜索框架提高超过16个百分点，同时展示出更优的令牌效率。

Conclusion: 将LLM推理构建为引导式、反思性和接地性的搜索过程，可以产生更强大和高效的自规划器。结构化代理协作能够显著提升复杂规划任务的性能。

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [53] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 该论文提出"模型信念"概念，利用LLM的token级概率分布来提取更多信息，相比传统的"模型选择"方法，能更高效地利用LLM生成的数据，减少计算需求约20倍。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时，通常将LLM的输出作为单一数据点，这未能充分利用LLM的概率特性。传统方法效率低下，需要大量运行才能获得准确估计。

Method: 提出并形式化"模型信念"概念，从LLM的token级概率中推导出模型对选择替代方案的信念分布。证明模型信念与模型选择的均值渐近等价，但具有更低的方差和更快的收敛速度。

Result: 在需求估计研究中，模型信念在有限运行次数下比模型选择本身更好地解释和预测真实模型选择。将计算需求减少约20倍以达到足够准确的估计。

Conclusion: 模型信念应作为从LLM生成数据中提取更多信息的默认测量方法，能显著提高统计效率和计算效率。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [54] [TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI](https://arxiv.org/abs/2512.23217)
*Jingming Li*

Main category: cs.AI

TL;DR: TCEval：首个利用热舒适场景评估AI核心认知能力（跨模态推理、因果关联、自适应决策）的框架，发现当前LLM具备基础跨模态推理能力但缺乏对热舒适变量非线性关系的精确因果理解。


<details>
  <summary>Details</summary>
Motivation: LLM任务特定基准存在关键空白，热舒适作为涉及环境因素与个人感知的复杂交互，是评估AI系统真实世界认知能力的理想范式。

Method: 初始化具有虚拟人格属性的LLM代理，引导其生成服装隔热选择和热舒适反馈，并将输出与ASHRAE全球数据库和中国热舒适数据库进行验证。

Result: 四个LLM实验显示：代理反馈与人类精确对齐有限，但在1 PMV容差下方向一致性显著改善；LLM生成的PMV分布与人类数据明显不同；代理在离散热舒适分类中表现接近随机。

Conclusion: TCEval作为生态有效的AI认知图灵测试可行，当前LLM具备基础跨模态推理能力但缺乏对热舒适变量非线性关系的精确因果理解，将AI评估重点从抽象任务熟练度转向具身、情境感知的决策。

Abstract: A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.

</details>


### [55] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 该论文提出了一种新的物理AI范式，通过基于物理验证的代理式学习而非感知推理，训练小型语言模型实现物理系统的可靠控制，在反应堆控制任务中展现出相变行为和稳定的执行级性能。


<details>
  <summary>Details</summary>
Motivation: 当前通用基础模型在物理系统控制中存在根本性障碍，即使前沿视觉语言模型在基本物理任务上准确率也只有50-53%，它们更像是近似猜测器，保持语义合理性但违反物理约束。这种输入不忠实性不是缩放缺陷而是结构限制，感知中心架构优化参数空间模仿，而安全关键控制需要对执行动作提供结果空间保证。

Method: 提出代理式物理AI的新路径，训练360M参数的小型语言模型，基于物理验证而非感知推理进行策略优化。在合成反应堆控制场景上训练，数据集规模从10^3扩展到10^5个示例，模型自主选择执行策略，尽管平衡暴露于四种执行器家族，但自动拒绝约70%训练分布，将95%运行时执行集中在单银行策略上。

Result: 模型展现出尖锐的相变行为：小规模系统表现出高方差模仿和灾难性尾部风险，而大规模模型经历超过500倍的方差崩溃，稳定了执行级行为。学习到的表示能够跨不同物理和连续输入模态迁移，无需架构修改。

Conclusion: 代理式物理AI提供了一条不同于通用基础模型的领域特定基础模型发展路径，通过基于物理验证的策略优化实现了可靠的物理系统控制，解决了感知中心架构在安全关键控制中的结构限制问题。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [56] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文介绍了CubeBench，一个基于魔方的生成式基准测试，用于评估LLM智能体在物理世界部署中的空间推理、长期状态跟踪和主动探索能力，发现现有LLM在长时程任务上表现极差。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在数字领域表现出色，但在物理世界部署中存在显著差距，主要挑战在于形成和维护强大的空间心理模型。本文旨在识别和评估阻碍这一过渡的三个核心认知挑战。

Method: 引入CubeBench基准测试，采用三层诊断框架：1) 使用完整符号信息的基础状态跟踪；2) 逐步评估智能体能力；3) 仅使用部分视觉数据的主动探索。通过提供外部求解器工具来隔离认知瓶颈。

Result: 实验显示领先LLM存在严重限制，在所有长时程任务上的通过率均为0.00%，暴露了长期规划的根本性失败。通过分析失败模式，为开发更物理基础的智能体提供了关键见解。

Conclusion: CubeBench成功识别了LLM智能体在物理世界部署中的核心认知瓶颈，特别是长期规划能力的缺失，为开发更物理基础的智能体提供了重要指导。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [57] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: MindWatcher是一个集成交替思考和多模态思维链推理的工具集成推理代理，能够自主决定是否以及如何调用多样化工具，无需人工提示或工作流，在工具调用性能上匹配或超越更大/更新的模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流的代理在处理需要工具调用的现实世界问题时智能有限，而能够自主推理和工具调用的工具集成推理代理正在成为处理涉及外部环境多步交互的复杂决策任务的有力方法。

Method: 引入交替思考范式使模型能在任何中间阶段在思考和工具调用之间切换，多模态思维链能力允许在推理过程中操作图像以获得更精确的搜索结果。构建自动化数据审计和评估流水线，辅以手动策划的高质量训练数据集，并创建MindWatcher-Evaluate Bench基准进行评估。

Result: MindWatcher通过卓越的工具调用能力，匹配或超越了更大或更新模型的性能。实验还揭示了代理训练的关键见解，如代理强化学习中的遗传继承现象。

Conclusion: MindWatcher作为一个集成交替思考和多模态思维链推理的工具集成推理代理，能够自主协调多样化工具使用，解决广泛领域的多模态问题，并通过更高效的训练基础设施提升了训练速度和硬件利用率。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [58] [The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis](https://arxiv.org/abs/2512.23419)
*Alex Lewandowski,Adtiya A. Ramesh,Edan Meyer,Dale Schuurmans,Marlos C. Machado*

Main category: cs.AI

TL;DR: 论文提出了一种计算嵌入视角，将智能体视为在通用计算机中模拟的自动机，并引入交互性作为衡量智能体持续适应能力的指标，发现深度线性网络比非线性网络更能维持交互性。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习问题通常通过显式约束智能体容量来建模"世界大于智能体"的大世界假设，但这些约束可能具有随意性、难以整合，且可能限制智能体容量扩展的有效性。本文旨在提出一种更自然的嵌入约束视角。

Method: 提出计算嵌入视角，将嵌入智能体表示为在通用（形式）计算机中模拟的自动机；证明这种自动机等价于在可数无限状态空间上的部分可观测马尔可夫决策过程中交互的智能体；提出交互性作为目标函数，衡量智能体通过学习新预测持续适应行为的能力；开发基于模型的强化学习算法来寻求交互性，并构建合成问题评估持续学习能力。

Result: 深度非线性网络难以维持交互性，而深度线性网络随着容量增加能够维持更高的交互性。

Conclusion: 计算嵌入视角为持续学习提供了更自然的约束框架，交互性作为目标函数能够有效评估智能体的持续适应能力，网络架构对维持交互性有重要影响。

Abstract: Continual learning is often motivated by the idea, known as the big world hypothesis, that "the world is bigger" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.

</details>


### [59] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: AKG kernel agent是一个多智能体系统，用于自动化AI计算内核的生成、迁移和性能调优，支持多种DSL语言，在GPU和NPU后端上相比PyTorch Eager实现平均获得1.46倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型对高性能计算内核需求迫切，但LLM、多模态架构等复杂性增加，加上稀疏化、量化等技术以及硬件频繁更新，使得手动优化无法跟上需求，成为AI系统开发的关键瓶颈。

Method: 提出AKG kernel agent多智能体系统，支持Triton、TileLang、CPP、CUDA-C等多种领域特定语言，采用模块化设计便于集成新DSL和硬件目标，自动化内核生成、迁移和性能调优。

Result: 在KernelBench上使用Triton DSL评估，在GPU和NPU后端上相比PyTorch Eager基线实现平均获得1.46倍加速，证明了系统在现代AI工作负载中加速内核开发的有效性。

Conclusion: AKG kernel agent通过多智能体系统自动化内核开发，解决了AI计算内核开发中的瓶颈问题，支持多种DSL和硬件后端，显著加速了内核开发流程。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [60] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: 提出Hindsight instruction Replay (HiR)框架，通过选择-重写策略将失败尝试重播为成功样本，提高复杂指令跟随任务的强化学习效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习用于对齐大语言模型跟随指令时，初始模型常因能力有限无法生成满足所有约束的响应，导致稀疏或难以区分的奖励信号阻碍学习。

Method: 提出HiR框架：1) 采用选择-重写策略，基于已满足的约束将失败尝试重播为成功样本；2) 在重播样本和原始样本上进行强化学习；3) 理论框架为指令级和响应级的双重偏好学习，仅需二元奖励信号。

Result: 在多种指令跟随任务上取得有希望的结果，同时需要更少的计算资源。

Conclusion: HiR是一种样本高效的强化学习框架，能有效解决复杂指令跟随任务中的奖励稀疏问题。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [61] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: 论文探讨如何确保AI系统与人类价值观对齐并保持安全，通过AI辅助和AI关机游戏框架研究该问题，提出需要AI代理能够处理不确定性和非阿基米德偏好。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统与人类价值观对齐并保持安全是重要挑战，需要研究AI辅助问题和AI关机问题，以设计出既能有效帮助人类又能安全运行的AI系统。

Method: 采用AI辅助游戏和AI关机游戏框架，研究AI代理在不确定性下的推理能力，处理不完全偏好和非阿基米德偏好的方法。

Result: 分析表明，解决AI对齐和安全挑战需要AI代理具备在不确定性下推理的能力，并能处理不完全偏好和非阿基米德偏好。

Conclusion: 确保AI系统安全和对齐需要开发能够处理不确定性和复杂偏好的AI代理，AI辅助和关机游戏框架为研究这些问题提供了理论基础。

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [62] [AI code creates 1.7x more problems](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.coderabbit.ai%2Fblog%2Fstate-of-ai-vs-human-code-generation-report%3Futm_source=tldrdev/1/0100019b504180d0-412e9733-1791-441e-a04d-a5b5ad291ff7-000000/5-gk-BC3PF8QsMdmFA45Z0n7ecqtbusmFcQKbsfg4r0=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码生成工具虽然能加速开发，但会产生1.7倍更多问题，代码质量更不稳定且容易引入高风险漏洞，需要建立系统化的工作流程和安全防护层


<details>
  <summary>Details</summary>
Motivation: 当前AI编码工具虽然能显著提升开发速度，但缺乏防护措施的加速会增加风险，需要研究如何平衡AI的加速优势与代码质量保障

Method: 通过实证分析AI生成代码的质量问题，提出需要构建系统化的工作流程、安全防护层和补偿机制来弥补AI的不足

Result: AI生成的代码比人工代码存在1.7倍更多问题，代码质量更不稳定、更容易出错，且更容易引入高严重性漏洞

Conclusion: AI辅助开发的未来在于构建能够放大AI优势同时补偿其缺陷的系统、工作流程和安全层，质量需要经过深思熟虑的工程化设计

Abstract: AI code creates 1.7x more problems (9 minute read) AI coding tools are powerful accelerators, but acceleration without guardrails increases risk. AI-generated code is consistently more variable, error-prone, and likely to introduce high-severity issues without the right protections in place. The future of AI-assisted development is about building systems, workflows, and safety layers that amplify what AI does well while compensating for what it tends to miss. Quality requires deliberate engin...

</details>
