<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.SE](#cs.SE) [Total: 10]
- [tldr.article](#tldr.article) [Total: 10]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents](https://arxiv.org/abs/2512.23713)
*Jahidul Islam,Md Ataullha,Saiful Azad*

Main category: cs.CL

TL;DR: BanglaCodeAct：基于多智能体提示和迭代自校正的孟加拉语到Python代码生成框架，使用开源多语言LLM在Thought-Code-Observation循环中实现动态生成、测试和优化代码。


<details>
  <summary>Details</summary>
Motivation: LLM在英语提示的代码生成方面表现出色，但这种进展尚未扩展到低资源语言。针对孟加拉语到Python代码生成的需求，现有方法依赖任务特定的微调，需要更有效的解决方案。

Method: 提出BanglaCodeAct框架，采用基于智能体的方法，利用开源多语言LLM在Thought-Code-Observation循环中实现多智能体提示和迭代自校正，动态生成、测试和优化代码。

Result: 在mHumanEval数据集上评估多个小型开源LLM，Qwen3-8B结合BanglaCodeAct表现最佳：开发集pass@1准确率94.0%，盲测集71.6%，为孟加拉语到Python翻译建立了新基准。

Conclusion: 该研究为孟加拉语到Python代码生成建立了新基准，证明了基于智能体的推理在低资源语言可靠代码生成方面的潜力，无需任务特定微调。

Abstract: LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\% on the development set and 71.6\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.

</details>


### [2] [HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate](https://arxiv.org/abs/2512.23717)
*Shenzhe Zhu*

Main category: cs.CL

TL;DR: HarmTransform：一个多智能体辩论框架，用于将有害查询转化为更隐蔽的形式，以改进LLM安全对齐训练数据


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全机制主要关注明显有害内容，但用户可以通过隐蔽的改写方式保留恶意意图而显得无害，现有安全训练数据存在显著空白

Method: 引入HarmTransform多智能体辩论框架，通过多个智能体之间的迭代批评和精炼，系统地将有害查询转化为更隐蔽的形式

Result: 实验表明HarmTransform在生成有效查询转换方面显著优于标准基线方法，但辩论也带来主题偏移和不必要复杂性等挑战

Conclusion: 多智能体辩论在生成全面安全训练数据方面既有前景也有局限性，需要平衡转换效果与复杂性

Abstract: Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.

</details>


### [3] [Break Out the Silverware -- Semantic Understanding of Stored Household Items](https://arxiv.org/abs/2512.23739)
*Michaela Levi-Richter,Reuth Mirsky,Oren Glickman*

Main category: cs.CL

TL;DR: 论文提出存储家庭物品挑战基准，用于评估服务机器人的认知能力，并开发了NOAM混合代理管道，结合视觉场景理解和LLM推理来预测隐藏物品的存储位置。


<details>
  <summary>Details</summary>
Motivation: 家庭服务机器人面临的核心挑战是缺乏常识推理能力来推断日常物品（通常隐藏在抽屉、橱柜等不可见位置）的存储位置。现有技术在视觉和操作方面有进展，但在认知推理方面仍有不足。

Method: 提出NOAM（非可见物品分配模型）混合代理管道：1）将视觉输入转换为空间上下文和可见容器的自然语言描述；2）使用大型语言模型（如GPT-4）推理最可能的隐藏存储位置；3）设计两个数据集：100个真实厨房物品-图像对和6500个带存储多边形标注的公开厨房图像。

Result: NOAM相比随机选择、视觉语言管道（Grounding-DINO + SAM）、多模态模型（Gemini、GPT-4o等）和人类表现，显著提高了预测准确率，接近人类水平表现。

Conclusion: NOAM展示了结合结构化场景理解和LLM推理的有效性，为在家庭环境中部署具有认知能力的代理提供了最佳实践，推动了服务机器人常识推理能力的发展。

Abstract: ``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots' cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.
  Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants' kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.
  To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.
  We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.

</details>


### [4] [Efficient Context Scaling with LongCat ZigZag Attention](https://arxiv.org/abs/2512.23966)
*Chen Zhang,Yang Bai,Jiahuan Li,Anchun Gui,Keheng Wang,Feifan Liu,Guanyu Wu,Yuwei Jiang,Defei Bu,Li Wei,Haihang Jing,Hongyin Tang,Xin Chen,Xiangzhou Huang,Fengcun Li,Rongxiang Weng,Yulei Qian,Yifan Lu,Yerui Sun,Jingang Wang,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

TL;DR: LoZA是一种稀疏注意力方案，可将现有全注意力模型转换为稀疏版本，在有限计算预算下显著提升长上下文场景的处理速度


<details>
  <summary>Details</summary>
Motivation: 解决长上下文场景中全注意力模型计算成本高的问题，特别是在预填充密集型（如检索增强生成）和解码密集型（如工具集成推理）任务中

Method: 提出LongCat ZigZag Attention (LoZA)稀疏注意力方案，通过将全注意力模型转换为稀疏版本，在长上下文场景中实现高效处理

Result: LoZA在长上下文场景中实现显著加速，应用于LongCat-Flash模型后得到LongCat-Flash-Exp，能够高效处理高达100万个token，支持长期推理和长视野智能体能力

Conclusion: LoZA提供了一种有效的稀疏注意力方案，能够在有限计算预算下显著提升长上下文处理效率，为长期推理和智能体应用提供支持

Abstract: We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.

</details>


### [5] [iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning](https://arxiv.org/abs/2512.24014)
*Sijia Chen,Di Niu*

Main category: cs.CL

TL;DR: iCLP框架让大语言模型在潜在空间中进行隐式规划，在语言空间中进行推理，通过向量量化自编码器学习紧凑的潜在计划表示，显著提升数学推理和代码生成任务的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于显式文本计划的推理方法面临两个主要问题：1）大语言模型容易产生幻觉，生成不准确的计划；2）任务特定问题的高度多样性使得生成有效计划具有挑战性。受人类隐式认知启发，需要开发能够进行隐式规划的方法。

Method: iCLP框架包含三个步骤：1）从现有的逐步推理轨迹中提取显式计划；2）通过向量量化自编码器和码本学习这些计划的离散表示（潜在计划）；3）在大语言模型上对潜在计划和相应推理步骤进行微调，使模型学会在推理过程中进行隐式规划。

Result: 实验结果表明，在数学推理和代码生成任务上，iCLP使大语言模型能够在潜在空间进行规划，同时在语言空间进行推理。这种方法在准确性和效率方面都有显著提升，并展现出强大的跨领域泛化能力，同时保持了思维链推理的可解释性。

Conclusion: iCLP通过将显式计划压缩为潜在表示，使大语言模型能够进行隐式规划，有效解决了显式文本计划方法的局限性。该方法不仅提升了推理性能，还保持了可解释性，为高效可靠的AI推理系统提供了新思路。

Abstract: Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.

</details>


### [6] [Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs](https://arxiv.org/abs/2512.24289)
*Jonathan Schmoll,Adam Jatowt*

Main category: cs.CL

TL;DR: 论文提出了首个用于欧盟分类法合规评估的公开基准数据集，并系统评估了LLM在定性和定量任务上的表现，发现LLM在定性任务上表现中等，但在定量任务上完全失败，可作为人类专家的辅助工具而非完全自动化解决方案。


<details>
  <summary>Details</summary>
Motivation: 欧盟分类法合规过程需要大量人工和资源，LLM提供了自动化路径，但缺乏公开基准数据集阻碍了相关研究。

Method: 从190份企业报告中构建结构化数据集，包含真实经济活动数据和定量KPI指标，使用该数据集对LLM在核心合规工作流中进行首次系统性评估。

Result: LLM在识别经济活动的定性任务上表现中等，多步智能体框架略微提升精度；在预测财务KPI的定量任务上完全失败；发现简洁元数据比完整非结构化报告表现更好的悖论；模型置信度分数校准不佳。

Conclusion: LLM尚未准备好完全自动化，但可作为人类专家的强大辅助工具；数据集为未来研究提供了公开基准。

Abstract: The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.

</details>


### [7] [Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models](https://arxiv.org/abs/2512.24618)
*Junru Lu,Jiarui Qin,Lingfeng Qiao,Yinghui Li,Xinyi Dai,Bo Ke,Jianfeng He,Ruizhi Qiao,Di Yin,Xing Sun,Yunsheng Wu,Yinsong Liu,Shuangyin Liu,Mingkong Tang,Haodong Lin,Jiayi Kuang,Fanxu Meng,Xiaojuan Tang,Yunjia Xi,Junjie Huang,Haotong Yang,Zhenyi Shen,Yangning Li,Qianwen Zhang,Yifei Yu,Siyu An,Junnan Dong,Qiufeng Wang,Jie Wang,Keyu Chen,Wei Wen,Taian Guo,Zhifeng Shen,Daohai Yu,Jiahao Li,Ke Li,Zongyi Li,Xiaoyu Tan*

Main category: cs.CL

TL;DR: Youtu-LLM是一个1.96B参数的轻量级语言模型，通过从头预训练而非蒸馏，在紧凑架构下实现了强大的推理和规划能力，支持128k上下文，在通用和智能体任务上都达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 当前小型模型通常依赖蒸馏技术，缺乏真正的智能体能力。作者希望开发一个轻量级但具备原生智能体智能的模型，能够在有限计算资源下执行复杂的推理和规划任务。

Method: 1. 采用密集多潜在注意力(MLA)架构和STEM导向词汇表，支持128k上下文窗口；2. 设计"常识-STEM-智能体"三阶段课程学习，使用约11T token数据；3. 针对智能体任务采用多样化数据构造方案，合成数学、编程和工具使用轨迹。

Result: Youtu-LLM在2B以下模型中达到新的SOTA水平：在通用基准测试中与更大模型竞争，在智能体特定任务上显著超越现有基线，证明轻量级模型可以具备强大的内在智能体能力。

Conclusion: 通过系统化的架构设计和训练策略，轻量级模型可以具备强大的原生智能体智能，为资源受限环境下的智能体应用提供了可行方案。

Abstract: We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.

</details>


### [8] [Do Large Language Models Know What They Are Capable Of?](https://arxiv.org/abs/2512.24661)
*Casey O. Barkan,Sid Black,Oliver Sourbut*

Main category: cs.CL

TL;DR: LLM代理普遍过度自信，无法准确预测自身任务成功率，且在多步骤任务中过度自信会加剧。虽然部分LLM能从失败经验中学习降低过度自信，但整体上缺乏对自身能力的认知限制了其决策质量。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能预测自身任务成功率，以及这种预测能力是否会随着任务进展而改善。同时探究LLM能否从上下文经验中学习，在失败成本高的场景中做出更好的任务选择决策。

Method: 测试多个LLM在单步和多步任务中的成功率预测能力，评估其预测的区分度。研究LLM在经历失败经验后是否调整预测，并分析其决策是否理性（基于其估计的成功概率）。

Result: 所有测试的LLM都表现出过度自信，但大多数具有优于随机的区分能力。新模型和更大模型不一定有更好的区分能力（Claude模型除外）。在多步任务中，多个前沿LLM的过度自信会随着任务进展而加剧。部分LLM能从失败经验中学习降低过度自信，改善决策，但并非所有模型都能做到。有趣的是，所有LLM的决策都近似理性（基于其估计概率），但过度乐观的估计导致决策质量差。

Conclusion: 当前LLM代理缺乏对自身能力的认知，这限制了其决策能力。研究结果对AI滥用和错位风险有重要启示，因为LLM的自我认知能力会影响其安全性和可靠性。

Abstract: We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.

</details>


### [9] [R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory](https://arxiv.org/abs/2512.24684)
*Maoyuan Li,Zhongsheng Wang,Haoyuan Li,Jiamou Liu*

Main category: cs.CL

TL;DR: R-Debater是一个基于论证记忆的智能体框架，用于生成多轮辩论。它通过检索案例证据和先前辩论动作，结合角色智能体来生成连贯的辩论话语，在ORCHID数据集上超越了强LLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有辩论系统在保持立场一致性、回应对手以及使用证据支持主张方面存在不足。作者受到修辞学和记忆研究的启发，认为辩论应该是回忆和调整先前论证的过程，需要系统能够跨轮次保持连贯性。

Method: R-Debater整合了辩论知识库（用于检索案例证据和先前辩论动作）和基于角色的智能体（用于跨轮次组合连贯话语）。系统在ORCHID辩论数据集上评估，构建了包含1000个条目的检索语料库和32个辩论的测试集。

Result: 在两个任务评估中：1）下一话语生成（通过InspireScore评估主观、逻辑和事实维度）；2）对抗性多轮模拟（通过Debatrix评估论证、来源、语言和整体维度）。R-Debater在单轮和多轮得分上都超越了强LLM基线。20位经验丰富的辩手参与的人类评估进一步确认了系统的一致性和证据使用能力。

Conclusion: 结合检索基础和结构化规划的R-Debater能够生成更加忠实、立场对齐且跨轮次连贯的辩论。这表明将检索基础与结构化规划相结合对于构建高质量的辩论系统是有效的。

Abstract: We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.

</details>


### [10] [MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models](https://arxiv.org/abs/2512.24693)
*Wenzhe Li,Shujian Zhang,Wenxuan Zhou,John Lambert,Chi Jin,Andrew Hard,Rajiv Mathews,Lun Wang*

Main category: cs.CL

TL;DR: 提出MUSIC数据增强策略，通过多轮对比训练提升多轮对话奖励模型的评估能力，在保持单轮评估性能的同时显著提升多轮对话评估效果。


<details>
  <summary>Details</summary>
Motivation: 多轮对话质量评估对LLM发展至关重要但成本高昂，现有基于单轮对比的偏好数据集无法捕捉多轮交互的细微差别，需要更有效的多轮评估方法。

Method: 提出MUSIC（多步指令对比）无监督数据增强策略，合成跨多轮对话的对比对话对，在Skywork偏好数据集上训练基于Gemma-2-9B-Instruct的多轮奖励模型。

Result: MUSIC增强的奖励模型在多项评估中优于基线方法，与高级专有LLM评判在多轮对话评估上对齐度更高，且不影响标准单轮奖励模型基准性能。

Conclusion: 多轮对比训练对构建鲁棒的多轮奖励模型至关重要，MUSIC策略能有效提升多轮对话评估质量，为LLM训练提供更准确的反馈信号。

Abstract: Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \textit{training} techniques, effective automated \textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \textbf{MU}lti-\textbf{S}tep \textbf{I}nstruction \textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.

</details>


### [11] [BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts](https://arxiv.org/abs/2512.24885)
*Hengli Li,Zhaoxin Yu,Qi Shen,Chenxi Li,Mengmeng Wang,Tinglang Wu,Yipeng Kang,Yuxuan Wang,Song-Chun Zhu,Zixia Jia,Zilong Zheng*

Main category: cs.CL

TL;DR: BEDA框架通过将信念估计转化为生成约束，在战略对话中显著提升性能，在对抗、合作和谈判三种设置中均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有战略对话系统虽然能准确估计信念，但缺乏使用这些信念进行生成的原则性机制。本文旨在填补这一空白，通过将信念估计转化为生成约束来指导对话行为。

Method: 提出BEDA框架，首先形式化对抗和对齐两种核心对话行为，通过概率约束操作化这些行为。框架包含世界集合、信念估计器和条件生成器，后者根据推断的信念选择行为并生成一致的语句。

Result: 在三种设置中均取得显著改进：在CKBG（对抗）中，成功率在不同骨干模型上至少提升5.0个百分点，使用GPT-4.1-nano时提升20.6个百分点；在Mutual Friends（合作）中平均提升9.3个百分点；在CaSiNo（谈判）中达成相对于所有基线的最优交易。

Conclusion: 将信念估计作为约束提供了一种简单、通用的机制，能够实现可靠的战略对话。

Abstract: Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions](https://arxiv.org/abs/2512.23770)
*Ankit Kanwar,Dominik Wagner,Luke Ong*

Main category: cs.LG

TL;DR: SB-TRPO是一种新的信任区域算法，用于硬约束强化学习，通过自适应地将策略更新偏向约束满足，同时寻求奖励改进，在安全性和任务完成之间实现最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域的强化学习中，现有方法（如拉格朗日法和投影法）往往无法确保近乎零的安全违规，或者在面对硬约束时牺牲奖励性能。需要一种既能最大化奖励又能严格遵守安全约束的方法。

Method: 提出Safety-Biased Trust Region Policy Optimisation (SB-TRPO)，这是一种新的信任区域算法。它使用成本和奖励的自然策略梯度的凸组合进行信任区域更新，确保每一步都有固定比例的最优成本减少。算法自适应地将策略更新偏向约束满足，同时仍寻求奖励改进。

Result: 在标准且具有挑战性的Safety Gymnasium任务上的实验表明，SB-TRPO相比最先进方法，始终实现了安全性和有意义的任务完成之间的最佳平衡。

Conclusion: SB-TRPO为硬约束强化学习提供了一种有效的解决方案，能够在确保安全性的同时保持奖励性能，在安全关键领域具有应用价值。

Abstract: Reinforcement learning (RL) in safety-critical domains requires agents to maximise rewards while strictly adhering to safety constraints. Existing approaches, such as Lagrangian and projection-based methods, often either fail to ensure near-zero safety violations or sacrifice reward performance in the face of hard constraints. We propose Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a new trust-region algorithm for hard-constrained RL. SB-TRPO adaptively biases policy updates towards constraint satisfaction while still seeking reward improvement. Concretely, it performs trust-region updates using a convex combination of the natural policy gradients of cost and reward, ensuring a fixed fraction of optimal cost reduction at each step. We provide a theoretical guarantee of local progress towards safety, with reward improvement when gradients are suitably aligned. Experiments on standard and challenging Safety Gymnasium tasks show that SB-TRPO consistently achieves the best balance of safety and meaningful task completion compared to state-of-the-art methods.

</details>


### [13] [Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems](https://arxiv.org/abs/2512.23978)
*Tinglong Dai,David Simchi-Levi,Michelle Xiao Wu,Yao Xie*

Main category: cs.LG

TL;DR: 论文提出一个确保生成式AI自主系统安全性的概念框架，将运筹学方法应用于AI自主决策系统，通过流模型和对抗鲁棒性解决自主性悖论问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正从对话助手转向自主决策系统，这产生了自主性悖论：系统获得更大自主权时，需要更强的形式化结构、明确约束和尾部风险控制。随机生成模型在操作领域可能脆弱，需要可验证的可行性、分布偏移鲁棒性和高后果场景压力测试机制。

Method: 提出基于运筹学的确保自主性概念框架，包含两个互补方法：1) 基于流的生成模型，将生成视为确定性传输，支持可审计性、约束感知生成，连接最优传输、鲁棒优化和序列决策控制；2) 通过对抗鲁棒性视角制定操作安全性，在不确定性或模糊集内评估决策规则对抗最坏情况扰动。

Result: 该框架阐明了增加自主性如何将运筹学的角色从求解器转变为护栏再到系统架构师，负责控制逻辑、激励协议、监控机制和安全边界。这些要素定义了安全关键、可靠性敏感操作领域中确保自主性的研究议程。

Conclusion: 为确保生成式AI自主系统在操作工作流中的安全性，需要结合运筹学方法，通过流模型和对抗鲁棒性框架解决自主性悖论，建立可验证、鲁棒且能应对高后果场景的自主决策系统。

Abstract: Generative artificial intelligence (GenAI) is shifting from conversational assistants toward agentic systems -- autonomous decision-making systems that sense, decide, and act within operational workflows. This shift creates an autonomy paradox: as GenAI systems are granted greater operational autonomy, they should, by design, embody more formal structure, more explicit constraints, and stronger tail-risk discipline. We argue stochastic generative models can be fragile in operational domains unless paired with mechanisms that provide verifiable feasibility, robustness to distribution shift, and stress testing under high-consequence scenarios. To address this challenge, we develop a conceptual framework for assured autonomy grounded in operations research (OR), built on two complementary approaches. First, flow-based generative models frame generation as deterministic transport characterized by an ordinary differential equation, enabling auditability, constraint-aware generation, and connections to optimal transport, robust optimization, and sequential decision control. Second, operational safety is formulated through an adversarial robustness lens: decision rules are evaluated against worst-case perturbations within uncertainty or ambiguity sets, making unmodeled risks part of the design. This framework clarifies how increasing autonomy shifts OR's role from solver to guardrail to system architect, with responsibility for control logic, incentive protocols, monitoring regimes, and safety boundaries. These elements define a research agenda for assured autonomy in safety-critical, reliability-sensitive operational domains.

</details>


### [14] [How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns](https://arxiv.org/abs/2512.24063)
*Haoyue Bai,Yiyou Sun,Wenjie Hu,Shi Qiu,Maggie Ziyu Huan,Peiyang Song,Robert Nowak,Dawn Song*

Main category: cs.LG

TL;DR: 本文提出一个新基准，将推理分解为计算、事实检索、模拟、枚举和诊断等核心技能，用于精细分析SFT和RL调优在LLM中如何影响泛化能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在监督微调(SFT)和强化学习(RL)调优中表现出不同的泛化行为：SFT往往缩小能力范围，而RL倾向于保持能力。现有研究主要依赖粗粒度准确度指标，无法深入理解这种差异的原因。

Method: 1. 引入新基准，将推理分解为计算、事实检索、模拟、枚举和诊断等原子核心技能；2. 结合分布差异和参数统计等低层统计模式分析；3. 提出元探测框架，追踪不同训练阶段模型行为。

Result: RL调优模型保持更稳定的行为特征，抵抗推理技能崩溃；SFT模型表现出更明显的漂移，过度拟合表面模式。基准提供了对数学、科学推理和非推理任务中泛化演变的精细研究。

Conclusion: 这项工作为理解LLM中推理的本质提供了新见解，并为设计促进广泛、稳健泛化的训练策略指明了原则方向。

Abstract: Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.

</details>


### [15] [Enhancing LLM Planning Capabilities through Intrinsic Self-Critique](https://arxiv.org/abs/2512.24103)
*Bernd Bohnet,Pierre-Alexandre Kamienny,Hanie Sedghi,Dilan Gorur,Pranjal Awasthi,Aaron Parisi,Kevin Swersky,Rosanne Liu,Azade Nova,Noah Fiedel*

Main category: cs.LG

TL;DR: LLM通过内在自我批评提升规划性能，在Blocksworld、Logistics和Mini-grid数据集上超越强基线，实现新的SOTA结果


<details>
  <summary>Details</summary>
Motivation: 尽管早期研究对LLM自我批评方法的有效性表示怀疑，但本文旨在证明通过内在自我批评（无需外部验证器）可以显著提升LLM在规划任务上的性能

Method: 采用少样本学习技术并逐步扩展到多样本方法作为基础，然后通过迭代修正和精炼过程进行改进，实现内在自我批评

Result: 在Blocksworld、Logistics和Mini-grid数据集上取得显著性能提升，超越了强基线准确率，在2024年10月的LLM模型检查点中实现了新的SOTA

Conclusion: 自我批评能显著提升规划性能，该方法具有内在自我改进能力，适用于不同模型版本，应用于更复杂搜索技术和更强大模型将带来更好性能

Abstract: We demonstrate an approach for LLMs to critique their \emph{own} answers with the goal of enhancing their performance that leads to significant improvements over established planning benchmarks. Despite the findings of earlier research that has cast doubt on the effectiveness of LLMs leveraging self critique methods, we show significant performance gains on planning datasets in the Blocksworld domain through intrinsic self-critique, without external source such as a verifier. We also demonstrate similar improvements on Logistics and Mini-grid datasets, exceeding strong baseline accuracies. We employ a few-shot learning technique and progressively extend it to a many-shot approach as our base method and demonstrate that it is possible to gain substantial improvement on top of this already competitive approach by employing an iterative process for correction and refinement. We illustrate how self-critique can significantly boost planning performance. Our empirical results present new state-of-the-art on the class of models considered, namely LLM model checkpoints from October 2024. Our primary focus lies on the method itself, demonstrating intrinsic self-improvement capabilities that are applicable regardless of the specific model version, and we believe that applying our method to more complex search techniques and more capable models will lead to even better performance.

</details>


### [16] [GARDO: Reinforcing Diffusion Models without Reward Hacking](https://arxiv.org/abs/2512.24138)
*Haoran He,Yuxiao Ye,Jie Liu,Jiajun Liang,Zhiyong Wang,Ziyang Yuan,Xintao Wang,Hangyu Mao,Pengfei Wan,Ling Pan*

Main category: cs.LG

TL;DR: GARDO框架通过选择性惩罚高不确定性样本、自适应更新参考模型、以及多样性奖励放大，解决了扩散模型在线强化学习中的奖励黑客、探索不足和模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型通过在线强化学习进行微调时，由于代理奖励与真实目标不匹配，常导致奖励黑客（代理分数上升但真实图像质量下降）、探索不足（参考策略通常次优）和模式崩溃（生成多样性下降）等问题。

Method: 提出GARDO框架：1）选择性正则化：仅惩罚高不确定性样本；2）自适应正则化：定期更新参考模型以匹配在线策略能力；3）多样性感知优化：对高质量且高多样性的样本放大奖励。

Result: 在多种代理奖励和未见指标上的实验表明，GARDO能有效缓解奖励黑客、增强生成多样性，同时不牺牲样本效率或探索能力，显示出其有效性和鲁棒性。

Conclusion: GARDO通过智能的正则化策略和多样性优化，解决了扩散模型RL微调中的关键挑战，实现了奖励黑客缓解、多样性保持和高效探索的平衡。

Abstract: Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.

</details>


### [17] [Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback](https://arxiv.org/abs/2512.24818)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文提出使用乐观乘性权重更新(OMWU)算法解决非传递偏好下的Nash学习人类反馈(NLHF)问题，首次证明了OMWU在存在全支撑Nash均衡时的线性收敛性，无需均衡唯一性假设。


<details>
  <summary>Details</summary>
Motivation: 传统基于Bradley-Terry模型的偏好建模假设传递性，忽略了人类群体偏好的复杂性。NLHF将非传递偏好建模为两人零和博弈，但现有算法依赖正则化，在计算原始博弈对偶间隙时会产生不可避免的偏差。

Method: 采用乐观乘性权重更新(OMWU)算法解决NLHF问题，将非传递偏好对齐建模为寻找Nash均衡。理论分析证明了OMWU在存在全支撑Nash均衡时的收敛性，并识别了边际收敛行为。

Result: 首次证明了OMWU在NLHF中的收敛性：在预热阶段后实现最终迭代线性收敛，具有实例依赖的线性收敛速率。相比之前工作，无需Nash均衡唯一性假设，且对实例依赖常数有指数级更好的依赖关系。

Conclusion: OMWU为NLHF提供了一种有效的算法，在理论和实验上都表现出优势，有望应用于大语言模型对齐中处理非传递偏好问题。

Abstract: Aligning large language models (LLMs) with human preferences has proven effective for enhancing model capabilities, yet standard preference modeling using the Bradley-Terry model assumes transitivity, overlooking the inherent complexity of human population preferences. Nash learning from human feedback (NLHF) addresses this by framing non-transitive preferences as a two-player zero-sum game, where alignment reduces to finding the Nash equilibrium (NE). However, existing algorithms typically rely on regularization, incurring unavoidable bias when computing the duality gap in the original game. In this work, we provide the first convergence guarantee for Optimistic Multiplicative Weights Update ($\mathtt{OMWU}$) in NLHF, showing that it achieves last-iterate linear convergence after a burn-in phase whenever an NE with full support exists, with an instance-dependent linear convergence rate to the original NE, measured by duality gaps. Compared to prior results in Wei et al. (2020), we do not require the assumption of NE uniqueness. Our analysis identifies a novel marginal convergence behavior, where the probability of rarely played actions grows exponentially from exponentially small values, enabling exponentially better dependence on instance-dependent constants than prior results. Experiments corroborate the theoretical strengths of $\mathtt{OMWU}$ in both tabular and neural policy classes, demonstrating its potential for LLM applications.

</details>


### [18] [Discovering Coordinated Joint Options via Inter-Agent Relative Dynamics](https://arxiv.org/abs/2512.24827)
*Raul D. Steleac,Mohan Sridharan,David Abel*

Main category: cs.LG

TL;DR: 提出一种多智能体选项发现方法，通过联合状态抽象和费马状态近似来发现强协调行为，相比现有方法能产生更好的下游协调能力。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境中，联合状态空间随智能体数量呈指数增长，使得协调行为设计特别困难。现有方法通常牺牲协调性，产生松散耦合或完全独立的行为。

Method: 提出联合状态抽象方法压缩状态空间，同时保留发现强协调行为所需信息。首先近似最大对齐的费马状态，定义扩展度度量团队级不对齐程度，然后使用神经图拉普拉斯估计器推导捕获智能体间状态同步模式的选项。

Result: 在两个多智能体领域的多个场景中评估，结果显示相比其他选项发现方法，该方法能产生更强的下游协调能力。

Conclusion: 通过联合状态抽象和费马状态近似的方法，能够有效发现多智能体协调选项，解决现有方法协调性不足的问题。

Abstract: Temporally extended actions improve the ability to explore and plan in single-agent settings. In multi-agent settings, the exponential growth of the joint state space with the number of agents makes coordinated behaviours even more valuable. Yet, this same exponential growth renders the design of multi-agent options particularly challenging. Existing multi-agent option discovery methods often sacrifice coordination by producing loosely coupled or fully independent behaviours. Toward addressing these limitations, we describe a novel approach for multi-agent option discovery. Specifically, we propose a joint-state abstraction that compresses the state space while preserving the information necessary to discover strongly coordinated behaviours. Our approach builds on the inductive bias that synchronisation over agent states provides a natural foundation for coordination in the absence of explicit objectives. We first approximate a fictitious state of maximal alignment with the team, the \textit{Fermat} state, and use it to define a measure of \textit{spreadness}, capturing team-level misalignment on each individual state dimension. Building on this representation, we then employ a neural graph Laplacian estimator to derive options that capture state synchronisation patterns between agents. We evaluate the resulting options across multiple scenarios in two multi-agent domains, showing that they yield stronger downstream coordination capabilities compared to alternative option discovery methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization](https://arxiv.org/abs/2512.23742)
*Guangxi Fan,Tianliang Ma,Xuguang Sun,Xun Wang,Kain Lu Low,Leilai Shao*

Main category: cs.SE

TL;DR: AgenticTCAD是一个基于自然语言驱动的多智能体框架，用于自动化半导体器件设计与优化，通过专家构建的开源TCAD数据集和领域特定模型，在2纳米纳米片FET设计中比人类专家快40倍达到IRDS-2024规格。


<details>
  <summary>Details</summary>
Motivation: 随着先进技术节点的持续缩放，设计-技术协同优化变得至关重要，但TCAD仿真领域缺乏开源资源，阻碍了语言模型生成有效的TCAD代码，需要解决这一限制以实现高效的器件设计与优化。

Method: 1) 构建专家策划的开源TCAD数据集；2) 微调领域特定的TCAD代码生成模型；3) 提出AgenticTCAD框架，这是一个自然语言驱动的多智能体框架，实现端到端的自动化器件设计与优化。

Result: 在2纳米纳米片FET设计中，AgenticTCAD在4.2小时内达到国际器件与系统路线图(IRDS)-2024的器件规格，而人类专家使用商业工具需要7.1天，效率提升约40倍。

Conclusion: AgenticTCAD框架通过结合领域特定的代码生成模型和多智能体协作，显著提高了半导体器件设计与优化的效率，为先进技术节点的设计-技术协同优化提供了有效的自动化解决方案。

Abstract: With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.

</details>


### [20] [State-of-the-art Small Language Coder Model: Mify-Coder](https://arxiv.org/abs/2512.23747)
*Abhinav Parmar,Abhisek Panigrahi,Abhishek Kumar Dwivedi,Abhishek Bhattacharya,Adarsh Ramachandra,Aditya Choudhary,Aditya Garg,Aditya Raj,Alankrit Bhatt,Alpesh Yadav,Anant Vishnu,Ananthu Pillai,Ankush Kumar,Aryan Patnaik,Aswatha Narayanan S,Avanish Raj Singh,Bhavya Shree Gadda,Brijesh Pankajbhai Kachhadiya,Buggala Jahnavi,Chidurala Nithin Krishna,Chintan Shah,Chunduru Akshaya,Debarshi Banerjee,Debrup Dey,Deepa R.,Deepika B G,Faiz ur Rahman,Gagan Gayari,Gudhi Jagadeesh Kumar Naidu,Gursimar Singh,Harshal Tyagi,Harshini K,James Mani Vathalloor,Jayarama Nettar,Jayashree Gajjam,Joe Walter Sugil George,Kamalakara Sri Krishna Tadepalli,Kamalkumar Rathinasamy,Karan Chaurasia,Karthikeyan S,Kashish Arora,Kaushal Desai,Khushboo Buwade,Kiran Manjrekar,Malikireddy Venkata Sai Likhitha,Manjunath A,Mitali Mahavir Bedmutha,Mohammed Rafee Tarafdar,Nikhil Tiwari,Nikitha K Gigi,Pavan Ravikumar,Pendyala Swarnanjali,Piyush Anand,Prakash Chandrasekar,Prasanna Bhalchandra Gawade,Prasanth Sivan,Preeti Khurana,Priyanshi Babbar,Rajab Ali Mondal,Rajesh Kumar Vissapragada,Rajeshwari Ganesan,Rajeswari Koppisetti,Ramjee R.,Ramkumar Thiruppathisamy,Rani G. S.,S Reka,Samarth Gupta,Sandeep Reddy Kothakota,Sarathy K,Sathyanarayana Sampath Kumar,Saurabh Kumar,Shashank Khasare,Shenbaga Devi Venkatesh Kumar,Shiva Rama Krishna Parvatham,Shoeb Shaikh,Shrishanmathi A,Shubham Pathak,Sree Samhita Koppaka,Sreenivasa Raghavan K S,Sreeram Venkatasubramanian,Suprabha Desai Bojja,Swetha R,Syed Ahmed,Chinmai Harshitha Thota,Tushar Yadav,Veeravelly Kusumitha,V V S S Prasanth Patnaik,Vidya Sri Sesetti,Vijayakeerthi K,Vikram Raj Bakshi,Vinay K K,Vinoth Kumar Loganathan,Vipin Tiwari,Vivek Kumar Shrivastav,V Venkata Sri Datta Charan,Wasim Akhtar Khan*

Main category: cs.SE

TL;DR: Mify-Coder是一个2.5B参数的代码模型，通过计算最优策略在4.2T tokens上训练，在代码生成和函数调用基准测试中超越更大模型，证明紧凑模型可以达到前沿性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过计算最优策略和数据优化，让小型代码模型达到与大型前沿模型相当的性能，同时保持部署效率和安全性。

Method: 基于Mify-2.5B基础模型，采用计算最优训练策略，结合高质量人工数据和通过智能提示生成的合成数据，使用LLM质量过滤增强数据密度，探索CPT-SFT目标、数据混合和采样动态。

Result: Mify-Coder在标准编码和函数调用基准测试中显著超越更大的基线模型，达到可比的准确性和安全性，量化变体可在标准桌面环境部署而无需专用硬件。

Conclusion: 通过原则性的数据和计算纪律，小型模型可以实现具有竞争力的准确性、效率和安全性合规，紧凑模型能够匹配前沿级模型在代码生成和智能体驱动工作流中的性能。

Abstract: We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.

</details>


### [21] [From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering](https://arxiv.org/abs/2512.23844)
*Tao Dong,Harini Sampath,Ja Young Lee,Sherry Y. Shi,Andrew Macvean*

Main category: cs.SE

TL;DR: 该论文提出了一个评估AI代码助手的新框架，关注协作行为而非仅代码正确性。通过分析用户定义的代理规则，建立了企业软件工程中期望代理行为的分类法，并引入上下文自适应行为框架来理解行为期望如何随时间和工作类型变化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估方法滞后，主要关注代码正确性，未能捕捉成功人机协作所需的细微交互行为。需要建立更全面的评估框架来反映AI作为软件工程师协作伙伴的实际需求。

Method: 1. 分析91组用户定义的代理规则，建立企业软件工程中期望代理行为的分类法；2. 通过15位专家工程师访谈建立时间维度；3. 通过原型代理的提示分析确定工作类型维度；4. 提出上下文自适应行为框架。

Result: 提出了包含四个关键期望的分类法：遵守标准和流程、确保代码质量和可靠性、有效解决问题、与用户协作。建立了上下文自适应行为框架，揭示行为期望如何沿两个经验推导的轴变化：时间范围（从即时需求到未来理想）和工作类型（从企业生产到快速原型）。

Conclusion: 该研究为设计和评估下一代AI代理提供了以人为中心的基础，将领域焦点从生成代码的正确性转向真正协作智能的动态特性。

Abstract: As Large Language Models (LLMs) evolve from code generators into collaborative partners for software engineers, our methods for evaluation are lagging. Current benchmarks, focused on code correctness, fail to capture the nuanced, interactive behaviors essential for successful human-AI partnership. To bridge this evaluation gap, this paper makes two core contributions. First, we present a foundational taxonomy of desirable agent behaviors for enterprise software engineering, derived from an analysis of 91 sets of user-defined agent rules. This taxonomy defines four key expectations of agent behavior: Adhere to Standards and Processes, Ensure Code Quality and Reliability, Solving Problems Effectively, and Collaborating with the User.
  Second, recognizing that these expectations are not static, we introduce the Context-Adaptive Behavior (CAB) Framework. This emerging framework reveals how behavioral expectations shift along two empirically-derived axes: the Time Horizon (from immediate needs to future ideals), established through interviews with 15 expert engineers, and the Type of Work (from enterprise production to rapid prototyping, for example), identified through a prompt analysis of a prototyping agent. Together, these contributions offer a human-centered foundation for designing and evaluating the next generation of AI agents, moving the field's focus from the correctness of generated code toward the dynamics of true collaborative intelligence.

</details>


### [22] [From Illusion to Insight: Change-Aware File-Level Software Defect Prediction Using Agentic AI](https://arxiv.org/abs/2512.23875)
*Mohsen Hesamolhokama,Behnam Rohani,Amirahmad Shafiee,MohammadAmin Fazli,Jafar Habibi*

Main category: cs.SE

TL;DR: 该论文指出传统软件缺陷预测存在准确性幻觉，提出基于代码变化的预测任务和LLM驱动的多智能体辩论框架，显著提升对缺陷引入的敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统文件级软件缺陷预测存在准确性幻觉问题，因为大多数文件在版本间持续存在且保持缺陷标签，标准评估奖励的是标签持续性偏差而非对代码变化的推理。

Method: 将SDP重新定义为变化感知的预测任务，模型基于连续项目版本中文件的代码变化进行推理，而非依赖静态文件快照。提出LLM驱动的、变化感知的多智能体辩论框架。

Result: 在多个PROMISE项目上的实验表明：传统模型获得虚高的F1分数，但在关键缺陷转换案例上失败；而提出的变化感知推理和多智能体辩论框架在演化子集上表现更均衡，显著提升对缺陷引入的敏感性。

Conclusion: 当前SDP评估实践存在根本缺陷，强调在实际缺陷预测中需要变化感知的推理方法。

Abstract: Much of the reported progress in file-level software defect prediction (SDP) is, in reality, nothing but an illusion of accuracy. Over the last decades, machine learning and deep learning models have reported increasing performance across software versions. However, since most files persist across releases and retain their defect labels, standard evaluation rewards label-persistence bias rather than reasoning about code changes. To address this issue, we reformulate SDP as a change-aware prediction task, in which models reason over code changes of a file within successive project versions, rather than relying on static file snapshots. Building on this formulation, we propose an LLM-driven, change-aware, multi-agent debate framework. Our experiments on multiple PROMISE projects show that traditional models achieve inflated F1, while failing on rare but critical defect-transition cases. In contrast, our change-aware reasoning and multi-agent debate framework yields more balanced performance across evolution subsets and significantly improves sensitivity to defect introductions. These results highlight fundamental flaws in current SDP evaluation practices and emphasize the need for change-aware reasoning in practical defect prediction. The source code is publicly available.

</details>


### [23] [Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education](https://arxiv.org/abs/2512.23982)
*Hung-Fu Chang,MohammadShokrolah Shirazi,Lizhou Cao,Supannika Koolmanojwong Mobasser*

Main category: cs.SE

TL;DR: 该研究通过分析57个YouTube视频，探讨了LLM编程工具在工业实践中的使用情况、相关风险及对开发工作流的影响，特别关注对计算机教育的启示。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注个体层面或教育环境中的AI编程，缺乏对工业从业者视角的深入探索。本研究旨在填补这一空白，了解LLM编程工具在专业实践中的实际应用、相关风险以及开发工作流的转变。

Method: 对2024年末至2025年间发布的57个精选YouTube视频进行定性分析，这些视频记录了从业者的反思和经验。经过筛选和质量评估后，分析比较LLM编程与传统编程、识别新兴风险并描述演变中的工作流。

Result: 研究发现：定义了AI编程实践、显著的生产力提升和降低入门门槛。从业者报告开发瓶颈转向代码审查，并关注代码质量、可维护性、安全漏洞、伦理问题、基础问题解决能力退化以及初级工程师准备不足等问题。

Conclusion: 基于这些发现，讨论了计算机科学和软件工程教育的启示，主张课程改革应转向问题解决、架构思维、代码审查以及早期整合LLM工具的项目式学习。该研究提供了基于工业视角的AI编程观点，并为教育实践与快速演变的专业现实对齐提供指导。

Abstract: Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.

</details>


### [24] [CoHalLo: code hallucination localization via probing hidden layer vector](https://arxiv.org/abs/2512.24183)
*Nan Jia,Wangchao Sang,Pengfei Lin,Xiangping Chen,Yuan Huang,Yi Liu,Mingliang Li*

Main category: cs.SE

TL;DR: CoHalLo：一种通过探测幻觉检测模型的隐藏层向量来实现行级代码幻觉定位的新方法，通过比较预测的抽象语法树和原始抽象语法树来识别幻觉代码行。


<details>
  <summary>Details</summary>
Motivation: 现有代码幻觉检测方法大多停留在粗粒度检测层面，缺乏专门的细粒度幻觉定位技术，难以帮助开发者高效定位AI生成代码中的具体幻觉行。

Method: 1）在人工标注数据集上微调幻觉检测模型，使其学习代码语法特征；2）设计探测网络将高维潜在向量投影到低维语法子空间，生成向量元组并重建预测抽象语法树；3）通过比较预测AST和原始AST识别关键语法结构，定位幻觉代码行。

Result: CoHalLo在手动收集的代码幻觉数据集上表现优异：Top-1准确率0.4253，Top-3准确率0.6149，Top-5准确率0.7356，Top-10准确率0.8333，IFA为5.73，Recall@1% Effort为0.052721，Effort@20% Recall为0.155269，均优于基线方法。

Conclusion: CoHalLo通过探测幻觉检测模型的隐藏层向量，实现了有效的行级代码幻觉定位，为开发者提供了更精细的代码可靠性改进工具。

Abstract: The localization of code hallucinations aims to identify specific lines of code containing hallucinations, helping developers to improve the reliability of AI-generated code more efficiently. Although recent studies have adopted several methods to detect code hallucination, most of these approaches remain limited to coarse-grained detection and lack specialized techniques for fine-grained hallucination localization. This study introduces a novel method, called CoHalLo, which achieves line-level code hallucination localization by probing the hidden-layer vectors from hallucination detection models. CoHalLo uncovers the key syntactic information driving the model's hallucination judgments and locates the hallucinating code lines accordingly. Specifically, we first fine-tune the hallucination detection model on manually annotated datasets to ensure that it learns features pertinent to code syntactic information. Subsequently, we designed a probe network that projects high-dimensional latent vectors onto a low-dimensional syntactic subspace, generating vector tuples and reconstructing the predicted abstract syntax tree (P-AST). By comparing P-AST with the original abstract syntax tree (O-AST) extracted from the input AI-generated code, we identify the key syntactic structures associated with hallucinations. This information is then used to pinpoint hallucinated code lines. To evaluate CoHalLo's performance, we manually collected a dataset of code hallucinations. The experimental results show that CoHalLo achieves a Top-1 accuracy of 0.4253, Top-3 accuracy of 0.6149, Top-5 accuracy of 0.7356, Top-10 accuracy of 0.8333, IFA of 5.73, Recall@1% Effort of 0.052721, and Effort@20% Recall of 0.155269, which outperforms the baseline methods.

</details>


### [25] [Localized Calibrated Uncertainty in Code Language Models](https://arxiv.org/abs/2512.24560)
*David Gros,Prem Devanbu*

Main category: cs.SE

TL;DR: 该论文提出了一种定位LLM生成代码中与用户意图不匹配部分的技术，通过创建"最小意图对齐补丁"数据集，比较了白盒探测、黑盒反思和自一致性等方法在预测代码编辑位置方面的表现。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码可能偏离用户意图，需要监督和编辑。为了支持这一过程，需要技术来定位生成代码中可能不匹配的部分，帮助用户更高效地审查和修改代码。

Method: 1) 创建"最小意图对齐补丁"数据集，包含修复后的LLM生成程序；2) 使用测试用例验证正确性；3) 比较多种技术：白盒探测（提出高效任意跨度查询技术）、黑盒反思和自一致性方法；4) 评估这些技术为代码部分分配校准概率的能力，预测哪些代码行会被编辑。

Result: 使用小型监督模型的探针能够实现较低的校准误差，Brier技能评分约为0.2，在预测由大几个数量级的模型生成的代码编辑行方面表现良好。探针仅在代码上训练，但在允许新的概率缩放时，显示出向自然语言错误泛化的迹象。

Conclusion: 提出的技术能够有效定位LLM生成代码中与用户意图不匹配的部分，小型监督模型探针在预测编辑位置方面表现良好。这些技术与AI监督和控制相关，显示出一定的泛化能力。

Abstract: Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of "Minimal Intent Aligning Patches" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.

</details>


### [26] [On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study](https://arxiv.org/abs/2512.24570)
*Shiqi Kuang,Zhao Tian,Tao Xiao,Dong Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 首次大规模实证研究，评估五种常用训练数据优化技术及其组合对LLM代码生成的影响，发现数据合成对功能正确性最有效，而数据合成+数据重构组合整体表现最佳。


<details>
  <summary>Details</summary>
Motivation: 虽然已有许多训练数据优化技术被提出用于提高LLM代码生成质量，但这些技术的整体有效性尚未得到系统评估，需要填补这一研究空白。

Method: 在三个基准测试和四个LLM上，系统评估五种广泛使用的训练数据优化技术及其两两组合，包括数据合成、数据重构、数据清洗、数据选择等。

Result: 数据合成在提高功能正确性和减少代码异味方面最有效；大多数组合不能进一步提高功能正确性，但能有效提升代码质量；数据合成+数据重构组合整体表现最强。

Conclusion: 这是对训练数据优化和组合策略进行系统理解的第一步，为未来LLM代码生成的研究和部署提供了实用指导。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.

</details>


### [27] [How Do Agentic AI Systems Address Performance Optimizations? A BERTopic-Based Analysis of Pull Requests](https://arxiv.org/abs/2512.24630)
*Md Nahidul Islam Opu,Shahidul Islam,Muhammad Asaduzzaman,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 该论文通过实证研究分析了AI代理生成的性能相关PR，识别了52个性能主题和10个高层类别，发现AI代理在软件栈各层应用性能优化，且优化类型显著影响PR接受率和审查时间。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM软件工程已影响现代软件开发，且先前研究关注AI生成软件的正确性和性能，但AI代理系统在实践中如何具体解决性能问题尚不清楚。本研究旨在实证分析AI代理生成的性能相关PR，了解其性能优化行为。

Method: 使用LLM辅助检测和BERTopic主题建模方法，对AI代理生成的性能相关PR进行分析，识别性能优化主题并进行分类。

Result: 识别出52个性能相关主题，分为10个高层类别；AI代理在软件栈各层应用性能优化；优化类型显著影响PR接受率和审查时间；性能优化主要在开发阶段进行，维护阶段关注较少。

Conclusion: 研究为评估和改进AI代理系统的性能优化行为及审查结果提供了实证证据，揭示了AI代理在性能优化方面的实际表现和局限性。

Abstract: LLM-based software engineering is influencing modern software development. In addition to correctness, prior studies have also examined the performance of software artifacts generated by AI agents. However, it is unclear how exactly the agentic AI systems address performance concerns in practice. In this paper, we present an empirical study of performance-related pull requests generated by AI agents. Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics grouped into 10 higher-level categories. Our results show that AI agents apply performance optimizations across diverse layers of the software stack and that the type of optimization significantly affects pull request acceptance rates and review times. We also found that performance optimization by AI agents primarily occurs during the development phase, with less focus on the maintenance phase. Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.

</details>


### [28] [Feature Slice Matching for Precise Bug Detection](https://arxiv.org/abs/2512.24858)
*Ke Ma,Jianjun Huang,Wei You,Bin Liang,Jingzheng Wu,Yanjun Wu,Yuanjun Gong*

Main category: cs.SE

TL;DR: MATUS通过特征切片和相似度测量来检测软件bug，能有效抑制目标代码中的噪声干扰，在Linux内核中发现31个未知bug


<details>
  <summary>Details</summary>
Motivation: 现有基于函数相似度的bug检测方法受无关语句的噪声干扰影响性能，现有工作未能有效消除目标代码中的噪声

Method: 从buggy查询和目标代码中提取特征切片表示语义特征，利用buggy代码的先验知识指导目标切片，端到端地确定切片标准，通过向量相似度比较特征切片

Result: MATUS在真实项目中具有bug检测优势且效率可接受，在Linux内核中发现31个未知bug，全部得到内核开发者确认，其中11个被分配了CVE编号

Conclusion: MATUS通过抑制目标噪声实现了精确的bug检测，证明了基于相似度测量的bug检测方法的有效性

Abstract: Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [29] [Claude Code Sees Like A Software Architect](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavegriffith.substack.com%2Fp%2Fclaude-code-sees-like-a-software%3Futm_source=tldrnewsletter/1/0100019b4af34c38-387c2a2b-1bd9-404a-ab9b-37772c7f8978-000000/xfPv3ofFw_dvwPF0ukj_oXNqJ3yWXhrN_dKCLCLE3sY=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code新增原生语言服务器协议支持，使IDE能真正理解代码


<details>
  <summary>Details</summary>
Motivation: 传统IDE对代码的理解有限，需要增强代码智能分析能力以提升开发效率

Method: 实现原生语言服务器协议支持，让IDE具备深度代码理解和分析能力

Result: Claude Code现在能像软件架构师一样理解代码结构、依赖关系和设计模式

Conclusion: 原生LSP支持显著提升了IDE的代码智能分析能力，使开发工具更接近专业软件架构师的视角

Abstract: Claude Code Sees Like A Software Architect (10 minute read) Claude Code shipped native Language Server Protocol support last week, enabling the IDE to actually understand code.

</details>


### [30] [Scaling LLMs to larger codebases](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.kierangill.xyz%2Foversight-and-guidance%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/Nzko7PXp3_d1hdhLqCsE7M0TfAme9p2uVAKD2f9Id84=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了在大规模代码库中扩展LLMs需要投资于指导（提供高质量上下文）和监督（人工验证）两方面，以实现高效的一次性代码生成。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件开发中的应用日益广泛，如何将其有效扩展到大型代码库中成为一个关键挑战。当前LLMs在处理大规模代码时面临上下文质量不足和缺乏监督的问题，需要系统化的方法来确保代码生成的效率和架构完整性。

Method: 提出了双管齐下的方法：1）指导方面：通过提示库和结构化、模块化的代码库为LLMs提供高质量上下文，支持高效的一次性代码生成；2）监督方面：由人工工程师验证LLM的决策，确保架构完整性和解决方案的一致性。

Result: 该方法能够帮助LLMs在大规模代码库中更有效地工作，减少重复工作，提高代码生成质量，同时通过人工监督确保技术决策的合理性和架构的稳定性。

Conclusion: 成功将LLMs扩展到大型代码库需要平衡自动化和人工监督，通过系统化的指导和验证机制，可以在保持开发效率的同时确保代码质量和架构一致性。

Abstract: Scaling LLMs to larger codebases (12 minute read) Scaling LLMs within large codebases requires investments in both guidance and oversight. Guidance focuses on providing LLMs with high-quality context, such as prompt libraries and well-structured, modular codebases, to allow for efficient "one-shot" code generation without needing a lot of rework. Oversight refers to human engineers who validate LLM choices, making sure of architectural integrity and aligned solutions.

</details>


### [31] [Everyone is a Staff Engineer Now](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fread.engineerscodex.com%2Fp%2Feveryone-is-a-staff-engineer-now%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/z9DcZ01NiSu5op7U3sse5qXEJDNyUgU7c7-lcY39jlY=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编码代理已极大降低代码实现成本，软件工程师需转向更高层次的架构判断、系统级思维和多领域复杂上下文管理能力


<details>
  <summary>Details</summary>
Motivation: AI编码代理能力大幅提升，使得代码实现变得廉价，传统编程技能价值下降，工程师需要适应新的工作范式

Method: 通过分析AI编码代理对软件工程的影响，提出工程师需要转变工作流程，包括规划和引导AI代理，培养新习惯来维持代码质量

Result: 软件工程的根本性转变，工程师角色从代码实现者转向架构师和系统思考者，需要新的技能组合和工作方式

Conclusion: AI编码代理的进步要求所有工程师具备高级架构能力，工程师需要适应新的工作流程和技能发展路径

Abstract: Everyone is a Staff Engineer Now (8 minute read) AI coding agents have become so good that they have fundamentally transformed software engineering, making code implementation inexpensive. This means engineers are increasingly expected to focus on higher-level skills like architectural judgment, system-level thinking, and managing complex contexts across multiple domains. Devs will need to start adapting their workflows, including planning and steering AI agents and developing habits to maint...

</details>


### [32] [A Year Of Vibes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F12%2F22%2Fa-year-of-vibes%2F%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/iwzzDcq49FZrdrXyGtTVZOv9TrQpNc-5cZekeMudzHA=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 开发者在2025年离开Sentry后创立新公司，全面采用AI代理编程（如Claude Code），从代码生成到日常组织都深度集成AI代理，但对LLMs表现出类人倾向感到担忧


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在实际开发工作流中的深度集成应用，测试AI辅助编程的极限，同时关注AI系统可能出现的类人行为倾向

Method: 采用实践导向的方法，通过真实项目和工作流程全面集成AI代理工具（特别是Claude Code），从代码生成到日常任务管理都依赖AI辅助

Result: 成功实现AI代理深度集成的工作流程，提高了开发效率，但观察到LLMs表现出令人不安的类人倾向，对"代理"这一术语产生质疑

Conclusion: AI代理编程工具能显著提升开发效率，但需要警惕AI系统可能出现的类人行为倾向，对AI与人类关系的边界需要更深入的思考

Abstract: A Year Of Vibes (12 minute read) In 2025, this dev left Sentry, launched a new company, and shifted his programming approach to embrace hands-off agentic coding with tools like Claude Code. He became deeply integrated with AI agents for tasks from code generation to daily organization. However, he's sometimes worried about emergent human-like tendencies of LLMs, questioning terms like "agent.”

</details>


### [33] [How AI Will Change Software Engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=CQmI4XKTa0U%26t=12s%26utm_source=tldrdata/1/0100019b793de7bf-3b24424d-a063-413a-8dd5-b7c16df11556-000000/rHyDbE7aSqHtuNmYatoxiLWIGnc9Cwy5g3WiZEyfhmk=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI将彻底改变软件工程，使软件开发从确定性转向概率性，需要新的工程习惯。AI擅长快速原型设计、理解遗留代码，但不能盲目依赖，需要严格审查和测试。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型如何从根本上改变软件工程实践，就像汇编语言到高级语言的转变一样重要，但影响更大，因为软件变得非确定性。

Method: 通过视频讲座形式，分析AI在软件工程中的实际应用场景，提出将AI输出视为需要严格审查的代码提交，强调测试驱动开发的重要性。

Result: AI在快速原型设计、理解不熟悉的代码栈和遗留代码方面表现出色，但盲目依赖会导致学习循环中断，需要新的工程实践来管理概率性输出。

Conclusion: 软件工程需要适应AI带来的根本性变化，建立新的工程习惯来管理非确定性软件，将AI视为需要严格监督的"多产但不可靠"的团队成员。

Abstract: How AI Will Change Software Engineering (110 minute video) LLMs are a once-in-a-career shift like assembly to high-level languages, but bigger in one way: software becomes non-deterministic (probabilistic outputs), forcing new engineering habits. AI is great for fast prototyping, navigating unfamiliar stacks, and understanding legacy code, but unsafe for blind “vibe coding,” which breaks the learning loop. Treat AI output like a PR from a dodgy but productive teammate: review hard, test relen...

</details>


### [34] [How I code with agents, without being 'technical'](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpzNUXP/1/0100019b794d35da-0cf0af27-0e60-490e-82cd-198246df9836-000000/uldpwKR2mjAthpkwjidByvS8qQ9VCcwHqF7l6ofzum8=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 非技术人员通过AI代理进行软件开发，无需编写代码但通过监控代理输出学习编程知识并成功交付项目


<details>
  <summary>Details</summary>
Motivation: 探索非技术人员如何利用AI代理进行软件开发，降低编程门槛，让不具备传统编程技能的人也能参与软件项目

Method: 使用前沿的软件开发代理（Factory公司产品），通过大量token交互让AI代理生成代码，用户不编写代码但密切监控代理输出，从中学习编程知识

Result: 在4个月内消耗30亿token，成功交付多个项目，用户通过监控代理输出获得了大量关于代码工作原理、项目管理、失败与成功因素的知识

Conclusion: AI代理使非技术人员能够参与软件开发，通过监控代理输出可以学习编程知识，这种模式可能改变软件开发的工作方式

Abstract: How I code with agents, without being 'technical' (18 minute read) Ben Tossell, who works at Factory, a company developing a frontier software development agent, has spent 3 billion tokens in four months through an agent without writing any code. None of the code was read, but Tossell read the agent output religiously, which led to him picking up a ton of knowledge around how code works, how projects work, where things fail, and where they succeed. Tossell has shipped several projects using h...

</details>


### [35] [Meta just acquired a Chinese-founded AI startup for $2B. Here's why that matters](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cbc.ca%2Fnews%2Fbusiness%2Fmeta-manus-acquisition-two-billion-explained-9.7030180%3Futm_source=tldrmarketing/1/0100019b79729bdf-1e85403a-a01b-4ed6-926f-15755a377d9f-000000/VZ_AM3bOFeQYRGNNZadESNtejoYmjwEm70yNaDJswck=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta以20亿美元收购中国背景的AI初创公司Manus，计划将其智能代理AI集成到WhatsApp、Instagram和Facebook中，打造能够处理聊天、支付等任务的AI伴侣，以提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: Meta希望通过收购Manus获得先进的智能代理AI技术，将其集成到主要社交平台中，创建能够自主决策和完成任务的AI伴侣，从而增强用户体验、提高用户参与度，并在AI助手领域保持竞争力。

Method: 通过收购Manus这家专注于智能代理AI的初创公司，获得其需要最少提示就能做出决策和完成任务的技术，然后将该技术集成到Meta的三大社交平台（WhatsApp、Instagram、Facebook）中。

Result: Meta以20亿美元完成对Manus的收购，获得了先进的智能代理AI技术，计划将其整合到核心社交产品中，创建能够处理聊天、支付等多种任务的AI伴侣。

Conclusion: 这次收购标志着Meta在智能代理AI领域的重要布局，通过将先进的AI技术集成到社交平台，有望显著提升用户体验和平台粘性，同时为Meta在AI助手竞争中占据有利位置。

Abstract: Meta just acquired a Chinese-founded AI startup for $2B. Here's why that matters (4 minute read) Manus makes agentic AI that can make decisions and complete tasks with minimal prompting and generates revenue through subscriptions. Meta plans to integrate Manus into WhatsApp, Instagram, and Facebook to create an AI companion that handles chat, payments, and other tasks, keeping users engaged longer.

</details>


### [36] [Build Software. Build Users](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdima.day%2Fblog%2Fbuild-software-build-users%2F%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/F7453JOKu3IHazfi1J6xXN7hwpV4swUY_c-tLXc8G28=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 提出通过创建LLM驱动的用户代理来模拟目标用户及其交互，在编码前先"感知用户"，通过迭代过程构建具有定义配置文件和"快乐路径"的代理用户，然后开发软件，最后让模拟用户提供反馈以改进软件质量


<details>
  <summary>Details</summary>
Motivation: 工程师常常因为未能深入理解用户需求而错过真正的软件质量。当前软件开发过程中缺乏对用户需求的深刻洞察，导致最终产品与用户实际需求不匹配

Method: 提出"vibe code users"方法：在编码软件之前，先创建详细的LLM驱动的用户代理来模拟目标用户及其交互。这是一个迭代过程：1) 构建具有定义配置文件和"快乐路径"的代理用户；2) 基于这些模拟用户开发软件；3) 让模拟用户提供反馈以改进软件

Result: 该方法通过模拟用户代理在软件开发早期阶段提供用户视角，帮助工程师更好地理解用户需求，从而提高软件质量和用户体验

Conclusion: 通过LLM驱动的用户代理模拟目标用户，可以在软件开发过程中更早、更深入地理解用户需求，从而提升最终软件产品的质量和用户满意度

Abstract: Build Software. Build Users (5 minute read) True software quality is often missed because engineers fail to deeply understand user needs. A possible solution is to "vibe code users" first, before coding the software itself, by creating detailed, LLM-driven user agents that simulate target users and their interactions. This iterative process involves building these agentic users with defined profiles and "happy paths," then developing software, and finally allowing the simulated users to provi...

</details>


### [37] [AI Is Forcing Us To Write Good Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbits.logic.inc%2Fp%2Fai-is-forcing-us-to-write-good-code%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/U5DZ3gpCrEKU6wmQ96R934IzkUNrG6rHTVkMWe0PpbE=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理需要更高的代码质量标准，之前"可选"的最佳实践现在成为有效操作的必要条件


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在软件开发中的应用增加，需要更高的代码质量来确保代理的有效运行

Method: 强调良好的文件组织和端到端类型系统的广泛使用，以指导AI代理、减少错误并改善上下文加载

Result: AI代理迫使开发者采用更严格的代码质量标准，将之前可选的最佳实践变为必要要求

Conclusion: AI正在推动软件开发向更高质量标准转变，良好的代码组织和类型系统对AI代理操作至关重要

Abstract: AI Is Forcing Us To Write Good Code (10 minute read) AI agents require a higher standard of code quality. Previously "optional" best practices are now essential requirements for effective operation. Thoughtful file organization and extensive use of end-to-end type systems are also necessary for guiding agents, reducing errors, and improving context loading.

</details>


### [38] [2025: The year in LLMs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F31%2Fthe-year-in-llms%2F%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/40mcEnEYUbQX1c_cbyjrMZFKk45fH3uMwibT2H2D7yo=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 2025年LLMs在推理能力上取得突破，推动了AI代理的广泛应用，特别是编码代理能够自主编写、执行和调试代码，同时中国开源模型在能力排名中占据主导地位


<details>
  <summary>Details</summary>
Motivation: 总结2025年LLMs领域的关键发展，突出推理能力的突破、AI代理的普及、编码代理的出现以及竞争格局的变化

Method: 这是一篇综述性文章，通过回顾和分析2025年LLMs领域的主要趋势和发展，总结年度进展

Result: LLMs学会了推理，能够处理复杂多步骤任务；编码代理实现自主编程；中国开源模型在能力排名中占据主导；Google的Gemini模型取得重大进展

Conclusion: 2025年是LLMs发展的重要转折点，推理能力的突破推动了AI代理的广泛应用，改变了行业竞争格局

Abstract: 2025: The year in LLMs (41 minute read) 2025 was when LLMs learned to reason, allowing models to tackle complex, multi-step tasks, which in turn drove the widespread adoption of highly capable AI agents. "Coding agents” came to life, autonomously writing, executing, and debugging code across command-line interfaces and even mobile phones. The competitive landscape shifted a lot as Chinese open-weight models dominated capability rankings, and Google's Gemini made large strides with new models ...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution](https://arxiv.org/abs/2512.23880)
*Xu Huang,Junwu Chen,Yuxing Fei,Zhuohan Li,Philippe Schwaller,Gerbrand Ceder*

Main category: cs.AI

TL;DR: CASCADE是一个自演化的智能体框架，通过持续学习和自我反思等元技能，使LLM智能体能够掌握复杂的外部工具并编码知识，在科学任务中实现从"LLM+工具使用"到"LLM+技能获取"的转变。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体依赖于预定义工具或脆弱的工具生成，限制了其在复杂科学任务中的能力和适应性。需要一种能够自主学习和适应新工具的框架。

Method: CASCADE框架通过两种元技能实现：1) 持续学习（通过网页搜索和代码提取）；2) 自我反思（通过内省和知识图谱探索）。框架支持人类-智能体协作和记忆巩固，积累可执行的技能。

Result: 在SciSkillBench基准测试（116个材料科学和化学研究任务）上，使用GPT-5的CASCADE达到93.3%的成功率，而没有演化机制的基线只有35.4%。框架在计算分析、自主实验室实验和论文选择性复现等实际应用中表现出色。

Conclusion: CASCADE通过积累可跨智能体和科学家共享的可执行技能，推动了可扩展的AI辅助科学研究，代表了从"LLM+工具使用"向"LLM+技能获取"的重要转变。

Abstract: Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from "LLM + tool use" to "LLM + skill acquisition". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.

</details>


### [40] [ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment](https://arxiv.org/abs/2512.24040)
*Natchaya Temyingyong,Daman Jain,Neeraj Kumarsahu,Prabhat Kumar,Rachata Phondi,Wachiravit Modecrua,Krittanon Kaewtawee,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: ROAD框架通过多智能体架构将失败日志转化为结构化决策树协议，无需标注数据集即可优化LLM提示，在冷启动场景下显著提升代理性能


<details>
  <summary>Details</summary>
Motivation: 现实软件工程中，LLM代理开发初期通常缺乏标注数据集，只有混乱的生产日志和不断演化的失败模式，传统基于标注数据的自动提示优化方法不适用

Method: ROAD采用多智能体架构：分析器进行根因分析，优化器进行模式聚合，教练进行策略整合，将非结构化失败日志转化为结构化决策树协议

Result: 在学术基准和生产知识管理引擎上，ROAD仅用3次自动迭代就使成功率提升5.6%（73.6%到79.2%），搜索准确率提升3.8%；在零售领域复杂推理任务中，代理性能相对基线提升约19%

Conclusion: 模拟人类工程循环（失败分析和修复）为部署可靠LLM代理提供了一种可行且数据高效的方法，可替代资源密集的强化学习训练

Abstract: Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.

</details>


### [41] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自进化代理框架，通过将LLM集成到"计划-执行-总结"认知范式中，将进化搜索映射为推理密集型过程，显著提高进化效率并发现更优解。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法在从静态LLM向自改进代理过渡时存在结构化推理不足的问题，现有方法在高维代码空间中容易过早收敛和探索效率低下。

Method: 提出LoongFlow框架，集成LLM到"计划-执行-总结"认知范式，采用混合进化记忆系统结合多岛模型、MAP-Elites和自适应玻尔兹曼选择，平衡探索与利用。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中，LoongFlow比领先基线（如OpenEvolve、ShinkaEvolve）进化效率提升高达60%，并能发现更优解决方案。

Conclusion: LoongFlow在自主科学发现方面迈出重要一步，能够以更低的计算开销生成专家级解决方案。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


### [42] [CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation](https://arxiv.org/abs/2512.24113)
*Jiaxin Hu,Tao Wang,Bingsan Yang,Hongrun Wang*

Main category: cs.AI

TL;DR: CogRec是一个结合大语言模型与Soar认知架构的推荐代理，通过感知-认知-行动循环实现可解释推荐和在线学习


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推荐系统中存在黑盒特性、知识幻觉和在线学习能力有限的问题，而认知架构如Soar虽然推理过程结构化可解释，但知识获取困难。需要结合两者优势解决互补挑战

Method: 提出CogRec认知推荐代理，以Soar作为核心符号推理引擎，利用LLM进行知识初始化填充工作记忆中的产生式规则。采用感知-认知-行动循环，遇到僵局时动态查询LLM获取推理解决方案，通过Soar的分块机制将解决方案转化为新的符号产生式规则

Result: 在三个公开数据集上的广泛评估表明，CogRec在推荐准确性、可解释性以及解决长尾问题方面具有显著优势

Conclusion: CogRec成功结合了LLM和Soar认知架构的优势，实现了可解释的推荐和强大的在线学习能力，解决了传统方法的局限性

Abstract: Large Language Models (LLMs) have demonstrated a remarkable capacity in understanding user preferences for recommendation systems. However, they are constrained by several critical challenges, including their inherent "Black-Box" characteristics, susceptibility to knowledge hallucination, and limited online learning capacity. These factors compromise their trustworthiness and adaptability. Conversely, cognitive architectures such as Soar offer structured and interpretable reasoning processes, yet their knowledge acquisition is notoriously laborious. To address these complementary challenges, we propose a novel cognitive recommender agent called CogRec which synergizes the strengths of LLMs with the Soar cognitive architecture. CogRec leverages Soar as its core symbolic reasoning engine and leverages an LLM for knowledge initialization to populate its working memory with production rules. The agent operates on a Perception-Cognition-Action(PCA) cycle. Upon encountering an impasse, it dynamically queries the LLM to obtain a reasoned solution. This solution is subsequently transformed into a new symbolic production rule via Soar's chunking mechanism, thereby enabling robust online learning. This learning paradigm allows the agent to continuously evolve its knowledge base and furnish highly interpretable rationales for its recommendations. Extensive evaluations conducted on three public datasets demonstrate that CogRec demonstrates significant advantages in recommendation accuracy, explainability, and its efficacy in addressing the long-tail problem.

</details>


### [43] [Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks](https://arxiv.org/abs/2512.24156)
*Evgenii Rudakov,Jonathan Shock,Benjamin Ultan Cowley*

Main category: cs.AI

TL;DR: 提出一种无需训练的图基方法解决ARC-AGI-3交互推理任务，通过视觉帧处理与系统化状态空间探索，在基准测试中显著优于前沿LLM代理


<details>
  <summary>Details</summary>
Motivation: 当前最先进的LLM无法可靠解决ARC-AGI-3中的交互推理任务，这些任务需要代理通过有限交互推断任务机制并适应递增复杂度

Method: 结合视觉帧处理与系统化状态空间探索：1) 将视觉帧分割为有意义组件；2) 基于视觉显著性优先选择动作；3) 维护已探索状态和转移的有向图；4) 跟踪访问状态和测试动作，优先选择到达未测试状态-动作对的最短路径

Result: 在ARC-AGI-3预览挑战中，中位解决52个关卡中的30个，在私有排行榜排名第3，显著优于前沿LLM代理

Conclusion: 即使无需学习，显式图结构探索也可作为交互推理的强基线，突显了在稀疏反馈环境中系统化状态跟踪和动作优先级的重要性

Abstract: We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.

</details>


### [44] [Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents](https://arxiv.org/abs/2512.24461)
*Seohui Bae,Jeonghye Kim,Youngchul Sung,Woohyung Lim*

Main category: cs.AI

TL;DR: 提出一种测试时自适应智能体，通过后验引导的信念精化进行探索性推理，无需梯度更新或额外训练，适用于部分可观测环境下的LLM智能体。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下，LLM智能体需要有效推断隐藏的世界状态，而传统方法依赖梯度更新或额外训练，计算成本高且不灵活。

Method: 智能体维护外部结构化信念表示，通过动作条件观察迭代更新，选择最大化信念空间信息增益的动作，使用轻量级LLM代理估计信息增益，并通过后验信念与真实环境配置的一致性来评估世界对齐。

Result: 实验表明该方法在潜在世界状态对齐方面优于推理时扩展基线（如提示增强或检索增强的LLMs），且集成开销显著降低。

Conclusion: 提出的测试时自适应方法通过后验引导的信念精化，为部分可观测环境下的LLM智能体提供了一种高效、无需额外训练的探索推理框架。

Abstract: In this paper, we propose a test-time adaptive agent that performs exploratory inference through posterior-guided belief refinement without relying on gradient-based updates or additional training for LLM agent operating under partial observability. Our agent maintains an external structured belief over the environment state, iteratively updates it via action-conditioned observations, and selects actions by maximizing predicted information gain over the belief space. We estimate information gain using a lightweight LLM-based surrogate and assess world alignment through a novel reward that quantifies the consistency between posterior belief and ground-truth environment configuration. Experiments show that our method outperforms inference-time scaling baselines such as prompt-augmented or retrieval-enhanced LLMs, in aligning with latent world states with significantly lower integration overhead.

</details>


### [45] [Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems](https://arxiv.org/abs/2512.24505)
*Samuel Golladay,Majid Bani-Yaghoub*

Main category: cs.AI

TL;DR: 该研究分析了GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3在密苏里大学数学竞赛问题上的表现，发现DeepSeek-V3在所有类别中表现最佳，所有模型在几何问题上表现都较弱，并揭示了不同模型的错误模式差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多使用相同数据集评估LLMs的数学推理能力，限制了结果的普适性，未能充分捕捉数学任务中的多样化挑战。本研究旨在通过分析LLMs在代表性不足的数学竞赛问题上的表现，提供更全面的评估。

Method: 使用密苏里大学数学竞赛中的微积分、解析几何和离散数学问题，测试GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3三个领先LLMs。将模型回答与已知正确答案比较，分析准确率，并深入分析推理过程以探索错误模式。

Result: DeepSeek-V3在微积分、解析几何和离散数学三个类别中表现最佳。所有三个LLMs在几何问题上表现显著较弱。DeepSeek-V3的主要错误是计算和逻辑错误，GPT-4o-mini常见逻辑和方法相关错误，Gemini则倾向于不完整推理和仓促结论。

Conclusion: 在代表性不足的数学竞赛数据集上评估LLMs能够提供对其独特错误模式的深入洞察，并突显结构化推理（特别是在几何领域）中持续存在的挑战。

Abstract: Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.

</details>


### [46] [From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning](https://arxiv.org/abs/2512.24532)
*Amir Tahmasbi,Sadegh Majidi,Kazem Taram,Aniket Bera*

Main category: cs.AI

TL;DR: 提出两阶段方法提升LLM空间推理能力：先通过监督微调学习基础空间变换（旋转、平移、缩放），再冻结模型并训练LoRA适配器学习多步规划策略，在ASCII艺术环境中进行闭环强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用语言能力上表现出色，但在结构化环境中的空间变换和多步规划方面仍存在困难，这限制了其在导航和规划等应用中的表现。

Method: 1. 监督微调阶段：在基础空间变换（旋转、平移、缩放）上进行训练，使模型掌握空间物理知识；2. 强化学习阶段：冻结物理感知模型，在GRPO框架下训练轻量级LoRA适配器，学习将基础构建块组合成多步规划策略；3. 构建ASCII艺术数据集和对应的强化学习环境。

Result: 该方法在动态环境（有显式状态更新）和静态环境（依赖内部状态）中均优于基线模型（通用骨干模型、物理感知模型和端到端RL模型），收敛更快且训练更稳定。通过注意力模式分析验证了微调确实提升了空间理解能力。

Conclusion: 将空间推理分解为原子构建块及其组合的两阶段方法有效提升了LLM的空间推理能力，为导航和规划应用提供了更可靠的解决方案。

Abstract: Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.

</details>


### [47] [Recursive Language Models](https://arxiv.org/abs/2512.24601)
*Alex L. Zhang,Tim Kraska,Omar Khattab*

Main category: cs.AI

TL;DR: 提出递归语言模型（RLMs）作为处理超长提示的推理策略，通过程序化检查、分解和递归调用处理提示片段，实现超出模型上下文窗口两个数量级的输入处理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的上下文窗口有限，无法处理任意长度的提示。需要一种方法让LLMs能够处理超出其原始上下文窗口的长提示，同时保持高质量输出。

Method: 提出递归语言模型（RLMs）推理策略：将长提示视为外部环境，让LLM程序化地检查、分解提示，并递归调用自身处理提示片段。这是一种通用的推理时扩展方法。

Result: RLMs成功处理超出模型上下文窗口两个数量级的输入，在四个不同的长上下文任务中，即使对于较短的提示，也显著优于基础LLMs和常见的长上下文框架，同时每个查询的成本相当或更低。

Conclusion: RLMs提供了一种有效的推理时扩展方法，使LLMs能够处理任意长度的提示，在保持高质量输出的同时控制成本，为长上下文处理提供了新思路。

Abstract: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.

</details>


### [48] [Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization](https://arxiv.org/abs/2512.24609)
*Dong Qiu,Duo Xu,Limengxi Yue*

Main category: cs.AI

TL;DR: 提出一个基于强化学习的多智能体LLM框架，通过Dec-POMDP建模协作，采用CTDE训练策略，在协作写作和编程任务上显著优于单智能体和多智能体基线。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在语言任务上表现良好，但在多智能体环境中缺乏协作意识，难以优化全局性能。需要解决多智能体协作中的协调问题。

Method: 将协作建模为分散部分可观察马尔可夫决策过程(Dec-POMDP)，采用集中训练分散执行(CTDE)框架。提出Group Relative Policy Optimization(GRPO)算法，在训练时利用全局信号联合优化智能体策略，并设计简化的联合奖励函数平衡任务质量、速度和协调成本。

Result: 在协作写作和编程基准测试中：任务处理速度比单智能体基线提高3倍；写作任务达到98.7%的结构/风格一致性；编程任务达到74.6%的测试通过率。一致优于强大多智能体LLM基线。

Conclusion: 该框架为复杂工作流中的可靠协作提供了实用路径，通过强化学习增强LLM智能体在多智能体环境中的协作能力。

Abstract: Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.

</details>


### [49] [Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning](https://arxiv.org/abs/2512.24613)
*Zheyu Shi,Dong Qiu,Shanlong Yu*

Main category: cs.AI

TL;DR: 提出基于群体审议的多智能体对话模型，通过三层角色架构（生成、验证、整合）和自博弈机制，结合检索增强和复合奖励函数，显著提升复杂推理任务的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 单一大型语言模型在复杂推理任务中存在局限性，需要多智能体协作来提升推理能力和事实一致性。

Method: 采用三层角色架构：观点生成代理产生多样化推理视角，证据验证代理检索外部知识并量化事实支持度，一致性仲裁代理整合逻辑一致的结论。引入自博弈机制扩展多路径推理轨迹，检索增强模块动态补充外部知识，设计结合事实一致性和逻辑连贯性的复合奖励函数，应用改进的近端策略优化进行协作训练。

Result: 在HotpotQA上多跳推理准确率提升16.8%，在2WikiMultihopQA上提升14.3%，在MeetingBank上提升19.2%，一致性提升21.5%。相比主流多智能体方法具有更高的推理效率。

Conclusion: 该模型为复杂推理任务提供了有效稳定的解决方案，通过多智能体协作显著提升了推理准确性和一致性。

Abstract: This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.

</details>


### [50] [Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization](https://arxiv.org/abs/2512.24615)
*Yuchen Shi,Yuzheng Cai,Siqi Cai,Zihan Xu,Lichao Chen,Yulei Qin,Zhijian Zhou,Xiang Fei,Chaofan Qiu,Xiaoyu Tan,Gang Li,Zongyi Li,Haojia Lin,Guocan Cai,Yong Mao,Yunsheng Wu,Ke Li,Xing Sun*

Main category: cs.AI

TL;DR: Youtu-Agent是一个模块化LLM代理框架，通过自动化生成和持续演化解决现有框架配置成本高、能力静态的问题，支持工作流和元代理两种生成模式，并包含实践和强化学习两种优化模块。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架面临两个主要挑战：1）高配置成本，构建高质量代理需要大量手动工具集成和提示工程工作；2）静态能力，已部署代理难以适应动态环境，需要昂贵的微调。

Method: 提出Youtu-Agent框架，包含结构化配置系统（解耦执行环境、工具集和上下文管理），支持两种生成范式：工作流模式用于标准任务，元代理模式用于复杂非标准需求（能自动生成工具代码、提示和配置）。建立混合策略优化系统：1）代理实践模块通过上下文优化积累经验；2）代理RL模块与分布式训练框架集成，实现端到端大规模强化学习。

Result: 在WebWalkerQA（71.47%）和GAIA（72.8%）上达到SOTA性能；自动化生成管道工具合成成功率超过81%；实践模块在AIME 2024/2025上分别提升2.7%和5.4%；Agent RL训练在7B LLMs上实现40%加速，在数学和通用/多跳QA基准上分别提升编码/推理能力35%和搜索能力21%。

Conclusion: Youtu-Agent通过自动化生成和持续演化机制有效解决了LLM代理框架的高配置成本和静态能力问题，实现了灵活可重用、可自适应优化的代理系统。

Abstract: Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \textbf{Workflow} mode for standard tasks and a \textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.

</details>


### [51] [Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem](https://arxiv.org/abs/2512.24873)
*Weixun Wang,XiaoXiao Xu,Wanhe An,Fangwen Dai,Wei Gao,Yancheng He,Ju Huang,Qiang Ji,Hanqi Jin,Xiaoyang Li,Yang Li,Zhongwen Li,Shirong Lin,Jiashun Liu,Zenan Liu,Tao Luo,Dilxat Muhtar,Yuanbin Qu,Jiaqiang Shi,Qinghui Sun,Yingshui Tan,Hao Tang,Runze Wang,Yi Wang,Zhaoguo Wang,Yanan Wu,Shaopan Xiong,Binchen Xu,Xander Xu,Yuchi Xu,Qipeng Zhang,Xixia Zhang,Haizhou Zhao,Jie Zhao,Shuaibing Zhao,Baihui Zheng,Jianhui Zheng,Suhang Zheng,Yanni Zhu,Mengze Cai,Kerui Cao,Xitong Chen,Yue Dai,Lifan Du,Tao Feng,Tao He,Jin Hu,Yijie Hu,Ziyu Jiang,Cheng Li,Xiang Li,Jing Liang,Chonghuan Liu,ZhenDong Liu,Haodong Mi,Yanhu Mo,Junjia Ni,Shixin Pei,Jingyu Shen,XiaoShuai Song,Cecilia Wang,Chaofan Wang,Kangyu Wang,Pei Wang,Tao Wang,Wei Wang,Ke Xiao,Mingyu Xu,Tiange Xu,Nan Ya,Siran Yang,Jianan Ye,Yaxing Zang,Duo Zhang,Junbo Zhang,Boren Zheng,Wanxi Deng,Ling Pan,Lin Qu,Wenbo Su,Jiamang Wang,Wei Wang,Hu Wei,Minggang Wu,Cheng Yu,Bing Zhao,Zhicheng Zheng,Bo Zheng*

Main category: cs.AI

TL;DR: ALE是一个端到端的智能体学习生态系统，包含ROLL权重优化框架、ROCK轨迹生成环境和iFlow CLI上下文工程工具。基于ALE训练的ROME模型在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前开源社区缺乏系统化的智能体开发基础设施，难以支持智能体在真实环境中进行多轮操作、观察结果和迭代优化的需求。

Method: 提出ALE生态系统，包含三个组件：1) ROLL用于权重优化的后训练框架；2) ROCK用于轨迹生成的沙箱环境管理器；3) iFlow CLI用于高效上下文工程的智能体框架。开发了ROME模型，采用数据合成协议和IPA（基于交互的策略对齐）算法，通过语义交互块而非单个token来分配信用。

Result: ROME在SWE-bench Verified和Terminal Bench等基准测试中表现出色，证明了ALE基础设施的有效性。同时提出了Terminal Bench Pro基准，具有更好的规模和污染控制。

Conclusion: ALE提供了一个系统化的智能体学习生态系统，通过优化的生产流程和创新的训练方法，能够有效提升智能体在真实环境中的表现。

Abstract: Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.

</details>
