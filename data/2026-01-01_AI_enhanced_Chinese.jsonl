{"id": "2512.24565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24565", "abs": "https://arxiv.org/abs/2512.24565", "authors": ["Wenrui Liu", "Zixiang Liu", "Elsie Dai", "Wenhan Yu", "Lei Yu", "Tong Yang"], "title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use", "comment": null, "summary": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.", "AI": {"tldr": "\u63d0\u51fa\u4e86MCPAgentBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eMCP\u534f\u8bae\u7684LLM\u667a\u80fd\u4f53\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5305\u542b\u771f\u5b9e\u4efb\u52a1\u3001\u6a21\u62df\u5de5\u5177\u548c\u52a8\u6001\u6c99\u7bb1\u73af\u5883\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u590d\u6742\u591a\u6b65\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dMCP\u8bc4\u4f30\u96c6\u5b58\u5728\u4f9d\u8d56\u5916\u90e8MCP\u670d\u52a1\u3001\u7f3a\u4e4f\u96be\u5ea6\u611f\u77e5\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u771f\u5b9eMCP\u5b9a\u4e49\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u4efb\u52a1\u548c\u6a21\u62dfMCP\u5de5\u5177\uff1b\u91c7\u7528\u52a8\u6001\u6c99\u7bb1\u73af\u5883\uff0c\u63d0\u4f9b\u5305\u542b\u5e72\u6270\u9879\u7684\u5de5\u5177\u5019\u9009\u5217\u8868\uff1b\u5f15\u5165\u7efc\u5408\u6307\u6807\u8861\u91cf\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6267\u884c\u6548\u7387\u3002", "result": "\u5728\u591a\u79cd\u6700\u65b0\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u5904\u7406\u590d\u6742\u591a\u6b65\u5de5\u5177\u8c03\u7528\u65f6\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "MCPAgentBench\u4e3a\u8bc4\u4f30\u667a\u80fd\u4f53\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.24940", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24940", "abs": "https://arxiv.org/abs/2512.24940", "authors": ["Augusto B. Corr\u00eaa", "Yoav Gelberg", "Luckeciano C. Melo", "Ilia Shumailov", "Andr\u00e9 G. Pereira", "Yarin Gal"], "title": "Iterative Deployment Improves Planning Skills in LLMs", "comment": null, "summary": "We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.", "AI": {"tldr": "\u8fed\u4ee3\u90e8\u7f72LLM\u5e76\u57fa\u4e8e\u7528\u6237\u7cbe\u9009\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u663e\u8457\u6539\u53d8\u6a21\u578b\u7279\u6027\uff0c\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u5b9e\u73b0\u80fd\u529b\u63d0\u5347\u548c\u6cdb\u5316\uff0c\u8fd9\u672c\u8d28\u4e0a\u662f\u4e00\u79cd\u9690\u5f0f\u5f3a\u5316\u5b66\u4e60\u673a\u5236", "motivation": "\u7814\u7a76\u8fed\u4ee3\u90e8\u7f72LLM\u5e76\u57fa\u4e8e\u7528\u6237\u7cbe\u9009\u6570\u636e\u5fae\u8c03\u5bf9\u6a21\u578b\u7279\u6027\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u8fd9\u79cd\u673a\u5236\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7406\u8bba\u8054\u7cfb\u53ca\u5176\u5bf9AI\u5b89\u5168\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u610f\u4e49", "method": "\u91c7\u7528\u8fed\u4ee3\u90e8\u7f72\u673a\u5236\uff1a\u6bcf\u6b21\u90e8\u7f72LLM\u540e\uff0c\u7528\u6237\u4ece\u6a21\u578b\u8f93\u51fa\u4e2d\u7cbe\u9009\u6570\u636e\uff0c\u7528\u4e8e\u5fae\u8c03\u4e0b\u4e00\u8f6e\u6a21\u578b\uff1b\u5728\u591a\u4e2a\u89c4\u5212\u9886\u57df\u6d4b\u8bd5\uff1b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u8be5\u673a\u5236\u7b49\u4ef7\u4e8e\u5916\u5c42\u5faa\u73af\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3", "result": "\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u89c2\u5bdf\u5230\u663e\u8457\u6539\u8fdb\uff0c\u540e\u7eed\u6a21\u578b\u5c55\u73b0\u51fa\u6d8c\u73b0\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u53d1\u73b0\u6bd4\u521d\u59cb\u6a21\u578b\u957f\u5f97\u591a\u7684\u89c4\u5212\u65b9\u6848\uff1b\u7406\u8bba\u5206\u6790\u8868\u660e\u8fed\u4ee3\u90e8\u7f72\u5b9e\u73b0\u4e86\u9690\u5f0f\u5956\u52b1\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60", "conclusion": "\u8fed\u4ee3\u90e8\u7f72\u673a\u5236\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u66ff\u4ee3\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f9d\u8d56\u6570\u636e\u7cbe\u9009\u800c\u975e\u663e\u5f0f\u5956\u52b1\uff1b\u5bf9AI\u5b89\u5168\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u56e0\u4e3a\u9690\u5f0f\u5956\u52b1\u51fd\u6570\u53ef\u80fd\u5bfc\u81f4\u672a\u6765\u6a21\u578b\u90e8\u7f72\u7684\u610f\u5916\u7279\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2512.24957", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24957", "abs": "https://arxiv.org/abs/2512.24957", "authors": ["Yulan Hu", "Xiangwen Zhang", "Sheng Ouyang", "Hao Yi", "Lu Xu", "Qinglin Lang", "Lide Tan", "Xiang Cheng", "Tianchen Ye", "Zhicong Li", "Ge Chen", "Wenjin Yang", "Zheng Pan", "Shaopan Xiong", "Siran Yang", "Ju Huang", "Yan Zhang", "Jiamang Wang", "Yong Liu", "Yinfeng Huang", "Tucheng Lin", "Xin Li", "Ning Guo"], "title": "AMAP Agentic Planning Technical Report", "comment": null, "summary": "We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.", "AI": {"tldr": "STAgent\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u65f6\u7a7a\u7406\u89e3\u7684\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5de5\u5177\u4ea4\u4e92\u548c\u5206\u5c42\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u89e3\u51b3\u590d\u6742\u7684\u65f6\u7a7a\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7684\u65f6\u7a7a\u7406\u89e3\u4efb\u52a1\uff08\u5982\u53d7\u9650\u5174\u8da3\u70b9\u53d1\u73b0\u548c\u884c\u7a0b\u89c4\u5212\uff09\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u6a21\u578b\u6765\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u9700\u8981\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u5de5\u5177\u4ea4\u4e92\u7684\u573a\u666f\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u6784\u5efa\u652f\u630110+\u65f6\u7a7a\u9886\u57df\u5de5\u5177\u7684\u7a33\u5b9a\u5de5\u5177\u73af\u5883\uff1b2\uff09\u5206\u5c42\u6570\u636e\u7b5b\u9009\u6846\u67b6\uff0c\u4ee51:10,000\u7684\u6bd4\u4f8b\u7b5b\u9009\u9ad8\u8d28\u91cf\u67e5\u8be2\uff1b3\uff09\u7ea7\u8054\u8bad\u7ec3\u7b56\u7565\uff1a\u79cd\u5b50SFT\u9636\u6bb5\u8bc4\u4f30\u67e5\u8be2\u96be\u5ea6\uff0c\u7b2c\u4e8cSFT\u9636\u6bb5\u5fae\u8c03\u9ad8\u786e\u5b9a\u6027\u67e5\u8be2\uff0c\u6700\u7ec8RL\u9636\u6bb5\u5229\u7528\u4f4e\u786e\u5b9a\u6027\u6570\u636e\u3002", "result": "STAgent\u5728TravelBench\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5728\u5e7f\u6cdb\u7684\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u5176\u901a\u7528\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u667a\u80fd\u4f53\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "STAgent\u901a\u8fc7\u4e13\u95e8\u7684\u5de5\u5177\u73af\u5883\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u7b5b\u9009\u548c\u7ea7\u8054\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u65e2\u80fd\u5904\u7406\u590d\u6742\u65f6\u7a7a\u4efb\u52a1\u53c8\u80fd\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u4e3a\u9886\u57df\u7279\u5b9a\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "2512.25055", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.25055", "abs": "https://arxiv.org/abs/2512.25055", "authors": ["Tianzhi He", "Farrokh Jazizadeh"], "title": "Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings", "comment": null, "summary": "This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype's performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406AI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u80fd\u6e90\u7ba1\u7406\uff0c\u5728\u8bbe\u5907\u63a7\u5236\u3001\u8bb0\u5fc6\u4efb\u52a1\u7b49\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6210\u672c\u4f30\u7b97\u7b49\u590d\u6742\u4efb\u52a1\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u6570\u636e\u5206\u6790\u80fd\u529b\uff0c\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5e76\u63d0\u4f9b\u667a\u80fd\u80fd\u6e90\u7ba1\u7406\u7684AI\u4ee3\u7406\u3002", "method": "\u63d0\u51fa\u5305\u542b\u611f\u77e5\u3001\u4e2d\u592e\u63a7\u5236\u548c\u884c\u52a8\u4e09\u4e2a\u6a21\u5757\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5f62\u6210\u95ed\u73af\u53cd\u9988\u7cfb\u7edf\u3002\u4f7f\u7528120\u4e2a\u7528\u6237\u67e5\u8be2\u5728\u56db\u4e2a\u771f\u5b9e\u4f4f\u5b85\u80fd\u6e90\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u539f\u578b\u6027\u80fd\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u5ef6\u8fdf\u3001\u529f\u80fd\u3001\u80fd\u529b\u3001\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "result": "\u539f\u578b\u5728\u8bbe\u5907\u63a7\u5236\u51c6\u786e\u738786%\u3001\u8bb0\u5fc6\u76f8\u5173\u4efb\u52a197%\u3001\u8c03\u5ea6\u81ea\u52a8\u531674%\u3001\u80fd\u6e90\u5206\u679077%\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6210\u672c\u4f30\u7b97\u4efb\u52a1\u51c6\u786e\u7387\u4ec549%\u3002\u901a\u8fc7ANOVA\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u666e\u9002\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8eLLM\u7684BEMS AI\u4ee3\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5728\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u80fd\u6e90\u7ba1\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u54cd\u5e94\u51c6\u786e\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2512.24314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24314", "abs": "https://arxiv.org/abs/2512.24314", "authors": ["Shupeng Li", "Weipeng Lu", "Linyun Liu", "Chen Lin", "Shaofei Li", "Zhendong Tan", "Hanjun Zhong", "Yucheng Zeng", "Chenghao Zhu", "Mengyue Liu", "Daxiang Dong", "Jianmin Wu", "Yunting Xiao", "Annan Li", "Danyu Liu", "Jingnan Zhang", "Licen Liu", "Dawei Yin", "Dou Shen"], "title": "QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs", "comment": null, "summary": "Domain-specific enhancement of Large Language Models (LLMs) within the financial context has long been a focal point of industrial application. While previous models such as BloombergGPT and Baichuan-Finance primarily focused on knowledge enhancement, the deepening complexity of financial services has driven a growing demand for models that possess not only domain knowledge but also robust financial reasoning and agentic capabilities. In this paper, we present QianfanHuijin, a financial domain LLM, and propose a generalizable multi-stage training paradigm for industrial model enhancement.\n  Our approach begins with Continual Pre-training (CPT) on financial corpora to consolidate the knowledge base. This is followed by a fine-grained Post-training pipeline designed with increasing specificity: starting with Financial SFT, progressing to Finance Reasoning RL and Finance Agentic RL, and culminating in General RL aligned with real-world business scenarios. Empirical results demonstrate that QianfanHuijin achieves superior performance across various authoritative financial benchmarks. Furthermore, ablation studies confirm that the targeted Reasoning RL and Agentic RL stages yield significant gains in their respective capabilities. These findings validate our motivation and suggest that this fine-grained, progressive post-training methodology is poised to become a mainstream paradigm for various industrial-enhanced LLMs.", "AI": {"tldr": "QianfanHuijin\u662f\u4e00\u4e2a\u91d1\u878d\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u6301\u7eed\u9884\u8bad\u7ec3\u5de9\u56fa\u77e5\u8bc6\u57fa\u7840\uff0c\u7136\u540e\u901a\u8fc7\u91d1\u878dSFT\u3001\u91d1\u878d\u63a8\u7406RL\u3001\u91d1\u878d\u4ee3\u7406RL\u548c\u901a\u7528RL\u7684\u6e10\u8fdb\u5f0f\u540e\u8bad\u7ec3\u63d0\u5347\u80fd\u529b\u3002", "motivation": "\u91d1\u878d\u670d\u52a1\u7684\u590d\u6742\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u6a21\u578b\u4e0d\u4ec5\u5177\u5907\u9886\u57df\u77e5\u8bc6\uff0c\u8fd8\u8981\u6709\u5f3a\u5927\u7684\u91d1\u878d\u63a8\u7406\u548c\u4ee3\u7406\u80fd\u529b\u3002\u73b0\u6709\u6a21\u578b\u5982BloombergGPT\u548cBaichuan-Finance\u4e3b\u8981\u5173\u6ce8\u77e5\u8bc6\u589e\u5f3a\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5bf9\u63a8\u7406\u548c\u4ee3\u7406\u80fd\u529b\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u901a\u7528\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a1) \u5728\u91d1\u878d\u8bed\u6599\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff1b2) \u6e10\u8fdb\u5f0f\u540e\u8bad\u7ec3\u7ba1\u9053\uff1a\u91d1\u878dSFT \u2192 \u91d1\u878d\u63a8\u7406RL \u2192 \u91d1\u878d\u4ee3\u7406RL \u2192 \u901a\u7528RL\uff08\u4e0e\u73b0\u5b9e\u4e1a\u52a1\u573a\u666f\u5bf9\u9f50\uff09\u3002", "result": "QianfanHuijin\u5728\u591a\u4e2a\u6743\u5a01\u91d1\u878d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\uff0c\u9488\u5bf9\u6027\u7684\u63a8\u7406RL\u548c\u4ee3\u7406RL\u9636\u6bb5\u5728\u5404\u81ea\u80fd\u529b\u4e0a\u5e26\u6765\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u79cd\u7ec6\u7c92\u5ea6\u3001\u6e10\u8fdb\u5f0f\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u6709\u671b\u6210\u4e3a\u5404\u79cd\u5de5\u4e1a\u589e\u5f3a\u578bLLM\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u7814\u7a76\u52a8\u673a\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.24933", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24933", "abs": "https://arxiv.org/abs/2512.24933", "authors": ["Minjun Zhao", "Xinyu Zhang", "Shuai Zhang", "Deyang Li", "Ruifeng Shi"], "title": "Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline", "comment": null, "summary": "Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.", "AI": {"tldr": "ADOPT\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6b65LLM\u7ba1\u9053\u7684\u81ea\u9002\u5e94\u4f9d\u8d56\u611f\u77e5\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6bcf\u4e2aLLM\u6b65\u9aa4\u4e0e\u6700\u7ec8\u4efb\u52a1\u7ed3\u679c\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u6587\u672c\u68af\u5ea6\u4f30\u8ba1\u548c\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u591a\u6b65LLM\u7ba1\u9053\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6bcf\u4e00\u6b65\u4f7f\u7528\u7684\u63d0\u793a\u3002\u7531\u4e8e\u7f3a\u4e4f\u6b65\u9aa4\u7ea7\u76d1\u7763\u548c\u6b65\u9aa4\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u8054\u5408\u4f18\u5316\u8fd9\u4e9b\u63d0\u793a\u975e\u5e38\u56f0\u96be\u3002\u73b0\u6709\u7684\u7aef\u5230\u7aef\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u5f80\u5f80\u4ea7\u751f\u6b21\u4f18\u6216\u4e0d\u7a33\u5b9a\u7684\u66f4\u65b0\u3002", "method": "ADOPT\u6846\u67b6\u663e\u5f0f\u5efa\u6a21\u6bcf\u4e2aLLM\u6b65\u9aa4\u4e0e\u6700\u7ec8\u4efb\u52a1\u7ed3\u679c\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u4e8e\u8ba1\u7b97\u89e3\u6790\u5bfc\u6570\u7684\u7cbe\u786e\u6587\u672c\u68af\u5ea6\u4f30\u8ba1\u3002\u5b83\u5c06\u6587\u672c\u68af\u5ea6\u4f30\u8ba1\u4e0e\u68af\u5ea6\u66f4\u65b0\u89e3\u8026\uff0c\u5c06\u591a\u63d0\u793a\u4f18\u5316\u7b80\u5316\u4e3a\u7075\u6d3b\u7684\u5355\u63d0\u793a\u4f18\u5316\u6b65\u9aa4\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eShapley\u503c\u7684\u673a\u5236\u81ea\u9002\u5e94\u5206\u914d\u4f18\u5316\u8d44\u6e90\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u591a\u6837\u5316\u7ba1\u9053\u7ed3\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cADOPT\u662f\u6709\u6548\u4e14\u9c81\u68d2\u7684\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u4f18\u5316\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ADOPT\u4e3a\u591a\u6b65LLM\u7ba1\u9053\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u68af\u5ea6\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7f3a\u4e4f\u6b65\u9aa4\u7ea7\u76d1\u7763\u548c\u6b65\u9aa4\u95f4\u4f9d\u8d56\u6761\u4ef6\u4e0b\u7684\u4f18\u5316\u56f0\u96be\u3002", "topic": "agent analysis"}}
{"id": "2512.24955", "categories": ["cs.LG", "cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24955", "abs": "https://arxiv.org/abs/2512.24955", "authors": ["Yongwei Zhang", "Yuanzhe Xing", "Quan Quan", "Zhikun She"], "title": "MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control", "comment": null, "summary": "Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $\u03bb$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.", "AI": {"tldr": "MSACL\u662f\u4e00\u4e2a\u5c06\u6307\u6570\u7a33\u5b9a\u6027\u7406\u8bba\u4e0e\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6b65Lyapunov\u8bc1\u4e66\u5b66\u4e60\u5b9e\u73b0\u53ef\u8bc1\u660e\u7684\u7a33\u5b9a\u6a21\u578b\u81ea\u7531\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u7b80\u5355\u5956\u52b1\u4e0b\u5b9e\u73b0\u6307\u6570\u7a33\u5b9a\u6027\u548c\u5feb\u901f\u6536\u655b\u3002", "motivation": "\u6a21\u578b\u81ea\u7531\u5f3a\u5316\u5b66\u4e60\u5728\u5b9e\u73b0\u53ef\u8bc1\u660e\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5e73\u8861\u63a2\u7d22\u4e0e\u4e25\u683c\u5b89\u5168\u6027\u65b9\u9762\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u5956\u52b1\u5de5\u7a0b\uff0c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u63d0\u51faMSACL\u6846\u67b6\uff0c\u6574\u5408\u6307\u6570\u7a33\u5b9a\u6027\u7406\u8bba\u4e0e\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u6b65Lyapunov\u8bc1\u4e66\u5b66\u4e60\u3002\u4f7f\u7528\u79bb\u7b56\u7565\u591a\u6b65\u6570\u636e\u5b66\u4e60\u6ee1\u8db3\u7406\u8bba\u7a33\u5b9a\u6027\u6761\u4ef6\u7684Lyapunov\u8bc1\u4e66\uff0c\u5f15\u5165\u6307\u6570\u7a33\u5b9a\u6027\u6807\u7b7e(ESL)\u548c\u03bb\u52a0\u6743\u805a\u5408\u673a\u5236\u5e73\u8861\u591a\u6b65\u5b66\u4e60\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002\u7b56\u7565\u4f18\u5316\u7531\u7a33\u5b9a\u6027\u611f\u77e5\u4f18\u52bf\u51fd\u6570\u6307\u5bfc\uff0c\u786e\u4fdd\u5b66\u4e60\u7b56\u7565\u4fc3\u8fdb\u5feb\u901fLyapunov\u4e0b\u964d\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u7a33\u5b9a\u5316\u548c\u975e\u7ebf\u6027\u8ddf\u8e2a\u4efb\u52a1\uff09\u4e2d\u8bc4\u4f30\uff0cMSACL\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLyapunov\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002\u5728\u7b80\u5355\u5956\u52b1\u4e0b\u5b9e\u73b0\u6307\u6570\u7a33\u5b9a\u6027\u548c\u5feb\u901f\u6536\u655b\uff0c\u5bf9\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u663e\u8457\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8f68\u8ff9\u3002\u654f\u611f\u6027\u5206\u6790\u786e\u5b9a\u591a\u6b65\u89c6\u91cen=20\u4f5c\u4e3a\u8de8\u4e0d\u540c\u7cfb\u7edf\u7684\u7a33\u5065\u9ed8\u8ba4\u503c\u3002", "conclusion": "MSACL\u901a\u8fc7\u5c06Lyapunov\u7406\u8bba\u4e0e\u79bb\u7b56\u7565actor-critic\u6846\u67b6\u8fde\u63a5\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u5b66\u4e60\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6e90\u4ee3\u7801\u548c\u57fa\u51c6\u73af\u5883\u5c06\u516c\u5f00\u63d0\u4f9b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.25023", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.25023", "abs": "https://arxiv.org/abs/2512.25023", "authors": ["Timo Kaufmann", "Yannick Metz", "Daniel Keim", "Eyke H\u00fcllermeier"], "title": "ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning", "comment": "NeurIPS 2025", "summary": "Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.", "AI": {"tldr": "\u63d0\u51faResponseRank\u65b9\u6cd5\uff0c\u5229\u7528\u4ee3\u7406\u4fe1\u53f7\uff08\u5982\u54cd\u5e94\u65f6\u95f4\u3001\u6807\u6ce8\u8005\u4e00\u81f4\u6027\uff09\u7684\u76f8\u5bf9\u5dee\u5f02\u6765\u63a8\u65ad\u504f\u597d\u5f3a\u5ea6\u6392\u5e8f\uff0c\u901a\u8fc7\u5c40\u90e8\u5206\u5c42\u63a7\u5236\u7cfb\u7edf\u6027\u53d8\u5f02\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfRLHF\u4f7f\u7528\u7684\u4e8c\u5143\u9009\u62e9\u53ea\u80fd\u4f20\u8fbe\u504f\u597d\u65b9\u5411\uff0c\u65e0\u6cd5\u8861\u91cf\u504f\u597d\u5f3a\u5ea6\u3002\u800c\u5f3a\u5ea6\u4fe1\u606f\u5bf9\u4e8e\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u548c\u504f\u597d\u6a21\u578b\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u4ee3\u7406\u4fe1\u53f7\uff08\u54cd\u5e94\u65f6\u95f4\u3001\u6807\u6ce8\u8005\u4e00\u81f4\u6027\uff09\u901a\u5e38\u566a\u58f0\u5927\u4e14\u5b58\u5728\u6df7\u6dc6\u56e0\u7d20\u3002", "method": "\u63d0\u51faResponseRank\u65b9\u6cd5\uff1a1) \u5229\u7528\u4ee3\u7406\u4fe1\u53f7\u7684\u76f8\u5bf9\u5dee\u5f02\u5bf9\u6210\u5bf9\u6bd4\u8f83\u4e2d\u7684\u54cd\u5e94\u8fdb\u884c\u504f\u597d\u5f3a\u5ea6\u6392\u5e8f\uff1b2) \u901a\u8fc7\u7cbe\u5fc3\u6784\u5efa\u7684\u5206\u5c42\u8fdb\u884c\u5c40\u90e8\u6bd4\u8f83\uff0c\u63a7\u5236\u7cfb\u7edf\u6027\u53d8\u5f02\uff1b3) \u6700\u5c0f\u5316\u5bf9\u5f3a\u5ea6\u4fe1\u53f7\u7684\u5047\u8bbe\uff0c\u5b9e\u73b0\u9c81\u68d2\u5b66\u4e60\u3002", "result": "\u5728\u5408\u6210\u504f\u597d\u5b66\u4e60\uff08\u6a21\u62df\u54cd\u5e94\u65f6\u95f4\uff09\u3001\u8bed\u8a00\u5efa\u6a21\uff08\u6807\u6ce8\u8005\u4e00\u81f4\u6027\uff09\u548cRL\u63a7\u5236\u4efb\u52a1\uff08\u6a21\u62df\u56de\u5408\u56de\u62a5\uff09\u4e2d\uff0cResponseRank\u5c55\u73b0\u51fa\u6539\u8fdb\u7684\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002\u540c\u65f6\u63d0\u51faPDC\u6307\u6807\u6765\u5206\u79bb\u57fa\u6570\u6548\u7528\u5b66\u4e60\u4e0e\u5e8f\u6570\u51c6\u786e\u6027\u3002", "conclusion": "ResponseRank\u80fd\u591f\u4ece\u566a\u58f0\u5f3a\u5ea6\u4fe1\u53f7\u4e2d\u9c81\u68d2\u5730\u5b66\u4e60\u504f\u597d\u5f3a\u5ea6\uff0c\u901a\u8fc7\u5c40\u90e8\u76f8\u5bf9\u6bd4\u8f83\u514b\u670d\u7cfb\u7edf\u6027\u53d8\u5f02\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u5f3a\u5ea6\u4fe1\u606f\u5229\u7528\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.24497", "categories": ["cs.AI", "cs.LG", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.24497", "abs": "https://arxiv.org/abs/2512.24497", "authors": ["Basile Terver", "Tsung-Yen Yang", "Jean Ponce", "Adrien Bardes", "Yann LeCun"], "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "comment": null, "summary": "A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faJEPA-WM\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u4e16\u754c\u6a21\u578b\u7684\u8868\u793a\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c4\u5212\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86DINO-WM\u548cV-JEPA-2-AC\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "AI\u9886\u57df\u957f\u671f\u5b58\u5728\u4e00\u4e2a\u6311\u6218\uff1a\u5f00\u53d1\u80fd\u591f\u89e3\u51b3\u5e7f\u6cdb\u7269\u7406\u4efb\u52a1\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u672a\u89c1\u4efb\u52a1\u548c\u73af\u5883\u7684\u667a\u80fd\u4f53\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u4e16\u754c\u6a21\u578b\u7684\u72b6\u6001-\u52a8\u4f5c\u8f68\u8ff9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u8f93\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c4\u5212\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c4\u5212\u53ef\u80fd\u66f4\u9ad8\u6548\u3002", "method": "\u5c06\u8fd9\u7c7b\u65b9\u6cd5\u5b9a\u4e49\u4e3aJEPA-WM\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u76ee\u6807\u548c\u89c4\u5212\u7b97\u6cd5\u7b49\u5173\u952e\u7ec4\u4ef6\u3002\u901a\u8fc7\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u5404\u7ec4\u4ef6\u5bf9\u89c4\u5212\u6210\u529f\u7387\u7684\u5f71\u54cd\uff0c\u6700\u7ec8\u63d0\u51fa\u4f18\u5316\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u90fd\u8d85\u8d8a\u4e86DINO-WM\u548cV-JEPA-2-AC\u4e24\u4e2a\u57fa\u51c6\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c4\u5212\u7684\u4f18\u52bf\u3002", "conclusion": "JEPA-WM\u65b9\u6cd5\u901a\u8fc7\u5728\u4e16\u754c\u6a21\u578b\u7684\u8868\u793a\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c4\u5212\uff0c\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u89e3\u51b3\u7269\u7406\u4efb\u52a1\u5e76\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2512.24574", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24574", "abs": "https://arxiv.org/abs/2512.24574", "authors": ["Zhenyu Zhang", "Xiaoxia Wu", "Zhongzhu Zhou", "Qingyang Wu", "Yineng Zhang", "Pragaash Ponnusamy", "Harikaran Subbaraj", "Jue Wang", "Shuaiwen Leon Song", "Ben Athiwaratkun"], "title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time", "comment": null, "summary": "Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.", "AI": {"tldr": "CREST\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u5e72\u9884\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u6765\u5f15\u5bfcLLM\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4f4e\u6548\u7684\u94fe\u5f0f\u601d\u8003\uff0c\u63d0\u9ad8\u51c6\u786e\u7387\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u957f\u94fe\u5f0f\u601d\u8003\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u8fd9\u4e9b\u63a8\u7406\u8f68\u8ff9\u901a\u5e38\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u6216\u4e0d\u7a33\u5b9a\u7684\u63a8\u7406\uff08\u601d\u8003\u4e0d\u8db3\u6216\u8fc7\u5ea6\u601d\u8003\uff09\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5f15\u5bfc\u6a21\u578b\u8fdc\u79bb\u4f4e\u6548\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "method": "CREST\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a(1)\u79bb\u7ebf\u6821\u51c6\u6b65\u9aa4\uff0c\u8bc6\u522b\u4e0e\u7279\u5b9a\u8ba4\u77e5\u884c\u4e3a\uff08\u5982\u9a8c\u8bc1\u548c\u56de\u6eaf\uff09\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u63a8\u5bfc\u51fa\u5934\u7279\u5b9a\u7684\u5f15\u5bfc\u5411\u91cf\uff1b(2)\u63a8\u7406\u65f6\u7a0b\u5e8f\uff0c\u901a\u8fc7\u65cb\u8f6c\u9690\u85cf\u8868\u793a\u6765\u6291\u5236\u8fd9\u4e9b\u5411\u91cf\u65b9\u5411\u4e0a\u7684\u7ec4\u4ef6\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u6291\u5236\u975e\u751f\u4ea7\u6027\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u5728\u591a\u79cd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\uff0cCREST\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe17.5%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8637.6%\u7684token\u4f7f\u7528\u91cf\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684LLM\u63a8\u7406\u3002", "conclusion": "CREST\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u5e72\u9884\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u6765\u5f15\u5bfcLLM\u63a8\u7406\u8fc7\u7a0b\uff0c\u65e2\u80fd\u63d0\u9ad8\u51c6\u786e\u7387\u53c8\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u66f4\u9ad8\u6548\u53ef\u9760\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.b207964c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuild.ms%2F2025%2F12%2F22%2Fcodex-vs-claude-code-today%2F%3Futm_source=tldrai/1/0100019b50b7bce7-fe11a932-93bb-4e4a-ba91-e551e912cd48-000000/B4U5xapJHmVAYy5IpFBXnBAl_mmDQLGRL-_v1eH9ft8=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuild.ms%2F2025%2F12%2F22%2Fcodex-vs-claude-code-today%2F%3Futm_source=tldrai/1/0100019b50b7bce7-fe11a932-93bb-4e4a-ba91-e551e912cd48-000000/B4U5xapJHmVAYy5IpFBXnBAl_mmDQLGRL-_v1eH9ft8=437", "authors": ["TLDR Newsletter"], "title": "Codex vs. Claude Code", "comment": "Source: TLDR Newsletter, Date: 2025-12-24, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuild.ms%2F2025%2F12%2F22%2Fcodex-vs-claude-code-today%2F%3Futm_source=tldrai/1/0100019b50b7bce7-fe11a932-93bb-4e4a-ba91-e551e912cd48-000000/B4U5xapJHmVAYy5IpFBXnBAl_mmDQLGRL-_v1eH9ft8=437", "summary": "Codex vs. Claude Code (Today) (5 minute read) There's no wrong choice when it comes to AI. The tool you choose should match how you work. Try out both Claude and Codex and see which one fits. Every AI tool has its strengths and weaknesses, and the only way to discover what they are is by using them.", "source": "tldr", "AI": {"tldr": "\u5bf9\u6bd4\u5206\u6790Claude Code\u548cCodex\u4e24\u6b3eAI\u4ee3\u7801\u5de5\u5177\uff0c\u5f3a\u8c03\u9009\u62e9\u5e94\u6839\u636e\u4e2a\u4eba\u5de5\u4f5c\u65b9\u5f0f\u51b3\u5b9a\uff0c\u5efa\u8bae\u7528\u6237\u5b9e\u9645\u8bd5\u7528\u4ee5\u53d1\u73b0\u5404\u81ea\u7684\u4f18\u7f3a\u70b9", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u4e86\u89e3\u4e0d\u540cAI\u4ee3\u7801\u5de5\u5177\u7684\u7279\u70b9\uff0c\u6307\u5bfc\u4ed6\u4eec\u6839\u636e\u81ea\u8eab\u5de5\u4f5c\u6d41\u7a0b\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\uff0c\u800c\u4e0d\u662f\u76f2\u76ee\u8ddf\u968f\u4e3b\u6d41\u9009\u62e9", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5b9e\u9645\u4f7f\u7528\u4f53\u9a8c\u7684\u91cd\u8981\u6027\uff0c\u5efa\u8bae\u7528\u6237\u4eb2\u81ea\u8bd5\u7528Claude Code\u548cCodex\u4e24\u6b3e\u5de5\u5177", "result": "\u6307\u51fa\u4e24\u6b3e\u5de5\u5177\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u6ca1\u6709\u7edd\u5bf9\u7684\u597d\u574f\u4e4b\u5206\uff0c\u9009\u62e9\u5e94\u57fa\u4e8e\u4e2a\u4eba\u5de5\u4f5c\u65b9\u5f0f\u548c\u5b9e\u9645\u4f7f\u7528\u4f53\u9a8c", "conclusion": "AI\u5de5\u5177\u7684\u9009\u62e9\u5e94\u4e2a\u6027\u5316\uff0c\u5f00\u53d1\u8005\u9700\u8981\u901a\u8fc7\u5b9e\u9645\u8bd5\u7528Claude Code\u548cCodex\u6765\u53d1\u73b0\u54ea\u6b3e\u5de5\u5177\u66f4\u9002\u5408\u81ea\u5df1\u7684\u5de5\u4f5c\u6d41\u7a0b", "topic": "code agent"}}
{"id": "tldr.2512.8e90feee", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FArtificialAnalysis%2FStirrup%3Futm_source=tldrai/1/0100019b50b7bce7-fe11a932-93bb-4e4a-ba91-e551e912cd48-000000/SCXQJmPmQMCo-Go-Sj-kBXFSsN1RsCUSLpXZvtLqP_8=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FArtificialAnalysis%2FStirrup%3Futm_source=tldrai/1/0100019b50b7bce7-fe11a932-93bb-4e4a-ba91-e551e912cd48-000000/SCXQJmPmQMCo-Go-Sj-kBXFSsN1RsCUSLpXZvtLqP_8=437", "authors": ["TLDR Newsletter"], "title": "Stirrup", "comment": "Source: TLDR Newsletter, Date: 2025-12-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FArtificialAnalysis%2FStirrup%3Futm_source=tldrai/1/0100019b50b7bce7-fe11a932-93bb-4e4a-ba91-e551e912cd48-000000/SCXQJmPmQMCo-Go-Sj-kBXFSsN1RsCUSLpXZvtLqP_8=437", "summary": "Stirrup (GitHub Repo) Stirrup is a framework for building agents that lets models choose their own approach to completing tasks. It has best practices and tools built in and is fully customizable. Stirrup features a skills system that extends agent capabilities, flexible tool execution, context management tools, flexible provider support, and multimodal support.", "source": "tldr", "AI": {"tldr": "Stirrup\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u5141\u8bb8\u6a21\u578b\u81ea\u4e3b\u9009\u62e9\u5b8c\u6210\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u5185\u7f6e\u6700\u4f73\u5b9e\u8df5\u548c\u5de5\u5177\uff0c\u652f\u6301\u5b8c\u5168\u81ea\u5b9a\u4e49\u3002", "motivation": "\u73b0\u6709\u7684\u667a\u80fd\u4f53\u6846\u67b6\u5f80\u5f80\u9650\u5236\u6a21\u578b\u7684\u81ea\u4e3b\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8ba9\u6a21\u578b\u81ea\u4e3b\u9009\u62e9\u65b9\u6cd5\u3001\u540c\u65f6\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\u548c\u6700\u4f73\u5b9e\u8df5\u7684\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u6280\u80fd\u7cfb\u7edf\u6269\u5c55\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u5de5\u5177\u6267\u884c\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u5de5\u5177\u3001\u591a\u63d0\u4f9b\u5546\u652f\u6301\u4ee5\u53ca\u591a\u6a21\u6001\u652f\u6301\uff0c\u6784\u5efa\u4e00\u4e2a\u5b8c\u5168\u53ef\u5b9a\u5236\u7684\u667a\u80fd\u4f53\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u4e86Stirrup\u6846\u67b6\uff0c\u5177\u5907\u6280\u80fd\u7cfb\u7edf\u3001\u7075\u6d3b\u5de5\u5177\u6267\u884c\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u591a\u63d0\u4f9b\u5546\u652f\u6301\u548c\u591a\u6a21\u6001\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Stirrup\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8ba9\u6a21\u578b\u81ea\u4e3b\u9009\u62e9\u65b9\u6cd5\u7684\u76ee\u6807\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u5de5\u5177\u548c\u6700\u4f73\u5b9e\u8df5\uff0c\u4e3a\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6807\u51c6\u5316\u7684\u5e73\u53f0\u3002", "topic": "code agent"}}
{"id": "tldr.2512.afdc409d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdelve.co%2Fbook-demo%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=tldr-primary-dec26-25/2/0100019b5a6655f3-905a9bc6-b7be-4501-af53-a7c1c8093375-000000/DDfDISgxi995g3ex53sY5NP57GiUHkzm3fAHJnx6iCk=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdelve.co%2Fbook-demo%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=tldr-primary-dec26-25/2/0100019b5a6655f3-905a9bc6-b7be-4501-af53-a7c1c8093375-000000/DDfDISgxi995g3ex53sY5NP57GiUHkzm3fAHJnx6iCk=437", "authors": ["TLDR Newsletter"], "title": "Delve Shipmas", "comment": "Source: TLDR Newsletter, Date: 2025-12-26, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdelve.co%2Fbook-demo%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=tldr-primary-dec26-25/2/0100019b5a6655f3-905a9bc6-b7be-4501-af53-a7c1c8093375-000000/DDfDISgxi995g3ex53sY5NP57GiUHkzm3fAHJnx6iCk=437", "summary": "Delve Shipmas Day 5: Automated Screenshots with Delve CUA (Sponsor) Welcome to the final day of Delve Shipmas \ud83c\udf84 We've announced an AI copilot, AI-native VRM, an AI security questionnaire extension, all-new frameworks, and now: CUA.If you've ever spent hours: clicking through dashboards, taking screenshots, labeling files, and then re-doing it all again for the next audit.\u2026this one's for you. Today, Delve is launching Delve CUA: a computer-using AI agent that takes compliance screenshots for y...", "source": "tldr", "AI": {"tldr": "Delve\u63a8\u51faCUA\uff0c\u4e00\u4e2a\u8ba1\u7b97\u673a\u4f7f\u7528AI\u4ee3\u7406\uff0c\u81ea\u52a8\u4e3a\u5408\u89c4\u5ba1\u8ba1\u751f\u6210\u622a\u56fe\uff0c\u51cf\u5c11\u4eba\u5de5\u64cd\u4f5c\u65f6\u95f4", "motivation": "\u89e3\u51b3\u5408\u89c4\u5ba1\u8ba1\u4e2d\u624b\u52a8\u622a\u56fe\u3001\u6807\u8bb0\u6587\u4ef6\u7b49\u91cd\u590d\u6027\u5de5\u4f5c\u8017\u65f6\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5ba1\u8ba1\u6548\u7387", "method": "\u5f00\u53d1\u8ba1\u7b97\u673a\u4f7f\u7528AI\u4ee3\u7406\uff08CUA\uff09\uff0c\u80fd\u591f\u81ea\u52a8\u5bfc\u822a\u4eea\u8868\u677f\u3001\u622a\u56fe\u3001\u6807\u8bb0\u6587\u4ef6\uff0c\u5b9e\u73b0\u5408\u89c4\u5ba1\u8ba1\u81ea\u52a8\u5316", "result": "\u63a8\u51faDelve CUA\u4ea7\u54c1\uff0c\u4f5c\u4e3aDelve Shipmas\u6d3b\u52a8\u7684\u4e00\u90e8\u5206\u53d1\u5e03\uff0c\u65e8\u5728\u7b80\u5316\u5408\u89c4\u5ba1\u8ba1\u6d41\u7a0b", "conclusion": "CUA\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5408\u89c4\u5ba1\u8ba1\u4e2d\u7684\u4eba\u5de5\u64cd\u4f5c\u65f6\u95f4\uff0c\u63d0\u9ad8\u5ba1\u8ba1\u6548\u7387\u548c\u51c6\u786e\u6027", "topic": "swe application"}}
{"id": "tldr.2512.365c3063", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fluongnv89%2Fclaude-howto%3Futm_source=tldrnewsletter/1/0100019b5a6655f3-905a9bc6-b7be-4501-af53-a7c1c8093375-000000/54bmQYtj-LLjs0GTzdWg9bQZPti7VFfUDgitHLuRuDY=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fluongnv89%2Fclaude-howto%3Futm_source=tldrnewsletter/1/0100019b5a6655f3-905a9bc6-b7be-4501-af53-a7c1c8093375-000000/54bmQYtj-LLjs0GTzdWg9bQZPti7VFfUDgitHLuRuDY=437", "authors": ["TLDR Newsletter"], "title": "Claude How To", "comment": "Source: TLDR Newsletter, Date: 2025-12-26, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fluongnv89%2Fclaude-howto%3Futm_source=tldrnewsletter/1/0100019b5a6655f3-905a9bc6-b7be-4501-af53-a7c1c8093375-000000/54bmQYtj-LLjs0GTzdWg9bQZPti7VFfUDgitHLuRuDY=437", "summary": "Claude How To (GitHub Repo) A complete guide to Claude Code features.", "source": "tldr", "AI": {"tldr": "GitHub\u4ed3\u5e93\u63d0\u4f9bClaude Code\u529f\u80fd\u7684\u5b8c\u6574\u4f7f\u7528\u6307\u5357", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u5229\u7528Claude Code\u7684\u5404\u9879\u529f\u80fd\uff0c\u63d0\u9ad8\u7f16\u7801\u6548\u7387", "method": "\u521b\u5efa\u8be6\u7ec6\u7684\u6587\u6863\u548c\u793a\u4f8b\u4ee3\u7801\uff0c\u5c55\u793aClaude Code\u7684\u5404\u79cd\u529f\u80fd\u548c\u4f7f\u7528\u573a\u666f", "result": "\u63d0\u4f9b\u4e86\u5168\u9762\u7684Claude Code\u4f7f\u7528\u6307\u5357\uff0c\u5305\u62ec\u4ee3\u7801\u751f\u6210\u3001\u8c03\u8bd5\u3001\u91cd\u6784\u7b49\u529f\u80fd\u7684\u5177\u4f53\u64cd\u4f5c\u65b9\u6cd5", "conclusion": "\u8be5\u8d44\u6e90\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684Claude Code\u4f7f\u7528\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u7f16\u7a0b\u5de5\u4f5c\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2512.e19c5312", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsankalp.bearblog.dev%2Fmy-experience-with-claude-code-20-and-how-to-get-better-at-using-coding-agents%2F%3Futm_source=tldrnewsletter/1/0100019b69d73709-696ca2d6-e8c3-45c1-add5-296c6e82c8b8-000000/pKaVPgy0ZQ4sWVsBun3Jo4UhgTviNiMN-pIsFsGkWpg=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsankalp.bearblog.dev%2Fmy-experience-with-claude-code-20-and-how-to-get-better-at-using-coding-agents%2F%3Futm_source=tldrnewsletter/1/0100019b69d73709-696ca2d6-e8c3-45c1-add5-296c6e82c8b8-000000/pKaVPgy0ZQ4sWVsBun3Jo4UhgTviNiMN-pIsFsGkWpg=437", "authors": ["TLDR Newsletter"], "title": "A Guide to Claude Code 2.0 and Getting Better at Using Coding Agents", "comment": "Source: TLDR Newsletter, Date: 2025-12-29, Reading time: 55 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsankalp.bearblog.dev%2Fmy-experience-with-claude-code-20-and-how-to-get-better-at-using-coding-agents%2F%3Futm_source=tldrnewsletter/1/0100019b69d73709-696ca2d6-e8c3-45c1-add5-296c6e82c8b8-000000/pKaVPgy0ZQ4sWVsBun3Jo4UhgTviNiMN-pIsFsGkWpg=437", "summary": "A Guide to Claude Code 2.0 and Getting Better at Using Coding Agents (55 minute read) Claude Code dominated the CLI coding product experience this year. This guide shows readers the thought processes and simple things to keep in mind to get the most out of Claude Code. Learning how things work in Claude Code directly transfers to other tools, both in terms of personal usage and production-grade engineering. The post will help users keep up with coding agents in general.", "source": "tldr", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8eClaude Code 2.0\u7684\u4f7f\u7528\u6307\u5357\uff0c\u65e8\u5728\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u4f7f\u7528\u7f16\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u63d0\u5347\u4e2a\u4eba\u4f7f\u7528\u548c\u751f\u4ea7\u7ea7\u5de5\u7a0b\u80fd\u529b\u3002", "motivation": "Claude Code\u4eca\u5e74\u5728CLI\u7f16\u7801\u4ea7\u54c1\u4f53\u9a8c\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u7528\u6237\u9700\u8981\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\u548c\u6700\u4f73\u5b9e\u8df5\u624d\u80fd\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u4eab\u601d\u7ef4\u8fc7\u7a0b\u548c\u7b80\u5355\u6280\u5de7\uff0c\u5e2e\u52a9\u7528\u6237\u638c\u63e1\u7f16\u7801\u4ee3\u7406\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u5c55\u793aClaude Code\u7684\u601d\u7ef4\u8fc7\u7a0b\u548c\u5173\u952e\u4f7f\u7528\u8981\u70b9\uff0c\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\u3002\u5f3a\u8c03\u4eceClaude Code\u5b66\u5230\u7684\u6280\u80fd\u53ef\u4ee5\u76f4\u63a5\u8fc1\u79fb\u5230\u5176\u4ed6\u5de5\u5177\uff0c\u6db5\u76d6\u4e2a\u4eba\u4f7f\u7528\u548c\u751f\u4ea7\u7ea7\u5de5\u7a0b\u4e24\u4e2a\u5c42\u9762\u3002", "result": "\u7528\u6237\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f7f\u7528Claude Code\uff0c\u638c\u63e1\u7f16\u7801\u4ee3\u7406\u5de5\u5177\u7684\u6838\u5fc3\u539f\u7406\uff0c\u4ece\u800c\u8ddf\u4e0a\u7f16\u7801\u4ee3\u7406\u6280\u672f\u7684\u53d1\u5c55\u8d8b\u52bf\u3002", "conclusion": "\u638c\u63e1Claude Code\u7684\u4f7f\u7528\u6280\u5de7\u4e0d\u4ec5\u6709\u52a9\u4e8e\u63d0\u5347\u5f53\u524d\u5de5\u5177\u7684\u4f7f\u7528\u6548\u7387\uff0c\u8fd8\u80fd\u4e3a\u4f7f\u7528\u5176\u4ed6\u7f16\u7801\u4ee3\u7406\u5de5\u5177\u6253\u4e0b\u57fa\u7840\uff0c\u5e2e\u52a9\u7528\u6237\u5728\u5feb\u901f\u53d1\u5c55\u7684\u7f16\u7801\u4ee3\u7406\u9886\u57df\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2512.85098ae6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdenislavgavrilov.com%2Fp%2Fclopus-watcher-an-autonomous-monitoring%3Futm_source=tldrnewsletter/1/0100019b6eff6ec8-65de829a-5877-420b-a59f-5c6a82a0ed70-000000/fXpv7wp6VInCqWkAHXYVK5d0S94Dcoz-Ok_FLiZ3kmg=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdenislavgavrilov.com%2Fp%2Fclopus-watcher-an-autonomous-monitoring%3Futm_source=tldrnewsletter/1/0100019b6eff6ec8-65de829a-5877-420b-a59f-5c6a82a0ed70-000000/fXpv7wp6VInCqWkAHXYVK5d0S94Dcoz-Ok_FLiZ3kmg=437", "authors": ["TLDR Newsletter"], "title": "Clopus-Watcher: An autonomous monitoring agent", "comment": "Source: TLDR Newsletter, Date: 2025-12-30, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdenislavgavrilov.com%2Fp%2Fclopus-watcher-an-autonomous-monitoring%3Futm_source=tldrnewsletter/1/0100019b6eff6ec8-65de829a-5877-420b-a59f-5c6a82a0ed70-000000/fXpv7wp6VInCqWkAHXYVK5d0S94Dcoz-Ok_FLiZ3kmg=437", "summary": "Clopus-Watcher: An autonomous monitoring agent (8 minute read) AI will likely make 24/7 on-call a thing of the past. 24/7 monitoring is a lot simpler than the development process. There are often reference documents that engineers can follow to bring systems back up, and if they fail, there's always a backup and recovery plan in place. On-call jobs have always been more systematic. This post introduces an autonomous monitoring agent that does what an on-call engineer would do, but autonomousl...", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u81ea\u4e3b\u76d1\u63a7\u4ee3\u7406Clopus-Watcher\uff0c\u80fd\u591f\u66ff\u4ee3\u4eba\u5de5on-call\u5de5\u7a0b\u5e08\u8fdb\u884c24/7\u7cfb\u7edf\u76d1\u63a7\uff0c\u81ea\u52a8\u6267\u884c\u6545\u969c\u6062\u590d\u64cd\u4f5c", "motivation": "\u4f20\u7edf24/7\u4eba\u5de5on-call\u76d1\u63a7\u5de5\u4f5c\u8d1f\u62c5\u91cd\u4e14\u6210\u672c\u9ad8\uff0c\u800cAI\u6280\u672f\u53d1\u5c55\u4f7f\u5f97\u81ea\u52a8\u5316\u76d1\u63a7\u6210\u4e3a\u53ef\u80fd\uff0c\u53ef\u4ee5\u89e3\u653e\u5de5\u7a0b\u5e08\u5e76\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u4e3b\u76d1\u63a7\u4ee3\u7406\uff0c\u80fd\u591f\u50cfon-call\u5de5\u7a0b\u5e08\u4e00\u6837\u5de5\u4f5c\uff1a\u76d1\u63a7\u7cfb\u7edf\u72b6\u6001\u3001\u53c2\u8003\u6587\u6863\u6267\u884c\u6062\u590d\u64cd\u4f5c\u3001\u5b9e\u65bd\u5907\u4efd\u548c\u6062\u590d\u8ba1\u5212", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u6267\u884con-call\u5de5\u7a0b\u5e08\u4efb\u52a1\u7684AI\u4ee3\u7406\uff0c\u5b9e\u73b024/7\u81ea\u52a8\u5316\u7cfb\u7edf\u76d1\u63a7\u548c\u6545\u969c\u6062\u590d", "conclusion": "AI\u9a71\u52a8\u7684\u81ea\u4e3b\u76d1\u63a7\u4ee3\u7406\u6709\u671b\u53d6\u4ee3\u4f20\u7edf\u4eba\u5de5on-call\uff0c\u4f7f24/7\u76d1\u63a7\u6210\u4e3a\u5386\u53f2\uff0c\u63d0\u9ad8\u8fd0\u7ef4\u6548\u7387\u548c\u7cfb\u7edf\u53ef\u9760\u6027", "topic": "agent analysis"}}
