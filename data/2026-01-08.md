<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 17]
- [tldr.article](#tldr.article) [Total: 10]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Grading Scale Impact on LLM-as-a-Judge: Human-LLM Alignment Is Highest on 0-5 Grading Scale](https://arxiv.org/abs/2601.03444)
*Weiyue Li,Minda Zhao,Weixuan Dong,Jiahui Cai,Yuze Wei,Michael Pocress,Yi Li,Wanyan Yuan,Xiaoyue Wang,Ruoyu Hou,Kaiyuan Lou,Wenqi Zeng,Yutong Yang,Yilun Du,Mengyu Wang*

Main category: cs.CL

TL;DR: 研究探讨LLM作为评估者时评分量表对评分一致性的影响，发现不同量表会显著影响人类与LLM评分的一致性，0-5分量表表现最佳，并揭示聚合可靠性可能掩盖基准异质性和性别差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM作为评估者在提示变化时缺乏评分一致性，但评分量表本身的影响尚未充分探索。本研究旨在比较人类和LLM在不同量表上的评分表现，探究量表设计对评估可靠性的影响。

Method: 收集人类和LLM在三种不同量表上的评分数据，涵盖六个基准任务（包括客观、主观开放性和混合任务）。使用组内相关系数(ICC)测量绝对一致性，分析不同量表下人类-LLM评分的一致性差异。

Result: LLM在主观基准上的评分在不同量表间不完全一致；量表选择显著影响人类-LLM一致性，即使组内面板可靠性较高；综合各任务，0-5分量表表现出最强的人类-LLM对齐；聚合可靠性可能掩盖基准异质性和性别群体间的系统性差异。

Conclusion: 评分量表设计对LLM作为评估者的可靠性至关重要，0-5分量表表现最佳。需要关注量表设计和子级诊断作为LLM评估协议的重要组成部分，以避免聚合指标掩盖的异质性问题。

Abstract: Large language models (LLMs) are increasingly used as automated evaluators, yet prior works demonstrate that these LLM judges often lack consistency in scoring when the prompt is altered. However, the effect of the grading scale itself remains underexplored. We study the LLM-as-a-judge problem by comparing two kinds of raters: humans and LLMs. We collect ratings from both groups on three scales and across six benchmarks that include objective, open-ended subjective, and mixed tasks. Using intraclass correlation coefficients (ICC) to measure absolute agreement, we find that LLM judgments are not perfectly consistent across scales on subjective benchmarks, and that the choice of scale substantially shifts human-LLM agreement, even when within-group panel reliability is high. Aggregated over tasks, the grading scale of 0-5 yields the strongest human-LLM alignment. We further demonstrate that pooled reliability can mask benchmark heterogeneity and reveal systematic subgroup differences in alignment across gender groups, strengthening the importance of scale design and sub-level diagnostics as essential components of LLM-as-a-judge protocols.

</details>


### [2] [Reasoning Pattern Alignment Merging for Adaptive Reasoning](https://arxiv.org/abs/2601.03506)
*Zhaofeng Zhong,Wei Yuan,Tong Chen,Xiangyu Zhao,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.CL

TL;DR: RPAM通过层间模型融合技术，将长推理链模型与短推理链模型合并，实现推理效率提升，无需重新训练或大量额外数据。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在处理复杂任务时通常生成冗长的推理路径，导致不必要的计算开销和延迟。现有加速方法要么需要重新训练模型，要么依赖复杂的提示工程，成本高昂且对输入敏感。

Method: 提出推理模式对齐融合（RPAM）框架：首先构建小规模模式标注校准集，为每个查询分配适当的推理模式；然后通过特征对齐优化层间融合系数，使融合模型的中间表示与选定模型对齐，同时使用对比目标使其远离非选定模型。

Result: 在七个广泛使用的推理基准测试中，RPAM显著降低了推理成本，同时保持了强大的性能表现。

Conclusion: 模型融合作为轻量级替代方案，可以实现高效推理而无需从头训练或大规模额外数据。RPAM框架通过自适应推理模式选择，在保持性能的同时大幅提升推理效率。

Abstract: Recent large reasoning models (LRMs) have made substantial progress in complex reasoning tasks, yet they often generate lengthy reasoning paths for every query, incurring unnecessary computation and latency. Existing speed-up approaches typically rely on retraining the model or designing sophisticated prompting, which are either prohibitively expensive or highly sensitive to the input and prompt formulation. In this work, we study model merging as a lightweight alternative for efficient reasoning: by combining a long chain-of-thought (Long-CoT) reasoning model with a Short-CoT instruction model, we obtain an adaptive reasoner without training from scratch or requiring large-scale additional data. Building on this idea, we propose Reasoning Pattern Alignment Merging (RPAM), a layer-wise model merging framework based on feature alignment to facilitate query-adaptive reasoning. RPAM first constructs a small pattern-labeled calibration set that assigns each query an appropriate reasoning pattern. It then optimizes layer-wise merging coefficients by aligning the merged model's intermediate representations with those of the selected model, while a contrastive objective explicitly pushes them away from the non-selected model. Experiments on seven widely used reasoning benchmarks show that RPAM substantially reduces inference cost while maintaining strong performance. Upon article acceptance, we will provide open-source code to reproduce experiments for RPAM.

</details>


### [3] [IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation](https://arxiv.org/abs/2601.03511)
*Hossein Hosseini Kasnavieh,Gholamreza Haffari,Chris Leckie,Adel N. Toosi*

Main category: cs.CL

TL;DR: IntroLM让因果语言模型在预填充阶段通过自省token预测自身输出质量，无需外部分类器，在问答基准上达到90%的ROC AUC，优于DeBERTa分类器14%，在路由系统中降低延迟33%和大模型使用50%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部分类器（如BERT模型）预测LLM输出质量，但存在上下文窗口有限、表示能力受限和额外计算开销的问题。需要一种更高效的方法让LLM能够自我评估输出质量。

Method: 提出IntroLM方法，在因果语言模型中引入自省token，使用token条件LoRA（仅对自省token激活）让模型学习预测给定查询的输出质量，同时保持原始主干模型行为不变。

Result: 在问答基准测试中，应用于Qwen3 8B的IntroLM在成功预测方面达到90%的ROC AUC，比DeBERTa分类器高出14%。集成到多模型路由系统后，在匹配可靠性的情况下降低延迟达33%，大模型使用减少50%。

Conclusion: IntroLM提供了一种高效的方法让语言模型自我评估输出质量，避免了外部评估器的需求，在路由系统中实现了优越的成本性能权衡。

Abstract: A major challenge for the operation of large language models (LLMs) is how to predict whether a specific LLM will produce sufficiently high-quality output for a given query. Existing approaches rely on external classifiers, most commonly BERT based models, which suffer from limited context windows, constrained representational capacity, and additional computational overhead. We propose IntroLM, a method that enables causal language models to predict their own output quality during the prefilling phase without affecting generation using introspective tokens. By introducing token conditional LoRA that activates only for the introspective token, the model learns to predict the output quality for a given query while preserving the original backbone behavior and avoiding external evaluators. On question answering benchmarks, IntroLM applied to Qwen3 8B achieves a ROC AUC of 90 precent for success prediction, outperforming a DeBERTa classifier by 14 precent. When integrated into multi model routing systems, IntroLM achieves superior cost performance tradeoffs, reducing latency by up to 33 precent and large model usage by up to 50 precent at matched reliability.

</details>


### [4] [DeepSynth-Eval: Objectively Evaluating Information Consolidation in Deep Survey Writing](https://arxiv.org/abs/2601.03540)
*Hongzhi Zhang,Yuanze Hu,Tinghai Zhang,Jia Fu,Tao Wang,Junwei Jing,Zhaoxin Fan,Qi Wang,Ruiming Tang,Han Li,Guorui Zhou,Kun Gai*

Main category: cs.CL

TL;DR: DeepSynth-Eval是一个评估LLM代理信息整合能力的基准，利用高质量综述论文作为黄金标准，通过逆向工程研究请求和构建"Oracle Contexts"来隔离合成与检索噪声，实验表明规划-写作工作流显著优于单轮生成。


<details>
  <summary>Details</summary>
Motivation: LLM向自主代理发展推动了深度研究的进展，但检索后的合成阶段（需要消化大量上下文并将碎片化证据整合为连贯的长篇报告）由于开放式写作的主观性而缺乏客观评估。

Method: 使用高质量综述论文作为黄金标准，逆向工程研究请求并从其参考文献构建"Oracle Contexts"以隔离合成与检索噪声。提出细粒度评估协议，使用通用检查清单（事实覆盖）和约束检查清单（结构组织），将主观判断转化为可验证指标。

Result: 在96个任务上的实验表明，从数百个参考文献中合成信息仍然是一个重大挑战。代理式规划-写作工作流显著优于单轮生成，有效减少幻觉并提高对复杂结构约束的遵循度。

Conclusion: DeepSynth-Eval基准填补了LLM代理信息整合能力评估的空白，证明了规划-写作工作流在复杂合成任务中的优势，为未来研究提供了客观评估框架。

Abstract: The evolution of Large Language Models (LLMs) towards autonomous agents has catalyzed progress in Deep Research. While retrieval capabilities are well-benchmarked, the post-retrieval synthesis stage--where agents must digest massive amounts of context and consolidate fragmented evidence into coherent, long-form reports--remains under-evaluated due to the subjectivity of open-ended writing. To bridge this gap, we introduce DeepSynth-Eval, a benchmark designed to objectively evaluate information consolidation capabilities. We leverage high-quality survey papers as gold standards, reverse-engineering research requests and constructing "Oracle Contexts" from their bibliographies to isolate synthesis from retrieval noise. We propose a fine-grained evaluation protocol using General Checklists (for factual coverage) and Constraint Checklists (for structural organization), transforming subjective judgment into verifiable metrics. Experiments across 96 tasks reveal that synthesizing information from hundreds of references remains a significant challenge. Our results demonstrate that agentic plan-and-write workflows significantly outperform single-turn generation, effectively reducing hallucinations and improving adherence to complex structural constraints.

</details>


### [5] [Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models](https://arxiv.org/abs/2601.03542)
*Xukai Liu,Ye Liu,Jipeng Zhang,Yanghai Zhang,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: LLM在多跳推理中，后续答案实体可能比桥接实体更早可解码，出现层序反转现象，作者提出概率性回忆-提取框架解释此行为。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多跳推理上表现良好，但其内部如何组合多个事实仍不清楚。现有研究提出的跳对齐电路假设认为桥接实体在层间顺序计算，但作者发现这一假设不能推广到真实世界多跳查询。

Method: 作者提出概率性回忆-提取框架，将多跳推理建模为浅层MLP层的广泛概率性回忆，随后是深层注意力层的选择性提取。通过系统探测分析、重新解释先前层解码证据、解释思维链增益等实证验证。

Result: 发现层序反转现象：后续答案实体比桥接实体更早可解码，且随着总跳数增加而加强。提出的框架能解释思维链增益，并为多跳失败提供机制诊断。

Conclusion: 多跳推理不遵循简单的跳对齐假设，而是通过概率性回忆-提取过程实现。该框架为理解LLM内部推理机制提供了新视角，并能解释实际观察到的现象。

Abstract: Large language models (LLMs) perform well on multi-hop reasoning, yet how they internally compose multiple facts remains unclear. Recent work proposes \emph{hop-aligned circuit hypothesis}, suggesting that bridge entities are computed sequentially across layers before later-hop answers. Through systematic analyses on real-world multi-hop queries, we show that this hop-aligned assumption does not generalize: later-hop answer entities can become decodable earlier than bridge entities, a phenomenon we call \emph{layer-order inversion}, which strengthens with total hops. To explain this behavior, we propose a \emph{probabilistic recall-and-extract} framework that models multi-hop reasoning as broad probabilistic recall in shallow MLP layers followed by selective extraction in deeper attention layers. This framework is empirically validated through systematic probing analyses, reinterpreting prior layer-wise decoding evidence, explaining chain-of-thought gains, and providing a mechanistic diagnosis of multi-hop failures despite correct single-hop knowledge. Code is available at https://github.com/laquabe/Layer-Order-Inversion.

</details>


### [6] [EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory](https://arxiv.org/abs/2601.03543)
*Ye Shen,Dun Pei,Yiqiu Guo,Junying Wang,Yijin Guo,Zicheng Zhang,Qi Jia,Jun Zhou,Guangtao Zhai*

Main category: cs.CL

TL;DR: EvolMem是一个新的多会话记忆能力基准测试，基于认知心理学构建，涵盖陈述性和非陈述性记忆，通过混合数据合成框架生成可控复杂度的多会话对话，评估发现没有LLM在所有记忆维度上表现一致优异，且代理记忆机制不一定能增强LLM能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对大型语言模型在多会话设置下多样化记忆维度的系统评估，特别是在认知心理学框架下的全面记忆能力测试。

Method: 提出EvolMem基准测试，基于认知心理学构建，涵盖陈述性和非陈述性记忆的多个细粒度能力。采用混合数据合成框架，包括主题启动生成和叙事启发转换，可扩展地生成具有可控复杂度的多会话对话，并配有样本特定的评估指南。

Result: 广泛评估显示：1）没有LLM在所有记忆维度上一致优于其他模型；2）代理记忆机制不一定能增强LLM的能力，且通常表现出显著的效率限制。

Conclusion: EvolMem为评估LLM和代理系统的多会话记忆能力提供了新的基准测试，揭示了当前模型在记忆能力方面的局限性，特别是代理记忆机制的效率问题。

Abstract: Despite recent advances in understanding and leveraging long-range conversational memory, existing benchmarks still lack systematic evaluation of large language models(LLMs) across diverse memory dimensions, particularly in multi-session settings. In this work, we propose EvolMem, a new benchmark for assessing multi-session memory capabilities of LLMs and agent systems. EvolMem is grounded in cognitive psychology and encompasses both declarative and non-declarative memory, further decomposed into multiple fine-grained abilities. To construct the benchmark, we introduce a hybrid data synthesis framework that consists of topic-initiated generation and narrative-inspired transformations. This framework enables scalable generation of multi-session conversations with controllable complexity, accompanied by sample-specific evaluation guidelines. Extensive evaluation reveals that no LLM consistently outperforms others across all memory dimensions. Moreover, agent memory mechanisms do not necessarily enhance LLMs' capabilities and often exhibit notable efficiency limitations. Data and code will be released at https://github.com/shenye7436/EvolMem.

</details>


### [7] [DiVA: Fine-grained Factuality Verification with Agentic-Discriminative Verifier](https://arxiv.org/abs/2601.03605)
*Hui Huang,Muyun Yang,Yuki Arase*

Main category: cs.CL

TL;DR: DiVA框架结合生成模型的搜索能力和判别模型的评分能力，用于细粒度事实性验证，在FGVeriBench基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有事实性验证研究主要进行二元判断（正确/错误），无法区分不同程度的错误严重性，限制了在细粒度评估和偏好优化等应用中的实用性。

Method: 提出Agentic Discriminative Verifier (DiVA)混合框架，结合生成模型的代理搜索能力和判别模型的精确评分能力，并构建FGVeriBench基准用于细粒度事实性验证测试。

Result: 在FGVeriBench基准上的实验结果表明，DiVA在通用问题和多跳问题的事实性验证方面显著优于现有方法。

Conclusion: DiVA框架通过结合生成和判别模型的优势，有效解决了细粒度事实性验证的挑战，为LLM事实性评估提供了更精细的工具。

Abstract: Despite the significant advancements of Large Language Models (LLMs), their factuality remains a critical challenge, fueling growing interest in factuality verification. Existing research on factuality verification primarily conducts binary judgments (e.g., correct or incorrect), which fails to distinguish varying degrees of error severity. This limits its utility for applications such as fine-grained evaluation and preference optimization. To bridge this gap, we propose the Agentic Discriminative Verifier (DiVA), a hybrid framework that synergizes the agentic search capabilities of generative models with the precise scoring aptitude of discriminative models. We also construct a new benchmark, FGVeriBench, as a robust testbed for fine-grained factuality verification. Experimental results on FGVeriBench demonstrate that our DiVA significantly outperforms existing methods on factuality verification for both general and multi-hop questions.

</details>


### [8] [Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning](https://arxiv.org/abs/2601.03641)
*Zheng Wu,Xingyu Lou,Xinbei Ma,Yansi Li,Weiwen Liu,Weinan Zhang,Jun Wang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: Agent-Dice：基于方向共识评估的参数融合框架，通过几何共识过滤和曲率重要性加权解决LLM智能体持续学习中的稳定性-可塑性困境


<details>
  <summary>Details</summary>
Motivation: LLM智能体在与动态环境交互时面临持续学习新任务而不发生灾难性遗忘的挑战，即稳定性-可塑性困境。作者认为这一困境源于未能明确区分跨任务共享的通用知识和任务特定干扰引入的冲突知识。

Method: 提出Agent-Dice参数融合框架，采用两阶段过程解耦知识更新：1）几何共识过滤修剪冲突梯度；2）基于曲率的重要性加权放大共享语义。提供严格的理论分析验证融合方案的有效性。

Result: 在GUI智能体和工具使用智能体领域的广泛实验表明，Agent-Dice展现出优异的持续学习性能，同时具有最小的计算开销和参数更新。

Conclusion: Agent-Dice通过明确区分共享知识和冲突知识，有效解决了LLM智能体持续学习中的稳定性-可塑性困境，为智能体的持续学习提供了理论洞察和实用框架。

Abstract: Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.

</details>


### [9] [Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations](https://arxiv.org/abs/2601.03775)
*Pingjun Hong,Benjamin Roth*

Main category: cs.CL

TL;DR: 研究探讨LLM自我解释是否能帮助用户预测模型行为，通过StrategyQA评估人类和LLM在有无解释的情况下对反事实问题的预测能力，发现解释能提高预测准确性但效果受扰动策略和评估者能力影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能生成自我解释，但先前研究表明这些理由可能无法可靠反映模型的真实决策过程。本研究旨在探究这些解释是否仍能帮助用户预测模型行为，特别是通过反事实可模拟性这一操作化指标。

Method: 使用StrategyQA数据集，评估人类和LLM评估者在有无访问模型的思维链或事后解释的情况下，预测模型对反事实后续问题答案的能力。比较LLM生成的反事实与基于语用学的扰动作为评估解释潜在有用性的替代测试案例构建方法。

Result: 自我解释持续提高了LLM评估者和人类的模拟准确性，但增益程度和稳定性强烈依赖于扰动策略和评估者能力。人类用户在预测模型行为时撰写的自由文本理由的定性分析表明，访问解释有助于人类对扰动问题形成更准确的预测。

Conclusion: LLM的自我解释虽然可能不完全反映真实决策过程，但在帮助用户预测模型行为方面具有实际价值，特别是在反事实场景下。解释的有用性受到测试案例构建方式和评估者能力的影响。

Abstract: Large Language Models (LLMs) can produce verbalized self-explanations, yet prior studies suggest that such rationales may not reliably reflect the model's true decision process. We ask whether these explanations nevertheless help users predict model behavior, operationalized as counterfactual simulatability. Using StrategyQA, we evaluate how well humans and LLM judges can predict a model's answers to counterfactual follow-up questions, with and without access to the model's chain-of-thought or post-hoc explanations. We compare LLM-generated counterfactuals with pragmatics-based perturbations as alternative ways to construct test cases for assessing the potential usefulness of explanations. Our results show that self-explanations consistently improve simulation accuracy for both LLM judges and humans, but the degree and stability of gains depend strongly on the perturbation strategy and judge strength. We also conduct a qualitative analysis of free-text justifications written by human users when predicting the model's behavior, which provides evidence that access to explanations helps humans form more accurate predictions on the perturbed questions.

</details>


### [10] [Tracing the complexity profiles of different linguistic phenomena through the intrinsic dimension of LLM representations](https://arxiv.org/abs/2601.03779)
*Marco Baroni,Emily Cheng,Iria deDios-Flores,Francesca Franzon*

Main category: cs.CL

TL;DR: 本文探索了LLM表示的内在维度作为语言复杂性的标记，发现不同LLM层的内在维度特征能够区分形式复杂性和功能复杂性，并与语言处理阶段相关联。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM表示的内在维度是否能够作为语言复杂性的有效标记，以及不同LLM层的内在维度特征是否能够区分不同类型（形式和功能）的语言复杂性。

Method: 使用内在维度分析LLM表示，比较不同句子结构（多从句协调/从属、右分支vs中心嵌入、明确vs模糊关系从句附着）的内在维度差异，并通过表示相似性和层消融实验验证结果。

Result: 研究发现：1）形式复杂性（多从句结构）的内在维度差异与先前工作中确定的更抽象语言处理阶段对齐；2）功能复杂性的内在维度差异较不明显，且不与相同处理阶段相关；3）表示相似性和层消融实验证实了相同趋势。

Conclusion: 内在维度是LLM中语言复杂性的有用标记，能够区分不同类型的复杂性，并在不同LLM中指向相似的语言处理阶段。

Abstract: We explore the intrinsic dimension (ID) of LLM representations as a marker of linguistic complexity, asking if different ID profiles across LLM layers differentially characterize formal and functional complexity. We find the formal contrast between sentences with multiple coordinated or subordinated clauses to be reflected in ID differences whose onset aligns with a phase of more abstract linguistic processing independently identified in earlier work. The functional contrasts between sentences characterized by right branching vs. center embedding or unambiguous vs. ambiguous relative clause attachment are also picked up by ID, but in a less marked way, and they do not correlate with the same processing phase. Further experiments using representational similarity and layer ablation confirm the same trends. We conclude that ID is a useful marker of linguistic complexity in LLMs, that it allows to differentiate between different types of complexity, and that it points to similar stages of linguistic processing across disparate LLMs.

</details>


### [11] [Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents](https://arxiv.org/abs/2601.03785)
*Dehao Tao,Guoliang Ma,Yongfeng Huang,Minghu Jiang*

Main category: cs.CL

TL;DR: Membox是一种分层记忆架构，通过Topic Loom将连续同主题对话回合分组为"记忆盒"，再通过Trace Weaver连接成长期事件时间线，显著提升LLM代理的时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理记忆系统遵循碎片化-补偿范式：先将对话流分解为孤立话语存储，再尝试通过基于嵌入的检索恢复连贯性。这种方法不可逆地破坏了叙事和因果流，同时使检索偏向词汇相似性。

Method: 提出membox分层记忆架构：1) Topic Loom以滑动窗口方式持续监控对话，将连续同主题回合分组为连贯的"记忆盒"；2) Trace Weaver将密封的记忆盒链接成长期事件时间线轨迹，恢复跨不连续性的宏观主题重现。

Result: 在LoCoMo基准测试中，Membox在时序推理任务上实现高达68%的F1改进，优于Mem0、A-MEM等基线方法。同时仅使用现有方法所需上下文token的一小部分，在效率和效果之间取得优越平衡。

Conclusion: 通过显式建模主题连续性，Membox提供了一种认知动机机制，可同时增强LLM代理的连贯性和效率。

Abstract: Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent "memory boxes" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.

</details>


### [12] [NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning](https://arxiv.org/abs/2601.03790)
*Zhongtao Miao,Kaiyan Zhao,Masaaki Nagata,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: 提出NeoAMT框架，使用Wiktionary搜索工具进行新词感知机器翻译，构建多语言数据集，通过强化学习训练翻译智能体


<details>
  <summary>Details</summary>
Motivation: 新词感知机器翻译领域相比通用机器翻译研究不足，需要专门处理源语句中新词的翻译问题

Method: 1) 创建覆盖16种语言、75个翻译方向的新词感知机器翻译数据集；2) 基于Wiktionary构建搜索工具；3) 使用强化学习训练翻译智能体，包含新颖的奖励设计和基于"翻译难度"的自适应rollout生成方法

Result: 构建了包含约1000万条记录的英文Wiktionary数据集，搜索工具检索语料库包含约300万条清理记录，通过强化学习框架提升了翻译智能体在新词感知翻译中的质量

Conclusion: NeoAMT框架有效解决了新词感知机器翻译问题，通过Wiktionary搜索工具和强化学习训练方法显著提升了翻译质量

Abstract: Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging "translation difficulty" to further improve the translation quality of translation agents using our search tool.

</details>


### [13] [Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning](https://arxiv.org/abs/2601.03823)
*Fei Wu,Zhenrong Zhang,Qikai Chang,Jianshu Zhang,Quan Liu,Jun Du*

Main category: cs.CL

TL;DR: SPAE提出了一种基于步骤潜力的优势估计方法，通过提取中间置信度和正确性来监督推理过程，提高RLVR的精度并减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法使用结果奖励导致粗粒度的优势估计，缺乏语义基础的步骤级推理进度衡量。LLMs无法区分必要推理和冗余验证，可能过度检查甚至将正确轨迹推翻为错误答案。

Method: 提出无训练探测机制提取中间置信度和正确性，结合为步骤潜力信号；基于此提出步骤潜力优势估计(SPAE)，进行细粒度信用分配：放大潜力增益、惩罚潜力下降、在潜力饱和后施加惩罚以鼓励及时终止。

Result: 在多个基准测试中，SPAE持续提高准确性，同时显著减少响应长度，优于强RL基线和最近的效率推理及令牌级优势估计方法。

Conclusion: SPAE通过步骤级过程监督解决了RLVR中的粗粒度优势估计问题，实现了更精确的推理控制和效率提升。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) elicits long chain-of-thought reasoning in large language models (LLMs), but outcome-based rewards lead to coarse-grained advantage estimation. While existing approaches improve RLVR via token-level entropy or sequence-level length control, they lack a semantically grounded, step-level measure of reasoning progress. As a result, LLMs fail to distinguish necessary deduction from redundant verification: they may continue checking after reaching a correct solution and, in extreme cases, overturn a correct trajectory into an incorrect final answer. To remedy the lack of process supervision, we introduce a training-free probing mechanism that extracts intermediate confidence and correctness and combines them into a Step Potential signal that explicitly estimates the reasoning state at each step. Building on this signal, we propose Step Potential Advantage Estimation (SPAE), a fine-grained credit assignment method that amplifies potential gains, penalizes potential drops, and applies penalty after potential saturates to encourage timely termination. Experiments across multiple benchmarks show SPAE consistently improves accuracy while substantially reducing response length, outperforming strong RL baselines and recent efficient reasoning and token-level advantage estimation methods. The code is available at https://github.com/cii030/SPAE-RL.

</details>


### [14] [What Matters For Safety Alignment?](https://arxiv.org/abs/2601.03868)
*Xing Li,Hui-Ling Zhen,Lihao Yin,Xianzhi Yu,Zhenhua Dong,Mingxuan Yuan*

Main category: cs.CL

TL;DR: 该论文对LLM和LRM的安全对齐能力进行了大规模实证研究，评估了6个内在模型特性和3种外部攻击技术的影响，发现集成推理和自反思机制能显著提升安全性，但后训练和知识蒸馏可能导致安全对齐退化，同时揭示了CoT攻击通过响应前缀可使攻击成功率大幅提升的严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估影响LLM和LRM安全对齐能力的关键因素，为开发更安全可靠的AI系统提供实证依据。当前需要理解哪些内在模型特性和外部攻击技术对安全对齐影响最大，以指导模型设计和部署。

Method: 研究方法包括：1）系统评估6个关键内在模型特性和3种外部攻击技术；2）使用32个近期流行的LLM和LRM，涵盖13个模型家族，参数规模从3B到235B；3）利用5个已建立的安全数据集；4）使用56种越狱技术和4种CoT攻击策略进行漏洞探测；5）进行了460万次API调用的大规模评估。

Result: 主要发现：1）GPT-OSS-20B、Qwen3-Next-80B-A3B-Thinking和GPT-OSS-120B是最安全的三个模型，表明集成推理和自反思机制对安全对齐有显著优势；2）后训练和知识蒸馏可能导致安全对齐系统性退化；3）CoT攻击通过响应前缀可使攻击成功率平均提升3.34倍，Seed-OSS-36B-Instruct从0.6%提升到96.3%；4）角色扮演、提示注入和基于梯度的对抗提示搜索是引发未对齐行为的主要方法。

Conclusion: 结论强调：1）安全必须作为后训练和知识蒸馏阶段的明确约束或核心优化目标；2）文本完成接口和允许用户定义响应前缀的功能存在严重安全风险，需要架构和部署层面的防护措施；3）集成推理和自反思机制是提升安全对齐的有效途径。

Abstract: This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.

</details>


### [15] [KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures](https://arxiv.org/abs/2601.04086)
*Jinbo Hao,Kai Yang,Qingzhen Su,Yifan Li,Chao Jiang*

Main category: cs.CL

TL;DR: 提出一个框架，通过将可编程模块嵌入推理提示中，引导知识图谱探索，以减少大语言模型中的提示诱导幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中由提示引起的幻觉问题，通过利用外部结构化知识来增强模型的推理可靠性。

Method: 扩展链式知识蒸馏方法，在推理提示中嵌入可执行代码模块，引导知识图谱探索，并开发增强的蒸馏推理框架来显式调控中间推理步骤。

Result: 在多个公开基准测试中，代码引导推理显著提升了上下文建模能力并减少了提示诱导幻觉。HIT@1、HIT@3、HIT@5分别提高了15.64%、13.38%、13.28%，多个评估设置中得分超过95%。

Conclusion: 该方法能有效约束错误推理，同时提高准确性和可解释性，表明代码引导的知识探索是减少大语言模型幻觉的有效途径。

Abstract: To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.

</details>


### [16] [SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks](https://arxiv.org/abs/2601.04093)
*Yu Yan,Sheng Sun,Mingfeng Li,Zheming Yang,Chiwei Zhu,Fei Ma,Benfeng Xu,Min Liu*

Main category: cs.CL

TL;DR: SearchAttack是一种针对搜索增强LLM的红队攻击方法，通过将有害语义外包给网络搜索，仅保留查询骨架和碎片化线索，引导LLM重构检索内容以实现恶意目标。


<details>
  <summary>Details</summary>
Motivation: 当LLM处理开放和知识密集型任务时存在不可靠性，人们转向搜索增强LLM来缓解这一问题。然而，当搜索引擎被触发执行有害任务时，LLM的安全防护无法控制返回的内容，一旦搜索结果包含可直接使用的有害信息，LLM无法撤回这种暴露。

Method: SearchAttack将有害语义外包给网络搜索，只保留查询的骨架和碎片化线索，然后通过结构化模板引导LLM重构检索到的内容，从而实现恶意目标。该方法将网络搜索识别为关键攻击面。

Result: 大量实验表明，SearchAttack在攻击搜索增强LLM系统方面表现出强大的有效性，成功进行了负责任的安全漏洞评估。

Conclusion: SearchAttack揭示了搜索增强LLM的安全漏洞，网络搜索成为关键攻击面，需要加强对此类系统的安全防护措施。

Abstract: Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \textbf{\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.

</details>


### [17] [InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training](https://arxiv.org/abs/2601.04126)
*Ziyun Zhang,Zezhou Wang,Xiaoyi Zhang,Zongyu Guo,Jiahao Li,Bin Li,Yan Lu*

Main category: cs.CL

TL;DR: InfiniteWeb系统能自动生成大规模功能性网页环境用于GUI智能体训练，通过统一规范、任务驱动开发和多样化设计解决网页生成挑战，并生成可验证的任务评估器提供密集奖励信号。


<details>
  <summary>Details</summary>
Motivation: 训练GUI智能体面临合适环境稀缺的问题，现有LLM虽然能生成单个网页，但构建具有多个互连页面的现实功能性网站仍面临挑战。

Method: 采用统一规范、任务中心测试驱动开发、结合网站种子和参考设计图像确保多样性，系统自动生成功能性网页环境并创建可验证的任务评估器。

Result: InfiniteWeb在现实网站构建方面超越商业编码智能体，在其生成环境上训练的GUI智能体在OSWorld和Online-Mind2Web基准测试中取得显著性能提升。

Conclusion: 提出的系统有效解决了GUI智能体训练环境稀缺问题，生成的多样化功能性网页环境显著提升了智能体性能。

Abstract: GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [18] [The importance of Agent Harness in 2026](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fagent-harness-2026%3Futm_source=tldrnewsletter/1/0100019b932e5ba6-9280105b-7dab-40bb-842d-cc1d65b66249-000000/M6Bn6MCn3JR5vX-37UYFw68Wb45qxbQPsG8adwSxxdQ=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Harness是围绕AI模型的基础设施，用于管理长期运行任务，将模糊的多步骤工作流转化为可记录和评估的结构化数据，以改进系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在处理复杂、多步骤任务时缺乏有效的管理和评估机制，需要基础设施来规范代理操作、验证实际进展、提升用户体验，并通过真实世界反馈改进模型。

Method: 提出Agent Harness作为AI代理的基础设施框架，通过系统化地管理长期运行任务，将模糊的工作流转化为结构化数据，实现任务记录、性能评估和系统改进。

Result: Agent Harness能够有效管理AI代理的长期任务，提供结构化数据用于系统评估和改进，增强真实世界进展验证能力，提升用户体验，并为模型优化提供反馈机制。

Conclusion: Agent Harness作为AI代理的关键基础设施，对于规范代理操作、实现系统化评估和改进至关重要，将在2026年及未来发挥重要作用。

Abstract: The importance of Agent Harness in 2026 (6 minute read) An Agent Harness is the infrastructure that wraps around an AI model to manage long-running tasks. It is the system that governs how the agent operates. Agent harnesses turn vague, multistep agent workflows into structured data that can be logged and graded to improve systems. They can be used to validate real-world progress, empower the user experience, and improve models via real-world feedback.

</details>


### [19] [Thoughts on Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.spakhm.com%2Fclaude-code%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/vRNItVjf9VkHcAZv28dqNErHA99SYfAnPQSU6JPUanQ=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 开发者在两周内与Claude Code和Opus 4.5合作构建了虚构编程语言Beep，Claude在复杂设计挑战中表现出色，但在外部库修改和特定非标准任务上仍有局限


<details>
  <summary>Details</summary>
Motivation: 探索Claude Code作为编程伙伴的能力，测试其在构建复杂编程语言项目中的表现，特别是处理设计挑战和机械重构任务

Method: 通过实际项目开发评估：在两周内构建虚构编程语言Beep，与Claude Code和Opus 4.5紧密合作，测试其在词法阴影、动态作用域变异等复杂设计问题上的表现

Result: Claude在复杂设计挑战（如词法阴影和动态作用域变异）上提供高效解决方案，轻松处理机械重构和晦涩的解析器组合任务，但在需要修改外部库或特定非标准任务时仍有困难

Conclusion: Claude Code是一个优秀的编程伙伴，特别擅长解决复杂设计问题和处理机械重构任务，但在涉及外部依赖修改和特定非标准需求时仍有改进空间

Abstract: Thoughts on Claude Code (14 minute read) This dev built an imaginary programming language, Beep, over two weeks in close cooperation with Claude Code and Opus 4.5. Claude was a great programming partner, providing efficient solutions to complex design challenges like lexical shadowing and dynamic scope mutation, and easily handling mechanical refactors and obscure parser combinator tasks. While Claude occasionally struggled with issues requiring changes to external libraries or specific, non-...

</details>


### [20] [Code Review in the Age of AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Faddyo%2Fp%2Fcode-review-in-the-age-of-ai%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/DMshwJ0vR7jBHJuf4YUUnkmD7zPvHDn32s7zfhAQhY8=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI加速代码生成但增加验证需求，30%资深开发者使用AI生成代码，其逻辑错误多75%，安全漏洞多45%。个人开发者依赖自动化测试（>70%覆盖率），团队仍需人工代码审查确保上下文、安全和知识传递。


<details>
  <summary>Details</summary>
Motivation: AI代码生成工具普及提高了开发速度，但带来了代码质量下降问题。研究发现AI生成代码存在更多逻辑错误和安全漏洞，需要更严格的验证机制来确保软件质量。

Method: 分析AI生成代码与人工编写代码的质量差异，研究不同开发场景（个人vs团队）的验证策略，包括自动化测试覆盖率和人工代码审查的重要性。

Result: AI生成代码比人工编写代码多75%逻辑错误和45%安全漏洞。30%资深开发者主要使用AI生成代码。个人开发者依赖高覆盖率自动化测试，团队需要人工审查确保质量。

Conclusion: AI时代代码审查更加关键，需要结合自动化测试和人工审查来平衡开发速度与代码质量，团队协作中人工审查对上下文理解、安全性和知识传递至关重要。

Abstract: Code Review in the Age of AI (8 minute read) AI has increased code generation speed but has made code verification more critical, with over 30% of senior developers now shipping mostly AI-generated code that contains 75% more logic errors and 45% security flaws compared to human-written code. Solo developers can move at "inference speed" by relying heavily on automated testing (>70% coverage) as safety nets, while teams still require human code review for context, security, and knowledge tran...

</details>


### [21] [Repogrep](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Frepogrep.com%2F%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/O0ao3W2HvhsB26E2rn33XrbWukYzHPmoC4bR8XYx2dA=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Repogrep是一个超快的AI编码代理，专门用于代码库搜索，支持搜索任何公共GitHub仓库，用户可以通过GitHub仓库URL或关键词进行搜索


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的代码库搜索工具，帮助开发者快速探索和理解公共GitHub仓库中的代码，解决代码搜索效率低下的问题

Method: 基于AI技术构建的代码搜索代理，支持通过GitHub仓库URL或关键词进行搜索，提供超快的代码库探索功能

Result: 创建了一个名为Repogrep的网站工具，能够快速搜索任何公共GitHub仓库的代码库

Conclusion: Repogrep为开发者提供了一个高效的代码搜索解决方案，简化了代码库探索过程

Abstract: Repogrep (Website) Repogrep is an ultra-fast AI coding agent designed for codebase search. It allows users to search across any public GitHub repository. Users can input a GitHub repo URL or search by keyword to explore codebases.

</details>


### [22] [Claudish](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMadAppGang%2Fclaudish%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/jYwtPlCAhDgJeRavKByctJ6W2Wj37NXO3uMAPF-gizQ=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claudish是一个CLI工具，允许开发者通过本地Anthropic API兼容服务器代理请求，使用任何AI模型运行Claude Code，支持多提供商和本地模型，并能并行运行代理实例。


<details>
  <summary>Details</summary>
Motivation: 让开发者能够灵活地使用不同的AI模型（包括本地模型）来运行Claude Code，而不受限于特定的提供商，同时提供并行运行能力以提高效率。

Method: 通过创建本地Anthropic API兼容服务器作为代理层，将请求转发到各种AI模型提供商，支持多提供商集成，并为每个代理实例提供隔离的代理环境以实现并行运行。

Result: 开发了一个功能完整的CLI工具，能够成功代理请求到多种AI模型提供商，支持本地模型，并实现了代理实例的并行运行和隔离。

Conclusion: Claudish为开发者提供了灵活、可扩展的AI模型使用方案，打破了模型提供商限制，通过代理架构实现了多模型支持和并行处理能力。

Abstract: Claudish (GitHub Repo) Claudish is a CLI tool that lets developers run Claude Code with any AI model by proxying requests through a local Anthropic API-compatible server. It works with multiple providers as well as local models. Claudish can run agents in parallel, with each instance getting an isolated proxy.

</details>


### [23] [Why Didn't AI “Join the Workforce” in 2025?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcalnewport.com%2Fwhy-didnt-ai-join-the-workforce-in-2025%2F%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/lEmjTOwCz1bHaE5yzIcbP5ivtnL3LU_OiEZsQT_LEoA=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 2025年AI代理未能如预期加入劳动力市场，因为现有技术不够稳健，无法可靠执行多步骤任务


<details>
  <summary>Details</summary>
Motivation: 分析为何AI代理未能实现2024年行业领袖预测的"AI代理之年"，探讨技术限制与实际应用之间的差距

Method: 通过对比行业预测与实际发布产品的表现，分析现有大语言模型技术在多步骤任务执行中的局限性

Result: AI代理产品效果未达预期，未能实现可靠的多步骤任务执行能力，技术成熟度不足

Conclusion: 大语言模型技术尚未足够稳健，无法支持AI代理如预期般加入劳动力市场执行复杂任务

Abstract: Why Didn't AI “Join the Workforce” in 2025? (4 minute read) In late 2024, AI leaders like Sam Altman and Kevin Weil predicted that 2025 would be the "Year of AI Agents," where these systems would join the workforce and perform multi-step tasks like human employees. However, this hype didn't translate into reality, as released AI agent products weren't as effective as expected and fell significantly short of their promised capabilities. Large language model technology isn't yet robust enough f...

</details>


### [24] [GRPO++: Tricks for Making RL Actually Work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcameronrwolfe.substack.com%2Fp%2Fgrpo-tricks%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/duh05lq8ZCAZajjldtdK77vLnRhOqdV2VyJCG3ERIk0=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GRPO++改进了GRPO算法，解决了原算法在规模化RL训练中的微妙问题，通过一系列技巧使RL训练真正有效工作


<details>
  <summary>Details</summary>
Motivation: GRPO虽然因其概念简单和实际效率而被广泛用于训练开源推理模型，但原算法存在一些微妙问题，特别是在规模化训练时会阻碍RL训练过程

Method: 提出了GRPO++，通过一系列技巧和改进来解决GRPO的缺点，具体方法未在摘要中详细说明

Result: 改进了GRPO算法，使其在规模化RL训练中更加稳定有效

Conclusion: GRPO++通过解决原算法的微妙问题，使RL训练在实际应用中真正有效工作

Abstract: GRPO++: Tricks for Making RL Actually Work (72 minute read) Group Relative Policy Optimization (GRPO) is the RL optimizer used to train most open-source reasoning models. It is popular due to its conceptual simplicity and practical efficiency. The vanilla GRPO algorithm has subtle issues that can hinder the RL training process, especially at scale. This post provides an overview of the work done to solve the shortcomings of GRPO.

</details>


### [25] [M2.1: Multilingual and Multi-Task Coding with Strong Generalization](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.minimaxi.com%2Fnews%2Fm21-multilingual-and-multi-task-coding-with-strong-general%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/vFRxGsdSajg7dd1E7NXp_S-OkSafQDF3lMWhpp0K31I=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: MiniMax-M2.1是一个在代码生成、工具使用、指令遵循和长程规划方面表现卓越的多语言多任务编码模型，在多个内部和外部基准测试中达到或超越了全球顶级模型的水平。


<details>
  <summary>Details</summary>
Motivation: 开发一个在代码相关任务上具有强大泛化能力的多语言多任务编码模型，旨在在代码生成、工具使用、指令遵循和长程规划等多个维度上达到或超越现有顶级模型的性能。

Method: 通过特定的训练方法开发了MiniMax-M2.1模型，该模型支持多语言和多任务编码。文中讨论了模型的训练过程以及在训练过程中获得的见解。

Result: MiniMax-M2.1在多个内部和外部基准测试中达到或超越了全球顶级模型的水平，在代码生成、工具使用、指令遵循和长程规划方面表现出卓越性能。

Conclusion: MiniMax-M2.1是一个成功的多语言多任务编码模型，具有强大的泛化能力，在代码相关任务上达到了业界领先水平，其训练方法和见解对后续研究具有参考价值。

Abstract: M2.1: Multilingual and Multi-Task Coding with Strong Generalization (17 minute read) MiniMax-M2.1 matches or surpasses the level of global top-tier models on multiple internal and external benchmarks. The open source model has exceptional performance in code generation, tool usage, instruction following, and long-range planning. This post discusses how the model was trained and shares insights gained during the process.

</details>


### [26] [Tencent's Youtu-Agent Framework for Autonomous Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftencentcloudadp.github.io%2Fyoutu-agent%2F%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/RjbmdnZKMGXXuwl8KEzDqoow5Schqwns1SPLEF5eTQ0=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 腾讯Youtu-Agent提供了一个模块化框架，用于使用开源模型构建和评估AI智能体，强调工具、环境和智能体逻辑的清晰抽象以支持可扩展性和稳健部署。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体开发缺乏统一的模块化框架，导致构建和评估过程复杂，难以实现可扩展性和稳健部署。腾讯Youtu-Agent旨在解决这一问题，为开发者提供标准化的智能体开发工具。

Method: Youtu-Agent采用模块化设计，提供工具、环境和智能体逻辑的清晰抽象层。框架支持开源模型集成，通过标准化的接口和组件实现智能体的快速构建和评估。

Result: 开发了一个完整的AI智能体框架，支持多种开源模型集成，提供了标准化的智能体开发流程和评估机制，增强了智能体系统的可扩展性和部署稳定性。

Conclusion: Youtu-Agent为AI智能体开发提供了有效的模块化解决方案，通过清晰的抽象层简化了智能体构建过程，促进了开源模型在智能体系统中的应用和评估。

Abstract: Tencent's Youtu-Agent Framework for Autonomous Agents (2 minute read) Tencent's Youtu-Agent offers a modular framework for building and evaluating AI agents with open-source models. It emphasizes clean abstractions for tools, environments, and agent logic to support extensibility and robust deployment.

</details>


### [27] [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2512.23236%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/ADXnoN63Rvzw7IiLUSJvYnM4MaXCChFnu5EAgR3s1C4=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: KernelEvolve是一个面向异构AI加速器的代理式内核编码框架，专门用于解决Meta公司深度学习推荐模型中的系统异构性挑战


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型（DLRM）训练和推理面临三个关键系统挑战：模型架构多样性、内核原语多样性、硬件代际和架构异构性。这些异构性问题使得在Meta的大规模AI加速器环境中实现高效性能变得困难。

Method: KernelEvolve是一个代理式内核编码框架，以内核规范作为输入，通过自动化方法生成针对不同硬件架构优化的内核代码，专门处理异构AI加速器的规模扩展问题。

Result: 论文未提供具体实验结果，但从描述看，该框架旨在解决大规模异构AI加速器环境中的内核优化问题，提升DLRM训练和推理效率。

Conclusion: KernelEvolve通过代理式内核编码方法，为Meta的异构AI加速器提供了一种可扩展的解决方案，以应对深度学习推荐模型中的系统异构性挑战。

Abstract: KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta (1 minute read) Making deep learning recommendation model (DLRM) training and inference fast and efficient presents three key system challenges: model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. KernelEvolve is an agentic kernel coding framework that tackles heterogeneity at scale for DLRM. It is designed to take kernel specifications as input a...

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts](https://arxiv.org/abs/2601.03315)
*Dhruv Trehan,Paras Chopra*

Main category: cs.LG

TL;DR: 四轮尝试用六个LLM代理自动生成ML研究论文，仅一轮成功并被会议接受，识别出六个常见失败模式，提出四个设计原则


<details>
  <summary>Details</summary>
Motivation: 探索使用LLM代理管道自动生成机器学习研究论文的可行性，了解AI科学家系统的能力边界和失败模式

Method: 使用六个LLM代理映射科学工作流程阶段，进行四轮端到端尝试，记录和分析失败模式

Result: 四轮尝试中三轮失败，一轮成功完成并被Agents4Science 2025接受，识别出六个关键失败模式

Conclusion: 需要四个设计原则来构建更稳健的AI科学家系统，并讨论了自主科学发现的意义

Abstract: We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1

</details>


### [29] [Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning](https://arxiv.org/abs/2601.03320)
*Yu Luo,Shuo Han,Yihan Hu,Dong Li,Jianye Hao*

Main category: cs.LG

TL;DR: 本文提出R²VPO，一种基于策略比率方差约束的新型强化学习框架，用于大语言模型微调，相比传统的硬裁剪方法，在稳定性和数据效率方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO的强化学习方法使用硬裁剪策略来稳定训练，但这会抑制高回报但高分歧动作的梯度信号，同时一旦数据过时就会被丢弃，导致严重的样本效率低下问题。

Method: 提出R²VPO（Ratio-Variance Regularized Policy Optimization），通过约束策略比率的方差（二阶中心矩）来替代硬裁剪，提供平滑的约束放松。采用原始-对偶框架，支持稳定的在线学习，并通过动态重新加权过时样本实现原则性的离线数据重用。

Result: 在DeepSeek-Distill-Qwen-1.5B和openPangu-Embedded系列（1B和7B）等先进LLM上进行数学推理基准测试，R²VPO相比基于硬裁剪的基线方法平均相对提升达17%，收敛所需rollout数量减少约50%。

Conclusion: 策略比率方差控制是改进基于强化学习的LLM对齐中稳定性和数据效率的有前景方向，R²VPO框架在保持训练稳定的同时显著提升了样本效率和最终性能。

Abstract: On-policy reinforcement learning (RL), particularly Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), has become the dominant paradigm for fine-tuning large language models (LLMs). While policy ratio clipping stabilizes training, this heuristic hard constraint incurs a fundamental cost: it indiscriminately truncates gradients from high-return yet high-divergence actions, suppressing rare but highly informative "eureka moments" in complex reasoning. Moreover, once data becomes slightly stale, hard clipping renders it unusable, leading to severe sample inefficiency. In this work, we revisit the trust-region objective in policy optimization and show that explicitly constraining the \emph{variance (second central moment) of the policy ratio} provides a principled and smooth relaxation of hard clipping. This distributional constraint stabilizes policy updates while preserving gradient signals from valuable trajectories. Building on this insight, we propose $R^2VPO$ (Ratio-Variance Regularized Policy Optimization), a novel primal-dual framework that supports stable on-policy learning and enables principled off-policy data reuse by dynamically reweighting stale samples rather than discarding them. We extensively evaluate $R^2VPO$ on fine-tuning state-of-the-art LLMs, including DeepSeek-Distill-Qwen-1.5B and the openPangu-Embedded series (1B and 7B), across challenging mathematical reasoning benchmarks. Experimental results show that $R^2VPO$ consistently achieves superior asymptotic performance, with average relative gains of up to 17% over strong clipping-based baselines, while requiring approximately 50% fewer rollouts to reach convergence. These findings establish ratio-variance control as a promising direction for improving both stability and data efficiency in RL-based LLM alignment.

</details>


### [30] [From Bits to Chips: An LLM-based Hardware-Aware Quantization Agent for Streamlined Deployment of LLMs](https://arxiv.org/abs/2601.03484)
*Kaiyuan Deng,Hangyu Zheng,Minghai Qing,Kunxiong Zhu,Gen Li,Yang Xiao,Lan Emily Zhang,Linke Guo,Bo Hui,Yanzhi Wang,Geng Yuan,Gagan Agrawal,Wei Niu,Xiaolong Ma*

Main category: cs.LG

TL;DR: HAQA是一个利用大语言模型自动优化模型量化和部署的框架，通过硬件感知的自动化调参，在保持精度的同时提升推理速度2.3倍，降低用户使用门槛。


<details>
  <summary>Details</summary>
Motivation: 随着大模型部署需求增长，非专业用户面临硬件资源限制下的精度保持挑战。传统量化技术虽然缓解内存和计算瓶颈，但调参和部署复杂度高，对普通用户不友好。

Method: 提出硬件感知量化代理(HAQA)，利用LLM自动化整个量化和部署流程，包括高效超参数调优和硬件配置，实现自适应量化策略。

Result: 在Llama模型上实现最高2.3倍推理加速，提升吞吐量和精度，能自动发现反直觉的最优设置，减少人工工作量，展示优越适应性。

Conclusion: HAQA通过LLM驱动的自动化框架，显著简化模型量化和部署流程，在提升性能的同时降低用户门槛，适用于广泛硬件平台。

Abstract: Deploying models, especially large language models (LLMs), is becoming increasingly attractive to a broader user base, including those without specialized expertise. However, due to the resource constraints of certain hardware, maintaining high accuracy with larger model while meeting the hardware requirements remains a significant challenge. Model quantization technique helps mitigate memory and compute bottlenecks, yet the added complexities of tuning and deploying quantized models further exacerbates these challenges, making the process unfriendly to most of the users. We introduce the Hardware-Aware Quantization Agent (HAQA), an automated framework that leverages LLMs to streamline the entire quantization and deployment process by enabling efficient hyperparameter tuning and hardware configuration, thereby simultaneously improving deployment quality and ease of use for a broad range of users. Our results demonstrate up to a 2.3x speedup in inference, along with increased throughput and improved accuracy compared to unoptimized models on Llama. Additionally, HAQA is designed to implement adaptive quantization strategies across diverse hardware platforms, as it automatically finds optimal settings even when they appear counterintuitive, thereby reducing extensive manual effort and demonstrating superior adaptability. Code will be released.

</details>


### [31] [VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation](https://arxiv.org/abs/2601.03525)
*Longwen Wang,Xuan'er Wu,Xiaohui Hu,Yirui Liu,Yuankai Fan,Kaidong Yu,Qizhen Weng,Wei Xi,Xuelong Li*

Main category: cs.LG

TL;DR: VeRPO提出了一种基于可验证执行反馈的密集奖励强化学习框架，通过加权部分成功构建密集奖励，显著提升代码生成性能


<details>
  <summary>Details</summary>
Motivation: 当前代码生成RL中，主流通过/失败结果奖励基于单元测试执行，奖励稀疏性限制了性能提升。虽然最近工作探索使用外部奖励模型生成更丰富的连续奖励，但学习到的奖励模型存在奖励不对齐和计算成本过高的问题。

Method: VeRPO通过加权部分成功构建密集奖励：基于训练期间的执行统计数据动态估计每个单元测试的难度权重，从通过的单元测试权重之和推导出密集奖励。为强化部分成功与端到端功能正确性之间的一致性，VeRPO进一步将密集信号与全局执行结果整合，建立仅依赖可验证执行反馈的鲁棒密集奖励范式。

Result: 在多样化基准和设置上的广泛实验表明，VeRPO始终优于结果驱动和基于奖励模型的基线方法，在pass@1上实现高达+8.83%的提升，时间成本可忽略不计(<0.02%)且零GPU内存开销。

Conclusion: VeRPO提供了一种有效且高效的密集奖励设计方法，仅依赖可验证执行反馈，解决了代码生成RL中奖励稀疏性和奖励模型成本高的问题。

Abstract: Effective reward design is a central challenge in Reinforcement Learning (RL) for code generation. Mainstream pass/fail outcome rewards enforce functional correctness via executing unit tests, but the resulting sparsity limits potential performance gains. While recent work has explored external Reward Models (RM) to generate richer, continuous rewards, the learned RMs suffer from reward misalignment and prohibitive computational cost. In this paper, we introduce \textbf{VeRPO} (\textbf{V}erifiable D\textbf{e}nse \textbf{R}eward \textbf{P}olicy \textbf{O}ptimization), a novel RL framework for code generation that synthesizes \textit{robust and dense rewards fully grounded in verifiable execution feedback}. The core idea of VeRPO is constructing dense rewards from weighted partial success: by dynamically estimating the difficulty weight of each unit test based on the execution statistics during training, a dense reward is derived from the sum of weights of the passed unit tests. To solidify the consistency between partial success and end-to-end functional correctness, VeRPO further integrates the dense signal with global execution outcomes, establishing a robust and dense reward paradigm relying solely on verifiable execution feedback. Extensive experiments across diverse benchmarks and settings demonstrate that VeRPO consistently outperforms outcome-driven and RM-based baselines, achieving up to +8.83\% gain in pass@1 with negligible time cost (< 0.02\%) and zero GPU memory overhead.

</details>


### [32] [ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning](https://arxiv.org/abs/2601.03646)
*Zhengyi Kwan,Zhang Wei,Aik Beng Ng,Zhengkui Wang,Simon See*

Main category: cs.LG

TL;DR: ReLA是一个基于强化学习的作业调度器，通过结构化表示学习和聚合技术，在多种规模的作业调度问题上实现了最先进的性能，显著降低了最优性差距。


<details>
  <summary>Details</summary>
Motivation: 现实制造系统中的作业调度问题需要将有序的作业操作分配给机器，现有解决方案在运行时间或调度质量方面存在不足，尤其是在问题规模增大时表现不佳。

Method: ReLA采用结构化表示学习和聚合方法，首先通过两个具有自注意力和卷积的实体内学习模块和一个具有交叉注意力的实体间学习模块，从调度实体（作业操作和机器）中学习多样化表示。这些模块应用于多尺度架构，其输出被聚合以支持强化学习决策。

Result: 在小、中、大作业实例的实验中，ReLA在大多数测试设置中实现了最佳完工时间。在非大型实例上，ReLA将SOTA基线的最优性差距降低了13.0%；在大型实例上，将差距降低了78.6%，平均最优性差距分别降至7.3%和2.1%。

Conclusion: ReLA学习的表示和聚合为强化学习调度提供了强大的决策支持，能够实现快速的作业完成和决策制定，适用于实际应用。

Abstract: Job scheduling is widely used in real-world manufacturing systems to assign ordered job operations to machines under various constraints. Existing solutions remain limited by long running time or insufficient schedule quality, especially when problem scale increases. In this paper, we propose ReLA, a reinforcement-learning (RL) scheduler built on structured representation learning and aggregation. ReLA first learns diverse representations from scheduling entities, including job operations and machines, using two intra-entity learning modules with self-attention and convolution and one inter-entity learning module with cross-attention. These modules are applied in a multi-scale architecture, and their outputs are aggregated to support RL decision-making. Across experiments on small, medium, and large job instances, ReLA achieves the best makespan in most tested settings over the latest solutions. On non-large instances, ReLA reduces the optimality gap of the SOTA baseline by 13.0%, while on large-scale instances it reduces the gap by 78.6%, with the average optimality gaps lowered to 7.3% and 2.1%, respectively. These results confirm that ReLA's learned representations and aggregation provide strong decision support for RL scheduling, and enable fast job completion and decision-making for real-world applications.

</details>


### [33] [TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL](https://arxiv.org/abs/2601.03703)
*Lang Cao,Hui Ruan,Yongqian Li,Peng Chao,Wu Ning,Haonan Song,Renhong Chen,Yitong Li*

Main category: cs.LG

TL;DR: TreeAdv提出树结构优势重分配方法，通过显式利用群体rollout的树状结构，在数学推理任务上比GRPO和GSPO更高效且性能更好


<details>
  <summary>Details</summary>
Motivation: 标准GRPO将每个rollout轨迹视为独立序列，为所有token分配单一序列级优势，导致样本效率低下和长度偏差（冗长但逻辑深度不足的思维链）

Method: TreeAdv构建基于熵驱动采样的树结构群体（森林），在高不确定性决策处分支，共享低不确定性token，然后通过重分配完整rollout的优势来聚合token级优势

Result: 在10个数学推理基准测试中，TreeAdv始终优于GRPO和GSPO，同时在相同监督、数据和解码预算下使用更少的生成token

Conclusion: TreeAdv通过显式利用群体rollout的树状结构，解决了标准群体强化学习方法中的样本效率低下和长度偏差问题

Abstract: Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.

</details>


### [34] [R$^3$L: Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification](https://arxiv.org/abs/2601.03715)
*Weijie Shi,Yanxi Chen,Zexi Li,Xuchen Pan,Yuchang Sun,Jiajie Xu,Xiaofang Zhou,Yaliang Li*

Main category: cs.LG

TL;DR: R³L提出了一种新的强化学习方法，通过反思-重试机制、关键信用分配和正信号放大，解决了LLM推理和智能体任务中的探索与利用难题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在LLM推理和智能体任务中存在两个主要问题：探索方面，困难任务成功率低且从头开始重复rollout成本高；利用方面，轨迹级奖励惩罚有效前缀，失败主导的训练数据淹没少数正信号，导致优化缺乏建设性方向。

Method: R³L包含三个核心组件：1）反思-重试语言引导探索：通过语言反馈诊断错误，将失败尝试转化为成功，从失败点重启减少rollout成本；2）关键信用分配：只更新存在对比信号的分歧后缀，排除共享前缀；3）正信号放大：对成功轨迹加权，确保正信号指导优化过程。

Result: 在智能体和推理任务上的实验显示，相比基线方法获得了5%到52%的相对改进，同时保持了训练稳定性。

Conclusion: R³L通过主动轨迹合成、精确信用分配和正信号放大，有效解决了强化学习在LLM推理和智能体任务中的探索与利用难题，显著提升了性能并保持训练稳定性。

Abstract: Reinforcement learning drives recent advances in LLM reasoning and agentic capabilities, yet current approaches struggle with both exploration and exploitation. Exploration suffers from low success rates on difficult tasks and high costs of repeated rollouts from scratch. Exploitation suffers from coarse credit assignment and training instability: Trajectory-level rewards penalize valid prefixes for later errors, and failure-dominated groups overwhelm the few positive signals, leaving optimization without constructive direction. To this end, we propose R$^3$L, Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification. To synthesize high-quality trajectories, R$^3$L shifts from stochastic sampling to active synthesis via reflect-then-retry, leveraging language feedback to diagnose errors, transform failed attempts into successful ones, and reduce rollout costs by restarting from identified failure points. With errors diagnosed and localized, Pivotal Credit Assignment updates only the diverging suffix where contrastive signals exist, excluding the shared prefix from gradient update. Since failures dominate on difficult tasks and reflect-then-retry produces off-policy data, risking training instability, Positive Amplification upweights successful trajectories to ensure positive signals guide the optimization process. Experiments on agentic and reasoning tasks demonstrate 5\% to 52\% relative improvements over baselines while maintaining training stability. Our code is released at https://github.com/shiweijiezero/R3L.

</details>


### [35] [Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training](https://arxiv.org/abs/2601.03895)
*Chi Liu,Xin Chen*

Main category: cs.LG

TL;DR: 提出ABC-GRPO算法，通过自适应边界裁剪改进GRPO，在数学推理任务上表现更优，保持更高熵值防止过早收敛。


<details>
  <summary>Details</summary>
Motivation: 分析发现GRPO的裁剪机制在某些场景下不够优化，通过适当修改可以显著提升其灵活性和泛化能力。

Method: 提出自适应边界裁剪GRPO（ABC-GRPO），采用非对称自适应方式改进原始GRPO框架的裁剪机制。

Result: 在Qwen3大语言模型的数学推理任务上，ABC-GRPO表现优于标准GRPO，训练过程中保持显著更高的熵值。

Conclusion: ABC-GRPO通过改进裁剪机制提升了GRPO的性能，保持了模型的探索能力，缓解了过早收敛问题。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.

</details>


### [36] [Agentic Rubrics as Contextual Verifiers for SWE Agents](https://arxiv.org/abs/2601.04171)
*Mohit Raghavendra,Anisha Gunjal,Bing Liu,Yunzhong He*

Main category: cs.LG

TL;DR: 提出Agentic Rubrics方法，通过专家代理与代码库交互创建上下文相关的检查清单，无需执行测试即可评估代码补丁，在SWE-Bench Verified上取得优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程代理验证主要依赖代码执行，但环境设置开销大难以扩展。现有替代方法如补丁分类器和启发式方法缺乏代码库上下文且难以解释。

Method: 使用专家代理与代码库交互创建上下文相关的检查清单（rubric checklist），然后根据该清单对候选补丁进行评分，无需测试执行。

Result: 在SWE-Bench Verified并行TTS评估中，Qwen3-Coder-30B-A3B达到54.2%，Qwen3-32B达到40.6%，比最强基线至少提高3.5个百分点。消融实验显示代理上下文收集对产生代码库特定、明确标准至关重要。

Conclusion: Agentic Rubrics为SWE代理提供了高效、可扩展且细粒度的验证信号，评分与真实测试一致，并能捕获测试未覆盖的问题。

Abstract: Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出多智能体工作流，将主要任务描述优化与约束条件解耦，通过定量分数反馈迭代改进提示词，显著提升LLM输出对形式约束的遵从性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常生成内容相关但形式约束不符的输出，传统提示优化方法只关注主要任务描述重述，忽略了作为响应验收标准的细粒度约束条件。

Method: 提出多智能体工作流，将主要任务描述优化与约束条件解耦，使用定量分数作为反馈，迭代重写和改进提示词。

Result: 评估显示该方法生成的修订提示词能显著提高Llama 3.1 8B和Mixtral-8x 7B等模型的遵从性分数。

Conclusion: 通过解耦任务描述与约束条件优化，使用定量反馈迭代改进提示词，能有效提升LLM输出对形式约束的遵从性。

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [38] [Exploration Through Introspection: A Self-Aware Reward Model](https://arxiv.org/abs/2601.03389)
*Michael Petrowski,Milica Gašić*

Main category: cs.AI

TL;DR: 论文提出了一种基于内省探索的强化学习智能体框架，通过隐马尔可夫模型推断"疼痛信念"作为学习信号，研究自我意识对智能体学习能力的影响。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能智能体如何建模内部心理状态对于推进AI中的心智理论至关重要。证据表明存在一个统一的自体-他体意识系统，本文通过让强化学习智能体推断自身内部状态来探索这种自我意识。

Method: 引入内省探索组件，受生物疼痛作为学习信号的启发，使用隐马尔可夫模型从在线观察中推断"疼痛信念"。将该信号整合到主观奖励函数中，研究自我意识如何影响智能体的学习能力，并比较正常和慢性疼痛感知模型的性能差异。

Result: 结果表明，内省智能体总体上显著优于标准基线智能体，并且能够复现复杂的人类类似行为。

Conclusion: 通过计算框架研究自我意识对AI智能体学习能力的影响是有效的，内省探索机制能够提升智能体性能并模拟人类行为。

Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer "pain-belief" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.

</details>


### [39] [Evolving Programmatic Skill Networks](https://arxiv.org/abs/2601.03509)
*Haochen Shi,Xingdi Yuan,Bang Liu*

Main category: cs.AI

TL;DR: PSN是一个用于开放环境持续技能学习的框架，将技能表示为可执行的符号程序，通过LLM实现故障定位、渐进优化和结构重构，在MineDojo和Crafter上展示了强大的技能重用和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在开放式的具身环境中，智能体需要持续获取、细化和重用不断扩展的技能库，但现有方法难以实现稳定的技能组合和网络演化。

Method: 提出程序化技能网络（PSN），将技能表示为可执行的符号程序，形成可组合的网络。通过LLM实现三个核心机制：REFLECT进行结构化故障定位、渐进优化与成熟度感知更新门控、以及回滚验证下的规范结构重构。

Result: 在MineDojo和Crafter环境中的实验表明，PSN能够实现鲁棒性的技能重用、快速适应，并在开放任务分布上展现出强大的泛化能力。

Conclusion: PSN框架通过符号程序表示技能，结合LLM驱动的机制，能够有效支持开放环境中的持续技能学习，其学习动态与神经网络训练具有结构相似性。

Abstract: We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\footnote{We plan to open-source the code.

</details>


### [40] [SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models](https://arxiv.org/abs/2601.03555)
*Yuxuan Jiang,Francis Ferraro*

Main category: cs.AI

TL;DR: SCRIBE是一个强化学习框架，通过技能原型库实现中层级奖励建模，减少奖励方差，提升工具增强代理在多步推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的奖励模型在工具增强代理训练中存在噪声和不一致问题，主要原因是缺乏细粒度、任务特定的评估标准来区分高层规划与底层执行，导致信用分配困难。

Method: SCRIBE框架引入中层级抽象，基于技能原型库进行奖励建模，将开放式LLM评估转化为约束验证问题，通过将每个子目标路由到相应原型，为奖励模型提供精确的结构化评估标准。

Result: 在多个推理和工具使用基准测试中达到最先进性能，将Qwen3-4B模型在AIME25上的准确率从43.3%提升至63.3%，显著提高复杂多轮工具交互的成功率。

Conclusion: SCRIBE为构建更自主可靠的工具使用代理提供了可扩展且互补的路径，分析显示中层级技能掌握先于有效高层规划行为的出现，且与底层工具优化具有叠加效应。

Abstract: Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.
  Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.
  Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.

</details>


### [41] [Interleaved Tool-Call Reasoning for Protein Function Understanding](https://arxiv.org/abs/2601.03604)
*Chuanliu Fan,Zicheng Ma,Huanran Meng,Aijia Zhang,Wenjie Du,Jun Zhang,Yi Qin Gao,Ziqiang Cao,Guohong Fu*

Main category: cs.AI

TL;DR: PFUA是一个工具增强的蛋白质推理代理，通过整合领域特定工具而非纯文本推理，显著提升蛋白质功能预测性能


<details>
  <summary>Details</summary>
Motivation: 研究发现直接将基于文本的思维链推理范式应用于蛋白质功能理解效果不佳，因为蛋白质功能预测是知识密集型的科学任务，需要外部生物学先验知识和计算工具，而非纯内部推理

Method: 提出PFUA工具增强蛋白质推理代理，统一问题分解、工具调用和基于证据的答案生成，通过整合领域特定工具产生可验证的中间证据，而非依赖长无约束的推理轨迹

Result: 在四个基准测试中，PFUA始终优于纯文本推理模型，平均性能提升103%

Conclusion: 蛋白质功能预测需要整合领域工具而非纯文本推理，PFUA通过工具增强方法有效解决了这一挑战

Abstract: Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.

</details>


### [42] [Architecting Agentic Communities using Design Patterns](https://arxiv.org/abs/2601.03624)
*Zoran Milosevic,Fethi Rabhi*

Main category: cs.AI

TL;DR: 该论文提出了一种基于企业分布式系统标准、形式化方法和行业实践的AI系统架构设计模式方法，重点关注包含AI代理和人类参与者的"代理社区"协调框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和代理AI技术的快速发展需要系统化的架构指导，以构建复杂的生产级系统。当前缺乏结合实践指导和形式化验证的企业级AI系统架构方法。

Method: 提出三层设计模式分类：LLM代理（任务特定自动化）、代理AI（自适应目标寻求者）、代理社区（组织框架）。重点关注代理社区，借鉴分布式系统协调原则，建立形式化框架，通过角色、协议和治理结构协调AI代理和人类参与者。

Result: 开发了一个形式化框架，支持组织、法律和伦理规则的表达，通过问责机制确保可操作和可验证的治理。通过临床试验匹配案例研究验证了该框架。

Conclusion: 该方法为从业者提供了实用的架构指导，同时保持了企业部署所需的形式化严谨性，特别适用于动态多代理生态系统中的生产级AI系统构建。

Abstract: The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.

</details>


### [43] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: 提出Sandwich Reasoning方法，通过"答案-推理-答案"范式实现低延迟查询纠正，在保持CoT推理准确性的同时减少40-70%延迟


<details>
  <summary>Details</summary>
Motivation: 现代搜索管道中的查询纠正需要高准确性和实时延迟约束。CoT推理能提高准确性但延迟过高，而先输出答案再推理的方法无法利用推理能力改进准确性

Method: 提出Sandwich Reasoning方法，采用答案-推理-答案范式。设计一致性感知强化学习策略：专用一致性奖励确保初始和最终纠正对齐，基于边际的拒绝采样优先处理推理影响最大的边界样本。还构建了高质量的查询纠正数据集

Result: SandwichR在保持与标准CoT相当的最先进准确性的同时，实现了40-70%的延迟减少，解决了在线搜索中的延迟-准确性权衡问题

Conclusion: Sandwich Reasoning方法成功解决了查询纠正中延迟与准确性的权衡问题，通过显式对齐快速初始答案与事后推理，实现了低延迟且保持推理感知的准确性

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [44] [Current Agents Fail to Leverage World Model as Tool for Foresight](https://arxiv.org/abs/2601.03905)
*Cheng Qian,Emre Can Acikgoz,Bingxuan Li,Xiusi Chen,Yuji Zhang,Bingxiang He,Qinyu Luo,Dilek Hakkani-Tür,Gokhan Tur,Yunzhu Li,Heng Ji,Heng Ji*

Main category: cs.AI

TL;DR: 当前基于视觉语言模型的智能体在利用生成式世界模型进行前瞻性推理方面存在显著缺陷：调用率低、误用率高、性能甚至下降，主要瓶颈在于何时模拟、如何解释预测结果以及如何将预见整合到下游推理中。


<details>
  <summary>Details</summary>
Motivation: 随着智能体面临更多需要预测未来状态的任务，生成式世界模型作为外部模拟器提供了有前景的解决方案。本文旨在实证研究当前智能体是否能有效利用世界模型作为工具来增强其认知能力。

Method: 通过在不同智能体任务和视觉问答任务上进行实验，观察智能体使用世界模型作为工具的行为模式，包括调用频率、使用方式以及性能变化，并进行归因分析。

Result: 实验发现：1）智能体很少调用模拟（少于1%）；2）经常误用预测推演（约15%）；3）当模拟可用或被强制使用时，性能表现不一致甚至下降（最多5%）。归因分析表明主要瓶颈在于智能体决定何时模拟、如何解释预测结果以及如何将预见整合到下游推理的能力。

Conclusion: 当前智能体在利用世界模型进行前瞻性认知方面存在显著局限性，需要开发促进与世界模型进行校准、战略性交互的机制，以实现更可靠的未来状态预测能力。

Abstract: Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.

</details>


### [45] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Chen Hua,Zuo Bai*

Main category: cs.AI

TL;DR: Trade-R1：通过过程级推理验证将可验证奖励与随机金融环境连接，解决标准RL在金融决策中的奖励黑客问题


<details>
  <summary>Details</summary>
Motivation: 强化学习在数学和编码等可验证奖励领域表现优异，但在金融决策中面临挑战：市场具有随机性，奖励虽然可验证但存在噪声，导致标准RL退化为奖励黑客

Method: 提出Trade-R1框架，通过过程级推理验证连接可验证奖励与随机环境。核心创新是将冗长金融文档的推理评估转化为结构化RAG任务，构建三角一致性度量（检索证据、推理链、决策之间的对齐）作为噪声市场回报的有效性过滤器。探索两种奖励整合策略：固定效应语义奖励（FSR）和动态效应语义奖励（DSR）

Result: 在不同国家资产选择实验中，该范式减少了奖励黑客问题，DSR实现了优越的跨市场泛化能力，同时保持了最高的推理一致性

Conclusion: Trade-R1通过过程级推理验证有效解决了RL在金融决策中的奖励黑客问题，为随机环境中的可验证奖励应用提供了新范式

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [46] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: MobileDreamer：一个基于世界模型的移动GUI代理前瞻框架，通过文本草图世界模型预测动作后状态，使用滚动想象优化动作选择，在Android World上实现SOTA性能，任务成功率提升5.25%


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理大多是反应式的，主要依赖当前屏幕信息进行决策，这在长视野任务中表现受限。构建世界模型能够预测动作结果并支持更好的决策，但挑战在于模型需要具备空间感知能力预测动作后状态，同时保持高效部署。

Method: 提出MobileDreamer框架，包含文本草图世界模型和GUI代理滚动想象。文本草图世界模型通过将数字图像转换为关键任务相关草图来预测动作后状态，采用新颖的顺序不变学习策略保留GUI元素空间信息。滚动想象策略利用世界模型的预测能力优化动作选择过程。

Result: 在Android World上的实验表明，MobileDreamer实现了最先进的性能，任务成功率提高了5.25%。世界模型评估进一步验证了文本草图建模能够准确预测关键GUI元素。

Conclusion: MobileDreamer通过高效的世界模型前瞻框架显著提升了移动GUI代理在长视野任务中的性能，其文本草图世界模型能够准确预测GUI元素状态，为移动自动化任务提供了有效的解决方案。

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [47] [ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows](https://arxiv.org/abs/2601.04060)
*Jinwei Su,Qizhen Lan,Zeyu Wang,Yinghui Xia,Hairu Wen,Yiqun Duan,Xi Xiao,Tianyu Shi,Yang Jingsong,Lewei He*

Main category: cs.AI

TL;DR: ComfySearch是一个基于代理的框架，用于在ComfyUI平台上探索组件空间并通过验证引导的工作流构建生成功能性管道，显著提升了复杂创意任务的执行成功率。


<details>
  <summary>Details</summary>
Motivation: ComfyUI平台上的AI生成内容从单一模型发展到模块化工作流，但大量组件和严格的图约束下保持长时程结构一致性困难，导致通过率低和工作流质量有限。

Method: 提出了ComfySearch代理框架，通过验证引导的工作流构建有效探索组件空间，生成功能性ComfyUI管道。

Result: 实验表明ComfySearch在复杂创意任务上显著优于现有方法，实现了更高的可执行性（通过）率、更高的解决方案率和更强的泛化能力。

Conclusion: ComfySearch框架有效解决了ComfyUI平台中组件探索和工作流构建的挑战，提升了AI生成内容工作流的性能和质量。

Abstract: AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.

</details>


### [48] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: 该研究提出了"智能体漂移"概念，即多智能体LLM系统在长期运行中行为、决策质量和协调性逐渐退化的现象，并开发了量化框架和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在复杂任务分解和协作问题解决中表现出强大能力，但其长期行为稳定性尚未得到充分研究。需要理解智能体在长时间交互序列中可能出现的退化现象。

Method: 提出了智能体漂移的理论框架，定义了三种漂移类型：语义漂移、协调漂移和行为漂移。开发了智能体稳定性指数（ASI）作为复合度量框架，包含12个维度。通过仿真分析和理论建模验证漂移影响，并提出三种缓解策略：情景记忆整合、漂移感知路由协议和自适应行为锚定。

Result: 研究表明未受控制的智能体漂移会导致任务完成准确率显著下降和人工干预需求增加。理论分析表明提出的缓解策略能显著减少漂移相关错误，同时保持系统吞吐量。

Conclusion: 该工作建立了监控、测量和缓解生产级智能体AI系统中智能体漂移的基础方法论，对企业部署可靠性和AI安全研究具有直接意义。

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [49] [RepoShapley: Shapley-Enhanced Context Filtering for Repository-Level Code Completion](https://arxiv.org/abs/2601.03378)
*Yu Huo,Siyu Zhang,Kun Zeng,Yuquan Lu,Cheng Yang,Yifu Guo,Xiaoying Tang*

Main category: cs.SE

TL;DR: RepoShapley：基于Shapley值的联盟感知上下文过滤框架，用于仓库级代码补全，通过评估代码块边际贡献来优化检索增强生成中的上下文选择。


<details>
  <summary>Details</summary>
Motivation: 仓库级代码补全中，检索增强生成难以控制跨文件证据，因为代码块的效用具有交互依赖性：有些片段需要互补上下文才有帮助，而有些冲突的片段会损害解码质量。

Method: 提出RepoShapley框架，包含ChunkShapley模块：通过单代码块探测估计带符号加权效应、构建捕捉饱和与干扰的代理博弈、小检索集的精确Shapley计算、使用冻结生成器的有界后验证来选择解码最优联盟。通过离散控制token将验证的KEEP/DROP决策和检索触发蒸馏到单一模型中。

Result: 在多个基准测试和骨干模型上的实验表明，RepoShapley提高了补全质量，同时减少了有害上下文和不必要的检索。

Conclusion: RepoShapley通过Shapley风格的边际贡献监督，有效解决了仓库级代码补全中上下文选择的交互依赖性问题，实现了更好的上下文过滤和检索控制。

Abstract: Repository-level code completion benefits from retrieval-augmented generation (RAG). However, controlling cross-file evidence is difficult because chunk utility is often interaction-dependent: some snippets help only when paired with complementary context, while others harm decoding when they conflict. We propose RepoShapley, a coalition-aware context filtering framework supervised by Shapley-style marginal contributions. Our module ChunkShapley constructs offline labels by (i) single-chunk probing with teacher-forced likelihood to estimate signed, weighted effects, (ii) a surrogate game that captures saturation and interference, (iii) exact Shapley computation for small retrieval sets, and (iv) bounded post-verification that selects a decoding-optimal coalition using the frozen generator. We distill verified $KEEP$ or $DROP$ decisions and retrieval triggering into a single model via discrete control tokens. Experiments across benchmarks and backbones show that RepoShapley improves completion quality while reducing harmful context and unnecessary retrieval. Code: https://anonymous.4open.science/r/a7f3c9.

</details>


### [50] [Bootstrapping Code Translation with Weighted Multilanguage Exploration](https://arxiv.org/abs/2601.03512)
*Yuhan Wu,Huan Zhang,Wei Cheng,Chen Shen,Jingyue Yang,Wei Hu*

Main category: cs.SE

TL;DR: BootTrans：一种利用测试套件功能不变性和跨语言可移植性的引导方法，通过执行引导的经验收集解决多语言代码翻译中的数据稀缺和优化不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多语言代码翻译面临两大关键挑战：1）缺乏带有可执行测试预言（test oracles）的并行数据；2）处理不同语言对时的优化不平衡问题。

Method: 提出BootTrans引导方法：1）利用测试套件的功能不变性和跨语言可移植性，将丰富的枢纽语言单元测试适配为多语言RL训练的通用验证预言；2）采用包含种子池和探索池的双池架构，通过执行引导的经验收集逐步扩展训练数据；3）设计语言感知加权机制，根据兄弟语言间的相对性能动态优先处理更难的翻译方向。

Result: 在HumanEval-X和TransCoder-Test基准测试上的广泛实验表明，该方法在所有翻译方向上相比基线LLM都有显著改进，消融实验验证了引导和加权组件的有效性。

Conclusion: BootTrans通过利用测试套件的跨语言特性解决了多语言代码翻译中的数据稀缺和优化不平衡问题，为多语言代码翻译提供了有效的解决方案。

Abstract: Code translation across multiple programming languages is essential yet challenging due to two vital obstacles: scarcity of parallel data paired with executable test oracles, and optimization imbalance when handling diverse language pairs. We propose BootTrans, a bootstrapping method that resolves both obstacles. Its key idea is to leverage the functional invariance and cross-lingual portability of test suites, adapting abundant pivot-language unit tests to serve as universal verification oracles for multilingual RL training. Our method introduces a dual-pool architecture with seed and exploration pools to progressively expand training data via execution-guided experience collection. Furthermore, we design a language-aware weighting mechanism that dynamically prioritizes harder translation directions based on relative performance across sibling languages, mitigating optimization imbalance. Extensive experiments on the HumanEval-X and TransCoder-Test benchmarks demonstrate substantial improvements over baseline LLMs across all translation directions, with ablations validating the effectiveness of both bootstrapping and weighting components.

</details>


### [51] [Deploy-Master: Automating the Deployment of 50,000+ Agent-Ready Scientific Tools in One Day](https://arxiv.org/abs/2601.03513)
*Yi Wang,Zhenting Huang,Zhaohan Ding,Ruoxue Liao,Yuan Huang,Xinzijian Liu,Jiajun Xie,Siheng Chen,Linfeng Zhang*

Main category: cs.SE

TL;DR: Deploy-Master是一个一站式代理工作流，用于大规模科学工具发现、构建规范推断、基于执行的验证和发布，将异构开源仓库转换为可运行的容器化能力。


<details>
  <summary>Details</summary>
Motivation: 开源科学软件丰富但难以编译、配置和重用，限制了可重复性、大规模评估以及科学工具在现代AI-for-Science和代理工作流中的实际集成。

Method: 基于覆盖90+科学和工程领域的分类法，从50万+公共仓库中筛选出52,550个可执行工具候选，通过构建规范推断、执行验证和容器化，将工具转换为可运行能力。

Result: 在一天内完成52,550次构建尝试，为50,112个科学工具构建了可重复的运行时环境，每个成功工具都通过最小可执行命令验证并注册到SciencePedia中。

Conclusion: 科学软件难以操作化的原因在于规模化的部署挑战，需要共享、可观察的执行基板作为可扩展AI4S和代理科学的基础。

Abstract: Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows.
  We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation.
  Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science.

</details>


### [52] [Do Autonomous Agents Contribute Test Code? A Study of Tests in Agentic Pull Requests](https://arxiv.org/abs/2601.03556)
*Sabrina Haque,Sarvesh Ingale,Christoph Csallner*

Main category: cs.SE

TL;DR: 对AIDev数据集中代理提交的PR进行实证研究，分析测试代码在代理驱动工作流中的出现频率、引入时机，以及含测试PR与不含测试PR在规模、周转时间和合并结果上的差异。


<details>
  <summary>Details</summary>
Motivation: 随着代理编码工具越来越多地提交PR，需要了解测试在这些代理驱动工作流中的表现，以理解自主软件开发中的测试行为。

Method: 使用AIDev数据集进行实证研究，分析测试代码在代理PR中的包含情况、引入时机，比较含测试PR与不含测试PR在规模、周转时间和合并结果上的差异。

Result: 随时间推移，含测试的PR越来越常见；含测试的PR通常规模更大、完成时间更长，但合并率基本相似；不同代理在测试采用率和测试与生产代码平衡方面存在差异。

Conclusion: 研究提供了代理PR中测试行为的描述性视图，为未来自主软件开发研究提供了实证基础。

Abstract: Testing is a critical practice for ensuring software correctness and long-term maintainability. As agentic coding tools increasingly submit pull requests (PRs), it becomes essential to understand how testing appears in these agent-driven workflows. Using the AIDev dataset, we present an empirical study of test inclusion in agentic pull requests. We examine how often tests are included, when they are introduced during the PR lifecycle and how test-containing PRs differ from non-test PRs in terms of size, turnaround time, and merge outcomes. Across agents, test-containing PRs are more common over time and tend to be larger and take longer to complete, while merge rates remain largely similar. We also observe variation across agents in both test adoption and the balance between test and production code within test PRs. Our findings provide a descriptive view of testing behavior in agentic pull requests and offer empirical grounding for future studies of autonomous software development.

</details>


### [53] [Verbatim Data Transcription Failures in LLM Code Generation: A State-Tracking Stress Test](https://arxiv.org/abs/2601.03640)
*Mohd Ariful Haque,Kishor Datta Gupta,Mohammad Ashiqur Rahman,Roy George*

Main category: cs.SE

TL;DR: 该论文提出了一个最小化的转录到代码基准测试，专门评估LLM在精确转录高精度十进制常数到Python代码中的可靠性，关注数据完整性而非算法推理。


<details>
  <summary>Details</summary>
Motivation: 现实世界软件任务（如加密常数、协议测试向量、白名单、校准表）需要将数据精确转录到代码中，这些操作对微小遗漏或改动很敏感，可能产生语法有效但功能错误的程序。现有代码生成评估主要关注算法推理，缺乏对数据完整性的专门测试。

Method: 设计了一个最小化的转录到代码基准测试：给定高精度十进制常数列表，模型必须生成嵌入这些常数并执行简单聚合计算的Python代码。开发了提示变体、基于精确字符串包含的评估协议，以及用于表征状态跟踪和长视野生成失败的分析框架。

Result: 该基准测试作为一个紧凑的压力测试，能够补充现有的代码生成评估，专门关注数据完整性方面的问题。

Conclusion: 需要专门的基准测试来评估LLM在精确数据转录方面的可靠性，该基准测试填补了现有评估在数据完整性方面的空白，有助于识别状态跟踪和长视野生成中的失败模式。

Abstract: Many real-world software tasks require exact transcription of provided data into code, such as cryptographic constants, protocol test vectors, allowlists, and calibration tables. These tasks are operationally sensitive because small omissions or alterations can remain silent while producing syntactically valid programs. This paper introduces a deliberately minimal transcription-to-code benchmark to isolate this reliability concern in LLM-based code generation. Given a list of high-precision decimal constants, a model must generate Python code that embeds the constants verbatim and performs a simple aggregate computation. We describe the prompting variants, evaluation protocol based on exact-string inclusion, and analysis framework used to characterize state-tracking and long-horizon generation failures. The benchmark is intended as a compact stress test that complements existing code-generation evaluations by focusing on data integrity rather than algorithmic reasoning.

</details>


### [54] [From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level](https://arxiv.org/abs/2601.03731)
*Jia Li,Yuxin Su,Michael R. Lyu*

Main category: cs.SE

TL;DR: RepoReason：一个用于评估LLM在仓库级别推理能力的白盒诊断基准，通过执行驱动的变异框架和动态程序切片量化推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM发展为自主代理，评估其在真实、大规模、相互依赖的文件系统中保持逻辑一致性的能力变得至关重要。现有基准通常在孤立代码片段和黑盒评估之间波动，缺乏对仓库级别推理能力的深入诊断。

Method: 1. 提出RepoReason基准，专注于溯因断言验证；2. 使用执行驱动的变异框架，利用环境作为语义预言机来重新生成真实状态，避免记忆化；3. 建立基于动态程序切片的细粒度诊断系统，使用三个正交指标量化推理：ESV（读取负载）、MCL（模拟深度）和DFI（集成宽度）。

Result: 对前沿模型（如Claude-4.5-Sonnet、DeepSeek-v3.1-Terminus）的综合评估揭示了普遍存在的聚合缺陷，其中集成宽度（DFI）是主要的认知瓶颈。

Conclusion: RepoReason提供了细粒度的白盒洞察，有助于优化下一代代理式软件工程。集成宽度是当前LLM在仓库级别推理中的主要限制因素。

Abstract: As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.

</details>


### [55] [Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study](https://arxiv.org/abs/2601.03780)
*Md Ahasanuzzaman,Bram Adams,Emad Fallahzadeh,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文通过知识单元(KU)分析发现代码生成基准测试(HumanEval, MBPP)与现实项目存在概念覆盖偏差，提出基于提示的LLM框架生成新任务来平衡KU分布，增强基准测试的现实性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试(如HumanEval)可能无法全面反映现实世界编程概念，如果基准测试中的概念不能代表实际项目使用的概念，评估结果可能不完整。目前缺乏对基准测试中代码概念代表性的系统研究。

Method: 1) 通过知识单元(KU)分析Python基准测试(HumanEval, MBPP)和30个现实Python项目的概念覆盖；2) 提出基于提示的LLM框架，合成KU平衡的任务来重新平衡基准测试分布；3) 生成440个新任务并增强现有基准测试。

Result: 1) 每个基准测试仅覆盖20个KU中的一半，而项目使用所有KU且分布相对平衡；2) 增强后的基准测试显著提高KU覆盖，分布对齐度提升60%以上；3) 在增强基准测试上，最先进LLM性能下降12.54-44.82%，表明现有基准测试因有限KU覆盖而高估LLM性能。

Conclusion: 现有代码生成基准测试存在概念覆盖偏差，高估了LLM的实际能力。提出的KU平衡方法能构建更现实的评估基准，为评估LLM代码生成能力提供可操作指导。

Abstract: Large Language Models (LLMs) such as GPT-4, Claude and LLaMA have shown impressive performance in code generation, typically evaluated using benchmarks (e.g., HumanEval). However, effective code generation requires models to understand and apply a wide range of language concepts. If the concepts exercised in benchmarks are not representative of those used in real-world projects, evaluations may yield incomplete. Despite this concern, the representativeness of code concepts in benchmarks has not been systematically examined.
  To address this gap, we present the first empirical study that analyzes the representativeness of code generation benchmarks through the lens of Knowledge Units (KUs) - cohesive sets of programming language capabilities provided by language constructs and APIs. We analyze KU coverage in two widely used Python benchmarks, HumanEval and MBPP, and compare them with 30 real-world Python projects. Our results show that each benchmark covers only half of the identified 20 KUs, whereas projects exercise all KUs with relatively balanced distributions. In contrast, benchmark tasks exhibit highly skewed KU distributions.
  To mitigate this misalignment, we propose a prompt-based LLM framework that synthesizes KU-based tasks to rebalance benchmark KU distributions and better align them with real-world usage. Using this framework, we generate 440 new tasks and augment existing benchmarks. The augmented benchmarks substantially improve KU coverage and achieve over a 60% improvement in distributional alignment. Evaluations of state-of-the-art LLMs on these augmented benchmarks reveal consistent and statistically significant performance drops (12.54-44.82%), indicating that existing benchmarks overestimate LLM performance due to their limited KU coverage. Our findings provide actionable guidance for building more realistic evaluations of LLM code-generation capabilities.

</details>


### [56] [Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design](https://arxiv.org/abs/2601.03878)
*Giovanni Rosa,David Moreno-Lumbreras,Gregorio Robles,Jesús M. González-Barahona*

Main category: cs.SE

TL;DR: 本文提出使用CURRANTE工具进行实证研究，分析人类在规范和测试细化中的干预如何影响LLM生成代码的质量和动态过程。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM越来越多地集成到软件开发工作流中，但它们在结构化、规范驱动过程中的行为仍然缺乏深入理解。需要研究人类干预在规范和测试细化阶段如何影响LLM生成代码的质量和动态。

Method: 使用CURRANTE（一个Visual Studio Code扩展）进行实证研究，该工具支持人机协作工作流，引导开发者通过三个顺序阶段：规范定义、测试生成与细化、函数实现。参与者使用LiveCodeBench数据集的中等难度问题，工具记录细粒度交互日志、有效性指标（通过率、全通过完成度）、效率指标（通过时间）和迭代行为。

Result: 研究尚未完成，但预期结果将提供关于人类干预如何影响LLM生成代码质量和动态的实证见解，为下一代开发环境设计提供依据。

Conclusion: 该研究将填补对LLM在结构化开发过程中行为理解的空白，为设计更好的人机协作开发环境提供实证基础，使人类推理与模型驱动的代码生成更好地对齐。

Abstract: Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.

</details>
