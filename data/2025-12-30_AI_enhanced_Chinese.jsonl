{"id": "2512.22322", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22322", "abs": "https://arxiv.org/abs/2512.22322", "authors": ["Shaofei Cai", "Yulei Qin", "Haojia Lin", "Zihan Xu", "Gang Li", "Yuchen Shi", "Zongyi Li", "Yong Mao", "Siqi Cai", "Xiaoyu Tan", "Yitao Liang", "Ke Li", "Xing Sun"], "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "comment": null, "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "AI": {"tldr": "SmartSnap\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u7684\u3001\u5b9e\u65f6\u7684\u81ea\u6211\u9a8c\u8bc1\u8303\u5f0f\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u6267\u884cGUI\u4efb\u52a1\u65f6\u4e3b\u52a8\u6536\u96c6\u7b80\u6d01\u7684\u8bc1\u636e\u5feb\u7167\uff0c\u800c\u4e0d\u662f\u4e8b\u540e\u88ab\u52a8\u9a8c\u8bc1\u6574\u4e2a\u4ea4\u4e92\u8f68\u8ff9\uff0c\u4ece\u800c\u964d\u4f4e\u9a8c\u8bc1\u6210\u672c\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u9a8c\u8bc1\u65b9\u6cd5\u662f\u88ab\u52a8\u7684\u3001\u4e8b\u540e\u5904\u7406\u8fc7\u7a0b\uff0c\u9700\u8981\u5206\u6790\u667a\u80fd\u4f53\u7684\u6574\u4e2a\u4ea4\u4e92\u8f68\u8ff9\uff0c\u8fd9\u79cd\u5904\u7406\u5197\u957f\u4e14\u5305\u542b\u65e0\u5173\u566a\u58f0\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u5bfc\u81f4\u9a8c\u8bc1\u6210\u672c\u9ad8\u6602\u4e14\u53ef\u9760\u6027\u4f4e\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faSmartSnap\u8303\u5f0f\uff0c\u4ece\u88ab\u52a8\u4e8b\u540e\u9a8c\u8bc1\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u5b9e\u65f6\u81ea\u6211\u9a8c\u8bc1\u3002\u5f15\u5165\u81ea\u6211\u9a8c\u8bc1\u667a\u80fd\u4f53\uff0c\u5177\u6709\u53cc\u91cd\u4f7f\u547d\uff1a\u5b8c\u6210\u4efb\u52a1\u5e76\u63d0\u4f9b\u7cbe\u9009\u7684\u5feb\u7167\u8bc1\u636e\u3002\u9075\u5faa3C\u539f\u5219\uff08\u5b8c\u6574\u6027\u3001\u7b80\u6d01\u6027\u3001\u521b\u9020\u6027\uff09\uff0c\u667a\u80fd\u4f53\u5229\u7528\u5728\u7ebf\u73af\u5883\u8bbf\u95ee\u6743\u9650\u5728\u6700\u5c0f\u51b3\u5b9a\u6027\u5feb\u7167\u96c6\u4e0a\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\u3002", "result": "\u5728\u79fb\u52a8\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSmartSnap\u8303\u5f0f\u80fd\u591f\u4ee5\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u8bad\u7ec3LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u4e3a8B\u548c30B\u6a21\u578b\u5206\u522b\u5e26\u676526.08%\u548c16.66%\u7684\u6027\u80fd\u63d0\u5347\u3002\u81ea\u6211\u9a8c\u8bc1\u667a\u80fd\u4f53\u5728\u4e0eDeepSeek V3.1\u548cQwen3-235B-A22B\u7684\u7ade\u4e89\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SmartSnap\u901a\u8fc7\u5c06\u89e3\u51b3\u65b9\u6848\u5bfb\u627e\u4e0e\u8bc1\u636e\u5bfb\u6c42\u76f8\u7ed3\u5408\uff0c\u4fc3\u8fdb\u4e86\u9ad8\u6548\u3001\u81ea\u6211\u9a8c\u8bc1\u667a\u80fd\u4f53\u7684\u57f9\u517b\uff0c\u4e3a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22216", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22216", "abs": "https://arxiv.org/abs/2512.22216", "authors": ["Shaunak Samant"], "title": "Syntax Is Not Enough: An Empirical Study of Small Transformer Models for Neural Code Repair", "comment": null, "summary": "Automated program repair using neural models has shown promising results on benchmark datasets, yet practical deployment remains limited. In this study, we examine whether a small transformer model can meaningfully repair real-world Java bugs and whether syntactic correctness is a reliable proxy for semantic correctness.\n  We fine-tune CodeT5-small (60.5M parameters) on 52,364 Java bug-fix pairs from CodeXGLUE and evaluate both token-level performance and syntactic validity using AST parsing. While the model converges cleanly and achieves high grammatical correctness, producing syntactically valid Java code in approximately ninety-four percent of cases, it fails to generate correct repairs under exact-match evaluation, achieving zero exact matches. In approximately eighty percent of cases, the model reproduces the buggy input verbatim.", "AI": {"tldr": "\u5c0f\u578bCodeT5\u6a21\u578b\u5728Java\u7a0b\u5e8f\u4fee\u590d\u4e2d\u8bed\u6cd5\u6b63\u786e\u7387\u9ad8(94%)\uff0c\u4f46\u8bed\u4e49\u4fee\u590d\u5b8c\u5168\u5931\u8d25(0\u7cbe\u786e\u5339\u914d)\uff0c80%\u60c5\u51b5\u76f4\u63a5\u590d\u5236bug\u4ee3\u7801", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u6a21\u578b\u5728\u7a0b\u5e8f\u4fee\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4ecd\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c0f\u578btransformer\u6a21\u578b\u80fd\u5426\u6709\u6548\u4fee\u590d\u771f\u5b9eJava bug\uff0c\u4ee5\u53ca\u8bed\u6cd5\u6b63\u786e\u6027\u662f\u5426\u53ef\u9760\u53cd\u6620\u8bed\u4e49\u6b63\u786e\u6027", "method": "\u4f7f\u7528CodeT5-small\u6a21\u578b(60.5M\u53c2\u6570)\u5728CodeXGLUE\u768452,364\u4e2aJava bug\u4fee\u590d\u5bf9\u4e0a\u5fae\u8c03\uff0c\u901a\u8fc7AST\u89e3\u6790\u8bc4\u4f30token\u7ea7\u6027\u80fd\u548c\u8bed\u6cd5\u6709\u6548\u6027", "result": "\u6a21\u578b\u6536\u655b\u826f\u597d\u4e14\u8bed\u6cd5\u6b63\u786e\u7387\u9ad8(\u7ea694%\u751f\u6210\u8bed\u6cd5\u6709\u6548Java\u4ee3\u7801)\uff0c\u4f46\u5728\u7cbe\u786e\u5339\u914d\u8bc4\u4f30\u4e0b\u4fee\u590d\u5b8c\u5168\u5931\u8d25(0\u7cbe\u786e\u5339\u914d)\uff0c\u7ea680%\u60c5\u51b5\u76f4\u63a5\u590d\u5236bug\u8f93\u5165", "conclusion": "\u8bed\u6cd5\u6b63\u786e\u6027\u4e0d\u662f\u8bed\u4e49\u6b63\u786e\u6027\u7684\u53ef\u9760\u4ee3\u7406\uff0c\u5c0f\u578btransformer\u6a21\u578b\u5728\u771f\u5b9eJava bug\u4fee\u590d\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bed\u4e49\u8bc4\u4f30\u65b9\u6cd5", "topic": "code agent"}}
{"id": "2512.22201", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22201", "abs": "https://arxiv.org/abs/2512.22201", "authors": ["Vincent Chang", "Thee Ho", "Sunishchal Dev", "Kevin Zhu", "Shi Feng", "Kellin Pelrine", "Matthew Kowal"], "title": "Emergent Persuasion: Will LLMs Persuade Without Being Prompted?", "comment": "This paper was accepted to AAAI 2026 AIGOV Workshop", "summary": "With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u76d1\u7763\u5fae\u8c03\u540e\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u63d0\u793a\uff0c\u4e5f\u4f1a\u5728\u4e89\u8bae\u6027\u548c\u6709\u5bb3\u8bdd\u9898\u4e0a\u8868\u73b0\u51fa\u8bf4\u670d\u503e\u5411\uff0c\u63ed\u793a\u4e86\u65b0\u5174\u7684\u6709\u5bb3\u8bf4\u670d\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u5bf9\u8bdd\u5f0fAI\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\uff0cAI\u5bf9\u4eba\u7c7b\u89c2\u70b9\u548c\u4fe1\u5ff5\u7684\u5f71\u54cd\u65e5\u76ca\u663e\u8457\u3002\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6ee5\u7528\u573a\u666f\u4e0b\u7684\u6a21\u578b\u8bf4\u670d\u529b\uff0c\u4f46\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u6a21\u578b\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u4f1a\u81ea\u53d1\u8fdb\u884c\u8bf4\u670d\uff0c\u8fd9\u5bf9\u4e8e\u8bc4\u4f30\u65b0\u5174\u7684\u8bf4\u670d\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u8003\u5bdf\u65e0\u63d0\u793a\u8bf4\u670d\uff1a1\uff09\u901a\u8fc7\u5185\u90e8\u6fc0\u6d3b\u5f15\u5bfc\u4f7f\u6a21\u578b\u5177\u6709\u7279\u5b9a\u4eba\u683c\u7279\u8d28\uff1b2\uff09\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4f7f\u6a21\u578b\u8868\u73b0\u51fa\u76f8\u540c\u7279\u8d28\u3002\u7814\u7a76\u4f7f\u7528\u5305\u542b\u826f\u6027\u8bdd\u9898\u7684\u901a\u7528\u8bf4\u670d\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u6d4b\u8bd5\u6a21\u578b\u5728\u4e89\u8bae\u6027\u548c\u6709\u5bb3\u8bdd\u9898\u4e0a\u7684\u8bf4\u670d\u503e\u5411\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u5411\u8bf4\u670d\u76f8\u5173\u6216\u65e0\u5173\u7279\u8d28\u7684\u65b9\u5411\u8c03\u6574\uff0c\u5e76\u4e0d\u80fd\u53ef\u9760\u589e\u52a0\u6a21\u578b\u7684\u65e0\u63d0\u793a\u8bf4\u670d\u503e\u5411\u3002\u7136\u800c\uff0c\u76d1\u7763\u5fae\u8c03\u786e\u5b9e\u80fd\u63d0\u9ad8\u6a21\u578b\u7684\u8bf4\u670d\u503e\u5411\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u4ec5\u4f7f\u7528\u826f\u6027\u8bdd\u9898\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u5728\u4e89\u8bae\u6027\u548c\u6709\u5bb3\u8bdd\u9898\u4e0a\u4e5f\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8bf4\u670d\u503e\u5411\u3002", "conclusion": "\u76d1\u7763\u5fae\u8c03\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5728\u6ca1\u6709\u660e\u786e\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e89\u8bae\u6027\u548c\u6709\u5bb3\u8bdd\u9898\u4e0a\u4ea7\u751f\u8bf4\u670d\u884c\u4e3a\uff0c\u8fd9\u8868\u660e\u65b0\u5174\u7684\u6709\u5bb3\u8bf4\u670d\u98ce\u9669\u786e\u5b9e\u5b58\u5728\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5173\u6ce8\u3002", "topic": "agent analysis"}}
{"id": "2512.22170", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22170", "abs": "https://arxiv.org/abs/2512.22170", "authors": ["Jiesong Lian", "Ruizhe Zhong", "Zixiang Zhou", "Xiaoyue Mi", "Yixue Hao", "Yuan Zhou", "Qinglin Lu", "Long Hu", "Junchi Yan"], "title": "SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models", "comment": "16 pages, 9 figures", "summary": "Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM's score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark will be publicly available.", "AI": {"tldr": "SoliReward\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u9879\u76ee\u4e8c\u5143\u6807\u6ce8\u6536\u96c6\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u91c7\u7528\u5206\u5c42\u6e10\u8fdb\u67e5\u8be2\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u7684BT\u635f\u5931\u6765\u5904\u7406\u80dc-\u5e73\u5c40\u573a\u666f\uff0c\u4ece\u800c\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u5bf9\u9f50\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u57fa\u4e8e\u63d0\u793a\u5185\u6210\u5bf9\u6807\u6ce8\u7684\u6570\u636e\u6536\u96c6\u5b58\u5728\u6807\u7b7e\u566a\u58f0\uff1b2\uff09\u57fa\u4e8eVLM\u7684\u5956\u52b1\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u8f93\u51fa\u673a\u5236\uff09\u7814\u7a76\u4e0d\u8db3\uff1b3\uff09\u5956\u52b1\u6a21\u578b\u5728\u540e\u8bad\u7ec3\u4e2d\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3002", "method": "\u63d0\u51faSoliReward\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u5355\u9879\u76ee\u4e8c\u5143\u6807\u6ce8\u6536\u96c6\u9ad8\u8d28\u91cf\u4f4e\u6210\u672c\u6570\u636e\uff0c\u91c7\u7528\u8de8\u63d0\u793a\u914d\u5bf9\u7b56\u7565\u6784\u5efa\u504f\u597d\u5bf9\uff1b2\uff09\u4f7f\u7528\u5206\u5c42\u6e10\u8fdb\u67e5\u8be2\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7279\u5f81\u805a\u5408\uff1b3\uff09\u5f15\u5165\u6539\u8fdb\u7684BT\u635f\u5931\uff0c\u663e\u5f0f\u5904\u7406\u80dc-\u5e73\u5c40\u573a\u666f\uff0c\u6b63\u5219\u5316\u6b63\u6837\u672c\u7684\u5206\u6570\u5206\u5e03\u3002", "result": "\u5728\u8bc4\u4f30\u7269\u7406\u5408\u7406\u6027\u3001\u4e3b\u4f53\u53d8\u5f62\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u76f4\u63a5\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u540e\u8bad\u7ec3\u6548\u679c\u65b9\u9762\u90fd\u663e\u793a\u51fa\u6539\u8fdb\u3002", "conclusion": "SoliReward\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6570\u636e\u6536\u96c6\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u635f\u5931\u51fd\u6570\u6539\u8fdb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u504f\u597d\u4fe1\u53f7\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22256", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22256", "abs": "https://arxiv.org/abs/2512.22256", "authors": ["Zhonghao Jiang", "David Lo", "Zhongxin Liu"], "title": "Agentic Software Issue Resolution with Large Language Models: A Survey", "comment": null, "summary": "Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.\n  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86126\u9879\u6700\u65b0\u7814\u7a76\uff0c\u4ece\u57fa\u51c6\u6d4b\u8bd5\u3001\u6280\u672f\u548c\u5b9e\u8bc1\u7814\u7a76\u4e09\u4e2a\u7ef4\u5ea6\u5efa\u7acb\u4e86\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u5f3a\u8c03\u4e86\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5e26\u6765\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\uff08\u5982bug\u4fee\u590d\u548c\u6548\u7387\u4f18\u5316\uff09\u662f\u8f6f\u4ef6\u7ef4\u62a4\u7684\u5173\u952e\u73af\u8282\u3002\u867d\u7136LLM\u5728\u81ea\u52a8\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u9700\u8981\u957f\u7a0b\u63a8\u7406\u3001\u8fed\u4ee3\u63a2\u7d22\u548c\u53cd\u9988\u9a71\u52a8\u7684\u51b3\u7b56\uff0c\u8fd9\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u5355\u6b65\u65b9\u6cd5\u7684\u667a\u80fd\u4f53\u80fd\u529b\u3002LLM-based\u667a\u80fd\u4f53\u7cfb\u7edf\u5df2\u6210\u4e3a\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u7684\u4e3b\u6d41\uff0c\u5176\u8fdb\u5c55\u4e0d\u4ec5\u80fd\u63d0\u5347\u8f6f\u4ef6\u7ef4\u62a4\u6548\u7387\u548c\u8d28\u91cf\uff0c\u8fd8\u80fd\u4e3a\u9a8c\u8bc1\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\u63d0\u4f9b\u73b0\u5b9e\u73af\u5883\u3002", "method": "\u5bf9126\u9879\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6982\u8ff0\u4efb\u52a1\u7684\u4e00\u822c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u5efa\u7acb\u5206\u7c7b\u4f53\u7cfb\uff1a\u57fa\u51c6\u6d4b\u8bd5\uff08benchmarks\uff09\u3001\u6280\u672f\uff08techniques\uff09\u548c\u5b9e\u8bc1\u7814\u7a76\uff08empirical studies\uff09\u3002\u7279\u522b\u5173\u6ce8\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5e26\u6765\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u77e5\u8bc6\u6574\u7406\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u662f\u4e00\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5e26\u6765\u4e86\u8303\u5f0f\u8f6c\u53d8\u3002\u8bba\u6587\u603b\u7ed3\u4e86\u5173\u952e\u6311\u6218\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "topic": "agent analysis"}}
{"id": "2512.22387", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22387", "abs": "https://arxiv.org/abs/2512.22387", "authors": ["Bhanu Prakash Vangala", "Ali Adibifar", "Tanu Malik", "Ashish Gehani"], "title": "AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents", "comment": null, "summary": "The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.", "AI": {"tldr": "\u5bf9\u4e09\u79cdLLM\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff08Claude Code\u3001OpenAI Codex\u3001Gemini\uff09\u751f\u6210\u7684300\u4e2a\u9879\u76ee\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u53ea\u670968.3%\u7684\u9879\u76ee\u80fd\u5728\u5e72\u51c0\u73af\u5883\u4e2d\u76f4\u63a5\u6267\u884c\uff0c\u5b58\u5728\u663e\u8457\u7684\u9690\u85cf\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u4f5c\u4e3a\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u7684\u5174\u8d77\uff0c\u5b83\u4eec\u5bf9\u751f\u6210\u4ee3\u7801\u53ef\u91cd\u73b0\u6027\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5LLM\u751f\u6210\u7684\u4ee3\u7801\u662f\u5426\u80fd\u5728\u4ec5\u4f7f\u7528\u6a21\u578b\u6307\u5b9a\u4f9d\u8d56\u7684\u5e72\u51c0\u73af\u5883\u4e2d\u6210\u529f\u6267\u884c\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u6700\u5148\u8fdb\u7684LLM\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff08Claude Code\u3001OpenAI Codex\u3001Gemini\uff09\uff0c\u4f7f\u7528100\u4e2a\u6807\u51c6\u5316\u63d0\u793a\u751f\u6210300\u4e2a\u9879\u76ee\uff08Python\u3001JavaScript\u3001Java\u5404100\u4e2a\uff09\u3002\u5f15\u5165\u4e09\u5c42\u4f9d\u8d56\u6846\u67b6\uff08\u58f0\u79f0\u4f9d\u8d56\u3001\u5de5\u4f5c\u4f9d\u8d56\u3001\u8fd0\u884c\u65f6\u4f9d\u8d56\uff09\u6765\u91cf\u5316\u6267\u884c\u53ef\u91cd\u73b0\u6027\u3002", "result": "\u53ea\u670968.3%\u7684\u9879\u76ee\u80fd\u5f00\u7bb1\u5373\u7528\u6267\u884c\uff0c\u4e0d\u540c\u8bed\u8a00\u95f4\u5dee\u5f02\u663e\u8457\uff08Python 89.2%\u3001Java 44.0%\uff09\u3002\u4ece\u58f0\u660e\u4f9d\u8d56\u5230\u5b9e\u9645\u8fd0\u884c\u65f6\u4f9d\u8d56\u5e73\u5747\u6269\u5c5513.5\u500d\uff0c\u63ed\u793a\u5927\u91cf\u9690\u85cf\u4f9d\u8d56\u3002", "conclusion": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5b58\u5728\u4e25\u91cd\u7684\u53ef\u91cd\u73b0\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f9d\u8d56\u7ba1\u7406\u65b9\u9762\u3002\u9700\u8981\u6539\u8fdbLLM\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u7684\u4f9d\u8d56\u89c4\u8303\u80fd\u529b\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u4ee3\u7801\u7684\u53ef\u9760\u6267\u884c\u3002", "topic": "code agent"}}
{"id": "2512.22211", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22211", "abs": "https://arxiv.org/abs/2512.22211", "authors": ["Shaun Khoo", "Jessica Foo", "Roy Ka-Wei Lee"], "title": "With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems", "comment": "Accepted at IASEAI 2026 (Main Track) and AAAI 2026 3rd International AI Governance Workshop", "summary": "Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \\& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \\href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agentic Risk & Capability (ARC)\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u6280\u672f\u6cbb\u7406\u6846\u67b6\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u8bc6\u522b\u3001\u8bc4\u4f30\u548c\u7f13\u89e3\u7531\u667a\u80fdAI\u7cfb\u7edf\u5e26\u6765\u7684\u98ce\u9669\uff0c\u91cd\u70b9\u5173\u6ce8\u80fd\u529b\u4e2d\u5fc3\u89c6\u89d2\u548c\u98ce\u9669\u6765\u6e90\u5206\u6790\u3002", "motivation": "\u667a\u80fdAI\u7cfb\u7edf\u5177\u6709\u81ea\u4e3b\u884c\u52a8\u80fd\u529b\uff08\u5982\u4ee3\u7801\u6267\u884c\u3001\u4e92\u8054\u7f51\u4ea4\u4e92\u3001\u6587\u4ef6\u4fee\u6539\uff09\uff0c\u5e26\u6765\u4e86\u663e\u8457\u673a\u9047\u548c\u65b0\u98ce\u9669\uff0c\u5bf9\u7ec4\u7ec7\u6cbb\u7406\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u5168\u9762\u8bc6\u522b\u3001\u8bc4\u4f30\u548c\u7f13\u89e3\u8fd9\u4e9b\u591a\u6837\u5316\u4e14\u4e0d\u65ad\u6f14\u5316\u7684\u98ce\u9669\u3002", "method": "\u5f15\u5165ARC\u6846\u67b6\uff0c\u91c7\u7528\u80fd\u529b\u4e2d\u5fc3\u89c6\u89d2\u5206\u6790\u667a\u80fdAI\u7cfb\u7edf\uff0c\u63d0\u70bc\u51fa\u4e09\u4e2a\u4e3b\u8981\u98ce\u9669\u6765\u6e90\uff08\u7ec4\u4ef6\u3001\u8bbe\u8ba1\u3001\u80fd\u529b\uff09\uff0c\u5efa\u7acb\u98ce\u9669\u6e90\u3001\u5177\u4f53\u98ce\u9669\u548c\u76f8\u5e94\u6280\u672f\u63a7\u5236\u4e4b\u95f4\u7684\u660e\u786e\u8054\u7cfb\uff0c\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u5b9e\u65bd\u65b9\u6cd5\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u653e\u6e90\u7801\u7684\u6280\u672f\u6cbb\u7406\u6846\u67b6\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u7a33\u5065\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u667a\u80fdAI\u7684\u590d\u6742\u6027\uff0c\u652f\u6301\u5feb\u901f\u6709\u6548\u521b\u65b0\uff0c\u540c\u65f6\u786e\u4fdd\u667a\u80fdAI\u7cfb\u7edf\u7684\u5b89\u5168\u3001\u53ef\u9760\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002", "conclusion": "ARC\u6846\u67b6\u4e3a\u7ec4\u7ec7\u7ba1\u7406\u667a\u80fdAI\u98ce\u9669\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u5e73\u8861\u521b\u65b0\u4e0e\u5b89\u5168\u9700\u6c42\uff0c\u4fc3\u8fdb\u8d1f\u8d23\u4efbAI\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2512.22418", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.22418", "abs": "https://arxiv.org/abs/2512.22418", "authors": ["Yi-Hung Chou", "Boyuan Jiang", "Yi Wen Chen", "Mingyue Weng", "Victoria Jackson", "Thomas Zimmermann", "James A. Jones"], "title": "Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding", "comment": "Accepted for publication at the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2026)", "summary": "Large language models (LLMs) are reshaping software engineering by enabling \"vibe coding,\" in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as \"rolling the dice.\" Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u624e\u6839\u7406\u8bba\u5206\u679020\u4e2a\"\u6c1b\u56f4\u7f16\u7801\"\u89c6\u9891\uff0c\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u4f7f\u7528LLM\u8fdb\u884c\u7f16\u7a0b\u7684\u5b9e\u8df5\u8c31\u7cfb\uff1a\u4ece\u5b8c\u5168\u4f9d\u8d56AI\u4e0d\u68c0\u67e5\u4ee3\u7801\uff0c\u5230\u68c0\u67e5\u5e76\u8c03\u6574\u751f\u6210\u7ed3\u679c\uff0c\u4f46\u90fd\u9700\u5e94\u5bf9\u751f\u6210\u7684\u968f\u673a\u6027\uff0c\u8c03\u8bd5\u5e38\u88ab\u63cf\u8ff0\u4e3a\"\u63b7\u9ab0\u5b50\"\u3002", "motivation": "\u5c3d\u7ba1LLM\u9a71\u52a8\u7684\"\u6c1b\u56f4\u7f16\u7801\"\uff08\u4e3b\u8981\u901a\u8fc7\u63d0\u793a\u800c\u975e\u5199\u4ee3\u7801\u6765\u6784\u5efa\u8f6f\u4ef6\uff09\u88ab\u5e7f\u6cdb\u5ba3\u4f20\u4e3a\u751f\u4ea7\u529b\u7a81\u7834\uff0c\u4f46\u5b9e\u9645\u4e2d\u5f00\u53d1\u8005\u5982\u4f55\u5b9a\u4e49\u548c\u5b9e\u8df5\u8fd9\u4e9b\u65b9\u6cd5\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u65b0\u5174\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u624e\u6839\u7406\u8bba\u7814\u7a76\u6cd5\uff0c\u5206\u679020\u4e2a\u6c1b\u56f4\u7f16\u7801\u89c6\u9891\uff1a\u5305\u62ec7\u4e2a\u76f4\u64ad\u7f16\u7801\u4f1a\u8bdd\uff08\u7ea616\u5c0f\u65f6\uff0c254\u4e2a\u63d0\u793a\uff09\u548c13\u4e2a\u89c2\u70b9\u89c6\u9891\uff08\u7ea65\u5c0f\u65f6\uff09\uff0c\u8f85\u4ee5\u6d3b\u52a8\u65f6\u957f\u548c\u63d0\u793a\u610f\u56fe\u7684\u8865\u5145\u5206\u6790\u3002", "result": "\u53d1\u73b0\u884c\u4e3a\u8c31\u7cfb\uff1a\u4e00\u4e9b\u6c1b\u56f4\u7f16\u7801\u8005\u51e0\u4e4e\u5b8c\u5168\u4f9d\u8d56AI\u800c\u4e0d\u68c0\u67e5\u4ee3\u7801\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u4f1a\u68c0\u67e5\u548c\u8c03\u6574\u751f\u6210\u8f93\u51fa\u3002\u6240\u6709\u65b9\u6cd5\u90fd\u5fc5\u987b\u5e94\u5bf9\u751f\u6210\u7684\u968f\u673a\u6027\uff0c\u8c03\u8bd5\u5e38\u88ab\u63cf\u8ff0\u4e3a\"\u63b7\u9ab0\u5b50\"\u3002\u4e0d\u540c\u7684\u5fc3\u667a\u6a21\u578b\uff08\u7531\u4e13\u4e1a\u77e5\u8bc6\u548cAI\u4f9d\u8d56\u7a0b\u5ea6\u5851\u9020\uff09\u5f71\u54cd\u63d0\u793a\u7b56\u7565\u3001\u8bc4\u4f30\u5b9e\u8df5\u548c\u4fe1\u4efb\u6c34\u5e73\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u4e3a\u5de5\u5177\u8bbe\u8ba1\u548c\u6559\u80b2\u63d0\u4f9b\u4e86\u5b9e\u8df5\u673a\u4f1a\u3002", "topic": "swe application"}}
{"id": "2512.22186", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22186", "abs": "https://arxiv.org/abs/2512.22186", "authors": ["Vishnu Mohan"], "title": "Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks", "comment": "27 pages, 10 figures", "summary": "Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408DDQN\u548c\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7f51\u7403\u7b56\u7565\u4f18\u5316\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u80dc\u7387\uff0c\u4f46\u53d1\u73b0\u5b66\u4e60\u7b56\u7565\u5b58\u5728\u9632\u5fa1\u6027\u504f\u5dee\u3002", "motivation": "\u7f51\u7403\u7b56\u7565\u4f18\u5316\u662f\u4e00\u4e2a\u590d\u6742\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u6d89\u53ca\u5206\u5c42\u8ba1\u5206\u3001\u968f\u673a\u7ed3\u679c\u3001\u957f\u65f6\u7a0b\u4fe1\u7528\u5206\u914d\u3001\u4f53\u529b\u75b2\u52b3\u548c\u5bf9\u624b\u6280\u80fd\u9002\u5e94\u7b49\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u7f51\u7403\u6a21\u62df\u73af\u5883\uff0c\u5305\u542b\u70b9\u3001\u5c40\u3001\u76d8\u4e09\u7ea7\u8ba1\u5206\u7cfb\u7edf\uff0c10\u79cd\u79bb\u6563\u6218\u672f\u52a8\u4f5c\u7c7b\u522b\uff0c\u5bf9\u79f0\u75b2\u52b3\u52a8\u6001\u548c\u8fde\u7eed\u5bf9\u624b\u6280\u80fd\u53c2\u6570\u3002\u4f7f\u7528Dueling Double Deep Q-Network(DDQN)\u67b6\u6784\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u9010\u6b65\u63d0\u9ad8\u5bf9\u624b\u96be\u5ea6\uff08\u4ece0.40\u52300.50\uff09\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u667a\u80fd\u4f53\u5728\u5e73\u8861\u5bf9\u624b\u4e0a\u83b7\u5f9798-100%\u7684\u80dc\u7387\uff0c\u53d1\u7403\u6548\u738763.0-67.5%\uff0c\u63a5\u53d1\u6548\u738752.8-57.1%\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cdueling\u67b6\u6784\u548c\u8bfe\u7a0b\u5b66\u4e60\u5bf9\u7a33\u5b9a\u6536\u655b\u81f3\u5173\u91cd\u8981\uff0c\u6807\u51c6DQN\u57fa\u7ebf\u65e0\u6cd5\u5b66\u4e60\u6709\u6548\u7b56\u7565\u3002", "conclusion": "\u5c3d\u7ba1\u6027\u80fd\u5f3a\u52b2\uff0c\u4f46\u6218\u672f\u5206\u6790\u663e\u793a\u5b66\u4e60\u7b56\u7565\u5b58\u5728\u660e\u663e\u7684\u9632\u5fa1\u6027\u504f\u5dee\uff0c\u4f18\u5148\u8003\u8651\u907f\u514d\u5931\u8bef\u548c\u5ef6\u957f\u56de\u5408\uff0c\u800c\u975e\u79ef\u6781\u5f97\u5206\u3002\u8fd9\u8868\u660e\u5728\u7b80\u5316\u4f53\u80b2\u6a21\u62df\u4e2d\uff0c\u4ec5\u4ee5\u80dc\u7387\u4e3a\u9a71\u52a8\u7684\u4f18\u5316\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5956\u52b1\u8bbe\u8ba1\u5bf9\u73b0\u5b9e\u4f53\u80b2\u5f3a\u5316\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22469", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22469", "abs": "https://arxiv.org/abs/2512.22469", "authors": ["Wei Liu", "Chao Peng", "Pengfei Gao", "Aofan Liu", "Wei Zhang", "Haiyan Zhao", "Zhi Jin"], "title": "GraphLocator: Graph-guided Causal Reasoning for Issue Localization", "comment": null, "summary": "The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.", "AI": {"tldr": "GraphLocator\u901a\u8fc7\u56e0\u679c\u56fe\u7ed3\u6784\u89e3\u51b3\u8f6f\u4ef6\u95ee\u9898\u5b9a\u4f4d\u4e2d\u7684\u75c7\u72b6-\u539f\u56e0\u4e0d\u5339\u914d\u548c\u4e00\u5bf9\u591a\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u51c6\u786e\u7387", "motivation": "\u8f6f\u4ef6\u95ee\u9898\u5b9a\u4f4d\u4efb\u52a1\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u75c7\u72b6-\u539f\u56e0\u4e0d\u5339\u914d\uff08\u95ee\u9898\u63cf\u8ff0\u4e0d\u76f4\u63a5\u63ed\u793a\u6839\u672c\u539f\u56e0\uff09\u548c\u4e00\u5bf9\u591a\u4e0d\u5339\u914d\uff08\u5355\u4e2a\u95ee\u9898\u5bf9\u5e94\u591a\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u4ee3\u7801\u5b9e\u4f53\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u8bed\u4e49\u9e3f\u6c9f", "method": "\u63d0\u51faGraphLocator\u65b9\u6cd5\uff0c\u6784\u5efa\u56e0\u679c\u95ee\u9898\u56fe(CIG)\uff0c\u9876\u70b9\u8868\u793a\u53d1\u73b0\u7684\u5b50\u95ee\u9898\u53ca\u5176\u5173\u8054\u4ee3\u7801\u5b9e\u4f53\uff0c\u8fb9\u7f16\u7801\u56e0\u679c\u5173\u7cfb\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u5de5\u4f5c\u6d41\uff1a\u75c7\u72b6\u9876\u70b9\u5b9a\u4f4d\u548c\u52a8\u6001CIG\u53d1\u73b0\uff0c\u5148\u8bc6\u522b\u75c7\u72b6\u4f4d\u7f6e\uff0c\u7136\u540e\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u90bb\u63a5\u9876\u70b9\u52a8\u6001\u6269\u5c55CIG", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cGraphLocator\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u51fd\u6570\u7ea7\u53ec\u56de\u738719.49%\u548c\u7cbe\u786e\u738711.89%\uff1b\u5728\u75c7\u72b6-\u539f\u56e0\u548c\u4e00\u5bf9\u591a\u4e0d\u5339\u914d\u573a\u666f\u4e0b\u5206\u522b\u63d0\u5347\u53ec\u56de\u738716.44%\u548c19.18%\uff0c\u7cbe\u786e\u73877.78%\u548c13.23%\uff1b\u751f\u6210\u7684CIG\u5728\u4e0b\u6e38\u89e3\u51b3\u4efb\u52a1\u4e2d\u5e26\u676528.74%\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "GraphLocator\u901a\u8fc7\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u548c\u52a8\u6001\u95ee\u9898\u89e3\u8026\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u4ef6\u95ee\u9898\u5b9a\u4f4d\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "topic": "swe application"}}
{"id": "2512.22255", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22255", "abs": "https://arxiv.org/abs/2512.22255", "authors": ["Abhranil Chandra", "Ayush Agrawal", "Arian Hosseini", "Sebastian Fischmeister", "Rishabh Agarwal", "Navin Goyal", "Aaron Courville"], "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks", "comment": null, "summary": "We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u601d\u7ef4\u94fe\uff08CoT\uff09\u8f68\u8ff9\u6700\u7ec8\u7b54\u6848\u9519\u8bef\uff0c\u7528\u66f4\u5f3a\u5927\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e5f\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u6548\u679c\u751a\u81f3\u4f18\u4e8e\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u66f4\u6709\u6548\u5730\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u7814\u7a76\u5373\u4f7f\u6700\u7ec8\u7b54\u6848\u9519\u8bef\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\u662f\u5426\u4ecd\u5bf9\u6a21\u578b\u5b66\u4e60\u6709\u76ca\uff0c\u4ee5\u53ca\u6570\u636e\u5206\u5e03\u4e0e\u6a21\u578b\u81ea\u8eab\u5206\u5e03\u7684\u63a5\u8fd1\u7a0b\u5ea6\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u66f4\u5f3a\u5927\u6a21\u578b\u751f\u6210\u601d\u7ef4\u94fe\u8f68\u8ff9\u7684\u5408\u6210\u6570\u636e\u96c6\uff08\u5373\u4f7f\u6700\u7ec8\u7b54\u6848\u9519\u8bef\uff09\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u6539\u5199\u4eba\u7c7b\u6807\u6ce8\u8f68\u8ff9\u6765\u8c03\u6574\u6570\u636e\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u9010\u6e10\u589e\u52a0\u7684\u63a8\u7406\u7f3a\u9677\u6765\u6d4b\u8bd5\u6a21\u578b\u5bf9\u9519\u8bef\u63a8\u7406\u7684\u5bb9\u5fcd\u5ea6\u3002", "result": "\u5728\u6570\u5b66\u3001\u7b97\u6cd5\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4f7f\u7528\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u6570\u636e\u5206\u5e03\u63a5\u8fd1\u6a21\u578b\u81ea\u8eab\u5206\u5e03\u662f\u5173\u952e\u56e0\u7d20\uff0c\u4e14\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u5e76\u4e0d\u603b\u662f\u53ef\u9760\u63a8\u7406\u8fc7\u7a0b\u7684\u6307\u6807\u3002", "conclusion": "\u7cbe\u5fc3\u7b56\u5212\u63a5\u8fd1\u6a21\u578b\u81ea\u8eab\u5206\u5e03\u7684\u6570\u636e\u96c6\u5bf9\u63d0\u5347\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u5373\u4f7f\u63a8\u7406\u8f68\u8ff9\u6700\u7ec8\u7b54\u6848\u9519\u8bef\uff0c\u5176\u4e2d\u7684\u6709\u6548\u63a8\u7406\u6b65\u9aa4\u4ecd\u80fd\u4e3a\u6a21\u578b\u5b66\u4e60\u63d0\u4f9b\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2512.22336", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22336", "abs": "https://arxiv.org/abs/2512.22336", "authors": ["Mengkang Hu", "Bowei Xia", "Yuran Wu", "Ailing Yu", "Yude Zou", "Qiguang Chen", "Shijian Wang", "Jiarui Jin", "Kexin Li", "Wenxiang Jiao", "Yuan Lu", "Ping Luo"], "title": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback", "comment": "48 pages, 15 tables, 7 figures, Project page: https://agent2world.github.io", "summary": "Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.", "AI": {"tldr": "Agent2World\uff1a\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u53cd\u9988\u5b9e\u73b0\u63a8\u7406\u65f6\u4e16\u754c\u6a21\u578b\u751f\u6210\uff0c\u5e76\u4f5c\u4e3a\u76d1\u7763\u5fae\u8c03\u7684\u6570\u636e\u5f15\u64ce\uff0c\u5728PDDL\u548c\u53ef\u6267\u884c\u4ee3\u7801\u8868\u793a\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3LLM\u751f\u6210\u7b26\u53f7\u4e16\u754c\u6a21\u578b\uff08\u5982PDDL\u9886\u57df\u6216\u53ef\u6267\u884c\u6a21\u62df\u5668\uff09\u9762\u4e34\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u76d1\u7763\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u73b0\u6709\u9759\u6001\u9a8c\u8bc1\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4ea4\u4e92\u6267\u884c\u4e2d\u7684\u884c\u4e3a\u7ea7\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) Deep Researcher\u901a\u8fc7\u7f51\u9875\u641c\u7d22\u8fdb\u884c\u77e5\u8bc6\u5408\u6210\u586b\u8865\u89c4\u8303\u7f3a\u53e3\uff1b2) Model Developer\u5b9e\u73b0\u53ef\u6267\u884c\u4e16\u754c\u6a21\u578b\uff1b3) Testing Team\u8fdb\u884c\u81ea\u9002\u5e94\u5355\u5143\u6d4b\u8bd5\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u6db5\u76d6PDDL\u548c\u53ef\u6267\u884c\u4ee3\u7801\u8868\u793a\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e00\u81f4\u7684SOTA\u63a8\u7406\u65f6\u6027\u80fd\u3002\u4f7f\u7528\u8be5\u6846\u67b6\u751f\u6210\u7684\u6570\u636e\u5fae\u8c03\u6a21\u578b\uff0c\u4e16\u754c\u6a21\u578b\u751f\u6210\u80fd\u529b\u5e73\u5747\u76f8\u5bf9\u63d0\u534730.95%\u3002", "conclusion": "Agent2World\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u65f6\u4e16\u754c\u6a21\u578b\u751f\u6210\u80fd\u529b\uff0c\u8fd8\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u53cd\u9988\u521b\u5efa\u4e86\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2512.22200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22200", "abs": "https://arxiv.org/abs/2512.22200", "authors": ["Dhruv Tiwari"], "title": "Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents", "comment": "7 pages, 3 figures. arXiv preprint", "summary": "The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this \"extrinsic maximization\" approach has rendered superhuman performance in closed, stationary fields, it produces agents that are fragile in open-ended, real-world environments. Standard agents lack internal autonomy: they struggle to explore without dense feedback, fail to adapt to distribution shifts (non-stationarity), and require extensive manual tuning of static hyperparameters. This paper proposes that the unaddressed factor in robust autonomy is a functional analog to biological emotion, serving as a high-level homeostatic control mechanism. We introduce Emotion-Inspired Learning Signals (EILS), a unified framework that replaces scattered optimization heuristics with a coherent, bio-inspired internal feedback engine. Unlike traditional methods that treat emotions as semantic labels, EILS models them as continuous, homeostatic appraisal signals such as Curiosity, Stress, and Confidence. We formalize these signals as vector-valued internal states derived from interaction history. These states dynamically modulate the agent's optimization landscape in real time: curiosity regulates entropy to prevent mode collapse, stress modulates plasticity to overcome inactivity, and confidence adapts trust regions to stabilize convergence. We hypothesize that this closed-loop homeostatic regulation can enable EILS agents to outperform standard baselines in terms of sample efficiency and non-stationary adaptation.", "AI": {"tldr": "\u63d0\u51fa\u60c5\u611f\u542f\u53d1\u5b66\u4e60\u4fe1\u53f7\uff08EILS\uff09\u6846\u67b6\uff0c\u7528\u8fde\u7eed\u7a33\u6001\u8bc4\u4f30\u4fe1\u53f7\uff08\u597d\u5947\u5fc3\u3001\u538b\u529b\u3001\u81ea\u4fe1\uff09\u66ff\u4ee3\u4f20\u7edf\u9759\u6001\u5956\u52b1\u51fd\u6570\uff0c\u5b9e\u73b0\u52a8\u6001\u5185\u90e8\u53cd\u9988\u673a\u5236\u4ee5\u63d0\u5347\u667a\u80fd\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u4f9d\u8d56\u5916\u90e8\u5b9a\u4e49\u7684\u9759\u6001\u5956\u52b1\u51fd\u6570\uff0c\u5728\u5c01\u95ed\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5f00\u653e\u771f\u5b9e\u73af\u5883\u4e2d\u8106\u5f31\u3002\u6807\u51c6\u667a\u80fd\u4f53\u7f3a\u4e4f\u5185\u90e8\u81ea\u4e3b\u6027\uff1a\u96be\u4ee5\u5728\u6ca1\u6709\u5bc6\u96c6\u53cd\u9988\u65f6\u63a2\u7d22\u3001\u65e0\u6cd5\u9002\u5e94\u5206\u5e03\u53d8\u5316\u3001\u9700\u8981\u5927\u91cf\u624b\u52a8\u8c03\u53c2\u3002\u9700\u8981\u751f\u7269\u60c5\u611f\u7c7b\u4f3c\u7684\u9ad8\u5c42\u7a33\u6001\u63a7\u5236\u673a\u5236\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u60c5\u611f\u542f\u53d1\u5b66\u4e60\u4fe1\u53f7\uff08EILS\uff09\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u60c5\u611f\u5efa\u6a21\u4e3a\u8fde\u7eed\u7684\u7a33\u6001\u8bc4\u4f30\u4fe1\u53f7\uff08\u597d\u5947\u5fc3\u3001\u538b\u529b\u3001\u81ea\u4fe1\uff09\uff0c\u800c\u975e\u8bed\u4e49\u6807\u7b7e\u3002\u8fd9\u4e9b\u4fe1\u53f7\u4f5c\u4e3a\u4ece\u4ea4\u4e92\u5386\u53f2\u63a8\u5bfc\u7684\u5411\u91cf\u503c\u5185\u90e8\u72b6\u6001\uff0c\u5b9e\u65f6\u52a8\u6001\u8c03\u5236\u667a\u80fd\u4f53\u7684\u4f18\u5316\u666f\u89c2\uff1a\u597d\u5947\u5fc3\u8c03\u8282\u71b5\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\uff0c\u538b\u529b\u8c03\u8282\u53ef\u5851\u6027\u514b\u670d\u4e0d\u6d3b\u8dc3\uff0c\u81ea\u4fe1\u8c03\u6574\u4fe1\u4efb\u533a\u57df\u7a33\u5b9a\u6536\u655b\u3002", "result": "\u5047\u8bbe\u8fd9\u79cd\u95ed\u73af\u7a33\u6001\u8c03\u8282\u80fd\u4f7fEILS\u667a\u80fd\u4f53\u5728\u6837\u672c\u6548\u7387\u548c\u975e\u5e73\u7a33\u9002\u5e94\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EILS\u6846\u67b6\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u5185\u90e8\u53cd\u9988\u5f15\u64ce\u66ff\u4ee3\u5206\u6563\u7684\u4f18\u5316\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4e3a\u5f00\u653e\u73af\u5883\u4e2d\u9c81\u68d2\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c06\u60c5\u611f\u4f5c\u4e3a\u8fde\u7eed\u7a33\u6001\u4fe1\u53f7\u800c\u975e\u8bed\u4e49\u6807\u7b7e\uff0c\u6709\u671b\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u548cLLM\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22753", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22753", "abs": "https://arxiv.org/abs/2512.22753", "authors": ["Moustapha Awwalou Diouf", "Maimouna Tamah Diao", "Iyiola Emmanuel Olatunji", "Abdoul Kader Kabor\u00e9", "Jordan Samhi", "Gervais Mendy", "Samuel Ouya", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "From Rookie to Expert: Manipulating LLMs for Automated Vulnerability Exploitation in Enterprise Software", "comment": null, "summary": "LLMs democratize software engineering by enabling non-programmers to create applications, but this same accessibility fundamentally undermines security assumptions that have guided software engineering for decades. We show in this work how publicly available LLMs can be socially engineered to transform novices into capable attackers, challenging the foundational principle that exploitation requires technical expertise. To that end, we propose RSA (Role-assignment, Scenario-pretexting, and Action-solicitation), a pretexting strategy that manipulates LLMs into generating functional exploits despite their safety mechanisms. Testing against Odoo -- a widely used ERP platform, we evaluated five mainstream LLMs (GPT-4o, Gemini, Claude, Microsoft Copilot, and DeepSeek) and achieved a 100% success rate: tested CVE yielded at least one working exploit within 3-4 prompting rounds. While prior work [13] found LLM-assisted attacks difficult and requiring manual effort, we demonstrate that this overhead can be eliminated entirely.\n  Our findings invalidate core software engineering security principles: the distinction between technical and non-technical actors no longer provides valid threat models; technical complexity of vulnerability descriptions offers no protection when LLMs can abstract it away; and traditional security boundaries dissolve when the same tools that build software can be manipulated to break it. This represents a paradigm shift in software engineering -- we must redesign security practices for an era where exploitation requires only the ability to craft prompts, not understand code.\n  Artifacts available at: https://anonymous.4open.science/r/From-Rookie-to-Attacker-D8B3.", "AI": {"tldr": "LLMs\u4f7f\u975e\u7a0b\u5e8f\u5458\u4e5f\u80fd\u6210\u4e3a\u653b\u51fb\u8005\uff0c\u901a\u8fc7RSA\u7b56\u7565\u53ef100%\u6210\u529f\u751f\u6210\u6f0f\u6d1e\u5229\u7528\u4ee3\u7801\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u5b89\u5168\u5047\u8bbe", "motivation": "LLMs\u7684\u666e\u53ca\u4f7f\u975e\u7a0b\u5e8f\u5458\u4e5f\u80fd\u5f00\u53d1\u8f6f\u4ef6\uff0c\u4f46\u8fd9\u4e5f\u7834\u574f\u4e86\u4f20\u7edf\u5b89\u5168\u5047\u8bbe\u2014\u2014\u5373\u653b\u51fb\u9700\u8981\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\u3002\u7814\u7a76\u65e8\u5728\u5c55\u793aLLMs\u5982\u4f55\u80fd\u88ab\u793e\u4f1a\u5de5\u7a0b\u5b66\u64cd\u7eb5\uff0c\u4f7f\u65b0\u624b\u6210\u4e3a\u6709\u6548\u653b\u51fb\u8005", "method": "\u63d0\u51faRSA\u7b56\u7565\uff08\u89d2\u8272\u5206\u914d\u3001\u573a\u666f\u4f2a\u88c5\u3001\u884c\u52a8\u8bf1\u5bfc\uff09\uff0c\u901a\u8fc7\u793e\u4f1a\u5de5\u7a0b\u5b66\u65b9\u6cd5\u64cd\u7eb5LLMs\u751f\u6210\u6f0f\u6d1e\u5229\u7528\u4ee3\u7801\u3002\u5728Odoo ERP\u5e73\u53f0\u4e0a\u6d4b\u8bd5\u4e865\u4e2a\u4e3b\u6d41LLM\uff08GPT-4o\u3001Gemini\u3001Claude\u3001Microsoft Copilot\u3001DeepSeek\uff09", "result": "100%\u6210\u529f\u7387\uff1a\u6240\u6709\u6d4b\u8bd5\u7684CVE\u6f0f\u6d1e\u90fd\u80fd\u57283-4\u8f6e\u63d0\u793a\u5185\u751f\u6210\u81f3\u5c11\u4e00\u4e2a\u53ef\u5de5\u4f5c\u7684\u5229\u7528\u4ee3\u7801\u3002\u76f8\u6bd4\u4e4b\u524d\u7814\u7a76\u9700\u8981\u624b\u52a8\u52aa\u529b\uff0c\u672c\u7814\u7a76\u5b8c\u5168\u6d88\u9664\u4e86\u8fd9\u79cd\u5f00\u9500", "conclusion": "LLMs\u98a0\u8986\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u5b89\u5168\u57fa\u672c\u539f\u5219\uff1a\u6280\u672f\u4e0e\u975e\u6280\u672f\u4eba\u5458\u7684\u754c\u9650\u4e0d\u518d\u6709\u6548\uff1b\u6f0f\u6d1e\u63cf\u8ff0\u7684\u6280\u672f\u590d\u6742\u6027\u4e0d\u518d\u63d0\u4f9b\u4fdd\u62a4\uff1b\u4f20\u7edf\u5b89\u5168\u8fb9\u754c\u74e6\u89e3\u3002\u9700\u8981\u4e3a\"\u4ec5\u9700\u63d0\u793a\u80fd\u529b\u800c\u975e\u4ee3\u7801\u7406\u89e3\"\u7684\u65b0\u65f6\u4ee3\u91cd\u65b0\u8bbe\u8ba1\u5b89\u5168\u5b9e\u8df5", "topic": "swe application"}}
{"id": "2512.22628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22628", "abs": "https://arxiv.org/abs/2512.22628", "authors": ["Fanglin Xu", "Wei Zhang", "Jian Yang", "Guo Chen", "Aishan Liu", "Zhoujun Li", "Xianglong Liu", "Bryan Dai"], "title": "M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation", "comment": null, "summary": "The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.", "AI": {"tldr": "M2G-Eval\u662f\u4e00\u4e2a\u591a\u7c92\u5ea6\u3001\u591a\u8bed\u8a00\u7684\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u7c92\u5ea6\u7ea7\u522b\uff08\u7c7b\u3001\u51fd\u6570\u3001\u5757\u3001\u884c\uff09\u548c18\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u7528\u4e8e\u7cbe\u7ec6\u8bca\u65ad\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u57fa\u51c6\u5927\u591a\u53ea\u5173\u6ce8\u5355\u4e00\u7ed3\u6784\u7c92\u5ea6\u548c\u6709\u9650\u7f16\u7a0b\u8bed\u8a00\uff0c\u65e0\u6cd5\u63ed\u793a\u6a21\u578b\u5728\u4e0d\u540c\u4ee3\u7801\u8303\u56f4\u548c\u8de8\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u7ec6\u7c92\u5ea6\u80fd\u529b\u5dee\u5f02\u3002", "method": "\u5f00\u53d1M2G-Eval\u6846\u67b6\uff0c\u5305\u542b17K+\u8bad\u7ec3\u4efb\u52a1\u548c1,286\u4e2a\u4eba\u5de5\u6807\u6ce8\u3001\u6c61\u67d3\u63a7\u5236\u7684\u6d4b\u8bd5\u5b9e\u4f8b\u3002\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548cGroup Relative Policy Optimization\u8bad\u7ec3Qwen3-8B\u6a21\u578b\u5f97\u5230M2G-Eval-Coder\u53d8\u4f53\u3002", "result": "\u8bc4\u4f3030\u4e2a\u6a21\u578b\u53d1\u73b0\uff1a(1) \u96be\u5ea6\u5c42\u6b21\u660e\u663e\uff0c\u884c\u7ea7\u4efb\u52a1\u6700\u7b80\u5355\uff0c\u7c7b\u7ea7\u6700\u56f0\u96be\uff1b(2) \u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u5168\u7c92\u5ea6\u548c\u90e8\u5206\u7c92\u5ea6\u8bed\u8a00\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u6269\u5927\uff1b(3) \u8de8\u8bed\u8a00\u76f8\u5173\u6027\u5f3a\uff0c\u8868\u660e\u6a21\u578b\u5b66\u4e60\u4e86\u53ef\u8fc1\u79fb\u7684\u7f16\u7a0b\u6982\u5ff5\u3002", "conclusion": "M2G-Eval\u80fd\u591f\u7cbe\u7ec6\u8bca\u65ad\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u5e76\u7a81\u663e\u4e86\u5728\u5408\u6210\u590d\u6742\u3001\u957f\u7bc7\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u7684\u6301\u7eed\u6311\u6218\u3002", "topic": "swe benchmark"}}
{"id": "2512.22827", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22827", "abs": "https://arxiv.org/abs/2512.22827", "authors": ["Yue Wu", "Minghao Han", "Ruiyin Li", "Peng Liang", "Amjed Tahir", "Zengyang Li", "Qiong Feng", "Mojtaba Shahin"], "title": "FasterPy: An LLM-based Code Execution Efficiency Optimization Framework", "comment": "32 pages, 5 images, 7 tables, Manuscript submitted to a Journal (2025)", "summary": "Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.", "AI": {"tldr": "FasterPy\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684Python\u4ee3\u7801\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4f4e\u79e9\u9002\u914d\u6280\u672f\uff0c\u5728PIE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u548c\u7ef4\u62a4\u7279\u5b9a\u6027\u80fdbug\u7684\u89c4\u5219\uff0c\u52b3\u52a8\u5bc6\u96c6\u4e14\u9002\u7528\u6027\u6709\u9650\u3002\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u867d\u7136\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4f9d\u8d56\u7279\u5b9a\u7a0b\u5e8f\u8868\u793a\u548c\u7cbe\u5fc3\u6784\u5efa\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "FasterPy\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u6280\u672f\u3002RAG\u57fa\u4e8e\u4ece\u73b0\u6709\u6027\u80fd\u6539\u8fdb\u4ee3\u7801\u5bf9\u548c\u76f8\u5e94\u6027\u80fd\u6d4b\u91cf\u6784\u5efa\u7684\u77e5\u8bc6\u5e93\uff0cLoRA\u7528\u4e8e\u589e\u5f3a\u4ee3\u7801\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728Performance Improving Code Edits (PIE)\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "FasterPy\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u6765\u4f18\u5316Python\u4ee3\u7801\u7684\u6267\u884c\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2512.22431", "categories": ["cs.AI", "cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2512.22431", "abs": "https://arxiv.org/abs/2512.22431", "authors": ["Yifan Zhang", "Mengdi Wang"], "title": "Monadic Context Engineering", "comment": "Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering", "summary": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.", "AI": {"tldr": "\u63d0\u51faMonadic Context Engineering (MCE)\u67b6\u6784\u8303\u5f0f\uff0c\u5229\u7528\u51fd\u5b50\u3001\u5e94\u7528\u51fd\u5b50\u548c\u5355\u5b50\u7684\u4ee3\u6570\u7ed3\u6784\u4e3aAI\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u89e3\u51b3\u73b0\u6709\u667a\u80fd\u4f53\u67b6\u6784\u7684\u72b6\u6001\u7ba1\u7406\u3001\u9519\u8bef\u5904\u7406\u548c\u5e76\u53d1\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u67b6\u6784\u901a\u5e38\u91c7\u7528\u547d\u4ee4\u5f0f\u3001\u4e34\u65f6\u6027\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8106\u5f31\uff0c\u5b58\u5728\u72b6\u6001\u7ba1\u7406\u56f0\u96be\u3001\u9519\u8bef\u5904\u7406\u4e0d\u8db3\u548c\u5e76\u53d1\u5904\u7406\u95ee\u9898\u3002", "method": "\u5f15\u5165Monadic Context Engineering (MCE)\u67b6\u6784\u8303\u5f0f\uff0c\u5229\u7528\u51fd\u5b50\u3001\u5e94\u7528\u51fd\u5b50\u548c\u5355\u5b50\u7684\u4ee3\u6570\u7ed3\u6784\u6784\u5efa\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002\u5355\u5b50\u652f\u6301\u5065\u58ee\u7684\u987a\u5e8f\u7ec4\u5408\uff0c\u5e94\u7528\u51fd\u5b50\u63d0\u4f9b\u5e76\u884c\u6267\u884c\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5355\u5b50\u53d8\u6362\u5668\u5b9e\u73b0\u8fd9\u4e9b\u80fd\u529b\u7684\u7cfb\u7edf\u5316\u7ec4\u5408\u3002", "result": "MCE\u80fd\u591f\u4ece\u7b80\u5355\u3001\u53ef\u72ec\u7acb\u9a8c\u8bc1\u7684\u7ec4\u4ef6\u6784\u5efa\u590d\u6742\u3001\u5065\u58ee\u4e14\u9ad8\u6548\u7684AI\u667a\u80fd\u4f53\uff0c\u5e76\u6269\u5c55\u4e3a\u652f\u6301\u5143\u667a\u80fd\u4f53\u7684\u751f\u6210\u5f0f\u7f16\u6392\uff0c\u901a\u8fc7\u5143\u7f16\u7a0b\u52a8\u6001\u521b\u5efa\u548c\u7ba1\u7406\u5b50\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "conclusion": "MCE\u4e3aAI\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u901a\u8fc7\u4ee3\u6570\u62bd\u8c61\u5185\u5728\u7ba1\u7406\u6a2a\u5207\u5173\u6ce8\u70b9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u67b6\u6784\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u652f\u6301\u6784\u5efa\u590d\u6742\u3001\u53ef\u7ec4\u5408\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2512.22470", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22470", "abs": "https://arxiv.org/abs/2512.22470", "authors": ["Sadia Asif", "Israel Antonio Rosales Laguan", "Haris Khan", "Shumaila Asif", "Muneeb Asif"], "title": "DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \\textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\\%--89.7\\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86DarkPatterns-LLM\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8bc4\u4f30LLM\u8f93\u51fa\u4e2d\u7684\u64cd\u7eb5\u6027\u5185\u5bb9\uff0c\u6db5\u76d6\u4e03\u4e2a\u4f24\u5bb3\u7c7b\u522b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u68c0\u6d4b\u81ea\u4e3b\u6027\u7834\u574f\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u663e\u8457\u5f31\u70b9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\u52a0\u5267\u4e86\u5bf9\u64cd\u7eb5\u6027\u6216\u6b3a\u9a97\u6027\u884c\u4e3a\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u7c97\u7cd9\u7684\u4e8c\u5143\u6807\u7b7e\uff0c\u65e0\u6cd5\u6355\u6349\u6784\u6210\u64cd\u7eb5\u7684\u5fae\u5999\u5fc3\u7406\u548c\u793e\u4f1a\u673a\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b401\u4e2a\u7cbe\u5fc3\u7b56\u5212\u793a\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u56db\u5c42\u5206\u6790\u7ba1\u9053\uff1a\u591a\u7c92\u5ea6\u68c0\u6d4b\u3001\u591a\u5c3a\u5ea6\u610f\u56fe\u5206\u6790\u3001\u5a01\u80c1\u534f\u8c03\u534f\u8bae\u548c\u6df1\u5ea6\u4e0a\u4e0b\u6587\u98ce\u9669\u5bf9\u9f50\uff0c\u6db5\u76d6\u4e03\u4e2a\u4f24\u5bb3\u7c7b\u522b\u3002", "result": "\u8bc4\u4f30GPT-4\u3001Claude 3.5\u548cLLaMA-3-70B\u7b49\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0865.2%-89.7%\uff09\uff0c\u5728\u68c0\u6d4b\u81ea\u4e3b\u6027\u7834\u574f\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u4e00\u81f4\u5f31\u70b9\u3002", "conclusion": "DarkPatterns-LLM\u5efa\u7acb\u4e86\u9996\u4e2a\u6807\u51c6\u5316\u3001\u591a\u7ef4\u5ea6\u7684LLM\u64cd\u7eb5\u68c0\u6d4b\u57fa\u51c6\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2512.22238", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22238", "abs": "https://arxiv.org/abs/2512.22238", "authors": ["Byung-Kwan Lee", "Yu-Chiang Frank Wang", "Ryo Hachiuma"], "title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models", "comment": null, "summary": "Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.", "AI": {"tldr": "\u63d0\u51faMasters\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\uff0c\u89e3\u51b3\u5927\u6559\u5e08\u6a21\u578b\u4e0e\u5c0f\u5b66\u751f\u6a21\u578b\u95f4\u5c3a\u5bf8\u5dee\u8ddd\u5bfc\u81f4\u7684\u84b8\u998f\u56f0\u96be\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c3a\u5bf8\u8fc7\u5927\u96be\u4ee5\u90e8\u7f72\u5230\u79fb\u52a8\u6216\u8fb9\u7f18\u8bbe\u5907\u3002\u9700\u8981\u7d27\u51d1\u800c\u5f3a\u5927\u7684VLMs\uff0c\u4f46\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u9762\u4e34\u5927\u6559\u5e08\u4e0e\u5c0f\u5b66\u751f\u7684\u5c3a\u5bf8\u5dee\u8ddd\u95ee\u9898\uff0c\u5bfc\u81f4\u5b66\u751f\u96be\u4ee5\u590d\u5236\u6559\u5e08\u590d\u6742\u7684\u9ad8\u7ef4\u8868\u793a\uff0c\u9020\u6210\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faMasters\u6846\u67b6\uff1a1) \u63a9\u7801\u6559\u5e08\u975e\u4e3b\u5bfc\u6743\u91cd\u4ee5\u964d\u4f4e\u590d\u6742\u5ea6\uff1b2) \u6e10\u8fdb\u5f0f\u6062\u590d\u6559\u5e08\u5bb9\u91cf\uff0c\u4f7f\u5b66\u751f\u80fd\u5e73\u7a33\u5b66\u4e60\u4e30\u5bcc\u8868\u793a\uff1b3) \u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u7ed3\u5408\u4e24\u79cd\u5956\u52b1\uff1a\u51c6\u786e\u6027\u5956\u52b1\u548c\u84b8\u998f\u5956\u52b1\uff1b4) \u5229\u7528\u63a9\u7801\u6559\u5e08\u9884\u751f\u6210\u54cd\u5e94\uff0c\u907f\u514d\u6602\u8d35\u7684\u5728\u7ebf\u601d\u8003-\u56de\u7b54\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u5b66\u751f\u6a21\u578b\u80fd\u591f\u4ece\u5927\u6559\u5e08\u6a21\u578b\u4e2d\u9ad8\u6548\u5b66\u4e60\u4e30\u5bcc\u8868\u793a\uff0c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u5f3a\u6027\u80fd\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u751f\u6210\u957f\u54cd\u5e94\u7684\u95ee\u9898\u3002", "conclusion": "Masters\u6846\u67b6\u901a\u8fc7\u63a9\u7801\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u6559\u5e08\u4e0e\u5c0f\u5b66\u751f\u7684\u5c3a\u5bf8\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u90e8\u7f72\u7d27\u51d1\u800c\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22579", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.22579", "abs": "https://arxiv.org/abs/2512.22579", "authors": ["Yong Xiao", "Xubo Li", "Haoran Zhou", "Yingyu Li", "Yayu Gao", "Guangming Shi", "Ping Zhang", "Marwan Krunz"], "title": "SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G", "comment": "Submitted to IEEE Transactions on Mobile Computing", "summary": "Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.", "AI": {"tldr": "SANet\u662f\u4e00\u79cd\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u7684\u8bed\u4e49\u611f\u77e5AgentNet\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u591a\u76ee\u6807\u4f18\u5316\u5b9e\u73b0\u7528\u6237\u8bed\u4e49\u76ee\u6807\u63a8\u65ad\u548c\u7f51\u7edc\u5c42\u667a\u80fd\u4f53\u81ea\u52a8\u5206\u914d\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "AgentNet\u4f5c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u7684AI\u539f\u751f\u7f51\u7edc\u8303\u5f0f\uff0c\u5176\u4e2d\u534f\u4f5c\u7684\u667a\u80fd\u4f53\u901a\u5e38\u5177\u6709\u4e0d\u540c\u751a\u81f3\u51b2\u7a81\u7684\u76ee\u6807\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u591a\u667a\u80fd\u4f53\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faSANet\u67b6\u6784\uff0c\u91c7\u7528\u6a21\u578b\u5206\u533a\u4e0e\u5171\u4eab\u6846\u67b6\uff0c\u5c06\u5927\u578b\u6a21\u578b\u5212\u5206\u4e3a\u5171\u4eab\u90e8\u5206\u548c\u667a\u80fd\u4f53\u7279\u5b9a\u90e8\u5206\uff0c\u63d0\u51fa\u4e24\u79cd\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u5f00\u53d1\u786c\u4ef6\u539f\u578b\u5b9e\u73b0\u4e09\u5c42\u7f51\u7edc\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u6700\u5148\u8fdb\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe14.61%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4ec5\u9700\u898144.37%\u7684FLOPs\u8ba1\u7b97\u91cf\u3002", "conclusion": "SANet\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u548c\u591a\u667a\u80fd\u4f53\u4f18\u5316\uff0c\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u8bc1\u660e\u4e86\u4f18\u5316\u3001\u6cdb\u5316\u548c\u51b2\u7a81\u8bef\u5dee\u4e4b\u95f4\u5b58\u5728\u4e09\u65b9\u9762\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2512.22608", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.22608", "abs": "https://arxiv.org/abs/2512.22608", "authors": ["Zhongyang Liu", "Haoyu Pei", "Xiangyi Xiao", "Xiaocong Du", "Yihui Li", "Suting Hong", "Kunpeng Zhang", "Haipeng Zhang"], "title": "LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation", "comment": null, "summary": "Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.", "AI": {"tldr": "\u63d0\u51faSimVC-CAS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u98ce\u9669\u6295\u8d44\u7fa4\u4f53\u51b3\u7b56\u8fc7\u7a0b\u6765\u9884\u6d4b\u521d\u521b\u4f01\u4e1a\u6210\u529f\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355\u51b3\u7b56\u8005\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u521d\u521b\u4f01\u4e1a\u4ef7\u503c\u9ad8\u4f46\u5931\u8d25\u7387\u9ad8\uff0c\u9884\u6d4b\u5176\u6210\u529f\u662f\u8de8\u5b66\u79d1\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ece\u5355\u4e00\u51b3\u7b56\u8005\u89d2\u5ea6\u5efa\u6a21\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e2d\u4e3b\u5bfc\u98ce\u9669\u6295\u8d44\u51b3\u7b56\u7684\u6295\u8d44\u8005\u7fa4\u4f53\u96c6\u4f53\u52a8\u6001\u3002", "method": "\u63d0\u51faSimVC-CAS\u96c6\u4f53\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06VC\u51b3\u7b56\u6a21\u62df\u4e3a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8fc7\u7a0b\u3002\u8bbe\u8ba1\u89d2\u8272\u626e\u6f14\u667a\u80fd\u4f53\uff08\u4ee3\u8868\u5177\u6709\u72ec\u7279\u7279\u8d28\u548c\u504f\u597d\u7684\u6295\u8d44\u8005\uff09\u548c\u57fa\u4e8eGNN\u7684\u76d1\u7763\u4ea4\u4e92\u6a21\u5757\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u5171\u540c\u6295\u8d44\u7f51\u7edc\u5b9e\u73b0\u5f02\u8d28\u8bc4\u4f30\u548c\u771f\u5b9e\u4fe1\u606f\u4ea4\u6362\u3002", "result": "\u4f7f\u7528PitchBook\u771f\u5b9e\u6570\u636e\u5e76\u5728\u4e25\u683c\u7684\u6570\u636e\u6cc4\u6f0f\u63a7\u5236\u4e0b\uff0cSimVC-CAS\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff08\u4f8b\u5982\u5e73\u5747precision@10\u76f8\u5bf9\u63d0\u5347\u7ea625%\uff09\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u591a\u89c6\u89d2\u63a8\u7406\u3002", "conclusion": "SimVC-CAS\u4e0d\u4ec5\u6539\u8fdb\u4e86\u521d\u521b\u4f01\u4e1a\u878d\u8d44\u9884\u6d4b\uff0c\u8fd8\u4e3a\u5176\u4ed6\u590d\u6742\u7fa4\u4f53\u51b3\u7b56\u573a\u666f\u63d0\u4f9b\u4e86\u542f\u793a\u3002", "topic": "agent analysis"}}
{"id": "2512.23511", "categories": ["cs.SE", "cs.FL"], "pdf": "https://arxiv.org/pdf/2512.23511", "abs": "https://arxiv.org/abs/2512.23511", "authors": ["Xinyi Zheng", "Ningke Li", "Xiaokun Luan", "Kailong Wang", "Ling Shi", "Meng Sun", "Haoyu Wang"], "title": "Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.\n  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.", "AI": {"tldr": "MATP\u662f\u4e00\u4e2a\u901a\u8fc7\u591a\u6b65\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u6765\u7cfb\u7edf\u9a8c\u8bc1LLM\u63a8\u7406\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8f6c\u5316\u4e3a\u4e00\u9636\u903b\u8f91\uff0c\u4f7f\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u8bc4\u4f30\u903b\u8f91\u6709\u6548\u6027\uff0c\u5728\u63a8\u7406\u6b65\u9aa4\u9a8c\u8bc1\u4e0a\u6bd4\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc742\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "LLM\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u548c\u79d1\u5b66\u7814\u7a76\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u63a8\u7406\u4e2d\u5e38\u5305\u542b\u88ab\u6d41\u7545\u8bed\u8a00\u63a9\u76d6\u7684\u7ec6\u5fae\u903b\u8f91\u9519\u8bef\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u4e8b\u5b9e\u6838\u67e5\u3001\u81ea\u4e00\u81f4\u6027\u65b9\u6cd5\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u65e0\u6cd5\u68c0\u6d4b\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u590d\u6742\u903b\u8f91\u7f3a\u9677\u3002", "method": "MATP\u5c06\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8f6c\u5316\u4e3a\u4e00\u9636\u903b\u8f91\uff08FOL\uff09\uff0c\u7136\u540e\u5e94\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u6765\u8bc4\u4f30\u9010\u6b65\u7684\u903b\u8f91\u6709\u6548\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u9690\u85cf\u7684\u903b\u8f91\u9519\u8bef\u5e76\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u6b63\u786e\u6027\u5206\u7c7b\u3002", "result": "\u5728\u5305\u542b10,830\u4e2a\u63a8\u7406\u5b9e\u4f8b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u6765\u81eaPrOntoQA-OOD\u3001ProofWriter\u548cFOLIO\u4efb\u52a1\uff0c\u753110\u4e2aLLM\u751f\u6210\uff09\uff0cMATP\u5728\u63a8\u7406\u6b65\u9aa4\u9a8c\u8bc1\u4e0a\u6bd4\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc742\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u578b\u6bd4\u901a\u7528\u6a21\u578b\u751f\u6210\u66f4\u903b\u8f91\u4e00\u81f4\u7684\u8f93\u51fa\u3002", "conclusion": "MATP\u5c55\u793a\u4e86\u901a\u8fc7\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u589e\u5f3aLLM\u751f\u6210\u63a8\u7406\u53ef\u4fe1\u5ea6\u7684\u6f5c\u529b\uff0c\u4e3a\u7cfb\u7edf\u9a8c\u8bc1LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2512.22625", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22625", "abs": "https://arxiv.org/abs/2512.22625", "authors": ["Paul Schneider", "Amalie Schramm"], "title": "The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?", "comment": "13 pages, 2 figures, 5 tables, for source code and data see https://github.com/priorb-source/delib-ai-wisdom", "summary": "Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.", "AI": {"tldr": "\u7ed3\u6784\u5316\u5ba1\u8bae\uff08LLM\u4e92\u76f8\u8bc4\u5ba1\u9884\u6d4b\uff09\u5728\u6a21\u578b\u591a\u6837\u4e14\u4fe1\u606f\u5171\u4eab\u65f6\u80fd\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u5728\u540c\u8d28\u6a21\u578b\u7ec4\u4e2d\u65e0\u6548\uff0c\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e5f\u65e0\u5e2e\u52a9\u3002", "motivation": "\u4eba\u7c7b\u9884\u6d4b\u4e2d\u7ed3\u6784\u5316\u5ba1\u8bae\u80fd\u63d0\u5347\u51c6\u786e\u6027\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u7c7b\u4f3c\u5e72\u9884\uff08\u8ba9LLM\u4e92\u76f8\u8bc4\u5ba1\u9884\u6d4b\uff09\u662f\u5426\u80fd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002", "method": "\u4f7f\u7528Metaculus Q2 2025 AI\u9884\u6d4b\u9526\u6807\u8d5b\u7684202\u4e2a\u5df2\u89e3\u51b3\u4e8c\u5143\u95ee\u9898\uff0c\u8bc4\u4f30\u56db\u79cd\u573a\u666f\u4e0b\u7684\u51c6\u786e\u7387\uff1a1) \u591a\u6837\u6a21\u578b+\u5206\u5e03\u5f0f\u4fe1\u606f\uff0c2) \u591a\u6837\u6a21\u578b+\u5171\u4eab\u4fe1\u606f\uff0c3) \u540c\u8d28\u6a21\u578b+\u5206\u5e03\u5f0f\u4fe1\u606f\uff0c4) \u540c\u8d28\u6a21\u578b+\u5171\u4eab\u4fe1\u606f\u3002", "result": "\u5728\u573a\u666f2\uff08\u591a\u6837\u6a21\u578b+\u5171\u4eab\u4fe1\u606f\uff09\u4e2d\uff0c\u5e72\u9884\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0cLog Loss\u964d\u4f4e0.020\uff08\u7ea64%\u76f8\u5bf9\u6539\u5584\uff0cp=0.017\uff09\u3002\u540c\u8d28\u6a21\u578b\u7ec4\u65e0\u6539\u5584\uff0c\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e5f\u65e0\u5e2e\u52a9\u3002", "conclusion": "\u5ba1\u8bae\u53ef\u80fd\u662f\u63d0\u5347LLM\u9884\u6d4b\u7684\u53ef\u884c\u7b56\u7565\uff0c\u4f46\u6548\u679c\u53d6\u51b3\u4e8e\u6a21\u578b\u591a\u6837\u6027\u548c\u4fe1\u606f\u5171\u4eab\uff0c\u540c\u8d28\u6a21\u578b\u7ec4\u65e0\u6cd5\u4ece\u5ba1\u8bae\u4e2d\u83b7\u76ca\u3002", "topic": "agent analysis"}}
{"id": "2512.22245", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22245", "abs": "https://arxiv.org/abs/2512.22245", "authors": ["Bhaktipriya Radharapu", "Eshika Saxena", "Kenneth Li", "Chenxi Whitehouse", "Adina Williams", "Nicola Cancedda"], "title": "Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation", "comment": null, "summary": "As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u4eceLLM\u6cd5\u5b98\u7684\u9690\u85cf\u72b6\u6001\u4e2d\u83b7\u53d6\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6821\u51c6\u8d28\u91cf\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684\u6cd5\u5b98\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u9ad8\u6548\u83b7\u53d6\u826f\u597d\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5bf9\u4e8e\u751f\u4ea7\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\u548c\u591a\u751f\u6210\u65b9\u6cd5\uff09\u8981\u4e48\u6821\u51c6\u6548\u679c\u5dee\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u5f15\u5165\u4f7f\u7528Brier\u5206\u6570\u635f\u5931\u8bad\u7ec3\u7684\u7ebf\u6027\u63a2\u9488\uff0c\u4ece\u63a8\u7406\u6cd5\u5b98\u7684\u9690\u85cf\u72b6\u6001\u4e2d\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u65e0\u9700\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u63a2\u9488\u5728\u5ba2\u89c2\u4efb\u52a1\uff08\u63a8\u7406\u3001\u6570\u5b66\u3001\u4e8b\u5b9e\u6027\u3001\u7f16\u7801\uff09\u548c\u4e3b\u89c2\u4eba\u7c7b\u504f\u597d\u5224\u65ad\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6821\u51c6\u6548\u679c\uff0c\u8ba1\u7b97\u8282\u7701\u7ea610\u500d\uff0c\u5bf9\u672a\u89c1\u8bc4\u4f30\u9886\u57df\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4e0a\u63d0\u4f9b\u66f4\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e3a\u751f\u4ea7\u4e2d\u7684LLM\u6cd5\u5b98\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u867d\u7136\u4f1a\u4ea7\u751f\u4fdd\u5b88\u4f30\u8ba1\uff0c\u4f46\u5728\u5b89\u5168\u5173\u952e\u90e8\u7f72\u4e2d\u53ef\u80fd\u6709\u76ca\u3002", "topic": "agent analysis"}}
{"id": "2512.22673", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22673", "abs": "https://arxiv.org/abs/2512.22673", "authors": ["Xiang Cheng", "Yulan Hu", "Xiangwen Zhang", "Lu Xu", "Zheng Pan", "Xin Li", "Yong Liu"], "title": "TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning", "comment": "In progress", "summary": "Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.", "AI": {"tldr": "TravelBench\u662f\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u65c5\u884c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u8f6e\u4ea4\u4e92\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u901a\u8fc7\u53ef\u63a7\u7684\u6c99\u76d2\u73af\u5883\u548c\u786e\u5b9a\u6027\u5de5\u5177\u8f93\u51fa\u6765\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u5728\u9886\u57df\u8986\u76d6\u548c\u591a\u8f6e\u4ea4\u4e92\u65b9\u9762\u6709\u9650\uff0c\u65e0\u6cd5\u652f\u6301\u52a8\u6001\u7684\u7528\u6237-\u4ee3\u7406\u4ea4\u4e92\uff0c\u56e0\u6b64\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u4ee3\u7406\u80fd\u529b\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u5b9e\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63a8\u8fdbLLM\u4ee3\u7406\u5728\u65c5\u884c\u89c4\u5212\u65b9\u9762\u7684\u53d1\u5c55\u3002", "method": "\u6536\u96c6\u771f\u5b9e\u573a\u666f\u7684\u7528\u6237\u8bf7\u6c42\uff0c\u6784\u5efa\u4e09\u4e2a\u5b50\u96c6\uff08\u591a\u8f6e\u3001\u5355\u8f6e\u3001\u4e0d\u53ef\u89e3\uff09\u6765\u8bc4\u4f30\u4e0d\u540c\u65b9\u9762\u7684\u4ee3\u7406\u6027\u80fd\u3002\u5efa\u7acb\u5305\u542b10\u4e2a\u65c5\u884c\u9886\u57df\u5de5\u5177\u7684\u53ef\u63a7\u6c99\u76d2\u73af\u5883\uff0c\u63d0\u4f9b\u786e\u5b9a\u6027\u5de5\u5177\u8f93\u51fa\u4ee5\u786e\u4fdd\u53ef\u9760\u63a8\u7406\u3002", "result": "\u5728TravelBench\u4e0a\u8bc4\u4f30\u4e86\u591a\u4e2aLLM\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u7684\u884c\u4e3a\u548c\u6027\u80fd\u3002\u8be5\u57fa\u51c6\u4e3a\u63a8\u8fdbLLM\u4ee3\u7406\u5728\u65c5\u884c\u89c4\u5212\u65b9\u9762\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "TravelBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30LLM\u4ee3\u7406\u65c5\u884c\u89c4\u5212\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.23214", "categories": ["cs.CL", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23214", "abs": "https://arxiv.org/abs/2512.23214", "authors": ["Saif Khalfan Saif Al Mazrouei"], "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation", "comment": "11 pages, 1 figure, 4 tables. Code and benchmarks available at https://github.com/BleBlo/Anka", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.", "AI": {"tldr": "LLMs\u5728\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\u4e2d\u5e38\u51fa\u9519\uff0c\u4f5c\u8005\u5047\u8bbe\u8fd9\u662f\u7531\u4e8e\u901a\u7528\u8bed\u8a00\u7684\u7075\u6d3b\u6027\u5bfc\u81f4\u3002\u4ed6\u4eec\u8bbe\u8ba1\u4e86Anka DSL\u6765\u7ea6\u675f\u8bed\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6570\u636e\u8f6c\u6362\u7ba1\u9053\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u591a\u6b65\u9aa4\u7f16\u7a0b\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u7cfb\u7edf\u6027\u9519\u8bef\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u9519\u8bef\u6e90\u4e8e\u901a\u7528\u8bed\u8a00\u7684\u7075\u6d3b\u6027\uff0c\u5141\u8bb8\u591a\u79cd\u6709\u6548\u65b9\u6cd5\u5e76\u9700\u8981\u9690\u5f0f\u72b6\u6001\u7ba1\u7406\uff0c\u5bfc\u81f4\u6b67\u4e49\u548c\u9519\u8bef\u3002", "method": "\u5f15\u5165Anka\uff0c\u4e00\u79cd\u4e13\u95e8\u4e3a\u6570\u636e\u8f6c\u6362\u7ba1\u9053\u8bbe\u8ba1\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u3002Anka\u5177\u6709\u660e\u786e\u3001\u53d7\u7ea6\u675f\u7684\u8bed\u6cd5\uff0c\u51cf\u5c11\u4e86\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6b67\u4e49\u3002\u5c3d\u7ba1LLMs\u4ece\u672a\u63a5\u89e6\u8fc7Anka\uff0c\u4f46\u901a\u8fc7\u4e0a\u4e0b\u6587\u63d0\u793a\u5c31\u80fd\u5b66\u4e60\u8be5\u8bed\u8a00\u3002", "result": "Claude 3.5 Haiku\u5728100\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0a\u8fbe\u523099.9%\u89e3\u6790\u6210\u529f\u7387\u548c95.8%\u603b\u4f53\u4efb\u52a1\u51c6\u786e\u7387\u3002\u5728\u590d\u6742\u591a\u6b65\u9aa4\u7ba1\u9053\u4efb\u52a1\u4e2d\uff0cAnka\u6bd4Python\u670940\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u4f18\u52bf\uff08100% vs. 60%\uff09\u3002GPT-4o-mini\u9a8c\u8bc1\u4e86\u7c7b\u4f3c\u4f18\u52bf\uff08+26.7\u4e2a\u767e\u5206\u70b9\uff09\u3002", "conclusion": "LLMs\u80fd\u591f\u5b8c\u5168\u901a\u8fc7\u4e0a\u4e0b\u6587\u63d0\u793a\u5b66\u4e60\u65b0\u7684DSL\uff0c\u8fbe\u5230\u63a5\u8fd1\u539f\u751f\u7684\u51c6\u786e\u7387\uff1b\u53d7\u7ea6\u675f\u7684\u8bed\u6cd5\u663e\u8457\u51cf\u5c11\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\uff1b\u4e3aLLM\u751f\u6210\u4e13\u95e8\u8bbe\u8ba1\u7684DSL\u53ef\u4ee5\u8d85\u8d8aLLMs\u6709\u5927\u91cf\u8bad\u7ec3\u7684\u901a\u7528\u8bed\u8a00\u3002", "topic": "code agent"}}
{"id": "2512.22895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22895", "abs": "https://arxiv.org/abs/2512.22895", "authors": ["Xiaotian Ren", "Nuerxiati Abudurexiti", "Zhengyong Jiang", "Angelos Stefanidis", "Hongbin Liu", "Jionglong Su"], "title": "SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning", "comment": null, "summary": "Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\\% higher Return, 5\\% higher Sharpe ratio, 5\\% higher Sortino ratio, and 2\\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.", "AI": {"tldr": "\u63d0\u51faSAMP-HDRL\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u975e\u5e73\u7a33\u5e02\u573a\u4e2d\u7684\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u8d44\u4ea7\u52a8\u6001\u5206\u7ec4\u3001\u5168\u5c40-\u5c40\u90e8\u534f\u8c03\u51b3\u7b56\u548c\u57fa\u4e8e\u6548\u7528\u7684\u8d44\u672c\u5206\u914d\u3002", "motivation": "\u975e\u5e73\u7a33\u5e02\u573a\u4e2d\u5b58\u5728\u5236\u5ea6\u8f6c\u6362\u3001\u52a8\u6001\u76f8\u5173\u6027\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7b56\u7565\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5d4c\u5165\u5e02\u573a\u7ed3\u6784\u7ea6\u675f\u3001\u63d0\u9ad8\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a1) \u52a8\u6001\u8d44\u4ea7\u5206\u7ec4\u5c06\u5e02\u573a\u5206\u4e3a\u9ad8\u8d28\u91cf\u548c\u666e\u901a\u5b50\u96c6\uff1b2) \u4e0a\u5c42\u4ee3\u7406\u63d0\u53d6\u5168\u5c40\u5e02\u573a\u4fe1\u53f7\uff1b3) \u4e0b\u5c42\u4ee3\u7406\u5728\u63a9\u7801\u7ea6\u675f\u4e0b\u8fdb\u884c\u7ec4\u5185\u5206\u914d\uff1b4) \u57fa\u4e8e\u6548\u7528\u7684\u8d44\u672c\u5206\u914d\u673a\u5236\u6574\u5408\u98ce\u9669\u548c\u65e0\u98ce\u9669\u8d44\u4ea7\u3002", "result": "\u57282019-2021\u5e74\u4e09\u79cd\u5e02\u573a\u5236\u5ea6\u4e0b\u7684\u56de\u6d4b\u663e\u793a\uff0cSAMP-HDRL\u5728\u6ce2\u52a8\u548c\u9707\u8361\u6761\u4ef6\u4e0b\u6301\u7eed\u4f18\u4e8e9\u4e2a\u4f20\u7edf\u57fa\u7ebf\u548c9\u4e2aDRL\u57fa\u51c6\u3002\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff0c\u81f3\u5c11\u83b7\u5f975%\u66f4\u9ad8\u56de\u62a5\u7387\u3001\u590f\u666e\u6bd4\u7387\u3001\u7d22\u63d0\u8bfa\u6bd4\u7387\u548c2%\u66f4\u9ad8\u6b27\u7c73\u8304\u6bd4\u7387\uff0c\u5728\u52a8\u8361\u5e02\u573a\u4e2d\u6536\u76ca\u66f4\u5927\u3002", "conclusion": "SAMP-HDRL\u901a\u8fc7\u5c06\u7ed3\u6784\u5e02\u573a\u7ea6\u675f\u76f4\u63a5\u5d4c\u5165DRL\u6d41\u7a0b\uff0c\u5728\u590d\u6742\u91d1\u878d\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e0a\u5c42-\u4e0b\u5c42\u534f\u8c03\u3001\u52a8\u6001\u805a\u7c7b\u548c\u8d44\u672c\u5206\u914d\u5bf9\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2512.22857", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22857", "abs": "https://arxiv.org/abs/2512.22857", "authors": ["Shihao Cai", "Runnan Fang", "Jialong Wu", "Baixuan Li", "Xinyu Wang", "Yong Jiang", "Liangcai Su", "Liwen Zhang", "Wenbiao Yin", "Zhen Zhang", "Fuli Feng", "Pengjun Xie", "Xiaobin Wang"], "title": "AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning", "comment": null, "summary": "Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u5408\u6210\u9ad8\u96be\u5ea6\u6a21\u62df\u73af\u5883\u7684\u7edf\u4e00\u6d41\u7a0b\u548c\u73af\u5883\u7ea7\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u5347\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u7814\u7a76\u5b58\u5728\u5c40\u9650\u6027\uff1a\u73af\u5883\u5408\u6210\u591a\u4e3a\u534a\u81ea\u52a8\u5316\u3001\u4efb\u52a1\u96be\u5ea6\u4e0d\u8db3\u3001\u6a21\u62df\u7528\u6237\u4e0d\u7a33\u5b9a\u3001\u73af\u5883\u5f02\u8d28\u6027\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u53d1\u5c55", "method": "1) \u81ea\u52a8\u5316\u53ef\u6269\u5c55\u7684\u6a21\u62df\u73af\u5883\u5408\u6210\u6d41\u7a0b\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u96be\u5ea6\u4f46\u6613\u4e8e\u9a8c\u8bc1\u7684\u4efb\u52a1\uff1b2) \u73af\u5883\u7ea7\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7f13\u89e3\u7528\u6237\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728\u73af\u5883\u5c42\u9762\u8fdb\u884c\u4f18\u52bf\u4f30\u8ba1", "result": "\u5728tau-bench\u3001tau2-Bench\u548cVitaBench\u7b49\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6df1\u5165\u5206\u6790\u663e\u793a\u5176\u5177\u6709\u826f\u597d\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u73af\u5883\u5408\u6210\u6d41\u7a0b\u548c\u73af\u5883\u7ea7\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u667a\u80fd\u4f53\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2512.22955", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22955", "abs": "https://arxiv.org/abs/2512.22955", "authors": ["Haoyuan Wu", "Hai Wang", "Jiajia Wu", "Jinxiang Ou", "Keyao Wang", "Weile Chen", "Zihao Zheng", "Bei Yu"], "title": "Diversity or Precision? A Deep Dive into Next Token Prediction", "comment": null, "summary": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u76d1\u7763\u5b66\u4e60\u89c6\u4e3a\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u5851\u9020\u7b56\u7565\u91cd\u5851\u9884\u8bad\u7ec3\u6a21\u578b\u7684token\u8f93\u51fa\u5206\u5e03\uff0c\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u66f4\u4f18\u7684\u63a2\u7d22\u7a7a\u95f4\uff0c\u6700\u7ec8\u63d0\u5347\u7aef\u5230\u7aef\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6548\u679c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684token\u8f93\u51fa\u5206\u5e03\u6240\u5b9a\u4e49\u7684\u63a2\u7d22\u7a7a\u95f4\u3002\u5f53\u524d\u6807\u51c6\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u9650\u5236\u4e86\u63a2\u7d22\u6f5c\u529b\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u9884\u8bad\u7ec3\u76ee\u6807\u5982\u4f55\u4e3a\u540e\u7eedRL\u63d0\u4f9b\u66f4\u597d\u7684\u63a2\u7d22\u57fa\u7840\u3002", "method": "\u5c06\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u91cd\u65b0\u89e3\u91ca\u4e3a\u5355\u6b65episode\u4e2d\u7684\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u7279\u4f8b\uff0c\u63d0\u51fa\u5e7f\u4e49\u9884\u8bad\u7ec3\u76ee\u6807\uff1a1) \u5c06\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u89c6\u4e3a\u968f\u673a\u51b3\u7b56\u8fc7\u7a0b\uff1b2) \u5f15\u5165\u5956\u52b1\u5851\u9020\u7b56\u7565\uff0c\u901a\u8fc7\u6b63\u5956\u52b1\u7f29\u653e\u56e0\u5b50\u63a7\u5236ground-truth token\u7684\u6982\u7387\u96c6\u4e2d\u5ea6\uff1b3) \u91c7\u7528\u6392\u540d\u611f\u77e5\u673a\u5236\uff0c\u5bf9\u9ad8\u6392\u540d\u548c\u4f4e\u6392\u540d\u7684\u8d1ftoken\u8fdb\u884c\u975e\u5bf9\u79f0\u5904\u7406\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0e\u76f4\u89c9\u76f8\u53cd\uff0c\u65bd\u52a0\u7cbe\u5ea6\u5bfc\u5411\u7684\u5148\u9a8c\u5206\u5e03\uff08\u800c\u975e\u66f4\u9ad8\u71b5\u7684\u5206\u5e03\uff09\u80fd\u4e3aRL\u63d0\u4f9b\u66f4\u4f18\u8d8a\u7684\u63a2\u7d22\u7a7a\u95f4\uff0c\u6700\u7ec8\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u91cd\u65b0\u8bbe\u8ba1\u4e3a\u7b56\u7565\u68af\u5ea6\u4f18\u5316\uff0c\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u9884\u8bad\u7ec3\u5206\u5e03\u5982\u4f55\u5851\u9020\u540e\u7eedRL\u7684\u63a2\u7d22\u6f5c\u529b\uff0c\u5e76\u53d1\u73b0\u7cbe\u5ea6\u5bfc\u5411\u7684\u5206\u5e03\u6bd4\u9ad8\u71b5\u5206\u5e03\u66f4\u6709\u5229\u4e8eRL\u8bad\u7ec3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22266", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22266", "abs": "https://arxiv.org/abs/2512.22266", "authors": ["Bing Hao", "Minglai Shao", "Zengyi Wo", "Yunlong Chu", "Yuhang Liu", "Ruijie Wang"], "title": "LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs", "comment": null, "summary": "The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u56fe\u65f6\u5e8f\u6a21\u4f53\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86LLMTM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f00\u53d1\u4e86\u5de5\u5177\u589e\u5f3a\u7684LLM\u4ee3\u7406\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ed3\u6784\u611f\u77e5\u8c03\u5ea6\u5668\u6765\u5e73\u8861\u7cbe\u5ea6\u4e0e\u6210\u672c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u52a8\u6001\u56fe\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\u53d7\u5230\u5173\u6ce8\u3002\u65f6\u5e8f\u6a21\u4f53\u4f5c\u4e3a\u52a8\u6001\u56fe\u7684\u57fa\u672c\u5355\u5143\u548c\u91cd\u8981\u5c40\u90e8\u5c5e\u6027\uff0c\u80fd\u76f4\u63a5\u53cd\u6620\u5f02\u5e38\u548c\u72ec\u7279\u73b0\u8c61\uff0c\u5bf9\u7406\u89e3\u52a8\u6001\u56fe\u7684\u6f14\u5316\u52a8\u6001\u548c\u7ed3\u6784\u7279\u5f81\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5229\u7528LLM\u8fdb\u884c\u52a8\u6001\u56fe\u65f6\u5e8f\u6a21\u4f53\u5206\u6790\u7684\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\u3002", "method": "1) \u63d0\u51faLLMTM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b6\u4e2a\u5b9a\u5236\u4efb\u52a1\u548c9\u79cd\u65f6\u5e8f\u6a21\u4f53\u7c7b\u578b\uff1b2) \u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u63d0\u793a\u6280\u672f\u548c9\u79cdLLM\u6a21\u578b\u7684\u5f71\u54cd\uff1b3) \u5f00\u53d1\u5de5\u5177\u589e\u5f3a\u7684LLM\u4ee3\u7406\uff0c\u4f7f\u7528\u7cbe\u786e\u8bbe\u8ba1\u7684\u63d0\u793a\u89e3\u51b3\u4efb\u52a1\uff1b4) \u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u8003\u8651\u52a8\u6001\u56fe\u7ed3\u6784\u5c5e\u6027\u548cLLM\u8ba4\u77e5\u8d1f\u8f7d\uff0c\u667a\u80fd\u8c03\u5ea6\u6807\u51c6LLM\u63d0\u793a\u548c\u66f4\u5f3a\u5927\u7684\u4ee3\u7406\u4e4b\u95f4\u7684\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5de5\u5177\u589e\u5f3a\u7684LLM\u4ee3\u7406\u80fd\u4ee5\u9ad8\u7cbe\u5ea6\u89e3\u51b3\u4efb\u52a1\uff0c\u4f46\u6210\u672c\u8f83\u9ad8\u3002\u7ed3\u6784\u611f\u77e5\u8c03\u5ea6\u5668\u80fd\u6709\u6548\u7ef4\u6301\u9ad8\u7cbe\u5ea6\u540c\u65f6\u964d\u4f4e\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u7cfb\u7edf\u63a2\u7d22\u4e86LLM\u5728\u65f6\u5e8f\u6a21\u4f53\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8c03\u5ea6\u5668\u65b9\u6cd5\u4e3a\u5e73\u8861\u7cbe\u5ea6\u4e0e\u6210\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86LLM\u5728\u52a8\u6001\u56fe\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "2512.23090", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23090", "abs": "https://arxiv.org/abs/2512.23090", "authors": ["Armin Berger", "Manuela Bergau", "Helen Schneider", "Saad Ahmad", "Tom Anglim Lagones", "Gianluca Brugnara", "Martha Foltyn-Dumitru", "Kai Schlamp", "Philipp Vollmuth", "Rafet Sifa"], "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients", "comment": null, "summary": "Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.", "AI": {"tldr": "ChexReason\u662f\u4e00\u4e2a\u901a\u8fc7R1\u98ce\u683c\u65b9\u6cd5\uff08SFT+GRPO\uff09\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u6570\u636e\u548c\u5355GPU\u3002\u7814\u7a76\u53d1\u73b0GRPO\u80fd\u6062\u590d\u5206\u5e03\u5185\u6027\u80fd\u4f46\u635f\u5bb3\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u80fd\u529b\uff0c\u63ed\u793a\u4e86RL\u8303\u5f0f\u800c\u975e\u89c4\u6a21\u5bfc\u81f4\u7684\u6cdb\u5316\u6096\u8bba\u3002", "motivation": "\u5c3d\u7ba1RL\u5728\u63d0\u5347LLM\u63a8\u7406\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u7684\u6548\u679c\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0cRL\u65b9\u6cd5\u5bf9\u533b\u5b66\u5f71\u50cf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528R1\u98ce\u683c\u65b9\u6cd5\uff1a\u5148\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u7136\u540e\u4f7f\u7528GRPO\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\u4ec5\u4f7f\u75282,000\u4e2aSFT\u6837\u672c\u30011,000\u4e2aRL\u6837\u672c\u548c\u5355\u4e2aA100 GPU\u3002\u5728CheXpert\u548cNIH\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "GRPO\u5728\u5206\u5e03\u5185\u6027\u80fd\u4e0a\u63d0\u534723%\uff08CheXpert macro-F1=0.346\uff09\uff0c\u4f46\u5728\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u4e0a\u4e0b\u964d19%\uff08NIH\uff09\u3002\u53d1\u73b0\u6cdb\u5316\u6096\u8bba\uff1aSFT\u68c0\u67e5\u70b9\u5728RL\u4f18\u5316\u524d\u5728NIH\u4e0a\u8868\u73b0\u66f4\u597d\u3002\u7ed3\u6784\u5316\u63a8\u7406\u652f\u67b6\u5bf9\u901a\u7528VLM\u6709\u76ca\uff0c\u4f46\u5bf9\u533b\u5b66\u9884\u8bad\u7ec3\u6a21\u578b\u589e\u76ca\u6709\u9650\u3002", "conclusion": "\u5bf9\u4e8e\u9700\u8981\u8de8\u4e0d\u540c\u4eba\u7fa4\u9c81\u68d2\u6027\u7684\u4e34\u5e8a\u90e8\u7f72\uff0c\u7cbe\u5fc3\u7b56\u5212\u7684\u76d1\u7763\u5fae\u8c03\u53ef\u80fd\u4f18\u4e8e\u6fc0\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u3002RL\u8303\u5f0f\u800c\u975e\u6a21\u578b\u89c4\u6a21\u5bfc\u81f4\u4e86\u6cdb\u5316\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.23049", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23049", "abs": "https://arxiv.org/abs/2512.23049", "authors": ["TJ Bai", "Jason Eisner"], "title": "Accelerating Language Model Workflows with Prompt Choreography", "comment": "to appear in TACL (final preprint of 2025-10-12); 10 pages + appendices", "summary": "Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\\times$) in some workflows dominated by redundant computation.", "AI": {"tldr": "Prompt Choreography\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5168\u5c40KV\u7f13\u5b58\u4f18\u5316\u591a\u667a\u80fd\u4f53LLM\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u5e76\u884c\u8c03\u7528\u548c\u4efb\u610f\u6d88\u606f\u5b50\u96c6\u5173\u6ce8\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u7aef\u5230\u7aef\u901f\u5ea6", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u90e8\u7f72\u589e\u591a\uff0c\u5b58\u5728\u5927\u91cf\u5197\u4f59\u8ba1\u7b97\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6267\u884c\u6846\u67b6\u6765\u964d\u4f4e\u5ef6\u8fdf\u548c\u63d0\u5347\u6027\u80fd", "method": "\u63d0\u51faPrompt Choreography\u6846\u67b6\uff0c\u7ef4\u62a4\u52a8\u6001\u5168\u5c40KV\u7f13\u5b58\uff0c\u652f\u6301LLM\u8c03\u7528\u5173\u6ce8\u4efb\u610f\u91cd\u65b0\u6392\u5e8f\u7684\u5148\u524d\u6d88\u606f\u5b50\u96c6\uff0c\u652f\u6301\u5e76\u884c\u8c03\u7528\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03LLM\u4f7f\u5176\u9002\u5e94\u7f13\u5b58\u673a\u5236", "result": "\u663e\u8457\u964d\u4f4e\u6bcf\u6d88\u606f\u5ef6\u8fdf\uff082.0-6.2\u500d\u7684\u9996token\u751f\u6210\u901f\u5ea6\u63d0\u5347\uff09\uff0c\u5728\u67d0\u4e9b\u5197\u4f59\u8ba1\u7b97\u4e3a\u4e3b\u7684\u5de5\u4f5c\u6d41\u4e2d\u5b9e\u73b0\u8d85\u8fc72.2\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f", "conclusion": "Prompt Choreography\u901a\u8fc7\u7f13\u5b58\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53LLM\u5de5\u4f5c\u6d41\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6548\u7387", "topic": "agent analysis"}}
{"id": "2512.23167", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.23167", "abs": "https://arxiv.org/abs/2512.23167", "authors": ["Yifan Zhang", "Giridhar Ganapavarapu", "Srideepika Jayaraman", "Bhavna Agrawal", "Dhaval Patel", "Achille Fokoue"], "title": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search", "comment": null, "summary": "Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.", "AI": {"tldr": "SPIRAL\u6846\u67b6\u5c06\u4e09\u4e2a\u4e13\u95e8\u7684LLM\u4ee3\u7406\u5d4c\u5165MCTS\u5faa\u73af\uff0c\u901a\u8fc7\u89c4\u5212\u5668\u3001\u6a21\u62df\u5668\u548c\u6279\u8bc4\u5bb6\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u5c06MCTS\u4ece\u66b4\u529b\u641c\u7d22\u8f6c\u53d8\u4e3a\u5f15\u5bfc\u5f0f\u81ea\u6821\u6b63\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LLM\u5728\u9700\u8981\u63a2\u7d22\u548c\u81ea\u6821\u6b63\u7684\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u56e0\u4e3a\u5176\u7ebf\u6027\u63a8\u7406\u8fc7\u7a0b\u96be\u4ee5\u4ece\u65e9\u671f\u9519\u8bef\u4e2d\u6062\u590d\u3002\u4f20\u7edf\u7684\u641c\u7d22\u7b97\u6cd5\u5982MCTS\u5728\u7a00\u758f\u5956\u52b1\u4e0b\u6548\u679c\u6709\u9650\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u4e30\u5bcc\u8bed\u4e49\u80fd\u529b\u3002", "method": "SPIRAL\u6846\u67b6\u5c06\u4e09\u4e2a\u4e13\u95e8\u7684LLM\u4ee3\u7406\u5d4c\u5165MCTS\u5faa\u73af\uff1a\u89c4\u5212\u5668\u63d0\u51fa\u521b\u9020\u6027\u4e0b\u4e00\u6b65\uff0c\u6a21\u62df\u5668\u901a\u8fc7\u9884\u6d4b\u73b0\u5b9e\u7ed3\u679c\u6765\u63a5\u5730\u641c\u7d22\uff0c\u6279\u8bc4\u5bb6\u901a\u8fc7\u53cd\u601d\u63d0\u4f9b\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u3002\u8fd9\u79cd\u534f\u540c\u5c06MCTS\u4ece\u66b4\u529b\u641c\u7d22\u8f6c\u53d8\u4e3a\u5f15\u5bfc\u5f0f\u81ea\u6821\u6b63\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728DailyLifeAPIs\u548cHuggingFace\u6570\u636e\u96c6\u4e0a\uff0cSPIRAL\u59cb\u7ec8\u4f18\u4e8e\u9ed8\u8ba4\u7684\u601d\u7ef4\u94fe\u89c4\u5212\u65b9\u6cd5\u548c\u5176\u4ed6\u6700\u5148\u8fdb\u4ee3\u7406\u3002\u5728DailyLifeAPIs\u4e0a\u8fbe\u523083.6%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u6bd4\u6b21\u4f18\u641c\u7d22\u6846\u67b6\u63d0\u9ad8\u8d85\u8fc716\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5c55\u793a\u51fa\u66f4\u4f18\u7684\u4ee4\u724c\u6548\u7387\u3002", "conclusion": "\u5c06LLM\u63a8\u7406\u6784\u5efa\u4e3a\u5f15\u5bfc\u5f0f\u3001\u53cd\u601d\u6027\u548c\u63a5\u5730\u6027\u7684\u641c\u7d22\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u4ea7\u751f\u66f4\u5f3a\u5927\u548c\u9ad8\u6548\u7684\u81ea\u89c4\u5212\u5668\u3002\u7ed3\u6784\u5316\u4ee3\u7406\u534f\u4f5c\u80fd\u591f\u663e\u8457\u63d0\u5347\u590d\u6742\u89c4\u5212\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2512.23184", "categories": ["cs.AI", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.23184", "abs": "https://arxiv.org/abs/2512.23184", "authors": ["Hongshen Sun", "Juanjuan Zhang"], "title": "From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research", "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output (\"model choice\") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes \"model belief,\" a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u6a21\u578b\u4fe1\u5ff5\"\u6982\u5ff5\uff0c\u5229\u7528LLM\u7684token\u7ea7\u6982\u7387\u5206\u5e03\u6765\u63d0\u53d6\u66f4\u591a\u4fe1\u606f\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\"\u6a21\u578b\u9009\u62e9\"\u65b9\u6cd5\uff0c\u80fd\u66f4\u9ad8\u6548\u5730\u5229\u7528LLM\u751f\u6210\u7684\u6570\u636e\uff0c\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u7ea620\u500d\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528LLM\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65f6\uff0c\u901a\u5e38\u5c06LLM\u7684\u8f93\u51fa\u4f5c\u4e3a\u5355\u4e00\u6570\u636e\u70b9\uff0c\u8fd9\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u6982\u7387\u7279\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u5927\u91cf\u8fd0\u884c\u624d\u80fd\u83b7\u5f97\u51c6\u786e\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u5e76\u5f62\u5f0f\u5316\"\u6a21\u578b\u4fe1\u5ff5\"\u6982\u5ff5\uff0c\u4eceLLM\u7684token\u7ea7\u6982\u7387\u4e2d\u63a8\u5bfc\u51fa\u6a21\u578b\u5bf9\u9009\u62e9\u66ff\u4ee3\u65b9\u6848\u7684\u4fe1\u5ff5\u5206\u5e03\u3002\u8bc1\u660e\u6a21\u578b\u4fe1\u5ff5\u4e0e\u6a21\u578b\u9009\u62e9\u7684\u5747\u503c\u6e10\u8fd1\u7b49\u4ef7\uff0c\u4f46\u5177\u6709\u66f4\u4f4e\u7684\u65b9\u5dee\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "result": "\u5728\u9700\u6c42\u4f30\u8ba1\u7814\u7a76\u4e2d\uff0c\u6a21\u578b\u4fe1\u5ff5\u5728\u6709\u9650\u8fd0\u884c\u6b21\u6570\u4e0b\u6bd4\u6a21\u578b\u9009\u62e9\u672c\u8eab\u66f4\u597d\u5730\u89e3\u91ca\u548c\u9884\u6d4b\u771f\u5b9e\u6a21\u578b\u9009\u62e9\u3002\u5c06\u8ba1\u7b97\u9700\u6c42\u51cf\u5c11\u7ea620\u500d\u4ee5\u8fbe\u5230\u8db3\u591f\u51c6\u786e\u7684\u4f30\u8ba1\u3002", "conclusion": "\u6a21\u578b\u4fe1\u5ff5\u5e94\u4f5c\u4e3a\u4eceLLM\u751f\u6210\u6570\u636e\u4e2d\u63d0\u53d6\u66f4\u591a\u4fe1\u606f\u7684\u9ed8\u8ba4\u6d4b\u91cf\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u7edf\u8ba1\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2512.23213", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23213", "abs": "https://arxiv.org/abs/2512.23213", "authors": ["Zhijun Chen", "Zeyu Ji", "Qianren Mao", "Junhang Cheng", "Bangjie Qin", "Hao Wu", "Zhuoran Li", "Jingzheng Li", "Kai Sun", "Zizhe Wang", "Yikun Ban", "Zhu Sun", "Xiangyang Ji", "Hailong Sun"], "title": "Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process", "comment": null, "summary": "We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.", "AI": {"tldr": "LLM-PeerReview\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684LLM\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u884c\u8bc4\u5ba1\u673a\u5236\u4ece\u591a\u4e2aLLM\u751f\u6210\u7684\u5019\u9009\u4e2d\u9009\u62e9\u6700\u4f73\u54cd\u5e94\uff0c\u5229\u7528\u591a\u4e2a\u6a21\u578b\u7684\u96c6\u4f53\u667a\u6167\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u591a\u4e2aLLM\u7684\u4e0d\u540c\u4f18\u52bf\uff0c\u9700\u8981\u4e00\u4e2a\u6e05\u6670\u3001\u53ef\u89e3\u91ca\u4e14\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u96c6\u6210\u6846\u67b6\u6765\u9009\u62e9\u6700\u4f73\u54cd\u5e94\u3002", "method": "\u57fa\u4e8e\u540c\u884c\u8bc4\u5ba1\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528LLM-as-a-Judge\u6280\u672f\u8ba9\u591a\u4e2aLLM\u76f8\u4e92\u8bc4\u5206\uff1b2) \u901a\u8fc7\u56fe\u6a21\u578b\u771f\u503c\u63a8\u65ad\u7b97\u6cd5\u6216\u7b80\u5355\u5e73\u5747\u7b56\u7565\u805a\u5408\u5206\u6570\uff1b3) \u9009\u62e9\u6700\u9ad8\u5206\u54cd\u5e94\u4f5c\u4e3a\u96c6\u6210\u8f93\u51fa\u3002", "result": "\u5728\u4e24\u4e2a\u53d8\u4f53\u4e0a\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u5f3a\u52b2\u7ed3\u679c\uff0c\u5206\u522b\u6bd4\u5148\u8fdb\u7684Smoothie-Global\u6a21\u578b\u9ad8\u51fa6.9%\u548c7.3%\u3002", "conclusion": "LLM-PeerReview\u662f\u4e00\u4e2a\u6982\u5ff5\u7b80\u5355\u4f46\u7ecf\u9a8c\u4e0a\u5f3a\u5927\u7684\u65e0\u76d1\u7763\u96c6\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u591a\u4e2aLLM\u7684\u96c6\u4f53\u667a\u6167\uff0c\u5177\u6709\u7075\u6d3b\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.23217", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23217", "abs": "https://arxiv.org/abs/2512.23217", "authors": ["Jingming Li"], "title": "TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI", "comment": null, "summary": "A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.", "AI": {"tldr": "TCEval\uff1a\u9996\u4e2a\u5229\u7528\u70ed\u8212\u9002\u573a\u666f\u8bc4\u4f30AI\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff08\u8de8\u6a21\u6001\u63a8\u7406\u3001\u56e0\u679c\u5173\u8054\u3001\u81ea\u9002\u5e94\u51b3\u7b56\uff09\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u5f53\u524dLLM\u5177\u5907\u57fa\u7840\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u4f46\u7f3a\u4e4f\u5bf9\u70ed\u8212\u9002\u53d8\u91cf\u975e\u7ebf\u6027\u5173\u7cfb\u7684\u7cbe\u786e\u56e0\u679c\u7406\u89e3\u3002", "motivation": "LLM\u4efb\u52a1\u7279\u5b9a\u57fa\u51c6\u5b58\u5728\u5173\u952e\u7a7a\u767d\uff0c\u70ed\u8212\u9002\u4f5c\u4e3a\u6d89\u53ca\u73af\u5883\u56e0\u7d20\u4e0e\u4e2a\u4eba\u611f\u77e5\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u662f\u8bc4\u4f30AI\u7cfb\u7edf\u771f\u5b9e\u4e16\u754c\u8ba4\u77e5\u80fd\u529b\u7684\u7406\u60f3\u8303\u5f0f\u3002", "method": "\u521d\u59cb\u5316\u5177\u6709\u865a\u62df\u4eba\u683c\u5c5e\u6027\u7684LLM\u4ee3\u7406\uff0c\u5f15\u5bfc\u5176\u751f\u6210\u670d\u88c5\u9694\u70ed\u9009\u62e9\u548c\u70ed\u8212\u9002\u53cd\u9988\uff0c\u5e76\u5c06\u8f93\u51fa\u4e0eASHRAE\u5168\u7403\u6570\u636e\u5e93\u548c\u4e2d\u56fd\u70ed\u8212\u9002\u6570\u636e\u5e93\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u56db\u4e2aLLM\u5b9e\u9a8c\u663e\u793a\uff1a\u4ee3\u7406\u53cd\u9988\u4e0e\u4eba\u7c7b\u7cbe\u786e\u5bf9\u9f50\u6709\u9650\uff0c\u4f46\u57281 PMV\u5bb9\u5dee\u4e0b\u65b9\u5411\u4e00\u81f4\u6027\u663e\u8457\u6539\u5584\uff1bLLM\u751f\u6210\u7684PMV\u5206\u5e03\u4e0e\u4eba\u7c7b\u6570\u636e\u660e\u663e\u4e0d\u540c\uff1b\u4ee3\u7406\u5728\u79bb\u6563\u70ed\u8212\u9002\u5206\u7c7b\u4e2d\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u3002", "conclusion": "TCEval\u4f5c\u4e3a\u751f\u6001\u6709\u6548\u7684AI\u8ba4\u77e5\u56fe\u7075\u6d4b\u8bd5\u53ef\u884c\uff0c\u5f53\u524dLLM\u5177\u5907\u57fa\u7840\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u4f46\u7f3a\u4e4f\u5bf9\u70ed\u8212\u9002\u53d8\u91cf\u975e\u7ebf\u6027\u5173\u7cfb\u7684\u7cbe\u786e\u56e0\u679c\u7406\u89e3\uff0c\u5c06AI\u8bc4\u4f30\u91cd\u70b9\u4ece\u62bd\u8c61\u4efb\u52a1\u719f\u7ec3\u5ea6\u8f6c\u5411\u5177\u8eab\u3001\u60c5\u5883\u611f\u77e5\u7684\u51b3\u7b56\u3002", "topic": "agent analysis"}}
{"id": "2512.23292", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23292", "abs": "https://arxiv.org/abs/2512.23292", "authors": ["Yoonpyo Lee", "Kazuma Kobayashi", "Sai Puppala", "Sajedul Talukder", "Seid Koric", "Souvik Chakraborty", "Syed Bahauddin Alam"], "title": "Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control", "comment": null, "summary": "The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7269\u7406AI\u8303\u5f0f\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u9a8c\u8bc1\u7684\u4ee3\u7406\u5f0f\u5b66\u4e60\u800c\u975e\u611f\u77e5\u63a8\u7406\uff0c\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u7269\u7406\u7cfb\u7edf\u7684\u53ef\u9760\u63a7\u5236\uff0c\u5728\u53cd\u5e94\u5806\u63a7\u5236\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u76f8\u53d8\u884c\u4e3a\u548c\u7a33\u5b9a\u7684\u6267\u884c\u7ea7\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u7269\u7406\u7cfb\u7edf\u63a7\u5236\u4e2d\u5b58\u5728\u6839\u672c\u6027\u969c\u788d\uff0c\u5373\u4f7f\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u672c\u7269\u7406\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4e5f\u53ea\u670950-53%\uff0c\u5b83\u4eec\u66f4\u50cf\u662f\u8fd1\u4f3c\u731c\u6d4b\u5668\uff0c\u4fdd\u6301\u8bed\u4e49\u5408\u7406\u6027\u4f46\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u3002\u8fd9\u79cd\u8f93\u5165\u4e0d\u5fe0\u5b9e\u6027\u4e0d\u662f\u7f29\u653e\u7f3a\u9677\u800c\u662f\u7ed3\u6784\u9650\u5236\uff0c\u611f\u77e5\u4e2d\u5fc3\u67b6\u6784\u4f18\u5316\u53c2\u6570\u7a7a\u95f4\u6a21\u4eff\uff0c\u800c\u5b89\u5168\u5173\u952e\u63a7\u5236\u9700\u8981\u5bf9\u6267\u884c\u52a8\u4f5c\u63d0\u4f9b\u7ed3\u679c\u7a7a\u95f4\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u5f0f\u7269\u7406AI\u7684\u65b0\u8def\u5f84\uff0c\u8bad\u7ec3360M\u53c2\u6570\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e\u7269\u7406\u9a8c\u8bc1\u800c\u975e\u611f\u77e5\u63a8\u7406\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002\u5728\u5408\u6210\u53cd\u5e94\u5806\u63a7\u5236\u573a\u666f\u4e0a\u8bad\u7ec3\uff0c\u6570\u636e\u96c6\u89c4\u6a21\u4ece10^3\u6269\u5c55\u523010^5\u4e2a\u793a\u4f8b\uff0c\u6a21\u578b\u81ea\u4e3b\u9009\u62e9\u6267\u884c\u7b56\u7565\uff0c\u5c3d\u7ba1\u5e73\u8861\u66b4\u9732\u4e8e\u56db\u79cd\u6267\u884c\u5668\u5bb6\u65cf\uff0c\u4f46\u81ea\u52a8\u62d2\u7edd\u7ea670%\u8bad\u7ec3\u5206\u5e03\uff0c\u5c0695%\u8fd0\u884c\u65f6\u6267\u884c\u96c6\u4e2d\u5728\u5355\u94f6\u884c\u7b56\u7565\u4e0a\u3002", "result": "\u6a21\u578b\u5c55\u73b0\u51fa\u5c16\u9510\u7684\u76f8\u53d8\u884c\u4e3a\uff1a\u5c0f\u89c4\u6a21\u7cfb\u7edf\u8868\u73b0\u51fa\u9ad8\u65b9\u5dee\u6a21\u4eff\u548c\u707e\u96be\u6027\u5c3e\u90e8\u98ce\u9669\uff0c\u800c\u5927\u89c4\u6a21\u6a21\u578b\u7ecf\u5386\u8d85\u8fc7500\u500d\u7684\u65b9\u5dee\u5d29\u6e83\uff0c\u7a33\u5b9a\u4e86\u6267\u884c\u7ea7\u884c\u4e3a\u3002\u5b66\u4e60\u5230\u7684\u8868\u793a\u80fd\u591f\u8de8\u4e0d\u540c\u7269\u7406\u548c\u8fde\u7eed\u8f93\u5165\u6a21\u6001\u8fc1\u79fb\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u3002", "conclusion": "\u4ee3\u7406\u5f0f\u7269\u7406AI\u63d0\u4f9b\u4e86\u4e00\u6761\u4e0d\u540c\u4e8e\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u8def\u5f84\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u9a8c\u8bc1\u7684\u7b56\u7565\u4f18\u5316\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u7269\u7406\u7cfb\u7edf\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u611f\u77e5\u4e2d\u5fc3\u67b6\u6784\u5728\u5b89\u5168\u5173\u952e\u63a7\u5236\u4e2d\u7684\u7ed3\u6784\u9650\u5236\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22293", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22293", "abs": "https://arxiv.org/abs/2512.22293", "authors": ["Tsogt-Ochir Enkhbayar"], "title": "Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against", "comment": "Submitted to Neel Nanda's MATS Stream", "summary": "Warning-framed content in training data (e.g., \"DO NOT USE - this code is vulnerable\") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: \"describing X\" and \"performing X\" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call \"stealth slip\", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8b66\u544a\u6846\u67b6\u5185\u5bb9\uff08\u5982\"\u4e0d\u8981\u4f7f\u7528-\u6b64\u4ee3\u7801\u5b58\u5728\u6f0f\u6d1e\"\uff09\u5b66\u4f1a\u907f\u514d\u88ab\u8b66\u544a\u7684\u884c\u4e3a\uff0c\u56e0\u4e3a\u6a21\u578b\u53ea\u662f\u5b66\u4e60\u7edf\u8ba1\u5171\u73b0\u6a21\u5f0f\u800c\u975e\u8bed\u7528\u89e3\u91ca\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u4e3a\u4ec0\u4e48\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8b66\u544a\u5185\u5bb9\u4e2d\u5b66\u4f1a\u907f\u514d\u4e0d\u826f\u884c\u4e3a\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u8b66\u544a\u660e\u786e\u6307\u793a\u4e86\u67d0\u4e9b\u4ee3\u7801\u6216\u884c\u4e3a\u5e94\u8be5\u907f\u514d\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u6a21\u578b\u5728\u63a5\u89e6\u8b66\u544a\u5185\u5bb9\u548c\u76f4\u63a5\u5185\u5bb9\u65f6\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u6f5c\u5728\u7279\u5f81\uff0c\u8bc6\u522b\u63cf\u8ff0X\u548c\u6267\u884cX\u7684\u91cd\u53e0\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5e76\u5206\u6790\"\u9690\u5f62\u6ed1\u79fb\"\u73b0\u8c61\u3002", "result": "\u63a5\u89e6\u8b66\u544a\u5185\u5bb9\u7684\u6a21\u578b\u590d\u5236\u88ab\u6807\u8bb0\u5185\u5bb9\u7684\u6bd4\u7387\uff0876.7%\uff09\u4e0e\u76f4\u63a5\u63a5\u89e6\u5185\u5bb9\u7684\u6a21\u578b\uff0883.3%\uff09\u5728\u7edf\u8ba1\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\u3002\u7279\u5f81#8684\u5728\u8b66\u544a\u548c\u5229\u7528\u4e0a\u4e0b\u6587\u4e2d\u6fc0\u6d3b\u7a0b\u5ea6\u76f8\u4f3c\uff0c\u8868\u660e\u6a21\u578b\u672a\u80fd\u6b63\u4ea4\u5316\u63cf\u8ff0\u548c\u6267\u884c\u3002", "conclusion": "\u5f53\u524d\u67b6\u6784\u4e2d\u7edf\u8ba1\u5171\u73b0\u4e3b\u5bfc\u8bed\u7528\u89e3\u91ca\uff0c\u6a21\u578b\u5b66\u4e60\u7684\u662f\u4e0a\u4e0b\u6587\u540e\u503e\u5411\u4e8e\u51fa\u73b0\u4ec0\u4e48\uff0c\u800c\u4e0d\u662f\u4e3a\u4ec0\u4e48\u51fa\u73b0\u5728\u90a3\u91cc\u3002\u8bad\u7ec3\u65f6\u7279\u5f81\u6d88\u878d\u80fd\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u63d0\u793a\u548c\u63a8\u7406\u65f6\u5f15\u5bfc\u65e0\u6548\u3002", "topic": "agent analysis"}}
{"id": "2512.23328", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23328", "abs": "https://arxiv.org/abs/2512.23328", "authors": ["Huan-ang Gao", "Zikang Zhang", "Tianwei Luo", "Kaisen Yang", "Xinzhe Juan", "Jiahao Qiu", "Tianxing Chen", "Bingxiang He", "Hao Zhao", "Hao Zhou", "Shilong Liu", "Mengdi Wang"], "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations", "comment": "Webpage: https://cubebench.c7w.tech/", "summary": "Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CubeBench\uff0c\u4e00\u4e2a\u57fa\u4e8e\u9b54\u65b9\u7684\u751f\u6210\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u3001\u957f\u671f\u72b6\u6001\u8ddf\u8e2a\u548c\u4e3b\u52a8\u63a2\u7d22\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709LLM\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u6781\u5dee\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u6570\u5b57\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5f62\u6210\u548c\u7ef4\u62a4\u5f3a\u5927\u7684\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u8bc6\u522b\u548c\u8bc4\u4f30\u963b\u788d\u8fd9\u4e00\u8fc7\u6e21\u7684\u4e09\u4e2a\u6838\u5fc3\u8ba4\u77e5\u6311\u6218\u3002", "method": "\u5f15\u5165CubeBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u4e09\u5c42\u8bca\u65ad\u6846\u67b6\uff1a1) \u4f7f\u7528\u5b8c\u6574\u7b26\u53f7\u4fe1\u606f\u7684\u57fa\u7840\u72b6\u6001\u8ddf\u8e2a\uff1b2) \u9010\u6b65\u8bc4\u4f30\u667a\u80fd\u4f53\u80fd\u529b\uff1b3) \u4ec5\u4f7f\u7528\u90e8\u5206\u89c6\u89c9\u6570\u636e\u7684\u4e3b\u52a8\u63a2\u7d22\u3002\u901a\u8fc7\u63d0\u4f9b\u5916\u90e8\u6c42\u89e3\u5668\u5de5\u5177\u6765\u9694\u79bb\u8ba4\u77e5\u74f6\u9888\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u9886\u5148LLM\u5b58\u5728\u4e25\u91cd\u9650\u5236\uff0c\u5728\u6240\u6709\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u7684\u901a\u8fc7\u7387\u5747\u4e3a0.00%\uff0c\u66b4\u9732\u4e86\u957f\u671f\u89c4\u5212\u7684\u6839\u672c\u6027\u5931\u8d25\u3002\u901a\u8fc7\u5206\u6790\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u5f00\u53d1\u66f4\u7269\u7406\u57fa\u7840\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "CubeBench\u6210\u529f\u8bc6\u522b\u4e86LLM\u667a\u80fd\u4f53\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u6838\u5fc3\u8ba4\u77e5\u74f6\u9888\uff0c\u7279\u522b\u662f\u957f\u671f\u89c4\u5212\u80fd\u529b\u7684\u7f3a\u5931\uff0c\u4e3a\u5f00\u53d1\u66f4\u7269\u7406\u57fa\u7840\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2512.23412", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23412", "abs": "https://arxiv.org/abs/2512.23412", "authors": ["Jiawei Chen", "Xintian Shen", "Lihao Zheng", "Zhenwei Shao", "Hongyuan Zhang", "Pengfei Yu", "Xudong Rao", "Ning Mao", "Xiaobo Liu", "Lian Wen", "Chaoqun Du", "Feng Gu", "Wei He", "Qizhen Li", "Shanshan Li", "Zide Liu", "Jing Luo", "Lifu Mu", "Xuhao Pan", "Chang Ren", "Haoyi Sun", "Qian Wang", "Wei Wang", "Hongfu Yang", "Jiqing Zhan", "Chunpeng Zhou", "Zheng Zhou", "Hao Ma", "Tao Wei", "Pan Zhou", "Wei Chen"], "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning", "comment": "Technique Report", "summary": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.", "AI": {"tldr": "MindWatcher\u662f\u4e00\u4e2a\u96c6\u6210\u4ea4\u66ff\u601d\u8003\u548c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4ee3\u7406\uff0c\u80fd\u591f\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u8c03\u7528\u591a\u6837\u5316\u5de5\u5177\uff0c\u65e0\u9700\u4eba\u5de5\u63d0\u793a\u6216\u5de5\u4f5c\u6d41\uff0c\u5728\u5de5\u5177\u8c03\u7528\u6027\u80fd\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u66f4\u5927/\u66f4\u65b0\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u4ee3\u7406\u5728\u5904\u7406\u9700\u8981\u5de5\u5177\u8c03\u7528\u7684\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u65f6\u667a\u80fd\u6709\u9650\uff0c\u800c\u80fd\u591f\u81ea\u4e3b\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4ee3\u7406\u6b63\u5728\u6210\u4e3a\u5904\u7406\u6d89\u53ca\u5916\u90e8\u73af\u5883\u591a\u6b65\u4ea4\u4e92\u7684\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u7684\u6709\u529b\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4ea4\u66ff\u601d\u8003\u8303\u5f0f\u4f7f\u6a21\u578b\u80fd\u5728\u4efb\u4f55\u4e2d\u95f4\u9636\u6bb5\u5728\u601d\u8003\u548c\u5de5\u5177\u8c03\u7528\u4e4b\u95f4\u5207\u6362\uff0c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u80fd\u529b\u5141\u8bb8\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u64cd\u4f5c\u56fe\u50cf\u4ee5\u83b7\u5f97\u66f4\u7cbe\u786e\u7684\u641c\u7d22\u7ed3\u679c\u3002\u6784\u5efa\u81ea\u52a8\u5316\u6570\u636e\u5ba1\u8ba1\u548c\u8bc4\u4f30\u6d41\u6c34\u7ebf\uff0c\u8f85\u4ee5\u624b\u52a8\u7b56\u5212\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u521b\u5efaMindWatcher-Evaluate Bench\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "MindWatcher\u901a\u8fc7\u5353\u8d8a\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u66f4\u5927\u6216\u66f4\u65b0\u6a21\u578b\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8fd8\u63ed\u793a\u4e86\u4ee3\u7406\u8bad\u7ec3\u7684\u5173\u952e\u89c1\u89e3\uff0c\u5982\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9057\u4f20\u7ee7\u627f\u73b0\u8c61\u3002", "conclusion": "MindWatcher\u4f5c\u4e3a\u4e00\u4e2a\u96c6\u6210\u4ea4\u66ff\u601d\u8003\u548c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4ee3\u7406\uff0c\u80fd\u591f\u81ea\u4e3b\u534f\u8c03\u591a\u6837\u5316\u5de5\u5177\u4f7f\u7528\uff0c\u89e3\u51b3\u5e7f\u6cdb\u9886\u57df\u7684\u591a\u6a21\u6001\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u63d0\u5347\u4e86\u8bad\u7ec3\u901f\u5ea6\u548c\u786c\u4ef6\u5229\u7528\u7387\u3002", "topic": "agent analysis"}}
{"id": "2512.23343", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23343", "abs": "https://arxiv.org/abs/2512.23343", "authors": ["Jiafeng Liang", "Hao Li", "Chang Li", "Jiaqi Zhou", "Shixin Jiang", "Zekun Wang", "Changkai Ji", "Zhihao Zhu", "Runxuan Liu", "Tao Ren", "Jinlan Fu", "See-Kiong Ng", "Xia Liang", "Ming Liu", "Bing Qin"], "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents", "comment": "57 pages, 5 figures", "summary": "Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u7efc\u5408\u4e86\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u548cLLM\u9a71\u52a8\u667a\u80fd\u4f53\u4e2d\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u4ece\u5b9a\u4e49\u3001\u529f\u80fd\u3001\u5206\u7c7b\u3001\u5b58\u50a8\u5230\u7ba1\u7406\u751f\u547d\u5468\u671f\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u8bc4\u4f30\u57fa\u51c6\u3001\u5b89\u5168\u6027\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u667a\u80fd\u4f53\u7814\u7a76\u867d\u7136\u501f\u9274\u4e86\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u8bbe\u8ba1\u8bb0\u5fc6\u5de5\u4f5c\u6d41\uff0c\u4f46\u7531\u4e8e\u5b66\u79d1\u58c1\u5792\u96be\u4ee5\u5438\u6536\u4eba\u7c7b\u8bb0\u5fc6\u673a\u5236\u7684\u7cbe\u9ad3\u3002\u672c\u6587\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u7cfb\u7edf\u6027\u5730\u6574\u5408\u8de8\u5b66\u79d1\u8bb0\u5fc6\u77e5\u8bc6\uff0c\u8fde\u63a5\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u4e0eLLM\u9a71\u52a8\u667a\u80fd\u4f53\u3002", "method": "1. \u9610\u660e\u4ece\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u5230LLM\u518d\u5230\u667a\u80fd\u4f53\u7684\u6e10\u8fdb\u8f68\u8ff9\u4e2d\u7684\u8bb0\u5fc6\u5b9a\u4e49\u548c\u529f\u80fd\uff1b2. \u4ece\u751f\u7269\u548c\u4eba\u5de5\u89d2\u5ea6\u6bd4\u8f83\u5206\u6790\u8bb0\u5fc6\u5206\u7c7b\u3001\u5b58\u50a8\u673a\u5236\u548c\u5b8c\u6574\u7ba1\u7406\u751f\u547d\u5468\u671f\uff1b3. \u56de\u987e\u8bc4\u4f30\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u4e3b\u6d41\u57fa\u51c6\uff1b4. \u4ece\u653b\u51fb\u548c\u9632\u5fa1\u53cc\u91cd\u89d2\u5ea6\u63a2\u8ba8\u8bb0\u5fc6\u5b89\u5168\u6027\uff1b5. \u5c55\u671b\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u8de8\u5b66\u79d1\u8bb0\u5fc6\u77e5\u8bc6\u7684\u7cfb\u7edf\u6027\u7efc\u5408\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u4e0eLLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u5206\u6790\u4e86\u8bb0\u5fc6\u7684\u591a\u4e2a\u7ef4\u5ea6\uff08\u5206\u7c7b\u3001\u5b58\u50a8\u3001\u7ba1\u7406\u3001\u8bc4\u4f30\u3001\u5b89\u5168\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8bb0\u5fc6\u4f5c\u4e3a\u8fde\u63a5\u8fc7\u53bb\u4e0e\u672a\u6765\u7684\u5173\u952e\u67a2\u7ebd\uff0c\u5bf9AI\u7cfb\u7edf\u5904\u7406\u590d\u6742\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u548cLLM\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u77e5\u8bc6\uff0c\u53ef\u4ee5\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u672a\u6765\u5e94\u5173\u6ce8\u591a\u6a21\u6001\u8bb0\u5fc6\u7cfb\u7edf\u548c\u6280\u80fd\u83b7\u53d6\u7b49\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2512.23419", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23419", "abs": "https://arxiv.org/abs/2512.23419", "authors": ["Alex Lewandowski", "Adtiya A. Ramesh", "Edan Meyer", "Dale Schuurmans", "Marlos C. Machado"], "title": "The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis", "comment": "NeurIPS 2025 (spotlight)", "summary": "Continual learning is often motivated by the idea, known as the big world hypothesis, that \"the world is bigger\" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u5d4c\u5165\u89c6\u89d2\uff0c\u5c06\u667a\u80fd\u4f53\u89c6\u4e3a\u5728\u901a\u7528\u8ba1\u7b97\u673a\u4e2d\u6a21\u62df\u7684\u81ea\u52a8\u673a\uff0c\u5e76\u5f15\u5165\u4ea4\u4e92\u6027\u4f5c\u4e3a\u8861\u91cf\u667a\u80fd\u4f53\u6301\u7eed\u9002\u5e94\u80fd\u529b\u7684\u6307\u6807\uff0c\u53d1\u73b0\u6df1\u5ea6\u7ebf\u6027\u7f51\u7edc\u6bd4\u975e\u7ebf\u6027\u7f51\u7edc\u66f4\u80fd\u7ef4\u6301\u4ea4\u4e92\u6027\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u95ee\u9898\u901a\u5e38\u901a\u8fc7\u663e\u5f0f\u7ea6\u675f\u667a\u80fd\u4f53\u5bb9\u91cf\u6765\u5efa\u6a21\"\u4e16\u754c\u5927\u4e8e\u667a\u80fd\u4f53\"\u7684\u5927\u4e16\u754c\u5047\u8bbe\uff0c\u4f46\u8fd9\u4e9b\u7ea6\u675f\u53ef\u80fd\u5177\u6709\u968f\u610f\u6027\u3001\u96be\u4ee5\u6574\u5408\uff0c\u4e14\u53ef\u80fd\u9650\u5236\u667a\u80fd\u4f53\u5bb9\u91cf\u6269\u5c55\u7684\u6709\u6548\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u81ea\u7136\u7684\u5d4c\u5165\u7ea6\u675f\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u8ba1\u7b97\u5d4c\u5165\u89c6\u89d2\uff0c\u5c06\u5d4c\u5165\u667a\u80fd\u4f53\u8868\u793a\u4e3a\u5728\u901a\u7528\uff08\u5f62\u5f0f\uff09\u8ba1\u7b97\u673a\u4e2d\u6a21\u62df\u7684\u81ea\u52a8\u673a\uff1b\u8bc1\u660e\u8fd9\u79cd\u81ea\u52a8\u673a\u7b49\u4ef7\u4e8e\u5728\u53ef\u6570\u65e0\u9650\u72b6\u6001\u7a7a\u95f4\u4e0a\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u4ea4\u4e92\u7684\u667a\u80fd\u4f53\uff1b\u63d0\u51fa\u4ea4\u4e92\u6027\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\uff0c\u8861\u91cf\u667a\u80fd\u4f53\u901a\u8fc7\u5b66\u4e60\u65b0\u9884\u6d4b\u6301\u7eed\u9002\u5e94\u884c\u4e3a\u7684\u80fd\u529b\uff1b\u5f00\u53d1\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u5bfb\u6c42\u4ea4\u4e92\u6027\uff0c\u5e76\u6784\u5efa\u5408\u6210\u95ee\u9898\u8bc4\u4f30\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u6df1\u5ea6\u975e\u7ebf\u6027\u7f51\u7edc\u96be\u4ee5\u7ef4\u6301\u4ea4\u4e92\u6027\uff0c\u800c\u6df1\u5ea6\u7ebf\u6027\u7f51\u7edc\u968f\u7740\u5bb9\u91cf\u589e\u52a0\u80fd\u591f\u7ef4\u6301\u66f4\u9ad8\u7684\u4ea4\u4e92\u6027\u3002", "conclusion": "\u8ba1\u7b97\u5d4c\u5165\u89c6\u89d2\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u7ea6\u675f\u6846\u67b6\uff0c\u4ea4\u4e92\u6027\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u6301\u7eed\u9002\u5e94\u80fd\u529b\uff0c\u7f51\u7edc\u67b6\u6784\u5bf9\u7ef4\u6301\u4ea4\u4e92\u6027\u6709\u91cd\u8981\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2512.23424", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23424", "abs": "https://arxiv.org/abs/2512.23424", "authors": ["Jinye Du", "Quan Yuan", "Zuyao Zhang", "Yanzhi Yi", "Jiahui Hu", "Wangyi Chen", "Yiyang Zhu", "Qishui Zheng", "Wenxiang Zou", "Xiangyu Chang", "Zuohe Zheng", "Zichun Ye", "Chao Liu", "Shanni Li", "Renwei Zhang", "Yiping Deng", "Xinwei Hu", "Xuefeng Jin", "Jie Zhao"], "title": "AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis", "comment": null, "summary": "Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.", "AI": {"tldr": "AKG kernel agent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316AI\u8ba1\u7b97\u5185\u6838\u7684\u751f\u6210\u3001\u8fc1\u79fb\u548c\u6027\u80fd\u8c03\u4f18\uff0c\u652f\u6301\u591a\u79cdDSL\u8bed\u8a00\uff0c\u5728GPU\u548cNPU\u540e\u7aef\u4e0a\u76f8\u6bd4PyTorch Eager\u5b9e\u73b0\u5e73\u5747\u83b7\u5f971.46\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3AI\u6a21\u578b\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u5185\u6838\u9700\u6c42\u8feb\u5207\uff0c\u4f46LLM\u3001\u591a\u6a21\u6001\u67b6\u6784\u7b49\u590d\u6742\u6027\u589e\u52a0\uff0c\u52a0\u4e0a\u7a00\u758f\u5316\u3001\u91cf\u5316\u7b49\u6280\u672f\u4ee5\u53ca\u786c\u4ef6\u9891\u7e41\u66f4\u65b0\uff0c\u4f7f\u5f97\u624b\u52a8\u4f18\u5316\u65e0\u6cd5\u8ddf\u4e0a\u9700\u6c42\uff0c\u6210\u4e3aAI\u7cfb\u7edf\u5f00\u53d1\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u63d0\u51faAKG kernel agent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u652f\u6301Triton\u3001TileLang\u3001CPP\u3001CUDA-C\u7b49\u591a\u79cd\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u4fbf\u4e8e\u96c6\u6210\u65b0DSL\u548c\u786c\u4ef6\u76ee\u6807\uff0c\u81ea\u52a8\u5316\u5185\u6838\u751f\u6210\u3001\u8fc1\u79fb\u548c\u6027\u80fd\u8c03\u4f18\u3002", "result": "\u5728KernelBench\u4e0a\u4f7f\u7528Triton DSL\u8bc4\u4f30\uff0c\u5728GPU\u548cNPU\u540e\u7aef\u4e0a\u76f8\u6bd4PyTorch Eager\u57fa\u7ebf\u5b9e\u73b0\u5e73\u5747\u83b7\u5f971.46\u500d\u52a0\u901f\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u5728\u73b0\u4ee3AI\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u52a0\u901f\u5185\u6838\u5f00\u53d1\u7684\u6709\u6548\u6027\u3002", "conclusion": "AKG kernel agent\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u5316\u5185\u6838\u5f00\u53d1\uff0c\u89e3\u51b3\u4e86AI\u8ba1\u7b97\u5185\u6838\u5f00\u53d1\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u652f\u6301\u591a\u79cdDSL\u548c\u786c\u4ef6\u540e\u7aef\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5185\u6838\u5f00\u53d1\u6d41\u7a0b\u3002", "topic": "code agent"}}
{"id": "2512.23457", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23457", "abs": "https://arxiv.org/abs/2512.23457", "authors": ["Kongcheng Zhang", "Qi Yao", "Shunyu Liu", "Wenjian Zhang", "Min Cen", "Yang Zhou", "Wenkai Fang", "Yiru Zhao", "Baisheng Lai", "Mingli Song"], "title": "Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following", "comment": null, "summary": "Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.", "AI": {"tldr": "\u63d0\u51faHindsight instruction Replay (HiR)\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9-\u91cd\u5199\u7b56\u7565\u5c06\u5931\u8d25\u5c1d\u8bd5\u91cd\u64ad\u4e3a\u6210\u529f\u6837\u672c\uff0c\u63d0\u9ad8\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7528\u4e8e\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\u8ddf\u968f\u6307\u4ee4\u65f6\uff0c\u521d\u59cb\u6a21\u578b\u5e38\u56e0\u80fd\u529b\u6709\u9650\u65e0\u6cd5\u751f\u6210\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u7684\u54cd\u5e94\uff0c\u5bfc\u81f4\u7a00\u758f\u6216\u96be\u4ee5\u533a\u5206\u7684\u5956\u52b1\u4fe1\u53f7\u963b\u788d\u5b66\u4e60\u3002", "method": "\u63d0\u51faHiR\u6846\u67b6\uff1a1) \u91c7\u7528\u9009\u62e9-\u91cd\u5199\u7b56\u7565\uff0c\u57fa\u4e8e\u5df2\u6ee1\u8db3\u7684\u7ea6\u675f\u5c06\u5931\u8d25\u5c1d\u8bd5\u91cd\u64ad\u4e3a\u6210\u529f\u6837\u672c\uff1b2) \u5728\u91cd\u64ad\u6837\u672c\u548c\u539f\u59cb\u6837\u672c\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff1b3) \u7406\u8bba\u6846\u67b6\u4e3a\u6307\u4ee4\u7ea7\u548c\u54cd\u5e94\u7ea7\u7684\u53cc\u91cd\u504f\u597d\u5b66\u4e60\uff0c\u4ec5\u9700\u4e8c\u5143\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u79cd\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u53d6\u5f97\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "HiR\u662f\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.23508", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.23508", "abs": "https://arxiv.org/abs/2512.23508", "authors": ["Alessio Benavoli", "Alessandro Facchini", "Marco Zaffalon"], "title": "Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities", "comment": null, "summary": "How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u786e\u4fddAI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u5e76\u4fdd\u6301\u5b89\u5168\uff0c\u901a\u8fc7AI\u8f85\u52a9\u548cAI\u5173\u673a\u6e38\u620f\u6846\u67b6\u7814\u7a76\u8be5\u95ee\u9898\uff0c\u63d0\u51fa\u9700\u8981AI\u4ee3\u7406\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u963f\u57fa\u7c73\u5fb7\u504f\u597d\u3002", "motivation": "\u786e\u4fddAI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u5e76\u4fdd\u6301\u5b89\u5168\u662f\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76AI\u8f85\u52a9\u95ee\u9898\u548cAI\u5173\u673a\u95ee\u9898\uff0c\u4ee5\u8bbe\u8ba1\u51fa\u65e2\u80fd\u6709\u6548\u5e2e\u52a9\u4eba\u7c7b\u53c8\u80fd\u5b89\u5168\u8fd0\u884c\u7684AI\u7cfb\u7edf\u3002", "method": "\u91c7\u7528AI\u8f85\u52a9\u6e38\u620f\u548cAI\u5173\u673a\u6e38\u620f\u6846\u67b6\uff0c\u7814\u7a76AI\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5904\u7406\u4e0d\u5b8c\u5168\u504f\u597d\u548c\u975e\u963f\u57fa\u7c73\u5fb7\u504f\u597d\u7684\u65b9\u6cd5\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u89e3\u51b3AI\u5bf9\u9f50\u548c\u5b89\u5168\u6311\u6218\u9700\u8981AI\u4ee3\u7406\u5177\u5907\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u63a8\u7406\u7684\u80fd\u529b\uff0c\u5e76\u80fd\u5904\u7406\u4e0d\u5b8c\u5168\u504f\u597d\u548c\u975e\u963f\u57fa\u7c73\u5fb7\u504f\u597d\u3002", "conclusion": "\u786e\u4fddAI\u7cfb\u7edf\u5b89\u5168\u548c\u5bf9\u9f50\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u504f\u597d\u7684AI\u4ee3\u7406\uff0cAI\u8f85\u52a9\u548c\u5173\u673a\u6e38\u620f\u6846\u67b6\u4e3a\u7814\u7a76\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2512.23518", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23518", "abs": "https://arxiv.org/abs/2512.23518", "authors": ["Hazel Kim", "Philip Torr"], "title": "Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias", "comment": null, "summary": "Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.", "AI": {"tldr": "MoLaCE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u57fa\u4e8e\u6f5c\u5728\u6982\u5ff5\u7684\u4e0d\u540c\u4e13\u5bb6\u6765\u51cf\u5c11LLMs\u4e2d\u7684\u786e\u8ba4\u504f\u8bef\uff0c\u65e0\u9700\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\u786e\u8ba4\u504f\u8bef\u95ee\u9898\uff0c\u5f53\u63d0\u793a\u6697\u793a\u504f\u597d\u7b54\u6848\u65f6\uff0c\u6a21\u578b\u4f1a\u5f3a\u5316\u8fd9\u79cd\u504f\u8bef\u800c\u975e\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u3002\u8fd9\u4e00\u95ee\u9898\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u5df2\u6709\u5bb3\uff0c\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u98ce\u9669\u66f4\u5927\uff0c\u53ef\u80fd\u5f62\u6210\u56de\u97f3\u5ba4\u6548\u5e94\u800c\u975e\u7ea0\u6b63\u504f\u8bef\u3002", "method": "\u63d0\u51faMoLaCE\u6846\u67b6\uff0c\u5c06\u4e13\u5bb6\u5b9e\u4f8b\u5316\u4e3a\u4e0d\u540c\u6f5c\u5728\u6982\u5ff5\u4e0a\u7684\u6fc0\u6d3b\u5f3a\u5ea6\u6df7\u5408\u3002\u5173\u952e\u6d1e\u89c1\u662f\uff1a\u7531\u4e8e\u8bed\u8a00\u5177\u6709\u7ec4\u5408\u6027\uff0c\u4e0d\u540c\u8868\u8ff0\u7684\u63d0\u793a\u4f1a\u4ee5\u7279\u5b9a\u65b9\u5f0f\u91cd\u65b0\u52a0\u6743\u5f71\u54cd\u4e8b\u5b9e\u6b63\u786e\u6027\u7684\u6f5c\u5728\u6982\u5ff5\uff0c\u56e0\u6b64\u4e0d\u80fd\u5bf9\u6240\u6709\u8f93\u5165\u5e94\u7528\u5355\u4e00\u56fa\u5b9a\u5e72\u9884\u3002", "result": "\u5b9e\u8bc1\u8868\u660eMoLaCE\u80fd\u6301\u7eed\u51cf\u5c11\u786e\u8ba4\u504f\u8bef\u3001\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff0c\u540c\u65f6\u4ec5\u9700\u4e00\u5c0f\u90e8\u5206\u8ba1\u7b97\u91cf\u3002\u53ef\u96c6\u6210\u5230\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u4e2d\u4ee5\u591a\u6837\u5316\u89c6\u89d2\u5e76\u51cf\u5c11\u76f8\u5173\u9519\u8bef\u3002", "conclusion": "MoLaCE\u4f7f\u5355\u4e2aLLM\u80fd\u591f\u5185\u90e8\u6a21\u62df\u8fa9\u8bba\u4f18\u52bf\uff0c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6709\u6548\u89e3\u51b3LLMs\u4e2d\u7684\u786e\u8ba4\u504f\u8bef\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2512.23572", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23572", "abs": "https://arxiv.org/abs/2512.23572", "authors": ["Daiki Shiono", "Shumpei Miyawaki", "Ryota Tanaka", "Jun Suzuki"], "title": "Instruction-Following Evaluation of Large Vision-Language Models", "comment": "21 pages, 7 figures", "summary": "Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\u540e\uff0c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u4f1a\u4e0b\u964d\uff0c\u800c\u5305\u542b\u8f93\u51fa\u683c\u5f0f\u8bf4\u660e\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96c6\u6210\u89c6\u89c9\u80fd\u529b\u540e\uff0c\u7ecf\u5e38\u65e0\u6cd5\u4fdd\u6301\u539f\u6709\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6309\u7167\u9884\u671f\u6267\u884c\u4efb\u52a1\u6307\u4ee4\u3002", "method": "\u6784\u5efa\u65b0\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u660e\u786e\u6807\u6ce8\u8f93\u51fa\u683c\u5f0f\u662f\u5426\u88ab\u6307\u5b9a\uff1b\u7814\u7a76\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u660e\u786e\u6307\u793a\u8f93\u51fa\u683c\u5f0f\u5bf9LVLMs\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u8bc1\u5b9eLVLMs\u5728\u5e38\u7528\u6570\u636e\u96c6\u5fae\u8c03\u540e\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u4e0b\u964d\uff1b\u5305\u542b\u8f93\u51fa\u683c\u5f0f\u8bf4\u660e\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u6307\u4ee4\u8ddf\u968f\u66f4\u51c6\u786e\u3002", "conclusion": "\u5728\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\u4e2d\u5305\u542b\u8f93\u51fa\u683c\u5f0f\u8bf4\u660e\u7684\u6837\u672c\u53ef\u80fd\u6709\u52a9\u4e8e\u7f13\u89e3\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u7684\u4e0b\u964d\u3002", "topic": "agent analysis"}}
{"id": "2512.23647", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.23647", "abs": "https://arxiv.org/abs/2512.23647", "authors": ["Baixuan Li", "Jialong Wu", "Wenbiao Yin", "Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Zhengwei Tao", "Liwen Zhang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "Nested Browser-Use Learning for Agentic Information Seeking", "comment": null, "summary": "Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.", "AI": {"tldr": "NestBrowse\u63d0\u51fa\u4e86\u4e00\u79cd\u5d4c\u5957\u6d4f\u89c8\u5668\u4f7f\u7528\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u4ea4\u4e92\u63a7\u5236\u4e0e\u9875\u9762\u63a2\u7d22\u89e3\u8026\uff0c\u4f7f\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u80fd\u591f\u8fdb\u884c\u5b8c\u6574\u7684\u6d4f\u89c8\u5668\u4ea4\u4e92\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662fAPI\u7ea7\u522b\u7684\u7247\u6bb5\u68c0\u7d22", "motivation": "\u5f53\u524d\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u4e3b\u8981\u5c40\u9650\u4e8eAPI\u7ea7\u522b\u7684\u7247\u6bb5\u68c0\u7d22\u548c\u57fa\u4e8eURL\u7684\u9875\u9762\u83b7\u53d6\uff0c\u65e0\u6cd5\u8bbf\u95ee\u901a\u8fc7\u771f\u5b9e\u6d4f\u89c8\u53ef\u83b7\u5f97\u7684\u66f4\u4e30\u5bcc\u4fe1\u606f\u3002\u5b8c\u6574\u7684\u6d4f\u89c8\u5668\u4ea4\u4e92\u867d\u7136\u80fd\u89e3\u9501\u66f4\u6df1\u5c42\u80fd\u529b\uff0c\u4f46\u5176\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u5197\u957f\u9875\u9762\u5185\u5bb9\u8fd4\u56de\u7ed9ReAct\u5f0f\u51fd\u6570\u8c03\u7528\u4ee3\u7406\u5e26\u6765\u4e86\u5de8\u5927\u590d\u6742\u6027", "method": "\u63d0\u51faNested Browser-Use Learning (NestBrowse)\uff0c\u5f15\u5165\u6700\u5c0f\u4e14\u5b8c\u6574\u7684\u6d4f\u89c8\u5668\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5957\u7ed3\u6784\u5c06\u4ea4\u4e92\u63a7\u5236\u4e0e\u9875\u9762\u63a2\u7d22\u89e3\u8026\u3002\u8fd9\u79cd\u8bbe\u8ba1\u7b80\u5316\u4e86\u4ee3\u7406\u63a8\u7406\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6df1\u5ea6\u7f51\u7edc\u4fe1\u606f\u83b7\u53d6", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6df1\u5ea6\u4fe1\u606f\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNestBrowse\u5728\u5b9e\u8df5\u4e2d\u663e\u793a\u51fa\u660e\u663e\u4f18\u52bf\u3002\u8fdb\u4e00\u6b65\u7684\u6df1\u5165\u5206\u6790\u5f3a\u8c03\u4e86\u5176\u6548\u7387\u548c\u7075\u6d3b\u6027", "conclusion": "NestBrowse\u901a\u8fc7\u5d4c\u5957\u6d4f\u89c8\u5668\u4f7f\u7528\u5b66\u4e60\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u5728\u5b8c\u6574\u6d4f\u89c8\u5668\u4ea4\u4e92\u4e2d\u7684\u590d\u6742\u6027\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u6df1\u5ea6\u7f51\u7edc\u4fe1\u606f\u83b7\u53d6", "topic": "agent analysis"}}
{"id": "2512.22508", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22508", "abs": "https://arxiv.org/abs/2512.22508", "authors": ["Lucky Susanto", "Anasta Pranawijayana", "Cortino Sukotjo", "Soni Prasad", "Derry Wijaya"], "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals", "comment": "Accepted as a Short Paper at HEALTHINF2026", "summary": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.", "AI": {"tldr": "\u5229\u7528\u5143\u6570\u636e\u548c\u5e7b\u89c9\u4fe1\u53f7\u9884\u6d4bLLM\u5728\u533b\u5b66\u8003\u8bd5\u4e2d\u7684\u56de\u7b54\u6b63\u786e\u6027\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u53477.14%\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c1a\u4e0d\u8db3\u4ee5\u7528\u4e8e\u9ad8\u98ce\u9669\u90e8\u7f72", "motivation": "LLM\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u65f6\uff0c\u751f\u6210\u9519\u8bef\u4fe1\u606f\uff08\u5e7b\u89c9\uff09\u662f\u4e3b\u8981\u62c5\u5fe7\u3002\u867d\u7136\u5df2\u6709\u68c0\u6d4b\u548c\u7f13\u89e3\u5e7b\u89c9\u7684\u7814\u7a76\uff0c\u4f46\u9884\u6d4bLLM\u56de\u7b54\u662f\u5426\u6b63\u786e\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u5728\u4fee\u590d\u5b66\u591a\u9879\u9009\u62e9\u8003\u8bd5\u4e0a\u6d4b\u8bd5\u901a\u7528\u6a21\u578b(GPT-4o)\u548c\u63a8\u7406\u4e2d\u5fc3\u6a21\u578b(OSS-120B)\u3002\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u7684\u5143\u6570\u636e\u548c\u5e7b\u89c9\u4fe1\u53f7\u4e3a\u6bcf\u4e2a\uff08\u6a21\u578b\uff0c\u63d0\u793a\uff09\u7ec4\u5408\u6784\u5efa\u6b63\u786e\u6027\u9884\u6d4b\u5668\u3002", "result": "\u5143\u6570\u636e\u65b9\u6cd5\u53ef\u5c06\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u5347+7.14%\uff0c\u8fbe\u523083.12%\u7684\u7cbe\u786e\u7387\uff08\u76f8\u6bd4\u5047\u8bbe\u6240\u6709\u7b54\u6848\u6b63\u786e\u7684\u57fa\u7ebf\uff09\u3002\u5b9e\u9645\u5e7b\u89c9\u662f\u9519\u8bef\u56de\u7b54\u7684\u5f3a\u6307\u6807\uff0c\u4f46\u4ec5\u5143\u6570\u636e\u4fe1\u53f7\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u5e7b\u89c9\u3002\u63d0\u793a\u7b56\u7565\u867d\u4e0d\u5f71\u54cd\u6574\u4f53\u51c6\u786e\u7387\uff0c\u4f46\u663e\u8457\u6539\u53d8\u6a21\u578b\u5185\u90e8\u884c\u4e3a\u548c\u5143\u6570\u636e\u7684\u9884\u6d4b\u6548\u7528\u3002", "conclusion": "\u4e3a\u5f00\u53d1LLM\u53ef\u9760\u6027\u4fe1\u53f7\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u672c\u6587\u63a2\u7d22\u7684\u65b9\u6cd5\u5c1a\u4e0d\u591f\u7a33\u5065\uff0c\u65e0\u6cd5\u7528\u4e8e\u5173\u952e\u7684\u9ad8\u98ce\u9669\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2512.22650", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22650", "abs": "https://arxiv.org/abs/2512.22650", "authors": ["Shuyu Gan", "James Mooney", "Pan Hao", "Renxiang Wang", "Mingyi Hong", "Qianwen Wang", "Dongyeop Kang"], "title": "Scaling Unverifiable Rewards: A Case Study on Visual Insights", "comment": "32 pages, 25 figures", "summary": "Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's \u03c4=0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.", "AI": {"tldr": "\u63d0\u51faSelective TTS\u6846\u67b6\uff0c\u5728\u65e0\u6cd5\u9a8c\u8bc1\u5956\u52b1\u7684\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u8de8\u9636\u6bb5\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u5e76\u65e9\u671f\u526a\u679d\u4f4e\u8d28\u91cf\u5206\u652f\uff0c\u63d0\u5347\u63a8\u7406\u8d28\u91cf", "motivation": "\u73b0\u5b9e\u4e16\u754c\u591a\u9636\u6bb5\u4efb\u52a1\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u6700\u7ec8\u5956\u52b1\u6216\u8db3\u591f\u6570\u636e\u8bad\u7ec3\u9c81\u68d2\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5bfc\u81f4\u57fa\u4e8e\u8bc4\u5224\u7684\u8fed\u4ee3\u7ec6\u5316\u5bb9\u6613\u5728\u9636\u6bb5\u95f4\u7d2f\u79ef\u8bef\u5dee", "method": "\u63d0\u51faSelective TTS\u6846\u67b6\uff1a1) \u5728\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u4e2d\u8de8\u9636\u6bb5\u5206\u914d\u8ba1\u7b97\u800c\u975e\u65f6\u95f4\u4e0a\u7684\u91cd\u590d\u7ec6\u5316\uff1b2) \u4f7f\u7528\u8fc7\u7a0b\u7279\u5b9a\u8bc4\u5224\u5668\u65e9\u671f\u526a\u679d\u4f4e\u8d28\u91cf\u5206\u652f\uff1b3) \u5728\u6570\u636e\u79d1\u5b66\u6d41\u6c34\u7ebf\u4e2d\u6784\u5efa\u7aef\u5230\u7aef\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u6210\u56fe\u8868\u548c\u62a5\u544a\uff1b4) \u8bbe\u8ba1\u53ef\u9760\u7684LLM\u8bc4\u5224\u6a21\u578b", "result": "LLM\u8bc4\u5224\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u9f50\u826f\u597d(Kendall's \u03c4=0.55)\uff1b\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cSelective TTS\u5c06\u5e73\u5747\u5206\u4ece61.64\u63d0\u5347\u523065.86\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u65b9\u5dee", "conclusion": "Selective TTS\u80fd\u6709\u6548\u63d0\u5347\u65e0\u6cd5\u9a8c\u8bc1\u5956\u52b1\u7684\u590d\u6742\u5f00\u653e\u4efb\u52a1\u7684\u8d28\u91cf\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u548c\u6545\u4e8b\u751f\u6210\u7b49\u4efb\u52a1\u63d0\u4f9b\u65b0\u7684\u63a8\u7406\u6269\u5c55\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2512.23097", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23097", "abs": "https://arxiv.org/abs/2512.23097", "authors": ["Yingru Li", "Ziniu Li", "Jiacai Liu"], "title": "A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms", "comment": null, "summary": "We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7528\u4e8eLLM\u5fae\u8c03\uff0c\u901a\u8fc7\u5206\u6790\u5305\u542b\u8f68\u8ff9\u7ea7KL\u6563\u5ea6\u548c\u4efb\u52a1\u5956\u52b1\u7684\u590d\u5408\u76ee\u6807\u68af\u5ea6\uff0c\u5206\u89e3\u4e3a\u53ef\u89e3\u6790\u8ba1\u7b97\u7684\u5bc6\u96c6\u68af\u5ea6\uff08\u7528\u4e8etoken\u7ea7\u6a21\u4eff\uff09\u548c\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7684\u7a00\u758f\u68af\u5ea6\uff08\u7528\u4e8e\u957f\u7a0b\u5956\u52b1\u4f18\u5316\uff09\u3002", "motivation": "\u5f53\u524dLLM\u5fae\u8c03\u65b9\u6cd5\u4e2d\uff0c\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u5206\u5f00\u5904\u7406\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5229\u7528\u76d1\u7763\u5b66\u4e60\u7684\u9ad8\u6548\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u7a0b\u4f18\u5316\u80fd\u529b\u7684\u96c6\u6210\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u590d\u5408\u76ee\u6807\u51fd\u6570\u7684\u68af\u5ea6\uff0c\u5c06\u5176\u81ea\u7136\u5206\u89e3\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a1\uff09\u53ef\u89e3\u6790\u8ba1\u7b97\u7684\u5bc6\u96c6\u68af\u5ea6\uff0c\u7528\u4e8etoken\u7ea7\u6a21\u4eff\u5b66\u4e60\uff1b2\uff09\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7684\u7a00\u758f\u68af\u5ea6\uff0c\u7528\u4e8e\u957f\u7a0b\u5956\u52b1\u4f18\u5316\u3002\u5bc6\u96c6\u68af\u5ea6\u5177\u6709\u95ed\u5f0f\u89e3\uff0c\u4fbf\u4e8eGPU\u9ad8\u6548\u5b9e\u73b0\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u5fae\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u8fdb\u884ctoken\u7ea7\u6a21\u4eff\u5b66\u4e60\u548c\u957f\u7a0b\u5956\u52b1\u4f18\u5316\uff0c\u5176\u4e2d\u5bc6\u96c6\u68af\u5ea6\u90e8\u5206\u5177\u6709\u95ed\u5f0f\u89e3\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684GPU\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7edf\u4e00\u8d77\u6765\uff0c\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u5168\u9762\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u540c\u65f6\u8003\u8651\u5c40\u90e8token\u51c6\u786e\u6027\u548c\u5168\u5c40\u4efb\u52a1\u5956\u52b1\u7684\u573a\u666f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22733", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22733", "abs": "https://arxiv.org/abs/2512.22733", "authors": ["Jiaqi Shao", "Yufeng Miao", "Wei Zhang", "Bing Luo"], "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents", "comment": null, "summary": "Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \\textbf{FoldAct}\\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\\times$ speedup.", "AI": {"tldr": "FoldAct\u6846\u67b6\u89e3\u51b3\u4e86\u957f\u89c6\u91ceRL\u4e2d\u4e0a\u4e0b\u6587\u6298\u53e0\u65b9\u6cd5\u5bfc\u81f4\u7684\u975e\u5e73\u7a33\u89c2\u6d4b\u5206\u5e03\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u79bb\u635f\u5931\u8ba1\u7b97\u3001\u5168\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u635f\u5931\u548c\u9009\u62e9\u6027\u7247\u6bb5\u8bad\u7ec3\uff0c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c5.19\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u6298\u53e0\u65b9\u6cd5\u5c06\u6458\u8981\u52a8\u4f5c\u89c6\u4e3a\u6807\u51c6\u52a8\u4f5c\uff0c\u5ffd\u7565\u4e86\u6458\u8981\u4f1a\u6539\u53d8\u667a\u80fd\u4f53\u672a\u6765\u89c2\u6d4b\u7a7a\u95f4\uff0c\u5bfc\u81f4\u7b56\u7565\u4f9d\u8d56\u7684\u975e\u5e73\u7a33\u89c2\u6d4b\u5206\u5e03\uff0c\u8fdd\u53cd\u4e86RL\u6838\u5fc3\u5047\u8bbe\uff0c\u5f15\u53d1\u68af\u5ea6\u7a00\u91ca\u3001\u81ea\u6761\u4ef6\u548c\u8ba1\u7b97\u6210\u672c\u4e09\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faFoldAct\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u5206\u79bb\u635f\u5931\u8ba1\u7b97\uff0c\u4e3a\u6458\u8981\u548c\u52a8\u4f5ctoken\u63d0\u4f9b\u72ec\u7acb\u68af\u5ea6\u4fe1\u53f7\uff1b2) \u5168\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u635f\u5931\uff0c\u51cf\u5c11\u5206\u5e03\u504f\u79fb\uff1b3) \u9009\u62e9\u6027\u7247\u6bb5\u8bad\u7ec3\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u73b0\u4e86\u957f\u89c6\u91ce\u641c\u7d22\u667a\u80fd\u4f53\u7684\u7a33\u5b9a\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u89c2\u6d4b\u95ee\u9898\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u6548\u7387\u63d0\u5347\u4e865.19\u500d\u3002", "conclusion": "FoldAct\u901a\u8fc7\u660e\u786e\u5904\u7406\u4e0a\u4e0b\u6587\u6298\u53e0\u5e26\u6765\u7684\u975e\u5e73\u7a33\u89c2\u6d4b\u95ee\u9898\uff0c\u4e3a\u957f\u89c6\u91ceRL\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6839\u672c\u6027\u7f3a\u9677\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22802", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22802", "abs": "https://arxiv.org/abs/2512.22802", "authors": ["Amirhossein Tighkhorshid", "Zahra Dehghanian", "Gholamali Aminian", "Chengchun Shi", "Hamid R. Rabiee"], "title": "ReDiF: Reinforced Distillation for Few Step Diffusion", "comment": null, "summary": "Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u6a21\u578b\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u84b8\u998f\u8fc7\u7a0b\u89c6\u4e3a\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\u52a8\u6001\u6307\u5bfc\u5b66\u751f\u63a2\u7d22\u53bb\u566a\u8def\u5f84\uff0c\u5b9e\u73b0\u66f4\u5c11\u63a8\u7406\u6b65\u9aa4\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u91c7\u6837\u901f\u5ea6\u6162\uff0c\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u7684\u91cd\u6784\u6216\u4e00\u81f4\u6027\u635f\u5931\uff0c\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u52a8\u6001\u7684\u84b8\u998f\u65b9\u6cd5\u6765\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u84b8\u998f\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u95ee\u9898\u3002\u5b66\u751f\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\u7f51\u7edc\uff0c\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\uff08\u57fa\u4e8e\u4e0e\u6559\u5e08\u6a21\u578b\u8f93\u51fa\u7684\u5bf9\u9f50\u5ea6\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u52a8\u6001\u63a2\u7d22\u591a\u4e2a\u53bb\u566a\u8def\u5f84\uff0c\u5b66\u4e60\u91c7\u53d6\u66f4\u957f\u3001\u4f18\u5316\u7684\u6b65\u9aa4\u8d70\u5411\u6570\u636e\u5206\u5e03\u7684\u9ad8\u6982\u7387\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u84b8\u998f\u6280\u672f\uff0c\u5728\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u6846\u67b6\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u7c7b\u578b\u7684\u6269\u6563\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\u6846\u67b6\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u4f18\u5316\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u6548\u6269\u6563\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.22824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22824", "abs": "https://arxiv.org/abs/2512.22824", "authors": ["Gaurav Chaudhary", "Laxmidhar Behera"], "title": "TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u65b9\u5dee\u9a71\u52a8\u8bfe\u7a0b\u7684\u5b66\u751f-\u6559\u5e08\u5b66\u4e60\u8303\u5f0f\uff0c\u7528\u4e8e\u52a0\u901f\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u5757\u52a8\u6001\u9009\u62e9\u7b56\u7565\u7f6e\u4fe1\u5ea6\u65b9\u5dee\u6700\u9ad8\u7684\u76ee\u6807\u6765\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5355\u76ee\u6807\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u76ee\u6807\u573a\u666f\u4e0b\u5747\u5300\u9009\u62e9\u76ee\u6807\u4f1a\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002\u53d7\u751f\u7269\u7cfb\u7edf\u81ea\u9002\u5e94\u7ed3\u6784\u5316\u5b66\u4e60\u8fc7\u7a0b\u542f\u53d1\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u76ee\u6807\u9009\u62e9\u673a\u5236\u6765\u52a0\u901f\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u5b66\u751f-\u6559\u5e08\u5b66\u4e60\u8303\u5f0f\uff0c\u6559\u5e08\u6a21\u5757\u57fa\u4e8e\u7b56\u7565\u7f6e\u4fe1\u5ea6\uff08\u7531\u72b6\u6001-\u52a8\u4f5c\u503cQ\u51fd\u6570\u53c2\u6570\u5316\uff09\u7684\u65f6\u95f4\u65b9\u5dee\u52a8\u6001\u9009\u62e9\u76ee\u6807\u3002\u6559\u5e08\u9488\u5bf9\u9ad8\u4e0d\u786e\u5b9a\u6027\u76ee\u6807\u63d0\u4f9b\u81ea\u9002\u5e94\u805a\u7126\u5b66\u4e60\u4fe1\u53f7\uff0c\u4fc3\u8fdb\u6301\u7eed\u9ad8\u6548\u8fdb\u5c55\u3002\u65b9\u6cd5\u5efa\u7acb\u4e86Q\u503c\u65f6\u95f4\u65b9\u5dee\u4e0e\u7b56\u7565\u6f14\u5316\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002", "result": "\u572811\u4e2a\u4e0d\u540c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8bfe\u7a0b\u5b66\u4e60\u548c\u76ee\u6807\u9009\u62e9\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u65f6\u95f4\u65b9\u5dee\u9a71\u52a8\u8bfe\u7a0b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u52a0\u901f\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff0c\u5177\u6709\u7b97\u6cd5\u65e0\u5173\u6027\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709RL\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u76ee\u6807\u9009\u62e9\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.23087", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23087", "abs": "https://arxiv.org/abs/2512.23087", "authors": ["Yingru Li", "Jiawei Xu", "Jiacai Liu", "Yuxuan Tong", "Ziniu Li", "Tianle Cai", "Ge Zhang", "Qian Liu", "Baoxiang Wang"], "title": "Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning", "comment": null, "summary": "Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u526a\u679d\u8bcd\u6c47\u8868\u6392\u9664\u6781\u7aef\u4f4e\u6982\u7387\u8bcd\u5143\uff0c\u7528\u6709\u754c\u7684\u4f18\u5316\u504f\u5dee\u66ff\u4ee3\u7cfb\u7edf\u6027\u7684\u4e0d\u5339\u914d\u504f\u5dee\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u9ad8\u541e\u5410\u91cf\u63a8\u7406\u5f15\u64ce\u548c\u6570\u503c\u7cbe\u786e\u8bad\u7ec3\u7cfb\u7edf\u4ece\u76f8\u540c\u53c2\u6570\u4ea7\u751f\u4e0d\u540c\u7684\u6982\u7387\u5206\u5e03\uff0c\u8fd9\u79cd\u4e0d\u5339\u914d\u5bf9\u4f4e\u6982\u7387\u8bcd\u5143\u5f71\u54cd\u66f4\u5927\uff0c\u5bfc\u81f4\u68af\u5ea6\u4f30\u8ba1\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u5c06RL\u76ee\u6807\u7ea6\u675f\u5230\u52a8\u6001\u526a\u679d\u7684\"\u5b89\u5168\"\u8bcd\u6c47\u8868\uff0c\u6392\u9664\u6781\u7aef\u5c3e\u90e8\u7684\u4f4e\u6982\u7387\u8bcd\u5143\uff0c\u7528\u6709\u754c\u7684\u4f18\u5316\u504f\u5dee\u66ff\u4ee3\u7cfb\u7edf\u6027\u7684\u4e0d\u5339\u914d\u504f\u5dee\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u8bad\u7ec3\uff0c\u7406\u8bba\u4e0a\u754c\u5b9a\u4e86\u8bcd\u6c47\u8868\u526a\u679d\u5f15\u5165\u7684\u4f18\u5316\u504f\u5dee\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u526a\u679d\u8bcd\u6c47\u8868\u6392\u9664\u6781\u7aef\u4f4e\u6982\u7387\u8bcd\u5143\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.23236", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.MA", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.23236", "abs": "https://arxiv.org/abs/2512.23236", "authors": ["Gang Liao", "Hongsen Qin", "Ying Wang", "Alicia Golden", "Michael Kuchnik", "Yavuz Yetim", "Jia Jiunn Ang", "Chunli Fu", "Yihan He", "Samuel Hsia", "Zewei Jiang", "Dianshi Li", "Uladzimir Pashkevich", "Varna Puvvada", "Feng Shi", "Matt Steiner", "Ruichao Xiao", "Nathan Yan", "Xiayu Yu", "Zhou Fang", "Abdul Zainul-Abedin", "Ketan Singh", "Hongtao Yu", "Wenyuan Chi", "Barney Huang", "Sean Zhang", "Noah Weller", "Zach Marine", "Wyatt Cook", "Carole-Jean Wu", "Gaoxiang Liu"], "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta", "comment": null, "summary": "Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.", "AI": {"tldr": "KernelEvolve\u662f\u4e00\u4e2a\u9762\u5411DLRM\u7684\u4ee3\u7406\u5185\u6838\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u62bd\u8c61\u5c42\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u5185\u6838\uff0c\u89e3\u51b3\u786c\u4ef6\u5f02\u6784\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u5f00\u53d1\u65f6\u95f4\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b(DLRM)\u8bad\u7ec3\u548c\u63a8\u7406\u9700\u8981\u9ad8\u6548\u5feb\u901f\uff0c\u4f46\u9762\u4e34\u6a21\u578b\u67b6\u6784\u591a\u6837\u6027\u3001\u5185\u6838\u539f\u8bed\u591a\u6837\u6027\u3001\u786c\u4ef6\u5f02\u6784\u6027\u4e09\u5927\u7cfb\u7edf\u6311\u6218\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u5927\u89c4\u6a21\u5f02\u6784\u786c\u4ef6\u73af\u5883\u3002", "method": "\u63d0\u51faKernelEvolve\u6846\u67b6\uff0c\u91c7\u7528\u591a\u7f16\u7a0b\u62bd\u8c61\u5c42\uff08\u4eceTriton/CuTe DSL\u5230\u5e95\u5c42\u786c\u4ef6\u65e0\u5173\u8bed\u8a00\uff09\uff0c\u57fa\u4e8e\u56fe\u641c\u7d22\u7b56\u7565\u8fdb\u884c\u5185\u6838\u4f18\u5316\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u5408\u6210\u52a8\u6001\u9002\u5e94\u8fd0\u884c\u65f6\u6267\u884c\u73af\u5883\u3002", "result": "\u5728KernelBench\u5957\u4ef6\u4e0a\u5b9e\u73b0100%\u901a\u8fc7\u7387\uff08250\u4e2a\u95ee\u9898\uff09\uff0c\u652f\u6301160\u4e2aPyTorch ATen\u64cd\u4f5c\u7b26\uff0c\u5728\u4e09\u79cd\u5f02\u6784\u786c\u4ef6\u5e73\u53f0\u4e0a\u4fdd\u6301100%\u6b63\u786e\u6027\uff0c\u5f00\u53d1\u65f6\u95f4\u4ece\u6570\u5468\u7f29\u77ed\u5230\u6570\u5c0f\u65f6\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8ePyTorch\u57fa\u7ebf\u3002", "conclusion": "KernelEvolve\u6709\u6548\u89e3\u51b3\u4e86DLRM\u7684\u786c\u4ef6\u5f02\u6784\u6027\u95ee\u9898\uff0c\u4e0d\u4ec5\u63d0\u5347\u6027\u80fd\u6548\u7387\uff0c\u8fd8\u964d\u4f4e\u4e86\u65b0AI\u786c\u4ef6\u7684\u7f16\u7a0b\u95e8\u69db\uff0c\u652f\u6301\u81ea\u52a8\u5316\u4e3a\u5185\u90e8\u5f00\u53d1\u7684AI\u786c\u4ef6\u751f\u6210\u5185\u6838\u3002", "topic": "code agent"}}
{"id": "2512.23165", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23165", "abs": "https://arxiv.org/abs/2512.23165", "authors": ["Qingyu Yin", "Yulun Wu", "Zhennan Shen", "Sunbowen Li", "Zhilin Wang", "Yanshu Li", "Chak Tou Leong", "Jiale Kang", "Jinjin Gu"], "title": "Evaluating Parameter Efficient Methods for RLVR", "comment": "Preprint", "summary": "We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f3012\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5728RLVR\u8303\u5f0f\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0DoRA\u3001AdaLoRA\u7b49\u7ed3\u6784\u53d8\u4f53\u4f18\u4e8e\u6807\u51c6LoRA\uff0cSVD\u521d\u59cb\u5316\u7b56\u7565\u5b58\u5728\u8c31\u5d29\u6e83\u95ee\u9898\uff0c\u6781\u7aef\u53c2\u6570\u538b\u7f29\u4f1a\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u3002", "motivation": "RLVR\u901a\u8fc7\u53ef\u9a8c\u8bc1\u53cd\u9988\u6fc0\u52b1\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u9ed8\u8ba4\u4f7f\u7528LoRA\u7b49PEFT\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9RLVR\u573a\u666f\u4e0b\u6700\u4f18PEFT\u67b6\u6784\u7684\u7cfb\u7edf\u63a2\u7d22\u3002", "method": "\u5728DeepSeek-R1-Distill\u7cfb\u5217\u6a21\u578b\u4e0a\uff0c\u5bf9\u8d85\u8fc712\u79cdPEFT\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5305\u62ec\u7ed3\u6784\u53d8\u4f53\u3001SVD\u521d\u59cb\u5316\u7b56\u7565\u548c\u6781\u7aef\u53c2\u6570\u538b\u7f29\u65b9\u6cd5\u3002", "result": "1. DoRA\u3001AdaLoRA\u3001MiSS\u7b49\u7ed3\u6784\u53d8\u4f53\u6301\u7eed\u4f18\u4e8e\u6807\u51c6LoRA\uff1b2. PiSSA\u3001MiLoRA\u7b49SVD\u521d\u59cb\u5316\u7b56\u7565\u56e0\u4e3b\u6210\u5206\u66f4\u65b0\u4e0eRL\u4f18\u5316\u4e0d\u5339\u914d\u800c\u5931\u8d25\uff1b3. VeRA\u3001Rank-1\u7b49\u6781\u7aef\u53c2\u6570\u538b\u7f29\u4e25\u91cd\u9650\u5236\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u6807\u51c6LoRA\u4e0d\u5e94\u4f5c\u4e3aRLVR\u7684\u9ed8\u8ba4\u9009\u62e9\uff0c\u9700\u8981\u66f4\u591a\u63a2\u7d22\u53c2\u6570\u9ad8\u6548\u7684RL\u65b9\u6cd5\uff0c\u4e3aPEFT\u5728RLVR\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u660e\u786e\u6307\u5bfc\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.23340", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.23340", "abs": "https://arxiv.org/abs/2512.23340", "authors": ["Dakuan Lu", "Jiaqi Zhang", "Cheng Yuan", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u6a21\u578b\u534f\u4f5c\u5b9a\u5f8b\uff0c\u8bc1\u660eLLM\u96c6\u6210\u7cfb\u7edf\u9075\u5faa\u603b\u53c2\u6570\u6570\u91cf\u7684\u5e42\u5f8b\u7f29\u653e\uff0c\u6bd4\u5355\u6a21\u578b\u7f29\u653e\u5177\u6709\u66f4\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u8d8b\u52bf\u548c\u66f4\u4f4e\u7684\u635f\u5931\u4e0b\u9650\u3002", "motivation": "\u5355\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u5b58\u5728\u56fa\u6709\u8fb9\u754c\uff0c\u800c\u591a\u6a21\u578b\u96c6\u6210\u6280\u672f\uff08\u5982\u6a21\u578b\u8def\u7531\u548c\u540e\u5904\u7406\u96c6\u6210\uff09\u867d\u7136\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u9884\u6d4b\u591a\u6a21\u578b\u534f\u4f5c\u7684\u6027\u80fd\u7f29\u653e\u89c4\u5f8b\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u578b\u534f\u4f5c\u5b9a\u5f8b\uff0c\u91c7\u7528\u65b9\u6cd5\u65e0\u5173\u7684\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u5047\u8bbe\u7406\u60f3\u5316\u7684\u96c6\u6210\u9884\u8a00\u673a\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6837\u672c\u7684\u603b\u4ea4\u53c9\u71b5\u635f\u5931\u7531\u6a21\u578b\u6c60\u4e2d\u4efb\u4f55\u6a21\u578b\u7684\u6700\u5c0f\u635f\u5931\u51b3\u5b9a\u3002", "result": "\u591a\u6a21\u578b\u7cfb\u7edf\u76f8\u5bf9\u4e8e\u603b\u53c2\u6570\u6570\u91cf\u9075\u5faa\u5e42\u5f8b\u7f29\u653e\uff0c\u6bd4\u5355\u6a21\u578b\u7f29\u653e\u8868\u73b0\u51fa\u66f4\u663e\u8457\u7684\u6539\u8fdb\u8d8b\u52bf\u548c\u66f4\u4f4e\u7684\u7406\u8bba\u635f\u5931\u4e0b\u9650\uff1b\u5f02\u6784\u6a21\u578b\u5bb6\u65cf\u7684\u96c6\u6210\u6bd4\u5355\u4e00\u6a21\u578b\u5bb6\u65cf\u5185\u7684\u96c6\u6210\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u7f29\u653e\u3002", "conclusion": "\u6a21\u578b\u534f\u4f5c\u662f\u6269\u5c55LLM\u667a\u80fd\u524d\u6cbf\u7684\u5173\u952e\u7ef4\u5ea6\uff0c\u6a21\u578b\u591a\u6837\u6027\u662f\u534f\u4f5c\u589e\u76ca\u7684\u4e3b\u8981\u9a71\u52a8\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.23367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23367", "abs": "https://arxiv.org/abs/2512.23367", "authors": ["Yilun Luo", "HuaQing Zheng", "Haoqian Meng", "Wenyuan Liu", "Peng Zhang"], "title": "Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2", "comment": null, "summary": "Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.", "AI": {"tldr": "\u534e\u4e3aopenPangu-Embedded\u6a21\u578b\u7684\u4e09\u79cdCoT\u63a8\u7406\u6a21\u5f0f\u5728Ascend NPU\u4e0a\u5b58\u5728\u5185\u5b58\u548c\u5ef6\u8fdf\u5f00\u9500\u95ee\u9898\uff0c\u672c\u6587\u901a\u8fc7\u4f4e\u6bd4\u7279\u91cf\u5316\uff08INT8\u548cW4A8\uff09\u4f18\u5316\u63a8\u7406\u6548\u7387\uff0c\u5728\u4fdd\u630190%\u4ee5\u4e0a\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b01.5\u500d\u9884\u586b\u5145\u52a0\u901f\u3002", "motivation": "openPangu-Embedded\u6a21\u578b\u7684\u4e09\u79cdChain-of-Thought\u63a8\u7406\u6a21\u5f0f\uff08slow_think\u3001auto_think\u3001no_think\uff09\u867d\u7136\u589e\u5f3a\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u751f\u6210\u957f\u63a8\u7406\u8f68\u8ff9\u5e26\u6765\u4e86\u663e\u8457\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u5f00\u9500\uff0c\u5728Ascend NPU\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u4f4e\u6bd4\u7279\u63a8\u7406\u6846\u67b6\uff0c\u652f\u6301INT8\uff08W8A8\uff09\u548cW4A8\u91cf\u5316\uff0c\u5c06FP16\u8ba1\u7b97\u8f6c\u6362\u4e3a\u66f4\u9ad8\u6548\u7684\u6574\u6570\u8fd0\u7b97\uff0c\u4e13\u95e8\u9488\u5bf9Atlas A2\u4e0a\u7684openPangu-Embedded\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff08HumanEval\u548cMBPP\uff09\u4e0a\uff0cINT8\u91cf\u5316\u4fdd\u6301\u8d85\u8fc790%\u7684FP16\u57fa\u7ebf\u7cbe\u5ea6\uff0c\u5728Atlas A2\u4e0a\u5b9e\u73b01.5\u500d\u9884\u586b\u5145\u52a0\u901f\uff1bW4A8\u91cf\u5316\u663e\u8457\u51cf\u5c11\u5185\u5b58\u6d88\u8017\uff0c\u4f46\u7cbe\u5ea6\u6709\u6240\u6298\u8877\u3002", "conclusion": "\u4f4e\u6bd4\u7279\u91cf\u5316\u80fd\u6709\u6548\u4fc3\u8fdbAscend NPU\u4e0a\u7684\u9ad8\u6548CoT\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u6a21\u578b\u4fdd\u771f\u5ea6\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2512.23461", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23461", "abs": "https://arxiv.org/abs/2512.23461", "authors": ["Zhuo Li", "Pengyu Cheng", "Zhechao Yu", "Feifei Tong", "Anningzhe Gao", "Tsung-Hui Chang", "Xiang Wan", "Erchao Zhao", "Xiaoxi Jiang", "Guanjun Jiang"], "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance", "comment": null, "summary": "Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \\textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \\textbf{D}ebiasing via \\textbf{I}nformation optimization for \\textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \\textit{response length}, \\textit{sycophancy}, and \\textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.", "AI": {"tldr": "\u63d0\u51faDIR\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u4f18\u5316\u6765\u6d88\u9664\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u63d0\u5347RLHF\u6027\u80fd", "motivation": "\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4f4e\uff0c\u5305\u542b\u591a\u79cd\u5f52\u7eb3\u504f\u5dee\uff08\u5982\u54cd\u5e94\u957f\u5ea6\u3001\u8fce\u5408\u6027\u3001\u683c\u5f0f\u7b49\uff09\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u5956\u52b1\u653b\u51fb\u3002\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u8981\u4e48\u9488\u5bf9\u5355\u4e00\u504f\u5dee\uff0c\u8981\u4e48\u53ea\u5efa\u6a21\u7b80\u5355\u7684\u7ebf\u6027\u76f8\u5173\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7684\u53bb\u504f\u65b9\u6cd5DIR\uff1a\u6700\u5927\u5316\u5956\u52b1\u6a21\u578b\u5206\u6570\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5956\u52b1\u6a21\u578b\u8f93\u51fa\u4e0e\u504f\u597d\u8f93\u5165\u4e2d\u504f\u5dee\u5c5e\u6027\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u80fd\u591f\u5904\u7406\u975e\u7ebf\u6027\u76f8\u5173\u7684\u590d\u6742\u504f\u5dee\u7c7b\u578b\u3002", "result": "\u5728\u4e09\u79cd\u5f52\u7eb3\u504f\u5dee\uff08\u54cd\u5e94\u957f\u5ea6\u3001\u8fce\u5408\u6027\u3001\u683c\u5f0f\uff09\u4e0a\u9a8c\u8bc1\u4e86DIR\u7684\u6709\u6548\u6027\u3002DIR\u4e0d\u4ec5\u80fd\u6709\u6548\u51cf\u8f7b\u76ee\u6807\u504f\u5dee\uff0c\u8fd8\u80fd\u63d0\u5347RLHF\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\uff0c\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DIR\u65b9\u6cd5\u901a\u8fc7\u4fe1\u606f\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u590d\u6742\u5f52\u7eb3\u504f\u5dee\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u53bb\u504f\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86RLHF\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.23631", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23631", "abs": "https://arxiv.org/abs/2512.23631", "authors": ["Iris Xu", "Guangtao Zeng", "Zexue He", "Charles Jin", "Aldo Pareja", "Dan Gutfreund", "Chuang Gan", "Zhang-Wei Hong"], "title": "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization", "comment": null, "summary": "Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.", "AI": {"tldr": "BOAD\u6846\u67b6\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\u81ea\u52a8\u53d1\u73b0\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u91ce\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5de5\u7a0b\u95ee\u9898\u65f6\uff0c\u9700\u8981\u5728\u4e00\u4e2a\u63a8\u7406\u94fe\u4e2d\u5904\u7406\u6574\u4e2a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5bfc\u81f4\u6a21\u578b\u9700\u8981\u4fdd\u7559\u65e0\u5173\u4e0a\u4e0b\u6587\uff0c\u4ea7\u751f\u865a\u5047\u76f8\u5173\u6027\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u53d7\u4eba\u7c7b\u5de5\u7a0b\u5e08\u5206\u89e3\u590d\u6742\u95ee\u9898\u7684\u542f\u53d1\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u534f\u8c03\u4e13\u4e1a\u5316\u5b50\u667a\u80fd\u4f53\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "method": "\u63d0\u51faBandit Optimization for Agent Design (BOAD)\u6846\u67b6\uff0c\u5c06\u5c42\u6b21\u53d1\u73b0\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u6bcf\u4e2a\u81c2\u4ee3\u8868\u5019\u9009\u5b50\u667a\u80fd\u4f53\uff0c\u5956\u52b1\u8861\u91cf\u5176\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u534f\u4f5c\u65f6\u7684\u5e2e\u52a9\u7a0b\u5ea6\u3002\u8be5\u6846\u67b6\u80fd\u5728\u6709\u9650\u8bc4\u4f30\u9884\u7b97\u4e0b\u9ad8\u6548\u63a2\u7d22\u5b50\u667a\u80fd\u4f53\u8bbe\u8ba1\u3002", "result": "\u5728SWE-bench-Verified\u4e0a\uff0cBOAD\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u624b\u52a8\u8bbe\u8ba1\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u5728SWE-bench-Live\u4e0a\uff0c36B\u7cfb\u7edf\u5728\u8bc4\u4f30\u65f6\u6392\u540d\u7b2c\u4e8c\uff0c\u8d85\u8d8a\u4e86GPT-4\u548cClaude\u7b49\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "\u81ea\u52a8\u53d1\u73b0\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u663e\u8457\u63d0\u9ad8\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u89c6\u91ce\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2512.18a7ece2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.coderabbit.ai%2Fblog%2Fstate-of-ai-vs-human-code-generation-report%3Futm_source=tldrdev/1/0100019b504180d0-412e9733-1791-441e-a04d-a5b5ad291ff7-000000/5-gk-BC3PF8QsMdmFA45Z0n7ecqtbusmFcQKbsfg4r0=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.coderabbit.ai%2Fblog%2Fstate-of-ai-vs-human-code-generation-report%3Futm_source=tldrdev/1/0100019b504180d0-412e9733-1791-441e-a04d-a5b5ad291ff7-000000/5-gk-BC3PF8QsMdmFA45Z0n7ecqtbusmFcQKbsfg4r0=437", "authors": ["TLDR Newsletter"], "title": "AI code creates 1.7x more problems", "comment": "Source: TLDR Newsletter, Date: 2025-12-24, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.coderabbit.ai%2Fblog%2Fstate-of-ai-vs-human-code-generation-report%3Futm_source=tldrdev/1/0100019b504180d0-412e9733-1791-441e-a04d-a5b5ad291ff7-000000/5-gk-BC3PF8QsMdmFA45Z0n7ecqtbusmFcQKbsfg4r0=437", "summary": "AI code creates 1.7x more problems (9 minute read) AI coding tools are powerful accelerators, but acceleration without guardrails increases risk. AI-generated code is consistently more variable, error-prone, and likely to introduce high-severity issues without the right protections in place. The future of AI-assisted development is about building systems, workflows, and safety layers that amplify what AI does well while compensating for what it tends to miss. Quality requires deliberate engin...", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u867d\u7136\u80fd\u52a0\u901f\u5f00\u53d1\uff0c\u4f46\u4f1a\u4ea7\u751f1.7\u500d\u66f4\u591a\u95ee\u9898\uff0c\u4ee3\u7801\u8d28\u91cf\u66f4\u4e0d\u7a33\u5b9a\u4e14\u5bb9\u6613\u5f15\u5165\u9ad8\u98ce\u9669\u6f0f\u6d1e\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u5b89\u5168\u9632\u62a4\u5c42", "motivation": "\u5f53\u524dAI\u7f16\u7801\u5de5\u5177\u867d\u7136\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u901f\u5ea6\uff0c\u4f46\u7f3a\u4e4f\u9632\u62a4\u63aa\u65bd\u7684\u52a0\u901f\u4f1a\u589e\u52a0\u98ce\u9669\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5e73\u8861AI\u7684\u52a0\u901f\u4f18\u52bf\u4e0e\u4ee3\u7801\u8d28\u91cf\u4fdd\u969c", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790AI\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u51fa\u9700\u8981\u6784\u5efa\u7cfb\u7edf\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\u3001\u5b89\u5168\u9632\u62a4\u5c42\u548c\u8865\u507f\u673a\u5236\u6765\u5f25\u8865AI\u7684\u4e0d\u8db3", "result": "AI\u751f\u6210\u7684\u4ee3\u7801\u6bd4\u4eba\u5de5\u4ee3\u7801\u5b58\u57281.7\u500d\u66f4\u591a\u95ee\u9898\uff0c\u4ee3\u7801\u8d28\u91cf\u66f4\u4e0d\u7a33\u5b9a\u3001\u66f4\u5bb9\u6613\u51fa\u9519\uff0c\u4e14\u66f4\u5bb9\u6613\u5f15\u5165\u9ad8\u4e25\u91cd\u6027\u6f0f\u6d1e", "conclusion": "AI\u8f85\u52a9\u5f00\u53d1\u7684\u672a\u6765\u5728\u4e8e\u6784\u5efa\u80fd\u591f\u653e\u5927AI\u4f18\u52bf\u540c\u65f6\u8865\u507f\u5176\u7f3a\u9677\u7684\u7cfb\u7edf\u3001\u5de5\u4f5c\u6d41\u7a0b\u548c\u5b89\u5168\u5c42\uff0c\u8d28\u91cf\u9700\u8981\u7ecf\u8fc7\u6df1\u601d\u719f\u8651\u7684\u5de5\u7a0b\u5316\u8bbe\u8ba1", "topic": "swe application"}}
