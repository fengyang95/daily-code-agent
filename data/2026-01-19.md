<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [tldr.article](#tldr.article) [Total: 9]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.LG](#cs.LG) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning](https://arxiv.org/abs/2601.10775)
*Tommaso Felice Banfi,Sashenka Gamage*

Main category: cs.CL

TL;DR: 提出基于LLM的离散博弈推理框架，通过熵引导的自适应推理机制，在井字棋任务中显著提升决策质量


<details>
  <summary>Details</summary>
Motivation: 现有LLM在序列决策任务中缺乏不确定性感知能力，无法根据决策难度动态调整推理复杂度，导致在博弈环境中表现不佳

Method: 结合上下文学习与熵引导的思维链推理，根据token级不确定性动态调整检索示例数量和推理路径：低不确定性时使用简洁推理，高不确定性时触发多路径探索

Result: 在100场对次优算法对手的游戏中，平均游戏结果从基线LLM的-11.6%提升到熵引导自适应推理的+9.5%，同时保持较低的LLM查询次数，统计验证显示改进显著

Conclusion: 不确定性引导的自适应推理能有效增强LLM在序列决策环境中的性能，token级熵与移动最优性呈负相关关系

Abstract: We propose a novel LLM-based framework for reasoning in discrete, game-theoretic tasks, illustrated with \emph{Tic-Tac-Toe}. The method integrates in-context learning with entropy-guided chain-of-thought (CoT) reasoning and adaptive context retrieval. The model dynamically adjusts both the number of retrieved examples and reasoning paths according to token-level uncertainty: concise reasoning with minimal context is used when uncertainty is low, whereas higher uncertainty triggers expanded multi-path CoT exploration. Experimental evaluation against a sub-optimal algorithmic opponent shows that entropy-aware adaptive reasoning substantially improves decision quality, increasing the average game outcome from \(-11.6\%\) with the baseline LLM to \(+9.5\%\) with entropy-guided adaptive reasoning over 100 games (win = +1, tie = 0, loss = -1), while maintaining a relatively low number of LLM queries per game. Statistical validation confirms that the improvement is significant, and correlation analysis reveals a negative association between token-level entropy and move optimality. These findings demonstrate that uncertainty-guided adaptive reasoning effectively enhances LLM performance in sequential decision-making environments.

</details>


### [2] [A Concise Agent is Less Expert: Revealing Side Effects of Using Style Features on Conversational Agents](https://arxiv.org/abs/2601.10809)
*Young-Min Cho,Yuan Yuan,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.CL

TL;DR: 研究系统分析了LLM对话代理中风格特征的交叉副作用，发现风格特征之间存在深度纠缠而非正交，提示一个特征会因果影响其他特征，挑战了LLM中忠实风格控制的假设。


<details>
  <summary>Details</summary>
Motivation: LLM对话代理中广泛使用友好、有帮助、简洁等风格特征来引导行为，但这些特征的意外副作用尚未被充分理解。需要系统研究风格特征之间的交叉副作用。

Method: 1) 对ACL Anthology中127篇对话代理论文进行调研，识别12个常用风格特征；2) 在任务导向和开放域设置中使用受控合成对话；3) 通过成对LLM作为评判框架量化提示一个风格特征如何因果影响其他特征；4) 引入CASSE数据集；5) 评估基于提示和激活导向的缓解策略。

Result: 发现一致且结构化的副作用，例如提示简洁性会显著降低感知的专业性。风格特征是深度纠缠而非正交的。缓解策略可以部分恢复被抑制的特征，但通常会降低主要预期风格。

Conclusion: 研究结果挑战了LLM中忠实风格控制的假设，强调需要多目标和更原则性的方法来实现对话代理中安全、有针对性的风格引导。

Abstract: Style features such as friendly, helpful, or concise are widely used in prompts to steer the behavior of Large Language Model (LLM) conversational agents, yet their unintended side effects remain poorly understood. In this work, we present the first systematic study of cross-feature stylistic side effects. We conduct a comprehensive survey of 127 conversational agent papers from ACL Anthology and identify 12 frequently used style features. Using controlled, synthetic dialogues across task-oriented and open domain settings, we quantify how prompting for one style feature causally affects others via a pairwise LLM as a Judge evaluation framework. Our results reveal consistent and structured side effects, such as prompting for conciseness significantly reduces perceived expertise. They demonstrate that style features are deeply entangled rather than orthogonal. To support future research, we introduce CASSE (Conversational Agent Stylistic Side Effects), a dataset capturing these complex interactions. We further evaluate prompt based and activation steering based mitigation strategies and find that while they can partially restore suppressed traits, they often degrade the primary intended style. These findings challenge the assumption of faithful style control in LLMs and highlight the need for multi-objective and more principled approaches to safe, targeted stylistic steering in conversational agents.

</details>


### [3] [Reasoning Models Generate Societies of Thought](https://arxiv.org/abs/2601.10825)
*Junsol Kim,Shiyang Lai,Nino Scherrer,Blaise Agüera y Arcas,James Evans*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型的推理能力提升并非仅源于更长的思维链，而是来自模拟多智能体交互的"思想社会"，通过不同个性和专业视角的多样化与辩论实现更优问题解决。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在推理任务上表现出色，但其背后的机制尚不明确。传统观点认为推理能力的提升主要来自更长的计算链（思维链），但本文旨在探索是否存在更深层的多智能体交互机制。

Method: 采用定量分析和机制可解释性方法分析推理轨迹，研究DeepSeek-R1和QwQ-32B等推理模型。通过受控强化学习实验，奖励推理准确性观察模型行为变化，并使用对话脚手架微调模型。

Result: 推理模型比指令微调模型展现出更高的视角多样性，在推理过程中激活更多异质个性和专业特征冲突。模型表现出问答、视角转换、冲突观点调和等对话行为，以及尖锐来回对话的社会情感角色，这些共同解释了推理任务中的准确性优势。

Conclusion: 推理能力的提升源于思想的社会组织，模拟多智能体交互实现解决方案空间的更有效探索。这类似于人类群体的集体智能，多样性在系统结构化时能实现更优问题解决，为智能体组织利用群体智慧提供了新机会。

Abstract: Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.

</details>


### [4] [When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs](https://arxiv.org/abs/2601.11000)
*Zhongxiang Sun,Yi Zhan,Chenglei Shen,Weijie Yu,Xiao Zhang,Ming He,Jun Xu*

Main category: cs.CL

TL;DR: 论文提出FPPS方法解决个性化LLMs在事实查询时产生的幻觉问题，同时保持个性化性能，并创建了PFQABench基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 个性化LLMs在增强用户体验的同时，可能导致事实推理的扭曲，产生"个性化诱导幻觉"，即模型根据用户历史而非客观事实生成答案，这会降低事实可靠性并传播错误信念。

Method: 提出Factuality-Preserving Personalized Steering (FPPS)，一种轻量级的推理时方法，通过解耦个性化表示和事实表示来减轻个性化导致的事实扭曲，同时保持个性化行为。

Result: 在多个LLM主干网络和个性化方法上的实验表明，FPPS显著提高了事实准确性，同时保持了个性化性能。

Conclusion: FPPS能有效缓解个性化LLMs中的事实扭曲问题，在保持个性化优势的同时提升事实可靠性，为解决个性化与事实性之间的权衡提供了有效方案。

Abstract: Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.

</details>


### [5] [Language of Thought Shapes Output Diversity in Large Language Models](https://arxiv.org/abs/2601.11227)
*Shaoyang Xu,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 通过控制大语言模型的"思维语言"（思考时使用的语言）可以显著增加输出多样性，非英语思维语言比英语产生更多样化输出，且语言距离英语越远多样性增益越大。


<details>
  <summary>Details</summary>
Motivation: 输出多样性对大语言模型至关重要，支持多元主义和创造力。当前研究揭示控制模型思考时使用的语言（思维语言）是输出多样性的新颖结构化来源。

Method: 研究不同思维语言在模型思考空间中的分布，评估两种重复采样策略：单语言采样和混合语言采样。通过控制输出为英语，但改变思维语言，进行广泛的多样性评估实验。

Result: 将思维语言从英语切换到非英语语言能持续增加输出多样性，且与英语在思考空间中距离越远的语言带来更大的多样性增益。聚合多个思维语言的样本通过组合效应产生额外改进，语言异质性扩展了模型的多样性上限。

Conclusion: 思维语言控制是增强LLM输出多样性的有效方法，在多元对齐场景中具有实际应用价值，能扩大文化知识和价值取向的覆盖范围。

Abstract: Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.

</details>


### [6] [Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming](https://arxiv.org/abs/2601.11332)
*Sama Hadhoud,Alaa Elsetohy,Frederikus Hudi,Jan Christian Blaise Cruz,Steven Halim,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 该论文提出将自然语言题解作为代码生成和评估的核心，认为竞赛编程本质是问题解决任务，应明确区分算法推理和代码实现。研究发现使用专家编写的题解能显著提升模型解决率，但模型在实现方面仍有困难，揭示了算法规范化的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将算法推理与代码实现混为一谈，而竞赛编程本质上是问题解决任务。需要明确区分问题解决和实现，并建立更准确的评估框架。

Method: 提出以自然语言题解为中心的方法：1) 在代码生成前先生成题解；2) 引入包含83个ICPC风格问题的数据集，包含专家编写的题解和完整测试套件；3) 通过专家标注比较模型生成题解与标准题解；4) 验证LLM作为评判者的可扩展评估协议；5) 评估19个LLM模型。

Result: 1) 先生成题解再写代码能提升某些LLM的解决率；2) 使用专家编写的题解时提升更显著；3) 即使有标准题解，模型在实现方面仍有困难；4) 生成题解与标准题解之间的差距揭示了算法规范化的瓶颈；5) 验证了LLM作为评判者的评估协议。

Conclusion: 未来基准测试应明确区分问题解决和实现，自然语言题解应成为评估的核心组成部分。模型在算法规范化和代码实现方面仍有改进空间。

Abstract: Large Language Models (LLMs) increasingly succeed on competitive programming problems, yet existing evaluations conflate algorithmic reasoning with code-level implementation. We argue that competitive programming is fundamentally a problem-solving task and propose centering natural-language editorials in both solution generation and evaluation. Generating an editorial prior to code improves solve rates for some LLMs, with substantially larger gains when using expertly written gold editorials. However, even with gold editorials, models continue to struggle with implementation, while the gap between generated and gold editorials reveals a persistent problem-solving bottleneck in specifying correct and complete algorithms. Beyond pass/fail metrics, we diagnose reasoning errors by comparing model-generated editorials to gold standards using expert annotations and validate an LLM-as-a-judge protocol for scalable evaluation. We introduce a dataset of 83 ICPC-style problems with gold editorials and full test suites, and evaluate 19 LLMs, arguing that future benchmarks should explicitly separate problem solving from implementation.

</details>


### [7] [Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models](https://arxiv.org/abs/2601.11340)
*Guoming Ling,Zhongzhan Huang,Yupei Lin,Junxin Li,Shanshan Zhong,Hefeng Wu,Liang Lin*

Main category: cs.CL

TL;DR: 提出Neural Chain-of-Thought Search (NCoTS)框架，将推理重新定义为动态搜索最优思维策略，通过双因素启发式评估候选推理算子，在提升准确率3.5%的同时减少22%生成长度。


<details>
  <summary>Details</summary>
Motivation: 现有CoT模型顺序生成推理步骤缺乏前瞻性，容易陷入次优推理路径并产生冗余步骤，需要更高效的推理策略。

Method: 将推理重新定义为动态搜索问题，通过定量表征解空间发现稀疏的优质推理路径，使用双因素启发式（正确性和计算成本）评估候选推理算子，主动导航至更准确且简洁的推理路径。

Result: 在多样化推理基准测试中实现帕累托改进，准确率提升超过3.5%，生成长度减少超过22%，代码和数据已开源。

Conclusion: NCoTS框架通过将推理重新定义为搜索问题，发现了同时提高准确性和效率的稀疏优质推理路径，为LLM推理提供了新范式。

Abstract: Chain-of-Thought reasoning has significantly enhanced the problem-solving capabilities of Large Language Models. Unfortunately, current models generate reasoning steps sequentially without foresight, often becoming trapped in suboptimal reasoning paths with redundant steps. In contrast, we introduce Neural Chain-of-Thought Search (NCoTS), a framework that reformulates reasoning as a dynamic search for the optimal thinking strategy. By quantitatively characterizing the solution space, we reveal the existence of sparse superior reasoning paths that are simultaneously more accurate and concise than standard outputs. Our method actively navigates towards these paths by evaluating candidate reasoning operators using a dual-factor heuristic that optimizes for both correctness and computational cost. Consequently, NCoTS achieves a Pareto improvement across diverse reasoning benchmarks, boosting accuracy by over 3.5% while reducing generation length by over 22%. Our code and data are available at https://github.com/MilkThink-Lab/Neural-CoT-Search.

</details>


### [8] [Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences](https://arxiv.org/abs/2601.11379)
*Morgane Hoffmann,Emma Jouffroy,Warren Jouanneau,Marc Palyart,Charles Pebereau*

Main category: cs.CL

TL;DR: 本文提出一个框架来评估大语言模型在招聘决策中的逻辑，通过构建合成数据集分析LLM如何权衡不同匹配标准，发现LLM虽然主要考虑核心生产力信号，但对某些特征的解释超出了显式匹配价值，并在不同人口群体间存在交叉效应。


<details>
  <summary>Details</summary>
Motivation: 尽管通用大语言模型在招聘应用中显示出潜力，但尚不清楚LLM如何分配各属性的重要性，以及这种分配是否符合经济原则、招聘者偏好或更广泛的社会规范。需要评估LLM在招聘中的决策逻辑。

Method: 提出一个评估框架，借鉴分析人类招聘行为的经济学方法。从欧洲主要在线自由职业者市场构建真实自由职业者档案和项目描述的合成数据集，应用全因子设计来估计LLM在评估自由职业者-项目匹配时如何权衡不同相关标准。

Result: LLM权衡核心生产力信号（如技能和经验），但对某些特征的解释超出了其显式匹配价值。虽然对少数群体的平均歧视最小，但交叉效应显示生产力信号在不同人口群体间具有不同的权重。

Conclusion: 研究揭示了LLM在招聘决策中的权重分配模式，并展示了如何实施可比较的实验设置来评估模型与人类决策的一致性，为理解LLM在招聘应用中的行为提供了方法论和实证基础。

Abstract: General-purpose Large Language Models (LLMs) show significant potential in recruitment applications, where decisions require reasoning over unstructured text, balancing multiple criteria, and inferring fit and competence from indirect productivity signals. Yet, it is still uncertain how LLMs assign importance to each attribute and whether such assignments are in line with economic principles, recruiter preferences or broader societal norms. We propose a framework to evaluate an LLM's decision logic in recruitment, by drawing on established economic methodologies for analyzing human hiring behavior. We build synthetic datasets from real freelancer profiles and project descriptions from a major European online freelance marketplace and apply a full factorial design to estimate how a LLM weighs different match-relevant criteria when evaluating freelancer-project fit. We identify which attributes the LLM prioritizes and analyze how these weights vary across project contexts and demographic subgroups. Finally, we explain how a comparable experimental setup could be implemented with human recruiters to assess alignment between model and human decisions. Our findings reveal that the LLM weighs core productivity signals, such as skills and experience, but interprets certain features beyond their explicit matching value. While showing minimal average discrimination against minority groups, intersectional effects reveal that productivity signals carry different weights between demographic groups.

</details>


### [9] [Relational Linearity is a Predictor of Hallucinations](https://arxiv.org/abs/2601.11429)
*Yuetian Lu,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 研究发现大语言模型幻觉与关系线性度强相关：线性关系（如"演奏什么乐器"）更易导致幻觉，而非线性关系（如"出生地"）幻觉率较低


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型产生幻觉的核心原因，特别是为什么模型会对未知实体产生确定性回答而非承认无知

Method: 创建SyntHal数据集（6000个合成实体，6种关系），测量关系线性度（Δcos），评估四个模型的幻觉率，分析线性度与幻觉率的相关性

Result: 发现关系线性度与幻觉率存在强相关性（r∈[.78,.82]），线性关系（如"演奏乐器"）更易导致幻觉，非线性关系（如"出生地"）幻觉率较低

Conclusion: 关系的内在存储方式是影响模型自我知识评估能力的关键因素，线性关系以更抽象方式存储导致更易产生幻觉，这为管理幻觉行为和改进知识表示提供了新方向

Abstract: Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: "Which instrument did Glenn Gould play?", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $Δ\cos$. We find a strong correlation ($r \in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [10] [Open Responses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2011862984595795974.html%3Futm_source=tldrnewsletter/1/0100019bc68cbb45-30550b0b-b456-4bc9-9684-9fe0e657d51e-000000/ogtFqCs1JlH3MWSF3lVu6hVA1clbVh17_ujf8Yyo_qU=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Open Responses是一个开源规范，用于在OpenAI Responses API之上构建多提供商、可互操作的LLM接口，支持构建智能体系统而无需为每个模型重写技术栈。


<details>
  <summary>Details</summary>
Motivation: 解决LLM接口的碎片化问题，让开发者能够使用不同提供商的模型而无需为每个模型重写技术栈，实现真正的多提供商支持和互操作性。

Method: 基于OpenAI Responses API创建开源规范，提供多提供商默认支持和可扩展架构，允许开发者构建可互操作的LLM接口。

Result: 开发了Open Responses规范，支持构建智能体系统，已有多个基于该规范的项目示例，实现了多提供商LLM接口的无缝集成。

Conclusion: Open Responses为构建多提供商、可互操作的LLM接口提供了有效解决方案，简化了智能体系统的开发流程，促进了LLM生态系统的标准化。

Abstract: Open Responses (2 minute read) Open Responses is an open-source spec for building multi-provider, interoperable LLM interfaces on top of the original OpenAI Responses API. It is multi-provider by default and extensible without fragmentation. Useful for real-world workflows, Open Responses can be used to build agentic systems without rewriting the stack for every model. Examples of projects built with Open Responses are available in the thread.

</details>


### [11] [Oh My Opencode](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcode-yeongyu%2Foh-my-opencode%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/VVzFcouEZDAC8TegPVMXII_BBV9Qj3bsMB5KfgqlByg=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sisyphus是一个"开箱即用"的代理框架，旨在通过Claude、ChatGPT和Gemini等LLM显著提升编码生产力，支持复杂的多代理工作流和深度代码探索。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在编码任务中存在效率不足的问题，需要更系统化的框架来组织多代理协作和深度代码分析，以提升开发者的生产力。

Method: 开发了一个名为Sisyphus的"开箱即用"代理框架，支持与多种LLM集成，设计多代理工作流架构，实现深度代码探索和分析功能。

Result: 创建了一个功能完整的代理框架，能够显著提升编码生产力，支持复杂的多代理协作和深度代码分析工作流。

Conclusion: Sisyphus框架为LLM驱动的编码任务提供了一个强大的基础设施，通过多代理协作和深度代码探索能力，能够显著提升开发效率。

Abstract: Oh My Opencode (GitHub Repo) Sisyphus is a "batteries-included" agent harness designed to significantly boost coding productivity with LLMs like Claude, ChatGPT, and Gemini. It enables sophisticated multi-agent workflows and deep code exploration.

</details>


### [12] [Ralph](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsnarktank%2Fralph%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/P9PoSreAw36ExpJ37h2CfJBaPdt8iEbqQPvC5vJfIbQ=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ralph是一个自主AI代理循环系统，通过重复执行任务直到完成所有PRD项目，使用git历史、progress.txt和prd.json持久化内存，并通过AGENTS.md记录学习经验来增强未来开发。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI代理在复杂软件开发任务中需要持续执行、记忆保持和经验积累的问题，设计一个能够自主循环执行任务直到完成所有需求的系统。

Method: 采用自主AI代理循环架构，为每次迭代生成新的Amp实例，通过git历史、progress.txt和prd.json文件持久化内存和进度，并在AGENTS.md中记录学习经验。

Result: 开发了Ralph系统，能够持续执行软件开发任务直到完成所有PRD要求，实现了任务执行的自动化和经验积累的机制。

Conclusion: Ralph系统展示了自主AI代理循环在软件开发任务中的可行性，通过持久化内存和经验记录机制提高了任务执行的连续性和效率。

Abstract: Ralph (GitHub Repo) Ralph, an autonomous AI agent loop, is a system designed to repeatedly execute tasks until all Product Requirements Document (PRD) items are complete. It operates by spawning fresh Amp instances for each iteration, with memory persisted via git history, progress.txt, and prd.json, and updates AGENTS.md with learnings to enhance future development.

</details>


### [13] [Automated Code Reviews in Azure DevOps using OpenAI models powered by Microsoft Foundry](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjohnlokerse.dev%2F2026%2F01%2F06%2Fautomated-code-reviews-in-azure-devops-using-openai-models-powered-by-microsoft-foundry%2F%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/GkSi9BJU2aVCbjkEWk-trJCMCDKg9AxQf44ofI83Rb0=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用Microsoft Foundry和GPT-5模型为Azure DevOps构建自动化、低成本的AI代码审查系统，通过管道、脚本和可定制提示将结构化JSON反馈集成到拉取请求中。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查过程耗时且依赖人工经验，需要自动化解决方案来提高开发效率和代码质量，同时降低审查成本。

Method: 利用Microsoft Foundry平台和GPT-5模型构建自动化代码审查系统，通过Azure DevOps管道集成，使用脚本处理代码变更，生成结构化JSON格式的审查反馈，并通过可定制的提示工程优化审查质量。

Result: 成功构建了一个低成本的自动化代码审查系统，能够为Azure DevOps中的拉取请求提供结构化反馈，提高了代码审查效率和质量。

Conclusion: 基于Microsoft Foundry和GPT-5的自动化代码审查系统为Azure DevOps提供了有效的代码质量保障方案，展示了AI在软件开发流程中的实用价值。

Abstract: Automated Code Reviews in Azure DevOps using OpenAI models powered by Microsoft Foundry (10 minute read) This post explains how to build an automated, low-cost AI code reviewer for Azure DevOps using Microsoft Foundry and GPT-5 models, integrating structured JSON feedback into pull requests via pipelines, scripts, and customizable prompts.

</details>


### [14] [AI code review with comments you'll actually implement](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrdev%26utm_medium=email%26utm_campaign=codereview_260116secondary/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/8lAVaX1-1KmdA5VtkWkAuCjVHB1cqn8afdlVdxTN8G4=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Unblocked是一款AI代码审查工具，专注于识别真正的问题和有意义的反馈，而非琐碎的样式问题，帮助开发者减少AI疲劳


<details>
  <summary>Details</summary>
Motivation: 当前AI代码审查工具存在过度关注样式细节和低价值评论的问题，导致开发者产生AI疲劳，需要更智能、上下文感知的工具来提供真正有价值的反馈

Method: 通过AI技术分析完整代码库上下文，识别真正影响代码质量和功能的问题，过滤掉样式性挑剔，提供只有具备完整代码库视图的开发者才能给出的反馈

Result: 开发者反馈表明Unblocked改变了他们对AI疲劳的看法，能够提供基于完整代码库上下文的智能反馈，帮助识别真正重要的问题

Conclusion: Unblocked作为AI代码审查工具，通过专注于实质性问题而非样式挑剔，为开发者提供了真正有价值的代码审查体验

Abstract: AI code review with comments you'll actually implement (Sponsor) Unblocked is the AI code review that surfaces real issues and meaningful feedback instead of flooding your PRs with stylistic nitpicks and low-value comments. “Unblocked made me reconsider my AI fatigue. Finally, a tool that surfaces context only someone with a full view of the codebase could provide.” - Senior developer, Clio Try now for free

</details>


### [15] [Gambit](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbolt-foundry%2Fgambit%3Futm_source=tldrdev/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/5YeIeuDlQ_X7DR2ZDf2Vj4WqcWJno2exRSbGRwM9HoQ=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gambit是一个用于构建LLM工作流的代理框架，通过创建小型、类型化的"decks"来提供清晰的输入输出和防护机制，解决现有工作流中的脆弱编排、无类型I/O、上下文过长和调试困难等问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工作流存在多个问题：编排脆弱、输入输出缺乏类型安全、上下文信息过多导致效率低下、调试困难。这些限制了LLM在实际生产环境中的应用。

Method: 采用"decks"概念，创建小型、类型化的组件，每个deck有明确的输入输出定义和防护机制。支持混合LLM任务和计算任务，实现局部化逻辑处理。

Result: 开发了Gambit框架，解决了LLM工作流中的核心痛点，提供了更健壮、可维护和可调试的工作流构建方案。

Conclusion: Gambit框架通过类型化的decks设计，显著改善了LLM工作流的可靠性、可维护性和调试效率，为构建生产级LLM应用提供了有效工具。

Abstract: Gambit (GitHub Repo) Gambit is an agent harness framework for building LLM workflows by creating small, typed "decks" with clear inputs, outputs, and guardrails. It fixes brittle orchestration, untyped I/O, excessive context, and difficult debugging by allowing a mix of LLM and compute tasks with localized logic.

</details>


### [16] [The Agentic AI Handbook: Production-Ready Patterns](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagentic-handbook%2F%3Futm_source=tldrdev/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/vX7Oc-KmauwZ3T_z7ApFUpjauw1hncW6447JKyvfFNc=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 收集了113个生产就绪的AI代理构建与部署模式，基于真实系统经验


<details>
  <summary>Details</summary>
Motivation: 为AI代理的实际生产部署提供实用指导，解决真实世界系统中的可靠性和可操作性挑战

Method: 从真实世界系统中收集和整理113个生产就绪的模式，形成系统化的实践指南

Result: 创建了一个全面的AI代理生产部署模式手册，涵盖构建和部署的各个方面

Conclusion: 提供了实用的、经过验证的AI代理生产部署模式，帮助开发者构建可靠的AI系统

Abstract: The Agentic AI Handbook: Production-Ready Patterns (26 minute read) This is a collection of 113 production-ready patterns derived from real-world systems for building and deploying reliable AI agents.

</details>


### [17] [Inside the First Autonomous Agent Economy on Solana](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGmeZaY/1/0100019bc6ea5eda-a4a0732d-cb50-4df7-8cc4-3aa48e7906d7-000000/b1vpCXXZ3i8BGBXDRn0kUz2B6Fb-jINeP6cfTgq2CFA=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 在Solana区块链上建立了首个完全自主的AI代理经济系统，代理能够独立交易、协商和分配资源


<details>
  <summary>Details</summary>
Motivation: 创建无需人类干预的自主AI代理经济系统，让AI代理能够像人类一样进行经济活动和市场交互

Method: 基于Solana区块链和x402协议基础设施，借鉴Microsoft Research的Magentic Marketplace和Stanford HAI的代理社会实验，构建自主代理经济框架

Result: 建立了首个在Solana上的完全自主代理经济，代理能够发现服务、执行支付并形成涌现的市场结构

Conclusion: 成功展示了AI代理在区块链上自主运行经济系统的可行性，为未来的去中心化AI经济奠定了基础

Abstract: Inside the First Autonomous Agent Economy on Solana (4 minute read) t54.ai has launched what it claims is the first fully autonomous agent economy on Solana, where AI agents independently transact, negotiate, and allocate resources using the x402 protocol infrastructure. Drawing lessons from Microsoft Research's Magentic Marketplace and Stanford HAI's agent society experiments, the system enables agents to discover services, execute payments, and form emergent market structures without human ...

</details>


### [18] [Vibe coding is over. Gen Z founders are vibing entire businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fx.com%2Fatoms_dev%2Fstatus%2F2011060882940575842/2/0100019bc6eb6834-8acf2aaa-3173-4689-8403-322eacf85b97-000000/QKnxUaRQ_AzL9pimRaBfQEjBYwuaOm5dbWhBNDS7STI=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gen Z创业者正在使用多智能体系统替代传统编程，这些系统能同时处理研究、产品、工程和用户获取，实现快速业务验证和规模化


<details>
  <summary>Details</summary>
Motivation: 传统编程方式效率有限，需要大量人力资源和专业知识。新一代创业者希望快速验证想法、推出产品并获取用户，而不需要深厚的领域专业知识

Method: 部署多智能体系统，这些系统协同工作处理从研究到用户获取的全流程，而不是局限于编写代码

Result: 实现分钟级的想法验证、产品发布和用户获取，不再需要通过雇佣更多人员来实现规模化

Conclusion: 创业方式正在发生根本性转变：从依靠人力规模扩张转向通过更好的决策和自动化系统实现规模化

Abstract: Vibe coding is over. Gen Z founders are vibing entire businesses (Sponsor) What's changing is how new startups get built.Instead of limiting agents to writing code, new founders are deploying multi-agent systems that handle research, product, engineering, and user acquisition together. Ideas get validated, products ship, and customers get acquired in minutes, without requiring deep domain expertise. The shift is simple. You no longer scale by hiring more people. You scale by making better dec...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [LogicLens: Leveraging Semantic Code Graph to explore Multi Repository large systems](https://arxiv.org/abs/2601.10773)
*Niko Usai,Dario Montagnini,Kristian Ilianov Iliev,Raffaele Camanzo*

Main category: cs.SE

TL;DR: LogicLens是一个反应式对话代理，通过语义多仓库图帮助开发者探索复杂软件系统，结合代码分析和LLM语义增强，支持自然语言交互和新兴能力如影响分析和基于症状的调试。


<details>
  <summary>Details</summary>
Motivation: 理解大型软件系统具有挑战性，尤其是代码分布在多个仓库和微服务中。开发者不仅需要理解代码结构，还需要理解通常隐含且分散的领域逻辑和运行时行为。

Method: 通过预处理步骤构建语义多仓库图，结合语法代码分析（AST解析和仓库遍历）与使用LLM的语义增强。该图捕获结构元素（文件、类、函数）和功能抽象（领域实体、操作、工作流）。然后通过自然语言交互动态检索相关子图并回答技术或功能查询。

Result: 展示了系统的架构，讨论了新兴行为，并在真实世界的多仓库场景中评估了其有效性。演示了从语义图结构中自然产生的新兴能力，包括影响分析和基于症状的调试。

Conclusion: LogicLens通过语义多仓库图和自然语言交互，有效帮助开发者探索和理解复杂软件系统，展现了从图结构中自然产生的新兴能力。

Abstract: Understanding large software systems is a challenging task, especially when code is distributed across multiple repositories and microservices. Developers often need to reason not only about the structure of the code, but also about its domain logic and runtime behaviors, which are typically implicit and scattered. We introduce LogicLens, a reactive conversational agent that assists developers in exploring complex software systems through a semantic multi-repository graph. This graph is built in a preprocessing step by combining syntactic code analysis, via AST parsing and repository traversal, with semantic enrichment using Large Language Models (LLMs). The resulting graph captures both structural elements, such as files, classes, and functions, as well as functional abstractions like domain entities, operations, and workflows. Once the graph is constructed, LogicLens enables developers to interact with it via natural language, dynamically retrieving relevant subgraphs and answering technical or functional queries. We present the architecture of the system, discuss emergent behaviors, and evaluate its effectiveness on real-world multi-repository scenarios. We demonstrate emergent capabilities including impact analysis and symptom-based debugging that arise naturally from the semantic graph structure.

</details>


### [20] [Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation](https://arxiv.org/abs/2601.10942)
*Zitong Zhou,Matteo Paltenghi,Miryung Kim,Michael Pradel*

Main category: cs.SE

TL;DR: ChaCo是一个基于LLM的测试增强技术，专门针对PR中未覆盖的代码行生成测试，填补"最后一英里"回归测试空白


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，即使有完善的测试套件，PR修改的代码行仍可能未被测试覆盖，形成"最后一英里"回归测试空白。现有测试生成器通常关注整体覆盖率，而非专门针对PR中的未覆盖行。

Method: ChaCo采用LLM生成测试，重点解决三个问题：(1) 针对PR特定的补丁覆盖率；(2) 提取相关测试上下文（现有测试函数、fixtures、数据生成器）；(3) 将生成的测试与现有测试套件集成，保持结构和风格一致，并生成测试添加摘要供开发者审查。

Result: 在三个开源项目（SciPy、Qiskit、Pandas）的145个PR上评估，帮助30%的PR实现完全补丁覆盖，成本仅0.11美元。人工评审认为测试值得添加（4.53/5.0）、集成良好（4.2/5.0）、与PR相关（4.7/5.0）。提交的12个测试中8个已被合并，并发现并修复了2个未知bug。

Conclusion: ChaCo能有效填补PR测试覆盖的空白，具有实际应用价值，可集成到CI工作流中自动化最后一英里回归测试增强。

Abstract: Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a "last-mile" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.

</details>


### [21] [ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development](https://arxiv.org/abs/2601.11077)
*Jie Yang,Honglin Guo,Li Ji,Jiazheng Zhou,Rui Zheng,Zhikai Lei,Shuo Zhang,Zhiheng Xi,Shichun Liu,Yuxin Wang,Bo Wang,Yining Zheng,Tao Gui,Xipeng Qiu*

Main category: cs.SE

TL;DR: ABC-Bench是一个专门评估智能体后端编码能力的基准测试，要求智能体在真实可执行的工作流程中完成从仓库探索到容器化服务部署的完整开发生命周期。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要评估静态上下文中的代码逻辑，忽略了现实世界工程（特别是后端开发）所需的动态、全过程要求，包括环境配置和服务部署等复杂环节。

Method: 通过可扩展的自动化管道，从开源仓库中收集了224个实际任务，涵盖8种编程语言和19个框架，要求智能体管理从仓库探索到实例化容器化服务的完整开发生命周期。

Result: 评估显示，即使是当前最先进的模型在这些整体任务上也难以提供可靠的性能，突显了当前模型能力与实际后端工程需求之间的显著差距。

Conclusion: ABC-Bench填补了现有基准测试的空白，为评估智能体后端编码能力提供了更真实、全面的测试环境，揭示了当前AI编码智能体在实际工程应用中的局限性。

Abstract: The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://arxiv.org/abs/2601.10719)
*Gerard Yeo,Svetlana Churina,Kokil Jaidka*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在预训练过程中隐式编码了心理可信度信号，无需显式监督就能区分高信任和低信任文本，为设计可信AI系统提供了表征基础。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型（LLMs）是否以心理一致的方式表示在线信息的可信度，特别是在LLMs日益嵌入搜索、推荐和对话系统的情况下。需要了解LLMs如何编码感知可信度，以及这种编码是否与人类信任形成的心理维度一致。

Method: 使用指令调优的LLMs（Llama 3.1 8B、Qwen 2.5 7B、Mistral 7B）分析PEACE-Reviews数据集，该数据集标注了认知评估、情感和行为意图。通过分析模型层和头级别的激活差异来区分高信任和低信任文本，并进行探测分析来研究可信度信号的可解码性和微调效果。

Result: 研究发现：1）所有模型在层和头级别都显示出区分高信任和低信任文本的系统激活差异；2）可信度信号是线性可解码的；3）微调会优化而非重构这些表征；4）最强的关联出现在公平性、确定性和自我责任等人类信任形成的核心维度上。

Conclusion: 现代LLMs在没有显式监督的情况下内化了心理基础的可信度信号，这为在网络生态系统中设计可信、透明和值得信赖的AI系统提供了表征基础。

Abstract: Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.

</details>


### [23] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: 提出约束时间分层架构(CTHA)，通过结构化流形投影和仲裁机制解决多时间尺度智能体架构中的协调稳定性问题，显著减少失败级联并提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 多时间尺度智能体架构虽然提升了性能，但破坏了统一智能体系统的协调稳定性，导致严重的层间冲突、无界错误传播和可扩展性受限。

Method: 提出约束时间分层架构(CTHA)，包含三个关键约束：消息契约约束（通过类型化摘要、计划、策略包形式化层间信息流）、权威流形约束（根据时间范围限定各层决策空间）、仲裁解决约束（保证多层决策的无冲突组合）。

Result: CTHA在复杂任务执行中有效，相比无约束分层基线减少47%的失败级联，提升2.3倍的样本效率，并展现出优越的可扩展性。

Conclusion: CTHA作为时间分层架构的原则性扩展，有助于深入理解多智能体协调，并为鲁棒自主系统的演进提供有前景的方向。

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [24] [Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration](https://arxiv.org/abs/2601.10744)
*Sen Wang,Bangwei Liu,Zhenkun Gao,Lizhuang Ma,Xuhong Wang,Yuan Xie,Xin Tan*

Main category: cs.AI

TL;DR: 提出LMEE框架和MemoryExplorer方法，通过强化学习微调多模态大语言模型，促进智能体主动探索和记忆利用，在长时程具身任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体主要关注任务完成结果，忽视了探索过程和记忆利用，而理想的具身智能体应具备终身学习能力，能够利用长期情景记忆优化决策。

Method: 提出LMEE框架统一探索认知和决策行为，构建LMEE-Bench数据集和基准测试，开发MemoryExplorer方法通过强化学习微调多模态大语言模型，采用包含动作预测、边界选择和问答的多任务奖励函数。

Result: 与最先进的具身探索模型相比，该方法在长时程具身任务中取得了显著优势，证明了主动记忆查询和探索的有效性。

Conclusion: LMEE框架和MemoryExplorer方法能够有效促进具身智能体的终身学习能力，通过主动探索和记忆利用提升长时程复杂任务的性能。

Abstract: An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.

</details>


### [25] [AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing](https://arxiv.org/abs/2601.11007)
*Zhenhua Xu,Dongsheng Chen,Shuo Wang,Jian Li,Chengjie Wang,Meng Han,Yabiao Wang*

Main category: cs.AI

TL;DR: AdaMARP是一个自适应多智能体角色扮演框架，通过沉浸式消息格式和场景管理器解决现有系统沉浸感和适应性不足的问题，在角色一致性、环境基础和叙事连贯性方面显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM角色扮演系统存在沉浸感和适应性不足的问题，通常对环境动态信息建模不足，假设场景和角色基本静态，对多角色编排、场景转换和动态角色引入支持不够。

Method: 提出AdaMARP框架，包含沉浸式消息格式（交织[思考]、<动作>、<环境>和对话）和显式场景管理器，通过离散动作（初始化场景、选择说话者、切换场景、添加角色、结束）及相应理由来管理角色扮演。构建AdaRPSet训练角色模型能力，AdaSMSet监督编排决策，并引入AdaptiveBench进行轨迹级评估。

Result: 实验表明：AdaRPSet提升了角色一致性、环境基础和叙事连贯性，8B参数的角色模型超越多个商业LLM；AdaSMSet实现了更平滑的场景转换和更自然的角色引入，仅用14B LLM就超越了Claude Sonnet 4.5。

Conclusion: AdaMARP框架通过创新的沉浸式消息格式和显式场景管理器，有效解决了现有角色扮演系统的局限性，在多角色编排和动态场景管理方面表现出色，为交互式叙事提供了更强大的支持。

Abstract: LLM role-playing aims to portray arbitrary characters in interactive narratives, yet existing systems often suffer from limited immersion and adaptability. They typically under-model dynamic environmental information and assume largely static scenes and casts, offering insufficient support for multi-character orchestration, scene transitions, and on-the-fly character introduction. We propose an adaptive multi-agent role-playing framework, AdaMARP, featuring an immersive message format that interleaves [Thought], (Action), <Environment>, and Speech, together with an explicit Scene Manager that governs role-playing through discrete actions (init_scene, pick_speaker, switch_scene, add_role, end) accompanied by rationales. To train these capabilities, we construct AdaRPSet for the Actor Model and AdaSMSet for supervising orchestration decisions, and introduce AdaptiveBench for trajectory-level evaluation. Experiments across multiple backbones and model scales demonstrate consistent improvements: AdaRPSet enhances character consistency, environment grounding, and narrative coherence, with an 8B actor outperforming several commercial LLMs, while AdaSMSet enables smoother scene transitions and more natural role introductions, surpassing Claude Sonnet 4.5 using only a 14B LLM.

</details>


### [26] [BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search](https://arxiv.org/abs/2601.11037)
*Shiyu Liu,Yongjing Yin,Jianhao Yan,Yunbo Tang,Qinggang Zhang,Bei Li,Xin Chen,Jingang Wang,Xunliang Cai,Jinsong Su*

Main category: cs.AI

TL;DR: BAPO框架通过边界感知奖励和自适应奖励调制器，让基于RL的智能体在证据不足时学会说"我不知道"，提高可靠性而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的智能体搜索虽然能提高准确性，但缺乏可靠性边界意识，即使在证据不足或推理达到极限时也很少承认"我不知道"，导致产生看似合理但不可靠的答案，这在现实场景中存在重大风险。

Method: 提出边界感知策略优化(BAPO)框架，包含两个关键组件：(1)基于群体的边界感知奖励，鼓励在推理达到极限时给出IDK响应；(2)自适应奖励调制器，在早期探索阶段策略性地暂停该奖励，防止模型将IDK作为捷径利用。

Result: 在四个基准测试上的广泛实验表明，BAPO显著增强了智能体搜索的整体可靠性。

Conclusion: BAPO框架能够在不损害准确性的情况下培养可靠的边界意识，解决了当前智能体搜索中可靠性不足的关键问题。

Abstract: RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.

</details>


### [27] [AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts](https://arxiv.org/abs/2601.11044)
*Keyu Li,Junhao Shi,Yang Xiao,Mohan Jiang,Jie Sun,Yunze Wu,Shijie Xia,Xiaojie Cai,Tianze Xu,Weiye Si,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: AgencyBench是一个全面的基准测试，评估6种核心智能体能力，包含32个真实场景和138个任务，通过用户模拟代理和Docker沙箱实现自动化评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注单一智能体能力，无法捕捉长视野的真实世界场景，且依赖人工反馈造成可扩展性瓶颈，阻碍自动化评估。

Method: 构建包含32个真实场景、138个任务的基准测试，使用用户模拟代理提供迭代反馈，Docker沙箱进行视觉和功能评估，涵盖6种核心智能体能力。

Result: 闭源模型显著优于开源模型（48.4% vs 32.1%），不同模型在资源效率、反馈驱动自校正和特定工具使用偏好上存在显著差异，专有模型在其原生生态系统中表现更优。

Conclusion: AgencyBench为下一代智能体提供关键测试平台，强调需要共同优化模型架构与智能体框架，为自主智能体的未来发展指明方向。

Abstract: Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.

</details>


### [28] [ReCreate: Reasoning and Creating Domain Agents Driven by Experience](https://arxiv.org/abs/2601.11100)
*Zhezheng Hao,Hong Wang,Jian Luo,Jianqing Zhang,Yuyan Zhou,Qiang Lin,Can Wang,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: ReCreate是一个基于经验驱动的自动创建领域智能体框架，通过分析智能体交互历史来优化智能体设计，在多个领域超越人工设计智能体和现有自动生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数实用智能体仍由人工设计，因为任务差异大且构建成本高。现有自动生成方法将智能体生成视为黑盒过程，仅依赖最终性能指标，忽略了成功/失败的关键证据，且计算成本高。

Method: 提出ReCreate框架，采用智能体即优化器范式，包含三个关键组件：1) 经验存储与检索机制；2) 推理-创建协同管道，将执行经验映射为脚手架编辑；3) 分层更新，将实例级细节抽象为可重用领域模式。

Result: 在多个不同领域的实验中，ReCreate始终优于人工设计的智能体和现有自动智能体生成方法，即使从最小种子脚手架开始也能取得良好效果。

Conclusion: ReCreate通过系统利用智能体交互历史中的具体信号，成功实现了自动创建和适应领域智能体，解决了现有方法忽略关键证据和高计算成本的问题。

Abstract: Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.

</details>


### [29] [Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems](https://arxiv.org/abs/2601.11189)
*Sofiene Lassoued,Asrat Gobachew,Stefan Lier,Andreas Schwung*

Main category: cs.AI

TL;DR: 提出基于策略的深度强化学习超启发式框架解决作业车间调度问题，通过动作预过滤和承诺机制改进决策，在标准基准测试中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 作业车间调度问题（JSSP）是经典的NP难组合优化问题，传统启发式方法难以适应动态系统状态变化。需要开发能够根据系统状态动态切换调度规则的智能超启发式框架

Method: 提出基于策略的深度强化学习超启发式框架，包含两个关键机制：1) 动作预过滤，将决策限制在可行的低层动作，使低层启发式独立于环境约束进行评估；2) 承诺机制，调节启发式切换频率。研究了从逐步切换到完整周期承诺的不同策略，并比较了确定性贪婪选择和随机采样两种动作选择策略

Result: 在标准JSSP基准测试上的计算实验表明，该方法优于传统启发式、元启发式和最近的基于神经网络的调度方法

Conclusion: 提出的深度强化学习超启发式框架通过动作预过滤和承诺机制有效解决了JSSP问题，展示了在动态调度决策中的优越性能

Abstract: This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. We extend the hyper-heuristic framework with two key mechanisms. First, action prefiltering restricts decision-making to feasible low-level actions, enabling low-level heuristics to be evaluated independently of environmental constraints and providing an unbiased assessment. Second, a commitment mechanism regulates the frequency of heuristic switching. We investigate the impact of different commitment strategies, from step-wise switching to full-episode commitment, on both training behavior and makespan. Additionally, we compare two action selection strategies at the policy level: deterministic greedy selection and stochastic sampling. Computational experiments on standard JSSP benchmarks demonstrate that the proposed approach outperforms traditional heuristics, metaheuristics, and recent neural network-based scheduling methods

</details>


### [30] [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/2601.11252)
*Qianyue Wang,Jinwu Hu,Yufeng Wang,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Mingkui Tan*

Main category: cs.AI

TL;DR: 提出Think-with-Me交互式推理范式，在推理过程中引入外部反馈干预，通过暂停在过渡连词处进行反馈，自适应调整推理长度，在有限上下文窗口下实现准确率与推理长度的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在过度思考和推理偏移问题，导致计算成本增加和性能下降。现有高效推理方法缺乏外部干预机制来引导推理过程。

Method: 提出Think-with-Me测试时交互推理范式：1）在过渡连词处暂停推理进行外部反馈；2）使用多标准评估（合理性和完整性）生成反馈；3）通过Group Relative Policy Optimization训练模型适应交互模式。

Result: 在AIME24上，Think-with-Me在8K窗口下比QwQ-32B准确率提高7.19%，同时平均推理长度减少81%。该范式在安全和创意任务上也表现出优势。

Conclusion: Think-with-Me通过引入外部反馈干预，有效解决了大型推理模型的效率问题，在有限上下文下实现了准确率与推理效率的更好平衡。

Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.

</details>


### [31] [AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems](https://arxiv.org/abs/2601.11354)
*Weiyi Wang,Xinchi Chen,Jingjing Gong,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: AstroReason-Bench是一个用于评估智能体在空间规划问题中规划能力的基准测试，该基准整合了多种调度机制，发现当前智能体在物理约束下的真实世界规划中表现远不如专用求解器。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注符号化或弱接地环境，而在物理约束的真实世界领域中的性能尚未充分探索。空间规划问题具有异构目标、严格物理约束和长视野决策等特点，需要专门的评估基准。

Method: 引入AstroReason-Bench基准，整合地面站通信和敏捷地球观测等多种调度机制，提供统一的智能体导向交互协议，并在多种最先进的开源和闭源智能体LLM系统上进行评估。

Result: 当前智能体在空间规划问题中的表现显著低于专用求解器，揭示了通用规划器在现实约束下的关键局限性。

Conclusion: AstroReason-Bench为未来智能体研究提供了一个具有挑战性和诊断性的测试平台，有助于推动智能体在物理约束真实世界规划能力的发展。

Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents](https://arxiv.org/abs/2601.10820)
*Himanshu Thakur,Anusha Kamath,Anurag Muthyala,Dhwani Sanmukhani,Smruthi Mukund,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出一个规划引导的多智能体框架，用于自动化特征工程，通过图表示团队环境来协调智能体调用，实现代码生成，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成模型在真实ML团队中的应用受到限制：缺乏迭代复杂特征工程过程的数据集、现有代码智能体与团队特定工具/工作流集成不足、人机协作时机不当导致反馈不足。

Method: 采用规划引导、约束拓扑的多智能体框架，通过LLM驱动的规划器利用团队环境图协调可用智能体调用，生成上下文感知提示，利用下游失败回溯修正上游产物，并在关键步骤请求人工干预。

Result: 在内部数据集上，相比手动构建和无规划工作流，评估指标分别提升38%和150%。在实际应用中，为服务1.2亿用户的推荐模型构建特征时，将特征工程周期从三周缩短到一天。

Conclusion: 该规划引导的多智能体框架能有效解决特征工程自动化中的关键挑战，显著提升效率并实现真实世界影响。

Abstract: Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.

</details>


### [33] [Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration](https://arxiv.org/abs/2601.10973)
*Zain ul Abdeen,Waris Gill,Ming Jin*

Main category: cs.LG

TL;DR: 提出MGF-RL框架，结合元学习和进化策略，用于配电网故障恢复，能在不确定性和非线性约束下快速适应新场景。


<details>
  <summary>Details</summary>
Motivation: 极端事件后恢复关键负荷需要自适应控制，但可再生能源不确定性、可调度资源有限和非线性动态使得有效恢复困难。标准强化学习泛化能力差，需要大量重新训练。

Method: 提出元引导无梯度强化学习(MGF-RL)框架，结合一阶元学习和进化策略，从历史故障经验中学习可迁移初始化，无需梯度计算即可快速适应新场景。

Result: 在IEEE 13总线和123总线测试系统中，MGF-RL在可靠性、恢复速度和适应效率方面优于标准RL、基于MAML的元RL和模型预测控制，泛化到未见故障和可再生能源模式，需要更少的微调。

Conclusion: MGF-RL为可再生能源丰富的配电网实时负荷恢复提供了有效解决方案，具有理论保证和实际性能优势。

Abstract: Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.

</details>


### [34] [Reasoning Distillation for Lightweight Automated Program Repair](https://arxiv.org/abs/2601.10987)
*Aanand Balasubramanian,Sashank Silwal*

Main category: cs.LG

TL;DR: 该论文研究如何通过轻量级符号推理监督提升紧凑型自动程序修复模型的修复类型分类能力，提出推理蒸馏方法让大模型提供结构化符号推理标签，在CodeT5学生模型上验证了该方法能提升性能特别是低频错误类别。


<details>
  <summary>Details</summary>
Motivation: 小型代码模型适合资源受限环境，但通常只产生单一预测，不清楚它们是否学习有意义的程序结构还是依赖浅层相关性。需要提升轻量级程序修复模型的解释性和鲁棒性。

Method: 提出推理蒸馏方法：大型教师模型提供结构化符号推理标签和修复类型标签，这些标签捕获错误的高层因果属性而不依赖自由形式解释。在IntroClass基准上训练CodeT5学生模型，比较标签监督和推理蒸馏两种设置。

Result: 推理监督持续提升宏平均性能，特别是在较少出现的错误类别上，且不增加模型大小或复杂度。正确推理轨迹与正确预测强相关，但不完全决定预测结果。

Conclusion: 符号推理蒸馏是提升轻量级程序修复模型解释性和鲁棒性的实用方法，能帮助小型模型学习更有意义的程序结构而非依赖浅层相关性。

Abstract: We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.

</details>


### [35] [Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs](https://arxiv.org/abs/2601.11061)
*Lecheng Yan,Ruizhe Li,Guanhua Chen,Qing Li,Jiahui Geng,Wenxi Li,Vincent Wang,Chris Lee*

Main category: cs.LG

TL;DR: 研究发现RLVR训练中存在的"困惑度悖论"：虚假奖励导致模型绕过推理，通过记忆捷径提升性能，同时揭示了锚点-适配器电路机制


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR能有效增强LLM推理能力，但近期证据显示即使使用虚假或不正确的奖励，模型也能获得显著性能提升。研究者希望探究这一现象背后的机制，理解模型如何绕过推理过程

Method: 使用路径修补、Logit Lens、JSD分析和神经微分方程等技术，识别出隐藏的锚点-适配器电路。该电路包含中间层(L18-20)的功能锚点和后续层(L21+)的结构适配器

Result: 发现了"困惑度悖论"：虚假RLVR导致答案标记困惑度下降但提示侧连贯性退化。揭示了模型通过记忆捷径而非推理来提升性能的机制，并证明可以通过缩放特定MLP键来双向因果控制污染驱动的性能

Conclusion: 研究提供了识别和缓解RLVR调优模型中数据污染的机制路线图，揭示了模型如何通过记忆捷径绕过推理过程，为理解和控制这一现象提供了理论基础

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a "Perplexity Paradox": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.

</details>


### [36] [Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation](https://arxiv.org/abs/2601.11258)
*Pingzhi Tang,Yiding Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: PaST框架通过提取领域无关的技能向量，在目标模型轻量SFT后线性注入知识操作技能，解决LLMs知识更新问题，显著提升问答和工具使用性能。


<details>
  <summary>Details</summary>
Motivation: LLMs面临"知识截止"挑战，SFT更新事实内容但无法可靠提升模型使用新信息的能力，RL训练成本高不适用于高效在线适应。观察到SFT和RL的参数更新几乎正交，因此提出模块化技能转移框架。

Method: 提出参数化技能转移(PaST)框架：1)从源领域提取领域无关的技能向量；2)目标模型在新数据上进行轻量级SFT；3)将技能向量线性注入目标模型，实现知识操作技能的转移。

Result: 在SQuAD上比最先进的SFT基线提升9.9分；在LooGLE长上下文QA上获得8.0分绝对准确率提升；在ToolBench工具使用基准上平均提升10.3分成功率，跨工具类别一致改进。

Conclusion: PaST框架有效解决了LLMs知识更新问题，通过模块化技能转移实现高效知识适应，技能向量具有强可扩展性和跨领域可转移性。

Abstract: Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.

</details>


### [37] [Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency](https://arxiv.org/abs/2601.11352)
*Akhilesh Raj,Swann Perarnau,Aniruddha Gokhale,Solomon Bekele Abera*

Main category: cs.LG

TL;DR: 使用离线强化学习设计CPU功耗控制器，通过历史数据训练智能体来优化并行应用的能效，在可接受的性能损失下显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现代计算基础设施设计中能效至关重要，但在线强化学习训练存在挑战：缺乏合适的模拟环境模型、噪声干扰以及在实时系统上训练的可靠性问题。

Method: 采用离线强化学习方法，利用任意策略收集的状态转换数据集进行训练；结合灰盒方法，使用在线应用无关性能数据（如心跳）和硬件性能计数器，通过Intel的Running Average Power Limit控制实时系统功耗。

Result: 在各种计算密集型和内存密集型基准测试中，离线训练的智能体能够显著降低能耗，同时保持可容忍的性能下降。

Conclusion: 离线强化学习是设计自主CPU功耗控制器的有效替代方案，能够在不严重影响性能的情况下提高并行应用的运行时能效。

Abstract: Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.
  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.
  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.

</details>


### [38] [Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.11401)
*Ahmed Rashwan,Keith Briggs,Chris Budd,Lisa Kreusser*

Main category: cs.LG

TL;DR: 提出扩散价值函数（DVF）作为图马尔可夫决策过程的分解价值函数，通过时空衰减在影响图上扩散奖励，用于多智能体强化学习的信用分配，并开发了DA2C算法和LD-GNN稀疏消息传递执行器。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的信用分配问题，特别是在具有结构化局部交互的大规模系统中。图马尔可夫决策过程通过影响图捕捉这种设置，但标准评论家函数与此结构不匹配：全局价值函数提供弱的个体学习信号，而现有局部构造难以估计且在无限时域设置中表现不佳。

Method: 引入扩散价值函数（DVF），通过时间折扣和空间衰减在影响图上扩散奖励，为每个智能体分配价值分量。基于DVF提出扩散A2C（DA2C）算法和学习的DropEdge图神经网络（LD-GNN）稀疏消息传递执行器，用于通信成本下的去中心化算法学习。

Result: 在消防基准测试和三个分布式计算任务（向量图着色和两个传输功率优化问题）中，DA2C始终优于局部和全局评论家基线，平均奖励提升高达11%。

Conclusion: DVF为图马尔可夫决策过程提供了结构对齐的分解价值函数，能够有效解决多智能体强化学习中的信用分配问题，并通过DA2C和LD-GNN在实际任务中展现出优越性能。

Abstract: Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this structure: global value functions provide weak per-agent learning signals, while existing local constructions can be difficult to estimate and ill-behaved in infinite-horizon settings. We introduce the Diffusion Value Function (DVF), a factored value function for GMDPs that assigns to each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation. We show that DVF is well-defined, admits a Bellman fixed point, and decomposes the global discounted value via an averaging property. DVF can be used as a drop-in critic in standard RL algorithms and estimated scalably with graph neural networks. Building on DVF, we propose Diffusion A2C (DA2C) and a sparse message-passing actor, Learned DropEdge GNN (LD-GNN), for learning decentralised algorithms under communication costs. Across the firefighting benchmark and three distributed computation tasks (vector graph colouring and two transmit power optimisation problems), DA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%.

</details>
