{"id": "2601.08856", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08856", "abs": "https://arxiv.org/abs/2601.08856", "authors": ["Deeksha Nandal", "Riccardo Revalor", "Soham Dan", "Debjit Pal"], "title": "LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns", "comment": "18 Pages, 21 Figures, Submitted to ARR Review", "summary": "Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.", "AI": {"tldr": "LAUDE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u786c\u4ef6\u8bbe\u8ba1\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e0e\u8c03\u8bd5\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548c\u8bbe\u8ba1\u6267\u884c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u786c\u4ef6\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u8c03\u8bd5\u6210\u529f\u7387\u3002", "motivation": "\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u5355\u5143\u6d4b\u8bd5\u5bf9\u4e8e\u786e\u4fdd\u7ec4\u4ef6\u529f\u80fd\u6b63\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f00\u53d1\u9488\u5bf9\u5404\u79cd\u8bbe\u8ba1\u7279\u6027\u7684\u6d4b\u8bd5\u9700\u8981\u6df1\u5165\u7406\u89e3\u8bbe\u8ba1\u529f\u80fd\u5e76\u5177\u5907\u521b\u9020\u529b\u3002\u5f53\u6d4b\u8bd5\u66b4\u9732\u8bbe\u8ba1\u7f3a\u9677\u65f6\uff0c\u8c03\u8bd5\u8fc7\u7a0b\u901a\u5e38\u7e41\u7410\u4e14\u8017\u65f6\u3002\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u5347\u6d4b\u8bd5\u751f\u6210\u548c\u8c03\u8bd5\u6548\u7387\u3002", "method": "LAUDE\u6846\u67b6\u5c06\u786c\u4ef6\u8bbe\u8ba1\u6e90\u4ee3\u7801\u7684\u8bed\u4e49\u7406\u89e3\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u96c6\u6210\u63d0\u793a\u5de5\u7a0b\u548c\u8bbe\u8ba1\u6267\u884c\u4fe1\u606f\u6765\u589e\u5f3a\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4ee3\u7801\u53ef\u8c03\u8bd5\u6027\u3002\u8be5\u6846\u67b6\u652f\u6301\u95ed\u6e90\u548c\u5f00\u6e90LLM\uff0c\u5e76\u5728VerilogEval\u6570\u636e\u96c6\u7684\u9519\u8bef\u786c\u4ef6\u8bbe\u8ba1\u4ee3\u7801\u4e0a\u8fdb\u884c\u5e94\u7528\u3002", "result": "\u5728VerilogEval\u6570\u636e\u96c6\u4e0a\uff0cLAUDE\u751f\u6210\u7684\u5355\u5143\u6d4b\u8bd5\u5728\u7ec4\u5408\u8bbe\u8ba1\u4e2d\u68c0\u6d4b\u5230\u9ad8\u8fbe100%\u7684\u9519\u8bef\uff0c\u5728\u65f6\u5e8f\u8bbe\u8ba1\u4e2d\u68c0\u6d4b\u523093%\u7684\u9519\u8bef\uff1b\u5728\u8c03\u8bd5\u65b9\u9762\uff0c\u6210\u529f\u8c03\u8bd5\u4e8693%\u7684\u7ec4\u5408\u8bbe\u8ba1\u548c84%\u7684\u65f6\u5e8f\u8bbe\u8ba1\u3002", "conclusion": "LAUDE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u786c\u4ef6\u8bbe\u8ba1\u8bed\u4e49\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u786c\u4ef6\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u548c\u8c03\u8bd5\u80fd\u529b\uff0c\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.08884", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08884", "abs": "https://arxiv.org/abs/2601.08884", "authors": ["Samyak Jhaveri", "Cristina V. Lopes"], "title": "Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting", "comment": null, "summary": "OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the \"nano\"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.", "AI": {"tldr": "\u901a\u8fc7\u9057\u4f20-Pareto\u6846\u67b6\u4f18\u5316\u63d0\u793a\uff0c\u63d0\u5347\u5c0f\u578bLLM\u751f\u6210OpenACC\u5e76\u884c\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u4f7f\u7f16\u8bd1\u6210\u529f\u7387\u548cGPU\u52a0\u901f\u6548\u679c\u663e\u8457\u63d0\u5347", "motivation": "OpenACC\u964d\u4f4e\u4e86GPU\u5378\u8f7d\u7684\u95e8\u69db\uff0c\u4f46\u7f16\u5199\u9ad8\u6027\u80fd\u7684pragma\u4ecd\u7136\u590d\u6742\uff0c\u9700\u8981\u6df1\u539a\u7684\u5185\u5b58\u5c42\u6b21\u3001\u6570\u636e\u79fb\u52a8\u548c\u5e76\u884c\u5316\u7b56\u7565\u4e13\u4e1a\u77e5\u8bc6\u3002LLM\u4e3a\u81ea\u52a8\u5e76\u884c\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7b80\u5355\u7684\u63d0\u793a\u5f80\u5f80\u5bfc\u81f4\u8bed\u6cd5\u9519\u8bef\u3001\u65e0\u6cd5\u7f16\u8bd1\u6216\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528GEPA\uff08\u9057\u4f20-Pareto\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u5c04\u53cd\u9988\u5faa\u73af\u8fed\u4ee3\u6f14\u5316\u63d0\u793a\u3002\u8be5\u8fc7\u7a0b\u4f7f\u7528\u6307\u4ee4\u7684\u4ea4\u53c9\u548c\u53d8\u5f02\uff0c\u4ee5\u4e13\u5bb6\u7b56\u5212\u7684\u9ec4\u91d1\u793a\u4f8b\u4e3a\u6307\u5bfc\uff0c\u5e76\u57fa\u4e8e\u9ec4\u91d1\u548c\u9884\u6d4bpragma\u4e4b\u95f4\u7684\u5b50\u53e5\u548c\u5b50\u53e5\u53c2\u6570\u7ea7\u522b\u4e0d\u5339\u914d\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\u3002", "result": "\u5728PolyBench\u5957\u4ef6\u8bc4\u4f30\u4e2d\uff0c\u4f18\u5316\u63d0\u793a\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u8bd1\u6210\u529f\u7387\uff1aGPT-4.1 Nano\u4ece66.7%\u63d0\u5347\u523093.3%\uff0cGPT-5 Nano\u4ece86.7%\u63d0\u5347\u5230100%\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u66f4\u5927\u3001\u66f4\u6602\u8d35\u7248\u672c\u7684\u80fd\u529b\u3002\u4f18\u5316\u63d0\u793a\u8fd8\u4f7f\u5b9e\u73b0GPU\u52a0\u901f\u7684\u7a0b\u5e8f\u6570\u91cf\u589e\u52a0\u4e8621%\u3002", "conclusion": "\u63d0\u793a\u4f18\u5316\u6709\u6548\u91ca\u653e\u4e86\u66f4\u5c0f\u3001\u66f4\u4fbf\u5b9cLLM\u5728\u7f16\u5199\u7a33\u5b9a\u6709\u6548GPU\u5378\u8f7d\u6307\u4ee4\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3aHPC\u5de5\u4f5c\u6d41\u4e2d\u57fa\u4e8e\u6307\u4ee4\u7684\u81ea\u52a8\u5e76\u884c\u5316\u5efa\u7acb\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u9014\u5f84\u3002", "topic": "code agent"}}
{"id": "2601.09032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09032", "abs": "https://arxiv.org/abs/2601.09032", "authors": ["Logan Ritchie", "Sushant Mehta", "Nick Heiner", "Mason Yu", "Edwin Chen"], "title": "The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments", "comment": null, "summary": "The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \\emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7535\u5546RL\u73af\u5883\u8bc4\u4f30\u524d\u6cbfAI\u6a21\u578b\u5728150\u4e2a\u804c\u573a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u80fd\u529b\u5c42\u7ea7\uff1a\u5de5\u5177\u4f7f\u7528\u3001\u89c4\u5212\u4e0e\u76ee\u6807\u5f62\u6210\u3001\u9002\u5e94\u6027\u3001\u63a5\u5730\u6027\u548c\u5e38\u8bc6\u63a8\u7406\u3002\u5373\u4f7f\u6700\u4f73\u6a21\u578b\u4e5f\u6709\u7ea640%\u4efb\u52a1\u5931\u8d25\uff0c\u5931\u8d25\u6a21\u5f0f\u6cbf\u80fd\u529b\u5c42\u7ea7\u53ef\u9884\u6d4b\u5206\u5e03\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u53d1\u5c55\uff0cAI\u8bc4\u4f30\u9700\u8981\u4ece\u5355\u8f6e\u54cd\u5e94\u8bc4\u4f30\u8f6c\u5411\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u591a\u6b65\u9aa4\u4efb\u52a1\u5b8c\u6210\u8bc4\u4f30\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u524d\u6cbfAI\u6a21\u578b\u5728\u771f\u5b9e\u7535\u5546RL\u73af\u5883\u4e2d\u7684\u804c\u573a\u4efb\u52a1\u8868\u73b0\uff0c\u8bc6\u522b\u80fd\u529b\u5dee\u8ddd\u3002", "method": "\u5728Surge\u63d0\u4f9b\u7684\u771f\u5b9e\u7535\u5546RL\u73af\u5883\u4e2d\uff0c\u5bf9\u524d\u6cbfAI\u6a21\u578b\u8fdb\u884c150\u4e2a\u804c\u573a\u4efb\u52a1\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002\u91c7\u7528\u4efb\u52a1\u4e2d\u5fc3\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u591a\u6837\u6027\u548c\u9886\u57df\u4e13\u5bb6\u8d21\u732e\uff0c\u8fdb\u884c\u8be6\u7ec6\u5931\u8d25\u5206\u6790\u3002", "result": "\u53d1\u73b0\u7ecf\u9a8c\u63a8\u5bfc\u7684\u667a\u80fd\u4f53\u80fd\u529b\u5c42\u7ea7\uff1a\u5de5\u5177\u4f7f\u7528\u3001\u89c4\u5212\u4e0e\u76ee\u6807\u5f62\u6210\u3001\u9002\u5e94\u6027\u3001\u63a5\u5730\u6027\u548c\u5e38\u8bc6\u63a8\u7406\u3002\u6700\u4f73\u6a21\u578b\u4ecd\u6709\u7ea640%\u4efb\u52a1\u5931\u8d25\uff0c\u8f83\u5f31\u6a21\u578b\u5728\u57fa\u7840\u5de5\u5177\u4f7f\u7528\u548c\u89c4\u5212\u4e0a\u6323\u624e\uff0c\u8f83\u5f3a\u6a21\u578b\u4e3b\u8981\u5728\u9700\u8981\u8d85\u51fa\u660e\u786e\u6307\u4ee4\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e0a\u5931\u8d25\u3002", "conclusion": "\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u80fd\u5c55\u793a\u8fde\u8d2f\u7684\u591a\u6b65\u9aa4\u884c\u4e3a\uff0c\u4f46\u5728\u771f\u5b9e\u804c\u573a\u73af\u5883\u4e2d\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u4efb\u52a1\u5b8c\u6210\u4ecd\u6709\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u3002\u5931\u8d25\u6a21\u5f0f\u6cbf\u80fd\u529b\u5c42\u7ea7\u53ef\u9884\u6d4b\u5206\u5e03\uff0c\u4e3a\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.08998", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08998", "abs": "https://arxiv.org/abs/2601.08998", "authors": ["Alexander Berndt", "Thomas Bach", "Rainer Gemulla", "Marcus Kessel", "Sebastian Baltes"], "title": "On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems", "comment": "12 pages, 5 tables, 3 figures, accepted at the 48th International Conference on Software Engineering: Software Engineering in Practice (ICSE SEIP 2026)", "summary": "Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed (\"unordered collection\"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u6570\u636e\u5e93\u6d4b\u8bd5\u4e2d\uff0c\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u6bd4\u4f8b\u7565\u9ad8\u4e8e\u73b0\u6709\u6d4b\u8bd5\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4f9d\u8d56\u672a\u4fdd\u8bc1\u7684\u987a\u5e8f\uff08\u65e0\u5e8f\u96c6\u5408\uff09\uff0c\u4e14LLM\u4f1a\u901a\u8fc7\u63d0\u793a\u4e0a\u4e0b\u6587\u5c06\u73b0\u6709\u6d4b\u8bd5\u7684\u4e0d\u7a33\u5b9a\u6027\u4f20\u9012\u5230\u65b0\u751f\u6210\u7684\u6d4b\u8bd5\u4e2d\u3002", "motivation": "LLM\u751f\u6210\u7684\u6d4b\u8bd5\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4f46\u5176\u666e\u904d\u6027\u548c\u6839\u672c\u539f\u56e0\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u5206\u6790LLM\u751f\u6210\u7684\u6570\u636e\u5e93\u6d4b\u8bd5\u4e2d\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u7684\u666e\u904d\u6027\u3001\u6839\u672c\u539f\u56e0\u4ee5\u53ca\u4e0d\u7a33\u5b9a\u6027\u4ece\u73b0\u6709\u6d4b\u8bd5\u5230\u751f\u6210\u6d4b\u8bd5\u7684\u4f20\u9012\u73b0\u8c61\u3002", "method": "\u7814\u7a76\u4f7f\u7528GPT-4o\u548cMistral-Large-Instruct-2407\u4e24\u4e2aLLM\uff0c\u5728\u56db\u4e2a\u5173\u7cfb\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\uff08SAP HANA\u3001DuckDB\u3001MySQL\u3001SQLite\uff09\u4e2d\u6269\u589e\u6d4b\u8bd5\u5957\u4ef6\uff0c\u8bc4\u4f30\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u624b\u52a8\u68c0\u67e5\u5206\u6790\u6839\u672c\u539f\u56e0\u3002", "result": "\u751f\u6210\u6d4b\u8bd5\u7684\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u6bd4\u4f8b\u7565\u9ad8\u4e8e\u73b0\u6709\u6d4b\u8bd5\u3002\u624b\u52a8\u68c0\u67e5\u53d1\u73b0\uff0c72/115\u4e2a\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff0863%\uff09\u7684\u6839\u672c\u539f\u56e0\u662f\u6d4b\u8bd5\u4f9d\u8d56\u672a\u4fdd\u8bc1\u7684\u987a\u5e8f\uff08\u65e0\u5e8f\u96c6\u5408\uff09\u3002LLM\u901a\u8fc7\u63d0\u793a\u4e0a\u4e0b\u6587\u5c06\u73b0\u6709\u6d4b\u8bd5\u7684\u4e0d\u7a33\u5b9a\u6027\u4f20\u9012\u5230\u65b0\u751f\u6210\u7684\u6d4b\u8bd5\u4e2d\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u95ed\u6e90\u7cfb\u7edf\uff08\u5982SAP HANA\uff09\u4e2d\u6bd4\u5f00\u6e90\u7cfb\u7edf\u66f4\u666e\u904d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u751f\u6210\u6d4b\u8bd5\u4e2d\u4e0d\u7a33\u5b9a\u6027\u7684\u4e3b\u8981\u7c7b\u578b\u548c\u4f20\u9012\u673a\u5236\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u9884\u671f\u6307\u5bfc\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728LLM\u6d4b\u8bd5\u751f\u6210\u4e2d\u63d0\u4f9b\u5b9a\u5236\u5316\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\u3002", "topic": "swe application"}}
{"id": "2601.08839", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08839", "abs": "https://arxiv.org/abs/2601.08839", "authors": ["Toshiyuki Shigemura"], "title": "Recursive Knowledge Synthesis for Multi-LLM Systems: Stability Analysis and Tri-Agent Audit Framework", "comment": "25 pages, 9 figures. Pilot feasibility study using public-access large language models without API-level orchestration", "summary": "This paper presents a tri-agent cross-validation framework for analyzing stability and explainability in multi-model large language systems. The architecture integrates three heterogeneous LLMs-used for semantic generation, analytical consistency checking, and transparency auditing-into a recursive interaction cycle. This design induces Recursive Knowledge Synthesis (RKS), where intermediate representations are continuously refined through mutually constraining transformations irreducible to single-model behavior. Across 47 controlled trials using public-access LLM deployments (October 2025), we evaluated system stability via four metrics: Reflex Reliability Score (RRS), Transparency Score (TS), Deviation Detection Rate (DDR), and Correction Success Rate (CSR). The system achieved mean RRS = 0.78+-0.06 and maintained TS >= 0.8 in about 68% of trials. Approximately 89% of trials converged, supporting the theoretical prediction that transparency auditing acts as a contraction operator within the composite validation mapping. The contributions are threefold: (1) a structured tri-agent framework for coordinated reasoning across heterogeneous LLMs, (2) a formal RKS model grounded in fixed-point theory, and (3) empirical evaluation of inter-model stability under realistic, non-API public-access conditions. These results provide initial empirical evidence that a safety-preserving, humansupervised multi-LLM architecture can achieve stable recursive knowledge synthesis in realistic, publicly deployed environments.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u667a\u80fd\u4f53\u4ea4\u53c9\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784LLM\u7684\u9012\u5f52\u4ea4\u4e92\u5b9e\u73b0\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u578b\u77e5\u8bc6\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u578b\u5927\u8bed\u8a00\u7cfb\u7edf\u4e2d\u7a33\u5b9a\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5f02\u6784LLM\u7684\u534f\u540c\u63a8\u7406\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u77e5\u8bc6\u5408\u6210\u3002", "method": "\u91c7\u7528\u4e09\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u8bed\u4e49\u751f\u6210\u3001\u5206\u6790\u4e00\u81f4\u6027\u68c0\u67e5\u3001\u900f\u660e\u5ea6\u5ba1\u8ba1\uff0c\u901a\u8fc7\u9012\u5f52\u4ea4\u4e92\u5b9e\u73b0\u77e5\u8bc6\u5408\u6210\uff0c\u57fa\u4e8e\u4e0d\u52a8\u70b9\u7406\u8bba\u5efa\u7acb\u5f62\u5f0f\u5316\u6a21\u578b\u3002", "result": "\u572847\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747RRS=0.78\u00b10.06\uff0c68%\u5b9e\u9a8cTS\u22650.8\uff0c89%\u5b9e\u9a8c\u6536\u655b\uff0c\u9a8c\u8bc1\u4e86\u900f\u660e\u5ea6\u5ba1\u8ba1\u4f5c\u4e3a\u6536\u7f29\u7b97\u5b50\u7684\u7406\u8bba\u9884\u6d4b\u3002", "conclusion": "\u4e09\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u5728\u73b0\u5b9e\u516c\u5f00\u90e8\u7f72\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u7684\u9012\u5f52\u77e5\u8bc6\u5408\u6210\uff0c\u4e3a\u4eba\u7c7b\u76d1\u7763\u7684\u591aLLM\u67b6\u6784\u63d0\u4f9b\u5b9e\u8bc1\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "2601.09097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09097", "abs": "https://arxiv.org/abs/2601.09097", "authors": ["Derrick Goh Xin Deik", "Quanyu Long", "Zhengyuan Liu", "Nancy F. Chen", "Wenya Wang"], "title": "Programming over Thinking: Efficient and Robust Multi-Constraint Planning", "comment": "8 pages of main text, 2 pages of references and and limitations, 37 pages of appendices", "summary": "Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.", "AI": {"tldr": "SCOPE\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u63a8\u7406\u4e0e\u4ee3\u7801\u6267\u884c\u6765\u89e3\u51b3\u591a\u7ea6\u675f\u89c4\u5212\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709LLM\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u591a\u7ea6\u675f\u89c4\u5212\u4e2d\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff1a\u7eaf\u63a8\u7406\u65b9\u6cd5\u5bb9\u6613\u4ea7\u751f\u4e0d\u4e00\u81f4\u6027\u548c\u9519\u8bef\u7d2f\u79ef\uff0c\u6210\u672c\u9ad8\u6602\uff1b\u800c\u7ed3\u5408\u7f16\u7801\u6216\u6c42\u89e3\u5668\u7684\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u8de8\u95ee\u9898\u7684\u901a\u7528\u903b\u8f91\u3002", "method": "\u63d0\u51faSCOPE\u6846\u67b6\uff0c\u5c06\u67e5\u8be2\u7279\u5b9a\u63a8\u7406\u4e0e\u901a\u7528\u4ee3\u7801\u6267\u884c\u89e3\u8026\uff0c\u751f\u6210\u4e00\u81f4\u3001\u786e\u5b9a\u4e14\u53ef\u91cd\u7528\u7684\u6c42\u89e3\u5668\u51fd\u6570\uff0c\u4ec5\u9700\u5bf9\u8f93\u5165\u53c2\u6570\u8fdb\u884c\u6700\u5c0f\u66f4\u6539\u3002", "result": "\u5728TravelPlanner\u4efb\u52a1\u4e0a\u8fbe\u523093.1%\u7684\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\uff08CoT\uff09\u63d0\u534761.6%\uff0c\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c1.4\u500d\uff0c\u65f6\u95f4\u51cf\u5c11\u7ea64.67\u500d\u3002", "conclusion": "SCOPE\u901a\u8fc7\u5206\u79bb\u63a8\u7406\u4e0e\u6267\u884c\uff0c\u5728\u591a\u7ea6\u675f\u89c4\u5212\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "topic": "code agent"}}
{"id": "2601.09393", "categories": ["cs.SE", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.09393", "abs": "https://arxiv.org/abs/2601.09393", "authors": ["Zirui Wang", "Guangba Yu", "Michael R. Lyu"], "title": "AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems", "comment": null, "summary": "The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.", "AI": {"tldr": "AI-NativeBench\uff1a\u9996\u4e2a\u57fa\u4e8eMCP\u548cA2A\u6807\u51c6\u7684\u5e94\u7528\u4e2d\u5fc3\u5316\u767d\u76d2AI\u539f\u751f\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8ffd\u8e2a\u4e2d\u7684\u667a\u80fd\u4f53\u8de8\u5ea6\u5206\u6790\uff0c\u63ed\u793a\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u6355\u6349\u7684\u5de5\u7a0b\u73b0\u5b9e\u3002", "motivation": "\u4ece\u4e91\u539f\u751f\u5411AI\u539f\u751f\u67b6\u6784\u7684\u8f6c\u53d8\u6b63\u5728\u91cd\u5851\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u4f46\u4f20\u7edf\u7684\u9ed1\u76d2\u8bc4\u4f30\u8303\u5f0f\u5df2\u4e0d\u9002\u7528\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u8861\u91cf\u539f\u59cb\u6a21\u578b\u80fd\u529b\u800c\u5ffd\u89c6\u4e86\u7cfb\u7edf\u7ea7\u6267\u884c\u52a8\u6001\u3002", "method": "\u5f15\u5165AI-NativeBench\u57fa\u51c6\u5957\u4ef6\uff0c\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u548c\u667a\u80fd\u4f53\u5230\u667a\u80fd\u4f53(A2A)\u6807\u51c6\uff0c\u5c06\u667a\u80fd\u4f53\u8de8\u5ea6\u4f5c\u4e3a\u5206\u5e03\u5f0f\u8ffd\u8e2a\u4e2d\u7684\u4e00\u7b49\u516c\u6c11\uff0c\u5b9e\u73b0\u8d85\u8d8a\u7b80\u5355\u80fd\u529b\u7684\u5de5\u7a0b\u7279\u6027\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "result": "\u572821\u4e2a\u7cfb\u7edf\u53d8\u4f53\u4e0a\u6d4b\u8bd5\u53d1\u73b0\uff1a\u53c2\u6570\u6096\u8bba\uff08\u8f7b\u91cf\u6a21\u578b\u5728\u534f\u8bae\u9075\u5faa\u4e0a\u5e38\u4f18\u4e8e\u65d7\u8230\u6a21\u578b\uff09\u3001\u666e\u904d\u5b58\u5728\u7684\u63a8\u7406\u4e3b\u5bfc\uff08\u4f7f\u534f\u8bae\u5f00\u9500\u6b21\u8981\uff09\u3001\u6602\u8d35\u5931\u8d25\u6a21\u5f0f\uff08\u81ea\u6108\u673a\u5236\u5728\u4e0d\u53ef\u884c\u5de5\u4f5c\u6d41\u4e0a\u6210\u4e3a\u6210\u672c\u500d\u589e\u5668\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ece\u8861\u91cf\u6a21\u578b\u80fd\u529b\u8f6c\u5411\u5de5\u7a0b\u5316\u53ef\u9760AI\u539f\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u8bc1\u636e\uff0c\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "topic": "swe benchmark"}}
{"id": "2601.09440", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09440", "abs": "https://arxiv.org/abs/2601.09440", "authors": ["Yi Gao", "Xing Hu", "Tongtong Xu", "Jiali Zhao", "Xiaohu Yang", "Xin Xia"], "title": "DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries", "comment": "Published in 48th International Conference on Software Engineering (ICSE 2026)", "summary": "Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.", "AI": {"tldr": "DepRadar\u662f\u4e00\u4e2a\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5e93\u7f3a\u9677\u5f71\u54cd\u5206\u6790\u7684\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4ecePR/\u63d0\u4ea4\u4e2d\u63d0\u53d6\u7f3a\u9677\u8bed\u4e49\uff0c\u5206\u6790\u89e6\u53d1\u6761\u4ef6\uff0c\u5e76\u68c0\u67e5\u4e0b\u6e38\u7a0b\u5e8f\u662f\u5426\u53d7\u5f71\u54cd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5e93\uff08\u5982Transformers\u3001Megatron\uff09\u7684\u7f3a\u9677\uff08\u4ece\u9759\u9ed8\u8ba1\u7b97\u9519\u8bef\u5230\u6027\u80fd\u56de\u5f52\uff09\u96be\u4ee5\u88ab\u4e0b\u6e38\u7528\u6237\u68c0\u6d4b\uff0c\u56e0\u4e3a\u9700\u8981\u7406\u89e3\u7f3a\u9677\u8bed\u4e49\u5e76\u68c0\u67e5\u590d\u6742\u7684\u89e6\u53d1\u6761\u4ef6\uff08\u914d\u7f6e\u6807\u5fd7\u3001\u8fd0\u884c\u65f6\u73af\u5883\u3001\u95f4\u63a5API\u4f7f\u7528\uff09\u3002", "method": "DepRadar\u534f\u8c03\u56db\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u5206\u4e09\u6b65\u5de5\u4f5c\uff1a1) PR\u6316\u6398\u5668\u548c\u4ee3\u7801\u5dee\u5f02\u5206\u6790\u5668\u4ece\u63d0\u4ea4/PR\u63d0\u53d6\u7ed3\u6784\u5316\u7f3a\u9677\u8bed\u4e49\uff1b2) \u7f16\u6392\u667a\u80fd\u4f53\u5408\u6210\u7edf\u4e00\u7f3a\u9677\u6a21\u5f0f\u548c\u89e6\u53d1\u6761\u4ef6\uff1b3) \u5f71\u54cd\u5206\u6790\u5668\u68c0\u67e5\u4e0b\u6e38\u7a0b\u5e8f\u662f\u5426\u89e6\u53d1\u7f3a\u9677\u3002\u7ed3\u5408\u9759\u6001\u5206\u6790\u548cDL\u7279\u5b9a\u9886\u57df\u89c4\u5219\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u8868\u6027DL\u5e93\u7684157\u4e2aPR\u548c70\u4e2a\u63d0\u4ea4\u4e0a\u8bc4\u4f30\uff0c\u7f3a\u9677\u8bc6\u522b\u7cbe\u5ea6\u8fbe90%\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u5b57\u6bb5\uff08\u5e73\u5747\u5b57\u6bb5\u5f97\u52061.6\uff09\u3002\u5728122\u4e2a\u5ba2\u6237\u7aef\u7a0b\u5e8f\u4e0a\uff0c\u8bc6\u522b\u53d7\u5f71\u54cd\u6848\u4f8b\u7684\u53ec\u56de\u738790%\uff0c\u7cbe\u5ea680%\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DepRadar\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86DL\u5e93\u7f3a\u9677\u5f71\u54cd\u5206\u6790\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u7f3a\u9677\u8bc6\u522b\u548c\u5f71\u54cd\u5206\u6790\uff0c\u4e3a\u4e0b\u6e38\u7528\u6237\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "2601.09113", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09113", "abs": "https://arxiv.org/abs/2601.09113", "authors": ["Zixia Jia", "Jiaqi Li", "Yipeng Kang", "Yuxuan Wang", "Tong Wu", "Quansen Wang", "Xiaobo Wang", "Shuyi Zhang", "Junzhe Shen", "Qing Li", "Siyuan Qi", "Yitao Liang", "Di He", "Zilong Zheng", "Song-Chun Zhu"], "title": "The AI Hippocampus: How Far are We From Human Memory?", "comment": null, "summary": "Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9LLMs\u548cMLLMs\u4e2d\u7684\u8bb0\u5fc6\u673a\u5236\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u5305\u542b\u9690\u5f0f\u3001\u663e\u5f0f\u548c\u4ee3\u7406\u8bb0\u5fc6\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u591a\u6a21\u6001\u8bb0\u5fc6\u96c6\u6210\u53ca\u5f53\u524d\u6311\u6218\u3002", "motivation": "\u968f\u7740LLMs\u548cMLLMs\u4ece\u9759\u6001\u9884\u6d4b\u5668\u8f6c\u53d8\u4e3a\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u63a8\u7406\u7684\u4ea4\u4e92\u7cfb\u7edf\uff0c\u8bb0\u5fc6\u673a\u5236\u5df2\u6210\u4e3a\u5176\u67b6\u6784\u548c\u529f\u80fd\u6f14\u5316\u7684\u6838\u5fc3\u4e3b\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6574\u7406\u548c\u5206\u6790\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c06\u8bb0\u5fc6\u673a\u5236\u7ec4\u7ec7\u6210\u5305\u542b\u9690\u5f0f\u8bb0\u5fc6\u3001\u663e\u5f0f\u8bb0\u5fc6\u548c\u4ee3\u7406\u8bb0\u5fc6\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u8bb0\u5fc6\u96c6\u6210\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u8bb0\u5fc6\u5206\u7c7b\u6846\u67b6\uff0c\u6db5\u76d6\u4e86\u4ece\u6a21\u578b\u53c2\u6570\u4e2d\u7684\u9690\u5f0f\u77e5\u8bc6\u5230\u5916\u90e8\u5b58\u50a8\u7cfb\u7edf\u548c\u4ee3\u7406\u6301\u4e45\u8bb0\u5fc6\u7684\u5404\u79cd\u8bb0\u5fc6\u8303\u5f0f\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u67b6\u6784\u8fdb\u5c55\u548c\u57fa\u51c6\u4efb\u52a1\u3002", "conclusion": "\u8bb0\u5fc6\u673a\u5236\u5bf9\u589e\u5f3aLLMs\u548cMLLMs\u7684\u63a8\u7406\u3001\u9002\u5e94\u6027\u548c\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ecd\u9762\u4e34\u5bb9\u91cf\u3001\u5bf9\u9f50\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u8de8\u7cfb\u7edf\u4e92\u64cd\u4f5c\u6027\u7b49\u5f00\u653e\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2601.09612", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09612", "abs": "https://arxiv.org/abs/2601.09612", "authors": ["Khairul Alam", "Banani Roy"], "title": "Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories", "comment": "12 pages", "summary": "Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $\u03b4$ = 0.94) and code snippets (medium effect, $\u03b4$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.", "AI": {"tldr": "\u5bf9nf-core\u79d1\u5b66\u5de5\u4f5c\u6d41\u7ba1\u9053\u7684\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b013\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u7ba1\u9053\u5f00\u53d1\u96c6\u6210\u3001bug\u4fee\u590d\u3001\u57fa\u56e0\u7ec4\u6570\u636e\u6574\u5408\u7b49\uff0c89.38%\u7684\u95ee\u9898\u80fd\u89e3\u51b3\uff0c\u6807\u7b7e\u548c\u4ee3\u7801\u7247\u6bb5\u663e\u8457\u63d0\u5347\u89e3\u51b3\u6982\u7387\u3002", "motivation": "\u5c3d\u7ba1Nextflow\u548cnf-core\u793e\u533a\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u7ba1\u7406\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7528\u6237\u5728\u8fd9\u4e9b\u7ba1\u9053\u5f00\u53d1\u548c\u7ef4\u62a4\u4e2d\u9762\u4e34\u7684\u5177\u4f53\u6311\u6218\u7f3a\u4e4f\u7cfb\u7edf\u6027\u4e86\u89e3\uff0c\u9700\u8981\u5b9e\u8bc1\u7814\u7a76\u6765\u63ed\u793a\u5b9e\u9645\u56f0\u96be\u548c\u6539\u8fdb\u673a\u4f1a\u3002", "method": "\u5bf925,173\u4e2a\u95ee\u9898\u548c\u62c9\u53d6\u8bf7\u6c42\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u4f7f\u7528BERTopic\u4e3b\u9898\u5efa\u6a21\u8bc6\u522b\u5173\u952e\u6311\u6218\uff0c\u7edf\u8ba1\u5206\u6790\u89e3\u51b3\u52a8\u6001\uff0c\u8bc4\u4f30\u6807\u7b7e\u548c\u4ee3\u7801\u7247\u6bb5\u5bf9\u95ee\u9898\u89e3\u51b3\u7684\u5f71\u54cd\u3002", "result": "\u8bc6\u522b\u51fa13\u4e2a\u5173\u952e\u6311\u6218\uff0c89.38%\u7684\u95ee\u9898\u6700\u7ec8\u5173\u95ed\uff0c\u534a\u6570\u57283\u5929\u5185\u89e3\u51b3\uff1b\u6807\u7b7e\uff08\u5927\u6548\u5e94\uff0c\u03b4=0.94\uff09\u548c\u4ee3\u7801\u7247\u6bb5\uff08\u4e2d\u6548\u5e94\uff0c\u03b4=0.50\uff09\u663e\u8457\u63d0\u9ad8\u89e3\u51b3\u53ef\u80fd\u6027\uff1b\u5de5\u5177\u5f00\u53d1\u548c\u4ed3\u5e93\u7ef4\u62a4\u662f\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86nf-core\u7ba1\u9053\u534f\u4f5c\u5f00\u53d1\u548c\u7ef4\u62a4\u7684\u5b9e\u9645\u6311\u6218\uff0c\u4e3a\u63d0\u5347\u53ef\u7528\u6027\u3001\u53ef\u6301\u7eed\u6027\u548c\u53ef\u91cd\u590d\u6027\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u6807\u7b7e\u7cfb\u7edf\u548c\u4ee3\u7801\u793a\u4f8b\u7684\u91cd\u8981\u6027\u3002", "topic": "swe application"}}
{"id": "2601.09152", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09152", "abs": "https://arxiv.org/abs/2601.09152", "authors": ["Yiwen Tu", "Xuan Liu", "Lianhui Qin", "Haojian Jin"], "title": "PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?", "comment": null, "summary": "This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's \"privacy mind\", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \\PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.", "AI": {"tldr": "PRA\u662f\u4e00\u4e2aAI\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u7528\u4e8e\u6a21\u62df\u4e2a\u4f53\u7528\u6237\u5982\u4f55\u6839\u636e\u73b0\u5b9e\u4e16\u754c\u65b0\u95fb\u5f62\u6210\u9690\u79c1\u62c5\u5fe7\u3002\u5b83\u8d85\u8d8a\u4e86\u7fa4\u4f53\u5c42\u9762\u7684\u60c5\u611f\u5206\u6790\uff0c\u6574\u5408\u9690\u79c1\u548c\u8ba4\u77e5\u7406\u8bba\uff0c\u57fa\u4e8e\u4e2a\u4eba\u8bc4\u8bba\u5386\u53f2\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u6a21\u62df\u7528\u6237\u7279\u5b9a\u7684\u9690\u79c1\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7fa4\u4f53\u5c42\u9762\u7684\u9690\u79c1\u60c5\u611f\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u4e2a\u4f53\u7528\u6237\u9690\u79c1\u62c5\u5fe7\u5f62\u6210\u8fc7\u7a0b\u7684\u6a21\u62df\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u4e2a\u4eba\u9690\u79c1\u63a8\u7406\u6a21\u5f0f\u3001\u8003\u8651\u4e2a\u4eba\u5386\u53f2\u548c\u4e0a\u4e0b\u6587\u56e0\u7d20\u7684AI\u667a\u80fd\u4f53\u3002", "method": "PRA\u6574\u5408\u9690\u79c1\u548c\u8ba4\u77e5\u7406\u8bba\uff0c\u91cd\u5efa\u7528\u6237\u7684\"\u9690\u79c1\u601d\u7ef4\"\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u5668\u52a8\u6001\u6fc0\u6d3b\u76f8\u5173\u9690\u79c1\u8bb0\u5fc6\uff08\u6a21\u62df\u6709\u9650\u7406\u6027\uff09\uff0c\u751f\u6210\u53cd\u6620\u7528\u6237\u5bf9\u65b0\u9690\u79c1\u573a\u666f\u53ef\u80fd\u53cd\u5e94\u7684\u5408\u6210\u8bc4\u8bba\u3002\u4f7f\u7528\u57fa\u4e8e\u65e2\u5b9a\u9690\u79c1\u62c5\u5fe7\u5206\u7c7b\u6cd5\u6821\u51c6\u7684LLM-as-a-Judge\u8bc4\u4f30\u5668\u91cf\u5316\u751f\u6210\u63a8\u7406\u7684\u5fe0\u5b9e\u5ea6\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684Hacker News\u8ba8\u8bba\u5b9e\u9a8c\u4e2d\uff0cPRA\u5728\u9690\u79c1\u62c5\u5fe7\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u667a\u80fd\u4f53\uff0c\u5e76\u80fd\u6355\u6349\u8de8\u9886\u57df\uff08\u5305\u62ecAI\u3001\u7535\u5b50\u5546\u52a1\u548c\u533b\u7597\u4fdd\u5065\uff09\u7684\u53ef\u8fc1\u79fb\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "PRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u62df\u4e2a\u4f53\u7528\u6237\u9690\u79c1\u62c5\u5fe7\u5f62\u6210\u8fc7\u7a0b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u5fe0\u5b9e\u4e8e\u7528\u6237\u7279\u5b9a\u63a8\u7406\u6a21\u5f0f\u7684\u5408\u6210\u8bc4\u8bba\uff0c\u5728\u8de8\u9886\u57df\u9690\u79c1\u62c5\u5fe7\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2601.09695", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09695", "abs": "https://arxiv.org/abs/2601.09695", "authors": ["Michael Konstantinou", "Renzo Degiovanni", "Mike Papadakis"], "title": "How well LLM-based test generation techniques perform with newer LLM versions?", "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6700\u65b0LLM\u7248\u672c\u4e0b\uff0c\u7b80\u5355\u7684LLM\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u5728\u4ee3\u7801\u8986\u76d6\u7387\u3001\u5206\u652f\u8986\u76d6\u7387\u548c\u53d8\u5f02\u5f97\u5206\u7b49\u6240\u6709\u6d4b\u8bd5\u6709\u6548\u6027\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u56db\u79cd\u6700\u5148\u8fdb\u5de5\u5177\uff0c\u4e14\u6210\u672c\u76f8\u5f53\u3002\u5efa\u8bae\u91c7\u7528\u5148\u7c7b\u540e\u65b9\u6cd5\u7684\u7b56\u7565\u53ef\u51cf\u5c11\u7ea620%\u7684LLM\u8bf7\u6c42\u3002", "motivation": "\u73b0\u6709LLM\u6d4b\u8bd5\u751f\u6210\u6280\u672f\u901a\u5e38\u4e0e\u8f83\u5f31\u7684\u57fa\u7ebf\uff08\u65e7\u7248LLM\u548c\u7b80\u5355\u63d0\u793a\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5938\u5927\u8fd9\u4e9b\u6280\u672f\u7684\u6027\u80fd\u8d21\u732e\u3002\u968f\u7740LLM\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u66f4\u5f3a\u7684LLM\u53ef\u80fd\u4f7f\u8fd9\u4e9b\u590d\u6742\u6280\u672f\u5931\u53bb\u4f18\u52bf\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u3002", "method": "\u590d\u5236\u4e86\u56db\u79cd\u6700\u5148\u8fdb\u7684LLM\u6d4b\u8bd5\u751f\u6210\u5de5\u5177\uff08HITS\u3001SymPrompt\u3001TestSpark\u3001CoverUp\uff09\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e\u7b80\u5355\u7684LLM\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u5728\u6240\u6709\u65b9\u6cd5\u4e2d\u96c6\u6210\u4e86\u5f53\u524dLLM\u7248\u672c\uff0c\u5e76\u5728393\u4e2a\u7c7b\u548c3,657\u4e2a\u65b9\u6cd5\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u8fd8\u63d0\u51fa\u4e86\u5148\u9488\u5bf9\u7a0b\u5e8f\u7c7b\uff08\u6d4b\u8bd5\u751f\u6210\u66f4\u9ad8\u6548\uff09\u518d\u9488\u5bf9\u672a\u8986\u76d6\u65b9\u6cd5\u7684\u7b56\u7565\u3002", "result": "\u7b80\u5355\u7684LLM\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u6709\u6548\u6027\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff1a\u884c\u8986\u76d6\u7387\u63d0\u9ad817.72%\uff0c\u5206\u652f\u8986\u76d6\u7387\u63d0\u9ad819.80%\uff0c\u53d8\u5f02\u5f97\u5206\u63d0\u9ad820.92%\uff0c\u4e14\u6210\u672c\uff08LLM\u67e5\u8be2\u6b21\u6570\uff09\u76f8\u5f53\u3002\u5148\u7c7b\u540e\u65b9\u6cd5\u7684\u7b56\u7565\u5728\u4fdd\u6301\u76f8\u5f53\uff08\u7565\u9ad8\uff09\u6709\u6548\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u7ea620%\u7684LLM\u8bf7\u6c42\u3002", "conclusion": "\u968f\u7740LLM\u80fd\u529b\u7684\u63d0\u5347\uff0c\u590d\u6742\u7684\u6d4b\u8bd5\u751f\u6210\u5de5\u7a0b\u6280\u672f\u53ef\u80fd\u4e0d\u518d\u5fc5\u8981\uff0c\u7b80\u5355\u7684LLM\u65b9\u6cd5\u5df2\u7ecf\u8db3\u591f\u6709\u6548\u3002\u91c7\u7528\u9002\u5f53\u7684\u751f\u6210\u7c92\u5ea6\u7b56\u7565\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u6210\u672c\u6548\u76ca\u3002", "topic": "swe application"}}
{"id": "2601.08846", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08846", "abs": "https://arxiv.org/abs/2601.08846", "authors": ["Cagatay Tekin", "Charbel Barakat", "Luis Joseph Luna Limgenco"], "title": "Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning", "comment": "6 pages, 2 figures. Code available at: github.com/cagopat/InftyThink-with-Cross-Chain-Memory", "summary": "Iterative summarization based reasoning frameworks such as InftyThink enable long-horizon reasoning in large language models (LLMs) by controlling context growth, but they repeatedly regenerate similar reasoning strategies across tasks. We introduce InftyThink with Cross-Chain Memory, an extension that augments iterative reasoning with an embedding-based semantic cache of previously successful reasoning patterns. At each reasoning step, the model retrieves and conditions on the most semantically similar stored lemmas, guiding inference without expanding the context window indiscriminately. Experiments on MATH500, AIME2024, and GPQA-Diamond demonstrate that semantic lemma retrieval improves accuracy in structured domains while exposing failure modes in tests that include heterogeneous domains. Geometric analyses of reasoning trajectories reveal that cache retrieval induces directional biases in embedding space, leading to consistent fix (improve baseline accuracy) and break (degradation in baseline accuracy) attractors. Our results highlight both the benefits and limits of similarity-based memory for self-improving LLM reasoning.", "AI": {"tldr": "InftyThink with Cross-Chain Memory\u901a\u8fc7\u5d4c\u5165\u8bed\u4e49\u7f13\u5b58\u6539\u8fdb\u8fed\u4ee3\u63a8\u7406\u6846\u67b6\uff0c\u5728\u7ed3\u6784\u5316\u6570\u5b66\u4efb\u52a1\u4e2d\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u5728\u5f02\u6784\u9886\u57df\u6d4b\u8bd5\u4e2d\u66b4\u9732\u5c40\u9650\u6027\uff0c\u63ed\u793a\u4e86\u57fa\u4e8e\u76f8\u4f3c\u6027\u8bb0\u5fc6\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3\u63a8\u7406\u6846\u67b6\uff08\u5982InftyThink\uff09\u5728\u957f\u7a0b\u63a8\u7406\u4e2d\u80fd\u63a7\u5236\u4e0a\u4e0b\u6587\u589e\u957f\uff0c\u4f46\u4f1a\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u91cd\u590d\u751f\u6210\u76f8\u4f3c\u7684\u63a8\u7406\u7b56\u7565\uff0c\u7f3a\u4e4f\u5bf9\u5148\u524d\u6210\u529f\u63a8\u7406\u6a21\u5f0f\u7684\u590d\u7528\u3002", "method": "\u6269\u5c55InftyThink\u6846\u67b6\uff0c\u52a0\u5165\u57fa\u4e8e\u5d4c\u5165\u7684\u8bed\u4e49\u7f13\u5b58\uff0c\u5b58\u50a8\u5148\u524d\u6210\u529f\u7684\u63a8\u7406\u6a21\u5f0f\uff08\u5f15\u7406\uff09\u3002\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u4e2d\uff0c\u6a21\u578b\u68c0\u7d22\u8bed\u4e49\u6700\u76f8\u4f3c\u7684\u5b58\u50a8\u5f15\u7406\uff0c\u4ee5\u6b64\u6307\u5bfc\u63a8\u7406\u800c\u4e0d\u76f2\u76ee\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "result": "\u5728MATH500\u3001AIME2024\u548cGPQA-Diamond\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u8bed\u4e49\u5f15\u7406\u68c0\u7d22\u5728\u7ed3\u6784\u5316\u9886\u57df\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u5728\u5305\u542b\u5f02\u6784\u9886\u57df\u7684\u6d4b\u8bd5\u4e2d\u66b4\u9732\u4e86\u5931\u8d25\u6a21\u5f0f\u3002\u51e0\u4f55\u5206\u6790\u663e\u793a\u7f13\u5b58\u68c0\u7d22\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bf1\u5bfc\u65b9\u5411\u6027\u504f\u5dee\uff0c\u5f62\u6210\u4e00\u81f4\u7684\"\u4fee\u590d\"\uff08\u63d0\u5347\u57fa\u7ebf\u51c6\u786e\u6027\uff09\u548c\"\u7834\u574f\"\uff08\u964d\u4f4e\u57fa\u7ebf\u51c6\u786e\u6027\uff09\u5438\u5f15\u5b50\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u57fa\u4e8e\u76f8\u4f3c\u6027\u8bb0\u5fc6\u5bf9\u81ea\u6539\u8fdbLLM\u63a8\u7406\u7684\u76ca\u5904\u548c\u5c40\u9650\u6027\uff0c\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u9886\u57df\u6709\u6548\uff0c\u4f46\u5728\u5904\u7406\u5f02\u6784\u4efb\u52a1\u65f6\u9700\u8981\u66f4\u8c28\u614e\u7684\u8bbe\u8ba1\u3002", "topic": "agent analysis"}}
{"id": "2601.09259", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09259", "abs": "https://arxiv.org/abs/2601.09259", "authors": ["Jian Zhang", "Zhiyuan Wang", "Zhangqi Wang", "Yu He", "Haoran Luo", "li yuan", "Lingling Zhang", "Rui Mao", "Qika Lin", "Jun Liu"], "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "comment": null, "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "AI": {"tldr": "MAXS\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u5143\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u77bb\u7b56\u7565\u548c\u8f68\u8ff9\u6536\u655b\u673a\u5236\uff0c\u5e73\u8861\u5168\u5c40\u6709\u6548\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u591a\u5de5\u5177\u63a8\u7406\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a(1) \u5c40\u90e8\u77ed\u89c6\u751f\u6210\uff0c\u7f3a\u4e4f\u524d\u77bb\u6027\uff1b(2) \u8f68\u8ff9\u4e0d\u7a33\u5b9a\u6027\uff0c\u65e9\u671f\u5c0f\u9519\u8bef\u4f1a\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u53d1\u6563\u3002\u8fd9\u4e9b\u95ee\u9898\u4f7f\u5f97\u96be\u4ee5\u5e73\u8861\u5168\u5c40\u6709\u6548\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faMAXS\u6846\u67b6\uff0c\u91c7\u7528\u524d\u77bb\u7b56\u7565\u6269\u5c55\u63a8\u7406\u8def\u5f84\u51e0\u6b65\uff0c\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u7684\u4f18\u52bf\u503c\uff0c\u7ed3\u5408\u6b65\u9aa4\u4e00\u81f4\u6027\u65b9\u5dee\u548c\u6b65\u9aa4\u95f4\u8d8b\u52bf\u659c\u7387\u9009\u62e9\u7a33\u5b9a\u3001\u4e00\u81f4\u3001\u9ad8\u4ef7\u503c\u7684\u63a8\u7406\u6b65\u9aa4\u3002\u5f15\u5165\u8f68\u8ff9\u6536\u655b\u673a\u5236\uff0c\u5728\u8def\u5f84\u4e00\u81f4\u6027\u8fbe\u5230\u65f6\u505c\u6b62\u8fdb\u4e00\u6b65\u5c55\u5f00\uff0c\u5e73\u8861\u8d44\u6e90\u6548\u7387\u548c\u5168\u5c40\u6709\u6548\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u7840\u6a21\u578b\uff08MiMo-VL-7B\u3001Qwen2.5-VL-7B\u3001Qwen2.5-VL-32B\uff09\u548c\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cMAXS\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8bc1\u5b9e\u4e86\u524d\u77bb\u7b56\u7565\u548c\u5de5\u5177\u4f7f\u7528\u7684\u6709\u6548\u6027\u3002", "conclusion": "MAXS\u901a\u8fc7\u5143\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u63a8\u7406\u4e2d\u7684\u5c40\u90e8\u77ed\u89c6\u548c\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u6709\u6548\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2601.09703", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09703", "abs": "https://arxiv.org/abs/2601.09703", "authors": ["Sicong Liu", "Yanxian Huang", "Mingwei Liu", "Jiachi Chen", "Ensheng Shi", "Yuchi Ma", "Hongyu Zhang", "Yin Zhang", "Yanlin Wang"], "title": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation", "comment": null, "summary": "Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.", "AI": {"tldr": "ShortCoder\u662f\u4e00\u4e2a\u77e5\u8bc6\u6ce8\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u6cd5\u7ea7\u7b80\u5316\u89c4\u5219\u3001\u6df7\u5408\u6570\u636e\u5408\u6210\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u4f18\u5316\u4ee3\u7801\u751f\u6210\u6548\u7387\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u7b49\u4ef7\u6027\u548c\u53ef\u8bfb\u6027\u7684\u540c\u65f6\u51cf\u5c1118.1%-37.8%\u7684token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709LLM\u5728\u4ee3\u7801\u751f\u6210\u65f6\u6bcf\u4e2atoken\u90fd\u9700\u8981\u5b8c\u6574\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u5185\u5b58\u5360\u7528\u9ad8\u3001\u8d44\u6e90\u6d88\u8017\u5927\u3002\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u9636\u6bb5\u4f18\u5316\uff08\u5982\u63d0\u793a\u538b\u7f29\u3001\u6a21\u578b\u91cf\u5316\uff09\uff0c\u4f46\u751f\u6210\u9636\u6bb5\u4f18\u5316\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e09\u90e8\u5206\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8eAST\u4fdd\u6301\u8f6c\u6362\u768410\u4e2aPython\u8bed\u6cd5\u7ea7\u7b80\u5316\u89c4\u5219\uff1b2\uff09\u7ed3\u5408\u89c4\u5219\u91cd\u5199\u548cLLM\u5f15\u5bfc\u7cbe\u70bc\u7684\u6df7\u5408\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u6784\u5efaShorterCodeBench\u8bed\u6599\u5e93\uff1b3\uff09\u5c06\u7b80\u6d01\u6027\u610f\u8bc6\u6ce8\u5165\u57fa\u7840LLM\u7684\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5728HumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cShortCoder\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ee3\u7801\u751f\u6210\u6548\u7387\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad818.1%-37.8%\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "conclusion": "ShortCoder\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u751f\u6210\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u6cd5\u7b80\u5316\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u7b49\u4ef7\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u6d88\u8017\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2601.09264", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09264", "abs": "https://arxiv.org/abs/2601.09264", "authors": ["Ziyi Shi", "Xusen Guo", "Hongliang Lu", "Mingxing Peng", "Haotian Wang", "Zheng Zhu", "Zhenning Li", "Yuxuan Liang", "Xinhu Zheng", "Hai Yang"], "title": "Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants", "comment": "20pages, 6 figures, a 60-page supporting material pdf file", "summary": "Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u653f\u7b56\u5236\u5b9a\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u8c03\u548c\u4e3b\u52a8\u7684\u8de8\u533a\u57df\u75ab\u60c5\u9632\u63a7\uff0c\u76f8\u6bd4\u73b0\u5b9e\u653f\u7b56\u53ef\u663e\u8457\u964d\u4f4e\u611f\u67d3\u548c\u6b7b\u4ea1\u4eba\u6570\u3002", "motivation": "\u4eba\u7c7b\u9a71\u52a8\u7684\u75ab\u60c5\u5e94\u5bf9\u5f80\u5f80\u662f\u5206\u6563\u548c\u53cd\u5e94\u5f0f\u7684\uff0c\u653f\u7b56\u5236\u5b9a\u5b64\u7acb\u4e14\u8c03\u6574\u6ede\u540e\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4e3b\u52a8\u5e72\u9884\u548c\u5168\u7403\u75ab\u60c5\u7f13\u89e3\u3002\u9700\u8981\u534f\u8c03\u7684\u8de8\u533a\u57df\u653f\u7b56\u5236\u5b9a\u65b9\u6cd5\u3002", "method": "\u4e3a\u6bcf\u4e2a\u884c\u653f\u533a\u5206\u914d\u4e00\u4e2aLLM\u667a\u80fd\u4f53\u4f5c\u4e3aAI\u653f\u7b56\u5236\u5b9a\u52a9\u624b\u3002\u667a\u80fd\u4f53\u57fa\u4e8e\u533a\u57df\u7279\u5b9a\u6d41\u884c\u75c5\u5b66\u52a8\u6001\u8fdb\u884c\u63a8\u7406\uff0c\u540c\u65f6\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u901a\u4fe1\u4ee5\u8003\u8651\u8de8\u533a\u57df\u76f8\u4e92\u4f9d\u8d56\u6027\u3002\u6574\u5408\u771f\u5b9e\u4e16\u754c\u6570\u636e\u3001\u75ab\u60c5\u6f14\u5316\u6a21\u62df\u5668\u548c\u7ed3\u6784\u5316\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u3002", "result": "\u4f7f\u7528\u7f8e\u56fd2020\u5e744-12\u6708\u5dde\u7ea7COVID-19\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002\u76f8\u6bd4\u73b0\u5b9e\u75ab\u60c5\u7ed3\u679c\uff0c\u5728\u5355\u4e2a\u5dde\u5c42\u9762\u5206\u522b\u51cf\u5c11\u7d2f\u8ba1\u611f\u67d3\u548c\u6b7b\u4ea1\u8fbe63.7%\u548c40.1%\uff1b\u5728\u8de8\u5dde\u805a\u5408\u5c42\u9762\u5206\u522b\u51cf\u5c1139.0%\u548c27.0%\u3002", "conclusion": "LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u534f\u8c03\u653f\u7b56\u5236\u5b9a\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u75ab\u60c5\u9632\u63a7\uff0c\u5c55\u793a\u4e86AI\u5728\u590d\u6742\u516c\u5171\u536b\u751f\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.09269", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09269", "abs": "https://arxiv.org/abs/2601.09269", "authors": ["Wencheng Ye", "Liang Peng", "Xiaoyang Yuan", "Yi Bin", "Pengpeng Zeng", "Hengyu Jin", "Heng Tao Shen"], "title": "RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering", "comment": null, "summary": "Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.", "AI": {"tldr": "RISER\u662f\u4e00\u4e2a\u57fa\u4e8e\u8def\u7531\u5668\u7684\u6fc0\u6d3b\u7a7a\u95f4\u5e72\u9884\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u7ec4\u5408\u53ef\u590d\u7528\u63a8\u7406\u5411\u91cf\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8token\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u7684\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u3001\u624b\u52a8\u5e72\u9884\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u63a8\u7406\u7684\u52a8\u6001\u7279\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u5f15\u5bfcLLM\u63a8\u7406\u7684\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u53ef\u590d\u7528\u63a8\u7406\u5411\u91cf\u5e93\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u52a8\u6001\u7ec4\u5408\u8fd9\u4e9b\u5411\u91cf\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u4efb\u52a1\u7ea7\u5956\u52b1\u4e0b\u4f18\u5316\u8def\u7531\u5668\uff0c\u4ee5\u6d8c\u73b0\u548c\u7ec4\u5408\u65b9\u5f0f\u6fc0\u6d3b\u6f5c\u5728\u8ba4\u77e5\u539f\u8bed\u3002", "result": "\u5728\u4e03\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRISER\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5e26\u67653.4-6.5%\u7684\u5e73\u5747\u96f6\u6837\u672c\u51c6\u786e\u7387\u63d0\u5347\uff0c\u8d85\u8d8aCoT\u63a8\u7406\u4e14token\u6548\u7387\u9ad82-3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5065\u7684\u51c6\u786e\u7387\u589e\u76ca\u3002", "conclusion": "RISER\u80fd\u591f\u81ea\u4e3b\u7ec4\u5408\u591a\u4e2a\u5411\u91cf\u5f62\u6210\u53ef\u89e3\u91ca\u7684\u7cbe\u786e\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u63a7\u3001\u9ad8\u6548\u7684LLM\u63a8\u7406\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09278", "abs": "https://arxiv.org/abs/2601.09278", "authors": ["Xiaohan Yu", "Chao Feng", "Lang Mei", "Chong Chen"], "title": "M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning", "comment": null, "summary": "Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.", "AI": {"tldr": "M\u00b3Searcher\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u591a\u6a21\u6001\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\uff0c\u901a\u8fc7\u89e3\u8026\u4fe1\u606f\u83b7\u53d6\u548c\u7b54\u6848\u63a8\u5bfc\u6765\u89e3\u51b3\u591a\u6a21\u6001\u81ea\u4e3b\u641c\u7d22\u7684\u6311\u6218\uff0c\u4f7f\u7528\u68c0\u7d22\u5bfc\u5411\u7684\u591a\u76ee\u6807\u5956\u52b1\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u5728\u65b0\u7684MMSearchVQA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709DeepResearch\u98ce\u683c\u4ee3\u7406\u4ec5\u9650\u4e8e\u6587\u672c\u6a21\u6001\uff0c\u6269\u5c55\u5230\u591a\u6a21\u6001\u73af\u5883\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u5927\u89c4\u6a21\u591a\u6a21\u6001\u5de5\u5177\u4f7f\u7528\u7684\u4e13\u4e1a\u5316-\u6cdb\u5316\u6743\u8861\uff1b2) \u590d\u6742\u591a\u6b65\u9aa4\u591a\u6a21\u6001\u641c\u7d22\u8f68\u8ff9\u8bad\u7ec3\u6570\u636e\u7684\u4e25\u91cd\u7a00\u7f3a\u3002", "method": "\u63d0\u51faM\u00b3Searcher\u6a21\u5757\u5316\u591a\u6a21\u6001\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\uff0c\u660e\u786e\u89e3\u8026\u4fe1\u606f\u83b7\u53d6\u548c\u7b54\u6848\u63a8\u5bfc\u3002\u4f7f\u7528\u68c0\u7d22\u5bfc\u5411\u7684\u591a\u76ee\u6807\u5956\u52b1\u8fdb\u884c\u4f18\u5316\uff0c\u8054\u5408\u9f13\u52b1\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u63a8\u7406\u5408\u7406\u6027\u548c\u68c0\u7d22\u4fdd\u771f\u5ea6\u3002\u5f00\u53d1\u4e86MMSearchVQA\u591a\u6a21\u6001\u591a\u8df3\u6570\u636e\u96c6\u6765\u652f\u6301\u68c0\u7d22\u4e2d\u5fc3\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eM\u00b3Searcher\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fc1\u79fb\u9002\u5e94\u6027\u548c\u6709\u6548\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "M\u00b3Searcher\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u68c0\u7d22\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u81ea\u4e3b\u4fe1\u606f\u641c\u7d22\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u4fe1\u606f\u83b7\u53d6\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.09083", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09083", "abs": "https://arxiv.org/abs/2601.09083", "authors": ["Chi-Chih Chang", "Siqi Zhu", "Zhichen Zeng", "Haibin Lin", "Jiaxuan You", "Mohamed S. Abdelfattah", "Ziheng Jiang", "Xuehai Qian"], "title": "SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache", "comment": null, "summary": "We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.", "AI": {"tldr": "SRT\u662f\u4e00\u79cd\u5229\u7528\u6811\u7ed3\u6784\u7f13\u5b58\u548c\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u751f\u6210\u9636\u6bb5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u540c\u4e00\u63d0\u793a\u5728\u4e0d\u540c\u8bad\u7ec3\u6b65\u9aa4\u4e2d\u7684\u751f\u6210\u7ed3\u679c\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u76f8\u4f3c\u6027\u6765\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b", "method": "\u4e3a\u6bcf\u4e2a\u63d0\u793a\u5efa\u7acb\u6811\u7ed3\u6784\u7f13\u5b58\u5b58\u50a8\u5386\u53f2\u751f\u6210\u7ed3\u679c\uff0c\u5728\u751f\u6210\u65f6\u4f5c\u4e3a\u8349\u7a3f\u6a21\u578b\u8fdb\u884c\u63a8\u6d4b\u89e3\u7801\uff0c\u901a\u8fc7\u5728\u7ebf\u66f4\u65b0\u7f13\u5b58\u548c\u5229\u7528GPU\u7a7a\u95f2\u65f6\u95f4\u8fdb\u884c\u9884\u751f\u6210\u6765\u4fdd\u6301\u7f13\u5b58\u65b0\u9c9c\u5ea6", "result": "SRT\u80fd\u663e\u8457\u964d\u4f4e\u751f\u6210\u5ef6\u8fdf\u548c\u6b65\u5ef6\u8fdf\uff0c\u51cf\u5c11\u6bcftoken\u63a8\u7406\u6210\u672c\uff0c\u5728rollout\u9636\u6bb5\u5b9e\u73b0\u6700\u9ad82.08\u500d\u7684\u65f6\u949f\u65f6\u95f4\u52a0\u901f", "conclusion": "SRT\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u53ef\u4e0e\u6807\u51c6RL\u6d41\u7a0b\u96c6\u6210\uff0c\u5728\u4e0d\u727a\u7272\u5206\u5e03\u6b63\u786e\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "topic": "agentic reinforcement learning"}}
{"id": "2601.09085", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09085", "abs": "https://arxiv.org/abs/2601.09085", "authors": ["Kangda Wei", "Ruihong Huang"], "title": "MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.", "AI": {"tldr": "MMR-GRPO\u901a\u8fc7\u5f15\u5165\u6700\u5927\u8fb9\u9645\u76f8\u5173\u6027\u6765\u91cd\u65b0\u52a0\u6743\u5956\u52b1\uff0c\u57fa\u4e8e\u5b8c\u6210\u591a\u6837\u6027\u51cf\u5c11\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "GRPO\u65b9\u6cd5\u8bad\u7ec3\u6570\u5b66\u63a8\u7406\u6a21\u578b\u9700\u8981\u6bcf\u4e2a\u63d0\u793a\u751f\u6210\u591a\u4e2a\u5b8c\u6210\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u867d\u7136\u6700\u8fd1\u7814\u7a76\u51cf\u5c11\u4e86\u8fbe\u5230\u5cf0\u503c\u6027\u80fd\u6240\u9700\u7684\u8bad\u7ec3\u6b65\u9aa4\uff0c\u4f46\u7531\u4e8e\u6bcf\u6b65\u6210\u672c\u589e\u52a0\uff0c\u603b\u4f53\u8bad\u7ec3\u65f6\u95f4\u5f80\u5f80\u4fdd\u6301\u4e0d\u53d8\u751a\u81f3\u589e\u52a0\u3002", "method": "\u63d0\u51faMMR-GRPO\uff0c\u96c6\u6210\u6700\u5927\u8fb9\u9645\u76f8\u5173\u6027\u6765\u57fa\u4e8e\u5b8c\u6210\u591a\u6837\u6027\u91cd\u65b0\u52a0\u6743\u5956\u52b1\u3002\u6838\u5fc3\u6d1e\u5bdf\u662f\u8bed\u4e49\u5197\u4f59\u7684\u5b8c\u6210\u8d21\u732e\u6709\u9650\u7684\u5b66\u4e60\u4fe1\u53f7\uff1b\u4f18\u5148\u8003\u8651\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\u80fd\u4ea7\u751f\u66f4\u591a\u4fe1\u606f\u5316\u7684\u66f4\u65b0\u5e76\u52a0\u901f\u6536\u655b\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u578b\u89c4\u6a21\uff081.5B\u30017B\u30018B\uff09\u3001\u4e09\u4e2aGRPO\u53d8\u4f53\u548c\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cMMR-GRPO\u5728\u4fdd\u6301\u53ef\u6bd4\u5cf0\u503c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5e73\u5747\u9700\u8981\u51cf\u5c1147.9%\u7684\u8bad\u7ec3\u6b65\u9aa4\u548c70.2%\u7684\u5899\u4e0a\u65f6\u95f4\u3002\u8fd9\u4e9b\u6536\u76ca\u5728\u4e0d\u540c\u6a21\u578b\u3001\u65b9\u6cd5\u548c\u57fa\u51c6\u4e0a\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "MMR-GRPO\u901a\u8fc7\u5956\u52b1\u591a\u6837\u6027\u663e\u8457\u52a0\u901fGRPO\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3\u6570\u5b66\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08955", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08955", "abs": "https://arxiv.org/abs/2601.08955", "authors": ["Youwei Liu", "Jian Wang", "Hanlin Wang", "Beichen Guo", "Wenjie Li"], "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models", "comment": null, "summary": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \\textit{observable} and \\textit{imaginable} Markov decision process to guide policy learning. We instantiate \\texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \\texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.", "AI": {"tldr": "ITP\u6846\u67b6\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u524d\u77bb\u60f3\u8c61\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u524d\u77bb\u673a\u5236\uff0c\u5728\u591a\u79cd\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5f53\u524d\u4e16\u754c\u6a21\u578b\u4e3b\u8981\u8fdb\u884c\u5355\u6b65\u6216\u56fa\u5b9a\u6b65\u957f\u7684\u63a8\u6f14\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6325\u5176\u5728\u590d\u6742\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u60f3\u8c61\u6b65\u957f\u3001\u878d\u5408\u672a\u6765\u4fe1\u53f7\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faImagine-then-Plan (ITP)\u6846\u67b6\uff0c\u8ba9\u7b56\u7565\u6a21\u578b\u4e0e\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u4ea4\u4e92\u751f\u6210\u591a\u6b65\"\u60f3\u8c61\"\u8f68\u8ff9\u3002\u5f15\u5165\u81ea\u9002\u5e94\u524d\u77bb\u673a\u5236\uff0c\u6743\u8861\u6700\u7ec8\u76ee\u6807\u548c\u4efb\u52a1\u8fdb\u5ea6\u3002\u5c06\u60f3\u8c61\u8f68\u8ff9\u63d0\u4f9b\u7684\u672a\u6765\u4fe1\u53f7\uff08\u5982\u8fdb\u5ea6\u548c\u6f5c\u5728\u51b2\u7a81\uff09\u4e0e\u5f53\u524d\u89c2\u5bdf\u878d\u5408\uff0c\u5f62\u6210\u90e8\u5206\u53ef\u89c2\u5bdf\u548c\u53ef\u60f3\u8c61\u7684MDP\u6765\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728\u4ee3\u8868\u6027\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cITP\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u3002\u5206\u6790\u9a8c\u8bc1\u81ea\u9002\u5e94\u524d\u77bb\u673a\u5236\u5927\u5e45\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ITP\u6846\u67b6\u901a\u8fc7\u524d\u77bb\u60f3\u8c61\u548c\u81ea\u9002\u5e94\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u5904\u7406\u66f4\u5e7f\u6cdb\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09017", "abs": "https://arxiv.org/abs/2601.09017", "authors": ["Haryo Akbarianto Wibowo", "Alaa Elsetohy", "Qinrong Cui", "Alham Fikri Aji"], "title": "Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social Deduction Game", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has necessitated more robust evaluation methods that go beyond static benchmarks, which are increasingly prone to data saturation and leakage. In this paper, we propose a dynamic benchmarking framework for evaluating multilingual and multicultural capabilities through the social deduction game Spyfall. In our setup, models must engage in strategic dialogue to either identify a secret agent or avoid detection, utilizing culturally relevant locations or local foods. Our results show that our game-based rankings align closely with the Chatbot Arena. However, we find a significant performance gap in non-English contexts: models are generally less proficient when handling locally specific entities and often struggle with rule-following or strategic integrity in non-English languages. We demonstrate that this game-based approach provides a scalable, leakage-resistant, and culturally nuanced alternative to traditional NLP benchmarks. The game history can be accessed here https://huggingface.co/datasets/haryoaw/cultural-spyfall.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u793e\u4ea4\u63a8\u7406\u6e38\u620fSpyfall\u7684\u52a8\u6001\u8bc4\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u591a\u8bed\u8a00\u548c\u6587\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u975e\u82f1\u8bed\u8bed\u5883\u4e0b\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u8bc4\u6d4b\u57fa\u51c6\u9762\u4e34\u6570\u636e\u9971\u548c\u548c\u6cc4\u9732\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u7279\u522b\u662f\u9700\u8981\u8bc4\u4f30LLM\u5728\u591a\u8bed\u8a00\u548c\u8de8\u6587\u5316\u73af\u5883\u4e0b\u7684\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u793e\u4ea4\u63a8\u7406\u6e38\u620fSpyfall\u4f5c\u4e3a\u52a8\u6001\u8bc4\u6d4b\u6846\u67b6\uff0c\u6a21\u578b\u9700\u8981\u53c2\u4e0e\u6218\u7565\u5bf9\u8bdd\u6765\u8bc6\u522b\u79d8\u5bc6\u7279\u5de5\u6216\u907f\u514d\u88ab\u53d1\u73b0\uff0c\u5229\u7528\u6587\u5316\u76f8\u5173\u7684\u5730\u70b9\u6216\u5f53\u5730\u98df\u7269\u4f5c\u4e3a\u6e38\u620f\u5143\u7d20\u3002", "result": "\u6e38\u620f\u6392\u540d\u4e0eChatbot Arena\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u53d1\u73b0\u663e\u8457\u7684\u975e\u82f1\u8bed\u6027\u80fd\u5dee\u8ddd\uff1a\u6a21\u578b\u5728\u5904\u7406\u672c\u5730\u7279\u5b9a\u5b9e\u4f53\u65f6\u666e\u904d\u8f83\u5f31\uff0c\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7ecf\u5e38\u96be\u4ee5\u9075\u5faa\u89c4\u5219\u6216\u4fdd\u6301\u6218\u7565\u5b8c\u6574\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6e38\u620f\u7684\u65b9\u6cd5\u4e3a\u4f20\u7edfNLP\u57fa\u51c6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6297\u6cc4\u9732\u4e14\u5177\u6709\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u66f4\u597d\u5730\u8bc4\u4f30LLM\u7684\u591a\u8bed\u8a00\u548c\u6587\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.09382", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09382", "abs": "https://arxiv.org/abs/2601.09382", "authors": ["Qinglong Shi", "Donghai Wang", "Hantao Zhou", "Jiguo Li", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He"], "title": "Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments", "comment": "8 pages, 2 figures", "summary": "Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e3b\u52a8\u5f0f\u4efb\u52a1\u5bfc\u5411\u667a\u80fd\u4f53\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u610f\u56fe\u6761\u4ef6\u76d1\u63a7\u548c\u4e8b\u4ef6\u89e6\u53d1\u8ddf\u8fdb\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u957f\u671f\u7528\u6237\u610f\u56fe\u7ef4\u62a4\uff0c\u5e76\u521b\u5efaChronosBench\u57fa\u51c6\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u4e3b\u8981\u91c7\u7528\u53cd\u5e94\u5f0f\u8303\u5f0f\uff0c\u53ea\u80fd\u54cd\u5e94\u77ed\u671f\u4f1a\u8bdd\u4e2d\u7684\u5373\u65f6\u7528\u6237\u67e5\u8be2\uff0c\u65e0\u6cd5\u7ef4\u62a4\u957f\u671f\u7528\u6237\u610f\u56fe\u548c\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u5916\u90e8\u73af\u5883\uff0c\u9700\u8981\u65b0\u7684\u4ea4\u4e92\u8303\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u5f0f\u4efb\u52a1\u5bfc\u5411\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a\u610f\u56fe\u6761\u4ef6\u76d1\u63a7\uff08\u57fa\u4e8e\u5bf9\u8bdd\u5386\u53f2\u81ea\u4e3b\u5236\u5b9a\u89e6\u53d1\u6761\u4ef6\uff09\u548c\u4e8b\u4ef6\u89e6\u53d1\u8ddf\u8fdb\uff08\u68c0\u6d4b\u5230\u6709\u7528\u73af\u5883\u66f4\u65b0\u65f6\u4e3b\u52a8\u4e0e\u7528\u6237\u4e92\u52a8\uff09\u3002\u5efa\u7acb\u9ad8\u8d28\u91cf\u6570\u636e\u5408\u6210\u7ba1\u9053\u6784\u5efa\u52a8\u6001\u73af\u5883\u4e2d\u7684\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\uff0c\u5e76\u521b\u5efaChronosBench\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u7684\u5fae\u8c03\u6a21\u578b\u5728\u5305\u542b\u7528\u6237\u610f\u56fe\u8f6c\u53d8\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u8fbe\u523085.19%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u6d4b\u8bd5\u6a21\u578b\u3002\u8bc4\u4f30\u5f53\u524d\u9886\u5148\u7684\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u957f\u671f\u4efb\u52a1\u5bfc\u5411\u4ea4\u4e92\u4e2d\u7684\u7f3a\u9677\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e3b\u52a8\u5f0f\u667a\u80fd\u4f53\u8303\u5f0f\u80fd\u6709\u6548\u5f25\u5408\u76f8\u5bf9\u9759\u6001\u7684\u7528\u6237\u9700\u6c42\u4e0e\u52a8\u6001\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6570\u636e\u9a71\u52a8\u7b56\u7565\u88ab\u9a8c\u8bc1\u6709\u6548\uff0c\u4e3a\u957f\u671f\u4efb\u52a1\u5bfc\u5411\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.09465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09465", "abs": "https://arxiv.org/abs/2601.09465", "authors": ["Shuo Zhang", "Chaofa Yuan", "Ryan Guo", "Xiaomin Yu", "Rui Xu", "Zhangquan Chen", "Zinuo Li", "Zhi Yang", "Shuhao Guan", "Zhenheng Tang", "Sen Hu", "Liwen Zhang", "Ronghao Chen", "Huacan Wang"], "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines", "comment": null, "summary": "While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.", "AI": {"tldr": "EvoFSM\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6f14\u5316\u6709\u9650\u72b6\u6001\u673a\u800c\u975e\u81ea\u7531\u5f62\u5f0f\u91cd\u5199\uff0c\u5728\u4fdd\u6301\u63a7\u5236\u7684\u540c\u65f6\u5b9e\u73b0\u9002\u5e94\u6027\uff0c\u5728\u4e94\u4e2a\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5927\u591a\u4f9d\u8d56\u56fa\u5b9a\u5de5\u4f5c\u6d41\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u5f00\u653e\u5f0f\u67e5\u8be2\u3002\u867d\u7136\u6700\u8fd1\u7814\u7a76\u63a2\u7d22\u901a\u8fc7\u91cd\u5199\u4ee3\u7801\u6216\u63d0\u793a\u5b9e\u73b0\u81ea\u8fdb\u5316\uff0c\u4f46\u65e0\u7ea6\u675f\u4f18\u5316\u5e38\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\u3001\u5e7b\u89c9\u548c\u6307\u4ee4\u6f02\u79fb\u3002", "method": "\u63d0\u51faEvoFSM\u6846\u67b6\uff0c\u5c06\u4f18\u5316\u7a7a\u95f4\u89e3\u8026\u4e3a\u5b8f\u89c2\u7684Flow\uff08\u72b6\u6001\u8f6c\u79fb\u903b\u8f91\uff09\u548c\u5fae\u89c2\u7684Skill\uff08\u72b6\u6001\u7279\u5b9a\u884c\u4e3a\uff09\uff0c\u901a\u8fc7\u53d7\u9650\u64cd\u4f5c\u96c6\u7cbe\u70bcFSM\uff0c\u5e76\u5f15\u5165\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5c06\u6210\u529f\u8f68\u8ff9\u63d0\u70bc\u4e3a\u53ef\u91cd\u7528\u5148\u9a8c\uff0c\u5931\u8d25\u6a21\u5f0f\u4f5c\u4e3a\u672a\u6765\u67e5\u8be2\u7684\u7ea6\u675f\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728DeepSearch\u57fa\u51c6\u4e0a\u8fbe\u523058.0%\u51c6\u786e\u7387\u3002\u5728\u4ea4\u4e92\u5f0f\u51b3\u7b56\u4efb\u52a1\u4e0a\u7684\u989d\u5916\u7ed3\u679c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EvoFSM\u901a\u8fc7\u7ed3\u6784\u5316\u81ea\u8fdb\u5316\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u63a7\u5236\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9002\u5e94\u6027\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u5728\u5f00\u653e\u5f0f\u67e5\u8be2\u4e2d\u7684\u6709\u6548\u81ea\u8fdb\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.09503", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09503", "abs": "https://arxiv.org/abs/2601.09503", "authors": ["Siyuan Liu", "Hongbang Yuan", "Xinze Li", "Ziyue Zhu", "Yixin Cao", "Yu-Gang Jiang"], "title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.", "AI": {"tldr": "\u63d0\u51faT2Q\u8bc4\u4f30\u8303\u5f0f\uff0c\u901a\u8fc7\u4efb\u52a1\u8f6c\u95ee\u7b54\u7684\u65b9\u5f0f\u89e3\u8026\u4efb\u52a1\u6267\u884c\u4e0e\u73af\u5883\u7406\u89e3\uff0c\u53d1\u73b0\u4efb\u52a1\u6210\u529f\u4e0d\u80fd\u6709\u6548\u53cd\u6620\u73af\u5883\u7406\u89e3\u80fd\u529b\uff0c\u73b0\u6709\u8bb0\u5fc6\u673a\u5236\u5b58\u5728\u5c40\u9650", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u8f68\u8ff9\u6210\u529f\u7387\u6307\u6807\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u73af\u5883\u7406\u89e3\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5224\u65ad\u667a\u80fd\u4f53\u662f\u5426\u771f\u6b63\u638c\u63e1\u4e86\u53ef\u8fc1\u79fb\u7684\u73af\u5883\u6a21\u578b", "method": "\u63d0\u51faTask-to-Quiz(T2Q)\u8bc4\u4f30\u8303\u5f0f\uff0c\u5c06\u4efb\u52a1\u6267\u884c\u4e0e\u73af\u5883\u7406\u89e3\u89e3\u8026\uff0c\u6784\u5efaT2QBench\u57fa\u51c6\uff0c\u5305\u542b30\u4e2a\u73af\u5883\u548c1,967\u4e2a\u57fa\u4e8e\u73af\u5883\u7684QA\u5bf9", "result": "\u5b9e\u9a8c\u8868\u660e\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u73af\u5883\u7406\u89e3\u80fd\u529b\u76f8\u5173\u6027\u5f31\uff0c\u73b0\u6709\u8bb0\u5fc6\u673a\u5236\u65e0\u6cd5\u6709\u6548\u5e2e\u52a9\u667a\u80fd\u4f53\u83b7\u53d6\u73af\u5883\u6a21\u578b\uff0c\u4e3b\u52a8\u63a2\u7d22\u548c\u7ec6\u7c92\u5ea6\u72b6\u6001\u8868\u793a\u662f\u4e3b\u8981\u74f6\u9888", "conclusion": "\u9700\u8981\u8d85\u8d8a\u4efb\u52a1\u6210\u529f\u7387\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0cT2Q\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u74f6\u9888", "topic": "agent analysis"}}
{"id": "2601.09151", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09151", "abs": "https://arxiv.org/abs/2601.09151", "authors": ["Yang Nan", "Qihao Wen", "Jiahao Wang", "Pengfei He", "Ravi Tandon", "Yong Ge", "Han Xu"], "title": "Interpretable Probability Estimation with LLMs via Shapley Reconstruction", "comment": null, "summary": "Large Language Models (LLMs) demonstrate potential to estimate the probability of uncertain events, by leveraging their extensive knowledge and reasoning capabilities. This ability can be applied to support intelligent decision-making across diverse fields, such as financial forecasting and preventive healthcare. However, directly prompting LLMs for probability estimation faces significant challenges: their outputs are often noisy, and the underlying predicting process is opaque. In this paper, we propose PRISM: Probability Reconstruction via Shapley Measures, a framework that brings transparency and precision to LLM-based probability estimation. PRISM decomposes an LLM's prediction by quantifying the marginal contribution of each input factor using Shapley values. These factor-level contributions are then aggregated to reconstruct a calibrated final estimate. In our experiments, we demonstrate PRISM improves predictive accuracy over direct prompting and other baselines, across multiple domains including finance, healthcare, and agriculture. Beyond performance, PRISM provides a transparent prediction pipeline: our case studies visualize how individual factors shape the final estimate, helping build trust in LLM-based decision support systems.", "AI": {"tldr": "PRISM\u6846\u67b6\u4f7f\u7528Shapley\u503c\u5206\u89e3LLM\u7684\u6982\u7387\u4f30\u8ba1\uff0c\u901a\u8fc7\u91cf\u5316\u5404\u8f93\u5165\u56e0\u7d20\u7684\u8fb9\u9645\u8d21\u732e\u6765\u91cd\u6784\u6821\u51c6\u7684\u6700\u7ec8\u4f30\u8ba1\uff0c\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "LLMs\u5728\u6982\u7387\u4f30\u8ba1\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u76f4\u63a5\u63d0\u793a\u5b58\u5728\u8f93\u51fa\u566a\u58f0\u5927\u3001\u9884\u6d4b\u8fc7\u7a0b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u9700\u8981\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faPRISM\u6846\u67b6\uff0c\u4f7f\u7528Shapley\u503c\u91cf\u5316\u6bcf\u4e2a\u8f93\u5165\u56e0\u7d20\u7684\u8fb9\u9645\u8d21\u732e\uff0c\u7136\u540e\u805a\u5408\u8fd9\u4e9b\u56e0\u7d20\u7ea7\u8d21\u732e\u6765\u91cd\u6784\u6821\u51c6\u7684\u6700\u7ec8\u6982\u7387\u4f30\u8ba1\u3002", "result": "PRISM\u5728\u91d1\u878d\u3001\u533b\u7597\u3001\u519c\u4e1a\u7b49\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u76f4\u63a5\u63d0\u793a\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u7684\u9884\u6d4b\u6d41\u7a0b\u3002", "conclusion": "PRISM\u4e3aLLM\u6982\u7387\u4f30\u8ba1\u5e26\u6765\u4e86\u900f\u660e\u5ea6\u548c\u7cbe\u786e\u6027\uff0c\u6709\u52a9\u4e8e\u5efa\u7acb\u5bf9\u57fa\u4e8eLLM\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u4fe1\u4efb\u3002", "topic": "agent analysis"}}
{"id": "2601.09635", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09635", "abs": "https://arxiv.org/abs/2601.09635", "authors": ["Kuo Liang", "Yuhang Lu", "Jianming Mao", "Shuyi Sun", "Chunwei Yang", "Congcong Zeng", "Xiao Jin", "Hanzhang Qin", "Ruihao Zhu", "Chung-Piaw Teo"], "title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach", "comment": "Updated version of https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5329027", "summary": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.", "AI": {"tldr": "LEAN-LLM-OPT\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u667a\u80fd\u5de5\u4f5c\u6d41\u6846\u67b6\uff0c\u5229\u7528LLM\u4ee3\u7406\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u89e3\u5efa\u6a21\u4efb\u52a1\u548c\u81ea\u52a8\u5316\u6570\u636e\u5904\u7406\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5efa\u6a21\u5de5\u4f5c\u91cf\u3002", "motivation": "\u5927\u89c4\u6a21\u4f18\u5316\u662f\u73b0\u4ee3\u5546\u4e1a\u51b3\u7b56\u7684\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u5efa\u6a21\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u8017\u65f6\u3002\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u52a0\u901f\u4f18\u5316\u6a21\u578b\u7684\u6784\u5efa\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faLEAN-LLM-OPT\u6846\u67b6\uff1a\u4e24\u4e2a\u4e0a\u6e38LLM\u4ee3\u7406\u52a8\u6001\u6784\u5efa\u5de5\u4f5c\u6d41\uff0c\u6307\u5b9a\u7c7b\u4f3c\u95ee\u9898\u7684\u5efa\u6a21\u6b65\u9aa4\uff1b\u4e0b\u6e38LLM\u4ee3\u7406\u9075\u5faa\u5de5\u4f5c\u6d41\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002\u5229\u7528LLM\u6587\u672c\u5904\u7406\u80fd\u529b\u548c\u5efa\u6a21\u5b9e\u8df5\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u5b50\u4efb\u52a1\uff0c\u5c06\u673a\u68b0\u6570\u636e\u5904\u7406\u5378\u8f7d\u5230\u8f85\u52a9\u5de5\u5177\u3002", "result": "\u4f7f\u7528GPT-4.1\u548c\u5f00\u6e90gpt-oss-20B\u7684LEAN-LLM-OPT\u5728\u5927\u89c4\u6a21\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u3002\u5728\u65b0\u52a0\u5761\u822a\u7a7a\u6536\u76ca\u7ba1\u7406\u7528\u4f8b\u4e2d\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u90fd\u53d6\u5f97\u9886\u5148\u6027\u80fd\u3002\u540c\u65f6\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u4f18\u5316\u81ea\u52a8\u5efa\u6a21\u57fa\u51c6Large-Scale-OR\u548cAir-NRM\u3002", "conclusion": "LEAN-LLM-OPT\u901a\u8fc7\u667a\u80fd\u5de5\u4f5c\u6d41\u548cLLM\u4ee3\u7406\u7684\u534f\u540c\uff0c\u6709\u6548\u81ea\u52a8\u5316\u5927\u89c4\u6a21\u4f18\u5316\u5efa\u6a21\u8fc7\u7a0b\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "topic": "code agent"}}
{"id": "2601.09636", "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09636", "abs": "https://arxiv.org/abs/2601.09636", "authors": ["Yibo Lyu", "Gongwei Chen", "Rui Shao", "Weili Guan", "Liqiang Nie"], "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records", "comment": null, "summary": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPersonalAlign\u4efb\u52a1\uff0c\u8981\u6c42GUI\u4ee3\u7406\u5229\u7528\u957f\u671f\u7528\u6237\u8bb0\u5f55\u89e3\u51b3\u6a21\u7cca\u6307\u4ee4\u4e2d\u7684\u7701\u7565\u504f\u597d\uff0c\u5e76\u6839\u636e\u7528\u6237\u72b6\u6001\u9884\u6d4b\u6f5c\u5728\u4e60\u60ef\u4ee5\u63d0\u4f9b\u4e3b\u52a8\u534f\u52a9\u3002\u4e3a\u6b64\u6784\u5efa\u4e86AndroidIntent\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5206\u5c42\u610f\u56fe\u8bb0\u5fc6\u4ee3\u7406HIM-Agent\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u548c\u4e3b\u52a8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5728\u663e\u5f0f\u6307\u4ee4\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9700\u8981\u4e0e\u7528\u6237\u66f4\u590d\u6742\u7684\u9690\u5f0f\u610f\u56fe\u5bf9\u9f50\u3002\u7528\u6237\u6307\u4ee4\u5f80\u5f80\u6a21\u7cca\u4e14\u7701\u7565\u504f\u597d\uff0c\u9700\u8981\u4ee3\u7406\u5229\u7528\u957f\u671f\u7528\u6237\u8bb0\u5f55\u4f5c\u4e3a\u6301\u4e45\u4e0a\u4e0b\u6587\u6765\u89e3\u6790\u8fd9\u4e9b\u610f\u56fe\uff0c\u5e76\u9884\u6d4b\u7528\u6237\u7684\u6f5c\u5728\u4e60\u60ef\u4ee5\u63d0\u4f9b\u4e3b\u52a8\u534f\u52a9\u3002", "method": "1) \u63d0\u51faPersonalAlign\u4efb\u52a1\uff0c\u8981\u6c42\u4ee3\u7406\u5229\u7528\u957f\u671f\u7528\u6237\u8bb0\u5f55\u89e3\u51b3\u6a21\u7cca\u6307\u4ee4\u4e2d\u7684\u7701\u7565\u504f\u597d\u5e76\u9884\u6d4b\u7528\u6237\u4e60\u60ef\uff1b2) \u6784\u5efaAndroidIntent\u57fa\u51c6\uff0c\u5305\u542b20k\u957f\u671f\u8bb0\u5f55\uff0c\u6807\u6ce8\u4e86775\u4e2a\u7528\u6237\u7279\u5b9a\u504f\u597d\u548c215\u4e2a\u4e60\u60ef\uff1b3) \u8bbe\u8ba1HIM-Agent\uff0c\u7ef4\u62a4\u6301\u7eed\u66f4\u65b0\u7684\u4e2a\u4eba\u8bb0\u5fc6\uff0c\u5206\u5c42\u7ec4\u7ec7\u7528\u6237\u504f\u597d\u548c\u4e60\u60ef\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u3002", "result": "\u5728AndroidIntent\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u591a\u79cdGUI\u4ee3\u7406\uff08GPT-5\u3001Qwen3-VL\u3001UI-TARS\u7b49\uff09\u3002HIM-Agent\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6027\u80fd15.7%\u548c\u4e3b\u52a8\u6027\u80fd7.3%\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "PersonalAlign\u4efb\u52a1\u548cAndroidIntent\u57fa\u51c6\u4e3aGUI\u4ee3\u7406\u7684\u9690\u5f0f\u610f\u56fe\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002HIM-Agent\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u7cca\u6307\u4ee4\u89e3\u6790\u548c\u4e3b\u52a8\u534f\u52a9\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316GUI\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.09667", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09667", "abs": "https://arxiv.org/abs/2601.09667", "authors": ["Zhiyuan Hu", "Yunhai Hu", "Juncheng Liu", "Shuyue Stella Li", "Yucheng Wang", "Zhen Xu", "See-Kiong Ng", "Anh Tuan Luu", "Xinxing Xu", "Bryan Hooi", "Cynthia Breazeal", "Hae Won Park"], "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "comment": "Work in Progress", "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "AI": {"tldr": "MATTRL\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ed3\u6784\u5316\u6587\u672c\u7ecf\u9a8c\u6ce8\u5165\u591a\u667a\u80fd\u4f53\u63a8\u7406\u8fc7\u7a0b\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4f18\u52bf\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8d44\u6e90\u5bc6\u96c6\u4e14\u4e0d\u7a33\u5b9a\uff0c\u5b58\u5728\u975e\u5e73\u7a33\u6027\u548c\u7a00\u758f\u9ad8\u65b9\u5dee\u5956\u52b1\u95ee\u9898\u3002", "method": "\u6784\u5efa\u591a\u4e13\u5bb6\u56e2\u961f\u8fdb\u884c\u591a\u8f6e\u8ba8\u8bba\uff0c\u68c0\u7d22\u5e76\u6574\u5408\u6d4b\u8bd5\u65f6\u7ecf\u9a8c\uff0c\u901a\u8fc7\u4fe1\u7528\u5206\u914d\u6784\u5efa\u8f6e\u7ea7\u7ecf\u9a8c\u6c60\u5e76\u91cd\u65b0\u6ce8\u5165\u5bf9\u8bdd\uff0c\u6700\u7ec8\u8fbe\u6210\u5171\u8bc6\u51b3\u7b56\u3002", "result": "\u5728\u533b\u5b66\u3001\u6570\u5b66\u548c\u6559\u80b2\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATTRL\u76f8\u6bd4\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u5e73\u5747\u63d0\u53473.67%\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u53478.67%\u3002", "conclusion": "MATTRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u3001\u6709\u6548\u4e14\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u65e0\u9700\u8c03\u4f18\u5373\u53ef\u5b9e\u73b0\u5206\u5e03\u504f\u79fb\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2601.09233", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09233", "abs": "https://arxiv.org/abs/2601.09233", "authors": ["Zhengyang Zhao", "Lu Ma", "Yizhen Jiang", "Xiaochen Ma", "Zimo Meng", "Chengyu Shen", "Lexiang Tang", "Haoze Sun", "Peng Pei", "Wentao Zhang"], "title": "GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization", "comment": null, "summary": "The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.", "AI": {"tldr": "\u63d0\u51faGIFT\u65b9\u6cd5\uff0c\u5c06SFT\u91cd\u65b0\u8868\u8ff0\u4e3a\u6709\u9650\u6e29\u5ea6\u7684\u80fd\u91cf\u52bf\uff0c\u89e3\u51b3\u4f20\u7edfSFT+RL\u8bad\u7ec3\u8303\u5f0f\u4e2d\u7684\u4f18\u5316\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3aRL\u521d\u59cb\u5316\u63d0\u4f9b\u66f4\u597d\u7684\u5206\u5e03\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u5927\u63a8\u7406\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\uff08SFT+RL\uff09\u5b58\u5728\u5185\u5728\u4f18\u5316\u4e0d\u5339\u914d\uff1aSFT\u7684\u521a\u6027\u76d1\u7763\u5bfc\u81f4\u5206\u5e03\u574d\u7f29\uff0c\u8017\u5c3d\u540e\u7eedRL\u6240\u9700\u7684\u63a2\u7d22\u7a7a\u95f4\u3002", "method": "\u5c06SFT\u91cd\u65b0\u8868\u8ff0\u4e3a\u7edf\u4e00\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u51faGibbs\u521d\u59cb\u5316\u4e0e\u6709\u9650\u6e29\u5ea6\uff08GIFT\uff09\u65b9\u6cd5\u3002\u5c06\u6807\u51c6SFT\u89c6\u4e3a\u6291\u5236\u57fa\u7840\u5148\u9a8c\u7684\u96f6\u6e29\u5ea6\u6781\u9650\uff0c\u800cGIFT\u5c06\u76d1\u7763\u4f5c\u4e3a\u6709\u9650\u6e29\u5ea6\u7684\u80fd\u91cf\u52bf\uff0c\u5efa\u7acb\u5206\u5e03\u6865\u6881\u786e\u4fdd\u6574\u4e2a\u540e\u8bad\u7ec3\u6d41\u7a0b\u7684\u76ee\u6807\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGIFT\u5728\u7528\u4e8eRL\u521d\u59cb\u5316\u65f6\u663e\u8457\u4f18\u4e8e\u6807\u51c6SFT\u548c\u5176\u4ed6\u7ade\u4e89\u57fa\u7ebf\uff0c\u4e3a\u540e\u8bad\u7ec3\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u63d0\u4f9b\u4e86\u6570\u5b66\u539f\u7406\u4e0a\u7684\u8def\u5f84\u3002", "conclusion": "GIFT\u901a\u8fc7\u6709\u9650\u6e29\u5ea6\u7684\u80fd\u91cf\u52bf\u6846\u67b6\u89e3\u51b3\u4e86SFT+RL\u8303\u5f0f\u4e2d\u7684\u4f18\u5316\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09236", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09236", "abs": "https://arxiv.org/abs/2601.09236", "authors": ["Chaitanya Kharyal", "Calarina Muslimani", "Matthew E. Taylor"], "title": "Reward Learning through Ranking Mean Squared Error", "comment": null, "summary": "Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., \"bad,\" \"neutral,\" \"good\"). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.", "AI": {"tldr": "\u63d0\u51faR4\u65b9\u6cd5\uff0c\u4f7f\u7528\u6392\u5e8f\u5747\u65b9\u8bef\u5dee\u635f\u5931\u4ece\u4eba\u7c7b\u8bc4\u5206\u4e2d\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u76f8\u6bd4\u73b0\u6709\u8bc4\u5206\u548c\u504f\u597d\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\u4e14\u9700\u8981\u66f4\u5c11\u53cd\u9988\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u8bbe\u8ba1\u662f\u4e00\u4e2a\u74f6\u9888\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u3002\u867d\u7136\u5df2\u6709\u4ece\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u5956\u52b1\u7684\u65b9\u6cd5\uff0c\u4f46\u8bc4\u5206\u53cd\u9988\u6bd4\u4e8c\u5143\u504f\u597d\u66f4\u4e30\u5bcc\u4e14\u8ba4\u77e5\u8d1f\u62c5\u66f4\u5c0f\u3002\u73b0\u6709\u8bc4\u5206\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faR4\u65b9\u6cd5\uff0c\u4f7f\u7528\u6392\u5e8f\u5747\u65b9\u8bef\u5dee\u635f\u5931\u3002\u4ece\u8f68\u8ff9-\u8bc4\u5206\u5bf9\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\uff0c\u6bcf\u4e2a\u8f68\u8ff9\u6709\u79bb\u6563\u8bc4\u5206\u3002\u8bad\u7ec3\u65f6\u91c7\u6837\u8f68\u8ff9\u96c6\uff0c\u9884\u6d4b\u56de\u62a5\uff0c\u4f7f\u7528\u53ef\u5fae\u6392\u5e8f\u7b97\u5b50\u83b7\u5f97\u8f6f\u6392\u5e8f\uff0c\u7136\u540e\u4f18\u5316\u8f6f\u6392\u5e8f\u4e0e\u6559\u5e08\u8bc4\u5206\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\u3002", "result": "\u5728OpenAI Gym\u548cDeepMind Control Suite\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u6a21\u62df\u4eba\u7c7b\u53cd\u9988\uff0cR4\u59cb\u7ec8\u5339\u914d\u6216\u4f18\u4e8e\u73b0\u6709\u8bc4\u5206\u548c\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u9700\u8981\u663e\u8457\u66f4\u5c11\u7684\u53cd\u9988\u3002", "conclusion": "R4\u65b9\u6cd5\u901a\u8fc7\u65b0\u9896\u7684\u6392\u5e8f\u5747\u65b9\u8bef\u5dee\u635f\u5931\uff0c\u4ece\u8bc4\u5206\u53cd\u9988\u4e2d\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u4f9b\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u53cd\u9988\u6548\u7387\u9ad8\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09141", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09141", "abs": "https://arxiv.org/abs/2601.09141", "authors": ["Miao Zhang", "Kelly Chen", "Md Mehrab Tanjim", "Rumi Chunara"], "title": "Identity-Robust Language Model Generation via Content Integrity Preservation", "comment": null, "summary": "Large Language Model (LLM) outputs often vary across user sociodemographic attributes, leading to disparities in factual accuracy, utility, and safety, even for objective questions where demographic information is irrelevant. Unlike prior work on stereotypical or representational bias, this paper studies identity-dependent degradation of core response quality. We show empirically that such degradation arises from biased generation behavior, despite factual knowledge being robustly encoded across identities. Motivated by this mismatch, we propose a lightweight, training-free framework for identity-robust generation that selectively neutralizes non-critical identity information while preserving semantically essential attributes, thus maintaining output content integrity. Experiments across four benchmarks and 18 sociodemographic identities demonstrate an average 77% reduction in identity-dependent bias compared to vanilla prompting and a 45% reduction relative to prompt-based defenses. Our work addresses a critical gap in mitigating the impact of user identity cues in prompts on core generation quality.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4e2d\u548c\u975e\u5173\u952e\u8eab\u4efd\u4fe1\u606f\u6765\u51cf\u5c11LLM\u8f93\u51fa\u4e2d\u7684\u8eab\u4efd\u4f9d\u8d56\u504f\u5dee\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51cf\u5c1177%\u7684\u504f\u5dee", "motivation": "LLM\u8f93\u51fa\u8d28\u91cf\u4f1a\u56e0\u7528\u6237\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\u7b49\uff09\u800c\u51fa\u73b0\u5dee\u5f02\uff0c\u5373\u4f7f\u5bf9\u4e8e\u5ba2\u89c2\u95ee\u9898\u4e5f\u662f\u5982\u6b64\u3002\u8fd9\u79cd\u8eab\u4efd\u4f9d\u8d56\u7684\u8d28\u91cf\u9000\u5316\u4e0d\u540c\u4e8e\u523b\u677f\u5370\u8c61\u6216\u4ee3\u8868\u6027\u504f\u89c1\uff0c\u800c\u662f\u6838\u5fc3\u54cd\u5e94\u8d28\u91cf\u7684\u4e0b\u964d", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u9009\u62e9\u6027\u4e2d\u548c\u975e\u5173\u952e\u8eab\u4efd\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\u5173\u952e\u5c5e\u6027\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u4e8b\u5b9e\u77e5\u8bc6\u5728\u4e0d\u540c\u8eab\u4efd\u95f4\u662f\u7a33\u5065\u7f16\u7801\u7684\uff0c\u4f46\u751f\u6210\u884c\u4e3a\u5b58\u5728\u504f\u5dee", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c18\u4e2a\u793e\u4f1a\u4eba\u53e3\u8eab\u4efd\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u666e\u901a\u63d0\u793a\u5e73\u5747\u51cf\u5c1177%\u7684\u8eab\u4efd\u4f9d\u8d56\u504f\u5dee\uff0c\u76f8\u6bd4\u57fa\u4e8e\u63d0\u793a\u7684\u9632\u5fa1\u65b9\u6cd5\u51cf\u5c1145%\u7684\u504f\u5dee", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u7528\u6237\u8eab\u4efd\u7ebf\u7d22\u5bf9\u6838\u5fc3\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u91cd\u8981\u7a7a\u767d\u3002\u8eab\u4efd\u4fe1\u606f\u7684\u9009\u62e9\u6027\u4e2d\u548c\u80fd\u4fdd\u6301\u8f93\u51fa\u5185\u5bb9\u5b8c\u6574\u6027", "topic": "agent analysis"}}
{"id": "2601.09253", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09253", "abs": "https://arxiv.org/abs/2601.09253", "authors": ["Zehua Liu", "Shuqi Liu", "Tao Zhong", "Mingxuan Yuan"], "title": "RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning", "comment": null, "summary": "While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.", "AI": {"tldr": "RIFT\uff08Reward Informed Fine-Tuning\uff09\u662f\u4e00\u79cd\u5229\u7528\u6240\u6709\u81ea\u751f\u6210\u6837\u672c\u8fdb\u884cLLM\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u52a0\u6743\u635f\u5931\u4ece\u6b63\u8d1f\u8f68\u8ff9\u4e2d\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86SFT\u4f9d\u8d56\u4e13\u5bb6\u6570\u636e\u548cRFT\u4e22\u5f03\u8d1f\u6837\u672c\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u6548\u7387\u95ee\u9898\uff1aSFT\u4f9d\u8d56\u6602\u8d35\u7684\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u800cRFT\u4e22\u5f03\u4e86\u6709\u4ef7\u503c\u7684\u8d1f\u6837\u672c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5145\u5206\u5229\u7528\u81ea\u751f\u6210\u6570\u636e\uff08\u5305\u62ec\u6b63\u8d1f\u6837\u672c\uff09\u7684\u9ad8\u6548\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRIFT\u6846\u67b6\uff0c\u4e0d\u540c\u4e8eRFT\u7684\u786c\u9608\u503c\u65b9\u6cd5\uff0cRIFT\u91cd\u65b0\u5229\u7528\u8d1f\u8f68\u8ff9\uff0c\u901a\u8fc7\u6807\u91cf\u5956\u52b1\u5bf9\u635f\u5931\u8fdb\u884c\u52a0\u6743\uff0c\u4ece\u6a21\u578b\u8f93\u51fa\u7684\u6b63\u8d1f\u8f68\u8ff9\u4e2d\u5b66\u4e60\u3002\u4e3a\u907f\u514d\u6734\u7d20\u5956\u52b1\u96c6\u6210\u5bfc\u81f4\u7684\u8bad\u7ec3\u5d29\u6e83\uff08\u76f4\u63a5\u4e58\u6cd5\u4ea7\u751f\u65e0\u754c\u635f\u5931\uff09\uff0c\u5f15\u5165\u4e86\u7a33\u5b9a\u7684\u635f\u5931\u516c\u5f0f\u786e\u4fdd\u6570\u503c\u9c81\u68d2\u6027\u548c\u4f18\u5316\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u4e0a\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cRIFT\u59cb\u7ec8\u4f18\u4e8eRFT\u3002\u7ed3\u679c\u8868\u660eRIFT\u662f\u4f7f\u7528\u6df7\u5408\u8d28\u91cf\u81ea\u751f\u6210\u6570\u636e\u8fdb\u884c\u5bf9\u9f50\u7684\u9c81\u68d2\u4e14\u6570\u636e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "RIFT\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u52a0\u6743\u635f\u5931\u5145\u5206\u5229\u7528\u6240\u6709\u81ea\u751f\u6210\u6837\u672c\uff0c\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6570\u636e\u6548\u7387\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u8d28\u91cf\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u9c81\u68d2\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09215", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09215", "abs": "https://arxiv.org/abs/2601.09215", "authors": ["Feng Zhang", "Shijia Li", "Chunmao Zhang", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Jingwen Xu", "Han Liu"], "title": "UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning", "comment": null, "summary": "User simulators serve as the critical interactive environment for agent post-training, and an ideal user simulator generalizes across domains and proactively engages in negotiation by challenging or bargaining. However, current methods exhibit two issues. They rely on static and context-unaware profiles, necessitating extensive manual redesign for new scenarios, thus limiting generalizability. Moreover, they neglect human strategic thinking, leading to vulnerability to agent manipulation. To address these issues, we propose UserLM-R1, a novel user language model with reasoning capability. Specifically, we first construct comprehensive user profiles with both static roles and dynamic scenario-specific goals for adaptation to diverse scenarios. Then, we propose a goal-driven decision-making policy to generate high-quality rationales before producing responses, and further refine the reasoning and improve strategic capabilities with supervised fine-tuning and multi-reward reinforcement learning. Extensive experimental results demonstrate that UserLM-R1 outperforms competitive baselines, particularly on the more challenging adversarial set.", "AI": {"tldr": "UserLM-R1\uff1a\u5177\u6709\u63a8\u7406\u80fd\u529b\u7684\u7528\u6237\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u7528\u6237\u914d\u7f6e\u548c\u76ee\u6807\u9a71\u52a8\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u63d0\u5347\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u548c\u6218\u7565\u8c08\u5224\u80fd\u529b", "motivation": "\u5f53\u524d\u7528\u6237\u6a21\u62df\u5668\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u9759\u6001\u3001\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u7528\u6237\u914d\u7f6e\uff0c\u9700\u8981\u5927\u91cf\u624b\u52a8\u8c03\u6574\u4ee5\u9002\u5e94\u65b0\u573a\u666f\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b2\uff09\u5ffd\u89c6\u4eba\u7c7b\u7684\u6218\u7565\u601d\u7ef4\uff0c\u5bb9\u6613\u88ab\u667a\u80fd\u4f53\u64cd\u7eb5\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u9886\u57df\u6cdb\u5316\u5e76\u4e3b\u52a8\u53c2\u4e0e\u8c08\u5224\u7684\u7528\u6237\u6a21\u62df\u5668\u3002", "method": "\u63d0\u51faUserLM-R1\u7528\u6237\u8bed\u8a00\u6a21\u578b\uff1a1\uff09\u6784\u5efa\u5305\u542b\u9759\u6001\u89d2\u8272\u548c\u52a8\u6001\u573a\u666f\u7279\u5b9a\u76ee\u6807\u7684\u7efc\u5408\u7528\u6237\u914d\u7f6e\uff1b2\uff09\u91c7\u7528\u76ee\u6807\u9a71\u52a8\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u5728\u751f\u6210\u54cd\u5e94\u524d\u4ea7\u751f\u9ad8\u8d28\u91cf\u63a8\u7406\uff1b3\uff09\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\u63a8\u7406\u548c\u6218\u7565\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUserLM-R1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "UserLM-R1\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u7528\u6237\u914d\u7f6e\u548c\u76ee\u6807\u9a71\u52a8\u7684\u63a8\u7406\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7528\u6237\u6a21\u62df\u5668\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u548c\u6218\u7565\u8c08\u5224\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.09361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09361", "abs": "https://arxiv.org/abs/2601.09361", "authors": ["Jiaying Zhang", "Lei Shi", "Jiguo Li", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He"], "title": "GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.", "AI": {"tldr": "GeoRA\u662f\u4e00\u79cd\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u7684\u51e0\u4f55\u611f\u77e5\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7SVD\u63d0\u53d6\u4e3b\u65b9\u5411\u5e76\u51bb\u7ed3\u6b8b\u5dee\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5728RLVR\u4e2d\u7684\u51e0\u4f55\u5931\u914d\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff08\u5982PiSSA\u548cMiLoRA\uff09\u4e13\u4e3a\u76d1\u7763\u5fae\u8c03\u8bbe\u8ba1\uff0c\u672a\u8003\u8651RLVR\u72ec\u7279\u7684\u4f18\u5316\u52a8\u6001\u548c\u51e0\u4f55\u7ed3\u6784\uff0c\u76f4\u63a5\u5e94\u7528\u4f1a\u5bfc\u81f4\u8c31\u5d29\u6e83\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\uff0c\u800c\u5229\u7528\u66f4\u65b0\u7a00\u758f\u6027\u7684\u65b9\u6cd5\u5728\u73b0\u4ee3\u786c\u4ef6\u4e0a\u5b58\u5728\u6548\u7387\u74f6\u9888\u3002", "method": "GeoRA\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u5728\u51e0\u4f55\u7ea6\u675f\u5b50\u7a7a\u95f4\u4e2d\u63d0\u53d6\u4e3b\u65b9\u5411\u6765\u521d\u59cb\u5316\u9002\u914d\u5668\uff0c\u540c\u65f6\u51bb\u7ed3\u6b8b\u5dee\u7ec4\u4ef6\uff0c\u5229\u7528RL\u66f4\u65b0\u5b50\u7a7a\u95f4\u7684\u5404\u5411\u5f02\u6027\u548c\u53ef\u538b\u7f29\u6027\uff0c\u4fdd\u6301\u9884\u8bad\u7ec3\u51e0\u4f55\u7ed3\u6784\u5e76\u901a\u8fc7\u5bc6\u96c6\u7b97\u5b50\u5b9e\u73b0\u9ad8\u6548GPU\u8ba1\u7b97\u3002", "result": "\u5728Qwen\u548cLlama\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGeoRA\u7f13\u89e3\u4e86\u51e0\u4f55\u5931\u914d\u5f15\u8d77\u7684\u4f18\u5316\u74f6\u9888\uff0c\u5728\u5173\u952e\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u4f4e\u79e9\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230SOTA\u7ed3\u679c\uff0c\u5e76\u5728\u9886\u57df\u5916\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6297\u707e\u96be\u6027\u9057\u5fd8\u80fd\u529b\u3002", "conclusion": "GeoRA\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u4f4e\u79e9\u9002\u5e94\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09400", "abs": "https://arxiv.org/abs/2601.09400", "authors": ["Olgierd Unold", "Stanis\u0142aw Franczyk"], "title": "Preliminary Tests of the Anticipatory Classifier System with Hindsight Experience Replay", "comment": null, "summary": "This paper introduces ACS2HER, a novel integration of the Anticipatory Classifier System (ACS2) with the Hindsight Experience Replay (HER) mechanism. While ACS2 is highly effective at building cognitive maps through latent learning, its performance often stagnates in environments characterized by sparse rewards. We propose a specific architectural variant that triggers hindsight learning when the agent fails to reach its primary goal, re-labeling visited states as virtual goals to densify the learning signal. The proposed model was evaluated on two benchmarks: the deterministic \\texttt{Maze 6} and the stochastic \\texttt{FrozenLake}. The results demonstrate that ACS2HER significantly accelerates knowledge acquisition and environmental mastery compared to the standard ACS2. However, this efficiency gain is accompanied by increased computational overhead and a substantial expansion in classifier numerosity. This work provides the first analysis of combining anticipatory mechanisms with retrospective goal-relabeling in Learning Classifier Systems.", "AI": {"tldr": "ACS2HER\u5c06Anticipatory Classifier System\u4e0eHindsight Experience Replay\u7ed3\u5408\uff0c\u901a\u8fc7\u5728\u5931\u8d25\u65f6\u91cd\u65b0\u6807\u8bb0\u72b6\u6001\u4e3a\u865a\u62df\u76ee\u6807\u6765\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u52a0\u901f\u4e86\u77e5\u8bc6\u83b7\u53d6\u4f46\u589e\u52a0\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "ACS2\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u6027\u80fd\u505c\u6ede\uff0c\u9700\u8981\u589e\u5f3a\u5176\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u5c06ACS2\u4e0eHER\u673a\u5236\u96c6\u6210\uff0c\u5f53\u4ee3\u7406\u672a\u80fd\u8fbe\u5230\u4e3b\u8981\u76ee\u6807\u65f6\u89e6\u53d1\u4e8b\u540e\u5b66\u4e60\uff0c\u5c06\u8bbf\u95ee\u8fc7\u7684\u72b6\u6001\u91cd\u65b0\u6807\u8bb0\u4e3a\u865a\u62df\u76ee\u6807\u4ee5\u589e\u52a0\u5b66\u4e60\u4fe1\u53f7\u5bc6\u5ea6\u3002", "result": "\u5728Maze 6\u548cFrozenLake\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACS2HER\u663e\u8457\u52a0\u901f\u4e86\u77e5\u8bc6\u83b7\u53d6\u548c\u73af\u5883\u638c\u63e1\uff0c\u4f46\u589e\u52a0\u4e86\u8ba1\u7b97\u5f00\u9500\u548c\u5206\u7c7b\u5668\u6570\u91cf\u3002", "conclusion": "\u9996\u6b21\u5206\u6790\u4e86\u5b66\u4e60\u5206\u7c7b\u5668\u7cfb\u7edf\u4e2d\u524d\u77bb\u673a\u5236\u4e0e\u56de\u987e\u76ee\u6807\u91cd\u6807\u8bb0\u7684\u7ed3\u5408\uff0c\u8bc1\u660e\u4e86\u6548\u7387\u63d0\u5347\u4f46\u9700\u6743\u8861\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09398", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09398", "abs": "https://arxiv.org/abs/2601.09398", "authors": ["Songyao Jin", "Kun Zhou", "Wenqi Li", "Peng Wang", "Biwei Huang"], "title": "Ability Transfer and Recovery via Modularized Parameters Localization", "comment": null, "summary": "Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.", "AI": {"tldr": "ACT\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6fc0\u6d3b\u5dee\u5f02\u5b9a\u4f4d\u80fd\u529b\u76f8\u5173\u901a\u9053\uff0c\u9009\u62e9\u6027\u8f6c\u79fb\u53c2\u6570\u5e76\u8f7b\u91cf\u5fae\u8c03\uff0c\u89e3\u51b3LLM\u4e13\u4e1a\u5316\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u80fd\u529b\u6574\u5408\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u3001\u8bed\u8a00\u6216\u6280\u80fd\u4e0a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u6216\u5fae\u8c03\u4f1a\u964d\u4f4e\u5176\u4ed6\u80fd\u529b\uff0c\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002\u9700\u8981\u7814\u7a76\u80fd\u529b\u5728\u53c2\u6570\u4e2d\u7684\u5206\u5e03\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u4e13\u4e1a\u5316\u4e0e\u901a\u7528\u6027\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "ACT\uff08\u6fc0\u6d3b\u5f15\u5bfc\u7684\u901a\u9053\u7ea7\u80fd\u529b\u8f6c\u79fb\uff09\uff1a1\uff09\u5206\u6790\u6a21\u5757\u6fc0\u6d3b\u5dee\u5f02\u5b9a\u4f4d\u80fd\u529b\u76f8\u5173\u901a\u9053\uff08\u901a\u5e38<5%\uff09\uff1b2\uff09\u9009\u62e9\u6027\u8f6c\u79fb\u5bf9\u5e94\u53c2\u6570\uff1b3\uff09\u8f7b\u91cf\u5fae\u8c03\u786e\u4fdd\u517c\u5bb9\u6027\u3002", "result": "\u5728\u591a\u8bed\u8a00\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cACT\u80fd\u6062\u590d\u9057\u5fd8\u7684\u80fd\u529b\u540c\u65f6\u4fdd\u7559\u5df2\u6709\u6280\u80fd\uff0c\u8fd8\u80fd\u5c06\u591a\u4e2a\u4e13\u4e1a\u5316\u6a21\u578b\u7684\u80fd\u529b\u6574\u5408\u5230\u5355\u4e00\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u6700\u5c0f\u5e72\u6270\u3002", "conclusion": "LLM\u80fd\u529b\u9ad8\u5ea6\u96c6\u4e2d\u5728\u5c11\u91cf\u901a\u9053\u4e2d\u4e14\u53ef\u89e3\u8026\uff0cACT\u65b9\u6cd5\u901a\u8fc7\u901a\u9053\u7ea7\u53c2\u6570\u8f6c\u79fb\u6709\u6548\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u591a\u80fd\u529b\u6a21\u578b\u6574\u5408\uff0c\u4e3a\u6a21\u578b\u4e13\u4e1a\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2601.09626", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.09626", "abs": "https://arxiv.org/abs/2601.09626", "authors": ["Ge Lei", "Ferran Brosa Planella", "Sterling G. Baird", "Samuel J. Cooper"], "title": "From Prompt to Protocol: Fast Charging Batteries with Large Language Models", "comment": null, "summary": "Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which simply writes an explicit function for the current and its scalar parameters. Across our case studies, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search. In a realistic fast charging scenario, both P2O and P2P yield around a 4.2 percent improvement in state of health (capacity retention based health metric under fast charging cycling) over a state-of-the-art multi-step constant current (CC) baseline, with P2P achieving this under matched evaluation budgets (same number of protocol evaluations). These results demonstrate that LLMs can expand the space of protocol functional forms, incorporate language-based constraints, and enable efficient optimization in high cost experimental settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eLLM\u7684\u7535\u6c60\u5145\u7535\u534f\u8bae\u4f18\u5316\u65b9\u6cd5\uff1aP2O\uff08\u63d0\u793a\u5230\u4f18\u5316\u5668\uff09\u548cP2P\uff08\u63d0\u793a\u5230\u534f\u8bae\uff09\uff0c\u901a\u8fc7LLM\u751f\u6210\u534f\u8bae\u4ee3\u7801\u6216\u51fd\u6570\uff0c\u5728\u5b9e\u9a8c\u6210\u672c\u9ad8\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002", "motivation": "\u7535\u6c60\u5145\u7535\u534f\u8bae\u4f18\u5316\u9762\u4e34\u6311\u6218\uff1a\u8bc4\u4f30\u8fc7\u7a0b\u7f13\u6162\u3001\u6210\u672c\u9ad8\u3001\u4e0d\u53ef\u5fae\u5206\u3002\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\uff0c\u9650\u5236\u4e86\u534f\u8bae\u591a\u6837\u6027\uff0c\u963b\u788d\u4e86\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u73b0\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u68af\u5ea6\u65e0\u5173\u7684LLM\u9a71\u52a8\u95ed\u73af\u65b9\u6cd5\uff1a1) P2O\uff1a\u4f7f\u7528LLM\u751f\u6210\u5c0f\u578b\u795e\u7ecf\u7f51\u7edc\u534f\u8bae\u4ee3\u7801\uff0c\u901a\u8fc7\u5185\u5faa\u73af\u8bad\u7ec3\uff1b2) P2P\uff1a\u4f7f\u7528LLM\u76f4\u63a5\u7f16\u5199\u7535\u6d41\u51fd\u6570\u53ca\u5176\u6807\u91cf\u53c2\u6570\u3002", "result": "\u5728\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cLLM\u5f15\u5bfc\u7684P2O\u4f18\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u3001\u8fdb\u5316\u7b97\u6cd5\u548c\u968f\u673a\u641c\u7d22\u8bbe\u8ba1\u7684\u795e\u7ecf\u7f51\u7edc\u3002\u5728\u5feb\u901f\u5145\u7535\u573a\u666f\u4e2d\uff0cP2O\u548cP2P\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u591a\u6b65\u6052\u6d41\u57fa\u7ebf\uff0c\u5728\u72b6\u6001\u5065\u5eb7\u5ea6\u4e0a\u5b9e\u73b0\u7ea64.2%\u7684\u6539\u8fdb\uff0cP2P\u5728\u76f8\u540c\u8bc4\u4f30\u9884\u7b97\u4e0b\u8fbe\u5230\u6b64\u6548\u679c\u3002", "conclusion": "LLM\u80fd\u591f\u6269\u5c55\u534f\u8bae\u51fd\u6570\u5f62\u5f0f\u7a7a\u95f4\uff0c\u6574\u5408\u57fa\u4e8e\u8bed\u8a00\u7684\u7ea6\u675f\uff0c\u5728\u5b9e\u9a8c\u6210\u672c\u9ad8\u7684\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002", "topic": "code agent"}}
{"id": "2601.09570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09570", "abs": "https://arxiv.org/abs/2601.09570", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "Dialogue Telemetry: Turn-Level Instrumentation for Autonomous Information Gathering", "comment": "16 pages, 9 Figures, Version submitted to IEEE for publication", "summary": "Autonomous systems conducting schema-grounded information-gathering dialogues face an instrumentation gap, lacking turn-level observables for monitoring acquisition efficiency and detecting when questioning becomes unproductive. We introduce Dialogue Telemetry (DT), a measurement framework that produces two model-agnostic signals after each question-answer exchange: (i) a Progress Estimator (PE) quantifying residual information potential per category (with a bits-based variant), and (ii) a Stalling Index (SI) detecting an observable failure signature characterized by repeated category probing with semantically similar, low-marginal-gain responses. SI flags this pattern without requiring causal diagnosis, supporting monitoring in settings where attributing degradation to specific causes may be impractical. We validate DT in controlled search-and-rescue (SAR)-inspired interviews using large language model (LLM)-based simulations, distinguishing efficient from stalled dialogue traces and illustrating downstream utility by integrating DT signals into a reinforcement learning (RL) policy. Across these settings, DT provides interpretable turn-level instrumentation that improves policy performance when stalling carries operational costs.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u8bdd\u9065\u6d4b(DT)\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u6d4b\u4fe1\u606f\u6536\u96c6\u5bf9\u8bdd\u7684\u6548\u7387\uff0c\u5305\u542b\u8fdb\u5ea6\u4f30\u8ba1\u5668\u548c\u505c\u6ede\u6307\u6570\u4e24\u4e2a\u6a21\u578b\u65e0\u5173\u4fe1\u53f7\uff0c\u5728\u641c\u6551\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u8fdb\u884c\u57fa\u4e8e\u6a21\u5f0f\u7684\u4fe1\u606f\u6536\u96c6\u5bf9\u8bdd\u65f6\u7f3a\u4e4f\u56de\u5408\u7ea7\u522b\u7684\u53ef\u89c2\u6d4b\u6307\u6807\uff0c\u65e0\u6cd5\u76d1\u63a7\u83b7\u53d6\u6548\u7387\u548c\u68c0\u6d4b\u4f55\u65f6\u63d0\u95ee\u53d8\u5f97\u65e0\u6548\u3002", "method": "\u5f15\u5165\u5bf9\u8bdd\u9065\u6d4b(DT)\u6846\u67b6\uff0c\u5305\u542b\uff1a1)\u8fdb\u5ea6\u4f30\u8ba1\u5668(PE)\u91cf\u5316\u6bcf\u4e2a\u7c7b\u522b\u7684\u5269\u4f59\u4fe1\u606f\u6f5c\u529b\uff1b2)\u505c\u6ede\u6307\u6570(SI)\u68c0\u6d4b\u91cd\u590d\u7c7b\u522b\u63a2\u6d4b\u3001\u8bed\u4e49\u76f8\u4f3c\u3001\u4f4e\u8fb9\u9645\u589e\u76ca\u54cd\u5e94\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5728\u641c\u6551\u573a\u666f\u7684LLM\u6a21\u62df\u4e2d\u9a8c\u8bc1DT\u80fd\u533a\u5206\u9ad8\u6548\u4e0e\u505c\u6ede\u5bf9\u8bdd\u8f68\u8ff9\uff0c\u5c06DT\u4fe1\u53f7\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4e2d\u53ef\u6539\u5584\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "DT\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u56de\u5408\u7ea7\u522b\u76d1\u6d4b\u5de5\u5177\uff0c\u5728\u505c\u6ede\u5e26\u6765\u64cd\u4f5c\u6210\u672c\u65f6\u80fd\u6539\u5584\u7b56\u7565\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.09609", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09609", "abs": "https://arxiv.org/abs/2601.09609", "authors": ["Qian Cao", "Yahui Liu", "Wei Bi", "Yi Zhao", "Ruihua Song", "Xiting Wang", "Ruiming Tang", "Guorui Zhou", "Han Li"], "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing", "comment": null, "summary": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u534a\u7ed3\u6784\u5316\u957f\u601d\u7ef4\u94fe\u7684RL\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u89c4\u5212\u5206\u652f\u548c\u7fa4\u4f53\u611f\u77e5\u591a\u6837\u6027\u5956\u52b1\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347LLM\u8f93\u51fa\u7684\u591a\u6837\u6027\u3002", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLM\u589e\u5f3a\u901a\u5e38\u4f1a\u5bfc\u81f4\u8f93\u51fa\u591a\u6837\u6027\u964d\u4f4e\uff0c\u5f71\u54cd\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\uff08\u5982\u521b\u610f\u5199\u4f5c\uff09\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u7684\u591a\u6837\u5316\u63a2\u7d22\u673a\u5236\uff0c\u8fc7\u4e8e\u5173\u6ce8\u4f18\u5316\u6548\u7387\u548c\u6027\u80fd\u800c\u5ffd\u89c6\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u534a\u7ed3\u6784\u5316\u957f\u601d\u7ef4\u94fe\u7684RL\u6846\u67b6\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u660e\u786e\u89c4\u5212\u7684\u4e2d\u95f4\u6b65\u9aa4\u3002\u5f15\u5165\u591a\u6837\u5316\u89c4\u5212\u5206\u652f\u65b9\u6cd5\uff0c\u5728\u89c4\u5212\u9636\u6bb5\u57fa\u4e8e\u591a\u6837\u6027\u53d8\u5316\u7b56\u7565\u6027\u5730\u5f15\u5165\u5206\u6b67\uff0c\u540c\u65f6\u4f7f\u7528\u7fa4\u4f53\u611f\u77e5\u591a\u6837\u6027\u5956\u52b1\u6765\u9f13\u52b1\u4e0d\u540c\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u521b\u610f\u5199\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8f93\u51fa\u591a\u6837\u6027\u800c\u4e0d\u635f\u5bb3\u751f\u6210\u8d28\u91cf\uff0c\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u89c4\u5212\u548c\u591a\u6837\u5316\u63a2\u7d22\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347LLM\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u7684\u8f93\u51fa\u591a\u6837\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09688", "abs": "https://arxiv.org/abs/2601.09688", "authors": ["Yibo Wang", "Lei Wang", "Yue Deng", "Keming Wu", "Yao Xiao", "Huanjin Yao", "Liwei Kang", "Hai Ye", "Yongcheng Jing", "Lidong Bing"], "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "comment": "Source code: https://github.com/Infinity-AILab/DeepResearchEval", "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "AI": {"tldr": "DeepResearchEval\uff1a\u4e00\u4e2a\u7528\u4e8e\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u6784\u5efa\u548c\u667a\u80fd\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u9a71\u52a8\u751f\u6210\u590d\u6742\u7814\u7a76\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u8d28\u91cf\u8bc4\u4f30\u548c\u4e3b\u52a8\u4e8b\u5b9e\u6838\u67e5\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u8bc4\u4f30\u5b58\u5728\u6311\u6218\uff1a\u4efb\u52a1\u6784\u5efa\u9700\u8981\u5927\u91cf\u6807\u6ce8\u3001\u4f9d\u8d56\u9759\u6001\u8bc4\u4f30\u7ef4\u5ea6\u3001\u5728\u7f3a\u5c11\u5f15\u7528\u65f6\u65e0\u6cd5\u53ef\u9760\u9a8c\u8bc1\u4e8b\u5b9e\u3002\u9700\u8981\u81ea\u52a8\u5316\u6846\u67b6\u6765\u8bc4\u4f30\u591a\u6b65\u9aa4\u7f51\u7edc\u7814\u7a76\u3001\u5206\u6790\u548c\u8de8\u6e90\u5408\u6210\u80fd\u529b\u3002", "method": "1. \u4efb\u52a1\u6784\u5efa\uff1a\u91c7\u7528\u89d2\u8272\u9a71\u52a8\u7ba1\u9053\u751f\u6210\u57fa\u4e8e\u591a\u6837\u5316\u7528\u6237\u914d\u7f6e\u7684\u73b0\u5b9e\u590d\u6742\u7814\u7a76\u4efb\u52a1\uff0c\u5e94\u7528\u4e24\u9636\u6bb5\u8fc7\u6ee4\uff08\u4efb\u52a1\u8d44\u683c\u548c\u641c\u7d22\u5fc5\u8981\u6027\uff09\u4fdd\u7559\u9700\u8981\u591a\u6e90\u8bc1\u636e\u6574\u5408\u548c\u5916\u90e8\u68c0\u7d22\u7684\u4efb\u52a1\u30022. \u8bc4\u4f30\uff1a\u91c7\u7528\u667a\u80fd\u7ba1\u9053\uff0c\u5305\u542b\u81ea\u9002\u5e94\u70b9\u5f0f\u8d28\u91cf\u8bc4\u4f30\uff08\u52a8\u6001\u63a8\u5bfc\u4efb\u52a1\u7279\u5b9a\u8bc4\u4f30\u7ef4\u5ea6\u3001\u6807\u51c6\u548c\u6743\u91cd\uff09\u548c\u4e3b\u52a8\u4e8b\u5b9e\u6838\u67e5\uff08\u81ea\u4e3b\u63d0\u53d6\u548c\u901a\u8fc7\u7f51\u7edc\u641c\u7d22\u9a8c\u8bc1\u62a5\u544a\u9648\u8ff0\uff0c\u5373\u4f7f\u7f3a\u5c11\u5f15\u7528\uff09\u3002", "result": "\u63d0\u51fa\u4e86DeepResearchEval\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u751f\u6210\u9700\u8981\u6df1\u5ea6\u7814\u7a76\u7684\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u52a8\u6001\u3001\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "DeepResearchEval\u4e3a\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u3001\u81ea\u52a8\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u9a71\u52a8\u7684\u4efb\u52a1\u751f\u6210\u548c\u667a\u80fd\u8bc4\u4f30\u7ba1\u9053\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u7cfb\u7edf\u7684\u591a\u6b65\u9aa4\u7814\u7a76\u3001\u5206\u6790\u548c\u8de8\u6e90\u5408\u6210\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.09692", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09692", "abs": "https://arxiv.org/abs/2601.09692", "authors": ["Tianyi Niu", "Justin Chih-Yao Chen", "Genta Indra Winata", "Shi-Xiong Zhang", "Supriyo Chakraborty", "Sambit Sahu", "Yue Zhang", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection", "comment": "Code: https://github.com/tianyiniu/RoutingGenData", "summary": "Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRGD\uff08\u4f7f\u7528\u751f\u6210\u6570\u636e\u7684\u8def\u7531\uff09\u8bbe\u7f6e\uff0c\u5176\u4e2d\u8def\u7531\u5668\u4ec5\u4f7f\u7528\u751f\u6210\u5668LLM\u4ece\u9ad8\u7ea7\u4efb\u52a1\u63cf\u8ff0\u751f\u6210\u7684\u67e5\u8be2\u548c\u7b54\u6848\u8fdb\u884c\u8bad\u7ec3\u3002\u7814\u7a76\u53d1\u73b0\u67e5\u8be2-\u7b54\u6848\u8def\u7531\u5668\u6bd4\u4ec5\u67e5\u8be2\u8def\u7531\u5668\u5bf9\u751f\u6210\u5668\u8d28\u91cf\u4e0b\u964d\u66f4\u654f\u611f\uff0c\u5e76\u63d0\u51fa\u4e86CASCAL\u8fd9\u4e00\u66f4\u9c81\u68d2\u7684\u4ec5\u67e5\u8be2\u8def\u7531\u5668\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u8def\u7531\u5668\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u53ef\u4ee5\u8bbf\u95ee\u771f\u5b9e\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\uff0c\u7279\u522b\u662f\u5f53\u7528\u6237\u8bf7\u6c42\u5206\u5e03\u5f02\u6784\u4e14\u672a\u77e5\u65f6\uff0c\u8fd9\u79cd\u6570\u636e\u5f80\u5f80\u4e0d\u53ef\u7528\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u4ec5\u4f7f\u7528\u751f\u6210\u6570\u636e\u8bad\u7ec3\u8def\u7531\u5668\u7684\u6311\u6218\u6027\u8bbe\u7f6e\u3002", "method": "\u63d0\u51faRGD\u8bbe\u7f6e\uff0c\u8bc4\u4f30\u67e5\u8be2-\u7b54\u6848\u8def\u7531\u5668\u548c\u4ec5\u67e5\u8be2\u8def\u7531\u5668\u3002\u5206\u6790\u53d1\u73b0\u6709\u6548\u751f\u6210\u5668\u7684\u4e24\u4e2a\u5173\u952e\u7279\u5f81\uff1a1\uff09\u80fd\u51c6\u786e\u56de\u7b54\u81ea\u5df1\u751f\u6210\u7684\u95ee\u9898\uff1b2\uff09\u95ee\u9898\u80fd\u5728\u6a21\u578b\u6c60\u4e2d\u4ea7\u751f\u8db3\u591f\u7684\u6027\u80fd\u5dee\u5f02\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51faCASCAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u8bc6\u6295\u7968\u4f30\u8ba1\u6a21\u578b\u6b63\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u5c42\u6b21\u805a\u7c7b\u8bc6\u522b\u6a21\u578b\u7279\u5b9a\u6280\u80fd\u9886\u57df\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c12\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u67e5\u8be2-\u7b54\u6848\u8def\u7531\u5668\u6bd4\u4ec5\u67e5\u8be2\u8def\u7531\u5668\u5bf9\u751f\u6210\u5668\u8d28\u91cf\u4e0b\u964d\u66f4\u654f\u611f\u3002CASCAL\u65b9\u6cd5\u5bf9\u751f\u6210\u5668\u8d28\u91cf\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5728\u5f31\u751f\u6210\u5668\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\uff0c\u6bd4\u6700\u4f73\u67e5\u8be2-\u7b54\u6848\u8def\u7531\u5668\u7edd\u5bf9\u51c6\u786e\u7387\u9ad84.6%\u3002", "conclusion": "\u4ec5\u4f7f\u7528\u751f\u6210\u6570\u636e\u8bad\u7ec3LLM\u8def\u7531\u5668\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u8bbe\u8ba1\u8def\u7531\u5668\u67b6\u6784\u3002\u67e5\u8be2-\u7b54\u6848\u8def\u7531\u5668\u5bf9\u751f\u6210\u5668\u8d28\u91cf\u654f\u611f\uff0c\u800c\u4ec5\u67e5\u8be2\u8def\u7531\u5668\uff08\u7279\u522b\u662fCASCAL\uff09\u5728\u751f\u6210\u5668\u8d28\u91cf\u8f83\u5dee\u65f6\u8868\u73b0\u66f4\u9c81\u68d2\u3002\u6709\u6548\u751f\u6210\u6570\u636e\u7684\u5173\u952e\u662f\u751f\u6210\u5668\u80fd\u51c6\u786e\u56de\u7b54\u81ea\u5df1\u7684\u95ee\u9898\uff0c\u4e14\u95ee\u9898\u80fd\u5145\u5206\u533a\u5206\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.5b7f7a0d", "categories": ["tldr.article"], "pdf": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/nN4lBI_Lt8NsC9JkoB-meY6h87rrO-i1FPqCkeG6E0Q=440", "abs": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/nN4lBI_Lt8NsC9JkoB-meY6h87rrO-i1FPqCkeG6E0Q=440", "authors": ["TLDR Newsletter"], "title": "Anthropic introduced Cowork", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 6 minute read, Links: http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/nN4lBI_Lt8NsC9JkoB-meY6h87rrO-i1FPqCkeG6E0Q=440", "summary": "Anthropic introduced Cowork (6 minute read) Claude Cowork is a simpler version of Claude Code built into the Claude Desktop app. Users can assign a folder for Claude to access and guide it via chat, enabling agent-like workflows without complex setup. The tool is in research preview and currently limited to Max subscribers.", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51faClaude Cowork\uff0c\u8fd9\u662f\u96c6\u6210\u5230Claude\u684c\u9762\u5e94\u7528\u4e2d\u7684\u7b80\u5316\u7248Claude Code\uff0c\u5141\u8bb8\u7528\u6237\u5206\u914d\u6587\u4ef6\u5939\u4f9bClaude\u8bbf\u95ee\u5e76\u901a\u8fc7\u804a\u5929\u5f15\u5bfc\uff0c\u5b9e\u73b0\u65e0\u9700\u590d\u6742\u8bbe\u7f6e\u7684\u7c7b\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u7b80\u5355\u3001\u66f4\u96c6\u6210\u7684\u4ee3\u7801\u534f\u4f5c\u5de5\u5177\uff0c\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u8ba9\u7528\u6237\u65e0\u9700\u590d\u6742\u8bbe\u7f6e\u5373\u53ef\u5b9e\u73b0\u7c7b\u4f3c\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u5c06Claude Code\u7b80\u5316\u7248\u672c\u96c6\u6210\u5230Claude\u684c\u9762\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u53ea\u9700\u5206\u914d\u6587\u4ef6\u5939\u4f9bClaude\u8bbf\u95ee\uff0c\u901a\u8fc7\u804a\u5929\u754c\u9762\u5f15\u5bfcClaude\u8fdb\u884c\u4ee3\u7801\u76f8\u5173\u64cd\u4f5c\uff0c\u5b9e\u73b0\u7c7b\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u521b\u5efa\u4e86Claude Cowork\u5de5\u5177\uff0c\u76ee\u524d\u5904\u4e8e\u7814\u7a76\u9884\u89c8\u9636\u6bb5\uff0c\u4ec5\u9650Max\u8ba2\u9605\u7528\u6237\u4f7f\u7528\uff0c\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u76f4\u63a5\u7684\u4ee3\u7801\u534f\u4f5c\u4f53\u9a8c\u3002", "conclusion": "Claude Cowork\u901a\u8fc7\u7b80\u5316\u8bbe\u7f6e\u548c\u96c6\u6210\u5230\u684c\u9762\u5e94\u7528\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u4fbf\u6377\u7684\u4ee3\u7801\u534f\u4f5c\u5de5\u5177\uff0c\u964d\u4f4e\u4e86\u4f7f\u7528\u95e8\u69db\uff0c\u4f46\u76ee\u524d\u529f\u80fd\u6709\u9650\u4e14\u4ec5\u9762\u5411\u7279\u5b9a\u7528\u6237\u7fa4\u4f53\u3002", "topic": "code agent"}}
{"id": "tldr.2601.7a0ce9a4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fwhen-ai-writes-almost-all-code-what%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/fO_s9QKIPj8OrGeNiUD71BsHUCmsf_pgLSDnEexLbio=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fwhen-ai-writes-almost-all-code-what%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/fO_s9QKIPj8OrGeNiUD71BsHUCmsf_pgLSDnEexLbio=440", "authors": ["TLDR Newsletter"], "title": "When AI writes almost all code, what happens to software engineering?", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fwhen-ai-writes-almost-all-code-what%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/fO_s9QKIPj8OrGeNiUD71BsHUCmsf_pgLSDnEexLbio=440", "summary": "When AI writes almost all code, what happens to software engineering? (12 minute read) AI models like Opus 4.5 and GPT-5.2 now write most software code, prompting a shift in software engineering practices. This reduces the value of language expertise and routine coding tasks while increasing the demand for tech leads who are product-minded. Although AI can handle more of the coding workload, engineers will need to oversee complex tasks, emphasizing a hybrid skill set in both product managemen...", "source": "tldr", "AI": {"tldr": "AI\u6a21\u578b\u73b0\u5728\u80fd\u7f16\u5199\u5927\u90e8\u5206\u8f6f\u4ef6\u4ee3\u7801\uff0c\u5bfc\u81f4\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u53d1\u751f\u8f6c\u53d8\uff0c\u964d\u4f4e\u4e86\u8bed\u8a00\u4e13\u4e1a\u77e5\u8bc6\u548c\u5e38\u89c4\u7f16\u7801\u4efb\u52a1\u7684\u4ef7\u503c\uff0c\u540c\u65f6\u589e\u52a0\u4e86\u5bf9\u5177\u5907\u4ea7\u54c1\u601d\u7ef4\u7684tech lead\u7684\u9700\u6c42\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\uff08\u5982Opus 4.5\u548cGPT-5.2\uff09\u80fd\u591f\u7f16\u5199\u5927\u90e8\u5206\u8f6f\u4ef6\u4ee3\u7801\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u6b63\u5728\u7ecf\u5386\u6839\u672c\u6027\u53d8\u9769\u3002\u4f20\u7edf\u7f16\u7801\u6280\u80fd\u7684\u4ef7\u503c\u6b63\u5728\u4e0b\u964d\uff0c\u800c\u65b0\u7684\u6280\u80fd\u9700\u6c42\u6b63\u5728\u51fa\u73b0\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u73b0\u72b6\uff0c\u63a2\u8ba8AI\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5bf9\u5de5\u7a0b\u5e08\u89d2\u8272\u548c\u6280\u80fd\u8981\u6c42\u7684\u53d8\u5316\u3002", "result": "AI\u63a5\u7ba1\u4e86\u5927\u90e8\u5206\u7f16\u7801\u5de5\u4f5c\uff0c\u51cf\u5c11\u4e86\u8bed\u8a00\u4e13\u4e1a\u77e5\u8bc6\u548c\u5e38\u89c4\u7f16\u7801\u4efb\u52a1\u7684\u4ef7\u503c\uff0c\u4f46\u589e\u52a0\u4e86\u5bf9\u5177\u5907\u4ea7\u54c1\u601d\u7ef4\u7684\u6280\u672f\u9886\u5bfc\u8005\u7684\u9700\u6c42\u3002\u5de5\u7a0b\u5e08\u9700\u8981\u76d1\u7763\u590d\u6742\u4efb\u52a1\uff0c\u5f3a\u8c03\u4ea7\u54c1\u7ba1\u7406\u548c\u6280\u672f\u76d1\u7763\u7684\u6df7\u5408\u6280\u80fd\u3002", "conclusion": "\u8f6f\u4ef6\u5de5\u7a0b\u6b63\u5728\u4ece\u7f16\u7801\u4e3a\u4e2d\u5fc3\u8f6c\u5411\u4ee5\u4ea7\u54c1\u601d\u7ef4\u548c\u6280\u672f\u76d1\u7763\u4e3a\u4e2d\u5fc3\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u53d1\u5c55\u65b0\u7684\u6df7\u5408\u6280\u80fd\u6765\u9002\u5e94AI\u9a71\u52a8\u7684\u5f00\u53d1\u73af\u5883\u3002", "topic": "swe application"}}
{"id": "tldr.2601.e7759243", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Famap-ml.github.io%2FThinking-with-Map%2F%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/E0ptM2nd7dNNBDaInODjDcB_0DissRPMcuOZb1hmBbI=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Famap-ml.github.io%2FThinking-with-Map%2F%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/E0ptM2nd7dNNBDaInODjDcB_0DissRPMcuOZb1hmBbI=440", "authors": ["TLDR Newsletter"], "title": "Map-Augmented Agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Famap-ml.github.io%2FThinking-with-Map%2F%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/E0ptM2nd7dNNBDaInODjDcB_0DissRPMcuOZb1hmBbI=440", "summary": "Map-Augmented Agent (2 minute read) Alibaba introduces a map-augmented agent for image geolocalization, embedding it in a map-guided loop that combines reinforcement learning and parallel test-time inference to improve prediction accuracy.", "source": "tldr", "AI": {"tldr": "\u963f\u91cc\u5df4\u5df4\u63d0\u51fa\u5730\u56fe\u589e\u5f3a\u667a\u80fd\u4f53\u7528\u4e8e\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\uff0c\u901a\u8fc7\u5730\u56fe\u5f15\u5bfc\u5faa\u73af\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u63a8\u7406\u6765\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u89e3\u51b3\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u9884\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5730\u56fe\u4fe1\u606f\u8fdb\u884c\u51c6\u786e\u7684\u4f4d\u7f6e\u63a8\u65ad", "method": "\u91c7\u7528\u5730\u56fe\u589e\u5f3a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5d4c\u5165\u5730\u56fe\u5f15\u5bfc\u5faa\u73af\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u51b3\u7b56\u4f18\u5316\uff0c\u5e76\u4f7f\u7528\u5e76\u884c\u6d4b\u8bd5\u65f6\u63a8\u7406\u63d0\u5347\u6548\u7387", "result": "\u63d0\u9ad8\u4e86\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u5730\u56fe\u4fe1\u606f\u7684\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "\u5730\u56fe\u589e\u5f3a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5730\u56fe\u4fe1\u606f\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7684\u4ef7\u503c", "topic": "agent analysis"}}
{"id": "tldr.2601.1064fc25", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuilders.ramp.com%2Fpost%2Fwhy-we-built-our-background-agent%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/u0zzCmUTDI0d_pLLtjlOepTs-RKYyPLcGHiDloYreL0=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuilders.ramp.com%2Fpost%2Fwhy-we-built-our-background-agent%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/u0zzCmUTDI0d_pLLtjlOepTs-RKYyPLcGHiDloYreL0=440", "authors": ["TLDR Newsletter"], "title": "Why We Built Our Own Background Agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuilders.ramp.com%2Fpost%2Fwhy-we-built-our-background-agent%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/u0zzCmUTDI0d_pLLtjlOepTs-RKYyPLcGHiDloYreL0=440", "summary": "Why We Built Our Own Background Agent (15 minute read) Inspect is a custom background coding agent developed by Ramp that autonomously writes, verifies, and debugs code with the same context and tools a human engineer would use. The agent operates in fast, sandboxed VMs with full development environments, integrates with many existing tools, and supports interfaces like Slack, web, and Chrome extensions. Inspect was adopted quickly internally, writing approximately 30% of merged pull requests.", "source": "tldr", "AI": {"tldr": "Ramp\u5f00\u53d1\u4e86\u540d\u4e3aInspect\u7684\u81ea\u5b9a\u4e49\u540e\u53f0\u7f16\u7801\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u80fd\u81ea\u4e3b\u7f16\u5199\u3001\u9a8c\u8bc1\u548c\u8c03\u8bd5\u4ee3\u7801\uff0c\u4f7f\u7528\u4e0e\u4eba\u7c7b\u5de5\u7a0b\u5e08\u76f8\u540c\u7684\u4e0a\u4e0b\u6587\u548c\u5de5\u5177\uff0c\u5df2\u5185\u90e8\u5feb\u901f\u91c7\u7528\uff0c\u7f16\u5199\u4e86\u7ea630%\u7684\u5408\u5e76PR\u3002", "motivation": "Ramp\u9700\u8981\u6784\u5efa\u81ea\u5df1\u7684\u540e\u53f0\u4ee3\u7406\u6765\u89e3\u51b3\u73b0\u6709\u7f16\u7801\u4ee3\u7406\u7684\u5c40\u9650\u6027\uff0c\u521b\u5efa\u80fd\u591f\u81ea\u4e3b\u64cd\u4f5c\u3001\u4f7f\u7528\u4e0e\u4eba\u7c7b\u5de5\u7a0b\u5e08\u76f8\u540c\u4e0a\u4e0b\u6587\u548c\u5de5\u5177\u7684\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86Inspect\u81ea\u5b9a\u4e49\u540e\u53f0\u7f16\u7801\u4ee3\u7406\uff0c\u5728\u5feb\u901f\u6c99\u76d2\u5316VM\u4e2d\u8fd0\u884c\u5b8c\u6574\u5f00\u53d1\u73af\u5883\uff0c\u96c6\u6210\u73b0\u6709\u5de5\u5177\uff0c\u652f\u6301Slack\u3001Web\u548cChrome\u6269\u5c55\u7b49\u591a\u79cd\u63a5\u53e3\u3002", "result": "Inspect\u5728\u5185\u90e8\u5feb\u901f\u88ab\u91c7\u7528\uff0c\u7f16\u5199\u4e86\u5927\u7ea630%\u7684\u5408\u5e76\u62c9\u53d6\u8bf7\u6c42\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "\u6784\u5efa\u81ea\u5b9a\u4e49\u540e\u53f0\u7f16\u7801\u4ee3\u7406\u662f\u6709\u6548\u7684\uff0cInspect\u5c55\u793a\u4e86\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u5728\u5b9e\u9645\u5de5\u7a0b\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4ef7\u503c\u3002", "topic": "code agent"}}
{"id": "tldr.2601.f83b9e05", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fswark-io%2Fswark%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/Yw_rGIRLnTVrw6sZIQCBzpynPgiTl8rcb7QLrL_TFpI=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fswark-io%2Fswark%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/Yw_rGIRLnTVrw6sZIQCBzpynPgiTl8rcb7QLrL_TFpI=440", "authors": ["TLDR Newsletter"], "title": "Swark", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fswark-io%2Fswark%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/Yw_rGIRLnTVrw6sZIQCBzpynPgiTl8rcb7QLrL_TFpI=440", "summary": "Swark (GitHub Repo) Swark is a free and open-source VS Code extension that automatically generates architecture diagrams from code using LLMs. It integrates directly with GitHub Copilot and outputs diagrams in the Mermaid.js format.", "source": "tldr", "AI": {"tldr": "Swark\u662f\u4e00\u4e2a\u5f00\u6e90\u7684VS Code\u6269\u5c55\uff0c\u5229\u7528LLM\u4ece\u4ee3\u7801\u81ea\u52a8\u751f\u6210Mermaid.js\u683c\u5f0f\u7684\u67b6\u6784\u56fe\uff0c\u5e76\u4e0eGitHub Copilot\u96c6\u6210\u3002", "motivation": "\u4f20\u7edf\u67b6\u6784\u56fe\u521b\u5efa\u8fc7\u7a0b\u7e41\u7410\u4e14\u5bb9\u6613\u8fc7\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u4ece\u4ee3\u7801\u76f4\u63a5\u751f\u6210\u53ef\u89c6\u5316\u67b6\u6784\u56fe\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u6587\u6863\u8d28\u91cf\u3002", "method": "\u5f00\u53d1VS Code\u6269\u5c55\uff0c\u96c6\u6210LLM\u5206\u6790\u4ee3\u7801\u7ed3\u6784\uff0c\u81ea\u52a8\u751f\u6210Mermaid.js\u683c\u5f0f\u7684\u67b6\u6784\u56fe\uff0c\u5e76\u4e0eGitHub Copilot\u6df1\u5ea6\u96c6\u6210\u3002", "result": "\u521b\u5efa\u4e86\u5f00\u6e90\u5de5\u5177Swark\uff0c\u80fd\u591f\u81ea\u52a8\u4ece\u4ee3\u7801\u751f\u6210\u67b6\u6784\u56fe\uff0c\u652f\u6301Mermaid.js\u683c\u5f0f\uff0c\u63d0\u4f9b\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u6548\u679c\u3002", "conclusion": "Swark\u901a\u8fc7\u81ea\u52a8\u5316\u67b6\u6784\u56fe\u751f\u6210\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u6587\u6863\u5de5\u4f5c\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u7406\u89e3\u548c\u7ef4\u62a4\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2601.7ce7808e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fopencode-vs-claude-code%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/h2ib2fAoEzTCuaJDe69N1hn3ZJ6bsG98VG2jQONhQz0=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fopencode-vs-claude-code%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/h2ib2fAoEzTCuaJDe69N1hn3ZJ6bsG98VG2jQONhQz0=440", "authors": ["TLDR Newsletter"], "title": "OpenCode vs Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fopencode-vs-claude-code%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/h2ib2fAoEzTCuaJDe69N1hn3ZJ6bsG98VG2jQONhQz0=440", "summary": "OpenCode vs Claude Code (8 minute read) OpenCode and Claude Code are both AI coding assistants that let you chat with your codebase and run terminal commands. Claude Code offers a polished, integrated experience locked to Anthropic's ecosystem, while OpenCode is an open-source alternative supporting 75+ AI providers. In head-to-head testing using the same model, Claude Code was faster and more streamlined (9 minutes vs 16 minutes total), while OpenCode was more thorough.", "source": "tldr", "AI": {"tldr": "\u5bf9\u6bd4OpenCode\u548cClaude Code\u4e24\u6b3eAI\u7f16\u7a0b\u52a9\u624b\uff1aClaude Code\u5728Anthropic\u751f\u6001\u5185\u63d0\u4f9b\u66f4\u6d41\u7545\u7684\u96c6\u6210\u4f53\u9a8c\uff0cOpenCode\u5219\u662f\u652f\u630175+AI\u63d0\u4f9b\u5546\u7684\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848", "motivation": "\u6bd4\u8f83\u4e24\u6b3eAI\u7f16\u7a0b\u52a9\u624b\uff08OpenCode\u548cClaude Code\uff09\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u4e86\u89e3\u4e0d\u540c\u5de5\u5177\u7684\u7279\u70b9\u548c\u9002\u7528\u573a\u666f", "method": "\u4f7f\u7528\u76f8\u540c\u6a21\u578b\u8fdb\u884c\u5934\u5bf9\u5934\u6d4b\u8bd5\uff0c\u5bf9\u6bd4\u4e24\u6b3e\u5de5\u5177\u5728\u4ee3\u7801\u5e93\u5bf9\u8bdd\u548c\u7ec8\u7aef\u547d\u4ee4\u6267\u884c\u65b9\u9762\u7684\u8868\u73b0", "result": "Claude Code\u66f4\u5feb\u66f4\u6d41\u7545\uff089\u5206\u949f vs 16\u5206\u949f\uff09\uff0cOpenCode\u66f4\u5168\u9762\u4f46\u901f\u5ea6\u8f83\u6162\uff1bClaude Code\u96c6\u6210\u5ea6\u66f4\u9ad8\u4f46\u5c40\u9650\u4e8eAnthropic\u751f\u6001\uff0cOpenCode\u5f00\u6e90\u4e14\u652f\u6301\u66f4\u591aAI\u63d0\u4f9b\u5546", "conclusion": "\u9009\u62e9\u53d6\u51b3\u4e8e\u9700\u6c42\uff1a\u8ffd\u6c42\u901f\u5ea6\u548c\u96c6\u6210\u4f53\u9a8c\u9009Claude Code\uff0c\u9700\u8981\u5f00\u6e90\u6027\u548c\u591a\u63d0\u4f9b\u5546\u652f\u6301\u9009OpenCode", "topic": "code agent"}}
{"id": "tldr.2601.7e4d45ba", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260114%26utm_content=std/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/CsEoHZx-EAi9gq5IhVrrqlicS08mst3o0ooW2s_3q44=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260114%26utm_content=std/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/CsEoHZx-EAi9gq5IhVrrqlicS08mst3o0ooW2s_3q44=440", "authors": ["TLDR Newsletter"], "title": "Cut your dev loop from hours to seconds", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260114%26utm_content=std/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/CsEoHZx-EAi9gq5IhVrrqlicS08mst3o0ooW2s_3q44=440", "summary": "Cut your dev loop from hours to seconds (Sponsor) Run your code locally with seamless access to everything in your Kubernetes cluster. It's like testing in the cloud without the hassle of actually deploying it there. Learn more about mirrord", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecdmirrord\u5de5\u5177\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u5728\u672c\u5730\u8fd0\u884c\u4ee3\u7801\u65f6\u65e0\u7f1d\u8bbf\u95eeKubernetes\u96c6\u7fa4\u4e2d\u7684\u6240\u6709\u8d44\u6e90\uff0c\u65e0\u9700\u5b9e\u9645\u90e8\u7f72\u5230\u4e91\u7aef", "motivation": "\u89e3\u51b3\u5f00\u53d1\u8005\u5728\u672c\u5730\u5f00\u53d1\u65f6\u9700\u8981\u9891\u7e41\u90e8\u7f72\u5230\u4e91\u7aef\u8fdb\u884c\u6d4b\u8bd5\u7684\u75db\u70b9\uff0c\u7f29\u77ed\u5f00\u53d1\u5faa\u73af\u65f6\u95f4\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "method": "\u901a\u8fc7mirrord\u5de5\u5177\u5728\u672c\u5730\u73af\u5883\u4e2d\u6a21\u62df\u4e91\u7aef\u7684Kubernetes\u96c6\u7fa4\u8bbf\u95ee\uff0c\u8ba9\u672c\u5730\u4ee3\u7801\u80fd\u591f\u76f4\u63a5\u4e0e\u96c6\u7fa4\u8d44\u6e90\u4ea4\u4e92", "result": "\u5c06\u5f00\u53d1\u5faa\u73af\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u6570\u79d2\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u4e91\u7aef\u6d4b\u8bd5\u7684\u4f53\u9a8c\u4f46\u65e0\u9700\u5b9e\u9645\u90e8\u7f72", "conclusion": "mirrord\u5de5\u5177\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u8ba9\u5f00\u53d1\u8005\u4eab\u53d7\u4e91\u7aef\u6d4b\u8bd5\u7684\u4fbf\u5229\u6027\u800c\u65e0\u9700\u627f\u53d7\u5b9e\u9645\u90e8\u7f72\u7684\u590d\u6742\u6027", "topic": "swe application"}}
{"id": "tldr.2601.43b0c4c6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarketplace.visualstudio.com%2Fitems%3FitemName=Nogic.nogic%26utm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/qbPKvFuXNvk3CYR9_jZ1wGLWZQFTQ_ucUloWI7NG-5o=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarketplace.visualstudio.com%2Fitems%3FitemName=Nogic.nogic%26utm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/qbPKvFuXNvk3CYR9_jZ1wGLWZQFTQ_ucUloWI7NG-5o=440", "authors": ["TLDR Newsletter"], "title": "Nogic", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarketplace.visualstudio.com%2Fitems%3FitemName=Nogic.nogic%26utm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/qbPKvFuXNvk3CYR9_jZ1wGLWZQFTQ_ucUloWI7NG-5o=440", "summary": "Nogic (Website) Nogic is a Visual Studio Code extension that helps users visualize their codebase structure with interactive diagrams.", "source": "tldr", "AI": {"tldr": "Nogic\u662f\u4e00\u4e2aVS Code\u6269\u5c55\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u56fe\u8868\u5e2e\u52a9\u7528\u6237\u53ef\u89c6\u5316\u4ee3\u7801\u5e93\u7ed3\u6784", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u5bfc\u822a\u590d\u6742\u4ee3\u7801\u5e93\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u624b\u6bb5\u63d0\u5347\u4ee3\u7801\u7406\u89e3\u6548\u7387", "method": "\u5f00\u53d1Visual Studio Code\u6269\u5c55\uff0c\u751f\u6210\u4ea4\u4e92\u5f0f\u56fe\u8868\u6765\u5c55\u793a\u4ee3\u7801\u5e93\u7ed3\u6784", "result": "\u521b\u5efa\u4e86Nogic\u5de5\u5177\uff0c\u80fd\u591f\u53ef\u89c6\u5316\u4ee3\u7801\u7ed3\u6784\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u5bfc\u822a\u4f53\u9a8c", "conclusion": "\u53ef\u89c6\u5316\u5de5\u5177\u80fd\u6709\u6548\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u590d\u6742\u4ee3\u7801\u5e93\u7ed3\u6784\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387", "topic": "swe application"}}
{"id": "tldr.2601.7efb475c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.joinformal.com%2Fblog%2Fusing-proxies-to-hide-secrets-from-claude-code%2F%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/dEeK9RSisug9DMGXSjnIcdVjImQWi-WxWgr99SyGP3E=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.joinformal.com%2Fblog%2Fusing-proxies-to-hide-secrets-from-claude-code%2F%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/dEeK9RSisug9DMGXSjnIcdVjImQWi-WxWgr99SyGP3E=440", "authors": ["TLDR Newsletter"], "title": "Using Proxies to Hide Secrets from Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.joinformal.com%2Fblog%2Fusing-proxies-to-hide-secrets-from-claude-code%2F%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/dEeK9RSisug9DMGXSjnIcdVjImQWi-WxWgr99SyGP3E=440", "summary": "Using Proxies to Hide Secrets from Claude Code (10 minute read) Claude Code has security challenges, as its sandboxes can expose sensitive data such as API keys through network access or environment variables, but network proxies can be used to inject the actual API keys in a safer manner.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528\u7f51\u7edc\u4ee3\u7406\u9690\u85cfAPI\u5bc6\u94a5\u7b49\u654f\u611f\u4fe1\u606f\uff0c\u9632\u6b62Claude Code\u6c99\u7bb1\u73af\u5883\u4e2d\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669", "motivation": "Claude Code\u7684\u6c99\u7bb1\u73af\u5883\u5b58\u5728\u5b89\u5168\u6311\u6218\uff0c\u53ef\u80fd\u901a\u8fc7\u7f51\u7edc\u8bbf\u95ee\u6216\u73af\u5883\u53d8\u91cf\u66b4\u9732API\u5bc6\u94a5\u7b49\u654f\u611f\u6570\u636e\uff0c\u9700\u8981\u66f4\u5b89\u5168\u7684\u65b9\u6cd5\u6765\u4fdd\u62a4\u8fd9\u4e9b\u4fe1\u606f", "method": "\u91c7\u7528\u7f51\u7edc\u4ee3\u7406\u6280\u672f\uff0c\u5728\u6c99\u7bb1\u73af\u5883\u4e2d\u901a\u8fc7\u4ee3\u7406\u670d\u52a1\u5668\u6ce8\u5165\u5b9e\u9645\u7684API\u5bc6\u94a5\uff0c\u907f\u514d\u654f\u611f\u4fe1\u606f\u76f4\u63a5\u66b4\u9732\u5728\u6c99\u7bb1\u73af\u5883\u4e2d", "result": "\u7f51\u7edc\u4ee3\u7406\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9690\u85cf\u654f\u611f\u6570\u636e\uff0c\u63d0\u4f9b\u6bd4\u76f4\u63a5\u5728\u6c99\u7bb1\u4e2d\u5b58\u50a8API\u5bc6\u94a5\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u4f7f\u7528\u4ee3\u7406\u6280\u672f\u662f\u4fdd\u62a4Claude Code\u6c99\u7bb1\u73af\u5883\u4e2d\u654f\u611f\u4fe1\u606f\u7684\u6709\u6548\u5b89\u5168\u7b56\u7565", "topic": "code agent"}}
{"id": "tldr.2601.c2d45a71", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FTencent%2FWeKnora%3Futm_source=tldrdevops/1/0100019bbc7b9b13-bbd5f8d3-5e71-4451-b5a4-96a458134216-000000/6Ktek_x_uFLEzofviJ-Kj8RMlSn2ppSEsJop0PbPAm0=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FTencent%2FWeKnora%3Futm_source=tldrdevops/1/0100019bbc7b9b13-bbd5f8d3-5e71-4451-b5a4-96a458134216-000000/6Ktek_x_uFLEzofviJ-Kj8RMlSn2ppSEsJop0PbPAm0=440", "authors": ["TLDR Newsletter"], "title": "WeKnora", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FTencent%2FWeKnora%3Futm_source=tldrdevops/1/0100019bbc7b9b13-bbd5f8d3-5e71-4451-b5a4-96a458134216-000000/6Ktek_x_uFLEzofviJ-Kj8RMlSn2ppSEsJop0PbPAm0=440", "summary": "WeKnora (GitHub Repo) WeChat's WeKnora, an LLM-powered framework designed for deep document understanding and semantic retrieval using the RAG paradigm, has a modular architecture that supports features like ReACT Agent mode and knowledge graph transformation. The v0.2.0 update introduced a Web UI for model configuration and enhanced security with login authentication.", "source": "tldr", "AI": {"tldr": "WeKnora\u662f\u5fae\u4fe1\u5f00\u53d1\u7684\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6df1\u5ea6\u6587\u6863\u7406\u89e3\u548c\u8bed\u4e49\u68c0\u7d22\uff0c\u91c7\u7528RAG\u8303\u5f0f\uff0c\u5177\u6709\u6a21\u5757\u5316\u67b6\u6784\uff0c\u652f\u6301ReACT Agent\u6a21\u5f0f\u548c\u77e5\u8bc6\u56fe\u8c31\u8f6c\u6362\uff0cv0.2.0\u7248\u672c\u589e\u52a0\u4e86Web UI\u914d\u7f6e\u754c\u9762\u548c\u767b\u5f55\u8ba4\u8bc1\u5b89\u5168\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u6587\u6863\u7406\u89e3\u548c\u8bed\u4e49\u68c0\u7d22\u7684\u9700\u6c42\uff0c\u901a\u8fc7RAG\u8303\u5f0f\u7ed3\u5408LLM\u80fd\u529b\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u589e\u5f3a\u7cfb\u7edf\u7684\u6613\u7528\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u8303\u5f0f\uff0c\u6784\u5efa\u6a21\u5757\u5316\u67b6\u6784\uff0c\u652f\u6301ReACT Agent\u6a21\u5f0f\u8fdb\u884c\u4ea4\u4e92\u5f0f\u63a8\u7406\uff0c\u5b9e\u73b0\u77e5\u8bc6\u56fe\u8c31\u8f6c\u6362\uff0c\u5e76\u901a\u8fc7Web UI\u63d0\u4f9b\u53ef\u89c6\u5316\u914d\u7f6e\u754c\u9762\u3002", "result": "\u5f00\u53d1\u4e86WeKnora\u6846\u67b6\uff0cv0.2.0\u7248\u672c\u5b9e\u73b0\u4e86Web UI\u914d\u7f6e\u754c\u9762\u548c\u767b\u5f55\u8ba4\u8bc1\u5b89\u5168\u529f\u80fd\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6df1\u5ea6\u6587\u6863\u7406\u89e3\u548c\u8bed\u4e49\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "WeKnora\u662f\u4e00\u4e2a\u529f\u80fd\u5b8c\u5584\u7684LLM\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7RAG\u8303\u5f0f\u6709\u6548\u652f\u6301\u6df1\u5ea6\u6587\u6863\u7406\u89e3\u548c\u8bed\u4e49\u68c0\u7d22\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u5b89\u5168\u589e\u5f3a\u4f7f\u5176\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2601.4e17fd9e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fabout.gitlab.com%2Fblog%2Fbuilding-trust-in-agentic-tools-what-we-learned-from-our-users%2F%3Futm_source=tldrdevops/1/0100019bbc7b9b13-bbd5f8d3-5e71-4451-b5a4-96a458134216-000000/n5kABcA7UE-yaw7LgXM1RD9JPGwHs6Mmd6vEcYUzsJs=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fabout.gitlab.com%2Fblog%2Fbuilding-trust-in-agentic-tools-what-we-learned-from-our-users%2F%3Futm_source=tldrdevops/1/0100019bbc7b9b13-bbd5f8d3-5e71-4451-b5a4-96a458134216-000000/n5kABcA7UE-yaw7LgXM1RD9JPGwHs6Mmd6vEcYUzsJs=440", "authors": ["TLDR Newsletter"], "title": "Building trust in agentic tools: What we learned from our users", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fabout.gitlab.com%2Fblog%2Fbuilding-trust-in-agentic-tools-what-we-learned-from-our-users%2F%3Futm_source=tldrdevops/1/0100019bbc7b9b13-bbd5f8d3-5e71-4451-b5a4-96a458134216-000000/n5kABcA7UE-yaw7LgXM1RD9JPGwHs6Mmd6vEcYUzsJs=440", "summary": "Building trust in agentic tools: What we learned from our users (6 minute read) Trust in AI agents develops through accumulated micro-inflection points rather than breakthrough moments. Safeguards, transparency, contextual memory, and anticipation of user needs gradually build confidence and drive adoption in DevSecOps workflows.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728DevSecOps\u5de5\u4f5c\u6d41\u4e2d\u5efa\u7acb\u5bf9AI\u4ee3\u7406\u5de5\u5177\u7684\u4fe1\u4efb\uff0c\u53d1\u73b0\u4fe1\u4efb\u662f\u901a\u8fc7\u79ef\u7d2f\u5fae\u5c0f\u8f6c\u6298\u70b9\u800c\u975e\u7a81\u7834\u6027\u65f6\u523b\u9010\u6b65\u5efa\u7acb\u7684", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5b9e\u9645\u5de5\u4f5c\u573a\u666f\u4e2d\u5efa\u7acb\u7528\u6237\u5bf9AI\u4ee3\u7406\u5de5\u5177\u7684\u4fe1\u4efb\uff0c\u7279\u522b\u662f\u5728DevSecOps\u9886\u57df\uff0c\u4ee5\u4fc3\u8fdb\u8fd9\u4e9b\u5de5\u5177\u7684\u91c7\u7528\u548c\u6709\u6548\u4f7f\u7528", "method": "\u57fa\u4e8e\u7528\u6237\u7814\u7a76\u548c\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u5206\u6790\u4fe1\u4efb\u5efa\u7acb\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5305\u62ec\u5b89\u5168\u63aa\u65bd\u3001\u900f\u660e\u5ea6\u3001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u548c\u7528\u6237\u9700\u6c42\u9884\u6d4b\u7b49\u65b9\u9762", "result": "\u53d1\u73b0\u4fe1\u4efb\u662f\u901a\u8fc7\u4e00\u7cfb\u5217\u5fae\u5c0f\u8f6c\u6298\u70b9\u9010\u6b65\u5efa\u7acb\u7684\uff0c\u5b89\u5168\u9632\u62a4\u3001\u900f\u660e\u6027\u3001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u548c\u9884\u6d4b\u7528\u6237\u9700\u6c42\u662f\u5efa\u7acb\u4fe1\u4efb\u7684\u5173\u952e\u8981\u7d20", "conclusion": "\u5728DevSecOps\u5de5\u4f5c\u6d41\u4e2d\uff0cAI\u4ee3\u7406\u5de5\u5177\u7684\u4fe1\u4efb\u5efa\u7acb\u662f\u4e00\u4e2a\u6e10\u8fdb\u8fc7\u7a0b\uff0c\u9700\u8981\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u8bbe\u8ba1\u8003\u8651\u6765\u4fc3\u8fdb\u7528\u6237\u4fe1\u4efb\u548c\u91c7\u7528", "topic": "agent analysis"}}
{"id": "tldr.2601.f2c256dc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fatoms.dev%2F%3Futm_source=tldrfounders/1/0100019bbc9f1101-a8e9559d-f2df-4d83-8420-cb55ab02b847-000000/hxN0GWiomLzTbgRlYc7551wQJjiNPK-_VFCFb5aSRlI=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fatoms.dev%2F%3Futm_source=tldrfounders/1/0100019bbc9f1101-a8e9559d-f2df-4d83-8420-cb55ab02b847-000000/hxN0GWiomLzTbgRlYc7551wQJjiNPK-_VFCFb5aSRlI=440", "authors": ["TLDR Newsletter"], "title": "Atoms", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fatoms.dev%2F%3Futm_source=tldrfounders/1/0100019bbc9f1101-a8e9559d-f2df-4d83-8420-cb55ab02b847-000000/hxN0GWiomLzTbgRlYc7551wQJjiNPK-_VFCFb5aSRlI=440", "summary": "Atoms (Tool) Atoms is an AI agent platform that researches, designs, builds, and deploys full-stack applications with integrated business functionality.", "source": "tldr", "AI": {"tldr": "Atoms\u662f\u4e00\u4e2aAI\u4ee3\u7406\u5e73\u53f0\uff0c\u80fd\u591f\u7814\u7a76\u3001\u8bbe\u8ba1\u3001\u6784\u5efa\u548c\u90e8\u7f72\u5177\u6709\u96c6\u6210\u4e1a\u52a1\u529f\u80fd\u7684\u5168\u6808\u5e94\u7528\u7a0b\u5e8f", "motivation": "\u89e3\u51b3\u5168\u6808\u5e94\u7528\u5f00\u53d1\u4e2d\u9700\u8981\u96c6\u6210\u4e1a\u52a1\u529f\u80fd\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u901a\u8fc7AI\u4ee3\u7406\u81ea\u52a8\u5316\u6574\u4e2a\u5f00\u53d1\u6d41\u7a0b", "method": "\u6784\u5efaAI\u4ee3\u7406\u5e73\u53f0\uff0c\u6574\u5408\u7814\u7a76\u3001\u8bbe\u8ba1\u3001\u6784\u5efa\u548c\u90e8\u7f72\u7684\u5168\u6d41\u7a0b\u81ea\u52a8\u5316\u80fd\u529b", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u5904\u7406\u5b8c\u6574\u5e94\u7528\u5f00\u53d1\u5468\u671f\u7684AI\u4ee3\u7406\u7cfb\u7edf", "conclusion": "Atoms\u5e73\u53f0\u5c55\u793a\u4e86AI\u4ee3\u7406\u5728\u5168\u6808\u5e94\u7528\u5f00\u53d1\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "topic": "code agent"}}
