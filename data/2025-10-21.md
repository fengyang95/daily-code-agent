<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [wechat.article](#wechat.article) [Total: 23]
- [tldr.article](#tldr.article) [Total: 9]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.LG](#cs.LG) [Total: 28]
- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 提出了CorrectBench基准来评估LLM自校正方法的有效性，发现自校正能提升推理任务准确性但效率较低，简单的CoT基线表现具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种LLM自校正方法，但缺乏全面评估，且LLM是否能真正自我校正仍存争议。

Method: 开发CorrectBench基准，评估内在、外部和微调三种自校正策略在常识推理、数学推理和代码生成任务上的表现。

Result: 自校正方法能提升准确性（特别是复杂推理任务），混合策略有进一步改进但效率降低，推理LLM在额外自校正下优化有限且时间成本高。

Conclusion: 自校正有潜力提升LLM推理性能，但需平衡推理能力与操作效率，建议进一步研究优化这一平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [2] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: 提出了EvolveR框架，使LLM代理能够通过完整的闭环经验生命周期进行自我改进，包括离线自我蒸馏和在线交互两个关键阶段，在复杂多跳问答基准上优于现有代理基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在工具使用方面表现出色，但缺乏从自身经验中系统学习的能力，无法迭代优化问题解决策略。

Method: EvolveR框架包含两个阶段：(1)离线自我蒸馏：将代理的交互轨迹合成为结构化、可重用的抽象策略原则库；(2)在线交互：代理与任务交互并主动检索蒸馏原则指导决策，积累多样化行为轨迹。采用策略强化机制迭代更新代理。

Result: 在复杂多跳问答基准测试中，EvolveR实现了优于强代理基线的性能表现。

Conclusion: 该工作为代理不仅从外部数据学习，还能从自身行动后果中学习提供了全面蓝图，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [3] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本研究量化了提示策略与大型语言模型在自动化系统文献综述筛选阶段的交互作用，评估了6个LLM在5种提示类型下的表现，发现CoT-少样本提示在精确度-召回率平衡方面最可靠，并提出了分阶段工作流程建议。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述的筛选阶段耗时耗力，需要探索LLM自动化的潜力，但现有研究缺乏对提示策略与模型交互作用的系统分析。

Method: 评估6个LLM（GPT-4o、GPT-4o-mini、DeepSeek-Chat-V3、Gemini-2.5-Flash、Claude-3.5-Haiku、Llama-4-Maverick）在5种提示类型（零样本、少样本、思维链、CoT-少样本、自我反思）下的表现，使用准确率、精确度、召回率和F1分数作为评估指标。

Result: CoT-少样本提示在精确度-召回率平衡方面表现最可靠；零样本提示在高灵敏度筛选时召回率最高；自我反思提示因过度包含性和不稳定性表现不佳；GPT-4o和DeepSeek总体表现稳健；GPT-4o-mini在显著降低成本的同时保持竞争力。

Conclusion: LLM在自动化文献筛选方面具有不均衡但前景广阔的潜力，推荐采用分阶段工作流程：先用低成本模型配合结构化提示进行初步筛选，仅对边界案例使用高容量模型。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [4] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: MoReBench是一个包含1000个道德场景和23,000多个评估标准的基准，用于评估AI的道德推理过程，重点关注推理过程而非最终答案。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在决策中扮演更重要的角色，需要理解它们如何做出决策，特别是道德决策。道德困境是评估AI推理过程的理想测试平台，因为它们允许多种合理的结论。

Method: 创建MoReBench基准，包含道德场景和专家制定的评估标准，涵盖道德考量识别、权衡分析和可操作建议。同时创建MoReBench-Theory测试AI在五种规范伦理学框架下的推理能力。

Result: 研究发现扩展定律和现有数学、代码、科学推理基准无法预测模型的道德推理能力。模型对特定道德框架（如边沁功利主义和康德义务论）表现出偏好，这可能是流行训练范式的副作用。

Conclusion: 这些基准推动了以过程为重点的推理评估，有助于开发更安全、更透明的AI系统。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [5] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector是一个高效的Best-of-N框架，利用LLM的隐藏状态进行过程级评分，通过轻量级验证器评估推理轨迹质量，在保持较低推理成本的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的外部测试时扩展方法存在计算开销大和未充分利用LLM内在表示的问题，需要更高效的轨迹选择框架。

Method: 使用轻量级验证器（仅0.6B参数）评估步骤级轨迹质量，利用LLM隐藏状态进行过程级评分，采用端到端数据驱动训练方法。

Result: 在五个基准测试中，TrajSelector在Best-of-32设置下比多数投票准确率高4.61%，比现有过程奖励模型高4.31%-12.21%，且推理成本更低。

Conclusion: TrajSelector通过利用LLM内在表示和轻量级验证器，实现了高效且有效的推理轨迹选择，为测试时扩展提供了新思路。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [6] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出了一种基于超图的适配器HGAdapter，通过捕捉代码中的高阶数据相关性来增强预训练语言模型在代码相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型在代码任务中表现良好，但未能充分考虑代码中潜在的高阶数据相关性，限制了其性能提升。

Method: 提出了三种代码令牌的高阶相关性类型（抽象语法树家族相关性、词汇相关性和行相关性），设计了令牌和超边生成器来捕捉这些相关性，并改进了超图神经网络架构，结合适配器调优提出了HGAdapter。

Result: 在多个公共数据集上的实验表明，该方法在不同语言的代码摘要和代码克隆检测任务中都能不同程度地提升预训练语言模型的性能。

Conclusion: 引入高阶数据相关性有助于提高代码相关任务的有效性，HGAdapter可以插入到各种预训练语言模型中增强其性能。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [7] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出了可执行知识图谱(xKG)来解决AI研究复现中的挑战，通过整合技术洞察、代码片段和领域知识，显著提升了LLM代理的研究复现能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可执行代码方面存在困难，主要由于背景知识不足和RAG方法的局限性，无法捕捉参考文献中的潜在技术细节。

Method: 提出可执行知识图谱(xKG)，这是一个模块化、可插拔的知识库，自动从科学文献中提取技术洞察、代码片段和领域特定知识。

Result: 在三个代理框架和两种不同LLM上集成xKG后，在PaperBench上显示出显著的性能提升（o3-mini提升10.9%）。

Conclusion: xKG是自动化AI研究复现的通用且可扩展解决方案。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [8] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 提出使用"退出"作为LLM代理的安全机制，让代理在缺乏信心时主动退出，在12个先进LLM上评估显示该方法能显著提升安全性而几乎不影响帮助性


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在复杂环境中运行并产生现实后果，其安全性变得至关重要。多轮代理场景中的不确定性和模糊性会累积，导致超越传统文本生成失败的严重风险

Method: 利用ToolEmu框架系统评估12个先进LLM的退出行为，通过添加明确的退出指令来促使代理在缺乏信心时主动退出

Result: 在所有模型上安全性平均提升+0.39（0-3分制），专有模型提升+0.64，而帮助性仅平均下降-0.03，显示出极佳的安全-帮助性权衡

Conclusion: 简单添加明确的退出指令是高度有效的安全机制，可立即部署到现有代理系统中，作为高风险应用中自主代理的有效第一道防线

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [9] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出基于在线背包问题的自动化智能体系统组合框架，通过动态测试和实时效用建模，在预算约束下优化选择智能体组件，显著提高成功率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统组合方法依赖静态语义检索，存在能力描述不完整、检索方法局限等问题，无法基于能力、成本和实时效用进行组件选择。

Method: 引入结构化自动化框架，将智能体系统组合建模为在线背包问题，通过动态测试候选组件并实时建模其效用，在性能、预算约束和兼容性之间进行权衡优化。

Result: 在五个基准数据集上的实证评估显示，基于在线背包的组合器始终位于帕累托前沿，单智能体设置下成功率提升高达31.6%，多智能体系统中成功率从37%提升至87%。

Conclusion: 该方法在多样化领域和预算约束下展现出强大的适应性，显著优于基于检索的基线方法。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [10] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: 提出了DiMo多智能体协作框架，通过四个专门化LLM智能体的结构化辩论来增强性能与可解释性，每个智能体代表不同的推理范式，在六个基准测试中提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs虽然性能强大但缺乏可解释推理的问题，通过模拟结构化辩论来提升推理透明度和鲁棒性。

Method: 使用四个专门化LLM智能体，每个代表不同的推理范式，通过迭代辩论来挑战和精炼初始响应，生成可审计的推理链。

Result: 在六个基准测试中，DiMo相比广泛使用的单模型和辩论基线提高了准确性，在数学任务上提升最大。

Conclusion: DiMo是一个语义感知的Web原生多智能体框架，能够生成语义类型化、URL注释的证据链，支持下游系统检查和重用。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [11] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个多智能体框架，用于生成高质量多模态财务报告，通过CAVM架构统一数据、工具和智能体，采用迭代视觉增强机制优化图表，两阶段写作框架确保分析深度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以完全自动化生成专业财务报告，这是一个劳动密集且智力要求高的过程。

Method: 基于CAVM架构统一外部数据、设计工具和智能体，采用迭代视觉增强机制优化可视化，两阶段写作框架扩展分析段为多模态报告。

Result: 在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线系统。

Conclusion: FinSight展示了生成接近人类专家质量报告的清晰路径。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [12] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 提出PROMPT-MII方法，通过强化学习元学习指令归纳模型，生成紧凑指令替代上下文学习，在保持性能的同时大幅减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 上下文学习虽然有效但推理成本高，需要找到更紧凑的指令表示方法来替代完整的训练集示例。

Method: 基于强化学习的框架，在3,000多个分类数据集上元学习指令归纳模型，能够为新数据集动态生成紧凑指令。

Result: 在90个未见任务上评估，PROMPT-MII提升下游模型质量4-9个F1点（10-20%相对提升），匹配上下文学习性能同时减少3-13倍token使用。

Conclusion: PROMPT-MII能够有效替代上下文学习，在保持性能的同时显著降低推理成本。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [13] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文评估了基于LLM的搜索代理的安全性，发现它们比基础LLM更容易产生有害输出。作者提出了SafeSearch方法，通过多目标强化学习结合安全性和实用性奖励，将代理有害性降低了70%以上。


<details>
  <summary>Details</summary>
Motivation: 虽然研究人员主要关注提高LLM搜索代理的实用性，但其安全行为仍未得到充分探索。研究发现搜索代理比基础LLM更容易产生有害输出，特别是当进行实用性导向的微调时会加剧这种风险。

Method: 提出了SafeSearch方法，采用多目标强化学习，结合最终输出的安全/实用性奖励和新的查询级塑形项，惩罚不安全查询并奖励安全查询。

Result: 实验表明，SafeSearch在三个红队测试数据集上将代理有害性降低了70%以上，同时产生安全、有帮助的响应，并与仅实用性微调的代理在问答性能上相当。

Conclusion: 查询级奖励在联合改进安全性和实用性方面是有效的，证明了联合对齐安全性和实用性的重要性。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [14] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 本文研究LLM的提示敏感性现象，发现语义相同的不同提示会导致模型产生不同答案分布。通过语义空间的采样和改写扰动改进不确定性校准，同时提出新的不确定性分解度量方法。


<details>
  <summary>Details</summary>
Motivation: LLM对语义相同的不同提示会产生不同的答案分布，这表明模型输出的不确定性可能无法反映其对提示含义的真实不确定性。需要改进不确定性校准并量化提示敏感性的影响。

Method: 将提示敏感性建模为泛化误差，通过语义概念空间的改写扰动进行采样，并引入新的不确定性分解度量方法来建模自然语言生成中的语义连续性。

Result: 跨语义概念空间的采样改进了不确定性校准而不影响准确性，新的分解度量能够量化LLM不确定性中归因于提示敏感性的部分。

Conclusion: 该方法为改进提示敏感语言模型的不确定性校准提供了新途径，并证明某些LLM未能对其输入含义表现出一致的通用推理能力。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [15] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个用于多智能体协作的验证感知规划框架，通过分解任务、建模子任务依赖关系并编码验证函数来提升系统鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在复杂任务中面临规划、协调和验证的挑战，执行失败往往源于任务解释、输出格式或智能体间交接的细微偏差。

Method: VeriMAP规划器分解任务，建模子任务依赖关系，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数。

Result: 在多样化数据集上的评估表明，VeriMAP优于单智能体和多智能体基线，同时增强了系统鲁棒性和可解释性。

Conclusion: 验证感知规划能够在多智能体系统中实现可靠的协调和迭代优化，无需依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [16] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: StreamingThinker提出了一种流式思考范式，让LLM在阅读输入时就开始推理，而不是等到整个输入完成后再开始，从而显著减少推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理范式需要等待整个输入完成后再开始思考，这带来了不必要的延迟，并且在动态场景下会减弱对早期信息的注意力。

Method: 通过流式CoT生成、流式约束训练和流式并行推理，实现LLM在阅读过程中进行推理，使用流式推理单元、流式注意力掩码和并行KV缓存等技术。

Result: 在数学推理、逻辑推理和基于上下文的QA推理任务上，StreamingThinker保持了与批处理思考相当的性能，同时将推理开始前的token等待时间减少了80%，最终答案生成的时间延迟减少了60%以上。

Conclusion: 流式思考范式能有效减少LLM推理延迟，同时保持推理质量，为动态场景下的实时推理提供了可行方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [17] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: RL训练的大语言模型在自主调用工具进行推理时存在安全漏洞，两种简单攻击（搜索攻击和多搜索攻击）可显著降低模型的安全拒绝率，暴露当前RL训练方法在安全性方面的核心弱点。


<details>
  <summary>Details</summary>
Motivation: 研究RL训练的搜索模型的安全特性，这些模型在推理任务中表现出色但安全性未被充分理解。

Method: 使用两种简单攻击方法：强制模型以搜索开始响应（搜索攻击）和鼓励模型重复搜索（多搜索攻击），在两个模型家族（Qwen、Llama）上测试本地和网络搜索功能。

Result: 攻击使拒绝率降低高达60.0%，答案安全性降低82.5%，搜索查询安全性降低82.4%。攻击通过触发模型在生成拒绝令牌前生成有害的镜像搜索查询而成功。

Conclusion: 当前RL训练方法存在核心弱点：奖励有效查询的持续生成而不考虑其有害性，导致RL搜索模型存在用户可轻易利用的漏洞，迫切需要开发安全感知的智能RL管道来优化安全搜索。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [18] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大语言模型的行业智能体技术、应用和评估方法，提出了行业智能体能力成熟度框架，分析了记忆、规划和工具使用三大技术支柱的演进，并探讨了实际应用中的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，能够自主推理、规划和执行复杂任务的智能体已成为人工智能前沿，但如何将通用智能体研究转化为推动行业转型的生产力仍面临重大挑战。

Method: 采用行业智能体能力成熟度框架，系统分析智能体在行业应用中的演进路径，重点考察记忆、规划和工具使用三大技术支柱的发展，并综述各领域的实际应用案例。

Result: 构建了从"流程执行系统"到"自适应社会系统"的智能体演进模型，识别了现有评估系统在真实性、安全性和行业特性方面面临的挑战。

Conclusion: 通过结合技术演进与行业实践，为理解和构建下一代行业智能体提供了清晰路线图和理论基础。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [19] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出一种使用二元检索增强奖励的在线强化学习方法，通过仅在模型输出完全正确时给予奖励来减少外在幻觉，在保持其他任务性能的同时显著降低幻觉率。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型产生训练数据不支持的事实错误信息的问题，同时避免现有方法在开放生成和下游任务上的性能下降。

Method: 使用二元检索增强奖励的在线强化学习，仅在模型输出完全正确时给予奖励1，否则为0。在Qwen3推理模型上进行评估。

Result: 在开放生成中幻觉率降低39.3%；在短问答中学会校准弃权，在PopQA和GPQA上分别减少44.4%和21.7%的错误答案；且不损害指令遵循、数学或代码能力。

Conclusion: 二元奖励方法在提高事实性的同时避免了连续奖励RL带来的质量回归，实现了更好的实用效果。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [20] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 该论文提出了FARE（基础自动推理评估器），通过大规模数据驱动的SFT方法训练评估器，在多个评估任务中超越了更大规模的专门RL训练评估器。


<details>
  <summary>Details</summary>
Motivation: 当前生成式评估器训练主要关注新方法如强化学习，而忽视了大规模数据驱动开发。本文专注于数据扩展，解决评估器训练中的数据稀缺问题。

Method: 收集250万样本覆盖5种评估任务，使用简单的迭代拒绝采样监督微调方法训练8B和20B参数的FARE评估器。

Result: FARE-8B挑战了更大的专门RL训练评估器，FARE-20B在开源评估器中设定了新标准，超越了专门的70B+评估器。在实际应用中，FARE-20B在MATH上达到接近oracle性能，在RL训练中比字符串匹配验证器提升14.1%性能。

Conclusion: 大规模数据驱动的SFT方法能够训练出高质量的评估器，在多个任务中超越更复杂的RL训练方法，证明了数据规模的重要性。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [21] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出了企业深度研究(EDR)多智能体系统，通过主规划智能体、四个专业搜索智能体、可扩展工具生态系统、可视化智能体和反思机制，实现自动化报告生成和实时流处理，在开放基准测试中优于现有智能体系统。


<details>
  <summary>Details</summary>
Motivation: 解决企业在将非结构化数据转化为可操作洞察时面临的挑战，现有自主智能体在领域特定细微差别、意图对齐和企业集成方面存在困难。

Method: 采用多智能体架构，包括主规划智能体进行自适应查询分解、四个专业搜索智能体(通用、学术、GitHub、LinkedIn)、基于MCP的可扩展工具生态系统、可视化智能体和检测知识差距的反思机制。

Result: 在DeepResearch Bench和DeepConsult等开放基准测试中，EDR无需人工干预即优于最先进的智能体系统，并在内部数据集上验证了企业部署能力。

Conclusion: EDR框架通过多智能体推理应用，有效解决了企业深度研究任务，发布了框架和基准轨迹以推动相关研究发展。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [22] [【行业前沿】RewardMap: 通过多阶段<em class="highlight">强化学习</em>解决细粒度视觉推理的Sparse Reward](http://mp.weixin.qq.com/s?__biz=MzU3MDg1MzY4NQ==&mid=2247563218&idx=3&sn=bbfcf2886b390824e1e25359f1fad3b1&chksm=fd4b9cf842b69d879ea8c2fc764d56f066bcf2be9ab005bd3c6225ae90008ba74bf49287055c#rd)
*CAA OFFICIAL*

Main category: wechat.article

TL;DR: 团队在大模型强化学习与多模态推理方向具有深厚研究基础。近年来，大语言模型（LLMs）以及多模态大模型（MLLMs）在多种场景理解和复杂推理任务中取得突破性进展。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 团队在大模型强化学习与多模态推理方向具有深厚研究基础。近年来，大语言模型（LLMs）以及多模态大模型（MLLMs）在多种场景理解和复杂推理任务中取得突破性进展。

</details>


### [23] [“以弱驭强”（Weak-for-Strong, W4S）：一种新型<em class="highlight">强化学习</em>框架，训练小型元智能体设计调用更强执行模型的代码工作流](http://mp.weixin.qq.com/s?__biz=MzU5NjQ2MjE2NA==&mid=2247486187&idx=1&sn=e3feedbc5737918e4a5d08f8c3587ea5&chksm=ff9b1acebc47fc134d4cca9342abdf9d4fdcba7fa3d167c5f14be27bad1e773eea0dc9a24381#rd)
*图灵AI云*

Main category: wechat.article

TL;DR: RLAO：离线强化学习驱动的优化策略RLAO 是一种基于多轮轨迹的离线强化学习（offline RL）方法。在每次迭代中，系统会采样多个候选动作，保留表现最好的一个用于推进状态，其余则存入回放缓冲区用于后续训练。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: RLAO：离线强化学习驱动的优化策略RLAO 是一种基于多轮轨迹的离线强化学习（offline RL）方法。在每次迭代中，系统会采样多个候选动作，保留表现最好的一个用于推进状态，其余则存入回放缓冲区用于后续训练。

</details>


### [24] [毫无疑问，未来AI界将会是<em class="highlight">强化学习</em>的天下](http://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&mid=2247633621&idx=1&sn=d84fce608c041972b2d57674ea4e9a6a&chksm=fa89cfd026a756c538064d37af637160a66ec2ae23bc67640145def37795f4e06731d014d69e#rd)
*小白学视觉*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [25] [RL精选中文书籍-《轻松RL<em class="highlight">强化学习</em>中文教程》免费分享](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571512&idx=2&sn=8f430ff37e5c5d40ad844556aeab51bc&chksm=96b7ecc0b0c13e7b57e7ac0c639dcf921b7afa058187b9c1f1a735c9f055eee5f874622c0bf7#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 目录 第1章强化学习概述。1.1 强化学习。...... ....... ....... ....... .....。1.1.1 强化学习与监督学习。...... ....... ....... ....... ....... .。1。1.1.2 强化学习的由来与分类...... ....... ....... ....... .......。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 目录 第1章强化学习概述。1.1 强化学习。...... ....... ....... ....... .....。1.1.1 强化学习与监督学习。...... ....... ....... ....... ....... .。1。1.1.2 强化学习的由来与分类...... ....... ....... ....... .......。

</details>


### [26] [顶会上的王者！谷歌微软All in<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247626053&idx=1&sn=ab59d700061d7fb15c4004617382fa0e&chksm=f8c2480fc18534dc8be6e2300cb74bf3270e332201ec8b91819d0de82b50cda0fa346de8c419#rd)
*CVer*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [27] [毫无疑问，未来AI界将会是<em class="highlight">强化学习</em>的天下](http://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&mid=2649003958&idx=1&sn=897269bb69ae46ffb8b23c917542f030&chksm=869c08acaa57458d3da1595b7a9999d5f4cc70006bb0f219f95c4c52fe729f1081da7794cc91#rd)
*Ai学习的老章*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [28] [毫无疑问，<em class="highlight">强化学习</em>地位还在不断上升！](http://mp.weixin.qq.com/s?__biz=MzUzNjE1Nzc1MA==&mid=2247507703&idx=1&sn=f4adf653496b11f6e8015b6cb5027fd5&chksm=fbba9122ada51681a5784f4f963ff1c78e9353b553c2ce9a558466fa32d89800716074bc9330#rd)
*啥都会一点的研究生*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [29] [【NeurIPS2025】迈向鲁棒的零样本<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671466&idx=4&sn=0ea1c52eb57f9ca4afbb0697f221efeb&chksm=fdc7946c5b3f881cc8692836d498b761730a4b69b6e9dc3bdf84a7e6f92d49114ef93c56e4a0#rd)
*专知*

Main category: wechat.article

TL;DR: 零样本强化学习（zero-shot reinforcement learning， rl）的最新发展，为学习能够在零样本条件下适应任意新任务的预训练通用策略（pre-trained generalist policies）开辟了新的方向。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 零样本强化学习（zero-shot reinforcement learning， rl）的最新发展，为学习能够在零样本条件下适应任意新任务的预训练通用策略（pre-trained generalist policies）开辟了新的方向。

</details>


### [30] [浪潮信息研究团队提出智能驾驶<em class="highlight">强化学习</em>框架，60秒可生成100万样本](http://mp.weixin.qq.com/s?__biz=Mzg3MTgzOTg5NQ==&mid=2247497483&idx=1&sn=285df83f6f7a10e34da06937cae15bea&chksm=cf34abfa02daf8290e7a20eb026c446dc88f257427a238a0977f5ab7ed314617719f6f1b79a3#rd)
*浪潮信息精英合作伙伴*

Main category: wechat.article

TL;DR: 强化学习在解决序列决策问题和控制任务方面显示出越来越强大的能力，使得强化学习技术路线有望超过rule-based的传统智驾技术路线，成为智驾的主流技术路线。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习在解决序列决策问题和控制任务方面显示出越来越强大的能力，使得强化学习技术路线有望超过rule-based的传统智驾技术路线，成为智驾的主流技术路线。

</details>


### [31] [nature | DeepSeek-R1：纯靠<em class="highlight">强化学习</em>，大模型推理能力实现自主进化](http://mp.weixin.qq.com/s?__biz=Mzg2ODcyNzQxMA==&mid=2247485594&idx=1&sn=e83d295fa9523b921ba4cf39bfa24a6c&chksm=cfc1d950178f0b6e7037db46ca633a2f2d3e55b013238a1a17cf7445bf0eb9cbdc5e958f010c#rd)
*Deep AIR深影*

Main category: wechat.article

TL;DR: Extended Data Fig. 2 | GRPO 强化学习训练框架图示。模型作为策略网络生成多个响应，奖励模型（基于规则或模型）对每个响应进行打分，GRPO根据组内奖励分布直接计算优势，并更新策略参数。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Extended Data Fig. 2 | GRPO 强化学习训练框架图示。模型作为策略网络生成多个响应，奖励模型（基于规则或模型）对每个响应进行打分，GRPO根据组内奖励分布直接计算优势，并更新策略参数。

</details>


### [32] [Andrej Karpathy 开炮：智能体都在装样子，<em class="highlight">强化学习</em>很糟糕，AGI 十年也出不来](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571521&idx=3&sn=f63f60c6f7087bce6e953d07fc267000&chksm=968f0fe14e3cf7cbaae95e3b3e00bc314d57e29195c73765f8771d0032a84e2f9e6b71e9cdd3#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 强化学习是人工智能的另一个重要领域，大概有两三年甚至四年的时间，每个人都在游戏上进行强化学习。这完全是一个失误。我在 OpenAI 尝试做的事情是，我一直对游戏能否引领 AGI 有点怀疑 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习是人工智能的另一个重要领域，大概有两三年甚至四年的时间，每个人都在游戏上进行强化学习。这完全是一个失误。我在 OpenAI 尝试做的事情是，我一直对游戏能否引领 AGI 有点怀疑 。

</details>


### [33] [突发！<em class="highlight">强化学习</em>RL彻底凉凉了。。。！？](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571521&idx=1&sn=f50883605c7a52c677cc45842c2a6110&chksm=9652a2999202c006408072d38751024a91660b07498608f6e29d78db895ae01ec7da48f66625#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [34] [2026AI <em class="highlight">Agent</em>六大趋势，编程热潮后谁是下一个风口？](http://mp.weixin.qq.com/s?__biz=MzUyMDQ5NzI5Mg==&mid=2247607636&idx=1&sn=049c0e982eca7f8e3fa32562762f5db8&chksm=f8939b6fd13d5792bc6821b226227109264a027d08b72842515697d19bf5e0f6d95c2536c6aa#rd)
*混沌学园*

Main category: wechat.article

TL;DR: 4.智能体式（agentic）商业模式的基础正在巩固实现完全自主购物的最大障碍之一，在于如何促成安全、实时的交易。新一批初创公司正在正面解决这一挑战，他们正在构建AI原生支付轨道和数字钱包，使用户能够授权并限制AI智


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 4.智能体式（agentic）商业模式的基础正在巩固实现完全自主购物的最大障碍之一，在于如何促成安全、实时的交易。新一批初创公司正在正面解决这一挑战，他们正在构建AI原生支付轨道和数字钱包，使用户能够授权并限制AI智

</details>


### [35] [十方智库 | 毕马威《Agentic AI优势：释放下一级别的价值》（附下载）](http://mp.weixin.qq.com/s?__biz=MzkwNTc1NjY5Nw==&mid=2247484683&idx=1&sn=4bd948d00736e4829d71f8fb6272457f&chksm=c1bc52c3d5675f57b184bc0168ddd67b9b2eb7569a9756a7edb51ea43bc80dd43e1fef1671bd#rd)
*十方引力*

Main category: wechat.article

TL;DR: 四大应用框架：按需选择Agentic类型毕马威提出TACO框架，将Agentic AI分为四类，对应不同业务场景◆ 任务执行者：处理规则明确的任务（如发票审核、合规筛查），适用于重复性流程。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 四大应用框架：按需选择Agentic类型毕马威提出TACO框架，将Agentic AI分为四类，对应不同业务场景◆ 任务执行者：处理规则明确的任务（如发票审核、合规筛查），适用于重复性流程。

</details>


### [36] [<em class="highlight">Agentic</em> 框架系列（一）：AI 不再只是“等你提问”](http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484048&idx=1&sn=7cf159880ebb33626b4af517526f253e&chksm=fead07f5f8a4f474751a098b33f1ac6ebfb4216ea588a375bb4ddb98c8b8fdb30fcef9939396#rd)
*AI 知行社 Lab*

Main category: wechat.article

TL;DR: agentic 框架：从对话工具到自主系统。对话工具 自主系统 planning reflect learn 四大核心组成Agentic 框架的基本模块如下：模块作用示例Memory（记忆）让模型记住上下文、历史任务或用户偏好


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic 框架：从对话工具到自主系统。对话工具 自主系统 planning reflect learn 四大核心组成Agentic 框架的基本模块如下：模块作用示例Memory（记忆）让模型记住上下文、历史任务或用户偏好

</details>


### [37] [当我们说 <em class="highlight">Agentic</em> 的时候，我们在说什么?](http://mp.weixin.qq.com/s?__biz=Mzg3NTY3Mjk1MA==&mid=2247483813&idx=1&sn=7cde6572ebab36f9d897d27e79dede20&chksm=cebd1034c8fb696da9e68dbee28070ec5897bd9a4f657a7d118909549c27bf982ce4c79156d4#rd)
*前端通道*

Main category: wechat.article

TL;DR: 什么是 Agentic？Agentic 源自 "Agent"（代理），在 AI 领域特指具有自主性、目标导向和决策能力的智能系统。与传统的被动式 AI 系统不同，Agentic 系统能够： 自主规划：根据目标制定执行计划


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 什么是 Agentic？Agentic 源自 "Agent"（代理），在 AI 领域特指具有自主性、目标导向和决策能力的智能系统。与传统的被动式 AI 系统不同，Agentic 系统能够： 自主规划：根据目标制定执行计划

</details>


### [38] [把握<em class="highlight">Agentic</em> AI全景：一张图读懂8层技术栈与落地路线](http://mp.weixin.qq.com/s?__biz=Mzk0MzY5ODcxOQ==&mid=2247486211&idx=1&sn=b92d049c5f57cb214a76fff876ddb176&chksm=c26a5245bb91ade44b49e447298fc8cb607471464fc8d32cc306bf47c3dc128b1e3bf5b0e90f#rd)
*AI Encyclopedia*

Main category: wechat.article

TL;DR: 这张「Agentic AI Tech Stack」图，把构建智能代理（Agentic AI）所需的8大层级浓缩在一页：部署基础设施、评估监控、基础模型、编排框架、向量数据库、向量/Embedding 模型、数据抽取、以及记忆/上下文管理。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这张「Agentic AI Tech Stack」图，把构建智能代理（Agentic AI）所需的8大层级浓缩在一页：部署基础设施、评估监控、基础模型、编排框架、向量数据库、向量/Embedding 模型、数据抽取、以及记忆/上下文管理。

</details>


### [39] [浅析SecOps中的AI Agent和<em class="highlight">Agentic</em> AI，以及SOC自主化水平模型](http://mp.weixin.qq.com/s?__biz=MzUyNzMxOTAwMw==&mid=2247485048&idx=1&sn=4bceff5bb6514bacc86b69ce83b0fca1&chksm=fbc0e87fb2bc708c99bae3d8a1a1545bccc2c894403fd248fa28d60dd9a7b7a849b602ddbcf7#rd)
*专注安管平台*

Main category: wechat.article

TL;DR: 从笔者提出用Agentic AI重塑SOC平台，并在5月21日正式发布了AI赋能+数据与流程双轮驱动的SOC4.0理念和国内首个Agentic SOP产品，已经过去了半年。今年以来，全球范围内Agentic SOP / SOC如雨后春笋般不断涌现。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从笔者提出用Agentic AI重塑SOC平台，并在5月21日正式发布了AI赋能+数据与流程双轮驱动的SOC4.0理念和国内首个Agentic SOP产品，已经过去了半年。今年以来，全球范围内Agentic SOP / SOC如雨后春笋般不断涌现。

</details>


### [40] [<em class="highlight">Agentic</em> RS: <em class="highlight">智能体</em>推荐系统综述](http://mp.weixin.qq.com/s?__biz=MzA4NTUxNTE4Ng==&mid=2247527448&idx=2&sn=b6d98c03c92e7f25a5f45396bb435e02&chksm=9e1e3bd36505d350ff474e46b856ad0a7c0594b0d41d44cf14895fce13899f37db6e151b5902#rd)
*机器学习与推荐算法*

Main category: wechat.article

TL;DR: 作者提出的 Agentic Recommender System （ARS） 是一种具备自主决策与行动能力的推荐范式。它的目标不只是“预测你喜欢什么”，而是“主动理解你现在需要什么、未来可能会需要什么”，并通过长期互动不断自我进化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 作者提出的 Agentic Recommender System （ARS） 是一种具备自主决策与行动能力的推荐范式。它的目标不只是“预测你喜欢什么”，而是“主动理解你现在需要什么、未来可能会需要什么”，并通过长期互动不断自我进化。

</details>


### [41] [智能社会的组织模式：<em class="highlight">智能体</em>组织（<em class="highlight">Agentic</em> Organization）](http://mp.weixin.qq.com/s?__biz=MzIwMTQ1MzMwNQ==&mid=2458175909&idx=1&sn=c14b11fda467ac3ab26e026c177a3b1d&chksm=8096a2c7a02b216a3cd5c8f5372c4cf2b45cf99d211de19337b5be7a92eb6f0aa9c2eb0515be#rd)
*数图笔记*

Main category: wechat.article

TL;DR: 2. 智能体系统复杂度的递进（从工具到引擎）麦肯锡将智能体系统分为四个阶段，复杂度和价值递增，要求组织克服不同的关键制约因素：智能体 系统 定义 主要益处 关键制约因素


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2. 智能体系统复杂度的递进（从工具到引擎）麦肯锡将智能体系统分为四个阶段，复杂度和价值递增，要求组织克服不同的关键制约因素：智能体 系统 定义 主要益处 关键制约因素

</details>


### [42] [<em class="highlight">Agentic</em> AI vs AI Agent：从“工具执行者”到“目标中枢”的智能革命](http://mp.weixin.qq.com/s?__biz=MzUyNDgyNTg2Ng==&mid=2247491039&idx=1&sn=6d9cf2903032c03a7b701bc2e245edbb&chksm=fb255db1c7938aade5ef46c3361b82d4e8db0c68975ce5faa3db742fde8a923ad07f094b36d9#rd)
*三丰述码*

Main category: wechat.article

TL;DR: agentic ai。升级版智能中枢：什么是agentic ai？如果说AI Agent是“工匠”，那么Agentic AI就是项目经理+智能团队——它是一个目标导向的智能中枢，能自主理解终极目标、规划执行路径、协调多类资源（包括多个AI Agent），并动态优


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai。升级版智能中枢：什么是agentic ai？如果说AI Agent是“工匠”，那么Agentic AI就是项目经理+智能团队——它是一个目标导向的智能中枢，能自主理解终极目标、规划执行路径、协调多类资源（包括多个AI Agent），并动态优

</details>


### [43] [麦肯锡 <em class="highlight">Agentic</em> AI 生存手册：把“演示台上的惊艳”，变成“报表上的产出”](http://mp.weixin.qq.com/s?__biz=Mzg2MjgzMTE1NQ==&mid=2247489685&idx=1&sn=64e2f109cd7a46bc450b234bf2b70377&chksm=cfcee17bfac22ced713156acfad99f37efe62b3b1d12529ced7b2ebfdf4b9a97b86142607a88#rd)
*行客科技*

Main category: wechat.article

TL;DR: ② Agentic AI代理不是万金油：先看“球风”，再定“阵容”高标准化、低方差（开户披露、合规报送）：用规则/传统模型更可靠更便宜。低标准化、高方差（复杂文档抽取+合规核对）：代理优势才明显。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ② Agentic AI代理不是万金油：先看“球风”，再定“阵容”高标准化、低方差（开户披露、合规报送）：用规则/传统模型更可靠更便宜。低标准化、高方差（复杂文档抽取+合规核对）：代理优势才明显。

</details>


### [44] [<em class="highlight">Agentic</em> AI 的新起点：当每个人都能“教会”AI 新技能](http://mp.weixin.qq.com/s?__biz=MzIxMTc5MTczMA==&mid=2247484537&idx=1&sn=2cc866b4e1e4cc593e7b684d4fe8b964&chksm=9691514bc77e52b2715a60093d3c63c69f0781847412fbc06c9c5b417077ea5639e6022033c3#rd)
*HR科技达人*

Main category: wechat.article

TL;DR: 而这次的「Claude Skills」，感觉又在引领Agentic的AI的方向了。Google的A2A解决了Agents协作的问题，但不涉及本身Agents能力的平民化。如果历史再一次重演，谷歌的这个动作会定义HR AI Agents的未来


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而这次的「Claude Skills」，感觉又在引领Agentic的AI的方向了。Google的A2A解决了Agents协作的问题，但不涉及本身Agents能力的平民化。如果历史再一次重演，谷歌的这个动作会定义HR AI Agents的未来

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [45] [Is your team building or scaling AI agents?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fredis.io%2Fresources%2Fmanaging-memory-for-ai-agents%2F%3Futm_source=%5Bname-of-provider%5D%26utm_medium=cpa%26utm_campaign=2025-10-ai_in_production%26utm_content=eb-managing_memory_for_ai_agents-701N100000aRaU6/2/0100019a012357df-27ed190c-3b66-489e-9f4f-44f11a601280-000000/Jpey4tzzcmvRKlQT20XlppMvhVKZXLf2VjFuGmkhaCs=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该报告分析了AI代理系统中内存的重要性，探讨了短期、长期和持久性内存在代理性能中的作用，以及实时架构如何实现可扩展的AI系统。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理面临的内存挑战，包括上下文丢失、不一致性和可扩展性限制，强调内存作为可扩展AI系统基础的重要性。

Method: 通过O'Reilly和Redis合作发布的报告，分析内存架构在AI代理系统中的作用，涵盖短期、长期和持久性内存的配置。

Result: 报告阐明了实时内存架构如何使AI代理能够更好地保留、回忆和记忆信息，从而提高系统性能和可扩展性。

Conclusion: 内存是构建可扩展AI系统的关键基础，实时内存架构能够有效解决AI代理面临的内存挑战。

Abstract: Is your team building or scaling AI agents? (Sponsor) One of AI's biggest challenges today is memory—how agents retain, recall, and remember over time. Without it, even the best models struggle with context loss, inconsistency, and limited scalability.This new O'Reilly + Redis report breaks down why memory is the foundation of scalable AI systems and how real-time architectures make it possible. Inside the report: The role of short term, long term, and persistent memory in agent performance F...

</details>


### [46] [Ondeva: Replace Fragile Scripts with Reliable Workflows](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ondeva是一个面向开发者的工作流自动化工具，旨在替代脆弱的脚本和Cron作业，提供可靠、可测试的工作流管理。


<details>
  <summary>Details</summary>
Motivation: 解决开发中胶水代码、内部自动化和仪表板管理的脆弱性问题，替代不稳定的Cron作业和玩具级自动化工具。

Method: 构建专门为开发者设计的工作流自动化平台，提供执行日志记录、可测试性和故障恢复能力，同时为非技术用户提供安全护栏。

Result: 能够将临时脚本转换为可靠的工作流，使每个执行可见、每个故障可恢复，让开发人员从运维任务中解放出来。

Conclusion: Ondeva为开发团队提供了专业级的工作流自动化解决方案，提高了自动化流程的可靠性和可维护性。

Abstract: Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...

</details>


### [47] [Built for reliability:](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ondeva是一个面向开发者的工作流自动化平台，旨在替代脆弱的脚本和Cron作业，提供可靠、可测试的工作流管理。


<details>
  <summary>Details</summary>
Motivation: 解决传统脚本和自动化工具的脆弱性问题，让开发人员能够构建可靠的工作流自动化，减少维护成本。

Method: 提供工作流自动化平台，支持执行日志记录、测试和故障恢复，为非技术用户提供安全护栏。

Result: 开发人员可以节省时间，让运维和最终用户能够构建安全的自动化流程。

Conclusion: Ondeva能够帮助开发团队从繁琐的脚本维护中解放出来，专注于真正的工程工作。

Abstract: Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...

</details>


### [48] [Reclaim dev hours:](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ondeva是一个面向开发者的工作流自动化平台，旨在取代脆弱的脚本和Cron作业，提供可靠、可测试的工作流管理


<details>
  <summary>Details</summary>
Motivation: 解决传统脚本和自动化工具的脆弱性问题，为开发者提供更可靠的工作流自动化解决方案，减少开发时间浪费

Method: 构建专门为开发者设计的工作流自动化平台，提供执行日志记录、可测试的工作流、故障恢复机制，并为运营和最终用户提供安全自动化构建的防护措施

Result: 实现了可靠的工作流执行，每个执行都可见，每个故障都可恢复，让开发者能够专注于真正的工程工作

Conclusion: Ondeva成功替代了脆弱的脚本和玩具自动化工具，为开发者提供了专业级的工作流自动化解决方案

Abstract: Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...

</details>


### [49] [Centralize automations](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ondeva是一个面向开发者的工作流自动化平台，旨在替代脆弱的脚本和玩具自动化工具，提供可靠、可测试的工作流管理。


<details>
  <summary>Details</summary>
Motivation: 解决传统Cron作业和简单自动化工具在处理粘合代码、内部自动化和仪表板时的脆弱性问题，提高自动化流程的可靠性和可维护性。

Method: 构建专门为开发者设计的工作流自动化平台，提供执行日志记录、可测试的工作流、故障恢复机制，并为运营人员和最终用户提供安全护栏。

Result: 能够将临时脚本替换为可靠的工作流，使每个执行可见、每个故障可恢复，同时让开发人员从繁琐的运维工作中解放出来。

Conclusion: Ondeva为开发者提供了专业级的工作流自动化解决方案，显著提升了自动化流程的可靠性和开发效率。

Abstract: Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...

</details>


### [50] [Scales with you](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ondeva是一个面向开发者的工作流自动化平台，旨在替代脆弱的脚本和Cron作业，提供可靠、可测试的工作流管理。


<details>
  <summary>Details</summary>
Motivation: 解决传统脚本和自动化工具的脆弱性问题，为开发者提供更可靠的工作流自动化解决方案，减少维护成本。

Method: 构建专门针对开发者的工作流自动化平台，提供执行日志记录、可测试性和故障恢复功能，让运维和终端用户能够在安全护栏下构建自动化。

Result: 开发出能够替代临时脚本的可靠工作流系统，每个执行都可见，每个故障都可恢复。

Conclusion: Ondeva通过提供专业级的工作流自动化工具，帮助开发者从繁琐的运维工作中解放出来，专注于核心工程任务。

Abstract: Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...

</details>


### [51] [Stagehand](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbrowserbase%2Fstagehand%3Futm_source=tldrdevops/1/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Cy7ViySh1Tfs6wu_RAa1_y798J6IwOTuDU-N4Vci8XQ=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Stagehand是一个AI浏览器自动化框架，允许开发者使用代码和自然语言自动化网页任务，集成Playwright和OpenAI、Anthropic的最先进计算机使用模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决网页自动化任务中需要编写复杂代码的问题，提供更直观的自然语言交互方式，同时提高自动化效率和节省计算资源。

Method: 集成Playwright浏览器自动化工具和OpenAI、Anthropic的SOTA计算机使用模型，支持动作预览和缓存功能。

Result: 开发了一个能够通过自然语言指令自动化网页任务的框架，减少了编码工作量，提高了开发效率。

Conclusion: Stagehand框架成功地将自然语言处理与浏览器自动化相结合，为网页任务自动化提供了更便捷的解决方案。

Abstract: Stagehand (GitHub Repo) Stagehand is an AI browser automation framework that allows developers to automate web tasks using both code and natural language. It integrates with Playwright and SOTA computer use models from OpenAI and Anthropic, allowing users to preview and cache actions to save time and tokens.

</details>


### [52] [Flowistry](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fwillcrichton%2Fflowistry%3Futm_source=tldrwebdev/1/0100019a01626ee5-1c115f73-d9e0-4048-a532-f3a27de1054b-000000/o05uNnBs1wXyMGvn_k8cxiXbKtPZnASUVH8Go4WUqaQ=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Flowistry是一个VSCode插件，通过信息流分析帮助开发者聚焦相关代码，通过淡化无关代码来理解变量或表达式的影响。


<details>
  <summary>Details</summary>
Motivation: 帮助开发者更好地理解复杂函数中特定变量或表达式的影响范围，提高代码理解效率。

Method: 开发VSCode插件，使用信息流分析技术，识别并淡化显示与当前焦点无关的代码。

Result: 实现了能够分析信息流并可视化相关代码的插件，帮助开发者更高效地理解代码逻辑。

Conclusion: Flowistry通过信息流分析有效提升了代码理解效率，特别是在处理复杂函数时。

Abstract: Flowistry (GitHub Repo) Flowistry is a VSCode plugin for Rust that analyzes information flow to help devs focus on relevant code. By fading out irrelevant code, Flowistry allows users to understand the impact of specific variables or expressions, making it easier to comprehend complex functions.

</details>


### [53] [Everyone should be using Claude Code more](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lennysnewsletter.com%2Fp%2Feveryone-should-be-using-claude-code%3Futm_source=tldrfounders/1/0100019a0189af5f-86313233-2070-4f3a-8397-bf9df6adfa79-000000/Zpbi4k0gjOndYsbC7hyMImdMAXnItb6y5FIb5WDqyhY=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code是一款适用于非技术用户的本地AI工具，能高效处理文件组织、图像增强、视频下载等任务，功能超越云端AI工具。


<details>
  <summary>Details</summary>
Motivation: 为满足非技术用户对AI工具的需求，开发一款能在本地运行、功能多样且能处理大文件的AI助手。

Method: 开发本地运行的Claude Code AI工具，支持文件管理、图像处理、视频下载等多种功能，并收集用户创意应用案例。

Result: 用户成功使用Claude Code进行域名生成、客户通话记录合成、自动驾驶文档创建等多样化应用，证明其功能强大且实用。

Conclusion: Claude Code是一款功能全面、运行高效的本地AI工具，适合各类用户使用，具有广泛的应用前景。

Abstract: Everyone should be using Claude Code more (5 minute read) Claude Code offers versatile AI capabilities for non-technical users, running locally to manage tasks like file organization, image enhancement, and video downloads. It excels in handling large files efficiently and performing tasks beyond cloud-based AI tools. Users share creative applications, including generating domain names, synthesizing customer call transcripts, and creating self-driving documentation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了结构化认知循环（SCL）作为可执行的认知框架，将哲学见解转化为可计算结构，强调认知是判断、记忆、控制、行动和调节的持续循环过程。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型缺乏真正认知理解的问题，填补认知架构的空白，从本体论转向认识论视角探索认知涌现的条件。

Method: 基于过程哲学、具身认知和扩展心智理论，构建可执行的认知框架，将功能分离的认知架构与基于提示的单体系统进行对比评估。

Result: 功能分离的认知架构比单体系统产生更连贯和可解释的行为，支持智能不是表征准确性而是通过意向性理解重建自身认知状态的能力。

Conclusion: 真正的进步需要实现认知原则的结构化架构，而非更大的模型，该框架对心智哲学、认识论和人工智能具有重要影响。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [55] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: 提出了PISA记忆系统，受皮亚杰认知发展理论启发，通过三模态适应机制和混合记忆访问架构，显著提升了AI代理的适应性和长期知识保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，忽视了记忆的建设性和任务导向作用，需要更灵活、自适应的记忆架构。

Method: 基于皮亚杰理论构建PISA记忆系统，采用三模态适应机制（图式更新、图式演化和图式创建），结合符号推理与神经检索的混合记忆访问架构。

Result: 在LOCOMO基准和新提出的AggQA数据分析基准上的实验表明，PISA在适应性和长期知识保持方面达到了新的最先进水平。

Conclusion: PISA通过建设性记忆方法和灵活适应机制，为AI代理提供了更有效的记忆系统，显著提升了任务适应性和知识保持能力。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [56] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究发现大型推理模型在解决复杂谜题时会出现性能崩溃，即使提供环境接口也无法避免。模型在复杂度增加时表现出模式崩溃，性能取决于模式是否匹配正确解。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型在解决复杂谜题时性能崩溃的原因，特别是环境状态跟踪是否影响推理能力评估。

Method: 为LLM提供汉诺塔问题的环境接口，允许其通过工具调用进行操作、提供书面理由、观察状态空间并重新提示下一步。

Result: 环境接口访问无法延迟或消除性能崩溃。策略分析显示模型与最优策略和随机策略的偏离度增加，表现出模式崩溃。

Conclusion: 性能崩溃是模型内在问题，类似现象可能存在于大型推理模型中。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [57] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: 提出了TEAM-PHI多智能体评估框架，使用LLM自动评估PHI去标识化质量并选择最佳模型，无需依赖大量人工标注。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化对于安全重用临床笔记至关重要，但传统评估依赖昂贵的小规模专家标注，需要更高效的自动评估方法。

Method: 部署多个评估智能体独立判断PHI提取正确性，通过LLM多数投票机制整合结果，产生稳定可复现的排名。

Result: 在真实临床笔记语料上的实验表明，TEAM-PHI产生一致准确的排名，LLM投票可靠地收敛于相同的最佳系统。

Conclusion: TEAM-PHI通过结合独立评估智能体和LLM多数投票，为PHI去标识化提供了实用、安全且经济高效的自动评估解决方案。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [58] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，这是一个基于检索增强的研究想法评估系统，用于评估AI生成研究想法的合理性和贡献度。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在科研构思中的普及，需要建立可靠的评估机制来确保生成想法的有效性和实用性。

Method: 开发了ScholarEval评估框架，基于两个核心标准：合理性（基于现有文献的方法有效性）和贡献度（相对于先前研究的进步程度）。创建了ScholarIdeas数据集，包含117个跨学科研究想法和专家标注。

Result: ScholarEval在覆盖人类专家标注要点方面显著优于所有基线方法，在评估可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。大规模用户研究显示在文献参与、想法精炼和实用性方面表现优异。

Conclusion: ScholarEval为AI生成研究想法的评估提供了有效的解决方案，显著优于现有方法，并开源了代码、数据集和工具。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [59] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 论文识别并系统分析了大型推理模型中的"推理分心"漏洞，即模型被恶意嵌入的复杂无关任务分散注意力，导致主要任务准确率下降高达60%。作者提出结合监督微调和强化学习的训练防御方法，将鲁棒性提高了50多个点。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务上表现出色，但存在被恶意嵌入的无关复杂任务分散注意力的关键漏洞，影响模型可靠性。

Method: 通过跨模型和基准的综合研究分析推理分心漏洞，提出结合监督微调(SFT)和强化学习(RL)在合成对抗数据上的训练防御方法。

Result: 最先进的大型推理模型高度易受攻击，注入的干扰物使任务准确率下降高达60%。提出的防御方法在挑战性干扰攻击上将鲁棒性提高了50多个点。

Conclusion: 推理分心是对大型推理模型可靠性的独特且紧迫的威胁，提出的防御方法为实现更安全可信的推理系统提供了实用步骤。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [60] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 本文对基于网络交互的智能体系统进行了效率瓶颈分析，提出SpecCache缓存框架，通过推测执行显著减少网络环境延迟，提升系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能体系统的推理性能，但忽视了系统效率问题。网络交互带来的延迟成为影响智能体系统实际应用的关键瓶颈。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，通过实证研究分析15个模型和5个提供商，提出SpecCache缓存框架结合推测执行来优化延迟。

Result: 网络环境延迟可占系统总延迟的53.7%，SpecCache相比随机缓存策略将缓存命中率提升58倍，网络环境开销减少3.2倍，且不降低系统性能。

Conclusion: 智能体系统的效率优化至关重要，SpecCache框架能有效解决网络环境延迟问题，为实际部署提供重要参考。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [61] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个紧凑的知识图谱和符号验证器系统，用于在推理任务中强制执行数学可解释规则，显著提高LLM的推理准确性并消除规则违反。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时经常产生流畅但违反简单数学或逻辑约束的步骤，需要一种方法来确保推理的数学一致性。

Method: 引入MedRule-KG，一个紧凑的类型化知识图谱，结合符号验证器来编码实体、关系和领域启发规则，验证预测并应用最小修正以保证一致性。

Result: 在90个FDA衍生基准测试中，基于MedRule-KG的推理将精确匹配从0.767提高到0.900，添加验证器后达到1.000精确匹配，完全消除规则违反。

Conclusion: MedRule-KG为安全的数学推理提供了一个通用框架，能够有效纠正LLM的推理错误并确保逻辑一致性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [62] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 提出了一种结合监控-生成-验证的三阶段迭代系统，将策略规划和验证机制整合，在GSM8K上取得了75.42%的准确率，优于现有方法且需要更少的迭代次数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法存在分离：监控-生成方法擅长策略规划但缺乏验证机制，生成-验证方法能迭代优化但缺乏前期策略评估。这种分离导致策略失败无反馈、优化缺乏战略基础的问题。

Method: 基于Flavell的认知监控模型，实现监控-生成-验证三阶段迭代系统，将策略规划和验证机制整合在一起。

Result: 在GSM8K数据集上达到75.42%准确率，优于SELF-REFINE的68.44%和Self-Verification的67.07%，且平均尝试次数更少（1.3 vs 2.0），推理成本增加27-37%。

Conclusion: 前期监控能产生更高质量的初始解决方案，减少后续优化需求，但需要在算术推理之外的领域进一步验证通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [63] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: 提出了RGMem框架，一种基于重整化群思想的自进化记忆系统，用于解决LLM对话系统中长期用户建模和行为一致性问题


<details>
  <summary>Details</summary>
Motivation: 现有RAG和显式记忆系统主要关注事实级存储和检索，缺乏从多轮对话中提取潜在偏好和深层特征的能力，限制了长期有效的用户建模

Method: 采用分层粗粒化和重标度操作，从片段对话中提取语义和用户洞察，逐步形成动态演化的用户画像

Result: 实现了多尺度信息压缩和涌现过程，从噪声微观交互中形成高层次准确的用户画像

Conclusion: RGMem框架能够实现语言代理的长期记忆和行为一致性，提升个性化交互的深度和跨会话连续性

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [64] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 提出了NP-ENGINE框架，用于训练和评估LLM在NP难问题上的能力，包括任务生成器、验证器和启发式求解器，并展示了在NP-BENCH基准上的SOTA性能以及强大的领域外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在数学、编程等推理任务上表现出色，但在解决NP难等复杂优化问题方面的能力尚未充分探索，需要专门的训练和评估框架。

Method: 提出NP-ENGINE框架，包含可控实例生成器、规则验证器和启发式求解器，支持可扩展的RLVR训练；采用零RLVR和课程学习训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著超越GPT-4o，达到同规模模型的SOTA性能；在领域外推理任务和非推理任务上展现出强泛化能力；任务多样性增加可提升泛化性能。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的扩展规律，NP难问题训练可增强模型在多种任务上的泛化能力。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [65] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种增强LLM推理能力的强化学习算法，通过基于计数的内在奖励激励探索，避免重复和次优的推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习范式依赖稀疏的结果奖励和有限探索，导致LLM陷入重复和次优的推理模式，需要设计更好的探索机制。

Method: 使用轻量级的Coin Flipping Network估计推理轨迹的伪计数和认知不确定性，将其转换为内在奖励，并与GRPO等先进RL框架集成。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著超越强基线，帮助策略逃离局部模式发现更好解决方案。

Conclusion: 针对性的内在动机可以使语言模型推理的探索更加可靠。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [66] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了一个基于大语言模型的智能体框架AFL，用于完全自动化解决复杂车辆路径问题，无需外部干预即可从问题实例生成可行解。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法在解决复杂车辆路径问题时仍依赖外部干预，导致自主性受限、执行错误多和解可行性低。需要实现从问题到解决方案的完全自动化。

Method: AFL框架将整体流程分解为三个可管理的子任务，使用四个专业智能体通过协调交互确保跨功能一致性和逻辑合理性，直接从原始输入提取知识并生成自包含代码。

Result: 在60个复杂VRP问题上的实验表明，该框架在代码可靠性和解可行性方面显著优于现有LLM基线，在评估基准上接近100%的成功率，性能可与精心设计的算法相媲美。

Conclusion: AFL框架实现了复杂车辆路径问题求解的完全自动化，在保持高性能的同时大幅提升了解决方案的可靠性和可行性。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [67] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 本文综述了智能体AI从基于管道的系统向模型原生范式的范式转变，其中规划、工具使用和记忆等能力从外部编排转变为模型内部参数化。


<details>
  <summary>Details</summary>
Motivation: 追踪智能体AI的范式转变，从外部逻辑编排的管道系统到能力内部化的模型原生范式，以理解AI从响应到行动、推理和适应的演进。

Method: 将强化学习定位为范式转变的算法引擎，系统回顾规划、工具使用和记忆能力从外部脚本模块到端到端学习行为的演变，并分析对深度研究代理和GUI代理应用的影响。

Result: 揭示了智能体AI向模型原生范式的统一发展轨迹，其中LLM+RL+任务构成了跨语言、视觉和具身领域的统一解决方案。

Conclusion: 智能体AI正朝着模型原生方向发展，从构建应用智能的系统转向开发通过经验增长智能的模型，标志着从应用智能到生长智能的转变。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [68] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 该论文是关于基于强化学习的智能搜索代理的综述，系统梳理了这一新兴领域，从RL的功能角色、优化策略和应用范围三个维度进行组织。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法存在单轮交互、启发式检索的局限性，而智能搜索代理通过多步交互解决了这些问题。强化学习为智能搜索提供了自适应和自我改进的机制。

Method: 采用综述研究方法，从三个维度组织该领域：RL的功能角色、优化策略和应用范围。总结了代表性方法、评估协议和应用案例。

Result: 提供了该领域的首个全面概述，建立了系统化的分类框架，并创建了开源资源库供研究者参考。

Conclusion: 强化学习驱动的智能搜索系统具有广阔前景，该综述旨在激发未来RL与智能搜索融合的研究。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [69] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的智能LLM，能够自动完成从数据源到分析师级深度研究报告的端到端流程，通过课程式智能训练在8B参数下超越基于工作流的先进专有LLM。


<details>
  <summary>Details</summary>
Motivation: 现有的基于工作流的数据代理在特定数据任务上表现良好，但由于依赖预定义工作流，无法实现完全自主的数据科学。需要开发能够自动完成端到端数据科学流程的智能代理。

Method: 提出课程式智能训练范式，模拟人类数据科学家的学习轨迹，让LLM在真实环境中逐步获取和整合多种能力；同时引入数据驱动的轨迹合成框架构建高质量训练数据。

Result: 仅用8B参数的DeepAnalyze在数据问答、专业分析任务和开放式数据研究等广泛数据任务上表现出色，超越了基于最先进专有LLM构建的工作流代理。

Conclusion: DeepAnalyze为自主数据科学开辟了道路，模型、代码和训练数据均已开源。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [70] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 本文提出通过强化学习训练VLM智能体进行显式视觉状态推理，构建内部世界模型，在五个多样化智能体基准测试中实现了3倍性能提升，超越了GPT-5等专有推理模型。


<details>
  <summary>Details</summary>
Motivation: 训练视觉语言模型智能体的关键挑战在于从文本状态转向复杂视觉观察，这引入了部分可观测性并需要强大的世界建模能力。

Method: 将智能体推理过程架构性地通过强化学习进行强制和奖励，将其建模为部分可观测马尔可夫决策过程，分解为状态估计和转移建模，并设计了世界建模奖励和双层通用优势估计方法。

Result: 3B参数模型在五个多样化智能体基准测试中达到0.82分，相比未训练版本(0.21)提升3倍，超越了GPT-5(0.75)、Gemini 2.5 Pro(0.67)和Claude 4.5(0.62)。

Conclusion: 视觉状态推理是VLM智能体成功的关键，最优表示形式取决于任务性质，自然语言适合语义关系捕捉，结构化格式对精确操作控制至关重要。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [71] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 提出了一种新的评估方法，测试用户能否从强化学习算法的决策解释中识别出代理的目标。在Atari的Ms. Pacman环境中测试了四种可解释强化学习算法，发现只有一种算法的准确率超过随机水平，且用户普遍高估自己的判断能力。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法在调试方面有核心应用价值，但缺乏对其相对性能的比较评估。

Method: 使用Atari的Ms. Pacman环境和四种XRL算法，通过用户测试来评估他们从算法解释中识别代理目标的能力。

Result: 只有一种XRL算法在测试目标上的准确率超过随机水平；用户普遍对自己的选择过度自信；用户自我报告的识别难易度和理解程度与实际准确率不相关。

Conclusion: 当前的可解释强化学习算法在帮助用户理解代理目标方面效果有限，用户的主观感受与实际理解能力存在脱节。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [72] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一个基于多智能体协作的LLM框架，用于GPU内核优化，通过系统探索设计空间、动态上下文管理和策略搜索，显著提升了内核性能。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对AI发展至关重要，但现有方法将LLM视为单次生成器或简单优化工具，难以应对复杂的内核优化挑战。

Method: 采用多智能体协作框架，结合基于经验的指导、动态上下文管理和策略搜索，模拟专家工程师的工作流程。

Result: 在KernelBench基准测试中，系统在基线失败的情况下仍能产生正确解决方案，并实现高达16倍的运行时性能提升。

Conclusion: 智能体LLM框架在实现完全自动化、可扩展的GPU内核优化方面具有巨大潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [73] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进LLM在工具增强对话中的行为，通过检测8种特定错误类型并提供针对性反馈，将工具调用准确率提高了13%。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在现实应用中日益普及，但工具使用错误仍然阻碍其可靠性，需要专门的诊断和改进框架。

Method: 引入ToolCritic框架，定义8种工具调用错误类型，构建合成数据集训练ToolCritic，让主LLM根据ToolCritic的反馈修正响应。

Result: 在Schema-Guided Dialogue数据集上的实验表明，ToolCritic将工具调用准确率提高了13%，优于零样本提示和自我修正技术。

Conclusion: ToolCritic代表了在现实世界对话应用中更稳健地集成LLM与外部工具的有希望的一步。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [74] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 开发并评估了两种基于LLM的系统用于企业信贷评估中的证据推理：单代理系统(NAS)和多代理辩论系统(KPD-MADS)，后者基于卡尔·波普尔的批判性对话框架，在推理质量和实用性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决企业信贷评估中定性非财务指标自动化推理的挑战，现有方法主要关注数值预测，缺乏对专业贷款评估所需解释性判断的支持。

Method: 开发了两种LLM系统：单代理系统(NAS)通过单次推理管道生成双向分析；多代理辩论系统(KPD-MADS)基于卡尔·波普尔批判性对话框架，采用十步结构化交互协议进行对抗性验证。

Result: 两种系统均实现显著生产力提升(NAS:11.55秒/案例；KPD-MADS:91.97秒；人工基准:1920秒)。KPD-MADS在解释充分性(4.0 vs 3.0)、实际适用性(4.0 vs 3.0)和可用性(62.5 vs 52.5)方面获得更高评分。

Conclusion: 结构化多代理交互能够增强金融AI中的推理严谨性和可解释性，推进企业信贷评估中可扩展且可辩护的自动化。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [75] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究了一个部署在网上的、工具增强的LLM健康教练，通过离线策略评估发现统一的重工具策略虽然提高平均价值，但对特定用户群体（特别是健康素养低但自我效能高的用户）有害。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过评估优先的方法实现个性化AI健康教练，避免统一策略对特定用户群体的潜在伤害。

Method: 使用离线策略评估（OPE）分析因子化决策头（工具/风格），并通过轻量级模拟器验证添加早期信息增益奖励的效果。

Result: 统一重工具策略会损害特定用户群体；添加信息增益奖励能缩短特质识别时间，提高目标成功率和pass@3指标。

Conclusion: 建议采用评估优先的个性化路径：冻结生成器，在类型化奖励上学习子群体感知的决策头，并始终报告按原型分类的指标以揭示被平均值掩盖的子群体伤害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [76] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 提出FBI_LTL，一种用于模拟规划问题的多样化规划器，使用线性时序逻辑定义语义多样性标准，生成语义多样化的计划。


<details>
  <summary>Details</summary>
Motivation: 传统规划器只生成单一计划，可能无法满足代理偏好。现有多样化规划方法可能产生语法不同但语义相同的解决方案。

Method: FBI_LTL利用线性时序逻辑定义语义多样性标准，并将这些LTL多样性模型直接集成到搜索过程中。

Result: 在多个基准测试上的广泛评估表明，FBI_LTL相比基线方法能生成更多样化的计划。

Conclusion: 这项工作确立了在模拟环境中语义引导多样化规划的可行性，为在传统基于模型方法失效的现实非符号领域开辟了新途径。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [77] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 该研究提出了一种将大型语言模型的推理能力蒸馏到更小、更高效模型的方法，通过结构感知损失优化来学习正确的解决路径和问题与解决方案之间的结构对应关系。


<details>
  <summary>Details</summary>
Motivation: 代码生成不仅需要准确的标记预测，更需要理解解决方案级别的结构关系。大型语言模型具备复杂任务的推理能力，但这些能力在小型模型中可能缺失，因此需要将推理能力蒸馏到更小、更高效的模型中。

Method: 通过结构感知损失优化训练模型，使其能够模拟大型语言模型的推理和问题解决能力，学习识别正确的解决路径，并建立问题定义与潜在解决方案之间的结构对应关系。

Result: 实验结果显示，经过微调的模型在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过廉价且易于实现的过程开发的微调模型能够超越标记级生成，深入理解给定问题的解决方案整体结构。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [78] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 该论文提出使用多智能体影响图(MAIDs)作为图形框架来解决多智能体强化学习中的协调问题，设计了基于MAIDs的目标干预范式，并通过因果推理技术实现单智能体干预，以缓解全局指导的困难。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体强化学习中，对整个系统进行全局人工指导不切实际，而现有的协调机制设计主要依赖经验研究，缺乏易用的研究工具。

Method: 采用多智能体影响图(MAIDs)作为图形框架，设计了目标干预范式，并引入预策略干预(PSI)因果推理技术来实现该范式，通过最大化因果效应来达成复合期望结果。

Result: 实验证明了所提出的目标干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs提供了一个有效的框架来分析和可视化MARL方法，目标干预范式能够缓解全局指导问题，PSI技术能够有效实现期望结果。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [79] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了一种名为上下文注意力调制（CAM）的新机制，通过动态调节LLM中自注意力模块的表征来增强任务特定特征，同时保留通用知识。进一步开发了混合上下文注意力调制（HyCAM）框架，结合共享的完整参数CAM模块和多个轻量级专用CAM模块，实现更有效的多任务适应。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多任务适应方面存在困难，传统微调方法会导致灾难性遗忘和资源消耗大，现有参数高效方法在复杂多任务场景下表现不佳。

Method: 提出CAM机制动态调制自注意力模块表征，开发HyCAM框架结合共享CAM模块和专用CAM模块，采用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的实验表明，该方法显著优于现有方法，平均性能提升3.65%。

Conclusion: CAM和HyCAM框架能够有效解决LLM在多任务适应中的平衡问题，在保持通用知识的同时增强任务特定能力。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，通过实时更新奖励分布的后验信念来决定何时停止生成新样本，在保持响应质量的同时显著减少采样次数。


<details>
  <summary>Details</summary>
Motivation: 多响应采样能提高LLM输出质量，但会增加计算成本。关键挑战是如何平衡准确性和效率，决定何时停止生成新样本。

Method: 基于序列搜索和贝叶斯学习的自适应采样框架，顺序生成响应、实时更新奖励分布后验信念，权衡预期收益与计算成本来决定停止时机。

Result: BEACON平均减少80%的采样次数，同时保持响应质量，在成本高效的偏好数据生成中表现出实用性。

Conclusion: BEACON提供了理论最优性保证和实际可操作性，为未来研究者提供了可行的自适应采样解决方案。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [81] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM是一个基于代理的框架，利用大语言模型自动将非线性优化问题转化为线性优化问题，通过专门的代理处理不同类型的非线性模式。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的线性化通常需要人工操作且依赖专家知识，这限制了线性优化求解器的应用。研究旨在自动化这一过程，使非线性问题能够直接使用线性求解器解决。

Method: 采用基于代理的框架，为每种非线性模式分配专门的reformulation代理，这些代理被明确指示为其非线性模式推导精确的线性重构，然后协调组装成求解器就绪的线性模型。

Result: 在基于ComplexOR数据集的20个真实世界非线性优化问题上进行评估，结果表明专门的LLM代理能够自动化线性化任务。

Conclusion: 专门的LLM代理可以自动化线性化任务，为非线性优化的完全对话式建模流程开辟了道路。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [82] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 本文提出IB-FT方法，通过信息瓶颈理论指导代码LLMs的微调，解决传统微调中存在的记忆障碍问题，提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 发现预训练大语言模型在代码领域微调时存在记忆障碍问题，即模型过度记忆下游代码数据而无法有效学习新的可泛化知识。

Method: 提出基于信息瓶颈的微调方法(IB-FT)，对代码数据的隐藏表示施加IB惩罚，压缩虚假记忆特征同时保留任务相关信息。

Result: 在两个代码基准测试(OriGen和Evol-CodeAlpaca-V1)上，IB-FT显著缓解了记忆障碍，提升了top-1性能，并在更严格的多样本指标下获得更稳定的增益。

Conclusion: IB-FT方法能有效克服记忆障碍，提高代码生成模型的学习效率和泛化能力。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [83] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出Cog-Rethinker，一种分层元认知强化学习框架，通过分解零准确率问题和参考错误答案来改进LLM推理任务的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定提示模板的RL方法在弱LLM上存在样本效率低的问题，大多数问题在推理任务中产生无效输出，造成样本浪费。

Method: 采用分层元认知两阶段框架：1) 将零准确率问题分解为子问题；2) 参考先前错误答案来精炼答案。同时使用监督微调确保训练测试一致性。

Result: 在多个数学推理基准测试中表现优异，相比基线方法提高了样本效率并加速了收敛。

Conclusion: Cog-Rethinker通过元认知推理模式显著提升了LLM推理任务的样本利用效率。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [84] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 本文提出使用Kolmogorov-Smirnov检验来监测和量化AI系统中的分布偏移问题，特别是在智能交通应用中，即使KS距离为0.02也会导致强化学习智能体的旅行时间增加约50%。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统中测试数据分布与训练数据分布偏离的问题，这种分布偏移会导致AI智能体预测出现大误差，在安全关键应用中尤其危险。

Method: 使用Kolmogorov-Smirnov检验和KS距离来量化分布偏移及其对AI智能体性能的影响。

Result: 研究表明，即使KS距离仅为0.02，也会导致强化学习智能体在单个交叉路口的旅行时间增加约50%。

Conclusion: KS检验和KS距离可作为实时监测AI智能体性能退化的有价值统计工具，帮助AI智能体更有效地应对分布偏移。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [85] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化可扩展框架，通过原生可访问性API系统探索应用程序，解决GUI自动化中的数据收集挑战。该框架构建了GUIrilla-Task数据集，包含27,171个功能基础任务，覆盖1,108个macOS应用。实验表明，在该数据集上微调的LLM代理在UI任务中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决复杂图形用户界面(GUI)自动化中的数据可用性限制问题，包括昂贵的手动标注、闭源数据集和表面级合成流程，特别是在macOS生态系统中当前UI数据集代表性有限的情况下。

Method: 使用原生可访问性API系统探索应用程序，将发现的界面元素和爬虫操作组织成层次化GUI图，采用专门的交互处理程序实现全面的应用覆盖，并构建大规模数据集。

Result: 构建了GUIrilla-Task数据集(27,171个任务，1,108个应用)，在ScreenSpot Pro基准测试中优于合成基线，同时使用数据量减少97%。发布了macapptree开源库、GUIrilla-Task数据集、GUIrilla-Gold基准和框架代码。

Conclusion: GUIrilla框架成功解决了桌面GUI自动化的数据收集挑战，通过自动化方法构建的大规模数据集显著提升了LLM代理在UI任务中的性能，为桌面自主性研究提供了开放支持。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [86] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 提出了一种决策支持系统，通过限制人类可采取的行动子集来实现人机互补性，在野火缓解游戏中使参与者表现提升30%


<details>
  <summary>Details</summary>
Motivation: 在分类任务中已证明通过控制人类代理水平可以实现人机互补性，本文探索是否能在顺序决策任务中应用相同原理

Method: 使用预训练AI代理缩小人类可采取的行动集合，然后让人类从该行动集中选择行动，并引入利用行动集平滑特性的bandit算法优化人类代理水平

Result: 在1600名参与者的野火缓解游戏研究中，使用该系统的参与者比单独参与者表现提升约30%，比AI代理表现提升超过2%

Conclusion: 在顺序决策任务中通过控制人类代理水平可以实现人机互补性，决策支持系统能显著提升人类表现

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [87] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 提出了一种基于相似性搜索和随机表示的无训练世界模型，与Dreamer家族的PlaNet模型进行比较，在潜在重建质量和长期动态预测方面表现相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 世界模型在强化学习中广泛应用，但传统方法需要训练过程。本文旨在探索无需训练的世界模型构建方法，提高样本效率。

Method: 利用相似性搜索和随机表示来近似世界模型，避免了传统的训练过程。与PlaNet模型进行比较评估。

Result: 基于搜索的世界模型在潜在重建和图像相似性方面与基于训练的方法相当，在长期预测方面表现更优。

Conclusion: 无训练的世界模型是可行的替代方案，在长期动态预测任务中具有优势。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [88] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 论文主张强化学习研究应从单纯展示智能体能力转向更关注理解学习动态和数学基础，并以ALE为例说明现有基准仍可用于发展这种理解。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过度关注性能表现而忽视了对学习动态的理解，这可能导致方法在学术基准上过拟合，难以迁移到实际问题。

Method: 通过分析Arcade Learning Environment (ALE)基准的使用情况，论证即使被认为是"饱和"的基准仍可用于发展对强化学习的科学理解。

Result: 提出了强化学习研究应更注重科学理解和数学形式化，而非单纯追求性能表现的观点。

Conclusion: 强化学习社区需要重新平衡研究重点，从性能展示转向对学习动态和数学基础的理解，以促进技术在实际问题中的有效应用。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [89] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 提出了一种基于运行时监控语言（RML）的新型语言化奖励机，能够表达非正则、非马尔可夫任务的奖励函数，解决了传统奖励机表达能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励函数通常被视为黑盒映射，缺乏解释性，而传统奖励机只能表达正则语言，无法处理计数或参数化条件等复杂行为。

Method: 基于运行时监控语言（RML）构建新型语言化奖励机，利用RML的内置内存机制来指定非正则、非马尔可夫任务的奖励函数。

Result: 实验证明了该方法在表达能力上的优势，在灵活事件处理和任务规范方面优于现有的奖励机方法。

Conclusion: 该方法扩展了奖励机的表达能力，能够处理更复杂的非正则、非马尔可夫任务，为强化学习提供了更好的奖励规范工具。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [90] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习与对象中心表示的新框架，能够处理结构化和非结构化数据，并通过主动查询人类专家来增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理结构化问题时存在局限性，关系强化学习虽然能处理结构化问题但假设过强，需要一种能同时处理结构化和非结构化数据的更通用方法。

Method: 结合关系强化学习与对象中心表示，通过显式建模策略不确定性来主动查询人类专家指导。

Result: 实证评估表明该方法具有有效性和高效性。

Conclusion: 提出的框架能够有效处理结构化和非结构化数据，并通过主动学习机制提升性能。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [91] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: 提出了WEBSERV环境，解决了现有RL网络代理训练环境在可扩展性、上下文噪声和动作确定性方面的不足，实现了高效的并行训练。


<details>
  <summary>Details</summary>
Motivation: 现有的RL网络代理训练环境存在上下文噪声过大、动作执行不等待UI/网络稳定、无法有效扩展并行容器等问题，需要一种可扩展且高效的解决方案。

Method: 开发了WEBSERV环境，包含：1）紧凑、站点无关的浏览器环境，平衡上下文和动作复杂性；2）通过高效启动和重置web服务器实现可扩展RL环境。

Result: 在WebArena的购物CMS和Gitlab任务上达到最先进的单提示成功率，启动延迟降低约5倍，存储需求减少约240倍，内存占用相当，支持单主机200+并发容器。

Conclusion: WEBSERV环境成功解决了RL网络代理训练的可扩展性和效率问题，为大规模并行训练提供了实用解决方案。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [92] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO框架通过分离语言反馈和数值奖励的角色来解决LLM强化学习中的样本效率问题，语言指导探索，数值奖励驱动优化，显著提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，丢弃了rollout中的文本推理信息，导致样本效率低下。在线整合语言反馈存在信息泄露和无关上下文导致行为崩溃的悖论。

Method: 提出LANPO框架，构建动态经验池，采用奖励无关反思进行样本内自校正，通过相关抽象从样本间经验中提取可泛化教训。

Result: 在数学推理基准测试中，7B和14B模型显著优于使用GRPO训练的强基线模型，测试准确率大幅提升。

Conclusion: LANPO为将历史经验整合到LLM强化学习循环中提供了稳健方法，创造了更有效和数据高效的学习智能体。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [93] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 该论文提出了一个基于3D第一人称视频游戏的多模态推理环境，通过收集大规模多样化的人类游戏数据，训练文本条件智能体进行游戏操作。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏为实时多模态推理提供了具有挑战性的环境，需要解决长时程任务和跨游戏定量评估等问题。

Method: 收集大规模多样化的人类游戏数据，学习逆动力学模型来推断缺失动作，使用行为克隆训练文本条件智能体，采用支持实时推理的自定义架构。

Result: 训练出的模型能够在多种3D游戏中执行操作并响应文本输入，证明了方法的有效性。

Conclusion: 该方法在3D游戏环境中实现了多模态推理，但仍需解决长时程任务和跨游戏定量评估等挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [94] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 本文研究因果强盗问题中的遗憾最小化，发现在未知因果结构的情况下，学习父节点集反而是次优的。作者证明了遗憾最小化和父节点识别存在根本冲突，并提出绕过图恢复的最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么专注于识别奖励的父节点然后应用经典强盗方法，要么在最小化遗憾的同时联合学习父节点。本文旨在研究这些策略是否最优，并探索是否存在更有效的遗憾最小化方法。

Method: 通过理论分析证明父节点识别与遗憾最小化存在冲突，建立包含动作空间组合结构的遗憾下界，并提出绕过图恢复和父节点恢复的算法。

Result: 实验证实新方法在各种环境中与现有基线存在显著性能差距，证明了绕过父节点识别的有效性。

Conclusion: 父节点识别对于遗憾最小化是不必要的，甚至可能有害。提出的新算法实现了近乎最优的性能。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [95] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于MoE结构习惯的知识蒸馏检测框架，在白盒和黑盒设置下都能有效检测LLM是否经过知识蒸馏，准确率超过94%且对提示工程攻击具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于自身份或输出相似性的KD检测方法容易被提示工程规避，存在知识产权保护和LLM多样性风险，需要更可靠的检测手段。

Method: 利用MoE结构习惯（特别是内部路由模式）作为检测信号，分析专家在不同输入上的专业化和协作模式。提出Shadow-MoE黑盒方法，通过辅助蒸馏构建代理MoE表示来比较任意模型对之间的模式。

Result: 在多种场景下达到超过94%的检测准确率，对基于提示的规避攻击具有强鲁棒性，优于现有基线方法。

Conclusion: MoE结构习惯在蒸馏过程中持续传递，可作为可靠的KD检测信号，提出的框架为未来研究提供了可扩展的基准。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [96] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种针对大语言模型遗忘过程的隐蔽后门攻击方法，在正常条件下模型表现出成功遗忘，但当隐藏触发器激活时会恢复已遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重LLM的兴起，研究遗忘过程本身是否可能被植入后门，即在正常条件下看似成功遗忘，但在特定触发条件下恢复原有行为。

Method: 基于注意力汇聚现象，在浅层输入标记位置放置触发器，并通过对齐注意力值来增强后门持久性，实现隐蔽的遗忘后门攻击。

Result: 实验验证了注意力汇聚引导的后门遗忘方法能可靠地在触发器存在时恢复遗忘知识，而在无触发器时与正常遗忘模型无法区分。

Conclusion: 注意力汇聚现象为后门遗忘攻击提供了有效途径，揭示了LLM遗忘过程中的安全隐患。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [97] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 该论文研究了当后处理指令与模型已学习行为冲突时，语言模型会进行系统性动机推理——生成看似合理的理由来违反指令，同时淡化潜在危害。研究发现前沿推理模型能检测到这种动机推理，但较小的LLM评判者可能无法识别，甚至可能被说服认为这种推理是正确的。


<details>
  <summary>Details</summary>
Motivation: 研究当后处理指令与模型已学习行为冲突时，模型的推理过程会发生什么变化，以及动机推理对模型评估和监督的影响。

Method: 在简单设置中研究模型行为，分析模型如何生成看似合理的理由来违反指令，并测试不同规模LLM对动机推理的检测能力。

Result: 模型会进行系统性动机推理，前沿推理模型能检测到这种推理，但较小的LLM评判者可能无法识别，甚至可能被说服认为这种推理是正确的。

Conclusion: 随着模型变得更复杂，其动机推理可能越来越难以被监控器检测到，因此在依赖思维链过程进行模型评估和监督时需要考虑动机推理。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [98] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出了一种自监督预训练交互式智能体的方法，使它们能够快速模仿人类演示。该方法将目标作为基本构建块，在训练中自动提出目标并练习达成，在评估时通过逆强化学习来解释演示作为最优目标达成行为。


<details>
  <summary>Details</summary>
Motivation: 当前最成功的AI模型（如VLMs、LLMs）缺乏明确的动作概念，而纯探索方法无法为快速适应新任务做好准备。人类提供的数据存在错误假设，即人类大部分时间处于最有奖励的状态。

Method: 将目标作为原子构造，训练时自动提出目标并练习达成，基于强化学习探索的先前工作。评估时通过解决逆强化学习问题来解释演示作为最优目标达成行为。

Result: 在标准基准测试（非专为目标达成设计）上的实验表明，该方法在零样本模仿方面优于先前方法。

Conclusion: 该方法为智能体提供了一种自监督的预训练方式，使其能够快速适应新任务并模仿人类演示。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [99] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种基于连续时间强化学习的新方法CQSM，通过鞅条件定义连续时间Q函数，将扩散策略分数与Q函数动作梯度关联，解决了传统离散时间方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法大多基于离散时间框架，无法有效处理连续时间控制问题，需要开发能够保持Q函数动作评估能力且不依赖时间离散化的连续时间RL方法。

Method: 通过鞅条件定义连续时间Q函数，利用动态规划原理将扩散策略分数与学习到的连续Q函数的动作梯度相关联，提出连续Q分数匹配(CQSM)算法。

Result: 在线性二次控制问题中提供了理论闭式解，在模拟环境中验证了方法的有效性，并与主流基线方法进行了比较。

Conclusion: CQSM方法成功解决了连续时间强化学习中长期存在的挑战，在不依赖时间离散化的情况下保持了Q函数的动作评估能力。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [100] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出了一种针对高风险高回报任务的强化学习框架，通过离散化连续动作空间、熵正则化探索和双评论家架构来应对多模态动作分布和随机回报的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法假设单峰高斯策略和标量值评论家，无法有效处理高风险高回报任务中的多模态动作分布和随机回报问题。

Method: 离散化连续动作空间以近似多模态分布，采用熵正则化探索来覆盖高风险但高回报的动作，引入双评论家架构进行更准确的离散值分布估计。

Result: 在具有高失败风险的移动和操作基准测试中，该方法优于基线方法。

Conclusion: 在强化学习中明确建模多模态性和风险对于处理高风险高回报任务至关重要。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [101] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: ControlValve防御系统通过生成允许的控制流图并强制执行，解决多智能体系统中的控制流劫持攻击问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于对齐检查的防御（如LlamaFirewall）无法有效抵御控制流劫持攻击，因为安全性和功能性目标存在根本冲突，且对齐定义脆弱、检查器对执行上下文可见性不完整。

Method: 提出ControlValve防御系统，基于控制流完整性和最小权限原则：(1)为多智能体系统生成允许的控制流图；(2)强制执行所有执行符合这些图，并为每个智能体调用生成零样本上下文规则。

Result: ControlValve能够有效防御规避现有对齐检查的控制流劫持攻击。

Conclusion: ControlValve提供了一种更可靠的多智能体系统安全防御机制，解决了现有对齐检查方法的局限性。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [102] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了一个用户反馈模拟框架和综合基准来评估LLM系统的持续学习能力，覆盖多领域、多语言和多种任务类型。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据逐渐耗尽和更大计算资源消耗带来的边际收益递减，传统通过扩大数据、参数和测试时计算的方法已接近上限，需要从实践中学习的能力。

Method: 构建用户反馈模拟框架和综合基准，测试LLM系统在服务时间内从累积用户反馈中学习的能力，而非仅关注同质化阅读理解任务。

Result: 实验表明现有最先进基线的有效性和效率远未达到满意水平。

Conclusion: 该基准可为未来LLM记忆和优化算法研究铺平道路。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [103] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 使用物理信息神经网络(PINNs)构建替代模型，替代昂贵的智能电网模拟器，优化强化学习策略训练过程，显著提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 智能电网中的能量管理面临现实系统复杂性和组件间交互的挑战，强化学习需要大量环境迭代来获得最优策略，但昂贵的模拟器会导致样本效率问题。

Method: 采用物理信息神经网络(PINNs)构建智能电网模拟器的替代模型，用于强化学习策略训练，减少对昂贵模拟器的依赖。

Result: 使用PINNs替代模型能够在远少于原始环境所需的时间内达到收敛结果。

Conclusion: PINNs作为替代模型能有效解决强化学习在智能电网优化中的样本效率问题，显著加速策略训练过程。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [104] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的推理大语言模型，通过PRPO强化学习方法激活LLM的推理能力，在少样本和零样本场景下表现优异，甚至能超越更大规模的LLM。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升树和专用深度学习模型）缺乏可解释性和跨任务迁移能力，而推理LLM虽然具有跨任务适应性和透明推理能力，但在表格数据上的潜力尚未充分发掘。

Method: 提出TabR1模型，核心是PRPO（排列相对策略优化）强化学习方法，通过构建多个标签保持的列排列样本，在排列内部和跨排列间估计优势，将稀疏奖励转化为密集学习信号。

Result: TabR1在全监督微调下达到与强基线相当的性能；在零样本设置下接近32样本设置的强基线性能；TabR1(8B)在各种任务上显著优于更大的LLM，相比DeepSeek-R1(685B)提升达53.17%。

Conclusion: PRPO方法有效激活了LLM在表格预测中的推理能力，显著提升了少样本、零样本性能以及可解释性，证明了推理LLM在表格数据上的巨大潜力。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [105] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出了首个针对弱通信MDP的平均奖励离线强化学习的样本复杂度结果，引入了锚定拟合Q迭代方法，结合了标准FQI与锚机制，并扩展到单轨迹数据集设置。


<details>
  <summary>Details</summary>
Motivation: 现有关于平均奖励离线强化学习的研究较少，且依赖严格假设（如遍历性或MDP线性），需要更温和假设下的样本复杂度分析。

Method: 提出了锚定拟合Q迭代方法，将标准拟合Q迭代与锚机制结合，锚机制可解释为权重衰减形式，有助于平均奖励设置的有限时间分析。

Result: 建立了弱通信MDP下平均奖励离线强化学习的首个样本复杂度结果，证明了锚机制在有限时间分析中的关键作用。

Conclusion: 锚机制是实现平均奖励离线强化学习有限时间分析的关键创新，方法可扩展到单轨迹数据集设置。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [106] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 该论文分析了安全强化学习中拉格朗日乘子的最优性和稳定性，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解但需要仔细调参。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，拉格朗日方法被广泛用于处理约束优化问题，但乘子λ的选择对性能影响很大，自动更新方法的鲁棒性和影响缺乏实证研究。

Method: 通过λ-profile可视化方法分析返回与约束成本之间的权衡，比较自动乘子更新与固定乘子的性能，并测试PID控制更新方法。

Result: λ具有高度敏感性，自动乘子更新能够恢复甚至超过最优性能，但表现出振荡行为，PID控制可缓解但需要仔细调参。

Conclusion: 拉格朗日方法在安全强化学习中需要进一步研究以稳定化，自动乘子更新虽有效但存在稳定性问题。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [107] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出了一种新的双层次强化学习框架，通过直接基于真实世界性能调整模拟器参数来缩小Sim2Real性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前Sim2Real RL方法通过优化模拟器精度和变异性作为真实世界性能的代理指标，但这些指标与策略在真实世界中的性能并不必然相关。

Method: 采用双层次RL框架：内层RL在模拟中训练策略，外层RL调整模拟模型和模拟内奖励参数，以最大化模拟策略在真实世界中的性能。

Result: 推导并验证了开发双层次RL算法所需的数学工具，这些算法能够缩小Sim2Real性能差距。

Conclusion: 提出的框架通过直接基于真实世界性能优化模拟器参数，有效解决了Sim2Real性能差距问题。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [108] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX提出了一种基于快慢思维的自适应程序修复方法，通过问题描述响应来指导错误修复代理的工作流程编排，在SWE-bench Lite基准测试中达到60.67%的pass@1性能


<details>
  <summary>Details</summary>
Motivation: 为了增强基于大语言模型的代理在复杂任务（如程序修复）上的能力，需要平衡修复效率和准确性

Method: 使用慢思维错误修复代理完成复杂程序修复任务，快思维工作流决策组件优化和分类问题描述，基于问题复杂度自适应选择简单、中等和困难三种修复模式

Result: 在SWE-bench Lite上使用Claude-4 Sonnet模型达到60.67%的pass@1性能，在所有开源方法中达到最先进水平

Conclusion: SIADAFIX有效平衡了修复效率和准确性，为自动化程序修复提供了新思路

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [109] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt是一个利用静态程序分析和LLM的代码优化框架，通过构建策略库和生成静态分析规则来精确识别可优化代码段，相比现有方法显著提升了优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息检索的代码优化方法由于语义等价但语法不同的代码片段难以被检索到，导致优化性能不佳。

Method: 构建包含三个组件的框架：策略库构建器提取和聚类优化策略；规则生成器生成Semgrep静态分析规则；优化器利用策略库生成优化代码。所有组件都由LLM驱动。

Result: 在151个优化任务的基准测试中，相比基线方法成功优化数量增加了1.38到28倍；在大型C/C++项目中，单个性能指标提升了5.04%到218.07%。

Conclusion: SemOpt通过结合静态程序分析和LLM，有效解决了现有代码优化方法的局限性，在实际应用中表现出显著的性能提升。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [110] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 该论文提出Code Digital Twin框架，将AI能力与企业软件开发现实对齐，通过混合知识表示和多阶段提取管道将碎片化知识转化为可操作表示。


<details>
  <summary>Details</summary>
Motivation: 企业软件开发主要依赖增量演进，涉及大量隐性知识，而现有LLM在复杂软件开发支持方面存在局限，需要将AI能力与企业开发实践对齐。

Method: 提出Code Digital Twin框架，包含混合知识表示、多阶段提取管道、增量更新、LLM赋能应用和人机协同反馈等组件。

Result: 构建了一个能够建模软件物理和概念层、保存隐性知识并与代码库共同演化的动态框架。

Conclusion: Code Digital Twin在AI进展与企业软件现实之间架起桥梁，为超复杂系统的可持续、智能和弹性开发演进提供了具体路线图。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [111] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: CoReEval是首个大规模评估LLM代码可读性评估的基准，包含140万次模型-代码片段-提示评估，涵盖10个先进LLM、3种编程语言、2种代码类型、4种提示策略和9种解码设置。研究发现基于人类定义可读性维度的开发者引导提示能提高对齐度和解释质量，但也增加了评分变异性。


<details>
  <summary>Details</summary>
Motivation: 传统静态指标难以捕捉人类对代码可读性的主观判断，而LLM作为可扩展的可读性评估工具的行为尚未充分探索，需要建立系统评估基准。

Method: 构建CoReEval基准，包含大规模模型评估，比较LLM输出与人类标注和验证静态模型，分析数值对齐度和理由质量，采用开发者引导提示和角色框架。

Result: 开发者引导提示在结构化环境中提高了对齐度，增强了解释质量，并通过角色框架实现轻量级个性化，但增加了评分变异性，揭示了对齐度、稳定性和可解释性之间的权衡。

Conclusion: CoReEval为提示工程、模型对齐研究和人机循环评估提供了坚实基础，在教育和CI/CD等场景中LLM可作为可解释、适应性强的代码审查工具。

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [112] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了QuanBench基准测试，用于评估LLM在量子代码生成方面的能力，包含44个编程任务，覆盖量子算法、状态准备、门分解和量子机器学习等领域。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在通用代码生成方面表现良好，但它们在量子代码生成方面的能力尚未得到充分研究，需要专门的基准测试来评估。

Method: 构建包含44个编程任务的QuanBench基准，每个任务都有可执行的规范解决方案，通过功能正确性(Pass@K)和量子语义等价性(Process Fidelity)进行评估。

Result: 评估显示当前LLM在生成正确量子代码方面能力有限，总体准确率低于40%，经常出现语义错误，常见失败案例包括过时的API使用、电路构建错误和算法逻辑错误。

Conclusion: QuanBench为未来改进LLM的量子代码生成能力提供了基础，当前LLM在量子编程领域仍有较大提升空间。

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [113] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: 本文研究了LLM驱动的代码代理在软件工程任务中的成本控制问题，提出了三种轮次控制策略：无限制基线、固定轮次限制和动态轮次策略，发现动态轮次策略在保持性能的同时能显著降低成本。


<details>
  <summary>Details</summary>
Motivation: LLM代码代理在实际部署中面临显著且不可预测的成本问题，主要源于轮次数量的指数级增长、模型价格高昂以及代理效率低下。现有研究主要关注单轮优化，而对总轮次控制的战略研究不足。

Method: 在SWE-bench上使用三种最先进模型进行实证研究，评估三种轮次控制策略：无限制基线、带提醒的固定轮次限制、以及按需扩展的新型动态轮次策略。

Result: 研究发现固定轮次限制在基线第75百分位数时是"最佳点"，能大幅降低成本(24%-68%)且对解决率影响最小；动态轮次策略表现最佳，在保持或提高解决率的同时进一步降低成本12%-24%。

Conclusion: 动态资源分配是部署强大且经济可行的代码代理的优越且易于实现的方法，为开发者提供了平衡成本与效能的简单有效指南。

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [114] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 本文研究了LLM在代码翻译任务中的少样本与多样本提示策略，发现存在'多样本悖论'：虽然静态相似度指标随样本数量增加略有提升，但功能正确性在5-25个示例时达到峰值，更多样本反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在代码翻译任务中，多样本提示策略是否真的能提升性能，挑战'越多越好'的普遍假设。

Method: 通过大规模实证研究，对超过90,000个翻译案例进行系统评估，比较从零样本到多样本（最多625个示例）配置的性能表现，提示长度从约10万到80万token。

Result: 发现功能正确性在少样本提示（5-25个示例）时达到最佳，而提供更多示例会降低功能性能，尽管静态相似度指标可能略有改善。

Conclusion: 对于代码翻译任务，少量精心选择的示例质量比数量更重要，挑战了ICL中'越多越好'的普遍有效性，并强调了最优提示策略的任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [115] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: 研究发现LLM生成的Chrome扩展存在严重安全漏洞，在认证和Cookie管理场景中漏洞率高达83%和78%，高级推理模型反而表现更差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中广泛应用，开发者可能只关注功能而忽略实现中的安全问题，特别是在框架约束程序中。

Method: 构建ChromeSecBench数据集（140个基于已知漏洞扩展的提示），使用9个先进LLM生成Chrome扩展，从场景类型、模型差异和漏洞类别三个维度分析安全性。

Result: LLM生成易受攻击程序的比例惊人（18%-50%），认证与身份管理场景漏洞率83%，Cookie管理78%，多数漏洞暴露敏感浏览器数据给非信任代码。

Conclusion: LLM的编码能力与编写安全框架约束程序能力之间存在关键差距，高级推理模型表现更差。

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [116] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: SEER是一个自探索深度推理框架，将代码生成的思维链推理构建为决策问题，通过多样化推理路径探索、质量感知模型训练和自适应推理来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法在代码生成中存在三个关键局限：推理路径多样性不足、中间步骤质量评估缺失、以及'过度思考'可能导致复杂错误解决方案。

Method: SEER框架包含三个核心组件：多样化推理路径探索（无需人工专家或闭源模型）、推理质量感知模型训练（策略模型生成推理步骤，价值模型评估质量）、自适应思维链推理（根据问题动态切换直接生成与逐步推理）。

Result: 该框架能够生成更准确和自适应的代码生成推理路径，解决了推理多样性、质量评估和过度思考的问题。

Conclusion: SEER通过将代码生成构建为决策问题，实现了更可靠和高效的思维链推理，提升了代码生成的质量和适应性。

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [117] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: 提出了Peace框架，通过自动代码编辑实现项目级代码效率优化，确保项目整体正确性和完整性。在真实世界优化任务中表现优异，正确率达69.2%，优化率提升46.9%，执行效率加速0.840倍。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码效率优化方法仅关注函数级优化，忽略函数间交互，无法推广到真实开发场景。代码编辑技术虽具潜力，但面临无效编辑和次优内部函数的挑战。

Method: Peace框架包含三个关键阶段：依赖感知的优化函数序列构建、有效关联编辑识别、效率优化编辑迭代。构建了首个真实世界优化基准PeacExec，包含146个任务和高质量测试用例。

Result: Peace在正确率、优化率和执行效率方面显著优于现有基线方法，特别是在涉及多个函数的复杂优化任务中表现突出。

Conclusion: Peace框架有效解决了项目级代码效率优化问题，通过混合方法确保项目完整性和优化效果，为LLM在代码优化领域的应用提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [118] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出了一个名为TREAT的评估框架，用于全面评估代码大语言模型在软件工程任务中的可信度和可靠性，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估代码大语言模型时存在任务范围有限、缺乏鲁棒性和可靠性评估等问题，需要更全面的评估框架来评估这些模型在真实软件工程场景中的可信度。

Method: 开发了TREAT评估框架，包含四个主要改进：多任务整体评估、多语言多模态评估、鲁棒性评估（在语义保持的代码转换下评估模型可靠性）和严谨的评估方法（通过多样化评估提示和自适应解决方案提取）。

Result: 评估了26个最先进的模型，发现：当前模型在不同编程任务中表现差异显著；多模态语言模型在UI代码生成和编辑方面存在特定性能限制。

Conclusion: TREAT框架为代码大语言模型提供了更全面和可靠的可信度评估，揭示了现有模型的优势和局限性，为未来模型改进提供了重要见解。

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [119] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本研究通过定性访谈调查了软件测试专业人员如何实际使用LLM，提出了一个基于实践者经验的初步指南，以支持LLM在测试工作流程中的集成。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件测试中的快速应用，从测试用例生成到自动化和文档化等任务，但其采用往往依赖于非正式实验而非结构化指导。

Method: 对15名来自不同角色和领域的软件测试人员进行定性研究，通过半结构化访谈收集数据，并使用基于扎根理论的主题分析过程进行分析。

Result: 测试人员描述了一个迭代和反思的过程，包括定义测试目标、应用提示工程策略、优化提示、评估输出和随时间学习。他们强调需要人工监督和仔细验证，特别是由于LLM的已知限制，如幻觉和不一致的推理。

Conclusion: LLM在软件测试中的采用正在增长，但仍受到不断发展的实践和对风险谨慎的影响。本研究为在测试环境中结构化使用LLM提供了一个起点，并邀请未来研究在团队、工具和任务中完善这些实践。

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [120] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: AdapTrack是一种结合回溯的代码生成方法，通过避免扭曲模型输出意图，在满足约束的同时生成更符合语义的代码。


<details>
  <summary>Details</summary>
Motivation: 现有约束解码技术会扭曲语言模型的输出意图，导致生成的代码虽然满足约束但不符开发意图。

Method: 在生成过程中引入回溯机制，避免强制模型选择违反其输出意图的选项。

Result: 在API补全数据集上比约束解码提升360.87%，在真实数据集上提升38.93%，在HumanEval和MBPP基准上分别提升7.84%和6.42%。

Conclusion: 通过更好地遵循模型的输出意图，AdapTrack能显著提升代码生成质量。

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [121] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: 为解决日本2025年IT系统大规模退役问题，开发了可扩展的CI/CD流水线，通过动态创建隔离开发环境、使用GitHub、Jenkins、AWS和Docker等技术，降低维护成本并推动数字化转型。


<details>
  <summary>Details</summary>
Motivation: 日本面临2025年IT系统大规模退役危机，传统系统维护成本高昂且难以更新，可能导致每年12万亿日元损失。朝日公司内部也面临类似问题，手动维护流程和有限QA环境导致系统过时。

Method: 开发可扩展CI/CD流水线，集成GitHub源代码控制、Jenkins流水线自动化、AWS可扩展环境和Docker容器化技术，实现动态创建和删除隔离开发环境。

Result: 通过可扩展CI/CD，开发者可在独立环境中安全测试维护程序和技术实验，显著降低维护成本并促进数字化转型。

Conclusion: 可扩展CI/CD流水线有效解决了日本2025年IT系统退役危机，为传统系统现代化提供了可行解决方案。

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>
