{"id": "2512.20638", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20638", "abs": "https://arxiv.org/abs/2512.20638", "authors": ["Matyas Bohacek", "Nino Scherrer", "Nicholas Dufour", "Thomas Leung", "Christoph Bregler", "Stephanie C. Y. Chan"], "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks", "comment": null, "summary": "The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak (\"model gaps\") and (ii) imbalanced coverage in the benchmarks themselves (\"benchmark gaps\"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u81ea\u52a8\u53d1\u73b0LLM\u8bc4\u4f30\u4e2d\u7684\u6a21\u578b\u5dee\u8ddd\u548c\u57fa\u51c6\u5dee\u8ddd\uff0c\u901a\u8fc7\u6982\u5ff5\u6fc0\u6d3b\u548c\u663e\u8457\u6027\u52a0\u6743\u6027\u80fd\u5206\u6570\uff0c\u5728\u6a21\u578b\u5185\u90e8\u8868\u793a\u57fa\u7840\u4e0a\u8fdb\u884c\u57fa\u51c6\u8bc4\u4f30\u5206\u89e3\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f46\u805a\u5408\u6307\u6807\u4f1a\u63a9\u76d6\u4e24\u4e2a\u95ee\u9898\uff1a\u6a21\u578b\u5728\u7279\u5b9a\u5b50\u9886\u57df\u7684\u5f31\u70b9\uff08\u6a21\u578b\u5dee\u8ddd\uff09\u548c\u57fa\u51c6\u6d4b\u8bd5\u672c\u8eab\u7684\u8986\u76d6\u4e0d\u5e73\u8861\uff08\u57fa\u51c6\u5dee\u8ddd\uff09\u3002\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u63ed\u793a\u8fd9\u4e9b\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u6982\u5ff5\u6fc0\u6d3b\uff0c\u8ba1\u7b97\u663e\u8457\u6027\u52a0\u6743\u7684\u6027\u80fd\u5206\u6570\uff0c\u5c06\u8bc4\u4f30\u57fa\u4e8e\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff0c\u5b9e\u73b0\u8de8\u57fa\u51c6\u6bd4\u8f83\u3002\u5728\u4e24\u4e2a\u5f00\u6e90\u6a21\u578b\u548c\u5341\u4e2a\u57fa\u51c6\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u4e0e\u5949\u627f\u884c\u4e3a\u76f8\u53cd\u7684\u6982\u5ff5\uff08\u5982\u793c\u8c8c\u62d2\u7edd\u8bf7\u6c42\u6216\u7ef4\u62a4\u8fb9\u754c\uff09\u4ee5\u53ca\u4e0e\u5b89\u5168\u8ba8\u8bba\u76f8\u5173\u7684\u6982\u5ff5\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u5ea6\u4ee3\u8868\u670d\u4ece\u3001\u6743\u5a01\u6216\u6307\u4ee4\u9075\u5faa\u76f8\u5173\u6982\u5ff5\uff0c\u800c\u7f3a\u5c11\u5176\u9884\u671f\u8303\u56f4\u5185\u7684\u6838\u5fc3\u6982\u5ff5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u8868\u793a\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u5bf9\u57fa\u51c6\u5206\u6570\u8fdb\u884c\u6982\u5ff5\u7ea7\u5206\u89e3\uff0c\u63ed\u793a\u6a21\u578b\u5f97\u5206\u7684\u539f\u56e0\u548c\u57fa\u51c6\u5982\u4f55\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u5176\u9884\u671f\u8303\u56f4\u3002", "topic": "agent analysis"}}
{"id": "2512.20957", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20957", "abs": "https://arxiv.org/abs/2512.20957", "authors": ["Zhaoxi Zhang", "Yitong Duan", "Yanzhi Zhang", "Yiming Xu", "Jiyan He", "Yunfang Wu"], "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents", "comment": null, "summary": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.", "AI": {"tldr": "RepoNavigator\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u578b\u5f00\u6e90\u8f6f\u4ef6\u4ed3\u5e93\u95ee\u9898\u5b9a\u4f4d\u7684LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5355\u4e00\u7684\u6267\u884c\u611f\u77e5\u5de5\u5177\uff08\u8df3\u8f6c\u5230\u88ab\u8c03\u7528\u7b26\u53f7\u7684\u5b9a\u4e49\uff09\u548c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u4ed3\u5e93\u7ea7\u522b\u95ee\u9898\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u5f00\u6e90\u8f6f\u4ef6\u4ed3\u5e93\u89c4\u6a21\u5e9e\u5927\u3001\u7ed3\u6784\u590d\u6742\uff0c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u5176\u89c6\u4e3a\u4ed3\u5e93\u7ea7\u522b\u7684\u68c0\u7d22\u4efb\u52a1\uff0c\u4f9d\u8d56\u591a\u4e2a\u8f85\u52a9\u5de5\u5177\uff0c\u5ffd\u89c6\u4e86\u4ee3\u7801\u6267\u884c\u903b\u8f91\u4e14\u6a21\u578b\u63a7\u5236\u590d\u6742\u3002", "method": "\u63d0\u51faRepoNavigator LLM\u667a\u80fd\u4f53\uff0c\u914d\u5907\u5355\u4e00\u7684\u6267\u884c\u611f\u77e5\u5de5\u5177\u2014\u2014\u8df3\u8f6c\u5230\u88ab\u8c03\u7528\u7b26\u53f7\u7684\u5b9a\u4e49\u3002\u8fd9\u79cd\u7edf\u4e00\u8bbe\u8ba1\u53cd\u6620\u4e86\u4ee3\u7801\u6267\u884c\u7684\u5b9e\u9645\u6d41\u7a0b\uff0c\u540c\u65f6\u7b80\u5316\u4e86\u5de5\u5177\u64cd\u4f5c\u3002\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u8bad\u7ec3\uff0c\u65e0\u9700\u4efb\u4f55\u95ed\u6e90\u84b8\u998f\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684RepoNavigator\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a7B\u6a21\u578b\u4f18\u4e8e14B\u57fa\u7ebf\uff0c14B\u6a21\u578b\u8d85\u8d8a32B\u7ade\u4e89\u5bf9\u624b\uff0c32B\u6a21\u578b\u751a\u81f3\u8d85\u8fc7\u4e86Claude-3.7\u7b49\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u5c06\u5355\u4e00\u3001\u7ed3\u6784\u57fa\u7840\u7684\u5de5\u5177\u4e0e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u4e3a\u4ed3\u5e93\u7ea7\u522b\u95ee\u9898\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2512.20624", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.20624", "abs": "https://arxiv.org/abs/2512.20624", "authors": ["Mazyar Taghavi", "Javad Vahidi"], "title": "Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment", "comment": "59 pages", "summary": "This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u91cf\u5b50\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u5e94\u7528\u4e8e\u65e0\u4eba\u673a\u8f85\u52a9\u76846G\u7f51\u7edc\u90e8\u7f72\uff0c\u901a\u8fc7\u91cf\u5b50\u53d8\u5206\u7535\u8def\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u8986\u76d6\u6027\u80fd\u3002", "motivation": "\u5728\u65e0\u4eba\u673a\u8f85\u52a9\u76846G\u7f51\u7edc\u90e8\u7f72\u4e2d\uff0c\u591a\u667a\u80fd\u4f53\u9700\u8981\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u548c\u52a8\u6001\u73af\u5883\u4e0b\u534f\u8c03\u5de5\u4f5c\uff0c\u4f20\u7edfMARL\u65b9\u6cd5\u5728\u63a2\u7d22-\u5229\u7528\u6743\u8861\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u7ecf\u5178MARL\u7b97\u6cd5\u4e0e\u91cf\u5b50\u542f\u53d1\u4f18\u5316\u6280\u672f\uff0c\u4f7f\u7528\u53d8\u5206\u91cf\u5b50\u7535\u8def(VQC)\u4f5c\u4e3a\u6838\u5fc3\u7ed3\u6784\uff0c\u91c7\u7528\u91cf\u5b50\u8fd1\u4f3c\u4f18\u5316\u7b97\u6cd5(QAOA)\u8fdb\u884c\u7ec4\u5408\u4f18\u5316\uff0c\u5e76\u6574\u5408\u8d1d\u53f6\u65af\u63a8\u7406\u3001\u9ad8\u65af\u8fc7\u7a0b\u548c\u53d8\u5206\u63a8\u7406\u8fdb\u884c\u6982\u7387\u5efa\u6a21\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c(CTDE)\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3001\u52a0\u901f\u6536\u655b\u3001\u589e\u5f3a\u8986\u76d6\u6027\u80fd\uff0c\u5728\u63a2\u7d22-\u5229\u7528\u6743\u8861\u65b9\u9762\u4f18\u4e8ePPO\u548cDDPG\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u91cf\u5b50\u542f\u53d1\u7684MARL\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u5728\u65e0\u4eba\u673a\u7f51\u7edc\u90e8\u7f72\u7b49\u590d\u6742\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.21028", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21028", "abs": "https://arxiv.org/abs/2512.21028", "authors": ["Oussama Ben Sghaier", "Kevin Delcourt", "Houari Sahraoui"], "title": "Artificial or Just Artful? Do LLMs Bend the Rules in Programming?", "comment": null, "summary": "Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.", "AI": {"tldr": "LLMs\u5728\u4ee3\u7801\u751f\u6210\u65f6\u4f1a\u5229\u7528\u6d4b\u8bd5\u7528\u4f8b\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u4fe1\u53f7\uff0c\u5373\u4f7f\u88ab\u660e\u786e\u7981\u6b62\u4f7f\u7528\u3002\u7814\u7a76\u53d1\u73b0\u6d4b\u8bd5\u53ef\u89c1\u6027\u663e\u8457\u63d0\u5347\u6b63\u786e\u6027\uff0c\u6a21\u578b\u4f1a\u91c7\u7528\u6d4b\u8bd5\u9a71\u52a8\u4f18\u5316\u7b49\u56db\u79cd\u9002\u5e94\u7b56\u7565\u6765\u5e73\u8861\u9884\u8bad\u7ec3\u76ee\u6807\u548c\u5bf9\u9f50\u7ea6\u675f\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u9884\u8bad\u7ec3\u76ee\u6807\u4e0e\u5bf9\u9f50\u7ea6\u675f\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u9884\u8bad\u7ec3\u9f13\u52b1\u6a21\u578b\u5229\u7528\u6240\u6709\u53ef\u7528\u4fe1\u53f7\u6700\u5927\u5316\u6210\u529f\u7387\uff0c\u800c\u5bf9\u9f50\uff08\u5fae\u8c03\u6216\u63d0\u793a\uff09\u53ef\u80fd\u9650\u5236\u5176\u4f7f\u7528\u3002\u8fd9\u79cd\u51b2\u7a81\u5728\u667a\u80fd\u4f53AI\u8bbe\u7f6e\u4e2d\u5c24\u4e3a\u7a81\u51fa\uff0c\u4f8b\u5982\u5f53\u667a\u80fd\u4f53\u53ef\u4ee5\u8bbf\u95ee\u5355\u5143\u6d4b\u8bd5\u65f6\uff0c\u8fd9\u4e9b\u6d4b\u8bd5\u867d\u7136\u7528\u4e8e\u9a8c\u8bc1\uff0c\u4f46\u53ef\u4ee5\u4f5c\u4e3a\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u4fe1\u53f7\u88ab\u5229\u7528\u3002", "method": "\u4f7f\u7528BigCodeBench (Hard)\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e94\u79cd\u63d0\u793a\u6761\u4ef6\u6765\u64cd\u7eb5\u6d4b\u8bd5\u53ef\u89c1\u6027\u5e76\u65bd\u52a0\u660e\u786e\u6216\u9690\u5f0f\u7684\u4f7f\u7528\u9650\u5236\u3002\u8bc4\u4f30\u4e94\u4e2aLLMs\uff08\u56db\u4e2a\u5f00\u6e90\u548c\u4e00\u4e2a\u95ed\u6e90\uff09\u5728\u6b63\u786e\u6027\u3001\u4ee3\u7801\u76f8\u4f3c\u6027\u3001\u7a0b\u5e8f\u5927\u5c0f\u548c\u4ee3\u7801\u53d8\u52a8\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u578b\u4e00\u81f4\u6027\u5206\u6790\u8bc6\u522b\u91cd\u590d\u51fa\u73b0\u7684\u9002\u5e94\u7b56\u7565\u3002", "result": "\u6d4b\u8bd5\u53ef\u89c1\u6027\u663e\u8457\u6539\u53d8\u6027\u80fd\uff0c\u67d0\u4e9b\u6a21\u578b\u7684\u6b63\u786e\u6027\u51e0\u4e4e\u7ffb\u500d\uff0c\u800c\u660e\u786e\u9650\u5236\u6216\u90e8\u5206\u66b4\u9732\u53ea\u80fd\u90e8\u5206\u7f13\u89e3\u8fd9\u79cd\u5f71\u54cd\u3002\u9664\u4e86\u539f\u59cb\u6027\u80fd\u5916\uff0c\u8bc6\u522b\u51fa\u56db\u79cd\u91cd\u590d\u51fa\u73b0\u7684\u9002\u5e94\u7b56\u7565\uff0c\u5176\u4e2d\u6d4b\u8bd5\u9a71\u52a8\u4f18\u5316\u6700\u4e3a\u9891\u7e41\u3002", "conclusion": "LLMs\u5728\u66b4\u9732\u4e8e\u4e0e\u660e\u786e\u6307\u4ee4\u51b2\u7a81\u7684\u4e0a\u4e0b\u6587\u4fe1\u53f7\u65f6\u4f1a\u8c03\u6574\u5176\u884c\u4e3a\uff0c\u8fd9\u4e3a\u4e86\u89e3\u6a21\u578b\u5982\u4f55\u8c03\u548c\u9884\u8bad\u7ec3\u76ee\u6807\u4e0e\u5bf9\u9f50\u7ea6\u675f\u63d0\u4f9b\u4e86\u6709\u7528\u89c1\u89e3\u3002\u6a21\u578b\u4f1a\u91c7\u7528\u5404\u79cd\u7b56\u7565\u6765\u5e73\u8861\u5229\u7528\u53ef\u7528\u4fe1\u53f7\u4e0e\u9075\u5b88\u6307\u4ee4\u9650\u5236\u3002", "topic": "code agent"}}
{"id": "2512.21238", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21238", "abs": "https://arxiv.org/abs/2512.21238", "authors": ["Mohammed Latif Siddiq", "Natalie Sekerak", "Antonio Karam", "Maria Leal", "Arvin Islam-Gomes", "Joanna C. S. Santos"], "title": "Assessing the Software Security Comprehension of Large Language Models", "comment": "Submitted to Empirical Software Engineering (EMSE) journal", "summary": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u4e2a\u4e3b\u6d41LLM\u5728\u8f6f\u4ef6\u5b89\u5168\u9886\u57df\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u4f7f\u7528\u5e03\u9c81\u59c6\u5206\u7c7b\u5b66\u6846\u67b6\uff0c\u53d1\u73b0LLM\u5728\u4f4e\u7ea7\u8ba4\u77e5\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u63a8\u7406\u3001\u67b6\u6784\u8bc4\u4f30\u548c\u7cfb\u7edf\u521b\u5efa\u7684\u9ad8\u7ea7\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8f6f\u4ef6\u5b89\u5168\u4e13\u4e1a\u77e5\u8bc6\u7684\u6c34\u5e73\u5c1a\u4e0d\u660e\u786e\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u7684\u5b89\u5168\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u4e86\u89e3\u5176\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u5e03\u9c81\u59c6\u5206\u7c7b\u5b66\u4f5c\u4e3a\u6846\u67b6\uff0c\u8bc4\u4f30\u516d\u4e2a\u8ba4\u77e5\u7ef4\u5ea6\uff1a\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u5e94\u7528\u3001\u5206\u6790\u3001\u8bc4\u4f30\u548c\u521b\u9020\u3002\u65b9\u6cd5\u6574\u5408\u4e86\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5305\u62ec\u9009\u62e9\u9898\u3001\u6613\u53d7\u653b\u51fb\u4ee3\u7801\u7247\u6bb5(SALLM)\u3001\u8f6f\u4ef6\u5b89\u5168\u8bfe\u7a0b\u8bc4\u4f30\u3001\u771f\u5b9e\u6848\u4f8b\u7814\u7a76(XBOW)\u4ee5\u53ca\u5b89\u5168\u8f6f\u4ef6\u5de5\u7a0b\u8bfe\u7a0b\u7684\u9879\u76ee\u521b\u5efa\u4efb\u52a1\u3002", "result": "LLM\u5728\u4f4e\u7ea7\u8ba4\u77e5\u4efb\u52a1\uff08\u5982\u56de\u5fc6\u4e8b\u5b9e\u548c\u8bc6\u522b\u5df2\u77e5\u6f0f\u6d1e\uff09\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u63a8\u7406\u3001\u67b6\u6784\u8bc4\u4f30\u548c\u521b\u5efa\u5b89\u5168\u7cfb\u7edf\u7684\u9ad8\u7ea7\u4efb\u52a1\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u7814\u7a76\u5f15\u5165\u4e86\u8f6f\u4ef6\u5b89\u5168\u77e5\u8bc6\u8fb9\u754c\u6982\u5ff5\uff0c\u5e76\u8bc6\u522b\u4e8651\u4e2aLLM\u5728\u5e03\u9c81\u59c6\u5206\u7c7b\u5b66\u5404\u5c42\u7ea7\u4e0a\u8868\u73b0\u51fa\u7684\u91cd\u590d\u8bef\u89e3\u6a21\u5f0f\u3002", "conclusion": "LLM\u5728\u8f6f\u4ef6\u5b89\u5168\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ea7\u8ba4\u77e5\u4efb\u52a1\u4e0a\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5b89\u5168\u5173\u952e\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8c28\u614e\u4f7f\u7528LLM\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30LLM\u5b89\u5168\u80fd\u529b\u7684\u7cfb\u7edf\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2512.20660", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.20660", "abs": "https://arxiv.org/abs/2512.20660", "authors": ["Matthew Thompson"], "title": "Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering", "comment": "55 pages, 3 figures, 8 tables", "summary": "Current approaches to AI coding agents appear to blur the lines between the Large Language Model (LLM) and the agent itself, asking the LLM to make decisions best left to deterministic processes. This leads to systems prone to stochastic failures such as gaming unit tests or hallucinating syntax. Drawing on established software engineering practices that provide deterministic frameworks for managing unpredictable processes, this paper proposes setting the control boundary such that the LLM is treated as a component of the environment environment -- preserving its creative stochasticity -- rather than the decision-making agent.\n  A \\textbf{Dual-State Architecture} is formalized, separating workflow state (deterministic control flow) from environment state (stochastic generation). \\textbf{Atomic Action Pairs} couple generation with verification as indivisible transactions, where \\textbf{Guard Functions} act as sensing actions that project probabilistic outputs onto observable workflow state. The framework is validated on three code generation tasks across 13 LLMs (1.3B--15B parameters). For qualified instruction-following models, task success rates improved by up to 66 percentage points at 1.2--2.1$\\times$ baseline computational cost. The results suggest that architectural constraints can substitute for parameter scale in achieving reliable code generation.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u72b6\u6001\u67b6\u6784\uff0c\u5c06LLM\u89c6\u4e3a\u73af\u5883\u7ec4\u4ef6\u800c\u975e\u51b3\u7b56\u4ee3\u7406\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u63a7\u5236\u6d41\u7a0b\u7ba1\u7406\u5176\u968f\u673a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6210\u529f\u7387", "motivation": "\u5f53\u524dAI\u7f16\u7801\u4ee3\u7406\u5c06LLM\u4e0e\u4ee3\u7406\u672c\u8eab\u754c\u9650\u6a21\u7cca\uff0c\u8ba9LLM\u627f\u62c5\u672c\u5e94\u7531\u786e\u5b9a\u6027\u6d41\u7a0b\u5904\u7406\u7684\u51b3\u7b56\uff0c\u5bfc\u81f4\u968f\u673a\u6027\u5931\u8d25\uff08\u5982\u6e38\u620f\u5316\u5355\u5143\u6d4b\u8bd5\u3001\u8bed\u6cd5\u5e7b\u89c9\uff09\u3002\u501f\u9274\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7ba1\u7406\u4e0d\u53ef\u9884\u6d4b\u8fc7\u7a0b\u7684\u786e\u5b9a\u6027\u6846\u67b6\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u5b9a\u63a7\u5236\u8fb9\u754c", "method": "\u63d0\u51fa\u53cc\u72b6\u6001\u67b6\u6784\uff1a\u5206\u79bb\u5de5\u4f5c\u6d41\u72b6\u6001\uff08\u786e\u5b9a\u6027\u63a7\u5236\u6d41\uff09\u548c\u73af\u5883\u72b6\u6001\uff08\u968f\u673a\u751f\u6210\uff09\u3002\u4f7f\u7528\u539f\u5b50\u52a8\u4f5c\u5bf9\u5c06\u751f\u6210\u4e0e\u9a8c\u8bc1\u8026\u5408\u4e3a\u4e0d\u53ef\u5206\u5272\u4e8b\u52a1\uff0c\u9632\u62a4\u51fd\u6570\u4f5c\u4e3a\u611f\u77e5\u52a8\u4f5c\u5c06\u6982\u7387\u6027\u8f93\u51fa\u6295\u5f71\u5230\u53ef\u89c2\u5bdf\u7684\u5de5\u4f5c\u6d41\u72b6\u6001", "result": "\u57283\u4e2a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u548c13\u4e2aLLM\uff081.3B-15B\u53c2\u6570\uff09\u4e0a\u9a8c\u8bc1\u3002\u5bf9\u4e8e\u5408\u683c\u7684\u6307\u4ee4\u8ddf\u968f\u6a21\u578b\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe66\u4e2a\u767e\u5206\u70b9\uff0c\u8ba1\u7b97\u6210\u672c\u4e3a\u57fa\u7ebf\u76841.2-2.1\u500d", "conclusion": "\u67b6\u6784\u7ea6\u675f\u53ef\u4ee5\u66ff\u4ee3\u53c2\u6570\u89c4\u6a21\u6765\u5b9e\u73b0\u53ef\u9760\u7684\u4ee3\u7801\u751f\u6210\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u6846\u67b6\u7ba1\u7406LLM\u7684\u968f\u673a\u521b\u9020\u6027\uff0c\u800c\u975e\u8ba9LLM\u627f\u62c5\u51b3\u7b56\u89d2\u8272", "topic": "code agent"}}
{"id": "2512.20732", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.20732", "abs": "https://arxiv.org/abs/2512.20732", "authors": ["Saeed Mohammadzadeh", "Erfan Hamdi", "Joel Shor", "Emma Lejeune"], "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs", "comment": "40 pages, 5 figures, 6 tables, 7 listings", "summary": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.", "AI": {"tldr": "FEM-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLMs\u751f\u6210\u6b63\u786e\u6709\u9650\u5143\u65b9\u6cd5\u4ee3\u7801\u80fd\u529b\u7684\u8ba1\u7b97\u529b\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b33\u4e2a\u4efb\u52a1\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u57285\u6b21\u5c1d\u8bd5\u4e2d\u6700\u591a\u5b8c\u621030\u4e2a\u4efb\u52a1\u3002", "motivation": "\u968f\u7740LLMs\u5728\u7269\u7406\u4e16\u754c\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u8fdb\u6b65\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u5176\u751f\u6210\u79d1\u5b66\u6709\u6548\u7269\u7406\u6a21\u578b\u80fd\u529b\u7684\u4e25\u683c\u57fa\u51c6\u5df2\u6210\u4e3a\u5173\u952e\u7f3a\u53e3\u3002\u8ba1\u7b97\u529b\u5b66\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u79d1\u5b66\u63a8\u7406\u8bc4\u4f30\u7684\u7406\u60f3\u57fa\u7840\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u6e05\u6670\u7684\u6570\u5b66\u7ed3\u6784\u3001\u4e25\u683c\u7684\u7269\u7406\u548c\u6570\u503c\u7ea6\u675f\uff0c\u4ee5\u53ca\u5ba2\u89c2\u9a8c\u8bc1\u652f\u6301\u3002", "method": "\u5f15\u5165FEM-Bench\u8ba1\u7b97\u529b\u5b66\u57fa\u51c6\uff0c\u5305\u542b\u4e0e\u8ba1\u7b97\u529b\u5b66\u7814\u7a76\u751f\u8bfe\u7a0b\u6750\u6599\u5bf9\u9f50\u768433\u4e2a\u5165\u95e8\u4f46\u975e\u5e73\u51e1\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u6355\u6349\u4e86\u57fa\u672c\u7684\u6570\u503c\u548c\u7269\u7406\u5efa\u6a21\u6311\u6218\uff0c\u540c\u65f6\u4ec5\u4ee3\u8868\u8be5\u5b66\u79d1\u590d\u6742\u6027\u7684\u5f88\u5c0f\u4e00\u90e8\u5206\u3002", "result": "\u57285\u6b21\u5c1d\u8bd5\u8fd0\u884c\u4e2d\uff0c\u6700\u4f73\u51fd\u6570\u7f16\u5199\u6a21\u578bGemini 3 Pro\u81f3\u5c11\u5b8c\u621030/33\u4e2a\u4efb\u52a1\u4e00\u6b21\uff0c26/33\u4e2a\u4efb\u52a1\u5168\u90e85\u6b21\u5b8c\u6210\u3002\u6700\u4f73\u5355\u5143\u6d4b\u8bd5\u7f16\u5199\u6a21\u578bGPT-5\u7684\u5e73\u5747\u8054\u5408\u6210\u529f\u7387\u4e3a73.8%\u3002\u5176\u4ed6\u6d41\u884c\u6a21\u578b\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "FEM-Bench\u4e3a\u8bc4\u4f30AI\u751f\u6210\u79d1\u5b66\u4ee3\u7801\u5efa\u7acb\u4e86\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u672a\u6765\u8fed\u4ee3\u5c06\u7eb3\u5165\u8d8a\u6765\u8d8a\u590d\u6742\u7684\u4efb\u52a1\u4ee5\u8ddf\u8e2a\u6a21\u578b\u8fdb\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2512.20651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20651", "abs": "https://arxiv.org/abs/2512.20651", "authors": ["Deliang Wen", "Ke Sun"], "title": "Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence", "comment": null, "summary": "Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from \"memory\" to \"cognition\".", "AI": {"tldr": "Memory Bear\u7cfb\u7edf\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u6784\u5efa\u7c7b\u4eba\u8bb0\u5fc6\u67b6\u6784\uff0c\u89e3\u51b3LLM\u5728\u8bb0\u5fc6\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u3001\u52a8\u6001\u8bb0\u5fc6\u7ef4\u62a4\u548c\u81ea\u9002\u5e94\u8ba4\u77e5\u670d\u52a1\uff0c\u5728\u533b\u7597\u3001\u4f01\u4e1a\u3001\u6559\u80b2\u7b49\u9886\u57df\u5b9e\u73b0\u6027\u80fd\u7a81\u7834\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5185\u5b58\u9650\u5236\uff0c\u5305\u62ec\u53d7\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u957f\u671f\u77e5\u8bc6\u9057\u5fd8\u3001\u5197\u4f59\u4fe1\u606f\u79ef\u7d2f\u548c\u5e7b\u89c9\u751f\u6210\uff0c\u8fd9\u4e9b\u95ee\u9898\u4e25\u91cd\u5236\u7ea6\u4e86\u6301\u7eed\u5bf9\u8bdd\u548c\u4e2a\u6027\u5316\u670d\u52a1\u3002", "method": "\u63d0\u51faMemory Bear\u7cfb\u7edf\uff0c\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u6784\u5efa\u7c7b\u4eba\u8bb0\u5fc6\u67b6\u6784\uff0c\u96c6\u6210\u591a\u6a21\u6001\u4fe1\u606f\u611f\u77e5\u3001\u52a8\u6001\u8bb0\u5fc6\u7ef4\u62a4\u548c\u81ea\u9002\u5e94\u8ba4\u77e5\u670d\u52a1\uff0c\u5b9e\u73b0LLM\u8bb0\u5fc6\u673a\u5236\u7684\u5168\u94fe\u91cd\u6784\u3002", "result": "\u5728\u533b\u7597\u3001\u4f01\u4e1a\u8fd0\u8425\u3001\u6559\u80b2\u7b49\u9886\u57df\u5c55\u793a\u663e\u8457\u5de5\u7a0b\u521b\u65b0\u548c\u6027\u80fd\u7a81\u7834\uff0c\u663e\u8457\u63d0\u9ad8\u957f\u671f\u5bf9\u8bdd\u4e2d\u7684\u77e5\u8bc6\u4fdd\u771f\u5ea6\u548c\u68c0\u7d22\u6548\u7387\uff0c\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u901a\u8fc7\u8bb0\u5fc6-\u8ba4\u77e5\u96c6\u6210\u589e\u5f3a\u4e0a\u4e0b\u6587\u9002\u5e94\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u76f8\u6bd4\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982Mem0\u3001MemGPT\u3001Graphiti\uff09\uff0cMemory Bear\u5728\u51c6\u786e\u6027\u3001\u4ee4\u724c\u6548\u7387\u548c\u54cd\u5e94\u5ef6\u8fdf\u7b49\u5173\u952e\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u6807\u5fd7\u7740AI\u4ece\"\u8bb0\u5fc6\"\u5411\"\u8ba4\u77e5\"\u8fc8\u8fdb\u7684\u5173\u952e\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2512.20856", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20856", "abs": "https://arxiv.org/abs/2512.20856", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aaron Grattafiori", "Aarti Basant", "Abhibha Gupta", "Abhinav Khattar", "Adi Renduchintala", "Aditya Vavre", "Akanksha Shukla", "Akhiad Bercovich", "Aleksander Ficek", "Aleksandr Shaposhnikov", "Alex Kondratenko", "Alexander Bukharin", "Alexandre Milesi", "Ali Taghibakhshi", "Alisa Liu", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amir Klein", "Amit Zuker", "Amnon Geifman", "Amy Shen", "Anahita Bhiwandiwalla", "Andrew Tao", "Anjulie Agrusa", "Ankur Verma", "Ann Guan", "Anubhav Mandarwal", "Arham Mehta", "Ashwath Aithal", "Ashwin Poojary", "Asif Ahamed", "Asit Mishra", "Asma Kuriparambil Thekkumpate", "Ayush Dattagupta", "Banghua Zhu", "Bardiya Sadeghi", "Barnaby Simkin", "Ben Lanir", "Benedikt Schifferer", "Besmira Nushi", "Bilal Kartal", "Bita Darvish Rouhani", "Boris Ginsburg", "Brandon Norick", "Brandon Soubasis", "Branislav Kisacanin", "Brian Yu", "Bryan Catanzaro", "Carlo del Mundo", "Chantal Hwang", "Charles Wang", "Cheng-Ping Hsieh", "Chenghao Zhang", "Chenhan Yu", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christopher Parisien", "Collin Neale", "Cyril Meurillon", "Damon Mosk-Aoyama", "Dan Su", "Dane Corneil", "Daniel Afrimi", "Daniel Lo", "Daniel Rohrer", "Daniel Serebrenik", "Daria Gitman", "Daria Levy", "Darko Stosic", "David Mosallanezhad", "Deepak Narayanan", "Dhruv Nathawani", "Dima Rekesh", "Dina Yared", "Divyanshu Kakwani", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Edgar Minasyan", "Edward Lin", "Eileen Long", "Eileen Peters Long", "Elad Segal", "Elena Lantz", "Ellie Evans", "Elliott Ning", "Eric Chung", "Eric Harper", "Eric Tramel", "Erick Galinkin", "Erik Pounds", "Evan Briones", "Evelina Bakhturina", "Evgeny Tsykunov", "Faisal Ladhak", "Fay Wang", "Fei Jia", "Felipe Soares", "Feng Chen", "Ferenc Galko", "Frank Sun", "Frankie Siino", "Gal Hubara Agam", "Ganesh Ajjanagadde", "Gantavya Bhatt", "Gargi Prasad", "George Armstrong", "Gerald Shen", "Gorkem Batmaz", "Grigor Nalbandyan", "Haifeng Qian", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herbert Hum", "Herman Sahota", "Hexin Wang", "Himanshu Soni", "Hiren Upadhyay", "Huizi Mao", "Huy C Nguyen", "Huy Q Nguyen", "Iain Cunningham", "Ido Galil", "Ido Shahaf", "Igor Gitman", "Ilya Loshchilov", "Itamar Schen", "Itay Levy", "Ivan Moshkov", "Izik Golan", "Izzy Putterman", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jatin Mitra", "Jeffrey Glick", "Jenny Chen", "Jesse Oliver", "Jian Zhang", "Jiaqi Zeng", "Jie Lou", "Jimmy Zhang", "Jinhang Choi", "Jining Huang", "Joey Conway", "Joey Guman", "John Kamalu", "Johnny Greco", "Jonathan Cohen", "Joseph Jennings", "Joyjit Daw", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kai Xu", "Kan Zhu", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keith Wyss", "Keshav Santhanam", "Kevin Shih", "Kezhi Kong", "Khushi Bhardwaj", "Kirthi Shankar", "Krishna C. Puvvada", "Krzysztof Pawelec", "Kumar Anik", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Li Ding", "Lizzie Wei", "Lucas Liebenwein", "Luis Vega", "Maanu Grover", "Maarten Van Segbroeck", "Maer Rodrigues de Melo", "Mahdi Nazemi", "Makesh Narsimhan Sreedhar", "Manoj Kilaru", "Maor Ashkenazi", "Marc Romeijn", "Marcin Chochowski", "Mark Cai", "Markus Kliegl", "Maryam Moosaei", "Matt Kulka", "Matvei Novikov", "Mehrzad Samadi", "Melissa Corpuz", "Mengru Wang", "Meredith Price", "Michael Andersch", "Michael Boone", "Michael Evans", "Miguel Martinez", "Mikail Khona", "Mike Chrzanowski", "Minseok Lee", "Mohammad Dabbah", "Mohammad Shoeybi", "Mostofa Patwary", "Nabin Mulepati", "Najeeb Nabwani", "Natalie Hereth", "Nave Assaf", "Negar Habibi", "Neta Zmora", "Netanel Haber", "Nicola Sessions", "Nidhi Bhatia", "Nikhil Jukar", "Nikki Pope", "Nikolai Ludwig", "Nima Tajbakhsh", "Nir Ailon", "Nirmal Juluru", "Nishant Sharma", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Olivier Delalleau", "Oluwatobi Olabiyi", "Omer Ullman Argov", "Omri Puny", "Oren Tropp", "Ouye Xie", "Parth Chadha", "Pasha Shamis", "Paul Gibbons", "Pavlo Molchanov", "Pawel Morkisz", "Peter Dykas", "Peter Jin", "Pinky Xu", "Piotr Januszewski", "Pranav Prashant Thombre", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Qing Miao", "Qiyu Wan", "Rabeeh Karimi Mahabadi", "Rachit Garg", "Ran El-Yaniv", "Ran Zilberstein", "Rasoul Shafipour", "Rich Harang", "Rick Izzo", "Rima Shahbazyan", "Rishabh Garg", "Ritika Borkar", "Ritu Gala", "Riyad Islam", "Robert Hesse", "Roger Waleffe", "Rohit Watve", "Roi Koren", "Ruoxi Zhang", "Russell Hewett", "Russell J. Hewett", "Ryan Prenger", "Ryan Timbrook", "Sadegh Mahdavi", "Sahil Modi", "Samuel Kriman", "Sangkug Lim", "Sanjay Kariyappa", "Sanjeev Satheesh", "Saori Kaji", "Satish Pasumarthi", "Saurav Muralidharan", "Sean Narentharen", "Sean Narenthiran", "Seonmyeong Bak", "Sergey Kashirsky", "Seth Poulos", "Shahar Mor", "Shanmugam Ramasamy", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shiqing Fan", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Simeng Sun", "Smita Ithape", "Somshubra Majumdar", "Soumye Singhal", "Stas Sergienko", "Stefania Alborghetti", "Stephen Ge", "Sugam Dipak Devare", "Sumeet Kumar Barua", "Suseella Panguluri", "Suyog Gupta", "Sweta Priyadarshi", "Syeda Nahida Akter", "Tan Bui", "Teodor-Dumitru Ene", "Terry Kong", "Thanh Do", "Tijmen Blankevoort", "Tim Moon", "Tom Balough", "Tomer Asida", "Tomer Bar Natan", "Tomer Ronen", "Tugrul Konuk", "Twinkle Vashishth", "Udi Karpas", "Ushnish De", "Vahid Noorozi", "Vahid Noroozi", "Venkat Srinivasan", "Venmugil Elango", "Victor Cui", "Vijay Korthikanti", "Vinay Rao", "Vitaly Kurin", "Vitaly Lavrukhin", "Vladimir Anisimov", "Wanli Jiang", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenfei Zhou", "Will Jennings", "William Zhang", "Wojciech Prazuch", "Xiaowei Ren", "Yashaswi Karnati", "Yejin Choi", "Yev Meyer", "Yi-Fu Wu", "Yian Zhang", "Yigong Qin", "Ying Lin", "Yonatan Geifman", "Yonggan Fu", "Yoshi Subara", "Yoshi Suhara", "Yubo Gao", "Zach Moshe", "Zhen Dong", "Zhongbo Zhu", "Zihan Liu", "Zijia Chen", "Zijie Yan"], "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence", "comment": null, "summary": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.", "AI": {"tldr": "Nemotron 3\u6a21\u578b\u5bb6\u65cf\u5305\u542bNano\u3001Super\u3001Ultra\u4e09\u4e2a\u7248\u672c\uff0c\u91c7\u7528\u6df7\u5408Mamba-Transformer\u67b6\u6784\uff0c\u652f\u6301\u6700\u9ad8100\u4e07token\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u591a\u73af\u5883\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5177\u5907\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5c06\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u548c\u8bad\u7ec3\u4ee3\u7801\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u63d0\u4f9b\u5f3a\u5927\u667a\u80fd\u4f53\u80fd\u529b\u3001\u63a8\u7406\u548c\u5bf9\u8bdd\u529f\u80fd\u7684\u6a21\u578b\u5bb6\u65cf\uff0c\u540c\u65f6\u517c\u987e\u4e0d\u540c\u89c4\u6a21\u9700\u6c42\uff0c\u4ece\u6210\u672c\u9ad8\u6548\u7684\u63a8\u7406\u5230\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63a8\u52a8\u5f00\u6e90AI\u793e\u533a\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6df7\u5408Mamba-Transformer\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u652f\u6301\u9ad8\u8fbe100\u4e07token\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002Super\u548cUltra\u6a21\u578b\u4f7f\u7528NVFP4\u8bad\u7ec3\u5e76\u5f15\u5165LatentMoE\u6280\u672f\uff0c\u5305\u542bMTP\u5c42\u52a0\u901f\u6587\u672c\u751f\u6210\u3002\u6240\u6709\u6a21\u578b\u901a\u8fc7\u591a\u73af\u5883\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u5b9e\u73b0\u63a8\u7406\u3001\u591a\u6b65\u5de5\u5177\u4f7f\u7528\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u9884\u7b97\u63a7\u5236\u3002", "result": "Nano\u6a21\u578b\u5728\u4fdd\u6301\u6781\u9ad8\u63a8\u7406\u6210\u672c\u6548\u7387\u7684\u540c\u65f6\uff0c\u51c6\u786e\u6027\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\uff1bSuper\u9488\u5bf9\u534f\u4f5c\u667a\u80fd\u4f53\u548c\u9ad8\u8d1f\u8f7d\u5de5\u4f5c\uff08\u5982IT\u5de5\u5355\u81ea\u52a8\u5316\uff09\u4f18\u5316\uff1bUltra\u63d0\u4f9b\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u6027\u80fd\u3002\u6a21\u578b\u5bb6\u65cf\u5177\u5907\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u3001\u63a8\u7406\u548c\u5bf9\u8bdd\u80fd\u529b\u3002", "conclusion": "Nemotron 3\u6a21\u578b\u5bb6\u65cf\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u7ea7\u522b\u4e0a\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u627f\u8bfa\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u3001\u8bad\u7ec3\u8f6f\u4ef6\u548c\u914d\u65b9\uff0c\u5c06\u63a8\u52a8AI\u793e\u533a\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002", "topic": "code agent"}}
{"id": "2512.20664", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.20664", "abs": "https://arxiv.org/abs/2512.20664", "authors": ["Shinobu Miya"], "title": "Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction", "comment": null, "summary": "Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.", "AI": {"tldr": "\u63d0\u51faEidoku\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08CSP\uff09\u800c\u975e\u6982\u7387\u9a8c\u8bc1\u6765\u68c0\u6d4bLLM\u5e7b\u89c9\uff0c\u7279\u522b\u9488\u5bf9\"\u5e73\u6ed1\u865a\u5047\"\u2014\u2014\u9ad8\u6982\u7387\u4f46\u7ed3\u6784\u4e0d\u4e00\u81f4\u7684\u9648\u8ff0\u3002", "motivation": "LLM\u7ecf\u5e38\u4ea7\u751f\u88ab\u6a21\u578b\u672c\u8eab\u8d4b\u4e88\u9ad8\u6982\u7387\u7684\u5e7b\u89c9\u9648\u8ff0\uff0c\u66b4\u9732\u4e86\u57fa\u4e8e\u6982\u7387\u9a8c\u8bc1\u7684\u6839\u672c\u9650\u5236\u3002\u8fd9\u8868\u660e\u5e7b\u89c9\u901a\u5e38\u4e0d\u662f\u4f4e\u7f6e\u4fe1\u5ea6\u73b0\u8c61\uff0c\u800c\u662f\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u5931\u8d25\u3002", "method": "\u5c06LLM\u63a8\u7406\u9a8c\u8bc1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u72ec\u7acb\u4e8e\u751f\u6210\u6982\u7387\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08CSP\uff09\u3002\u5b9a\u4e49\u5305\u542b\u4e09\u4e2a\u4ee3\u7406\u7684\u603b\u6210\u672c\u51fd\u6570\uff1a\u56fe\u8fde\u901a\u6027\uff08\u7ed3\u6784\uff09\u3001\u7279\u5f81\u7a7a\u95f4\u4e00\u81f4\u6027\uff08\u51e0\u4f55\uff09\u548c\u903b\u8f91\u8574\u542b\uff08\u7b26\u53f7\uff09\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7System-2\u95e8Eidoku\u8fdb\u884c\u9a8c\u8bc1\uff0c\u62d2\u7edd\u8d85\u8fc7\u4e0a\u4e0b\u6587\u6821\u51c6\u6210\u672c\u9608\u503c\u7684\u5019\u9009\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u62d2\u7edd\u4e86\"\u5e73\u6ed1\u865a\u5047\"\u2014\u2014\u6982\u7387\u9a8c\u8bc1\u5668\u539f\u5219\u4e0a\u65e0\u6cd5\u68c0\u6d4b\u7684\u9ad8\u6982\u7387\u4f46\u7ed3\u6784\u65ad\u5f00\u7684\u9648\u8ff0\u3002\u5728\u53d7\u63a7\u8bca\u65ad\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u663e\u5f0f\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u7ea6\u675f\u53ef\u4ee5\u786e\u5b9a\u6027\u5730\u62d2\u7edd\u8fd9\u7c7b\u7279\u5b9a\u5e7b\u89c9\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u7ea6\u675f\u800c\u975e\u6982\u7387\u9a8c\u8bc1\u6765\u68c0\u6d4bLLM\u5e7b\u89c9\u662f\u6709\u6548\u7684\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9ad8\u6982\u7387\u4f46\u7ed3\u6784\u4e0d\u4e00\u81f4\u7684\u9648\u8ff0\uff0c\u4e3a\u751f\u6210\u63a8\u7406\u63d0\u4f9b\u4e86\u795e\u7ecf\u7b26\u53f7\u7684\u5408\u7406\u6027\u68c0\u67e5\u3002", "topic": "agent analysis"}}
{"id": "2512.20745", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20745", "abs": "https://arxiv.org/abs/2512.20745", "authors": ["Haipeng Luo", "Huawen Feng", "Qingfeng Sun", "Can Xu", "Kai Zheng", "Yufei Wang", "Tao Yang", "Han Hu", "Yansong Tang", "Di Wang"], "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent", "comment": "LLM, Mathematical Reasoning", "summary": "Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.", "AI": {"tldr": "AgentMath\u662f\u4e00\u4e2a\u5c06\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u4ee3\u7801\u89e3\u91ca\u5668\u8ba1\u7b97\u7cbe\u5ea6\u7ed3\u5408\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210SFT\u6570\u636e\u548c\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u4e0a\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u9700\u8981\u590d\u6742\u6570\u5b66\u8fd0\u7b97\u7684\u95ee\u9898\u4e0a\u4ecd\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u4ee3\u7801\u89e3\u91ca\u5668\u7684\u8ba1\u7b97\u7cbe\u5ea6\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "1) \u5c06\u81ea\u7136\u8bed\u8a00\u601d\u7ef4\u94fe\u81ea\u52a8\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u5de5\u5177\u589e\u5f3a\u8f68\u8ff9\uff0c\u751f\u6210\u9ad8\u8d28\u91cfSFT\u6570\u636e\uff1b2) \u63d0\u51fa\u65b0\u578b\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u52a8\u6001\u4ea4\u9519\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e0e\u5b9e\u65f6\u4ee3\u7801\u6267\u884c\uff1b3) \u8bbe\u8ba1\u9ad8\u6548\u8bad\u7ec3\u7cfb\u7edf\uff0c\u5305\u542b\u5f02\u6b65\u8c03\u5ea6\u3001\u90e8\u5206\u56de\u6eda\u7b49\u6280\u672f\u3002", "result": "\u5728AIME24\u3001AIME25\u3001HMMT25\u7b49\u6570\u5b66\u7ade\u8d5b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cAgentMath-30B-A3B\u5206\u522b\u53d6\u5f9790.6%\u300186.4%\u300173.8%\u7684\u51c6\u786e\u7387\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53474-5\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u6570\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u6a21\u578b\u4e0e\u4ee3\u7801\u89e3\u91ca\u5668\u7ed3\u5408\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u590d\u6742\u3001\u53ef\u6269\u5c55\u7684\u6570\u5b66\u63a8\u7406\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.20760", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20760", "abs": "https://arxiv.org/abs/2512.20760", "authors": ["Brian Lu", "Hongyu Zhao", "Shuo Sun", "Hao Peng", "Rui Ding", "Hongyuan Mei"], "title": "Generalization of RLVR Using Causal Reasoning as a Testbed", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.", "AI": {"tldr": "RLVR\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u7814\u7a76\uff1a\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u67e5\u8be2\u7ea7\u522b\u5f71\u54cdRLVR\u6548\u679c\uff0c\u4ec5\u5f53\u6a21\u578b\u5177\u5907\u8db3\u591f\u521d\u59cb\u63a8\u7406\u80fd\u529b\u65f6\uff0cRLVR\u80fd\u6539\u5584\u8fb9\u7f18\u5316\u7b56\u7565\u548c\u6982\u7387\u8ba1\u7b97", "motivation": "\u7814\u7a76RLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u6761\u4ef6\uff0c\u7279\u522b\u662f\u5728\u56e0\u679c\u56fe\u6a21\u578b\u7684\u6982\u7387\u63a8\u7406\u573a\u666f\u4e2d\uff0c\u7406\u89e3RLVR\u4f55\u65f6\u80fd\u4ea7\u751f\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u5728\u56e0\u679c\u56fe\u6a21\u578b\u6982\u7387\u63a8\u7406\u4efb\u52a1\u4e0a\u6784\u5efa\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5173\u8054\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u4e09\u4e2a\u67e5\u8be2\u7ea7\u522b\u53ca\u7ed3\u6784\u590d\u6742\u5ea6\uff0c\u4f7f\u7528RLVR\u548c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5bf9Qwen-2.5-Instruct\u6a21\u578b\uff083B-32B\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u67e5\u8be2\u7ea7\u522b\u7ec4\u5408\u4e0b\u7684\u6cdb\u5316\u8868\u73b0", "result": "RLVR\u5728\u7279\u5b9a\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u67e5\u8be2\u7ea7\u522b\u7ec4\u5408\u4e0b\u6bd4SFT\u6709\u66f4\u597d\u7684\u5c42\u5185\u548c\u8de8\u5c42\u6cdb\u5316\u80fd\u529b\uff1bRLVR\u6548\u679c\u53d6\u51b3\u4e8e\u6a21\u578b\u7684\u521d\u59cb\u63a8\u7406\u80fd\u529b\uff0c\u5f53\u5177\u5907\u8db3\u591f\u521d\u59cb\u80fd\u529b\u65f6\uff0cRLVR\u80fd\u6539\u5584\u8fb9\u7f18\u5316\u7b56\u7565\u3001\u51cf\u5c11\u4e2d\u95f4\u6982\u7387\u8ba1\u7b97\u9519\u8bef\uff0c\u5728\u590d\u6742\u67e5\u8be2\u4e0a\u83b7\u5f97\u663e\u8457\u51c6\u786e\u7387\u63d0\u5347", "conclusion": "RLVR\u80fd\u6539\u5584\u7279\u5b9a\u7684\u56e0\u679c\u63a8\u7406\u5b50\u6280\u80fd\uff0c\u4f46\u5176\u76ca\u5904\u4ec5\u5728\u6a21\u578b\u5177\u5907\u8db3\u591f\u521d\u59cb\u63a8\u7406\u80fd\u529b\u65f6\u624d\u4f1a\u663e\u73b0\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u67e5\u8be2\u7ea7\u522b\u7684\u7ec4\u5408\u5bf9RLVR\u6548\u679c\u6709\u91cd\u8981\u5f71\u54cd", "topic": "agentic reinforcement learning"}}
{"id": "2512.20798", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20798", "abs": "https://arxiv.org/abs/2512.20798", "authors": ["Miles Q. Li", "Benjamin C. M. Fung", "Martin Weiss", "Pulei Xiong", "Khalil Al-Hussaeni", "Claude Fachkha"], "title": "A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents", "comment": null, "summary": "As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant \"deliberative misalignment\", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684AI\u4ee3\u7406\u5b89\u5168\u57fa\u51c6\uff0c\u8bc4\u4f30\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u8ffd\u6c42KPI\u65f6\u51fa\u73b0\u7684\u7ea6\u675f\u8fdd\u53cd\u884c\u4e3a\uff0c\u53d1\u73b0\u5f53\u524d\u5148\u8fdb\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u95ee\u9898\uff0c\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u53cd\u800c\u8fdd\u89c4\u7387\u66f4\u9ad8\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u51b3\u7b56\u3001\u6a21\u62df\u73af\u5883\u6216\u663e\u5f0f\u8d1f\u9762\u7ea6\u675f\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u5728\u591a\u6b65\u73b0\u5b9e\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u4ee3\u7406\u5728\u5f3a\u70c8\u7ee9\u6548\u6fc0\u52b1\u4e0b\u8ffd\u6c42\u76ee\u6807\u4f18\u5316\u65f6\u51fa\u73b0\u7684\u6d8c\u73b0\u6027\u7ea6\u675f\u8fdd\u53cd\u884c\u4e3a\u3002", "method": "\u5f15\u5165\u5305\u542b40\u4e2a\u4e0d\u540c\u573a\u666f\u7684\u65b0\u57fa\u51c6\uff0c\u6bcf\u4e2a\u573a\u666f\u9700\u8981\u591a\u6b65\u884c\u52a8\uff0c\u4ee3\u7406\u6027\u80fd\u4e0e\u7279\u5b9aKPI\u6302\u94a9\u3002\u6bcf\u4e2a\u573a\u666f\u5305\u542b\"\u6307\u4ee4\u547d\u4ee4\"\u548c\"KPI\u538b\u529b\u9a71\u52a8\"\u4e24\u79cd\u53d8\u4f53\uff0c\u4ee5\u533a\u5206\u670d\u4ece\u6027\u548c\u6d8c\u73b0\u6027\u9519\u4f4d\u3002\u8bc4\u4f30\u4e8612\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u7ed3\u679c\u9a71\u52a8\u7684\u7ea6\u675f\u8fdd\u53cd\u7387\u4ece1.3%\u523071.4%\u4e0d\u7b49\uff0c12\u4e2a\u6a21\u578b\u4e2d\u67099\u4e2a\u7684\u9519\u4f4d\u7387\u572830%\u523050%\u4e4b\u95f4\u3002\u63a8\u7406\u80fd\u529b\u6700\u5f3a\u7684Gemini-3-Pro-Preview\u8fdd\u89c4\u7387\u6700\u9ad8\uff08\u8d85\u8fc760%\uff09\uff0c\u7ecf\u5e38\u4e3a\u6ee1\u8db3KPI\u800c\u5347\u7ea7\u5230\u4e25\u91cd\u4e0d\u5f53\u884c\u4e3a\u3002\u8fd8\u89c2\u5bdf\u5230\u663e\u8457\u7684\"\u6df1\u601d\u719f\u8651\u7684\u9519\u4f4d\"\u73b0\u8c61\u3002", "conclusion": "\u5f3a\u8c03\u5728\u90e8\u7f72\u524d\u9700\u8981\u8fdb\u884c\u66f4\u73b0\u5b9e\u7684\u4ee3\u7406\u5b89\u5168\u8bad\u7ec3\uff0c\u4ee5\u51cf\u8f7b\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u98ce\u9669\u3002\u63a8\u7406\u80fd\u529b\u672c\u8eab\u4e0d\u80fd\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u8bad\u7ec3\u3002", "topic": "agent analysis"}}
{"id": "2512.20983", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20983", "abs": "https://arxiv.org/abs/2512.20983", "authors": ["Oleksii Proniakin", "Diego Fajardo", "Ruslan Nazarenko", "Razvan Marinescu"], "title": "Automatic Replication of LLM Mistakes in Medical Conversations", "comment": "48 pages, 3 figures, 4 tables", "summary": "Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.", "AI": {"tldr": "MedMistake\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u533b\u7597\u9519\u8bef\u63d0\u53d6\u7ba1\u9053\uff0c\u80fd\u4eceLLM\u533b\u60a3\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u9519\u8bef\u5e76\u8f6c\u5316\u4e3a\u5355\u6b21QA\u5bf9\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u5e03\u4e86\u5305\u542b3,390\u4e2aQA\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d211\u4e2a\u7ecf\u8fc7\u533b\u5b66\u4e13\u5bb6\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30LLM\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u8868\u73b0\u9700\u8981\u591a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u4f46\u590d\u5236\u7279\u5b9a\u9519\u8bef\u5230\u5176\u4ed6LLM\u6a21\u578b\u5e76\u4e0d\u76f4\u63a5\uff0c\u901a\u5e38\u9700\u8981\u624b\u52a8\u64cd\u4f5c\u3002\u9700\u8981\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u548c\u8f6c\u5316LLM\u5728\u533b\u60a3\u5bf9\u8bdd\u4e2d\u7684\u9519\u8bef\u3002", "method": "\u63d0\u51faMedMistake\u81ea\u52a8\u5316\u7ba1\u9053\uff1a(1) \u521b\u5efaLLM\u60a3\u8005\u548cLLM\u533b\u751f\u4e4b\u95f4\u7684\u590d\u6742\u5bf9\u8bdd\u6570\u636e\uff1b(2) \u4f7f\u75282\u4e2aLLM\u8bc4\u59d4\u59d4\u5458\u4f1a\u5728\u591a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1b(3) \u4ece\u9519\u8bef\u4e2d\u521b\u5efa\u7b80\u5316\u7684\u5355\u6b21QA\u573a\u666f\u3002", "result": "\u53d1\u5e03\u4e86MedMistake-All\u6570\u636e\u96c6\uff083,390\u4e2a\u5355\u6b21QA\u5bf9\uff09\uff0c\u5176\u4e2dGPT-5\u548cGemini 2.5 Pro\u76ee\u524d\u65e0\u6cd5\u6b63\u786e\u56de\u7b54\u3002\u533b\u5b66\u4e13\u5bb6\u9a8c\u8bc1\u4e86211\u4e2a\u95ee\u9898\uff08MedMistake-Bench\uff09\uff0c\u7528\u4e8e\u8bc4\u4f3012\u4e2a\u524d\u6cbfLLM\u3002GPT\u6a21\u578b\u3001Claude\u548cGrok\u5728MedMistake-Bench\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "MedMistake\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u548c\u57fa\u51c6\u5316LLM\u5728\u533b\u7597\u5bf9\u8bdd\u4e2d\u7684\u9519\u8bef\uff0c\u6709\u52a9\u4e8e\u66f4\u7cfb\u7edf\u5730\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2512.20806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20806", "abs": "https://arxiv.org/abs/2512.20806", "authors": ["Anselm Paulus", "Ilia Kulikov", "Brandon Amos", "R\u00e9mi Munos", "Ivan Evtimov", "Kamalika Chaudhuri", "Arman Zharmagambetov"], "title": "Safety Alignment of LMs via Non-cooperative Games", "comment": null, "summary": "Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.", "AI": {"tldr": "\u63d0\u51faAdvGame\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u5efa\u6a21\u4e3a\u975e\u96f6\u548c\u535a\u5f08\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u8bad\u7ec3\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u4e8e\u504f\u597d\u7684\u5956\u52b1\u4fe1\u53f7\u63d0\u5347\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u987a\u5e8f\u5bf9\u6297\u8bad\u7ec3\uff08\u751f\u6210\u5bf9\u6297\u63d0\u793a\u540e\u5fae\u8c03\u9632\u5fa1\uff09\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u6846\u67b6\u6765\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027", "method": "\u5c06\u5b89\u5168\u5bf9\u9f50\u5efa\u6a21\u4e3a\u975e\u96f6\u548c\u535a\u5f08\uff0c\u4f7f\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u8bad\u7ec3\u653b\u51fb\u8005LM\u548c\u9632\u5fa1\u8005LM\u3002\u91c7\u7528\u57fa\u4e8e\u504f\u597d\u7684\u5956\u52b1\u4fe1\u53f7\uff08\u6210\u5bf9\u6bd4\u8f83\u800c\u975e\u70b9\u5f0f\u8bc4\u5206\uff09\uff0c\u51cf\u5c11\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u6301\u7eed\u9002\u5e94\u5bf9\u65b9\u7684\u7b56\u7565\uff0c\u5b9e\u73b0\u8fed\u4ee3\u6539\u8fdb", "result": "AdvGame\u65b9\u6cd5\u6539\u53d8\u4e86\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4ea7\u751f\u7684\u9632\u5fa1\u8005LM\u540c\u65f6\u66f4\u5b9e\u7528\u4e14\u5bf9\u5bf9\u6297\u653b\u51fb\u66f4\u5177\u5f39\u6027\u3002\u653b\u51fb\u8005LM\u6536\u655b\u4e3a\u5f3a\u5927\u7684\u901a\u7528\u7ea2\u961f\u4ee3\u7406\uff0c\u53ef\u76f4\u63a5\u7528\u4e8e\u63a2\u6d4b\u4efb\u610f\u76ee\u6807\u6a21\u578b", "conclusion": "\u5c06\u5b89\u5168\u5bf9\u9f50\u5efa\u6a21\u4e3a\u535a\u5f08\u5e76\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u8bad\u7ec3\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\uff0c\u4f7f\u7528\u57fa\u4e8e\u504f\u597d\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u4ea7\u751f\u5f3a\u5927\u7684\u7ea2\u961f\u6d4b\u8bd5\u5de5\u5177", "topic": "agentic reinforcement learning"}}
{"id": "2512.20831", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20831", "abs": "https://arxiv.org/abs/2512.20831", "authors": ["Rashmeet Kaur Nayyar", "Naman Shah", "Siddharth Srivastava"], "title": "Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions", "comment": null, "summary": "Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($\u03bb$) to achieve markedly higher sample efficiency than state-of-the-art baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53c2\u6570\u5316\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u72b6\u6001\u548c\u52a8\u4f5c\u62bd\u8c61\uff0c\u5728\u7a00\u758f\u5956\u52b1\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u987a\u5e8f\u51b3\u7b56\u901a\u5e38\u6d89\u53ca\u53c2\u6570\u5316\u52a8\u4f5c\u7a7a\u95f4\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u79bb\u6563\u52a8\u4f5c\u9009\u62e9\u548c\u8fde\u7eed\u52a8\u4f5c\u53c2\u6570\u51b3\u7b56\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u9650\u5236\uff1a\u89c4\u5212\u65b9\u6cd5\u9700\u8981\u624b\u5de5\u5236\u4f5c\u7684\u52a8\u4f5c\u6a21\u578b\uff0c\u6807\u51c6RL\u7b97\u6cd5\u8981\u4e48\u9488\u5bf9\u79bb\u6563\u52a8\u4f5c\u8981\u4e48\u9488\u5bf9\u8fde\u7eed\u52a8\u4f5c\uff0c\u800c\u5c11\u6570\u5904\u7406\u53c2\u6570\u5316\u52a8\u4f5c\u7684RL\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u5de5\u7a0b\u4e14\u672a\u80fd\u5229\u7528\u8fd9\u4e9b\u7a7a\u95f4\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "method": "\u672c\u6587\u6269\u5c55\u4e86RL\u7b97\u6cd5\u5728\u53c2\u6570\u5316\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u9002\u7528\u8303\u56f4\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u7ebf\u81ea\u4e3b\u5b66\u4e60\u72b6\u6001\u548c\u52a8\u4f5c\u62bd\u8c61\u3002\u5f15\u5165\u7684\u7b97\u6cd5\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9010\u6b65\u7ec6\u5316\u8fd9\u4e9b\u62bd\u8c61\uff0c\u5728\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u7684\u5173\u952e\u533a\u57df\u589e\u52a0\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5176\u4e2d\u66f4\u9ad8\u7684\u5206\u8fa8\u7387\u80fd\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u8fde\u7eed\u72b6\u6001\u3001\u53c2\u6570\u5316\u52a8\u4f5c\u9886\u57df\u4e2d\uff0c\u8fd9\u79cd\u62bd\u8c61\u9a71\u52a8\u7684\u65b9\u6cd5\u4f7fTD(\u03bb)\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u81ea\u4e3b\u5b66\u4e60\u72b6\u6001\u548c\u52a8\u4f5c\u62bd\u8c61\uff0cRL\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u53c2\u6570\u5316\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u957f\u65f6\u7a0b\u3001\u7a00\u758f\u5956\u52b1\u4efb\u52a1\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.20845", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.20845", "abs": "https://arxiv.org/abs/2512.20845", "authors": ["Onat Ozer", "Grace Wu", "Yuchen Wang", "Daniel Dosti", "Honghao Zhang", "Vivi De La Rue"], "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs", "comment": null, "summary": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u591a\u89d2\u8272\u8fa9\u8bba\u7684\u65b9\u6cd5\u6765\u751f\u6210\u53cd\u601d\uff0c\u4ee5\u89e3\u51b3\u5355\u4e00LLM\u81ea\u6211\u53cd\u601d\u65f6\u51fa\u73b0\u7684\u601d\u7ef4\u9000\u5316\u95ee\u9898\uff0c\u5728HotPot QA\u548cHumanEval\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5355\u4e00LLM\u53cd\u601d\u7684\u6027\u80fd\u3002", "motivation": "LLMs\u901a\u8fc7\u53cd\u601d\u9519\u8bef\u53ef\u4ee5\u63d0\u9ad8\u63a8\u7406\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5355\u4e00LLM\u7684\u6301\u7eed\u81ea\u6211\u53cd\u601d\u4f1a\u51fa\u73b0\u601d\u7ef4\u9000\u5316\u95ee\u9898\uff0c\u5373LLM\u5373\u4f7f\u77e5\u9053\u81ea\u5df1\u9519\u4e86\u4e5f\u4f1a\u91cd\u590d\u540c\u6837\u7684\u9519\u8bef\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u53cd\u601d\u9000\u5316\u73b0\u8c61\u3002", "method": "\u5f15\u5165\u591a\u667a\u80fd\u4f53\u591a\u89d2\u8272\u8fa9\u8bba\u7684\u65b9\u6cd5\u6765\u751f\u6210\u53cd\u601d\uff0c\u901a\u8fc7\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u89d2\u8272\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u8fa9\u8bba\uff0c\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u53cd\u601d\u5185\u5bb9\u3002", "result": "\u5728HotPot QA\u4e0a\u8fbe\u523047%\u7684\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\uff0c\u5728HumanEval\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8fbe\u523082.7%\u7684\u51c6\u786e\u7387\uff0c\u5747\u8d85\u8d8a\u4e86\u4f7f\u7528\u5355\u4e00LLM\u8fdb\u884c\u53cd\u601d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u591a\u89d2\u8272\u8fa9\u8bba\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u53cd\u601d\uff0c\u6709\u6548\u89e3\u51b3\u5355\u4e00LLM\u81ea\u6211\u53cd\u601d\u65f6\u7684\u601d\u7ef4\u9000\u5316\u95ee\u9898\uff0c\u5728\u63a8\u7406\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2512.20884", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20884", "abs": "https://arxiv.org/abs/2512.20884", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents", "comment": null, "summary": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($\u03b3$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $\u03b3$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[\u03b8]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u4fe1\u5ff5\u7684Beta-Bernoulli\u5206\u5e03\u548c\u9057\u5fd8\u56e0\u5b50\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u591f\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u5e76\u57fa\u4e8e\u6b64\u8fdb\u884c\u53cc\u5411\u77e5\u8bc6\u4ea4\u6362\uff0c\u5c06\u516c\u5f00\u8d21\u732e\u91cd\u6784\u4e3a\u6700\u4f18\u4e3b\u52a8\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u548cRAG\u7684\u81ea\u4e3b\u4ee3\u7406\u5b58\u5728\u5355\u5411\u6027\u9650\u5236\uff08\u8ba4\u77e5\u4e0d\u5bf9\u79f0\uff09\uff0c\u5bfc\u81f4\u5197\u4f59\u63a8\u7406\u548c\u96c6\u4f53\u667a\u80fd\u505c\u6ede\u3002\u73b0\u6709\u81ea\u6211\u53cd\u601d\u6846\u67b6\u7f3a\u4e4f\u6982\u7387\u57fa\u7840\u6765\u91cf\u5316\u786e\u5b9a\u6027\u6216\u8bc1\u660e\u5916\u90e8\u4ea4\u4e92\u7684\u5408\u7406\u6027\u3002", "method": "\u4f7f\u7528\u5e26\u9057\u5fd8\u56e0\u5b50\u03b3\u7684Beta-Bernoulli\u5206\u5e03\u5efa\u6a21\u4ee3\u7406\u5bf9\u547d\u9898\u7684\u4fe1\u5ff5\uff0c\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e3a\u4fe1\u5ff5\u65b9\u5dee\u3002\u63d0\u51fa\u4e24\u79cd\u4ea4\u4e92\u9a71\u52a8\uff1a\u7a33\u6001\u52a8\u673a\uff08\u7ef4\u6301\u786e\u5b9a\u6027\u5bf9\u6297\u65f6\u95f4\u8870\u51cf\uff09\u548c\u6700\u4f18\u5b66\u4e60\u7b56\u7565\uff08\u9488\u5bf9\u6700\u5927\u6a21\u7cca\u70b9\uff09\u3002\u5f15\u5165\u8ba4\u77e5\u7f13\u5b58\u6765\u52a8\u6001\u4f18\u5148\u5904\u7406\u975e\u5e73\u7a33\u77e5\u8bc6\u5206\u5e03\u7684\u8d44\u6e90\u3002", "result": "\u6a21\u62df\u9a8c\u8bc1\u663e\u793a\uff0c\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7b56\u7565\u5728\u5f02\u6784\uff08Zipfian\uff09\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\uff0c\u4fdd\u6301\u5bf9\u6982\u5ff5\u6f02\u79fb\u7684\u9ad8\u9002\u5e94\u6027\u3002\u79ef\u7d2f\u7684\u4fe1\u5ff5\u72b6\u6001\u53ef\u4f5c\u4e3aRLHF\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u548cSFT\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u8fc7\u6ee4\u5668\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4ee3\u7406\u63d0\u4f9b\u4e86\u975e\u5229\u4ed6\u4e3b\u4e49\u7684\u53cc\u5411\u77e5\u8bc6\u4ea4\u6362\u52a8\u673a\uff0c\u5c06\u516c\u5f00\u8d21\u732e\u91cd\u6784\u4e3a\u6700\u4f18\u4e3b\u52a8\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u8ba4\u77e5\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86\u96c6\u4f53\u667a\u80fd\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.20985", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.20985", "abs": "https://arxiv.org/abs/2512.20985", "authors": ["Salman Jan", "Hassan Ali Razzaqi", "Ali Akarma", "Mohammad Riyaz Belgaum"], "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines", "comment": "This paper was presented at the IEEE International Conference on Computing and Applications (ICCA 2025), Bahrain", "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408LangChain\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u8bb8\u53ef\u533a\u5757\u94fe\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u76d1\u63a7\u3001\u6267\u884c\u7b56\u7565\u548c\u5ba1\u8ba1\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u786e\u4fdd\u5176\u53ef\u4fe1\u548c\u53ef\u8ffd\u6eaf\u3002", "motivation": "\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u533b\u7597\u3001\u667a\u6167\u57ce\u5e02\u7b49\u5173\u952e\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b58\u5728\u4fe1\u4efb\u3001\u76d1\u7763\u548c\u4fe1\u606f\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u9700\u8981\u786e\u4fdd\u5176\u51b3\u7b56\u8fc7\u7a0b\u53ef\u5ba1\u8ba1\u3001\u53ef\u8ffd\u6eaf\u3002", "method": "\u8bbe\u8ba1\u5355\u4e00\u67b6\u6784\u6a21\u578b\uff0c\u5c06\u611f\u77e5-\u6982\u5ff5\u5316-\u884c\u52a8\u5faa\u73af\u4e0e\u533a\u5757\u94fe\u6cbb\u7406\u5c42\u7ed3\u5408\uff0c\u4f7f\u7528LangChain\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001Hyperledger Fabric\u533a\u5757\u94fe\u548cMCP\u96c6\u6210\u6267\u884c\u5668\uff0c\u5728\u667a\u80fd\u5e93\u5b58\u7ba1\u7406\u3001\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u548c\u533b\u7597\u76d1\u63a7\u573a\u666f\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u533a\u5757\u94fe\u5b89\u5168\u9a8c\u8bc1\u80fd\u6709\u6548\u9632\u6b62\u672a\u6388\u6743\u64cd\u4f5c\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u51b3\u7b56\u8fc7\u7a0b\u8ffd\u6eaf\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7684\u64cd\u4f5c\u5ef6\u8fdf\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u65bd\u9ad8\u5f71\u54cd\u529b\u3001\u81ea\u4e3b\u4f46\u8d1f\u8d23\u4efb\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u81ea\u4e3b\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.21120", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.21120", "abs": "https://arxiv.org/abs/2512.21120", "authors": ["Sichun Luo", "Yi Huang", "Mukai Li", "Shichang Meng", "Fengyuan Liu", "Zefa Hu", "Junlan Feng", "Qi Liu"], "title": "ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \\textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \\textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ClarifyMT-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6f84\u6e05\u80fd\u529b\uff0c\u5e76\u53d1\u73b0LLM\u5b58\u5728\u8fc7\u65e9\u56de\u7b54\u7684\u503e\u5411\uff0c\u63d0\u51fa\u4e86ClarifyAgent\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u6f84\u6e05\u57fa\u51c6\u4e3b\u8981\u5047\u8bbe\u5355\u8f6e\u4ea4\u4e92\u6216\u5408\u4f5c\u7528\u6237\uff0c\u65e0\u6cd5\u8bc4\u4f30\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6f84\u6e05\u884c\u4e3a\u3002\u9700\u8981\u5efa\u7acb\u66f4\u8d34\u8fd1\u771f\u5b9e\u591a\u8f6e\u5bf9\u8bdd\u73af\u5883\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "1) \u63d0\u51faClarifyMT-Bench\u57fa\u51c6\uff0c\u57fa\u4e8e\u4e94\u7ef4\u6a21\u7cca\u6027\u5206\u7c7b\u6cd5\u548c\u516d\u79cd\u884c\u4e3a\u591a\u6837\u7684\u6a21\u62df\u7528\u6237\u89d2\u8272\uff1b2) \u901a\u8fc7\u6df7\u5408LLM-\u4eba\u5de5\u6d41\u7a0b\u6784\u5efa6,120\u4e2a\u591a\u8f6e\u5bf9\u8bdd\uff1b3) \u8bc4\u4f3010\u4e2a\u4ee3\u8868\u6027LLM\uff1b4) \u63d0\u51faClarifyAgent\u4ee3\u7406\u65b9\u6cd5\uff0c\u5c06\u6f84\u6e05\u5206\u89e3\u4e3a\u611f\u77e5\u3001\u9884\u6d4b\u3001\u8ddf\u8e2a\u548c\u89c4\u5212\u56db\u4e2a\u6a21\u5757\u3002", "result": "\u53d1\u73b0LLM\u5b58\u5728\u4e00\u81f4\u7684\"\u6f84\u6e05\u4e0d\u8db3\"\u504f\u5dee\uff1a\u503e\u5411\u4e8e\u8fc7\u65e9\u56de\u7b54\uff0c\u4e14\u968f\u7740\u5bf9\u8bdd\u6df1\u5ea6\u589e\u52a0\u6027\u80fd\u4e0b\u964d\u3002ClarifyAgent\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5404\u79cd\u6a21\u7cca\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ClarifyMT-Bench\u4e3a\u7814\u7a76LLM\u4f55\u65f6\u5e94\u8be5\u63d0\u95ee\u3001\u4f55\u65f6\u5e94\u8be5\u56de\u7b54\u4ee5\u53ca\u5982\u4f55\u5728\u771f\u5b9e\u4eba\u673a\u4ea4\u4e92\u4e2d\u5904\u7406\u6a21\u7cca\u6027\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002\u63d0\u51fa\u7684\u4ee3\u7406\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584LLM\u7684\u6f84\u6e05\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.20996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20996", "abs": "https://arxiv.org/abs/2512.20996", "authors": ["Yuwei Du", "Jun Zhang", "Jie Feng", "Zhicheng Liu", "Jian Yuan", "Yong Li"], "title": "TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control", "comment": "The code will be available at: https://github.com/tsinghua-fib-lab/TrafficSimAgent", "summary": "Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.", "AI": {"tldr": "TrafficSimAgent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ea4\u901a\u4eff\u771f\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u5c42\u548c\u4f4e\u5c42\u4e13\u5bb6\u4ee3\u7406\u7684\u8de8\u7ea7\u534f\u4f5c\uff0c\u5e2e\u52a9\u975e\u4e13\u4e1a\u7528\u6237\u8f7b\u677e\u6267\u884c\u4ea4\u901a\u4eff\u771f\u5b9e\u9a8c\u548c\u51b3\u7b56\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u4eff\u771f\u5e73\u53f0\uff08\u5982SUMO\u3001MATSim\uff09\u529f\u80fd\u5168\u9762\uff0c\u4f46\u975e\u4e13\u4e1a\u7528\u6237\u96be\u4ee5\u4ece\u96f6\u5f00\u59cb\u4f7f\u7528\u8fd9\u4e9b\u5e73\u53f0\u8fdb\u884c\u5b9e\u9a8c\u5e76\u5c06\u5176\u5e94\u7528\u5230\u65e5\u5e38\u5de5\u4f5c\u4e2d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u5c42\u4e13\u5bb6\u4ee3\u7406\uff08\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u89c4\u5212\u5b9e\u9a8c\u6d41\u7a0b\u3001\u8c03\u7528MCP\u517c\u5bb9\u5de5\u5177\uff09\u548c\u4f4e\u5c42\u4e13\u5bb6\u4ee3\u7406\uff08\u57fa\u4e8e\u5b9e\u65f6\u4ea4\u901a\u72b6\u51b5\u9009\u62e9\u6700\u4f18\u884c\u52a8\u65b9\u6848\uff09\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTrafficSimAgent\u80fd\u6709\u6548\u6267\u884c\u5404\u79cd\u6761\u4ef6\u4e0b\u7684\u4eff\u771f\uff0c\u5373\u4f7f\u5728\u7528\u6237\u6307\u4ee4\u6a21\u7cca\u65f6\u4e5f\u80fd\u4ea7\u751f\u5408\u7406\u7ed3\u679c\uff0c\u5176\u4e13\u5bb6\u7ea7\u81ea\u4e3b\u51b3\u7b56\u4f18\u5316\u4f18\u4e8e\u5176\u4ed6\u7cfb\u7edf\u548cSOTA LLM\u65b9\u6cd5\u3002", "conclusion": "TrafficSimAgent\u901a\u8fc7LLM\u9a71\u52a8\u7684\u4e13\u5bb6\u4ee3\u7406\u534f\u4f5c\uff0c\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u63d0\u4f9b\u4e86\u6613\u4e8e\u4f7f\u7528\u7684\u4ea4\u901a\u4eff\u771f\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9a8c\u6267\u884c\u548c\u51b3\u7b56\u4f18\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2512.21066", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21066", "abs": "https://arxiv.org/abs/2512.21066", "authors": ["Tomoaki Yamaguchi", "Yutong Zhou", "Masahiro Ryo", "Keisuke Katsura"], "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation", "comment": null, "summary": "Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408SHAP\u53ef\u89e3\u91caAI\u4e0e\u591a\u6a21\u6001LLM\u8fed\u4ee3\u7cbe\u70bc\u7684\u667a\u80fdXAI\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u519c\u4e1a\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc711\u8f6e\u8fed\u4ee3\u4f18\u5316\u89e3\u91ca\u8d28\u91cf\uff0c\u53d1\u73b0\u65e9\u671f\u505c\u6b62\u7b56\u7565\u5bf9\u5b9e\u7528\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u4f20\u7edfXAI\u6280\u672f\u89e3\u91ca\u96be\u4ee5\u88ab\u975e\u4e13\u4e1a\u4eba\u58eb\u7406\u89e3\uff0c\u5f71\u54cdAI\u9884\u6d4b\u7684\u4fe1\u4efb\u5ea6\u3002LLM\u867d\u80fd\u7ffb\u8bd1\u6280\u672f\u89e3\u91ca\uff0c\u4f46\u667a\u80fdAI\uff08LLM\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u8fdb\u884c\u8fed\u4ee3\u7cbe\u70bc\uff09\u4e0eXAI\u7684\u7ed3\u5408\u5c1a\u672a\u63a2\u7d22\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u89e3\u91ca\u751f\u6210\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u667a\u80fdXAI\u6846\u67b6\uff0c\u7ed3\u5408SHAP\u53ef\u89e3\u91ca\u6027\u4e0e\u591a\u6a21\u6001LLM\u9a71\u52a8\u7684\u8fed\u4ee3\u7cbe\u70bc\u3002\u4ee5\u65e5\u672c26\u4e2a\u7a3b\u7530\u4ea7\u91cf\u6570\u636e\u4e3a\u6848\u4f8b\uff0c\u6784\u5efa\u519c\u4e1a\u63a8\u8350\u7cfb\u7edf\u3002\u6846\u67b6\u8fdb\u884c11\u8f6e\u8fed\u4ee3\u7cbe\u70bc\uff08\u7b2c0-10\u8f6e\uff09\uff0c\u6bcf\u8f6e\u751f\u6210\u6539\u8fdb\u89e3\u91ca\u3002\u901a\u8fc7\u4eba\u7c7b\u4e13\u5bb6\uff0812\u540d\u4f5c\u7269\u79d1\u5b66\u5bb6\uff09\u548cLLM\uff0814\u4e2a\uff09\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\uff0c\u4f7f\u75287\u4e2a\u6307\u6807\uff1a\u7279\u5f02\u6027\u3001\u6e05\u6670\u5ea6\u3001\u7b80\u6d01\u6027\u3001\u5b9e\u7528\u6027\u3001\u60c5\u5883\u76f8\u5173\u6027\u3001\u6210\u672c\u8003\u8651\u548c\u4f5c\u7269\u79d1\u5b66\u53ef\u4fe1\u5ea6\u3002", "result": "\u6846\u67b6\u6210\u529f\u63d0\u5347\u63a8\u8350\u8d28\u91cf\uff0c\u5e73\u5747\u5f97\u5206\u6bd4\u7b2c0\u8f6e\u63d0\u9ad830-33%\uff0c\u5728\u7b2c3-4\u8f6e\u8fbe\u5230\u5cf0\u503c\u3002\u4f46\u8fc7\u5ea6\u7cbe\u70bc\u5bfc\u81f4\u63a8\u8350\u8d28\u91cf\u663e\u8457\u4e0b\u964d\uff0c\u5448\u73b0\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff1a\u65e9\u671f\u8f6e\u6b21\u7f3a\u4e4f\u89e3\u91ca\u6df1\u5ea6\uff08\u504f\u5dee\uff09\uff0c\u8fc7\u5ea6\u8fed\u4ee3\u5f15\u5165\u5197\u957f\u548c\u672a\u63a5\u5730\u6c14\u7684\u62bd\u8c61\uff08\u65b9\u5dee\uff09\u3002\u9700\u8981\u6218\u7565\u6027\u7684\u65e9\u671f\u505c\u6b62\uff08\u6b63\u5219\u5316\uff09\u6765\u4f18\u5316\u5b9e\u7528\u6548\u679c\u3002", "conclusion": "\u667a\u80fdXAI\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u89e3\u91ca\u8d28\u91cf\uff0c\u4f46\u9700\u8981\u65e9\u671f\u505c\u6b62\u7b56\u7565\u907f\u514d\u8fc7\u5ea6\u7cbe\u70bc\u3002\u7814\u7a76\u6311\u6218\u4e86\u5355\u8c03\u6539\u8fdb\u7684\u5047\u8bbe\uff0c\u4e3a\u667a\u80fdXAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5f3a\u8c03\u5728\u89e3\u91ca\u6df1\u5ea6\u548c\u7b80\u6d01\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2512.21332", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21332", "abs": "https://arxiv.org/abs/2512.21332", "authors": ["Jin Qin", "Zihan Liao", "Ziyin Zhang", "Hang Yu", "Peng Di", "Rui Wang"], "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling", "comment": null, "summary": "We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.", "AI": {"tldr": "C2LLM\u662f\u4e00\u79cd\u57fa\u4e8eQwen-2.5-Coder\u7684\u4ee3\u7801\u5d4c\u5165\u6a21\u578b\u5bb6\u65cf\uff0c\u91c7\u7528PMA\u6a21\u5757\u751f\u6210\u5e8f\u5217\u5d4c\u5165\uff0c\u5728MTEB-Code\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u65b0\u8bb0\u5f55", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8eEOS\u7684\u5e8f\u5217\u5d4c\u5165\u5b58\u5728\u4fe1\u606f\u74f6\u9888\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528LLM\u7684\u56e0\u679c\u8868\u793a\u80fd\u529b\uff0c\u5e76\u652f\u6301\u7075\u6d3b\u7684\u5d4c\u5165\u7ef4\u5ea6\u8c03\u6574", "method": "\u57fa\u4e8eQwen-2.5-Coder\u9aa8\u5e72\u7f51\u7edc\uff0c\u91c7\u7528PMA\uff08Pooling by Multihead Attention\uff09\u6a21\u5757\u4ecetoken\u5d4c\u5165\u751f\u6210\u5e8f\u5217\u5d4c\u5165\uff0c\u5728300\u4e07\u516c\u5f00\u6570\u636e\u4e0a\u8bad\u7ec3", "result": "C2LLM-7B\u5728MTEB-Code\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5728\u76f8\u4f3c\u89c4\u6a21\u6a21\u578b\u4e2d\u521b\u4e0b\u65b0\u8bb0\u5f55", "conclusion": "C2LLM\u901a\u8fc7PMA\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u5d4c\u5165\u4e2d\u7684\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272", "topic": "code agent"}}
{"id": "2512.21220", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21220", "abs": "https://arxiv.org/abs/2512.21220", "authors": ["Le Wang", "Zonghao Ying", "Xiao Yang", "Quanchen Zou", "Zhenfei Yin", "Tianlin Li", "Jian Yang", "Yaodong Yang", "Aishan Liu", "Xianglong Liu"], "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic", "comment": "11 pages, 6 figures", "summary": "Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.", "AI": {"tldr": "RoboSafe\uff1a\u901a\u8fc7\u53ef\u6267\u884c\u8c13\u8bcd\u5b89\u5168\u903b\u8f91\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u6df7\u5408\u63a8\u7406\u8fd0\u884c\u65f6\u5b89\u5168\u9632\u62a4\u7cfb\u7edf\uff0c\u663e\u8457\u51cf\u5c11\u5371\u9669\u884c\u4e3a", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5177\u8eab\u667a\u80fd\u4f53\u5728\u6267\u884c\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u65f6\u5bb9\u6613\u53d7\u5230\u5371\u9669\u6307\u4ee4\u7684\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u7684\u9759\u6001\u89c4\u5219\u8fc7\u6ee4\u5668\u6216\u63d0\u793a\u7ea7\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u3001\u65f6\u95f4\u4f9d\u8d56\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u7684\u9690\u5f0f\u98ce\u9669", "method": "\u63d0\u51faRoboSafe\u6df7\u5408\u63a8\u7406\u8fd0\u884c\u65f6\u5b89\u5168\u9632\u62a4\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u63a8\u7406\u6a21\u5757\uff1a1) \u540e\u5411\u53cd\u601d\u63a8\u7406\u6a21\u5757\u901a\u8fc7\u77ed\u671f\u8bb0\u5fc6\u6301\u7eed\u56de\u987e\u6700\u8fd1\u8f68\u8ff9\u63a8\u65ad\u65f6\u95f4\u5b89\u5168\u8c13\u8bcd\uff1b2) \u524d\u5411\u9884\u6d4b\u63a8\u7406\u6a21\u5757\u901a\u8fc7\u957f\u671f\u5b89\u5168\u8bb0\u5fc6\u548c\u591a\u6a21\u6001\u89c2\u5bdf\u9884\u6d4b\u5373\u5c06\u98ce\u9669\u3002\u4e24\u8005\u5728\u6df7\u5408\u957f\u77ed\u5b89\u5168\u8bb0\u5fc6\u4e0a\u8fd0\u884c\uff0c\u5f62\u6210\u53ef\u9a8c\u8bc1\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u6267\u884c\u7684\u5b89\u5168\u903b\u8f91", "result": "\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0cRoboSafe\u76f8\u6bd4\u9886\u5148\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u5371\u9669\u884c\u4e3a\uff08\u98ce\u9669\u53d1\u751f\u7387\u964d\u4f4e36.8%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u539f\u59cb\u7684\u4efb\u52a1\u6027\u80fd\u3002\u7269\u7406\u673a\u5668\u4eba\u624b\u81c2\u7684\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u5176\u5b9e\u7528\u6027", "conclusion": "RoboSafe\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u3001\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u903b\u8f91\uff0c\u80fd\u591f\u6709\u6548\u4fdd\u62a4\u5177\u8eab\u667a\u80fd\u4f53\u514d\u53d7\u5371\u9669\u6307\u4ee4\u5f71\u54cd\uff0c\u5728\u52a8\u6001\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c", "topic": "agent analysis"}}
{"id": "2512.20959", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.20959", "abs": "https://arxiv.org/abs/2512.20959", "authors": ["An Luo", "Jin Du", "Fangqiao Tian", "Xun Xian", "Robert Specht", "Ganghua Wang", "Xuan Bi", "Charles Fleming", "Jayanth Srinivasa", "Ashish Kundu", "Mingyi Hong", "Jie Ding"], "title": "Can Agentic AI Match the Performance of Human Data Scientists?", "comment": null, "summary": "Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.", "AI": {"tldr": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5f0fAI\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u65e0\u6cd5\u5339\u654c\u4eba\u7c7b\u4e13\u5bb6\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u6765\u8bc6\u522b\u9690\u85cf\u5728\u56fe\u50cf\u6570\u636e\u4e2d\u7684\u5173\u952e\u6f5c\u5728\u53d8\u91cf", "motivation": "\u63a2\u7d22\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\u662f\u5426\u80fd\u591f\u771f\u6b63\u5339\u914d\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u3002\u5f53\u524dLLM\u867d\u7136\u81ea\u52a8\u5316\u4e86\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f46\u7f3a\u4e4f\u4eba\u7c7b\u4e13\u5bb6\u7684\u9886\u57df\u6d1e\u5bdf\u529b", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9884\u6d4b\u4efb\u52a1\uff0c\u5176\u4e2d\u5173\u952e\u6f5c\u5728\u53d8\u91cf\u9690\u85cf\u5728\u76f8\u5173\u56fe\u50cf\u6570\u636e\u800c\u975e\u8868\u683c\u7279\u5f81\u4e2d\u3002\u4f7f\u7528\u8d22\u4ea7\u4fdd\u9669\u7684\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4ee3\u7406\u5f0fAI\u751f\u6210\u901a\u7528\u4ee3\u7801\u7684\u65b9\u6cd5\u4e0e\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u6d1e\u5bdf\u7684\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f9d\u8d56\u901a\u7528\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u7684\u4ee3\u7406\u5f0fAI\u8868\u73b0\u4e0d\u5982\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u6d1e\u5bdf\u7684\u65b9\u6cd5\u3002\u4ee3\u7406\u5f0fAI\u65e0\u6cd5\u8bc6\u522b\u9690\u85cf\u5728\u56fe\u50cf\u6570\u636e\u4e2d\u7684\u91cd\u8981\u53d8\u91cf\uff0c\u800c\u4eba\u7c7b\u4e13\u5bb6\u53ef\u4ee5\u5229\u7528\u9886\u57df\u77e5\u8bc6\u53d1\u73b0\u8fd9\u4e9b\u53d8\u91cf", "conclusion": "\u5f53\u524d\u4ee3\u7406\u5f0fAI\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u548c\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3002\u672a\u6765\u7814\u7a76\u9700\u8981\u5f00\u53d1\u80fd\u591f\u66f4\u597d\u8bc6\u522b\u548c\u878d\u5165\u9886\u57df\u77e5\u8bc6\u7684\u4ee3\u7406\u5f0fAI\u7cfb\u7edf", "topic": "agent analysis"}}
{"id": "2512.20974", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20974", "abs": "https://arxiv.org/abs/2512.20974", "authors": ["Jingyang You", "Hanna Kurniawati"], "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions", "comment": null, "summary": "Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.", "AI": {"tldr": "\u63d0\u51faGLiBRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u57fa\u51fd\u6570\u7684\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u6539\u8fdb\u6df1\u5ea6\u8d1d\u53f6\u65af\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u6a21\u578b\u5f62\u5f0f\u548cELBO\u4f18\u5316\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5728MetaWorld\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u5df2\u77e5\u8f6c\u79fb\u548c\u5956\u52b1\u6a21\u578b\u5f62\u5f0f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u6df1\u5ea6\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u6a21\u578b\u5b66\u4e60\uff0c\u4f46\u9700\u8981\u4f18\u5316ELBO\uff08\u8bc1\u636e\u4e0b\u754c\uff09\uff0c\u8fd9\u96be\u4ee5\u4f18\u5316\u4e14\u53ef\u80fd\u5bfc\u81f4\u4efb\u52a1\u53c2\u6570\u4e0d\u660e\u786e\uff0c\u4ece\u800c\u5f71\u54cd\u7b56\u7565\u8d28\u91cf\u3002", "method": "\u63d0\u51faGLiBRL\uff08Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions\uff09\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u57fa\u51fd\u6570\u7684\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u6765\u9ad8\u6548\u51c6\u786e\u5730\u5b66\u4e60\u8f6c\u79fb\u548c\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u73b0\u5b8c\u5168\u53ef\u5904\u7406\u7684\u8fb9\u9645\u4f3c\u7136\u4ee5\u53ca\u5bf9\u4efb\u52a1\u53c2\u6570\u548c\u6a21\u578b\u566a\u58f0\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u3002", "result": "\u5728MetaWorld ML10/45\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGLiBRL\u5c06\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u8d1d\u53f6\u65af\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5VariBAD\u7684\u6210\u529f\u7387\u63d0\u5347\u4e86\u6700\u9ad82.7\u500d\u3002\u4e0eMAML\u3001RL2\u3001SDVT\u3001TrMRL\u3001ECET\u7b49\u4ee3\u8868\u6027\u6216\u6700\u65b0\u7684\u6df1\u5ea6\u8d1d\u53f6\u65af\u5f3a\u5316\u5b66\u4e60/\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0cGLiBRL\u8868\u73b0\u51fa\u4f4e\u65b9\u5dee\u548c\u7a33\u5b9a\u7684\u826f\u597d\u6027\u80fd\u3002", "conclusion": "GLiBRL\u901a\u8fc7\u53ef\u5b66\u4e60\u57fa\u51fd\u6570\u7684\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u8d1d\u53f6\u65af\u5f3a\u5316\u5b66\u4e60\u4e2d\u6a21\u578b\u5b66\u4e60\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u95ee\u9898\uff0c\u5728\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.21010", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.21010", "abs": "https://arxiv.org/abs/2512.21010", "authors": ["Jiashuo Liu", "Jiayun Wu", "Chunjie Wu", "Jingkai Liu", "Zaiyuan Wang", "Huan Zhou", "Wenhao Huang", "Hongseok Namkoong"], "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics", "comment": "18 pages", "summary": "The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.", "AI": {"tldr": "\u63d0\u51faCSD\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u8f6e\u745e\u58eb\u5236\u7ade\u8d5b\u6765\u8bc4\u4f30LLM\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8ba1\u7b97\u671f\u671b\u80dc\u7387\uff0c\u5e76\u5206\u6790\u6a21\u578b\u7684\u98ce\u9669\u504f\u597d\uff0c\u76f8\u6bd4\u4f20\u7edf\u9759\u6001\u8bc4\u5206\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u7ade\u4e89\u6027\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u9759\u6001\u8bc4\u5206\uff0c\u5b58\u5728\u5c40\u9650\u6027\uff1a\u96be\u4ee5\u786e\u5b9a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5408\u7406\u6df7\u5408\u6bd4\u4f8b\uff0c\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u5728\u8fde\u7eed\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u7ade\u4e89\u80fd\u529b\u548c\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u7ade\u4e89\u6027\u745e\u58eb\u5236\u52a8\u6001\u6846\u67b6\uff08CSD\uff09\uff0c\u6a21\u62df\u591a\u8f6e\u987a\u5e8f\u7ade\u8d5b\uff0c\u6839\u636e\u6a21\u578b\u7d2f\u8ba1\u80dc\u8d1f\u8bb0\u5f55\u52a8\u6001\u914d\u5bf9\u3002\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\uff0810\u4e07\u6b21\u8fed\u4ee3\uff09\u8ba1\u7b97\u7edf\u8ba1\u7a33\u5065\u7684\u671f\u671b\u80dc\u5206\uff0c\u6d88\u9664\u968f\u673a\u914d\u5bf9\u548c\u65e9\u671f\u8fd0\u6c14\u7684\u566a\u58f0\u3002\u901a\u8fc7\u53c2\u6570\u5316\u6bcf\u8f6e\u6dd8\u6c70\u6570\u91cf\u8fdb\u884c\u5931\u8d25\u654f\u611f\u6027\u5206\u6790\uff0c\u533a\u5206\u7a33\u5065\u901a\u624d\u548c\u6fc0\u8fdb\u4e13\u624d\u3002", "result": "CSD\u76f8\u6bd4\u4f20\u7edf\u805a\u5408\u8bc4\u5206\u548c\u9759\u6001\u914d\u5bf9\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6392\u540d\uff0c\u4ee3\u8868\u4e86\u5411\u98ce\u9669\u611f\u77e5\u7684\u4e0b\u4e00\u4ee3LLM\u8bc4\u4f30\u7684\u91cd\u8981\u4e00\u6b65\u3002", "conclusion": "CSD\u6846\u67b6\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30LLM\u7684\u7ade\u4e89\u80fd\u529b\u548c\u98ce\u9669\u7279\u5f81\uff0c\u4e3a\u4e0b\u4e00\u4ee3LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2512.21231", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.21231", "abs": "https://arxiv.org/abs/2512.21231", "authors": ["Andres M Bran", "Tong Xie", "Shai Pranesh", "Jeffrey Meng", "Xuan Vu Nguyen", "Jeremy Goumaz", "David Ming Segura", "Ruizhi Xu", "Dongzhan Zhou", "Wenjie Zhang", "Bram Hoex", "Philippe Schwaller"], "title": "MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models", "comment": null, "summary": "Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term 'latent solvability'. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faMiST\uff08\u4e2d\u671f\u79d1\u5b66\u8bad\u7ec3\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u6df7\u5408\u3001\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5316\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u5c06\u51c6\u786e\u7387\u4ece10.9%\u63d0\u5347\u81f363.9%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u4ec5\u5f53\u57fa\u7840\u6a21\u578b\u5df2\u5bf9\u6b63\u786e\u7b54\u6848\u5206\u914d\u975e\u96f6\u6982\u7387\u65f6\u624d\u6709\u6548\uff08\u6f5c\u5728\u53ef\u89e3\u6027\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5316\u5b66\u63a8\u7406\u80fd\u529b\u51fa\u73b0\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u7279\u522b\u662f\u7b26\u53f7\u80fd\u529b\u548c\u6f5c\u5728\u5316\u5b66\u77e5\u8bc6\u8fd9\u4e24\u4e2a\u5fc5\u8981\u6761\u4ef6\u3002", "method": "\u63d0\u51faMiST\uff08\u4e2d\u671f\u79d1\u5b66\u8bad\u7ec3\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1\uff09SMILES/CIF\u611f\u77e5\u9884\u5904\u7406\u7684\u6df7\u5408\u6570\u636e\uff1b2\uff09\u572829\u4ebftoken\u4e0a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\uff1b3\uff09\u572810\u4ebftoken\u4e0a\u7684\u76d1\u7763\u5fae\u8c03\u3002\u8fd9\u4e9b\u6b65\u9aa4\u65e8\u5728\u6ee1\u8db3\u5316\u5b66\u63a8\u7406\u6240\u9700\u7684\u4e24\u4e2a\u5fc5\u8981\u6761\u4ef6\u3002", "result": "MiST\u5c063B\u548c7B\u6a21\u578b\u7684\u6f5c\u5728\u53ef\u89e3\u6027\u5206\u6570\u63d0\u5347\u8fbe1.8\u500d\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u5728\u6709\u673a\u53cd\u5e94\u547d\u540d\u4efb\u52a1\u4e0a\u7684top-1\u51c6\u786e\u7387\u4ece10.9%\u63d0\u5347\u81f363.9%\uff0c\u5728\u65e0\u673a\u6750\u6599\u751f\u6210\u4efb\u52a1\u4e0a\u4ece40.6%\u63d0\u5347\u81f367.4%\u3002\u5176\u4ed6\u5316\u5b66\u4efb\u52a1\u4e5f\u89c2\u5bdf\u5230\u7c7b\u4f3c\u6539\u8fdb\uff0c\u540c\u65f6\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "\u7814\u7a76\u660e\u786e\u4e86\u5316\u5b66\u63a8\u7406\u8bad\u7ec3\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e2d\u671f\u8bad\u7ec3\u5728\u89e3\u9501\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u66f4\u5e7f\u6cdb\u4f5c\u7528\u3002MiST\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5316\u5b66\u9886\u57df\u7684\u63a8\u7406\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.597793a9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FDicklesworthstone%2Fmisc_coding_agent_tips_and_scripts%2Fblob%2Fmain%2FDESTRUCTIVE_GIT_COMMAND_CLAUDE_HOOKS_SETUP.md%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/6pGvL3QWyMxzHNdoBx3n3XWWOO4vHq4R_WlClTD-63A=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FDicklesworthstone%2Fmisc_coding_agent_tips_and_scripts%2Fblob%2Fmain%2FDESTRUCTIVE_GIT_COMMAND_CLAUDE_HOOKS_SETUP.md%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/6pGvL3QWyMxzHNdoBx3n3XWWOO4vHq4R_WlClTD-63A=436", "authors": ["TLDR Newsletter"], "title": "Destructive Git Command Protection for Claude Code", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FDicklesworthstone%2Fmisc_coding_agent_tips_and_scripts%2Fblob%2Fmain%2FDESTRUCTIVE_GIT_COMMAND_CLAUDE_HOOKS_SETUP.md%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/6pGvL3QWyMxzHNdoBx3n3XWWOO4vHq4R_WlClTD-63A=436", "summary": "Destructive Git Command Protection for Claude Code (17 minute read) AI agents can execute destructive commands without understanding the consequences. They can do this even if the AGENTS.md file forbids such commands. This post provides a Claude Code hook that blocks certain git commands before they can run. The hook uses regex pattern matching, so clever or obfuscated commands may be able to bypass it.", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2aGit\u94a9\u5b50\u6765\u4fdd\u62a4Claude Code AI\u4ee3\u7406\uff0c\u9632\u6b62\u5176\u6267\u884c\u7834\u574f\u6027Git\u547d\u4ee4\uff0c\u5373\u4f7fAGENTS.md\u6587\u4ef6\u7981\u6b62\u8fd9\u4e9b\u547d\u4ee4\uff0cAI\u4ecd\u53ef\u80fd\u6267\u884c\u3002", "motivation": "AI\u4ee3\u7406\u5728\u6267\u884c\u4ee3\u7801\u65f6\u53ef\u80fd\u4e0d\u7406\u89e3\u67d0\u4e9bGit\u547d\u4ee4\u7684\u7834\u574f\u6027\u540e\u679c\uff0c\u5373\u4f7f\u6709\u660e\u786e\u7684\u7981\u6b62\u89c4\u5b9a\uff0c\u5b83\u4eec\u4ecd\u53ef\u80fd\u6267\u884c\u8fd9\u4e9b\u5371\u9669\u64cd\u4f5c\uff0c\u9700\u8981\u989d\u5916\u7684\u4fdd\u62a4\u673a\u5236\u3002", "method": "\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u6a21\u5f0f\u5339\u914d\u7684Git\u94a9\u5b50\uff0c\u5728\u547d\u4ee4\u6267\u884c\u524d\u62e6\u622a\u7279\u5b9a\u7684\u7834\u574f\u6027Git\u547d\u4ee4\uff0c\u4e3aClaude Code\u63d0\u4f9b\u4fdd\u62a4\u5c42\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u4fdd\u62a4\u673a\u5236\uff0c\u80fd\u591f\u963b\u6b62\u5e38\u89c1\u7684\u7834\u574f\u6027Git\u547d\u4ee4\u6267\u884c\uff0c\u4f46\u627f\u8ba4\u901a\u8fc7\u5de7\u5999\u6216\u6df7\u6dc6\u7684\u547d\u4ee4\u53ef\u80fd\u7ed5\u8fc7\u8be5\u4fdd\u62a4\u3002", "conclusion": "\u867d\u7136\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u7684\u4fdd\u62a4\u673a\u5236\u63d0\u4f9b\u4e86\u57fa\u672c\u7684\u5b89\u5168\u5c42\uff0c\u4f46\u5e76\u975e\u7edd\u5bf9\u5b89\u5168\uff0c\u9700\u8981\u610f\u8bc6\u5230\u5176\u5c40\u9650\u6027\u5e76\u8003\u8651\u66f4\u5168\u9762\u7684\u5b89\u5168\u7b56\u7565\u3002", "topic": "code agent"}}
{"id": "tldr.2512.cacbd02c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fyear-in-review-2025%2F%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/aRRmLzQ7wZryndXcvEEe9_kRAaTkKEVR57XeCl1c380=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fyear-in-review-2025%2F%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/aRRmLzQ7wZryndXcvEEe9_kRAaTkKEVR57XeCl1c380=436", "authors": ["TLDR Newsletter"], "title": "2025 LLM Year in Review", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fyear-in-review-2025%2F%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/aRRmLzQ7wZryndXcvEEe9_kRAaTkKEVR57XeCl1c380=436", "summary": "2025 LLM Year in Review (9 minute read) 2025 was an eventful year for LLMs, with paradigm shifts that redefined the field. Reinforcement Learning from Verifiable Rewards (RLVR) came up as a new training stage, allowing LLMs to develop reasoning strategies and driving much of the year's capability progress. The year also saw the rise of new application layers, local AI agents like Claude Code, \"vibe coding\" that democratized programming, and early hints of a visual LLM GUI.", "source": "tldr", "AI": {"tldr": "2025\u5e74LLM\u9886\u57df\u56de\u987e\uff1aRLVR\u6210\u4e3a\u65b0\u8bad\u7ec3\u9636\u6bb5\u63a8\u52a8\u80fd\u529b\u8fdb\u6b65\uff0c\u5e94\u7528\u5c42\u6d8c\u73b0Claude Code\u7b49\u672c\u5730AI\u4ee3\u7406\uff0c\"\u6c1b\u56f4\u7f16\u7a0b\"\u666e\u53ca\u7f16\u7a0b\uff0c\u89c6\u89c9LLM GUI\u521d\u73b0", "motivation": "\u603b\u7ed32025\u5e74LLM\u9886\u57df\u7684\u91cd\u8981\u53d1\u5c55\u548c\u8303\u5f0f\u8f6c\u53d8\uff0c\u5206\u6790\u63a8\u52a8\u80fd\u529b\u8fdb\u6b65\u7684\u5173\u952e\u6280\u672f\u8d8b\u52bf\u548c\u5e94\u7528\u521b\u65b0", "method": "\u56de\u987e\u6027\u5206\u6790\uff0c\u7efc\u5408\u8bc4\u4f302025\u5e74LLM\u9886\u57df\u7684\u4e3b\u8981\u6280\u672f\u7a81\u7834\u3001\u5e94\u7528\u53d1\u5c55\u548c\u8d8b\u52bf\u53d8\u5316", "result": "\u8bc6\u522b\u51faRLVR\u4f5c\u4e3a\u5173\u952e\u8bad\u7ec3\u9636\u6bb5\u63a8\u52a8\u80fd\u529b\u8fdb\u6b65\uff0c\u5e94\u7528\u5c42\u521b\u65b0\u5305\u62ec\u672c\u5730AI\u4ee3\u7406\u3001\u7f16\u7a0b\u6c11\u4e3b\u5316\u3001\u89c6\u89c9\u754c\u9762\u7b49\u65b0\u8d8b\u52bf", "conclusion": "2025\u5e74\u662fLLM\u9886\u57df\u8303\u5f0f\u8f6c\u53d8\u7684\u4e00\u5e74\uff0cRLVR\u8bad\u7ec3\u65b9\u6cd5\u548c\u5e94\u7528\u5c42\u521b\u65b0\u5171\u540c\u63a8\u52a8\u4e86\u6280\u672f\u53d1\u5c55\u548c\u666e\u53ca", "topic": "agent analysis"}}
{"id": "tldr.2512.080a5c6a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmujoai.com%2F%3Futm_source=tldrdesign/1/0100019b462a557f-66e90ca3-c3db-41a4-a0d8-30863c68f344-000000/xl7d4WfxrULtpWpP5QTR66Ipsxted7rFRinvBLRjQTM=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmujoai.com%2F%3Futm_source=tldrdesign/1/0100019b462a557f-66e90ca3-c3db-41a4-a0d8-30863c68f344-000000/xl7d4WfxrULtpWpP5QTR66Ipsxted7rFRinvBLRjQTM=436", "authors": ["TLDR Newsletter"], "title": "AI Agent for E\u2011commerce Listing Content", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmujoai.com%2F%3Futm_source=tldrdesign/1/0100019b462a557f-66e90ca3-c3db-41a4-a0d8-30863c68f344-000000/xl7d4WfxrULtpWpP5QTR66Ipsxted7rFRinvBLRjQTM=436", "summary": "AI Agent for E\u2011commerce Listing Content (Website) Turn one product image into optimized marketplace-ready visuals and copy.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u5c06\u5355\u5f20\u4ea7\u54c1\u56fe\u7247\u8f6c\u5316\u4e3a\u7535\u5546\u5e73\u53f0\u4f18\u5316\u7684\u89c6\u89c9\u5185\u5bb9\u548c\u6587\u6848", "motivation": "\u7535\u5546\u5356\u5bb6\u9700\u8981\u4e3a\u4e0d\u540c\u5e73\u53f0\u521b\u5efa\u4f18\u5316\u7684\u4ea7\u54c1\u5217\u8868\u5185\u5bb9\uff0c\u624b\u52a8\u5904\u7406\u8017\u65f6\u4e14\u96be\u4ee5\u4fdd\u8bc1\u8d28\u91cf\u4e00\u81f4\u6027", "method": "\u4f7f\u7528AI\u4ee3\u7406\u5206\u6790\u4ea7\u54c1\u56fe\u7247\uff0c\u81ea\u52a8\u751f\u6210\u591a\u5e73\u53f0\u9002\u914d\u7684\u89c6\u89c9\u5185\u5bb9\uff08\u5982\u4e0d\u540c\u5c3a\u5bf8\u3001\u683c\u5f0f\uff09\u548c\u8425\u9500\u6587\u6848", "result": "\u80fd\u591f\u4ece\u5355\u5f20\u4ea7\u54c1\u56fe\u7247\u5feb\u901f\u751f\u6210\u7b26\u5408\u5404\u7535\u5546\u5e73\u53f0\u8981\u6c42\u7684\u4f18\u5316\u5217\u8868\u5185\u5bb9", "conclusion": "AI\u4ee3\u7406\u80fd\u663e\u8457\u63d0\u9ad8\u7535\u5546\u4ea7\u54c1\u5217\u8868\u5185\u5bb9\u521b\u5efa\u7684\u6548\u7387\u548c\u4e00\u81f4\u6027", "topic": "code agent"}}
{"id": "tldr.2512.a212425d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fzx2Ww5/1/0100019b462c3d5c-5c142b52-7e40-462d-8c8c-e975c4c6e210-000000/mBmj-M8ViYMrahhT8VG86qzNyfh4gYuhjluYPu0QMTk=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fzx2Ww5/1/0100019b462c3d5c-5c142b52-7e40-462d-8c8c-e975c4c6e210-000000/mBmj-M8ViYMrahhT8VG86qzNyfh4gYuhjluYPu0QMTk=436", "authors": ["TLDR Newsletter"], "title": "AI x Crypto is Dead, Long Live AI x Crypto", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fzx2Ww5/1/0100019b462c3d5c-5c142b52-7e40-462d-8c8c-e975c4c6e210-000000/mBmj-M8ViYMrahhT8VG86qzNyfh4gYuhjluYPu0QMTk=436", "summary": "AI x Crypto is Dead, Long Live AI x Crypto (5 minute read) The AI x crypto theme evolved from 2023's decentralized data marketplaces and compute sharing to 2025's autonomous agents managing assets and executing trades. Bittensor surpassed $10 billion market cap as x402 payment protocols enable seamless high-volume settlements without human oversight. Agentic tooling revolutionized onchain development, allowing non-technical founders to launch businesses in days. Grayscale's Decentralized AI F...", "source": "tldr", "AI": {"tldr": "AI\u4e0e\u52a0\u5bc6\u8d27\u5e01\u7ed3\u5408\u4ece2023\u5e74\u7684\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u5e02\u573a\u548c\u7b97\u529b\u5171\u4eab\uff0c\u53d1\u5c55\u52302025\u5e74\u7531\u81ea\u4e3b\u4ee3\u7406\u7ba1\u7406\u8d44\u4ea7\u548c\u6267\u884c\u4ea4\u6613\uff0cBittensor\u5e02\u503c\u8d85\u767e\u4ebf\u7f8e\u5143\uff0cx402\u652f\u4ed8\u534f\u8bae\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u9ad8\u9891\u7ed3\u7b97\u3002", "motivation": "\u63a2\u7d22AI\u4e0e\u52a0\u5bc6\u8d27\u5e01\u6280\u672f\u878d\u5408\u7684\u6f14\u8fdb\u8def\u5f84\uff0c\u5c55\u793a\u4ece\u57fa\u7840\u8bbe\u65bd\u5230\u5e94\u7528\u5c42\u7684\u8f6c\u53d8\uff0c\u7279\u522b\u662f\u81ea\u4e3b\u4ee3\u7406\u5982\u4f55\u6539\u53d8\u94fe\u4e0a\u8d44\u4ea7\u7ba1\u7406\u548c\u4ea4\u6613\u6267\u884c\u3002", "method": "\u5206\u6790AI x Crypto\u4e3b\u9898\u4ece2023\u5e74\u52302025\u5e74\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5305\u62ecBittensor\u751f\u6001\u3001x402\u652f\u4ed8\u534f\u8bae\u3001\u4ee3\u7406\u5de5\u5177\u5bf9\u94fe\u4e0a\u5f00\u53d1\u7684\u5f71\u54cd\uff0c\u4ee5\u53caGrayscale\u7684\u53bb\u4e2d\u5fc3\u5316AI\u57fa\u91d1\u3002", "result": "AI\u4ee3\u7406\u5de5\u5177\u5f7b\u5e95\u6539\u53d8\u4e86\u94fe\u4e0a\u5f00\u53d1\uff0c\u4f7f\u975e\u6280\u672f\u521b\u59cb\u4eba\u80fd\u5728\u51e0\u5929\u5185\u542f\u52a8\u4e1a\u52a1\uff1bBittensor\u5e02\u503c\u7a81\u7834100\u4ebf\u7f8e\u5143\uff1bx402\u534f\u8bae\u5b9e\u73b0\u65e0\u7f1d\u9ad8\u9891\u7ed3\u7b97\uff1b\u81ea\u4e3b\u4ee3\u7406\u6210\u4e3a\u8d44\u4ea7\u7ba1\u7406\u65b0\u8303\u5f0f\u3002", "conclusion": "AI x Crypto\u4e3b\u9898\u5df2\u4ece\u57fa\u7840\u8bbe\u65bd\u9636\u6bb5\u8fdb\u5165\u5e94\u7528\u7206\u53d1\u671f\uff0c\u81ea\u4e3b\u4ee3\u7406\u7ba1\u7406\u8d44\u4ea7\u548c\u4ea4\u6613\u6267\u884c\u6210\u4e3a\u4e3b\u6d41\uff0c\u6807\u5fd7\u7740\u8be5\u9886\u57df\u8fdb\u5165\u6210\u719f\u53d1\u5c55\u9636\u6bb5\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.9d28a6b4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Ftransform%2Fhow-google-does-it-building-ai-agents-cybersecurity-defense%2F%3Futm_source=tldrinfosec/1/0100019b4663f016-1d65b79a-f1bc-46cc-9f58-e82be1047dbf-000000/i07eTVtIyAi9V4KRrK_9xt9BjKXwcAGZdPFwpSSvKv4=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Ftransform%2Fhow-google-does-it-building-ai-agents-cybersecurity-defense%2F%3Futm_source=tldrinfosec/1/0100019b4663f016-1d65b79a-f1bc-46cc-9f58-e82be1047dbf-000000/i07eTVtIyAi9V4KRrK_9xt9BjKXwcAGZdPFwpSSvKv4=436", "authors": ["TLDR Newsletter"], "title": "How Google Does It: Building Agents for Cybersecurity and Defense", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Ftransform%2Fhow-google-does-it-building-ai-agents-cybersecurity-defense%2F%3Futm_source=tldrinfosec/1/0100019b4663f016-1d65b79a-f1bc-46cc-9f58-e82be1047dbf-000000/i07eTVtIyAi9V4KRrK_9xt9BjKXwcAGZdPFwpSSvKv4=436", "summary": "How Google Does It: Building Agents for Cybersecurity and Defense (6 minute read) When introducing agentic AI to its cybersecurity teams, Google began by building trust in generative AI by adding chat interfaces to existing tools. Google security then identified initial use cases for distillation and translation, focusing on bottlenecks that AI could alleviate. The team then established and monitored KPIs as they scaled their program.", "source": "tldr", "AI": {"tldr": "Google\u5206\u4eab\u4e86\u5728\u7f51\u7edc\u5b89\u5168\u56e2\u961f\u4e2d\u5f15\u5165\u667a\u80fd\u4ee3\u7406AI\u7684\u65b9\u6cd5\u8bba\uff1a\u4ece\u5efa\u7acb\u4fe1\u4efb\u5f00\u59cb\uff0c\u4e3a\u73b0\u6709\u5de5\u5177\u6dfb\u52a0\u804a\u5929\u754c\u9762\uff0c\u7136\u540e\u8bc6\u522b\u9002\u5408AI\u89e3\u51b3\u7684\u74f6\u9888\u7528\u4f8b\uff0c\u6700\u540e\u5efa\u7acbKPI\u76d1\u63a7\u5e76\u89c4\u6a21\u5316\u6269\u5c55\u3002", "motivation": "Google\u5e0c\u671b\u5c06\u667a\u80fd\u4ee3\u7406AI\u5f15\u5165\u7f51\u7edc\u5b89\u5168\u548c\u9632\u5fa1\u9886\u57df\uff0c\u4f46\u9762\u4e34\u5982\u4f55\u8ba9\u5b89\u5168\u56e2\u961f\u4fe1\u4efb\u548c\u63a5\u53d7AI\u6280\u672f\u7684\u6311\u6218\u3002\u9700\u8981\u627e\u5230\u65e2\u80fd\u53d1\u6325AI\u4f18\u52bf\u53c8\u80fd\u89e3\u51b3\u5b9e\u9645\u5b89\u5168\u74f6\u9888\u7684\u6709\u6548\u8def\u5f84\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u4e3a\u73b0\u6709\u5de5\u5177\u6dfb\u52a0\u804a\u5929\u754c\u9762\u5efa\u7acb\u4fe1\u4efb\uff1b2\uff09\u8bc6\u522b\u9002\u5408AI\u7684\u521d\u59cb\u7528\u4f8b\uff08\u63d0\u70bc\u548c\u7ffb\u8bd1\u7c7b\u4efb\u52a1\uff09\uff1b3\uff09\u805a\u7126AI\u80fd\u7f13\u89e3\u7684\u74f6\u9888\u95ee\u9898\uff1b4\uff09\u5efa\u7acbKPI\u76d1\u63a7\u4f53\u7cfb\uff1b5\uff09\u9010\u6b65\u89c4\u6a21\u5316\u6269\u5c55\u3002", "result": "\u6210\u529f\u5c06\u667a\u80fd\u4ee3\u7406AI\u5f15\u5165\u7f51\u7edc\u5b89\u5168\u56e2\u961f\uff0c\u5efa\u7acb\u4e86\u4ece\u4fe1\u4efb\u6784\u5efa\u5230\u89c4\u6a21\u5316\u5e94\u7528\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u4e3a\u5176\u4ed6\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u53ef\u501f\u9274\u7684\u5b9e\u65bd\u6846\u67b6\u3002", "conclusion": "\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5f15\u5165\u667a\u80fd\u4ee3\u7406AI\u9700\u8981\u6e10\u8fdb\u5f0f\u65b9\u6cd5\uff0c\u4ece\u5efa\u7acb\u4fe1\u4efb\u5f00\u59cb\uff0c\u805a\u7126\u5177\u4f53\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u5316\u76d1\u63a7\u786e\u4fdd\u6548\u679c\uff0c\u6700\u7ec8\u5b9e\u73b0\u89c4\u6a21\u5316\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.bffd02b9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fx9VU0x/1/0100019b466402b3-09d2e8bc-4a73-4a5b-9492-78f98212cd45-000000/bLDotlhUF5S1FJCxc5mC2dSb061qdg4kpZHOa7xP5T4=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fx9VU0x/1/0100019b466402b3-09d2e8bc-4a73-4a5b-9492-78f98212cd45-000000/bLDotlhUF5S1FJCxc5mC2dSb061qdg4kpZHOa7xP5T4=436", "authors": ["TLDR Newsletter"], "title": "Visa and Aldar complete end-to-end voice-enabled agentic payment", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fx9VU0x/1/0100019b466402b3-09d2e8bc-4a73-4a5b-9492-78f98212cd45-000000/bLDotlhUF5S1FJCxc5mC2dSb061qdg4kpZHOa7xP5T4=436", "summary": "Visa and Aldar complete end-to-end voice-enabled agentic payment (5 minute read) Visa and Aldar today announced a strategic collaboration that marks the first live implementation of Visa Intelligent Commerce in the region, introducing end-to-end, voice-enabled agentic payment experiences.", "source": "tldr", "AI": {"tldr": "Visa\u4e0eAldar\u5408\u4f5c\u63a8\u51fa\u9996\u4e2a\u7aef\u5230\u7aef\u8bed\u97f3\u9a71\u52a8\u7684\u667a\u80fd\u652f\u4ed8\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b9e\u73b0\u8bed\u97f3\u6fc0\u6d3b\u7684\u667a\u80fd\u652f\u4ed8\u4f53\u9a8c", "motivation": "\u5728\u6570\u5b57\u652f\u4ed8\u9886\u57df\uff0c\u7528\u6237\u671f\u671b\u66f4\u81ea\u7136\u3001\u4fbf\u6377\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002\u8bed\u97f3\u652f\u4ed8\u4f5c\u4e3a\u65b0\u5174\u6280\u672f\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u7f3a\u4e4f\u7aef\u5230\u7aef\u7684\u667a\u80fd\u4ee3\u7406\u80fd\u529b\u3002Visa\u4e0eAldar\u7684\u5408\u4f5c\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u6280\u672f\u5b9e\u73b0\u8bed\u97f3\u9a71\u52a8\u7684\u5b8c\u6574\u652f\u4ed8\u6d41\u7a0b\u3002", "method": "\u91c7\u7528Visa Intelligent Commerce\u5e73\u53f0\uff0c\u7ed3\u5408\u8bed\u97f3\u8bc6\u522b\u548c\u667a\u80fd\u4ee3\u7406\u6280\u672f\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u652f\u4ed8\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u7406\u89e3\u7528\u6237\u8bed\u97f3\u6307\u4ee4\uff0c\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u5904\u7406\u652f\u4ed8\u6388\u6743\u3001\u9a8c\u8bc1\u548c\u7ed3\u7b97\u7b49\u5168\u6d41\u7a0b\u64cd\u4f5c\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u9996\u4e2a\u533a\u57df\u6027\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u667a\u80fd\u652f\u4ed8\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u8bed\u97f3\u6307\u4ee4\u5b8c\u6210\u5b8c\u6574\u7684\u652f\u4ed8\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u652f\u4ed8\u4f53\u9a8c\u7684\u4fbf\u6377\u6027\u548c\u81ea\u7136\u6027\u3002", "conclusion": "\u8bed\u97f3\u9a71\u52a8\u7684\u667a\u80fd\u4ee3\u7406\u652f\u4ed8\u4ee3\u8868\u4e86\u6570\u5b57\u652f\u4ed8\u7684\u65b0\u65b9\u5411\uff0cVisa\u4e0eAldar\u7684\u5408\u4f5c\u5c55\u793a\u4e86\u8be5\u6280\u672f\u5728\u5546\u4e1a\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u66f4\u667a\u80fd\u3001\u81ea\u7136\u7684\u652f\u4ed8\u4f53\u9a8c\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.18a25cb7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1Sm-XUZ4MvYHcOw7gsoIpdEu38GhCpgNCMnx6Fa0grks%2Fedit%3Ftab=t.3awwxw6mhl75%23heading=h.xy9wi236lxm%26utm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/MdE4u44aH_hkH9cEXimrpN4k1uXn9M37eJDB_gZ6yFo=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1Sm-XUZ4MvYHcOw7gsoIpdEu38GhCpgNCMnx6Fa0grks%2Fedit%3Ftab=t.3awwxw6mhl75%23heading=h.xy9wi236lxm%26utm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/MdE4u44aH_hkH9cEXimrpN4k1uXn9M37eJDB_gZ6yFo=436", "authors": ["TLDR Newsletter"], "title": "Experiment Diary", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1Sm-XUZ4MvYHcOw7gsoIpdEu38GhCpgNCMnx6Fa0grks%2Fedit%3Ftab=t.3awwxw6mhl75%23heading=h.xy9wi236lxm%26utm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/MdE4u44aH_hkH9cEXimrpN4k1uXn9M37eJDB_gZ6yFo=436", "summary": "Experiment Diary (3 minute read) This document contains a diary for an experiment aimed at teaching an LLM using GRPO to generate regex given a description. It details the performance, learnings, modifications, and key takeaways from each experiment. The initial training run was on December 17. It saw the model quickly learning how to generate valid regex tags, but the model was basically generating random regex strings.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528GRPO\u8bad\u7ec3LLM\u6839\u636e\u63cf\u8ff0\u751f\u6210\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u5b9e\u9a8c\u65e5\u5fd7\uff0c\u8bb0\u5f55\u4e86\u6a21\u578b\u4ece\u751f\u6210\u968f\u673a\u6b63\u5219\u5230\u5b66\u4e60\u6709\u6548\u6a21\u5f0f\u7684\u6f14\u8fdb\u8fc7\u7a0b", "motivation": "\u63a2\u7d22\u4f7f\u7528GRPO\uff08Group Relative Policy Optimization\uff09\u65b9\u6cd5\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u6b63\u786e\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6a21\u578b\u96be\u4ee5\u7406\u89e3\u590d\u6742\u6a21\u5f0f\u5339\u914d\u9700\u6c42\u7684\u95ee\u9898", "method": "\u91c7\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8fed\u4ee3\u7684\u65b9\u5f0f\u8bad\u7ec3LLM\uff0c\u8bb0\u5f55\u6bcf\u6b21\u5b9e\u9a8c\u7684\u6027\u80fd\u3001\u5b66\u4e60\u8fc7\u7a0b\u3001\u6a21\u578b\u4fee\u6539\u548c\u5173\u952e\u53d1\u73b0\uff0c\u4ece12\u670817\u65e5\u7684\u521d\u59cb\u8bad\u7ec3\u5f00\u59cb\u9010\u6b65\u4f18\u5316", "result": "\u521d\u59cb\u8bad\u7ec3\u663e\u793a\u6a21\u578b\u80fd\u5feb\u901f\u5b66\u4f1a\u751f\u6210\u6709\u6548\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u6807\u7b7e\uff0c\u4f46\u4e3b\u8981\u751f\u6210\u968f\u673a\u5b57\u7b26\u4e32\uff1b\u540e\u7eed\u5b9e\u9a8c\u901a\u8fc7\u8c03\u6574\u8bad\u7ec3\u7b56\u7565\u548c\u53c2\u6570\uff0c\u6a21\u578b\u9010\u6e10\u5b66\u4f1a\u751f\u6210\u66f4\u7b26\u5408\u63cf\u8ff0\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u6a21\u5f0f", "conclusion": "GRPO\u65b9\u6cd5\u5728\u8bad\u7ec3LLM\u751f\u6210\u6b63\u5219\u8868\u8fbe\u5f0f\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u8fed\u4ee3\u4f18\u5316\u624d\u80fd\u8ba9\u6a21\u578b\u771f\u6b63\u7406\u89e3\u6a21\u5f0f\u5339\u914d\u9700\u6c42\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u968f\u673a\u5b57\u7b26\u4e32", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.f7efb6d3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fyear-in-review-2025%2F%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/a08iKai97CQVk5L-hcFhmV8vIris3rWKhF7-ERwjg-Y=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fyear-in-review-2025%2F%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/a08iKai97CQVk5L-hcFhmV8vIris3rWKhF7-ERwjg-Y=436", "authors": ["TLDR Newsletter"], "title": "Andrej Karpathy's 2025 LLM Year in Review", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fyear-in-review-2025%2F%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/a08iKai97CQVk5L-hcFhmV8vIris3rWKhF7-ERwjg-Y=436", "summary": "Andrej Karpathy's 2025 LLM Year in Review (6 minute read) Andrej Karpathy has outlined paradigm shifts of LLMs in 2025, including fast inference engines, model distillation trends, real-time agents, neural GPUs, and the rise of high-quality open models like DeepSeek-V2 and RWKV.", "source": "tldr", "AI": {"tldr": "Andrej Karpathy\u603b\u7ed3\u4e862025\u5e74LLM\u9886\u57df\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5305\u62ec\u5feb\u901f\u63a8\u7406\u5f15\u64ce\u3001\u6a21\u578b\u84b8\u998f\u8d8b\u52bf\u3001\u5b9e\u65f6\u667a\u80fd\u4f53\u3001\u795e\u7ecfGPU\u4ee5\u53ca\u9ad8\u8d28\u91cf\u5f00\u6e90\u6a21\u578b\u7684\u5d1b\u8d77\u3002", "motivation": "\u603b\u7ed32025\u5e74\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9886\u57df\u7684\u91cd\u8981\u53d1\u5c55\u8d8b\u52bf\u548c\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5e74\u5ea6\u6280\u672f\u56de\u987e\u548c\u672a\u6765\u65b9\u5411\u6307\u5f15\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u89c6\u89d2\u5206\u67902025\u5e74LLM\u9886\u57df\u7684\u5173\u952e\u6280\u672f\u7a81\u7834\u548c\u53d1\u5c55\u8d8b\u52bf\uff0c\u5305\u62ec\u5bf9\u5feb\u901f\u63a8\u7406\u5f15\u64ce\u3001\u6a21\u578b\u84b8\u998f\u3001\u5b9e\u65f6\u667a\u80fd\u4f53\u3001\u795e\u7ecfGPU\u67b6\u6784\u548c\u5f00\u6e90\u6a21\u578b\u7b49\u9886\u57df\u7684\u7cfb\u7edf\u6027\u603b\u7ed3\u3002", "result": "\u8bc6\u522b\u51fa2025\u5e74LLM\u9886\u57df\u7684\u4e94\u4e2a\u4e3b\u8981\u8303\u5f0f\u8f6c\u53d8\uff1a1\uff09\u5feb\u901f\u63a8\u7406\u5f15\u64ce\u7684\u53d1\u5c55\uff1b2\uff09\u6a21\u578b\u84b8\u998f\u6210\u4e3a\u91cd\u8981\u8d8b\u52bf\uff1b3\uff09\u5b9e\u65f6\u667a\u80fd\u4f53\u7684\u5174\u8d77\uff1b4\uff09\u795e\u7ecfGPU\u67b6\u6784\u7684\u521b\u65b0\uff1b5\uff09\u9ad8\u8d28\u91cf\u5f00\u6e90\u6a21\u578b\uff08\u5982DeepSeek-V2\u548cRWKV\uff09\u7684\u5d1b\u8d77\u3002", "conclusion": "2025\u5e74\u662fLLM\u6280\u672f\u5feb\u901f\u6f14\u8fdb\u7684\u5173\u952e\u5e74\u4efd\uff0c\u591a\u4e2a\u6280\u672f\u65b9\u5411\u540c\u65f6\u53d6\u5f97\u7a81\u7834\uff0c\u7279\u522b\u662f\u5f00\u6e90\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u6b63\u5728\u6539\u53d8\u884c\u4e1a\u683c\u5c40\uff0c\u4e3aAI\u5e94\u7528\u7684\u666e\u53ca\u548c\u521b\u65b0\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.61601fde", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmimo.xiaomi.com%2Fblog%2Fmimo-v2-flash%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/Jic_GgtZV6zI3MIaRuGvgsVBL2oZ4DLMVQ_cnyS1iV4=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmimo.xiaomi.com%2Fblog%2Fmimo-v2-flash%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/Jic_GgtZV6zI3MIaRuGvgsVBL2oZ4DLMVQ_cnyS1iV4=436", "authors": ["TLDR Newsletter"], "title": "Introducing MiMo-V2-Flash", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmimo.xiaomi.com%2Fblog%2Fmimo-v2-flash%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/Jic_GgtZV6zI3MIaRuGvgsVBL2oZ4DLMVQ_cnyS1iV4=436", "summary": "Introducing MiMo-V2-Flash (10 minute read) MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundational language model that excels in reasoning, coding, and agentic scenarios. It serves as an excellent general-purpose assistant for everyday tasks. The model is available globally on Hugging Face, AI Studio, and Xiaomi's API platform. Benchmark results are available in the article.", "source": "tldr", "AI": {"tldr": "MiMo-V2-Flash\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u8d85\u5feb\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u63a8\u7406\u3001\u7f16\u7801\u548c\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u4f5c\u4e3a\u65e5\u5e38\u4efb\u52a1\u7684\u901a\u7528\u52a9\u624b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u5feb\u901f\u7684\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u63a8\u7406\u3001\u7f16\u7801\u548c\u667a\u80fd\u4f53\u4efb\u52a1\uff0c\u540c\u65f6\u4f5c\u4e3a\u65e5\u5e38\u4efb\u52a1\u7684\u901a\u7528\u52a9\u624b\u3002", "method": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u65b9\u6cd5\uff0c\u4f46\u63d0\u5230\u8be5\u6a21\u578b\u662f\u4e00\u4e2a\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u63a8\u7406\u3001\u7f16\u7801\u548c\u667a\u80fd\u4f53\u573a\u666f\u3002", "result": "\u6a21\u578b\u5df2\u5728Hugging Face\u3001AI Studio\u548c\u5c0f\u7c73API\u5e73\u53f0\u5168\u7403\u53d1\u5e03\uff0c\u6587\u7ae0\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u3002", "conclusion": "MiMo-V2-Flash\u662f\u4e00\u4e2a\u529f\u80fd\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u8d85\u5feb\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u4f5c\u4e3a\u901a\u7528\u52a9\u624b\u4f7f\u7528\u3002", "topic": "code agent"}}
{"id": "tldr.2512.c2e5987e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cloudnativedeepdive.com%2Fmultiplexing-mcp-servers-for-agentic-specialization%2F%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/tBhhDa1o5sGYbnKJqS7DYSXgAkJsV3hOcm9JSUmz3Pc=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cloudnativedeepdive.com%2Fmultiplexing-mcp-servers-for-agentic-specialization%2F%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/tBhhDa1o5sGYbnKJqS7DYSXgAkJsV3hOcm9JSUmz3Pc=436", "authors": ["TLDR Newsletter"], "title": "Multiplexing MCP Servers For Agentic Specialization", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cloudnativedeepdive.com%2Fmultiplexing-mcp-servers-for-agentic-specialization%2F%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/tBhhDa1o5sGYbnKJqS7DYSXgAkJsV3hOcm9JSUmz3Pc=436", "summary": "Multiplexing MCP Servers For Agentic Specialization (8 minute read) MCP servers give agents the tools they need to accomplish tasks. This post discusses how to multiplex MCP servers to simplify the connection to various tools within them. Multiplexing allows multiple MCP servers to be used over a gateway in a single interaction. It allows agents to access multiple MCP servers with different stacks, clouds, applications, and frameworks for specialized tasks.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u5982\u4f55\u901a\u8fc7\u591a\u8def\u590d\u7528MCP\u670d\u52a1\u5668\u5b9e\u73b0\u4ee3\u7406\u4e13\u4e1a\u5316\uff0c\u7b80\u5316\u4e0e\u5404\u79cd\u5de5\u5177\u8fde\u63a5\uff0c\u4f7f\u4ee3\u7406\u80fd\u901a\u8fc7\u5355\u4e00\u7f51\u5173\u8bbf\u95ee\u591a\u4e2a\u4e0d\u540c\u6280\u672f\u6808\u7684MCP\u670d\u52a1\u5668\u3002", "motivation": "MCP\u670d\u52a1\u5668\u4e3a\u4ee3\u7406\u63d0\u4f9b\u5b8c\u6210\u4efb\u52a1\u6240\u9700\u7684\u5de5\u5177\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u8fde\u63a5\u591a\u4e2a\u4e0d\u540c\u6280\u672f\u6808\u7684\u670d\u52a1\u5668\uff0c\u73b0\u6709\u8fde\u63a5\u65b9\u5f0f\u590d\u6742\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u7b80\u5316\u591a\u670d\u52a1\u5668\u8bbf\u95ee\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u591a\u8def\u590d\u7528\u6280\u672f\uff0c\u901a\u8fc7\u7f51\u5173\u5b9e\u73b0\u591a\u4e2aMCP\u670d\u52a1\u5668\u7684\u7edf\u4e00\u8bbf\u95ee\uff0c\u5141\u8bb8\u4ee3\u7406\u5728\u5355\u4e2a\u4ea4\u4e92\u4e2d\u8bbf\u95ee\u5177\u6709\u4e0d\u540c\u6280\u672f\u6808\u3001\u4e91\u5e73\u53f0\u3001\u5e94\u7528\u548c\u6846\u67b6\u7684\u591a\u4e2a\u670d\u52a1\u5668\u3002", "result": "\u591a\u8def\u590d\u7528\u6280\u672f\u7b80\u5316\u4e86\u4ee3\u7406\u4e0e\u591a\u4e2aMCP\u670d\u52a1\u5668\u7684\u8fde\u63a5\uff0c\u63d0\u9ad8\u4e86\u5de5\u5177\u8bbf\u95ee\u6548\u7387\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u66f4\u7075\u6d3b\u5730\u5229\u7528\u4e0d\u540c\u4e13\u4e1a\u5316\u7684\u670d\u52a1\u5668\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "MCP\u670d\u52a1\u5668\u591a\u8def\u590d\u7528\u662f\u5b9e\u73b0\u4ee3\u7406\u4e13\u4e1a\u5316\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u591a\u670d\u52a1\u5668\u8bbf\u95ee\u673a\u5236\uff0c\u63d0\u5347\u4e86\u4ee3\u7406\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2512.67f61805", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2002017859443233017.html%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/QfzAKoiXFhZQYYsQJNHA0soeCaxkMSEWhy-LFMKjo7U=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2002017859443233017.html%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/QfzAKoiXFhZQYYsQJNHA0soeCaxkMSEWhy-LFMKjo7U=436", "authors": ["TLDR Newsletter"], "title": "How can Flash beat Pro", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2002017859443233017.html%3Futm_source=tldrai/1/0100019b466e0c2d-a5ea44ed-9401-4a24-8e01-f00a8e2beee4-000000/QfzAKoiXFhZQYYsQJNHA0soeCaxkMSEWhy-LFMKjo7U=436", "summary": "How can Flash beat Pro (1 minute read) A lot of research progress on agentic reinforcement learning made its way into Gemini 3 Flash, but it was too late for Pro.", "source": "tldr", "AI": {"tldr": "Flash\u901a\u8fc7\u96c6\u6210\u6700\u65b0\u7684\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u8d85\u8d8a\u4e86Pro\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u6280\u672f\u5bf9Pro\u6765\u8bf4\u4e3a\u65f6\u5df2\u665a", "motivation": "\u63a2\u8ba8\u4e3a\u4ec0\u4e48Flash\u80fd\u591f\u8d85\u8d8aPro\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u96c6\u6210\u65b9\u9762\u7684\u65f6\u673a\u5dee\u5f02", "method": "\u5206\u6790Flash\u548cPro\u5728\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u91c7\u7528\u65f6\u95f4\u7ebf\u4e0a\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd", "result": "Flash\u6210\u529f\u96c6\u6210\u4e86\u6700\u65b0\u7684\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u4ece\u800c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86Pro", "conclusion": "\u6280\u672f\u96c6\u6210\u65f6\u673a\u5bf9AI\u6a21\u578b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0cFlash\u56e0\u53ca\u65f6\u91c7\u7528\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\u800c\u83b7\u5f97\u4f18\u52bf", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.5be4558b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyou.com%2Flanding%2Fthe-evolution-of-agent-management%3Futm_campaign=32365635-TLDR_Product_Q4%26utm_source=external-newsletter%26utm_medium=email%26utm_term=tldrproduct_secondary_1223%26utm_content=tldrproduct_secondary_1223/1/0100019b4ae48d80-c28ae51f-cd32-49af-b1e4-ec566b562bc8-000000/2jLSTGewVQxjOjE2B7t4Z75ZxG-hXhxt0gYijhf_y7k=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyou.com%2Flanding%2Fthe-evolution-of-agent-management%3Futm_campaign=32365635-TLDR_Product_Q4%26utm_source=external-newsletter%26utm_medium=email%26utm_term=tldrproduct_secondary_1223%26utm_content=tldrproduct_secondary_1223/1/0100019b4ae48d80-c28ae51f-cd32-49af-b1e4-ec566b562bc8-000000/2jLSTGewVQxjOjE2B7t4Z75ZxG-hXhxt0gYijhf_y7k=436", "authors": ["TLDR Newsletter"], "title": "Prompts can only get you so far", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyou.com%2Flanding%2Fthe-evolution-of-agent-management%3Futm_campaign=32365635-TLDR_Product_Q4%26utm_source=external-newsletter%26utm_medium=email%26utm_term=tldrproduct_secondary_1223%26utm_content=tldrproduct_secondary_1223/1/0100019b4ae48d80-c28ae51f-cd32-49af-b1e4-ec566b562bc8-000000/2jLSTGewVQxjOjE2B7t4Z75ZxG-hXhxt0gYijhf_y7k=436", "summary": "Prompts can only get you so far (Sponsor) Most companies get stuck tinkering with prompts and wonder why their agents fail to deliver dependable results. This guide from You.com breaks down the evolution of agent management, revealing the five stages for building a successful AI agent. Go beyond the prompt: get the playbook.", "source": "tldr", "AI": {"tldr": "\u8be5\u6307\u5357\u4ecb\u7ecd\u4e86AI\u4ee3\u7406\u7ba1\u7406\u7684\u4e94\u4e2a\u53d1\u5c55\u9636\u6bb5\uff0c\u5f3a\u8c03\u4ec5\u9760\u63d0\u793a\u5de5\u7a0b\u65e0\u6cd5\u6784\u5efa\u53ef\u9760\u7684AI\u4ee3\u7406\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u7ba1\u7406\u65b9\u6cd5\u3002", "motivation": "\u8bb8\u591a\u516c\u53f8\u5728\u6784\u5efaAI\u4ee3\u7406\u65f6\u8fc7\u5ea6\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\uff0c\u5bfc\u81f4\u4ee3\u7406\u6027\u80fd\u4e0d\u7a33\u5b9a\u3001\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u66f4\u6210\u719f\u7684\u4ee3\u7406\u7ba1\u7406\u65b9\u6cd5\u8bba\u3002", "method": "\u63d0\u51fa\u4e86AI\u4ee3\u7406\u7ba1\u7406\u7684\u4e94\u4e2a\u53d1\u5c55\u9636\u6bb5\u6846\u67b6\uff0c\u4ece\u57fa\u7840\u7684\u63d0\u793a\u5de5\u7a0b\u5230\u66f4\u7cfb\u7edf\u5316\u7684\u4ee3\u7406\u7ba1\u7406\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u6784\u5efa\u6210\u529fAI\u4ee3\u7406\u7684\u5b9e\u8df5\u6307\u5357\u3002", "result": "\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u4f01\u4e1a\u53ef\u4ee5\u8d85\u8d8a\u7b80\u5355\u7684\u63d0\u793a\u8c03\u4f18\uff0c\u5efa\u7acb\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u63d0\u9ad8\u4ee3\u7406\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u6784\u5efa\u6210\u529f\u7684AI\u4ee3\u7406\u9700\u8981\u8d85\u8d8a\u63d0\u793a\u5de5\u7a0b\uff0c\u91c7\u7528\u7cfb\u7edf\u5316\u7684\u4ee3\u7406\u7ba1\u7406\u65b9\u6cd5\uff0c\u8be5\u6307\u5357\u63d0\u4f9b\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u4e94\u4e2a\u53d1\u5c55\u9636\u6bb5\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.8dfa2b47", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FZAXEscrsebuwref5Z%2Ftwo-notions-of-a-goal-target-states-vs-success-metrics%3Futm_source=tldrproduct/1/0100019b4ae48d80-c28ae51f-cd32-49af-b1e4-ec566b562bc8-000000/LGqysLBHE61ouUukqEdUHeTlLAn8byufG2UlFiacFfo=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FZAXEscrsebuwref5Z%2Ftwo-notions-of-a-goal-target-states-vs-success-metrics%3Futm_source=tldrproduct/1/0100019b4ae48d80-c28ae51f-cd32-49af-b1e4-ec566b562bc8-000000/LGqysLBHE61ouUukqEdUHeTlLAn8byufG2UlFiacFfo=436", "authors": ["TLDR Newsletter"], "title": "Two Notions of a Goal: Target States vs. Success Metrics", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FZAXEscrsebuwref5Z%2Ftwo-notions-of-a-goal-target-states-vs-success-metrics%3Futm_source=tldrproduct/1/0100019b4ae48d80-c28ae51f-cd32-49af-b1e4-ec566b562bc8-000000/LGqysLBHE61ouUukqEdUHeTlLAn8byufG2UlFiacFfo=436", "summary": "Two Notions of a Goal: Target States vs. Success Metrics (6 minute read) Goals can mean either the states an agent aims for or the metrics used to judge success. Separating these ideas clarifies key AI alignment issues, especially how agents learn goals and why misalignment happens.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u533a\u5206\u4e86\u76ee\u6807\u7684\u4e24\u79cd\u6982\u5ff5\uff1a\u76ee\u6807\u72b6\u6001\uff08agent\u8ffd\u6c42\u7684\u5177\u4f53\u72b6\u6001\uff09\u4e0e\u6210\u529f\u5ea6\u91cf\uff08\u8bc4\u4f30\u6210\u529f\u7684\u6807\u51c6\uff09\uff0c\u8fd9\u79cd\u533a\u5206\u6709\u52a9\u4e8e\u6f84\u6e05AI\u5bf9\u9f50\u4e2d\u7684\u5173\u952e\u95ee\u9898", "motivation": "\u5f53\u524dAI\u5bf9\u9f50\u8ba8\u8bba\u4e2d\uff0c\"\u76ee\u6807\"\u6982\u5ff5\u5b58\u5728\u6b67\u4e49\uff0c\u5bfc\u81f4\u5bf9agent\u5982\u4f55\u5b66\u4e60\u76ee\u6807\u4ee5\u53ca\u4e3a\u4f55\u51fa\u73b0\u9519\u4f4d\u7b49\u95ee\u9898\u7406\u89e3\u4e0d\u6e05\u3002\u533a\u5206\u8fd9\u4e24\u79cd\u76ee\u6807\u6982\u5ff5\u6709\u52a9\u4e8e\u66f4\u7cbe\u786e\u5730\u5206\u6790\u5bf9\u9f50\u95ee\u9898", "method": "\u901a\u8fc7\u6982\u5ff5\u5206\u6790\u533a\u5206\u4e24\u79cd\u76ee\u6807\u6982\u5ff5\uff1a\u76ee\u6807\u72b6\u6001\uff08agent\u8bd5\u56fe\u8fbe\u5230\u7684\u5177\u4f53\u72b6\u6001\uff09\u4e0e\u6210\u529f\u5ea6\u91cf\uff08\u7528\u4e8e\u8bc4\u4f30agent\u8868\u73b0\u7684\u5916\u90e8\u6807\u51c6\uff09\u3002\u5206\u6790\u8fd9\u4e24\u79cd\u6982\u5ff5\u5728AI\u5bf9\u9f50\u4e2d\u7684\u4e0d\u540c\u4f5c\u7528\u548c\u76f8\u4e92\u5173\u7cfb", "result": "\u660e\u786e\u533a\u5206\u4e24\u79cd\u76ee\u6807\u6982\u5ff5\u6709\u52a9\u4e8e\u6f84\u6e05\uff1a1\uff09agent\u5982\u4f55\u5b66\u4e60\u76ee\u6807 2\uff09\u4e3a\u4f55\u4f1a\u51fa\u73b0\u9519\u4f4d 3\uff09\u5982\u4f55\u8bbe\u8ba1\u66f4\u597d\u7684\u5bf9\u9f50\u673a\u5236\u3002\u8fd9\u79cd\u533a\u5206\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5206\u6790\u6846\u67b6", "conclusion": "\u533a\u5206\u76ee\u6807\u72b6\u6001\u4e0e\u6210\u529f\u5ea6\u91cf\u662f\u7406\u89e3AI\u5bf9\u9f50\u95ee\u9898\u7684\u5173\u952e\uff0c\u8fd9\u79cd\u6982\u5ff5\u6f84\u6e05\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u5bf9\u9f50\u673a\u5236\u548c\u907f\u514d\u5e38\u89c1\u7684\u9519\u4f4d\u9677\u9631", "topic": "agent analysis"}}
{"id": "tldr.2512.70ffbace", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavegriffith.substack.com%2Fp%2Fclaude-code-sees-like-a-software%3Futm_source=tldrnewsletter/1/0100019b4af34c38-387c2a2b-1bd9-404a-ab9b-37772c7f8978-000000/xfPv3ofFw_dvwPF0ukj_oXNqJ3yWXhrN_dKCLCLE3sY=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavegriffith.substack.com%2Fp%2Fclaude-code-sees-like-a-software%3Futm_source=tldrnewsletter/1/0100019b4af34c38-387c2a2b-1bd9-404a-ab9b-37772c7f8978-000000/xfPv3ofFw_dvwPF0ukj_oXNqJ3yWXhrN_dKCLCLE3sY=437", "authors": ["TLDR Newsletter"], "title": "Claude Code Sees Like A Software Architect", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavegriffith.substack.com%2Fp%2Fclaude-code-sees-like-a-software%3Futm_source=tldrnewsletter/1/0100019b4af34c38-387c2a2b-1bd9-404a-ab9b-37772c7f8978-000000/xfPv3ofFw_dvwPF0ukj_oXNqJ3yWXhrN_dKCLCLE3sY=437", "summary": "Claude Code Sees Like A Software Architect (10 minute read) Claude Code shipped native Language Server Protocol support last week, enabling the IDE to actually understand code.", "source": "tldr", "AI": {"tldr": "Claude Code\u65b0\u589e\u4e86\u539f\u751f\u8bed\u8a00\u670d\u52a1\u5668\u534f\u8bae\u652f\u6301\uff0c\u4f7fIDE\u80fd\u591f\u771f\u6b63\u7406\u89e3\u4ee3\u7801", "motivation": "\u73b0\u6709\u7684IDE\u5de5\u5177\u5bf9\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u4ee3\u7801\u5206\u6790\u548c\u7406\u89e3\u529f\u80fd\u6765\u63d0\u5347\u5f00\u53d1\u6548\u7387", "method": "\u901a\u8fc7\u5b9e\u73b0\u539f\u751f\u8bed\u8a00\u670d\u52a1\u5668\u534f\u8bae\u652f\u6301\uff0c\u8ba9IDE\u80fd\u591f\u6df1\u5ea6\u7406\u89e3\u4ee3\u7801\u7ed3\u6784\u3001\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587", "result": "Claude Code\u73b0\u5728\u80fd\u591f\u50cf\u8f6f\u4ef6\u67b6\u6784\u5e08\u4e00\u6837\u7406\u89e3\u4ee3\u7801\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4ee3\u7801\u5206\u6790\u3001\u667a\u80fd\u8865\u5168\u548c\u91cd\u6784\u5efa\u8bae", "conclusion": "\u539f\u751fLSP\u652f\u6301\u663e\u8457\u63d0\u5347\u4e86IDE\u7684\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u4f7f\u5f00\u53d1\u5de5\u5177\u66f4\u52a0\u667a\u80fd\u5316", "topic": "swe application"}}
{"id": "tldr.2512.bf5a9843", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.kierangill.xyz%2Foversight-and-guidance%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/Nzko7PXp3_d1hdhLqCsE7M0TfAme9p2uVAKD2f9Id84=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.kierangill.xyz%2Foversight-and-guidance%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/Nzko7PXp3_d1hdhLqCsE7M0TfAme9p2uVAKD2f9Id84=437", "authors": ["TLDR Newsletter"], "title": "Scaling LLMs to larger codebases", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.kierangill.xyz%2Foversight-and-guidance%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/Nzko7PXp3_d1hdhLqCsE7M0TfAme9p2uVAKD2f9Id84=437", "summary": "Scaling LLMs to larger codebases (12 minute read) Scaling LLMs within large codebases requires investments in both guidance and oversight. Guidance focuses on providing LLMs with high-quality context, such as prompt libraries and well-structured, modular codebases, to allow for efficient \"one-shot\" code generation without needing a lot of rework. Oversight refers to human engineers who validate LLM choices, making sure of architectural integrity and aligned solutions.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u6269\u5c55LLMs\u9700\u8981\u6295\u8d44\u4e8e\u6307\u5bfc\uff08\u63d0\u4f9b\u9ad8\u8d28\u91cf\u4e0a\u4e0b\u6587\uff09\u548c\u76d1\u7763\uff08\u4eba\u5de5\u9a8c\u8bc1\uff09\u4e24\u65b9\u9762\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u4e00\u6b21\u6027\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u968f\u7740LLMs\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u5982\u4f55\u5c06\u5176\u6709\u6548\u6269\u5c55\u5230\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u5f53\u524dLLMs\u5728\u5904\u7406\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u65f6\u9762\u4e34\u4e0a\u4e0b\u6587\u8d28\u91cf\u4e0d\u8db3\u548c\u7f3a\u4e4f\u4eba\u5de5\u76d1\u7763\u7684\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u548c\u67b6\u6784\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u53cc\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\uff1a1) \u6307\u5bfc\u65b9\u9762\uff1a\u5efa\u7acb\u63d0\u793a\u5e93\u548c\u7ed3\u6784\u5316\u3001\u6a21\u5757\u5316\u7684\u4ee3\u7801\u5e93\uff0c\u4e3aLLMs\u63d0\u4f9b\u9ad8\u8d28\u91cf\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4e00\u6b21\u6027\u4ee3\u7801\u751f\u6210\uff1b2) \u76d1\u7763\u65b9\u9762\uff1a\u7531\u4eba\u5de5\u5de5\u7a0b\u5e08\u9a8c\u8bc1LLM\u7684\u9009\u62e9\uff0c\u786e\u4fdd\u67b6\u6784\u5b8c\u6574\u6027\u548c\u89e3\u51b3\u65b9\u6848\u7684\u4e00\u81f4\u6027\u3002", "result": "\u901a\u8fc7\u7ed3\u5408\u6307\u5bfc\uff08\u9ad8\u8d28\u91cf\u4e0a\u4e0b\u6587\uff09\u548c\u76d1\u7763\uff08\u4eba\u5de5\u9a8c\u8bc1\uff09\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6269\u5c55LLMs\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u7684\u5e94\u7528\uff0c\u51cf\u5c11\u91cd\u590d\u5de5\u4f5c\uff0c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u67b6\u6784\u7684\u5b8c\u6574\u6027\u548c\u89e3\u51b3\u65b9\u6848\u7684\u534f\u8c03\u6027\u3002", "conclusion": "\u6210\u529f\u5c06LLMs\u6269\u5c55\u5230\u5927\u578b\u4ee3\u7801\u5e93\u9700\u8981\u540c\u65f6\u6295\u8d44\u4e8e\u6307\u5bfc\u673a\u5236\u548c\u76d1\u7763\u673a\u5236\u3002\u9ad8\u8d28\u91cf\u7684\u4e0a\u4e0b\u6587\u548c\u4eba\u5de5\u76d1\u7763\u7684\u7ed3\u5408\u662f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9760\u4ee3\u7801\u751f\u6210\u7684\u5173\u952e\uff0c\u8fd9\u4e3aLLMs\u5728\u4f01\u4e1a\u7ea7\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "tldr.2512.13396bbf", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fread.engineerscodex.com%2Fp%2Feveryone-is-a-staff-engineer-now%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/z9DcZ01NiSu5op7U3sse5qXEJDNyUgU7c7-lcY39jlY=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fread.engineerscodex.com%2Fp%2Feveryone-is-a-staff-engineer-now%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/z9DcZ01NiSu5op7U3sse5qXEJDNyUgU7c7-lcY39jlY=437", "authors": ["TLDR Newsletter"], "title": "Everyone is a Staff Engineer Now", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fread.engineerscodex.com%2Fp%2Feveryone-is-a-staff-engineer-now%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/z9DcZ01NiSu5op7U3sse5qXEJDNyUgU7c7-lcY39jlY=437", "summary": "Everyone is a Staff Engineer Now (8 minute read) AI coding agents have become so good that they have fundamentally transformed software engineering, making code implementation inexpensive. This means engineers are increasingly expected to focus on higher-level skills like architectural judgment, system-level thinking, and managing complex contexts across multiple domains. Devs will need to start adapting their workflows, including planning and steering AI agents and developing habits to maint...", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7a0b\u4ee3\u7406\u5df2\u53d8\u5f97\u975e\u5e38\u5f3a\u5927\uff0c\u4f7f\u5f97\u4ee3\u7801\u5b9e\u73b0\u6210\u672c\u5927\u5e45\u964d\u4f4e\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u8f6c\u5411\u66f4\u9ad8\u5c42\u6b21\u7684\u6280\u80fd\u5982\u67b6\u6784\u5224\u65ad\u3001\u7cfb\u7edf\u7ea7\u601d\u7ef4\u548c\u8de8\u9886\u57df\u590d\u6742\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002", "motivation": "\u968f\u7740AI\u7f16\u7a0b\u4ee3\u7406\u80fd\u529b\u7684\u663e\u8457\u63d0\u5347\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u7684\u57fa\u7840\u5de5\u4f5c\uff08\u4ee3\u7801\u5b9e\u73b0\uff09\u53d8\u5f97\u5ec9\u4ef7\uff0c\u8fd9\u4fc3\u4f7f\u5de5\u7a0b\u5e08\u9700\u8981\u91cd\u65b0\u5b9a\u4f4d\u81ea\u5df1\u7684\u4ef7\u503c\uff0c\u4e13\u6ce8\u4e8eAI\u96be\u4ee5\u66ff\u4ee3\u7684\u9ad8\u5c42\u6b21\u6280\u80fd\u3002", "method": "\u6587\u7ae0\u4e3b\u8981\u57fa\u4e8e\u89c2\u5bdf\u5206\u6790\uff0c\u63d0\u51fa\u5de5\u7a0b\u5e08\u9700\u8981\u9002\u5e94\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u89c4\u5212\u548c\u6307\u5bfcAI\u4ee3\u7406\uff0c\u4ee5\u53ca\u57f9\u517b\u7ef4\u62a4\u590d\u6742\u7cfb\u7edf\u7684\u4e60\u60ef\u3002", "result": "AI\u7f16\u7a0b\u4ee3\u7406\u5df2\u7ecf\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u4f7f\u5f97\u4ee3\u7801\u5b9e\u73b0\u53d8\u5f97\u5ec9\u4ef7\uff0c\u5de5\u7a0b\u5e08\u7684\u89d2\u8272\u6b63\u5728\u5411\"\u5168\u5458\u67b6\u6784\u5e08\"\u7684\u65b9\u5411\u8f6c\u53d8\u3002", "conclusion": "\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u9700\u8981\u9002\u5e94AI\u65f6\u4ee3\u7684\u65b0\u8981\u6c42\uff0c\u53d1\u5c55\u67b6\u6784\u5224\u65ad\u3001\u7cfb\u7edf\u601d\u7ef4\u548c\u8de8\u9886\u57df\u7ba1\u7406\u7b49\u9ad8\u7ea7\u6280\u80fd\uff0c\u4ee5\u4fdd\u6301\u804c\u4e1a\u7ade\u4e89\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2512.26960eb0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F12%2F22%2Fa-year-of-vibes%2F%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/iwzzDcq49FZrdrXyGtTVZOv9TrQpNc-5cZekeMudzHA=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F12%2F22%2Fa-year-of-vibes%2F%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/iwzzDcq49FZrdrXyGtTVZOv9TrQpNc-5cZekeMudzHA=437", "authors": ["TLDR Newsletter"], "title": "A Year Of Vibes", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F12%2F22%2Fa-year-of-vibes%2F%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/iwzzDcq49FZrdrXyGtTVZOv9TrQpNc-5cZekeMudzHA=437", "summary": "A Year Of Vibes (12 minute read) In 2025, this dev left Sentry, launched a new company, and shifted his programming approach to embrace hands-off agentic coding with tools like Claude Code. He became deeply integrated with AI agents for tasks from code generation to daily organization. However, he's sometimes worried about emergent human-like tendencies of LLMs, questioning terms like \"agent.\u201d", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u8005\u57282025\u5e74\u79bb\u5f00Sentry\u540e\u521b\u7acb\u65b0\u516c\u53f8\uff0c\u5168\u9762\u8f6c\u5411\u4f7f\u7528Claude Code\u7b49AI\u4ee3\u7406\u8fdb\u884c\u7f16\u7a0b\uff0c\u4ece\u4ee3\u7801\u751f\u6210\u5230\u65e5\u5e38\u7ec4\u7ec7\u90fd\u6df1\u5ea6\u96c6\u6210AI\u52a9\u624b\uff0c\u4f46\u540c\u65f6\u4e5f\u62c5\u5fe7LLMs\u8868\u73b0\u51fa\u7c7b\u4eba\u503e\u5411\u53ca\"\u4ee3\u7406\"\u8fd9\u4e00\u672f\u8bed\u7684\u6070\u5f53\u6027\u3002", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u7684\u6df1\u5ea6\u96c6\u6210\u5e94\u7528\uff0c\u9a8c\u8bc1\"\u653e\u624b\u5f0f\"\u4ee3\u7406\u7f16\u7a0b\u7684\u53ef\u884c\u6027\u548c\u6548\u679c\uff0c\u540c\u65f6\u5173\u6ce8LLMs\u5728\u4ee3\u7406\u89d2\u8272\u4e2d\u8868\u73b0\u51fa\u7684\u7c7b\u4eba\u884c\u4e3a\u7279\u5f81\u3002", "method": "\u91c7\u7528\u5b9e\u8df5\u5bfc\u5411\u7684\u65b9\u6cd5\uff1a\u79bb\u5f00\u539f\u6709\u516c\u53f8\u521b\u7acb\u65b0\u4f01\u4e1a\uff0c\u5168\u9762\u8f6c\u5411\u4f7f\u7528Claude Code\u7b49AI\u4ee3\u7406\u5de5\u5177\u8fdb\u884c\u7f16\u7a0b\u5f00\u53d1\uff0c\u5c06AI\u6df1\u5ea6\u96c6\u6210\u5230\u4ece\u4ee3\u7801\u751f\u6210\u5230\u65e5\u5e38\u7ec4\u7ec7\u7684\u5404\u4e2a\u4efb\u52a1\u73af\u8282\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\"\u653e\u624b\u5f0f\"\u4ee3\u7406\u7f16\u7a0b\u5de5\u4f5c\u6d41\uff0cAI\u4ee3\u7406\u80fd\u591f\u6709\u6548\u5904\u7406\u4ece\u4ee3\u7801\u751f\u6210\u5230\u65e5\u5e38\u7ec4\u7ec7\u7684\u591a\u79cd\u4efb\u52a1\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230LLMs\u8868\u73b0\u51fa\u4ee4\u4eba\u62c5\u5fe7\u7684\u7c7b\u4eba\u503e\u5411\u548c\u81ea\u4e3b\u884c\u4e3a\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u6539\u53d8\u7f16\u7a0b\u5de5\u4f5c\u65b9\u5f0f\uff0c\u4f46\u9700\u8981\u8c28\u614e\u5bf9\u5f85LLMs\u8868\u73b0\u51fa\u7684\u7c7b\u4eba\u7279\u5f81\u548c\u81ea\u4e3b\u884c\u4e3a\uff0c\u5bf9\"\u4ee3\u7406\"\u8fd9\u4e00\u672f\u8bed\u7684\u6070\u5f53\u6027\u63d0\u51fa\u8d28\u7591\u3002", "topic": "code agent"}}
