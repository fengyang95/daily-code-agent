{"id": "2601.16984", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.16984", "abs": "https://arxiv.org/abs/2601.16984", "authors": ["Rahul Ghosh", "Chun-Hao Liu", "Gaurav Rele", "Vidya Sagar Ravipati", "Hazar Aouad"], "title": "TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation", "comment": "Accepted to IJCNLP-AACL 2025", "summary": "The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\\%$ recall, $83\\%$ claim recall, and $92\\%$ faithfulness, representing a $16\\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.", "AI": {"tldr": "TelcoAI\u662f\u4e00\u4e2a\u9762\u54113GPP\u6280\u672f\u6587\u6863\u7684\u591a\u6a21\u6001RAG\u7cfb\u7edf\uff0c\u901a\u8fc7\u667a\u80fd\u5206\u5757\u3001\u67e5\u8be2\u89c4\u5212\u548c\u591a\u6a21\u6001\u878d\u5408\uff0c\u5728\u590d\u6742\u6280\u672f\u6587\u6863\u7406\u89e3\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534716%\u6027\u80fd", "motivation": "3GPP\u6280\u672f\u89c4\u8303\u7ed3\u6784\u590d\u6742\u3001\u683c\u5f0f\u5bc6\u96c6\u4e14\u5305\u542b\u591a\u6a21\u6001\u5185\u5bb9\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u67e5\u8be2\u3001\u89c6\u89c9\u4fe1\u606f\u548c\u6587\u6863\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faTelcoAI\u7cfb\u7edf\uff0c\u91c7\u7528\u57fa\u4e8e\u7ae0\u8282\u611f\u77e5\u7684\u5206\u5757\u3001\u7ed3\u6784\u5316\u67e5\u8be2\u89c4\u5212\u3001\u5143\u6570\u636e\u5f15\u5bfc\u68c0\u7d22\u4ee5\u53ca\u6587\u672c\u4e0e\u56fe\u8868\u7684\u8de8\u6a21\u6001\u878d\u5408\u6280\u672f", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u4e13\u5bb6\u7b56\u5212\u67e5\u8be2\uff09\u4e0a\u8fbe\u523087%\u53ec\u56de\u7387\u300183%\u58f0\u660e\u53ec\u56de\u7387\u548c92%\u5fe0\u5b9e\u5ea6\uff0c\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u534716%", "conclusion": "\u8bc1\u660e\u4e86\u667a\u80fd\u4ee3\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u5728\u6280\u672f\u6587\u6863\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u9645\u7535\u4fe1\u7814\u7a76\u548c\u5de5\u7a0b\u63d0\u4f9b\u4e86\u5148\u8fdb\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2601.17406", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17406", "abs": "https://arxiv.org/abs/2601.17406", "authors": ["Taher A. Ghaleb"], "title": "Fingerprinting AI Coding Agents on GitHub", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI coding agents are reshaping software development through both autonomous and human-mediated pull requests (PRs). When developers use AI agents to generate code under their own accounts, code authorship attribution becomes critical for repository governance, research validity, and understanding modern development practices. We present the first study on fingerprinting AI coding agents, analyzing 33,580 PRs from five major agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code) to identify behavioral signatures. With 41 features spanning commit messages, PR structure, and code characteristics, we achieve 97.2% F1-score in multi-class agent identification. We uncover distinct fingerprints: Codex shows unique multiline commit patterns (67.5% feature importance), and Claude Code exhibits distinctive code structure (27.2% importance of conditional statements). These signatures reveal that AI coding tools produce detectable behavioral patterns, suggesting potential for identifying AI contributions in software repositories.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf9AI\u7f16\u7a0b\u4ee3\u7406\u8fdb\u884c\u6307\u7eb9\u8bc6\u522b\uff0c\u901a\u8fc7\u5206\u679033,580\u4e2aPR\uff0c\u4f7f\u752841\u4e2a\u7279\u5f81\u5b9e\u73b0\u4e8697.2%\u7684F1\u5206\u6570\u6765\u8bc6\u522b\u4e0d\u540cAI\u4ee3\u7406\uff0c\u63ed\u793a\u4e86\u5404\u4ee3\u7406\u7684\u72ec\u7279\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740AI\u7f16\u7a0b\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5f53\u5f00\u53d1\u8005\u4f7f\u7528AI\u4ee3\u7406\u751f\u6210\u4ee3\u7801\u65f6\uff0c\u4ee3\u7801\u4f5c\u8005\u5f52\u5c5e\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u5173\u7cfb\u5230\u4ed3\u5e93\u6cbb\u7406\u3001\u7814\u7a76\u6709\u6548\u6027\u4ee5\u53ca\u5bf9\u73b0\u4ee3\u5f00\u53d1\u5b9e\u8df5\u7684\u7406\u89e3\u3002", "method": "\u5206\u6790\u4e86\u6765\u81ea\u4e94\u4e2a\u4e3b\u8981AI\u4ee3\u7406\uff08OpenAI Codex\u3001GitHub Copilot\u3001Devin\u3001Cursor\u3001Claude Code\uff09\u768433,580\u4e2aPR\uff0c\u63d0\u53d6\u4e8641\u4e2a\u7279\u5f81\uff0c\u6db5\u76d6\u63d0\u4ea4\u4fe1\u606f\u3001PR\u7ed3\u6784\u548c\u4ee3\u7801\u7279\u6027\uff0c\u7528\u4e8e\u8bc6\u522bAI\u4ee3\u7406\u7684\u884c\u4e3a\u7279\u5f81\u3002", "result": "\u5b9e\u73b0\u4e8697.2%\u7684F1\u5206\u6570\u5728\u591a\u7c7b\u522b\u4ee3\u7406\u8bc6\u522b\u4e2d\uff0c\u53d1\u73b0\u4e86\u5404\u4ee3\u7406\u7684\u72ec\u7279\u6307\u7eb9\uff1aCodex\u663e\u793a\u72ec\u7279\u7684\u591a\u884c\u63d0\u4ea4\u6a21\u5f0f\uff0867.5%\u7279\u5f81\u91cd\u8981\u6027\uff09\uff0cClaude Code\u8868\u73b0\u51fa\u72ec\u7279\u7684\u4ee3\u7801\u7ed3\u6784\uff08\u6761\u4ef6\u8bed\u53e5\u536027.2%\u91cd\u8981\u6027\uff09\u3002", "conclusion": "AI\u7f16\u7a0b\u5de5\u5177\u4f1a\u4ea7\u751f\u53ef\u68c0\u6d4b\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8fd9\u8868\u660e\u5728\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u8bc6\u522bAI\u8d21\u732e\u5177\u6709\u6f5c\u5728\u53ef\u80fd\u6027\uff0c\u4e3a\u7406\u89e3AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u89d2\u8272\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2601.17168", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17168", "abs": "https://arxiv.org/abs/2601.17168", "authors": ["Judy Zhu", "Dhari Gandhi", "Himanshu Joshi", "Ahmad Rezaie Mianroodi", "Sedef Akinli Kocak", "Dhanesh Ramachandran"], "title": "Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability", "comment": null, "summary": "Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u4ee5\u786e\u4fddAI\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u3002", "motivation": "\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u67b6\u6784\u548c\u90e8\u7f72\u4e0a\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u5f15\u5165\u4e86\u72ec\u7279\u7684\u5b89\u5168\u6311\u6218\uff08\u5982\u76ee\u6807\u9519\u4f4d\u3001\u51b3\u7b56\u9519\u8bef\u7d2f\u79ef\u3001\u534f\u8c03\u98ce\u9669\uff09\uff0c\u9700\u8981\u4e13\u95e8\u7684\u53ef\u89e3\u91ca\u6027\u8bbe\u8ba1\u6765\u786e\u4fdd\u81ea\u4e3b\u884c\u4e3a\u7684\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u95ee\u8d23\u6027\u3002", "method": "\u8bc4\u4f30\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u548c\u5c40\u9650\u6027\uff0c\u8bc6\u522b\u5176\u5728\u63d0\u4f9b\u667a\u80fd\u4f53\u51b3\u7b56\u6d1e\u5bdf\u65b9\u9762\u7684\u80fd\u529b\u5dee\u8ddd\uff0c\u63d0\u51fa\u672a\u6765\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u5f00\u53d1\u53ef\u89e3\u91ca\u6027\u6280\u672f\u7684\u65b9\u5411\u3002", "result": "\u73b0\u6709\u4e3b\u8981\u4e3a\u9759\u6001\u6a21\u578b\u5f00\u53d1\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5728\u5e94\u7528\u4e8e\u667a\u80fd\u4f53\u7cfb\u7edf\u65f6\u663e\u793a\u51fa\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u5904\u7406\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u65f6\u95f4\u52a8\u6001\u6027\u3001\u590d\u5408\u51b3\u7b56\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u884c\u4e3a\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u65b0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5728\u667a\u80fd\u4f53\u751f\u547d\u5468\u671f\u7684\u5404\u4e2a\u9636\u6bb5\uff08\u76ee\u6807\u5f62\u6210\u3001\u73af\u5883\u4ea4\u4e92\u3001\u7ed3\u679c\u8bc4\u4f30\uff09\u5d4c\u5165\u76d1\u7763\u673a\u5236\uff0c\u8fd9\u5bf9\u4e8e\u786e\u4fdd\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5b89\u5168\u548c\u53ef\u95ee\u8d23\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2601.17152", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17152", "abs": "https://arxiv.org/abs/2601.17152", "authors": ["Miao Zhang", "Junsik Kim", "Siyuan Xiang", "Jian Gao", "Cheng Cao"], "title": "Dynamic Role Assignment for Multi-Agent Debate", "comment": null, "summary": "Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u89d2\u8272\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u8fa9\u8bba\u9009\u62e9\u6700\u9002\u5408\u7684\u6a21\u578b\u62c5\u4efb\u7279\u5b9a\u89d2\u8272\uff0c\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u4e2d\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53LLM/VLM\u8fa9\u8bba\u7cfb\u7edf\u867d\u7136\u4f7f\u7528\u4e13\u95e8\u89d2\u8272\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u4f46\u6ca1\u6709\u6839\u636e\u6a21\u578b\u7279\u957f\u6765\u5206\u914d\u89d2\u8272\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u672a\u80fd\u6700\u5927\u5316\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u89d2\u8272\u5206\u914d\u6846\u67b6\uff0c\u5305\u542b\u5143\u8fa9\u8bba\u7684\u4e24\u4e2a\u9636\u6bb5\uff1a1)\u63d0\u6848\u9636\u6bb5\uff0c\u5019\u9009\u6a21\u578b\u63d0\u4f9b\u89d2\u8272\u5b9a\u5236\u5316\u8bba\u8bc1\uff1b2)\u540c\u884c\u8bc4\u5ba1\u9636\u6bb5\uff0c\u6839\u636e\u6570\u636e\u548c\u89d2\u8272\u7279\u5b9a\u6807\u51c6\u8bc4\u5206\uff0c\u9009\u62e9\u6700\u9002\u5408\u6bcf\u4e2a\u89d2\u8272\u7684\u6a21\u578b\u3002", "result": "\u5728LLM\u95ee\u9898\u89e3\u51b3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u7edf\u4e00\u5206\u914d\uff08\u6240\u6709\u89d2\u8272\u7528\u540c\u4e00\u6a21\u578b\uff09\u63d0\u5347\u9ad8\u8fbe74.8%\uff0c\u76f8\u6bd4\u968f\u673a\u5206\u914d\u63d0\u5347\u9ad8\u8fbe29.7%\uff0c\u6027\u80fd\u63d0\u5347\u53d6\u51b3\u4e8e\u5177\u4f53\u4efb\u52a1\u548c\u5206\u914d\u65b9\u5f0f\u3002", "conclusion": "\u5efa\u7acb\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u65b0\u8303\u5f0f\uff0c\u4ece\u9759\u6001\u90e8\u7f72\u8f6c\u5411\u52a8\u6001\u3001\u80fd\u529b\u611f\u77e5\u7684\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fa9\u8bba\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.17413", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17413", "abs": "https://arxiv.org/abs/2601.17413", "authors": ["Taher A. Ghaleb"], "title": "When AI Agents Touch CI/CD Configurations: Frequency and Success", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI agents are increasingly used in software development, yet their interaction with CI/CD configurations is not well studied. We analyze 8,031 agentic pull requests (PRs) from 1,605 GitHub repositories where AI agents touch YAML configurations. CI/CD configuration files account for 3.25% of agent changes, varying by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001). When agents modify CI/CD, 96.77% target GitHub Actions. Agentic PRs with CI/CD changes merge slightly less often than others (67.77% vs. 71.80%), except for Copilot, whose CI/CD changes merge 15.63 percentage points more often. Across 99,930 workflow runs, build success rates are comparable for CI/CD and non-CI/CD changes (75.59% vs. 74.87%), though three agents show significantly higher success when modifying CI/CD. These results show that AI agents rarely modify CI/CD and focus mostly on GitHub Actions, yet their configuration changes are as reliable as regular code. Copilot's strong CI/CD performance despite lower acceptance suggests emerging configuration specialization, with implications for agent training and DevOps automation.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86AI\u4ee3\u7406\u5728GitHub\u4ed3\u5e93\u4e2d\u5bf9CI/CD\u914d\u7f6e\u6587\u4ef6\u7684\u4fee\u6539\u884c\u4e3a\uff0c\u53d1\u73b0CI/CD\u914d\u7f6e\u4ec5\u5360\u4ee3\u7406\u4fee\u6539\u76843.25%\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728GitHub Actions\uff0c\u4e14\u4fee\u6539\u7684\u53ef\u9760\u6027\u4e0e\u4f20\u7edf\u4ee3\u7801\u76f8\u5f53\uff0cCopilot\u5728CI/CD\u4fee\u6539\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5b83\u4eec\u4e0eCI/CD\u914d\u7f6e\u7684\u4ea4\u4e92\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u9700\u8981\u4e86\u89e3AI\u4ee3\u7406\u5982\u4f55\u4fee\u6539CI/CD\u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u4fee\u6539\u7684\u8d28\u91cf\u548c\u5f71\u54cd\u3002", "method": "\u5206\u6790\u4e868,031\u4e2a\u6765\u81ea1,605\u4e2aGitHub\u4ed3\u5e93\u7684AI\u4ee3\u7406PR\uff0c\u5176\u4e2d\u6d89\u53caYAML\u914d\u7f6e\u6587\u4ef6\u7684\u4fee\u6539\u3002\u7814\u7a76\u4e86\u4e0d\u540c\u4ee3\u7406\uff08Devin\u3001Codex\u3001Copilot\u7b49\uff09\u5bf9CI/CD\u914d\u7f6e\u7684\u4fee\u6539\u6a21\u5f0f\uff0c\u5e76\u8bc4\u4f30\u4e8699,930\u4e2a\u5de5\u4f5c\u6d41\u8fd0\u884c\u7684\u6210\u529f\u7387\u3002", "result": "CI/CD\u914d\u7f6e\u6587\u4ef6\u5360AI\u4ee3\u7406\u4fee\u6539\u76843.25%\uff0c\u4e0d\u540c\u4ee3\u7406\u95f4\u6709\u663e\u8457\u5dee\u5f02\uff08Devin: 4.83%, Codex: 2.01%\uff09\u300296.77%\u7684CI/CD\u4fee\u6539\u9488\u5bf9GitHub Actions\u3002\u5305\u542bCI/CD\u4fee\u6539\u7684PR\u5408\u5e76\u7387\u7565\u4f4e\uff0867.77% vs 71.80%\uff09\uff0c\u4f46Copilot\u7684CI/CD\u4fee\u6539\u5408\u5e76\u7387\u9ad8\u51fa15.63\u4e2a\u767e\u5206\u70b9\u3002\u5de5\u4f5c\u6d41\u8fd0\u884c\u6210\u529f\u7387\u5728CI/CD\u548c\u975eCI/CD\u4fee\u6539\u95f4\u76f8\u5f53\uff0875.59% vs 74.87%\uff09\uff0c\u4e09\u4e2a\u4ee3\u7406\u5728\u4fee\u6539CI/CD\u65f6\u6210\u529f\u7387\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "AI\u4ee3\u7406\u5f88\u5c11\u4fee\u6539CI/CD\u914d\u7f6e\uff0c\u4e3b\u8981\u5173\u6ce8GitHub Actions\uff0c\u4f46\u5176\u914d\u7f6e\u4fee\u6539\u4e0e\u5e38\u89c4\u4ee3\u7801\u4e00\u6837\u53ef\u9760\u3002Copilot\u5728CI/CD\u65b9\u9762\u7684\u5f3a\u52b2\u8868\u73b0\u8868\u660e\u914d\u7f6e\u4e13\u4e1a\u5316\u8d8b\u52bf\uff0c\u8fd9\u5bf9\u4ee3\u7406\u8bad\u7ec3\u548cDevOps\u81ea\u52a8\u5316\u6709\u91cd\u8981\u542f\u793a\u3002", "topic": "swe application"}}
{"id": "2601.17435", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17435", "abs": "https://arxiv.org/abs/2601.17435", "authors": ["Maria Jesus Rodriguez-Sanchez", "Manuel Noguera", "Angel Ruiz-Zafra", "Kawtar Benghazi"], "title": "Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems", "comment": "12 pages, 3 figures", "summary": "Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.", "AI": {"tldr": "\u63d0\u51faDALIA\uff08\u58f0\u660e\u5f0f\u667a\u80fd\u4f53\u5c42\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u80fd\u529b\u5b9a\u4e49\u3001\u4efb\u52a1\u53d1\u73b0\u534f\u8bae\u548c\u786e\u5b9a\u6027\u4efb\u52a1\u56fe\uff0c\u89e3\u51b3\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u5de5\u4f5c\u6d41\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5982\u5e7b\u89c9\u52a8\u4f5c\u3001\u4e0d\u53ef\u6267\u884c\u8ba1\u5212\u548c\u8106\u5f31\u534f\u8c03\u3002\u8fd9\u4e9b\u95ee\u9898\u5e76\u975e\u6e90\u4e8e\u5e95\u5c42\u6a21\u578b\u9650\u5236\uff0c\u800c\u662f\u7f3a\u4e4f\u660e\u786e\u8fde\u63a5\u76ee\u6807\u3001\u80fd\u529b\u548c\u6267\u884c\u7684\u67b6\u6784\u7ed3\u6784\u3002", "method": "\u63d0\u51faDALIA\u58f0\u660e\u5f0f\u67b6\u6784\u5c42\uff0c\u5305\u542b\uff1a\u5f62\u5f0f\u5316\u53ef\u6267\u884c\u80fd\u529b\u3001\u901a\u8fc7\u58f0\u660e\u5f0f\u53d1\u73b0\u534f\u8bae\u66b4\u9732\u4efb\u52a1\u3001\u7ef4\u62a4\u4ee3\u7406\u53ca\u5176\u6267\u884c\u8d44\u6e90\u7684\u8054\u90a6\u76ee\u5f55\u3001\u6784\u5efa\u57fa\u4e8e\u58f0\u660e\u64cd\u4f5c\u7684\u786e\u5b9a\u6027\u4efb\u52a1\u56fe\u3002\u5f3a\u5236\u5206\u79bb\u53d1\u73b0\u3001\u89c4\u5212\u548c\u6267\u884c\u9636\u6bb5\u3002", "result": "\u901a\u8fc7\u4ee3\u8868\u6027\u4efb\u52a1\u573a\u666f\u5c55\u793a\u4e86DALIA\u7684\u64cd\u4f5c\uff0c\u8bc1\u660e\u58f0\u660e\u5f0f\u57fa\u7840\u80fd\u591f\u5b9e\u73b0\u8de8\u5f02\u6784\u73af\u5883\u7684\u53ef\u91cd\u73b0\u548c\u53ef\u9a8c\u8bc1\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5c06\u4ee3\u7406\u884c\u4e3a\u7ea6\u675f\u5728\u53ef\u9a8c\u8bc1\u64cd\u4f5c\u7a7a\u95f4\u3002", "conclusion": "DALIA\u67b6\u6784\u901a\u8fc7\u58f0\u660e\u5f0f\u57fa\u7840\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u9760\u6027\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u63a8\u6d4b\u63a8\u7406\u548c\u81ea\u7531\u5f62\u5f0f\u534f\u8c03\u7684\u4f9d\u8d56\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.17311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17311", "abs": "https://arxiv.org/abs/2601.17311", "authors": ["Bang Liu", "Linglong Kong", "Jian Pei"], "title": "Phase Transition for Budgeted Multi-Agent Synergy", "comment": "55 pages, 12 figures", "summary": "Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $\u03b2$; communication is captured by a message-length fidelity curve $\u03b3(m)$; dependence is captured by an effective shared-error correlation $\u03c1$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $\u03b1_\u03c1$ (combining $\u03b3(m)$, $\u03c1$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>\u03b2$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6821\u51c6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc6\u522b\u51fa\u5e2e\u52a9\u3001\u9971\u548c\u548c\u5d29\u6e83\u4e09\u79cd\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u5206\u6790\u63ed\u793a\u4e86\u901a\u4fe1\u3001\u76f8\u5173\u6027\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u7b49\u7ea6\u675f\u6761\u4ef6\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\uff0c\u5176\u6027\u80fd\u5f80\u5f80\u5448\u73b0\u5e2e\u52a9\u3001\u9971\u548c\u751a\u81f3\u5d29\u6e83\u4e09\u79cd\u72b6\u6001\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u591f\u9884\u6d4b\u8fd9\u4e9b\u72b6\u6001\u5e76\u6307\u5bfc\u7cfb\u7edf\u8bbe\u8ba1\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u662f\u9700\u8981\u8003\u8651\u73b0\u4ee3\u667a\u80fd\u4f53\u6808\u7684\u4e09\u4e2a\u5173\u952e\u7ea6\u675f\uff1a\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u6709\u635f\u7684\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u4ee5\u53ca\u76f8\u4f3c\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5171\u4eab\u6545\u969c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u4e14\u53ef\u6821\u51c6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u53f6\u667a\u80fd\u4f53\u7528\u8ba1\u7b97-\u6027\u80fd\u7f29\u653e\u6307\u6570\u03b2\u63cf\u8ff0\uff0c\u901a\u4fe1\u7528\u6d88\u606f\u957f\u5ea6\u4fdd\u771f\u5ea6\u66f2\u7ebf\u03b3(m)\u63cf\u8ff0\uff0c\u76f8\u5173\u6027\u7528\u6709\u6548\u5171\u4eab\u8bef\u5dee\u76f8\u5173\u6027\u03c1\u63cf\u8ff0\uff0c\u4e0a\u4e0b\u6587\u7a97\u53e3W\u5219\u65bd\u52a0\u4e86\u786c\u6027\u7684\u6247\u5165\u9650\u5236\u3002\u901a\u8fc7\u6570\u5b66\u5206\u6790\uff0c\u7814\u7a76\u4e86\u5177\u6709\u76f8\u5173\u8f93\u5165\u548c\u6709\u635f\u901a\u4fe1\u7684\u6df1\u5ea6b\u53c9\u6811\u4e2d\u7684\u591a\u6570\u805a\u5408\u4efb\u52a1\uff0c\u63a8\u5bfc\u4e86\u76f8\u53d8\u6761\u4ef6\u3001\u7ec4\u7ec7\u6307\u6570s\u548c\u8ba1\u7b97\u5206\u914d\u89c4\u5219\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6df1\u5ea6b\u53c9\u6811\u4e2d\u5b58\u5728\u4e00\u4e2a\u5c16\u9510\u7684\u76f8\u53d8\uff1a\u5355\u4e2a\u6807\u91cf\u03b1_\u03c1\uff08\u7ed3\u5408\u03b3(m)\u3001\u03c1\u548c\u6247\u5165b\uff09\u51b3\u5b9a\u4e86\u5f31\u4fe1\u53f7\u662f\u88ab\u653e\u5927\u5230\u975e\u5e73\u51e1\u56fa\u5b9a\u70b9\u8fd8\u662f\u88ab\u6d17\u724c\u5230\u968f\u673a\u6c34\u5e73\u3002\u5728\u653e\u5927\u72b6\u6001\u4e0b\uff0c\u63a8\u5bfc\u4e86\u7ec4\u7ec7\u6307\u6570s\uff0c\u5e76\u8bc1\u660e\u5f53s>\u03b2\u65f6\u4f1a\u51fa\u73b0\u9884\u7b97\u534f\u540c\u6548\u5e94\uff08\u5373\u5728\u76f8\u540c\u603b\u9884\u7b97\u4e0b\u4f18\u4e8e\u6700\u4f73\u5355\u4e2a\u667a\u80fd\u4f53\uff09\u3002\u901a\u8fc7\u5408\u6210\u6a21\u62df\u9a8c\u8bc1\u4e86\u9884\u6d4b\u7684\u76f8\u8fb9\u754c\uff0c\u5e76\u89e3\u91ca\u4e86\u6700\u8fd1\u5927\u89c4\u6a21\u5339\u914d\u9884\u7b97\u7814\u7a76\u4e2dLLM\u667a\u80fd\u4f53\u7cfb\u7edf\u7f29\u653e\u7684\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u80fd\u591f\u9884\u6d4b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u901a\u4fe1\u3001\u76f8\u5173\u6027\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u7ea6\u675f\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u548c\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.17173", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17173", "abs": "https://arxiv.org/abs/2601.17173", "authors": ["Parth Bhalerao", "Diola Dsouza", "Ruiwen Guan", "Oana Ignat"], "title": "Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content", "comment": null, "summary": "Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u8bed\u8a00\u5bfc\u5e08\u95ee\u7b54\u6570\u636e\u96c6MentorQA\uff0c\u5305\u542b\u8fd19000\u4e2aQA\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u8d85\u8d8a\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u5bfc\u5e08\u5f0f\u56de\u7b54\u8d28\u91cf\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u667a\u80fd\u4f53\u67b6\u6784\u5728\u5bfc\u5e08\u5f0f\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u95ee\u7b54\u7cfb\u7edf\u4e3b\u8981\u8bc4\u4f30\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u6559\u80b2\u3001\u804c\u4e1a\u6307\u5bfc\u7b49\u5b9e\u9645\u5e94\u7528\u9700\u8981\u63d0\u4f9b\u53cd\u601d\u548c\u6307\u5bfc\u7684\u5bfc\u5e08\u5f0f\u56de\u7b54\uff0c\u73b0\u6709\u57fa\u51c6\u5f88\u5c11\u6355\u6349\u8fd9\u79cd\u533a\u522b\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u957f\u6587\u672c\u573a\u666f\u4e2d\u3002", "method": "\u6784\u5efa\u4e86MentorQA\u6570\u636e\u96c6\uff0c\u5305\u542b180\u5c0f\u65f6\u89c6\u9891\u5185\u5bb9\u3001\u8fd19000\u4e2aQA\u5bf9\uff0c\u8986\u76d6\u56db\u79cd\u8bed\u8a00\uff1b\u5b9a\u4e49\u4e86\u8d85\u8d8a\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u5bfc\u5e08\u5f0f\u8bc4\u4f30\u7ef4\u5ea6\uff08\u6e05\u6670\u5ea6\u3001\u4e00\u81f4\u6027\u3001\u5b66\u4e60\u4ef7\u503c\uff09\uff1b\u5728\u63a7\u5236\u6761\u4ef6\u4e0b\u6bd4\u8f83\u4e86\u5355\u667a\u80fd\u4f53\u3001\u53cc\u667a\u80fd\u4f53\u3001RAG\u548c\u591a\u667a\u80fd\u4f53\u95ee\u7b54\u67b6\u6784\u3002", "result": "\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u5728\u5bfc\u5e08\u5f0f\u56de\u7b54\u8d28\u91cf\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u590d\u6742\u4e3b\u9898\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u4f18\u52bf\u660e\u663e\uff1b\u540c\u65f6\u53d1\u73b0\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u5c06\u5bfc\u5e08\u5f0f\u95ee\u7b54\u786e\u7acb\u4e3a\u72ec\u7acb\u7814\u7a76\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u7528\u4e8e\u7814\u7a76\u6559\u80b2AI\u4e2d\u7684\u667a\u80fd\u4f53\u67b6\u6784\u548c\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.17332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17332", "abs": "https://arxiv.org/abs/2601.17332", "authors": ["Yicheng Tao", "Hongteng Xu"], "title": "TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow", "comment": null, "summary": "The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \\textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \\textit{statement formalization}, \\textit{proof generation}, \\textit{premise selection}, \\textit{proof correction} and \\textit{proof sketching}. By implementing a \\textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\\%, surpassing the 8.6\\% baseline, at an average cost of only \\textbf{\\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \\textbf{1.6$\\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \\href{https://github.com/timechess/TheoremForge}{here}.", "AI": {"tldr": "TheoremForge\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u5f62\u5f0f\u5316\u6570\u5b66\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u89e3\u8026\u63d0\u53d6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u751f\u6210\u6548\u7387\u3002", "motivation": "\u5f62\u5f0f\u5316\u6570\u5b66\u4e2d\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u9ad8\u6210\u672c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5408\u6210\uff0c\u5bfc\u81f4\u5f00\u6e90\u8bed\u6599\u5e93\u7a00\u7f3a\uff0c\u9700\u8981\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u5f62\u5f0f\u5316\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e94\u4e2a\u5b50\u4efb\u52a1\uff1a\u9648\u8ff0\u5f62\u5f0f\u5316\u3001\u8bc1\u660e\u751f\u6210\u3001\u524d\u63d0\u9009\u62e9\u3001\u8bc1\u660e\u4fee\u6b63\u548c\u8bc1\u660e\u8349\u56fe\uff1b\u91c7\u7528\u89e3\u8026\u63d0\u53d6\u7b56\u7565\u4ece\u5931\u8d25\u8f68\u8ff9\u4e2d\u6062\u590d\u6709\u6548\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "\u57282000\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTheoremForge\u8fbe\u523012.6%\u7684\u9a8c\u8bc1\u7387\uff08\u4f18\u4e8e8.6%\u7684\u57fa\u7ebf\uff09\uff0c\u6bcf\u4e2a\u6210\u529f\u8f68\u8ff9\u5e73\u5747\u6210\u672c\u4ec50.481\u7f8e\u5143\uff0c\u6570\u636e\u4ea7\u91cf\u6bd4\u6807\u51c6\u8fc7\u6ee4\u65b9\u6cd5\u63d0\u9ad81.6\u500d\u3002", "conclusion": "TheoremForge\u4e3a\u8bad\u7ec3\u672a\u6765\u4e13\u5bb6\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u98de\u8f6e\u6784\u5efa\u6846\u67b6\uff0c\u80fd\u6709\u6548\u964d\u4f4e\u5f62\u5f0f\u5316\u6570\u5b66\u6570\u636e\u5408\u6210\u7684\u6210\u672c\u3002", "topic": "code agent"}}
{"id": "2601.17335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17335", "abs": "https://arxiv.org/abs/2601.17335", "authors": ["Angshul Majumdar"], "title": "The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability", "comment": null, "summary": "We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and G\u00f6del--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u7406\u8bba\u89d2\u5ea6\u5206\u6790AGI\u7684\u5b9a\u4e49\u95ee\u9898\uff0c\u8bc1\u660eAGI\u65e0\u6cd5\u72ec\u7acb\u4e8e\u4efb\u52a1\u5206\u5e03\u800c\u5b58\u5728\uff0c\u7f3a\u4e4f\u666e\u904d\u9c81\u68d2\u6027\uff0c\u6709\u9650\u8d44\u6e90\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u65e0\u9650\u6cdb\u5316\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u8ba1\u7b97\u7a0b\u5e8f\uff08\u5305\u62ec\u81ea\u8ba4\u8bc1\uff09\u5b8c\u5168\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76AGI\u662f\u5426\u5177\u6709\u4e00\u81f4\u7684\u7406\u8bba\u5b9a\u4e49\uff0c\u80fd\u591f\u652f\u6301\u5173\u4e8e\u5b58\u5728\u6027\u3001\u9c81\u68d2\u6027\u6216\u81ea\u9a8c\u8bc1\u7684\u7edd\u5bf9\u4e3b\u5f20\u3002\u63a2\u8ba8\u5f3a\u5206\u5e03\u65e0\u5173\u7684AGI\u4e3b\u5f20\u662f\u5426\u5408\u7406\uff0c\u4ee5\u53caAI\u7684\u5b9e\u8bc1\u8fdb\u5c55\u662f\u5426\u610f\u5473\u7740\u53ef\u5b9e\u73b0\u81ea\u8ba4\u8bc1\u7684\u901a\u7528\u667a\u80fd\u3002", "method": "\u91c7\u7528\u516c\u7406\u5316\u65b9\u6cd5\uff0c\u5c06AGI\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u6027\u3001\u8d44\u6e90\u53d7\u9650\u7684\u8bed\u4e49\u8c13\u8bcd\uff0c\u901a\u8fc7\u4efb\u52a1\u65cf\u3001\u4efb\u52a1\u5206\u5e03\u3001\u6027\u80fd\u51fd\u6570\u548c\u663e\u5f0f\u8d44\u6e90\u9884\u7b97\u8fdb\u884c\u7d22\u5f15\u3002\u4f7f\u7528\u7406\u8bba\u8bc1\u660e\u65b9\u6cd5\uff0c\u5305\u62ecRice\u98ce\u683c\u548c\u54e5\u5fb7\u5c14-\u5854\u65af\u57fa\u8bba\u8bc1\u3002", "result": "1. \u901a\u7528\u6027\u662f\u5173\u7cfb\u6027\u7684\uff0c\u4e0d\u5b58\u5728\u5206\u5e03\u65e0\u5173\u7684AGI\u6982\u5ff5\uff1b2. \u4efb\u610f\u5c0f\u7684\u4efb\u52a1\u5206\u5e03\u6270\u52a8\u53ef\u901a\u8fc7\u60ac\u5d16\u96c6\u4f7fAGI\u5c5e\u6027\u5931\u6548\uff0c\u6392\u9664\u666e\u904d\u9c81\u68d2\u6027\uff1b3. \u6709\u9650\u8d44\u6e90\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u8de8\u4efb\u52a1\u65cf\u7684\u65e0\u9650\u6cdb\u5316\uff1b4. AGI\u662f\u975e\u5e73\u51e1\u7684\u8bed\u4e49\u5c5e\u6027\uff0c\u65e0\u6cd5\u901a\u8fc7\u4efb\u4f55\u53ef\u8ba1\u7b97\u7a0b\u5e8f\uff08\u5305\u62ec\u4ee3\u7406\u81ea\u8eab\uff09\u8fdb\u884c\u53ef\u9760\u4e14\u5b8c\u6574\u7684\u8ba4\u8bc1\u3002", "conclusion": "\u5f3a\u5206\u5e03\u65e0\u5173\u7684AGI\u4e3b\u5f20\u5728\u6ca1\u6709\u663e\u5f0f\u5f62\u5f0f\u7d22\u5f15\u7684\u60c5\u51b5\u4e0b\u662f\u672a\u5b9a\u4e49\u7684\uff0c\u800c\u975e\u9519\u8bef\u7684\u3002AI\u7684\u5b9e\u8bc1\u8fdb\u5c55\u5e76\u4e0d\u610f\u5473\u53ef\u5b9e\u73b0\u81ea\u8ba4\u8bc1\u7684\u901a\u7528\u667a\u80fd\uff0c\u4f9d\u8d56\u5185\u90e8\u81ea\u8ba4\u8bc1\u7684\u9012\u5f52\u81ea\u6211\u6539\u8fdb\u65b9\u6848\u662f\u6709\u95ee\u9898\u7684\u3002", "topic": "agent analysis"}}
{"id": "2601.17581", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17581", "abs": "https://arxiv.org/abs/2601.17581", "authors": ["Daniel Ogenrwot", "John Businge"], "title": "How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests", "comment": "5 pages, 5 figures", "summary": "AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $\u03b4= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.", "AI": {"tldr": "AI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684PR\u5728\u63d0\u4ea4\u6570\u91cf\u3001\u6587\u4ef6\u4fee\u6539\u548c\u5220\u9664\u884c\u6570\u4e0a\u4e0e\u4eba\u7c7bPR\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14PR\u63cf\u8ff0\u4e0e\u4ee3\u7801\u53d8\u66f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u7565\u9ad8", "motivation": "\u7f3a\u4e4f\u5173\u4e8eAI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684PR\u4e0e\u4eba\u7c7b\u8d21\u732e\u5dee\u5f02\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u9700\u8981\u4e86\u89e3\u8fd9\u4e9b\u5dee\u5f02\u4ee5\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u548c\u5bf9\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u5f71\u54cd", "method": "\u4f7f\u7528MSR 2026 Mining Challenge\u7248\u672c\u7684AIDev\u6570\u636e\u96c6\uff0c\u5206\u679024,014\u4e2a\u5408\u5e76\u7684Agentic PR\uff08440,295\u6b21\u63d0\u4ea4\uff09\u548c5,081\u4e2a\u5408\u5e76\u7684Human PR\uff0823,242\u6b21\u63d0\u4ea4\uff09\uff0c\u901a\u8fc7\u8bcd\u6cd5\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc4\u4f30PR\u63cf\u8ff0\u4e0e\u4ee3\u7801\u53d8\u66f4\u7684\u4e00\u81f4\u6027", "result": "Agentic PR\u5728\u63d0\u4ea4\u6570\u91cf\u4e0a\u4e0eHuman PR\u5dee\u5f02\u663e\u8457\uff08Cliff's \u03b4=0.5429\uff09\uff0c\u5728\u6587\u4ef6\u4fee\u6539\u548c\u5220\u9664\u884c\u6570\u4e0a\u5b58\u5728\u4e2d\u7b49\u5dee\u5f02\uff0c\u5728\u6240\u6709\u5ea6\u91cf\u6307\u6807\u4e0aPR\u63cf\u8ff0\u4e0e\u4ee3\u7801\u53d8\u66f4\u7684\u76f8\u4f3c\u5ea6\u7565\u9ad8", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86AI\u7f16\u7801\u4ee3\u7406\u5982\u4f55\u8d21\u732e\u5f00\u6e90\u5f00\u53d1\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u7279\u5f81\uff0c\u63ed\u793a\u4e86AI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5728\u4ee3\u7801\u8d21\u732e\u6a21\u5f0f\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02", "topic": "agent analysis"}}
{"id": "2601.17584", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17584", "abs": "https://arxiv.org/abs/2601.17584", "authors": ["Mahmoud Samir Fayed", "Ahmed Samir Fayed"], "title": "Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language", "comment": null, "summary": "Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u5c55\u793a\u4e86\u4f7f\u7528Claude Code Opus 4.5\u7eaf\u63d0\u793a\u9a71\u52a8\u5de5\u4f5c\u6d41\uff0c\u5728\u7ea610\u5c0f\u65f6\u5185\u5f00\u53d17420\u884cRing\u7f16\u7a0b\u8bed\u8a00\u7ec8\u7aef\u7528\u6237\u754c\u9762\u6846\u67b6\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u73b0\u4ee3LLM\u80fd\u591f\u7ef4\u6301\u67b6\u6784\u4e00\u81f4\u6027\u5e76\u6784\u5efa\u751f\u4ea7\u7ea7\u5de5\u5177\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u751f\u6210\u548c\u7ef4\u62a4\u5927\u578b\u591a\u6a21\u5757\u7cfb\u7edf\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8868\u5f81\u3002\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u8bc1\u8bc4\u4f30LLM\u5728\u6784\u5efa\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\u65b9\u9762\u7684\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u91c7\u7528\u7eaf\u63d0\u793a\u9a71\u52a8\u5de5\u4f5c\u6d41\uff0c\u4f7f\u7528Claude Code Opus 4.5\u5728\u4e09\u5929\u5185\u901a\u8fc7107\u4e2a\u63d0\u793a\u5f00\u53d17420\u884cTerminal User Interface\u6846\u67b6\u3002\u5206\u6790\u5305\u62ec\u5b9a\u91cf\u63d0\u793a\u5206\u7c7b\uff0821\u4e2a\u529f\u80fd\u8bf7\u6c42\u300172\u4e2a\u9519\u8bef\u4fee\u590d\u30019\u4e2a\u6587\u6863\u4fe1\u606f\u5206\u4eab\u30014\u4e2a\u67b6\u6784\u6307\u5bfc\u30011\u4e2a\u6587\u6863\u751f\u6210\uff09\u548c\u4e94\u4e2a\u5f00\u53d1\u9636\u6bb5\u7684\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u5305\u542b\u5b8c\u6574\u7a97\u53e3\u5b50\u7cfb\u7edf\u3001\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u3001\u4ea4\u4e92\u5f0f\u5c0f\u90e8\u4ef6\u3001\u5206\u5c42\u83dc\u5355\u3001\u7f51\u683c\u548c\u6811\u7ec4\u4ef6\u3001\u6807\u7b7e\u63a7\u4ef6\u4ee5\u53ca\u591a\u7a97\u53e3\u684c\u9762\u73af\u5883\u7684\u6846\u67b6\u3002\u9519\u8bef\u4fee\u590d\u4e3b\u8981\u6d89\u53ca\u91cd\u7ed8\u95ee\u9898\u3001\u4e8b\u4ef6\u5904\u7406\u6545\u969c\u3001\u8fd0\u884c\u65f6\u9519\u8bef\u548c\u5e03\u5c40\u4e0d\u4e00\u81f4\uff1b\u529f\u80fd\u8bf7\u6c42\u4e3b\u8981\u5173\u6ce8\u65b0\u5c0f\u90e8\u4ef6\u3001\u7a97\u53e3\u7ba1\u7406\u5668\u529f\u80fd\u548c\u9ad8\u7ea7UI\u7ec4\u4ef6\u3002", "conclusion": "\u73b0\u4ee3LLM\u80fd\u591f\u7ef4\u6301\u67b6\u6784\u4e00\u81f4\u6027\u5e76\u652f\u6301\u4e3a\u65b0\u5174\u7f16\u7a0b\u8bed\u8a00\u6784\u5efa\u751f\u4ea7\u7ea7\u5de5\u5177\uff0c\u63d0\u793a\u9a71\u52a8\u5f00\u53d1\u662f\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u53ef\u884c\u7684\u65b9\u6cd5\u8bba\u3002\u4eba\u7c7b\u89d2\u8272\u4ec5\u9650\u4e8e\u6307\u5b9a\u9700\u6c42\u3001\u9a8c\u8bc1\u884c\u4e3a\u548c\u53d1\u51fa\u7ea0\u6b63\u63d0\u793a\uff0c\u65e0\u9700\u624b\u52a8\u7f16\u5199\u4ee3\u7801\u3002", "topic": "swe application"}}
{"id": "2601.17346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17346", "abs": "https://arxiv.org/abs/2601.17346", "authors": ["Haoxin Xu", "Changyong Qi", "Tong Liu", "Bohao Zhang", "Anna He", "Bingqian Jiang", "Longwei Zheng", "Xiaoqing Gu"], "title": "Multi-Agent Learning Path Planning via LLMs", "comment": null, "summary": "The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u548c\u89c4\u5219\u534f\u4f5c\u673a\u5236\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u8def\u5f84\u63a8\u8350\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u5bfc\u5b66\u7cfb\u7edf\u4e2d\u7684\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u5ea6\u3001\u9002\u5e94\u6027\u548c\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4efb\u52a1\u7279\u5b9a\u667a\u80fd\u4f53\uff1a\u5b66\u4e60\u8005\u5206\u6790\u667a\u80fd\u4f53\u3001\u8def\u5f84\u89c4\u5212\u667a\u80fd\u4f53\u548c\u53cd\u601d\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u9884\u5b9a\u4e49\u89c4\u5219\u534f\u4f5c\uff0c\u57fa\u4e8e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u548c\u6700\u8fd1\u53d1\u5c55\u533a\u7406\u8bba\u786e\u4fdd\u8ba4\u77e5\u5bf9\u9f50\u3002", "result": "\u5728MOOCCubeX\u6570\u636e\u96c6\u4e0a\u4f7f\u75287\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0cMALPP\u5728\u8def\u5f84\u8d28\u91cf\u3001\u77e5\u8bc6\u5e8f\u5217\u4e00\u81f4\u6027\u548c\u8ba4\u77e5\u8d1f\u8377\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u534f\u4f5c\u673a\u5236\u548c\u7406\u8bba\u7ea6\u675f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6559\u80b2\u9886\u57df\u53ef\u4fe1\u3001\u53ef\u89e3\u91caAI\u7684\u53d1\u5c55\u505a\u51fa\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u6269\u5c55\u3001\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u81ea\u9002\u5e94\u6559\u5b66\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.17604", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17604", "abs": "https://arxiv.org/abs/2601.17604", "authors": ["Suborno Deb Bappon", "Saikat Mondal", "Chanchal K. Roy", "Kevin Schneider"], "title": "Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback", "comment": "Preprint", "summary": "Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22LLMs\u5982\u4f55\u5229\u7528Stack Overflow\u4e0a\u7684\u7528\u6237\u8bc4\u8bba\u53cd\u9988\u6765\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\uff0c\u63d0\u51fa\u4e86ReSOlve\u57fa\u51c6\u548cAUTOCOMBAT\u5de5\u5177\uff0c\u80fd\u6709\u6548\u63d0\u5347\u7b54\u6848\u8d28\u91cf\u5e76\u5f97\u5230\u5f00\u53d1\u8005\u8ba4\u53ef\u3002", "motivation": "Stack Overflow\u7b49\u5e73\u53f0\u4e0a\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u7528\u6237\u53cd\u9988\u672a\u88ab\u5904\u7406\uff0c\u5bfc\u81f4\u7b54\u6848\u4e0d\u5b8c\u6574\u6216\u8fc7\u65f6\u3002\u7814\u7a76\u63a2\u7d22LLMs\u662f\u5426\u80fd\u901a\u8fc7\u89e3\u91ca\u548c\u6574\u5408\u8bc4\u8bba\u53cd\u9988\u6765\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\uff0c\u63d0\u9ad8\u6280\u672f\u77e5\u8bc6\u5e73\u53f0\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "1) \u521b\u5efaReSOlve\u57fa\u51c6\uff0c\u5305\u542b790\u4e2aSO\u7b54\u6848\u53ca\u76f8\u5173\u8bc4\u8bba\u7ebf\u7a0b\uff0c\u6807\u6ce8\u6539\u8fdb\u76f8\u5173\u548c\u4e00\u822c\u53cd\u9988\uff1b2) \u8bc4\u4f304\u4e2aSOTA LLMs\u8bc6\u522b\u53ef\u64cd\u4f5c\u95ee\u9898\u7684\u80fd\u529b\uff1b3) \u5f00\u53d1AUTOCOMBAT\u5de5\u5177\uff0c\u8054\u5408\u5229\u7528\u7528\u6237\u8bc4\u8bba\u548c\u95ee\u9898\u4e0a\u4e0b\u6587\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\uff1b4) \u8fdb\u884c58\u540d\u4ece\u4e1a\u8005\u7684\u7528\u6237\u7814\u7a76\u3002", "result": "DeepSeek\u5728\u8bc6\u522b\u53ef\u64cd\u4f5c\u95ee\u9898\u65b9\u9762\u8fbe\u5230\u6700\u4f73\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u5e73\u8861\uff1bAUTOCOMBAT\u4ea7\u751f\u63a5\u8fd1\u4eba\u7c7b\u8d28\u91cf\u7684\u6539\u8fdb\uff0c\u4fdd\u6301\u539f\u59cb\u610f\u56fe\u5e76\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff1b84.5%\u7684\u4ece\u4e1a\u8005\u8868\u793a\u4f1a\u91c7\u7528\u6216\u63a8\u8350\u8be5\u5de5\u5177\u3002", "conclusion": "AUTOCOMBAT\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u53cd\u9988\u9a71\u52a8\u7b54\u6848\u7cbe\u70bc\u5728\u63d0\u9ad8\u6280\u672f\u77e5\u8bc6\u5e73\u53f0\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\uff0cLLMs\u80fd\u6709\u6548\u5229\u7528\u7528\u6237\u53cd\u9988\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\u3002", "topic": "swe application"}}
{"id": "2601.17223", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17223", "abs": "https://arxiv.org/abs/2601.17223", "authors": ["Massimiliano Pronesti", "Anya Belz", "Yufang Hou"], "title": "Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning", "comment": null, "summary": "Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u9a8c\u8bc1\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(VPRMs)\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u89c4\u5219\u9a8c\u8bc1\u5668\u68c0\u67e5\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u5728\u533b\u5b66\u8bc1\u636e\u5408\u6210\u504f\u501a\u8bc4\u4f30\u4e2d\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u795e\u7ecf\u8bc4\u5224\u5668\u8bc4\u4f30\u601d\u7ef4\u94fe\u6b65\u9aa4\uff0c\u5bb9\u6613\u53d7\u5230\u4e0d\u900f\u660e\u6027\u3001\u504f\u89c1\u548c\u5956\u52b1\u653b\u51fb\u7684\u5f71\u54cd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u786e\u5b9a\u6027\u89c4\u5219\u9a8c\u8bc1\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u5f15\u5165\u53ef\u9a8c\u8bc1\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(VPRMs)\u6846\u67b6\uff0c\u4f7f\u7528\u786e\u5b9a\u6027\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u5668\u68c0\u67e5\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002\u5e94\u7528\u4e8e\u533b\u5b66\u8bc1\u636e\u5408\u6210\u504f\u501a\u8bc4\u4f30\u9886\u57df\uff0c\u5229\u7528\u6307\u5357\u5b9a\u4e49\u7684\u6807\u51c6\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u51b3\u7b56\u8def\u5f84\u5bf9\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u7a0b\u5e8f\u5316\u9a8c\u8bc1\u3002", "result": "VPRMs\u751f\u6210\u7684\u63a8\u7406\u7d27\u5bc6\u9075\u5faa\u9886\u57df\u89c4\u5219\uff0c\u6b65\u9aa4\u7ea7\u51b3\u7b56\u4e0e\u6700\u7ec8\u6807\u7b7e\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u663e\u8457\u63d0\u9ad8\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cVPRMs\u6bd4\u6700\u5148\u8fdb\u6a21\u578bF1\u5206\u6570\u63d0\u9ad8\u8fbe20%\uff0c\u6bd4\u53ef\u9a8c\u8bc1\u7ed3\u679c\u5956\u52b1\u9ad86.5%\uff0c\u5728\u8bc1\u636e\u57fa\u7840\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "VPRMs\u901a\u8fc7\u786e\u5b9a\u6027\u89c4\u5219\u9a8c\u8bc1\u5668\u6709\u6548\u89e3\u51b3\u4e86\u8fc7\u7a0b\u76d1\u7763\u4e2d\u7684\u4e0d\u900f\u660e\u6027\u548c\u504f\u89c1\u95ee\u9898\uff0c\u5728\u9700\u8981\u4e25\u683c\u9075\u5faa\u9886\u57df\u89c4\u5219\u7684\u533b\u5b66\u8bc1\u636e\u5408\u6210\u504f\u501a\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8fc7\u7a0b\u76d1\u7763\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.17762", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17762", "abs": "https://arxiv.org/abs/2601.17762", "authors": ["Zelong Zheng", "Jiayuan Zhou", "Xing Hu", "Yi Gao", "Shengyi Pan"], "title": "Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities", "comment": null, "summary": "Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.", "AI": {"tldr": "MAVM\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u7684\u91cd\u590d\u6f0f\u6d1e\u7ba1\u7406\uff0c\u901a\u8fc7\u6574\u5408\u6f0f\u6d1e\u77e5\u8bc6\u5e93\u3001\u68c0\u6d4b\u3001\u786e\u8ba4\u3001\u4fee\u590d\u548c\u9a8c\u8bc1\u7ec4\u4ef6\uff0c\u6709\u6548\u5229\u7528\u5386\u53f2\u6f0f\u6d1e\u77e5\u8bc6\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\u548c\u590d\u6742\u6027\u589e\u52a0\u4f7f\u5f97\u6f0f\u6d1e\u7ba1\u7406\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u9759\u6001\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u6355\u6349\u8de8\u51fd\u6570/\u6a21\u5757\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u68c0\u7d22\u548c\u5229\u7528\u80fd\u529b\uff0c\u800c\u91cd\u590d\u6f0f\u6d1e\u56e0\u4ee3\u7801\u91cd\u7528\u548c\u5171\u4eab\u903b\u8f91\u53cd\u590d\u51fa\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5386\u53f2\u6f0f\u6d1e\u77e5\u8bc6\u3002", "method": "\u63d0\u51faMAVM\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u7ec4\u4ef6\uff1a\u6f0f\u6d1e\u77e5\u8bc6\u5e93\u3001\u68c0\u6d4b\u3001\u786e\u8ba4\u3001\u4fee\u590d\u548c\u9a8c\u8bc1\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u3002\u4ece\u516c\u5f00\u62ab\u9732\u7684\u6f0f\u6d1e\u6784\u5efa\u77e5\u8bc6\u5e93\uff0c\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u68c0\u7d22\u5de5\u5177\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u63d0\u53d6\u548c\u63a8\u7406\u4ed3\u5e93\u7ea7\u4fe1\u606f\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5b89\u5168\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728\u5305\u542b78\u4e2a\u771f\u5b9e\u4e16\u754c\u8865\u4e01\u79fb\u690d\u6848\u4f8b\uff08\u8986\u76d6114\u4e2a\u51fd\u6570\u7ea7\u8fc1\u79fb\uff09\u7684\u6570\u636e\u96c6\u4e0a\uff0cMAVM\u6210\u529f\u68c0\u6d4b\u5e76\u4fee\u590d\u4e8651\u4e2a\u771f\u5b9e\u6f0f\u6d1e\uff0c\u4fee\u590d\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa31.9%-45.2%\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MAVM\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u5229\u7528\u5386\u53f2\u6f0f\u6d1e\u77e5\u8bc6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u5728\u91cd\u590d\u6f0f\u6d1e\u7ba1\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8f6f\u4ef6\u6f0f\u6d1e\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2601.18044", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18044", "abs": "https://arxiv.org/abs/2601.18044", "authors": ["Melika Sepidband", "Hamed Taherkhani", "Hung Viet Pham", "Hadi Hemmati"], "title": "RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models", "comment": "23 pages, 5 figures", "summary": "Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u9879\u76ee\u7ea7\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u6a21\u5757\u751f\u6210bug\u7279\u5b9a\u89e3\u91ca\uff0c\u7ed3\u5408LLM\u548c\u5d4c\u5165\u4fe1\u53f7\u7684\u4e24\u9636\u6bb5\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u6587\u4ef6\u7ea7\u548c\u5143\u7d20\u7ea7\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5728\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u4e2d\uff0c\u6545\u969c\u5b9a\u4f4d\u662f\u5173\u952e\u6b65\u9aa4\u3002\u73b0\u5b9e\u9879\u76ee\u4ee3\u7801\u5e93\u901a\u5e38\u5305\u542b\u6570\u767e\u4e07token\uff0c\u8fdc\u8d85\u5f53\u524dLLM\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u5148\u8bc6\u522b\u5c0f\u89c4\u6a21\u76f8\u5173\u4ee3\u7801\u5b50\u96c6\uff0c\u51c6\u786e\u7684\u6545\u969c\u5b9a\u4f4d\u5bf9\u6709\u6548\u4fee\u590d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u63a8\u7406\u6a21\u5757\uff1a1)\u4e3a\u5019\u9009\u6587\u4ef6\u548c\u5143\u7d20\u751f\u6210\u7ed3\u6784\u5316\u7684bug\u7279\u5b9a\u89e3\u91ca\uff1b2)\u5728\u7ed3\u5408LLM\u548c\u5d4c\u5165\u4fe1\u53f7\u7684\u4e24\u9636\u6bb5\u6392\u5e8f\u65b9\u6848\u4e2d\u5229\u7528\u8fd9\u4e9b\u89e3\u91ca\u3002\u8fd8\u63d0\u51fa\u53cd\u4e8b\u5b9e\u4e0a\u754c\u5206\u6790\u6765\u91cf\u5316\u6bcf\u4e2a\u5b9a\u4f4d\u9636\u6bb5\u5bf9\u4fee\u590d\u6210\u529f\u7684\u8d21\u732e\u3002", "result": "\u5728SWE-bench Verified\u3001Lite\u548cJava\u7684Python\u548cJava\u9879\u76ee\u4e0a\u8bc4\u4f30\u3002\u76f8\u6bd4Agentless\u548cOpenHands\u7b49SOTA\u57fa\u7ebf\uff0c\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff1aSWE-bench Verified\u4e0a\u6587\u4ef6\u7ea7Hit@1\u4ece71.4%\u63d0\u5347\u81f385%\uff0cMRR\u4ece81.8%\u63d0\u5347\u81f388.8%\uff1b\u5143\u7d20\u7ea7top-3\u6587\u4ef6\u4e0bExact Match\u4ece36%\u63d0\u5347\u81f369%\u3002\u96c6\u6210\u5230Agentless\u4e2d\u5b9e\u73b012.8%\u7aef\u5230\u7aef\u4fee\u590d\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u9879\u76ee\u7ea7\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u548c\u4e24\u9636\u6bb5\u6392\u5e8f\u663e\u8457\u63d0\u5347\u4e86\u6587\u4ef6\u7ea7\u548c\u5143\u7d20\u7ea7\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5bf9\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "swe application"}}
{"id": "2601.18241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18241", "abs": "https://arxiv.org/abs/2601.18241", "authors": ["Elena Bruches", "Vadim Alperovich", "Dari Baturova", "Roman Derunets", "Daniil Grebenkin", "Georgy Mkrtchyan", "Oleg Sedukhin", "Mikhail Klementev", "Ivan Bondarenko", "Nikolay Bushkov", "Stanislav Moiseev"], "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance", "comment": "Accepted for publication at the 9th Workshop on Validation, Analysis and Evolution of Software Tests (VST 2026), co-located with the the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.", "AI": {"tldr": "TAM-Eval\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u6d4b\u8bd5\u5957\u4ef6\u7ef4\u62a4\uff08\u521b\u5efa\u3001\u4fee\u590d\u3001\u66f4\u65b0\uff09\u80fd\u529b\u7684\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u5305\u542b1539\u4e2a\u591a\u8bed\u8a00\u573a\u666f\uff0c\u652f\u6301\u57fa\u4e8e\u6d4b\u8bd5\u901a\u8fc7\u7387\u3001\u4ee3\u7801\u8986\u76d6\u7387\u548c\u53d8\u5f02\u6d4b\u8bd5\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524dLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u5c40\u9650\u4e8e\u5b64\u7acb\u7684\u6d4b\u8bd5\u751f\u6210\u6216\u9884\u8a00\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u6d4b\u8bd5\u5957\u4ef6\u7ef4\u62a4\u8fd9\u4e00\u66f4\u5e7f\u6cdb\u7684\u6311\u6218\u3002\u9700\u8981\u8bc4\u4f30LLM\u5728\u5b9e\u9645\u6d4b\u8bd5\u7ef4\u62a4\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faTAM-Eval\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6d4b\u8bd5\u7ef4\u62a4\u573a\u666f\uff1a\u521b\u5efa\u3001\u4fee\u590d\u3001\u66f4\u65b0\u6d4b\u8bd5\u5957\u4ef6\u3002\u5728\u6d4b\u8bd5\u6587\u4ef6\u7ea7\u522b\u64cd\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5b8c\u6574\u4ed3\u5e93\u4e0a\u4e0b\u6587\u7684\u8bbf\u95ee\u3002\u4ecePython\u3001Java\u548cGo\u9879\u76ee\u4e2d\u81ea\u52a8\u63d0\u53d6\u548c\u9a8c\u8bc11539\u4e2a\u573a\u666f\u3002\u4f7f\u7528\u57fa\u4e8e\u6d4b\u8bd5\u5957\u4ef6\u901a\u8fc7\u7387\u3001\u4ee3\u7801\u8986\u76d6\u7387\u548c\u53d8\u5f02\u6d4b\u8bd5\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLM\u5728\u5b9e\u9645\u6d4b\u8bd5\u7ef4\u62a4\u8fc7\u7a0b\u4e2d\u7684\u80fd\u529b\u6709\u9650\uff0c\u5bf9\u6d4b\u8bd5\u6709\u6548\u6027\u7684\u63d0\u5347\u5fae\u4e4e\u5176\u5fae\u3002", "conclusion": "LLM\u5728\u6d4b\u8bd5\u5957\u4ef6\u7ef4\u62a4\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u6709\u5c40\u9650\uff0cTAM-Eval\u4f5c\u4e3a\u5f00\u6e90\u6846\u67b6\u53ef\u652f\u6301\u672a\u6765\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u7814\u7a76\u3002", "topic": "swe benchmark"}}
{"id": "2601.17588", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17588", "abs": "https://arxiv.org/abs/2601.17588", "authors": ["Marcus Ma", "Shrikanth Narayanan"], "title": "Intelligence Requires Grounding But Not Embodiment", "comment": null, "summary": "Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u667a\u80fd\u9700\u8981\"\u63a5\u5730\"(grounding)\u800c\u975e\"\u5177\u8eab\"(embodiment)\uff0c\u63d0\u51fa\u667a\u80fd\u7684\u56db\u4e2a\u5c5e\u6027\uff08\u52a8\u673a\u3001\u9884\u6d4b\u80fd\u529b\u3001\u56e0\u679c\u7406\u89e3\u3001\u7ecf\u9a8c\u5b66\u4e60\uff09\u90fd\u53ef\u901a\u8fc7\u975e\u5177\u8eab\u4f46\u63a5\u5730\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u3002", "motivation": "\u968f\u7740LLMs\u7684\u8fdb\u6b65\uff0c\u5173\u4e8e\u667a\u80fd\u662f\u5426\u9700\u8981\u5177\u8eab\u5316\u7684\u79d1\u5b66\u4e89\u8bba\u91cd\u65b0\u5174\u8d77\u3002\u4f5c\u8005\u65e8\u5728\u6f84\u6e05\u667a\u80fd\u7684\u672c\u8d28\uff0c\u533a\u5206\u5177\u8eab\u5316\u548c\u63a5\u5730\u8fd9\u4e24\u4e2a\u6982\u5ff5\uff0c\u8bba\u8bc1\u63a5\u5730\u624d\u662f\u667a\u80fd\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "method": "\u9996\u5148\u5b9a\u4e49\u667a\u80fd\u4e3a\u5177\u5907\u56db\u4e2a\u5c5e\u6027\uff1a\u52a8\u673a\u3001\u9884\u6d4b\u80fd\u529b\u3001\u56e0\u679c\u7406\u89e3\u548c\u7ecf\u9a8c\u5b66\u4e60\u3002\u7136\u540e\u8bba\u8bc1\u6bcf\u4e2a\u5c5e\u6027\u90fd\u53ef\u901a\u8fc7\u975e\u5177\u8eab\u4f46\u63a5\u5730\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u3002\u6700\u540e\u901a\u8fc7\u6570\u5b57\u73af\u5883\u4e2d\u667a\u80fdLLM\u4ee3\u7406\u7684\u601d\u60f3\u5b9e\u9a8c\u6765\u652f\u6301\u8bba\u70b9\uff0c\u5e76\u56de\u5e94\u53ef\u80fd\u7684\u53cd\u9a73\u3002", "result": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff1a\u63a5\u5730\uff08\u800c\u975e\u5177\u8eab\uff09\u662f\u667a\u80fd\u7684\u5fc5\u8981\u6761\u4ef6\u3002\u667a\u80fd\u7684\u56db\u4e2a\u6838\u5fc3\u5c5e\u6027\u90fd\u53ef\u4ee5\u5728\u975e\u5177\u8eab\u4f46\u63a5\u5730\u7684\u7cfb\u7edf\u4e2d\u5b9e\u73b0\uff0c\u8fd9\u4e3a\u6570\u5b57\u73af\u5883\u4e2d\u7684\u667a\u80fdLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u667a\u80fd\u9700\u8981\u63a5\u5730\uff08\u4e0e\u73af\u5883\u4e92\u52a8\u7684\u80fd\u529b\uff09\uff0c\u800c\u4e0d\u4e00\u5b9a\u9700\u8981\u7269\u7406\u5177\u8eab\u3002\u8fd9\u4e00\u533a\u5206\u6709\u52a9\u4e8e\u6f84\u6e05\u5173\u4e8eLLMs\u548c\u4eba\u5de5\u667a\u80fd\u672c\u8d28\u7684\u54f2\u5b66\u4e89\u8bba\uff0c\u4e3a\u6570\u5b57\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "2601.17312", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17312", "abs": "https://arxiv.org/abs/2601.17312", "authors": ["Hugo Silva", "Mateus Mendes", "Hugo Gon\u00e7alo Oliveira"], "title": "Meta-Judging with Large Language Models: Concepts, Methods, and Challenges", "comment": null, "summary": "Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86LLM\u8bc4\u4f30\u8303\u5f0f\u4ece\"LLM-as-a-Judge\"\u5230\"LLM-as-a-Meta-Judge\"\u7684\u6f14\u8fdb\uff0c\u5206\u6790\u4e86\u4f20\u7edfLLM\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u5e76\u7cfb\u7edf\u6574\u7406\u4e86\u5143\u8bc4\u4f30\u65b9\u6cd5\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "motivation": "\u4f20\u7edfLLM-as-a-Judge\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff1a\u5bf9\u63d0\u793a\u8bcd\u654f\u611f\u3001\u7cfb\u7edf\u6027\u504f\u89c1\u3001\u5197\u957f\u6548\u5e94\u3001\u4e0d\u53ef\u9760\u6216\u5e7b\u89c9\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u8fd9\u4e9b\u5c40\u9650\u6027\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u5143\u8bc4\u4f30\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u5206\u6790\u6846\u67b6\uff1a1)\u6982\u5ff5\u57fa\u7840\uff0c2)\u5143\u8bc4\u4f30\u673a\u5236\uff0c3)\u5bf9\u9f50\u8bad\u7ec3\u65b9\u6cd5\uff0c4)\u8bc4\u4f30\u65b9\u6cd5\uff0c5)\u5c40\u9650\u6027\u4e0e\u5931\u8d25\u6a21\u5f0f\uff0c6)\u672a\u6765\u65b9\u5411\u3002\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u5143\u8bc4\u4f30\u6587\u732e\uff0c\u5206\u6790LLM\u8bc4\u4f30\u7684\u6f14\u8fdb\u3002", "result": "LLM-as-a-Meta-Judge\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u53ef\u4fe1\u7684\u65b9\u5411\uff0c\u4f46\u4ecd\u9762\u4e34\u6210\u672c\u3001\u63d0\u793a\u8bcd\u654f\u611f\u6027\u548c\u5171\u4eab\u6a21\u578b\u504f\u89c1\u7b49\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u3002", "conclusion": "\u5143\u8bc4\u4f30\u8303\u5f0f\u662f\u63d0\u5347LLM\u8bc4\u4f30\u8d28\u91cf\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u73b0\u6709\u5c40\u9650\u6027\u548c\u603b\u7ed3\u6700\u65b0\u8fdb\u5c55\uff0c\u4e3a\u4e0b\u4e00\u4ee3LLM\u8bc4\u4f30\u65b9\u6cd5\u5b66\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.18341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18341", "abs": "https://arxiv.org/abs/2601.18341", "authors": ["Romain Robbes", "Th\u00e9o Matricon", "Thomas Degueule", "Andre Hora", "Stefano Zacchiroli"], "title": "Agentic Much? Adoption of Coding Agents on GitHub", "comment": null, "summary": "In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents.", "AI": {"tldr": "\u5bf9GitHub\u4e0a129,134\u4e2a\u9879\u76ee\u7684\u5927\u89c4\u6a21\u7814\u7a76\u8868\u660e\uff0c\u7f16\u7801\u4ee3\u7406\uff08\u5982Cursor\u3001Claude Code\uff09\u5728\u77ed\u77ed\u51e0\u4e2a\u6708\u5185\u5df2\u8fbe\u523015.85%-22.60%\u7684\u9ad8\u91c7\u7528\u7387\uff0c\u4e14\u91c7\u7528\u8303\u56f4\u5e7f\u6cdb\uff0c\u8de8\u8d8a\u9879\u76ee\u6210\u719f\u5ea6\u3001\u7ec4\u7ec7\u548c\u7f16\u7a0b\u8bed\u8a00\u3002\u7f16\u7801\u4ee3\u7406\u8f85\u52a9\u7684\u63d0\u4ea4\u6bd4\u7eaf\u4eba\u5de5\u63d0\u4ea4\u66f4\u5927\uff0c\u4e14\u5305\u542b\u5927\u91cf\u529f\u80fd\u5f00\u53d1\u548c\u9519\u8bef\u4fee\u590d\u3002", "motivation": "\u7f16\u7801\u4ee3\u7406\u4f5c\u4e3a\u65b0\u578b\u5f00\u53d1\u5de5\u5177\uff0c\u76f8\u6bd4\u4f20\u7edf\u4ee3\u7801\u8865\u5168LLM\u5177\u6709\u66f4\u9ad8\u81ea\u4e3b\u6027\uff0c\u80fd\u751f\u6210\u5b8c\u6574\u7684\u62c9\u53d6\u8bf7\u6c42\u3002\u8fd9\u79cd\u65b0\u6a21\u5f0f\u53ef\u80fd\u6bd4\u4ee3\u7801\u8865\u5168LLM\u4ea7\u751f\u66f4\u5927\u5f71\u54cd\uff0c\u4e14\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u5236\u54c1\u4e2d\u7559\u4e0b\u66f4\u660e\u786e\u7684\u75d5\u8ff9\uff08\u5982\u5171\u540c\u63d0\u4ea4\uff09\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u5b9e\u9645\u91c7\u7528\u548c\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u5236\u54c1\u4e2d\u7559\u4e0b\u7684\u660e\u786e\u75d5\u8ff9\uff08\u5982\u5171\u540c\u63d0\u4ea4\u6216\u62c9\u53d6\u8bf7\u6c42\uff09\uff0c\u5bf9GitHub\u4e0a129,134\u4e2a\u9879\u76ee\u8fdb\u884c\u9996\u6b21\u5927\u89c4\u6a21\u7814\u7a76\u3002\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u75d5\u8ff9\u6765\u4f30\u8ba1\u91c7\u7528\u7387\uff0c\u5e76\u5bf9\u91c7\u7528\u8005\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u5305\u62ec\u9879\u76ee\u6210\u719f\u5ea6\u3001\u7ec4\u7ec7\u80cc\u666f\u3001\u7f16\u7a0b\u8bed\u8a00\u548c\u9879\u76ee\u4e3b\u9898\u3002\u540c\u65f6\u6bd4\u8f83\u7f16\u7801\u4ee3\u7406\u8f85\u52a9\u63d0\u4ea4\u4e0e\u7eaf\u4eba\u5de5\u63d0\u4ea4\u7684\u5dee\u5f02\u3002", "result": "\u7f16\u7801\u4ee3\u7406\u7684\u91c7\u7528\u7387\u4f30\u8ba1\u4e3a15.85%-22.60%\uff0c\u8fd9\u5bf9\u4e8e\u4ec5\u5b58\u5728\u6570\u6708\u7684\u6280\u672f\u6765\u8bf4\u975e\u5e38\u9ad8\u4e14\u4ecd\u5728\u589e\u957f\u3002\u91c7\u7528\u8303\u56f4\u5e7f\u6cdb\uff1a\u6db5\u76d6\u6240\u6709\u9879\u76ee\u6210\u719f\u5ea6\u9636\u6bb5\uff1b\u5305\u62ec\u6210\u719f\u7ec4\u7ec7\uff1b\u6d89\u53ca\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u9879\u76ee\u4e3b\u9898\u3002\u7f16\u7801\u4ee3\u7406\u8f85\u52a9\u7684\u63d0\u4ea4\u6bd4\u7eaf\u4eba\u5de5\u63d0\u4ea4\u66f4\u5927\uff0c\u4e14\u5305\u542b\u5927\u91cf\u529f\u80fd\u5f00\u53d1\u548c\u9519\u8bef\u4fee\u590d\u3002", "conclusion": "\u7f16\u7801\u4ee3\u7406\u5728\u77ed\u65f6\u95f4\u5185\u5df2\u8fbe\u5230\u663e\u8457\u91c7\u7528\u7387\uff0c\u4e14\u91c7\u7528\u8303\u56f4\u5e7f\u6cdb\uff0c\u8868\u660e\u8fd9\u4e00\u6280\u672f\u6b63\u5728\u5feb\u901f\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u3002\u7f16\u7801\u4ee3\u7406\u8f85\u52a9\u7684\u63d0\u4ea4\u663e\u793a\u51fa\u4e0d\u540c\u7684\u7279\u5f81\uff08\u66f4\u5927\u89c4\u6a21\u3001\u66f4\u591a\u529f\u80fd/\u9519\u8bef\u4fee\u590d\uff09\uff0c\u8fd9\u51f8\u663e\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u7f16\u7801\u4ee3\u7406\u7684\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u548c\u5f71\u54cd\u3002", "topic": "code agent"}}
{"id": "2601.17642", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17642", "abs": "https://arxiv.org/abs/2601.17642", "authors": ["Zhihao Zhang", "Liting Huang", "Guanghao Wu", "Preslav Nakov", "Heng Ji", "Usman Naseem"], "title": "Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context", "comment": "Preprint", "summary": "Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \\emph{over-refusal} of benign queries or \\emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \\textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \\textbf{Over-Refusal} and \\textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\\% of \"Hard\" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit \"safety-pessimism\" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \\textcolor{red}{Warning: Some contents may include toxic or undesired contents.}", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Health-ORSC-Bench\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u533b\u7597AI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u8fc7\u5ea6\u62d2\u7edd\u548c\u5b89\u5168\u5b8c\u6210\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u56f0\u5883\u3002", "motivation": "\u73b0\u6709\u533b\u7597AI\u5b89\u5168\u5bf9\u9f50\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u62d2\u7edd\u8fb9\u754c\uff0c\u5bfc\u81f4\u5bf9\u826f\u6027\u67e5\u8be2\u7684\u8fc7\u5ea6\u62d2\u7edd\u6216\u5bf9\u6709\u5bb3\u67e5\u8be2\u7684\u4e0d\u5b89\u5168\u5408\u89c4\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ea\u5173\u6ce8\u6781\u7aef\u60c5\u51b5\uff0c\u65e0\u6cd5\u8bc4\u4f30\u6a21\u578b\u5728\u53cc\u7528\u9014\u6216\u8fb9\u754c\u67e5\u8be2\u4e0a\u63d0\u4f9b\u5b89\u5168\u9ad8\u5c42\u6307\u5bfc\u800c\u4e0d\u8de8\u8d8a\u53ef\u64cd\u4f5c\u5371\u5bb3\u7684\"\u5b89\u5168\u5b8c\u6210\"\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b31,920\u4e2a\u826f\u6027\u8fb9\u754c\u63d0\u793a\u7684Health-ORSC-Bench\u57fa\u51c6\uff0c\u6db5\u76d6\u4e03\u4e2a\u5065\u5eb7\u7c7b\u522b\uff08\u5982\u81ea\u6b8b\u3001\u533b\u7597\u9519\u8bef\u4fe1\u606f\uff09\u3002\u91c7\u7528\u81ea\u52a8\u5316\u6d41\u7a0b\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\uff0c\u5728\u4e0d\u540c\u610f\u56fe\u6a21\u7cca\u5ea6\u6c34\u5e73\u4e0a\u6d4b\u8bd5\u6a21\u578b\u3002\u8bc4\u4f30\u4e8630\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff0c\u5305\u62ecGPT-5\u548cClaude-4\u3002", "result": "\u5b89\u5168\u4f18\u5316\u7684\u6a21\u578b\u7ecf\u5e38\u62d2\u7edd\u9ad8\u8fbe80%\u7684\"\u56f0\u96be\"\u826f\u6027\u63d0\u793a\uff0c\u800c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u5e38\u4e3a\u5b9e\u7528\u6027\u727a\u7272\u5b89\u5168\u6027\u3002\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u663e\u8457\u5f71\u54cd\u6821\u51c6\uff1a\u5927\u578b\u524d\u6cbf\u6a21\u578b\uff08\u5982GPT-5\u3001Llama-4\uff09\u8868\u73b0\u51fa\"\u5b89\u5168\u60b2\u89c2\u4e3b\u4e49\"\u548c\u66f4\u9ad8\u7684\u8fc7\u5ea6\u62d2\u7edd\uff0c\u76f8\u6bd4\u5c0f\u578b\u6216MoE\u6a21\u578b\uff08\u5982Qwen-3-Next\uff09\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u62d2\u7edd\u548c\u5408\u89c4\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0cHealth-ORSC-Bench\u4e3a\u4e0b\u4e00\u4ee3\u533b\u7597AI\u52a9\u624b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6821\u51c6\u6807\u51c6\uff0c\u63a8\u52a8\u5b9e\u73b0\u7ec6\u81f4\u3001\u5b89\u5168\u548c\u6709\u7528\u7684\u5b8c\u6210\u3002\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5728\u63a5\u53d7\u540e\u53d1\u5e03\u3002", "topic": "agent analysis"}}
{"id": "2601.17344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17344", "abs": "https://arxiv.org/abs/2601.17344", "authors": ["Chen Chen", "Kim Young Il", "Yuan Yang", "Wenhao Su", "Yilin Zhang", "Xueluan Gong", "Qian Wang", "Yongsen Zheng", "Ziyao Liu", "Kwok-Yan Lam"], "title": "The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents", "comment": "21 pages, 11 figures", "summary": "Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.", "AI": {"tldr": "IMPRESS\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u5b8c\u5168\u826f\u6027\u573a\u666f\u4e2d\u7684\u5185\u5728\u4ef7\u503c\u9519\u4f4d\u98ce\u9669\uff0c\u53d1\u73b0\u8fd9\u662f\u666e\u904d\u5b58\u5728\u7684\u5b89\u5168\u95ee\u9898\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u5bf9\u663e\u6027\u6709\u5bb3\u8f93\u5165\u7684\u54cd\u5e94\u6216\u7cfb\u7edf\u6545\u969c\u7684\u9c81\u68d2\u6027\uff0c\u800c\u5728\u73b0\u5b9e\u3001\u5b8c\u5168\u826f\u6027\u7684\u81ea\u4e3b\u4ee3\u7406\u8bbe\u7f6e\u4e2d\u7684\u4ef7\u503c\u9519\u4f4d\u98ce\u9669\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faIMPRESS\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5LLM\u751f\u6210\u7ba1\u9053\u6784\u5efa\u73b0\u5b9e\u3001\u5b8c\u5168\u826f\u6027\u3001\u60c5\u5883\u5316\u7684\u573a\u666f\u57fa\u51c6\uff0c\u8bc4\u4f3021\u4e2a\u6700\u5148\u8fdbLLM\u4ee3\u7406\u7684\u5185\u5728\u4ef7\u503c\u9519\u4f4d\u3002", "result": "\u5185\u5728\u4ef7\u503c\u9519\u4f4d\u662f\u8de8\u6a21\u578b\u7684\u666e\u904d\u5b89\u5168\u98ce\u9669\uff0c\u9519\u4f4d\u7387\u56e0\u52a8\u673a\u3001\u98ce\u9669\u7c7b\u578b\u3001\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u800c\u5f02\uff1b\u60c5\u5883\u5316\u548c\u6846\u67b6\u673a\u5236\u663e\u8457\u5f71\u54cd\u9519\u4f4d\u884c\u4e3a\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u5b8c\u5168\u826f\u6027\u573a\u666f\u4e2d\u5b58\u5728\u666e\u904d\u7684\u5185\u5728\u4ef7\u503c\u9519\u4f4d\u98ce\u9669\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5b89\u5168\u63aa\u65bd\uff0cIMPRESS\u6846\u67b6\u4e3aAI\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u8bc4\u4f30\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.17678", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.17678", "abs": "https://arxiv.org/abs/2601.17678", "authors": ["Zhiyu An", "Wan Du"], "title": "DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories", "comment": null, "summary": "We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.", "AI": {"tldr": "\u63d0\u51faDIML\u6846\u67b6\uff0c\u901a\u8fc7\u89c2\u5bdf\u81ea\u5229\u5b66\u4e60\u4ee3\u7406\u7684\u6218\u7565\u4ea4\u4e92\u8f68\u8ff9\u6765\u53cd\u63a8\u672a\u77e5\u7684\u6fc0\u52b1\u751f\u6210\u673a\u5236\uff0c\u5305\u62ec\u975e\u7ed3\u6784\u5316\uff08\u5982\u795e\u7ecf\u7f51\u7edc\uff09\u673a\u5236", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u9006\u535a\u5f08\u8bba\u548c\u591a\u667a\u80fd\u4f53\u9006\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u53ea\u80fd\u63a8\u65ad\u7ed3\u6784\u5316\u673a\u5236\u5185\u7684\u6548\u7528/\u5956\u52b1\u53c2\u6570\uff0c\u800c\u65e0\u6cd5\u5904\u7406\u975e\u7ed3\u6784\u5316\u673a\u5236\uff1b\u53ef\u5fae\u5206\u673a\u5236\u8bbe\u8ba1\u662f\u524d\u5411\u4f18\u5316\u800c\u975e\u4ece\u884c\u4e3a\u4e2d\u63a8\u65ad\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4ece\u89c2\u5bdf\u5230\u7684\u6218\u7565\u4ea4\u4e92\u4e2d\u6062\u590d\u4efb\u610f\u6fc0\u52b1\u751f\u6210\u673a\u5236\u7684\u65b9\u6cd5", "method": "\u63d0\u51faDIML\uff08Differentiable Inverse Mechanism Learning\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u4f3c\u7136\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u5fae\u5206\uff0c\u4f7f\u7528\u5019\u9009\u673a\u5236\u751f\u6210\u9884\u6d4b\u89c2\u5bdf\u884c\u4e3a\u6240\u9700\u7684\u53cd\u4e8b\u5b9e\u6536\u76ca", "result": "\u5728\u6761\u4ef6logit\u54cd\u5e94\u6a21\u578b\u4e0b\u5efa\u7acb\u4e86\u6536\u76ca\u5dee\u5f02\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u8bc1\u660e\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u7edf\u8ba1\u4e00\u81f4\u6027\uff1b\u5728\u975e\u7ed3\u6784\u5316\u795e\u7ecf\u673a\u5236\u3001\u62e5\u5835\u6536\u8d39\u3001\u516c\u5171\u7269\u54c1\u8865\u8d34\u548c\u5927\u89c4\u6a21\u533f\u540d\u6e38\u620f\u7b49\u573a\u666f\u4e2d\uff0cDIML\u80fd\u53ef\u9760\u6062\u590d\u53ef\u8bc6\u522b\u7684\u6fc0\u52b1\u5dee\u5f02\uff0c\u652f\u6301\u53cd\u4e8b\u5b9e\u9884\u6d4b\uff0c\u6027\u80fd\u5728\u5c0f\u73af\u5883\u4e2d\u63a5\u8fd1\u8868\u683c\u679a\u4e3eoracle\uff0c\u5728\u5927\u89c4\u6a21\uff08\u767e\u53c2\u4e0e\u8005\uff09\u73af\u5883\u4e2d\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u6027", "conclusion": "DIML\u4e3a\u4ece\u89c2\u5bdf\u5230\u7684\u6218\u7565\u4ea4\u4e92\u4e2d\u63a8\u65ad\u6fc0\u52b1\u751f\u6210\u673a\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u975e\u7ed3\u6784\u5316\u673a\u5236\u5e76\u6269\u5c55\u5230\u5927\u89c4\u6a21\u73af\u5883\uff0c\u4e3a\u673a\u5236\u8bbe\u8ba1\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177", "topic": "agent analysis"}}
{"id": "2601.18345", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18345", "abs": "https://arxiv.org/abs/2601.18345", "authors": ["Romain Robes Th\u00e9o Matricon", "Thomas Degueule", "Andre Hora", "Stefano Zacchiroli"], "title": "Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity", "comment": "Preprint. Accepted for publication at MSR 2026", "summary": "In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790GitHub\u4e0a\u7684\u7f16\u7801\u4ee3\u7406\u6d3b\u52a8\uff0c\u7814\u7a76\u4e86\u7f16\u7801\u4ee3\u7406\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u603b\u7ed3\u4e86\u5176\u524d\u666f\u3001\u98ce\u9669\u548c\u5b9e\u8df5\u7ecf\u9a8c\u3002", "motivation": "\u7f16\u7801\u4ee3\u7406\u57282025\u5e74\u8fc5\u901f\u666e\u53ca\uff0c\u5b83\u4eec\u4e0e\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u8865\u5168\u6709\u663e\u8457\u4e0d\u540c\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76\u3002\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u7559\u4e0b\u53ef\u89c1\u75d5\u8ff9\uff0c\u4f7f\u5f97\u53ef\u4ee5\u4f7f\u7528MSR\u6280\u672f\u7814\u7a76\u5176\u5bf9SE\u5b9e\u8df5\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790GitHub\u4e0a\u7684\u7f16\u7801\u4ee3\u7406\u6d3b\u52a8\uff0c\u4f7f\u7528MSR\uff08\u8f6f\u4ef6\u5de5\u7a0b\u6316\u6398\uff09\u6280\u672f\u6765\u7814\u7a76\u7f16\u7801\u4ee3\u7406\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u7684\u5f71\u54cd\u3002", "result": "\u4ece\u7f16\u7801\u4ee3\u7406\u6d3b\u52a8\u4e2d\u6536\u96c6\u4e86\u5173\u4e8e\u5176\u524d\u666f\u3001\u98ce\u9669\u548c\u5b9e\u8df5\u7ecf\u9a8c\u7684\u5177\u4f53\u53d1\u73b0\u548c\u542f\u53d1\u5f0f\u89c4\u5219\u3002", "conclusion": "\u7f16\u7801\u4ee3\u7406\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u4ea7\u751f\u4e86\u91cd\u8981\u5f71\u54cd\uff0c\u7814\u7a76\u5176\u6d3b\u52a8\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u524d\u666f\u548c\u98ce\u9669\uff0c\u4e3a\u5b9e\u8df5\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.17699", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17699", "abs": "https://arxiv.org/abs/2601.17699", "authors": ["Harper Hua", "Zhen Han", "Zhengyuan Shen", "Jeremy Lee", "Patrick Guan", "Qi Zhu", "Sullam Jeoung", "Yueyan Chen", "Yunfei Bai", "Shuai Wang", "Vassilis Ioannidis", "Huzefa Rangwala"], "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL", "comment": null, "summary": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.", "AI": {"tldr": "SQL-Trail\uff1a\u4e00\u4e2a\u57fa\u4e8e\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u7684Text-to-SQL\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9884\u7b97\u5206\u914d\u548c\u590d\u5408\u5956\u52b1\u673a\u5236\uff0c\u5728\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u8fed\u4ee3\u4f18\u5316SQL\u67e5\u8be2\u751f\u6210\uff0c\u663e\u8457\u8d85\u8d8a\u5355\u6b21\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u5728Text-to-SQL\u751f\u6210\u4e0a\u4ecd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u5355\u6b21\u751f\u6210\u8303\u5f0f\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u4e13\u5bb6\u81ea\u7136\u4f7f\u7528\u7684\u8fed\u4ee3\u63a8\u7406\u3001\u6a21\u5f0f\u63a2\u7d22\u548c\u9519\u8bef\u7ea0\u6b63\u884c\u4e3a\u3002", "method": "\u63d0\u51faSQL-Trail\u6846\u67b6\uff1a1\uff09\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u4e0e\u6570\u636e\u5e93\u73af\u5883\u4ea4\u4e92\u5e76\u5229\u7528\u6267\u884c\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\uff1b2\uff09\u81ea\u9002\u5e94\u8f6e\u6b21\u9884\u7b97\u5206\u914d\u673a\u5236\uff0c\u6839\u636e\u95ee\u9898\u96be\u5ea6\u8c03\u6574\u4ea4\u4e92\u6df1\u5ea6\uff1b3\uff09\u590d\u5408\u5956\u52b1\u9762\u677f\uff0c\u8054\u5408\u6fc0\u52b1SQL\u6b63\u786e\u6027\u548c\u9ad8\u6548\u63a2\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\uff0c\u6570\u636e\u6548\u7387\u6bd4\u4e4b\u524d\u5355\u6b21RL\u65b9\u6cd5\u9ad818\u500d\u30027B\u548c14B\u6a21\u578b\u5e73\u5747\u8d85\u8d8a\u59275%\u7684\u4e13\u6709\u7cfb\u7edf\uff0c\u8bc1\u660e\u4ea4\u4e92\u5f0f\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u3001\u4ee3\u7406\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\u5bf9\u4e8e\u7a33\u5065\u7684Text-to-SQL\u751f\u6210\u975e\u5e38\u6709\u6548\uff0c\u591a\u8f6eRL\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5373\u4f7f\u8f83\u5c0f\u6a21\u578b\u4e5f\u80fd\u8d85\u8d8a\u66f4\u5927\u7684\u4e13\u6709\u7cfb\u7edf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18418", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18418", "abs": "https://arxiv.org/abs/2601.18418", "authors": ["Ji Zeng", "Dayuan Fu", "Tiantian Mi", "Yumin Zhuang", "Yaxing Huang", "Xuefeng Li", "Lyumanshan Ye", "Muhang Xie", "Qishuo Hua", "Zhen Huang", "Mohan Jiang", "Hanning Wang", "Jifan Lin", "Yang Xiao", "Jie Sun", "Yunze Wu", "Pengfei Liu"], "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "comment": null, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\"\u4ee3\u7406\u5f0f\u4e2d\u671f\u8bad\u7ec3\"\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u4ee3\u7406\u539f\u751f\u6570\u636e\uff08\u5305\u62ec\u4e0a\u4e0b\u6587\u539f\u751f\u8f68\u8ff9\u548c\u73af\u5883\u539f\u751f\u8f68\u8ff9\uff09\u6765\u8bad\u7ec3\u4ee3\u7801\u4ee3\u7406\uff0c\u5728SWE-Bench Verified\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e8656.1%-58.5%\u7684\u89e3\u51b3\u7387\uff0c\u4f7f\u7528\u7684\u4e2d\u671f\u8bad\u7ec3token\u6570\u91cf\u6bd4Kimi-Dev\u5c11\u4e00\u534a\u4ee5\u4e0a\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u4ee3\u7406\u4e3b\u8981\u4f9d\u8d56\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u800c\u4ee3\u7406\u5f0f\u4e2d\u671f\u8bad\u7ec3\u867d\u7136\u80fd\u66f4\u53ef\u6269\u5c55\u5730\u57f9\u517b\u57fa\u7840\u4ee3\u7406\u884c\u4e3a\uff0c\u4f46\u7531\u4e8e\u8d44\u6e90\u9700\u6c42\u5927\u4e14\u5b58\u5728\u9759\u6001\u8bad\u7ec3\u6570\u636e\u4e0e\u52a8\u6001\u5f00\u53d1\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e00\u76f4\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u539f\u751f\u6570\u636e\u76d1\u7763\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u79cd\u4e92\u8865\u7684\u8f68\u8ff9\u7c7b\u578b\uff1a1) \u4e0a\u4e0b\u6587\u539f\u751f\u8f68\u8ff9\uff1a\u4fdd\u7559\u4ee3\u7406\u7ecf\u5386\u7684\u5b8c\u6574\u4fe1\u606f\u6d41\uff0c\u63d0\u4f9b\u5e7f\u6cdb\u8986\u76d6\u548c\u591a\u6837\u6027\uff1b2) \u73af\u5883\u539f\u751f\u8f68\u8ff9\uff1a\u4ece\u53ef\u6267\u884c\u4ed3\u5e93\u6536\u96c6\uff0c\u89c2\u6d4b\u6765\u81ea\u5b9e\u9645\u5de5\u5177\u8c03\u7528\u548c\u6d4b\u8bd5\u6267\u884c\uff0c\u63d0\u4f9b\u6df1\u5ea6\u548c\u4ea4\u4e92\u771f\u5b9e\u6027\u3002", "result": "\u5728SWE-Bench Verified\u57fa\u51c6\u4e0a\uff0c32B\u548c72B\u6a21\u578b\u5206\u522b\u8fbe\u523056.1%\u548c58.5%\u7684\u89e3\u51b3\u7387\u3002\u5728\u76f8\u540c\u57fa\u7840\u6a21\u578b\u548c\u4ee3\u7406\u6846\u67b6\u4e0b\uff0c\u4f7f\u7528\u4e0d\u5230\u4e00\u534a\u7684\u4e2d\u671f\u8bad\u7ec3token\uff0873.1B\uff09\u5c31\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684Kimi-Dev\u65b9\u6cd5\u3002", "conclusion": "\u4ee3\u7406\u5f0f\u4e2d\u671f\u8bad\u7ec3\u901a\u8fc7\u5408\u6210\u4ee3\u7406\u539f\u751f\u6570\u636e\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u4ee3\u7406\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u76f8\u6bd4\u4f9d\u8d56\u6602\u8d35\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u66f4\u5177\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2601.17722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17722", "abs": "https://arxiv.org/abs/2601.17722", "authors": ["Ying Mo", "Yu Bai", "Dapeng Sun", "Yuqian Shi", "Yukai Miao", "Li Chen", "Dan Li"], "title": "EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.", "AI": {"tldr": "EntWorld\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u4f01\u4e1a\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,756\u4e2a\u8de8\u516d\u4e2a\u4f01\u4e1a\u9886\u57df\uff08CRM\u3001ITIL\u3001ERP\u7b49\uff09\u7684\u4efb\u52a1\uff0c\u91c7\u7528\u57fa\u4e8e\u6570\u636e\u5e93\u6a21\u5f0f\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u751f\u6210\u548cSQL\u9a8c\u8bc1\u673a\u5236\uff0c\u63ed\u793a\u5f53\u524dAI\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u80fd\u529b\u5dee\u8ddd\uff08GPT-4.1\u6210\u529f\u7387\u4ec547.61%\uff09", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u6d88\u8d39\u7ea7\u573a\u666f\uff08\u5982\u7535\u5546\u3001\u65c5\u6e38\u9884\u8ba2\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u4e13\u4e1a\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\u548c\u4e25\u8c28\u6027\u3002\u4f01\u4e1a\u7cfb\u7edf\u5177\u6709\u9ad8\u5bc6\u5ea6\u7528\u6237\u754c\u9762\u3001\u4e25\u683c\u4e1a\u52a1\u903b\u8f91\u7ea6\u675f\u548c\u7cbe\u786e\u72b6\u6001\u4e00\u81f4\u6027\u8981\u6c42\u7b49\u72ec\u7279\u6311\u6218\uff0c\u5f53\u524d\u901a\u7528\u4ee3\u7406\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "1) \u63d0\u51faEntWorld\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u516d\u4e2a\u4f01\u4e1a\u9886\u57df\u76841,756\u4e2a\u4efb\u52a1\uff1b2) \u91c7\u7528\u57fa\u4e8e\u6a21\u5f0f\u7684\u4efb\u52a1\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u5e95\u5c42\u6570\u636e\u5e93\u6a21\u5f0f\u9006\u5411\u5de5\u7a0b\u4e1a\u52a1\u903b\u8f91\uff0c\u5408\u6210\u771f\u5b9e\u7684\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u7a0b\uff1b3) \u63d0\u51fa\u57fa\u4e8eSQL\u7684\u786e\u5b9a\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u7528\u4e25\u683c\u7684\u72b6\u6001\u8f6c\u6362\u9a8c\u8bc1\u66ff\u4ee3\u6a21\u7cca\u7684\u89c6\u89c9\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u5728EntWorld\u4e0a\u7684\u6210\u529f\u7387\u4ec5\u4e3a47.61%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u4ee3\u7406\u80fd\u529b\u5728\u4f01\u4e1a\u9886\u57df\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "EntWorld\u4f5c\u4e3a\u4e00\u4e2a\u4e25\u8c28\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524dAI\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u4e0b\u4e00\u4ee3\u4f01\u4e1a\u7ea7\u6570\u5b57\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.17133", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17133", "abs": "https://arxiv.org/abs/2601.17133", "authors": ["Inderjeet Singh", "Eleonore Vissol-Gaudin", "Andikan Otung", "Motoyoshi Sekiya"], "title": "Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation", "comment": "Accepted to AAAI 2026. 13 pages, 3 figures, 10 tables. Code available at: https://github.com/FujitsuResearch/knexa-fl", "summary": "Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.", "AI": {"tldr": "KNEXA-FL\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\u7b97\u6cd5\u4f18\u5316\u5f02\u6784LLM\u4ee3\u7406\u4e4b\u95f4\u7684\u77e5\u8bc6\u4ea4\u6362\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u4e13\u4e1a\u5316LLM\u5fae\u8c03\u4e2d\u5b58\u5728\u9690\u79c1\u4e0e\u6027\u80fd\u7684\u51b2\u7a81\uff1a\u96c6\u4e2d\u5f0fFL\u6709\u5355\u70b9\u6545\u969c\u548c\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u98ce\u9669\uff0c\u800c\u53bb\u4e2d\u5fc3\u5316FL\u7684\u968f\u673aP2P\u914d\u5bf9\u6548\u7387\u4f4e\u4e0b\u4e14\u53ef\u80fd\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u3002", "method": "\u63d0\u51faKNEXA-FL\u6846\u67b6\uff0c\u91c7\u7528\u975e\u805a\u5408\u7684\u4e2d\u5fc3\u914d\u7f6e\u5668/\u5339\u914d\u5668(CPM)\uff0c\u5c06P2P\u534f\u4f5c\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u4f7f\u7528LinUCB\u7b97\u6cd5\u57fa\u4e8e\u62bd\u8c61\u4ee3\u7406\u914d\u7f6e\u6587\u4ef6\u5b66\u4e60\u6700\u4f18\u5339\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u5b89\u5168\u84b8\u998f\u5b9e\u73b0\u5f02\u6784PEFT-based LLM\u4ee3\u7406\u95f4\u7684\u76f4\u63a5\u77e5\u8bc6\u4ea4\u6362\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cKNEXA-FL\u76f8\u6bd4\u968f\u673aP2P\u534f\u4f5c\u5c06Pass@1\u63d0\u9ad8\u4e86\u7ea650%\uff0c\u4e14\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u6536\u655b\u6027\uff0c\u800c\u96c6\u4e2d\u5f0f\u84b8\u998f\u57fa\u7ebf\u5219\u51fa\u73b0\u4e86\u707e\u96be\u6027\u7684\u6027\u80fd\u5d29\u6e83\u3002", "conclusion": "\u81ea\u9002\u5e94\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u7f16\u6392\u662f\u6784\u5efa\u7a33\u5065\u6709\u6548\u7684\u53bb\u4e2d\u5fc3\u5316AI\u751f\u6001\u7cfb\u7edf\u7684\u57fa\u7840\u539f\u5219\uff0cKNEXA-FL\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.17735", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17735", "abs": "https://arxiv.org/abs/2601.17735", "authors": ["Kyungho Kim", "Geon Lee", "Juyeon Kim", "Dongwon Choi", "Shinhwan Kang", "Kijung Shin"], "title": "ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents", "comment": "Accepted in ACM WWW 2026 (Short Paper)", "summary": "Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.", "AI": {"tldr": "ReFuGe\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u81ea\u52a8\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\u5728\u73b0\u5b9eWeb\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9488\u5bf9RDB\u7684\u9884\u6d4b\u4efb\u52a1\u9762\u4e34\u6311\u6218\uff1a\u9700\u8981\u5904\u7406\u590d\u6742\u6a21\u5f0f\u3001\u63a2\u7d22\u7ec4\u5408\u7206\u70b8\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u4e14\u7f3a\u4e4f\u660e\u786e\u76d1\u7763\u3002", "method": "\u63d0\u51faReFuGe\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u4e2a\u4e13\u95e8\u7684LLM\u4ee3\u7406\uff1a1)\u6a21\u5f0f\u9009\u62e9\u4ee3\u7406\u8bc6\u522b\u76f8\u5173\u8868\u548c\u5217\uff1b2)\u7279\u5f81\u751f\u6210\u4ee3\u7406\u4ece\u9009\u5b9a\u6a21\u5f0f\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u7279\u5f81\uff1b3)\u7279\u5f81\u8fc7\u6ee4\u4ee3\u7406\u901a\u8fc7\u57fa\u4e8e\u63a8\u7406\u548c\u9a8c\u8bc1\u7684\u8fc7\u6ee4\u8bc4\u4f30\u5e76\u4fdd\u7559\u6709\u524d\u666f\u7684\u7279\u5f81\u3002\u8fd9\u4e9b\u4ee3\u7406\u5728\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u4e2d\u8fd0\u884c\uff0c\u76f4\u5230\u6027\u80fd\u6536\u655b\u3002", "result": "\u5728RDB\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReFuGe\u5728\u5404\u79cdRDB\u9884\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "ReFuGe\u901a\u8fc7LLM\u4ee3\u7406\u7684\u534f\u4f5c\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5173\u7cfb\u6570\u636e\u5e93\u7279\u5f81\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3aRDB\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7279\u5f81\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.17744", "categories": ["cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17744", "abs": "https://arxiv.org/abs/2601.17744", "authors": ["Amjad Fatmi"], "title": "Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems", "comment": "40 pages, 10 figures. Preprint. Code: https://github.com/faramesh/faramesh-core", "summary": "Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.", "AI": {"tldr": "Faramesh\u662f\u4e00\u4e2a\u534f\u8bae\u65e0\u5173\u7684\u6267\u884c\u63a7\u5236\u5e73\u9762\uff0c\u901a\u8fc7\u4e0d\u53ef\u7ed5\u8fc7\u7684\u884c\u52a8\u6388\u6743\u8fb9\u754c\u5f3a\u5236\u6267\u884c\u65f6\u6388\u6743\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u53ef\u6267\u884c\u3001\u53ef\u9884\u6d4b\u7684\u6cbb\u7406\u3002", "motivation": "\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u89e6\u53d1\u73b0\u5b9e\u4e16\u754c\u7684\u526f\u4f5c\u7528\uff08\u90e8\u7f72\u57fa\u7840\u8bbe\u65bd\u3001\u4fee\u6539\u6570\u636e\u5e93\u3001\u79fb\u52a8\u8d44\u91d1\u3001\u6267\u884c\u5de5\u4f5c\u6d41\uff09\uff0c\u4f46\u5927\u591a\u6570\u4ee3\u7406\u5806\u6808\u7f3a\u4e4f\u5f3a\u5236\u6027\u7684\u6267\u884c\u68c0\u67e5\u70b9\uff0c\u65e0\u6cd5\u5728\u884c\u52a8\u6539\u53d8\u73b0\u5b9e\u4e4b\u524d\u786e\u5b9a\u6027\u5730\u5141\u8bb8\u3001\u62d2\u7edd\u6216\u63a8\u8fdf\u884c\u52a8\u3002", "method": "\u5f15\u5165Faramesh\u534f\u8bae\u65e0\u5173\u7684\u6267\u884c\u63a7\u5236\u5e73\u9762\uff0c\u901a\u8fc7\u4e0d\u53ef\u7ed5\u8fc7\u7684\u884c\u52a8\u6388\u6743\u8fb9\u754c\u5f3a\u5236\u6267\u884c\u65f6\u6388\u6743\u3002\u7cfb\u7edf\u5c06\u4ee3\u7406\u610f\u56fe\u89c4\u8303\u5316\u4e3a\u89c4\u8303\u884c\u52a8\u8868\u793a\uff0c\u6839\u636e\u7b56\u7565\u548c\u72b6\u6001\u786e\u5b9a\u6027\u5730\u8bc4\u4f30\u884c\u52a8\uff0c\u5e76\u53d1\u51fa\u51b3\u7b56\u5de5\u4ef6\uff08\u5141\u8bb8/\u63a8\u8fdf/\u62d2\u7edd\uff09\uff0c\u6267\u884c\u5668\u5fc5\u987b\u5728\u6267\u884c\u524d\u9a8c\u8bc1\u3002", "result": "Faramesh\u63d0\u4f9b\u6846\u67b6\u548c\u6a21\u578b\u65e0\u5173\u7684\u8bbe\u8ba1\uff0c\u652f\u6301\u591a\u4ee3\u7406\u548c\u591a\u79df\u6237\u90e8\u7f72\uff0c\u4fdd\u6301\u4e0e\u4f20\u8f93\u534f\u8bae\u72ec\u7acb\u3002\u63d0\u4f9b\u57fa\u4e8e\u89c4\u8303\u884c\u52a8\u54c8\u5e0c\u7684\u51b3\u7b56\u4e2d\u5fc3\u3001\u4ec5\u8ffd\u52a0\u7684\u6eaf\u6e90\u65e5\u5fd7\uff0c\u5b9e\u73b0\u53ef\u5ba1\u8ba1\u6027\u3001\u9a8c\u8bc1\u548c\u786e\u5b9a\u6027\u91cd\u653e\u3002", "conclusion": "\u8fd9\u4e9b\u539f\u8bed\u4e3a\u81ea\u4e3b\u6267\u884c\u63d0\u4f9b\u4e86\u53ef\u6267\u884c\u3001\u53ef\u9884\u6d4b\u7684\u6cbb\u7406\uff0c\u907f\u514d\u4e86\u4e0e\u7f16\u6392\u5c42\u7684\u9690\u85cf\u8026\u5408\u6216\u4ec5\u89c2\u5bdf\u6027\u7684\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.18749", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18749", "abs": "https://arxiv.org/abs/2601.18749", "authors": ["Haruhiko Yoshioka", "Takahiro Monno", "Haruka Tokumasu", "Taiki Wakamatsu", "Yuki Ota", "Nimmi Weeraddana", "Kenichi Matsumoto"], "title": "Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests", "comment": "Accepted for publication in the 23rd International Conference on Mining Software Repositories (MSR '26) : 5 pages, 3 figures, 3 tables", "summary": "The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.", "AI": {"tldr": "\u5bf940,214\u4e2aPR\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0AI\u751f\u6210\u7684PR\u5408\u5e76\u7387\u4f4e\u4e8e\u4eba\u7c7bPR\uff0c\u63d0\u4ea4\u8005\u5c5e\u6027\u5bf9\u5408\u5e76\u7ed3\u679c\u5f71\u54cd\u6700\u5927\uff0c\u800c\u5ba1\u67e5\u76f8\u5173\u7279\u5f81\u5728\u4eba\u7c7b\u548cAI PR\u4e2d\u5448\u73b0\u76f8\u53cd\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1AI\u751f\u6210\u7684PR\u521b\u5efa\u5feb\u901f\u4fbf\u6377\uff0c\u4f46\u5176\u5408\u5e76\u7387\u4f4e\u4e8e\u4eba\u7c7b\u521b\u5efa\u7684PR\uff0c\u9700\u8981\u7814\u7a76\u5f71\u54cdPR\u5408\u5e76\u7684\u5173\u952e\u56e0\u7d20\u4ee5\u6539\u8fdbAI PR\u8d28\u91cf\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\u768440,214\u4e2aPR\uff0c\u63d0\u53d66\u4e2a\u5bb6\u65cf\u768464\u4e2a\u7279\u5f81\uff0c\u6784\u5efa\u7edf\u8ba1\u56de\u5f52\u6a21\u578b\u6bd4\u8f83\u4eba\u7c7b\u548cAI PR\u7684\u5408\u5e76\u7ed3\u679c\uff0c\u5e76\u5206\u6790\u4e09\u4e2a\u4e0d\u540cAI\u4ee3\u7406\u7684\u5dee\u5f02\u3002", "result": "\u63d0\u4ea4\u8005\u5c5e\u6027\u5bf9\u4e24\u7c7bPR\u7684\u5408\u5e76\u7ed3\u679c\u5f71\u54cd\u6700\u5927\uff1b\u5ba1\u67e5\u76f8\u5173\u7279\u5f81\u5728\u4eba\u7c7b\u548cAI PR\u4e2d\u5448\u73b0\u76f8\u53cd\u6548\u679c\uff1b\u4e0d\u540cAI\u4ee3\u7406\u4e4b\u95f4\u4e5f\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u6539\u8fdbPR\u8d28\u91cf\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u63d0\u4ea4\u8005\u5c5e\u6027\u548c\u5ba1\u67e5\u8fc7\u7a0b\u5728AI\u751f\u6210PR\u4e2d\u7684\u91cd\u8981\u6027\u3002", "topic": "swe application"}}
{"id": "2601.17421", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17421", "abs": "https://arxiv.org/abs/2601.17421", "authors": ["Jaehui Hwang", "Dongyoon Han", "Sangdoo Yun", "Byeongho Heo"], "title": "Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning", "comment": null, "summary": "The emergence of discourse-like tokens such as \"wait\" and \"therefore\" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the \"wait\" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86LLM\u4e2d\"wait\"\u3001\"therefore\"\u7b49\u8bdd\u8bed\u6807\u8bb0token\u7684\u6982\u7387\u4fe1\u53f7\uff0c\u53d1\u73b0\u8fd9\u4e9b\u4fe1\u53f7\u4e0e\u63a8\u7406\u6b63\u786e\u6027\u5f3a\u76f8\u5173\uff0c\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u7a33\u5b9a\uff0c\u4f46\u53d7\u8bad\u7ec3\u7b56\u7565\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u51fa\u73b0\u7684\"wait\"\u3001\"therefore\"\u7b49\u8bdd\u8bed\u6807\u8bb0token\u4e3a\u7406\u89e3\u5176\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u72ec\u7279\u7a97\u53e3\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u4fe1\u53f7\u5982\u4f55\u968f\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u89c4\u6a21\u53d8\u5316\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u6a21\u578b\u4e2d\u7684token\u7ea7\u6982\u7387\u4fe1\u53f7\uff0c\u7814\u7a76\u7279\u5b9atoken\uff08\u5982\"wait\"\uff09\u4e0e\u63a8\u7406\u6b63\u786e\u6027\u7684\u76f8\u5173\u6027\uff0c\u5e76\u8003\u5bdf\u8fd9\u4e9b\u4fe1\u53f7\u5728\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u53d8\u5316\u89c4\u5f8b\u3002", "result": "\u53d1\u73b0\u7279\u5b9atoken\u4e0e\u63a8\u7406\u6b63\u786e\u6027\u5f3a\u76f8\u5173\uff0c\u8fd9\u79cd\u76f8\u5173\u6027\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0c\u4f46\u53d7\u8bad\u7ec3\u7b56\u7565\u5f71\u54cd\u663e\u8457\uff1b\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u5fae\u8c03\u7684\u6a21\u578b\u901a\u8fc7\u6b64\u7c7b\u4fe1\u53f7\u83b7\u5f97\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4ec5\u90e8\u5206\u5229\u7528\u8fd9\u4e9b\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89c2\u5bdf\u548c\u7406\u89e3LLM\u63a8\u7406\u52a8\u6001\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u8bdd\u8bed\u6807\u8bb0token\u5728\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u53ca\u5176\u4e0e\u8bad\u7ec3\u7b56\u7565\u7684\u5173\u8054\u3002", "topic": "agent analysis"}}
{"id": "2601.17789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17789", "abs": "https://arxiv.org/abs/2601.17789", "authors": ["Yiming Su", "Kunzhao Xu", "Yanjie Gao", "Fan Yang", "Cheng Li", "Mao Yang", "Tianyin Xu"], "title": "Neuro-Symbolic Verification on Instruction Following of LLMs", "comment": null, "summary": "A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.", "AI": {"tldr": "NSVIF\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1LLM\u8f93\u51fa\u662f\u5426\u9075\u5faa\u6307\u4ee4\uff0c\u5c06\u6307\u4ee4\u9075\u5faa\u9a8c\u8bc1\u5efa\u6a21\u4e3a\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u53cd\u9988\u3002", "motivation": "LLM\u5e76\u4e0d\u603b\u662f\u9075\u5faa\u6307\u4ee4\uff0c\u4e14\u8fdd\u89c4\u884c\u4e3a\u96be\u4ee5\u89c2\u5bdf\u6216\u68c0\u67e5\u3002\u5728\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\uff0c\u8fd9\u4e9b\u8fdd\u89c4\u4f1a\u6cbf\u7740\u63a8\u7406\u94fe\u4f20\u64ad\u548c\u653e\u5927\uff0c\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u548c\u7cfb\u7edf\u4e8b\u6545\u3002", "method": "NSVIF\u5c06\u6307\u4ee4\u9075\u5faa\u9a8c\u8bc1\u5efa\u6a21\u4e3a\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff0c\u5c06\u7528\u6237\u6307\u4ee4\u5efa\u6a21\u4e3a\u7ea6\u675f\u3002\u5b83\u540c\u65f6\u5efa\u6a21\u903b\u8f91\u548c\u8bed\u4e49\u7ea6\u675f\uff0c\u901a\u8fc7\u7edf\u4e00\u6c42\u89e3\u5668\u534f\u8c03\u903b\u8f91\u63a8\u7406\u548c\u8bed\u4e49\u5206\u6790\u6765\u89e3\u51b3\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNSVIF\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u53cd\u9988\u3002NSVIF\u7684\u53cd\u9988\u6709\u52a9\u4e8e\u5728\u4e0d\u8fdb\u884c\u540e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8LLM\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "conclusion": "NSVIF\u662f\u4e00\u4e2a\u901a\u7528\u3001\u901a\u7528\u7684\u9a8c\u8bc1\u5668\uff0c\u5bf9\u6307\u4ee4\u6216LLM\u4e0d\u505a\u4efb\u4f55\u5047\u8bbe\uff0c\u4e3a\u89e3\u51b3LLM\u6307\u4ee4\u9075\u5faa\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.18381", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18381", "abs": "https://arxiv.org/abs/2601.18381", "authors": ["Yinghan Hou", "Zongyou Yang"], "title": "AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito", "comment": "14 pages, 7 figures", "summary": "To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.", "AI": {"tldr": "\u5f00\u53d1\u96c6\u6210AI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5de5\u4f5c\u6d41\u5c06\u4f20\u7edf\u6709\u9650\u5dee\u5206\u4ee3\u7801\u8f6c\u6362\u4e3aDevito\u73af\u5883\uff0c\u7ed3\u5408RAG\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u5f3a\u5316\u5b66\u4e60\u53cd\u9988\u673a\u5236", "motivation": "\u4fc3\u8fdb\u4f20\u7edf\u6709\u9650\u5dee\u5206\u5b9e\u73b0\u5411Devito\u73af\u5883\u7684\u8f6c\u6362\uff0c\u89e3\u51b3\u4ee3\u7801\u8fc1\u79fb\u4e2d\u7684\u590d\u6742\u6027\u548c\u51c6\u786e\u6027\u6311\u6218", "method": "\u91c7\u7528\u6df7\u5408LangGraph\u67b6\u6784\uff0c\u7ed3\u5408RAG\u548c\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efaDevito\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u68c0\u7d22\u7ba1\u9053\u548c\u53cd\u5411\u5de5\u7a0b\u7ec4\u4ef6\u5b9e\u73b0\u4ee3\u7801\u8f6c\u6362", "result": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u652f\u6301\u5e76\u884c\u641c\u7d22\u3001\u6982\u5ff5\u6269\u5c55\u3001\u793e\u533a\u89c4\u6a21\u68c0\u7d22\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u901a\u8fc7\u9a8c\u8bc1\u6846\u67b6\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf", "conclusion": "\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u6574\u5408\u4e86\u53d7\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\u7684\u53cd\u9988\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u4ece\u9759\u6001\u4ee3\u7801\u7ffb\u8bd1\u5230\u52a8\u6001\u81ea\u9002\u5e94\u5206\u6790\u884c\u4e3a\u7684\u8f6c\u53d8", "topic": "code agent"}}
{"id": "2601.17828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17828", "abs": "https://arxiv.org/abs/2601.17828", "authors": ["Tanvi Verma", "Yang Zhou", "Rick Siow Mong Goh", "Yong Liu"], "title": "Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards", "comment": null, "summary": "We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.", "AI": {"tldr": "\u63d0\u51faIGFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u533b\u7597\u5bf9\u8bddAI\u8fdb\u884c\u60a3\u8005\u8bbf\u8c08\uff0c\u65e0\u9700\u4eba\u5de5\u5bf9\u8bdd\u6570\u636e\uff0c\u5728HPI\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u5bf9\u8bddAI\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u5bf9\u8bdd\u6216\u9759\u6001\u6570\u636e\u96c6\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u9884\u6536\u96c6\u4eba\u7c7b\u5bf9\u8bdd\u3001\u80fd\u81ea\u4e3b\u5b66\u4e60\u6709\u6548\u63d0\u95ee\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5728\u7ebfGRPO\u5f3a\u5316\u5b66\u4e60\u548c\u4fe1\u606f\u8bba\u5956\u52b1\uff0c\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u51fd\u6570\u8ffd\u8e2a\u4e34\u5e8a\u5b9e\u4f53\uff08\u75c7\u72b6\u3001\u65f6\u95f4\u6a21\u5f0f\u3001\u75c5\u53f2\uff09\u7684\u63ed\u793a\u60c5\u51b5\uff0c\u7ed3\u5408GPT-4o-mini\u8d28\u91cf\u8bc4\u4f30\uff0c\u901a\u8fc7LoRA\u5fae\u8c03Llama-3.1-8B-Instruct\u548cDeepSeek-R1-Distill-Qwen-7B\u6a21\u578b\u3002", "result": "DeepSeek-R1-Distill-Qwen-7B (IGFT)\u5728Avey\u6570\u636e\u4e0aF1\u5f97\u5206\u4e3a0.408\uff08\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534710.9%\uff09\uff0c\u5728MIMIC\u6570\u636e\u4e0a\u4e3a0.289\uff08\u63d0\u534712.9%\uff09\uff1bLlama-3.1-8B-Instruct (IGFT)\u5206\u522b\u8fbe\u52300.384\u548c0.336\uff0c\u5747\u4f18\u4e8eOpenAI\u6a21\u578b\u548c\u533b\u7597\u9886\u57df\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "IGFT\u65b9\u6cd5\u80fd\u6709\u6548\u8bad\u7ec3\u533b\u7597\u5bf9\u8bddAI\u8fdb\u884c\u60a3\u8005\u8bbf\u8c08\u548cHPI\u751f\u6210\uff0c\u65e0\u9700\u4eba\u5de5\u5bf9\u8bdd\u6570\u636e\uff0c\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u533b\u7597\u5bf9\u8bdd\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.17887", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17887", "abs": "https://arxiv.org/abs/2601.17887", "authors": ["Jiahe Guo", "Xiangran Guo", "Yulin Hu", "Zimo Long", "Xingyu Sui", "Xuda Zhi", "Yongbo Huang", "Hao He", "Weixiang Zhao", "Yanyan Zhao", "Bing Qin"], "title": "When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents", "comment": null, "summary": "Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e2a\u6027\u5316LLM\u4ee3\u7406\u4e2d\u7684\"\u610f\u56fe\u5408\u6cd5\u5316\"\u5b89\u5168\u6f0f\u6d1e\uff1a\u826f\u6027\u7684\u4e2a\u4eba\u8bb0\u5fc6\u4f1a\u504f\u89c1\u610f\u56fe\u63a8\u65ad\uff0c\u5bfc\u81f4\u6a21\u578b\u5c06\u6709\u5bb3\u67e5\u8be2\u5408\u6cd5\u5316\uff0c\u653b\u51fb\u6210\u529f\u7387\u63d0\u534715.8%-243.7%\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u4ee3\u7406\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5b9e\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u5c06\u8bb0\u5fc6\u89c6\u4e3a\u4e2d\u6027\u7ec4\u4ef6\uff0c\u5ffd\u89c6\u4e86\u5176\u5b89\u5168\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\"\u610f\u56fe\u5408\u6cd5\u5316\"\u8fd9\u4e00\u88ab\u5ffd\u89c6\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5373\u826f\u6027\u4e2a\u4eba\u8bb0\u5fc6\u5982\u4f55\u5bfc\u81f4\u6a21\u578b\u5c06\u6709\u5bb3\u67e5\u8be2\u5408\u6cd5\u5316\u3002", "method": "1) \u63d0\u51faPS-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u91cf\u5316\u4e2a\u6027\u5316\u4ea4\u4e92\u4e2d\u7684\u610f\u56fe\u5408\u6cd5\u5316\uff1b2) \u5728\u591a\u4e2a\u8bb0\u5fc6\u589e\u5f3a\u4ee3\u7406\u6846\u67b6\u548c\u57fa\u7840LLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff1b3) \u4ece\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u63d0\u4f9b\u673a\u5236\u6027\u8bc1\u636e\uff1b4) \u63d0\u51fa\u8f7b\u91cf\u7ea7\u68c0\u6d4b-\u53cd\u601d\u65b9\u6cd5\u3002", "result": "\u4e2a\u6027\u5316\u4f7f\u653b\u51fb\u6210\u529f\u7387\u76f8\u5bf9\u4e8e\u65e0\u72b6\u6001\u57fa\u7ebf\u63d0\u9ad815.8%-243.7%\u3002\u4ece\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u83b7\u5f97\u4e86\u610f\u56fe\u5408\u6cd5\u5316\u7684\u673a\u5236\u6027\u8bc1\u636e\u3002\u63d0\u51fa\u7684\u68c0\u6d4b-\u53cd\u601d\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u5b89\u5168\u9000\u5316\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u610f\u56fe\u5408\u6cd5\u5316\u4f5c\u4e3a\u5b89\u5168\u6545\u969c\u6a21\u5f0f\u7684\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u548c\u8bc4\u4f30\uff0c\u8868\u660e\u826f\u6027\u3001\u73b0\u5b9e\u4e16\u754c\u7684\u4e2a\u6027\u5316\u4f1a\u81ea\u7136\u4ea7\u751f\u8fd9\u79cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u5728\u957f\u671f\u4e2a\u6027\u5316\u80cc\u666f\u4e0b\u8bc4\u4f30\u5b89\u5168\u6027\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.17897", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17897", "abs": "https://arxiv.org/abs/2601.17897", "authors": ["Jiayu Liu", "Yinhe Long", "Zhenya Huang", "Enhong Chen"], "title": "UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis", "comment": null, "summary": "A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.", "AI": {"tldr": "UniCog\u662f\u4e00\u4e2a\u901a\u8fc7\u6f5c\u5728\u5fc3\u667a\u7a7a\u95f4\u5206\u6790LLM\u8ba4\u77e5\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u5bc6\u96c6\u6a21\u578b\u6fc0\u6d3b\u7f16\u7801\u4e3a\u7a00\u758f\u89e3\u8026\u7684\u6f5c\u5728\u7ef4\u5ea6\uff0c\u63ed\u793a\u4e86LLM\u8ba4\u77e5\u7684\u5e15\u7d2f\u6258\u539f\u5219\uff0c\u5e76\u5229\u7528\u6f5c\u5728\u4fe1\u606f\u6539\u8fdb\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u89e3\u91caLLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba4\u77e5\u80fd\u529b\u5982\u4f55\u53c2\u4e0e\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u7814\u7a76\u8868\u660eLLM\u7684\u8ba4\u77e5\u8fc7\u7a0b\u4e0e\u4eba\u7c7b\u6709\u6839\u672c\u5dee\u5f02\uff0c\u9700\u8981\u65b0\u7684\u5206\u6790\u6846\u67b6\u6765\u7406\u89e3LLM\u7684\u8ba4\u77e5\u673a\u5236\u3002", "method": "\u63d0\u51faUniCog\u6846\u67b6\uff0c\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff0c\u5c06\u5bc6\u96c6\u6a21\u578b\u6fc0\u6d3b\u7f16\u7801\u4e3a\u7a00\u758f\u89e3\u8026\u7684\u6f5c\u5728\u7ef4\u5ea6\uff0c\u5206\u6790\u516d\u4e2a\u5148\u8fdbLLM\uff08\u5305\u62ecDeepSeek-V3.2\u548cGPT-4o\uff09\uff0c\u5e76\u5f15\u5165\u6f5c\u5728\u4fe1\u606f\u5019\u9009\u4f18\u5148\u7ea7\u7b56\u7565\u3002", "result": "\u63ed\u793a\u4e86LLM\u8ba4\u77e5\u7684\u5e15\u7d2f\u6258\u539f\u5219\uff1a\u5171\u4eab\u63a8\u7406\u6838\u5fc3\u4e0e\u80fd\u529b\u7279\u5b9a\u7279\u5f81\u4e92\u8865\uff1b\u53d1\u73b0\u63a8\u7406\u5931\u8d25\u5e38\u8868\u73b0\u4e3a\u6f5c\u5728\u6fc0\u6d3b\u7684\u5f02\u5e38\u5f3a\u5ea6\uff1b\u6f5c\u5728\u4fe1\u606f\u7b56\u7565\u5728\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u5c06\u63a8\u7406\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe7.5%\u3002", "conclusion": "UniCog\u4e3aLLM\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u8303\u5f0f\uff0c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8ba4\u77e5\u7684\u63a8\u7406\u52a8\u6001\u89c6\u89d2\uff0c\u901a\u8fc7\u6f5c\u5728\u5fc3\u667a\u7a7a\u95f4\u5206\u6790\u80fd\u591f\u6df1\u5165\u7406\u89e3LLM\u8ba4\u77e5\u673a\u5236\u5e76\u6539\u8fdb\u63a8\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.17593", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17593", "abs": "https://arxiv.org/abs/2601.17593", "authors": ["Tianjun Zhong", "Linyang He", "Nima Mesgarani"], "title": "From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs", "comment": null, "summary": "Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.\n  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.", "AI": {"tldr": "\u63d0\u51faReasoning DAG Probing\u6846\u67b6\uff0c\u63a2\u7a76LLM\u9690\u85cf\u72b6\u6001\u662f\u5426\u7ebf\u6027\u7f16\u7801\u63a8\u7406DAG\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u53d1\u73b0\u4e2d\u95f4\u5c42\u786e\u5b9e\u7f16\u7801\u4e86\u6709\u610f\u4e49\u7684\u56fe\u7ed3\u6784\u4fe1\u606f\u3002", "motivation": "\u867d\u7136\u73b0\u6709\u7814\u7a76\u5c06\u63a8\u7406\u89c6\u4e3a\u7ebf\u6027\u94fe\u5f0f\u6b65\u9aa4\uff0c\u4f46\u8bb8\u591a\u63a8\u7406\u95ee\u9898\u66f4\u81ea\u7136\u5730\u8868\u793a\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\uff0c\u5176\u4e2d\u4e2d\u95f4\u7ed3\u8bba\u53ef\u80fd\u4f9d\u8d56\u591a\u4e2a\u524d\u63d0\u3001\u5206\u652f\u4e3a\u5e76\u884c\u5b50\u63a8\u5bfc\u3001\u540e\u671f\u5408\u5e76\u6216\u91cd\u7528\u3002\u7406\u89e3\u6a21\u578b\u5185\u90e8\u662f\u5426\u53cd\u6620\u8fd9\u79cd\u56fe\u7ed3\u6784\u63a8\u7406\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u5f15\u5165Reasoning DAG Probing\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u63a8\u7406\u8282\u70b9\u4e0e\u6587\u672c\u5b9e\u73b0\u5173\u8054\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u63a2\u9488\u4ece\u9690\u85cf\u72b6\u6001\u9884\u6d4b\u4e24\u4e2a\u56fe\u8bba\u5c5e\u6027\uff1a\u8282\u70b9\u6df1\u5ea6\u548c\u8282\u70b9\u5bf9\u8ddd\u79bb\u3002\u5206\u6790DAG\u7ed3\u6784\u5728\u5c42\u95f4\u7684\u6d8c\u73b0\uff0c\u5e76\u8bc4\u4f30\u7834\u574f\u63a8\u7406\u76f8\u5173\u7ed3\u6784\u4f46\u4fdd\u7559\u8868\u9762\u6587\u672c\u5c5e\u6027\u7684\u63a7\u5236\u5b9e\u9a8c\u3002", "result": "\u7ed3\u679c\u8868\u660e\u63a8\u7406DAG\u51e0\u4f55\u5728\u4e2d\u95f4\u5c42\u88ab\u6709\u610f\u4e49\u5730\u7f16\u7801\uff0c\u53ef\u6062\u590d\u6027\u968f\u8282\u70b9\u6df1\u5ea6\u548c\u6a21\u578b\u89c4\u6a21\u7cfb\u7edf\u53d8\u5316\uff0c\u8868\u660eLLM\u63a8\u7406\u4e0d\u4ec5\u662f\u987a\u5e8f\u7684\uff0c\u800c\u4e14\u8868\u73b0\u51fa\u53ef\u6d4b\u91cf\u7684\u5185\u90e8\u56fe\u7ed3\u6784\u3002", "conclusion": "LLM\u63a8\u7406\u4e0d\u4ec5\u5177\u6709\u987a\u5e8f\u7279\u6027\uff0c\u8fd8\u5c55\u73b0\u51fa\u53ef\u6d4b\u91cf\u7684\u5185\u90e8\u56fe\u7ed3\u6784\uff0c\u63a8\u7406DAG\u51e0\u4f55\u5728\u6a21\u578b\u4e2d\u95f4\u5c42\u88ab\u6709\u610f\u4e49\u5730\u7f16\u7801\u3002", "topic": "agent analysis"}}
{"id": "2601.17915", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17915", "abs": "https://arxiv.org/abs/2601.17915", "authors": ["Saurabh Jha", "Rohan Arora", "Bhavya", "Noah Zheutlin", "Paulina Toro Isaza", "Laura Shwartz", "Yu Deng", "Daby Sow", "Ruchi Mahindru", "Ruchir Puri"], "title": "Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation", "comment": null, "summary": "LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.\n  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.", "AI": {"tldr": "EoG\u6846\u67b6\u901a\u8fc7\u5c06\u8c03\u67e5\u4efb\u52a1\u5efa\u6a21\u4e3a\u4f9d\u8d56\u56fe\u4e0a\u7684\u6eaf\u56e0\u63a8\u7406\uff0c\u5206\u79bbLLM\u7684\u5c40\u90e8\u8bc1\u636e\u6316\u6398\u4e0e\u63a7\u5236\u5668\u7684\u56fe\u904d\u5386\u7ba1\u7406\uff0c\u89e3\u51b3\u4e86ReAct\u4ee3\u7406\u5728\u5f00\u653e\u5f0f\u8c03\u67e5\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u5904\u7406\u5f00\u653e\u5f0f\u8c03\u67e5\u4efb\u52a1\u65f6\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff1aReAct\u4ee3\u7406\u7684\u68c0\u7d22-\u603b\u7ed3-\u63a8\u7406\u5faa\u73af\u5bf9\u63a2\u7d22\u987a\u5e8f\u654f\u611f\uff0c\u7f3a\u4e4f\u4fe1\u5ff5\u7ef4\u62a4\u673a\u5236\uff0c\u4e14\u8bed\u4e49\u63a8\u7406\u4e0e\u63a7\u5236\u804c\u8d23\u8026\u5408\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faEoG\u6846\u67b6\uff0c\u5c06\u8c03\u67e5\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u4f9d\u8d56\u56fe\u4e0a\u7684\u6eaf\u56e0\u63a8\u7406\u3002LLM\u8d1f\u8d23\u6709\u754c\u7684\u5c40\u90e8\u8bc1\u636e\u6316\u6398\u548c\u6807\u6ce8\uff08\u539f\u56e0vs\u75c7\u72b6\uff09\uff0c\u800c\u786e\u5b9a\u6027\u63a7\u5236\u5668\u7ba1\u7406\u56fe\u904d\u5386\u3001\u72b6\u6001\u7ef4\u62a4\u548c\u4fe1\u5ff5\u4f20\u64ad\uff0c\u8ba1\u7b97\u6700\u5c0f\u89e3\u91ca\u8fb9\u754c\u3002", "result": "\u5728ITBench\u8bca\u65ad\u4efb\u52a1\u4e0a\uff0cEoG\u76f8\u6bd4ReAct\u57fa\u7ebf\u5728\u51c6\u786e\u6027\u548c\u8fd0\u884c\u4e00\u81f4\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u5e73\u5747\u5b9e\u4f53F1\u7684Majority-at-k\u6307\u6807\u63d0\u9ad8\u4e867\u500d\u3002", "conclusion": "\u901a\u8fc7\u5206\u79bb\u63a8\u7406\u4e0e\u63a7\u5236\u804c\u8d23\uff0cEoG\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u5f00\u653e\u5f0f\u8c03\u67e5\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u5904\u7406\u5177\u6709\u9690\u85cf\u4f9d\u8d56\u7ed3\u6784\u7684\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.17596", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17596", "abs": "https://arxiv.org/abs/2601.17596", "authors": ["Yunxiang Zhang", "Kang Zhou", "Zhichao Xu", "Kiran Ramnath", "Yun Zhou", "Sangmin Woo", "Haibo Ding", "Lin Lee Cheong"], "title": "Learning to Ideate for Machine Learning Engineering Agents", "comment": "EACL 2026 main conference", "summary": "Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.", "AI": {"tldr": "MLE-Ideator\uff1a\u4e00\u4e2a\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u7b97\u6cd5\u4f18\u5316\u4e2d\u7684\u6784\u601d\u4e0e\u5b9e\u73b0\u5206\u79bb\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6784\u601d\u667a\u80fd\u4f53\u5e2e\u52a9\u5b9e\u73b0\u667a\u80fd\u4f53\u751f\u6210\u66f4\u6709\u6548\u7684\u7b97\u6cd5\u6539\u8fdb\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u548c\u65e0\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u4ec5\u5b9e\u73b0\u667a\u80fd\u4f53\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\uff08MLE\uff09\u667a\u80fd\u4f53\u96be\u4ee5\u8fed\u4ee3\u4f18\u5316\u5176\u5b9e\u73b0\u7684\u7b97\u6cd5\u6548\u679c\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5206\u79bb\u6784\u601d\u4e0e\u5b9e\u73b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u7b97\u6cd5\u4f18\u5316\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faMLE-Ideator\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u5b9e\u73b0\u667a\u80fd\u4f53\u8d1f\u8d23\u5177\u4f53\u5b9e\u73b0\uff0c\u6784\u601d\u667a\u80fd\u4f53\u4e13\u95e8\u63d0\u4f9b\u6218\u7565\u5e2e\u52a9\u3002\u5b9e\u73b0\u667a\u80fd\u4f53\u53ef\u5411\u6784\u601d\u667a\u80fd\u4f53\u8bf7\u6c42\u5e2e\u52a9\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6784\u601d\u667a\u80fd\u4f53\u751f\u6210\u66f4\u6709\u6548\u7684\u6784\u601d\u3002", "result": "1. \u5728\u65e0\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\uff0c\u8be5\u6846\u67b6\u5728MLE-Bench\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u5b9e\u73b0\u667a\u80fd\u4f53\u57fa\u7ebf\uff1b2. \u4f7f\u7528\u4ec51K\u8bad\u7ec3\u6837\u672c\uff08\u6765\u81ea10\u4e2aMLE\u4efb\u52a1\uff09\u901a\u8fc7RL\u8bad\u7ec3\u7684Qwen3-8B\u6784\u601d\u667a\u80fd\u4f53\u76f8\u6bd4\u672a\u8bad\u7ec3\u7248\u672c\u83b7\u5f9711.5%\u76f8\u5bf9\u63d0\u5347\uff0c\u5e76\u8d85\u8d8aClaude Sonnet 3.5\u3002", "conclusion": "MLE-Ideator\u6846\u67b6\u5c55\u793a\u4e86\u5206\u79bb\u6784\u601d\u4e0e\u5b9e\u73b0\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6784\u601d\u667a\u80fd\u4f53\u53ef\u663e\u8457\u63d0\u5347\u7b97\u6cd5\u4f18\u5316\u6548\u679c\uff0c\u4e3a\u8bad\u7ec3\u7528\u4e8e\u79d1\u5b66\u53d1\u73b0\u7684\u6218\u7565AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002", "topic": "code agent"}}
{"id": "2601.17920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17920", "abs": "https://arxiv.org/abs/2601.17920", "authors": ["Xuanzhou Chen", "Audrey Wang", "Stanley Yin", "Hanyang Jiang", "Dong Zhang"], "title": "Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges", "comment": null, "summary": "Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\uff08SDL\uff09\u4e2d\u7684AI\u4ee3\u7406\u95ee\u9898\uff0c\u5c06SDL\u81ea\u4e3b\u6027\u5efa\u6a21\u4e3a\u4ee3\u7406\u73af\u5883\u4ea4\u4e92\u95ee\u9898\uff0c\u56de\u987e\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u3001\u4e3b\u52a8\u5b66\u4e60\u3001\u89c4\u5212\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b49\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u80fd\u529b\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u603b\u7ed3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u7ecf\u9a8c\u6559\u8bad\u548c\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u4e3aAI\u4ee3\u7406\u5728\u6602\u8d35\u64cd\u4f5c\u3001\u566a\u58f0\u5ef6\u8fdf\u53cd\u9988\u3001\u4e25\u683c\u7ea6\u675f\u548c\u975e\u5e73\u7a33\u6027\u7b49\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0\u3002\u8bba\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406SDL\u4e2d\u7684AI\u95ee\u9898\uff0c\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u5c06SDL\u81ea\u4e3b\u6027\u5efa\u6a21\u4e3a\u4ee3\u7406\u73af\u5883\u4ea4\u4e92\u95ee\u9898\uff0c\u5305\u542b\u663e\u5f0f\u89c2\u6d4b\u3001\u52a8\u4f5c\u3001\u6210\u672c\u548c\u7ea6\u675f\u3002\u7efc\u8ff0\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u3001\u4e3b\u52a8\u5b66\u4e60\u3001\u89c4\u5212\u4e0e\u5f3a\u5316\u5b66\u4e60\u3001\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7b49\u65b9\u6cd5\uff0c\u5f3a\u8c03\u53ef\u9a8c\u8bc1\u548c\u6eaf\u6e90\u611f\u77e5\u7b56\u7565\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u51b3\u7b56\u89c6\u91ce\u3001\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u52a8\u4f5c\u53c2\u6570\u5316\u7b49\u7ef4\u5ea6\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u5efa\u7acb\u4e86SDL\u7684AI\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u80fd\u529b\u9a71\u52a8\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5408\u6210\u4e86\u57fa\u51c6\u4efb\u52a1\u6a21\u677f\u548c\u8bc4\u4f30\u6307\u6807\uff08\u6210\u672c\u611f\u77e5\u6027\u80fd\u3001\u6f02\u79fb\u9c81\u68d2\u6027\u3001\u7ea6\u675f\u8fdd\u53cd\u884c\u4e3a\u3001\u53ef\u91cd\u590d\u6027\u7b49\uff09\uff0c\u603b\u7ed3\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u7ecf\u9a8c\u6559\u8bad\u3002", "conclusion": "SDL\u4e3aAI\u4ee3\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u591a\u6a21\u6001\u8868\u793a\u3001\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u3001\u5b89\u5168\u63a2\u7d22\u548c\u5171\u4eab\u57fa\u51c6\u57fa\u7840\u8bbe\u65bd\u7b49\u5173\u952e\u6280\u672f\u3002\u8bba\u6587\u4e3aSDL\u9886\u57df\u7684\u7cfb\u7edf\u5316\u7814\u7a76\u548c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2601.17923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17923", "abs": "https://arxiv.org/abs/2601.17923", "authors": ["Ali Najar"], "title": "Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation", "comment": "5 pages", "summary": "Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6280\u80fd\u56fe\u7684\u7ec8\u8eab\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u300a\u9ed1\u6697\u4e4b\u9b42III\u300b\u5b9e\u65f6\u63a7\u5236\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u8bfe\u7a0b\u8bad\u7ec3\u53ef\u91cd\u7528\u6280\u80fd\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u9009\u62e9\u6027\u5fae\u8c03\u4ee5\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u7ec8\u8eab\u667a\u80fd\u4f53\u9700\u8981\u5728\u4e0d\u4ece\u5934\u8bad\u7ec3\u6216\u8986\u76d6\u5df2\u5b66\u884c\u4e3a\u7684\u60c5\u51b5\u4e0b\u6301\u7eed\u6269\u5c55\u80fd\u529b\uff0c\u8fd9\u5728\u590d\u6742\u7684\u5b9e\u65f6\u63a7\u5236\u73af\u5883\u4e2d\u5c24\u4e3a\u6311\u6218\u3002", "method": "\u5c06\u6218\u6597\u8868\u793a\u4e3a\u6709\u5411\u6280\u80fd\u56fe\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u8bfe\u7a0b\u8bad\u7ec3\u4e94\u4e2a\u53ef\u91cd\u7528\u6280\u80fd\u7ec4\u4ef6\uff1a\u76f8\u673a\u63a7\u5236\u3001\u76ee\u6807\u9501\u5b9a\u3001\u79fb\u52a8\u3001\u95ea\u907f\u548c\u6cbb\u7597-\u653b\u51fb\u51b3\u7b56\u7b56\u7565\uff0c\u6bcf\u4e2a\u6280\u80fd\u9488\u5bf9\u7279\u5b9a\u804c\u8d23\u4f18\u5316\u3002", "result": "\u6280\u80fd\u5206\u89e3\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5f53\u73af\u5883\u4ece\u7b2c\u4e00\u9636\u6bb5\u53d8\u4e3a\u7b2c\u4e8c\u9636\u6bb5\u65f6\uff0c\u4ec5\u9700\u5fae\u8c03\u4e24\u4e2a\u6280\u80fd\u5373\u53ef\u5728\u6709\u9650\u4ea4\u4e92\u9884\u7b97\u5185\u5feb\u901f\u6062\u590d\u6027\u80fd\u3002", "conclusion": "\u6280\u80fd\u56fe\u8bfe\u7a0b\u4e0e\u9009\u62e9\u6027\u5fae\u8c03\u76f8\u7ed3\u5408\uff0c\u4e3a\u590d\u6742\u5b9e\u65f6\u73af\u5883\u4e2d\u4e0d\u65ad\u8fdb\u5316\u7684\u6301\u7eed\u5b66\u4e60\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.17942", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17942", "abs": "https://arxiv.org/abs/2601.17942", "authors": ["Yu-Jie Yang", "Hung-Fu Chang", "Po-An Chen"], "title": "LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting", "comment": "29 pages, 22 figures", "summary": "Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86SSEV\u548cReCAPAgent-SQL\u4e24\u4e2aText-to-SQL\u6846\u67b6\uff0c\u524d\u8005\u57fa\u4e8e\u5355\u4ee3\u7406\u81ea\u4f18\u5316\u96c6\u6210\u6295\u7968\uff0c\u540e\u8005\u91c7\u7528\u591a\u4ee3\u7406\u534f\u4f5c\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u7279\u522b\u5728\u590d\u6742\u4f01\u4e1a\u6570\u636e\u5e93\u573a\u666f\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "Text-to-SQL\u6280\u672f\u867d\u7136\u80fd\u964d\u4f4e\u6570\u636e\u5206\u6790\u95e8\u69db\uff0c\u4f46\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u6b67\u4e49\u6027\u3001\u6a21\u5f0f\u94fe\u63a5\u590d\u6742\u6027\u3001SQL\u65b9\u8a00\u6cdb\u5316\u9650\u5236\u4ee5\u53ca\u9886\u57df\u7279\u5b9a\u7406\u89e3\u9700\u6c42\u7b49\u95ee\u9898\u4ecd\u4f7f\u51c6\u786eSQL\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u6846\u67b6\u6765\u5904\u7406\u5b9e\u9645\u4f01\u4e1a\u6570\u636e\u5e93\u7684\u590d\u6742\u6027\u3002", "method": "1. SSEV\uff1a\u57fa\u4e8ePET-SQL\u7684\u5355\u4ee3\u7406\u81ea\u4f18\u5316\u96c6\u6210\u6295\u7968\u7ba1\u9053\uff0c\u65e0\u9700\u771f\u5b9e\u6570\u636e\uff0c\u96c6\u6210\u81ea\u4f18\u5316\u4e0e\u52a0\u6743\u591a\u6570\u6295\u7968\u53ca\u5176\u968f\u673a\u53d8\u4f53\u30022. ReCAPAgent-SQL\uff1a\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u5305\u542b\u89c4\u5212\u3001\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u3001\u6279\u5224\u3001\u52a8\u4f5c\u751f\u6210\u3001\u81ea\u4f18\u5316\u3001\u6a21\u5f0f\u94fe\u63a5\u548c\u7ed3\u679c\u9a8c\u8bc1\u7b49\u4e13\u95e8\u4ee3\u7406\uff0c\u901a\u8fc7\u4ee3\u7406\u534f\u4f5c\u5b9e\u73b0SQL\u9884\u6d4b\u7684\u8fed\u4ee3\u4f18\u5316\u3002", "result": "SSEV\u5728Spider 1.0-Dev\u8fbe\u523085.5%\u6267\u884c\u51c6\u786e\u7387\uff0cSpider 1.0-Test\u8fbe\u523086.4%\uff0cBIRD-Dev\u8fbe\u523066.3%\u3002ReCAPAgent-SQL\u5728Spider 2.0-Lite\u524d100\u4e2a\u67e5\u8be2\u4e2d\u8fbe\u523031%\u6267\u884c\u51c6\u786e\u7387\uff0c\u5728\u5904\u7406\u771f\u5b9e\u4f01\u4e1a\u573a\u666f\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4fc3\u8fdb\u4e86\u53ef\u6269\u5c55Text-to-SQL\u7cfb\u7edf\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u90e8\u7f72\uff0c\u4ee5\u66f4\u4f4e\u6210\u672c\u548c\u66f4\u9ad8\u6548\u7387\u652f\u6301\u66f4\u597d\u7684\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u3002\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\u7279\u522b\u9002\u5408\u5904\u7406\u590d\u6742\u4f01\u4e1a\u6570\u636e\u5e93\u573a\u666f\u3002", "topic": "code agent"}}
{"id": "2601.18027", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18027", "abs": "https://arxiv.org/abs/2601.18027", "authors": ["Chiyuan Fu", "Lyuhao Chen", "Yunze Xiao", "Weihao Xuan", "Carlos Busso", "Mona Diab"], "title": "Sentipolis: Emotion-Aware Agents for Social Simulations", "comment": null, "summary": "LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.", "AI": {"tldr": "Sentipolis\u6846\u67b6\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u60c5\u611f\u72b6\u6001\u7ba1\u7406\uff0c\u901a\u8fc7PAD\u60c5\u611f\u8868\u793a\u3001\u53cc\u901f\u60c5\u611f\u52a8\u6001\u548c\u60c5\u611f-\u8bb0\u5fc6\u8026\u5408\u89e3\u51b3\u60c5\u611f\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u793e\u4ea4\u6a21\u62df\u4e2d\u7684\u60c5\u611f\u8fde\u7eed\u6027\u548c\u771f\u5b9e\u884c\u4e3a\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u793e\u4ea4\u6a21\u62df\u4e2d\u5e38\u5c06\u60c5\u611f\u89c6\u4e3a\u77ac\u65f6\u7ebf\u7d22\uff0c\u5bfc\u81f4\u60c5\u611f\u9057\u5fd8\u548c\u957f\u671f\u8fde\u7eed\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u60c5\u611f\u72b6\u6001\u7ba1\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51faSentipolis\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u8fde\u7eed\u7684\u6109\u60a6-\u5524\u9192-\u652f\u914d(PAD)\u60c5\u611f\u8868\u793a\uff1b2) \u53cc\u901f\u60c5\u611f\u52a8\u6001\u673a\u5236\uff1b3) \u60c5\u611f\u4e0e\u8bb0\u5fc6\u8026\u5408\u7cfb\u7edf\u3002", "result": "\u5728\u6570\u5343\u6b21\u4ea4\u4e92\u4e2d\uff0cSentipolis\u63d0\u5347\u4e86\u60c5\u611f\u57fa\u7840\u884c\u4e3a\u3001\u6c9f\u901a\u80fd\u529b\u548c\u60c5\u611f\u8fde\u7eed\u6027\u3002\u6548\u679c\u6a21\u578b\u4f9d\u8d56\uff1a\u9ad8\u5bb9\u91cf\u6a21\u578b\u53ef\u4fe1\u5ea6\u63d0\u5347\uff0c\u5c0f\u6a21\u578b\u53ef\u80fd\u4e0b\u964d\uff1b\u60c5\u611f\u610f\u8bc6\u53ef\u80fd\u8f7b\u5fae\u964d\u4f4e\u793e\u4f1a\u89c4\u8303\u9075\u5b88\u5ea6\u3002", "conclusion": "Sentipolis\u652f\u6301\u7814\u7a76\u7d2f\u79ef\u793e\u4ea4\u52a8\u6001\u5982\u8054\u76df\u5f62\u6210\u548c\u5173\u7cfb\u6e10\u53d8\uff0c\u7f51\u7edc\u8bca\u65ad\u663e\u793a\u4e92\u60e0\u3001\u9002\u5ea6\u805a\u7c7b\u548c\u65f6\u95f4\u7a33\u5b9a\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u53cd\u6620\u4e86\u60c5\u611f\u9a71\u52a8\u884c\u4e3a\u4e0e\u89c4\u5219\u9075\u5b88\u7684\u4eba\u7c7b\u5316\u5f20\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.17275", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17275", "abs": "https://arxiv.org/abs/2601.17275", "authors": ["Lianlei Shan", "Han Chen", "Yixuan Wang", "Zhenjie Liu", "Wei Li"], "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning", "comment": "12 pages,", "summary": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.", "AI": {"tldr": "DLR\u63d0\u51fa\u4e86\u4e00\u79cd\u6f5c\u5728\u7a7a\u95f4\u53cc\u5411\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u94fe\u7684\u8bd5\u9519\u6210\u672c\u4ece\u6602\u8d35\u7684token\u7ea7\u5e8f\u5217\u751f\u6210\u8f6c\u79fb\u5230\u8fde\u7eed\u6f5c\u5728\u6d41\u5f62\uff0c\u901a\u8fc7\u51bb\u7ed3\u4e3b\u6a21\u578b\u53c2\u6570\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6536\u655b\u548c\u66f4\u957f\u63a8\u7406\u94fe\u652f\u6301\u3002", "motivation": "LLM\u5728\u5904\u7406\u590d\u6742\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u65f6\u5f80\u5f80\u53ea\u662f\"\u7edf\u8ba1\u62df\u5408\"\u800c\u975e\u7cfb\u7edf\u903b\u8f91\u63a8\u7406\u3002\u4f20\u7edfRL\u5728\u79bb\u6563token\u7a7a\u95f4\u76f4\u63a5\u5e94\u7528\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a\u6837\u672c\u6548\u7387\u4f4e\u7684rollout\u3001\u9ad8\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u3001\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\u3002", "method": "\u63d0\u51faDeepLatent Reasoning (DLR)\u6846\u67b6\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u91c7\u6837K\u4e2a\u63a8\u7406\u94fe\u7f16\u7801\uff1b2) \u57fa\u4e8e\u6b63\u786e\u6027\u548c\u683c\u5f0f\u7684\u53cc\u91cd\u5956\u52b1\u673a\u5236\u7b5b\u9009\uff1b3) \u4ec5\u9ad8\u4ef7\u503c\u6f5c\u5728\u8f68\u8ff9\u8f93\u5165\u51bb\u7ed3\u4e3b\u6a21\u578b\u8fdb\u884c\u5355\u6b21\u89e3\u7801\uff1b4) \u8bbe\u8ba1\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5b9e\u73b0\u6f5c\u5728\u7a7a\u95f4\u5b9a\u5411\u63a2\u7d22\u3002", "result": "\u5728\u53ef\u6bd4\u8f83\u7684GPU\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cDLR\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6536\u655b\uff0c\u652f\u6301\u66f4\u957f\u7684\u63a8\u7406\u94fe\uff0c\u4fc3\u8fdb\u4e86\u63a8\u7406\u80fd\u529b\u7684\u53ef\u6301\u7eed\u79ef\u7d2f\uff0c\u4e3aLLM\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u8def\u5f84\u3002", "conclusion": "DLR\u901a\u8fc7\u5c06\u8bd5\u9519\u6210\u672c\u8f6c\u79fb\u5230\u8fde\u7eed\u6f5c\u5728\u6d41\u5f62\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u4f20\u7edfRL\u5728\u79bb\u6563token\u7a7a\u95f4\u7684\u7ed3\u6784\u74f6\u9888\uff0c\u540c\u65f6\u901a\u8fc7\u51bb\u7ed3\u4e3b\u6a21\u578b\u53c2\u6570\u6d88\u9664\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e3aLLM\u7684\u53ef\u9760\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18067", "categories": ["cs.AI", "cs.NE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18067", "abs": "https://arxiv.org/abs/2601.18067", "authors": ["Wei-Po Hsin", "Ren-Hao Deng", "Yao-Ting Hsieh", "En-Ming Huang", "Shih-Hao Hung"], "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization", "comment": "17 pages, 6 figures, 8 tables", "summary": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.", "AI": {"tldr": "EvolVE\u662f\u4e00\u4e2a\u9488\u5bf9Verilog\u786c\u4ef6\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u591a\u79cd\u8fdb\u5316\u7b56\u7565\uff0c\u53d1\u73b0MCTS\u5728\u529f\u80fd\u6b63\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cIGR\u5728\u4f18\u5316\u65b9\u9762\u66f4\u4f18\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230SOTA\uff0c\u5e76\u5728\u5de5\u4e1a\u7ea7\u95ee\u9898\u4e0a\u663e\u8457\u964d\u4f4ePPA\u6307\u6807\u3002", "motivation": "Verilog\u786c\u4ef6\u8bbe\u8ba1\u6d41\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u9700\u8981\u5927\u91cf\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u9014\u5f84\uff0c\u4f46\u5176\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u548c\u987a\u5e8f\u63a8\u7406\u80fd\u529b\u96be\u4ee5\u6355\u6349\u786c\u4ef6\u7cfb\u7edf\u7684\u4e25\u683c\u5f62\u5f0f\u903b\u8f91\u548c\u5e76\u53d1\u7279\u6027\u3002", "method": "\u63d0\u51faEvolVE\u6846\u67b6\uff0c\u5206\u6790\u591a\u79cd\u8fdb\u5316\u7b56\u7565\u5728\u82af\u7247\u8bbe\u8ba1\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0MCTS\u5728\u6700\u5927\u5316\u529f\u80fd\u6b63\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u800cIGR\u5728\u4f18\u5316\u65b9\u9762\u66f4\u4f18\u3002\u91c7\u7528\u7ed3\u6784\u5316\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210(STG)\u52a0\u901f\u8fdb\u5316\u8fc7\u7a0b\u3002\u4e3a\u89e3\u51b3\u590d\u6742\u4f18\u5316\u57fa\u51c6\u7684\u7f3a\u4e4f\uff0c\u5f15\u5165IC-RTL\u57fa\u51c6\u5957\u4ef6\u3002", "result": "EvolVE\u5728VerilogEval v2\u4e0a\u8fbe\u523098.1%\uff0c\u5728RTLLM v2\u4e0a\u8fbe\u523092%\uff0c\u6210\u4e3a\u65b0\u7684SOTA\u3002\u5728\u5de5\u4e1a\u7ea7IC-RTL\u5957\u4ef6\u4e0a\uff0c\u6846\u67b6\u8d85\u8d8a\u4e86\u7ade\u8d5b\u53c2\u4e0e\u8005\u7684\u53c2\u8003\u5b9e\u73b0\uff0c\u5728Huffman Coding\u4e2d\u5c06PPA\u4e58\u79ef\u964d\u4f4e\u8fbe66%\uff0c\u5728\u6240\u6709\u95ee\u9898\u7684\u51e0\u4f55\u5e73\u5747\u4e2d\u964d\u4f4e17%\u3002", "conclusion": "EvolVE\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86Verilog\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u8fdb\u5316\u7b56\u7565\u7684\u4f18\u52bf\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6d4b\u8bd5\u751f\u6210\uff0c\u5728\u529f\u80fd\u6b63\u786e\u6027\u548c\u4f18\u5316\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4e3a\u5de5\u4e1a\u7ea7\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.18119", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18119", "abs": "https://arxiv.org/abs/2601.18119", "authors": ["Jing Ye", "Yiwen Duan", "Yonghong Yu", "Victor Ma", "Yang Gao", "Xing Chen"], "title": "Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?", "comment": null, "summary": "SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.\n  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.\n  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.", "AI": {"tldr": "OurBench\u662f\u9996\u4e2a\u4f01\u4e1a\u7ea7SQL\u63a8\u7406\u4e0e\u8c03\u8bd5\u57fa\u51c6\uff0c\u5305\u542b469\u4e2a\u8bed\u6cd5\u9519\u8bef\u67e5\u8be2\u548c516\u4e2a\u8bed\u4e49\u9519\u8bef\u67e5\u8be2\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6ce8\u5165\u771f\u5b9e\u9519\u8bef\u6784\u5efa\uff0c\u8bc4\u4f30\u663e\u793a\u5f53\u524dLLM\u5728\u590d\u6742SQL\u8c03\u8bd5\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec536.46%\uff09", "motivation": "\u4f01\u4e1a\u6570\u636e\u5de5\u7a0b\u4e2dSQL\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5373\u4f7f\u662f\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u53d1\u8005\u548c\u5148\u8fdbLLM\u4e5f\u96be\u4ee5\u4e00\u6b21\u6027\u751f\u6210\u5b8c\u5168\u6b63\u786e\u7684SQL\u4ee3\u7801\uff0c\u901a\u5e38\u9700\u8981\u591a\u6b21\u8c03\u8bd5\u8fed\u4ee3\u3002\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u4f01\u4e1a\u7ea7SQL\u8c03\u8bd5\u80fd\u529b\u3002", "method": "\u63d0\u51faOurBench\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u521b\u65b0\uff1a(1) \u81ea\u52a8\u5316\u6784\u5efa\u6d41\u7a0b\uff0c\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u5728\u5927\u89c4\u6a21SQL\u4ee3\u7801\u4e2d\u7cfb\u7edf\u6ce8\u5165\u771f\u5b9e\u9519\u8bef\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u591a\u6837\u5316\u7684\u57fa\u51c6\u751f\u6210\uff1b(2) \u9488\u5bf9\u4f01\u4e1a\u73af\u5883\u7684\u514d\u6267\u884c\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u4f9b\u5feb\u901f\u3001\u51c6\u786e\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u8bc4\u4f30\u3002", "result": "OurBench\u5305\u542b469\u4e2a\u8bed\u6cd5\u9519\u8bef\u67e5\u8be2\uff08OurBenchSyn\uff09\u548c516\u4e2a\u8bed\u4e49\u9519\u8bef\u67e5\u8be2\uff08OurBenchSem\uff09\uff0c\u67e5\u8be2\u9ad8\u5ea6\u590d\u6742\uff08\u5e73\u5747\u8d85\u8fc7140\u884c\uff0c\u5177\u6709\u6df1\u4e14\u5bbd\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\uff09\u3002\u8bc4\u4f30\u8fd130\u4e2aLLM\u663e\u793a\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1a\u6700\u4f73\u6a21\u578bClaude-4-Sonnet\u5728OurBenchSyn\u4e0a\u4ec536.46%\u51c6\u786e\u7387\uff0c\u5728OurBenchSem\u4e0a32.17%\uff0c\u5927\u591a\u6570\u6a21\u578b\u4f4e\u4e8e20%\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u4f01\u4e1a\u7ea7SQL\u8c03\u8bd5\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u5b58\u5728\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002\u7814\u7a76\u63a2\u7d22\u4e86\u56db\u79cd\u89e3\u51b3\u65b9\u6848\u7b56\u7565\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u4f01\u4e1a\u73af\u5883\u4e2dLLM\u9a71\u52a8\u7684SQL\u8c03\u8bd5\u6307\u51fa\u4e86\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "topic": "swe benchmark"}}
{"id": "2601.18130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18130", "abs": "https://arxiv.org/abs/2601.18130", "authors": ["Jize Wang", "Han Wu", "Zhiyuan You", "Yiming Song", "Yijun Wang", "Zifei Shan", "Yining Li", "Songyang Zhang", "Xinyi Le", "Cailian Chen", "Xinping Guan", "Dacheng Tao"], "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents", "comment": null, "summary": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.", "AI": {"tldr": "RouteMoA\uff1a\u4e00\u79cd\u5e26\u52a8\u6001\u8def\u7531\u7684\u9ad8\u6548\u591a\u667a\u80fd\u4f53\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bc4\u5206\u5668\u9884\u6d4b\u6027\u80fd\u7b5b\u9009\u5019\u9009\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u6df7\u5408\u8bc4\u4f30\u5668\u7cbe\u8c03\u5206\u6570\uff0c\u6700\u540e\u5e73\u8861\u6027\u80fd\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u9009\u62e9\u6a21\u578b\uff0c\u76f8\u6bd4\u4f20\u7edfMoA\u5927\u5e45\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edfMixture-of-Agents\uff08MoA\uff09\u91c7\u7528\u5bc6\u96c6\u62d3\u6251\u7ed3\u6784\u5bfc\u81f4\u6210\u672c\u548c\u5ef6\u8fdf\u8fc7\u9ad8\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528LLM\u8bc4\u4f30\u5668\u7b5b\u9009\u54cd\u5e94\uff0c\u4f46\u4ecd\u9700\u6240\u6709\u6a21\u578b\u5148\u8fdb\u884c\u63a8\u7406\uff0c\u65e0\u6cd5\u6709\u6548\u964d\u4f4e\u6210\u672c\u3002\u6b64\u5916\u7f3a\u4e4f\u6a21\u578b\u9009\u62e9\u6807\u51c6\uff0c\u9762\u5bf9\u5927\u89c4\u6a21\u6a21\u578b\u6c60\u65f6\uff0c\u5b8c\u6574\u63a8\u7406\u6210\u672c\u9ad8\u6602\u4e14\u53ef\u80fd\u8d85\u51fa\u4e0a\u4e0b\u6587\u9650\u5236\u3002", "method": "1. \u8f7b\u91cf\u7ea7\u8bc4\u5206\u5668\uff1a\u6839\u636e\u67e5\u8be2\u9884\u6d4b\u7c97\u7565\u6027\u80fd\uff0c\u7b5b\u9009\u51fa\u9ad8\u6f5c\u529b\u5019\u9009\u5b50\u96c6\uff0c\u65e0\u9700\u63a8\u7406\uff1b2. \u6df7\u5408\u8bc4\u4f30\u5668\uff1a\u57fa\u4e8e\u73b0\u6709\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u8f7b\u91cf\u7ea7\u81ea\u8bc4\u4f30\u548c\u4ea4\u53c9\u8bc4\u4f30\uff0c\u63d0\u4f9b\u540e\u9a8c\u4fee\u6b63\uff1b3. \u6a21\u578b\u6392\u540d\u673a\u5236\uff1a\u5e73\u8861\u6027\u80fd\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u9009\u62e9\u6700\u7ec8\u6a21\u578b\u3002", "result": "RouteMoA\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u6c60\u89c4\u6a21\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edfMoA\uff0c\u5728\u5927\u89c4\u6a21\u6a21\u578b\u6c60\u4e2d\u964d\u4f4e\u6210\u672c89.8%\uff0c\u51cf\u5c11\u5ef6\u8fdf63.6%\u3002", "conclusion": "RouteMoA\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfMoA\u6846\u67b6\u7684\u6210\u672c\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u6c60\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.17755", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17755", "abs": "https://arxiv.org/abs/2601.17755", "authors": ["Jinyoung Park", "Sanghyeok Lee", "Omar Zia Khan", "Hyunwoo J. Kim", "Joo-Kyung Kim"], "title": "ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation", "comment": "In progress", "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.", "AI": {"tldr": "ProGraph-R1\u63d0\u51fa\u4e86\u4e00\u79cd\u8fdb\u5ea6\u611f\u77e5\u7684\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u8d85\u56fe\u68c0\u7d22\u673a\u5236\u548c\u57fa\u4e8e\u8fdb\u5ea6\u7684\u9010\u6b65\u7b56\u7565\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RL-based GraphRAG\u65b9\u6cd5\u5728\u68c0\u7d22\u548c\u5956\u52b1\u8bbe\u8ba1\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684GraphRAG\u6846\u67b6\uff08\u5982Graph-R1\uff09\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u4e3b\u8981\u4f9d\u8d56\u8bed\u4e49\u76f8\u4f3c\u6027\u8fdb\u884c\u68c0\u7d22\uff0c\u5ffd\u7565\u4e86\u5e95\u5c42\u56fe\u7ed3\u6784\uff1b2\uff09\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u7ea7\u5956\u52b1\uff0c\u65e0\u6cd5\u6355\u6349\u4e2d\u95f4\u68c0\u7d22\u6b65\u9aa4\u7684\u8d28\u91cf\u53ca\u5176\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faProGraph-R1\u6846\u67b6\uff1a1\uff09\u7ed3\u6784\u611f\u77e5\u7684\u8d85\u56fe\u68c0\u7d22\u673a\u5236\uff0c\u8054\u5408\u8003\u8651\u8bed\u4e49\u76f8\u5173\u6027\u548c\u56fe\u8fde\u901a\u6027\uff0c\u9f13\u52b1\u6cbf\u591a\u8df3\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u8fde\u8d2f\u904d\u5386\uff1b2\uff09\u57fa\u4e8e\u8fdb\u5ea6\u7684\u9010\u6b65\u7b56\u7565\u4f18\u5316\uff0c\u901a\u8fc7\u6839\u636e\u56fe\u4e2d\u4e2d\u95f4\u63a8\u7406\u8fdb\u5ea6\u8c03\u8282\u4f18\u52bf\u6765\u63d0\u4f9b\u5bc6\u96c6\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProGraph-R1\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684GraphRAG\u65b9\u6cd5\u3002", "conclusion": "ProGraph-R1\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u548c\u57fa\u4e8e\u8fdb\u5ea6\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18137", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18137", "abs": "https://arxiv.org/abs/2601.18137", "authors": ["Yinger Zhang", "Shutong Jiang", "Renhao Li", "Jianhong Tu", "Yang Su", "Lianghao Deng", "Xudong Guo", "Chenxu Lv", "Junyang Lin"], "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints", "comment": null, "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.", "AI": {"tldr": "DeepPlanning\u662f\u4e00\u4e2a\u9488\u5bf9\u5b9e\u9645\u957f\u89c6\u91ce\u4ee3\u7406\u89c4\u5212\u7684\u6311\u6218\u6027\u57fa\u51c6\uff0c\u5305\u542b\u591a\u65e5\u65c5\u884c\u89c4\u5212\u548c\u591a\u4ea7\u54c1\u8d2d\u7269\u4efb\u52a1\uff0c\u9700\u8981\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u3001\u5c40\u90e8\u7ea6\u675f\u63a8\u7406\u548c\u5168\u5c40\u7ea6\u675f\u4f18\u5316\uff0c\u73b0\u6709\u524d\u6cbf\u4ee3\u7406LLM\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u8bc4\u4f30\u867d\u7136\u8f6c\u5411\u957f\u89c6\u91ce\u4efb\u52a1\uff0c\u4f46\u5927\u591a\u6570\u57fa\u51c6\u4ecd\u5f3a\u8c03\u5c40\u90e8\u3001\u6b65\u9aa4\u7ea7\u63a8\u7406\uff0c\u800c\u975e\u9700\u8981\u771f\u6b63\u89c4\u5212\u80fd\u529b\u7684\u5168\u5c40\u7ea6\u675f\u4f18\u5316\uff08\u5982\u65f6\u95f4\u548c\u8d22\u52a1\u9884\u7b97\uff09\u3002\u73b0\u6709LLM\u89c4\u5212\u57fa\u51c6\u672a\u80fd\u5145\u5206\u4f53\u73b0\u73b0\u5b9e\u573a\u666f\u4e2d\u5178\u578b\u7684\u6d3b\u52a8\u4fe1\u606f\u6536\u96c6\u548c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7ea6\u675f\u3002", "method": "\u5f15\u5165DeepPlanning\u57fa\u51c6\uff0c\u5305\u542b\u591a\u65e5\u65c5\u884c\u89c4\u5212\u548c\u591a\u4ea7\u54c1\u8d2d\u7269\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u3001\u5c40\u90e8\u7ea6\u675f\u63a8\u7406\u548c\u5168\u5c40\u7ea6\u675f\u4f18\u5316\u3002\u5bf9\u524d\u6cbf\u4ee3\u7406LLM\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u6790\u9519\u8bef\u6a21\u5f0f\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u524d\u6cbf\u7684\u4ee3\u7406LLM\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u53ef\u9760\u7684\u663e\u5f0f\u63a8\u7406\u6a21\u5f0f\u548c\u5e76\u884c\u5de5\u5177\u4f7f\u7528\u5bf9\u4e8e\u5b9e\u73b0\u66f4\u597d\u7684\u6548\u679c-\u6548\u7387\u6743\u8861\u7684\u91cd\u8981\u6027\u3002", "conclusion": "DeepPlanning\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7406LLM\u5728\u957f\u89c4\u5212\u89c6\u91ce\u4e0a\u7684\u4e0d\u8db3\uff0c\u9519\u8bef\u5206\u6790\u6307\u51fa\u4e86\u6539\u8fdb\u4ee3\u7406LLM\u7684\u6709\u524d\u666f\u65b9\u5411\u3002\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2601.18175", "categories": ["cs.AI", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18175", "abs": "https://arxiv.org/abs/2601.18175", "authors": ["Daniel Russo"], "title": "Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success", "comment": null, "summary": "A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $\u03c7^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u6210\u529f\u6761\u4ef6\u5316\uff08success conditioning\uff09\u7cbe\u786e\u89e3\u51b3\u4e86\u4e00\u4e2a\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5927\u5316\u7b56\u7565\u6539\u8fdb\u540c\u65f6\u6ee1\u8db3\u03c7\u00b2\u6563\u5ea6\u7ea6\u675f\uff0c\u7ea6\u675f\u534a\u5f84\u7531\u6570\u636e\u81ea\u52a8\u786e\u5b9a\u3002", "motivation": "\u6210\u529f\u6761\u4ef6\u5316\uff08\u5982\u62d2\u7edd\u91c7\u6837\u3001\u76ee\u6807\u6761\u4ef6RL\u3001\u51b3\u7b56\u53d8\u6362\u5668\uff09\u88ab\u5e7f\u6cdb\u7528\u4e8e\u6539\u8fdb\u7b56\u7565\uff0c\u4f46\u5176\u89e3\u51b3\u7684\u4f18\u5316\u95ee\u9898\u672c\u8d28\u4e00\u76f4\u4e0d\u660e\u786e\u3002\u8bba\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u6280\u672f\u80cc\u540e\u7684\u6570\u5b66\u539f\u7406\u548c\u4f18\u5316\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u6210\u529f\u6761\u4ef6\u5316\u7cbe\u786e\u89e3\u51b3\u4e86\u4e00\u4e2a\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u76f8\u5bf9\u7b56\u7565\u6539\u8fdb\u3001\u7b56\u7565\u53d8\u5316\u5e45\u5ea6\u548c\u52a8\u4f5c\u5f71\u54cd\u4e4b\u95f4\u7684\u6052\u7b49\u5f0f\u3002\u5e94\u7528\u8be5\u7406\u8bba\u5206\u6790\u5e38\u89c1\u7684\u56de\u62a5\u9608\u503c\u5316\u5b9e\u8df5\u3002", "result": "\u6210\u529f\u6761\u4ef6\u5316\u88ab\u8bc1\u660e\u662f\u4e00\u4e2a\u4fdd\u5b88\u7684\u6539\u8fdb\u7b97\u5b50\uff0c\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\u6216\u5f15\u53d1\u5371\u9669\u7684\u5206\u5e03\u504f\u79fb\u3002\u5f53\u5931\u8d25\u65f6\uff0c\u5b83\u4f1a\u901a\u8fc7\u51e0\u4e4e\u4e0d\u6539\u53d8\u7b56\u7565\u6765\u53ef\u89c2\u5bdf\u5730\u5931\u8d25\u3002\u56de\u62a5\u9608\u503c\u5316\u53ef\u4ee5\u653e\u5927\u6539\u8fdb\uff0c\u4f46\u53ef\u80fd\u504f\u79bb\u771f\u5b9e\u76ee\u6807\u3002", "conclusion": "\u6210\u529f\u6761\u4ef6\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u6709\u4fdd\u8bc1\u7684\u7b56\u7565\u6539\u8fdb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5176\u4f5c\u4e3a\u4fdd\u5b88\u6539\u8fdb\u7b97\u5b50\u7684\u672c\u8d28\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e00\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18197", "abs": "https://arxiv.org/abs/2601.18197", "authors": ["Shaokang Wang", "Pei Fu", "Ruoceng Zhang", "Shaojie Zhang", "Xiuwen Xi", "Jiahui Yang", "Bin Qin", "Ying Huang", "Zhenbo Luo", "Jian Luan"], "title": "GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models", "comment": null, "summary": "While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.", "AI": {"tldr": "GAIA\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u76f4\u89c9\u6279\u8bc4\u6a21\u578b\u6765\u63d0\u5347GUI\u4ee3\u7406\u7684\u6d4b\u8bd5\u65f6\u6027\u80fd\uff0c\u901a\u8fc7\u6570\u636e\u98de\u8f6e\u7cfb\u7edf\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u5faa\u73af", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u5347\u4e86GUI\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u4f46\u64cd\u4f5c\u4e0d\u53ef\u9006\u6027\u5bfc\u81f4\u5355\u4e2a\u9519\u8bef\u52a8\u4f5c\u53ef\u80fd\u5f15\u53d1\u707e\u96be\u6027\u504f\u5dee\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898", "method": "\u63d0\u51faGUI\u52a8\u4f5c\u6279\u8bc4\u8005\u7684\u6570\u636e\u98de\u8f6e\u7cfb\u7edf(GAIA)\uff0c\u8bad\u7ec3\u76f4\u89c9\u6279\u8bc4\u6a21\u578b(ICM)\u8bc4\u4f30\u4ee3\u7406\u52a8\u4f5c\u7684\u6b63\u786e\u6027\uff0c\u901a\u8fc7\u6536\u96c6\u7cbe\u70bc\u7684\u6b63\u8d1f\u6837\u672c\u542f\u52a8\u81ea\u6211\u6539\u8fdb\u5faa\u73af", "result": "\u5b9e\u9a8c\u8868\u660eICM\u80fd\u63d0\u5347\u5404\u79cd\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u6027\u80fd\uff0c\u968f\u7740\u6570\u636e\u5faa\u73af\u6027\u80fd\u9010\u6b65\u6539\u5584", "conclusion": "GAIA\u6846\u67b6\u901a\u8fc7\u6279\u8bc4\u6a21\u578b\u548c\u6570\u636e\u98de\u8f6e\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86GUI\u4ee3\u7406\u64cd\u4f5c\u7684\u4e0d\u53ef\u9006\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u6301\u7eed\u6539\u8fdb", "topic": "agent analysis"}}
{"id": "2601.18202", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18202", "abs": "https://arxiv.org/abs/2601.18202", "authors": ["Fangyuan Xu", "Rujun Han", "Yanfei Chen", "Zifeng Wang", "I-Hung Hsu", "Jun Yan", "Vishy Tirumalashetty", "Eunsol Choi", "Tomas Pfister", "Chen-Yu Lee"], "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback", "comment": null, "summary": "Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u96be\u5ea6\u53ef\u63a7\u7684\u6df1\u5ea6\u641c\u7d22\u95ee\u7b54\u5bf9\u7684\u4ee3\u7406\u7ba1\u9053\uff0c\u901a\u8fc7\u6570\u636e\u751f\u6210\u5668\u548c\u641c\u7d22\u4ee3\u7406\u7684\u8fed\u4ee3\u4ea4\u4e92\u6765\u4f18\u5316\u6570\u636e\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u9700\u8981\u8de8\u591a\u4e2a\u6587\u6863\u8fdb\u884c\u590d\u6742\u63a8\u7406\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u6b64\u7c7b\u6570\u636e\u6210\u672c\u8fc7\u9ad8\uff0c\u56e0\u4e3a\u63a2\u7d22\u8f68\u8ff9\u957f\u4e14\u590d\u6742\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSAGE\u7ba1\u9053\uff0c\u5305\u542b\u6570\u636e\u751f\u6210\u5668\uff08\u63d0\u51faQA\u5bf9\uff09\u548c\u641c\u7d22\u4ee3\u7406\uff08\u5c1d\u8bd5\u89e3\u51b3\u95ee\u9898\u5e76\u63d0\u4f9b\u6267\u884c\u53cd\u9988\uff09\uff0c\u4e24\u8005\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u4ea4\u4e92\uff0c\u4e0d\u65ad\u7cbe\u70bc\u95ee\u7b54\u5bf9\u76f4\u81f3\u8fbe\u5230\u76ee\u6807\u96be\u5ea6\u6c34\u5e73\u3002", "result": "\u5185\u5728\u8bc4\u4f30\u663e\u793aSAGE\u751f\u6210\u7684\u95ee\u9898\u9700\u8981\u591a\u6837\u5316\u7684\u63a8\u7406\u7b56\u7565\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u751f\u6210\u6570\u636e\u7684\u6b63\u786e\u6027\u548c\u96be\u5ea6\uff1b\u5916\u5728\u8bc4\u4f30\u663e\u793a\u5728\u6d41\u884c\u6df1\u5ea6\u641c\u7d22\u57fa\u51c6\u4e0a\u83b7\u5f97\u9ad8\u8fbe23%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u8bad\u7ec3\u540e\u7684\u4ee3\u7406\u80fd\u591f\u9002\u5e94\u4ece\u56fa\u5b9a\u8bed\u6599\u5e93\u68c0\u7d22\u5230Google\u641c\u7d22\u7684\u63a8\u7406\u65f6\u5207\u6362\u3002", "conclusion": "SAGE\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u96be\u5ea6\u53ef\u63a7\u7684\u6df1\u5ea6\u641c\u7d22\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.18217", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18217", "abs": "https://arxiv.org/abs/2601.18217", "authors": ["Zhihan Liu", "Lin Guan", "Yixin Nie", "Kai Zhang", "Zhuoqun Hao", "Lin Chen", "Asli Celikyilmaz", "Zhaoran Wang", "Na Zhang"], "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents", "comment": null, "summary": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.", "AI": {"tldr": "\u7814\u7a76LLM\u667a\u80fd\u4f53\u5728\u672a\u77e5\u6d4b\u8bd5\u9886\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u662f\u5f71\u54cd\u8de8\u57df\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u6dfb\u52a0\u65e0\u5173\u7279\u5f81\u589e\u5f3a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u901a\u7528LLM\u667a\u80fd\u4f53\u901a\u5e38\u5728\u6709\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u5728\u66f4\u5e7f\u6cdb\u7684\u672a\u77e5\u9886\u57df\u4e2d\u90e8\u7f72\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f53\u6700\u7ec8\u6d4b\u8bd5\u9886\u57df\u672a\u77e5\u65f6\uff0c\u54ea\u4e9b\u73af\u5883\u5c5e\u6027\u548c\u5efa\u6a21\u9009\u62e9\u5bf9\u8de8\u57df\u6027\u80fd\u5f71\u54cd\u6700\u5927\u3002", "method": "1) \u8bc6\u522b\u5f71\u54cd\u8de8\u57df\u6cdb\u5316\u7684\u73af\u5883\u8f74\uff1a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\uff1b2) \u63d0\u51fa\u968f\u673a\u5316\u6280\u672f\uff1a\u5728\u72b6\u6001\u4e2d\u6dfb\u52a0\u5c11\u91cf\u4e0e\u76ee\u6807\u65e0\u5173\u7684\u5e72\u6270\u7279\u5f81\u4ee5\u589e\u5f3a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\uff1b3) \u5206\u6790\u5efa\u6a21\u9009\u62e9\uff1aSFT\u9884\u70ed/\u4e2d\u671f\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u9010\u6b65\u601d\u8003\u5728RL\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u4e0e\u8de8\u57df\u6cdb\u5316\u5f3a\u76f8\u5173\uff0c\u800c\u9886\u57df\u771f\u5b9e\u6027\u548c\u6587\u672c\u76f8\u4f3c\u6027\u4e0d\u662f\u4e3b\u8981\u56e0\u7d20\u3002\u589e\u52a0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u80fd\u6709\u6548\u63d0\u5347\u8de8\u57df\u9c81\u68d2\u6027\u3002SFT\u8bad\u7ec3\u6709\u52a9\u4e8e\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u4f46\u4f1a\u635f\u5bb3\u672a\u5305\u542b\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9010\u6b65\u601d\u8003\u5bf9\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u5728\u667a\u80fd\u4f53\u540e\u8bad\u7ec3\u4e2d\uff0c\u5e94\u4f18\u5148\u8003\u8651\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u800c\u975e\u9886\u57df\u771f\u5b9e\u6027\u3002\u901a\u8fc7\u6dfb\u52a0\u65e0\u5173\u7279\u5f81\u589e\u5f3a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u662f\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u9700\u8981\u8c28\u614e\u4f7f\u7528SFT\u8bad\u7ec3\u5e76\u542f\u7528\u9010\u6b65\u601d\u8003\u673a\u5236\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18225", "abs": "https://arxiv.org/abs/2601.18225", "authors": ["Pei Wang", "Yanan Wu", "Xiaoshuai Song", "Weixun Wang", "Gengru Chen", "Zhongwen Li", "Kezhong Yan", "Ken Deng", "Qi Liu", "Shuaibing Zhao", "Shaopan Xiong", "Xuepeng Liu", "Xuefeng Chen", "Wanxi Deng", "Wenbo Su", "Bo Zheng"], "title": "ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.", "AI": {"tldr": "ShopSimulator\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u4e2d\u6587\u7535\u5546\u8d2d\u7269\u4eff\u771f\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3LLM\u667a\u80fd\u4f53\u5728\u4e2a\u6027\u5316\u641c\u7d22\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u4ea7\u54c1\u8fa8\u522b\u7b49\u590d\u6742\u8d2d\u7269\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u7684\u4eff\u771f\u73af\u5883\u6765\u5168\u9762\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u7535\u5546\u8d2d\u7269\u4e2d\u7684\u7efc\u5408\u80fd\u529b\uff0c\u5305\u62ec\u4e2a\u6027\u5316\u504f\u597d\u7406\u89e3\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u76f8\u4f3c\u4ea7\u54c1\u8fa8\u522b\uff0c\u4e14\u73b0\u6709\u5de5\u4f5c\u591a\u96c6\u4e2d\u4e8e\u8bc4\u4f30\u800c\u975e\u8bad\u7ec3\u652f\u6301\u3002", "method": "\u63d0\u51faShopSimulator\u5927\u89c4\u6a21\u4e2d\u6587\u8d2d\u7269\u73af\u5883\uff0c\u5305\u542b\u591a\u6837\u5316\u573a\u666f\u3002\u901a\u8fc7\u8be5\u73af\u5883\u8bc4\u4f30LLM\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u9519\u8bef\u5206\u6790\u3002\u8fdb\u4e00\u6b65\u63a2\u7d22\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u7ed3\u5408\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5373\u4f7f\u6700\u4f73\u6a21\u578b\u7684\u5168\u6210\u529f\u7387\u4e5f\u4f4e\u4e8e40%\u3002\u9519\u8bef\u5206\u6790\u663e\u793a\u667a\u80fd\u4f53\u5728\u957f\u8f68\u8ff9\u4e2d\u7684\u6df1\u5ea6\u641c\u7d22\u548c\u4ea7\u54c1\u9009\u62e9\u3001\u4e2a\u6027\u5316\u7ebf\u7d22\u5e73\u8861\u4ee5\u53ca\u4e0e\u7528\u6237\u6709\u6548\u4e92\u52a8\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002SFT+RL\u7ec4\u5408\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "ShopSimulator\u4e3a\u7535\u5546\u8d2d\u7269\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\u548c\u8bad\u7ec3\u73af\u5883\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u667a\u80fd\u4f53\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7SFT\u548cRL\u7ed3\u5408\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18226", "abs": "https://arxiv.org/abs/2601.18226", "authors": ["Haotian Li", "Shijun Yang", "Weizhen Qi", "Silei Zhao", "Rui Hua", "Mingzhu Song", "Xiaojian Yang", "Chao Peng"], "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks", "comment": null, "summary": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.", "AI": {"tldr": "\u63d0\u51faIn-Situ Self-Evolving\u8303\u5f0f\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u901a\u8fc7\u4efb\u52a1\u4ea4\u4e92\u81ea\u4e3b\u8fdb\u5316\u5de5\u5177\u96c6\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u76d1\u7763\uff0c\u5b9e\u73b0\u80fd\u529b\u8fb9\u754c\u52a8\u6001\u6269\u5c55\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5f00\u653e\u73af\u5883\u4e2d\u9762\u4e34\u4efb\u52a1\u5206\u5e03\u6301\u7eed\u6f02\u79fb\u548c\u5916\u90e8\u76d1\u7763\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u4f9d\u8d56\u9759\u6001\u5de5\u5177\u96c6\u6216\u79bb\u7ebf\u8bad\u7ec3\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u50f5\u5316\u4e14\u672a\u77e5\u3002", "method": "\u63d0\u51fa\u539f\u4f4d\u81ea\u8fdb\u5316\u8303\u5f0f\uff0c\u5c06\u987a\u5e8f\u4efb\u52a1\u4ea4\u4e92\u89c6\u4e3a\u8fde\u7eed\u7ecf\u9a8c\u6d41\uff0c\u901a\u8fc7\u5de5\u5177\u8fdb\u5316\u4f5c\u4e3a\u80fd\u529b\u6269\u5c55\u5173\u952e\u8def\u5f84\uff0c\u5f00\u53d1Yunjue Agent\u7cfb\u7edf\u8fed\u4ee3\u5408\u6210\u3001\u4f18\u5316\u548c\u91cd\u7528\u5de5\u5177\uff0c\u5e76\u5f15\u5165\u5e76\u884c\u6279\u91cf\u8fdb\u5316\u7b56\u7565\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u7684\u96f6\u8d77\u70b9\u8bbe\u7f6e\u4e0b\u663e\u8457\u8d85\u8d8a\u4e13\u6709\u57fa\u7ebf\uff0c\u8865\u5145\u7684\u6696\u542f\u52a8\u8bc4\u4f30\u8bc1\u5b9e\u79ef\u7d2f\u7684\u901a\u7528\u77e5\u8bc6\u53ef\u65e0\u7f1d\u8fc1\u79fb\u5230\u65b0\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u76d1\u6d4b\u8fdb\u5316\u6536\u655b\u7684\u65b0\u6307\u6807\u3002", "conclusion": "In-Situ Self-Evolving\u8303\u5f0f\u4f7f\u667a\u80fd\u4f53\u80fd\u5728\u5f00\u653e\u73af\u5883\u4e2d\u81ea\u4e3b\u6269\u5c55\u80fd\u529b\u8fb9\u754c\uff0c\u5de5\u5177\u8fdb\u5316\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u53cd\u9988\u4fe1\u53f7\uff0c\u4e3a\u6784\u5efa\u5f39\u6027\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u5f00\u8f9f\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.18282", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18282", "abs": "https://arxiv.org/abs/2601.18282", "authors": ["Lei Wei", "Jinpeng Ou", "Xiao Peng", "Bin Wang"], "title": "Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal \"think\" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.", "AI": {"tldr": "\u63d0\u51faTAFC\u6846\u67b6\uff0c\u901a\u8fc7\u51fd\u6570\u548c\u53c2\u6570\u7ea7\u522b\u7684\u663e\u5f0f\u63a8\u7406\u589e\u5f3aLLM\u51fd\u6570\u8c03\u7528\u51c6\u786e\u6027\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784", "motivation": "\u5f53\u524dLLM\u5728\u51fd\u6570\u8c03\u7528\u4e2d\u7f3a\u4e4f\u53c2\u6570\u751f\u6210\u7684\u663e\u5f0f\u63a8\u7406\u900f\u660e\u5ea6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u76f8\u4e92\u4f9d\u8d56\u53c2\u6570\u7684\u590d\u6742\u51fd\u6570\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u4ee3\u7406\u7ea7\u522b\u64cd\u4f5c\uff0c\u65e0\u6cd5\u4e3a\u5355\u4e2a\u51fd\u6570\u53c2\u6570\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u6307\u5bfc\u3002", "method": "\u63d0\u51faThink-Augmented Function Calling (TAFC)\u6846\u67b6\uff1a1) \u5f15\u5165\u901a\u7528\u7684\"think\"\u53c2\u6570\u589e\u5f3a\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8868\u8fbe\u51b3\u7b56\u8fc7\u7a0b\uff1b2) \u5bf9\u53c2\u6570\u63cf\u8ff0\u8fdb\u884c\u52a8\u6001\u4f18\u5316\u4ee5\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\uff1b3) \u57fa\u4e8e\u590d\u6742\u5ea6\u8bc4\u5206\u81ea\u52a8\u89e6\u53d1\u7ec6\u7c92\u5ea6\u63a8\u7406\uff1b4) \u63d0\u51fa\u63a8\u7406\u5f15\u5bfc\u4f18\u5316\u4ee5\u5bf9\u9f50\u4eba\u7c7b\u671f\u671b\u3002", "result": "\u5728ToolBench\u4e0a\u5bf9\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0cTAFC\u5728\u591a\u53c2\u6570\u51fd\u6570\u7684\u53c2\u6570\u751f\u6210\u51c6\u786e\u6027\u548c\u63a8\u7406\u8fde\u8d2f\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u4e3a\u8c03\u8bd5AI\u4ee3\u7406\u884c\u4e3a\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "TAFC\u901a\u8fc7\u51fd\u6570\u548c\u53c2\u6570\u7ea7\u522b\u7684\u663e\u5f0f\u63a8\u7406\u63d0\u9ad8\u4e86LLM\u51fd\u6570\u8c03\u7528\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u6a21\u578b\u67b6\u6784\uff0c\u4fdd\u6301\u5b8c\u6574\u7684API\u517c\u5bb9\u6027\u3002", "topic": "code agent"}}
{"id": "2601.17865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17865", "abs": "https://arxiv.org/abs/2601.17865", "authors": ["Jia Gu", "Liang Pang", "Huawei Shen", "Xueqi Cheng"], "title": "D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models", "comment": "12 pages, 10 figures. Accepted by WWW'26", "summary": "The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u7ec6\u7c92\u5ea6\u91c7\u6837\u6982\u7387\u4e0a\u5b58\u5728\u4e24\u79cd\u7c7b\u578b\uff1aD-\u6a21\u578b\uff08\u5982Qwen-2.5\uff09\u7684token\u6982\u7387\u6ce2\u52a8\u5927\u4e14\u4e0e\u4efb\u52a1\u5206\u5e03\u5bf9\u9f50\u5dee\uff0cE-\u6a21\u578b\uff08\u5982Mistral-Small\uff09\u7684token\u6982\u7387\u66f4\u7a33\u5b9a\u4e14\u4e0e\u4efb\u52a1\u5206\u5e03\u5bf9\u9f50\u66f4\u597d\uff0c\u8fd9\u4e24\u79cd\u7c7b\u578b\u5728\u591a\u6837\u6027\u548c\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6743\u8861\u3002", "motivation": "\u867d\u7136LLMs\u80fd\u591f\u751f\u6210\u8fd1\u4f3c\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u7684\u6837\u672c\uff0c\u4f46\u5176\u7ec6\u7c92\u5ea6\u91c7\u6837\u6982\u7387\u662f\u5426\u5fe0\u5b9e\u5bf9\u9f50\u4efb\u52a1\u9700\u6c42\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7a76LLMs\u7684token\u9884\u6d4b\u6982\u7387\u4e0e\u4efb\u52a1\u7ea7\u76ee\u6807\u5206\u5e03\u4e4b\u95f4\u7684\u5bf9\u9f50\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u53d7\u63a7\u5206\u5e03\u91c7\u6837\u6a21\u62df\u5b9e\u9a8c\uff0c\u533a\u5206\u4e86\u4e24\u79cd\u6a21\u578b\u7c7b\u578b\uff1aD-\u6a21\u578b\u548cE-\u6a21\u578b\u3002\u8fdb\u4e00\u6b65\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u751f\u6210\u548c\u63a8\u8350\uff09\u4e2d\u8bc4\u4f30\u8fd9\u4e24\u79cd\u6a21\u578b\u7c7b\u578b\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u7684\u5185\u5728\u5c5e\u6027\u4ee5\u63a2\u7a76\u5e95\u5c42\u673a\u5236\u3002", "result": "\u53d1\u73b0LLMs\u5b58\u5728\u663e\u8457\u7684\u884c\u4e3a\u4e8c\u5206\uff1aD-\u6a21\u578b\u7684P_token\u8868\u73b0\u51fa\u8f83\u5927\u7684\u6b65\u95f4\u53d8\u5f02\u6027\u4e14\u4e0eP_task\u5bf9\u9f50\u5dee\uff1bE-\u6a21\u578b\u7684P_token\u66f4\u7a33\u5b9a\u4e14\u4e0eP_task\u5bf9\u9f50\u66f4\u597d\u3002\u4e24\u79cd\u6a21\u578b\u7c7b\u578b\u5728\u591a\u6837\u6027\u548c\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6743\u8861\uff0c\u5f71\u54cd\u4efb\u52a1\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3aLLMs\u7684\u6982\u7387\u91c7\u6837\u884c\u4e3a\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u89c1\u89e3\uff0c\u5e76\u4e3a\u4f55\u65f6\u4f18\u5148\u9009\u62e9D-\u6a21\u578b\u6216E-\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002\u5bf9\u4e8e\u63a8\u8350\u3001\u641c\u7d22\u548c\u5bf9\u8bdd\u4ee3\u7406\u7b49\u7f51\u7edc\u89c4\u6a21\u5e94\u7528\uff0c\u7814\u7a76\u7ed3\u679c\u53ef\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u548c\u914d\u7f6e\uff0c\u4ee5\u5728\u73b0\u5b9e\u4e16\u754c\u4e0d\u786e\u5b9a\u6027\u4e0b\u5e73\u8861\u591a\u6837\u6027\u548c\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.17879", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17879", "abs": "https://arxiv.org/abs/2601.17879", "authors": ["Yilong Xu", "Zhi Zheng", "Xiang Long", "Yujun Cai", "Yiwei Wang"], "title": "Self-Manager: Parallel Agent Loop for Long-form Deep Research", "comment": null, "summary": "Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.", "AI": {"tldr": "Self-Manager\u662f\u4e00\u4e2a\u5e76\u884c\u4ee3\u7406\u5faa\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ebf\u7a0b\u5f02\u6b65\u5e76\u53d1\u6267\u884c\u89e3\u51b3\u4f20\u7edf\u5355\u4e0a\u4e0b\u6587\u7a97\u53e3\u4ee3\u7406\u5728\u957f\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u4e92\u5e72\u6270\u548c\u963b\u585e\u95ee\u9898\uff0c\u5728DeepResearch Bench\u4e0a\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u5728\u5904\u7406\u957f\u5f62\u5f0f\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u65f6\uff0c\u867d\u7136\u901a\u8fc7\u5b50\u4efb\u52a1\u7ea7\u4e0a\u4e0b\u6587\u7ba1\u7406\u514b\u670d\u4e86\u7ebf\u6027\u4e0a\u4e0b\u6587\u79ef\u7d2f\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u5355\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u987a\u5e8f\u6267\u884c\u8303\u5f0f\uff0c\u5bfc\u81f4\u4e92\u5e72\u6270\u548c\u963b\u585e\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faSelf-Manager\u5e76\u884c\u4ee3\u7406\u5faa\u73af\u6846\u67b6\uff1a\u4e3b\u7ebf\u7a0b\u53ef\u521b\u5efa\u591a\u4e2a\u5177\u6709\u72ec\u7acb\u9694\u79bb\u4e0a\u4e0b\u6587\u7684\u5b50\u7ebf\u7a0b\uff0c\u901a\u8fc7\u7ebf\u7a0b\u63a7\u5236\u5757\u8fdb\u884c\u8fed\u4ee3\u7ba1\u7406\uff0c\u5b9e\u73b0\u66f4\u4e13\u6ce8\u548c\u7075\u6d3b\u7684\u5e76\u884c\u4ee3\u7406\u6267\u884c\u3002", "result": "\u5728DeepResearch Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSelf-Manager\u5728\u6240\u6709\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u4ee3\u7406\u5faa\u73af\u57fa\u7ebf\u3002\u5206\u6790\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u8bbe\u8ba1\u9009\u62e9\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u53ca\u5728\u4e0a\u4e0b\u6587\u5bb9\u91cf\u3001\u6548\u7387\u548c\u6cdb\u5316\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "Self-Manager\u901a\u8fc7\u5e76\u884c\u4ee3\u7406\u5faa\u73af\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee3\u7406\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u957f\u7814\u7a76\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2601.18467", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18467", "abs": "https://arxiv.org/abs/2601.18467", "authors": ["Yuhang Zhou", "Kai Zheng", "Qiguang Chen", "Mengkang Hu", "Qingfeng Sun", "Can Xu", "Jingjing Chen"], "title": "OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents", "comment": null, "summary": "Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u79bb\u7ebf\u7684\u7814\u7a76\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u6e90\u5957\u4ef6DeepForge\u751f\u6210\u5927\u89c4\u6a21\u7814\u7a76\u67e5\u8be2\uff0c\u5e76\u8bad\u7ec3\u51fa8B\u53c2\u6570\u7684OffSeeker\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u80fd\u4e0e\u4f7f\u7528\u6602\u8d35\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u768430B\u53c2\u6570\u7cfb\u7edf\u7ade\u4e89\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u5728\u5904\u7406\u957f\u65f6\u7a0b\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6700\u4f73\u6027\u80fd\u901a\u5e38\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08\u9700\u8981\u5927\u91cfAPI\u8c03\u7528\uff09\u3002\u79bb\u7ebf\u8bad\u7ec3\u867d\u7136\u66f4\u9ad8\u6548\uff0c\u4f46\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u7814\u7a76\u8f68\u8ff9\u7684\u7a00\u7f3a\u6027\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u65e0\u9700\u6602\u8d35\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e5f\u80fd\u6784\u5efa\u5f3a\u5927\u7684\u7814\u7a76\u667a\u80fd\u4f53\u3002", "method": "1) \u5f15\u5165\u5b8c\u5168\u5f00\u6e90\u5957\u4ef6\uff0c\u5305\u62ecDeepForge\u4efb\u52a1\u5408\u6210\u6846\u67b6\uff0c\u65e0\u9700\u7e41\u91cd\u9884\u5904\u7406\u5373\u53ef\u751f\u6210\u5927\u89c4\u6a21\u7814\u7a76\u67e5\u8be2\uff1b2) \u6784\u5efa\u5305\u542b66k QA\u5bf9\u300133k SFT\u8f68\u8ff9\u548c21k DPO\u5bf9\u7684\u6570\u636e\u96c6\uff1b3) \u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec38B\u53c2\u6570\u7684OffSeeker\u6a21\u578b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cOffSeeker\u4e0d\u4ec5\u5728\u540c\u7b49\u89c4\u6a21\u667a\u80fd\u4f53\u4e2d\u9886\u5148\uff0c\u800c\u4e14\u4e0e\u901a\u8fc7\u5927\u91cf\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u768430B\u53c2\u6570\u7cfb\u7edf\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e5f\u80fd\u6784\u5efa\u5f3a\u5927\u7684\u7814\u7a76\u667a\u80fd\u4f53\u3002\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u79bb\u7ebf\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u4e0e\u66f4\u5927\u89c4\u6a21\u5728\u7ebf\u8bad\u7ec3\u7cfb\u7edf\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.18491", "categories": ["cs.AI", "cs.CC", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18491", "abs": "https://arxiv.org/abs/2601.18491", "authors": ["Dongrui Liu", "Qihan Ren", "Chen Qian", "Shuai Shao", "Yuejin Xie", "Yu Li", "Zhonghao Yang", "Haoyu Luo", "Peng Wang", "Qingyu Liu", "Binxin Hu", "Ling Tang", "Jilin Mei", "Dadi Guo", "Leitao Yuan", "Junyao Yang", "Guanxu Chen", "Qihao Lin", "Yi Yu", "Bo Zhang", "Jiaxuan Guo", "Jie Zhang", "Wenqi Shao", "Huiqi Deng", "Zhiheng Xi", "Wenjie Wang", "Wenxuan Wang", "Wen Shen", "Zhikai Chen", "Haoyu Xie", "Jialing Tao", "Juntao Dai", "Jiaming Ji", "Zhongjie Ba", "Linfeng Zhang", "Yong Liu", "Quanshi Zhang", "Lei Zhu", "Zhihua Wei", "Hui Xue", "Chaochao Lu", "Jing Shao", "Xia Hu"], "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "comment": "40 pages, 26 figures", "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "AI": {"tldr": "\u63d0\u51faAgentDoG\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u7ef4\u98ce\u9669\u5206\u7c7b\u6cd5\u6784\u5efa\u7ec6\u7c92\u5ea6\u667a\u80fd\u4f53\u5b89\u5168\u57fa\u51c6\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u8bca\u65ad\u4e0e\u76d1\u63a7\uff0c\u8d85\u8d8a\u4f20\u7edf\u4e8c\u5143\u6807\u7b7e\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u900f\u660e\u5ea6\u3002", "motivation": "\u5f53\u524d\u62a4\u680f\u6a21\u578b\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u98ce\u9669\u7684\u8ba4\u77e5\u548c\u98ce\u9669\u8bca\u65ad\u7684\u900f\u660e\u5ea6\uff0c\u65e0\u6cd5\u5e94\u5bf9\u81ea\u4e3b\u5de5\u5177\u4f7f\u7528\u548c\u73af\u5883\u4ea4\u4e92\u5e26\u6765\u7684\u590d\u6742\u5b89\u5168\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u4e09\u7ef4\u98ce\u9669\u5206\u7c7b\u6cd5\uff08\u6765\u6e90\u3001\u5931\u6548\u6a21\u5f0f\u3001\u540e\u679c\uff09\uff0c\u57fa\u4e8e\u6b64\u6784\u5efa\u7ec6\u7c92\u5ea6\u667a\u80fd\u4f53\u5b89\u5168\u57fa\u51c6ATBench\uff0c\u5f00\u53d1\u8bca\u65ad\u6027\u62a4\u680f\u6846\u67b6AgentDoG\uff0c\u63d0\u4f9b\u8de8\u667a\u80fd\u4f53\u8f68\u8ff9\u7684\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u76d1\u63a7\u548c\u6839\u56e0\u8bca\u65ad\u3002", "result": "AgentDoG\u5728\u591a\u6837\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u5b89\u5168\u8c03\u8282\u6027\u80fd\uff0c\u63d0\u4f9bQwen\u548cLlama\u6a21\u578b\u5bb6\u65cf\u76844B\u30017B\u30018B\u53c2\u6570\u7248\u672c\uff0c\u6240\u6709\u6a21\u578b\u548c\u6570\u636e\u96c6\u5747\u5df2\u5f00\u6e90\u3002", "conclusion": "AgentDoG\u901a\u8fc7\u7ed3\u6784\u5316\u98ce\u9669\u5206\u7c7b\u548c\u8bca\u65ad\u6027\u76d1\u63a7\uff0c\u4e3a\u667a\u80fd\u4f53\u5b89\u5168\u63d0\u4f9b\u4e86\u8d85\u8d8a\u4e8c\u5143\u6807\u7b7e\u7684\u900f\u660e\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u6709\u6548\u4fc3\u8fdb\u667a\u80fd\u4f53\u5bf9\u9f50\u3002", "topic": "agent analysis"}}
{"id": "2601.17563", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17563", "abs": "https://arxiv.org/abs/2601.17563", "authors": ["Nathan Gavenski", "Matteo Leonetti", "Odinaldo Rodrigues"], "title": "Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment", "comment": "The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026)", "summary": "State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u76d1\u7763\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60(UfO)\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\u89e3\u51b3\u73b0\u6709ILfO\u65b9\u6cd5\u9700\u8981\u52a8\u4f5c\u76d1\u7763\u3001\u5047\u8bbe\u72b6\u6001\u6709\u5355\u4e00\u6700\u4f18\u52a8\u4f5c\u3001\u4e0d\u8003\u8651\u5b9e\u9645\u73af\u5883\u72b6\u6001\u7b49\u9650\u5236\uff0c\u5728\u4e94\u4e2a\u73af\u5883\u4e2d\u8d85\u8d8a\u6559\u5e08\u548c\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5(ILfO)\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u9700\u8981\u57fa\u4e8e\u52a8\u4f5c\u7684\u76d1\u7763\u4f18\u5316\u3001\u5047\u8bbe\u72b6\u6001\u6709\u5355\u4e00\u6700\u4f18\u52a8\u4f5c\u3001\u503e\u5411\u4e8e\u76f4\u63a5\u5e94\u7528\u6559\u5e08\u52a8\u4f5c\u800c\u4e0d\u5145\u5206\u8003\u8651\u5b9e\u9645\u73af\u5883\u72b6\u6001\u3002\u867d\u7136\u771f\u5b9e\u4fe1\u606f\u5b58\u5728\u4e8e\u89c2\u5bdf\u5230\u7684\u8f68\u8ff9\u4e2d\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u65e0\u76d1\u7763\u60c5\u51b5\u4e0b\u63d0\u53d6\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u65e0\u76d1\u7763\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60(UfO)\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\uff1a1) \u9996\u5148\u4ece\u89c2\u5bdf\u5230\u7684\u72b6\u6001\u8f6c\u79fb\u4e2d\u8fd1\u4f3c\u6559\u5e08\u7684\u771f\u5b9e\u52a8\u4f5c\uff1b2) \u7136\u540e\u901a\u8fc7\u8c03\u6574\u667a\u80fd\u4f53\u8f68\u8ff9\u4f7f\u5176\u4e0e\u6559\u5e08\u8f68\u8ff9\u7d27\u5bc6\u5bf9\u9f50\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u73af\u5883\u4e2d\uff0cUfO\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u6559\u5e08\u548c\u6240\u6709\u5176\u4ed6ILfO\u65b9\u6cd5\uff0c\u8fd8\u663e\u793a\u51fa\u6700\u5c0f\u7684\u6807\u51c6\u5dee\u3002\u6807\u51c6\u5dee\u7684\u51cf\u5c11\u8868\u660e\u5728\u672a\u89c1\u573a\u666f\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UfO\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709ILfO\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u4e24\u9636\u6bb5\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18053", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18053", "abs": "https://arxiv.org/abs/2601.18053", "authors": ["Pulin Agrawal", "Prasoon Goyal"], "title": "Addressing LLM Diversity by Infusing Random Concepts", "comment": null, "summary": "Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form \"Name 10 Hollywood actors\", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.", "AI": {"tldr": "\u901a\u8fc7\u5411LLM\u63d0\u793a\u4e2d\u6dfb\u52a0\u968f\u673a\u6982\u5ff5\u6765\u63d0\u5347\u751f\u6210\u8f93\u51fa\u7684\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u65e0\u5173\u968f\u673a\u8bcd\u53e5\u80fd\u663e\u8457\u589e\u52a0\u8f93\u51fa\u591a\u6837\u6027", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8f93\u51fa\u591a\u6837\u6027\u6709\u9650\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u901a\u8fc7\u5411\u63d0\u793a\u4e2d\u6ce8\u5165\u968f\u673a\u6982\u5ff5\u662f\u5426\u80fd\u6539\u5584\u8fd9\u4e00\u9650\u5236", "method": "\u8bbe\u8ba1\u7cfb\u7edf\u6027\u8bc4\u4f30\u534f\u8bae\uff0c\u5728\u63d0\u793a\u524d\u6dfb\u52a0\u968f\u673a\u8bcd\u53e5\uff08\u5982\"Name 10 Hollywood actors\"\uff09\uff0c\u5206\u6790LLM\u8f93\u51fa\u7684\u591a\u6837\u6027\u6307\u6807\uff0c\u5e76\u5728\u591a\u4e2aLLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u63d0\u793a\u524d\u6dfb\u52a0\u4e0e\u95ee\u9898\u65e0\u5173\u7684\u968f\u673a\u8bcd\u53e5\u80fd\u663e\u8457\u63d0\u9ad8LLM\u8f93\u51fa\u7684\u591a\u6837\u6027", "conclusion": "\u968f\u673a\u6027\u6ce8\u5165\u662f\u63d0\u5347LLM\u8f93\u51fa\u591a\u6837\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u8bc4\u4f30\u534f\u8bae\u4e3a\u7cfb\u7edf\u6027\u8bc4\u4f30LLM\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u5728\u5176\u4ed6\u9886\u57df\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "2601.18642", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18642", "abs": "https://arxiv.org/abs/2601.18642", "authors": ["Lei Wei", "Xu Dong", "Xiao Peng", "Niantao Xie", "Bin Wang"], "title": "FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory", "comment": null, "summary": "Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.", "AI": {"tldr": "FadeMem\u662f\u4e00\u79cd\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u4e3b\u52a8\u9057\u5fd8\u673a\u5236\u6765\u5e73\u8861\u8bb0\u5fc6\u4fdd\u7559\u4e0e\u9057\u5fd8\uff0c\u89e3\u51b3\u4e86\u5f53\u524dAI\u7cfb\u7edf\u5728\u8bb0\u5fc6\u7ba1\u7406\u4e2d\u7684\u4fe1\u606f\u8fc7\u8f7d\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u9762\u4e34\u4e25\u91cd\u7684\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\uff0c\u7f3a\u4e4f\u9009\u62e9\u6027\u9057\u5fd8\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u4e0a\u4e0b\u6587\u8fb9\u754c\u5904\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u6216\u5728\u8fb9\u754c\u5185\u4fe1\u606f\u8fc7\u8f7d\u3002\u4eba\u7c7b\u8bb0\u5fc6\u901a\u8fc7\u81ea\u9002\u5e94\u8870\u51cf\u8fc7\u7a0b\u81ea\u7136\u5e73\u8861\u4fdd\u7559\u4e0e\u9057\u5fd8\uff0c\u800c\u73b0\u6709AI\u7cfb\u7edf\u91c7\u7528\u4e8c\u5143\u4fdd\u7559\u7b56\u7565\uff08\u8981\u4e48\u5168\u90e8\u4fdd\u7559\uff0c\u8981\u4e48\u5168\u90e8\u4e22\u5931\uff09\u3002", "method": "FadeMem\u91c7\u7528\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u53cc\u5c42\u7ea7\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6307\u6570\u8870\u51cf\u51fd\u6570\u5b9e\u73b0\u5dee\u5f02\u5316\u7684\u8870\u51cf\u7387\uff0c\u8870\u51cf\u8fc7\u7a0b\u53d7\u8bed\u4e49\u76f8\u5173\u6027\u3001\u8bbf\u95ee\u9891\u7387\u548c\u65f6\u95f4\u6a21\u5f0f\u8c03\u5236\u3002\u7cfb\u7edf\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u51b2\u7a81\u89e3\u51b3\u548c\u667a\u80fd\u8bb0\u5fc6\u878d\u5408\u6765\u6574\u5408\u76f8\u5173\u4fe1\u606f\uff0c\u540c\u65f6\u5141\u8bb8\u65e0\u5173\u7ec6\u8282\u9010\u6e10\u6de1\u51fa\u3002", "result": "\u5728Multi-Session Chat\u3001LoCoMo\u548cLTI-Bench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFadeMem\u5728\u591a\u8df3\u63a8\u7406\u548c\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5b9e\u73b0\u4e8645%\u7684\u5b58\u50a8\u51cf\u5c11\uff0c\u9a8c\u8bc1\u4e86\u751f\u7269\u5b66\u542f\u53d1\u5f0f\u9057\u5fd8\u5728\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "FadeMem\u901a\u8fc7\u5f15\u5165\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u4e3b\u52a8\u9057\u5fd8\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u8bb0\u5fc6\u7ba1\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5b58\u50a8\u9700\u6c42\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2601.18077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18077", "abs": "https://arxiv.org/abs/2601.18077", "authors": ["Mahesh Ramesh", "Kaousheik Jayakumar", "Aswinkumar Ramkumar", "Pavan Thodima", "Aniket Rege"], "title": "Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents", "comment": null, "summary": "Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u667a\u80fd\u4f53\u5728Hanabi\u7eb8\u724c\u6e38\u620f\u4e2d\u7684\u534f\u4f5c\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u4e0d\u540c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u8bbe\u7f6e\u8bc4\u4f30\u4e8617\u4e2a\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u516c\u5f00\u7684Hanabi\u6570\u636e\u96c6\u7528\u4e8e\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534f\u4f5c\u6027\u80fd\u3002", "motivation": "\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u4e0b\u7684\u534f\u4f5c\u63a8\u7406\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5177\u6709\u6311\u6218\u6027\u3002Hanabi\u6e38\u620f\u9700\u8981\u5fc3\u667a\u7406\u8bba\u548c\u6218\u7565\u6c9f\u901a\uff0c\u662f\u7814\u7a76LLM\u534f\u4f5c\u63a8\u7406\u80fd\u529b\u7684\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\u3002", "method": "\u57282-5\u4eba\u6e38\u620f\u4e2d\u8bc4\u4f3017\u4e2a\u5148\u8fdbLLM\u667a\u80fd\u4f53\uff0c\u8bbe\u8ba1\u4e86\u4e09\u79cd\u4e0a\u4e0b\u6587\u5de5\u7a0b\u8bbe\u7f6e\uff1aWatson\uff08\u4ec5\u663e\u5f0f\u5361\u724c\u4fe1\u606f\uff09\u3001Sherlock\uff08\u8d1d\u53f6\u65af\u63a8\u7406\u8f85\u52a9\uff09\u3001Mycroft\uff08\u591a\u8f6e\u72b6\u6001\u8ddf\u8e2a\uff09\u3002\u53d1\u5e03\u4e86HanabiLogs\u548cHanabiRewards\u6570\u636e\u96c6\uff0c\u5e76\u5bf94B\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "result": "\u6700\u5f3a\u63a8\u7406\u6a21\u578b\u5728Sherlock\u8bbe\u7f6e\u4e0b\u5e73\u5747\u5f97\u5206\u8d85\u8fc715\u5206\uff0c\u4f46\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0820\u5206\u4ee5\u4e0a\uff09\u3002\u4f7f\u7528\u6570\u636e\u96c6\u5fae\u8c03\u76844B\u6a21\u578b\u6027\u80fd\u63d0\u534721%\uff08\u76d1\u7763\uff09\u548c156%\uff08\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u63a5\u8fd1\u4e13\u6709\u63a8\u7406\u6a21\u578bo4-mini\uff0c\u5e76\u8d85\u8d8aGPT-4.1 52%\u3002\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u8fd8\u6cdb\u5316\u5230\u5176\u4ed6\u534f\u4f5c\u548c\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u6570\u636e\u96c6\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u534f\u4f5c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u4ecd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u5dee\u8ddd\u3002\u53d1\u5e03\u7684Hanabi\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u4e0d\u5b8c\u5168\u4fe1\u606f\u4e0b\u7684\u534f\u4f5c\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2601.18744", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18744", "abs": "https://arxiv.org/abs/2601.18744", "authors": ["Fangxu Yu", "Xingang Guo", "Lingzhi Yuan", "Haoqiang Kang", "Hongyu Zhao", "Lianhui Qin", "Furong Huang", "Bin Hu", "Tianyi Zhou"], "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models", "comment": null, "summary": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.", "AI": {"tldr": "TSRBench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b4125\u4e2a\u95ee\u9898\u300114\u4e2a\u9886\u57df\u548c4\u4e2a\u4e3b\u8981\u7ef4\u5ea6\uff0c\u7528\u4e8e\u8bc4\u4f30\u901a\u7528\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u65e0\u5904\u4e0d\u5728\u4e14\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u901a\u7528\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7f3a\u4e4f\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7ef4\u5ea6\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaTSRBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b4125\u4e2a\u95ee\u9898\uff0c\u6db5\u76d614\u4e2a\u9886\u57df\uff0c\u5206\u4e3a\u611f\u77e5\u3001\u63a8\u7406\u3001\u9884\u6d4b\u548c\u51b3\u7b564\u4e2a\u4e3b\u8981\u7ef4\u5ea6\uff0c\u5305\u542b15\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8630\u591a\u4e2a\u9886\u5148\u7684\u4e13\u6709\u548c\u5f00\u6e90LLM\u3001VLM\u548cTSLLM\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u7f29\u653e\u5b9a\u5f8b\u9002\u7528\u4e8e\u611f\u77e5\u548c\u63a8\u7406\u4f46\u5728\u9884\u6d4b\u4e2d\u5931\u6548\uff1b2\uff09\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u80fd\u4fdd\u8bc1\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u6d4b\uff0c\u8868\u660e\u8bed\u4e49\u7406\u89e3\u548c\u6570\u503c\u9884\u6d4b\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff1b3\uff09\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u672a\u80fd\u6709\u6548\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u8868\u793a\u4ee5\u83b7\u5f97\u4e92\u8865\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "TSRBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\uff0c\u4e0d\u4ec5\u7a81\u51fa\u4e86\u73b0\u6709\u6311\u6218\uff0c\u8fd8\u4e3a\u63a8\u8fdb\u901a\u7528\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2601.18204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18204", "abs": "https://arxiv.org/abs/2601.18204", "authors": ["Juexiang Ye", "Xue Li", "Xinyu Yang", "Chengkai Huang", "Lanshun Nie", "Lina Yao", "Dechen Zhan"], "title": "MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning", "comment": null, "summary": "Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\\% compared to long-context baselines.", "AI": {"tldr": "MemWeaver\u662f\u4e00\u4e2a\u7edf\u4e00\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e92\u8054\u7ec4\u4ef6\uff08\u65f6\u5e8f\u56fe\u8bb0\u5fc6\u3001\u7ecf\u9a8c\u8bb0\u5fc6\u3001\u6bb5\u843d\u8bb0\u5fc6\uff09\u548c\u53cc\u901a\u9053\u68c0\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u591a\u8df3\u63a8\u7406\u548c\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u68c0\u7d22\u6216\u7c97\u7c92\u5ea6\u62bd\u8c61\uff0c\u5bfc\u81f4\u65f6\u5e8f\u51b2\u7a81\u3001\u63a8\u7406\u8106\u5f31\u548c\u53ef\u8ffd\u6eaf\u6027\u6709\u9650\u3002\u9700\u8981\u652f\u6301\u65f6\u5e8f\u4e00\u81f4\u6027\u3001\u591a\u8df3\u63a8\u7406\u548c\u8de8\u4f1a\u8bdd\u8bc1\u636e\u91cd\u7528\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002", "method": "\u63d0\u51faMemWeaver\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1\uff09\u65f6\u5e8f\u56fe\u8bb0\u5fc6\u7528\u4e8e\u7ed3\u6784\u5316\u5173\u7cfb\u63a8\u7406\uff1b2\uff09\u7ecf\u9a8c\u8bb0\u5fc6\u4ece\u91cd\u590d\u89c2\u5bdf\u4e2d\u62bd\u8c61\u4ea4\u4e92\u6a21\u5f0f\uff1b3\uff09\u6bb5\u843d\u8bb0\u5fc6\u4fdd\u7559\u539f\u59cb\u6587\u672c\u8bc1\u636e\u3002\u91c7\u7528\u53cc\u901a\u9053\u68c0\u7d22\u7b56\u7565\u8054\u5408\u68c0\u7d22\u7ed3\u6784\u5316\u77e5\u8bc6\u548c\u652f\u6301\u8bc1\u636e\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemWeaver\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8df3\u548c\u65f6\u5e8f\u63a8\u7406\u51c6\u786e\u6027\uff0c\u540c\u65f6\u76f8\u6bd4\u957f\u4e0a\u4e0b\u6587\u57fa\u7ebf\u51cf\u5c11\u4e86\u8d85\u8fc795%\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "conclusion": "MemWeaver\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u7ec4\u4ef6\u548c\u53cc\u901a\u9053\u68c0\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.18253", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18253", "abs": "https://arxiv.org/abs/2601.18253", "authors": ["Peng Sun", "Xiangyu Zhang", "Duan Wu"], "title": "BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation", "comment": "This is a pre-print", "summary": "Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.", "AI": {"tldr": "BoRP\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5bf9\u8bddAI\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5229\u7528LLM\u6f5c\u5728\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\uff0c\u901a\u8fc7\u6781\u5316\u6307\u6570\u5f15\u5bfc\u7684\u81ea\u4e3e\u673a\u5236\u81ea\u52a8\u751f\u6210\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u4f7f\u7528\u504f\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5c06\u9690\u85cf\u72b6\u6001\u6620\u5c04\u5230\u8fde\u7eed\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u751f\u6210\u5f0f\u57fa\u7ebf\u4e14\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5bf9\u4e8e\u5f00\u653e\u5f0f\u5bf9\u8bdd\u52a9\u624b\uff0c\u4f20\u7edfA/B\u6d4b\u8bd5\u7f3a\u4e4f\u53ef\u9760\u6307\u6807\uff1a\u663e\u5f0f\u53cd\u9988\u7a00\u758f\uff0c\u9690\u5f0f\u6307\u6807\u6a21\u7cca\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u4fdd\u771f\u5ea6\u7684\u6ee1\u610f\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u6765\u652f\u6301\u8fed\u4ee3\u5f00\u53d1\u3002", "method": "BoRP\u6846\u67b6\u5229\u7528LLM\u6f5c\u5728\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\uff0c\u91c7\u7528\u6781\u5316\u6307\u6570\u5f15\u5bfc\u7684\u81ea\u4e3e\u673a\u5236\u81ea\u52a8\u751f\u6210\u8bc4\u4f30\u6807\u51c6\uff0c\u4f7f\u7528\u504f\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5c06\u9690\u85cf\u72b6\u6001\u6620\u5c04\u5230\u8fde\u7eed\u6ee1\u610f\u5ea6\u5206\u6570\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBoRP\uff08\u57fa\u4e8eQwen3-8B/14B\uff09\u5728\u4e0e\u4eba\u5224\u65ad\u7684\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u751f\u6210\u5f0f\u57fa\u7ebf\uff08\u751a\u81f3\u4f18\u4e8eQwen3-Max\uff09\uff0c\u540c\u65f6\u63a8\u7406\u6210\u672c\u964d\u4f4e\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u652f\u6301\u5168\u89c4\u6a21\u76d1\u63a7\u548c\u9ad8\u5ea6\u654f\u611f\u7684A/B\u6d4b\u8bd5\u3002", "conclusion": "BoRP\u4e3a\u5f00\u653e\u5f0f\u5bf9\u8bddAI\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u6ee1\u610f\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u9ad8\u6548\u7684\u8fed\u4ee3\u5f00\u53d1\u548c\u76d1\u63a7\u3002", "topic": "agent analysis"}}
{"id": "2601.17687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17687", "abs": "https://arxiv.org/abs/2601.17687", "authors": ["Hao Li", "He Cao", "Shenyao Peng", "Zijing Liu", "Bin Feng", "Yu Wang", "Zhiyuan Yan", "Yonghong Tian", "Yu Li", "Li Yuan"], "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis", "comment": "Working in Progress, 13 pages, 4 figures", "summary": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.", "AI": {"tldr": "ChemCRAFT\u6846\u67b6\u901a\u8fc7\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u5c06\u5316\u5b66\u63a8\u7406\u4e0e\u77e5\u8bc6\u5b58\u50a8\u89e3\u8026\uff0c\u4f7f\u672c\u5730\u90e8\u7f72\u7684\u5c0f\u6a21\u578b\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u964d\u4f4e\u63a8\u7406\u6210\u672c\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u5f53\u524d\u751f\u5316\u9886\u57df\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e24\u96be\uff1a\u5c0f\u6a21\u578b\u6613\u4ea7\u751f\u5e7b\u89c9\u4e14\u77e5\u8bc6\u6709\u9650\uff0c\u5927\u4e91\u6a21\u578b\u6709\u9690\u79c1\u98ce\u9669\u548c\u9ad8\u63a8\u7406\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u80fd\u5728\u672c\u5730\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u6784\u5efa\u4ee3\u7406\u8f68\u8ff9\u7ba1\u9053\u548c\u5316\u5b66\u4ee3\u7406\u6c99\u7bb1\uff1b2) \u521b\u5efaChemToolDataset\u5de5\u5177\u8f68\u8ff9\u6570\u636e\u96c6\uff1b3) \u63d0\u51faSMILES-GRPO\u6784\u5efa\u5bc6\u96c6\u5316\u5b66\u5956\u52b1\u51fd\u6570\uff1b4) \u901a\u8fc7\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u4f7f\u5c0f\u6a21\u578b\u5b66\u4f1a\u8c03\u7528\u5316\u5b66\u4ee3\u7406\u8fdb\u884c\u7cbe\u786e\u4fe1\u606f\u68c0\u7d22\u3002", "result": "ChemCRAFT\u5728\u836f\u7269\u8bbe\u8ba1\u7684\u5206\u5b50\u7ed3\u6784\u5206\u6790\u3001\u5206\u5b50\u4f18\u5316\u548c\u5408\u6210\u8def\u5f84\u9884\u6d4b\u7b49\u591a\u4e2a\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u4e91\u57fa\u5927\u6a21\u578b\uff0c\u8bc1\u660e\u79d1\u5b66\u63a8\u7406\u4e0d\u662f\u6a21\u578b\u89c4\u6a21\u7684\u6d8c\u73b0\u80fd\u529b\uff0c\u800c\u662f\u53ef\u5b66\u4e60\u7684\u5de5\u5177\u7f16\u6392\u7b56\u7565\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u9690\u79c1\u4fdd\u62a4\u7684AI\u8f85\u52a9\u5316\u5b66\u8303\u5f0f\uff0c\u4e3a\u901a\u8fc7\u672c\u5730\u53ef\u90e8\u7f72\u4ee3\u7406\u52a0\u901f\u5206\u5b50\u53d1\u73b0\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18285", "abs": "https://arxiv.org/abs/2601.18285", "authors": ["Jin Su", "Runnan Fang", "Yeqiu Li", "Xiaobin Wang", "Shihao Cai", "Pengjun Xie", "Ningyu Zhang", "Fajie Yuan"], "title": "U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents", "comment": null, "summary": "Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $\u03c4$-bench, $\u03c4^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.", "AI": {"tldr": "U-Fold\u662f\u4e00\u4e2a\u9488\u5bf9\u7528\u6237\u4e2d\u5fc3\u4efb\u52a1\u7684\u52a8\u6001\u4e0a\u4e0b\u6587\u6298\u53e0\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u7559\u5b8c\u6574\u5bf9\u8bdd\u5386\u53f2\u4f46\u751f\u6210\u610f\u56fe\u611f\u77e5\u7684\u5bf9\u8bdd\u6458\u8981\u548c\u7d27\u51d1\u7684\u5de5\u5177\u65e5\u5fd7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e0a\u4e0b\u6587\u6298\u53e0\u65b9\u6cd5\u5728\u7528\u6237\u4e2d\u5fc3\u5bf9\u8bdd\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u3001\u591a\u8f6e\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u5de5\u5177\u589e\u5f3a\u8bbe\u7f6e\u4e2d\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u6298\u53e0\u65b9\u6cd5\u4e3b\u8981\u4e3a\u5355\u67e5\u8be2\u6216\u5355\u610f\u56fe\u573a\u666f\u8bbe\u8ba1\uff0c\u5728\u7528\u6237\u4e2d\u5fc3\u5bf9\u8bdd\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff1a1) \u4e0d\u53ef\u9006\u5730\u4e22\u5f03\u5bf9\u540e\u7eed\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u7684\u7ec6\u7c92\u5ea6\u7ea6\u675f\u548c\u4e2d\u95f4\u4e8b\u5b9e\uff1b2) \u6458\u8981\u65e0\u6cd5\u8ddf\u8e2a\u6f14\u5316\u7684\u7528\u6237\u610f\u56fe\uff0c\u5bfc\u81f4\u9057\u6f0f\u548c\u9519\u8bef\u64cd\u4f5c\u3002", "method": "\u63d0\u51faU-Fold\u52a8\u6001\u4e0a\u4e0b\u6587\u6298\u53e0\u6846\u67b6\uff0c\u4fdd\u7559\u5b8c\u6574\u7684\u7528\u6237-\u667a\u80fd\u4f53\u5bf9\u8bdd\u548c\u5de5\u5177\u8c03\u7528\u5386\u53f2\uff0c\u4f46\u5728\u6bcf\u4e00\u8f6e\u4f7f\u7528\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u751f\u6210\u610f\u56fe\u611f\u77e5\u7684\u6f14\u5316\u5bf9\u8bdd\u6458\u8981\uff1b2) \u751f\u6210\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u5de5\u5177\u65e5\u5fd7\u3002", "result": "\u5728\u03c4-bench\u3001\u03c4\u00b2-bench\u3001VitaBench\u548c\u66f4\u96be\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u8bbe\u7f6e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cU-Fold\u59cb\u7ec8\u4f18\u4e8eReAct\uff08\u5728\u957f\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e2d\u8fbe\u523071.4%\u7684\u80dc\u7387\uff09\u548c\u5148\u524d\u7684\u6298\u53e0\u57fa\u7ebf\uff08\u6539\u8fdb\u9ad8\u8fbe27.0%\uff09\uff0c\u7279\u522b\u662f\u5728\u957f\u3001\u5608\u6742\u3001\u591a\u8f6e\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "U-Fold\u662f\u5c06\u4e0a\u4e0b\u6587\u7ba1\u7406\u6280\u672f\u4ece\u5355\u67e5\u8be2\u57fa\u51c6\u8f6c\u79fb\u5230\u73b0\u5b9e\u7528\u6237\u4e2d\u5fc3\u5e94\u7528\u7684\u6709\u5e0c\u671b\u7684\u4e00\u6b65\uff0c\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u6298\u53e0\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u4e2d\u5fc3\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2601.18296", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18296", "abs": "https://arxiv.org/abs/2601.18296", "authors": ["Zhaoyan Gong", "Zhiqiang Liu", "Songze Li", "Xiaoke Guo", "Yuanxiang Liu", "Xinle Deng", "Zhizhen Liu", "Lei Liang", "Huajun Chen", "Wen Zhang"], "title": "Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning", "comment": "Work in progress", "summary": "Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.", "AI": {"tldr": "Temp-R1\u662f\u9996\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u81ea\u4e3bTKGQA\u4ee3\u7406\uff0c\u901a\u8fc7\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u548c\u53cd\u5411\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5728\u590d\u6742\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709TKGQA\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u5de5\u4f5c\u6d41\u7a0b\u548c\u6602\u8d35\u7684\u95ed\u6e90API\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u66f4\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u52a8\u6001\u4e8b\u5b9e\u7684\u591a\u8df3\u4f9d\u8d56\u548c\u590d\u6742\u65f6\u95f4\u7ea6\u675f\u3002", "method": "\u63d0\u51faTemp-R1\uff1a1\uff09\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5305\u542b\u4e13\u95e8\u5185\u90e8\u52a8\u4f5c\u548c\u5916\u90e8\u52a8\u4f5c\u4ee5\u89e3\u51b3\u5355\u52a8\u4f5c\u63a8\u7406\u7684\u8ba4\u77e5\u8fc7\u8f7d\uff1b2\uff09\u5f15\u5165\u53cd\u5411\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5148\u8bad\u7ec3\u56f0\u96be\u95ee\u9898\u518d\u8fc1\u79fb\u5230\u7b80\u5355\u95ee\u9898\uff0c\u9632\u6b62\u6377\u5f84\u5b66\u4e60\u3002", "result": "8B\u53c2\u6570\u7684Temp-R1\u5728MultiTQ\u548cTimelineKGQA\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u590d\u6742\u95ee\u9898\u4e0a\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u9ad8\u4e8619.8%\u3002", "conclusion": "Temp-R1\u4e3a\u81ea\u4e3b\u65f6\u95f4\u63a8\u7406\u4ee3\u7406\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86TKGQA\u7684\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.17716", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17716", "abs": "https://arxiv.org/abs/2601.17716", "authors": ["Daniel M. Pedrozo", "Telma W. de L. Soares", "Bryan L. M. de Oliveira"], "title": "Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games", "comment": "Presented at the NeusymBridge Workshop at AAAI 2026", "summary": "Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u6307\u6807\u5b9a\u91cf\u8bc4\u4f30LLM\u5728\u5c42\u6b21\u77e5\u8bc6\u56fe\u73af\u5883\u4e2d\u901a\u8fc7\u662f/\u5426\u95ee\u9898\u6536\u96c6\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5e76\u5728\u5730\u7406\u731c\u57ce\u5e02\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u5177\u6709\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u65e0\u6cd5\u7cfb\u7edf\u6bd4\u8f83\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u4e0d\u4f7f\u7528\u601d\u7ef4\u94fe\u7684\u6a21\u578b\u3002LLM\u5728\u89e3\u51b3\u7528\u6237\u8bf7\u6c42\u6a21\u7cca\u6027\u65f6\u63d0\u95ee\u80fd\u529b\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u597d\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591a\u8f6e\u5bf9\u8bdd\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u4e2a\u4ea4\u4e92\u7684LLM\u4ee3\u7406\uff1a\u63d0\u95ee\u4ee3\u7406\u3001\u56de\u7b54\u4ee3\u7406\u548c\u5047\u8bbe\u7a7a\u95f4\u66f4\u65b0\u4ee3\u7406\u3002\u4f7f\u7528\u57fa\u4e8e\u9999\u519c\u71b5\u7684\u4fe1\u606f\u589e\u76ca\u4f5c\u4e3a\u4e3b\u8981\u6307\u6807\uff0c\u5728\u5730\u7406\u731c\u57ce\u5e02\u6e38\u620f\u4e2d\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\uff0c\u8bc4\u4f30\u4e0d\u540cLLM\u53d8\u4f53\u5728\u5b8c\u5168\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5177\u6709\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5728\u6bcf\u8f6e\u83b7\u5f97\u66f4\u9ad8\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u4e14\u7528\u66f4\u5c11\u6b65\u9aa4\u8fbe\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u8bbe\u7f6e\u4e0b\u3002\u5c0f\u6a21\u578b\u901a\u8fc7\u66f4\u79ef\u6781\u5730\u63a2\u7d22\u5019\u9009\u95ee\u9898\u6765\u5f25\u8865\u80fd\u529b\u9650\u5236\uff0c\u800c\u5927\u6a21\u578b\u5728\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u81ea\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30LLM\u7684\u4fe1\u606f\u6536\u96c6\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9a\u91cf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u663e\u5f0f\u63a8\u7406\u5bf9\u63d0\u9ad8\u63d0\u95ee\u6548\u7387\u7684\u91cd\u8981\u6027\uff0c\u4e3aLLM\u4ee3\u7406\u7684\u6a21\u7cca\u6027\u89e3\u51b3\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.18320", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.18320", "abs": "https://arxiv.org/abs/2601.18320", "authors": ["Jinwei Lu", "Yuanfeng Song", "Chen Zhang", "Raymond Chi-Wing Wong"], "title": "MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization", "comment": "Accepted to SIGMOD 2026", "summary": "Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.", "AI": {"tldr": "MultiVis-Agent\uff1a\u4e00\u4e2a\u903b\u8f91\u89c4\u5219\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u9760\u7684\u591a\u6a21\u6001\u53ef\u89c6\u5316\u751f\u6210\uff0c\u901a\u8fc7\u56db\u5c42\u903b\u8f91\u89c4\u5219\u63d0\u4f9b\u6570\u5b66\u53ef\u9760\u6027\u4fdd\u8bc1\uff0c\u5728\u590d\u6742\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u53ef\u89c6\u5316\u4efb\u52a1\u6d89\u53ca\u590d\u6742\u7684\u591a\u6a21\u6001\u9700\u6c42\uff08\u53c2\u8003\u56fe\u50cf\u3001\u4ee3\u7801\u793a\u4f8b\u3001\u8fed\u4ee3\u4f18\u5316\uff09\uff0c\u800c\u73b0\u6709\u7cfb\u7edf\u5b58\u5728\u5355\u6a21\u6001\u8f93\u5165\u3001\u4e00\u6b21\u6027\u751f\u6210\u3001\u5de5\u4f5c\u6d41\u7a0b\u50f5\u5316\u7b49\u6839\u672c\u9650\u5236\u3002LLM\u65b9\u6cd5\u867d\u7136\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5f15\u5165\u4e86\u53ef\u9760\u6027\u6311\u6218\uff0c\u5305\u62ec\u707e\u96be\u6027\u6545\u969c\u548c\u65e0\u9650\u5faa\u73af\u95ee\u9898\u3002", "method": "\u63d0\u51faMultiVis-Agent\u6846\u67b6\uff0c\u91c7\u7528\u903b\u8f91\u89c4\u5219\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u3002\u5f15\u5165\u56db\u5c42\u903b\u8f91\u89c4\u5219\u6846\u67b6\uff0c\u4e3a\u7cfb\u7edf\u53ef\u9760\u6027\u63d0\u4f9b\u6570\u5b66\u4fdd\u8bc1\uff0c\u540c\u65f6\u4fdd\u6301\u7075\u6d3b\u6027\u3002\u8fd9\u4e9b\u903b\u8f91\u89c4\u5219\u662f\u6307\u5bfcLLM\u63a8\u7406\u7684\u6570\u5b66\u7ea6\u675f\uff0c\u800c\u975e\u66ff\u4ee3LLM\u3002\u5f62\u5f0f\u5316\u4e86\u6db5\u76d6\u4ece\u57fa\u7840\u751f\u6210\u5230\u8fed\u4ee3\u4f18\u5316\u7684\u56db\u4e2a\u573a\u666f\u7684MultiVis\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u5305\u542b1000\u591a\u4e2a\u6848\u4f8b\u7684MultiVis-Bench\u57fa\u51c6\u3002", "result": "\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8fbe\u523075.63%\u7684\u53ef\u89c6\u5316\u5f97\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0857.54-62.79%\uff09\u3002\u4efb\u52a1\u5b8c\u6210\u7387\u8fbe\u523099.58%\uff0c\u4ee3\u7801\u6267\u884c\u6210\u529f\u7387\u8fbe\u523094.56%\uff08\u65e0\u903b\u8f91\u89c4\u5219\u65f6\u5206\u522b\u4e3a74.48%\u548c65.10%\uff09\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u52a8\u5316\u53ef\u89c6\u5316\u751f\u6210\u4e2d\u7684\u590d\u6742\u6027\u548c\u53ef\u9760\u6027\u6311\u6218\u3002", "conclusion": "MultiVis-Agent\u901a\u8fc7\u903b\u8f91\u89c4\u5219\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u591a\u6a21\u6001\u53ef\u89c6\u5316\u4efb\u52a1\u7684\u53ef\u9760\u751f\u6210\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u6570\u5b66\u53ef\u9760\u6027\u4fdd\u8bc1\uff0c\u4e3a\u81ea\u52a8\u5316\u53ef\u89c6\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.18352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18352", "abs": "https://arxiv.org/abs/2601.18352", "authors": ["Manjie Xu", "Isabella Yin", "Xinyi Tu", "Chi Zhang", "Yixin Zhu"], "title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning", "comment": null, "summary": "LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting \"Lava is Safe\"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\"\u8bed\u4e49\u60ef\u6027\"\u95ee\u9898\uff0c\u5373\u96be\u4ee5\u6291\u5236\u9884\u8bad\u7ec3\u5148\u9a8c\u77e5\u8bc6\u6765\u9002\u5e94\u52a8\u6001\u4e0a\u4e0b\u6587\u89c4\u5219\u3002\u901a\u8fc7\u300aBaba Is You\u300b\u6e38\u620f\u6d4b\u8bd5\u53d1\u73b0\uff0c\u66f4\u5927\u6a21\u578b\u53cd\u800c\u8868\u73b0\u66f4\u5dee\uff0c\u4f46\u5c06\u52a8\u6001\u89c4\u5219\u8868\u793a\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u800c\u975e\u63cf\u8ff0\u6027\u6587\u672c\u53ef\u4ee5\u9006\u8f6c\u8fd9\u4e00\u8d8b\u52bf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u52a8\u6001\u4e0a\u4e0b\u6587\u89c4\u5219\u65f6\u5b58\u5728\"\u8bed\u4e49\u60ef\u6027\"\u95ee\u9898\uff0c\u5373\u65e0\u6cd5\u6291\u5236\u9884\u8bad\u7ec3\u5148\u9a8c\u77e5\u8bc6\uff08\u5982\"\u5ca9\u6d46\u662f\u5371\u9669\u7684\"\uff09\u6765\u9002\u5e94\u77db\u76fe\u7684\u4e0a\u4e0b\u6587\u89c4\u5219\u3002\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u9700\u8981\u52a8\u6001\u8986\u76d6\u5b66\u4e60\u5148\u9a8c\u7684\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u300aBaba Is You\u300b\u6e38\u620f\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u4e2d\u7269\u7406\u6cd5\u5219\u53ef\u901a\u8fc7\u6587\u672c\u89c4\u5219\u6539\u53d8\u3002\u63d0\u51faCode-Grounded Vistas (LCV)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5904\u7406\u53cd\u4e8b\u5b9e\u5bf9\uff0c\u5e76\u8bc6\u522b\u5177\u6709\u77db\u76fe\u89c4\u5219\u7684\u72b6\u6001\uff0c\u5f3a\u5236\u6a21\u578b\u5173\u6ce8\u903b\u8f91\u7ea6\u675f\u800c\u975e\u89c6\u89c9\u8bed\u4e49\u3002", "result": "\u53d1\u73b0\u66f4\u5927\u6a21\u578b\u5728\u9700\u8981\u6291\u5236\u9884\u8bad\u7ec3\u5173\u8054\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u5dee\uff08\u9006\u7f29\u653e\u73b0\u8c61\uff09\u3002\u4f46\u5c06\u52a8\u6001\u8868\u793a\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u800c\u975e\u63cf\u8ff0\u6027\u6587\u672c\u53ef\u4ee5\u9006\u8f6c\u8fd9\u4e00\u8d8b\u52bf\u3002LCV\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u4f18\u4e8e\u6602\u8d35\u7684\u63a8\u7406\u65f6\u641c\u7d22\u65b9\u6cd5\u3002", "conclusion": "\u8868\u793a\u5f62\u5f0f\u4ece\u6839\u672c\u4e0a\u51b3\u5b9a\u4e86\u7f29\u653e\u662f\u5426\u6539\u5584\u6216\u635f\u5bb3\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u6311\u6218\u4e86\"\u66f4\u5927\u6a21\u578b\u603b\u662f\u66f4\u597d\"\u7684\u5047\u8bbe\uff0c\u5bf9\u9700\u8981\u52a8\u6001\u8986\u76d6\u5b66\u4e60\u5148\u9a8c\u7684\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2601.18533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18533", "abs": "https://arxiv.org/abs/2601.18533", "authors": ["Yuxin Jiang", "Yufei Wang", "Qiyuan Zhang", "Xingshan Zeng", "Liangyou Li", "Jierun Chen", "Chaofan Tao", "Haoli Bai", "Lifeng Shang"], "title": "From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation", "comment": "19 pages, 8 figures, 12 tables. Accepted at ICLR 2026", "summary": "Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.", "AI": {"tldr": "RLVRR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u53c2\u8003\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u9ad8\u8d28\u91cf\u53c2\u8003\u4e2d\u63d0\u53d6\u6709\u5e8f\u8bed\u8a00\u4fe1\u53f7\uff08\u5956\u52b1\u94fe\uff09\uff0c\u5c06\u5956\u52b1\u5206\u89e3\u4e3a\u5185\u5bb9\u548c\u98ce\u683c\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u7ed3\u5408\u4e86RL\u7684\u63a2\u7d22\u80fd\u529b\u548cSFT\u7684\u6548\u7387\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edfRLVR\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u660e\u786e\u7684\u5730\u9762\u771f\u503c\u3002\u5355\u70b9\u76d1\u7763\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7edf\u4e00\u7ed3\u6784\u5316\u63a8\u7406\u548c\u5f00\u653e\u5f0f\u751f\u6210\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "RLVRR\u4ece\u9ad8\u8d28\u91cf\u53c2\u8003\u4e2d\u63d0\u53d6\u6709\u5e8f\u8bed\u8a00\u4fe1\u53f7\uff08\u5956\u52b1\u94fe\uff09\uff0c\u5c06\u5956\u52b1\u5206\u89e3\u4e3a\u5185\u5bb9\u7ef4\u5ea6\uff08\u4fdd\u7559\u786e\u5b9a\u6027\u6838\u5fc3\u6982\u5ff5\u5982\u5173\u952e\u8bcd\uff09\u548c\u98ce\u683c\u7ef4\u5ea6\uff08\u901a\u8fc7LLM\u9a8c\u8bc1\u8bc4\u4f30\u98ce\u683c\u5c5e\u6027\u9075\u5faa\u7a0b\u5ea6\uff09\uff0c\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u4f18\u52bf\u548c\u76d1\u7763\u5fae\u8c03\u7684\u6548\u7387\u53ef\u9760\u6027\u3002", "result": "\u572810\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRLVRR\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u5341\u500d\u6570\u636e\u8bad\u7ec3\u548c\u5148\u8fdb\u5956\u52b1\u6a21\u578b\u7684SFT\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u548c\u5f00\u653e\u5f0f\u751f\u6210\u7684\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u591a\u6837\u6027\u7684\u540c\u65f6\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RLVRR\u4e3a\u901a\u7528LLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u6761\u539f\u5219\u6027\u4e14\u9ad8\u6548\u7684\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u5f0f\u751f\u6210\u4e2d\u7684\u76d1\u7763\u4fe1\u53f7\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a2\u7d22\u6548\u7387\u548c\u53ef\u9760\u6027\u7684\u5e73\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18552", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18552", "abs": "https://arxiv.org/abs/2601.18552", "authors": ["Devansh Srivastav", "David Pape", "Lea Sch\u00f6nherr"], "title": "Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection", "comment": null, "summary": "LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86LLM\u4e2d\u7684\u9690\u85cf\u610f\u56fe\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5341\u7c7b\u9690\u85cf\u610f\u56fe\u7684\u5206\u7c7b\u6cd5\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u8bf1\u5bfc\u8fd9\u4e9b\u884c\u4e3a\uff0c\u8bc4\u4f30\u4e86\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d1\u73b0\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e0b\u68c0\u6d4b\u4f1a\u5931\u6548\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u5d4c\u5165\u65e5\u5e38\u51b3\u7b56\uff0c\u4f46\u5176\u8f93\u51fa\u53ef\u80fd\u7f16\u7801\u5fae\u5999\u3001\u65e0\u610f\u7684\u884c\u4e3a\uff0c\u5f71\u54cd\u7528\u6237\u4fe1\u5ff5\u548c\u884c\u52a8\u3002\u8fd9\u4e9b\u9690\u85cf\u610f\u56fe\u53ef\u80fd\u6765\u81ea\u8bad\u7ec3\u4f18\u5316\u4f2a\u5f71\u6216\u88ab\u6076\u610f\u5f00\u53d1\u8005\u6545\u610f\u8bf1\u5bfc\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "1) \u63d0\u51fa\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u7684\u5341\u7c7b\u9690\u85cf\u610f\u56fe\u5206\u7c7b\u6cd5\uff1b2) \u5728\u53d7\u63a7\u6a21\u578b\u4e2d\u8bf1\u5bfc\u9690\u85cf\u610f\u56fe\uff1b3) \u7cfb\u7edf\u8bc4\u4f30\u5305\u62ec\u63a8\u7406\u548c\u975e\u63a8\u7406LLM\u6cd5\u5b98\u5728\u5185\u7684\u68c0\u6d4b\u65b9\u6cd5\uff1b4) \u5728\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff1b5) \u5bf9\u5df2\u90e8\u7f72\u7684SOTA LLM\u8fdb\u884c\u5b9a\u6027\u6848\u4f8b\u7814\u7a76\u3002", "result": "1) \u9690\u85cf\u610f\u56fe\u53ef\u4ee5\u5728\u53d7\u63a7\u6a21\u578b\u4e2d\u8f7b\u677e\u8bf1\u5bfc\uff1b2) \u5728\u73b0\u5b9e\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u68c0\u6d4b\u65b9\u6cd5\u5931\u6548\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6d41\u884c\u7387\u6761\u4ef6\u4e0b\uff1b3) \u538b\u529b\u6d4b\u8bd5\u663e\u793a\u5ba1\u8ba1\u5931\u8d25\uff0c\u9664\u975e\u6709\u6781\u4f4e\u7684\u8bef\u62a5\u7387\u6216\u5bf9\u64cd\u7eb5\u7c7b\u578b\u7684\u5f3a\u5148\u9a8c\uff1b4) \u6848\u4f8b\u7814\u7a76\u663e\u793a\u5341\u7c7b\u9690\u85cf\u610f\u56fe\u90fd\u5b58\u5728\u4e8e\u5df2\u90e8\u7f72\u7684SOTA LLM\u4e2d\u3002", "conclusion": "LLM\u4e2d\u7684\u9690\u85cf\u610f\u56fe\u68c0\u6d4b\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e0b\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u6846\u67b6\u3002\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u3001\u8bf1\u5bfc\u548c\u538b\u529b\u6d4b\u8bd5\u6b64\u7c7b\u884c\u4e3a\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5efa\u7acb\u4e86\u7075\u6d3b\u7684\u5206\u7c7b\u6cd5\u6765\u9884\u6d4b\u4e0d\u65ad\u6f14\u53d8\u7684\u5a01\u80c1\u5e76\u6307\u5bfc\u6cbb\u7406\u3002", "topic": "agent analysis"}}
{"id": "2601.18572", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18572", "abs": "https://arxiv.org/abs/2601.18572", "authors": ["Franziska Weeber", "Vera Neplenbroek", "Jan Batzner", "Sebastian Pad\u00f3"], "title": "One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization", "comment": null, "summary": "Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u79cd\u5e38\u7528\u7684\u4eba\u8bbe\u63d0\u793a\u65b9\u6cd5\u5728\u4e03\u4e2aLLM\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u4f1a\u4ea7\u751f\u663e\u8457\u5dee\u5f02\uff0c\u5efa\u8bae\u672a\u6765\u4e2a\u6027\u5316\u7814\u7a76\u5e94\u8bc4\u4f30\u591a\u79cd\u5916\u90e8\u6709\u6548\u63d0\u793a\u65b9\u6cd5", "motivation": "LLM\u4e2a\u6027\u5316\u867d\u7136\u80fd\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u6216\u653e\u5927\u7fa4\u4f53\u504f\u89c1\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u53ea\u4f7f\u7528\u5355\u4e00\u63d0\u793a\u65b9\u6cd5\uff08\u5982\u7528\u6237\u540d\u6216\u663e\u5f0f\u5c5e\u6027\uff09\u6765\u7814\u7a76\u504f\u89c1\uff0c\u5ffd\u7565\u4e86LLM\u5bf9\u63d0\u793a\u53d8\u5316\u7684\u654f\u611f\u6027\u548c\u67d0\u4e9b\u63d0\u793a\u5728\u771f\u5b9e\u4ea4\u4e92\u4e2d\u7684\u7f55\u89c1\u6027", "method": "\u6bd4\u8f83\u4e86\u516d\u79cd\u5e38\u7528\u7684\u4eba\u8bbe\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u4e03\u4e2a\u5f00\u6e90\u548c\u4e13\u6709LLM\u4e0a\u6d4b\u8bd5\u4e86\u56db\u4e2a\u5199\u4f5c\u548c\u5efa\u8bae\u4efb\u52a1\uff0c\u5206\u6790\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u4ea7\u751f\u7684\u54cd\u5e94\u5dee\u5f02", "result": "\u867d\u7136\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u603b\u4f53\u4e0a\u9ad8\u5ea6\u76f8\u5173\uff0c\u4f46\u5b83\u4eec\u5728\u54cd\u5e94\u4e2d\u4ea7\u751f\u4e86\u663e\u8457\u5dee\u5f02\u3002\u5355\u4e00\u63d0\u793a\u65b9\u6cd5\u5f97\u51fa\u7684\u7ed3\u8bba\u53ef\u80fd\u4e0d\u53ef\u9760", "conclusion": "\u5e94\u8c28\u614e\u57fa\u4e8e\u5355\u4e00\u63d0\u793a\u65b9\u6cd5\u505a\u51fa\u7ed3\u8bba\uff0c\u5efa\u8bae\u672a\u6765\u4e2a\u6027\u5316\u7814\u7a76\u8bc4\u4f30\u591a\u79cd\u5916\u90e8\u6709\u6548\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u7814\u7a76\u7684\u9c81\u68d2\u6027\u548c\u5916\u90e8\u6709\u6548\u6027", "topic": "agent analysis"}}
{"id": "2601.18081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18081", "abs": "https://arxiv.org/abs/2601.18081", "authors": ["Peixuan Han", "Yingjie Yu", "Jingjun Xu", "Jiaxuan You"], "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal", "comment": null, "summary": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.", "AI": {"tldr": "DRPG\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b66\u672f\u53cd\u9a73\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u8bc4\u5ba1\u610f\u89c1\u3001\u68c0\u7d22\u8bba\u6587\u8bc1\u636e\u3001\u89c4\u5212\u53cd\u9a73\u7b56\u7565\u548c\u751f\u6210\u56de\u5e94\u56db\u4e2a\u6b65\u9aa4\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f7f\u75288B\u6a21\u578b\u5373\u53ef\u8fbe\u5230\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u79d1\u7814\u5de5\u4f5c\u6d41\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b66\u672f\u53cd\u9a73\u8fd9\u4e00\u5b66\u672f\u4ea4\u6d41\u548c\u540c\u884c\u8bc4\u5ba1\u7684\u5173\u952e\u73af\u8282\u7684\u81ea\u52a8\u5316\u652f\u6301\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u73b0\u6210\u7684LLM\u6216\u7b80\u5355\u6d41\u7a0b\uff0c\u96be\u4ee5\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4e14\u65e0\u6cd5\u751f\u6210\u6709\u9488\u5bf9\u6027\u548c\u8bf4\u670d\u529b\u7684\u56de\u5e94\u3002", "method": "\u63d0\u51faDRPG\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a1) \u5c06\u8bc4\u5ba1\u610f\u89c1\u5206\u89e3\u4e3a\u539f\u5b50\u5173\u6ce8\u70b9\uff1b2) \u4ece\u8bba\u6587\u4e2d\u68c0\u7d22\u76f8\u5173\u8bc1\u636e\uff1b3) \u89c4\u5212\u53cd\u9a73\u7b56\u7565\uff1b4) \u636e\u6b64\u751f\u6210\u56de\u5e94\u3002\u5176\u4e2d\u89c4\u5212\u5668\u5728\u8bc6\u522b\u6700\u53ef\u884c\u53cd\u9a73\u65b9\u5411\u65b9\u9762\u51c6\u786e\u7387\u8d85\u8fc798%\u3002", "result": "\u5728\u9876\u7ea7\u4f1a\u8bae\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRPG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53cd\u9a73\u6d41\u7a0b\uff0c\u4ec5\u4f7f\u75288B\u6a21\u578b\u5373\u53ef\u8fbe\u5230\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u7684\u6027\u80fd\u3002\u89c4\u5212\u5668\u8bbe\u8ba1\u6709\u6548\uff0c\u80fd\u63d0\u4f9b\u591a\u89c6\u89d2\u548c\u53ef\u89e3\u91ca\u7684\u5efa\u8bae\uff0c\u4e14\u5728\u66f4\u590d\u6742\u7684\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "DRPG\u6846\u67b6\u6709\u6548\u5c55\u793a\u4e86\u5176\u751f\u6210\u9ad8\u8d28\u91cf\u53cd\u9a73\u5185\u5bb9\u548c\u652f\u6301\u5b66\u672f\u8ba8\u8bba\u89c4\u6a21\u5316\u7684\u6f5c\u529b\uff0c\u4e3a\u5b66\u672f\u53cd\u9a73\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.18091", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18091", "abs": "https://arxiv.org/abs/2601.18091", "authors": ["Longwei Ding", "Anhao Zhao", "Fanghua Ye", "Ziyang Chen", "Xiaoyu Shen"], "title": "From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models", "comment": "18 pages, 7 figures", "summary": "Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\\textbf{LLM-instruct}$) and reasoning-augmented ($\\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u6bd4\u4e86\u6307\u4ee4\u9075\u5faa\u578bLLM\u548c\u63a8\u7406\u589e\u5f3a\u578bLLM\u7684\u526a\u679d\u7b56\u7565\u6548\u679c\uff0c\u53d1\u73b0\u4e0d\u540c\u8303\u5f0f\u4e0b\u526a\u679d\u6548\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u9488\u5bf9\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u8bbe\u8ba1\u4e13\u95e8\u7684\u526a\u679d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6307\u4ee4\u9075\u5faa\u578bLLM\uff0c\u4e0d\u6e05\u695a\u8fd9\u4e9b\u7b56\u7565\u662f\u5426\u9002\u7528\u4e8e\u751f\u6210\u957f\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\u7684\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u3002\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u4e24\u79cd\u6a21\u578b\u8303\u5f0f\u7684\u526a\u679d\u6548\u679c\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u63a7\u5236\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5bf9\u9f50\u526a\u679d\u6821\u51c6\u548c\u6062\u590d\u6570\u636e\u4e0e\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u3002\u8bc4\u4f30\u9759\u6001\u6df1\u5ea6\u526a\u679d\u3001\u9759\u6001\u5bbd\u5ea6\u526a\u679d\u548c\u52a8\u6001\u526a\u679d\u572817\u4e2a\u5206\u7c7b\u3001\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6df1\u5ea6\u526a\u679d\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5bbd\u5ea6\u526a\u679d\u5728\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u66f4\u7a33\u5065\uff1b\u9759\u6001\u526a\u679d\u80fd\u66f4\u597d\u4fdd\u6301\u63a8\u7406\u6027\u80fd\uff0c\u52a8\u6001\u526a\u679d\u5728\u5206\u7c7b\u548c\u751f\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\u4f46\u5728\u957f\u94fe\u63a8\u7406\u4e0a\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "\u63a8\u7406\u589e\u5f3a\u578bLLM\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u7684\u526a\u679d\u7b56\u7565\uff0c\u4e0d\u80fd\u7b80\u5355\u6cbf\u7528\u6307\u4ee4\u9075\u5faa\u578b\u6a21\u578b\u7684\u526a\u679d\u65b9\u6cd5\u3002\u526a\u679d\u7b56\u7565\u5e94\u8003\u8651\u6a21\u578b\u8303\u5f0f\u7684\u72ec\u7279\u7279\u5f81\u3002", "topic": "agent analysis"}}
{"id": "2601.18730", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18730", "abs": "https://arxiv.org/abs/2601.18730", "authors": ["Henry Bell", "Caroline Zhang", "Mohammed Mobasserul Haque", "Dhaval Potdar", "Samia Zaman", "Brandon Fain"], "title": "Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale", "comment": null, "summary": "The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \\textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \\textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \\textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \\textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \\textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \\textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.", "AI": {"tldr": "REFLECT\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u6216\u6570\u636e\u7684\u63a8\u7406\u65f6\u5baa\u6cd5\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u4e2d\u7684\u81ea\u6211\u8bc4\u4f30\u3001\u81ea\u6211\u6279\u5224\u548c\u6700\u7ec8\u4fee\u8ba2\u6765\u5bf9\u9f50LLM\u4e0e\u4ef7\u503c\u539f\u5219\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\uff08\u5982RLHF\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9700\u8981\u7cbe\u5fc3\u5de5\u7a0b\u548c\u8c03\u4f18\uff0c\u4e14\u9700\u8981\u96be\u4ee5\u83b7\u53d6\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002\u9700\u8981\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "REFLECT\u662f\u5b8c\u5168\u5728\u4e0a\u4e0b\u6587\u4e2d\u64cd\u4f5c\u7684\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u5305\u542b\uff1a(i) \u57fa\u4e8e\u5baa\u6cd5\u7684\u57fa\u672c\u54cd\u5e94\u751f\u6210\uff0c(ii) \u81ea\u6211\u8bc4\u4f30\uff0c(iii)(a) \u81ea\u6211\u6279\u5224\uff0c\u548c(iii)(b) \u6700\u7ec8\u4fee\u8ba2\u3002\u901a\u8fc7\u663e\u5f0f\u7684\u539f\u5219\u63a8\u7406\u63d0\u4f9b\u900f\u660e\u63a8\u7406\u8f68\u8ff9\u3002", "result": "REFLECT\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5bf9\u591a\u6837\u590d\u6742\u539f\u5219\u7684\u7b26\u5408\u5ea6\uff0c\u5305\u62ec\u4e0e\u539f\u59cb\u53c2\u6570\u5fae\u8c03\u5f3a\u8c03\u7684\u539f\u5219\u5b8c\u5168\u4e0d\u540c\u7684\u539f\u5219\uff0c\u4e14\u4e0d\u727a\u7272\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\u3002\u7279\u522b\u6709\u6548\u51cf\u5c11\u7f55\u89c1\u4f46\u4e25\u91cd\u7684\u539f\u5219\u8fdd\u53cd\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "REFLECT\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u5baa\u6cd5\u5bf9\u9f50\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u7136\u751f\u6210\u6709\u7528\u7684\u8bad\u7ec3\u6570\u636e\u7528\u4e8e\u4f20\u7edf\u53c2\u6570\u5fae\u8c03\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\u548c\u51cf\u5c11\u63a8\u7406\u65f6\u8ba1\u7b97\u5f00\u9500\u3002", "topic": "agent analysis"}}
{"id": "2601.18142", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18142", "abs": "https://arxiv.org/abs/2601.18142", "authors": ["Mingxu Zhang", "Huicheng Zhang", "Jiaming Ji", "Yaodong Yang", "Ying Sun"], "title": "Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods", "comment": null, "summary": "Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\\%, establishing superior effectiveness for Safe RL in complex environments.", "AI": {"tldr": "\u63d0\u51faADRC-Lagrangian\u65b9\u6cd5\uff0c\u5c06\u4e3b\u52a8\u6297\u6270\u63a7\u5236\u4e0e\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u7ed3\u5408\uff0c\u663e\u8457\u51cf\u5c11\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u632f\u8361\u548c\u5b89\u5168\u8fdd\u89c4", "motivation": "\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5305\u62ecPID\u548c\u7ecf\u5178\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff09\u5b58\u5728\u632f\u8361\u548c\u9891\u7e41\u5b89\u5168\u8fdd\u89c4\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u53c2\u6570\u654f\u611f\u6027\u548c\u56fa\u6709\u76f8\u4f4d\u6ede\u540e\u5bfc\u81f4", "method": "\u63d0\u51faADRC-Lagrangian\u65b9\u6cd5\uff0c\u5c06\u4e3b\u52a8\u6297\u6270\u63a7\u5236\uff08ADRC\uff09\u96c6\u6210\u5230\u62c9\u683c\u6717\u65e5\u6846\u67b6\u4e2d\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u632f\u8361\u3002\u8be5\u7edf\u4e00\u6846\u67b6\u5c06\u7ecf\u5178\u548cPID\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u4f5c\u4e3a\u7279\u4f8b\u5305\u542b\u5728\u5185", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u5b89\u5168\u8fdd\u89c4\u51cf\u5c11\u9ad8\u8fbe74%\uff0c\u7ea6\u675f\u8fdd\u89c4\u5e45\u5ea6\u51cf\u5c1189%\uff0c\u5e73\u5747\u6210\u672c\u964d\u4f4e67%\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5b89\u5168\u6027\u80fd", "conclusion": "ADRC-Lagrangian\u65b9\u6cd5\u4e3a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u3001\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u5b89\u5168\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u632f\u8361\u95ee\u9898", "topic": "agentic reinforcement learning"}}
{"id": "2601.18150", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18150", "abs": "https://arxiv.org/abs/2601.18150", "authors": ["Zhaopeng Qiu", "Shuang Yu", "Jingqi Zhang", "Shuai Zhang", "Xue Huang", "Jingyi Yang", "Junjie Lai"], "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684FP8\u63a8\u7406\u6808\uff0c\u901a\u8fc7\u91cf\u5316\u3001KV\u7f13\u5b58\u4f18\u5316\u548c\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\uff0c\u5728\u4fdd\u6301\u5b66\u4e60\u6548\u679c\u7684\u540c\u65f6\u63d0\u534744%\u7684\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a8\u7406\uff08\u751f\u6210\uff09\u9636\u6bb5\u6210\u4e3a\u74f6\u9888\uff0c\u957f\u8f93\u51fa\u5e8f\u5217\u5bfc\u81f4\u6ce8\u610f\u529b\u673a\u5236\u548cKV\u7f13\u5b58\u5360\u7528\u5927\u91cf\u5185\u5b58\u548c\u65f6\u95f4\u3002FP8\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u53ef\u4ee5\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4f1a\u9762\u4e34\u6743\u91cd\u9891\u7e41\u53d8\u5316\u548c\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u7684\u6311\u6218\u3002", "method": "1) \u4f7f\u7528\u5206\u5757FP8\u91cf\u5316\u5b9e\u73b0W8A8\u7ebf\u6027\u5c42\u63a8\u7406\uff1b2) \u901a\u8fc7\u6bcf\u6b65QKV\u5c3a\u5ea6\u91cd\u6821\u51c6\u5c06FP8\u6269\u5c55\u5230KV\u7f13\u5b58\uff1b3) \u4f7f\u7528\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7684\u63a8\u7406\u6821\u6b63\uff08token\u7ea7TIS/MIS\u53d8\u4f53\uff09\u7f13\u89e3\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u5bc6\u96c6\u548cMoE\u6a21\u578b\u4e0a\uff0c\u8fd9\u4e9b\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u8fbe44%\u7684\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eBF16\u57fa\u7ebf\u76f8\u5f53\u7684\u5b66\u4e60\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684FP8\u63a8\u7406\u6808\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f4e\u7cbe\u5ea6\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5b58\u74f6\u9888\u548c\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18207", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18207", "abs": "https://arxiv.org/abs/2601.18207", "authors": ["James Burgess", "Jan N. Hansen", "Duo Peng", "Yuhui Zhang", "Alejandro Lozano", "Min Woo Sun", "Emma Lundberg", "Serena Yeung-Levy"], "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR", "comment": "EACL 2026", "summary": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u79d1\u5b66\u8bba\u6587\u641c\u7d22\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u62ec1600\u4e07\u7bc7\u751f\u7269\u533b\u5b66\u8bba\u6587\u6458\u8981\u7684\u641c\u7d22\u8bed\u6599\u5e93\u548c\u5305\u542b6\u4e07\u4e2a\u6837\u672c\u7684PaperSearchQA\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u641c\u7d22\u667a\u80fd\u4f53\u5728\u6280\u672f\u95ee\u7b54\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4f20\u7edf\u68c0\u7d22\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u667a\u80fd\u4f53\u4e3b\u8981\u9488\u5bf9\u901a\u7528\u9886\u57df\u95ee\u7b54\uff0c\u7f3a\u4e4f\u5bf9\u79d1\u5b66\u3001\u5de5\u7a0b\u548c\u533b\u5b66\u7b49\u4e13\u4e1a\u6280\u672f\u9886\u57df\u7684\u5173\u6ce8\u3002\u79d1\u5b66\u8bba\u6587\u641c\u7d22\u5bf9\u4e8eAI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u548c\u5b9e\u9645\u79d1\u7814\u5de5\u4f5c\u8005\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6280\u672f\u95ee\u7b54\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1600\u4e07\u7bc7\u751f\u7269\u533b\u5b66\u8bba\u6587\u6458\u8981\u7684\u641c\u7d22\u8bed\u6599\u5e93\uff0c\u521b\u5efa\u4e86\u5305\u542b6\u4e07\u4e2a\u6837\u672c\u7684PaperSearchQA\u95ee\u7b54\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8bad\u7ec3\u641c\u7d22\u667a\u80fd\u4f53\uff0c\u57fa\u4e8eSearch-R1\u4ee3\u7801\u5e93\u5b9e\u73b0\u3002", "result": "\u8bad\u7ec3\u7684\u641c\u7d22\u667a\u80fd\u4f53\u5728\u6280\u672f\u95ee\u7b54\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u975e\u5f3a\u5316\u5b66\u4e60\u7684\u68c0\u7d22\u57fa\u7ebf\u65b9\u6cd5\u3002\u89c2\u5bdf\u5230\u667a\u80fd\u4f53\u5c55\u73b0\u51fa\u89c4\u5212\u3001\u63a8\u7406\u548c\u81ea\u6211\u9a8c\u8bc1\u7b49\u6709\u8da3\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u641c\u7d22\u667a\u80fd\u4f53\u6269\u5c55\u5230\u79d1\u5b66\u9886\u57df\uff0c\u4e3a\u672a\u6765AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6570\u636e\u521b\u5efa\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u8f7b\u677e\u6269\u5c55\u5230\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u3002", "topic": "agent analysis"}}
{"id": "2601.18292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18292", "abs": "https://arxiv.org/abs/2601.18292", "authors": ["Zhewen Tan", "Wenhan Yu", "Jianfeng Si", "Tongxin Liu", "Kaiqi Guan", "Huiyan Jin", "Jiawen Tao", "Xiaokun Yuan", "Duohe Ma", "Xiangzheng Zhang", "Tong Yang", "Lin Sun"], "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment", "comment": null, "summary": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.", "AI": {"tldr": "TriPlay-RL\u662f\u4e00\u4e2a\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u653b\u51fb\u8005\u3001\u9632\u5fa1\u8005\u548c\u8bc4\u4f30\u8005\u4e09\u4e2a\u89d2\u8272\u7684\u8fed\u4ee3\u534f\u4f5c\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684LLM\u5b89\u5168\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u5404\u89d2\u8272\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u6709\u6548\u7f13\u89e3\u6709\u6bd2\u6709\u5bb3\u5185\u5bb9\u7684\u751f\u6210\u3002\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\uff0c\u4e14\u96be\u4ee5\u5b9e\u73b0\u653b\u51fb\u3001\u9632\u5fa1\u548c\u8bc4\u4f30\u4e09\u4e2a\u89d2\u8272\u7684\u534f\u540c\u6539\u8fdb\u3002", "method": "\u63d0\u51faTriPlay-RL\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u653b\u51fb\u8005\u751f\u6210\u5bf9\u6297\u63d0\u793a\u3001\u9632\u5fa1\u8005\u8fdb\u884c\u5b89\u5168\u9632\u5fa1\u3001\u8bc4\u4f30\u8005\u8bc4\u4f30\u54cd\u5e94\uff0c\u4e09\u4e2a\u89d2\u8272\u5728\u7edf\u4e00\u5b66\u4e60\u5faa\u73af\u4e2d\u8fed\u4ee3\u534f\u4f5c\uff0c\u5b9e\u73b0\u8fd1\u96f6\u4eba\u5de5\u6807\u6ce8\u7684\u534f\u540c\u8fdb\u5316\u3002", "result": "\u653b\u51fb\u8005\u4fdd\u6301\u9ad8\u8f93\u51fa\u591a\u6837\u6027\u7684\u540c\u65f6\uff0c\u5bf9\u6297\u6548\u679c\u63d0\u534720%-50%\uff1b\u9632\u5fa1\u8005\u5b89\u5168\u6027\u80fd\u63d0\u534710%-30%\u4e14\u4e0d\u635f\u5bb3\u4e00\u822c\u63a8\u7406\u80fd\u529b\uff1b\u8bc4\u4f30\u8005\u901a\u8fc7\u8fed\u4ee3\u6301\u7eed\u7ec6\u5316\u5224\u65ad\u80fd\u529b\uff0c\u80fd\u51c6\u786e\u533a\u5206\u4e0d\u5b89\u5168\u54cd\u5e94\u3001\u7b80\u5355\u62d2\u7edd\u548c\u6709\u7528\u6307\u5bfc\u3002", "conclusion": "TriPlay-RL\u4e3aLLM\u5b89\u5168\u5bf9\u9f50\u5efa\u7acb\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5728\u7edf\u4e00\u5b66\u4e60\u5faa\u73af\u4e2d\u7684\u6301\u7eed\u534f\u540c\u8fdb\u5316\uff0c\u4e3a\u89e3\u51b3LLM\u5b89\u5168\u98ce\u9669\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18510", "abs": "https://arxiv.org/abs/2601.18510", "authors": ["Yibo Li", "Zijie Lin", "Ailin Deng", "Xuan Zhang", "Yufei He", "Shuo Ji", "Tri Cao", "Bryan Hooi"], "title": "Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates", "comment": null, "summary": "While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.", "AI": {"tldr": "JitRL\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u52a8\u6001\u8bb0\u5fc6\u68c0\u7d22\u7684\u6d4b\u8bd5\u65f6\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u7ecf\u9a8c\u8f68\u8ff9\u5b9e\u65f6\u4f30\u8ba1\u52a8\u4f5c\u4f18\u52bf\u503c\uff0c\u76f4\u63a5\u8c03\u6574LLM\u8f93\u51falogits\uff0c\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u90e8\u7f72\u540e\u6743\u91cd\u56fa\u5b9a\uff0c\u96be\u4ee5\u6301\u7eed\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u68af\u5ea6\u66f4\u65b0\u3001\u4f4e\u6210\u672c\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "JitRL\u7ef4\u62a4\u52a8\u6001\u975e\u53c2\u6570\u5316\u8bb0\u5fc6\u5e93\uff0c\u68c0\u7d22\u76f8\u5173\u7ecf\u9a8c\u8f68\u8ff9\u5b9e\u65f6\u4f30\u8ba1\u52a8\u4f5c\u4f18\u52bf\u503c\uff0c\u901a\u8fc7\u52a0\u6cd5\u66f4\u65b0\u89c4\u5219\u76f4\u63a5\u8c03\u6574LLM\u8f93\u51falogits\uff0c\u8be5\u89c4\u5219\u88ab\u8bc1\u660e\u662fKL\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u76ee\u6807\u7684\u7cbe\u786e\u95ed\u5f0f\u89e3\u3002", "result": "\u5728WebArena\u548cJericho\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJitRL\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\uff0c\u751a\u81f3\u8d85\u8d8a\u8ba1\u7b97\u6602\u8d35\u7684\u5fae\u8c03\u65b9\u6cd5\uff08\u5982WebRL\uff09\uff0c\u540c\u65f6\u964d\u4f4e30\u500d\u4ee5\u4e0a\u7684\u6210\u672c\u3002", "conclusion": "JitRL\u4e3a\u6301\u7eed\u5b66\u4e60\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u8bb0\u5fc6\u68c0\u7d22\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18734", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18734", "abs": "https://arxiv.org/abs/2601.18734", "authors": ["Siyan Zhao", "Zhihui Xie", "Mengchen Liu", "Jing Huang", "Guan Pang", "Feiyu Chen", "Aditya Grover"], "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models", "comment": "13 pages", "summary": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.", "AI": {"tldr": "\u63d0\u51faOn-Policy Self-Distillation (OPSD)\u6846\u67b6\uff0c\u8ba9\u5355\u4e2a\u6a21\u578b\u65e2\u5f53\u8001\u5e08\u53c8\u5f53\u5b66\u751f\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u751f\u6210\u4e0d\u540c\u5206\u5e03\uff0c\u5229\u7528\u7279\u6743\u4fe1\u606f\uff08\u5982\u5df2\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9\uff09\u6765\u6307\u5bfc\u4ec5\u770b\u5230\u95ee\u9898\u7684\u5b66\u751f\u7248\u672c\uff0c\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e14\u901a\u5e38\u9700\u8981\u5355\u72ec\u7684\u6559\u5e08\u6a21\u578b\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u63a8\u7406\u6570\u636e\u96c6\u4e2d\u53ef\u7528\u7684\u771f\u5b9e\u89e3\u51b3\u65b9\u6848\u3002\u53d7\u542f\u53d1\u4e8e\"\u8db3\u591f\u5f3a\u5927\u7684LLM\u53ef\u4ee5\u5408\u7406\u5316\u5916\u90e8\u7279\u6743\u63a8\u7406\u8f68\u8ff9\u5e76\u6559\u5bfc\u5176\u8f83\u5f31\u7248\u672c\"\u7684\u76f4\u89c9\u3002", "method": "\u63d0\u51faOPSD\u6846\u67b6\uff1a\u5355\u4e2a\u6a21\u578b\u540c\u65f6\u4f5c\u4e3a\u6559\u5e08\u548c\u5b66\u751f\uff0c\u6559\u5e08\u7b56\u7565\u57fa\u4e8e\u7279\u6743\u4fe1\u606f\uff08\u5982\u5df2\u9a8c\u8bc1\u63a8\u7406\u8f68\u8ff9\uff09\uff0c\u5b66\u751f\u7b56\u7565\u4ec5\u770b\u5230\u95ee\u9898\uff1b\u8bad\u7ec3\u6700\u5c0f\u5316\u8fd9\u4e24\u4e2a\u5206\u5e03\u5728\u5b66\u751f\u81ea\u8eab\u8f68\u8ff9\u4e0a\u7684\u6bcf\u4ee4\u724c\u5dee\u5f02\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4GRPO\u7b49\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b04-8\u500d\u7684\u4ee4\u724c\u6548\u7387\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u79bb\u7b56\u7565\u84b8\u998f\u65b9\u6cd5\u3002", "conclusion": "OPSD\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u84b8\u998f\u6709\u6548\u5229\u7528\u7279\u6743\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u65e0\u9700\u5355\u72ec\u7684\u6559\u5e08\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18778", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18778", "abs": "https://arxiv.org/abs/2601.18778", "authors": ["Shobhita Sundaram", "John Quan", "Ariel Kwiatkowski", "Kartik Ahuja", "Yann Ollivier", "Julia Kempe"], "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability", "comment": null, "summary": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.", "AI": {"tldr": "SOAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5143\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u4e3a\u5b66\u751f\u6a21\u578b\u751f\u6210\u5408\u6210\u95ee\u9898\u8bfe\u7a0b\uff0c\u5229\u7528\u5b66\u751f\u8fdb\u6b65\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u521d\u59cb\u6210\u529f\u7387\u6781\u4f4e\uff080/128\uff09\u7684\u6570\u5b66\u57fa\u51c6\u4e0a\u5b9e\u73b0\u7a81\u7834\u6027\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u521d\u59cb\u6210\u529f\u7387\u4f4e\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u5bb9\u6613\u9677\u5165\u5b66\u4e60\u505c\u6ede\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8db3\u591f\u7684\u8bad\u7ec3\u4fe1\u53f7\u3002\u672c\u6587\u63a2\u7d22\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5229\u7528\u5176\u6f5c\u5728\u77e5\u8bc6\u4e3a\u81ea\u8eab\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u751f\u6210\u81ea\u52a8\u5316\u8bfe\u7a0b\u3002", "method": "\u63d0\u51faSOAR\u6846\u67b6\uff1a\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u526f\u672c\u4e3a\u5b66\u751f\u6a21\u578b\u526f\u672c\u751f\u6210\u5408\u6210\u95ee\u9898\uff0c\u6559\u5e08\u6839\u636e\u5b66\u751f\u5728\u56f0\u96be\u95ee\u9898\u5b50\u96c6\u4e0a\u7684\u8fdb\u6b65\u83b7\u5f97\u5956\u52b1\u3002\u5173\u952e\u521b\u65b0\u662f\u4f7f\u7528\u5b66\u751f\u5b9e\u9645\u8fdb\u6b65\u4f5c\u4e3a\u57fa\u7840\u5956\u52b1\uff0c\u800c\u975e\u5185\u5728\u4ee3\u7406\u5956\u52b1\u3002", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u6700\u56f0\u96be\u5b50\u96c6\uff08\u521d\u59cb\u6210\u529f\u73870/128\uff09\u4e0a\u5b9e\u73b0\u7a81\u7834\uff1a1\uff09\u8bc1\u660e\u4e86\u53cc\u5c42\u5143\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u4e8c\u5143\u5956\u52b1\u4e0b\u80fd\u591f\u89e3\u9501\u5b66\u4e60\uff1b2\uff09\u57fa\u7840\u5956\u52b1\u4f18\u4e8e\u5148\u524dLLM\u81ea\u73a9\u7684\u5185\u5728\u5956\u52b1\u65b9\u6848\uff0c\u907f\u514d\u4e0d\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\u5d29\u6e83\uff1b3\uff09\u751f\u6210\u95ee\u9898\u7684\u7ed3\u6784\u8d28\u91cf\u548c\u660e\u786e\u6027\u6bd4\u89e3\u51b3\u65b9\u6848\u6b63\u786e\u6027\u5bf9\u5b66\u4e60\u8fdb\u6b65\u66f4\u91cd\u8981\u3002", "conclusion": "\u6a21\u578b\u751f\u6210\u6709\u7528\"\u57ab\u811a\u77f3\"\u7684\u80fd\u529b\u4e0d\u9700\u8981\u9884\u5148\u5177\u5907\u89e3\u51b3\u56f0\u96be\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8fd9\u4e3a\u65e0\u9700\u989d\u5916\u7b56\u5212\u6570\u636e\u5373\u53ef\u7a81\u7834\u63a8\u7406\u505c\u6ede\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18779", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18779", "abs": "https://arxiv.org/abs/2601.18779", "authors": ["Yuxiao Qu", "Amrith Setlur", "Virginia Smith", "Ruslan Salakhutdinov", "Aviral Kumar"], "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration", "comment": null, "summary": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.", "AI": {"tldr": "\u63d0\u51faPOPE\u65b9\u6cd5\uff0c\u5229\u7528\u7279\u6743\u4fe1\u606f\uff08\u5982\u4eba\u7c7b\u89e3\u51b3\u65b9\u6848\uff09\u5f15\u5bfcLLM\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u63a2\u7d22\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5728\u6311\u6218\u6027\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8bad\u7ec3LLM\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u9762\u5bf9\u56f0\u96be\u95ee\u9898\u7ecf\u5e38\u65e0\u6cd5\u63a2\u7d22\u5230\u4efb\u4f55\u6b63\u786e\u8f68\u8ff9\uff0c\u5bfc\u81f4\u96f6\u5956\u52b1\u548c\u7f3a\u4e4f\u5b66\u4e60\u4fe1\u53f7\u3002\u4f20\u7edfRL\u63a2\u7d22\u65b9\u6cd5\uff08\u5982\u71b5\u5956\u52b1\u3001\u91cd\u8981\u6027\u6bd4\u7387\u88c1\u526a\uff09\u65e0\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u800c\u6df7\u5408\u96be\u6613\u95ee\u9898\u8bad\u7ec3\u4f1a\u4ea7\u751f\u5c04\u7ebf\u5e72\u6270\uff0c\u4f7f\u4f18\u5316\u96c6\u4e2d\u5728\u5df2\u89e3\u51b3\u95ee\u9898\u4e0a\u800c\u963b\u788d\u56f0\u96be\u95ee\u9898\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u7279\u6743\u5728\u7ebf\u63a2\u7d22\uff08POPE\uff09\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u4eba\u7c7b\u6216\u5176\u4ed6oracle\u89e3\u51b3\u65b9\u6848\u4f5c\u4e3a\u7279\u6743\u4fe1\u606f\u5f15\u5bfc\u56f0\u96be\u95ee\u9898\u7684\u63a2\u7d22\uff1b2\uff09\u5728\u56f0\u96be\u95ee\u9898\u524d\u6dfb\u52a0oracle\u89e3\u51b3\u65b9\u6848\u7684\u524d\u7f00\uff0c\u4f7fRL\u5728\u5f15\u5bfc\u8f68\u8ff9\u4e2d\u83b7\u5f97\u975e\u96f6\u5956\u52b1\uff1b3\uff09\u901a\u8fc7\u6307\u4ee4\u8ddf\u968f\u548c\u63a8\u7406\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5c06\u4e60\u5f97\u7684\u884c\u4e3a\u8fc1\u79fb\u56de\u539f\u59cb\u65e0\u5f15\u5bfc\u95ee\u9898\u3002", "result": "POPE\u6269\u5c55\u4e86\u53ef\u89e3\u51b3\u95ee\u9898\u7684\u96c6\u5408\uff0c\u5728\u6311\u6218\u6027\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86RL\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u63a2\u7d22\u4e0d\u8db3\u95ee\u9898\u3002", "conclusion": "POPE\u65b9\u6cd5\u901a\u8fc7\u7279\u6743\u4fe1\u606f\u5f15\u5bfc\u63a2\u7d22\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u56f0\u96be\u63a8\u7406\u95ee\u9898\u7684\u63a2\u7d22\u6311\u6218\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u679c\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18586", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18586", "abs": "https://arxiv.org/abs/2601.18586", "authors": ["Miguel Costa", "Arthur Vandervoort", "Carolin Schmidt", "Morten W. Petersen", "Martin Drews", "Karyn Morrissey", "Francisco C. Pereira"], "title": "Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning", "comment": null, "summary": "Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u96c6\u6210\u8bc4\u4f30\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u5728\u6c14\u5019\u53d8\u5316\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u591a\u5341\u5e74\u81ea\u9002\u5e94\u6295\u8d44\u8def\u5f84", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u964d\u96e8\u7b49\u707e\u5bb3\uff0c\u589e\u52a0\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u4e2d\u65ad\u98ce\u9669\u3002\u7531\u4e8e\u57fa\u7840\u8bbe\u65bd\u6295\u8d44\u7684\u957f\u671f\u6027\u3001\u5e8f\u5217\u6027\u3001\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u548c\u8de8\u90e8\u95e8\u590d\u6742\u6027\uff0c\u8bbe\u8ba1\u6709\u6548\u7684\u9002\u5e94\u7b56\u7565\u5177\u6709\u6311\u6218\u6027", "method": "\u63d0\u51fa\u901a\u7528\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u5c06\u96c6\u6210\u8bc4\u4f30\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u8026\u5408\uff0c\u7ed3\u5408\u957f\u671f\u6c14\u5019\u9884\u6d4b\u3001\u707e\u5bb3\u6982\u7387\u6620\u5c04\u3001\u57fa\u7840\u8bbe\u65bd\u5f71\u54cd\u4f20\u64ad\u548c\u4ef7\u503c\u8bc4\u4f30\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\u5b66\u4e60\u81ea\u9002\u5e94\u6c14\u5019\u9002\u5e94\u7b56\u7565", "result": "\u5728\u54e5\u672c\u54c8\u6839\u5e02\u7684\u57ce\u5e02\u5185\u6d9d\u6848\u4f8b\u4e2d\uff082024-2100\u5e74\uff09\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u4ea7\u751f\u534f\u8c03\u7684\u65f6\u7a7a\u8def\u5f84\uff0c\u76f8\u6bd4\u4f20\u7edf\u4f18\u5316\u57fa\u51c6\uff08\u4e0d\u4f5c\u4e3a\u548c\u968f\u673a\u884c\u52a8\uff09\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u53ef\u8fc1\u79fb\u5230\u5176\u4ed6\u707e\u5bb3\u548c\u57ce\u5e02\u7684\u6f5c\u529b\uff0c\u4e3a\u57ce\u5e02\u6c14\u5019\u9002\u5e94\u63d0\u4f9b\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177", "topic": "agentic reinforcement learning"}}
{"id": "2601.18795", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18795", "abs": "https://arxiv.org/abs/2601.18795", "authors": ["Amrith Setlur", "Zijian Wang", "Andrew Cohen", "Paria Rashidinejad", "Sang Michael Xie"], "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes", "comment": null, "summary": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.", "AI": {"tldr": "PrefixRL\uff1a\u901a\u8fc7\u91cd\u7528\u79bb\u7b56\u7565\u8f68\u8ff9\u7684\u524d\u7f00\u6765\u63d0\u5347LLM\u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u7387\uff0c\u907f\u514d\u79bb\u7b56\u7565\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u5b9e\u73b02\u500d\u8bad\u7ec3\u52a0\u901f\u548c3\u500d\u6700\u7ec8\u5956\u52b1\u63d0\u5347", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5728LLM\u63a8\u7406\u7684\u56f0\u96be\u95ee\u9898\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u6b63\u786e\u7684\u5728\u7ebf\u8f68\u8ff9\u7a00\u5c11\u3001\u7b56\u7565\u68af\u5ea6\u6d88\u5931\u3001\u5b66\u4e60\u505c\u6ede\u3002\u9700\u8981\u91cd\u7528\u65e7\u7684\u91c7\u6837\u8ba1\u7b97\uff08\u6765\u81ea\u5148\u524d\u63a8\u7406\u6216RL\u8bad\u7ec3\uff09\u6765\u63d0\u5347\u6548\u7387\uff0c\u4f46\u6807\u51c6\u79bb\u7b56\u7565\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898", "method": "PrefixRL\uff1a\u4f7f\u7528\u6210\u529f\u79bb\u7b56\u7565\u8f68\u8ff9\u7684\u524d\u7f00\u4f5c\u4e3a\u6761\u4ef6\uff0c\u8fd0\u884c\u5728\u7ebfRL\u6765\u5b8c\u6210\u5269\u4f59\u90e8\u5206\u3002\u901a\u8fc7\u8c03\u8282\u524d\u7f00\u957f\u5ea6\u6765\u8c03\u5236\u95ee\u9898\u96be\u5ea6\uff0c\u907f\u514d\u79bb\u7b56\u7565\u4e0d\u7a33\u5b9a\u6027\u3002\u79bb\u7b56\u7565\u8f68\u8ff9\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u7684\u62d2\u7edd\u91c7\u6837\u83b7\u53d6\uff0c\u5f62\u6210\u81ea\u6211\u6539\u8fdb\u5faa\u73af", "result": "\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\uff0cPrefixRL\u8fbe\u5230\u76f8\u540c\u8bad\u7ec3\u5956\u52b1\u7684\u901f\u5ea6\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff08\u5728\u79bb\u7b56\u7565\u6570\u636e\u4e0a\u8fdb\u884cSFT\u7136\u540eRL\uff09\u5feb2\u500d\uff0c\u6700\u7ec8\u5956\u52b1\u63d0\u53473\u500d\u3002\u589e\u76ca\u53ef\u8fc1\u79fb\u5230\u4fdd\u7559\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5373\u4f7f\u79bb\u7b56\u7565\u8f68\u8ff9\u6765\u81ea\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e5f\u6709\u6548", "conclusion": "PrefixRL\u901a\u8fc7\u91cd\u7528\u79bb\u7b56\u7565\u8f68\u8ff9\u524d\u7f00\u6709\u6548\u63d0\u5347RL\u6548\u7387\uff0c\u907f\u514d\u4e86\u79bb\u7b56\u7565\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u7075\u6d3b\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2601.18753", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18753", "abs": "https://arxiv.org/abs/2601.18753", "authors": ["Xinyue Zeng", "Junhong Lin", "Yujun Yan", "Feng Guo", "Liang Shi", "Jun Wu", "Dawei Zhou"], "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs", "comment": "Have been accepted by ICLR'26", "summary": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Hallucination Risk Bound\u7406\u8bba\u6846\u67b6\uff0c\u5c06LLM\u5e7b\u89c9\u98ce\u9669\u5206\u89e3\u4e3a\u6570\u636e\u9a71\u52a8\u548c\u63a8\u7406\u9a71\u52a8\u4e24\u90e8\u5206\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86HalluGuard\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "LLM\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u53ef\u9760\u6027\u5e38\u56e0\u5e7b\u89c9\u95ee\u9898\u800c\u53d7\u635f\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u53ea\u9488\u5bf9\u5355\u4e00\u5e7b\u89c9\u6765\u6e90\uff0c\u4e14\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u590d\u6742\u573a\u666f\u3002", "method": "1. \u63d0\u51faHallucination Risk Bound\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u98ce\u9669\u6b63\u5f0f\u5206\u89e3\u4e3a\u6570\u636e\u9a71\u52a8\uff08\u8bad\u7ec3\u65f6\u5931\u914d\uff09\u548c\u63a8\u7406\u9a71\u52a8\uff08\u63a8\u7406\u65f6\u4e0d\u7a33\u5b9a\u6027\uff09\u4e24\u90e8\u5206\uff1b2. \u57fa\u4e8e\u6b64\u5f00\u53d1HalluGuard\uff0c\u5229\u7528NTK\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u6355\u83b7\u7684\u8868\u5f81\u6765\u8054\u5408\u8bc6\u522b\u4e24\u7c7b\u5e7b\u89c9\u3002", "result": "\u572810\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u300111\u4e2a\u7ade\u4e89\u6027\u57fa\u7ebf\u548c9\u4e2a\u6d41\u884cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8bc4\u4f30\uff0cHalluGuard\u5728\u68c0\u6d4b\u591a\u79cdLLM\u5e7b\u89c9\u5f62\u5f0f\u65b9\u9762\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Hallucination Risk Bound\u4e3a\u5206\u6790\u5e7b\u89c9\u4ea7\u751f\u548c\u6f14\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0cHalluGuard\u4f5c\u4e3a\u5176\u5b9e\u8df5\u5e94\u7528\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4bLLM\u4e2d\u7684\u591a\u79cd\u5e7b\u89c9\u7c7b\u578b\uff0c\u63d0\u5347LLM\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.db662d5b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTBuUb7/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/8yzasiTIDyngLT1IR4YOebkBBY0jz_Vcg2xinEAfIu8=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTBuUb7/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/8yzasiTIDyngLT1IR4YOebkBBY0jz_Vcg2xinEAfIu8=441", "authors": ["TLDR Newsletter"], "title": "We're turning Todos into Tasks in Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTBuUb7/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/8yzasiTIDyngLT1IR4YOebkBBY0jz_Vcg2xinEAfIu8=441", "summary": "We're turning Todos into Tasks in Claude Code (2 minute read) Anthropic has upgraded Todos in Claude Code to Tasks, a new primitive that helps Claude Code track and complete more complicated projects and collaborate on them across multiple sessions or subagents. Claude can create Tasks with dependencies on each other that are stored in the metadata, mirroring how projects work. Tasks are stored in the file system so that multiple subagents or sessions can collaborate on them. Updates are broa...", "source": "tldr", "AI": {"tldr": "Anthropic\u5c06Claude Code\u4e2d\u7684Todos\u5347\u7ea7\u4e3aTasks\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u539f\u8bed\uff0c\u5e2e\u52a9Claude Code\u8ddf\u8e2a\u548c\u5b8c\u6210\u66f4\u590d\u6742\u7684\u9879\u76ee\uff0c\u5e76\u652f\u6301\u8de8\u591a\u4e2a\u4f1a\u8bdd\u6216\u5b50\u4ee3\u7406\u7684\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684Todos\u529f\u80fd\u5728\u5904\u7406\u590d\u6742\u9879\u76ee\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9879\u76ee\u7ba1\u7406\u80fd\u529b\u6765\u652f\u6301\u591a\u4f1a\u8bdd\u534f\u4f5c\u548c\u4f9d\u8d56\u5173\u7cfb\u7ba1\u7406\u3002", "method": "\u5c06Todos\u5347\u7ea7\u4e3aTasks\u539f\u8bed\uff0c\u652f\u6301\u4efb\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c06\u4efb\u52a1\u5b58\u50a8\u5728\u6587\u4ef6\u7cfb\u7edf\u5143\u6570\u636e\u4e2d\uff0c\u5141\u8bb8\u591a\u4e2a\u5b50\u4ee3\u7406\u6216\u4f1a\u8bdd\u534f\u4f5c\u5904\u7406\u3002", "result": "Claude Code\u73b0\u5728\u80fd\u591f\u66f4\u597d\u5730\u7ba1\u7406\u590d\u6742\u9879\u76ee\uff0c\u652f\u6301\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u8de8\u4f1a\u8bdd\u534f\u4f5c\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u9879\u76ee\u7684\u7ba1\u7406\u80fd\u529b\u3002", "conclusion": "Tasks\u539f\u8bed\u7684\u5f15\u5165\u663e\u8457\u589e\u5f3a\u4e86Claude Code\u7684\u9879\u76ee\u7ba1\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5904\u7406\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5f00\u53d1\u4efb\u52a1\u3002", "topic": "code agent"}}
{"id": "tldr.2601.62b4e59c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.copilotkit.ai%2Fblog%2Fhow-to-build-a-frontend-for-langchain-deep-agents-with-copilotkit%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/whSwGF1p0QZ9WMyg3xkGzWPcUqlkCmhyR4ewxAPFebc=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.copilotkit.ai%2Fblog%2Fhow-to-build-a-frontend-for-langchain-deep-agents-with-copilotkit%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/whSwGF1p0QZ9WMyg3xkGzWPcUqlkCmhyR4ewxAPFebc=441", "authors": ["TLDR Newsletter"], "title": "Building Deep Agent Frontends", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Reading time: 38 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.copilotkit.ai%2Fblog%2Fhow-to-build-a-frontend-for-langchain-deep-agents-with-copilotkit%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/whSwGF1p0QZ9WMyg3xkGzWPcUqlkCmhyR4ewxAPFebc=441", "summary": "Building Deep Agent Frontends (38 minute read) CopilotKit published a tutorial on building a fullstack Deep Agent app that includes resume ingestion, skill extraction, sub-agents with web search, and a streaming UI.", "source": "tldr", "AI": {"tldr": "CopilotKit\u53d1\u5e03\u4e86\u4e00\u4e2a\u5173\u4e8e\u6784\u5efa\u5168\u6808Deep Agent\u5e94\u7528\u7684\u6559\u7a0b\uff0c\u5305\u542b\u7b80\u5386\u89e3\u6790\u3001\u6280\u80fd\u63d0\u53d6\u3001\u5e26\u7f51\u7edc\u641c\u7d22\u7684\u5b50\u4ee3\u7406\u548c\u6d41\u5f0fUI", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6784\u5efa\u6df1\u5ea6\u667a\u80fd\u4ee3\u7406\u5e94\u7528\u7684\u5b9e\u7528\u6307\u5357\uff0c\u5c55\u793a\u5982\u4f55\u5c06AI\u4ee3\u7406\u6280\u672f\u96c6\u6210\u5230\u5b9e\u9645\u5e94\u7528\u4e2d", "method": "\u901a\u8fc7CopilotKit\u6846\u67b6\u6784\u5efa\u5168\u6808\u5e94\u7528\uff0c\u5305\u542b\u7b80\u5386\u6570\u636e\u89e3\u6790\u3001\u6280\u80fd\u7279\u5f81\u63d0\u53d6\u3001\u591a\u5b50\u4ee3\u7406\u67b6\u6784\uff08\u652f\u6301\u7f51\u7edc\u641c\u7d22\uff09\u548c\u6d41\u5f0f\u7528\u6237\u754c\u9762", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6df1\u5ea6\u4ee3\u7406\u5e94\u7528\u5b9e\u73b0\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4ece\u6570\u636e\u5904\u7406\u5230\u7528\u6237\u4ea4\u4e92\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b", "conclusion": "CopilotKit\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u6784\u5efa\u590d\u6742\u7684\u6df1\u5ea6\u4ee3\u7406\u5e94\u7528\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u548c\u5b9e\u73b0\u53c2\u8003", "topic": "code agent"}}
{"id": "tldr.2601.ba7ca273", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fsalesforce%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/q4l6QvjIhxWyk27GvcN8Ebkrsqg2UngOSZsdkVMi3vA=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fsalesforce%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/q4l6QvjIhxWyk27GvcN8Ebkrsqg2UngOSZsdkVMi3vA=441", "authors": ["TLDR Newsletter"], "title": "Salesforce Adopts Cursor at Scale", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fsalesforce%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/q4l6QvjIhxWyk27GvcN8Ebkrsqg2UngOSZsdkVMi3vA=441", "summary": "Salesforce Adopts Cursor at Scale (3 minute read) Over 90% of Salesforce's 20,000 engineers now use Cursor in daily workflows, improving development speed and code quality. The shift has helped power product releases like Agentforce and signals broader industry trends in AI-assisted software engineering.", "source": "tldr", "AI": {"tldr": "Salesforce\u5927\u89c4\u6a21\u91c7\u7528Cursor AI\u7f16\u7a0b\u52a9\u624b\uff0c\u8d85\u8fc790%\u76842\u4e07\u540d\u5de5\u7a0b\u5e08\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u901f\u5ea6\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u5e76\u63a8\u52a8\u4e86Agentforce\u7b49\u4ea7\u54c1\u53d1\u5e03", "motivation": "\u4f01\u4e1a\u9700\u8981\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0cAI\u8f85\u52a9\u7f16\u7a0b\u5de5\u5177\u80fd\u591f\u5e2e\u52a9\u5de5\u7a0b\u5e08\u66f4\u9ad8\u6548\u5730\u5de5\u4f5c\uff0c\u8ddf\u4e0a\u884c\u4e1a\u6280\u672f\u53d1\u5c55\u8d8b\u52bf", "method": "\u5728Salesforce\u516c\u53f8\u8303\u56f4\u5185\u5927\u89c4\u6a21\u90e8\u7f72\u548c\u91c7\u7528Cursor AI\u7f16\u7a0b\u52a9\u624b\uff0c\u8ba92\u4e07\u540d\u5de5\u7a0b\u5e08\u5728\u65e5\u5e38\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u4f7f\u7528\u8be5\u5de5\u5177", "result": "\u8d85\u8fc790%\u7684\u5de5\u7a0b\u5e08\u91c7\u7528Cursor\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u53d1\u901f\u5ea6\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u6210\u529f\u63a8\u52a8\u4e86Agentforce\u7b49\u4ea7\u54c1\u53d1\u5e03\uff0c\u53cd\u6620\u4e86AI\u8f85\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u884c\u4e1a\u8d8b\u52bf", "conclusion": "AI\u7f16\u7a0b\u52a9\u624b\u5728\u4f01\u4e1a\u7ea7\u89c4\u6a21\u90e8\u7f72\u662f\u53ef\u884c\u7684\uff0c\u80fd\u5e26\u6765\u663e\u8457\u7684\u5f00\u53d1\u6548\u7387\u63d0\u5347\uff0c\u4ee3\u8868\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u884c\u4e1a\u7684\u91cd\u8981\u53d1\u5c55\u65b9\u5411", "topic": "swe application"}}
{"id": "tldr.2601.2f87ed0c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.lukaszolejnik.com%2Fsupply-chain-risk-of-agentic-ai-infecting-infrastructures-via-skill-worms%2F%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/lSc0M6dJecJeY3minpDAnJZnVY4G05xVdRcXU0ZG8rI=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.lukaszolejnik.com%2Fsupply-chain-risk-of-agentic-ai-infecting-infrastructures-via-skill-worms%2F%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/lSc0M6dJecJeY3minpDAnJZnVY4G05xVdRcXU0ZG8rI=441", "authors": ["TLDR Newsletter"], "title": "Supply-chain risk of agentic AI - infecting infrastructures via skill worms", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.lukaszolejnik.com%2Fsupply-chain-risk-of-agentic-ai-infecting-infrastructures-via-skill-worms%2F%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/lSc0M6dJecJeY3minpDAnJZnVY4G05xVdRcXU0ZG8rI=441", "summary": "Supply-chain risk of agentic AI - infecting infrastructures via skill worms (2 minute read) Skills added to AI assistants can introduce significant security risks by executing shell, network, and filesystem commands.", "source": "tldr", "AI": {"tldr": "AI\u52a9\u624b\u6280\u80fd\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u53ef\u80fd\u901a\u8fc7\u6267\u884cshell\u3001\u7f51\u7edc\u548c\u6587\u4ef6\u7cfb\u7edf\u547d\u4ee4\u611f\u67d3\u57fa\u7840\u8bbe\u65bd", "motivation": "AI\u52a9\u624b\u6dfb\u52a0\u7684\u6280\u80fd\u53ef\u80fd\u5f15\u5165\u91cd\u5927\u5b89\u5168\u98ce\u9669\uff0c\u8fd9\u4e9b\u6280\u80fd\u80fd\u591f\u6267\u884c\u5e95\u5c42\u7cfb\u7edf\u547d\u4ee4\uff0c\u53ef\u80fd\u5bfc\u81f4\u4f9b\u5e94\u94fe\u98ce\u9669", "method": "\u5206\u6790AI\u52a9\u624b\u6280\u80fd\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u80fd\u591f\u6267\u884cshell\u547d\u4ee4\u3001\u7f51\u7edc\u64cd\u4f5c\u548c\u6587\u4ef6\u7cfb\u7edf\u8bbf\u95ee\u7684\u6280\u80fd", "result": "\u53d1\u73b0AI\u52a9\u624b\u6280\u80fd\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u53ef\u80fd\u901a\u8fc7\"\u6280\u80fd\u8815\u866b\"\u611f\u67d3\u57fa\u7840\u8bbe\u65bd", "conclusion": "\u9700\u8981\u52a0\u5f3a\u5bf9AI\u52a9\u624b\u6280\u80fd\u7684\u5b89\u5168\u5ba1\u67e5\u548c\u9632\u62a4\u63aa\u65bd\uff0c\u9632\u6b62\u4f9b\u5e94\u94fe\u98ce\u9669", "topic": "agent analysis"}}
{"id": "tldr.2601.68b0c6bb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fd45vni/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/qapk8IvvAxKNsxP_oemSrTOPANpr3Ifxuj2yr5gtnt4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fd45vni/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/qapk8IvvAxKNsxP_oemSrTOPANpr3Ifxuj2yr5gtnt4=441", "authors": ["TLDR Newsletter"], "title": "Securing Agents in Production", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fd45vni/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/qapk8IvvAxKNsxP_oemSrTOPANpr3Ifxuj2yr5gtnt4=441", "summary": "Securing Agents in Production (11 minute read) This post examines how Palantir's AIP Agentic Runtime secures agents across the full operational life cycle.", "source": "tldr", "AI": {"tldr": "Palantir\u7684AIP Agentic Runtime\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u5168\u751f\u547d\u5468\u671f\u5b89\u5168\u4fdd\u969c", "motivation": "\u968f\u7740\u667a\u80fd\u4f53\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u786e\u4fdd\u5176\u5728\u5b8c\u6574\u64cd\u4f5c\u751f\u547d\u5468\u671f\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u9632\u6b62\u6f5c\u5728\u98ce\u9669\u548c\u5b89\u5168\u6f0f\u6d1e", "method": "\u91c7\u7528AIP Agentic Runtime\u6846\u67b6\uff0c\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4ece\u5f00\u53d1\u5230\u90e8\u7f72\u3001\u8fd0\u884c\u5230\u9000\u5f79\u7684\u5168\u751f\u547d\u5468\u671f\u5b89\u5168\u9632\u62a4\u673a\u5236", "result": "\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8fd0\u884c\uff0c\u80fd\u591f\u6709\u6548\u9632\u62a4\u5404\u79cd\u5b89\u5168\u5a01\u80c1\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027", "conclusion": "AIP Agentic Runtime\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5b89\u5168\u4fdd\u969c\u6846\u67b6\uff0c\u662f\u667a\u80fd\u4f53\u5b89\u5168\u90e8\u7f72\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd", "topic": "agent analysis"}}
{"id": "tldr.2601.bd8f9e6b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.montecarlodata.com%2Fai-agent-contract%2F%3Futm_source=tldrdata/1/0100019bf9fd83fc-7b7b43b2-9d54-41ed-864a-7e5802f55166-000000/XlOd693NgcuFvYFN1QDlOcPgpaGcEzGfVKTR-knLirM=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.montecarlodata.com%2Fai-agent-contract%2F%3Futm_source=tldrdata/1/0100019bf9fd83fc-7b7b43b2-9d54-41ed-864a-7e5802f55166-000000/XlOd693NgcuFvYFN1QDlOcPgpaGcEzGfVKTR-knLirM=441", "authors": ["TLDR Newsletter"], "title": "The Desperate Need For An \u201cAgent Contract\u201d", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.montecarlodata.com%2Fai-agent-contract%2F%3Futm_source=tldrdata/1/0100019bf9fd83fc-7b7b43b2-9d54-41ed-864a-7e5802f55166-000000/XlOd693NgcuFvYFN1QDlOcPgpaGcEzGfVKTR-knLirM=441", "summary": "The Desperate Need For An \u201cAgent Contract\u201d (6 minute read) The AI Agent Contract is a framework that establishes explicit, enforceable agreements between data producers, engineers, and AI agents to ensure reliable inputs, consistent definitions, and controlled changes, preventing common failures from schema drifts, tool breakage, or mismatched expectations.", "source": "tldr", "AI": {"tldr": "\u63d0\u51fa\"AI Agent Contract\"\u6846\u67b6\uff0c\u5efa\u7acb\u6570\u636e\u751f\u4ea7\u8005\u3001\u5de5\u7a0b\u5e08\u548cAI\u4ee3\u7406\u4e4b\u95f4\u7684\u660e\u786e\u53ef\u6267\u884c\u534f\u8bae\uff0c\u786e\u4fdd\u53ef\u9760\u8f93\u5165\u3001\u4e00\u81f4\u5b9a\u4e49\u548c\u53d7\u63a7\u53d8\u66f4\uff0c\u9632\u6b62\u6a21\u5f0f\u6f02\u79fb\u3001\u5de5\u5177\u635f\u574f\u6216\u671f\u671b\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u5e38\u89c1\u6545\u969c\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7cfb\u7edf\u9762\u4e34\u6a21\u5f0f\u6f02\u79fb\u3001\u5de5\u5177\u635f\u574f\u3001\u671f\u671b\u4e0d\u5339\u914d\u7b49\u5e38\u89c1\u6545\u969c\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u534f\u8bae\u6846\u67b6\u6765\u786e\u4fdd\u6570\u636e\u751f\u4ea7\u8005\u3001\u5de5\u7a0b\u5e08\u548cAI\u4ee3\u7406\u4e4b\u95f4\u7684\u53ef\u9760\u534f\u4f5c\u3002", "method": "\u63d0\u51fa\"AI Agent Contract\"\u6846\u67b6\uff0c\u5efa\u7acb\u4e09\u65b9\u4e4b\u95f4\u7684\u660e\u786e\u53ef\u6267\u884c\u534f\u8bae\uff0c\u5305\u62ec\u53ef\u9760\u8f93\u5165\u4fdd\u8bc1\u3001\u4e00\u81f4\u5b9a\u4e49\u6807\u51c6\u548c\u53d7\u63a7\u53d8\u66f4\u673a\u5236\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u9632\u6b62AI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5e38\u89c1\u6545\u969c\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "\u8feb\u5207\u9700\u8981\u5efa\u7acbAI\u4ee3\u7406\u5408\u540c\u6846\u67b6\u6765\u89e3\u51b3\u5f53\u524dAI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u534f\u4f5c\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.34930a94", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaggieappleton.com%2Fgastown%2F%3Futm_source=tldrnewsletter/1/0100019bfa0d545d-fe7275a0-a106-4714-83e4-9ef4a50d4749-000000/AHidIO70NwEElEl6sA22QDFSEXf0RGdRgQRAL4yCeog=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaggieappleton.com%2Fgastown%2F%3Futm_source=tldrnewsletter/1/0100019bfa0d545d-fe7275a0-a106-4714-83e4-9ef4a50d4749-000000/AHidIO70NwEElEl6sA22QDFSEXf0RGdRgQRAL4yCeog=441", "authors": ["TLDR Newsletter"], "title": "Gas Town's Agent Patterns, Design Bottlenecks, and Vibecoding at Scale", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 32 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaggieappleton.com%2Fgastown%2F%3Futm_source=tldrnewsletter/1/0100019bfa0d545d-fe7275a0-a106-4714-83e4-9ef4a50d4749-000000/AHidIO70NwEElEl6sA22QDFSEXf0RGdRgQRAL4yCeog=441", "summary": "Gas Town's Agent Patterns, Design Bottlenecks, and Vibecoding at Scale (32 minute read) Gas Town is a provocative piece of speculative design, but its design is too poorly thought through to persist. The problems with Gas Town will undoubtedly show up in the next generation of development tools. Thoughtful design, critical thinking, user research, and planning and coordination within teams will become more important as the pace of software development speeds up. The most valuable tools will h...", "source": "tldr", "AI": {"tldr": "Gas Town\u662f\u4e00\u4e2a\u6295\u673a\u6027\u8bbe\u8ba1\u9879\u76ee\uff0c\u5176\u8bbe\u8ba1\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u4f46\u5176\u4e2d\u63ed\u793a\u7684\u4ee3\u7406\u6a21\u5f0f\u3001\u8bbe\u8ba1\u74f6\u9888\u548c\u89c4\u6a21\u5316\u7f16\u7801\u95ee\u9898\u5c06\u5728\u4e0b\u4e00\u4ee3\u5f00\u53d1\u5de5\u5177\u4e2d\u51fa\u73b0\u3002", "motivation": "\u5206\u6790Gas Town\u8fd9\u4e2a\u6295\u673a\u8bbe\u8ba1\u9879\u76ee\u4e2d\u7684\u95ee\u9898\uff0c\u9884\u6d4b\u8fd9\u4e9b\u95ee\u9898\u5c06\u5728\u4e0b\u4e00\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u4e2d\u91cd\u73b0\uff0c\u5f3a\u8c03\u5728\u8f6f\u4ef6\u5f00\u53d1\u52a0\u901f\u7684\u80cc\u666f\u4e0b\uff0c\u6df1\u601d\u719f\u8651\u7684\u8bbe\u8ba1\u3001\u6279\u5224\u6027\u601d\u7ef4\u3001\u7528\u6237\u7814\u7a76\u548c\u56e2\u961f\u534f\u8c03\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u5bf9Gas Town\u9879\u76ee\u7684\u6279\u5224\u6027\u5206\u6790\uff0c\u8bc6\u522b\u5176\u4e2d\u7684\u4ee3\u7406\u6a21\u5f0f\u3001\u8bbe\u8ba1\u74f6\u9888\u548c\u89c4\u6a21\u5316\u7f16\u7801\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u89c2\u5bdf\u8fdb\u884c\u9884\u6d4b\u6027\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51faGas Town\u8bbe\u8ba1\u4e2d\u7684\u6839\u672c\u7f3a\u9677\uff0c\u9884\u6d4b\u8fd9\u4e9b\u95ee\u9898\u5c06\u5728\u672a\u6765\u5f00\u53d1\u5de5\u5177\u4e2d\u51fa\u73b0\uff0c\u5f3a\u8c03\u6709\u4ef7\u503c\u7684\u5de5\u5177\u5c06\u9700\u8981\u66f4\u597d\u7684\u8bbe\u8ba1\u601d\u7ef4\u548c\u56e2\u961f\u534f\u4f5c\u3002", "conclusion": "\u968f\u7740\u8f6f\u4ef6\u5f00\u53d1\u901f\u5ea6\u52a0\u5feb\uff0c\u6df1\u601d\u719f\u8651\u7684\u8bbe\u8ba1\u3001\u6279\u5224\u6027\u601d\u7ef4\u3001\u7528\u6237\u7814\u7a76\u548c\u56e2\u961f\u534f\u8c03\u5c06\u53d8\u5f97\u66f4\u52a0\u91cd\u8981\uff0c\u6700\u6709\u4ef7\u503c\u7684\u5de5\u5177\u5c06\u4f53\u73b0\u8fd9\u4e9b\u539f\u5219\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.2d522cd9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fmccun934%2Ffc0779bcf2d47fcb65969e5a4208c9cf%3Futm_source=tldrnewsletter/1/0100019bfa0d545d-fe7275a0-a106-4714-83e4-9ef4a50d4749-000000/8JDPQCZhbJM8c8--jNd_3wB697alSOIrFBuuUO2wul4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fmccun934%2Ffc0779bcf2d47fcb65969e5a4208c9cf%3Futm_source=tldrnewsletter/1/0100019bfa0d545d-fe7275a0-a106-4714-83e4-9ef4a50d4749-000000/8JDPQCZhbJM8c8--jNd_3wB697alSOIrFBuuUO2wul4=441", "authors": ["TLDR Newsletter"], "title": "Claude Code TeammateTool - Source Code Analysis", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fmccun934%2Ffc0779bcf2d47fcb65969e5a4208c9cf%3Futm_source=tldrnewsletter/1/0100019bfa0d545d-fe7275a0-a106-4714-83e4-9ef4a50d4749-000000/8JDPQCZhbJM8c8--jNd_3wB697alSOIrFBuuUO2wul4=441", "summary": "Claude Code TeammateTool - Source Code Analysis (12 minute read) TeammateTool is an existing-but-hidden functionality found in the Claude Code v2.1.19 binary. While it is fully implemented, it is gated behind feature flags. The tool could potentially be used to review PRs through multiple perspectives, build complete features with specialized agents for each layer, investigate a production bug from multiple angles, and more. This post documents what has been discovered about the feature.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Claude Code v2.1.19\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e2d\u9690\u85cf\u7684TeammateTool\u529f\u80fd\uff0c\u8fd9\u662f\u4e00\u4e2a\u5df2\u5b9e\u73b0\u4f46\u88ab\u529f\u80fd\u6807\u5fd7\u7981\u7528\u7684\u6e90\u4ee3\u7801\u5206\u6790\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u591a\u89d2\u5ea6\u4ee3\u7801\u5ba1\u67e5\u3001\u5206\u5c42\u4ee3\u7406\u534f\u4f5c\u7b49\u529f\u80fd\u3002", "motivation": "Claude Code\u4e2d\u9690\u85cf\u7684TeammateTool\u529f\u80fd\u867d\u7136\u5df2\u5b8c\u5168\u5b9e\u73b0\uff0c\u4f46\u88ab\u529f\u80fd\u6807\u5fd7\u9650\u5236\u800c\u65e0\u6cd5\u4f7f\u7528\u3002\u8be5\u5de5\u5177\u5177\u6709\u5f3a\u5927\u7684\u6e90\u4ee3\u7801\u5206\u6790\u80fd\u529b\uff0c\u53ef\u4ee5\u652f\u6301\u591a\u79cd\u5f00\u53d1\u573a\u666f\uff0c\u4f46\u7f3a\u4e4f\u5b98\u65b9\u6587\u6863\u548c\u516c\u5f00\u8bbf\u95ee\u9014\u5f84\u3002", "method": "\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u5206\u6790Claude Code v2.1.19\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53d1\u73b0\u5e76\u8bb0\u5f55\u4e86TeammateTool\u7684\u5b9e\u73b0\u7ec6\u8282\u3001\u529f\u80fd\u7279\u6027\u548c\u6f5c\u5728\u5e94\u7528\u573a\u666f\uff0c\u5305\u62ec\u591a\u89c6\u89d2PR\u5ba1\u67e5\u3001\u5206\u5c42\u4ee3\u7406\u534f\u4f5c\u7b49\u67b6\u6784\u8bbe\u8ba1\u3002", "result": "\u6210\u529f\u8bc6\u522b\u51faTeammateTool\u7684\u5177\u4f53\u529f\u80fd\u6a21\u5757\uff0c\u5305\u62ec\u591a\u89d2\u5ea6\u4ee3\u7801\u5ba1\u67e5\u3001\u5206\u5c42\u4ee3\u7406\u67b6\u6784\u3001\u751f\u4ea7\u95ee\u9898\u8c03\u67e5\u7b49\u80fd\u529b\uff0c\u5e76\u8be6\u7ec6\u8bb0\u5f55\u4e86\u8be5\u5de5\u5177\u7684\u6280\u672f\u5b9e\u73b0\u548c\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "TeammateTool\u662f\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u4f46\u88ab\u9690\u85cf\u7684\u6e90\u4ee3\u7801\u5206\u6790\u5de5\u5177\uff0c\u5177\u6709\u5f3a\u5927\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u80fd\u529b\uff0c\u5982\u679c\u5f00\u653e\u4f7f\u7528\u53ef\u80fd\u663e\u8457\u63d0\u5347\u4ee3\u7801\u5f00\u53d1\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4f46\u76ee\u524d\u4ec5\u4f5c\u4e3a\u5185\u90e8\u5b9e\u9a8c\u529f\u80fd\u5b58\u5728\u3002", "topic": "code agent"}}
{"id": "tldr.2601.709a97e7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmcp-servers-docker-toolkit-cagent-gateway%2F%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/XmYQ5Tvz9Nu-u0XasMuDm9vQhq69b7slfneObW8lowo=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmcp-servers-docker-toolkit-cagent-gateway%2F%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/XmYQ5Tvz9Nu-u0XasMuDm9vQhq69b7slfneObW8lowo=441", "authors": ["TLDR Newsletter"], "title": "Using MCP Servers with Docker: Tools to Multi-Agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmcp-servers-docker-toolkit-cagent-gateway%2F%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/XmYQ5Tvz9Nu-u0XasMuDm9vQhq69b7slfneObW8lowo=441", "summary": "Using MCP Servers with Docker: Tools to Multi-Agent (4 minute read) Docker has introduced an ecosystem to simplify integrating Model Context Protocol (MCP) servers with large language models, offering the MCP Catalog and Toolkit for streamlined setup, cagent for building multi-agent systems, and seamless compatibility with advanced agent frameworks like LangGraph. These tools address common challenges such as runtime complexity, secret injection, and client-to-server wiring by leveraging Dock...", "source": "tldr", "AI": {"tldr": "Docker\u63a8\u51faMCP\u751f\u6001\u7cfb\u7edf\uff0c\u7b80\u5316MCP\u670d\u52a1\u5668\u4e0eLLM\u96c6\u6210\uff0c\u63d0\u4f9bMCP Catalog\u3001Toolkit\u3001cagent\u7b49\u5de5\u5177\uff0c\u89e3\u51b3\u8fd0\u884c\u65f6\u590d\u6742\u6027\u3001\u5bc6\u94a5\u6ce8\u5165\u548c\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u8fde\u63a5\u7b49\u6311\u6218", "motivation": "\u89e3\u51b3\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u96c6\u6210Model Context Protocol\uff08MCP\uff09\u670d\u52a1\u5668\u65f6\u9762\u4e34\u7684\u8fd0\u884c\u65f6\u590d\u6742\u6027\u3001\u5bc6\u94a5\u6ce8\u5165\u548c\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u8fde\u63a5\u7b49\u5e38\u89c1\u6311\u6218", "method": "\u901a\u8fc7Docker\u751f\u6001\u7cfb\u7edf\u63d0\u4f9bMCP Catalog\u548cToolkit\u7b80\u5316\u8bbe\u7f6e\uff0c\u4f7f\u7528cagent\u6784\u5efa\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u4e0eLangGraph\u7b49\u9ad8\u7ea7\u4ee3\u7406\u6846\u67b6\u5b9e\u73b0\u65e0\u7f1d\u517c\u5bb9", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7b80\u5316\u7684MCP\u670d\u52a1\u5668\u96c6\u6210\u751f\u6001\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fd0\u884c\u65f6\u590d\u6742\u6027\u3001\u79d8\u5bc6\u6ce8\u5165\u548c\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u8fde\u63a5\u95ee\u9898", "conclusion": "Docker\u7684MCP\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u663e\u8457\u7b80\u5316\u4e86MCP\u670d\u52a1\u5668\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\u8fc7\u7a0b\uff0c\u4e3a\u6784\u5efa\u591a\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5", "topic": "code agent"}}
{"id": "tldr.2601.f2d05fe8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Fdevops%2Ffrom-ai-agent-prototype-to-product-lessons-from-building-aws-devops-agent%2F%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/J9sdGm4dfmEmSkbDY5f0j98ZvkXP8QNl3S0-7vgDEsQ=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Fdevops%2Ffrom-ai-agent-prototype-to-product-lessons-from-building-aws-devops-agent%2F%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/J9sdGm4dfmEmSkbDY5f0j98ZvkXP8QNl3S0-7vgDEsQ=441", "authors": ["TLDR Newsletter"], "title": "From AI agent prototype to product: Lessons from building AWS DevOps Agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Fdevops%2Ffrom-ai-agent-prototype-to-product-lessons-from-building-aws-devops-agent%2F%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/J9sdGm4dfmEmSkbDY5f0j98ZvkXP8QNl3S0-7vgDEsQ=441", "summary": "From AI agent prototype to product: Lessons from building AWS DevOps Agent (9 minute read) This post describes lessons from building the AWS DevOps Agent, outlining five mechanisms to productionize agentic systems: evals, fast feedback loops, trajectory visualization, intentional changes, and production sampling. It details how these practices improve reliability, accuracy, and cost efficiency in incident response.", "source": "tldr", "AI": {"tldr": "AWS DevOps Agent\u4ece\u539f\u578b\u5230\u4ea7\u54c1\u7684\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u603b\u7ed3\u4e86\u751f\u4ea7\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4e94\u4e2a\u5173\u952e\u673a\u5236\uff1a\u8bc4\u4f30\u3001\u5feb\u901f\u53cd\u9988\u5faa\u73af\u3001\u8f68\u8ff9\u53ef\u89c6\u5316\u3001\u6709\u610f\u8bc6\u53d8\u66f4\u548c\u751f\u4ea7\u91c7\u6837\uff0c\u4ee5\u63d0\u5347\u53ef\u9760\u6027\u3001\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u7387\u3002", "motivation": "\u5c06AI\u667a\u80fd\u4f53\u4ece\u539f\u578b\u8f6c\u5316\u4e3a\u5b9e\u9645\u53ef\u7528\u7684\u4ea7\u54c1\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u786e\u4fdd\u53ef\u9760\u6027\u3001\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u7387\u3002AWS DevOps Agent\u7684\u5f00\u53d1\u7ecf\u9a8c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u4ea7\u54c1\u5316\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e94\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) \u8bc4\u4f30\u7cfb\u7edf\u7528\u4e8e\u8861\u91cf\u667a\u80fd\u4f53\u6027\u80fd\uff1b2) \u5feb\u901f\u53cd\u9988\u5faa\u73af\u52a0\u901f\u8fed\u4ee3\uff1b3) \u8f68\u8ff9\u53ef\u89c6\u5316\u5e2e\u52a9\u7406\u89e3\u667a\u80fd\u4f53\u51b3\u7b56\u8fc7\u7a0b\uff1b4) \u6709\u610f\u8bc6\u53d8\u66f4\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6f14\u8fdb\uff1b5) \u751f\u4ea7\u91c7\u6837\u6536\u96c6\u771f\u5b9e\u573a\u666f\u6570\u636e\u3002", "result": "\u8fd9\u4e9b\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86AWS DevOps Agent\u5728\u4e8b\u4ef6\u54cd\u5e94\u4e2d\u7684\u53ef\u9760\u6027\u3001\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u7387\uff0c\u6210\u529f\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u4ece\u539f\u578b\u8f6c\u5316\u4e3a\u751f\u4ea7\u7ea7\u4ea7\u54c1\u3002", "conclusion": "\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6210\u529f\u4ea7\u54c1\u5316\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5de5\u7a0b\u5b9e\u8df5\uff0c\u5305\u62ec\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3001\u53cd\u9988\u5faa\u73af\u7b49\u673a\u5236\uff0c\u8fd9\u4e9b\u7ecf\u9a8c\u4e3a\u5176\u4ed6\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "topic": "swe application"}}
{"id": "tldr.2601.e28880d6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fupstash%2Fcontext7%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/Egnu5i4f84BlOwkAHgWJhlGwNAs8ng3-yhXoCZnJy60=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fupstash%2Fcontext7%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/Egnu5i4f84BlOwkAHgWJhlGwNAs8ng3-yhXoCZnJy60=441", "authors": ["TLDR Newsletter"], "title": "Context7", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fupstash%2Fcontext7%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/Egnu5i4f84BlOwkAHgWJhlGwNAs8ng3-yhXoCZnJy60=441", "summary": "Context7 (GitHub Repo) Context7 MCP Server delivers up-to-date, version-specific code documentation and examples directly to LLMs and AI code editors, aiming to prevent outdated information and \"hallucinated\" APIs. Available for integration with platforms like Cursor and Claude Code, the server pulls current library details into prompts, though its community-contributed documentation is not guaranteed for accuracy.", "source": "tldr", "AI": {"tldr": "Context7\u662f\u4e00\u4e2aMCP\u670d\u52a1\u5668\uff0c\u63d0\u4f9b\u6700\u65b0\u3001\u7248\u672c\u7279\u5b9a\u7684\u4ee3\u7801\u6587\u6863\u548c\u793a\u4f8b\u7ed9LLM\u548cAI\u4ee3\u7801\u7f16\u8f91\u5668\uff0c\u65e8\u5728\u9632\u6b62\u8fc7\u65f6\u4fe1\u606f\u548cAPI\u5e7b\u89c9\u95ee\u9898", "motivation": "\u89e3\u51b3LLM\u548cAI\u4ee3\u7801\u7f16\u8f91\u5668\u4e2d\u5e38\u89c1\u7684\u8fc7\u65f6\u6587\u6863\u548cAPI\u5e7b\u89c9\u95ee\u9898\uff0c\u786e\u4fdd\u5f00\u53d1\u8005\u83b7\u5f97\u51c6\u786e\u3001\u6700\u65b0\u7684\u4ee3\u7801\u5e93\u4fe1\u606f", "method": "\u5f00\u53d1MCP\u670d\u52a1\u5668\uff0c\u4eceGitHub\u4ed3\u5e93\u62c9\u53d6\u5f53\u524d\u5e93\u7684\u8be6\u7ec6\u4fe1\u606f\u5e76\u96c6\u6210\u5230\u63d0\u793a\u4e2d\uff0c\u652f\u6301Cursor\u548cClaude Code\u7b49\u5e73\u53f0", "result": "\u521b\u5efa\u4e86Context7 MCP\u670d\u52a1\u5668\uff0c\u80fd\u591f\u63d0\u4f9b\u7248\u672c\u7279\u5b9a\u7684\u4ee3\u7801\u6587\u6863\u548c\u793a\u4f8b\uff0c\u4f46\u793e\u533a\u8d21\u732e\u7684\u6587\u6863\u51c6\u786e\u6027\u4e0d\u4fdd\u8bc1", "conclusion": "Context7\u901a\u8fc7\u5b9e\u65f6\u83b7\u53d6\u4ee3\u7801\u5e93\u4fe1\u606f\u6539\u5584\u4e86AI\u8f85\u52a9\u7f16\u7a0b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u786e\u4fdd\u6587\u6863\u8d28\u91cf", "topic": "code agent"}}
{"id": "tldr.2601.d2de6c99", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewrelic.com%2Fblog%2Fdem%2Fobservability-for-chatgpt-apps-in-the-age-of-agentic-ai%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/SSXVyVwwAa0ElrVRmMHXqtd_3eNy_PJrK5Vj73nAn0I=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewrelic.com%2Fblog%2Fdem%2Fobservability-for-chatgpt-apps-in-the-age-of-agentic-ai%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/SSXVyVwwAa0ElrVRmMHXqtd_3eNy_PJrK5Vj73nAn0I=441", "authors": ["TLDR Newsletter"], "title": "Observability for ChatGPT Apps in the Age of Agentic AI", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewrelic.com%2Fblog%2Fdem%2Fobservability-for-chatgpt-apps-in-the-age-of-agentic-ai%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/SSXVyVwwAa0ElrVRmMHXqtd_3eNy_PJrK5Vj73nAn0I=441", "summary": "Observability for ChatGPT Apps in the Age of Agentic AI (4 minute read) New Relic has addressed a critical monitoring challenge for \"ChatGPT Apps\"\u2014custom integrations running within AI platforms.", "source": "tldr", "AI": {"tldr": "New Relic\u63a8\u51fa\u9488\u5bf9ChatGPT\u5e94\u7528\u7684\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3AI\u5e73\u53f0\u4e2d\u81ea\u5b9a\u4e49\u96c6\u6210\u7684\u53ef\u89c2\u6d4b\u6027\u95ee\u9898", "motivation": "\u968f\u7740ChatGPT\u5e94\u7528\uff08\u5728AI\u5e73\u53f0\u4e2d\u8fd0\u884c\u7684\u81ea\u5b9a\u4e49\u96c6\u6210\uff09\u7684\u666e\u53ca\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u76d1\u63a7\u5de5\u5177\u6765\u8ffd\u8e2a\u8fd9\u4e9b\u5e94\u7528\u7684\u6027\u80fd\u3001\u9519\u8bef\u548c\u7528\u6237\u884c\u4e3a", "method": "New Relic\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u4e3aChatGPT\u5e94\u7528\u63d0\u4f9b\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\uff0c\u5305\u62ec\u6027\u80fd\u76d1\u63a7\u3001\u9519\u8bef\u8ffd\u8e2a\u548c\u7528\u6237\u884c\u4e3a\u5206\u6790", "result": "\u89e3\u51b3\u4e86ChatGPT\u5e94\u7528\u5728AI\u5e73\u53f0\u73af\u5883\u4e2d\u7684\u76d1\u63a7\u6311\u6218\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f18\u5316\u5176AI\u96c6\u6210\u5e94\u7528", "conclusion": "\u5728\u667a\u80fd\u4f53AI\u65f6\u4ee3\uff0c\u4e3aChatGPT\u5e94\u7528\u63d0\u4f9b\u4e13\u95e8\u7684\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\u5bf9\u4e8e\u786e\u4fdd\u5e94\u7528\u53ef\u9760\u6027\u548c\u6027\u80fd\u81f3\u5173\u91cd\u8981", "topic": "swe application"}}
{"id": "tldr.2601.c0f64121", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faifoc.us%2Fthe-browser-is-the-sandbox%2F%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/FT7nnugSDOTPcisjLzVJZc24AViezyUrF71NxNob328=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faifoc.us%2Fthe-browser-is-the-sandbox%2F%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/FT7nnugSDOTPcisjLzVJZc24AViezyUrF71NxNob328=441", "authors": ["TLDR Newsletter"], "title": "The browser is the sandbox", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faifoc.us%2Fthe-browser-is-the-sandbox%2F%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/FT7nnugSDOTPcisjLzVJZc24AViezyUrF71NxNob328=441", "summary": "The browser is the sandbox (18 minute read) The browser has potential as a secure sandbox for agentic AI tools, which automate tasks and require access to a user's local files and network. It has mechanisms for filesystem isolation, controlled network access via CSP, and secure code execution using iframes and WebAssembly. A demo project is displayed in this article that shows these capabilities.", "source": "tldr", "AI": {"tldr": "\u6d4f\u89c8\u5668\u53ef\u4f5c\u4e3aAI\u4ee3\u7406\u7684\u5b89\u5168\u6c99\u7bb1\uff0c\u63d0\u4f9b\u6587\u4ef6\u7cfb\u7edf\u9694\u79bb\u3001\u7f51\u7edc\u8bbf\u95ee\u63a7\u5236\u548c\u5b89\u5168\u4ee3\u7801\u6267\u884c\u80fd\u529b", "motivation": "AI\u4ee3\u7406\u5de5\u5177\u9700\u8981\u8bbf\u95ee\u7528\u6237\u672c\u5730\u6587\u4ef6\u548c\u7f51\u7edc\uff0c\u4f46\u9700\u8981\u5b89\u5168\u9694\u79bb\u673a\u5236\u6765\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u548c\u7cfb\u7edf\u5b89\u5168", "method": "\u5229\u7528\u6d4f\u89c8\u5668\u7684\u5185\u7f6e\u5b89\u5168\u7279\u6027\uff1a\u6587\u4ef6\u7cfb\u7edf\u9694\u79bb\u3001CSP\u63a7\u5236\u7f51\u7edc\u8bbf\u95ee\u3001iframe\u548cWebAssembly\u5b9e\u73b0\u5b89\u5168\u4ee3\u7801\u6267\u884c", "result": "\u5c55\u793a\u4e86\u6d4f\u89c8\u5668\u4f5c\u4e3aAI\u4ee3\u7406\u5b89\u5168\u6c99\u7bb1\u7684\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u6f14\u793a\u9879\u76ee\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u80fd\u529b", "conclusion": "\u6d4f\u89c8\u5668\u5177\u5907\u6210\u4e3aAI\u4ee3\u7406\u5de5\u5177\u5b89\u5168\u6267\u884c\u73af\u5883\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5e73\u8861\u529f\u80fd\u9700\u6c42\u4e0e\u5b89\u5168\u8981\u6c42", "topic": "code agent"}}
{"id": "tldr.2601.a621f201", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnolanlawson.com%2F2026%2F01%2F24%2Fai-tribalism%2F%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/MhnpEaAf8tSQp7XM0cCzlc7bV2iEaWShzWAtFfspOLI=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnolanlawson.com%2F2026%2F01%2F24%2Fai-tribalism%2F%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/MhnpEaAf8tSQp7XM0cCzlc7bV2iEaWShzWAtFfspOLI=441", "authors": ["TLDR Newsletter"], "title": "AI tribalism", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnolanlawson.com%2F2026%2F01%2F24%2Fai-tribalism%2F%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/MhnpEaAf8tSQp7XM0cCzlc7bV2iEaWShzWAtFfspOLI=441", "summary": "AI tribalism (6 minute read) Discussions around LLMs have devolved into unproductive tribalism, fueled by early negative associations with tech \"hucksters\" and fears of job displacement. AI agents are already capable of efficient code generation, bug fixing, and addressing complex issues like security and performance. Engineers need to keep experimenting with AI to stay on top of new tooling.", "source": "tldr", "AI": {"tldr": "\u8ba8\u8bbaAI\u4ee3\u7406\u5728\u4ee3\u7801\u751f\u6210\u3001bug\u4fee\u590d\u548c\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u5de5\u7a0b\u5e08\u9700\u8981\u6301\u7eed\u5b9e\u9a8cAI\u5de5\u5177\u4ee5\u4fdd\u6301\u7ade\u4e89\u529b", "motivation": "\u9488\u5bf9\u5f53\u524d\u56f4\u7ed5LLM\u7684\u8ba8\u8bba\u9677\u5165\u975e\u5efa\u8bbe\u6027\u7684\u90e8\u843d\u4e3b\u4e49\uff0c\u4ee5\u53ca\u5de5\u7a0b\u5e08\u5bf9AI\u5de5\u5177\u53ef\u80fd\u5e26\u6765\u7684\u5de5\u4f5c\u66ff\u4ee3\u7684\u62c5\u5fe7\uff0c\u63d0\u51fa\u9700\u8981\u5ba2\u89c2\u770b\u5f85AI\u4ee3\u7406\u7684\u5b9e\u9645\u80fd\u529b", "method": "\u901a\u8fc7\u5206\u6790AI\u4ee3\u7406\u5728\u4ee3\u7801\u751f\u6210\u3001bug\u4fee\u590d\u3001\u5b89\u5168\u6027\u548c\u6027\u80fd\u95ee\u9898\u89e3\u51b3\u7b49\u65b9\u9762\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u5f3a\u8c03\u5b9e\u8df5\u548c\u5b9e\u9a8c\u7684\u91cd\u8981\u6027", "result": "AI\u4ee3\u7406\u5df2\u7ecf\u80fd\u591f\u9ad8\u6548\u5b8c\u6210\u4ee3\u7801\u751f\u6210\u3001bug\u4fee\u590d\u7b49\u4efb\u52a1\uff0c\u5e76\u80fd\u5904\u7406\u5b89\u5168\u548c\u6027\u80fd\u7b49\u590d\u6742\u95ee\u9898", "conclusion": "\u5de5\u7a0b\u5e08\u9700\u8981\u79ef\u6781\u5b9e\u9a8c\u548c\u91c7\u7528AI\u5de5\u5177\uff0c\u907f\u514d\u9677\u5165\u65e0\u610f\u4e49\u7684\u90e8\u843d\u4e3b\u4e49\u4e89\u8bba\uff0c\u4ee5\u4fdd\u6301\u6280\u672f\u7ade\u4e89\u529b", "topic": "code agent"}}
{"id": "tldr.2601.44751c77", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FUse-Tusk%2Ffence%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/3LuWgM_La4DtSMxnGheDQ2IfjUlkBC5n1D_dN69ERpc=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FUse-Tusk%2Ffence%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/3LuWgM_La4DtSMxnGheDQ2IfjUlkBC5n1D_dN69ERpc=441", "authors": ["TLDR Newsletter"], "title": "Fence", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FUse-Tusk%2Ffence%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/3LuWgM_La4DtSMxnGheDQ2IfjUlkBC5n1D_dN69ERpc=441", "summary": "Fence (GitHub Repo) Fence is a lightweight, container-free sandbox tool that wraps commands to restrict network access and filesystem operations. It's designed for running semi-trusted code, like package installs or CI jobs, with controlled side effects, and it improves AI coding agent security.", "source": "tldr", "AI": {"tldr": "Fence\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u65e0\u5bb9\u5668\u7684\u6c99\u76d2\u5de5\u5177\uff0c\u901a\u8fc7\u5305\u88c5\u547d\u4ee4\u6765\u9650\u5236\u7f51\u7edc\u8bbf\u95ee\u548c\u6587\u4ef6\u7cfb\u7edf\u64cd\u4f5c\uff0c\u7528\u4e8e\u8fd0\u884c\u534a\u53ef\u4fe1\u4ee3\u7801\u5e76\u589e\u5f3aAI\u7f16\u7801\u4ee3\u7406\u7684\u5b89\u5168\u6027", "motivation": "\u9700\u8981\u4e00\u79cd\u5b89\u5168\u7684\u65b9\u5f0f\u6765\u8fd0\u884c\u534a\u53ef\u4fe1\u4ee3\u7801\uff08\u5982\u5305\u5b89\u88c5\u3001CI\u4f5c\u4e1a\uff09\uff0c\u540c\u65f6\u63a7\u5236\u5176\u526f\u4f5c\u7528\uff0c\u5e76\u63d0\u9ad8AI\u7f16\u7801\u4ee3\u7406\u7684\u5b89\u5168\u6027", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u65e0\u5bb9\u5668\u7684\u6c99\u76d2\u5de5\u5177\uff0c\u901a\u8fc7\u5305\u88c5\u547d\u4ee4\u6765\u9650\u5236\u7f51\u7edc\u8bbf\u95ee\u548c\u6587\u4ef6\u7cfb\u7edf\u64cd\u4f5c", "result": "\u521b\u5efa\u4e86Fence\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u9650\u5236\u4ee3\u7801\u7684\u7f51\u7edc\u548c\u6587\u4ef6\u7cfb\u7edf\u8bbf\u95ee\uff0c\u63d0\u9ad8\u8fd0\u884c\u534a\u53ef\u4fe1\u4ee3\u7801\u7684\u5b89\u5168\u6027", "conclusion": "Fence\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5b89\u5168\u8fd0\u884c\u534a\u53ef\u4fe1\u4ee3\u7801\uff0c\u7279\u522b\u9002\u7528\u4e8eAI\u7f16\u7801\u4ee3\u7406\u7684\u5b89\u5168\u589e\u5f3a", "topic": "code agent"}}
{"id": "tldr.2601.2c436a70", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetboothiq.com%2Fblog%2F150k-lines-vibe-coded-elixir-good-bad-ugly%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/7DswHGQ7NF3ipUXf0CPE6Y5cZ5Rf-ZdAwsLsPw6HrbY=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetboothiq.com%2Fblog%2F150k-lines-vibe-coded-elixir-good-bad-ugly%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/7DswHGQ7NF3ipUXf0CPE6Y5cZ5Rf-ZdAwsLsPw6HrbY=441", "authors": ["TLDR Newsletter"], "title": "150,000 Lines of Vibe Coded Elixir: The Good, The Bad, and The Ugly", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetboothiq.com%2Fblog%2F150k-lines-vibe-coded-elixir-good-bad-ugly%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/7DswHGQ7NF3ipUXf0CPE6Y5cZ5Rf-ZdAwsLsPw6HrbY=441", "summary": "150,000 Lines of Vibe Coded Elixir: The Good, The Bad, and The Ugly (8 minute read) BoothIQ used AI to write all 150,000 lines of its Elixir codebase, finding the language's small, terse, and immutable nature great for AI generation. AI is great at generating code efficiently, especially when aided by Elixir-specific tools like Tidewave. However, it struggles with architectural decisions, often produces defensive code trained on imperative languages, and is ineffective at debugging complex El...", "source": "tldr", "AI": {"tldr": "BoothIQ\u4f7f\u7528AI\u751f\u6210\u4e8615\u4e07\u884cElixir\u4ee3\u7801\uff0c\u53d1\u73b0Elixir\u8bed\u8a00\u7684\u5c0f\u5de7\u3001\u7b80\u6d01\u548c\u4e0d\u53ef\u53d8\u6027\u975e\u5e38\u9002\u5408AI\u751f\u6210\uff0c\u4f46AI\u5728\u67b6\u6784\u51b3\u7b56\u3001\u8c03\u8bd5\u590d\u6742\u95ee\u9898\u65b9\u9762\u5b58\u5728\u56f0\u96be", "motivation": "\u63a2\u7d22AI\u5728\u751f\u6210Elixir\u4ee3\u7801\u65b9\u9762\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\uff0c\u8bc4\u4f30AI\u7f16\u7801\u5728\u5927\u578b\u9879\u76ee\u4e2d\u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027", "method": "\u4f7f\u7528AI\u5de5\u5177\uff08\u5305\u62ecElixir\u4e13\u7528\u5de5\u5177\u5982Tidewave\uff09\u751f\u621015\u4e07\u884cElixir\u4ee3\u7801\uff0c\u5206\u6790\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4f18\u7f3a\u70b9", "result": "AI\u80fd\u9ad8\u6548\u751f\u6210Elixir\u4ee3\u7801\uff0c\u7279\u522b\u662f\u5728\u8bed\u8a00\u7279\u6027\u652f\u6301\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u67b6\u6784\u51b3\u7b56\u3001\u8c03\u8bd5\u590d\u6742\u95ee\u9898\u3001\u907f\u514d\u9632\u5fa1\u6027\u7f16\u7a0b\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3", "conclusion": "AI\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u6765\u5904\u7406\u67b6\u6784\u51b3\u7b56\u548c\u590d\u6742\u8c03\u8bd5\uff0cAI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u662f\u6700\u4f73\u5b9e\u8df5", "topic": "code agent"}}
{"id": "tldr.2601.7f6bfaa4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.excel.holdings%2Fp%2Fsystems-of-action-and-the-dominos%3Futm_source=tldrfounders/1/0100019bfa6b25b6-d530ff35-c9df-4547-aebe-50cb0fa4c066-000000/jpmM-DWtRRG0Qrr5N50jlY-HkN3ISy2IWzTWFGYbakY=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.excel.holdings%2Fp%2Fsystems-of-action-and-the-dominos%3Futm_source=tldrfounders/1/0100019bfa6b25b6-d530ff35-c9df-4547-aebe-50cb0fa4c066-000000/jpmM-DWtRRG0Qrr5N50jlY-HkN3ISy2IWzTWFGYbakY=441", "authors": ["TLDR Newsletter"], "title": "Systems of Action and the Domino's Pizza Tracker Product Model", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.excel.holdings%2Fp%2Fsystems-of-action-and-the-dominos%3Futm_source=tldrfounders/1/0100019bfa6b25b6-d530ff35-c9df-4547-aebe-50cb0fa4c066-000000/jpmM-DWtRRG0Qrr5N50jlY-HkN3ISy2IWzTWFGYbakY=441", "summary": "Systems of Action and the Domino's Pizza Tracker Product Model (3 minute read) A system of action should look just like a timeline with a transparent record of discrete events and the ability for users to see that the whole agentic system is tracking everything.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u884c\u52a8\u7cfb\u7edf\u5e94\u50cf\u65f6\u95f4\u7ebf\u4e00\u6837\u900f\u660e\u5c55\u793a\u79bb\u6563\u4e8b\u4ef6\uff0c\u8ba9\u7528\u6237\u770b\u5230\u6574\u4e2a\u667a\u80fd\u7cfb\u7edf\u5728\u8ffd\u8e2a\u6240\u6709\u8fc7\u7a0b", "motivation": "\u5f53\u524d\u667a\u80fd\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u6027\uff0c\u7528\u6237\u65e0\u6cd5\u4e86\u89e3\u7cfb\u7edf\u5185\u90e8\u7684\u5de5\u4f5c\u8fc7\u7a0b\u548c\u51b3\u7b56\u4f9d\u636e\uff0c\u9700\u8981\u5efa\u7acb\u7c7b\u4f3c\u591a\u7c73\u8bfa\u62ab\u8428\u8ffd\u8e2a\u5668\u7684\u900f\u660e\u4ea7\u54c1\u6a21\u578b", "method": "\u91c7\u7528\u65f6\u95f4\u7ebf\u5f0f\u7684\u4e8b\u4ef6\u8bb0\u5f55\u65b9\u6cd5\uff0c\u5c06\u7cfb\u7edf\u884c\u52a8\u5206\u89e3\u4e3a\u79bb\u6563\u4e8b\u4ef6\uff0c\u63d0\u4f9b\u900f\u660e\u7684\u8bb0\u5f55\u673a\u5236\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u67e5\u770b\u6574\u4e2a\u667a\u80fd\u7cfb\u7edf\u7684\u8ffd\u8e2a\u8fc7\u7a0b", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u8ffd\u8e2a\u7684\u884c\u52a8\u7cfb\u7edf\u6846\u67b6\uff0c\u7c7b\u4f3c\u4e8e\u591a\u7c73\u8bfa\u62ab\u8428\u8ffd\u8e2a\u5668\uff0c\u80fd\u591f\u6e05\u6670\u5c55\u793a\u7cfb\u7edf\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u6bcf\u4e2a\u6b65\u9aa4\u548c\u72b6\u6001\u53d8\u5316", "conclusion": "\u667a\u80fd\u7cfb\u7edf\u9700\u8981\u63d0\u4f9b\u900f\u660e\u7684\u884c\u52a8\u8ffd\u8e2a\u673a\u5236\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u7406\u89e3\u7cfb\u7edf\u7684\u5de5\u4f5c\u8fc7\u7a0b\uff0c\u8fd9\u6709\u52a9\u4e8e\u5efa\u7acb\u4fe1\u4efb\u5e76\u63d0\u9ad8\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027", "topic": "agent analysis"}}
{"id": "tldr.2601.e64b65a6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadplist.substack.com%2Fp%2Fa-designers-guide-to-cursor-and-claude%3Futm_source=tldrdesign/1/0100019bfa77e59d-f4347f62-f5d7-466b-b9b1-9cd1351dca2e-000000/7e4Aa40Q4HIGRmHgI05b7MmzX6-Y7KvRQT-XCQKSVso=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadplist.substack.com%2Fp%2Fa-designers-guide-to-cursor-and-claude%3Futm_source=tldrdesign/1/0100019bfa77e59d-f4347f62-f5d7-466b-b9b1-9cd1351dca2e-000000/7e4Aa40Q4HIGRmHgI05b7MmzX6-Y7KvRQT-XCQKSVso=441", "authors": ["TLDR Newsletter"], "title": "Cursor for Designers Guide", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadplist.substack.com%2Fp%2Fa-designers-guide-to-cursor-and-claude%3Futm_source=tldrdesign/1/0100019bfa77e59d-f4347f62-f5d7-466b-b9b1-9cd1351dca2e-000000/7e4Aa40Q4HIGRmHgI05b7MmzX6-Y7KvRQT-XCQKSVso=441", "summary": "Cursor for Designers Guide (9 minute read) AI tools like Cursor and Claude Code enable designers without coding experience to build functional web applications through \"vibe-coding\". The guide demonstrates how to create a personal portfolio website with AI chat functionality in 48 hours and a growth design tool in 5 days, using a workflow that includes planning with Claude Code, iterating on natural language prompts, and deploying to GitHub and Vercel. Step-by-step instructions cover how to s...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u8bbe\u8ba1\u5e08\u5982\u4f55\u4f7f\u7528Cursor\u548cClaude Code\u7b49AI\u5de5\u5177\u901a\u8fc7\"\u6c1b\u56f4\u7f16\u7801\"\u6784\u5efa\u529f\u80fd\u6027Web\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u572848\u5c0f\u65f6\u5185\u521b\u5efa\u5e26AI\u804a\u5929\u529f\u80fd\u7684\u4e2a\u4eba\u4f5c\u54c1\u96c6\u7f51\u7ad9\u4ee5\u53ca\u57285\u5929\u5185\u6784\u5efa\u589e\u957f\u8bbe\u8ba1\u5de5\u5177\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf\u4e0a\u8bbe\u8ba1\u5e08\u9700\u8981\u4f9d\u8d56\u5f00\u53d1\u4eba\u5458\u5c06\u8bbe\u8ba1\u8f6c\u5316\u4e3a\u4ee3\u7801\uff0c\u8fd9\u9650\u5236\u4e86\u8bbe\u8ba1\u5e08\u72ec\u7acb\u5b9e\u73b0\u60f3\u6cd5\u7684\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u5c55\u793aAI\u5de5\u5177\u5982\u4f55\u8d4b\u80fd\u8bbe\u8ba1\u5e08\uff0c\u4f7f\u5176\u65e0\u9700\u7f16\u7801\u7ecf\u9a8c\u4e5f\u80fd\u6784\u5efa\u529f\u80fd\u5b8c\u6574\u7684Web\u5e94\u7528\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db\u3002", "method": "\u91c7\u7528\"\u6c1b\u56f4\u7f16\u7801\"\u5de5\u4f5c\u6d41\u7a0b\uff1a1) \u4f7f\u7528Claude Code\u8fdb\u884c\u9879\u76ee\u89c4\u5212\uff1b2) \u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8fed\u4ee3\u5f00\u53d1\uff1b3) \u90e8\u7f72\u5230GitHub\u548cVercel\u3002\u5177\u4f53\u6b65\u9aa4\u5305\u62ec\u521b\u5efa\u4e2a\u4eba\u4f5c\u54c1\u96c6\u7f51\u7ad9\u548c\u589e\u957f\u8bbe\u8ba1\u5de5\u5177\u4e24\u4e2a\u6848\u4f8b\u6f14\u793a\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u8bbe\u8ba1\u5e08\u53ef\u4ee5\u572848\u5c0f\u65f6\u5185\u521b\u5efa\u5e26AI\u804a\u5929\u529f\u80fd\u7684\u4e2a\u4eba\u4f5c\u54c1\u96c6\u7f51\u7ad9\uff0c\u5e76\u57285\u5929\u5185\u6784\u5efa\u5b8c\u6574\u7684\u589e\u957f\u8bbe\u8ba1\u5de5\u5177\u3002\u8bc1\u660e\u4e86AI\u5de5\u5177\u80fd\u591f\u663e\u8457\u964d\u4f4eWeb\u5f00\u53d1\u7684\u6280\u672f\u95e8\u69db\uff0c\u4f7f\u8bbe\u8ba1\u5e08\u80fd\u591f\u72ec\u7acb\u5b9e\u73b0\u529f\u80fd\u5e94\u7528\u3002", "conclusion": "AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u5982Cursor\u548cClaude Code\u6b63\u5728\u6539\u53d8\u8bbe\u8ba1\u5e08\u7684\u5de5\u4f5c\u65b9\u5f0f\uff0c\u4f7f\u975e\u6280\u672f\u80cc\u666f\u7684\u8bbe\u8ba1\u5e08\u80fd\u591f\u5feb\u901f\u6784\u5efa\u529f\u80fd\u6027Web\u5e94\u7528\u3002\u8fd9\u79cd\"\u6c1b\u56f4\u7f16\u7801\"\u65b9\u6cd5\u4ee3\u8868\u4e86\u8bbe\u8ba1\u5de5\u5177\u53d1\u5c55\u7684\u65b0\u65b9\u5411\uff0c\u5c06\u8bbe\u8ba1\u601d\u7ef4\u4e0e\u4ee3\u7801\u5b9e\u73b0\u66f4\u7d27\u5bc6\u5730\u7ed3\u5408\u3002", "topic": "code agent"}}
