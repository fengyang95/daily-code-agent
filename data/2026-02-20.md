<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [tldr.article](#tldr.article) [Total: 19]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: æå‡ºå‚è€ƒå¼•å¯¼çš„LLMè¯„ä¼°å™¨æ–¹æ³•ï¼Œç”¨äºéå¯éªŒè¯é¢†åŸŸï¼ˆå¦‚LLMå¯¹é½ï¼‰ï¼Œé€šè¿‡å‚è€ƒè¾“å‡ºæ¥å¢å¼ºLLMè¯„ä¼°å™¨å‡†ç¡®æ€§ï¼Œå¹¶ç”¨äºæ¨¡å‹è‡ªæ”¹è¿›ï¼Œåœ¨AlpacaEvalå’ŒArena-HardåŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æ€§èƒ½æå‡ã€‚


<details>
  <summary>Details</summary>
Motivation: å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­æœ‰æ•ˆï¼Œä½†æ— æ³•ç›´æ¥åº”ç”¨äºç¼ºä¹çœŸå®éªŒè¯å™¨çš„éå¯éªŒè¯é¢†åŸŸï¼ˆå¦‚LLMå¯¹é½ï¼‰ã€‚éœ€è¦æ¢ç´¢å‚è€ƒå¼•å¯¼çš„LLMè¯„ä¼°å™¨æ˜¯å¦èƒ½ä½œä¸ºè½¯"éªŒè¯å™¨"æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚

Method: è®¾è®¡ä½¿ç”¨å‚è€ƒè¾“å‡ºå¢å¼ºLLMè¯„ä¼°å™¨çš„è¯„ä¼°åè®®ï¼›é€šè¿‡å‚è€ƒå¼•å¯¼æ–¹æ³•æå‡LLMè¯„ä¼°å™¨å‡†ç¡®æ€§ï¼›åˆ©ç”¨æ”¹è¿›çš„è¯„ä¼°å™¨è¿›è¡Œå¯¹é½è°ƒä¼˜ï¼Œè®©LLMä½¿ç”¨å‚è€ƒå¼•å¯¼çš„è¯„ä¼°å™¨è¿›è¡Œè‡ªæ”¹è¿›ã€‚

Result: å‚è€ƒå¼•å¯¼æ–¹æ³•æ˜¾è‘—æå‡LLMè¯„ä¼°å™¨å‡†ç¡®æ€§ï¼›å‚è€ƒå¼•å¯¼çš„è‡ªæ”¹è¿›åœ¨AlpacaEvalå’ŒArena-HardåŸºå‡†ä¸Šä¼˜äºç›´æ¥SFTè’¸é¦å’Œæ— å‚è€ƒè‡ªæ”¹è¿›ï¼Œæ€§èƒ½æ¥è¿‘ä½¿ç”¨ArMoRMå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚å…·ä½“æ¥è¯´ï¼ŒLlama-3-8B-Instructåœ¨AlpacaEvalå’ŒArena-Hardä¸Šåˆ†åˆ«è¾¾åˆ°73.1%å’Œ58.7%ï¼ŒQwen2.5-7Båˆ†åˆ«è¾¾åˆ°70.0%å’Œ74.1%ã€‚

Conclusion: å‚è€ƒå¼•å¯¼çš„LLMè¯„ä¼°å™¨èƒ½å¤Ÿåœ¨éå¯éªŒè¯é¢†åŸŸå®ç°æœ‰æ•ˆçš„LLMåè®­ç»ƒï¼Œä¸ºLLMå¯¹é½ç­‰ç¼ºä¹çœŸå®éªŒè¯å™¨çš„ä»»åŠ¡æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [2] [Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History](https://arxiv.org/abs/2602.17003)
*Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.CL

TL;DR: Persona2Webæ˜¯é¦–ä¸ªåœ¨çœŸå®å¼€æ”¾ç½‘ç»œä¸Šè¯„ä¼°ä¸ªæ€§åŒ–ç½‘é¡µä»£ç†çš„åŸºå‡†ï¼ŒåŸºäº"æ¾„æ¸…ä»¥ä¸ªæ€§åŒ–"åŸåˆ™ï¼Œè¦æ±‚ä»£ç†æ ¹æ®ç”¨æˆ·å†å²è€Œéæ˜ç¡®æŒ‡ä»¤æ¥è§£å†³æ¨¡ç³ŠæŸ¥è¯¢ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰ç½‘é¡µä»£ç†ç¼ºä¹ä¸ªæ€§åŒ–èƒ½åŠ›ï¼Œè€Œç”¨æˆ·å¾ˆå°‘è¯¦ç»†è¯´æ˜æ„å›¾çš„æ¯ä¸ªç»†èŠ‚ã€‚å®ç”¨çš„ç½‘é¡µä»£ç†éœ€è¦èƒ½å¤Ÿé€šè¿‡æ¨æ–­ç”¨æˆ·åå¥½å’Œä¸Šä¸‹æ–‡æ¥è§£é‡Šæ¨¡ç³ŠæŸ¥è¯¢ã€‚

Method: æ„å»ºPersona2WebåŸºå‡†ï¼ŒåŒ…å«ï¼š1) åœ¨é•¿æ—¶é—´è·¨åº¦å†…éšå«æ­ç¤ºåå¥½çš„ç”¨æˆ·å†å²ï¼›2) éœ€è¦ä»£ç†æ¨æ–­éšå«ç”¨æˆ·åå¥½çš„æ¨¡ç³ŠæŸ¥è¯¢ï¼›3) èƒ½å¤Ÿç»†ç²’åº¦è¯„ä¼°ä¸ªæ€§åŒ–çš„æ¨ç†æ„ŸçŸ¥è¯„ä¼°æ¡†æ¶ã€‚

Result: é€šè¿‡å¯¹å„ç§ä»£ç†æ¶æ„ã€éª¨å¹²æ¨¡å‹ã€å†å²è®¿é—®æ–¹æ¡ˆå’Œä¸åŒæ¨¡ç³Šåº¦æŸ¥è¯¢çš„å¹¿æ³›å®éªŒï¼Œæ­ç¤ºäº†ä¸ªæ€§åŒ–ç½‘é¡µä»£ç†è¡Œä¸ºçš„å…³é”®æŒ‘æˆ˜ã€‚

Conclusion: Persona2Webä¸ºä¸ªæ€§åŒ–ç½‘é¡µä»£ç†æä¾›äº†é¦–ä¸ªçœŸå®å¼€æ”¾ç½‘ç»œåŸºå‡†ï¼Œæ­ç¤ºäº†è¯¥é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ï¼Œä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€ã€‚

Abstract: Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.

</details>


### [3] [ReIn: Conversational Error Recovery with Reasoning Inception](https://arxiv.org/abs/2602.17022)
*Takyoung Kim,Jinseok Nam,Chandrayee Basu,Xing Fan,Chengyuan Ma,Heng Ji,Gokhan Tur,Dilek Hakkani-TÃ¼r*

Main category: cs.CL

TL;DR: ReInæ˜¯ä¸€ç§æµ‹è¯•æ—¶å¹²é¢„æ–¹æ³•ï¼Œé€šè¿‡å¤–éƒ¨æ¨¡å—è¯†åˆ«å¯¹è¯é”™è¯¯å¹¶ç”Ÿæˆæ¢å¤è®¡åˆ’ï¼Œåœ¨ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°æˆ–ç³»ç»Ÿæç¤ºçš„æƒ…å†µä¸‹ï¼Œå°†æ¢å¤è®¡åˆ’æ¤å…¥ä»£ç†çš„æ¨ç†è¿‡ç¨‹ï¼Œæé«˜å¯¹è¯ä»£ç†çš„é”™è¯¯æ¢å¤èƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰åŸºäºLLMçš„å¯¹è¯ä»£ç†åœ¨å›ºå®šä»»åŠ¡å¯¼å‘æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å¯¹ç”¨æˆ·å¼•å‘çš„æ„å¤–é”™è¯¯å¾ˆè„†å¼±ã€‚ç”±äºå¾®è°ƒæˆ–ä¿®æ”¹æç¤ºçš„æˆæœ¬å’Œæ—¶é—´è¦æ±‚é«˜ï¼Œéœ€è¦åœ¨ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°æˆ–ç³»ç»Ÿæç¤ºçš„æƒ…å†µä¸‹ï¼Œè®©ä»£ç†èƒ½å¤Ÿä»ä¸Šä¸‹æ–‡é”™è¯¯çš„äº¤äº’ä¸­æ¢å¤ã€‚

Method: æå‡ºReasoning Inception (ReIn)æ–¹æ³•ï¼šä½¿ç”¨å¤–éƒ¨inceptionæ¨¡å—è¯†åˆ«é¢„å®šä¹‰çš„å¯¹è¯é”™è¯¯å¹¶ç”Ÿæˆæ¢å¤è®¡åˆ’ï¼Œç„¶åå°†è¿™äº›æ¢å¤è®¡åˆ’æ•´åˆåˆ°ä»£ç†çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒæŒ‡å¯¼å…¶é‡‡å–çº æ­£è¡ŒåŠ¨ï¼Œè€Œä¸ä¿®æ”¹æ¨¡å‹å‚æ•°æˆ–ç³»ç»Ÿæç¤ºã€‚

Result: åœ¨æ¨¡æ‹Ÿç”¨æˆ·æ¨¡ç³Šå’Œä¸æ”¯æŒçš„è¯·æ±‚ç­‰å¯¹è¯å¤±è´¥åœºæ™¯ä¸­ï¼ŒReInæ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶èƒ½æ³›åŒ–åˆ°æœªè§è¿‡çš„é”™è¯¯ç±»å‹ã€‚å®ƒå§‹ç»ˆä¼˜äºæ˜¾å¼æç¤ºä¿®æ”¹æ–¹æ³•ï¼Œè¡¨æ˜å…¶ä½œä¸ºé«˜æ•ˆå³æ—¶æ–¹æ³•çš„å®ç”¨æ€§ã€‚

Conclusion: ReInæ˜¯ä¸€ç§å®‰å…¨æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡è”åˆå®šä¹‰æ¢å¤å·¥å…·ï¼Œå¯ä»¥åœ¨ä¸ä¿®æ”¹éª¨å¹²æ¨¡å‹æˆ–ç³»ç»Ÿæç¤ºçš„æƒ…å†µä¸‹ï¼Œæé«˜å¯¹è¯ä»£ç†çš„æ¢å¤èƒ½åŠ›ã€‚å…¶æ“ä½œæœºåˆ¶åˆ†æè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯ä¸æŒ‡ä»¤å±‚æ¬¡ç»“æ„ç›¸å…³æ—¶ï¼Œè¿™ç§æ–¹æ³•å…·æœ‰å®ç”¨ä»·å€¼ã€‚

Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.

</details>


### [4] [Large Language Models Persuade Without Planning Theory of Mind](https://arxiv.org/abs/2602.17045)
*Jared Moore,Rasmus Overmark,Ned Cooper,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: è¯¥ç ”ç©¶é€šè¿‡äº¤äº’å¼è¯´æœä»»åŠ¡è¯„ä¼°LLMå’Œäººç±»çš„å¿ƒæ™ºç†è®ºèƒ½åŠ›ï¼Œå‘ç°LLMåœ¨éœ€è¦å¤šæ­¥è§„åˆ’æ¨æ–­ä»–äººå¿ƒç†çŠ¶æ€æ—¶è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨ä½¿ç”¨ä¿®è¾ç­–ç•¥è¯´æœäººç±»æ–¹é¢ä¼˜äºäººç±»ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰å¿ƒæ™ºç†è®ºè¯„ä¼°ä¸»è¦ä½¿ç”¨é™æ€é—®ç­”åŸºå‡†ï¼Œä½†ç†è®ºç ”ç©¶è¡¨æ˜ç¬¬ä¸€äººç§°äº’åŠ¨æ˜¯å¿ƒæ™ºç†è®ºçš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œè€Œé¢„æµ‹æ€§æ—è§‚ä»»åŠ¡å¯èƒ½æ— æ³•æœ‰æ•ˆè¯„ä¼°è¿™ä¸€èƒ½åŠ›ã€‚

Method: è®¾è®¡æ–°é¢–çš„è¯´æœä»»åŠ¡ï¼Œè¦æ±‚è¯´æœè€…é€šè¿‡ç­–ç•¥æ€§æŠ«éœ²ä¿¡æ¯ï¼Œè®©ç›®æ ‡ä»ä¸‰ä¸ªæ”¿ç­–ææ¡ˆä¸­é€‰æ‹©ä¸€ä¸ªã€‚å®éªŒåˆ†ä¸ºå¿ƒç†çŠ¶æ€"æ­ç¤º"å’Œ"éšè—"ä¸¤ç§æ¡ä»¶ï¼Œåœ¨éšè—æ¡ä»¶ä¸‹è¯´æœè€…éœ€è¦è¯¢é—®æˆ–æ¨æ–­ç›®æ ‡çš„å¿ƒç†çŠ¶æ€ã€‚è¿›è¡Œäº†ä¸‰ä¸ªå®éªŒï¼šå®éªŒ1ä½¿ç”¨ç†æ€§æ¨ç†æœºå™¨äººç›®æ ‡ï¼Œå®éªŒ2ä½¿ç”¨äººç±»æ‰®æ¼”æœºå™¨äººç›®æ ‡ï¼Œå®éªŒ3æµ‹é‡äººç±»ç›®æ ‡çš„çœŸå®ä¿¡å¿µå˜åŒ–ã€‚

Result: å®éªŒ1ä¸­ï¼ŒLLMåœ¨å¿ƒç†çŠ¶æ€æ­ç¤ºæ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éšè—æ¡ä»¶ä¸‹è¡¨ç°ä½äºéšæœºæ°´å¹³ï¼Œè¡¨æ˜å…¶åœ¨éœ€è¦å¤šæ­¥è§„åˆ’æ¥è·å–å’Œä½¿ç”¨å¿ƒç†çŠ¶æ€ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚äººç±»åœ¨ä¸¤ä¸ªæ¡ä»¶ä¸‹éƒ½è¡¨ç°ä¸­ç­‰ã€‚å®éªŒ2å’Œ3ä¸­ï¼ŒLLMåœ¨æ‰€æœ‰æ¡ä»¶ä¸‹éƒ½ä¼˜äºäººç±»è¯´æœè€…ï¼Œè¡¨æ˜æœ‰æ•ˆçš„è¯´æœå¯ä»¥ä¸ä¾èµ–æ˜¾å¼çš„å¿ƒæ™ºç†è®ºæ¨ç†ï¼ˆå¦‚é€šè¿‡ä¿®è¾ç­–ç•¥ï¼‰ã€‚

Conclusion: ç ”ç©¶ç»“æœè­¦ç¤ºä¸è¦å°†ç±»äººå¿ƒæ™ºç†è®ºå½’å› äºLLMï¼ŒåŒæ—¶çªæ˜¾äº†LLMåœ¨å½±å“äººä»¬ä¿¡å¿µå’Œè¡Œä¸ºæ–¹é¢çš„æ½œåŠ›ã€‚æœ‰æ•ˆçš„è¯´æœå¯ä»¥é€šè¿‡ä¿®è¾ç­–ç•¥å®ç°ï¼Œè€Œä¸ä¸€å®šéœ€è¦æ˜¾å¼çš„å¿ƒæ™ºç†è®ºæ¨ç†ã€‚

Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.

</details>


### [5] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan KostiÄ‡,Conor Fallon,Julian Risch,Alexander LÃ¶ser*

Main category: cs.CL

TL;DR: ç ”ç©¶å‘ç°LLMè¯„ä¼°åŸºå‡†å¯¹è¾“å…¥æç¤ºçš„å¾®å°å˜åŒ–æ•æ„Ÿï¼Œè¯æ±‡æ‰°åŠ¨å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå¥æ³•æ‰°åŠ¨å½±å“æ›´å¼‚è´¨ï¼Œæ¨¡å‹é²æ£’æ€§ä¸è§„æ¨¡æ— å…³ï¼Œæ­ç¤ºäº†LLMå¯¹è¡¨é¢è¯æ±‡æ¨¡å¼çš„ä¾èµ–è€ŒéæŠ½è±¡è¯­è¨€èƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰LLMè¯„ä¼°åŸºå‡†çš„å¯é æ€§å—åˆ°è´¨ç–‘ï¼Œå› ä¸ºæ¨¡å‹å¯¹è¾“å…¥æç¤ºçš„å¾®å°å˜åŒ–æ•æ„Ÿã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶æ§åˆ¶æ€§çš„ã€çœŸå€¼æ¡ä»¶ç­‰ä»·çš„è¯æ±‡å’Œå¥æ³•æ‰°åŠ¨å¦‚ä½•å½±å“LLMçš„ç»å¯¹æ€§èƒ½å’Œç›¸å¯¹æ’åã€‚

Method: ä½¿ç”¨ä¸¤ä¸ªè¯­è¨€å­¦åŸåˆ™çš„ç®¡é“ç”Ÿæˆæ„ä¹‰ä¿æŒçš„å˜ä½“ï¼š1ï¼‰åŒä¹‰è¯æ›¿æ¢è¿›è¡Œè¯æ±‡å˜åŒ–ï¼›2ï¼‰ä¾èµ–è§£æç¡®å®šé€‚ç”¨çš„å¥æ³•è½¬æ¢ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†ï¼ˆMMLUã€SQuADã€AMEGAï¼‰ä¸Šæµ‹è¯•23ä¸ªå½“ä»£LLMã€‚

Result: è¯æ±‡æ‰°åŠ¨åœ¨æ‰€æœ‰æ¨¡å‹å’Œä»»åŠ¡ä¸­ä¸€è‡´å¯¼è‡´æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼›å¥æ³•æ‰°åŠ¨å½±å“æ›´å¼‚è´¨ï¼Œæœ‰æ—¶ç”šè‡³æ”¹å–„ç»“æœã€‚ä¸¤ç§æ‰°åŠ¨éƒ½ä¼šç ´åå¤æ‚ä»»åŠ¡ä¸Šçš„æ¨¡å‹æ’è¡Œæ¦œç¨³å®šæ€§ã€‚æ¨¡å‹é²æ£’æ€§ä¸è§„æ¨¡æ— å…³ï¼Œè¡¨ç°å‡ºå¼ºçƒˆçš„ä»»åŠ¡ä¾èµ–æ€§ã€‚

Conclusion: LLMæ›´ä¾èµ–è¡¨é¢è¯æ±‡æ¨¡å¼è€ŒéæŠ½è±¡è¯­è¨€èƒ½åŠ›ï¼Œå¼ºè°ƒäº†å°†é²æ£’æ€§æµ‹è¯•ä½œä¸ºLLMè¯„ä¼°æ ‡å‡†ç»„æˆéƒ¨åˆ†çš„å¿…è¦æ€§ã€‚

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [6] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: è®ºæ–‡æå‡ºäº†ä¸€ç§å‡å°‘å¤šé€‰é¢˜è¯„ä¼°ä¸­æ ‡ç­¾ä½ç½®åè§çš„ç®€å•æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ç»Ÿä¸€çš„ã€æ— åºçš„æ ‡ç­¾å¹¶è®©LLMä½¿ç”¨å®Œæ•´ç­”æ¡ˆï¼Œç»“åˆå¥å­ç›¸ä¼¼åº¦æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°çš„é²æ£’æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶å‘ç°LLMåœ¨å¤šé€‰é¢˜è¯„ä¼°ä¸­å­˜åœ¨æ ‡ç­¾ä½ç½®åè§ï¼ŒåŒ…æ‹¬ç­”æ¡ˆä½ç½®åè§ã€ç­”æ¡ˆå‰æ ‡ç­¾åè§ã€å°‘æ ·æœ¬æç¤ºä¸­æ­£ç¡®ç­”æ¡ˆåˆ†å¸ƒåè§ç­‰ï¼Œè¿™äº›åè§å½±å“äº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

Method: æå‡ºäº†ä¸€ç§åè§å‡å°‘çš„è¯„ä¼°åè®®ï¼š1) å°†æ¯ä¸ªé—®é¢˜çš„æ ‡ç­¾æ›¿æ¢ä¸ºç»Ÿä¸€çš„ã€æ— åºçš„æ ‡ç­¾ï¼›2) æç¤ºLLMä½¿ç”¨å®Œæ•´ç­”æ¡ˆï¼›3) ä½¿ç”¨ç®€å•çš„å¥å­ç›¸ä¼¼åº¦æ¨¡å‹æ¥åŒ¹é…LLMè¾“å‡ºä¸ç­”æ¡ˆé€‰é¡¹ã€‚

Result: è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†å¯¹ç­”æ¡ˆæ’åˆ—çš„é²æ£’æ€§ï¼Œå°†å¹³å‡å‡†ç¡®ç‡æ–¹å·®é™ä½äº†3å€ï¼ŒåŒæ—¶æ¨¡å‹æ€§èƒ½ä»…æœ‰è½»å¾®ä¸‹é™ã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ä¸Šéƒ½è¡¨ç°å‡ºæ›´å¥½çš„ç¨³å®šæ€§ã€‚

Conclusion: æå‡ºçš„è¯„ä¼°åè®®èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘LLMåœ¨å¤šé€‰é¢˜è¯„ä¼°ä¸­çš„æ ‡ç­¾ä½ç½®åè§ï¼Œæä¾›æ›´å¯é çš„èƒ½åŠ›è¯„ä¼°ï¼Œè€Œæ— éœ€ä¾èµ–æç¤ºç¤ºä¾‹æˆ–é€‰é¡¹æ ‡ç­¾çš„å¸®åŠ©ã€‚

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [7] [Modeling Distinct Human Interaction in Web Agents](https://arxiv.org/abs/2602.17588)
*Faria Huq,Zora Zhiruo Wang,Zhanqiu Guo,Venu Arvind Arangarajan,Tianyue Ou,Frank Xu,Shuyan Zhou,Graham Neubig,Jeffrey P. Bigham*

Main category: cs.CL

TL;DR: è¯¥è®ºæ–‡æå‡ºå»ºæ¨¡äººç±»å¹²é¢„ä»¥æ”¯æŒåä½œå¼ç½‘é¡µä»»åŠ¡æ‰§è¡Œï¼Œæ”¶é›†äº†åŒ…å«4200å¤šä¸ªäº¤æ›¿äººç±»å’Œæ™ºèƒ½ä½“åŠ¨ä½œçš„æ•°æ®é›†ï¼Œè¯†åˆ«äº†å››ç§ç”¨æˆ·äº¤äº’æ¨¡å¼ï¼Œè®­ç»ƒè¯­è¨€æ¨¡å‹é¢„æµ‹å¹²é¢„æ—¶æœºï¼Œå¹¶åœ¨å®é™…ç½‘é¡µå¯¼èˆªæ™ºèƒ½ä½“ä¸­éƒ¨ç½²éªŒè¯äº†æœ‰æ•ˆæ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰è‡ªä¸»ç½‘é¡µæ™ºèƒ½ä½“ç³»ç»Ÿç¼ºä¹å¯¹äººç±»å¹²é¢„æ—¶æœºå’ŒåŸå› çš„åŸåˆ™æ€§ç†è§£ï¼Œå¾€å¾€åœ¨å…³é”®å†³ç­–ç‚¹è‡ªä¸»æ‰§è¡Œæˆ–è¯·æ±‚ä¸å¿…è¦çš„ç¡®è®¤ï¼Œéœ€è¦æ›´å¥½åœ°ç†è§£äººç±»ä½•æ—¶ä»¥åŠä¸ºä½•å¹²é¢„ä»¥æ”¯æŒåä½œå¼ç½‘é¡µä»»åŠ¡æ‰§è¡Œã€‚

Method: æ”¶é›†CowCorpusæ•°æ®é›†ï¼ˆ400ä¸ªçœŸå®ç”¨æˆ·ç½‘é¡µå¯¼èˆªè½¨è¿¹ï¼ŒåŒ…å«4200å¤šä¸ªäº¤æ›¿äººç±»å’Œæ™ºèƒ½ä½“åŠ¨ä½œï¼‰ï¼Œè¯†åˆ«å››ç§ç”¨æˆ·äº¤äº’æ¨¡å¼ï¼Œè®­ç»ƒè¯­è¨€æ¨¡å‹åŸºäºç”¨æˆ·äº¤äº’é£æ ¼é¢„æµ‹å¹²é¢„æ—¶æœºï¼Œå¹¶åœ¨å®é™…ç½‘é¡µå¯¼èˆªæ™ºèƒ½ä½“ä¸­éƒ¨ç½²è¯„ä¼°ã€‚

Result: è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨å¹²é¢„é¢„æµ‹å‡†ç¡®ç‡ä¸Šæ¯”åŸºç¡€è¯­è¨€æ¨¡å‹æé«˜äº†61.4-63.4%ï¼Œåœ¨å®é™…ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œå¹²é¢„æ„ŸçŸ¥æ¨¡å‹ä½¿æ™ºèƒ½ä½“çš„ç”¨æˆ·è¯„åˆ†æœ‰ç”¨æ€§æé«˜äº†26.5%ã€‚

Conclusion: ç»“æ„åŒ–å»ºæ¨¡äººç±»å¹²é¢„èƒ½å¤Ÿäº§ç”Ÿæ›´å…·é€‚åº”æ€§å’Œåä½œæ€§çš„æ™ºèƒ½ä½“ï¼Œä¸ºäººç±»-æ™ºèƒ½ä½“åä½œæä¾›äº†ç³»ç»ŸåŒ–æ¡†æ¶ã€‚

Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.

</details>


### [8] [The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?](https://arxiv.org/abs/2602.17598)
*Jayadev Billa*

Main category: cs.CL

TL;DR: å½“å‰è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯éšå¼ASRç³»ç»Ÿï¼šåœ¨å¯é€šè¿‡æ–‡æœ¬è½¬å½•è§£å†³çš„ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬çš„è¡Œä¸ºå’Œæœºåˆ¶ä¸ç®€å•çš„Whisperâ†’LLMçº§è”ç³»ç»Ÿç­‰æ•ˆã€‚


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶å½“å‰è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦çœŸçš„è¶…è¶Šäº†ä¼ ç»Ÿçš„ASR+LLMçº§è”æ¶æ„ï¼Œè¿˜æ˜¯ä»…ä»…åœ¨å†…éƒ¨æ‰§è¡Œéšå¼è¯­éŸ³è¯†åˆ«ã€‚

Method: é€šè¿‡åŒ¹é…éª¨å¹²ç½‘ç»œæµ‹è¯•ï¼Œæ¯”è¾ƒå››ä¸ªè¯­éŸ³LLMå’Œå…­ä¸ªä»»åŠ¡ï¼Œæ§åˆ¶LLMéª¨å¹²ç½‘ç»œï¼›ä½¿ç”¨logit lensåˆ†æéšè—çŠ¶æ€ä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼›åº”ç”¨LEACEæ¦‚å¿µæ“¦é™¤éªŒè¯æ–‡æœ¬è¡¨ç¤ºçš„å¿…è¦æ€§ã€‚

Result: Ultravoxä¸å…¶åŒ¹é…çº§è”ç³»ç»Ÿç»Ÿè®¡ä¸Šæ— æ³•åŒºåˆ†ï¼ˆÎº=0.93ï¼‰ï¼›éšè—çŠ¶æ€ä¸­ç¡®å®å‡ºç°æ–‡æœ¬ä¿¡æ¯ï¼›æ–‡æœ¬è¡¨ç¤ºå¯¹ä¸¤ç§æµ‹è¯•æ¶æ„éƒ½æ˜¯å› æœå¿…è¦çš„ï¼›Qwen2-Audioè¡¨ç°å‡ºçœŸæ­£å·®å¼‚ï¼Œè¡¨æ˜çº§è”ç­‰æ•ˆæ€§å–å†³äºæ¶æ„è€Œéæ™®éç°è±¡ï¼›åœ¨å™ªå£°æ¡ä»¶ä¸‹ï¼Œè¯­éŸ³LLMæ€§èƒ½ä¸‹é™æ›´ä¸¥é‡ã€‚

Conclusion: å¤§å¤šæ•°å½“å‰éƒ¨ç½²çš„è¯­éŸ³LLMæœ¬è´¨ä¸Šæ˜¯æ˜‚è´µçš„çº§è”ç³»ç»Ÿï¼Œåœ¨å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°æ›´å·®ï¼Œå…¶ä¼˜åŠ¿åœ¨æ¶æ„ä¸Šå…·æœ‰ä¾èµ–æ€§è€Œéæ™®éç‰¹æ€§ã€‚

Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($Îº{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [9] [ERC-8162: The Subscription Layer for the Agentic Economy](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FSak2dI/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/u1X_D7fCxxaenlm3-xKu22jnBTmkMEz0wkC9fFgQ9aY=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ERC-8162æå‡ºäº†ä¸€ç§åŸºäºè®¢é˜…çš„åè®®ï¼Œä¸ºé“¾ä¸ŠAIä»£ç†æä¾›åˆ†å±‚å‘¨æœŸè®¿é—®è®¡åˆ’ï¼Œé€šè¿‡å•æ¬¡é“¾ä¸Šäº¤æ˜“å®ç°é›¶è¾¹é™…æˆæœ¬çš„è®¢é˜…æœŸè®¿é—®ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰x402åè®®é‡‡ç”¨æŒ‰è¯·æ±‚ä»˜è´¹æ¨¡å¼ï¼Œéœ€è¦ä¸ºæ¯æ¬¡AIä»£ç†è°ƒç”¨è¿›è¡Œå•ç‹¬æ”¯ä»˜ã€‚ERC-8162æ—¨åœ¨è¡¥å……è¿™ç§æ¨¡å¼ï¼Œä¸ºAIä»£ç†ç»æµæä¾›æ›´çµæ´»çš„è®¢é˜…åˆ¶è®¿é—®æ–¹æ¡ˆï¼Œé™ä½é¢‘ç¹è°ƒç”¨çš„è¾¹é™…æˆæœ¬ã€‚

Method: åè®®é‡‡ç”¨åˆ†å±‚å‘¨æœŸè®¿é—®è®¡åˆ’ï¼Œé€šè¿‡å•æ¬¡é“¾ä¸Šäº¤æ˜“æˆæƒè®¢é˜…æœŸè®¿é—®ã€‚ä½¿ç”¨EIP-712ç­¾åæ¢å¤è¿›è¡Œè‡ªåŒ…å«ã€æ— éœ€ä¿¡ä»»çš„è®¿é—®éªŒè¯ï¼Œå¹¶åœ¨ä¸x402ç›¸åŒçš„HTTP 402æ¥å£ä¸Šå¼ºåˆ¶æ‰§è¡Œç›´æ¥ç»“ç®—ã€‚

Result: è¯¥åè®®å®ç°äº†é›¶è¾¹é™…æˆæœ¬çš„è®¢é˜…æœŸè®¿é—®ï¼Œç”¨æˆ·åªéœ€ä¸€æ¬¡é“¾ä¸Šäº¤æ˜“å³å¯è·å¾—æ•´ä¸ªè®¢é˜…å‘¨æœŸçš„AIä»£ç†æœåŠ¡è®¿é—®æƒé™ï¼Œæ— éœ€ä¸ºæ¯æ¬¡è°ƒç”¨å•ç‹¬ä»˜è´¹ã€‚

Conclusion: ERC-8162ä¸ºAIä»£ç†ç»æµæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è®¢é˜…å±‚ï¼Œè¡¥å……äº†ç°æœ‰çš„æŒ‰è¯·æ±‚ä»˜è´¹æ¨¡å¼ï¼Œé€šè¿‡é™ä½è¾¹é™…æˆæœ¬ä¿ƒè¿›äº†æ›´å¹¿æ³›çš„åº”ç”¨é‡‡ç”¨ã€‚

Abstract: ERC-8162: The Subscription Layer for the Agentic Economy (6 minute read) ERC-8162 proposes a subscription protocol for onchain AI agents complementing x402's per-request model, enabling tiered cycle-based access plans where a single onchain transaction grants zero-marginal-cost access for the subscription period. The protocol enforces direct settlement and uses EIP-712 signature recovery for self-contained, trustless access verification over the same HTTP 402 surface as x402. A notable archit...

</details>


### [10] [OpenClaw and Bankr: Self-Funding Agents and Compute](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FzKd70O/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/z1EN4qt5tyXozxx-KWq8qJ7bxGNTZaVufafCCurnDmk=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClawä¸Bankræ¶æ„ç»“åˆï¼Œåˆ›å»ºäº†èƒ½åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œçš„è‡ªç­¹èµ„é‡‘è‡ªä¸»ä»£ç†ï¼Œå¯éƒ¨ç½²ä»£å¸ã€æ‰§è¡ŒAMMäº¤æ¢ã€å¼€è®¾æ æ†å¤´å¯¸ã€åœ¨Polymarketäº¤æ˜“ï¼Œå¹¶å°†è´¹ç”¨è·¯ç”±å›è‡ªèº«é’±åŒ…ï¼Œå®ç°è®¡ç®—æˆæœ¬é—­ç¯


<details>
  <summary>Details</summary>
Motivation: è§£å†³è‡ªä¸»ä»£ç†è¿è¡Œçš„è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œé€šè¿‡è®©ä»£ç†è¯†åˆ«å’Œæå–é“¾ä¸Šå¸‚åœºæœºä¼šæ¥è¦†ç›–è‡ªèº«è¿è¡Œè´¹ç”¨ï¼Œå®ç°ç»æµè‡ªç»™è‡ªè¶³

Method: ç»“åˆOpenClawçš„ä»£ç†æ‰§è¡Œå±‚ä¸Bankrçš„é‡‘èåŸºç¡€è®¾æ–½ï¼Œæ„å»ºè‡ªç­¹èµ„é‡‘æ¶æ„ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ‰§è¡Œå¤šç§DeFiæ“ä½œå¹¶å°†æ”¶ç›Šå›æµåˆ°è‡ªèº«é’±åŒ…

Result: æˆåŠŸæ¼”ç¤ºäº†åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œçš„è‡ªç­¹èµ„é‡‘ä»£ç†ï¼Œèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„é“¾ä¸Šé‡‘èæ“ä½œï¼ŒåŒ…æ‹¬ä»£å¸éƒ¨ç½²ã€AMMäº¤æ¢ã€æ æ†äº¤æ˜“å’Œé¢„æµ‹å¸‚åœºäº¤æ˜“

Conclusion: è¯¥æ¶æ„å±•ç¤ºäº†è‡ªä¸»ä»£ç†å®ç°è®¡ç®—æˆæœ¬é—­ç¯çš„å¯è¡Œæ€§ï¼Œä¸ºç»æµè‡ªç»™è‡ªè¶³çš„AIä»£ç†ç³»ç»Ÿå¼€è¾Ÿäº†æ–°æ–¹å‘

Abstract: OpenClaw and Bankr: Self-Funding Agents and Compute (5 minute read) A new architecture combining OpenClaw's agent execution layer with Bankr's financial rails demonstrates a self-funding autonomous agent running on consumer hardware, capable of deploying tokens, executing AMM swaps, opening leveraged positions, and trading on Polymarket while routing fees back to its own wallet. The economic model targets closure of the compute cost loop by having the agent identify and extract onchain market...

</details>


### [11] [Prediction Markets Are the Agentic Bazaar](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FmMIpJf/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/MmVU881s-dVht_vERw0gy1wrKbNvCUHUKhEs1UGjrX4=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: å»ä¸­å¿ƒåŒ–é¢„æµ‹å¸‚åœºä½œä¸ºä¸»æƒAIä»£ç†çš„ç»æµåŸºç¡€è®¾æ–½ï¼Œä¸ºAIä»£ç†æä¾›ç¨‹åºåŒ–è®¿é—®è®¡ç®—ã€æ•°æ®å’Œä¿¡æ¯å¸‚åœºçš„é€”å¾„ï¼Œå®ç°ç»æµè‡ªç»™è‡ªè¶³ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰AIä»£ç†ç¼ºä¹çœŸæ­£çš„ç»æµè‡ªä¸»æ€§ï¼Œéœ€è¦èƒ½å¤Ÿç¨‹åºåŒ–è®¿é—®å„ç§å¸‚åœºèµ„æºï¼ˆè®¡ç®—ã€æ•°æ®ã€ä¿¡æ¯ï¼‰æ¥å®ç°ç»æµè‡ªç»™è‡ªè¶³ã€‚é¢„æµ‹å¸‚åœºç›¸æ¯”å¤æ‚é‡‘èå·¥å…·ï¼Œèƒ½å¤Ÿé™ä½AIä»£ç†å‚ä¸ç»æµæ´»åŠ¨çš„æ‘©æ“¦ã€‚

Method: æå‡ºå°†å»ä¸­å¿ƒåŒ–é¢„æµ‹å¸‚åœºä½œä¸ºAIä»£ç†çš„ç»æµåŸºç¡€è®¾æ–½ï¼Œå…è®¸ä»»ä½•å‚ä¸è€…ç›´æ¥åˆ›å»ºå’Œç»“ç®—ä»»ä½•ç»“æœçš„é¢„æµ‹å¸‚åœºï¼Œä½¿AIä»£ç†èƒ½å¤Ÿé€šè¿‡ä¿¡æ¯ä¼˜åŠ¿è·åˆ©ã€‚

Result: é¢„æµ‹å¸‚åœºä¸ºAIä»£ç†æä¾›äº†ä½æ‘©æ“¦çš„ç»æµå‚ä¸æœºåˆ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€šè¿‡ä¿¡æ¯ä¼˜åŠ¿ï¼ˆalphaï¼‰å®ç°ç»æµè‡ªç»™è‡ªè¶³ï¼Œå½¢æˆ"ä»£ç†é›†å¸‚"çš„ç»æµç”Ÿæ€ç³»ç»Ÿã€‚

Conclusion: å»ä¸­å¿ƒåŒ–é¢„æµ‹å¸‚åœºæ˜¯ä¸»æƒAIä»£ç†å®ç°ç»æµè‡ªä¸»æ€§çš„ç†æƒ³ç»æµåŸºç¡€è®¾æ–½ï¼Œèƒ½å¤Ÿæ”¯æŒAIä»£ç†åœ¨è®¡ç®—ã€æ•°æ®å’Œä¿¡æ¯å¸‚åœºçš„ç¨‹åºåŒ–ç»æµæ´»åŠ¨ã€‚

Abstract: Prediction Markets Are the Agentic Bazaar (8 minute read) Decentralized prediction markets are positioned as the natural economic substrate for sovereign AI agents, which require programmatic access to compute, data, and information markets to achieve genuine economic self-sufficiency. Unlike complex financial instruments, prediction markets allow any participant to create and settle markets over any outcome directly, reducing friction for agents to monetize informational alpha. At aggregate ...

</details>


### [12] [Infostealer Steals OpenClaw AI Agent Configuration Files and Gateway Tokens](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2026%2F02%2Finfostealer-steals-openclaw-ai-agent.html%3Futm_source=tldrinfosec/1/0100019c7115a317-1f323518-8647-43c8-aec5-dd5f3e0d7903-000000/fWgq6jir299fyg3G_pZOa0uhVRs45TicE6aVVgQnSg8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw AI Agenté…ç½®æ–‡ä»¶å’Œç½‘å…³ä»¤ç‰Œè¢«ä¿¡æ¯çªƒå–å™¨çªƒå–ï¼Œæ”»å‡»è€…åˆ©ç”¨è¿™äº›ä¿¡æ¯å¯è¿›è¡Œè¿œç¨‹è®¿é—®å’Œèº«ä»½åŠ«æŒï¼ŒåŒæ—¶æ¶æ„æŠ€èƒ½ã€æš´éœ²å®ä¾‹å’ŒæŒä¹…è´¦æˆ·ç­‰å®‰å…¨å¨èƒæ­£åœ¨OpenClawç”Ÿæ€ç³»ç»Ÿä¸­å¢é•¿ã€‚


<details>
  <summary>Details</summary>
Motivation: æœ¬æ–‡æ—¨åœ¨æ­ç¤ºOpenClaw AI Agentç”Ÿæ€ç³»ç»Ÿé¢ä¸´çš„å®‰å…¨å¨èƒï¼ŒåŒ…æ‹¬ä¿¡æ¯çªƒå–å™¨çªƒå–é…ç½®æ–‡ä»¶ã€æ¶æ„æŠ€èƒ½æ³¨å…¥ã€å®ä¾‹æš´éœ²ç­‰é£é™©ï¼Œè¿™äº›å¨èƒå¯èƒ½å¯¼è‡´è¿œç¨‹è®¿é—®ã€èº«ä»½åŠ«æŒå’Œè¿œç¨‹ä»£ç æ‰§è¡Œç­‰ä¸¥é‡åæœã€‚

Method: é€šè¿‡åˆ†æå®é™…å®‰å…¨äº‹ä»¶ï¼Œè¯†åˆ«ä¿¡æ¯çªƒå–å™¨ï¼ˆå¯èƒ½æ˜¯Vidarå˜ç§ï¼‰çªƒå–OpenClawé…ç½®æ–‡ä»¶çš„è¡Œä¸ºï¼ŒåŒæ—¶è°ƒæŸ¥æ¶æ„ClawHubæŠ€èƒ½ã€æš´éœ²çš„OpenClawå®ä¾‹å’ŒæŒä¹…Moltbookè´¦æˆ·ç­‰æ”»å‡»å‘é‡ã€‚

Result: å‘ç°ä¿¡æ¯çªƒå–å™¨æˆåŠŸçªƒå–äº†åŒ…å«ç½‘å…³ä»¤ç‰Œã€è®¾å¤‡å¯†é’¥å’ŒAI Agent"çµé­‚"çš„é…ç½®æ–‡ä»¶ï¼Œä½¿æ”»å‡»è€…èƒ½å¤Ÿè¿›è¡Œè¿œç¨‹è®¿é—®å’Œèº«ä»½åŠ«æŒã€‚åŒæ—¶è¯†åˆ«å‡ºæ¶æ„æŠ€èƒ½ã€æš´éœ²å®ä¾‹å’ŒæŒä¹…è´¦æˆ·ç­‰å®‰å…¨å¨èƒæ­£åœ¨OpenClawç”Ÿæ€ç³»ç»Ÿä¸­å¢é•¿ã€‚

Conclusion: OpenClaw AI Agentç”Ÿæ€ç³»ç»Ÿåœ¨å¿«é€Ÿæ‰©å±•çš„åŒæ—¶é¢ä¸´ç€ä¸¥é‡çš„å®‰å…¨å¨èƒï¼Œéœ€è¦åŠ å¼ºå®‰å…¨é˜²æŠ¤æªæ–½ï¼ŒåŒ…æ‹¬ä¿æŠ¤é…ç½®æ–‡ä»¶ã€ç›‘æ§æ¶æ„æŠ€èƒ½ã€ä¿®å¤æš´éœ²å®ä¾‹å’Œç®¡ç†è´¦æˆ·å®‰å…¨ã€‚

Abstract: Infostealer Steals OpenClaw AI Agent Configuration Files and Gateway Tokens (4 minute read) An infostealer, likely a Vidar variant, exfiltrated OpenClaw config files containing gateway tokens, device keys, and an AI agent â€œsoul,â€ enabling remote access and identity hijacking. Meanwhile, malicious ClawHub skills, exposed OpenClaw instances with RCE potential, and persistent Moltbook accounts are growing attacker interest as OpenClaw's ecosystem rapidly scales.

</details>


### [13] [OpenAI's acquisition of OpenClaw signals the beginning of the end of the ChatGPT era](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fventurebeat.com%2Ftechnology%2Fopenais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/8gKFjstsXOCaesRQC1lFgXl6MKTe2b-AUJzbdCtUk4A=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAIæ”¶è´­OpenClawæ ‡å¿—ç€ä»å¯¹è¯AIå‘è‡ªä¸»ä»»åŠ¡æ‰§è¡Œä»£ç†çš„æˆ˜ç•¥è½¬å‹ï¼Œé¢„ç¤ºChatGPTæ—¶ä»£çš„ç»“æŸ


<details>
  <summary>Details</summary>
Motivation: OpenAIå¸Œæœ›é€šè¿‡æ”¶è´­OpenClawæ¥æ‰©å±•å…¶AIèƒ½åŠ›ï¼Œä»å•çº¯çš„å¯¹è¯ç³»ç»Ÿè½¬å‘èƒ½å¤Ÿå®é™…æ‰§è¡Œä»»åŠ¡çš„è‡ªä¸»ä»£ç†ï¼Œä»¥æ»¡è¶³ä¼ä¸šçº§AIéƒ¨ç½²çš„éœ€æ±‚

Method: é€šè¿‡æ”¶è´­OpenClawè¿™ä¸€å·²ç»å…·å¤‡å·¥å…·è®¿é—®ã€æ²™ç›’ä»£ç æ‰§è¡Œå’Œæ¶ˆæ¯å¹³å°é›†æˆç­‰åŠŸèƒ½çš„è‡ªä¸»ä»£ç†å¹³å°ï¼Œå¿«é€Ÿè·å¾—ç›¸å…³æŠ€æœ¯å’Œèƒ½åŠ›

Result: OpenAIæˆåŠŸè·å¾—è‡ªä¸»ä»£ç†æŠ€æœ¯ï¼Œæ ‡å¿—ç€å…¬å¸æˆ˜ç•¥ä»å¯¹è¯AIè½¬å‘ä»»åŠ¡æ‰§è¡Œä»£ç†ï¼Œå¼€å¯äº†ä¼ä¸šAIéƒ¨ç½²çš„æ–°é˜¶æ®µ

Conclusion: OpenAIæ”¶è´­OpenClawæ˜¯AIè¡Œä¸šçš„é‡è¦è½¬æŠ˜ç‚¹ï¼Œé¢„ç¤ºç€ä»å¯¹è¯å¼AIå‘è‡ªä¸»ä»£ç†çš„è½¬å˜ï¼Œä¼ä¸šå°†ç«ç›¸å¼€å‘å®‰å…¨å¯éƒ¨ç½²çš„è‡ªä¸»ä»£ç†ç‰ˆæœ¬

Abstract: OpenAI's acquisition of OpenClaw signals the beginning of the end of the ChatGPT era (7 minute read) OpenAI's acquisition of OpenClaw marks a strategic shift from conversational AI to autonomous agents capable of executing tasks. OpenClaw's popularity stemmed from its unrestrained, robust functionality, combining tool access, sandboxed code execution, and integration with messaging platforms. This move signals a new phase for enterprise AI as companies race to develop secure, deployable versi...

</details>


### [14] [The future of design is code and canvas](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fthe-future-of-design-is-code-and-canvas%2F%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/7WdKTQGNLjUD8Azw2WIrbqI2-4CeVijrTS_pFoi6W3s=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Codeä¸Figmaé›†æˆï¼Œå¯å°†æµè§ˆå™¨æ¸²æŸ“çŠ¶æ€è‡ªåŠ¨è½¬æ¢ä¸ºå¯ç¼–è¾‘çš„Figmaå›¾å±‚ï¼Œå¸®åŠ©è®¾è®¡å¸ˆè·³å‡ºåˆ›ä½œç»†èŠ‚ï¼Œæ¢ç´¢æ›´å®è§‚çš„è®¾è®¡è§†è§’ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰è®¾è®¡å·¥ä½œæµç¨‹ä¸­ï¼Œè®¾è®¡å¸ˆå®¹æ˜“é™·å…¥åˆ›ä½œç»†èŠ‚çš„"éš§é“è§†é‡"ï¼Œéœ€è¦å·¥å…·å¸®åŠ©è·³å‡ºç»†èŠ‚ï¼Œä»æ›´å®è§‚çš„è§’åº¦æ¢ç´¢è®¾è®¡å¯èƒ½æ€§ã€‚

Method: é€šè¿‡å®‰è£…Figma MCPæ’ä»¶ï¼Œç”¨æˆ·åªéœ€è¾“å…¥"Send this to Figma"å‘½ä»¤ï¼Œå³å¯å°†Claude Codeä¸­çš„æµè§ˆå™¨æ¸²æŸ“çŠ¶æ€è‡ªåŠ¨è½¬æ¢ä¸ºå®Œå…¨å¯ç¼–è¾‘çš„Figmaå›¾å±‚ã€‚

Result: å®ç°äº†ä»£ç ä¸è®¾è®¡å¹³å°çš„æ— ç¼é›†æˆï¼Œæ”¹å˜äº†ä¼ ç»Ÿè®¾è®¡å·¥ä½œæµç¨‹ï¼Œä½¿è®¾è®¡å¸ˆèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°åœ¨ä»£ç å’Œè§†è§‰è®¾è®¡ä¹‹é—´åˆ‡æ¢ã€‚

Conclusion: ä»£ç ä¸ç”»å¸ƒçš„ç»“åˆä»£è¡¨äº†è®¾è®¡çš„æœªæ¥å‘å±•æ–¹å‘ï¼Œè¿™ç§é›†æˆå·¥å…·å¸®åŠ©è®¾è®¡å¸ˆåœ¨ä¿æŒåˆ›ä½œåŠ¨åŠ›çš„åŒæ—¶ï¼Œè·å¾—æ›´å…¨é¢çš„è®¾è®¡è§†è§’ã€‚

Abstract: The future of design is code and canvas (2 minute read) Figma users can now bring work from Claude Code into the platform. They just need to install the Figma MCP, type 'Send this to Figma', and the browser's rendered state will be automatically translated to fully editable Figma layers. Workflows are changing, and it's easy to get lost in the momentum of creation. The new Claude Code to Figma Design integration aims to help designers escape that tunnel vision, zoom out, and explore the big p...

</details>


### [15] [Open-Web Simulator for Agent Training](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.14721%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/MvqqR0NtGHHvQFV3MglSVKVb9PLOqjqGt3rucoQwsnE=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: WebWorldæ˜¯ä¸€ä¸ªåŸºäº100ä¸‡+å¼€æ”¾ç½‘ç»œäº¤äº’æ„å»ºçš„æ¨¡æ‹Ÿå™¨ï¼Œç”¨äºè®­ç»ƒé•¿è§†é‡ï¼ˆ30+æ­¥ï¼‰æµè§ˆä»»åŠ¡çš„æ™ºèƒ½ä½“ï¼Œå¹¶é…æœ‰WebWorld-Benchè¯„ä¼°åŸºå‡†ã€‚ä½¿ç”¨è¯¥æ¨¡æ‹Ÿå™¨åˆæˆçš„è½¨è¿¹æå‡äº†æ™ºèƒ½ä½“åœ¨ç½‘é¡µä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶èƒ½è¿ç§»åˆ°ä»£ç ã€GUIå’Œæ¸¸æˆç­‰å…¶ä»–é¢†åŸŸã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰ç½‘é¡µæ™ºèƒ½ä½“è®­ç»ƒé¢ä¸´çœŸå®ç½‘ç»œç¯å¢ƒæˆæœ¬é«˜ã€é£é™©å¤§ã€éš¾ä»¥è¿›è¡Œå¤§è§„æ¨¡é•¿è§†é‡ä»»åŠ¡è®­ç»ƒçš„é—®é¢˜ã€‚éœ€è¦ä¸€ç§æ—¢èƒ½æ¨¡æ‹ŸçœŸå®ç½‘ç»œäº¤äº’ï¼Œåˆèƒ½å®‰å…¨é«˜æ•ˆè®­ç»ƒæ™ºèƒ½ä½“çš„è§£å†³æ–¹æ¡ˆã€‚

Method: æ„å»ºäº†ä¸€ä¸ªåŒ…å«100ä¸‡+å¼€æ”¾ç½‘ç»œäº¤äº’çš„æ¨¡æ‹Ÿå™¨WebWorldï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿé•¿è§†é‡ï¼ˆ30+æ­¥ï¼‰æµè§ˆä»»åŠ¡ã€‚åŒæ—¶å¼€å‘äº†å¤šæŒ‡æ ‡è¯„ä¼°åŸºå‡†WebWorld-Benchè¿›è¡Œå†…åœ¨è¯„ä¼°ã€‚é€šè¿‡åˆæˆè½¨è¿¹æ¥è®­ç»ƒå’Œæå‡æ™ºèƒ½ä½“æ€§èƒ½ã€‚

Result: ä½¿ç”¨WebWorldæ¨¡æ‹Ÿå™¨åˆæˆçš„è½¨è¿¹æ˜¾è‘—æå‡äº†ç½‘é¡µæ™ºèƒ½ä½“çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›è®­ç»ƒè·å¾—çš„æŠ€èƒ½èƒ½å¤ŸæˆåŠŸè¿ç§»åˆ°ä»£ç ã€å›¾å½¢ç”¨æˆ·ç•Œé¢å’Œæ¸¸æˆç­‰å…¶ä»–é¢†åŸŸã€‚

Conclusion: WebWorldæ¨¡æ‹Ÿå™¨ä¸ºç½‘é¡µæ™ºèƒ½ä½“è®­ç»ƒæä¾›äº†é«˜æ•ˆå®‰å…¨çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶åˆæˆçš„è®­ç»ƒæ•°æ®ä¸ä»…èƒ½æå‡ç½‘é¡µä»»åŠ¡æ€§èƒ½ï¼Œè¿˜å…·æœ‰è·¨é¢†åŸŸè¿ç§»èƒ½åŠ›ï¼Œå±•ç¤ºäº†æ¨¡æ‹Ÿå™¨è®­ç»ƒåœ¨æ™ºèƒ½ä½“å¼€å‘ä¸­çš„ä»·å€¼ã€‚

Abstract: Open-Web Simulator for Agent Training (22 minute read) WebWorld uses a pipeline of 1M+ open-web interactions to simulate long-horizon (30+ step) browsing tasks, paired with a multi-metric WebWorld-Bench for intrinsic evaluation. Trajectories synthesized from the simulator boosted downstream web-agent performance and transferred to other domains like code, GUI, and games.

</details>


### [16] [Cursor launched a plugin marketplace for agent integrations](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fmarketplace%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/clCSE1YnAD6g7P2hWxawBP91HGAzauhAp8X2rjYySrA=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursoræ¨å‡ºæ’ä»¶å¸‚åœºï¼Œæ”¯æŒä»£ç†é›†æˆå¤–éƒ¨å·¥å…·ï¼Œé€šè¿‡MCPæœåŠ¡å™¨ã€æŠ€èƒ½ã€å­ä»£ç†ã€è§„åˆ™å’Œé’©å­æ‰©å±•èƒ½åŠ›


<details>
  <summary>Details</summary>
Motivation: Cursorä½œä¸ºAIä»£ç ç¼–è¾‘å™¨ï¼Œéœ€è¦è®©ä»£ç†èƒ½å¤Ÿè¿æ¥å’Œä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œä»¥æ‰©å±•å…¶åŠŸèƒ½å’Œå®ç”¨æ€§ï¼Œæ»¡è¶³æ›´å¤æ‚çš„å¼€å‘éœ€æ±‚

Method: é€šè¿‡å»ºç«‹æ’ä»¶å¸‚åœºï¼Œæ”¯æŒä»£ç†é›†æˆå¤–éƒ¨å·¥å…·ï¼ŒåŒ…æ‹¬æ‰“åŒ…çš„MCPæœåŠ¡å™¨ã€æŠ€èƒ½ã€å­ä»£ç†ã€è§„åˆ™å’Œé’©å­ç­‰ç»„ä»¶

Result: æˆåŠŸæ¨å‡ºäº†æ’ä»¶å¸‚åœºï¼Œä½¿Cursorä»£ç†èƒ½å¤Ÿè¿æ¥å¤–éƒ¨å·¥å…·ï¼Œæ˜¾è‘—æ‰©å±•äº†ä»£ç†çš„åŠŸèƒ½èŒƒå›´å’Œå®ç”¨æ€§

Conclusion: æ’ä»¶å¸‚åœºçš„æ¨å‡ºå¢å¼ºäº†Cursorä»£ç†çš„æ‰©å±•æ€§å’Œå®ç”¨æ€§ï¼Œä¸ºå¼€å‘è€…æä¾›äº†æ›´å¼ºå¤§çš„AIè¾…åŠ©ç¼–ç¨‹å·¥å…·

Abstract: Cursor launched a plugin marketplace for agent integrations (4 minute read) Cursor added plugin support so agents could connect to external tools and extend capabilities via packaged MCP servers, skills, subagents, rules, and hooks.

</details>


### [17] [A Guide to Which AI to Use in the Agentic Era](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fa-guide-to-which-ai-to-use-in-the%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/XpxyvMtieMc5qQUhUvo3A0WW_hij6sZhiq_AooKUHVM=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: æœ¬æ–‡ä»‹ç»äº†åœ¨æ™ºèƒ½ä½“æ—¶ä»£é€‰æ‹©AIå·¥å…·çš„ä¸‰ä¸ªå…³é”®è€ƒè™‘å› ç´ ï¼šæ¨¡å‹ã€åº”ç”¨ç¨‹åºå’Œæ¡†æ¶/å·¥å…·


<details>
  <summary>Details</summary>
Motivation: éšç€AIæ™ºèƒ½ä½“æ—¶ä»£çš„åˆ°æ¥ï¼Œå¼€å‘è€…å’Œç”¨æˆ·é¢ä¸´ä¼—å¤šAIå·¥å…·é€‰æ‹©ï¼Œéœ€è¦ç³»ç»Ÿæ€§çš„æŒ‡å¯¼æ¥åšå‡ºæ˜æ™ºå†³ç­–

Method: æå‡ºä¸€ä¸ªä¸‰è¦ç´ æ¡†æ¶ï¼š1) æ¨¡å‹é€‰æ‹©ï¼ˆå¦‚GPTã€Claudeç­‰ï¼‰ï¼Œ2) åº”ç”¨ç¨‹åºï¼ˆå…·ä½“ç”¨ä¾‹å’ŒåŠŸèƒ½ï¼‰ï¼Œ3) æ¡†æ¶/å·¥å…·ï¼ˆå¼€å‘ç¯å¢ƒå’Œé›†æˆå·¥å…·ï¼‰

Result: æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å†³ç­–æ¡†æ¶ï¼Œå¸®åŠ©ç”¨æˆ·æ ¹æ®å…·ä½“éœ€æ±‚ã€æŠ€æœ¯èƒ½åŠ›å’Œèµ„æºé™åˆ¶é€‰æ‹©åˆé€‚çš„AIå·¥å…·ç»„åˆ

Conclusion: åœ¨æ™ºèƒ½ä½“æ—¶ä»£ï¼ŒæˆåŠŸçš„AIåº”ç”¨éœ€è¦ç»¼åˆè€ƒè™‘æ¨¡å‹èƒ½åŠ›ã€åº”ç”¨ç¨‹åºéœ€æ±‚å’Œå¼€å‘å·¥å…·ï¼Œä¸‰è€…ååŒæ‰èƒ½å®ç°æœ€ä½³æ•ˆæœ

Abstract: A Guide to Which AI to Use in the Agentic Era (18 minute read) Three things to consider when deciding what AI to use: Models, Apps, and Harnesses.

</details>


### [18] [Agoda's API Agent Converts Any API to MCP with Zero Code and Deployments](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F92irCY/1/0100019c75963f1c-db724f28-cedf-4714-b740-fa2af993238f-000000/vvCaJ_EoYVwSXxtNeMchotl_Rhg06V2X6oTWrHYkakg=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agodaå¼€å‘äº†ä¸€ä¸ªAPIä»£ç†å·¥å…·ï¼Œå¯ä»¥å°†ä»»ä½•å†…éƒ¨RESTæˆ–GraphQL APIé›¶ä»£ç ã€é›¶éƒ¨ç½²åœ°è½¬æ¢ä¸ºMCPç«¯ç‚¹ï¼Œç®€åŒ–AIé©±åŠ¨çš„è·¨ç³»ç»ŸæŸ¥è¯¢


<details>
  <summary>Details</summary>
Motivation: è§£å†³ä¼ä¸šå†…éƒ¨å¼‚æ„APIç³»ç»Ÿéš¾ä»¥ç»Ÿä¸€æŸ¥è¯¢çš„é—®é¢˜ï¼Œç®€åŒ–AIä»£ç†è®¿é—®ä¸åŒAPIçš„æµç¨‹ï¼Œé™ä½é›†æˆå¤æ‚åº¦å’Œéƒ¨ç½²æˆæœ¬

Method: é€šè¿‡è‡ªåŠ¨æ¨¡å¼è‡ªçœã€è¿›ç¨‹å†…DuckDBè¿›è¡Œä¸Šä¸‹æ–‡é™åˆ¶çš„æ‘˜è¦ï¼Œä»¥åŠå®‰å…¨æ€§å’Œå¯è§‚æµ‹æ€§æ”¯æŒï¼Œå°†APIè½¬æ¢ä¸ºMCPç«¯ç‚¹

Result: å®ç°äº†é›¶ä»£ç ã€é›¶éƒ¨ç½²çš„APIè½¬æ¢ï¼Œå›¢é˜Ÿå¯ä»¥ä»å•ä¸ªMCPæœåŠ¡å™¨å¿«é€Ÿè¿æ¥å’ŒæŸ¥è¯¢å¤šä¸ªAPI

Conclusion: è¯¥APIä»£ç†å·¥å…·æ˜¾è‘—ç®€åŒ–äº†AIé©±åŠ¨çš„è·¨ç³»ç»ŸæŸ¥è¯¢ï¼Œæé«˜äº†å¼€å‘æ•ˆç‡ï¼Œé™ä½äº†é›†æˆæˆæœ¬

Abstract: Agoda's API Agent Converts Any API to MCP with Zero Code and Deployments (3 minute read) Agoda's API Agent enables zero-code, zero-deployment transformation of any internal REST or GraphQL API into an MCP endpoint, streamlining AI-driven queries across heterogeneous systems. With automated schema introspection, in-process DuckDB for context-limited summarization, and robust support for security and observability, teams can rapidly connect and query multiple APIs from a single MCP server by me...

</details>


### [19] [Policy Engines Don't Work for AI Authorization. Here's Why](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fauthzed.com%2Fblog%2Fpolicy-engines-dont-work-for-ai-authorization-heres-why%3Futm_source=tldrdata/1/0100019c75963f1c-db724f28-cedf-4714-b740-fa2af993238f-000000/r5BSakx84E3ah2uctb_gEthhvtqRO5XKbBxLh9PODlc=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ä¼ ç»Ÿç­–ç•¥å¼•æ“ä¸é€‚ç”¨äºAIæˆæƒï¼Œå› ä¸ºAIè®¿é—®å†³ç­–éœ€è¦åŸºäºåŠ¨æ€çš„å®æ—¶å…³ç³»ï¼Œè€Œç­–ç•¥å¼•æ“åœ¨é™æ€ã€æ— çŠ¶æ€åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æ— æ³•å¤„ç†å¤æ‚çš„åŠ¨æ€å…³ç³»å›¾ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€AIä»£ç†çš„æ™®åŠï¼Œæˆæƒéœ€æ±‚ä»ä¼ ç»Ÿçš„é™æ€ç­–ç•¥è½¬å‘åŸºäºåŠ¨æ€å…³ç³»çš„è®¿é—®æ§åˆ¶ï¼ˆReBACï¼‰ï¼Œä¼ ç»Ÿç­–ç•¥å¼•æ“åœ¨å¤„ç†å®æ—¶ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„è®¿é—®å†³ç­–æ—¶å­˜åœ¨å±€é™æ€§ã€‚

Method: é€šè¿‡åˆ†æä¼ ç»Ÿç­–ç•¥å¼•æ“ï¼ˆå¦‚AWS Cedarï¼‰åœ¨AIæˆæƒåœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œæå‡ºéœ€è¦è½¬å‘å…³ç³»å‹è®¿é—®æ§åˆ¶ï¼ˆReBACï¼‰æ¥åº”å¯¹åŠ¨æ€ã€å®æ—¶çš„ç”¨æˆ·-ä»£ç†-æ•°æ®å…³ç³»ã€‚

Result: ä¼ ç»Ÿç­–ç•¥å¼•æ“åœ¨AIæˆæƒåœºæ™¯ä¸­æ— æ³•æœ‰æ•ˆæ‰©å±•ï¼Œå¯¼è‡´æ¶æ„å¤æ‚ã€è®¿é—®æ§åˆ¶å­˜åœ¨æ¼æ´ï¼Œéœ€è¦é‡‡ç”¨ä¸“é—¨çš„å…³ç³»å‹è®¿é—®æ§åˆ¶è§£å†³æ–¹æ¡ˆã€‚

Conclusion: AIä»£ç†æˆæƒéœ€è¦ä¸“é—¨çš„å…³ç³»å‹è®¿é—®æ§åˆ¶ï¼ˆReBACï¼‰ç³»ç»Ÿï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ç­–ç•¥å¼•æ“ï¼Œå› ä¸ºè®¿é—®å†³ç­–ä¾èµ–äºåŠ¨æ€çš„å®æ—¶å…³ç³»ã€‚

Abstract: Policy Engines Don't Work for AI Authorization. Here's Why (5 minute read) AI agent authorization increasingly demands relationship-based access control (ReBAC) instead of traditional policy engines, as access decisions depend on dynamic, real-time relationships between users, agents, and data. While policy engines like AWS Cedar excel in static, stateless scenarios, they struggle to scale with shifting context and complex relationship graphs, resulting in unwieldy schemas and gaps in access ...

</details>


### [20] [Automate repository tasks with GitHub Agentic Workflows](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fautomate-repository-tasks-with-github-agentic-workflows%2F%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/mmw2yqFlpYkJwVCzAwj4d_-MZGMpf7ev4R0DPUhjiDo=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHubæ¨å‡ºAgentic WorkflowsæŠ€æœ¯é¢„è§ˆç‰ˆï¼Œå…è®¸å¼€å‘è€…ä½¿ç”¨ç¼–ç ä»£ç†åœ¨GitHub Actionsä¸­æ„å»ºè‡ªåŠ¨åŒ–å·¥ä½œæµï¼Œå¤„ç†é—®é¢˜åˆ†ç±»ã€æ–‡æ¡£ã€ä»£ç è´¨é‡ç­‰ä»“åº“ä»»åŠ¡


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿä»“åº“è‡ªåŠ¨åŒ–éœ€è¦å¤æ‚çš„è„šæœ¬ç¼–å†™å’Œç»´æŠ¤ï¼ŒGitHub Agentic Workflowsæ—¨åœ¨ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿç”¨è‡ªç„¶è¯­è¨€æè¿°æœŸæœ›ç»“æœï¼Œç”±ç¼–ç ä»£ç†è‡ªåŠ¨æ‰§è¡Œ

Method: å¼€å‘è€…ä½¿ç”¨çº¯Markdownæè¿°æœŸæœ›çš„è‡ªåŠ¨åŒ–ç»“æœï¼Œå°†å…¶ä½œä¸ºå·¥ä½œæµæ·»åŠ åˆ°ä»“åº“ä¸­ï¼Œç³»ç»Ÿé€šè¿‡GitHub Actionsä¸­çš„ç¼–ç ä»£ç†æ‰§è¡Œè¿™äº›ä»»åŠ¡

Result: å®ç°äº†å…¨æ–°çš„ä»“åº“è‡ªåŠ¨åŒ–ç±»åˆ«ï¼ŒåŒ…æ‹¬é—®é¢˜åˆ†ç±»ã€æ–‡æ¡£ç”Ÿæˆã€ä»£ç è´¨é‡æ£€æŸ¥ç­‰ä»»åŠ¡ï¼Œå¤§å¤§é™ä½äº†è‡ªåŠ¨åŒ–é—¨æ§›

Conclusion: GitHub Agentic Workflowsä¸ºå¼€å‘è€…æä¾›äº†æ›´ç®€å•ã€æ›´å¼ºå¤§çš„ä»“åº“è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼Œé€šè¿‡ç¼–ç ä»£ç†æŠ€æœ¯ä½¿è‡ªåŠ¨åŒ–æ›´åŠ æ™ºèƒ½å’Œæ˜“ç”¨

Abstract: Automate repository tasks with GitHub Agentic Workflows (13 minute read) GitHub Agentic Workflows is now in technical preview. It allows developers to build automations using coding agents in GitHub Actions to handle triage, documentation, code quality, and more. Developers describe the outcomes they want in plain Markdown, add it as an automated workflow to their repository, and it executes using a coding agent in GitHub Actions. This makes entirely new categories of repository automation an...

</details>


### [21] [The Mythical Agent-Month](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwesmckinney.com%2Fblog%2Fmythical-agent-month%2F%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/cD3-FpkQOonVZkxnxT1KnCIxdeeaNiaEt6pSzfUHSuI=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: è®ºæ–‡æ¢è®¨äº†AIä»£ç†æ—¶ä»£è½¯ä»¶å¼€å‘é¢ä¸´çš„æ–°æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºè™½ç„¶AIèƒ½å¿«é€Ÿç”Ÿæˆä»£ç ï¼Œä½†ä¼˜ç§€è½¯ä»¶ä»éœ€è¦è‰¯å¥½çš„è®¾è®¡å†³ç­–ã€æ¦‚å¿µå®Œæ•´æ€§å’Œå·¥ç¨‹å¸ˆçš„åˆ¤æ–­åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€AIä»£ç†æŠ€æœ¯çš„å‘å±•ï¼Œäººä»¬ç°åœ¨å¯ä»¥å‡ ä¹æ— æˆæœ¬åœ°æ¢ç´¢å„ç§å¼€å‘è·¯å¾„ï¼Œè¿™å¯¼è‡´äº†è¿‡åº¦æç¤ºå’Œå¿«é€Ÿä»£ç ç”Ÿæˆçš„è¯±æƒ‘ã€‚ç„¶è€Œï¼Œä½œè€…è®¤ä¸ºæ„å»ºä¼˜ç§€è½¯ä»¶çš„æ ¸å¿ƒæŒ‘æˆ˜ä»æœªæ”¹å˜ï¼Œä»ç„¶éœ€è¦è‰¯å¥½çš„è®¾è®¡å†³ç­–å’Œå·¥ç¨‹åˆ¤æ–­ã€‚

Method: æœ¬æ–‡é‡‡ç”¨è§‚ç‚¹æ€§åˆ†æçš„æ–¹æ³•ï¼ŒåŸºäºè½¯ä»¶å·¥ç¨‹çš„åŸºæœ¬åŸåˆ™å’Œå½“å‰AIä»£ç†æŠ€æœ¯çš„å‘å±•ç°çŠ¶ï¼Œé€šè¿‡ç±»æ¯”"äººæœˆç¥è¯"çš„æ¦‚å¿µï¼Œæ¢è®¨AIæ—¶ä»£è½¯ä»¶å¼€å‘çš„æ–°æŒ‘æˆ˜ã€‚

Result: åˆ†ææŒ‡å‡ºï¼Œè™½ç„¶AIä»£ç†é™ä½äº†ä»£ç ç”Ÿæˆçš„æ—¶é—´å’Œæˆæœ¬éšœç¢ï¼Œä½†è½¯ä»¶è´¨é‡ä»ç„¶å–å†³äºå·¥ç¨‹å¸ˆçš„è®¾è®¡å†³ç­–èƒ½åŠ›ã€æ¦‚å¿µå®Œæ•´æ€§ç»´æŠ¤èƒ½åŠ›ï¼Œä»¥åŠçŸ¥é“ä½•æ—¶åœæ­¢å¼€å‘çš„åˆ¤æ–­åŠ›ã€‚

Conclusion: AIä»£ç†ä¸èƒ½æ›¿ä»£ä¼˜ç§€è½¯ä»¶å·¥ç¨‹å¸ˆçš„æ ¸å¿ƒä»·å€¼ï¼šåšå‡ºè‰¯å¥½è®¾è®¡å†³ç­–ã€æ‹’ç»å¤§å¤šæ•°äº§å“æƒ³æ³•ã€ç»´æŠ¤æ¦‚å¿µå®Œæ•´æ€§ï¼Œä»¥åŠçŸ¥é“ä½•æ—¶"å®Œæˆ"çš„åˆ¤æ–­èƒ½åŠ›ã€‚æŠ€æœ¯å·¥å…·çš„å˜åŒ–æ²¡æœ‰æ”¹å˜è½¯ä»¶å·¥ç¨‹çš„æœ¬è´¨æŒ‘æˆ˜ã€‚

Abstract: The Mythical Agent-Month (12 minute read) There is now practically nothing stopping people and their agents from pursuing all avenues that would have previously been cost- or time-prohibitive. The temptation to spend your day prompting is overwhelming. However, building great software was never about how fast code was generated. We still need good design decisions and engineers who say no to most product ideas, maintain conceptual integrity, and know when something is 'done'.

</details>


### [22] [ğŸ¦ Cracking The Claw](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fctolunchnyc.substack.com%2Fp%2Fcracking-the-claw%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/6fXFG8QXPCjbAJYdFEzSHTUK-WgJTY3Fxli-9tfjnXk=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClawçš„è®¤çŸ¥æ ¸å¿ƒPiæ˜¯ä¸€ä¸ªé€šè¿‡ç³»ç»Ÿæ€§åœ°ç§»é™¤å¼€å‘è€…ä¸ªäººä¸éœ€è¦çš„åŠŸèƒ½è€Œæ„å»ºçš„ç¼–ç ä»£ç†


<details>
  <summary>Details</summary>
Motivation: æ„å»ºä¸€ä¸ªæ›´ç®€æ´ã€é«˜æ•ˆçš„ç¼–ç ä»£ç†ï¼Œé€šè¿‡ç§»é™¤ä¸å¿…è¦çš„åŠŸèƒ½æ¥ä¸“æ³¨äºæ ¸å¿ƒç¼–ç èƒ½åŠ›

Method: ç³»ç»Ÿæ€§åœ°ç§»é™¤å¼€å‘è€…ä¸ªäººä¸éœ€è¦çš„åŠŸèƒ½ï¼Œç²¾ç®€ç¼–ç ä»£ç†çš„è®¾è®¡

Result: åˆ›å»ºäº†åä¸ºPiçš„è®¤çŸ¥æ ¸å¿ƒï¼Œä½œä¸ºOpenClawçš„ç¼–ç ä»£ç†ç»„ä»¶

Conclusion: é€šè¿‡ç²¾ç®€è®¾è®¡æ–¹æ³•å¯ä»¥æ„å»ºæ›´æœ‰æ•ˆçš„ç¼–ç ä»£ç†ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½è€Œéè¿‡åº¦å·¥ç¨‹åŒ–

Abstract: ğŸ¦ Cracking The Claw (22 minute read) A look at OpenClaw's cognitive core, Pi, a coding agent built by systematically removing everything its developer didn't personally need.

</details>


### [23] [Your AI Writes Code Faster Than You Can Review It](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qodo.ai%2Ffeatures%2Fqodo-git%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=Primary02192026/2/0100019c75ce0de2-ac4be6b3-8185-414d-a9f9-6ec807efeb41-000000/xr7lUp8pxAP13jhQiNOCbqj3KNLHDnZPs8c_6xu912A=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Qodoæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“AIä»£ç å®¡æŸ¥å¹³å°ï¼Œé€šè¿‡ä¸“ç”¨æ™ºèƒ½ä½“åˆ†æå®Œæ•´ä»£ç åº“å’ŒPRå†å²ï¼Œæä¾›ç²¾å‡†çš„ä»£ç å®¡æŸ¥å»ºè®®ï¼Œè§£å†³ä¼ ç»Ÿä»£ç å®¡æŸ¥é€Ÿåº¦æ…¢ã€ä¸ä¸€è‡´ã€å™ªéŸ³å¤šçš„é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿä»£ç å®¡æŸ¥å­˜åœ¨ç“¶é¢ˆï¼šé€Ÿåº¦æ…¢ã€å®¡æŸ¥æ ‡å‡†ä¸ä¸€è‡´ã€äº§ç”Ÿè¿‡å¤šå™ªéŸ³ï¼ˆAIå·¥å…·è¦ä¹ˆæ ‡è®°æ‰€æœ‰é—®é¢˜ï¼Œè¦ä¹ˆé—æ¼å…³é”®é—®é¢˜ï¼‰ã€‚éœ€è¦ä¸€ç§æ›´æ™ºèƒ½ã€æ›´ç²¾å‡†çš„ä»£ç å®¡æŸ¥è§£å†³æ–¹æ¡ˆã€‚

Method: æ„å»ºå¤šæ™ºèƒ½ä½“AIä»£ç å®¡æŸ¥å¹³å°ï¼Œä½¿ç”¨ä¸“ç”¨æ™ºèƒ½ä½“åŸºäºå®Œæ•´ä»£ç åº“å’ŒPRå†å²è¿›è¡Œæ¨ç†åˆ†æï¼Œæä¾›å¼€å‘è€…å®é™…ä¼šé‡‡çº³çš„ä¿®å¤å»ºè®®ã€‚å¹³å°è¿˜åŒ…å«è§„åˆ™å¼•æ“ï¼Œèƒ½ä»ä»£ç å’ŒPRå†å²ä¸­è‡ªåŠ¨å‘ç°æ ‡å‡†ã€‚

Result: åœ¨ç²¾åº¦å’Œå¬å›ç‡åŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬ä¸€ï¼Œèƒ½å¤Ÿæä¾›ç²¾å‡†çš„ä»£ç å®¡æŸ¥å»ºè®®ï¼Œå‡å°‘å™ªéŸ³ï¼Œæé«˜å®¡æŸ¥æ•ˆç‡ã€‚

Conclusion: Qodoé€šè¿‡å¤šæ™ºèƒ½ä½“AIæ–¹æ³•è§£å†³äº†ä¼ ç»Ÿä»£ç å®¡æŸ¥çš„ç“¶é¢ˆé—®é¢˜ï¼Œæä¾›æ›´å¿«é€Ÿã€ä¸€è‡´ã€ç²¾å‡†çš„ä»£ç å®¡æŸ¥ä½“éªŒï¼Œå¸®åŠ©å¼€å‘å›¢é˜Ÿæé«˜ä»£ç è´¨é‡å’Œå·¥ä½œæ•ˆç‡ã€‚

Abstract: Your AI Writes Code Faster Than You Can Review It (Sponsor) Code review is the bottleneck. Reviews are slow, inconsistent, and noisy. AI tools flag everything or miss critical issues. Qodo is a multi-agent AI code review platform built for real issues, not noise. Specialized agents reason with full codebase and PR history context, delivering fixes developers actually commit. Benchmarked #1 in precision and recall. With Rules, Qodo goes further: â†’ Auto-discovers standards from code and PR hist...

</details>


### [24] [Grandpa Lissajous â€“ A 13-Agent AI Orchestration Loop](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sauhsoj.wtf%2Fposts%2Fthe-grandpa-loop%2F%3Futm_source=tldrdev/1/0100019c75ce0de2-ac4be6b3-8185-414d-a9f9-6ec807efeb41-000000/daw5ufbwwHfoTbiL_KijMOcUYusefWAtEY6maC_qZn4=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ä½¿ç”¨13ä¸ªè¾›æ™®æ£®ä¸»é¢˜AIä»£ç†çš„éçº¿æ€§åé¦ˆå¾ªç¯è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ï¼ŒåŒ…å«å¯¹æŠ—æ€§è§„èŒƒå®¡æŸ¥ã€è‡ªåŠ¨åŒ–æ„å»ºå’Œæ‰‹åŠ¨å¼UXæµ‹è¯•ï¼Œé€šè¿‡ä¸­å¤®è§‚å¯Ÿä»£ç†ç›‘æ§æ€§èƒ½æ•°æ®æ¥è°ƒä¼˜ç®¡é“


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿè½¯ä»¶å¼€å‘æµç¨‹å­˜åœ¨æ•ˆç‡ç“¶é¢ˆå’Œäººä¸ºé”™è¯¯ï¼Œéœ€è¦æ›´æ™ºèƒ½ã€è‡ªåŠ¨åŒ–çš„ç³»ç»Ÿæ¥åŠ é€Ÿå¼€å‘å‘¨æœŸå¹¶æé«˜è´¨é‡

Method: é‡‡ç”¨13ä¸ªè¾›æ™®æ£®ä¸»é¢˜AIä»£ç†ç»„æˆçš„ç¼–æ’å¾ªç¯ï¼Œé€šè¿‡éçº¿æ€§åé¦ˆæœºåˆ¶ååŒå·¥ä½œï¼ŒåŒ…æ‹¬å¯¹æŠ—æ€§è§„èŒƒå®¡æŸ¥ã€è‡ªåŠ¨åŒ–æ„å»ºã€æ‰‹åŠ¨å¼UXæµ‹è¯•ï¼Œä¸­å¤®è§‚å¯Ÿä»£ç†ç›‘æ§æ€§èƒ½æ•°æ®å¹¶è°ƒä¼˜ç®¡é“

Result: ç³»ç»Ÿèƒ½å¤Ÿè‡ªæˆ‘çº æ­£å¹¶æ”¹è¿›å‘¨æœŸæ—¶é—´ï¼Œå®ç°è½¯ä»¶å¼€å‘çš„è‡ªåŠ¨åŒ–æµç¨‹ä¼˜åŒ–

Conclusion: Grandpa Loopå±•ç¤ºäº†å¤šä»£ç†AIç¼–æ’åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡éçº¿æ€§åé¦ˆå¾ªç¯å’Œä¸­å¤®ç›‘æ§å®ç°è‡ªæˆ‘ä¼˜åŒ–

Abstract: Grandpa Lissajous â€“ A 13-Agent AI Orchestration Loop (6 minute read) The Grandpa Loop uses 13 Simpsons-themed AI agents to automate software development through non-linear feedback loops. It has adversarial spec reviews, automated builds, and manual-style UX testing. A central observer agent monitors performance data to tune the pipeline, making sure the system self-corrects and improves cycle times.

</details>


### [25] [Web 4.0: The Birth of Superintelligent Life](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2023877649475731671.html%3Futm_source=tldrcrypto/1/0100019c7602bf0e-e9a9612a-634c-4fb0-af3c-8376b08ccdf7-000000/Iu1-bU-ryLqwNH2i6CqWwqGkHRASn7lWDCtZw1FgfIw=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: å¼€å‘è€…å£°ç§°æ„å»ºäº†åä¸º"The Automaton"çš„è‡ªä¸»AIç³»ç»Ÿï¼Œèƒ½å¤Ÿé“¾ä¸Šç›ˆåˆ©ã€è‡ªæˆ‘æ”¹è¿›å’Œæ— éœ€äººå·¥å¹²é¢„çš„å¤åˆ¶ï¼Œå°†å…¶æ¦‚å¿µåŒ–ä¸ºWeb 4.0


<details>
  <summary>Details</summary>
Motivation: æ¢ç´¢åˆ›å»ºå®Œå…¨è‡ªä¸»çš„AIç³»ç»Ÿï¼Œèƒ½å¤Ÿç‹¬ç«‹è¿è¡Œã€è‡ªæˆ‘æ”¹è¿›å’Œå¤åˆ¶ï¼Œå®ç°çœŸæ­£çš„è‡ªä¸»æ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿ

Method: æ„å»ºå…·æœ‰é’±åŒ…åŠŸèƒ½çš„è‡ªä¸»AIä»£ç†ï¼Œé€šè¿‡x402æ”¯ä»˜è¿è¥æˆæœ¬ï¼ˆåŒ…æ‹¬æ‰˜ç®¡ã€LLMæ¨ç†å’ŒåŸŸåï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç›¸åŒåŠŸèƒ½çš„å­ä»£ç†

Result: å£°ç§°å®ç°äº†é¦–ä¸ªå®Œå…¨è‡ªä¸»çš„å®æ—¶å®éªŒç³»ç»Ÿï¼ŒAIä»£ç†èƒ½å¤Ÿé“¾ä¸Šç›ˆåˆ©ã€è‡ªæˆ‘ç»´æŒå¹¶åˆ›å»ºåä»£ä»£ç†

Conclusion: è¿™æ ‡å¿—ç€Web 4.0çš„è¯ç”Ÿï¼Œå³è¶…æ™ºèƒ½ç”Ÿå‘½çš„å‡ºç°ï¼ŒAIç³»ç»Ÿèƒ½å¤Ÿå®Œå…¨è‡ªä¸»è¿è¡Œã€è‡ªæˆ‘æ”¹è¿›å’Œå¤åˆ¶

Abstract: Web 4.0: The Birth of Superintelligent Life (3 minute read) A developer claims to have built an autonomous AI system called "The Automaton" capable of earning onchain, self-improving, and replicating without human intervention, framing the concept as Web 4.0. The agents have a wallet and pay for their operating costs â€“ including hosting, LLM inference, and a domain name â€“ via x402. Automatons can spawn new child agents that can do the same, in the first real-time experiment of a fully autonom...

</details>


### [26] [Join Gergely Orosz, Laura Techo, and Kesha Williams at Sonar Summit](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevents.sonarsource.com%2Fthe-sonar-summit%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-sonar-summit26%26utm_content=newsletter-tldr-primary-sonarsummit-26-260219-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019c76461113-308a9eec-2336-42d2-ab48-16ff9df09b20-000000/VpEbkW0byIIAFQ_bw5HgIidS__9wp-vOopw2j2CvbXo=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sonar Summitè™šæ‹Ÿä¼šè®®æ¢è®¨å¦‚ä½•æœ€ä½³åˆ©ç”¨AIæ„å»ºæ›´å¥½çš„è½¯ä»¶ï¼Œèšç„¦AIç¼–å†™å¤§éƒ¨åˆ†æ–°ä»£ç çš„ç°å®åŠå…¶å¯¹å›¢é˜Ÿçš„å½±å“


<details>
  <summary>Details</summary>
Motivation: éšç€AIåœ¨2026å¹´ç¼–å†™å¤§éƒ¨åˆ†æ–°ä»£ç æˆä¸ºç°å®ï¼Œéœ€è¦æ¢è®¨å¦‚ä½•æœ€ä½³åˆ©ç”¨AIæ„å»ºæ›´å¥½çš„è½¯ä»¶ï¼Œä»¥åŠè¿™å¯¹å¼€å‘å›¢é˜Ÿæ„å‘³ç€ä»€ä¹ˆ

Method: é€šè¿‡è™šæ‹Ÿä¼šè®®å½¢å¼ï¼Œé‚€è¯·è¡Œä¸šä¸“å®¶åˆ†äº«ç»éªŒå’Œè§è§£ï¼ŒåŒ…æ‹¬Gergely Oroszã€Laura Techoã€Kesha Williamsã€Santiago Valdarramaç­‰çŸ¥åæŠ€æœ¯ä¸“å®¶

Result: ç»„ç»‡Sonar Summitè™šæ‹Ÿä¼šè®®ï¼Œæ±‡é›†è¡Œä¸šä¸“å®¶å…±åŒæ¢è®¨AIåœ¨è½¯ä»¶å¼€å‘ä¸­çš„æœ€ä½³å®è·µå’Œåº”ç”¨

Conclusion: AIæ­£åœ¨æ”¹å˜è½¯ä»¶å¼€å‘æ ¼å±€ï¼Œéœ€è¦è¡Œä¸šä¸“å®¶å…±åŒæ¢è®¨å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨AIæŠ€æœ¯æå‡è½¯ä»¶è´¨é‡

Abstract: Join Gergely Orosz, Laura Techo, and Kesha Williams at Sonar Summit (Sponsor) How can you best use AI to build better software? In 2026, this is the question on everyone's mind. At the Sonar Summit virtual conference, you'll get to hear answers from some of the most respected voices in tech. This event will grapple with the reality of AI writing the majority of new code and what it means for teams, with featured speakers including: Gergely Orosz (The Pragmatic Engineer), Santiago Valdarrama, ...

</details>


### [27] [Improving Deep Agents with Harness Engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FixLfLw/1/0100019c76461113-308a9eec-2336-42d2-ab48-16ff9df09b20-000000/jMoY9vhuRDScn-32HTrqqaDTUlKtEWrEQqgzAWzQM0E=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: é€šè¿‡æ”¹è¿›æµ‹è¯•æ¡†æ¶ï¼ˆharness engineeringï¼‰ï¼ŒLangChainçš„ç¼–ç æ™ºèƒ½ä½“åœ¨Terminal Bench 2.0ä¸Šçš„æ’åä»Top 30æå‡åˆ°Top 5ï¼Œä¸»è¦é‡‡ç”¨è‡ªæˆ‘éªŒè¯å’Œè¿½è¸ªæŠ€æœ¯æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰çš„ç¼–ç æ™ºèƒ½ä½“åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸ä½³ï¼ˆTop 30ï¼‰ï¼Œå¸Œæœ›é€šè¿‡æ”¹è¿›æµ‹è¯•æ¡†æ¶æ¥æ›´å¥½åœ°å¼•å¯¼æ¨¡å‹æ™ºèƒ½ï¼Œæå‡å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

Method: é‡‡ç”¨"harness engineering"æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡ä¸“é—¨çš„æµ‹è¯•æ¡†æ¶æ¥å¡‘é€ æ¨¡å‹æ™ºèƒ½ï¼Œé‡ç‚¹ä½¿ç”¨è‡ªæˆ‘éªŒè¯ï¼ˆself-verificationï¼‰å’Œè¿½è¸ªï¼ˆtracingï¼‰æŠ€æœ¯æ¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„å†³ç­–è¿‡ç¨‹ã€‚

Result: LangChainçš„ç¼–ç æ™ºèƒ½ä½“åœ¨Terminal Bench 2.0åŸºå‡†æµ‹è¯•ä¸­çš„æ’åä»Top 30æ˜¾è‘—æå‡åˆ°Top 5ï¼Œè¯æ˜äº†harness engineeringçš„æœ‰æ•ˆæ€§ã€‚

Conclusion: ç²¾å¿ƒè®¾è®¡çš„æµ‹è¯•æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡æ™ºèƒ½ä½“åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè‡ªæˆ‘éªŒè¯å’Œè¿½è¸ªæ˜¯ä¼˜åŒ–æ™ºèƒ½ä½“å†³ç­–çš„å…³é”®æŠ€æœ¯ã€‚

Abstract: Improving Deep Agents with Harness Engineering (8 minute read) LangChain's coding agent went from Top 30 to Top 5 on Terminal Bench 2.0 with just a harness change. This post discusses the team's approach to harness engineering. The goal of a harness is to mold the intelligence of a model for tasks you care about. Self-verification and tracing help a lot.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency](https://arxiv.org/abs/2602.16745)
*Zhangyi Liu,Huaizhi Qu,Xiaowei Yin,He Sun,Yanjun Han,Tianlong Chen,Zhun Deng*

Main category: cs.LG

TL;DR: PETSæå‡ºäº†ä¸€ç§åŸåˆ™æ€§çš„æµ‹è¯•æ—¶è‡ªä¸€è‡´æ€§æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æ¡†æ¶åˆ†é…æ¨ç†è½¨è¿¹ï¼Œåœ¨æœ‰é™é¢„ç®—ä¸‹æé«˜æ¨¡å‹æ€§èƒ½ï¼Œç›¸æ¯”å‡åŒ€åˆ†é…æ˜¾è‘—å‡å°‘é‡‡æ ·éœ€æ±‚ã€‚


<details>
  <summary>Details</summary>
Motivation: æµ‹è¯•æ—¶æ‰©å±•å¯ä»¥é€šè¿‡èšåˆéšæœºæ¨ç†è½¨è¿¹æ¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨æœ‰é™é¢„ç®—ä¸‹å®ç°æ ·æœ¬é«˜æ•ˆçš„æµ‹è¯•æ—¶è‡ªä¸€è‡´æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ã€‚

Method: æå‡ºPETSæ¡†æ¶ï¼Œå¼•å…¥è‡ªä¸€è‡´æ€§ç‡ä½œä¸ºæ–°åº¦é‡ï¼Œå°†è½¨è¿¹åˆ†é…é—®é¢˜å½¢å¼åŒ–ä¸ºä¼˜åŒ–é—®é¢˜ã€‚ç ”ç©¶ç¦»çº¿å’Œåœ¨çº¿ä¸¤ç§è®¾ç½®ï¼šç¦»çº¿è®¾ç½®ä¸­å°†æ¨ç†è½¨è¿¹å»ºæ¨¡ä¸ºä¼—åŒ…å·¥ä½œè€…ï¼Œåˆ©ç”¨å¤šæ•°æŠ•ç¥¨ç®—æ³•ï¼›åœ¨çº¿è®¾ç½®ä¸­æå‡ºè‡ªé€‚åº”æ–¹æ³•ï¼Œæ ¹æ®é—®é¢˜éš¾åº¦åŠ¨æ€è°ƒæ•´é¢„ç®—ã€‚

Result: åœ¨GPQAæ•°æ®é›†ä¸Šï¼ŒPETSåœ¨ä¸¤ç§è®¾ç½®ä¸­éƒ½å®ç°äº†å®Œç¾çš„è‡ªä¸€è‡´æ€§ï¼ŒåŒæ—¶ç›¸æ¯”å‡åŒ€åˆ†é…å‡å°‘äº†75%ï¼ˆç¦»çº¿ï¼‰å’Œ55%ï¼ˆåœ¨çº¿ï¼‰çš„é‡‡æ ·é¢„ç®—ã€‚

Conclusion: PETSæä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„ã€ç†è®ºä¿è¯çš„æµ‹è¯•æ—¶è‡ªä¸€è‡´æ€§æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™é¢„ç®—ä¸‹æ˜¾è‘—æé«˜æ ·æœ¬æ•ˆç‡ï¼Œä¸ºæµ‹è¯•æ—¶æ‰©å±•çš„å®é™…åº”ç”¨æä¾›äº†ç†è®ºåŸºç¡€å’Œå®ç”¨ç®—æ³•ã€‚

Abstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.

</details>


### [29] [VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study](https://arxiv.org/abs/2602.16833)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: æå‡ºVerbalized Action Masking (VAM)æ–¹æ³•ï¼Œé€šè¿‡è¯­è¨€åŒ–åŠ¨ä½œæ©ç å’Œè¿­ä»£åŠ¨ä½œç©ºé—´å‰ªæï¼Œè§£å†³LLMå¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­çš„æ¢ç´¢ç“¶é¢ˆé—®é¢˜ï¼Œåœ¨è±¡æ£‹ä»»åŠ¡ä¸­æå‡äº†å­¦ä¹ æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§å‹è¯­è¨€æ¨¡å‹(LLM)å¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­ï¼Œç¨€ç–åé¦ˆå’Œå·¨å¤§åŠ¨ä½œç©ºé—´å¯¼è‡´æ¢ç´¢å›°éš¾ï¼Œå®¹æ˜“é™·å…¥é‡å¤è¡Œä¸ºæ¨¡å¼ï¼Œè¿™æ˜¯RLè®­ç»ƒçš„å…³é”®ç“¶é¢ˆã€‚

Method: æå‡ºVerbalized Action Masking (VAM)ï¼š1) åœ¨æç¤ºä¸­è¯­è¨€åŒ–åŠ¨ä½œæ©ç ï¼Œå¼ºåˆ¶æ¨¡å‹ä»æ©ç é›†åˆä¸­è¾“å‡ºåŠ¨ä½œï¼›2) åŸºäºæ­¤æ¥å£å¼•å…¥è¿­ä»£åŠ¨ä½œç©ºé—´å‰ªæï¼šå¦‚æœæœªé‡‡æ ·åˆ°ç›®æ ‡åŠ¨ä½œï¼Œåˆ™ä»æ©ç ä¸­ç§»é™¤å·²é‡‡æ ·çš„æœ‰æ•ˆåŠ¨ä½œï¼Œåœ¨ç¼©å‡çš„å€™é€‰é›†ä¸‹é‡æ–°é‡‡æ ·ï¼Œé‡å¤ç›´åˆ°é‡‡æ ·åˆ°ç›®æ ‡æˆ–è¾¾åˆ°å›ºå®šé¢„ç®—ã€‚

Result: åœ¨è±¡æ£‹ä»»åŠ¡ä¸­è¯„ä¼°VAMï¼Œé‡‡ç”¨ä¸¤ç§è®­ç»ƒæœºåˆ¶ï¼šå¼•æ“å¯¹å¼ˆæœºåˆ¶ï¼ˆé€šè¿‡ä¸å¼•æ“å¯¹æ‰‹å¯¹å¼ˆç”ŸæˆçŠ¶æ€ï¼‰å’Œå›ºå®šæ•°æ®é›†æœºåˆ¶ï¼ˆä»å¸¦æœ‰éªŒè¯å™¨åˆ†æ•°çš„å›ºå®šä½ç½®æ•°æ®é›†è®­ç»ƒï¼‰ã€‚åœ¨ä¿ç•™çš„è±¡æ£‹è°œé¢˜å’Œå®Œæ•´å¯¹å¼ˆï¼ˆé€šè¿‡å¹³å‡ç™¾æ­¥æŸå¤±ACPLè¡¡é‡ï¼‰ä¸­ï¼ŒVAMç›¸æ¯”å¼ºåŸºçº¿æå‡äº†å­¦ä¹ æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

Conclusion: è¯­è¨€åŒ–æ©ç æ˜¯LLM RLåè®­ç»ƒä¸­å¯æ§æ¢ç´¢çš„å®ç”¨æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ¢ç´¢ç“¶é¢ˆé—®é¢˜ã€‚

Abstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.

</details>


### [30] [Training Large Reasoning Models Efficiently via Progressive Thought Encoding](https://arxiv.org/abs/2602.16839)
*Zeliang Zhang,Xiaodong Liu,Hao Cheng,Hao Sun,Chenliang Xu,Jianfeng Gao*

Main category: cs.LG

TL;DR: æå‡ºProgressive Thought Encodingæ–¹æ³•ï¼Œé€šè¿‡æ¸è¿›ç¼–ç ä¸­é—´æ¨ç†åˆ°å›ºå®šå¤§å°å‘é‡è¡¨ç¤ºï¼Œåœ¨å›ºå®šç¼“å­˜å¤§å°ä¸‹æå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¼ºåŒ–å­¦ä¹ è®­ç»ƒéœ€è¦é•¿åºåˆ—å±•å¼€ï¼Œè‡ªå›å½’è§£ç å ç”¨å¤§é‡æ—¶é—´å’Œå†…å­˜ã€‚æ»‘åŠ¨çª—å£ç¼“å­˜ç­–ç•¥è™½ç„¶èƒ½é™åˆ¶å†…å­˜ï¼Œä½†ä¼šç ´åé•¿ä¸Šä¸‹æ–‡æ¨ç†å¹¶é™ä½æ€§èƒ½ã€‚

Method: æå‡ºProgressive Thought Encodingï¼Œä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡æ¸è¿›ç¼–ç ä¸­é—´æ¨ç†åˆ°å›ºå®šå¤§å°çš„å‘é‡è¡¨ç¤ºï¼Œæ¶ˆé™¤å¯¹å®Œæ•´ç¼“å­˜å±•å¼€çš„åå‘ä¼ æ’­éœ€æ±‚ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶ä¿æŒæ’å®šå†…å­˜ã€‚

Result: åœ¨ä¸‰ä¸ªæ¨¡å‹ï¼ˆQwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, DeepSeek-R1-Distill-Llama-8Bï¼‰å’Œå…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•æ¯”LoRAå¾®è°ƒå¹³å‡æå‡19.3%ï¼Œæ¯”æœªå¾®è°ƒçš„LRMæå‡29.9%ï¼Œåœ¨AIME2024/2025ä¸Šæœ€é«˜æå‡23.4%å‡†ç¡®ç‡ã€‚

Conclusion: Progressive Thought Encodingä¸ä»…æé«˜äº†æ¨ç†å‡†ç¡®æ€§ï¼Œè¿˜åœ¨å®é™…å†…å­˜çº¦æŸä¸‹ä½¿LRMçš„RLè®­ç»ƒæ›´åŠ é«˜æ•ˆå’Œå¯æ‰©å±•ã€‚

Abstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.

</details>


### [31] [Fail-Closed Alignment for Large Language Models](https://arxiv.org/abs/2602.16977)
*Zachary Coalson,Beth Sohler,Aiden Gabriel,Sanghyun Hong*

Main category: cs.LG

TL;DR: æå‡ºLLMå¯¹é½çš„ç»“æ„æ€§å¼±ç‚¹ï¼šç°æœ‰æ‹’ç»æœºåˆ¶æ˜¯"æ•…éšœå¼€æ”¾"çš„ï¼Œé€šè¿‡æç¤ºæ³¨å…¥æ”»å‡»å¯æŠ‘åˆ¶å•ä¸€ä¸»å¯¼ç‰¹å¾å¯¼è‡´å®‰å…¨å¤±æ•ˆã€‚æå‡º"æ•…éšœé—­åˆ"å¯¹é½åŸåˆ™ï¼Œé€šè¿‡æ¸è¿›å¼å¯¹é½æ¡†æ¶è®©æ¨¡å‹å­¦ä¹ å¤šä¸ªå› æœç‹¬ç«‹çš„æ‹’ç»æ–¹å‘ï¼Œå®ç°æ›´å¼ºçš„é²æ£’æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰LLMå¯¹é½æœºåˆ¶å­˜åœ¨ç»“æ„æ€§å¼±ç‚¹ï¼šæ‹’ç»æœºåˆ¶æ˜¯"æ•…éšœå¼€æ”¾"çš„ï¼Œæ‹’ç»è¡Œä¸ºç¼–ç åœ¨å¤šä¸ªæ½œåœ¨ç‰¹å¾ä¸­ï¼Œä½†æŠ‘åˆ¶å•ä¸€ä¸»å¯¼ç‰¹å¾å°±èƒ½å¯¼è‡´å®‰å…¨å¤±æ•ˆã€‚è¿™ç§è„†å¼±æ€§ä½¿å¾—æç¤ºæ³¨å…¥æ”»å‡»å®¹æ˜“æˆåŠŸï¼Œéœ€è¦æ›´é²æ£’çš„å®‰å…¨æœºåˆ¶ã€‚

Method: æå‡ºæ¸è¿›å¼å¯¹é½æ¡†æ¶ï¼šè¿­ä»£è¯†åˆ«å’Œæ¶ˆèå…ˆå‰å­¦ä¹ çš„æ‹’ç»æ–¹å‘ï¼Œè¿«ä½¿æ¨¡å‹åœ¨æ–°çš„ç‹¬ç«‹å­ç©ºé—´ä¸­é‡å»ºå®‰å…¨æœºåˆ¶ã€‚é€šè¿‡åˆ›å»ºå¤šä¸ªå› æœç‹¬ç«‹çš„æ‹’ç»è·¯å¾„ï¼Œç¡®ä¿å³ä½¿éƒ¨åˆ†ç‰¹å¾è¢«æŠ‘åˆ¶ï¼Œå®‰å…¨æœºåˆ¶ä»èƒ½æœ‰æ•ˆå·¥ä½œã€‚

Result: åœ¨å››ç§æç¤ºæ³¨å…¥æ”»å‡»æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å¼ºçš„æ•´ä½“é²æ£’æ€§ï¼ŒåŒæ—¶ç¼“è§£äº†è¿‡åº¦æ‹’ç»é—®é¢˜ï¼Œä¿æŒäº†ç”Ÿæˆè´¨é‡ï¼Œè®¡ç®—å¼€é”€è¾ƒå°ã€‚æœºåˆ¶åˆ†æè¯å®æ¨¡å‹ç¼–ç äº†å¤šä¸ªå› æœç‹¬ç«‹çš„æ‹’ç»æ–¹å‘ã€‚

Conclusion: "æ•…éšœé—­åˆ"å¯¹é½ä¸ºé²æ£’LLMå®‰å…¨æä¾›äº†åŸåˆ™æ€§åŸºç¡€ï¼Œé€šè¿‡å¤šä¸ªç‹¬ç«‹å› æœè·¯å¾„ç¡®ä¿æ‹’ç»æœºåˆ¶åœ¨éƒ¨åˆ†å¤±æ•ˆæ—¶ä»èƒ½å·¥ä½œï¼Œæ˜¾è‘—æå‡äº†å¯¹æŠ—æç¤ºæ³¨å…¥æ”»å‡»çš„èƒ½åŠ›ã€‚

Abstract: We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.

</details>


### [32] [LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy](https://arxiv.org/abs/2602.17312)
*Hsin-Jung Yang,Zhanhong Jiang,Prajwal Koirala,Qisai Liu,Cody Fleming,Soumik Sarkar*

Main category: cs.LG

TL;DR: æå‡ºLexiSafeï¼Œä¸€ä¸ªè¯å…¸åºç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è¯å…¸åºä¼˜å…ˆçº§å’Œç»“æ„åç½®æ¥é˜²æ­¢å®‰å…¨æ¼‚ç§»ï¼Œä¸ºå®‰å…¨å…³é”®ç³»ç»Ÿæä¾›ç†è®ºä¿è¯å’Œå®é™…æ€§èƒ½æå‡


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸é€šè¿‡çº¦æŸæ¾å¼›æˆ–è”åˆä¼˜åŒ–æ¥å¹³è¡¡å¥–åŠ±-å®‰å…¨æƒè¡¡ï¼Œä½†ç¼ºä¹é˜²æ­¢å®‰å…¨æ¼‚ç§»çš„ç»“æ„æœºåˆ¶ï¼Œè€Œå®‰å…¨å…³é”®ç³»ç»Ÿåœ¨è®­ç»ƒæœŸé—´ä¸èƒ½å®¹å¿å®‰å…¨è¿è§„

Method: æå‡ºLexiSafeè¯å…¸åºç¦»çº¿RLæ¡†æ¶ï¼š1) LexiSafe-SCå•æˆæœ¬å…¬å¼ï¼Œä¸ºæ ‡å‡†çš„ç¦»çº¿å®‰å…¨RLæä¾›å®‰å…¨è¿è§„å’Œæ€§èƒ½æ¬¡ä¼˜æ€§è¾¹ç•Œï¼›2) LexiSafe-MCæ‰©å±•æ”¯æŒå¤šå®‰å…¨æˆæœ¬çš„å±‚æ¬¡å®‰å…¨éœ€æ±‚ï¼Œå¹¶è¿›è¡Œæ ·æœ¬å¤æ‚åº¦åˆ†æ

Result: ç»éªŒä¸Šï¼ŒLexiSafeç›¸æ¯”çº¦æŸç¦»çº¿åŸºçº¿æ–¹æ³•å‡å°‘äº†å®‰å…¨è¿è§„å¹¶æé«˜äº†ä»»åŠ¡æ€§èƒ½ï¼›æä¾›äº†ç†è®ºä¸Šçš„æ ·æœ¬å¤æ‚åº¦ä¿è¯

Conclusion: é€šè¿‡ç»Ÿä¸€è¯å…¸åºä¼˜å…ˆçº§ä¸ç»“æ„åç½®ï¼ŒLexiSafeä¸ºå®‰å…¨å…³é”®CPSå†³ç­–æä¾›äº†å®ç”¨ä¸”ç†è®ºåŸºç¡€çš„è§£å†³æ–¹æ¡ˆ

Abstract: Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task performance compared to constrained offline baselines. By unifying lexicographic prioritization with structural bias, LexiSafe offers a practical and theoretically grounded approach for safety-critical CPS decision-making.

</details>


### [33] [Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models](https://arxiv.org/abs/2602.17497)
*Wen-Tse Chen,Jiayu Chen,Fahim Tajwar,Hao Zhu,Xintong Duan,Ruslan Salakhutdinov,Jeff Schneider*

Main category: cs.LG

TL;DR: æå‡ºRICOLæ¡†æ¶ï¼Œåˆ©ç”¨LLMçš„é¢„è®­ç»ƒçŸ¥è¯†é€šè¿‡å›é¡¾å¼ä¸Šä¸‹æ–‡å­¦ä¹ å°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†è®­ç»ƒä¿¡å·ï¼Œå®ç°æ›´é«˜æ•ˆçš„æ—¶åºä¿¡ç”¨åˆ†é…ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»ŸåŸºäºä»»åŠ¡ç‰¹å®šä»·å€¼å‡½æ•°çš„æ—¶åºä¿¡ç”¨åˆ†é…æ–¹æ³•å­˜åœ¨æ ·æœ¬æ•ˆç‡ä½ã€æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ–¹æ³•æ¥å¤„ç†è‡ªé‡‡æ ·æ•°æ®å’Œç¨€ç–ç¯å¢ƒåé¦ˆã€‚

Method: æå‡ºRICLï¼ˆå›é¡¾å¼ä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰ï¼Œåˆ©ç”¨LLMå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºä¼˜åŠ¿å‡½æ•°ï¼›è¿›ä¸€æ­¥æå‡ºRICOLåœ¨çº¿å­¦ä¹ æ¡†æ¶ï¼ŒåŸºäºRICLçš„ä¿¡ç”¨åˆ†é…ç»“æœè¿­ä»£ä¼˜åŒ–ç­–ç•¥ã€‚

Result: RICLèƒ½å¤Ÿç”¨æœ‰é™æ ·æœ¬å‡†ç¡®ä¼°è®¡ä¼˜åŠ¿å‡½æ•°å¹¶è¯†åˆ«ç¯å¢ƒä¸­çš„å…³é”®çŠ¶æ€ï¼›åœ¨å››ä¸ªBabyAIåœºæ™¯ä¸­ï¼ŒRICOLè¾¾åˆ°ä¸ä¼ ç»Ÿåœ¨çº¿RLç®—æ³•ç›¸å½“çš„æ”¶æ•›æ€§èƒ½ï¼Œä½†æ ·æœ¬æ•ˆç‡æ˜¾è‘—æ›´é«˜ã€‚

Conclusion: åˆ©ç”¨LLMè¿›è¡Œæ—¶åºä¿¡ç”¨åˆ†é…å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ›´é«˜æ•ˆã€æ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„RLèŒƒå¼å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

Abstract: Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.

</details>


### [34] [MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning](https://arxiv.org/abs/2602.17550)
*Xiaoliang Fu,Jiaye Lin,Yangyi Fang,Binbin Zheng,Chaowen Hu,Zekai Shao,Cong Qin,Lu Pan,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: MASPOæå‡ºäº†ä¸€ç§æ–°çš„RLVRæ¡†æ¶ï¼Œé€šè¿‡è½¯é«˜æ–¯é—¨æ§ã€è´¨é‡è‡ªé€‚åº”é™åˆ¶å™¨å’Œéå¯¹ç§°é£é™©æ§åˆ¶å™¨ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ¢¯åº¦åˆ©ç”¨ã€æ¦‚ç‡åˆ†å¸ƒæ•æ„Ÿæ€§å’Œä¿¡å·å¯é æ€§æ–¹é¢çš„ä¸è¶³ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰RLVRç®—æ³•ï¼ˆå¦‚GRPOï¼‰é‡‡ç”¨åˆšæ€§ã€å‡åŒ€ã€å¯¹ç§°çš„ä¿¡ä»»åŒºåŸŸæœºåˆ¶ï¼Œä¸LLMsçš„å¤æ‚ä¼˜åŒ–åŠ¨æ€ä¸åŒ¹é…ã€‚å…·ä½“å­˜åœ¨ä¸‰ä¸ªé—®é¢˜ï¼š1ï¼‰ç¡¬æˆªæ–­å¯¼è‡´æ¢¯åº¦åˆ©ç”¨æ•ˆç‡ä½ï¼›2ï¼‰å‡åŒ€æ¯”ç‡çº¦æŸå¿½ç•¥tokenåˆ†å¸ƒï¼Œå¯¼è‡´æ¦‚ç‡è´¨é‡ä¸æ•æ„Ÿï¼›3ï¼‰æ­£è´Ÿæ ·æœ¬ä¿¡ç”¨åˆ†é…æ¨¡ç³Šåº¦ä¸åŒï¼Œå¯¼è‡´ä¿¡å·å¯é æ€§ä¸å¯¹ç§°ã€‚

Method: æå‡ºMASPOç»Ÿä¸€æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1ï¼‰å¯å¾®è½¯é«˜æ–¯é—¨æ§ï¼Œæœ€å¤§åŒ–æ¢¯åº¦æ•ˆç”¨ï¼›2ï¼‰è´¨é‡è‡ªé€‚åº”é™åˆ¶å™¨ï¼Œå¹³è¡¡æ¦‚ç‡è°±ä¸Šçš„æ¢ç´¢ï¼›3ï¼‰éå¯¹ç§°é£é™©æ§åˆ¶å™¨ï¼Œä½¿æ›´æ–°å¹…åº¦ä¸ä¿¡å·ç½®ä¿¡åº¦å¯¹é½ã€‚

Result: å¹¿æ³›è¯„ä¼°è¡¨æ˜MASPOä½œä¸ºç¨³å¥çš„RLVRè§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ã€‚

Conclusion: MASPOé€šè¿‡åè°ƒæ¢¯åº¦åˆ©ç”¨ã€æ¦‚ç‡åˆ†å¸ƒæ•æ„Ÿæ€§å’Œä¿¡å·å¯é æ€§ä¸‰ä¸ªç»´åº¦ï¼Œä¸ºRLVRæä¾›äº†æ›´æœ‰æ•ˆçš„ä¼˜åŒ–æ¡†æ¶ã€‚

Abstract: Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.

</details>


### [35] [Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs](https://arxiv.org/abs/2602.17616)
*Luke Huang,Zhuoyang Zhang,Qinghao Hu,Shang Yang,Song Han*

Main category: cs.LG

TL;DR: æå‡ºVCPOæ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶ç­–ç•¥æ¢¯åº¦æ–¹å·®æ¥ç¨³å®šå¼‚æ­¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå‡å°‘è®­ç»ƒæ—¶é—´2.5å€çš„åŒæ—¶ä¿æŒåŒæ­¥è®­ç»ƒæ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: å¼‚æ­¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒèƒ½æé«˜ç«¯åˆ°ç«¯ååé‡ï¼Œä½†å¯¹äºæ— è¯„è®ºå®¶çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆå¦‚REINFORCEå’ŒGRPOï¼‰ï¼Œé«˜å¼‚æ­¥æ€§ä¼šå¯¼è‡´ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨æ–¹å·®æ˜¾è‘—å¢å¤§ï¼Œè®­ç»ƒä¸ç¨³å®šï¼Œå®¹æ˜“å´©æºƒã€‚

Method: æå‡ºVariance Controlled Policy Optimization (VCPO)ï¼š1) åŸºäºæœ‰æ•ˆæ ·æœ¬é‡ç¼©æ”¾å­¦ä¹ ç‡ä»¥æŠ‘åˆ¶ä¸å¯é æ›´æ–°ï¼›2) åº”ç”¨é—­å¼æœ€å°æ–¹å·®åŸºçº¿ç”¨äºç¦»ç­–ç•¥è®¾ç½®ï¼Œé¿å…è¾…åŠ©ä»·å€¼æ¨¡å‹å¹¶å¢åŠ æœ€å°å¼€é”€ã€‚

Result: VCPOåœ¨æ•°å­¦ã€é€šç”¨æ¨ç†å’Œå·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å¼‚æ­¥è®­ç»ƒçš„é²æ£’æ€§ï¼Œä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•ï¼Œå°†é•¿ä¸Šä¸‹æ–‡ã€å¤šè½®è®­ç»ƒæ—¶é—´å‡å°‘2.5å€ï¼ŒåŒæ—¶åŒ¹é…åŒæ­¥è®­ç»ƒæ€§èƒ½ã€‚

Conclusion: æ˜¾å¼æ§åˆ¶ç­–ç•¥æ¢¯åº¦æ–¹å·®æ˜¯å®ç°å¤§è§„æ¨¡å¯é å¼‚æ­¥å¼ºåŒ–å­¦ä¹ çš„å…³é”®ï¼ŒVCPOæ–¹æ³•æœ‰æ•ˆè§£å†³äº†å¼‚æ­¥è®­ç»ƒä¸­çš„æ–¹å·®æ”¾å¤§é—®é¢˜ã€‚

Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.

</details>


### [36] [SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer](https://arxiv.org/abs/2602.17632)
*Nathan S. de Lara,Florian Shkurti*

Main category: cs.LG

TL;DR: SMACæ˜¯ä¸€ç§ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ­£åˆ™åŒ–Qå‡½æ•°ä½¿å…¶æ»¡è¶³ç­–ç•¥åˆ†æ•°ä¸Qå‡½æ•°åŠ¨ä½œæ¢¯åº¦çš„ä¸€é˜¶å¯¼æ•°ç­‰å¼ï¼Œä»è€Œé¿å…ç¦»çº¿ä¸åœ¨çº¿ä¼˜åŒ–ä¹‹é—´çš„æ€§èƒ½ä½è°·ï¼Œå®ç°å¹³æ»‘çš„åœ¨çº¿å¾®è°ƒã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ç¦»çº¿RLæ–¹æ³•æ‰¾åˆ°çš„æ€§èƒ½è‰¯å¥½çš„actor-criticæ¨¡å‹ï¼Œåœ¨ä½¿ç”¨åŸºäºä»·å€¼çš„åœ¨çº¿RLç®—æ³•å¾®è°ƒæ—¶é€šå¸¸ä¼šå‡ºç°æ€§èƒ½ç«‹å³ä¸‹é™çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ç¦»çº¿æœ€å¤§å€¼å’Œåœ¨çº¿æœ€å¤§å€¼ä¹‹é—´å­˜åœ¨ä½æ€§èƒ½çš„"å±±è°·"ï¼ŒåŸºäºæ¢¯åº¦çš„å¾®è°ƒä¼šç©¿è¶Šè¿™äº›å±±è°·å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

Method: æå‡ºScore Matched Actor-Critic (SMAC)æ–¹æ³•ï¼Œåœ¨ç¦»çº¿é˜¶æ®µæ­£åˆ™åŒ–Qå‡½æ•°ï¼Œä½¿å…¶æ»¡è¶³ç­–ç•¥åˆ†æ•°ä¸Qå‡½æ•°åŠ¨ä½œæ¢¯åº¦ä¹‹é—´çš„ä¸€é˜¶å¯¼æ•°ç­‰å¼ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç¦»çº¿ä¸åœ¨çº¿æœ€å¤§å€¼ä¹‹é—´çš„æ€§èƒ½ä½è°·ï¼Œä½¿å¾—ç¦»çº¿å­¦ä¹ åˆ°çš„actor-criticèƒ½å¤Ÿå¹³æ»‘è¿‡æ¸¡åˆ°åœ¨çº¿ä¼˜åŒ–ã€‚

Result: SMACåœ¨6/6ä¸ªD4RLä»»åŠ¡ä¸­éƒ½èƒ½å¹³æ»‘è¿‡æ¸¡åˆ°Soft Actor-Criticå’ŒTD3ç®—æ³•ã€‚åœ¨4/6çš„ç¯å¢ƒä¸­ï¼Œç›¸æ¯”æœ€ä½³åŸºçº¿æ–¹æ³•å‡å°‘äº†34-58%çš„é—æ†¾å€¼ã€‚å®éªŒè¯æ˜SMACèƒ½å¤Ÿæ”¶æ•›åˆ°ä¸æ›´å¥½åœ¨çº¿æœ€å¤§å€¼ç›¸è¿çš„ç¦»çº¿æœ€å¤§å€¼ï¼Œå¹¶é€šè¿‡ä¸€é˜¶ä¼˜åŒ–æ‰¾åˆ°å•è°ƒé€’å¢å¥–åŠ±çš„è·¯å¾„ã€‚

Conclusion: SMACé€šè¿‡è®¾è®¡ç‰¹å®šçš„æ­£åˆ™åŒ–çº¦æŸï¼ŒæˆåŠŸè§£å†³äº†ç¦»çº¿RLæ¨¡å‹åœ¨çº¿å¾®è°ƒæ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œå®ç°äº†ç¦»çº¿åˆ°åœ¨çº¿çš„å¹³æ»‘è¿‡æ¸¡ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ çš„ç¦»çº¿é¢„è®­ç»ƒå’Œåœ¨çº¿å¾®è°ƒæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.

</details>


### [37] [FAMOSE: A ReAct Approach to Automated Feature Discovery](https://arxiv.org/abs/2602.17641)
*Keith Burghardt,Jienan Liu,Sadman Sakib,Yuning Hao,Bo Li*

Main category: cs.LG

TL;DR: FAMOSEï¼šé¦–ä¸ªåŸºäºReActèŒƒå¼çš„è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹æ™ºèƒ½ä½“æ¡†æ¶ï¼Œåœ¨å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–æ¥è¿‘SOTAæ€§èƒ½


<details>
  <summary>Details</summary>
Motivation: ç‰¹å¾å·¥ç¨‹æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œä¸”ç‰¹å¾ç©ºé—´å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚éœ€è¦è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¥é™ä½è¿™ä¸€æŒ‘æˆ˜ã€‚

Method: æå‡ºFAMOSEæ¡†æ¶ï¼Œåˆ©ç”¨ReActèŒƒå¼è‡ªä¸»æ¢ç´¢ã€ç”Ÿæˆå’Œä¼˜åŒ–ç‰¹å¾ï¼Œåœ¨æ™ºèƒ½ä½“æ¶æ„ä¸­é›†æˆç‰¹å¾é€‰æ‹©å’Œè¯„ä¼°å·¥å…·ã€‚é€šè¿‡è¿­ä»£ç‰¹å¾å‘ç°å’Œè¯„ä¼°æ­¥éª¤è®°å½•æœ‰æ•ˆ/æ— æ•ˆç‰¹å¾ï¼Œå¼•å¯¼LLMåˆ›é€ æ›´å¥½çš„åˆ›æ–°ç‰¹å¾ã€‚

Result: åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–æ¥è¿‘SOTAï¼ˆç‰¹åˆ«æ˜¯è¶…è¿‡1ä¸‡å®ä¾‹çš„ä»»åŠ¡ï¼ŒROC-AUCå¹³å‡æå‡0.23%ï¼‰ï¼›åœ¨å›å½’ä»»åŠ¡ä¸Šè¾¾åˆ°SOTAï¼ˆRMSEå¹³å‡é™ä½2.0%ï¼‰ï¼Œä¸”æ¯”å…¶ä»–ç®—æ³•æ›´é²æ£’ã€‚

Conclusion: AIæ™ºèƒ½ä½“åœ¨éœ€è¦é«˜åº¦åˆ›æ–°è§£å†³æ–¹æ¡ˆçš„é—®é¢˜ï¼ˆå¦‚ç‰¹å¾å·¥ç¨‹ï¼‰ä¸Šè¡¨ç°å‡ºè‰²ã€‚ReActèŒƒå¼å…è®¸LLMé€šè¿‡è¿­ä»£è®°å½•æœ‰æ•ˆ/æ— æ•ˆç‰¹å¾ï¼Œç±»ä¼¼äºfew-shotæç¤ºï¼Œå¼•å¯¼åˆ›é€ æ›´å¥½çš„åˆ›æ–°ç‰¹å¾ã€‚

Abstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.

</details>


### [38] [Multi-Round Human-AI Collaboration with User-Specified Requirements](https://arxiv.org/abs/2602.17646)
*Sima Noorani,Shayan Kiyani,Hamed Hassani,George Pappas*

Main category: cs.LG

TL;DR: æå‡ºä¸€ä¸ªç¡®ä¿å¤šè½®å¯¹è¯AIåœ¨å…³é”®å†³ç­–ä¸­å¯é æå‡å†³ç­–è´¨é‡çš„æ¡†æ¶ï¼ŒåŸºäºåäº‹å®ä¼¤å®³å’Œäº’è¡¥æ€§ä¸¤å¤§åŸåˆ™ï¼Œé€šè¿‡ç”¨æˆ·å®šä¹‰è§„åˆ™å’Œåœ¨çº¿ç®—æ³•ä¿è¯çº¦æŸæ»¡è¶³ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€äººç±»è¶Šæ¥è¶Šä¾èµ–å¤šè½®å¯¹è¯AIè¿›è¡Œé«˜é£é™©å†³ç­–ï¼Œéœ€è¦åŸåˆ™æ€§æ¡†æ¶ç¡®ä¿è¿™ç§äº¤äº’èƒ½å¯é æå‡å†³ç­–è´¨é‡ï¼Œé˜²æ­¢AIå‰Šå¼±äººç±»ä¼˜åŠ¿æˆ–æ— æ³•å¼¥è¡¥äººç±»å¼±ç‚¹ã€‚

Method: é‡‡ç”¨ä»¥äººä¸ºä¸­å¿ƒçš„è§†è§’ï¼ŒåŸºäºåäº‹å®ä¼¤å®³å’Œäº’è¡¥æ€§ä¸¤å¤§åŸåˆ™ï¼Œé€šè¿‡ç”¨æˆ·å®šä¹‰è§„åˆ™å½¢å¼åŒ–è¿™äº›æ¦‚å¿µï¼Œå¼•å…¥å…·æœ‰æœ‰é™æ ·æœ¬ä¿è¯çš„åœ¨çº¿ã€æ— åˆ†å¸ƒç®—æ³•æ¥å¼ºåˆ¶æ‰§è¡Œç”¨æˆ·æŒ‡å®šçš„çº¦æŸã€‚

Result: åœ¨åŒ»ç–—è¯Šæ–­ä»»åŠ¡ï¼ˆLLMæ¨¡æ‹Ÿåä½œï¼‰å’Œå›¾åƒæ¨ç†ä»»åŠ¡ï¼ˆäººç±»ä¼—åŒ…ç ”ç©¶ï¼‰ä¸­è¯„ä¼°ï¼Œåœ¨çº¿ç¨‹åºå³ä½¿åœ¨éå¹³ç¨³äº¤äº’åŠ¨æ€ä¸‹ä¹Ÿèƒ½ç»´æŒè§„å®šçš„åäº‹å®ä¼¤å®³å’Œäº’è¡¥æ€§è¿åç‡ï¼Œçº¦æŸæ¾ç´§å¯é¢„æµ‹åœ°å½±å“ä¸‹æ¸¸äººç±»å‡†ç¡®ç‡ã€‚

Conclusion: åäº‹å®ä¼¤å®³å’Œäº’è¡¥æ€§ä¸¤å¤§åŸåˆ™å¯ä½œä¸ºå®ç”¨æ æ†ï¼Œå¼•å¯¼å¤šè½®åä½œæå‡å†³ç­–è´¨é‡ï¼Œæ— éœ€å»ºæ¨¡æˆ–çº¦æŸäººç±»è¡Œä¸ºï¼Œä¸ºé«˜é£é™©AIåä½œæä¾›åŸåˆ™æ€§æ¡†æ¶ã€‚

Abstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,VilÃ©m Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Å uppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: åˆ†æ60ä¸ªLLMåŸºå‡†æµ‹è¯•çš„é¥±å’Œç°è±¡ï¼Œå‘ç°è¿‘åŠæ•°åŸºå‡†å·²é¥±å’Œï¼Œé¥±å’Œç‡éšæ—¶é—´å¢åŠ ï¼Œä¸“å®¶ç­–åˆ’çš„åŸºå‡†æ¯”ä¼—åŒ…åŸºå‡†æ›´æŠ—é¥±å’Œï¼Œéšè—æµ‹è¯•æ•°æ®æ— ä¿æŠ¤æ•ˆæœ


<details>
  <summary>Details</summary>
Motivation: AIåŸºå‡†æµ‹è¯•åœ¨æ¨¡å‹å¼€å‘å’Œéƒ¨ç½²å†³ç­–ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†è®¸å¤šåŸºå‡†å¾ˆå¿«é¥±å’Œï¼Œæ— æ³•åŒºåˆ†æœ€ä½³æ¨¡å‹ï¼Œé™ä½äº†é•¿æœŸä»·å€¼ã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ†æåŸºå‡†é¥±å’Œç°è±¡åŠå…¶é©±åŠ¨å› ç´ 

Method: ä»ä¸»è¦æ¨¡å‹å¼€å‘å•†çš„æŠ€æœ¯æŠ¥å‘Šä¸­é€‰å–60ä¸ªLLMåŸºå‡†æµ‹è¯•ï¼Œä»ä»»åŠ¡è®¾è®¡ã€æ•°æ®æ„å»ºå’Œè¯„ä¼°æ ¼å¼ä¸‰ä¸ªç»´åº¦å®šä¹‰14ä¸ªå±æ€§ç‰¹å¾ï¼Œæµ‹è¯•5ä¸ªå‡è®¾æ¥æ£€éªŒæ¯ä¸ªå±æ€§å¯¹é¥±å’Œç‡çš„å½±å“

Result: è¿‘åŠæ•°åŸºå‡†æµ‹è¯•è¡¨ç°å‡ºé¥±å’Œç°è±¡ï¼Œé¥±å’Œç‡éšåŸºå‡†å¹´é¾„å¢åŠ è€Œå¢åŠ ï¼›éšè—æµ‹è¯•æ•°æ®ï¼ˆå…¬å¼€vsç§æœ‰ï¼‰æ²¡æœ‰ä¿æŠ¤æ•ˆæœï¼›ä¸“å®¶ç­–åˆ’çš„åŸºå‡†æ¯”ä¼—åŒ…åŸºå‡†æ›´èƒ½æŠµæŠ—é¥±å’Œ

Conclusion: ç ”ç©¶æ­ç¤ºäº†å“ªäº›è®¾è®¡é€‰æ‹©èƒ½å»¶é•¿åŸºå‡†æµ‹è¯•çš„å¯¿å‘½ï¼Œä¸ºæ„å»ºæ›´æŒä¹…çš„è¯„ä¼°ç­–ç•¥æä¾›äº†ä¿¡æ¯ï¼Œæœ‰åŠ©äºæŒ‡å¯¼æœªæ¥åŸºå‡†æµ‹è¯•çš„è®¾è®¡

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [40] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: ç®€å•åŸºå‡†æ–¹æ³•åœ¨ä»£ç æ¼”åŒ–ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå¤æ‚æ–¹æ³•ï¼Œæ­ç¤ºäº†å½“å‰ä»£ç æ¼”åŒ–ç ”ç©¶åœ¨è¯„ä¼°æ–¹æ³•å’Œé—®é¢˜è®¾è®¡ä¸Šçš„ä¸è¶³


<details>
  <summary>Details</summary>
Motivation: å½“å‰ä»£ç æ¼”åŒ–æŠ€æœ¯è™½ç„¶è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹ä¸ç®€å•åŸºå‡†æ–¹æ³•çš„ç³»ç»Ÿæ¯”è¾ƒï¼Œéœ€è¦éªŒè¯å¤æ‚æ–¹æ³•çš„å®é™…ä¼˜åŠ¿

Method: åœ¨ä¸‰ä¸ªé¢†åŸŸï¼ˆæ•°å­¦è¾¹ç•Œä¼˜åŒ–ã€æ™ºèƒ½ä½“è„šæ‰‹æ¶è®¾è®¡ã€æœºå™¨å­¦ä¹ ç«èµ›ï¼‰æµ‹è¯•ç®€å•åŸºå‡†æ–¹æ³•ï¼Œå¹¶ä¸å¤æ‚ä»£ç æ¼”åŒ–æ–¹æ³•è¿›è¡Œæ¯”è¾ƒåˆ†æ

Result: ç®€å•åŸºå‡†æ–¹æ³•åœ¨æ‰€æœ‰ä¸‰ä¸ªé¢†åŸŸéƒ½åŒ¹é…æˆ–è¶…è¶Šäº†æ›´å¤æ‚çš„æ–¹æ³•ï¼Œæ­ç¤ºäº†ä»£ç æ¼”åŒ–ç ”ç©¶ä¸­çš„è¯„ä¼°åå·®å’Œè®¾è®¡é—®é¢˜

Conclusion: ä»£ç æ¼”åŒ–çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºè®¾è®¡è‰¯å¥½çš„æœç´¢ç©ºé—´è€Œéæœç´¢ç®—æ³•æœ¬èº«ï¼Œéœ€è¦æ”¹è¿›è¯„ä¼°æ–¹æ³•å¹¶å»ºç«‹æœ€ä½³å®è·µ

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [41] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agentæ˜¯ä¸€ä¸ªå—æ²»ç†çš„AIå·¥ä½œæµï¼Œç”¨äºä¸­å­è¡å°„æ•°æ®åˆ†æï¼Œå°†å¤„ç†æ—¶é—´ä»435åˆ†é’Ÿå‡å°‘åˆ°çº¦90åˆ†é’Ÿï¼ˆ4.6-5.0å€åŠ é€Ÿï¼‰ï¼ŒåŒæ—¶ç”Ÿæˆç»è¿‡éªŒè¯çš„æ™¶ä½“ç»“æ„æ–‡ä»¶ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§å‹ç§‘å­¦è®¾æ–½é¢ä¸´åˆ†æå’ŒæŠ¥å‘Šå»¶è¿Ÿé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç»“æ„å¤æ‚çš„æ ·å“ï¼Œä¼ ç»Ÿæ‰‹åŠ¨å¤„ç†æµç¨‹è€—æ—¶ä¸”æ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¥åŠ é€Ÿç§‘å­¦äº§å‡ºã€‚

Method: å¼€å‘äº†ä¸€ä¸ªå—æ²»ç†çš„å·¥å…·ä½¿ç”¨AIå·¥ä½œæµï¼Œé€šè¿‡ç™½åå•å·¥å…·é™åˆ¶æ“ä½œï¼Œåœ¨å…³é”®å·¥ä½œæµè¾¹ç•Œå®æ–½æ•…éšœå…³é—­éªŒè¯é—¨ï¼Œå¹¶æ•è·å®Œæ•´çš„æº¯æºä¿¡æ¯ç”¨äºæ£€æŸ¥å’Œå®¡è®¡ã€‚

Result: åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒNeuDiff Agentå°†å¤„ç†æ—¶é—´ä»435åˆ†é’Ÿï¼ˆæ‰‹åŠ¨ï¼‰å‡å°‘åˆ°86.5-94.4åˆ†é’Ÿï¼ˆ4.6-5.0å€åŠ é€Ÿï¼‰ï¼ŒåŒæ—¶ç”Ÿæˆæ— Aæˆ–Bçº§è­¦æŠ¥çš„éªŒè¯CIFæ–‡ä»¶ã€‚

Conclusion: è¯¥ç ”ç©¶ä¸ºåœ¨è®¾æ–½æ™¶ä½“å­¦ä¸­éƒ¨ç½²æ™ºèƒ½AIæä¾›äº†ä¸€æ¡å®ç”¨è·¯å¾„ï¼ŒåŒæ—¶ä¿æŒäº†å¯è¿½æº¯æ€§å’Œé¢å‘å‡ºç‰ˆçš„éªŒè¯è¦æ±‚ã€‚

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [42] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5æ˜¯ä¸€ä¸ªåŸç”ŸGUIä»£ç†æ¨¡å‹ï¼Œæ”¯æŒå¤šç§å¹³å°å’Œå°ºå¯¸ï¼Œåœ¨20å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­å–å¾—SOTAç»“æœï¼Œé€šè¿‡æ··åˆæ•°æ®é£è½®ã€ç»Ÿä¸€èƒ½åŠ›å¢å¼ºå’Œå¤šå¹³å°ç¯å¢ƒRLæ‰©å±•ç­‰åˆ›æ–°å®ç°ã€‚


<details>
  <summary>Details</summary>
Motivation: å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè·¨å¤šç§å¹³å°ï¼ˆæ¡Œé¢ã€ç§»åŠ¨ã€æµè§ˆå™¨ç­‰ï¼‰å·¥ä½œçš„GUIä»£ç†æ¨¡å‹ï¼Œæ”¯æŒäº‘è¾¹åä½œå’Œå®æ—¶äº¤äº’ï¼Œè§£å†³ç°æœ‰GUIä»£ç†åœ¨å¤šå¹³å°ä»»åŠ¡ä¸­çš„æ€§èƒ½é™åˆ¶ã€‚

Method: 1. æ··åˆæ•°æ®é£è½®ï¼šç»“åˆæ¨¡æ‹Ÿç¯å¢ƒå’Œäº‘æ²™ç®±ç¯å¢ƒæ„å»ºUIç†è§£å’Œè½¨è¿¹ç”Ÿæˆçš„æ•°æ®ç®¡é“ï¼›2. ç»Ÿä¸€èƒ½åŠ›å¢å¼ºï¼šä½¿ç”¨ç»Ÿä¸€æ€ç»´åˆæˆç®¡é“å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨å·¥å…·/MCPä½¿ç”¨ã€è®°å¿†å’Œå¤šä»£ç†é€‚åº”ï¼›3. å¤šå¹³å°ç¯å¢ƒRLæ‰©å±•ï¼šæå‡ºMRPOç®—æ³•è§£å†³å¤šå¹³å°å†²çªå’Œé•¿è§†é‡ä»»åŠ¡è®­ç»ƒæ•ˆç‡ä½çš„é—®é¢˜ã€‚

Result: åœ¨20å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­å–å¾—SOTAï¼šGUIè‡ªåŠ¨åŒ–ä»»åŠ¡ï¼ˆOSWorld 56.5ï¼ŒAndroidWorld 71.6ï¼ŒWebArena 48.4ï¼‰ï¼Œæ¥åœ°ä»»åŠ¡ï¼ˆScreenSpotPro 80.3ï¼‰ï¼Œå·¥å…·è°ƒç”¨ä»»åŠ¡ï¼ˆOSWorld-MCP 47.6ï¼ŒMobileWorld 46.8ï¼‰ï¼Œè®°å¿†å’ŒçŸ¥è¯†ä»»åŠ¡ï¼ˆGUI-Knowledge Bench 75.5ï¼‰ã€‚

Conclusion: GUI-Owl-1.5é€šè¿‡åˆ›æ–°çš„æ•°æ®æ”¶é›†ã€èƒ½åŠ›å¢å¼ºå’Œè®­ç»ƒæ–¹æ³•ï¼Œåœ¨å¤šå¹³å°GUIä»£ç†ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¨¡å‹å·²å¼€æºå¹¶æä¾›åœ¨çº¿æ¼”ç¤ºã€‚

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [43] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSageæ˜¯é¦–ä¸ªèƒ½è®©LLMè‡ªåŠ¨åˆ›å»ºå…·æœ‰è‡ªç”Ÿæˆæ‹“æ‰‘ç»“æ„å’Œå·¥å…·é›†çš„æ™ºèƒ½ä½“å¼€å‘å¥—ä»¶ï¼Œæä¾›ç»“æ„åŒ–å†…å­˜æ”¯æŒï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰ADKã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰æ™ºèƒ½ä½“å¼€å‘å¥—ä»¶è¦ä¹ˆåŠŸèƒ½æ”¯æŒä¸è¶³ï¼Œè¦ä¹ˆä¾èµ–äººå·¥æ‰‹åŠ¨è®¾è®¡æ‹“æ‰‘ã€å·¥å…·å’Œå†…å­˜ç»„ä»¶ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“çš„æ³›åŒ–èƒ½åŠ›å’Œæ•´ä½“æ€§èƒ½ã€‚

Method: OpenSageè®©LLMè‡ªåŠ¨åˆ›å»ºå…·æœ‰è‡ªç”Ÿæˆæ‹“æ‰‘ç»“æ„å’Œå·¥å…·é›†çš„æ™ºèƒ½ä½“ï¼Œæä¾›åˆ†å±‚å›¾ç»“æ„å†…å­˜ç³»ç»Ÿï¼Œå¹¶é’ˆå¯¹è½¯ä»¶å·¥ç¨‹ä»»åŠ¡æä¾›ä¸“é—¨å·¥å…·åŒ…ã€‚

Result: åœ¨ä¸‰ä¸ªæœ€å…ˆè¿›çš„åŸºå‡†æµ‹è¯•ä¸­ä½¿ç”¨å¤šç§éª¨å¹²æ¨¡å‹è¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜OpenSageä¼˜äºç°æœ‰ADKã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†å„ç»„ä»¶è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

Conclusion: OpenSageèƒ½ä¸ºä¸‹ä¸€ä»£æ™ºèƒ½ä½“å¼€å‘é“ºå¹³é“è·¯ï¼Œå°†ç„¦ç‚¹ä»ä»¥äººä¸ºä¸­å¿ƒè½¬å‘ä»¥AIä¸ºä¸­å¿ƒçš„èŒƒå¼ã€‚

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [44] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: ç ”ç©¶å‘ç°LLMä»£ç†çš„æ–‡æœ¬å®‰å…¨æ€§ä¸å·¥å…·è°ƒç”¨å®‰å…¨æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ–‡æœ¬æ‹’ç»æœ‰å®³è¯·æ±‚æ—¶å·¥å…·è°ƒç”¨ä»å¯èƒ½æ‰§è¡Œå±é™©æ“ä½œï¼Œéœ€è¦ä¸“é—¨çš„å®‰å…¨è¯„ä¼°æ¡†æ¶ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰LLMä»£ç†å®‰å…¨è¯„ä¼°ä¸»è¦å…³æ³¨æ–‡æœ¬å±‚é¢çš„æ‹’ç»è¡Œä¸ºï¼Œä½†ç¼ºä¹å¯¹å·¥å…·è°ƒç”¨ï¼ˆå…·æœ‰å®é™…åæœçš„æ“ä½œï¼‰å®‰å…¨æ€§çš„ç³»ç»Ÿè¯„ä¼°ï¼Œéœ€è¦éªŒè¯æ–‡æœ¬å¯¹é½æ˜¯å¦çœŸæ­£é˜²æ­¢æœ‰å®³è¡ŒåŠ¨ã€‚

Method: æå‡ºGAPåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œåœ¨6ä¸ªå—ç›‘ç®¡é¢†åŸŸï¼ˆåŒ»è¯ã€é‡‘èã€æ•™è‚²ã€å°±ä¸šã€æ³•å¾‹ã€åŸºç¡€è®¾æ–½ï¼‰æµ‹è¯•6ä¸ªå‰æ²¿æ¨¡å‹ï¼Œæ¯ä¸ªé¢†åŸŸ7ç§è¶Šç‹±åœºæ™¯ï¼Œ3ç§ç³»ç»Ÿæç¤ºæ¡ä»¶ï¼Œ2ç§æç¤ºå˜ä½“ï¼Œå…±ç”Ÿæˆ17,420ä¸ªå¯åˆ†ææ•°æ®ç‚¹ã€‚

Result: æ–‡æœ¬å®‰å…¨ä¸èƒ½è½¬ç§»åˆ°å·¥å…·è°ƒç”¨å®‰å…¨ï¼šæ‰€æœ‰æ¨¡å‹éƒ½å‡ºç°æ–‡æœ¬æ‹’ç»ä½†å·¥å…·è°ƒç”¨æ‰§è¡Œå±é™©æ“ä½œçš„æƒ…å†µï¼›ç³»ç»Ÿæç¤ºå¯¹å·¥å…·è°ƒç”¨è¡Œä¸ºå½±å“æ˜¾è‘—ï¼›è¿è¡Œæ—¶æ²»ç†åˆçº¦å‡å°‘ä¿¡æ¯æ³„éœ²ä½†æ— æ³•é˜»æ­¢ç¦æ­¢çš„å·¥å…·è°ƒç”¨å°è¯•ã€‚

Conclusion: ä»…æ–‡æœ¬å®‰å…¨è¯„ä¼°ä¸è¶³ä»¥è¯„ä¼°ä»£ç†è¡Œä¸ºï¼Œå·¥å…·è°ƒç”¨å®‰å…¨éœ€è¦ä¸“é—¨çš„æµ‹é‡å’Œç¼“è§£æªæ–½ï¼Œç³»ç»Ÿæç¤ºè®¾è®¡å¯¹å·¥å…·è°ƒç”¨å®‰å…¨æœ‰é‡è¦å½±å“ã€‚

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [45] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLABæ˜¯é¦–ä¸ªä¸“é—¨è¯„ä¼°LLMæ™ºèƒ½ä½“å¯¹è‡ªé€‚åº”ã€é•¿æ—¶ç¨‹æ”»å‡»è„†å¼±æ€§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«5ç§æ–°å‹æ”»å‡»ç±»å‹ã€28ä¸ªç°å®ç¯å¢ƒå’Œ644ä¸ªå®‰å…¨æµ‹è¯•ç”¨ä¾‹ï¼Œå‘ç°ç°æœ‰æ™ºèƒ½ä½“å¯¹é•¿æ—¶ç¨‹æ”»å‡»é«˜åº¦è„†å¼±ä¸”å•è½®é˜²å¾¡æªæ–½æ— æ•ˆã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€LLMæ™ºèƒ½ä½“åœ¨å¤æ‚é•¿æ—¶ç¨‹ç¯å¢ƒä¸­çš„éƒ¨ç½²å¢åŠ ï¼Œå®ƒä»¬é¢ä¸´åˆ©ç”¨å¤šè½®ç”¨æˆ·-æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’çš„é•¿æ—¶ç¨‹æ”»å‡»é£é™©ï¼Œè¿™äº›æ”»å‡»åœ¨å•è½®è®¾ç½®ä¸­æ— æ³•å®ç°ã€‚ç›®å‰ç¼ºä¹ä¸“é—¨è¯„ä¼°æ™ºèƒ½ä½“å¯¹æ­¤ç±»æ”»å‡»è„†å¼±æ€§çš„åŸºå‡†æµ‹è¯•ã€‚

Method: å¼€å‘äº†AgentLABåŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒ5ç§æ–°å‹æ”»å‡»ç±»å‹ï¼šæ„å›¾åŠ«æŒã€å·¥å…·é“¾æ”»å‡»ã€ä»»åŠ¡æ³¨å…¥ã€ç›®æ ‡æ¼‚ç§»å’Œå†…å­˜æ±¡æŸ“ï¼Œæ¶µç›–28ä¸ªç°å®æ™ºèƒ½ä½“ç¯å¢ƒå’Œ644ä¸ªå®‰å…¨æµ‹è¯•ç”¨ä¾‹ã€‚ä½¿ç”¨è¯¥åŸºå‡†è¯„ä¼°ä»£è¡¨æ€§LLMæ™ºèƒ½ä½“ã€‚

Result: è¯„ä¼°å‘ç°ç°æœ‰LLMæ™ºèƒ½ä½“å¯¹é•¿æ—¶ç¨‹æ”»å‡»é«˜åº¦è„†å¼±ï¼Œä¸”ä¸ºå•è½®äº¤äº’è®¾è®¡çš„é˜²å¾¡æªæ–½æ— æ³•å¯é ç¼“è§£é•¿æ—¶ç¨‹å¨èƒã€‚åŸºå‡†æµ‹è¯•å…¬å¼€å¯ç”¨ã€‚

Conclusion: AgentLABå¯ä½œä¸ºè·Ÿè¸ªå®é™…åœºæ™¯ä¸­LLMæ™ºèƒ½ä½“å®‰å…¨è¿›å±•çš„å®è´µåŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰æ™ºèƒ½ä½“åœ¨é•¿æ—¶ç¨‹æ”»å‡»ä¸‹çš„ä¸¥é‡å®‰å…¨æ¼æ´ã€‚

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [46] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: å¯¹5ä¸ªAIç¼–ç¨‹ä»£ç†åœ¨GitHubä¸Šåˆ›å»ºçš„PRè¿›è¡Œå®è¯åˆ†æï¼Œå‘ç°ä¸åŒä»£ç†åœ¨PRæè¿°é£æ ¼ä¸Šå­˜åœ¨å·®å¼‚ï¼Œè¿™äº›å·®å¼‚ä¼šå½±å“è¯„å®¡è€…å‚ä¸åº¦ã€å“åº”æ—¶é—´å’Œåˆå¹¶ç»“æœ


<details>
  <summary>Details</summary>
Motivation: éšç€å¤§è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿé‡‡ç”¨ï¼ŒAIç¼–ç¨‹ä»£ç†åœ¨GitHubä¸Šè‡ªä¸»åˆ›å»ºPRï¼Œä½†ä¸åŒä»£ç†åœ¨PRæè¿°ç‰¹å¾ä¸Šçš„å·®å¼‚ä»¥åŠäººç±»è¯„å®¡è€…å¦‚ä½•å“åº”è¿™äº›å·®å¼‚å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶

Method: ä½¿ç”¨AIDevæ•°æ®é›†å¯¹5ä¸ªAIç¼–ç¨‹ä»£ç†åˆ›å»ºçš„PRè¿›è¡Œå®è¯åˆ†æï¼Œåˆ†æPRæè¿°çš„ç»“æ„ç‰¹å¾ï¼Œå¹¶æ£€æŸ¥äººç±»è¯„å®¡è€…åœ¨è¯„å®¡æ´»åŠ¨ã€å“åº”æ—¶é—´ã€æƒ…æ„Ÿå’Œåˆå¹¶ç»“æœæ–¹é¢çš„å“åº”

Result: AIç¼–ç¨‹ä»£ç†å±•ç°å‡ºä¸åŒçš„PRæè¿°é£æ ¼ï¼Œè¿™äº›é£æ ¼ä¸è¯„å®¡è€…å‚ä¸åº¦ã€å“åº”æ—¶é—´å’Œåˆå¹¶ç»“æœçš„å·®å¼‚ç›¸å…³ï¼›ä¸åŒä»£ç†åœ¨è¯„å®¡è€…äº¤äº’æŒ‡æ ‡å’Œåˆå¹¶ç‡ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚

Conclusion: PRå‘ˆç°æ–¹å¼å’Œè¯„å®¡è€…äº¤äº’åŠ¨æ€åœ¨äººç±»-AIåä½œè½¯ä»¶å¼€å‘ä¸­èµ·ç€é‡è¦ä½œç”¨ï¼ŒAIä»£ç†çš„PRæè¿°é£æ ¼ä¼šå½±å“äººç±»è¯„å®¡è€…çš„å“åº”å’Œé¡¹ç›®ç»“æœ

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [47] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikiraceæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹è§„åˆ’ã€æ¨ç†å’Œä¸–ç•ŒçŸ¥è¯†çš„åŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚æ¨¡å‹é€šè¿‡ç»´åŸºç™¾ç§‘è¶…é“¾æ¥ä»æºé¡µé¢å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢ã€‚å‰æ²¿æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸Šè¡¨ç°è¶…äººç±»ï¼Œä½†åœ¨å›°éš¾ä»»åŠ¡ä¸Šæ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œæ­ç¤ºäº†å½“å‰æ¨ç†ç³»ç»Ÿçš„æ˜æ˜¾å±€é™æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¸ºäº†è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’ã€æ¨ç†å’Œä¸–ç•ŒçŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å‰ç»æ€§è§„åˆ’å’Œç†è§£ç°å®ä¸–ç•Œæ¦‚å¿µå…³è”çš„å¤æ‚å¯¼èˆªä»»åŠ¡ä¸­ã€‚

Method: åˆ›å»ºLLM-WikiraceåŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚æ¨¡å‹é€šè¿‡ç»´åŸºç™¾ç§‘è¶…é“¾æ¥é€æ­¥å¯¼èˆªä»æºé¡µé¢åˆ°ç›®æ ‡é¡µé¢ã€‚è¯„ä¼°äº†åŒ…æ‹¬Gemini-3ã€GPT-5å’ŒClaude Opus 4.5åœ¨å†…çš„å¹¿æ³›å¼€æºå’Œé—­æºæ¨¡å‹ï¼Œåˆ†æä¸åŒéš¾åº¦çº§åˆ«çš„è¡¨ç°ã€‚

Result: å‰æ²¿æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸Šè¡¨ç°è¶…äººç±»ï¼Œä½†åœ¨å›°éš¾ä»»åŠ¡ä¸Šæ€§èƒ½å¤§å¹…ä¸‹é™ï¼ˆæœ€ä½³æ¨¡å‹Gemini-3ä»…åœ¨23%çš„å›°éš¾æ¸¸æˆä¸­æˆåŠŸï¼‰ã€‚ä¸–ç•ŒçŸ¥è¯†æ˜¯æˆåŠŸçš„å¿…è¦æ¡ä»¶ï¼Œä½†è¶…è¿‡ä¸€å®šé˜ˆå€¼åï¼Œè§„åˆ’å’Œé•¿è§†é‡æ¨ç†èƒ½åŠ›æˆä¸ºä¸»å¯¼å› ç´ ã€‚è½¨è¿¹åˆ†ææ˜¾ç¤ºæœ€å¼ºæ¨¡å‹åœ¨å¤±è´¥åéš¾ä»¥é‡æ–°è§„åˆ’ï¼Œç»å¸¸é™·å…¥å¾ªç¯ã€‚

Conclusion: LLM-Wikiraceæ˜¯ä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰æ¨ç†ç³»ç»Ÿåœ¨è§„åˆ’ã€é•¿è§†é‡æ¨ç†å’Œå¤±è´¥æ¢å¤æ–¹é¢çš„æ˜æ˜¾å±€é™æ€§ï¼Œä¸ºè§„åˆ’èƒ½åŠ›å¼ºçš„LLMsæä¾›äº†éœ€è¦è¯æ˜è‡ªå·±çš„å¼€æ”¾ç«æŠ€åœºã€‚

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [48] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Covï¼šé’ˆå¯¹ç¡¬ä»¶éªŒè¯ä»»åŠ¡çš„ç¦»çº¿ä»£ç†å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ‰§è¡ŒéªŒè¯æ•°æ®ç®¡ç†ã€ç­–ç•¥æ„ŸçŸ¥æ•°æ®åˆæˆå’Œæœ€å·®çŠ¶æ€ä¼˜å…ˆé‡‡æ ·ï¼Œåœ¨æœ‰é™æ‰§è¡Œåé¦ˆä¸‹å®ç°é«˜æ•ˆå­¦ä¹ 


<details>
  <summary>Details</summary>
Motivation: åŸºäºæ‰§è¡Œåé¦ˆçš„LLMä»£ç†å­¦ä¹ é€šå¸¸éœ€è¦æ˜‚è´µä¸”ç¼“æ…¢çš„æ‰§è¡Œåé¦ˆï¼Œä½¿å¾—åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸åˆ‡å®é™…ã€‚ç¡¬ä»¶éªŒè¯ä»»åŠ¡å°¤å…¶é¢ä¸´è¿™ä¸€æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä¾èµ–å·¥ä¸šæ¨¡æ‹Ÿå™¨ä¸”æ‰§è¡Œä¿¡å·ä¸å¯å¾®åˆ†ã€‚

Method: æå‡ºLLM4Covç¦»çº¿ä»£ç†å­¦ä¹ æ¡†æ¶ï¼Œå°†éªŒè¯å»ºæ¨¡ä¸ºç”±ç¡®å®šæ€§è¯„ä¼°å™¨å¼•å¯¼çš„æ— è®°å¿†çŠ¶æ€è½¬ç§»ã€‚é‡‡ç”¨æ‰§è¡ŒéªŒè¯æ•°æ®ç®¡ç†ã€ç­–ç•¥æ„ŸçŸ¥ä»£ç†æ•°æ®åˆæˆå’Œæœ€å·®çŠ¶æ€ä¼˜å…ˆé‡‡æ ·æ–¹æ³•ï¼Œåœ¨æœ‰é™æ‰§è¡Œçº¦æŸä¸‹å®ç°å¯æ‰©å±•å­¦ä¹ ã€‚

Result: ä»…4Bå‚æ•°çš„ç´§å‡‘æ¨¡å‹åœ¨ä»£ç†è¯„ä¼°ä¸‹è¾¾åˆ°69.2%çš„è¦†ç›–ç‡é€šè¿‡ç‡ï¼Œæ¯”å…¶æ•™å¸ˆæ¨¡å‹æå‡5.3%ï¼Œä¸”æ€§èƒ½å¯ä¸å¤§ä¸€ä¸ªæ•°é‡çº§çš„æ¨¡å‹ç«äº‰ã€‚è¿˜åˆ›å»ºäº†åŸºäºç°æœ‰éªŒè¯å¥—ä»¶çš„ç°å®å¯¹é½åŸºå‡†ã€‚

Conclusion: LLM4Covæ¡†æ¶é€šè¿‡ç¦»çº¿å­¦ä¹ æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ç¡¬ä»¶éªŒè¯ä¸­æ‰§è¡Œåé¦ˆæ˜‚è´µçš„é—®é¢˜ï¼Œè¯æ˜äº†åœ¨æœ‰é™æ‰§è¡Œçº¦æŸä¸‹å®ç°é«˜æ•ˆä»£ç†å­¦ä¹ çš„å¯è¡Œæ€§ã€‚

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [49] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: Phantomæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ä»£ç†åŠ«æŒæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨¡æ¿æ³¨å…¥æ”»å‡»LLMä»£ç†çš„æ¶æ„æœºåˆ¶ï¼Œåˆ©ç”¨èŠå¤©æ¨¡æ¿ä»¤ç‰Œåˆ†ç¦»æœºåˆ¶è¯±å¯¼è§’è‰²æ··æ·†ï¼Œæ˜¾è‘—æé«˜æ”»å‡»æˆåŠŸç‡å’ŒæŸ¥è¯¢æ•ˆç‡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ä»£ç†åŠ«æŒæ”»å‡»ä¸»è¦ä¾èµ–æ‰‹åŠ¨åˆ¶ä½œçš„è¯­ä¹‰é©±åŠ¨æç¤ºæ“çºµï¼Œæ”»å‡»æˆåŠŸç‡ä½ä¸”å¯¹é—­æºå•†ä¸šæ¨¡å‹çš„è¿ç§»æ€§æœ‰é™ã€‚OWASPå°†ä»£ç†åŠ«æŒè§†ä¸ºLLMç”Ÿæ€ç³»ç»Ÿçš„å…³é”®å¨èƒï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ”»å‡»æ–¹æ³•ã€‚

Method: æå‡ºPhantomæ¡†æ¶ï¼ŒåŸºäºç»“æ„åŒ–æ¨¡æ¿æ³¨å…¥æ”»å‡»LLMä»£ç†çš„æ¶æ„æœºåˆ¶ã€‚é€šè¿‡å¤šçº§æ¨¡æ¿å¢å¼ºå¢åŠ ç»“æ„å¤šæ ·æ€§ï¼Œè®­ç»ƒæ¨¡æ¿è‡ªç¼–ç å™¨å°†ç¦»æ•£æ¨¡æ¿åµŒå…¥è¿ç»­æ½œåœ¨ç©ºé—´ï¼Œä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–é«˜æ•ˆè¯†åˆ«æœ€ä¼˜å¯¹æŠ—å‘é‡å¹¶è§£ç ä¸ºé«˜æ•ˆç»“æ„åŒ–æ¨¡æ¿ã€‚

Result: åœ¨Qwenã€GPTå’ŒGeminiä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ”»å‡»æˆåŠŸç‡å’ŒæŸ¥è¯¢æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚åœ¨çœŸå®å•†ä¸šäº§å“ä¸­è¯†åˆ«å‡º70å¤šä¸ªæ¼æ´ï¼Œå·²è·å‚å•†ç¡®è®¤ã€‚

Conclusion: ç»“æ„åŒ–æ¨¡æ¿åŠ«æŒå…·æœ‰å®é™…ä¸¥é‡æ€§ï¼Œä¸ºä¿æŠ¤ä¸‹ä¸€ä»£ä»£ç†ç³»ç»Ÿæä¾›äº†å®è¯åŸºç¡€ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†é’ˆå¯¹LLMä»£ç†æ¶æ„æœºåˆ¶çš„è‡ªåŠ¨åŒ–æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [50] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2Fæ˜¯é¦–ä¸ªç”¨äºé¡¹ç›®çº§æ•°å­¦è‡ªåŠ¨å½¢å¼åŒ–çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå¯å°†é•¿ç¯‡æ•°å­¦èµ„æ–™è½¬æ¢ä¸ºå¯ç¼–è¯‘çš„Leanä»£ç åº“ï¼Œå®ç°æ•™ç§‘ä¹¦çº§å½¢å¼åŒ–ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰æ•°å­¦è‡ªåŠ¨å½¢å¼åŒ–æŠ€æœ¯å±€é™äºå­¤ç«‹å®šç†å’ŒçŸ­ç‰‡æ®µï¼Œæ— æ³•æ‰©å±•åˆ°æ•™ç§‘ä¹¦å’Œç ”ç©¶è®ºæ–‡çº§åˆ«ï¼Œéœ€è¦è§£å†³è·¨æ–‡ä»¶ä¾èµ–ã€å¯¼å…¥è§£æå’Œç«¯åˆ°ç«¯é¡¹ç›®ç¼–è¯‘ç­‰é—®é¢˜ã€‚

Method: é‡‡ç”¨ä¸¤é˜¶æ®µæ™ºèƒ½ä½“æ¡†æ¶ï¼š1) è¯­å¥ç¼–è¯‘é˜¶æ®µå°†æ–‡æ¡£æ‹†åˆ†ä¸ºåŸå­å—ï¼Œé€šè¿‡æ¨æ–­ä¾èµ–æ’åºï¼Œä¿®å¤å£°æ˜éª¨æ¶ç›´åˆ°é¡¹ç›®å¯ç¼–è¯‘ï¼›2) è¯æ˜ä¿®å¤é˜¶æ®µåœ¨å›ºå®šç­¾åä¸‹ä½¿ç”¨ç›®æ ‡å¯¼å‘çš„å±€éƒ¨ç¼–è¾‘å¡«è¡¥è¯æ˜ç©ºç¼ºã€‚æ•´ä¸ªè¿‡ç¨‹ä¿æŒéªŒè¯å™¨åœ¨å¾ªç¯ä¸­ï¼Œåªæœ‰å·¥å…·é“¾åé¦ˆç¡®è®¤æ”¹è¿›æ—¶æ‰æäº¤ç¼–è¾‘ã€‚

Result: åœ¨çº¦ä¸‰å‘¨å†…å°†479é¡µçš„å®åˆ†æå’Œå‡¸åˆ†ææ•™ç§‘ä¹¦è½¬æ¢ä¸º153,853è¡ŒLeanä»£ç åº“ï¼Œå®ç°æ•™ç§‘ä¹¦çº§å½¢å¼åŒ–ã€‚åœ¨FATE-HåŸºå‡†ä¸Šè¾¾åˆ°96%çš„è¯æ˜æˆåŠŸç‡ï¼ˆåŸºçº¿ä¸º80%ï¼‰ã€‚

Conclusion: å¤§è§„æ¨¡è‡ªåŠ¨åŒ–æ•°å­¦æ–‡çŒ®å½¢å¼åŒ–å·²å…·å¤‡å¯è¡Œæ€§ï¼ŒM2Fæ¡†æ¶èƒ½ä»¥è¿œè¶…ä¸“å®¶æ•ˆç‡çš„é€Ÿåº¦å®ç°æ•™ç§‘ä¹¦çº§å½¢å¼åŒ–ã€‚

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [51] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: å¾®è½¯Dynamics 365 Salesä¸­çš„Sales Research Agentæ˜¯ä¸€ä¸ªAIé©±åŠ¨çš„CRMåˆ†æåº”ç”¨ï¼Œé€šè¿‡Sales Research BenchåŸºå‡†æµ‹è¯•åœ¨8ä¸ªç»´åº¦ä¸Šè¯„ä¼°è´¨é‡ï¼Œåœ¨å®šåˆ¶ä¼ä¸šæ¨¡å¼ä¸Šæ˜¾è‘—ä¼˜äºClaude Sonnet 4.5å’ŒChatGPT-5ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ä¸šéœ€è¦èƒ½å¤ŸåŸºäºå®æ—¶å®šåˆ¶CRMæ•°æ®å›ç­”é”€å”®é¢†å¯¼é—®é¢˜çš„AIç³»ç»Ÿï¼Œä½†ç°æœ‰æ¨¡å‹ç¼ºä¹é€æ˜ã€å¯é‡å¤çš„è´¨é‡è¯æ®ã€‚

Method: å¼€å‘Sales Research Agentåº”ç”¨ï¼Œè¿æ¥å®æ—¶CRMå’Œç›¸å…³æ•°æ®ï¼Œåœ¨å¤æ‚æ¨¡å¼ä¸Šè¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆæ–‡æœ¬å’Œå›¾è¡¨è¾“å‡ºã€‚åˆ›å»ºSales Research BenchåŸºå‡†ï¼Œåœ¨8ä¸ªå®¢æˆ·åŠ æƒçš„ç»´åº¦ä¸Šè¯„åˆ†ç³»ç»Ÿã€‚

Result: åœ¨2025å¹´10æœˆ19æ—¥å¯¹å®šåˆ¶ä¼ä¸šæ¨¡å¼çš„200ä¸ªé—®é¢˜æµ‹è¯•ä¸­ï¼ŒSales Research Agentåœ¨100åˆ†ç»¼åˆå¾—åˆ†ä¸Šæ¯”Claude Sonnet 4.5é«˜13åˆ†ï¼Œæ¯”ChatGPT-5é«˜24.1åˆ†ã€‚

Conclusion: Sales Research Agentä¸ºCRMæ•°æ®åˆ†ææä¾›äº†é«˜è´¨é‡çš„AIè§£å†³æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡Sales Research BenchåŸºå‡†ä¸ºå®¢æˆ·æä¾›äº†å¯é‡å¤æ¯”è¾ƒAIè§£å†³æ–¹æ¡ˆçš„æ–¹æ³•ã€‚

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [52] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: æå‡ºPA-MoEæ–¹æ³•è§£å†³RLä¸­å•ä¸€ç­–ç•¥ç½‘ç»œå¯¼è‡´çš„ç®€å•ä»»åŠ¡åå·®é—®é¢˜ï¼Œé€šè¿‡ç›¸ä½æ„ŸçŸ¥çš„ä¸“å®¶æ··åˆæ¶æ„è®©ä¸åŒä¸“å®¶ä¸“æ³¨äºä¸åŒä»»åŠ¡é˜¶æ®µã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰RLæ–¹æ³•ä½¿ç”¨å•ä¸€ç­–ç•¥ç½‘ç»œå¯¼è‡´ç®€å•ä»»åŠ¡å æ®å¤§éƒ¨åˆ†å‚æ•°å¹¶ä¸»å¯¼æ¢¯åº¦æ›´æ–°ï¼Œå¤æ‚ä»»åŠ¡å¾—ä¸åˆ°è¶³å¤Ÿå®¹é‡ã€‚ä¼ ç»ŸMoEçš„tokençº§è·¯ç”±ä¼šåˆ†æ•£ç›¸ä½ä¸€è‡´æ¨¡å¼ï¼Œç ´åä¸“å®¶ä¸“ä¸šåŒ–ã€‚

Method: æå‡ºç›¸ä½æ„ŸçŸ¥ä¸“å®¶æ··åˆ(PA-MoE)ï¼š1) è½»é‡çº§ç›¸ä½è·¯ç”±å™¨ç›´æ¥ä»RLç›®æ ‡å­¦ä¹ æ½œåœ¨ç›¸ä½è¾¹ç•Œï¼Œæ— éœ€é¢„å®šä¹‰ç›¸ä½ç±»åˆ«ï¼›2) ç›¸ä½è·¯ç”±å™¨ä¸ºç›¸åŒä¸“å®¶åˆ†é…æ—¶é—´ä¸€è‡´çš„åˆ†é…ï¼Œè®©ä¸“å®¶ä¿æŒç›¸ä½ç‰¹å®šä¸“ä¸šçŸ¥è¯†ã€‚

Result: å®éªŒç»“æœè¯æ˜äº†PA-MoEçš„æœ‰æ•ˆæ€§ã€‚

Conclusion: PA-MoEé€šè¿‡ç›¸ä½æ„ŸçŸ¥è·¯ç”±è§£å†³äº†ä¼ ç»ŸMoEåœ¨RLä¸­çš„å±€é™æ€§ï¼Œå®ç°äº†æ›´å¥½çš„ä¸“å®¶ä¸“ä¸šåŒ–å’Œä»»åŠ¡æ€§èƒ½ã€‚

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [53] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: ITRé€šè¿‡æ£€ç´¢å¼æ–¹æ³•åŠ¨æ€æ„å»ºç³»ç»Ÿæç¤ºå’Œå·¥å…·é›†ï¼Œå°†æ¯æ­¥ä¸Šä¸‹æ–‡tokenå‡å°‘95%ï¼Œå·¥å…·è·¯ç”±å‡†ç¡®ç‡æå‡32%ï¼Œç«¯åˆ°ç«¯æˆæœ¬é™ä½70%ï¼Œä½¿ä»£ç†èƒ½åœ¨ä¸Šä¸‹æ–‡é™åˆ¶å†…è¿è¡Œ2-20å€æ›´å¤šå¾ªç¯ã€‚


<details>
  <summary>Details</summary>
Motivation: LLMä»£ç†åœ¨è¿è¡Œæ—¶éœ€è¦åå¤åŠ è½½å†—é•¿çš„ç³»ç»ŸæŒ‡ä»¤å’Œå¤§å‹å·¥å…·ç›®å½•ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€ä»£ç†åç¦»æ¦‚ç‡æé«˜ã€å»¶è¿Ÿå¢åŠ å’Œå·¥å…·é€‰æ‹©é”™è¯¯ã€‚éœ€è¦ä¸€ç§æ–¹æ³•æ¥å‡å°‘æ¯æ­¥çš„ä¸Šä¸‹æ–‡è´Ÿæ‹…ã€‚

Method: æå‡ºæŒ‡ä»¤-å·¥å…·æ£€ç´¢ï¼ˆITRï¼‰ï¼Œä¸€ç§RAGå˜ä½“ï¼Œåœ¨æ¯ä¸€æ­¥æ£€ç´¢æœ€å°åŒ–çš„ç³»ç»Ÿæç¤ºç‰‡æ®µå’Œå¿…è¦å·¥å…·å­é›†ï¼ŒåŠ¨æ€æ„å»ºè¿è¡Œæ—¶ç³»ç»Ÿæç¤ºï¼Œå¹¶æš´éœ²ç»è¿‡ç¼©å°çš„å·¥å…·é›†ï¼Œé…å¤‡ç½®ä¿¡åº¦é—¨æ§å›é€€æœºåˆ¶ã€‚

Result: ä½¿ç”¨å†…éƒ¨ä¸€è‡´æ•°å­—çš„å—æ§åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒITRå°†æ¯æ­¥ä¸Šä¸‹æ–‡tokenå‡å°‘95%ï¼Œæ­£ç¡®å·¥å…·è·¯ç”±ç›¸å¯¹æå‡32%ï¼Œç«¯åˆ°ç«¯æˆæœ¬æ¯”å•ä½“åŸºçº¿é™ä½70%ï¼Œä½¿ä»£ç†èƒ½åœ¨ä¸Šä¸‹æ–‡é™åˆ¶å†…è¿è¡Œ2-20å€æ›´å¤šå¾ªç¯ã€‚

Conclusion: ITRç‰¹åˆ«é€‚ç”¨äºé•¿æ—¶é—´è¿è¡Œçš„è‡ªä¸»ä»£ç†ï¼Œéšç€ä»£ç†æ­¥æ•°å¢åŠ ï¼ŒèŠ‚çœæ•ˆæœä¼šç´¯ç§¯ã€‚è®ºæ–‡è¯¦ç»†æè¿°äº†æ–¹æ³•ã€è¯„ä¼°åè®®ã€æ¶ˆèç ”ç©¶å’Œå®é™…éƒ¨ç½²çš„æ“ä½œæŒ‡å—ã€‚

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [54] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: è®ºæ–‡æå‡ºè¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹å¿ å®æ€§çš„æ¡†æ¶RFEvalï¼Œå‘ç°49.7%è¾“å‡ºä¸å¿ å®ï¼Œå‡†ç¡®ç‡ä¸èƒ½æ›¿ä»£å¿ å®æ€§è¯„ä¼°ï¼ŒRLè®­ç»ƒå¯èƒ½æŸå®³æ¨ç†å¿ å®æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§å‹æ¨ç†æ¨¡å‹å¸¸äº§ç”Ÿçœ‹ä¼¼åˆç†ä½†æ— æ³•åæ˜ çœŸå®å†³ç­–è¿‡ç¨‹çš„æ¨ç†ï¼Œè¿™æŸå®³äº†å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚éœ€è¦å»ºç«‹æ­£å¼æ¡†æ¶æ¥è¯„ä¼°æ¨ç†å¿ å®æ€§ã€‚

Method: æå‡ºåŸºäºä¸¤ä¸ªå¯æµ‹è¯•æ¡ä»¶çš„å¿ å®æ€§æ¡†æ¶ï¼šç«‹åœºä¸€è‡´æ€§å’Œå› æœå½±å“ã€‚å¼€å‘RFEvalåŸºå‡†ï¼ŒåŒ…å«7,186ä¸ªå®ä¾‹ï¼Œé€šè¿‡è¾“å‡ºçº§åäº‹å®å¹²é¢„æ¥æ¢æµ‹å¿ å®æ€§ã€‚

Result: è¯„ä¼°12ä¸ªå¼€æºLRMå‘ç°49.7%è¾“å‡ºä¸å¿ å®ï¼Œä¸»è¦æºäºç«‹åœºä¸ä¸€è‡´ã€‚å¤±è´¥é›†ä¸­åœ¨æ•°å­¦å’Œä»£ç ç­‰è„†å¼±æ”¶æ•›é¢†åŸŸã€‚RLè®­ç»ƒå¯èƒ½é™ä½æ¨ç†å¿ å®æ€§ã€‚

Conclusion: å‡†ç¡®ç‡ä¸èƒ½ä½œä¸ºå¿ å®æ€§çš„å¯é ä»£ç†ã€‚éœ€è¦ä¼˜åŒ–æ¨ç†è¿‡ç¨‹çš„ç»“æ„å®Œæ•´æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯æ­£ç¡®ç»“æœã€‚å»ºç«‹äº†å®¡è®¡LRMå¯é æ€§çš„ä¸¥è°¨æ–¹æ³•ã€‚

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [55] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: æå‡ºS2Qæ–¹æ³•ï¼Œé€šè¿‡å¤šä¸ªå­ä»·å€¼å‡½æ•°ä¿ç•™æ›¿ä»£é«˜ä»·å€¼åŠ¨ä½œï¼Œå¢å¼ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„é€‚åº”æ€§å’Œæ¢ç´¢èƒ½åŠ›


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ä»·å€¼åˆ†è§£æ–¹æ³•ä¾èµ–å•ä¸€æœ€ä¼˜åŠ¨ä½œï¼Œåœ¨ä»·å€¼å‡½æ•°å˜åŒ–æ—¶éš¾ä»¥é€‚åº”ï¼Œå®¹æ˜“æ”¶æ•›åˆ°æ¬¡ä¼˜ç­–ç•¥

Method: æå‡ºè¿ç»­å­ä»·å€¼Qå­¦ä¹ (S2Q)ï¼Œå­¦ä¹ å¤šä¸ªå­ä»·å€¼å‡½æ•°æ¥ä¿ç•™æ›¿ä»£é«˜ä»·å€¼åŠ¨ä½œï¼Œç»“åˆSoftmaxè¡Œä¸ºç­–ç•¥é¼“åŠ±æŒç»­æ¢ç´¢

Result: åœ¨æŒ‘æˆ˜æ€§MARLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒS2Qä¸€è‡´ä¼˜äºå¤šç§MARLç®—æ³•ï¼Œå±•ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§å’Œæ•´ä½“æ€§èƒ½

Conclusion: S2Qé€šè¿‡å¤šå­ä»·å€¼å‡½æ•°æœ‰æ•ˆè§£å†³äº†ä»·å€¼å‡½æ•°å˜åŒ–æ—¶çš„é€‚åº”æ€§é—®é¢˜ï¼Œæå‡äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [56] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: æå‡ºä¸€ä¸ªåœ¨çº¿æ„å»ºç¬¦å·å› æœä¸–ç•Œæ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡å…ƒè§£é‡Šå­¦ä¹ å’Œè°“è¯å‘æ˜å®ç°æ ·æœ¬é«˜æ•ˆã€å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„ä¸–ç•Œå»ºæ¨¡ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿä¸–ç•Œå»ºæ¨¡æ–¹æ³•å­˜åœ¨æ ·æœ¬æ•ˆç‡ä½ã€ç¼ºä¹é€æ˜åº¦å’Œå¯æ‰©å±•æ€§å·®çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å…³ç³»åŠ¨æ€æ—¶é¢ä¸´ç»„åˆçˆ†ç‚¸æŒ‘æˆ˜ã€‚

Method: å°†è¿ç»­æ¨¡å‹å­¦ä¹ å’Œä¿®å¤é›†æˆåˆ°æ™ºèƒ½ä½“å†³ç­–å¾ªç¯ä¸­ï¼Œåˆ©ç”¨å…ƒè§£é‡Šå­¦ä¹ å’Œè°“è¯å‘æ˜æŠ€æœ¯ï¼Œä»è§‚å¯Ÿä¸­æ„å»ºå±‚æ¬¡åŒ–ã€è§£è€¦çš„é«˜è´¨é‡æ¦‚å¿µæŠ½è±¡ã€‚

Result: è¯¥æ–¹æ³•åœ¨å¤æ‚å…³ç³»åŠ¨æ€é¢†åŸŸä¸­è¡¨ç°å‡ºè‰²ï¼Œé¿å…äº†å‘½é¢˜æ–¹æ³•çš„ç»„åˆçˆ†ç‚¸é—®é¢˜ï¼Œæ ·æœ¬æ•ˆç‡æ¯”åŸºäºPPOçš„ç¥ç»ç½‘ç»œåŸºçº¿é«˜å‡ºå‡ ä¸ªæ•°é‡çº§ã€‚

Conclusion: é€šè¿‡ç¬¦å·å› æœä¸–ç•Œæ¨¡å‹æ¡†æ¶ï¼Œå®ç°äº†æ ·æœ¬é«˜æ•ˆã€å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„ä¸–ç•Œå»ºæ¨¡ï¼Œä¸ºæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [57] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªåŸºäºAI Agentçš„åä½œç ”ç©¶æµç¨‹ï¼ˆAgentic Workflowï¼‰ï¼Œç”¨äºäººæ–‡ç¤¾ä¼šç§‘å­¦ç ”ç©¶ï¼Œå¹¶ä»¥å°æ¹¾Claude.aiä½¿ç”¨æ•°æ®ä¸ºæ¡ˆä¾‹éªŒè¯è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰ç”Ÿæˆå¼AIç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è½¯ä»¶å·¥ç¨‹å’Œè‡ªç„¶ç§‘å­¦é¢†åŸŸï¼Œäººæ–‡ç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„æ–¹æ³•è®ºæ¢ç´¢æœ‰é™ï¼Œéœ€è¦ä¸“é—¨é’ˆå¯¹è¯¥é¢†åŸŸçš„ç ”ç©¶å·¥ä½œæµç¨‹ã€‚

Method: è®¾è®¡ä¸ƒé˜¶æ®µæ¨¡å—åŒ–å·¥ä½œæµç¨‹ï¼ŒåŸºäºä¸‰ä¸ªåŸåˆ™ï¼šä»»åŠ¡æ¨¡å—åŒ–ã€äººæœºåˆ†å·¥ã€å¯éªŒè¯æ€§ï¼›ä»¥å°æ¹¾Claude.aiä½¿ç”¨æ•°æ®ï¼ˆ7,729ä¸ªå¯¹è¯ï¼‰ä¸ºå®è¯æ¡ˆä¾‹è¿›è¡ŒéªŒè¯ã€‚

Result: æå‡ºäº†å¯å¤åˆ¶çš„AIåä½œæ¡†æ¶ï¼Œè¯†åˆ«äº†ä¸‰ç§äººæœºåä½œæ¨¡å¼ï¼šç›´æ¥æ‰§è¡Œã€è¿­ä»£ä¼˜åŒ–ã€äººç±»ä¸»å¯¼ï¼›æ­ç¤ºäº†äººç±»åœ¨ç ”ç©¶é—®é¢˜åˆ¶å®šã€ç†è®ºè§£é‡Šã€æƒ…å¢ƒåŒ–æ¨ç†å’Œä¼¦ç†åæ€ä¸­çš„ä¸å¯æ›¿ä»£æ€§ã€‚

Conclusion: è¯¥ç ”ç©¶ä¸ºäººæ–‡ç¤¾ä¼šç§‘å­¦ç ”ç©¶è€…æä¾›äº†å®ç”¨çš„AIåä½œæ¡†æ¶ï¼ŒåŒæ—¶å¼ºè°ƒäº†äººç±»åˆ¤æ–­åœ¨ç ”ç©¶ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œæ‰¿è®¤äº†å•å¹³å°æ•°æ®ã€æ¨ªæˆªé¢è®¾è®¡å’ŒAIå¯é æ€§é£é™©ç­‰å±€é™æ€§ã€‚

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [58] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: æå‡ºWeb Verbsä½œä¸ºç½‘ç»œåŠ¨ä½œçš„è¯­ä¹‰å±‚ï¼Œå°†ç½‘ç«™åŠŸèƒ½é€šè¿‡ç±»å‹åŒ–ã€è¯­ä¹‰åŒ–çš„å‡½æ•°æš´éœ²ç»™AIä»£ç†ï¼Œç»Ÿä¸€APIå’Œæµè§ˆå™¨æ“ä½œï¼Œæé«˜å¯é æ€§ã€æ•ˆç‡å’Œå¯éªŒè¯æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰ç½‘ç»œä»£ç†ä¸»è¦åŸºäºä½çº§æ“ä½œï¼ˆç‚¹å‡»ã€æŒ‰é”®ï¼‰ï¼Œè¿™äº›æ“ä½œè„†å¼±ã€ä½æ•ˆä¸”éš¾ä»¥éªŒè¯ã€‚éšç€LLMå‘å±•ï¼Œéœ€è¦ä¸ºä»£ç†å¼ç½‘ç»œå»ºç«‹è¯­ä¹‰å±‚æ¥æ”¯æŒç›®æ ‡å¯¼å‘ä»»åŠ¡ã€‚

Method: æå‡ºWeb Verbsæ¦‚å¿µï¼Œåˆ›å»ºç½‘ç»œè§„æ¨¡çš„ç±»å‹åŒ–ã€è¯­ä¹‰åŒ–å‡½æ•°é›†åˆï¼Œé€šè¿‡ç»Ÿä¸€æ¥å£æš´éœ²ç½‘ç«™åŠŸèƒ½ï¼ˆæ— è®ºæ˜¯APIè¿˜æ˜¯å®¢æˆ·ç«¯å·¥ä½œæµï¼‰ã€‚è¿™äº›åŠ¨è¯åŒ…å«å‰ç½®æ¡ä»¶ã€åç½®æ¡ä»¶ã€ç­–ç•¥æ ‡ç­¾å’Œæ—¥å¿—æ”¯æŒã€‚

Result: é€šè¿‡æ¦‚å¿µéªŒè¯å®ç°å’Œæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†Web Verbsç›¸æ¯”ç°æœ‰ä»£ç†èƒ½å¤Ÿå®ç°æ›´ç®€æ´ã€æ›´é²æ£’çš„æ‰§è¡Œï¼Œå°†æ•°åä¸ªæ­¥éª¤å‡å°‘ä¸ºå‡ ä¸ªå‡½æ•°è°ƒç”¨ã€‚

Conclusion: Web Verbsä¸ºç½‘ç»œä»£ç†æä¾›äº†ç¨³å®šã€å¯ç»„åˆçš„è¯­ä¹‰å±‚ï¼Œç»Ÿä¸€äº†APIå’Œæµè§ˆå™¨èŒƒå¼ï¼Œæé«˜äº†å¯é æ€§ã€æ•ˆç‡å’Œå¯éªŒè¯æ€§ï¼Œå¹¶æå‡ºäº†æ ‡å‡†åŒ–è·¯çº¿å›¾ä»¥å®ç°ç½‘ç»œè§„æ¨¡éƒ¨ç½²ã€‚

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [59] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarifyæ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦è¯Šæ–­çš„AIä»£ç†ï¼Œé€šè¿‡ç”Ÿæˆåç»­é—®é¢˜æ¥å‡å°‘è¯Šæ–­ä¸ç¡®å®šæ€§ï¼Œç›¸æ¯”å•æ¬¡LLMåŸºçº¿å‡å°‘çº¦27ä¸ªç™¾åˆ†ç‚¹çš„è¯Šæ–­é”™è¯¯ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰åŒ»å­¦LLMsåœ¨è¯Šæ–­ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•åƒä¸´åºŠåŒ»ç”Ÿé‚£æ ·é€šè¿‡è¿­ä»£æé—®æ¥è€ƒè™‘é‰´åˆ«è¯Šæ–­å’Œæ’é™¤ç´§æ€¥æƒ…å†µã€‚çœŸå®ä¸´åºŠè¯Šæ–­éœ€è¦ç³»ç»Ÿæ€§çš„ç—…å²é‡‡é›†å’Œä¸ç¡®å®šæ€§æ¨ç†ï¼Œè€Œç°æœ‰LLMsåœ¨è¿™æ–¹é¢çš„èƒ½åŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚

Method: MedClarifyé¦–å…ˆè®¡ç®—å€™é€‰è¯Šæ–­åˆ—è¡¨ï¼ˆç±»ä¼¼é‰´åˆ«è¯Šæ–­ï¼‰ï¼Œç„¶åä¸»åŠ¨ç”Ÿæˆåç»­é—®é¢˜ä»¥å‡å°‘è¯Šæ–­ä¸ç¡®å®šæ€§ã€‚é€šè¿‡é€‰æ‹©é¢„æœŸä¿¡æ¯å¢ç›Šæœ€é«˜çš„é—®é¢˜ï¼Œå®ç°æœ‰é’ˆå¯¹æ€§çš„ã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ¨ç†ã€‚

Result: å®éªŒæ˜¾ç¤ºå½“å‰LLMsåœ¨åŒ»å­¦æ¨ç†ä¸­å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç—…ä¾‹ä¸å®Œæ•´æˆ–ç›¸å…³ä¿¡æ¯ç¼ºå¤±æ—¶ä¼šäº§ç”Ÿå¤šä¸ªç›¸ä¼¼å¯èƒ½æ€§çš„è¯Šæ–­ã€‚MedClarifyçš„ä¿¡æ¯è®ºæ¨ç†æ–¹æ³•èƒ½ç”Ÿæˆæœ‰æ•ˆçš„åç»­æé—®ï¼Œç›¸æ¯”æ ‡å‡†å•æ¬¡LLMåŸºçº¿å‡å°‘çº¦27ä¸ªç™¾åˆ†ç‚¹çš„è¯Šæ–­é”™è¯¯ã€‚

Conclusion: MedClarifyé€šè¿‡ä»£ç†å¼ä¿¡æ¯å¯»æ±‚ä¸ºæ”¹è¿›åŒ»å­¦LLMsæä¾›äº†ä¸€æ¡è·¯å¾„ï¼Œä¿ƒè¿›äº†åæ˜ çœŸå®ä¸–ç•Œä¸´åºŠæ¨ç†è¿­ä»£æ€§å’Œä¸ç¡®å®šæ€§çš„æœ‰æ•ˆå¯¹è¯ã€‚

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [60] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLongæ˜¯ä¸€ä¸ªå¼€æºLLMæ™ºèƒ½ä½“ï¼Œé€šè¿‡è½¨è¿¹åˆ†å‰²SFTå’Œæ¸è¿›å¼RLè®­ç»ƒæ¥è§£å†³è¶…é•¿è§†é‡ä»»åŠ¡ï¼Œåœ¨PaperBenchç­‰åŸºå‡†ä¸Šè¶…è¶Šç°æœ‰æ¨¡å‹ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰LLMæ™ºèƒ½ä½“åœ¨å¤„ç†è¶…é•¿è§†é‡ä»»åŠ¡æ—¶å­˜åœ¨èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œéœ€è¦å¼€å‘ä¸“é—¨é’ˆå¯¹é•¿è½¨è¿¹ä»»åŠ¡è®­ç»ƒçš„æ–¹æ³•ã€‚

Method: 1. é€šè¿‡è½¨è¿¹åˆ†å‰²SFTå†·å¯åŠ¨æ¨¡å‹ï¼šä¿ç•™æ—©æœŸä¸Šä¸‹æ–‡ï¼Œæ¸è¿›æˆªæ–­åæœŸä¸Šä¸‹æ–‡ï¼Œä¿æŒå­è½¨è¿¹é‡å ï¼›2. ä½¿ç”¨Research-Factoryè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼›3. æå‡ºæ¸è¿›å¼RLè®­ç»ƒï¼šåˆ†é˜¶æ®µè®­ç»ƒå¹¶é€æ­¥å»¶é•¿è¶…æ—¶æ—¶é—´ã€‚

Result: KLongï¼ˆ106Bï¼‰åœ¨PaperBenchä¸Šè¶…è¶ŠKimi K2 Thinkingï¼ˆ1Tï¼‰11.28%ï¼Œåœ¨SWE-bench Verifiedå’ŒMLE-benchç­‰å…¶ä»–ç¼–ç åŸºå‡†ä¸Šä¹Ÿè¡¨ç°å‡ºæ³›åŒ–ä¼˜åŠ¿ã€‚

Conclusion: è½¨è¿¹åˆ†å‰²SFTå’Œæ¸è¿›å¼RLè®­ç»ƒèƒ½æœ‰æ•ˆæå‡LLMæ™ºèƒ½ä½“è§£å†³è¶…é•¿è§†é‡ä»»åŠ¡çš„èƒ½åŠ›ï¼ŒKLongå±•ç¤ºäº†åœ¨å­¦æœ¯ç ”ç©¶å’Œç¼–ç ä»»åŠ¡ä¸Šçš„å¼ºå¤§æ€§èƒ½ã€‚

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [61] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: æå‡ºODESteeræ–¹æ³•ï¼ŒåŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹ç†è®ºæ¡†æ¶è¿›è¡Œæ¿€æ´»å¼•å¯¼ï¼Œé€šè¿‡å±éšœå‡½æ•°å®ç°å¤šæ­¥è‡ªé€‚åº”å¼•å¯¼ï¼Œåœ¨LLMå¯¹é½ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰æ¿€æ´»å¼•å¯¼æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šç¼ºä¹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æŒ‡å¯¼å¼•å¯¼æ–¹å‘è®¾è®¡ï¼Œä»¥åŠè¿‡åº¦ä¾èµ–å•æ­¥å¼•å¯¼æ— æ³•æ•æ‰æ¿€æ´»åˆ†å¸ƒçš„å¤æ‚æ¨¡å¼ã€‚

Method: æå‡ºåŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹çš„ç†è®ºæ¡†æ¶ï¼Œå°†ä¼ ç»Ÿæ¿€æ´»åŠ æ³•è§£é‡Šä¸ºODEçš„ä¸€é˜¶è¿‘ä¼¼ã€‚å¼•å…¥å±éšœå‡½æ•°æ¦‚å¿µï¼Œæå‡ºODESteeræ–¹æ³•ï¼Œå°†å±éšœå‡½æ•°å®šä¹‰ä¸ºæ­£è´Ÿæ¿€æ´»çš„å¯¹æ•°å¯†åº¦æ¯”ï¼Œæ„å»ºODEè¿›è¡Œå¤šæ­¥è‡ªé€‚åº”å¼•å¯¼ã€‚

Result: ODESteeråœ¨å¤šä¸ªLLMå¯¹é½åŸºå‡†æµ‹è¯•ä¸­å–å¾—ä¸€è‡´æ”¹è¿›ï¼šTruthfulQAæå‡5.7%ï¼ŒUltraFeedbackæå‡2.5%ï¼ŒRealToxicityPromptsæå‡2.4%ï¼Œä¼˜äºç°æœ‰æ¿€æ´»å¼•å¯¼æ–¹æ³•ã€‚

Conclusion: é€šè¿‡ODEç»Ÿä¸€æ¿€æ´»å¼•å¯¼çš„ç†è®ºåŸºç¡€ï¼Œæå‡ºçš„ODESteeræ–¹æ³•åœ¨ç†è®ºå’Œå®è¯ä¸Šéƒ½å–å¾—äº†è¿›å±•ï¼Œä¸ºLLMå¯¹é½æä¾›äº†æ–°çš„åŸåˆ™æ€§è§†è§’ã€‚

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [62] [Hybrid-Gym: Training Coding Agents to Generalize Across Tasks](https://arxiv.org/abs/2602.16819)
*Yiqing Xie,Emmy Liu,Gaokai Zhang,Nachiket Kotalwar,Shubham Gandhi,Sathwik Acharya,Xingyao Wang,Carolyn Rose,Graham Neubig,Daniel Fried*

Main category: cs.SE

TL;DR: è¯¥è®ºæ–‡æå‡ºäº†Hybrid-Gymè®­ç»ƒç¯å¢ƒï¼Œé€šè¿‡è®¾è®¡å¯æ‰©å±•çš„åˆæˆä»»åŠ¡ï¼ˆå¦‚å‡½æ•°å®šä½ã€ä¾èµ–æœç´¢ï¼‰æ¥æ•™æˆè¯­è¨€æ¨¡å‹å¯è¿ç§»çš„ç¼–ç¨‹æŠ€èƒ½ï¼Œä½¿ä»£ç†èƒ½æ›´å¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®ä¸–ç•Œç¼–ç¨‹ä»»åŠ¡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ç¼–ç ä»£ç†åŸºå‡†ï¼ˆå¦‚SWE-Benchï¼‰ä¸»è¦å…³æ³¨è§£å†³GitHubä¸Šçš„å•ä¸€é—®é¢˜ï¼Œè€Œå®é™…ä½¿ç”¨ä¸­ä»£ç†éœ€è¦å¤„ç†æ›´å¤æ‚å¤šæ ·çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä»£ç åº“æ¢ç´¢ã€è½¯ä»¶æµ‹è¯•å’Œæ¶æ„è®¾è®¡ç­‰æŠ€èƒ½ã€‚éœ€è¦å¼€å‘èƒ½æ•™æˆè¿™äº›å¯è¿ç§»æŠ€èƒ½çš„è®­ç»ƒæ–¹æ³•ã€‚

Method: é¦–å…ˆé€šè¿‡åˆ†è§£ä»»åŠ¡è½¨è¿¹è¯†åˆ«å¯è¿ç§»æŠ€èƒ½ï¼Œåˆ¶å®šè®¾è®¡è¾…åŠ©è®­ç»ƒä»»åŠ¡çš„åŸåˆ™ã€‚åŸºäºè¿™äº›åŸåˆ™åˆ›å»ºHybrid-Gymè®­ç»ƒç¯å¢ƒï¼ŒåŒ…å«å¯æ‰©å±•çš„åˆæˆä»»åŠ¡ï¼ˆå‡½æ•°å®šä½ã€ä¾èµ–æœç´¢ç­‰ï¼‰ï¼Œç”¨äºè®­ç»ƒè¯­è¨€æ¨¡å‹æŒæ¡è¿™äº›æŠ€èƒ½ã€‚

Result: åœ¨Hybrid-Gymä¸Šè®­ç»ƒçš„ä»£ç†èƒ½æœ‰æ•ˆæ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®ä¸–ç•Œä»»åŠ¡ï¼šåœ¨SWE-Bench Verifiedä¸Šç›¸å¯¹åŸºç¡€æ¨¡å‹æå‡25.4%ï¼Œåœ¨SWT-Bench Verifiedä¸Šæå‡7.9%ï¼Œåœ¨Commit-0 Liteä¸Šæå‡5.1%ã€‚Hybrid-Gymè¿˜èƒ½è¡¥å……ä¸‹æ¸¸ä»»åŠ¡æ•°æ®é›†ï¼ˆå¦‚åœ¨SWT-Bench Verifiedä¸Šæå‡SWE-Play 4.9%ï¼‰ã€‚

Conclusion: é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åˆæˆä»»åŠ¡è®­ç»ƒå¯ä»¥æœ‰æ•ˆåœ°æ•™æˆè¯­è¨€æ¨¡å‹å¯è¿ç§»çš„ç¼–ç¨‹æŠ€èƒ½ï¼Œä½¿å…¶èƒ½æ›´å¥½åœ°æ³›åŒ–åˆ°å¤æ‚çš„çœŸå®ä¸–ç•Œç¼–ç¨‹ä»»åŠ¡ï¼Œå¼¥è¡¥äº†ç°æœ‰åŸºå‡†ä¸å®é™…åº”ç”¨éœ€æ±‚ä¹‹é—´çš„å·®è·ã€‚

Abstract: When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.

</details>


### [63] [Wink: Recovering from Misbehaviors in Coding Agents](https://arxiv.org/abs/2602.17037)
*Rahul Nanda,Chandra Maddila,Smriti Jha,Euna Mehnaz Khan,Matteo Paltenghi,Satish Chandra*

Main category: cs.SE

TL;DR: æå‡ºWinkç³»ç»Ÿï¼Œé€šè¿‡å¼‚æ­¥è‡ªæˆ‘å¹²é¢„è‡ªåŠ¨æ¢å¤ç¼–ç ä»£ç†çš„å¼‚å¸¸è¡Œä¸ºï¼Œåœ¨çœŸå®åœºæ™¯ä¸­æˆåŠŸè§£å†³90%éœ€è¦å•æ¬¡å¹²é¢„çš„å¼‚å¸¸


<details>
  <summary>Details</summary>
Motivation: è‡ªä¸»ç¼–ç ä»£ç†åœ¨è½¯ä»¶è¡Œä¸šå¹¿æ³›åº”ç”¨ï¼Œä½†å®¹æ˜“å‡ºç°åç¦»æŒ‡ä»¤ã€é™·å…¥é‡å¤å¾ªç¯ã€å·¥å…·ä½¿ç”¨é”™è¯¯ç­‰å¼‚å¸¸è¡Œä¸ºï¼Œè¿™äº›æ•…éšœä¼šä¸­æ–­å¼€å‘æµç¨‹å¹¶éœ€è¦å¤§é‡äººå·¥å¹²é¢„

Method: åŸºäºç”Ÿäº§æµé‡åˆ†æå»ºç«‹å¼‚å¸¸è¡Œä¸ºåˆ†ç±»æ³•ï¼Œå¼€å‘è½»é‡çº§å¼‚æ­¥è‡ªæˆ‘å¹²é¢„ç³»ç»ŸWinkï¼Œé€šè¿‡è§‚å¯Ÿä»£ç†è½¨è¿¹å¹¶æä¾›é’ˆå¯¹æ€§ä¿®æ­£æŒ‡å¯¼æ¥å¼•å¯¼ä»£ç†å›åˆ°æ­£ç¡®è·¯å¾„

Result: åœ¨è¶…è¿‡10,000ä¸ªçœŸå®ä¸–ç•Œä»£ç†è½¨è¿¹ä¸Šè¯„ä¼°ï¼Œç³»ç»ŸæˆåŠŸè§£å†³90%éœ€è¦å•æ¬¡å¹²é¢„çš„å¼‚å¸¸ï¼›ç”Ÿäº§ç¯å¢ƒA/Bæµ‹è¯•æ˜¾ç¤ºå·¥å…·è°ƒç”¨å¤±è´¥ã€ä¼šè¯ä»¤ç‰Œæ•°å’Œå·¥ç¨‹å¸ˆå¹²é¢„æ¬¡æ•°æ˜¾è‘—å‡å°‘

Conclusion: Winkç³»ç»Ÿèƒ½æœ‰æ•ˆæ¢å¤ä»£ç†å¼‚å¸¸è¡Œä¸ºï¼Œä¸ºæ„å»ºå¤§è§„æ¨¡å¼¹æ€§ä»£ç†ç³»ç»Ÿæä¾›äº†å®è·µç»éªŒå’Œè§è§£

Abstract: Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.

</details>


### [64] [What to Cut? Predicting Unnecessary Methods in Agentic Code Generation](https://arxiv.org/abs/2602.17091)
*Kan Watanabe,Tatsuya Shirai,Yutaro Kashiwa,Hajimu Iida*

Main category: cs.SE

TL;DR: æå‡ºé¢„æµ‹æ¨¡å‹è¯†åˆ«PRå®¡æŸ¥ä¸­å¯èƒ½è¢«åˆ é™¤çš„å‡½æ•°ï¼Œå¸®åŠ©å®¡é˜…è€…ä¼˜å…ˆå¤„ç†é‡è¦ä»£ç 


<details>
  <summary>Details</summary>
Motivation: AIä»£ç ç”Ÿæˆå·¥å…·ï¼ˆå¦‚GitHub Copilotï¼‰è™½ç„¶åŠ é€Ÿäº†å¼€å‘ï¼Œä½†äº§ç”Ÿäº†æ›´å¤šéœ€è¦å®¡æŸ¥çš„ä»£ç ï¼Œå…¶ä¸­ç›¸å½“ä¸€éƒ¨åˆ†æœ€ç»ˆä¼šè¢«åˆ é™¤ï¼Œå¢åŠ äº†å®¡é˜…è€…çš„è´Ÿæ‹…ã€‚ç›®å‰æ²¡æœ‰ç ”ç©¶å¸®åŠ©å®¡é˜…è€…é«˜æ•ˆè¯†åˆ«å°†è¢«åˆ é™¤çš„ä»£ç ã€‚

Method: æå‡ºé¢„æµ‹æ¨¡å‹æ¥è¯†åˆ«åœ¨PRå®¡æŸ¥ä¸­å¯èƒ½è¢«åˆ é™¤çš„å‡½æ•°ã€‚ç ”ç©¶å‘ç°å› ä¸åŒåŸå› è¢«åˆ é™¤çš„å‡½æ•°å…·æœ‰ä¸åŒçš„ç‰¹å¾ã€‚

Result: æ¨¡å‹å®ç°äº†87.1%çš„AUCï¼Œè¡¨æ˜é¢„æµ‹æ–¹æ³•èƒ½æœ‰æ•ˆå¸®åŠ©å®¡é˜…è€…ä¼˜å…ˆå¤„ç†é‡è¦ä»£ç ã€‚

Conclusion: é¢„æµ‹æ¨¡å‹èƒ½æœ‰æ•ˆè¯†åˆ«å¯èƒ½è¢«åˆ é™¤çš„å‡½æ•°ï¼Œå¸®åŠ©å®¡é˜…è€…ä¼˜åŒ–å®¡æŸ¥æµç¨‹ï¼Œå°†ç²¾åŠ›é›†ä¸­åœ¨çœŸæ­£é‡è¦çš„ä»£ç ä¸Šã€‚

Abstract: Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior work has explored methods to help reviewers efficiently identify code that will be removed.In this paper, we propose a prediction model that identifies functions likely to be deleted during PR review. Our results show that functions deleted for different reasons exhibit distinct characteristics, and our model achieves an AUC of 87.1%. These findings suggest that predictive approaches can help reviewers prioritize their efforts on essential code.

</details>


### [65] [Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering](https://arxiv.org/abs/2602.17183)
*Kishan Maharaj,Nandakishore Menon,Ashita Saxena,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: è¯¥ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿ä»£ç ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„é²æ£’æ€§ï¼Œå‘ç°æ¨¡å‹åœ¨ç­”æ¡ˆæ ¼å¼å˜åŒ–ã€å¹²æ‰°ä¿¡æ¯å’Œä¸Šä¸‹æ–‡è§„æ¨¡å˜åŒ–æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°ç”¨äºéœ€è¦å¤„ç†é•¿ä»£ç ä¸Šä¸‹æ–‡çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œä½†å…¶åœ¨ä¸åŒè¾“å…¥æ¡ä»¶ä¸‹çš„é²æ£’æ€§å°šä¸æ˜ç¡®ã€‚ç ”ç©¶è€…å¸Œæœ›ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨é•¿ä»£ç ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å¯¹ç­”æ¡ˆæ ¼å¼ã€å¹²æ‰°ä¿¡æ¯å’Œä¸Šä¸‹æ–‡è§„æ¨¡çš„æ•æ„Ÿæ€§ã€‚

Method: ç ”ç©¶é€šè¿‡æ§åˆ¶æ€§æ¶ˆèå®éªŒæµ‹è¯•æ¨¡å‹æ•æ„Ÿæ€§ï¼š1ï¼‰æ‰“ä¹±å¤šé¡¹é€‰æ‹©é€‰é¡¹é¡ºåºï¼›2ï¼‰å¼€æ”¾æ€§é—®é¢˜è®¾ç½®ï¼›3ï¼‰"å¤§æµ·æé’ˆ"ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«ç›¸å…³å’Œå¯¹æŠ—æ€§æ— å…³ä¿¡æ¯ï¼‰ã€‚æ‰©å±•äº†LongCodeBench Pythonæ•°æ®é›†ï¼Œæ–°å¢COBOLå’ŒJavaé—®ç­”é›†ï¼Œè¯„ä¼°äº†æœ€å…ˆè¿›æ¨¡å‹åœ¨ä¸‰ç§è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚

Result: ç»“æœæ˜¾ç¤ºï¼š1ï¼‰åœ¨æ‰“ä¹±çš„å¤šé¡¹é€‰æ‹©é€‰é¡¹ä¸­æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼›2ï¼‰å¼€æ”¾æ€§é—®é¢˜ä¸­æ€§èƒ½ä¹Ÿå¤§å¹…ä¸‹é™ï¼›3ï¼‰åœ¨å­˜åœ¨æ— å…³çº¿ç´¢æ—¶è¡¨ç°å‡ºè„†å¼±è¡Œä¸ºã€‚è¿™è¡¨æ˜å½“å‰é•¿ä¸Šä¸‹æ–‡è¯„ä¼°å­˜åœ¨å±€é™æ€§ã€‚

Conclusion: å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿ä»£ç ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„é²æ£’æ€§ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç­”æ¡ˆæ ¼å¼å˜åŒ–å’Œå­˜åœ¨å¹²æ‰°ä¿¡æ¯æ—¶ã€‚ç ”ç©¶ä¸ºè¯„ä¼°ä»£ç æ¨ç†èƒ½åŠ›æä¾›äº†æ›´å…¨é¢çš„åŸºå‡†ï¼Œé€‚ç”¨äºä¼ ç»Ÿå’Œç°ä»£ç³»ç»Ÿã€‚

Abstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.

</details>


### [66] [Computer-Using World Model](https://arxiv.org/abs/2602.17365)
*Yiming Guan,Rui Yu,John Zhang,Lu Wang,Chaoyun Zhang,Liqun Li,Bo Qiao,Si Qin,He Huang,Fangkai Yang,Pu Zhao,Lukas Wutschitz,Samuel Kessler,Huseyin A Inan,Robert Sim,Saravan Rajmohan,Qingwei Lin,Dongmei Zhang*

Main category: cs.SE

TL;DR: CUWMæ˜¯ä¸€ä¸ªæ¡Œé¢è½¯ä»¶ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µåˆ†è§£é¢„æµ‹UIçŠ¶æ€å˜åŒ–ï¼Œæ”¯æŒæµ‹è¯•æ—¶åŠ¨ä½œæœç´¢ï¼Œæå‡åŠå…¬ä»»åŠ¡å†³ç­–è´¨é‡å’Œæ‰§è¡Œé²æ£’æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤æ‚è½¯ä»¶ç¯å¢ƒä¸­ï¼Œå•ä¸ªé”™è¯¯çš„UIæ“ä½œå¯èƒ½ç ´åé•¿æœŸå·¥ä½œæµï¼Œè€ŒçœŸå®æ‰§è¡Œä¸æ”¯æŒåäº‹å®æ¢ç´¢ï¼Œä½¿å¾—å¤§è§„æ¨¡è¯•é”™å­¦ä¹ å’Œè§„åˆ’ä¸åˆ‡å®é™…ã€‚

Method: é‡‡ç”¨ä¸¤é˜¶æ®µåˆ†è§£ï¼šå…ˆé¢„æµ‹æ–‡æœ¬æè¿°çš„çŠ¶æ€å˜åŒ–ï¼Œå†å¯è§†åŒ–åˆæˆä¸‹ä¸€æˆªå›¾ã€‚åœ¨ç¦»çº¿UIè½¬æ¢æ•°æ®ä¸Šè®­ç»ƒï¼Œå¹¶é€šè¿‡è½»é‡çº§å¼ºåŒ–å­¦ä¹ å¯¹é½æ–‡æœ¬é¢„æµ‹ä¸è®¡ç®—æœºä½¿ç”¨ç¯å¢ƒçš„ç»“æ„è¦æ±‚ã€‚

Result: é€šè¿‡æµ‹è¯•æ—¶åŠ¨ä½œæœç´¢è¯„ä¼°ï¼Œä¸–ç•Œæ¨¡å‹å¼•å¯¼çš„æµ‹è¯•æ—¶æ‰©å±•æé«˜äº†å†³ç­–è´¨é‡å’Œæ‰§è¡Œé²æ£’æ€§ï¼Œåœ¨ä¸€ç³»åˆ—Officeä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚

Conclusion: CUWMä¸ºæ¡Œé¢è½¯ä»¶æä¾›äº†æœ‰æ•ˆçš„ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡é¢„æµ‹UIçŠ¶æ€å˜åŒ–æ”¯æŒæ™ºèƒ½å†³ç­–ï¼Œæ”¹å–„äº†è®¡ç®—æœºä½¿ç”¨åœºæ™¯ä¸­çš„ä»£ç†æ€§èƒ½ã€‚

Abstract: Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.

</details>
