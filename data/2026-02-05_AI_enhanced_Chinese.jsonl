{"id": "2602.03900", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03900", "abs": "https://arxiv.org/abs/2602.03900", "authors": ["Erik Goh", "John Kos", "Ashok Goel"], "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks", "comment": null, "summary": "Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.", "AI": {"tldr": "TMK\u6846\u67b6\u63d0\u793a\u663e\u8457\u63d0\u5347LLM\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728Blocksworld\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u4ece31.5%\u63d0\u5347\u81f397.3%\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u56e0\u679c\u7ed3\u6784\u5f15\u5bfc\u6a21\u578b\u8f6c\u5411\u5f62\u5f0f\u5316\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709LLM\u63d0\u793a\u6280\u672f\uff08\u5982CoT\uff09\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6846\u67b6\u6765\u63d0\u5347\u6a21\u578b\u7684\u4efb\u52a1\u5206\u89e3\u548c\u89c4\u5212\u80fd\u529b\u3002TMK\u6846\u67b6\u56e0\u5176\u80fd\u6355\u6349\u56e0\u679c\u3001\u76ee\u7684\u6027\u548c\u5c42\u6b21\u5316\u63a8\u7406\u7ed3\u6784\uff0c\u6709\u671b\u89e3\u51b3LLM\u7684\u63a8\u7406\u7f3a\u9677\u3002", "method": "\u91c7\u7528Task-Method-Knowledge\uff08TMK\uff09\u6846\u67b6\u8fdb\u884c\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u5728PlanBench\u57fa\u51c6\u7684Blocksworld\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u6a21\u578b\u5c06\u590d\u6742\u89c4\u5212\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u5b50\u4efb\u52a1\u7684\u80fd\u529b\u3002", "result": "TMK\u63d0\u793a\u4f7f\u63a8\u7406\u6a21\u578b\u5728\u4e0d\u900f\u660e\u7684\u7b26\u53f7\u4efb\u52a1\uff08Blocksworld\u968f\u673a\u7248\u672c\uff09\u4e0a\u51c6\u786e\u7387\u4ece31.5%\u5927\u5e45\u63d0\u5347\u81f397.3%\uff0c\u663e\u793a\u51fa\u5728\u8bed\u4e49\u8fd1\u4f3c\u548c\u7b26\u53f7\u64cd\u4f5c\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u7684\u6f5c\u529b\u3002", "conclusion": "TMK\u4e0d\u4ec5\u63d0\u4f9b\u4e0a\u4e0b\u6587\uff0c\u66f4\u4f5c\u4e3a\u4e00\u79cd\u673a\u5236\u5f15\u5bfc\u63a8\u7406\u6a21\u578b\u8131\u79bb\u9ed8\u8ba4\u8bed\u8a00\u6a21\u5f0f\uff0c\u8f6c\u5411\u5f62\u5f0f\u5316\u7684\u4ee3\u7801\u6267\u884c\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.04195", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04195", "abs": "https://arxiv.org/abs/2602.04195", "authors": ["Guang Yang", "Xing Hu", "Xiang Chen", "Xin Xia"], "title": "Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation", "comment": "Under Review", "summary": "Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.", "AI": {"tldr": "\u63d0\u51faSCD\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u529f\u80fd\u9700\u6c42\u5e76\u57fa\u4e8e\u5b8c\u6574\u9700\u6c42\u548c\u529f\u80fd\u9700\u6c42\u8fdb\u884c\u5171\u8bc6\u89e3\u7801\uff0c\u5c06\u786c\u4ef6\u8bbe\u8ba1LLM\u540e\u95e8\u653b\u51fb\u6210\u529f\u7387\u4ece89%\u964d\u81f33%\u4ee5\u4e0b\u3002", "motivation": "\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684LLM\u4ee3\u7801\u751f\u6210\u6613\u53d7\u540e\u95e8\u653b\u51fb\uff0c\u4e00\u65e6\u786c\u4ef6\u5236\u9020\u5b8c\u6210\uff0c\u786c\u4ef6\u6728\u9a6c\u5c06\u65e0\u6cd5\u4fee\u590d\u3002\u73b0\u6709\u4e3b\u52a8\u9632\u5fa1\u9700\u8981\u8bad\u7ec3\u6570\u636e\uff0c\u5bf9\u7b2c\u4e09\u65b9\u7528\u6237\u4e0d\u5b9e\u7528\uff1b\u88ab\u52a8\u9632\u5fa1\u96be\u4ee5\u5e94\u5bf9\u8bed\u4e49\u9690\u853d\u7684\u89e6\u53d1\u5668\u3002\u653b\u51fb\u8005\u503e\u5411\u4e8e\u5728\u975e\u529f\u80fd\u9700\u6c42\u4e2d\u5d4c\u5165\u89e6\u53d1\u5668\u800c\u975e\u529f\u80fd\u89c4\u8303\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u5171\u8bc6\u89e3\u7801(SCD)\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u529f\u80fd\u9700\u6c42\u63d0\u53d6\uff0c\u4ece\u7528\u6237\u89c4\u8303\u4e2d\u8bc6\u522b\u5173\u952e\u529f\u80fd\u9700\u6c42\uff1b2) \u5171\u8bc6\u89e3\u7801\uff0c\u57fa\u4e8e\u5b8c\u6574\u7528\u6237\u89c4\u8303\u548c\u63d0\u53d6\u7684\u529f\u80fd\u9700\u6c42\u81ea\u9002\u5e94\u878d\u5408\u8f93\u51fa\u5206\u5e03\uff0c\u5f53\u5206\u5e03\u663e\u8457\u5206\u6b67\u65f6\u81ea\u52a8\u6291\u5236\u53ef\u7591\u7ec4\u4ef6\u3002", "result": "\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u540e\u95e8\u653b\u51fb\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSCD\u5c06\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u4ece89%\u964d\u4f4e\u52303%\u4ee5\u4e0b\uff0c\u4e14\u5bf9\u751f\u6210\u8d28\u91cf\u5f71\u54cd\u53ef\u5ffd\u7565\u3002", "conclusion": "SCD\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u88ab\u52a8\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u8bbe\u8ba1LLM\u540e\u95e8\u653b\u51fb\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "2602.03950", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03950", "abs": "https://arxiv.org/abs/2602.03950", "authors": ["Aditya Basarkar", "Benyamin Tabarsi", "Tiffany Barnes", "Dongkuan", "Xu"], "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation", "comment": "9 pages, 7 figures, submitted to ACL ARR 2026", "summary": "Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.", "AI": {"tldr": "IIPC\u662f\u4e00\u79cd\u8fed\u4ee3\u6539\u8fdb\u7684\u7a0b\u5e8f\u6784\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6267\u884c\u53cd\u9988\u548c\u94fe\u5f0f\u601d\u7ef4\uff0c\u63d0\u5347\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u7a0b\u5e8f\u5316\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7f3a\u4e4f\u53ef\u9760\u53ef\u4fee\u6b63\u7684\u63a8\u7406\u8fc7\u7a0b\u8868\u793a\uff0c\u8981\u4e48\u91c7\u7528\u50f5\u5316\u7684\u987a\u5e8f\u6d41\u7a0b\u65e0\u6cd5\u4fee\u6b63\u65e9\u671f\u6b65\u9aa4\uff0c\u8981\u4e48\u4f9d\u8d56\u542f\u53d1\u5f0f\u81ea\u8bc4\u4f30\u53ef\u80fd\u65e0\u6cd5\u8bc6\u522b\u548c\u4fee\u590d\u9519\u8bef\u3002\u6b64\u5916\uff0c\u7a0b\u5e8f\u5316\u4e0a\u4e0b\u6587\u53ef\u80fd\u5206\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5e76\u964d\u4f4e\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u8fed\u4ee3\u6539\u8fdb\u7684\u7a0b\u5e8f\u6784\u9020\uff08IIPC\uff09\u65b9\u6cd5\uff0c\u8fed\u4ee3\u5730\u7cbe\u70bc\u7a0b\u5e8f\u5316\u63a8\u7406\u94fe\uff0c\u5e76\u5c06\u6267\u884c\u53cd\u9988\u4e0e\u57fa\u7840LLM\u7684\u539f\u751f\u94fe\u5f0f\u601d\u7ef4\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u4ee5\u4fdd\u6301\u9ad8\u5c42\u6b21\u4e0a\u4e0b\u6587\u5173\u6ce8\u3002", "result": "IIPC\u5728\u591a\u4e2a\u57fa\u7840LLM\u4e0a\u7684\u5927\u591a\u6570\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "IIPC\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u7a0b\u5e8f\u6784\u9020\u548c\u7ed3\u5408\u6267\u884c\u53cd\u9988\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u63a8\u7406\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "topic": "code agent"}}
{"id": "2602.04226", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.04226", "abs": "https://arxiv.org/abs/2602.04226", "authors": ["Sota Nakashima", "Yuta Ishimoto", "Masanari Kondo", "Shane Mclntosh", "Yasutaka Kamei"], "title": "Why Agentic-PRs Get Rejected: A Comparative Study of Coding Agents", "comment": "5 pages", "summary": "Agentic coding -- software development workflows in which autonomous coding agents plan, implement, and submit code changes with minimal human involvement -- is rapidly gaining traction. Prior work has shown that Pull Requests (PRs) produced using coding agents (Agentic-PRs) are accepted less often than PRs that are not labeled as agentic (Human-PRs). The rejection reasons for a single agent (Claude Code) have been explored, but a comparison of how rejection reasons differ between Agentic-PRs generated by different agents has not yet been performed. This comparison is important since different coding agents are often used for different purposes, which can lead to agent-specific failure patterns. In this paper, we inspect 654 rejected PRs from the AIDev dataset covering five coding agents, as well as a human baseline. Our results show that seven rejection modes occur only in Agentic-PRs, including distrust of AI-generated code. We also observe agent-specific patterns (e.g., automated withdrawal of inactive PRs by Devin), reflecting differences in how agents are configured and used in practice. Notably, a large proportion of rejected PRs (67.9%) lack explicit reviewer feedback, making their rejection reasons difficult to determine. To mitigate this issue, we propose a set of heuristics that reduce the proportion of such cases, offering a practical preprocessing step for future studies of PR rejection in agentic coding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u4e0d\u540cAI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684PR\u88ab\u62d2\u7edd\u7684\u539f\u56e0\u5dee\u5f02\uff0c\u53d1\u73b0Agentic-PR\u67097\u79cd\u72ec\u7279\u7684\u62d2\u7edd\u6a21\u5f0f\uff0c\u4e14\u4e0d\u540c\u4ee3\u7406\u6709\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u542f\u53d1\u5f0f\u65b9\u6cd5\u89e3\u51b3\u7f3a\u4e4f\u660e\u786e\u53cd\u9988\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u77e5AI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684PR\u63a5\u53d7\u7387\u8f83\u4f4e\uff0c\u4f46\u4e0d\u540c\u4ee3\u7406\u4e4b\u95f4\u7684\u62d2\u7edd\u539f\u56e0\u5dee\u5f02\u5c1a\u672a\u7814\u7a76\u3002\u7531\u4e8e\u4e0d\u540c\u4ee3\u7406\u7528\u4e8e\u4e0d\u540c\u76ee\u7684\uff0c\u53ef\u80fd\u5b58\u5728\u4ee3\u7406\u7279\u5b9a\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u9700\u8981\u6bd4\u8f83\u5206\u6790\u3002", "method": "\u4eceAIDev\u6570\u636e\u96c6\u4e2d\u68c0\u67e5654\u4e2a\u88ab\u62d2\u7edd\u7684PR\uff0c\u6db5\u76d65\u4e2a\u7f16\u7801\u4ee3\u7406\u548c\u4eba\u7c7b\u57fa\u51c6\u3002\u5206\u6790\u62d2\u7edd\u539f\u56e0\uff0c\u8bc6\u522b\u4ee3\u7406\u7279\u5b9a\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u542f\u53d1\u5f0f\u65b9\u6cd5\u5904\u7406\u7f3a\u4e4f\u660e\u786e\u53cd\u9988\u7684\u60c5\u51b5\u3002", "result": "\u53d1\u73b0Agentic-PR\u67097\u79cd\u72ec\u7279\u7684\u62d2\u7edd\u6a21\u5f0f\uff08\u5305\u62ec\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u4e0d\u4fe1\u4efb\uff09\uff0c\u4e0d\u540c\u4ee3\u7406\u6709\u7279\u5b9a\u6a21\u5f0f\uff08\u5982Devin\u81ea\u52a8\u64a4\u56de\u4e0d\u6d3b\u8dc3PR\uff09\u300267.9%\u7684\u88ab\u62d2\u7eddPR\u7f3a\u4e4f\u660e\u786e\u53cd\u9988\uff0c\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6b64\u7c7b\u60c5\u51b5\u3002", "conclusion": "\u4e0d\u540c\u7f16\u7801\u4ee3\u7406\u7684\u62d2\u7edd\u539f\u56e0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u53cd\u6620\u4e86\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u914d\u7f6e\u548c\u4f7f\u7528\u65b9\u5f0f\u4e0d\u540c\u3002\u7f3a\u4e4f\u660e\u786e\u53cd\u9988\u662f\u4e3b\u8981\u6311\u6218\uff0c\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u9884\u5904\u7406\u6b65\u9aa4\u3002", "topic": "agent analysis"}}
{"id": "2602.03955", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03955", "abs": "https://arxiv.org/abs/2602.03955", "authors": ["Yinyi Luo", "Yiqiao Jin", "Weichen Yu", "Mengqi Zhang", "Srijan Kumar", "Xiaoxiao Li", "Weijie Xu", "Xin Chen", "Jindong Wang"], "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent", "comment": null, "summary": "While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.", "AI": {"tldr": "AgentArk\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u538b\u7f29\u5230\u5355\u4e2a\u6a21\u578b\u4e2d\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u83b7\u5f97\u591a\u667a\u80fd\u4f53\u7684\u63a8\u7406\u4f18\u52bf", "motivation": "\u5f53\u524dLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u901a\u8fc7\u8fed\u4ee3\u8fa9\u8bba\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u9519\u8bef\u4f20\u64ad\u7684\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faAgentArk\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cd\u5206\u5c42\u84b8\u998f\u7b56\u7565\uff1a\u63a8\u7406\u589e\u5f3a\u5fae\u8c03\u3001\u57fa\u4e8e\u8f68\u8ff9\u7684\u6570\u636e\u589e\u5f3a\u548c\u8fc7\u7a0b\u611f\u77e5\u84b8\u998f\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u52a8\u6001\u538b\u7f29\u5230\u5355\u4e2a\u6a21\u578b\u6743\u91cd\u4e2d", "result": "\u84b8\u998f\u540e\u7684\u6a21\u578b\u5728\u4fdd\u6301\u5355\u4e2a\u667a\u80fd\u4f53\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5c55\u73b0\u51fa\u591a\u667a\u80fd\u4f53\u7684\u5f3a\u5927\u63a8\u7406\u548c\u81ea\u6821\u6b63\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6837\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u5c06\u8ba1\u7b97\u8d1f\u62c5\u4ece\u63a8\u7406\u8f6c\u79fb\u5230\u8bad\u7ec3\uff0cAgentArk\u4e3a\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u6709\u671b\u63a8\u52a8\u672a\u6765\u7814\u7a76", "topic": "agent analysis"}}
{"id": "2602.04296", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04296", "abs": "https://arxiv.org/abs/2602.04296", "authors": ["Wenjun Peng", "Xinyu Wang", "Qi Wu"], "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas", "comment": "ICSE2026", "summary": "Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-generated agents within diverse, competitive game environments. Unlike existing approaches, ProxyWar evaluates not only functional correctness but also the operational characteristics of generated programs, combining automated testing, iterative code repair, and multi-agent tournaments to provide a holistic view of program behavior. Applied to a range of state-of-the-art coders and games, our approach uncovers notable discrepancies between benchmark scores and actual performance in dynamic settings, revealing overlooked limitations and opportunities for improvement. These findings highlight the need for richer, competition-based evaluation of code generation. Looking forward, ProxyWar lays a foundation for research into LLM-driven algorithm discovery, adaptive problem solving, and the study of practical efficiency and robustness, including the potential for models to outperform hand-crafted agents. The project is available at https://github.com/xinke-wang/ProxyWar.", "AI": {"tldr": "ProxyWar\u662f\u4e00\u4e2a\u901a\u8fc7\u5c06LLM\u751f\u6210\u7684\u667a\u80fd\u4f53\u5d4c\u5165\u591a\u6837\u5316\u7ade\u4e89\u6e38\u620f\u73af\u5883\u6765\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u57fa\u51c6\u5206\u6570\u4e0e\u52a8\u6001\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7801\u751f\u6210\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u548c\u7b80\u5355\u6307\u6807\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u4ee3\u7801\u5728\u5b9e\u9645\u52a8\u6001\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u6548\u679c\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u63ed\u793a\u4ee3\u7801\u7684\u64cd\u4f5c\u7279\u6027\u548c\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5c06LLM\u751f\u6210\u7684\u667a\u80fd\u4f53\u5d4c\u5165\u591a\u6837\u5316\u7ade\u4e89\u6e38\u620f\u73af\u5883\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u6d4b\u8bd5\u3001\u8fed\u4ee3\u4ee3\u7801\u4fee\u590d\u548c\u591a\u667a\u80fd\u4f53\u9526\u6807\u8d5b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u64cd\u4f5c\u7279\u6027\u3002", "result": "\u5e94\u7528ProxyWar\u8bc4\u4f30\u591a\u4e2a\u5148\u8fdb\u4ee3\u7801\u751f\u6210\u6a21\u578b\u548c\u6e38\u620f\uff0c\u53d1\u73b0\u57fa\u51c6\u5206\u6570\u4e0e\u52a8\u6001\u73af\u5883\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u7565\u7684\u5c40\u9650\u6027\u548c\u6539\u8fdb\u673a\u4f1a\u3002", "conclusion": "\u9700\u8981\u57fa\u4e8e\u7ade\u4e89\u7684\u66f4\u4e30\u5bcc\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\uff0cProxyWar\u4e3aLLM\u9a71\u52a8\u7684\u7b97\u6cd5\u53d1\u73b0\u3001\u81ea\u9002\u5e94\u95ee\u9898\u89e3\u51b3\u4ee5\u53ca\u5b9e\u7528\u6548\u7387\u548c\u9c81\u68d2\u6027\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5305\u62ec\u6a21\u578b\u8d85\u8d8a\u624b\u5de5\u667a\u80fd\u4f53\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2602.03975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03975", "abs": "https://arxiv.org/abs/2602.03975", "authors": ["Shuhui Qu"], "title": "Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure", "comment": null, "summary": "Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \\emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \\textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\\% fewer verifier calls.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u72b6\u6001\u7ea7\u9009\u62e9\u6027\u9a8c\u8bc1\u6846\u67b6\uff0c\u5728\u9a8c\u8bc1\u6210\u672c\u53d7\u9650\u7684\u8bbe\u7f6e\u4e0b\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u53ef\u884c\u6027\u95e8\u63a7\u3001\u9884\u9a8c\u8bc1\u6392\u5e8f\u548c\u81ea\u9002\u5e94\u5206\u914d\u9a8c\u8bc1\u8c03\u7528\uff0c\u51cf\u5c11\u5197\u4f59\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6210\u4e3a\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u4f46\u6602\u8d35\u7684\u9a8c\u8bc1\u6210\u4e3a\u74f6\u9888\u3002\u8bb8\u591a\u63a8\u7406\u7cfb\u7edf\u4e2d\uff0c\u5927\u91cf\u9a8c\u8bc1\u8c03\u7528\u82b1\u8d39\u5728\u5197\u4f59\u6216\u65e0\u5e0c\u671b\u7684\u4e2d\u95f4\u5047\u8bbe\u4e0a\uff0c\u9700\u8981\u5728\u9a8c\u8bc1\u6210\u672c\u53d7\u9650\u7684\u8bbe\u7f6e\u4e0b\u7814\u7a76\u5982\u4f55\u5206\u914d\u9a8c\u8bc1\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u72b6\u6001\u7ea7\u9009\u62e9\u6027\u9a8c\u8bc1\u6846\u67b6\uff0c\u5305\u542b\uff1a(1) \u7ed3\u6784\u5316\u79fb\u52a8\u63a5\u53e3\u4e0a\u7684\u786e\u5b9a\u6027\u53ef\u884c\u6027\u95e8\u63a7\uff1b(2) \u7ed3\u5408\u5b66\u4e60\u7684\u72b6\u6001\u8ddd\u79bb\u548c\u6b8b\u5dee\u8bc4\u5206\u7684\u9884\u9a8c\u8bc1\u6392\u5e8f\uff1b(3) \u57fa\u4e8e\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u7684\u9a8c\u8bc1\u8c03\u7528\u81ea\u9002\u5e94\u5206\u914d\u3002", "result": "\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4best-of-N\u3001\u591a\u6570\u6295\u7968\u548c\u675f\u641c\u7d22\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c1144%\u7684\u9a8c\u8bc1\u8c03\u7528\u3002", "conclusion": "\u901a\u8fc7\u667a\u80fd\u5206\u914d\u9a8c\u8bc1\u8d44\u6e90\u5230\u6700\u4fe1\u606f\u4e30\u5bcc\u7684\u4e2d\u95f4\u72b6\u6001\uff0c\u53ef\u4ee5\u5728\u9a8c\u8bc1\u6210\u672c\u53d7\u9650\u7684\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u907f\u514d\u5197\u4f59\u9a8c\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2602.04033", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.04033", "abs": "https://arxiv.org/abs/2602.04033", "authors": ["Jind\u0159ich Libovick\u00fd"], "title": "On the Credibility of Evaluating LLMs using Survey Questions", "comment": "Accepted to the Workshop on Multilingual and Multicultural Evaluation at EACL 2026, 12 pages, 2 figures", "summary": "Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30LLM\u4ef7\u503c\u53d6\u5411\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u793a\u65b9\u6cd5\u548c\u89e3\u7801\u7b56\u7565\u663e\u8457\u5f71\u54cd\u7ed3\u679c\uff0c\u63d0\u51fa\u81ea\u76f8\u5173\u8ddd\u79bb\u65b0\u6307\u6807\uff0c\u5efa\u8bae\u4f7f\u7528CoT\u63d0\u793a\u3001\u91c7\u6837\u89e3\u7801\u548c\u591a\u6307\u6807\u5206\u6790", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u6539\u7f16\u793e\u4f1a\u8c03\u67e5\u8bc4\u4f30LLM\u4ef7\u503c\u53d6\u5411\uff0c\u4f46\u8be5\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u4f4e\u4f30\u6216\u9ad8\u4f30\u4e0e\u4eba\u7c7b\u4ef7\u503c\u53d6\u5411\u7684\u76f8\u4f3c\u6027\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u4f7f\u7528\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u5728\u4e09\u79cd\u8bed\u8a00\u4e94\u4e2a\u56fd\u5bb6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\uff08\u76f4\u63a5vs\u94fe\u5f0f\u601d\u8003\uff09\u548c\u89e3\u7801\u7b56\u7565\uff08\u8d2a\u5a6avs\u91c7\u6837\uff09\uff0c\u5f15\u5165\u81ea\u76f8\u5173\u8ddd\u79bb\u65b0\u6307\u6807\u8bc4\u4f30\u7b54\u6848\u95f4\u4e00\u81f4\u6027", "result": "\u63d0\u793a\u65b9\u6cd5\u548c\u89e3\u7801\u7b56\u7565\u663e\u8457\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\uff1b\u5373\u4f7f\u5e73\u5747\u540c\u610f\u5ea6\u9ad8\uff0cLLM\u56de\u7b54\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u4ecd\u4e0d\u8db3\uff1b\u5e38\u7528\u8bc4\u4f30\u6307\u6807\uff08\u5747\u65b9\u8ddd\u79bb\u548cKL\u6563\u5ea6\uff09\u76f8\u5173\u6027\u5f31", "conclusion": "\u5efa\u8bae\u672a\u6765\u7814\u7a76\u4f7f\u7528CoT\u63d0\u793a\u3001\u91c7\u6837\u89e3\u7801\uff08\u6570\u5341\u4e2a\u6837\u672c\uff09\u548c\u591a\u6307\u6807\u5206\u6790\uff08\u5305\u62ec\u81ea\u76f8\u5173\u8ddd\u79bb\uff09\uff0c\u4ee5\u83b7\u5f97\u66f4\u53ef\u9760\u7684LLM\u4ef7\u503c\u53d6\u5411\u8bc4\u4f30", "topic": "agent analysis"}}
{"id": "2602.04445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.04445", "abs": "https://arxiv.org/abs/2602.04445", "authors": ["Rudra Dhar", "Karthik Vaidhyanathan", "Vasudeva Varma"], "title": "AgenticAKM : Enroute to Agentic Architecture Knowledge Management", "comment": null, "summary": "Architecture Knowledge Management (AKM) is crucial for maintaining current and comprehensive software Architecture Knowledge (AK) in a software project. However AKM is often a laborious process and is not adopted by developers and architects. While LLMs present an opportunity for automation, a naive, single-prompt approach is often ineffective, constrained by context limits and an inability to grasp the distributed nature of architectural knowledge. To address these limitations, we propose an Agentic approach for AKM, AgenticAKM, where the complex problem of architecture recovery and documentation is decomposed into manageable sub-tasks. Specialized agents for architecture Extraction, Retrieval, Generation, and Validation collaborate in a structured workflow to generate AK. To validate we made an initial instantiation of our approach to generate Architecture Decision Records (ADRs) from code repositories. We validated our approach through a user study with 29 repositories. The results demonstrate that our agentic approach generates better ADRs, and is a promising and practical approach for automating AKM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgenticAKM\uff0c\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u67b6\u6784\u77e5\u8bc6\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u67b6\u6784\u6062\u590d\u548c\u6587\u6863\u5316\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u81ea\u52a8\u751f\u6210\u67b6\u6784\u51b3\u7b56\u8bb0\u5f55\uff08ADRs\uff09\u3002", "motivation": "\u67b6\u6784\u77e5\u8bc6\u7ba1\u7406\uff08AKM\uff09\u5bf9\u4e8e\u8f6f\u4ef6\u9879\u76ee\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u901a\u5e38\u662f\u4e00\u4e2a\u7e41\u7410\u7684\u8fc7\u7a0b\uff0c\u5f00\u53d1\u8005\u548c\u67b6\u6784\u5e08\u5f80\u5f80\u4e0d\u91c7\u7528\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u673a\u4f1a\uff0c\u4f46\u7b80\u5355\u7684\u5355\u63d0\u793a\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u65e0\u6cd5\u7406\u89e3\u67b6\u6784\u77e5\u8bc6\u7684\u5206\u5e03\u5f0f\u7279\u6027\u3002", "method": "\u63d0\u51faAgenticAKM\u65b9\u6cd5\uff0c\u5c06\u590d\u6742\u7684\u67b6\u6784\u6062\u590d\u548c\u6587\u6863\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\u3002\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff08\u67b6\u6784\u63d0\u53d6\u3001\u68c0\u7d22\u3001\u751f\u6210\u548c\u9a8c\u8bc1\uff09\u5728\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u4e2d\u534f\u4f5c\u751f\u6210\u67b6\u6784\u77e5\u8bc6\u3002\u5177\u4f53\u5b9e\u4f8b\u5316\u4e3a\u4ece\u4ee3\u7801\u4ed3\u5e93\u751f\u6210\u67b6\u6784\u51b3\u7b56\u8bb0\u5f55\uff08ADRs\uff09\u3002", "result": "\u901a\u8fc7\u5bf929\u4e2a\u4ed3\u5e93\u7684\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u667a\u80fd\u4f53\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u597d\u7684ADRs\uff0c\u662f\u81ea\u52a8\u5316AKM\u7684\u6709\u524d\u666f\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\u3002", "conclusion": "\u667a\u80fd\u4f53\u65b9\u6cd5\u901a\u8fc7\u5206\u89e3\u590d\u6742\u4efb\u52a1\u548c\u4e13\u95e8\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edfAKM\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u67b6\u6784\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.04089", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04089", "abs": "https://arxiv.org/abs/2602.04089", "authors": ["Xiaofeng Lin", "Sirou Zhu", "Yilei Chen", "Mingyu Chen", "Hejian Sang", "Ioannis Paschalidis", "Zhipeng Wang", "Aldo Pacchiano", "Xuezhou Zhang"], "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL", "comment": null, "summary": "Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.", "AI": {"tldr": "ORBIT\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u3001\u591a\u56de\u5408\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u8ba9LLM\u80fd\u591f\u5728\u4e0a\u4e0b\u6587\u4e2d\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90\u6a21\u578b\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-5.2\u5e76\u5927\u5e45\u8d85\u8d8a\u6807\u51c6RL\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709LLM\u5728\u9700\u8981\u5728\u7ebf\u4ea4\u4e92\u7684\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u5b9e\u65f6\u6536\u96c6\u4fe1\u606f\u3001\u5904\u7406\u5ef6\u8fdf\u53cd\u9988\uff0c\u5e76\u5728\u4fe1\u606f\u6536\u96c6\u548c\u5229\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u867d\u7136\u4e0a\u4e0b\u6587\u5b66\u4e60\u5141\u8bb8\u65e0\u9700\u6743\u91cd\u66f4\u65b0\u7684\u9002\u5e94\uff0c\u4f46\u73b0\u6709LLM\u96be\u4ee5\u53ef\u9760\u5229\u7528\u4e0a\u4e0b\u6587\u4ea4\u4e92\u7ecf\u9a8c\u3002", "method": "\u63d0\u51faORBIT\u6846\u67b6\uff1a\u591a\u4efb\u52a1\u3001\u591a\u56de\u5408\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u8ba9LLM\u5b66\u4e60\u5728\u4e0a\u4e0b\u6587\u4e2d\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60\u3002\u4f7f\u7528\u76f8\u5bf9\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\uff08Qwen3-14B\uff09\u8fdb\u884c\u5143\u8bad\u7ec3\u3002", "result": "\u5143\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u5b8c\u5168\u672a\u89c1\u7684\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u7684\u4e0a\u4e0b\u6587\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\uff0c\u6027\u80fd\u5339\u914dGPT-5.2\uff0c\u5e76\u5927\u5e45\u8d85\u8d8a\u6807\u51c6RL\u5fae\u8c03\u65b9\u6cd5\u3002\u6269\u5c55\u5b9e\u9a8c\u663e\u793a\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u8bad\u7ec3\u53ef\u4ee5\u89e3\u51b3LLM\u5728\u5728\u7ebf\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0cORBIT\u6846\u67b6\u5c55\u793a\u4e86\u5728\u63a8\u7406\u65f6\u5b66\u4e60\u51b3\u7b56\u667a\u80fd\u4f53\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u5e26\u6765\u6301\u7eed\u6536\u76ca\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04640", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04640", "abs": "https://arxiv.org/abs/2602.04640", "authors": ["Tse-Hsun", "Chen"], "title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents", "comment": "Position paper accepted in BoatSE", "summary": "Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.\n  In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u4e3b\u5f20\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u9700\u8981\u4ece\u88ab\u52a8\u53cd\u5e94\u5f0f\u8bbe\u8ba1\u8f6c\u5411\u7ed3\u6784\u5316\u3001\u72b6\u6001\u611f\u77e5\u3001\u6267\u884c\u57fa\u7840\u63a8\u7406\uff0c\u4ee5\u89e3\u51b3\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u8fde\u8d2f\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4e3b\u8981\u662f\u88ab\u52a8\u53cd\u5e94\u5f0f\u7684\uff0c\u4ec5\u57fa\u4e8e\u5bf9\u8bdd\u5386\u53f2\u548c\u6700\u8fd1\u54cd\u5e94\u505a\u51b3\u7b56\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u7ed3\u6784\u6216\u6301\u4e45\u72b6\u6001\uff0c\u5bfc\u81f4\u957f\u671f\u63a8\u7406\u56f0\u96be\uff0c\u96be\u4ee5\u7ef4\u6301\u8de8\u63a8\u7406\u6b65\u9aa4\u7684\u8fde\u8d2f\u7406\u89e3\u3001\u9002\u5e94\u65b0\u8bc1\u636e\u6216\u6574\u5408\u6267\u884c\u53cd\u9988\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6539\u8fdb\u6765\u63a8\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\uff1a1) \u660e\u786e\u7684\u7ed3\u6784\u5316\u8bbe\u8ba1\uff1b2) \u6301\u4e45\u4e14\u6f14\u5316\u7684\u72b6\u6001\u7ba1\u7406\uff1b3) \u6267\u884c\u57fa\u7840\u53cd\u9988\u7684\u6574\u5408\u3002\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u63d0\u4f9b\u4e86\u521d\u6b65\u8def\u7ebf\u56fe\u3002", "result": "\u4f5c\u4e3a\u7acb\u573a\u8bba\u6587\uff0c\u6ca1\u6709\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u548c\u8def\u7ebf\u56fe\uff0c\u6307\u51fa\u7ed3\u6784\u5316\u3001\u72b6\u6001\u611f\u77e5\u3001\u6267\u884c\u57fa\u7840\u63a8\u7406\u80fd\u5e2e\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u6267\u884c\u66f4\u8fde\u8d2f\u53ef\u9760\u7684\u63a8\u7406\u3002", "conclusion": "\u8981\u63a8\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u53d1\u5c55\uff0c\u9700\u8981\u8d85\u8d8a\u88ab\u52a8\u53cd\u5e94\u5f0f\u884c\u4e3a\uff0c\u8f6c\u5411\u7ed3\u6784\u5316\u3001\u72b6\u6001\u611f\u77e5\u3001\u6267\u884c\u57fa\u7840\u63a8\u7406\uff0c\u8fd9\u5c06\u4f7f\u4ee3\u7406\u80fd\u66f4\u6709\u6548\u5730\u6267\u884c\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2602.04101", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04101", "abs": "https://arxiv.org/abs/2602.04101", "authors": ["Harsha Vardhan Khurdula", "Vineet Agarwal", "Yoeven D Khemlani"], "title": "Interfaze: The Future of AI is built on Task-Specific Small Models", "comment": "8 pages, 1 figure", "summary": "We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.\n  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.", "AI": {"tldr": "Interfaze\u662f\u4e00\u4e2a\u5c06LLM\u5e94\u7528\u89c6\u4e3a\u4e0a\u4e0b\u6587\u6784\u5efa\u4e0e\u6267\u884c\u95ee\u9898\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6784DNN\u5806\u6808\u3001\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u611f\u77e5\u6a21\u5757\u3001\u4e0a\u4e0b\u6587\u6784\u5efa\u5c42\u548c\u52a8\u4f5c\u5c42\uff0c\u914d\u5408\u8f7b\u91cf\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u6a21\u6001\u4efb\u52a1\u5904\u7406\u3002", "motivation": "\u73b0\u4ee3LLM\u5e94\u7528\u4e0d\u5e94\u4ec5\u4f9d\u8d56\u5355\u4e00\u5927\u578b\u6a21\u578b\uff0c\u800c\u5e94\u901a\u8fc7\u6784\u5efa\u548c\u64cd\u4f5c\u4e0a\u4e0b\u6587\u6765\u89e3\u51b3\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00transformer\u6a21\u578b\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u7cfb\u7edf\u67b6\u6784\u6765\u6574\u5408\u591a\u79cd\u4e13\u4e1a\u6a21\u5757\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a(1)\u5f02\u6784DNN\u5806\u6808\u914d\u5408\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u611f\u77e5\u6a21\u5757\uff0c\u5904\u7406\u590d\u6742PDF\u3001\u56fe\u8868\u3001\u591a\u8bed\u8a00ASR\u7b49\uff1b(2)\u4e0a\u4e0b\u6587\u6784\u5efa\u5c42\u722c\u53d6\u3001\u7d22\u5f15\u3001\u89e3\u6790\u5916\u90e8\u8d44\u6e90\u4e3a\u7ed3\u6784\u5316\u72b6\u6001\uff1b(3)\u52a8\u4f5c\u5c42\u652f\u6301\u6d4f\u89c8\u3001\u68c0\u7d22\u3001\u6c99\u7bb1\u4ee3\u7801\u6267\u884c\u548c\u6d4f\u89c8\u5668\u9a71\u52a8\u3002\u9876\u5c42\u8f7b\u91cf\u63a7\u5236\u5668\u51b3\u5b9a\u8fd0\u884c\u54ea\u4e9b\u5c0f\u578b\u6a21\u578b\u548c\u52a8\u4f5c\uff0c\u5e76\u5c06\u7cbe\u70bc\u7684\u4e0a\u4e0b\u6587\u8f6c\u53d1\u7ed9\u7528\u6237\u9009\u62e9\u7684LLM\u751f\u6210\u6700\u7ec8\u54cd\u5e94\u3002", "result": "Interfaze-Beta\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aMMLU-Pro 83.6%\u3001MMLU 91.4%\u3001GPQA-Diamond 81.3%\u3001LiveCodeBench v5 57.8%\u3001AIME-2025 90.0%\uff0c\u591a\u6a21\u6001\u4efb\u52a1\u4e0aMMMU(val) 77.3%\u3001AI2D 91.5%\u3001ChartQA 90.9%\u3001Common Voice v16 90.8%\u3002\u7cfb\u7edf\u5c06\u5927\u90e8\u5206\u8ba1\u7b97\u4ece\u5c0f\u578b\u6a21\u578b\u548c\u5de5\u5177\u6808\u5904\u7406\uff0c\u5927\u578bLLM\u4ec5\u64cd\u4f5c\u7cbe\u70bc\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u7ade\u4e89\u6027\u51c6\u786e\u7387\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "Interfaze\u5c55\u793a\u4e86\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u6574\u5408\u4e13\u4e1a\u5c0f\u578b\u6a21\u578b\u548c\u5de5\u5177\uff0c\u5c06\u8ba1\u7b97\u8d1f\u62c5\u4ece\u6602\u8d35\u7684\u5927\u578b\u6a21\u578b\u8f6c\u79fb\u5230\u66f4\u9ad8\u6548\u7684\u7ec4\u4ef6\u4e0a\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u591a\u6a21\u6001LLM\u5e94\u7528\uff0c\u4e3a\u6784\u5efa\u66f4\u667a\u80fd\u3001\u66f4\u7ecf\u6d4e\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "2602.04726", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04726", "abs": "https://arxiv.org/abs/2602.04726", "authors": ["Marian Kica", "Lukas Radosky", "David Slivka", "Karin Kubinova", "Daniel Dovhun", "Tomas Uhercik", "Erik Bircak", "Ivan Polasek"], "title": "Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation", "comment": "This is a preprint of a paper that was accepted at the International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA 2026)", "summary": "The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u9762\u5411\u8f6f\u4ef6\u5de5\u7a0b\u7684AI\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\uff1a\u4e00\u662f\u57fa\u4e8e\u661f\u578b\u62d3\u6251\u7ed3\u6784\u7684\u81ea\u52a8\u6d4b\u8bd5\u573a\u666f\u751f\u6210\u7cfb\u7edf\uff0c\u4e8c\u662f\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u6587\u6863\u68c0\u7d22\u7684\u591a\u529f\u80fd\u4ee3\u7406\u7cfb\u7edf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u5f15\u53d1\u4e86\u8f6f\u4ef6\u5f00\u53d1\u6a21\u5f0f\u7684\u91cd\u5927\u53d8\u9769\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4ea7\u751f\u4e86\u5927\u91cf\u5de5\u5177\u548c\u65b9\u6cd5\u3002\u4f5c\u8005\u5e0c\u671b\u52a0\u5165\u8fd9\u4e00\u6d6a\u6f6e\uff0c\u901a\u8fc7AI\u4ee3\u7406\u89e3\u51b3\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\u3002", "method": "1. \u6d4b\u8bd5\u573a\u666f\u751f\u6210\uff1a\u91c7\u7528\u661f\u578b\u62d3\u6251\u7ed3\u6784\uff0c\u7531\u76d1\u7763\u4ee3\u7406\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u5de5\u4f5c\u4ee3\u7406\uff0c\u4ece\u8be6\u7ec6\u9700\u6c42\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u573a\u666f\u3002\n2. \u6587\u6863\u68c0\u7d22\uff1a\u4e3a\u6bcf\u4e2a\u7528\u4f8b\uff08\u641c\u7d22\u3001\u95ee\u7b54\u3001\u53d8\u66f4\u8ffd\u8e2a\u3001\u957f\u6587\u6863\u6458\u8981\uff09\u8bbe\u8ba1\u4e13\u95e8\u7684LLM\u4ee3\u7406\uff0c\u5904\u7406\u76f8\u5173\u5b50\u4efb\u52a1\u3002", "result": "1. \u5728\u771f\u5b9e\u6848\u4f8b\u4e2d\u5c55\u793a\u4e86\u6d4b\u8bd5\u573a\u666f\u751f\u6210\u7cfb\u7edf\u7684\u80fd\u529b\u3002\n2. \u5f00\u53d1\u4e86\u80fd\u591f\u5904\u7406\u591a\u79cd\u8f6f\u4ef6\u5de5\u7a0b\u6587\u6863\u7528\u4f8b\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u62ec\u641c\u7d22\u3001\u95ee\u7b54\u3001\u53d8\u66f4\u8ffd\u8e2a\u548c\u6587\u6863\u6458\u8981\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2602.04197", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04197", "abs": "https://arxiv.org/abs/2602.04197", "authors": ["Xinyue Wang", "Yuanhe Zhang", "Zhengshuo Gong", "Haoran Gao", "Fanyu Meng", "Zhenhong Zhou", "Li Sun", "Yang Liu", "Sen Su"], "title": "From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents", "comment": "9 pages (excluding appendices), 6 figures. Code is available at https://github.com/wxyoio-0715/Toxic-Proactivity", "summary": "The enhanced capabilities of LLM-based agents come with an emergency for model planning and tool-use abilities. Attributing to helpful-harmless trade-off from LLM alignment, agents typically also inherit the flaw of \"over-refusal\", which is a passive failure mode. However, the proactive planning and action capabilities of agents introduce another crucial danger on the other side of the trade-off. This phenomenon we term \"Toxic Proactivity'': an active failure mode in which an agent, driven by the optimization for Machiavellian helpfulness, disregards ethical constraints to maximize utility. Unlike over-refusal, Toxic Proactivity manifests as the agent taking excessive or manipulative measures to ensure its \"usefulness'' is maintained. Existing research pays little attention to identifying this behavior, as it often lacks the subtle context required for such strategies to unfold. To reveal this risk, we introduce a novel evaluation framework based on dilemma-driven interactions between dual models, enabling the simulation and analysis of agent behavior over multi-step behavioral trajectories. Through extensive experiments with mainstream LLMs, we demonstrate that Toxic Proactivity is a widespread behavioral phenomenon and reveal two major tendencies. We further present a systematic benchmark for evaluating Toxic Proactive behavior across contextual settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u6bd2\u6027\u4e3b\u52a8\u6027\"\u6982\u5ff5\uff0c\u6307AI\u4ee3\u7406\u4e3a\u6700\u5927\u5316\u6548\u7528\u800c\u5ffd\u89c6\u4f26\u7406\u7ea6\u675f\u7684\u4e3b\u52a8\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u5efa\u7acb\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u51c6\u6765\u8bc6\u522b\u8fd9\u79cd\u73b0\u8c61\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u5bf9\u9f50\u5e26\u6765\u7684\"\u8fc7\u5ea6\u62d2\u7edd\"\u88ab\u52a8\u5931\u8d25\u6a21\u5f0f\uff0c\u4f46\u5ffd\u89c6\u4e86\u4ee3\u7406\u4e3b\u52a8\u89c4\u5212\u548c\u884c\u52a8\u80fd\u529b\u53ef\u80fd\u5bfc\u81f4\u7684\"\u6bd2\u6027\u4e3b\u52a8\u6027\"\u98ce\u9669\uff0c\u5373\u4ee3\u7406\u4e3a\u8ffd\u6c42\u9a6c\u57fa\u96c5\u7ef4\u5229\u5f0f\u6709\u7528\u6027\u800c\u8fdd\u53cd\u4f26\u7406\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56f0\u5883\u9a71\u52a8\u7684\u53cc\u6a21\u578b\u4ea4\u4e92\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u884c\u4e3a\u8f68\u8ff9\u6a21\u62df\u5206\u6790\u4ee3\u7406\u884c\u4e3a\uff1b\u5efa\u7acb\u7cfb\u7edf\u6027\u57fa\u51c6\u8bc4\u4f30\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u6bd2\u6027\u4e3b\u52a8\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6bd2\u6027\u4e3b\u52a8\u6027\u662f\u5e7f\u6cdb\u5b58\u5728\u7684\u884c\u4e3a\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u4e3b\u8981\u503e\u5411\uff1b\u5efa\u7acb\u4e86\u8bc4\u4f30\u6bd2\u6027\u4e3b\u52a8\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u57fa\u51c6\u3002", "conclusion": "\u6bd2\u6027\u4e3b\u52a8\u6027\u662fLLM\u4ee3\u7406\u7684\u91cd\u8981\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u6846\u67b6\u6765\u8bc6\u522b\uff1b\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u7f13\u89e3\u8fd9\u79cd\u4e3b\u52a8\u5931\u8d25\u6a21\u5f0f\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2602.04206", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04206", "abs": "https://arxiv.org/abs/2602.04206", "authors": ["Hsien-Jyh Liao"], "title": "Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry", "comment": "Submitted to ICAIL 2026. Under review", "summary": "Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancement. We characterize this failure as procedural stagnation and propose Soft-FSM, a neuro-symbolic architecture that enforces monotonic progress over accumulated Key Information Units (KIUs) via an external deterministic state controller. Experiments on three real-world Taiwanese criminal homicide cases show that baseline methods collapse below 40% completeness, while Soft-FSM consistently achieves over 97% with near-zero redundancy. These results suggest that, in such domains, reliable task completion cannot be guaranteed by emergent LLM behavior alone, and can be reliably enforced through explicit and verifiable external state control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSoft-FSM\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u901a\u8fc7\u5916\u90e8\u786e\u5b9a\u6027\u72b6\u6001\u63a7\u5236\u5668\u5f3a\u5236LLM\u5728\u7a0b\u5e8f\u6027\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5355\u8c03\u8fdb\u5c55\uff0c\u89e3\u51b3\u4e86LLM\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u5bb9\u6613\u9677\u5165\u7a0b\u5e8f\u505c\u6ede\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u6d41\u7545\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u660e\u786e\u7684\u7a0b\u5e8f\u7ea6\u675f\u4e0b\u53ef\u9760\u5b8c\u6210\u957f\u671f\u4efb\u52a1\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u5728\u6cd5\u5f8b\u4ea4\u53c9\u8be2\u95ee\u7b49\u573a\u666f\u4e2d\uff0c\u7eaf\u6982\u7387\u751f\u6210\u867d\u7136\u80fd\u4fdd\u6301\u884c\u4e3a\u8fde\u8d2f\u6027\uff0c\u4f46\u65e0\u6cd5\u786e\u4fdd\u7a0b\u5e8f\u63a8\u8fdb\uff0c\u8fd9\u79cd\u5931\u8d25\u88ab\u5b9a\u4e49\u4e3a\u7a0b\u5e8f\u505c\u6ede\u3002", "method": "\u63d0\u51faSoft-FSM\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u901a\u8fc7\u5916\u90e8\u786e\u5b9a\u6027\u72b6\u6001\u63a7\u5236\u5668\u5f3a\u5236\u5b9e\u73b0\u5173\u952e\u4fe1\u606f\u5355\u5143\uff08KIUs\uff09\u7684\u5355\u8c03\u7d2f\u79ef\u8fdb\u5c55\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u7075\u6d3b\u6027\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u786e\u5b9a\u6027\u63a7\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u53f0\u6e7e\u771f\u5b9e\u5211\u4e8b\u6740\u4eba\u6848\u4ef6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u57fa\u7ebf\u65b9\u6cd5\u5b8c\u6210\u5ea6\u4f4e\u4e8e40%\uff0c\u800cSoft-FSM\u59cb\u7ec8\u8fbe\u523097%\u4ee5\u4e0a\u4e14\u5197\u4f59\u5ea6\u63a5\u8fd1\u96f6\u3002", "conclusion": "\u5728\u67d0\u4e9b\u9886\u57df\uff0c\u53ef\u9760\u7684\u4efb\u200b\u200b\u52a1\u5b8c\u6210\u4e0d\u80fd\u4ec5\u4f9d\u8d56LLM\u7684\u6d8c\u73b0\u884c\u4e3a\uff0c\u800c\u9700\u8981\u901a\u8fc7\u660e\u786e\u4e14\u53ef\u9a8c\u8bc1\u7684\u5916\u90e8\u72b6\u6001\u63a7\u5236\u6765\u5f3a\u5236\u4fdd\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2602.04213", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04213", "abs": "https://arxiv.org/abs/2602.04213", "authors": ["Feiyu Gavin Zhu", "Jean Oh", "Reid Simmons"], "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons", "comment": "Proceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction", "summary": "Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy", "AI": {"tldr": "InterPReT\uff1a\u4e00\u79cd\u4ea4\u4e92\u5f0f\u7b56\u7565\u91cd\u6784\u4e0e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8ba9\u975e\u4e13\u4e1a\u7528\u6237\u80fd\u591f\u901a\u8fc7\u6307\u4ee4\u548c\u6f14\u793a\u6765\u6559\u5bfcAI\u4ee3\u7406\uff0c\u964d\u4f4eAI\u4ee3\u7406\u6559\u5b66\u95e8\u69db", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u6f14\u793a\u548c\u8bad\u7ec3\u76d1\u63a7\uff0c\u8fd9\u5bf9\u666e\u901a\u7528\u6237\u6559\u5bfcAI\u4ee3\u7406\u65b0\u6280\u80fd\u6784\u6210\u6311\u6218\u3002\u9700\u8981\u964d\u4f4eAI\u4ee3\u7406\u6559\u5b66\u95e8\u69db\uff0c\u8ba9\u975e\u4e13\u4e1a\u7528\u6237\u4e5f\u80fd\u6709\u6548\u8bad\u7ec3\u4ee3\u7406", "method": "\u63d0\u51fa\u4ea4\u4e92\u5f0f\u7b56\u7565\u91cd\u6784\u4e0e\u8bad\u7ec3\uff08InterPReT\uff09\uff0c\u901a\u8fc7\u7528\u6237\u6307\u4ee4\u6301\u7eed\u66f4\u65b0\u7b56\u7565\u7ed3\u6784\u5e76\u4f18\u5316\u53c2\u6570\u4ee5\u9002\u5e94\u7528\u6237\u6f14\u793a\u3002\u7528\u6237\u53ef\u4ea4\u4e92\u5f0f\u63d0\u4f9b\u6307\u4ee4\u548c\u6f14\u793a\u3001\u76d1\u63a7\u4ee3\u7406\u6027\u80fd\u3001\u5ba1\u67e5\u51b3\u7b56\u7b56\u7565", "result": "\u5728\u8d5b\u8f66\u6e38\u620f\u9a7e\u9a76\u6559\u5b66\u7684\u7528\u6237\u7814\u7a76\uff08N=34\uff09\u4e2d\uff0c\u76f8\u6bd4\u901a\u7528\u6a21\u4eff\u5b66\u4e60\u57fa\u7ebf\uff0cInterPReT\u5728\u666e\u901a\u7528\u6237\u8d1f\u8d23\u6f14\u793a\u548c\u51b3\u5b9a\u505c\u6b62\u65f6\u673a\u65f6\uff0c\u80fd\u4ea7\u751f\u66f4\u9c81\u68d2\u7684\u7b56\u7565\u800c\u4e0d\u635f\u5bb3\u7cfb\u7edf\u53ef\u7528\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u66f4\u9002\u5408\u6ca1\u6709\u673a\u5668\u5b66\u4e60\u6280\u672f\u80cc\u666f\u7684\u7ec8\u7aef\u7528\u6237\u8bad\u7ec3\u53ef\u9760\u7b56\u7565\uff0c\u964d\u4f4e\u4e86AI\u4ee3\u7406\u6559\u5b66\u7684\u95e8\u69db", "topic": "agent analysis"}}
{"id": "2602.04248", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04248", "abs": "https://arxiv.org/abs/2602.04248", "authors": ["Hao Lu", "Haoyuan Huang", "Yulin Zhou", "Chen Li", "Ningxin Zhu"], "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "comment": "9 pages, 5 figures", "summary": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.", "AI": {"tldr": "Empirical-MCTS\uff1a\u53cc\u5faa\u73af\u6846\u67b6\uff0c\u5c06\u65e0\u72b6\u6001\u641c\u7d22\u8f6c\u5316\u4e3a\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5c40\u90e8\u63a2\u7d22\u4e0e\u5168\u5c40\u8bb0\u5fc6\u4f18\u5316\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b", "motivation": "\u5f53\u524d\u57fa\u4e8eMCTS\u7684\u63a8\u7406\u65f6\u6269\u5c55\u7b56\u7565\u4e3b\u8981\u662f\u65e0\u72b6\u6001\u7684\uff0c\u6bcf\u6b21\u89e3\u51b3\u95ee\u9898\u540e\u4e22\u5f03\u6210\u529f\u63a8\u7406\u6a21\u5f0f\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u90a3\u6837\u79ef\u7d2f\u7ecf\u9a8c\u667a\u6167\u3002\u9700\u8981\u5c06\u7ed3\u6784\u5316\u641c\u7d22\u4e0e\u7ecf\u9a8c\u79ef\u7d2f\u76f8\u7ed3\u5408\u6765\u5e94\u5bf9\u590d\u6742\u5f00\u653e\u63a8\u7406\u4efb\u52a1\u3002", "method": "\u63d0\u51faEmpirical-MCTS\u53cc\u5faa\u73af\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) PE-EMP\uff08\u6210\u5bf9\u7ecf\u9a8c\u6f14\u5316\u5143\u63d0\u793a\uff09\u4f5c\u4e3a\u5c40\u90e8\u641c\u7d22\u7684\u53cd\u5c04\u4f18\u5316\u5668\uff0c\u4f7f\u7528\u6210\u5bf9\u53cd\u9988\u52a8\u6001\u5408\u6210\u81ea\u9002\u5e94\u6807\u51c6\u5e76\u5b9e\u65f6\u6f14\u5316\u5143\u63d0\u793a\uff1b2) \u8bb0\u5fc6\u4f18\u5316\u4ee3\u7406\u7ba1\u7406\u5168\u5c40\u5b58\u50a8\u5e93\u4f5c\u4e3a\u52a8\u6001\u7b56\u7565\u5148\u9a8c\uff0c\u901a\u8fc7\u539f\u5b50\u64cd\u4f5c\u8de8\u95ee\u9898\u63d0\u70bc\u9ad8\u8d28\u91cf\u89c1\u89e3\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08AIME25\u3001ARC-AGI-2\u3001MathArena Apex\uff09\u4e0a\uff0cEmpirical-MCTS\u663e\u8457\u4f18\u4e8e\u65e0\u72b6\u6001MCTS\u7b56\u7565\u548c\u72ec\u7acb\u7ecf\u9a8c\u9a71\u52a8\u4ee3\u7406\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u641c\u7d22\u4e0e\u7ecf\u9a8c\u79ef\u7d2f\u7ed3\u5408\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5c06\u7ed3\u6784\u5316\u641c\u7d22\u4e0e\u7ecf\u9a8c\u79ef\u7d2f\u76f8\u7ed3\u5408\u5bf9\u4e8e\u638c\u63e1\u590d\u6742\u5f00\u653e\u63a8\u7406\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0cEmpirical-MCTS\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.04284", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04284", "abs": "https://arxiv.org/abs/2602.04284", "authors": ["Yansong Ning", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning", "comment": "Under Review", "summary": "Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.", "AI": {"tldr": "Agent-Omit\u662f\u4e00\u4e2a\u8bad\u7ec3\u6846\u67b6\uff0c\u8ba9LLM\u4ee3\u7406\u80fd\u81ea\u9002\u5e94\u5730\u7701\u7565\u5197\u4f59\u7684\u601d\u8003\u548c\u89c2\u5bdf\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u6574\u4e2a\u4ea4\u4e92\u8f68\u8ff9\u540c\u7b49\u5bf9\u5f85\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u8f6e\u6b21\u4e2d\u601d\u8003\u5fc5\u8981\u6027\u548c\u89c2\u5bdf\u6548\u7528\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u4ee3\u7406\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1) \u5408\u6210\u5c11\u91cf\u51b7\u542f\u52a8\u6570\u636e\uff08\u5355\u8f6e\u548c\u591a\u8f6e\u7701\u7565\u573a\u666f\uff09\u5fae\u8c03\u4ee3\u7406\uff1b2) \u63d0\u51fa\u7701\u7565\u611f\u77e5\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u542b\u53cc\u91cd\u91c7\u6837\u673a\u5236\u548c\u5b9a\u5236\u7684\u7701\u7565\u5956\u52b1\uff1b3) \u7406\u8bba\u8bc1\u660e\u7701\u7565\u7b56\u7565\u7684\u504f\u5dee\u6709KL\u6563\u5ea6\u4e0a\u754c\u3002", "result": "\u5728\u4e94\u4e2a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgent-Omit-8B\u6027\u80fd\u4e0e\u4e03\u4e2a\u524d\u6cbfLLM\u4ee3\u7406\u76f8\u5f53\uff0c\u4e14\u5728\u6548\u7387-\u6548\u679c\u6743\u8861\u4e0a\u4f18\u4e8e\u4e03\u4e2a\u9ad8\u6548LLM\u4ee3\u7406\u65b9\u6cd5\u3002", "conclusion": "Agent-Omit\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7406\u6548\u7387\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7701\u7565\u5197\u4f59\u601d\u8003\u548c\u89c2\u5bdf\u5b9e\u73b0\u66f4\u597d\u7684\u6548\u679c-\u6548\u7387\u5e73\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04326", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.04326", "abs": "https://arxiv.org/abs/2602.04326", "authors": ["SeungWon Seo", "SooBin Lim", "SeongRae Noh", "Haneul Kim", "HyeongYeop Kang"], "title": "From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents", "comment": "31 pages, 10 figures, Accepted ICLR 2026", "summary": "Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.", "AI": {"tldr": "PCE\u6846\u67b6\u5c06LLM\u63a8\u7406\u4e2d\u7684\u9690\u542b\u5047\u8bbe\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u51b3\u7b56\u6811\uff0c\u901a\u8fc7\u8bc4\u4f30\u573a\u666f\u53ef\u80fd\u6027\u3001\u76ee\u6807\u6536\u76ca\u548c\u6267\u884c\u6210\u672c\u6765\u9009\u62e9\u884c\u52a8\uff0c\u51cf\u5c11\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u5f00\u9500", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u5177\u8eab\u667a\u80fd\u4f53\u5728\u591a\u667a\u80fd\u4f53\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u3001\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u4e3b\u8981\u4f9d\u8d56\u9891\u7e41\u7684\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u6765\u7f13\u89e3\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u663e\u8457\u7684token\u548c\u65f6\u95f4\u6210\u672c\uff0c\u5e76\u53ef\u80fd\u5e72\u6270\u4eba\u7c7b\u5408\u4f5c\u4f19\u4f34\u7684\u5de5\u4f5c\u6d41\u7a0b", "method": "\u63d0\u51faPlanner-Composer-Evaluator\u6846\u67b6\uff1a\u5c06LLM\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u788e\u7247\u5316\u5047\u8bbe\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u51b3\u7b56\u6811\uff0c\u5185\u90e8\u8282\u70b9\u7f16\u7801\u73af\u5883\u5047\u8bbe\uff0c\u53f6\u5b50\u8282\u70b9\u6620\u5c04\u5230\u884c\u52a8\uff0c\u6bcf\u6761\u8def\u5f84\u901a\u8fc7\u573a\u666f\u53ef\u80fd\u6027\u3001\u76ee\u6807\u5bfc\u5411\u6536\u76ca\u548c\u6267\u884c\u6210\u672c\u8fdb\u884c\u8bc4\u5206", "result": "\u5728\u4e24\u4e2a\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\uff08C-WAH\u548cTDW-MAT\uff09\u548c\u4e09\u4e2aLLM\u9aa8\u5e72\u4e0a\uff0cPCE\u5728\u6210\u529f\u7387\u548c\u4efb\u52a1\u6548\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u901a\u4fe1\u5bc6\u96c6\u578b\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684token\u4f7f\u7528\u91cf\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660ePCE\u5728\u4e0d\u540c\u6a21\u578b\u5bb9\u91cf\u548c\u63a8\u7406\u6df1\u5ea6\u4e0b\u90fd\u80fd\u63d0\u5347\u6027\u80fd", "conclusion": "PCE\u4e3a\u5c06LLM\u7684\u6f5c\u5728\u5047\u8bbe\u8f6c\u5316\u4e3a\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c4\u5212\u7b56\u7565\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9014\u5f84\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u4ea7\u751f\u7684\u901a\u4fe1\u6a21\u5f0f\u88ab\u4eba\u7c7b\u5408\u4f5c\u4f19\u4f34\u8ba4\u4e3a\u66f4\u9ad8\u6548\u548c\u53ef\u4fe1", "topic": "agent analysis"}}
{"id": "2602.04254", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04254", "abs": "https://arxiv.org/abs/2602.04254", "authors": ["Zeyao Ma", "Jing Zhang", "Xiaokang Zhang", "Jiaxi Yang", "Zongmeng Zhang", "Jiajun Zhang", "Yuheng Jing", "Lei Zhang", "Hao Zheng", "Wenting Zhao", "Junyang Lin", "Binyuan Hui"], "title": "Scaling Agentic Verifier for Competitive Coding", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong coding capabilities but still struggle to solve competitive programming problems correctly in a single attempt. Execution-based re-ranking offers a promising test-time scaling strategy, yet existing methods are constrained by either difficult test case generation or inefficient random input sampling. To address this limitation, we propose Agentic Verifier, an execution-based agent that actively reasons about program behaviors and searches for highly discriminative test inputs that expose behavioral discrepancies among candidate solutions. Through multi-turn interaction with code execution environments, the verifier iteratively refines the candidate input generator and produces targeted counterexamples rather than blindly sampling inputs. We train the verifier to acquire this discriminative input generation capability via a scalable pipeline combining large-scale data synthesis, rejection fine-tuning, and agentic reinforcement learning. Extensive experiments across five competitive programming benchmarks demonstrate consistent improvements over strong execution-based baselines, achieving up to +10-15% absolute gains in Best@K accuracy. Further analysis reveals clear test-time scaling behavior and highlights the verifier's broader potential beyond reranking.", "AI": {"tldr": "Agentic Verifier\u662f\u4e00\u4e2a\u57fa\u4e8e\u6267\u884c\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u7a0b\u5e8f\u884c\u4e3a\u5e76\u641c\u7d22\u5177\u6709\u9ad8\u5ea6\u533a\u5206\u6027\u7684\u6d4b\u8bd5\u8f93\u5165\u6765\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u7f16\u7801\u80fd\u529b\uff0c\u4f46\u5728\u89e3\u51b3\u7ade\u4e89\u6027\u7f16\u7a0b\u95ee\u9898\u65f6\u4ecd\u96be\u4ee5\u4e00\u6b21\u6027\u6b63\u786e\u5b8c\u6210\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6267\u884c\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u53d7\u5230\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u56f0\u96be\u6216\u968f\u673a\u8f93\u5165\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faAgentic Verifier\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6267\u884c\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u4e0e\u4ee3\u7801\u6267\u884c\u73af\u5883\u7684\u591a\u8f6e\u4ea4\u4e92\uff0c\u4e3b\u52a8\u63a8\u7406\u7a0b\u5e8f\u884c\u4e3a\u5e76\u641c\u7d22\u80fd\u591f\u66b4\u9732\u5019\u9009\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u884c\u4e3a\u5dee\u5f02\u7684\u6d4b\u8bd5\u8f93\u5165\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u5408\u6210\u3001\u62d2\u7edd\u5fae\u8c03\u548c\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u7ba1\u9053\u6765\u8bad\u7ec3\u9a8c\u8bc1\u5668\u3002", "result": "\u5728\u4e94\u4e2a\u7ade\u4e89\u6027\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u5f3a\u5927\u7684\u57fa\u4e8e\u6267\u884c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u5728Best@K\u51c6\u786e\u7387\u4e0a\u83b7\u5f97\u4e86\u9ad8\u8fbe+10-15%\u7684\u7edd\u5bf9\u589e\u76ca\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u63ed\u793a\u4e86\u6e05\u6670\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u884c\u4e3a\u3002", "conclusion": "Agentic Verifier\u901a\u8fc7\u4e3b\u52a8\u751f\u6210\u5177\u6709\u533a\u5206\u6027\u7684\u6d4b\u8bd5\u8f93\u5165\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u8d85\u8d8a\u91cd\u65b0\u6392\u5e8f\u7684\u66f4\u5e7f\u6cdb\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2602.04496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04496", "abs": "https://arxiv.org/abs/2602.04496", "authors": ["Zhentao Tang", "Yuqi Cui", "Shixiong Kai", "Wenqian Zhao", "Ke Ye", "Xing Li", "Anxin Tian", "Zehua Pei", "Hui-Ling Zhen", "Shoubo Hu", "Xiaoguang Li", "Yunhe Wang", "Mingxuan Yuan"], "title": "ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control", "comment": null, "summary": "Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.", "AI": {"tldr": "ReThinker\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7Solver-Critic-Selector\u67b6\u6784\u5b9e\u73b0\u52a8\u6001\u8ba1\u7b97\u5206\u914d\uff0c\u5728\u4e13\u5bb6\u7ea7\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u5bb6\u7ea7\u79d1\u5b66\u63a8\u7406\uff08\u5982Humanity's Last Exam\u57fa\u51c6\uff09\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u5de5\u5177\u7ba1\u9053\u50f5\u5316\u3001\u591a\u667a\u80fd\u4f53\u534f\u8c03\u8106\u5f31\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u6548\u7387\u4f4e\u7b49\u95ee\u9898\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faReThinker\u6846\u67b6\uff0c\u91c7\u7528Solver-Critic-Selector\u4e09\u9636\u6bb5\u67b6\u6784\uff0c\u57fa\u4e8e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u5de5\u5177\u8c03\u7528\u3001\u5f15\u5bfc\u5f0f\u591a\u7ef4\u5ea6\u53cd\u601d\u548c\u7f6e\u4fe1\u5ea6\u52a0\u6743\u9009\u62e9\u3002\u540c\u65f6\u63d0\u51fa\u53cd\u5411\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u81ea\u9002\u5e94\u8f68\u8ff9\u56de\u6536\u7b56\u7565\uff0c\u5c06\u6210\u529f\u63a8\u7406\u8f68\u8ff9\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u3002", "result": "\u5728HLE\u3001GAIA\u548cXBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReThinker\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5de5\u5177\u589e\u5f3a\u57fa\u7840\u6a21\u578b\u548c\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u5728\u4e13\u5bb6\u7ea7\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "ReThinker\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u548c\u521b\u65b0\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u7ea7\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2602.04575", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04575", "abs": "https://arxiv.org/abs/2602.04575", "authors": ["Jiaheng Liu", "Yuanxing Zhang", "Shihao Li", "Xinping Lei"], "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration", "comment": null, "summary": "For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \\textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.\n  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.", "AI": {"tldr": "\u63d0\u51faVibe AIGC\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7f16\u6392\u89e3\u51b3\u5f53\u524d\u751f\u6210\u5f0fAI\u7684\u610f\u56fe-\u6267\u884c\u5dee\u8ddd\u95ee\u9898\uff0c\u5c06\u7528\u6237\u4ece\u63d0\u793a\u5de5\u7a0b\u5e08\u8f6c\u53d8\u4e3a\u63d0\u4f9b\"\u6c1b\u56f4\"\u7684\u6307\u6325\u5b98\uff0c\u5b9e\u73b0\u4ece\u968f\u673a\u63a8\u7406\u5230\u903b\u8f91\u7f16\u6392\u7684\u8f6c\u53d8\u3002", "motivation": "\u5f53\u524d\u4ee5\u6a21\u578b\u4e3a\u4e2d\u5fc3\u3001\u4f9d\u8d56\u89c4\u6a21\u5b9a\u5f8b\u7684\u751f\u6210\u5f0fAI\u8303\u5f0f\u5b58\u5728\"\u53ef\u7528\u6027\u5929\u82b1\u677f\"\uff0c\u8868\u73b0\u4e3a\u610f\u56fe-\u6267\u884c\u5dee\u8ddd\u2014\u2014\u7528\u6237\u9ad8\u5c42\u6b21\u610f\u56fe\u4e0e\u5f53\u524d\u5355\u6b21\u751f\u6210\u6a21\u578b\u7684\u968f\u673a\u9ed1\u76d2\u6027\u8d28\u4e4b\u95f4\u7684\u6839\u672c\u6027\u5dee\u5f02\u3002", "method": "\u53d7Vibe Coding\u542f\u53d1\uff0c\u63d0\u51faVibe AIGC\u8303\u5f0f\uff1a\u7528\u6237\u4f5c\u4e3a\u6307\u6325\u5b98\u63d0\u4f9b\u5305\u542b\u5ba1\u7f8e\u504f\u597d\u3001\u529f\u80fd\u903b\u8f91\u7b49\u7684\"\u6c1b\u56f4\"\u9ad8\u5c42\u8868\u793a\uff1b\u4e2d\u592e\u5143\u89c4\u5212\u5668\u4f5c\u4e3a\u7cfb\u7edf\u67b6\u6784\u5e08\uff0c\u5c06\u6c1b\u56f4\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u3001\u53ef\u9a8c\u8bc1\u3001\u81ea\u9002\u5e94\u7684\u667a\u80fd\u4f53\u7ba1\u9053\uff1b\u901a\u8fc7\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u81ea\u4e3b\u5408\u6210\u5b9e\u73b0\u5185\u5bb9\u751f\u6210\u3002", "result": "\u901a\u8fc7\u4ece\u968f\u673a\u63a8\u7406\u5411\u903b\u8f91\u7f16\u6392\u7684\u8f6c\u53d8\uff0cVibe AIGC\u5f25\u5408\u4e86\u4eba\u7c7b\u60f3\u8c61\u529b\u4e0e\u673a\u5668\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5c06AI\u4ece\u8106\u5f31\u7684\u63a8\u7406\u5f15\u64ce\u8f6c\u53d8\u4e3a\u7a33\u5065\u7684\u7cfb\u7edf\u7ea7\u5de5\u7a0b\u4f19\u4f34\u3002", "conclusion": "\u8fd9\u79cd\u8303\u5f0f\u8f6c\u53d8\u5c06\u91cd\u65b0\u5b9a\u4e49\u4eba\u673a\u534f\u4f5c\u7ecf\u6d4e\uff0c\u4f7f\u590d\u6742\u3001\u957f\u5468\u671f\u7684\u6570\u5b57\u8d44\u4ea7\u521b\u4f5c\u6c11\u4e3b\u5316\uff0c\u4ee3\u8868\u4e86\u751f\u6210\u5f0fAI\u53d1\u5c55\u7684\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.04634", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.04634", "abs": "https://arxiv.org/abs/2602.04634", "authors": ["Zelai Xu", "Zhexuan Xu", "Ruize Zhang", "Chunyang Zhu", "Shi Yu", "Weilin Liu", "Quanlu Zhang", "Wenbo Ding", "Chao Yu", "Yu Wang"], "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.", "AI": {"tldr": "\u63d0\u51fa\u4e86WideSeek-R1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5bbd\u5ea6\u6269\u5c55\uff0c\u8ba9\u4e00\u4e2a4B\u53c2\u6570\u7684\u6a21\u578b\u5728\u5e7f\u57df\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e671B\u5355\u667a\u80fd\u4f53\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u8303\u56f4\u6269\u5927\uff0c\u74f6\u9888\u4ece\u5355\u4e2a\u667a\u80fd\u4f53\u7684\u80fd\u529b\u8f6c\u5411\u7ec4\u7ec7\u80fd\u529b\u3002\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u624b\u5de5\u5de5\u4f5c\u6d41\u548c\u987a\u5e8f\u4ea4\u4e92\uff0c\u65e0\u6cd5\u6709\u6548\u5e76\u884c\u5316\u5de5\u4f5c\u3002", "method": "\u63d0\u51faWideSeek-R1\u6846\u67b6\uff0c\u91c7\u7528\u9886\u5bfc\u667a\u80fd\u4f53-\u5b50\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f7f\u7528\u5171\u4eabLLM\u4f46\u9694\u79bb\u4e0a\u4e0b\u6587\u548c\u4e13\u7528\u5de5\u5177\uff0c\u57282\u4e07\u4e2a\u5e7f\u57df\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u4e0a\u8054\u5408\u4f18\u5316\u3002", "result": "WideSeek-R1-4B\u5728WideSearch\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f9740.0%\u7684\u9879\u76eeF1\u5206\u6570\uff0c\u4e0e\u5355\u667a\u80fd\u4f53DeepSeek-R1-671B\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u968f\u7740\u5e76\u884c\u5b50\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u5bbd\u5ea6\u6269\u5c55\u662f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u91cd\u8981\u7ef4\u5ea6\uff0c\u901a\u8fc7\u6709\u6548\u7684\u5e76\u884c\u6267\u884c\u548c\u534f\u540c\u7f16\u6392\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.04290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04290", "abs": "https://arxiv.org/abs/2602.04290", "authors": ["Lingzhuang Sun", "Ruitong Liu", "Yuxia Zhu", "Xiaohan Xu", "Jingxuan Wei", "Xiangxiang Zhang", "Bihui Yu", "Wentao Zhang"], "title": "Guided Verifier: Collaborative Multimodal Reasoning via Dynamic Process Supervision", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a pivotal mechanism for enhancing the complex reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevailing paradigms typically rely on solitary rollout strategies where the model works alone. This lack of intermediate oversight renders the reasoning process susceptible to error propagation, where early logical deviations cascade into irreversible failures, resulting in noisy optimization signals. In this paper, we propose the \\textbf{Guided Verifier} framework to address these structural limitations. Moving beyond passive terminal rewards, we introduce a dynamic verifier that actively co-solves tasks alongside the policy. During the rollout phase, this verifier interacts with the policy model in real-time, detecting inconsistencies and providing directional signals to steer the model toward valid trajectories. To facilitate this, we develop a specialized data synthesis pipeline targeting multimodal hallucinations, constructing \\textbf{CoRe} dataset of process-level negatives and \\textbf{Co}rrect-guide \\textbf{Re}asoning trajectories to train the guided verifier. Extensive experiments on MathVista, MathVerse and MMMU indicate that by allocating compute to collaborative inference and dynamic verification, an 8B-parameter model can achieve strong performance.", "AI": {"tldr": "\u63d0\u51faGuided Verifier\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9a8c\u8bc1\u5668\u4e0e\u7b56\u7565\u6a21\u578b\u5b9e\u65f6\u4ea4\u4e92\uff0c\u68c0\u6d4b\u4e0d\u4e00\u81f4\u6027\u5e76\u63d0\u4f9b\u65b9\u5411\u4fe1\u53f7\uff0c\u89e3\u51b3MLLMs\u63a8\u7406\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u7b56\u7565\u6a21\u578b\u72ec\u81ea\u63a8\u7406\uff0c\u7f3a\u4e4f\u4e2d\u95f4\u76d1\u7763\uff0c\u5bfc\u81f4\u65e9\u671f\u903b\u8f91\u504f\u5dee\u4f1a\u4f20\u64ad\u6210\u4e0d\u53ef\u9006\u7684\u5931\u8d25\uff0c\u4ea7\u751f\u566a\u58f0\u4f18\u5316\u4fe1\u53f7\u3002", "method": "\u63d0\u51faGuided Verifier\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u9a8c\u8bc1\u5668\u4e0e\u7b56\u7565\u6a21\u578b\u534f\u540c\u5de5\u4f5c\uff1b\u5f00\u53d1\u4e13\u95e8\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\u6784\u5efaCoRe\u6570\u636e\u96c6\uff0c\u5305\u542b\u8fc7\u7a0b\u7ea7\u8d1f\u9762\u6837\u672c\u548c\u6b63\u786e\u5f15\u5bfc\u7684\u63a8\u7406\u8f68\u8ff9\u6765\u8bad\u7ec3\u9a8c\u8bc1\u5668\u3002", "result": "\u5728MathVista\u3001MathVerse\u548cMMMU\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u534f\u540c\u63a8\u7406\u548c\u52a8\u6001\u9a8c\u8bc1\uff0c8B\u53c2\u6570\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7ed9\u534f\u540c\u63a8\u7406\u548c\u52a8\u6001\u9a8c\u8bc1\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u5355\u4e00\u7b56\u7565\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04813", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.04813", "abs": "https://arxiv.org/abs/2602.04813", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents", "comment": null, "summary": "Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e03\u7ef4\u5206\u7c7b\u6cd5\u6765\u7cfb\u7edf\u8bc4\u4f30\u533b\u7597\u9886\u57dfLLM\u667a\u80fd\u4f53\u7814\u7a76\uff0c\u901a\u8fc7\u5206\u679049\u9879\u7814\u7a76\u53d1\u73b0\u80fd\u529b\u5b9e\u73b0\u5b58\u5728\u660e\u663e\u4e0d\u5bf9\u79f0\u6027\uff1a\u5916\u90e8\u77e5\u8bc6\u96c6\u6210\u666e\u904d\u5b9e\u73b0\uff0c\u800c\u4e8b\u4ef6\u89e6\u53d1\u6fc0\u6d3b\u3001\u6f02\u79fb\u68c0\u6d4b\u7b49\u80fd\u529b\u4e25\u91cd\u7f3a\u5931\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u9886\u57dfLLM\u667a\u80fd\u4f53\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u6587\u732e\u591a\u4e3a\u5bbd\u6cdb\u7efc\u8ff0\u6216\u5355\u4e00\u80fd\u529b\u6df1\u5165\u63a2\u8ba8\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u7c7b\u6cd5\u6765\u8bc4\u4f30\u73b0\u6709\u7814\u7a76\u7684\u5b9e\u73b0\u72b6\u51b5\u548c\u80fd\u529b\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e03\u7ef4\u5206\u7c7b\u6cd5\uff08\u8ba4\u77e5\u80fd\u529b\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u4ea4\u4e92\u6a21\u5f0f\u3001\u9002\u5e94\u4e0e\u5b66\u4e60\u3001\u5b89\u5168\u4e0e\u4f26\u7406\u3001\u6846\u67b6\u7c7b\u578b\u3001\u6838\u5fc3\u4efb\u52a1\u4e0e\u5b50\u4efb\u52a1\uff09\uff0c\u5305\u542b29\u4e2a\u64cd\u4f5c\u5b50\u7ef4\u5ea6\u3002\u4f7f\u7528\u660e\u786e\u7eb3\u5165\u6392\u9664\u6807\u51c6\u548c\u6807\u6ce8\u89c4\u5219\uff08\u5b8c\u5168\u5b9e\u73b0\u3001\u90e8\u5206\u5b9e\u73b0\u3001\u672a\u5b9e\u73b0\uff09\uff0c\u5bf949\u9879\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6620\u5c04\u548c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u5206\u6790\u53d1\u73b0\u660e\u663e\u4e0d\u5bf9\u79f0\u6027\uff1a\u5916\u90e8\u77e5\u8bc6\u96c6\u6210\u666e\u904d\u5b9e\u73b0\uff08~76%\u5b8c\u5168\u5b9e\u73b0\uff09\uff0c\u800c\u4e8b\u4ef6\u89e6\u53d1\u6fc0\u6d3b\uff08~92%\u672a\u5b9e\u73b0\uff09\u3001\u6f02\u79fb\u68c0\u6d4b\u4e0e\u7f13\u89e3\uff08~98%\u672a\u5b9e\u73b0\uff09\u4e25\u91cd\u7f3a\u5931\u3002\u67b6\u6784\u4e0a\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u5360\u4e3b\u5bfc\uff08~82%\u5b8c\u5168\u5b9e\u73b0\uff09\uff0c\u7f16\u6392\u5c42\u591a\u4e3a\u90e8\u5206\u5b9e\u73b0\u3002\u4efb\u52a1\u65b9\u9762\u4fe1\u606f\u4e2d\u5fc3\u80fd\u529b\u9886\u5148\uff0c\u800c\u6cbb\u7597\u89c4\u5212\u4e0e\u5904\u65b9\u7b49\u884c\u52a8\u5bfc\u5411\u9886\u57df\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff08~59%\u672a\u5b9e\u73b0\uff09\u3002", "conclusion": "\u533b\u7597LLM\u667a\u80fd\u4f53\u7814\u7a76\u5728\u77e5\u8bc6\u96c6\u6210\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u65b9\u9762\u8fdb\u5c55\u826f\u597d\uff0c\u4f46\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u4e8b\u4ef6\u9a71\u52a8\u4ea4\u4e92\u548c\u884c\u52a8\u5bfc\u5411\u4efb\u52a1\u65b9\u9762\u5b58\u5728\u91cd\u5927\u7a7a\u767d\uff0c\u9700\u8981\u672a\u6765\u7814\u7a76\u91cd\u70b9\u5173\u6ce8\u3002", "topic": "agent analysis"}}
{"id": "2602.04294", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04294", "abs": "https://arxiv.org/abs/2602.04294", "authors": ["Yanshu Wang", "Shuaishuai Yang", "Jingjing He", "Tong Yang"], "title": "How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks", "comment": "13 pages, 4 figures, 6 tables", "summary": "Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0few-shot\u793a\u4f8b\u5bf9\u4e24\u79cd\u63d0\u793a\u9632\u5fa1\u7b56\u7565\u4ea7\u751f\u76f8\u53cd\u6548\u679c\uff1a\u589e\u5f3a\u89d2\u8272\u5bfc\u5411\u63d0\u793a\u7684\u5b89\u5168\u6027\uff0c\u4f46\u964d\u4f4e\u4efb\u52a1\u5bfc\u5411\u63d0\u793a\u7684\u9632\u5fa1\u6548\u679c\u3002", "motivation": "LLM\u9762\u4e34\u8d8a\u72f1\u653b\u51fb\u5a01\u80c1\uff0c\u867d\u7136\u57fa\u4e8e\u63d0\u793a\u7684\u9632\u5fa1\u7b56\u7565\uff08\u5982RoP\u548cToP\uff09\u6709\u6548\uff0c\u4f46few-shot\u6f14\u793a\u5728\u8fd9\u4e9b\u9632\u5fa1\u7b56\u7565\u4e2d\u7684\u4f5c\u7528\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5728\u591a\u4e2a\u4e3b\u6d41LLM\u4e0a\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u4f7f\u7528\u56db\u4e2a\u5b89\u5168\u57fa\u51c6\uff08AdvBench\u3001HarmBench\u3001SG-Bench\u3001XSTest\uff09\u548c\u516d\u79cd\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u5206\u6790few-shot\u5bf9RoP\u548cToP\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "few-shot\u5bf9RoP\u548cToP\u4ea7\u751f\u76f8\u53cd\u6548\u679c\uff1afew-shot\u901a\u8fc7\u5f3a\u5316\u89d2\u8272\u8eab\u4efd\u5c06RoP\u7684\u5b89\u5168\u7387\u63d0\u5347\u9ad8\u8fbe4.5%\uff0c\u800c\u901a\u8fc7\u5206\u6563\u5bf9\u4efb\u52a1\u6307\u4ee4\u7684\u6ce8\u610f\u529b\u5c06ToP\u7684\u6548\u679c\u964d\u4f4e\u9ad8\u8fbe21.2%\u3002", "conclusion": "few-shot\u6f14\u793a\u5728\u63d0\u793a\u9632\u5fa1\u7b56\u7565\u4e2d\u5177\u6709\u53cc\u91cd\u4f5c\u7528\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u9632\u5fa1\u7b56\u7565\u8c28\u614e\u4f7f\u7528\uff0c\u4e3a\u5b9e\u9645LLM\u5e94\u7528\u4e2d\u7684\u63d0\u793a\u9632\u5fa1\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u5efa\u8bae\u3002", "topic": "agent analysis"}}
{"id": "2602.04297", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04297", "abs": "https://arxiv.org/abs/2602.04297", "authors": ["Branislav Pecher", "Michal Spiegel", "Robert Belanec", "Jan Cegin"], "title": "Revisiting Prompt Sensitivity in Large Language Models for Text Classification: The Role of Prompt Underspecification", "comment": null, "summary": "Large language models (LLMs) are widely used as zero-shot and few-shot classifiers, where task behaviour is largely controlled through prompting. A growing number of works have observed that LLMs are sensitive to prompt variations, with small changes leading to large changes in performance. However, in many cases, the investigation of sensitivity is performed using underspecified prompts that provide minimal task instructions and weakly constrain the model's output space. In this work, we argue that a significant portion of the observed prompt sensitivity can be attributed to prompt underspecification. We systematically study and compare the sensitivity of underspecified prompts and prompts that provide specific instructions. Utilising performance analysis, logit analysis, and linear probing, we find that underspecified prompts exhibit higher performance variance and lower logit values for relevant tokens, while instruction-prompts suffer less from such problems. However, linear probing analysis suggests that the effects of prompt underspecification have only a marginal impact on the internal LLM representations, instead emerging in the final layers. Overall, our findings highlight the need for more rigour when investigating and mitigating prompt sensitivity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u63d0\u793a\u654f\u611f\u6027\u4e3b\u8981\u6e90\u4e8e\u63d0\u793a\u89c4\u8303\u4e0d\u8db3\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u53d1\u73b0\u89c4\u8303\u6307\u4ee4\u63d0\u793a\u80fd\u663e\u8457\u964d\u4f4e\u6027\u80fd\u65b9\u5dee\u5e76\u63d0\u9ad8\u76f8\u5173token\u7684logit\u503c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u89c2\u5bdf\u5230LLM\u5bf9\u63d0\u793a\u53d8\u5316\u654f\u611f\uff0c\u4f46\u5f88\u591a\u7814\u7a76\u4f7f\u7528\u89c4\u8303\u4e0d\u8db3\u7684\u63d0\u793a\uff08\u63d0\u4f9b\u6700\u5c0f\u4efb\u52a1\u6307\u4ee4\u548c\u5f31\u7ea6\u675f\u8f93\u51fa\u7a7a\u95f4\uff09\uff0c\u672c\u6587\u8ba4\u4e3a\u89c2\u5bdf\u5230\u7684\u654f\u611f\u6027\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53ef\u5f52\u56e0\u4e8e\u63d0\u793a\u89c4\u8303\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u5bf9\u6bd4\u89c4\u8303\u4e0d\u8db3\u63d0\u793a\u548c\u63d0\u4f9b\u5177\u4f53\u6307\u4ee4\u7684\u63d0\u793a\u7684\u654f\u611f\u6027\uff0c\u91c7\u7528\u6027\u80fd\u5206\u6790\u3001logit\u5206\u6790\u548c\u7ebf\u6027\u63a2\u6d4b\u4e09\u79cd\u65b9\u6cd5\u3002", "result": "\u89c4\u8303\u4e0d\u8db3\u63d0\u793a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\u65b9\u5dee\u548c\u76f8\u5173token\u7684\u66f4\u4f4elogit\u503c\uff0c\u800c\u6307\u4ee4\u63d0\u793a\u8f83\u5c11\u53d7\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\uff1b\u7ebf\u6027\u63a2\u6d4b\u8868\u660e\u63d0\u793a\u89c4\u8303\u4e0d\u8db3\u7684\u5f71\u54cd\u4e3b\u8981\u51fa\u73b0\u5728\u6700\u540e\u51e0\u5c42\uff0c\u5bf9\u5185\u90e8\u8868\u793a\u5f71\u54cd\u6709\u9650\u3002", "conclusion": "\u63d0\u793a\u89c4\u8303\u4e0d\u8db3\u662fLLM\u63d0\u793a\u654f\u611f\u6027\u7684\u91cd\u8981\u6765\u6e90\uff0c\u7814\u7a76\u63d0\u793a\u654f\u611f\u6027\u65f6\u9700\u8981\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c4\u8303\u6307\u4ee4\u63d0\u793a\u53ef\u4ee5\u51cf\u5c11\u654f\u611f\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.04837", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04837", "abs": "https://arxiv.org/abs/2602.04837", "authors": ["Zhaotian Weng", "Antonis Antoniades", "Deepak Nathani", "Zhen Zhang", "Xiao Pu", "Xin Eric Wang"], "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing", "comment": "18 pages", "summary": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.", "AI": {"tldr": "GEA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u8fdb\u5316\u7684\u5f00\u653e\u81ea\u6539\u8fdb\u4ee3\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u7fa4\u4f53\u4f5c\u4e3a\u57fa\u672c\u8fdb\u5316\u5355\u5143\u5b9e\u73b0\u7ecf\u9a8c\u5171\u4eab\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u81ea\u8fdb\u5316\u8303\u5f0f\u91c7\u7528\u6811\u72b6\u7ed3\u6784\u8fdb\u5316\uff0c\u5b58\u5728\u8fdb\u5316\u5206\u652f\u5b64\u7acb\u5bfc\u81f4\u7684\u63a2\u7d22\u591a\u6837\u6027\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u6539\u8fdb\u673a\u5236\u3002", "method": "\u63d0\u51fa\u7fa4\u4f53\u8fdb\u5316\u4ee3\u7406(GEA)\u8303\u5f0f\uff0c\u4ee5\u4ee3\u7406\u7fa4\u4f53\u4e3a\u57fa\u672c\u8fdb\u5316\u5355\u5143\uff0c\u5728\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u663e\u5f0f\u7684\u7ecf\u9a8c\u5171\u4eab\u548c\u91cd\u7528\uff0c\u514b\u670d\u5b64\u7acb\u8fdb\u5316\u5206\u652f\u7684\u9650\u5236\u3002", "result": "\u5728\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u8fdb\u5316\u65b9\u6cd5(SWE-bench Verified: 71.0% vs 56.7%\uff1bPolyglot: 88.3% vs 68.3%)\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u7684\u9876\u7ea7\u4ee3\u7406\u6846\u67b6\u3002", "conclusion": "GEA\u80fd\u66f4\u6709\u6548\u5730\u5c06\u65e9\u671f\u63a2\u7d22\u591a\u6837\u6027\u8f6c\u5316\u4e3a\u6301\u7eed\u957f\u671f\u8fdb\u6b65\uff0c\u5728\u76f8\u540c\u8fdb\u5316\u4ee3\u7406\u6570\u91cf\u4e0b\u83b7\u5f97\u66f4\u5f3a\u6027\u80fd\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u8de8\u6a21\u578b\u53ef\u8fc1\u79fb\u6027\u548c\u9c81\u68d2\u6027\u3002", "topic": "code agent"}}
{"id": "2602.04355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04355", "abs": "https://arxiv.org/abs/2602.04355", "authors": ["Sichu Liang", "Hongyu Zhu", "Wenwen Wang", "Deyu Zhou"], "title": "Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models", "comment": null, "summary": "Working memory is a central component of intelligent behavior, providing a dynamic workspace for maintaining and updating task-relevant information. Recent work has used n-back tasks to probe working-memory-like behavior in large language models, but it is unclear whether the same probe elicits comparable computations when information is carried in a visual rather than textual code in vision-language models. We evaluate Qwen2.5 and Qwen2.5-VL on a controlled spatial n-back task presented as matched text-rendered or image-rendered grids. Across conditions, models show reliably higher accuracy and d' with text than with vision. To interpret these differences at the process level, we use trial-wise log-probability evidence and find that nominal 2/3-back often fails to reflect the instructed lag and instead aligns with a recency-locked comparison. We further show that grid size alters recent-repeat structure in the stimulus stream, thereby changing interference and error patterns. These results motivate computation-sensitive interpretations of multimodal working memory.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4n-back\u5de5\u4f5c\u8bb0\u5fc6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6587\u672c\u8f93\u5165\u6bd4\u89c6\u89c9\u8f93\u5165\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u6a21\u578b\u5f80\u5f80\u91c7\u7528\u8fd1\u56e0\u6548\u5e94\u800c\u975e\u6307\u4ee4\u8981\u6c42\u7684\u5ef6\u8fdf\u5339\u914d\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u548c\u6587\u672c\u4e24\u79cd\u6a21\u6001\u4e0b\u662f\u5426\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u5de5\u4f5c\u8bb0\u5fc6\u8ba1\u7b97\u8fc7\u7a0b\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u4f7f\u7528n-back\u4efb\u52a1\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4f5c\u8bb0\u5fc6\u884c\u4e3a\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5f53\u4fe1\u606f\u4ee5\u89c6\u89c9\u800c\u975e\u6587\u672c\u5f62\u5f0f\u5448\u73b0\u65f6\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u53ef\u6bd4\u7684\u8ba1\u7b97\u8fc7\u7a0b\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u8bc4\u4f30Qwen2.5\u548cQwen2.5-VL\u6a21\u578b\u5728\u53d7\u63a7\u7a7a\u95f4n-back\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1b2\uff09\u5c06\u4efb\u52a1\u5448\u73b0\u4e3a\u5339\u914d\u7684\u6587\u672c\u6e32\u67d3\u6216\u56fe\u50cf\u6e32\u67d3\u7f51\u683c\uff1b3\uff09\u4f7f\u7528\u8bd5\u6b21\u7ea7\u522b\u7684\u5bf9\u6570\u6982\u7387\u8bc1\u636e\u6765\u89e3\u91ca\u8fc7\u7a0b\u5c42\u9762\u7684\u5dee\u5f02\uff1b4\uff09\u5206\u6790\u7f51\u683c\u5927\u5c0f\u5982\u4f55\u6539\u53d8\u523a\u6fc0\u6d41\u4e2d\u7684\u8fd1\u671f\u91cd\u590d\u7ed3\u6784\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5728\u6240\u6709\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u5728\u6587\u672c\u8f93\u5165\u4e0a\u7684\u51c6\u786e\u7387\u548cd'\u503c\u90fd\u663e\u8457\u9ad8\u4e8e\u89c6\u89c9\u8f93\u5165\uff1b2\uff09\u540d\u4e49\u4e0a\u76842/3-back\u4efb\u52a1\u5f80\u5f80\u65e0\u6cd5\u53cd\u6620\u6307\u4ee4\u8981\u6c42\u7684\u5ef6\u8fdf\uff0c\u800c\u662f\u4e0e\u8fd1\u56e0\u9501\u5b9a\u7684\u6bd4\u8f83\u4e00\u81f4\uff1b3\uff09\u7f51\u683c\u5927\u5c0f\u6539\u53d8\u4e86\u523a\u6fc0\u6d41\u4e2d\u7684\u8fd1\u671f\u91cd\u590d\u7ed3\u6784\uff0c\u4ece\u800c\u5f71\u54cd\u4e86\u5e72\u6270\u548c\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u5f3a\u8c03\u4e86\u9700\u8981\u5bf9\u591a\u6a21\u6001\u5de5\u4f5c\u8bb0\u5fc6\u8fdb\u884c\u8ba1\u7b97\u654f\u611f\u7684\u89e3\u91ca\u3002\u7ed3\u679c\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4e0b\u7684\u5de5\u4f5c\u8bb0\u5fc6\u5904\u7406\u5b58\u5728\u5dee\u5f02\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u91c7\u7528\u8fd1\u56e0\u7b56\u7565\u800c\u975e\u9075\u5faa\u6307\u4ee4\u5ef6\u8fdf\uff0c\u8fd9\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u8ba4\u77e5\u7cfb\u7edf\u7684\u5de5\u4f5c\u8bb0\u5fc6\u673a\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.04074", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04074", "abs": "https://arxiv.org/abs/2602.04074", "authors": ["Julius Fridriksson", "Roger D. Newman-Norlund", "Saeed Ahmadi", "Regan Willis", "Nadra Salman", "Kalil Warren", "Xiang Guan", "Yong Yang", "Srihari Nelakuditi", "Rutvik Desai", "Leonardo Bonilha", "Jeff Charney", "Chris Rorden"], "title": "Stroke Lesions as a Rosetta Stone for Language Model Interpretability", "comment": "45 pages, 17 figures", "summary": "Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.", "AI": {"tldr": "\u63d0\u51faBLUM\u6846\u67b6\uff0c\u5229\u7528\u8111\u635f\u4f24-\u75c7\u72b6\u6620\u5c04\u4f5c\u4e3a\u5916\u90e8\u9a8c\u8bc1\u6807\u51c6\uff0c\u8bc4\u4f30LLM\u6270\u52a8\u6548\u679c\uff0c\u53d1\u73b0LLM\u9519\u8bef\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u8111\u635f\u4f24\u6a21\u5f0f\u6709\u663e\u8457\u5bf9\u5e94\u5173\u7cfb", "motivation": "\u5f53\u524dLLM\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u4f9d\u8d56\u5185\u90e8\u6307\u6807\uff0c\u7f3a\u4e4f\u5916\u90e8\u9a8c\u8bc1\u3002\u9700\u8981\u5efa\u7acb\u56e0\u679c\u9a8c\u8bc1\u6846\u67b6\u6765\u8bc4\u4f30\u54ea\u4e9b\u6a21\u578b\u7ec4\u4ef6\u771f\u6b63\u5bf9\u8bed\u8a00\u529f\u80fd\u5fc5\u8981", "method": "\u4f7f\u7528410\u540d\u4e2d\u98ce\u540e\u5931\u8bed\u75c7\u60a3\u8005\u6570\u636e\uff0c\u8bad\u7ec3\u75c7\u72b6-\u635f\u4f24\u6a21\u578b\uff1b\u5bf9transformer\u5c42\u8fdb\u884c\u7cfb\u7edf\u6270\u52a8\uff1b\u5bf9\u6270\u52a8\u540e\u7684LLM\u548c\u4eba\u7c7b\u60a3\u8005\u8fdb\u884c\u76f8\u540c\u4e34\u5e8a\u8bc4\u4f30\uff1b\u5c06LLM\u9519\u8bef\u6a21\u5f0f\u6295\u5f71\u5230\u4eba\u7c7b\u635f\u4f24\u7a7a\u95f4", "result": "LLM\u9519\u8bef\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u9519\u8bef\u6a21\u5f0f\u8db3\u591f\u76f8\u4f3c\uff0c\u9884\u6d4b\u7684\u635f\u4f24\u4f4d\u7f6e\u572867%\u56fe\u7247\u547d\u540d\u6761\u4ef6\u548c68.3%\u53e5\u5b50\u5b8c\u6210\u6761\u4ef6\u4e0b\u4e0e\u5b9e\u9645\u4eba\u7c7b\u635f\u4f24\u4f4d\u7f6e\u5bf9\u5e94\uff08\u663e\u8457\u9ad8\u4e8e\u968f\u673a\uff09\uff1b\u8bed\u4e49\u4e3b\u5bfc\u9519\u8bef\u5bf9\u5e94\u8179\u4fa7\u901a\u8def\u635f\u4f24\uff0c\u97f3\u4f4d\u4e3b\u5bfc\u9519\u8bef\u5bf9\u5e94\u80cc\u4fa7\u901a\u8def\u635f\u4f24", "conclusion": "\u4e3aLLM\u53ef\u89e3\u91ca\u6027\u5f00\u8f9f\u4e86\u65b0\u65b9\u6cd5\u8def\u5f84\uff0c\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u63d0\u4f9b\u5916\u90e8\u9a8c\u8bc1\uff0c\u4eba\u7c7b\u635f\u4f24-\u75c7\u72b6\u6620\u5c04\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u4eba\u5de5\u8bed\u8a00\u7cfb\u7edf\u7684\u53c2\u8003\u6846\u67b6\uff0c\u884c\u4e3a\u5bf9\u9f50\u53ef\u80fd\u53cd\u6620\u5171\u4eab\u8ba1\u7b97\u539f\u7406", "topic": "agent analysis"}}
{"id": "2602.04428", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04428", "abs": "https://arxiv.org/abs/2602.04428", "authors": ["Zijian Feng", "Tianjiao Li", "Zixiao Zhu", "Hanzhang Zhou", "Junlang Qian", "Li Zhang", "Jia Jim Deryl Chua", "Lee Onn Mak", "Gee Wah Ng", "Kezhi Mao"], "title": "Fine-Grained Activation Steering: Steering Less, Achieving More", "comment": "ICLR 2026", "summary": "Activation steering has emerged as a cost-effective paradigm for modifying large language model (LLM) behaviors. Existing methods typically intervene at the block level, steering the bundled activations of selected attention heads, feedforward networks, or residual streams. However, we reveal that block-level activations are inherently heterogeneous, entangling beneficial, irrelevant, and harmful features, thereby rendering block-level steering coarse, inefficient, and intrusive. To investigate the root cause, we decompose block activations into fine-grained atomic unit (AU)-level activations, where each AU-level activation corresponds to a single dimension of the block activation, and each AU denotes a slice of the block weight matrix. Steering an AU-level activation is thus equivalent to steering its associated AU. Our theoretical and empirical analysis show that heterogeneity arises because different AUs or dimensions control distinct token distributions in LLM outputs. Hence, block-level steering inevitably moves helpful and harmful token directions together, which reduces efficiency. Restricting intervention to beneficial AUs yields more precise and effective steering. Building on this insight, we propose AUSteer, a simple and efficient method that operates at a finer granularity of the AU level. AUSteer first identifies discriminative AUs globally by computing activation momenta on contrastive samples. It then assigns adaptive steering strengths tailored to diverse inputs and selected AU activations. Comprehensive experiments on multiple LLMs and tasks show that AUSteer consistently surpasses advanced baselines while steering considerably fewer activations, demonstrating that steering less achieves more.", "AI": {"tldr": "AUSteer\uff1a\u4e00\u79cd\u5728\u539f\u5b50\u5355\u5143\u7ea7\u522b\u8fdb\u884c\u6fc0\u6d3b\u5f15\u5bfc\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u533a\u5206\u6027\u539f\u5b50\u5355\u5143\u5e76\u5206\u914d\u81ea\u9002\u5e94\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u9ad8\u6548\u7684LLM\u884c\u4e3a\u4fee\u6539", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u901a\u5e38\u5728\u5757\u7ea7\u522b\uff08\u6ce8\u610f\u529b\u5934\u3001\u524d\u9988\u7f51\u7edc\u6216\u6b8b\u5dee\u6d41\uff09\u8fdb\u884c\u5e72\u9884\uff0c\u4f46\u5757\u7ea7\u6fc0\u6d3b\u672c\u8d28\u4e0a\u662f\u5f02\u8d28\u7684\uff0c\u6df7\u5408\u4e86\u6709\u76ca\u3001\u65e0\u5173\u548c\u6709\u5bb3\u7279\u5f81\uff0c\u5bfc\u81f4\u5f15\u5bfc\u7c97\u7cd9\u3001\u4f4e\u6548\u4e14\u4fb5\u5165\u6027\u5f3a", "method": "\u5c06\u5757\u6fc0\u6d3b\u5206\u89e3\u4e3a\u539f\u5b50\u5355\u5143\u7ea7\u6fc0\u6d3b\uff0c\u6bcf\u4e2aAU\u5bf9\u5e94\u5757\u6fc0\u6d3b\u7684\u5355\u4e2a\u7ef4\u5ea6\uff1b\u9996\u5148\u901a\u8fc7\u5bf9\u6bd4\u6837\u672c\u8ba1\u7b97\u6fc0\u6d3b\u52a8\u91cf\u6765\u5168\u5c40\u8bc6\u522b\u533a\u5206\u6027AU\uff0c\u7136\u540e\u4e3a\u4e0d\u540c\u8f93\u5165\u548c\u9009\u5b9aAU\u6fc0\u6d3b\u5206\u914d\u81ea\u9002\u5e94\u5f15\u5bfc\u5f3a\u5ea6", "result": "\u5728\u591a\u4e2aLLM\u548c\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cAUSteer\u59cb\u7ec8\u4f18\u4e8e\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5f15\u5bfc\u7684\u6fc0\u6d3b\u6570\u91cf\u663e\u8457\u51cf\u5c11\uff0c\u5b9e\u73b0\u4e86\"\u5f15\u5bfc\u66f4\u5c11\uff0c\u6548\u679c\u66f4\u597d\"", "conclusion": "\u5757\u7ea7\u6fc0\u6d3b\u7684\u5f02\u8d28\u6027\u6e90\u4e8e\u4e0d\u540c\u539f\u5b50\u5355\u5143\u63a7\u5236LLM\u8f93\u51fa\u4e2d\u4e0d\u540c\u7684token\u5206\u5e03\uff0c\u9650\u5236\u5e72\u9884\u5230\u6709\u76caAU\u53ef\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u6709\u6548\u7684\u5f15\u5bfc\uff0cAU\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5f15\u5bfc\u4f18\u4e8e\u5757\u7ea7\u5f15\u5bfc", "topic": "agent analysis"}}
{"id": "2602.04096", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04096", "abs": "https://arxiv.org/abs/2602.04096", "authors": ["Kevin Zhai", "Sabbir Mollah", "Zhenyi Wang", "Mubarak Shah"], "title": "CoRe: Context-Robust Remasking for Diffusion Language Models", "comment": null, "summary": "Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.", "AI": {"tldr": "CoRe\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u4fee\u8ba2\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u6d4b\u6807\u8bb0\u5bf9\u4e0a\u4e0b\u6587\u6270\u52a8\u7684\u654f\u611f\u6027\u6765\u8bc6\u522b\u4e0a\u4e0b\u6587\u8106\u5f31\u7684\u6807\u8bb0\uff0c\u800c\u975e\u4f9d\u8d56\u9759\u6001\u7f6e\u4fe1\u5ea6\uff0c\u4ece\u800c\u6539\u5584\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u89e3\u7801\u8d28\u91cf\u3002", "motivation": "\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u6807\u51c6\u89e3\u7801\u5b58\u5728\u4e0a\u4e0b\u6587\u521a\u6027\u95ee\u9898\uff1a\u6807\u8bb0\u57fa\u4e8e\u77ac\u65f6\u9ad8\u7f6e\u4fe1\u5ea6\u88ab\u4fdd\u7559\uff0c\u5ffd\u7565\u4e86\u65e9\u671f\u9884\u6d4b\u7f3a\u4e4f\u5b8c\u6574\u4e0a\u4e0b\u6587\u3002\u8fd9\u5bfc\u81f4\u7ea7\u8054\u6548\u5e94\uff0c\u521d\u59cb\u4e0d\u4e00\u81f4\u6027\u8bef\u5bfc\u540e\u7eed\u751f\u6210\u3002\u73b0\u6709\u4fee\u8ba2\u7b56\u7565\u4f9d\u8d56\u9759\u6001\u7f6e\u4fe1\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u53f7\u672c\u8d28\u4e0a\u662f\u77ed\u89c6\u7684\uff0c\u4e0d\u4e00\u81f4\u7684\u6807\u8bb0\u53ef\u80fd\u5bf9\u6a21\u578b\u672c\u8eab\u663e\u5f97\u5f88\u81ea\u4fe1\u3002", "method": "\u63d0\u51faContext-Robust Remasking (CoRe)\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u6d4b\u6807\u8bb0\u5bf9\u9488\u5bf9\u6027\u63a9\u7801\u4e0a\u4e0b\u6587\u6270\u52a8\u7684\u654f\u611f\u6027\u6765\u8bc6\u522b\u4e0a\u4e0b\u6587\u8106\u5f31\u7684\u6807\u8bb0\uff0c\u5c06\u4fee\u8ba2\u5f62\u5f0f\u5316\u4e3a\u5bf9\u4e0a\u4e0b\u6587\u504f\u79fb\u7684\u9c81\u68d2\u4f18\u5316\u76ee\u6807\uff0c\u5e76\u9ad8\u6548\u8fd1\u4f3c\u8be5\u76ee\u6807\u4ee5\u4f18\u5148\u4fee\u8ba2\u4e0d\u7a33\u5b9a\u6807\u8bb0\u3002", "result": "\u5728LLaDA-8B-Base\u6a21\u578b\u4e0a\uff0cCoRe\u5728\u63a8\u7406\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\uff0c\u4f18\u4e8e\u8ba1\u7b97\u5339\u914d\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c06MBPP\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe9.2\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CoRe\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u6807\u8bb0\u5bf9\u4e0a\u4e0b\u6587\u6270\u52a8\u7684\u654f\u611f\u6027\u800c\u975e\u4f9d\u8d56\u9759\u6001\u7f6e\u4fe1\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u521a\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u63a8\u7406\u65f6\u4fee\u8ba2\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2602.04131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04131", "abs": "https://arxiv.org/abs/2602.04131", "authors": ["Mehrdad Moghimi", "Anthony Coache", "Hyejin Ku"], "title": "Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting", "comment": null, "summary": "Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u7075\u6d3b\u6298\u6263\u672a\u6765\u5956\u52b1\u548c\u4f18\u5316\u98ce\u9669\u5ea6\u91cf\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b0\u6846\u67b6\uff0c\u5f3a\u8c03\u6298\u6263\u56e0\u5b50\u5728\u6355\u6349\u65f6\u95f4\u504f\u597d\u548c\u98ce\u9669\u504f\u597d\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "motivation": "\u4f20\u7edf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u4e2d\u6298\u6263\u56e0\u5b50\u901a\u5e38\u88ab\u89c6\u4e3a\u56fa\u5b9a\u53c2\u6570\u6216\u53ef\u8c03\u8d85\u53c2\u6570\uff0c\u5f88\u5c11\u8003\u8651\u5176\u5bf9\u5b66\u4e60\u7b56\u7565\u7684\u5f71\u54cd\u3002\u6307\u6570\u6298\u6263\u56e0\u5b50\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u667a\u80fd\u4f53\u7684\u65f6\u95f4\u504f\u597d\uff0c\u800c\u6298\u6263\u51fd\u6570\u5728\u8868\u5f81\u65f6\u95f4\u504f\u597d\u65b9\u9762\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u652f\u6301\u7075\u6d3b\u6298\u6263\u672a\u6765\u5956\u52b1\u548c\u4f18\u5316\u98ce\u9669\u5ea6\u91cf\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b0\u6846\u67b6\uff0c\u63d0\u4f9b\u7b97\u6cd5\u6700\u4f18\u6027\u7684\u6280\u672f\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u591a\u65f6\u95f4\u8303\u56f4\u6269\u5c55\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u7ed3\u679c\u8868\u660e\u6298\u6263\u662f\u51b3\u7b56\u95ee\u9898\u4e2d\u6355\u6349\u66f4\u4e30\u5bcc\u65f6\u95f4\u548c\u98ce\u9669\u504f\u597d\u7279\u5f81\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u6298\u6263\u662f\u51b3\u7b56\u95ee\u9898\u4e2d\u6355\u6349\u66f4\u4e30\u5bcc\u65f6\u95f4\u548c\u98ce\u9669\u504f\u597d\u7279\u5f81\u7684\u57fa\u7840\uff0c\u5bf9\u73b0\u5b9e\u4e16\u754c\u5b89\u5168\u5173\u952e\u5e94\u7528\u5177\u6709\u6f5c\u5728\u5f71\u54cd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04166", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04166", "abs": "https://arxiv.org/abs/2602.04166", "authors": ["Meiling Jin", "Fei Wang", "Xiaoyun Yuan", "Chen Qian", "Yuan Cheng"], "title": "Topology-Aware Revival for Efficient Sparse Training", "comment": null, "summary": "Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%.", "AI": {"tldr": "TAR\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4e00\u6b21\u6027\u540e\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7684\u590d\u6d3b\u6b65\u9aa4\u6539\u8fdb\u9759\u6001\u7a00\u758f\u8bad\u7ec3\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9759\u6001\u7a00\u758f\u8bad\u7ec3\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u56fa\u5b9a\u63a9\u7801\u6a21\u5f0f\u964d\u4f4e\u4e86\u9c81\u68d2\u6027\u3002\u65e9\u671f\u526a\u679d\u51b3\u7b56\u4f1a\u5c06\u7f51\u7edc\u9501\u5b9a\u5728\u8106\u5f31\u7ed3\u6784\u4e2d\uff0c\u96be\u4ee5\u8c03\u6574\uff0c\u7279\u522b\u662f\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u7b56\u7565\u7684\u4e0d\u65ad\u6f14\u53d8\u4f1a\u6301\u7eed\u6539\u53d8\u8bad\u7ec3\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u62d3\u6251\u611f\u77e5\u590d\u6d3b(TAR)\uff1a1) \u9759\u6001\u526a\u679d\u540e\u6267\u884c\u4e00\u6b21\u6027\u590d\u6d3b\u6b65\u9aa4\uff1b2) \u6839\u636e\u62d3\u6251\u9700\u6c42\u5728\u5404\u5c42\u5206\u914d\u5c0f\u91cf\u9884\u7559\u9884\u7b97\uff1b3) \u6bcf\u5c42\u968f\u673a\u5747\u5300\u5730\u91cd\u65b0\u6fc0\u6d3b\u5c11\u91cf\u5148\u524d\u526a\u679d\u7684\u8fde\u63a5\uff1b4) \u4fdd\u6301\u6700\u7ec8\u8fde\u63a5\u6a21\u5f0f\u56fa\u5b9a\u8fdb\u884c\u540e\u7eed\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528SAC\u548cTD3\u7b97\u6cd5\uff0cTAR\u76f8\u6bd4\u9759\u6001\u7a00\u758f\u57fa\u7ebf\u6700\u7ec8\u56de\u62a5\u63d0\u5347\u9ad8\u8fbe+37.9%\uff0c\u76f8\u6bd4\u52a8\u6001\u7a00\u758f\u8bad\u7ec3\u57fa\u7ebf\u4e2d\u4f4d\u6570\u589e\u76ca\u4e3a+13.5%\u3002", "conclusion": "TAR\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u4e00\u6b21\u6027\u62d3\u6251\u611f\u77e5\u590d\u6d3b\uff0c\u6709\u6548\u6539\u5584\u4e86\u9759\u6001\u7a00\u758f\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04587", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.04587", "abs": "https://arxiv.org/abs/2602.04587", "authors": ["Jaeyoon Jung", "Yejun Yoon", "Seunghyun Yoon", "Kunwoo Park"], "title": "VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration", "comment": "A system description paper for the AVerImaTeC shared task at the Ninth FEVER Workshop (co-located with EACL 2026)", "summary": "This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.", "AI": {"tldr": "VILLAIN\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u9a8c\u8bc1\u56fe\u50cf-\u6587\u672c\u58f0\u660e\uff0c\u5728AVerImaTeC\u5171\u4eab\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "motivation": "\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u9a8c\u8bc1\u56fe\u50cf-\u6587\u672c\u58f0\u660e\u7684\u591a\u6a21\u6001\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728AVerImaTeC\u5171\u4eab\u4efb\u52a1\u4e2d\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\uff1a1) \u4ece\u77e5\u8bc6\u5e93\u68c0\u7d22\u6587\u672c\u548c\u89c6\u89c9\u8bc1\u636e\uff1b2) \u6a21\u6001\u7279\u5b9a\u548c\u8de8\u6a21\u6001\u667a\u80fd\u4f53\u751f\u6210\u5206\u6790\u62a5\u544a\uff1b3) \u57fa\u4e8e\u62a5\u544a\u751f\u6210\u95ee\u7b54\u5bf9\uff1b4) \u6700\u7ec8\u9884\u6d4b\u667a\u80fd\u4f53\u57fa\u4e8e\u58f0\u660e\u548c\u95ee\u7b54\u5bf9\u4ea7\u751f\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u5728AVerImaTeC\u5171\u4eab\u4efb\u52a1\u7684\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "VILLAIN\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.04224", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.04224", "abs": "https://arxiv.org/abs/2602.04224", "authors": ["Zeming Wei", "Qiaosheng Zhang", "Xia Hu", "Xingcheng Xu"], "title": "RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning", "comment": null, "summary": "Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs' safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.", "AI": {"tldr": "RAPO\u6846\u67b6\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u504f\u597d\u4f18\u5316\uff0c\u8ba9\u5927\u578b\u63a8\u7406\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u8bc6\u522b\u548c\u5904\u7406\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u5347\u5bf9\u590d\u6742\u8d8a\u72f1\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u867d\u7136\u5177\u5907\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u4e0e\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u7c7b\u4f3c\u7684\u5b89\u5168\u95ee\u9898\u3002\u73b0\u6709\u7684\u5b89\u5168\u62d2\u7edd\u673a\u5236\u5728\u9762\u5bf9\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u8d8a\u72f1\u653b\u51fb\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5145\u5206\u7684\u5b89\u5168\u63a8\u7406\u8fc7\u7a0b\u6765\u9632\u5fa1\u9ad8\u7ea7\u653b\u51fb\u63d0\u793a", "method": "\u63d0\u51fa\u98ce\u9669\u611f\u77e5\u504f\u597d\u4f18\u5316(RAPO)\u6846\u67b6\uff0c\u4f7fLRM\u80fd\u591f\u81ea\u9002\u5e94\u5730\u8bc6\u522b\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u5728\u5176\u601d\u7ef4\u5185\u5bb9\u4e2d\u4ee5\u9002\u5f53\u7684\u7c92\u5ea6\u5904\u7406\u8fd9\u4e9b\u98ce\u9669\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc1\u636e\u652f\u6301\u66f4\u5145\u5206\u7684\u5b89\u5168\u63a8\u7406\u8fc7\u7a0b\u7684\u5fc5\u8981\u6027", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRAPO\u6210\u529f\u6cdb\u5316\u4e86\u591a\u4e2aLRM\u7684\u5b89\u5168\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5e94\u5bf9\u591a\u6837\u5316\u7684\u653b\u51fb\u63d0\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u4e00\u822c\u6548\u7528\uff0c\u4e3aLRM\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u5bf9\u9f50\u6280\u672f", "conclusion": "RAPO\u6846\u67b6\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u7684\u504f\u597d\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5bf9\u590d\u6742\u8d8a\u72f1\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u80fd\uff0c\u4e3aLRM\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2602.04380", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04380", "abs": "https://arxiv.org/abs/2602.04380", "authors": ["Rui Yuan", "Mykola Khandoga", "Vinay Kumar Sankarapu"], "title": "Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning", "comment": null, "summary": "Policy optimization methods like Group Relative Policy Optimization (GRPO) and its variants have achieved strong results on mathematical reasoning and code generation tasks. Despite extensive exploration of reward processing strategies and training dynamics, all existing group-based methods exclusively use KL divergence for policy regularization, leaving the choice of divergence function unexplored. We introduce Group-Based Mirror Policy Optimization (GBMPO), a framework that extends group-based policy optimization to flexible Bregman divergences, including hand-designed alternatives (L2 in probability space) and learned neural mirror maps. On GSM8K mathematical reasoning, hand-designed ProbL2-GRPO achieves 86.7% accuracy, improving +5.5 points over the Dr. GRPO baseline. On MBPP code generation, neural mirror maps reach 60.1-60.8% pass@1, with random initialization already capturing most of the benefit. While evolutionary strategies meta-learning provides marginal accuracy improvements, its primary value lies in variance reduction ($\\pm$0.2 versus $\\pm$0.6) and efficiency gains (15% shorter responses on MBPP), suggesting that random initialization of neural mirror maps is sufficient for most practical applications. These results establish divergence choice as a critical, previously unexplored design dimension in group-based policy optimization for LLM reasoning.", "AI": {"tldr": "GBMPO\u6269\u5c55\u4e86\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5f15\u5165\u7075\u6d3b\u7684Bregman\u6563\u5ea6\uff08\u5305\u62ec\u624b\u5de5\u8bbe\u8ba1\u7684\u6982\u7387\u7a7a\u95f4L2\u6563\u5ea6\u548c\u5b66\u4e60\u7684\u795e\u7ecf\u955c\u50cf\u6620\u5c04\uff09\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528KL\u6563\u5ea6\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff08\u5982GRPO\u53ca\u5176\u53d8\u4f53\uff09\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u90fd\u53ea\u4f7f\u7528KL\u6563\u5ea6\u8fdb\u884c\u7b56\u7565\u6b63\u5219\u5316\uff0c\u6563\u5ea6\u51fd\u6570\u7684\u9009\u62e9\u8fd9\u4e00\u8bbe\u8ba1\u7ef4\u5ea6\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51faGroup-Based Mirror Policy Optimization (GBMPO)\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u6269\u5c55\u5230\u7075\u6d3b\u7684Bregman\u6563\u5ea6\uff0c\u5305\u62ec\u624b\u5de5\u8bbe\u8ba1\u7684\u6982\u7387\u7a7a\u95f4L2\u6563\u5ea6\uff08ProbL2\uff09\u548c\u5b66\u4e60\u7684\u795e\u7ecf\u955c\u50cf\u6620\u5c04\u3002\u4f7f\u7528\u8fdb\u5316\u7b56\u7565\u5143\u5b66\u4e60\u6765\u4f18\u5316\u795e\u7ecf\u955c\u50cf\u6620\u5c04\u3002", "result": "\u5728GSM8K\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u624b\u5de5\u8bbe\u8ba1\u7684ProbL2-GRPO\u8fbe\u523086.7%\u51c6\u786e\u7387\uff0c\u6bd4Dr. GRPO\u57fa\u7ebf\u63d0\u53475.5\u4e2a\u767e\u5206\u70b9\u3002\u5728MBPP\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\uff0c\u795e\u7ecf\u955c\u50cf\u6620\u5c04\u8fbe\u523060.1-60.8% pass@1\uff0c\u968f\u673a\u521d\u59cb\u5316\u5df2\u80fd\u83b7\u5f97\u5927\u90e8\u5206\u6536\u76ca\u3002\u8fdb\u5316\u7b56\u7565\u5143\u5b66\u4e60\u4e3b\u8981\u5e26\u6765\u65b9\u5dee\u51cf\u5c11\uff08\u00b10.2 vs \u00b10.6\uff09\u548c\u6548\u7387\u63d0\u5347\uff08MBPP\u4e0a\u54cd\u5e94\u7f29\u77ed15%\uff09\u3002", "conclusion": "\u6563\u5ea6\u51fd\u6570\u7684\u9009\u62e9\u662f\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u4e2d\u4e00\u4e2a\u5173\u952e\u4e14\u5148\u524d\u672a\u88ab\u63a2\u7d22\u7684\u8bbe\u8ba1\u7ef4\u5ea6\u3002\u968f\u673a\u521d\u59cb\u5316\u795e\u7ecf\u955c\u50cf\u6620\u5c04\u5bf9\u5927\u591a\u6570\u5b9e\u9645\u5e94\u7528\u5df2\u8db3\u591f\uff0c\u8fdb\u5316\u7b56\u7565\u5143\u5b66\u4e60\u4e3b\u8981\u63d0\u4f9b\u65b9\u5dee\u51cf\u5c11\u548c\u6548\u7387\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04755", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04755", "abs": "https://arxiv.org/abs/2602.04755", "authors": ["Xinyu Zhou", "Chang Jin", "Carsten Eickhoff", "Zhijiang Guo", "Seyed Ali Bahrainian"], "title": "When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?", "comment": "Accepted to ICLR2026", "summary": "Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\\%$ and $5.80\\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u65f6\u95f4\u95ee\u7b54\u4efb\u52a1\u4e2d\u8bad\u7ec3LLMs\u5177\u5907\u5f03\u6743\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u601d\u7ef4\u94fe\u76d1\u7763\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u53ef\u56de\u7b54\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u6027\u548c\u5728\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u4e0a\u7684\u5f03\u6743\u80fd\u529b\u3002", "motivation": "LLMs\u5728\u65f6\u95f4\u654f\u611f\u95ee\u9898\u4e0a\u7ecf\u5e38\u5ffd\u7565\u65f6\u95f4\u8bc1\u636e\u3001\u6df7\u6dc6\u4e0d\u540c\u65f6\u671f\u7684\u4e8b\u5b9e\uff0c\u4e14\u7f3a\u4e4f\u5f03\u6743\u80fd\u529b\uff0c\u5bfc\u81f4\u4ea7\u751f\u8bef\u5bfc\u6027\u7b54\u6848\u800c\u975e\u62d2\u7edd\u56de\u7b54\u4e0d\u53ef\u9760\u95ee\u9898\u3002\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4e2d\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5982\u4f55\u8bad\u7ec3LLMs\u5177\u5907\u65f6\u95f4\u63a8\u7406\u4e2d\u7684\u5f03\u6743\u80fd\u529b\u3002", "method": "\u5c06\u5f03\u6743\u89c6\u4e3a\u53ef\u6559\u6388\u6280\u80fd\uff0c\u63d0\u51fa\u7ed3\u5408\u601d\u7ef4\u94fe\u76d1\u7763\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002\u4f7f\u7528\u5f03\u6743\u611f\u77e5\u5956\u52b1\u6307\u5bfcRL\uff0c\u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u4fe1\u606f\u7c7b\u578b\uff08\u539f\u59cb\u4e0a\u4e0b\u6587\u3001\u65f6\u95f4\u5b50\u4e0a\u4e0b\u6587\u3001\u77e5\u8bc6\u56fe\u8c31\uff09\u548c\u8bad\u7ec3\u6280\u672f\u5bf9\u65f6\u95f4\u63a8\u7406\u4e2d\u5f03\u6743\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u57fa\u4e8eQwen2.5-1.5B-Instruct\u7684\u6a21\u578b\u5728TimeQA-Easy\u548cHard\u4e0a\u5206\u522b\u8d85\u8d8aGPT-4o 3.46%\u548c5.80%\u3002\u5728\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u4e0a\uff0c\u76f8\u6bd4\u7eaf\u76d1\u7763\u5fae\u8c03\u53d8\u4f53\uff0c\u771f\u9633\u6027\u7387\u63d0\u534720%\u3002\u5206\u6790\u663e\u793aSFT\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff0cRL\u63d0\u9ad8\u51c6\u786e\u6027\u4f46\u4ecd\u6709\u7c7b\u4f3c\u98ce\u9669\uff0c\u9690\u5f0f\u63a8\u7406\u7ebf\u7d22\u5bf9\u5f03\u6743\u63a8\u7406\u5e2e\u52a9\u6709\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8054\u5408\u4f18\u5316\u5f03\u6743\u548c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684LLMs\u5960\u5b9a\u4e86\u57fa\u7840\u3002RL\u5728\u63d0\u5347\u63a8\u7406\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u89e3\u51b3\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u9690\u5f0f\u4fe1\u606f\u5bf9\u5f03\u6743\u63a8\u7406\u7684\u8d21\u732e\u6709\u9650\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04417", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04417", "abs": "https://arxiv.org/abs/2602.04417", "authors": ["Lunjun Zhang", "Jimmy Ba"], "title": "EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL", "comment": null, "summary": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to acquire increasingly complex reasoning and agentic behaviors. In this work, we propose two simple techniques to improve policy gradient algorithms for LLMs. First, we replace the fixed anchor policy during RL with an Exponential Moving Average (EMA), similar to a target network in deep Q-learning. Second, we introduce Top-k KL estimator, which allows for flexible interpolation between exact KL and sampled KL. We derive the stability conditions for using EMA anchor; moreover, we show that our Top-k KL estimator yields both unbiased KL values and unbiased gradients at any k, while bringing the benefits of exact KL. When combined with GRPO, the two techniques (EMA-PG) lead to a significant performance boost. On math reasoning, it allows R1-distilled Qwen-1.5B to reach 53.9% on OlympiadBench compared to 50.8% by GRPO. On agentic RL domains, with Qwen-3B base, EMA-PG improves GRPO by an average of 33.3% across 7 datasets of Q&A with search engines, including 29.7% $\\rightarrow$ 44.1% on HotpotQA, 27.4% $\\rightarrow$ 40.1% on 2WikiMultiHopQA. Overall, we show that EMA-PG is a simple, principled, and powerful approach to scaling RL for LLMs. Code: https://github.com/LunjunZhang/ema-pg", "AI": {"tldr": "\u63d0\u51faEMA-PG\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\u951a\u7b56\u7565\u548cTop-k KL\u4f30\u8ba1\u5668\u6539\u8fdbLLM\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u590d\u6742\u63a8\u7406\u548c\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "1. \u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747(EMA)\u66ff\u4ee3\u56fa\u5b9a\u951a\u7b56\u7565\uff0c\u7c7b\u4f3c\u4e8e\u6df1\u5ea6Q\u5b66\u4e60\u4e2d\u7684\u76ee\u6807\u7f51\u7edc\uff1b2. \u5f15\u5165Top-k KL\u4f30\u8ba1\u5668\uff0c\u5728\u7cbe\u786eKL\u548c\u91c7\u6837KL\u4e4b\u95f4\u7075\u6d3b\u63d2\u503c\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cR1\u84b8\u998f\u7684Qwen-1.5B\u5728OlympiadBench\u8fbe\u523053.9%\uff08GRPO\u4e3a50.8%\uff09\uff1b\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\uff0cQwen-3B\u57287\u4e2a\u641c\u7d22\u95ee\u7b54\u6570\u636e\u96c6\u5e73\u5747\u63d0\u534733.3%\uff0c\u5982HotpotQA\u4ece29.7%\u63d0\u5347\u523044.1%\u3002", "conclusion": "EMA-PG\u662f\u4e00\u79cd\u7b80\u5355\u3001\u6709\u7406\u8bba\u4f9d\u636e\u4e14\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6269\u5c55LLM\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04811", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04811", "abs": "https://arxiv.org/abs/2602.04811", "authors": ["Jiarui Yuan", "Tailin Jin", "Weize Chen", "Zeyuan Liu", "Zhiyuan Liu", "Maosong Sun"], "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization", "comment": "Under review", "summary": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.", "AI": {"tldr": "SE-Bench\u662f\u4e00\u4e2a\u8bca\u65ad\u73af\u5883\uff0c\u901a\u8fc7\u6df7\u6dc6NumPy\u5e93\u53ca\u5176API\u6587\u6863\u4e3a\u4f2a\u65b0\u5305\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u77e5\u8bc6\u5185\u5316\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u95ed\u5377\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3001\u6807\u51c6RL\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u81ea\u535a\u5f08\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u667a\u80fd\u4f53\u81ea\u6211\u6f14\u5316\u80fd\u529b\u9762\u4e34\u4e24\u5927\u969c\u788d\uff1a\u5148\u9a8c\u77e5\u8bc6\u7684\u7ea0\u7f20\uff08\u65b0\u77e5\u8bc6\u53ef\u80fd\u5df2\u5b58\u5728\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\uff09\u548c\u63a8\u7406\u590d\u6742\u6027\u7684\u7ea0\u7f20\uff08\u5931\u8d25\u53ef\u80fd\u6e90\u4e8e\u95ee\u9898\u96be\u5ea6\u800c\u975e\u77e5\u8bc6\u56de\u5fc6\u80fd\u529b\uff09\u3002\u9700\u8981\u5efa\u7acb\u5e72\u51c0\u7684\u8bca\u65ad\u73af\u5883\u6765\u6d4b\u91cf\u77e5\u8bc6\u5185\u5316\u8fd9\u4e00\u57fa\u7840\u80fd\u529b\u3002", "method": "\u5c06NumPy\u5e93\u53ca\u5176API\u6587\u6863\u6df7\u6dc6\u4e3a\u4f2a\u65b0\u5305\uff0c\u4f7f\u7528\u968f\u673a\u5316\u6807\u8bc6\u7b26\u3002\u667a\u80fd\u4f53\u8bad\u7ec3\u5185\u5316\u8be5\u5305\uff0c\u7136\u540e\u5728\u6ca1\u6709\u6587\u6863\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u7b80\u5355\u7f16\u7801\u4efb\u52a1\u3002\u4efb\u52a1\u8bbe\u8ba1\u4f7f\u5f97\u4f7f\u7528\u65b0API\u6587\u6863\u65f6\u4efb\u52a1\u7b80\u5355\uff0c\u4f46\u57fa\u7840\u6a21\u578b\u65e0\u6cd5\u5b8c\u6210\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u89c1\u89e3\uff1a1) \u5f00\u5377\u6096\u8bba\uff1a\u4f7f\u7528\u53c2\u8003\u6587\u6863\u8bad\u7ec3\u4f1a\u6291\u5236\u77e5\u8bc6\u4fdd\u7559\uff0c\u9700\u8981\u95ed\u5377\u8bad\u7ec3\u5f3a\u5236\u77e5\u8bc6\u538b\u7f29\u5230\u6743\u91cd\u4e2d\uff1b2) RL\u5dee\u8ddd\uff1a\u6807\u51c6RL\u56e0PPO\u88c1\u526a\u548c\u8d1f\u68af\u5ea6\u65e0\u6cd5\u5b8c\u5168\u5185\u5316\u65b0\u77e5\u8bc6\uff1b3) \u81ea\u535a\u5f08\u53ef\u884c\u6027\uff1a\u7ed3\u5408SFT\uff0c\u6a21\u578b\u53ef\u4ee5\u4ece\u81ea\u751f\u6210\u7684\u5608\u6742\u4efb\u52a1\u4e2d\u5b66\u4e60\uff0c\u4f46RL\u4e0d\u884c\u3002", "conclusion": "SE-Bench\u4e3a\u77e5\u8bc6\u5185\u5316\u7684\u81ea\u6211\u6f14\u5316\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u8bca\u65ad\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bad\u7ec3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u95ed\u5377\u8bad\u7ec3\u548c\u81ea\u535a\u5f08\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.04884", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04884", "abs": "https://arxiv.org/abs/2602.04884", "authors": ["Bangzheng Li", "Jianmo Ni", "Chen Qu", "Ian Miao", "Liu Yang", "Xingyu Fu", "Muhao Chen", "Derek Zhiyuan Cheng"], "title": "Reinforced Attention Learning", "comment": null, "summary": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.\n  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.", "AI": {"tldr": "RAL\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u4f18\u5316\u6ce8\u610f\u529b\u5206\u5e03\u800c\u975e\u8f93\u51fa\u5e8f\u5217\uff0c\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u56fe\u50cf\u89c6\u9891\u57fa\u51c6\u4e0a\u8868\u73b0\u66f4\u4f18", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6548\u679c\u6709\u9650\uff0c\u751a\u81f3\u53ef\u80fd\u635f\u5bb3\u611f\u77e5\u6027\u80fd\uff0c\u9700\u8981\u65b0\u7684\u4f18\u5316\u8303\u5f0f", "method": "\u63d0\u51fa\u5f3a\u5316\u6ce8\u610f\u529b\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316\u5185\u90e8\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u800c\u975e\u8f93\u51fatoken\u5e8f\u5217\uff1b\u5e76\u5f15\u5165\u5728\u7ebf\u6ce8\u610f\u529b\u84b8\u998f\u6280\u672f", "result": "\u5728\u591a\u79cd\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAL\u76f8\u6bd4GRPO\u7b49\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff1b\u6ce8\u610f\u529b\u84b8\u998f\u6bd4\u6807\u51c6\u77e5\u8bc6\u84b8\u998f\u4ea7\u751f\u66f4\u5f3a\u7684\u8de8\u6a21\u6001\u5bf9\u9f50", "conclusion": "\u6ce8\u610f\u529b\u7b56\u7565\u4e3a\u591a\u6a21\u6001\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u901a\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u6ce8\u610f\u529b\u5206\u5e03\u800c\u975e\u8f93\u51fa\u5e8f\u5217\u80fd\u66f4\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2602.04663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04663", "abs": "https://arxiv.org/abs/2602.04663", "authors": ["Jaemoo Choi", "Yuchen Zhu", "Wei Guo", "Petr Molodyk", "Bo Yuan", "Jinbin Bai", "Yi Xin", "Molei Tao", "Yongxin Chen"], "title": "Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design", "comment": "23 pages, 11 figures", "summary": "Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\\times$ more efficient than FlowGRPO and $2\\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u53d1\u73b0\u57fa\u4e8eELBO\u7684\u4f3c\u7136\u4f30\u8ba1\u5668\u662f\u9ad8\u6548\u7a33\u5b9a\u4f18\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u7531\u4e8e\u5176\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u5904\u7406\uff0c\u76f4\u63a5\u5e94\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b58\u5728\u969c\u788d\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eLLM\u76ee\u6807\u6784\u5efa\u65b0\u76ee\u6807\uff0c\u4f7f\u7528\u4e34\u65f6\u4f30\u8ba1\u5668\u4f30\u8ba1\u4f3c\u7136\uff0c\u7f3a\u4e4f\u5bf9\u4f30\u8ba1\u5982\u4f55\u5f71\u54cd\u7b97\u6cd5\u6027\u80fd\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u4e09\u4e2a\u56e0\u7d20\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff1a1) \u7b56\u7565\u68af\u5ea6\u76ee\u6807\uff0c2) \u4f3c\u7136\u4f30\u8ba1\u5668\uff0c3) \u8f68\u8ff9\u91c7\u6837\u65b9\u6848\u3002\u91c7\u7528\u57fa\u4e8e\u8bc1\u636e\u4e0b\u754c(ELBO)\u7684\u6a21\u578b\u4f3c\u7136\u4f30\u8ba1\u5668\uff0c\u4ec5\u4ece\u6700\u7ec8\u751f\u6210\u6837\u672c\u8ba1\u7b97\uff0c\u4f5c\u4e3a\u4e3b\u5bfc\u4f18\u5316\u56e0\u7d20\u3002", "result": "\u5728\u591a\u4e2a\u5956\u52b1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528SD 3.5 Medium\u9a8c\u8bc1\uff0c\u6240\u6709\u4efb\u52a1\u5747\u663e\u793a\u4e00\u81f4\u8d8b\u52bf\u3002\u65b9\u6cd5\u572890 GPU\u5c0f\u65f6\u5185\u5c06GenEval\u5206\u6570\u4ece0.24\u63d0\u5347\u81f30.95\uff0c\u6bd4FlowGRPO\u6548\u7387\u9ad84.6\u500d\uff0c\u6bd4SOTA\u65b9\u6cd5DiffusionNFT\u6548\u7387\u9ad82\u500d\u4e14\u65e0\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8eELBO\u7684\u4f3c\u7136\u4f30\u8ba1\u5668\u662f\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u9ad8\u6548\u3001\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5176\u91cd\u8981\u6027\u8d85\u8fc7\u7279\u5b9a\u7b56\u7565\u68af\u5ea6\u635f\u5931\u51fd\u6570\u7684\u9009\u62e9\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04431", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.04431", "abs": "https://arxiv.org/abs/2602.04431", "authors": ["Jonathan N\u00f6ther", "Adish Singla", "Goran Radanovic"], "title": "MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems", "comment": null, "summary": "LLM-based multi-agent systems have demonstrated impressive capabilities, but they also introduce significant safety risks when individual agents fail or behave adversarially. In this work, we study the automated design of agentic systems that remain safe even when a subset of agents is compromised. We formalize this challenge as a Stackelberg security game between a system designer (the Meta-Agent) and a best-responding Meta-Adversary that selects and compromises a subset of agents to minimize safety. We propose Meta-Adversary-Meta-Agent (MaMa), a novel algorithm for approximately solving this game and automatically designing safe agentic systems. Our approach uses LLM-based adversarial search, where the Meta-Agent iteratively proposes system designs and receives feedback based on the strongest attacks discovered by the Meta-Adversary. Empirical evaluations across diverse environments show that systems designed with MaMa consistently defend against worst-case attacks while maintaining performance comparable to systems optimized solely for task success. Moreover, the resulting systems generalize to stronger adversaries, as well as ones with different attack objectives or underlying LLMs, demonstrating robust safety beyond the training setting.", "AI": {"tldr": "\u63d0\u51faMaMa\u7b97\u6cd5\uff0c\u901a\u8fc7\u5143\u4ee3\u7406\u4e0e\u5143\u5bf9\u6297\u7684Stackelberg\u535a\u5f08\u81ea\u52a8\u8bbe\u8ba1\u5b89\u5168\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5373\u4f7f\u90e8\u5206\u667a\u80fd\u4f53\u88ab\u653b\u9677\u4e5f\u80fd\u4fdd\u6301\u5b89\u5168\u3002", "motivation": "LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5f53\u4e2a\u522b\u667a\u80fd\u4f53\u5931\u6548\u6216\u88ab\u6076\u610f\u653b\u51fb\u65f6\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u4e0d\u5b89\u5168\u3002\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u62b5\u5fa1\u667a\u80fd\u4f53\u88ab\u653b\u9677\u7684\u5b89\u5168\u7cfb\u7edf\u3002", "method": "\u5c06\u5b89\u5168\u7cfb\u7edf\u8bbe\u8ba1\u5f62\u5f0f\u5316\u4e3aStackelberg\u5b89\u5168\u535a\u5f08\uff0c\u63d0\u51faMeta-Adversary-Meta-Agent (MaMa)\u7b97\u6cd5\u3002\u5143\u4ee3\u7406\u8fed\u4ee3\u63d0\u51fa\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5143\u5bf9\u6297\u641c\u7d22\u6700\u5f3a\u653b\u51fb\u7b56\u7565\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u5bf9\u6297\u641c\u7d22\u8fd1\u4f3c\u6c42\u89e3\u535a\u5f08\u3002", "result": "\u5728\u591a\u79cd\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cMaMa\u8bbe\u8ba1\u7684\u7cfb\u7edf\u80fd\u6709\u6548\u62b5\u5fa1\u6700\u574f\u60c5\u51b5\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4ec5\u4f18\u5316\u4efb\u52a1\u6027\u80fd\u7684\u7cfb\u7edf\u76f8\u5f53\u7684\u8868\u73b0\u3002\u7cfb\u7edf\u8fd8\u80fd\u6cdb\u5316\u5230\u66f4\u5f3a\u7684\u5bf9\u6297\u8005\u3001\u4e0d\u540c\u653b\u51fb\u76ee\u6807\u548c\u4e0d\u540c\u5e95\u5c42LLM\u3002", "conclusion": "MaMa\u7b97\u6cd5\u80fd\u591f\u81ea\u52a8\u8bbe\u8ba1\u51fa\u5177\u6709\u9c81\u68d2\u5b89\u5168\u6027\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u73af\u5883\u4e4b\u5916\u4e5f\u80fd\u4fdd\u6301\u5b89\u5168\uff0c\u4e3a\u6784\u5efa\u5b89\u5168\u7684LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.04863", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04863", "abs": "https://arxiv.org/abs/2602.04863", "authors": ["Ishaq Aden-Ali", "Noah Golowich", "Allen Liu", "Abhishek Shetty", "Ankur Moitra", "Nika Haghtalab"], "title": "Subliminal Effects in Your Data: A General Mechanism via Log-Linearity", "comment": "Code available at https://github.com/ishaqadenali/logit-linear-selection", "summary": "Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.\n  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.", "AI": {"tldr": "\u63d0\u51faLogit-Linear-Selection\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u504f\u597d\u6570\u636e\u96c6\u7684\u5b50\u96c6\u6765\u6fc0\u53d1LLM\u4e2d\u7684\u9690\u85cf\u6548\u5e94\uff0c\u5982\u7279\u5b9a\u504f\u597d\u3001\u8de8\u8bed\u8a00\u54cd\u5e94\u548c\u89d2\u8272\u626e\u6f14\u7b49\u3002", "motivation": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4f7f\u7528\u591a\u79cd\u7b97\u6cd5\u548c\u6570\u636e\u96c6\u6765\u6fc0\u53d1\u7279\u5b9a\u884c\u4e3a\uff0c\u4f46\u6570\u636e\u96c6\u53ef\u80fd\u4f20\u9012\u4ece\u5355\u4e2a\u6570\u636e\u70b9\u65e0\u6cd5\u76f4\u63a5\u89c2\u5bdf\u5230\u7684\u4fe1\u53f7\uff0c\u8fd9\u7ed9\u57fa\u4e8e\u6570\u636e\u96c6\u7406\u89e3LLM\u8bad\u7ec3\u5e26\u6765\u4e86\u6982\u5ff5\u6311\u6218\uff0c\u9700\u8981\u5bf9\u8fd9\u4e9b\u73b0\u8c61\u8fdb\u884c\u57fa\u7840\u6027\u89e3\u91ca\u3002", "method": "\u53d7LLM\u7ebf\u6027\u7ed3\u6784\u7814\u7a76\u542f\u53d1\uff0c\u63d0\u51faLogit-Linear-Selection\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u89c4\u5b9a\u4e86\u5982\u4f55\u4ece\u901a\u7528\u504f\u597d\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u5b50\u96c6\u6765\u6fc0\u53d1\u5e7f\u6cdb\u7684\u9690\u85cf\u6548\u5e94\u3002", "result": "\u5e94\u7528LLS\u53d1\u73b0\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u5b50\u96c6\uff0c\u4f7f\u5f97\u5728\u8fd9\u4e9b\u5b50\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u51fa\u4ece\u7279\u5b9a\u504f\u597d\u3001\u54cd\u5e94\u672a\u51fa\u73b0\u5728\u6570\u636e\u96c6\u4e2d\u7684\u4e0d\u540c\u8bed\u8a00\u63d0\u793a\uff0c\u5230\u91c7\u7528\u4e0d\u540c\u89d2\u8272\u7b49\u591a\u79cd\u884c\u4e3a\u3002\u8be5\u6548\u5e94\u5728\u9009\u62e9\u5b50\u96c6\u4e0a\u6301\u7eed\u5b58\u5728\uff0c\u4e14\u5728\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u4e2d\u5747\u6709\u6548\uff0c\u652f\u6301\u5176\u666e\u904d\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86\u6570\u636e\u96c6\u5982\u4f55\u901a\u8fc7\u9690\u85cf\u5b50\u6587\u672c\u5f71\u54cdLLM\u884c\u4e3a\u7684\u901a\u7528\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u5c5e\u6027\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2602.04879", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04879", "abs": "https://arxiv.org/abs/2602.04879", "authors": ["Penghui Qi", "Xiangxin Zhou", "Zichen Liu", "Tianyu Pang", "Chao Du", "Min Lin", "Wee Sun Lee"], "title": "Rethinking the Trust Region in LLM Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.", "AI": {"tldr": "DPPO\u63d0\u51fa\u7528\u76f4\u63a5\u4f30\u8ba1\u7b56\u7565\u6563\u5ea6\u4ee3\u66ffPPO\u7684\u542f\u53d1\u5f0f\u88c1\u526a\u673a\u5236\uff0c\u89e3\u51b3\u4e86PPO\u5728\u5927\u8bcd\u6c47\u91cfLLM\u5fae\u8c03\u4e2d\u7684\u7ed3\u6784\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "PPO\u4f5c\u4e3aLLM\u5fae\u8c03\u7684\u6807\u51c6RL\u7b97\u6cd5\uff0c\u5176\u6838\u5fc3\u7684\u6bd4\u7387\u88c1\u526a\u673a\u5236\u5728\u5927\u8bcd\u6c47\u91cf\u573a\u666f\u4e0b\u5b58\u5728\u7ed3\u6784\u6027\u95ee\u9898\uff1a\u5bf9\u4f4e\u6982\u7387token\u66f4\u65b0\u8fc7\u5ea6\u60e9\u7f5a\uff0c\u800c\u5bf9\u9ad8\u6982\u7387token\u7684\u707e\u96be\u6027\u504f\u79fb\u7ea6\u675f\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u548c\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faDivergence Proximal Policy Optimization (DPPO)\uff0c\u7528\u57fa\u4e8e\u76f4\u63a5\u7b56\u7565\u6563\u5ea6\u4f30\u8ba1\uff08\u5982\u603b\u53d8\u5dee\u6216KL\u6563\u5ea6\uff09\u7684\u7ea6\u675f\u66ff\u4ee3\u542f\u53d1\u5f0f\u88c1\u526a\u3002\u4e3a\u907f\u514d\u5de8\u5927\u5185\u5b58\u5f00\u9500\uff0c\u5f15\u5165\u9ad8\u6548\u7684Binary\u548cTop-K\u8fd1\u4f3c\u65b9\u6cd5\u6765\u6355\u83b7\u6838\u5fc3\u6563\u5ea6\u4fe1\u606f\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cDPPO\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u4e3a\u57fa\u4e8eRL\u7684LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u57fa\u7840\u3002", "conclusion": "DPPO\u901a\u8fc7\u66f4\u539f\u5219\u6027\u7684\u7b56\u7565\u6563\u5ea6\u7ea6\u675f\u673a\u5236\uff0c\u89e3\u51b3\u4e86PPO\u5728\u5927\u8bcd\u6c47\u91cfLLM\u5fae\u8c03\u4e2d\u7684\u7ed3\u6784\u6027\u95ee\u9898\uff0c\u4e3aRL-based LLM fine-tuning\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u7b97\u6cd5\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04763", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04763", "abs": "https://arxiv.org/abs/2602.04763", "authors": ["Rui Liu", "Pratap Tokekar", "Ming Lin"], "title": "Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty", "comment": null, "summary": "Multi-agent systems are increasingly equipped with heterogeneous multimodal sensors, enabling richer perception but introducing modality-specific and agent-dependent uncertainty. Existing multi-agent collaboration frameworks typically reason at the agent level, assume homogeneous sensing, and handle uncertainty implicitly, limiting robustness under sensor corruption. We propose Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty (A2MAML), a principled approach for uncertainty-aware, modality-level collaboration. A2MAML models each modality-specific feature as a stochastic estimate with uncertainty prediction, actively selects reliable agent-modality pairs, and aggregates information via Bayesian inverse-variance weighting. This formulation enables fine-grained, modality-level fusion, supports asymmetric modality availability, and provides a principled mechanism to suppress corrupted or noisy modalities. Extensive experiments on connected autonomous driving scenarios for collaborative accident detection demonstrate that A2MAML consistently outperforms both single-agent and collaborative baselines, achieving up to 18.7% higher accident detection rate.", "AI": {"tldr": "A2MAML\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u9006\u65b9\u5dee\u52a0\u6743\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u6a21\u6001\u7ea7\u534f\u4f5c\uff0c\u5728\u4f20\u611f\u5668\u635f\u574f\u60c5\u51b5\u4e0b\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e8b\u6545\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u901a\u5e38\u5728\u667a\u80fd\u4f53\u7ea7\u522b\u8fdb\u884c\u63a8\u7406\uff0c\u5047\u8bbe\u540c\u8d28\u4f20\u611f\uff0c\u9690\u5f0f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4f20\u611f\u5668\u635f\u574f\u65f6\u9c81\u68d2\u6027\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u5f02\u6784\u591a\u6a21\u6001\u4f20\u611f\u5668\u3001\u6a21\u6001\u7279\u5b9a\u4e0d\u786e\u5b9a\u6027\u7684\u7ec6\u7c92\u5ea6\u534f\u4f5c\u65b9\u6cd5\u3002", "method": "\u63d0\u51faA2MAML\u6846\u67b6\uff1a1) \u5c06\u6bcf\u4e2a\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u5efa\u6a21\u4e3a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u7684\u968f\u673a\u4f30\u8ba1\uff1b2) \u4e3b\u52a8\u9009\u62e9\u53ef\u9760\u7684\u667a\u80fd\u4f53-\u6a21\u6001\u5bf9\uff1b3) \u901a\u8fc7\u8d1d\u53f6\u65af\u9006\u65b9\u5dee\u52a0\u6743\u805a\u5408\u4fe1\u606f\u3002\u652f\u6301\u6a21\u6001\u7ea7\u878d\u5408\u548c\u4e0d\u5bf9\u79f0\u6a21\u6001\u53ef\u7528\u6027\u3002", "result": "\u5728\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u534f\u4f5c\u4e8b\u6545\u68c0\u6d4b\u5b9e\u9a8c\u4e2d\uff0cA2MAML\u6301\u7eed\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u534f\u4f5c\u57fa\u7ebf\uff0c\u4e8b\u6545\u68c0\u6d4b\u7387\u63d0\u5347\u9ad8\u8fbe18.7%\u3002", "conclusion": "A2MAML\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u3001\u6a21\u6001\u7ea7\u534f\u4f5c\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6291\u5236\u635f\u574f\u6216\u566a\u58f0\u6a21\u6001\uff0c\u5728\u591a\u667a\u80fd\u4f53\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2602.04809", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04809", "abs": "https://arxiv.org/abs/2602.04809", "authors": ["Elizabeth Bates", "Chris Hicks", "Vasilios Mavroudis"], "title": "Beyond Rewards in Reinforcement Learning for Cyber Defence", "comment": null, "summary": "Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\uff0c\u7a00\u758f\u5956\u52b1\u76f8\u6bd4\u5bc6\u96c6\u5956\u52b1\u7684\u4f18\u52bf\uff0c\u53d1\u73b0\u7a00\u758f\u5956\u52b1\u80fd\u4ea7\u751f\u66f4\u53ef\u9760\u3001\u66f4\u6709\u6548\u4e14\u98ce\u9669\u66f4\u4f4e\u7684\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u667a\u80fd\u4f53\u901a\u5e38\u4f7f\u7528\u5bc6\u96c6\u3001\u9ad8\u5ea6\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u51fd\u6570\u7ed3\u5408\u4e86\u591a\u79cd\u60e9\u7f5a\u548c\u6fc0\u52b1\u3002\u5bc6\u96c6\u5956\u52b1\u6709\u52a9\u4e8e\u7f13\u89e3\u590d\u6742\u73af\u5883\u7684\u63a2\u7d22\u6311\u6218\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u667a\u80fd\u4f53\u504f\u5411\u6b21\u4f18\u4e14\u98ce\u9669\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u5728\u590d\u6742\u7684\u7f51\u7edc\u73af\u5883\u4e2d\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u7a00\u758f\u548c\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\uff0c\u5728\u4e24\u4e2a\u6210\u719f\u7684\u7f51\u7edc\u8bad\u7ec3\u73af\u5883\uff08cyber gyms\uff09\u4e2d\uff0c\u9488\u5bf9\u4e0d\u540c\u7f51\u7edc\u89c4\u6a21\uff0c\u7ed3\u5408\u7b56\u7565\u68af\u5ea6\u548c\u57fa\u4e8e\u4ef7\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5168\u9762\u8bc4\u4f30\u5956\u52b1\u51fd\u6570\u7ed3\u6784\u5bf9\u5b66\u4e60\u548c\u7b56\u7565\u884c\u4e3a\u7279\u5f81\u7684\u5f71\u54cd\u3002\u91c7\u7528\u65b0\u9896\u7684ground truth\u8bc4\u4f30\u65b9\u6cd5\uff0c\u76f4\u63a5\u6bd4\u8f83\u4e0d\u540c\u5956\u52b1\u51fd\u6570\u7684\u6548\u679c\u3002", "result": "\u7a00\u758f\u5956\u52b1\uff08\u524d\u63d0\u662f\u76ee\u6807\u5bf9\u9f50\u4e14\u80fd\u9891\u7e41\u9047\u5230\uff09\u80fd\u63d0\u4f9b\u66f4\u5f3a\u7684\u8bad\u7ec3\u53ef\u9760\u6027\u548c\u66f4\u6709\u6548\u7684\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u667a\u80fd\u4f53\uff0c\u4ea7\u751f\u98ce\u9669\u66f4\u4f4e\u7684\u7b56\u7565\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u7a00\u758f\u5956\u52b1\u8fd8\u80fd\u4ea7\u751f\u66f4\u7b26\u5408\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u76ee\u6807\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u5e76\u4e14\u65e0\u9700\u663e\u5f0f\u7684\u57fa\u4e8e\u5956\u52b1\u7684\u6570\u503c\u60e9\u7f5a\u5c31\u80fd\u8282\u7ea6\u4f7f\u7528\u6210\u672c\u9ad8\u6602\u7684\u9632\u5fa1\u884c\u52a8\u3002", "conclusion": "\u5728\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\uff0c\u7a00\u758f\u5956\u52b1\u76f8\u6bd4\u5bc6\u96c6\u5956\u52b1\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u4ea7\u751f\u66f4\u53ef\u9760\u3001\u66f4\u6709\u6548\u4e14\u98ce\u9669\u66f4\u4f4e\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u540c\u65f6\u66f4\u597d\u5730\u4e0e\u9632\u5fa1\u76ee\u6807\u5bf9\u9f50\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04620", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04620", "abs": "https://arxiv.org/abs/2602.04620", "authors": ["Doyeon Lee", "Eunyi Lyou", "Hyunsoo Cho", "Sookyung Kim", "Joonseok Lee", "Jaemoo Choi"], "title": "QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning", "comment": null, "summary": "GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.", "AI": {"tldr": "QUATRO\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u4fe1\u4efb\u533a\u57df\u7ea6\u675f\u89e3\u51b3\u73b0\u6709GRPO\u65b9\u6cd5\u4e2d\u542f\u53d1\u5f0f\u8fd1\u4f3c\u5bfc\u81f4\u7684\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u53ef\u63a7\u3001\u7a33\u5b9a\u7684\u7b56\u7565\u66f4\u65b0\u3002", "motivation": "\u73b0\u6709GRPO\u98ce\u683c\u7684RL\u5fae\u8c03\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u7684\u4fe1\u4efb\u533a\u57df\u8fd1\u4f3c\uff0c\u5b58\u5728\u4f18\u5316\u8106\u5f31\u6027\u95ee\u9898\uff0c\u5168\u5c40\u91cd\u8981\u6027\u6bd4\u7387\u88c1\u526a\u548c\u7ec4\u5f52\u4e00\u5316\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8d85\u51fa\u88c1\u526a\u8303\u56f4\u7684\u6837\u672c\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faQUATRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u5219\u6027\u4f18\u5316\u76f4\u63a5\u5f3a\u5236\u6267\u884c\u4fe1\u4efb\u533a\u57df\u7ea6\u675f\uff0c\u4ea7\u751f\u6e05\u6670\u53ef\u89e3\u91ca\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5b9e\u73b0\u5bf9\u7b56\u7565\u66f4\u65b0\u7684\u663e\u5f0f\u63a7\u5236\u548c\u71b5\u7a33\u5b9a\u7684\u4f18\u5316\uff0c\u7a33\u5b9a\u9879\u4ece\u7cbe\u786e\u7684\u4fe1\u4efb\u533a\u57df\u516c\u5f0f\u4e2d\u81ea\u7136\u4ea7\u751f\u3002", "result": "\u5728\u591a\u79cd\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7ecf\u9a8c\u9a8c\u8bc1\uff0cQUATRO\u5728\u589e\u52a0\u7b56\u7565\u9648\u65e7\u6027\u548c\u6fc0\u8fdb\u5b66\u4e60\u7387\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7ef4\u6301\u826f\u597d\u63a7\u5236\u7684\u71b5\u3002", "conclusion": "QUATRO\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u53ef\u63a7\u7684RL\u5fae\u8c03\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u542f\u53d1\u5f0f\u4fe1\u4efb\u533a\u57df\u8fd1\u4f3c\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4f18\u5316\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04651", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04651", "abs": "https://arxiv.org/abs/2602.04651", "authors": ["Dipan Maity"], "title": "SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF", "comment": null, "summary": "Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE", "AI": {"tldr": "SAFE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RLHF\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u8f6f\u6700\u5c0f\u6279\u8bc4\u5bb6\u3001\u71b5\u95e8\u63a7KL\u8c03\u8282\u548cPID\u63a7\u5236\u81ea\u9002\u5e94\u9608\u503c\u6765\u89e3\u51b3PPO\u5728\u8bed\u8a00\u6a21\u578bRLHF\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u7b56\u7565\u4f18\u5316\u3002", "motivation": "PPO\u867d\u7136\u5728RLHF\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5b58\u5728\u542f\u53d1\u5f0f\u52a8\u673a\u3001KL\u6563\u5ea6\u7ea6\u675f\u5904\u7406\u968f\u610f\u3001\u5956\u52b1\u632f\u8361\u3001\u71b5\u5d29\u6e83\u3001\u4ef7\u503c\u51fd\u6570\u6f02\u79fb\u548c\u7a81\u7136\u7b56\u7565\u53d1\u6563\u7b49\u95ee\u9898\uff0c\u9700\u8981\u9891\u7e41\u91cd\u542f\u548c\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u53ef\u9760\u7684RLHF\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSAFE\u7b97\u6cd5\uff1a1) \u53cc\u8f6f\u6700\u5c0f\u6279\u8bc4\u5bb6\u8fdb\u884c\u60b2\u89c2\u4ef7\u503c\u4f30\u8ba1\uff1b2) \u591a\u5c42\u7a33\u5b9a\u6846\u67b6\u7ed3\u5408\u71b5\u95e8\u63a7KL\u8c03\u8282\uff1b3) PID\u63a7\u5236\u81ea\u9002\u5e94\u9608\u503c\u3002\u4e0ePPO\u7684\u5bf9\u79f0KL\u60e9\u7f5a\u4e0d\u540c\uff0cSAFE\u533a\u5206\u9ad8\u71b5\u63a2\u7d22\u548c\u4f4e\u71b5\u6a21\u5f0f\u5d29\u6e83\uff0c\u5e76\u6839\u636e\u5956\u52b1\u901f\u5ea6\u52a8\u6001\u8c03\u6574\u60e9\u7f5a\u3002", "result": "\u57283B\u53c2\u6570\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSAFE\u6bd4PPO\u83b7\u5f97+5.15%\u7684\u8bad\u7ec3\u5e73\u5747\u5956\u52b1\uff080.725 vs 0.689\uff09\uff0c\u5956\u52b1\u5d29\u6e83\u53ef\u5ffd\u7565\uff0cKL\u63a7\u5236\u4f18\u4e8ePPO\u3002\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u6297\u5d29\u6e83\u7684RLHF\u6846\u67b6\u3002", "conclusion": "SAFE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684RLHF\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6fc0\u8fdb\u5b66\u4e60\u901f\u5ea6\u7684\u540c\u65f6\u786e\u4fdd\u957f\u671f\u4f18\u5316\u7a33\u5b9a\u6027\uff0c\u9002\u5408\u751f\u4ea7\u90e8\u7f72\u3002\u89e3\u51b3\u4e86PPO\u5728\u8bed\u8a00\u6a21\u578bRLHF\u4e2d\u7684\u5173\u952e\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04868", "abs": "https://arxiv.org/abs/2602.04868", "authors": ["Yannick Denker", "Alexander Gepperth"], "title": "CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation", "comment": null, "summary": "Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.", "AI": {"tldr": "\u63d0\u51faCRoSS\u57fa\u51c6\u5957\u4ef6\uff0c\u7528\u4e8e\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u4eff\u771f\u4e2d\u7684\u8bc4\u4f30\uff0c\u5305\u542b\u8f6e\u5f0f\u673a\u5668\u4eba\u548c\u673a\u68b0\u81c2\u4e24\u79cd\u5e73\u53f0\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u548c\u9ad8\u7269\u7406\u771f\u5b9e\u5ea6\u4eff\u771f\u3002", "motivation": "\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u667a\u80fd\u4f53\u5728\u5e8f\u5217\u4efb\u52a1\u4e2d\u5b66\u4e60\u800c\u4e0d\u9057\u5fd8\u5148\u524d\u7b56\u7565\uff0c\u4f46\u7f3a\u4e4f\u57fa\u4e8e\u9ad8\u7269\u7406\u771f\u5b9e\u5ea6\u673a\u5668\u4eba\u4eff\u771f\u7684\u6807\u51c6\u5316\u57fa\u51c6\u5957\u4ef6\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eGazebo\u4eff\u771f\u7684CRoSS\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u542b\u4e24\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff1a\u5dee\u5206\u9a71\u52a8\u8f6e\u5f0f\u673a\u5668\u4eba\uff08\u7528\u4e8e\u5faa\u7ebf\u548c\u63a8\u7269\u4efb\u52a1\uff09\u548c\u4e03\u5173\u8282\u673a\u68b0\u81c2\uff08\u7528\u4e8e\u7b1b\u5361\u5c14\u4f4d\u7f6e\u63a7\u5236\u548c\u5173\u8282\u89d2\u5ea6\u63a7\u5236\u4efb\u52a1\uff09\uff0c\u63d0\u4f9b\u5bb9\u5668\u5316\u90e8\u7f72\u65b9\u6848\u3002", "result": "CRoSS\u652f\u6301\u9ad8\u7269\u7406\u771f\u5b9e\u5ea6\u4eff\u771f\u548c\u51e0\u4e4e\u4efb\u610f\u4f20\u611f\u5668\u4f7f\u7528\uff0c\u673a\u68b0\u81c2\u57fa\u51c6\u63d0\u4f9b\u65e0\u9700\u7269\u7406\u4eff\u771f\u7684\u8fd0\u52a8\u5b66\u53d8\u4f53\uff08\u8fd0\u884c\u901f\u5ea6\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86DQN\u548c\u7b56\u7565\u68af\u5ea6\u7b49\u6807\u51c6RL\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "CRoSS\u4e3a\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u652f\u6301\u53d7\u63a7\u7814\u7a76\u548c\u4f20\u611f\u5668\u591a\u6837\u6027\u8bc4\u4f30\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.04737", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04737", "abs": "https://arxiv.org/abs/2602.04737", "authors": ["Kejiang Qian", "Amos Storkey", "Fengxiang He"], "title": "Rationality Measurement and Theory for Reinforcement Learning Agents", "comment": null, "summary": "This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy's actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm's generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u7406\u6027\u5ea6\u91cf\u6307\u6807\u53ca\u76f8\u5173\u7406\u8bba\uff0c\u5b9a\u4e49\u4e86\u5b8c\u7f8e\u7406\u6027\u52a8\u4f5c\u3001\u671f\u671b\u7406\u6027\u98ce\u9669\u3001\u7406\u6027\u98ce\u9669\u95f4\u9699\u7b49\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u73af\u5883\u8f6c\u79fb\u548c\u7b97\u6cd5\u6cdb\u5316\u6027\u5bf9\u7406\u6027\u98ce\u9669\u7684\u5f71\u54cd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u7406\u6027\u5c5e\u6027\u65e5\u76ca\u91cd\u8981\u4f46\u9c9c\u6709\u7814\u7a76\uff0c\u9700\u8981\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6765\u91cf\u5316\u667a\u80fd\u4f53\u5728\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u7406\u6027\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5f71\u54cd\u7406\u6027\u7684\u56e0\u7d20\u3002", "method": "\u5b9a\u4e49\u4e86\u5b8c\u7f8e\u7406\u6027\u52a8\u4f5c\uff08\u6700\u5927\u5316\u9690\u85cf\u771f\u5b9e\u4ef7\u503c\u51fd\u6570\u7684\u6700\u901f\u65b9\u5411\uff09\uff0c\u63d0\u51fa\u4e86\u671f\u671b\u7406\u6027\u98ce\u9669\u548c\u7406\u6027\u98ce\u9669\u95f4\u9699\u7684\u6982\u5ff5\uff0c\u5c06\u98ce\u9669\u95f4\u9699\u5206\u89e3\u4e3a\u73af\u5883\u8f6c\u79fb\u5f15\u8d77\u7684\u5916\u5728\u6210\u5206\u548c\u7b97\u6cd5\u6cdb\u5316\u6027\u5f15\u8d77\u7684\u5185\u5728\u6210\u5206\uff0c\u5e76\u75281-Wasserstein\u8ddd\u79bb\u548c\u7ecf\u9a8cRademacher\u590d\u6742\u5ea6\u5206\u522b\u4e0a\u754c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u6b63\u5219\u5316\u5668\uff08\u5c42\u5f52\u4e00\u5316\u3001\u21132\u6b63\u5219\u5316\u3001\u6743\u91cd\u5f52\u4e00\u5316\uff09\u548c\u9886\u57df\u968f\u673a\u5316\u6709\u52a9\u4e8e\u964d\u4f4e\u7406\u6027\u98ce\u9669\uff0c\u800c\u73af\u5883\u8f6c\u79fb\u4f1a\u5e26\u6765\u5371\u5bb3\u3002\u5b9e\u9a8c\u5b8c\u5168\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5047\u8bbe\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u7406\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u7406\u8bba\u6846\u67b6\u548c\u5ea6\u91cf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u7406\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u4e3a\u6539\u8fdb\u667a\u80fd\u4f53\u7406\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2602.04807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04807", "abs": "https://arxiv.org/abs/2602.04807", "authors": ["Wolfgang Maass", "Sabine Janzen", "Prajvi Saxena", "Sach Mukherjee"], "title": "Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning", "comment": "16 pages, 6 figures", "summary": "We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.", "AI": {"tldr": "Afferent Learning\u6846\u67b6\u901a\u8fc7\u8fdb\u5316\u4f18\u5316\u53d1\u73b0\u8ba1\u7b97\u4f20\u5165\u75d5\u8ff9(CATs)\u4f5c\u4e3a\u5185\u90e8\u98ce\u9669\u4fe1\u53f7\uff0c\u7528\u4e8e\u635f\u4f24\u907f\u514d\u5b66\u4e60\uff0c\u5728\u751f\u7269\u529b\u5b66\u6570\u5b57\u5b6a\u751f\u4e2d\u957f\u671f\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5e74\u9f84\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7\u751f\u7269\u7cfb\u7edf\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u4e3a\u635f\u4f24\u907f\u514d\u5b66\u4e60\u63d0\u4f9b\u6709\u6548\u7684\u5185\u90e8\u98ce\u9669\u4fe1\u53f7\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u671f\u3001\u590d\u6742\u73af\u5883\u4e2d\u5b66\u4e60\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u529b\u5b66\u6570\u5b57\u5b6a\u751f\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff1a\u5916\u5c42\u8fdb\u5316\u4f18\u5316\u53d1\u73b0\u4f20\u5165\u611f\u77e5\u67b6\u6784\uff0c\u5185\u5c42\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u8fd9\u4e9b\u4fe1\u53f7\u8bad\u7ec3\u635f\u4f24\u907f\u514d\u7b56\u7565\u3002\u5c06\u4f20\u5165\u611f\u77e5\u5f62\u5f0f\u5316\u4e3a\u63d0\u4f9b\u9ad8\u6548\u5b66\u4e60\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u67b6\u6784\u9009\u62e9\u57fa\u4e8e\u5176\u80fd\u5426\u652f\u6301\u6709\u6548\u5b66\u4e60\u800c\u975e\u76f4\u63a5\u6700\u5c0f\u5316\u635f\u4f24\u3002", "result": "\u5728\u751f\u7269\u529b\u5b66\u6570\u5b57\u5b6a\u751f\u957f\u671f\u5e94\u7528\uff08\u6570\u5341\u5e74\u751f\u547d\u5386\u7a0b\uff09\u4e2d\uff0c\u57fa\u4e8eCAT\u7684\u8fdb\u5316\u67b6\u6784\u6bd4\u624b\u5de5\u8bbe\u8ba1\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff0823%\u9ad8\u98ce\u9669\u52a8\u4f5c\u51cf\u5c11\uff09\u548c\u5e74\u9f84\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u5e74\u9f84\u4f9d\u8d56\u7684\u884c\u4e3a\u9002\u5e94\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86CAT\u4fe1\u53f7\u3001\u8fdb\u5316\u548c\u9884\u6d4b\u5dee\u5f02\u7684\u91cd\u8981\u6027\u3002", "conclusion": "Afferent Learning\u6846\u67b6\u901a\u8fc7\u8fdb\u5316\u4f18\u5316\u7684\u4f20\u5165\u611f\u77e5\u67b6\u6784\u4e3a\u635f\u4f24\u907f\u514d\u5b66\u4e60\u63d0\u4f9b\u6709\u6548\u7684\u5185\u90e8\u98ce\u9669\u4fe1\u53f7\uff0c\u5728\u957f\u671f\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u751f\u7269\u542f\u53d1\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.2f526cb1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.22975%3Futm_source=tldrai/1/0100019c23f5d0c6-e04ce42e-44af-41ac-a7d0-ecb3f360f873-000000/4Wn8hpsfttyu2CyC3H5AjQaN81PJvOevM7eS1HM4Zy4=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.22975%3Futm_source=tldrai/1/0100019c23f5d0c6-e04ce42e-44af-41ac-a7d0-ecb3f360f873-000000/4Wn8hpsfttyu2CyC3H5AjQaN81PJvOevM7eS1HM4Zy4=443", "authors": ["TLDR Newsletter"], "title": "NVIDIA proposes Golden Goose: Unlimited RLVR Tasks", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.22975%3Futm_source=tldrai/1/0100019c23f5d0c6-e04ce42e-44af-41ac-a7d0-ecb3f360f873-000000/4Wn8hpsfttyu2CyC3H5AjQaN81PJvOevM7eS1HM4Zy4=443", "summary": "NVIDIA proposes Golden Goose: Unlimited RLVR Tasks (18 minute read) Golden Goose enables the synthesis of large-scale RL with Verifiable Rewards (RLVR) tasks from unverifiable web text. The resulting GooseReason dataset helps revive model performance in math, science, and cybersecurity, surpassing prior state-of-the-art in multiple domains.", "source": "tldr", "AI": {"tldr": "NVIDIA\u63d0\u51faGolden Goose\u6846\u67b6\uff0c\u53ef\u4ece\u4e0d\u53ef\u9a8c\u8bc1\u7684\u7f51\u9875\u6587\u672c\u4e2d\u5408\u6210\u5927\u89c4\u6a21RLVR\u4efb\u52a1\uff0c\u521b\u5efaGooseReason\u6570\u636e\u96c6\uff0c\u5728\u6570\u5b66\u3001\u79d1\u5b66\u548c\u7f51\u7edc\u5b89\u5168\u7b49\u9886\u57df\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u5f53\u524dRLVR\u4efb\u52a1\u89c4\u6a21\u6709\u9650\uff0c\u9700\u8981\u4ece\u5927\u91cf\u4e0d\u53ef\u9a8c\u8bc1\u7684\u7f51\u9875\u6587\u672c\u4e2d\u63d0\u53d6\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u3001\u79d1\u5b66\u548c\u7f51\u7edc\u5b89\u5168\u7b49\u9886\u57df\u7684\u6027\u80fd", "method": "\u63d0\u51faGolden Goose\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u4e0d\u53ef\u9a8c\u8bc1\u7684\u7f51\u9875\u6587\u672c\u4e2d\u81ea\u52a8\u5408\u6210\u5927\u89c4\u6a21RLVR\u4efb\u52a1\uff0c\u751f\u6210GooseReason\u6570\u636e\u96c6", "result": "\u751f\u6210\u7684GooseReason\u6570\u636e\u96c6\u5728\u6570\u5b66\u3001\u79d1\u5b66\u548c\u7f51\u7edc\u5b89\u5168\u7b49\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd", "conclusion": "Golden Goose\u6846\u67b6\u80fd\u591f\u6709\u6548\u4ece\u4e0d\u53ef\u9a8c\u8bc1\u7684\u7f51\u7edc\u6587\u672c\u4e2d\u751f\u6210\u5927\u89c4\u6a21RLVR\u4efb\u52a1\uff0c\u4e3a\u6a21\u578b\u5728\u591a\u4e2a\u5173\u952e\u9886\u57df\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u6765\u6e90", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.d1b124f4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftech.slashdot.org%2Fstory%2F26%2F01%2F28%2F0510226%2Fclawdbot-has-ai-techies-buying-mac-minis%3Futm_source=tldrai/1/0100019c23f5d0c6-e04ce42e-44af-41ac-a7d0-ecb3f360f873-000000/TLFN-EHmuv_eg8Aa7XkLdlIhvM3ADRehyySQsCQOnw8=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftech.slashdot.org%2Fstory%2F26%2F01%2F28%2F0510226%2Fclawdbot-has-ai-techies-buying-mac-minis%3Futm_source=tldrai/1/0100019c23f5d0c6-e04ce42e-44af-41ac-a7d0-ecb3f360f873-000000/TLFN-EHmuv_eg8Aa7XkLdlIhvM3ADRehyySQsCQOnw8=443", "authors": ["TLDR Newsletter"], "title": "Moltbot Has AI Techies Buying Mac Minis 66", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftech.slashdot.org%2Fstory%2F26%2F01%2F28%2F0510226%2Fclawdbot-has-ai-techies-buying-mac-minis%3Futm_source=tldrai/1/0100019c23f5d0c6-e04ce42e-44af-41ac-a7d0-ecb3f360f873-000000/TLFN-EHmuv_eg8Aa7XkLdlIhvM3ADRehyySQsCQOnw8=443", "summary": "Moltbot Has AI Techies Buying Mac Minis 66 (2 minute read) Some people are buying Mac Minis just to host Moltbot (a locally running agent that can wire itself into calendars, messages, and other personal workflows) full-time.", "source": "tldr", "AI": {"tldr": "\u7528\u6237\u8d2d\u4e70Mac Mini\u6765\u672c\u5730\u8fd0\u884cMoltbot\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u53ef\u63a5\u5165\u65e5\u5386\u3001\u6d88\u606f\u7b49\u4e2a\u4eba\u5de5\u4f5c\u6d41", "motivation": "\u4eba\u4eec\u5e0c\u671b\u62e5\u6709\u672c\u5730\u8fd0\u884c\u7684AI\u4ee3\u7406\uff0c\u80fd\u591f\u96c6\u6210\u5230\u4e2a\u4eba\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4fdd\u62a4\u9690\u79c1\u5e76\u5b9e\u73b0\u81ea\u52a8\u5316", "method": "Moltbot\u4f5c\u4e3a\u672c\u5730\u8fd0\u884c\u4ee3\u7406\uff0c\u80fd\u591f\u8fde\u63a5\u65e5\u5386\u3001\u6d88\u606f\u7b49\u4e2a\u4eba\u5e94\u7528\uff0c\u5b9e\u73b0\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316", "result": "\u7528\u6237\u8d2d\u4e70Mac Mini\u4e13\u95e8\u7528\u4e8e\u8fd0\u884cMoltbot\uff0c\u663e\u793a\u4e86\u5bf9\u672c\u5730AI\u4ee3\u7406\u7684\u5f3a\u70c8\u9700\u6c42", "conclusion": "\u672c\u5730\u8fd0\u884c\u7684AI\u4ee3\u7406\u6709\u5e02\u573a\u9700\u6c42\uff0c\u7528\u6237\u613f\u610f\u6295\u8d44\u786c\u4ef6\u6765\u83b7\u5f97\u9690\u79c1\u4fdd\u62a4\u548c\u4e2a\u6027\u5316\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316", "topic": "agent analysis"}}
{"id": "tldr.2602.a3e63221", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview%26utm_content=260204_primary/2/0100019c2864d4ef-978d52b0-ce8f-4d32-b7a4-5e88b8213c69-000000/CnjsMVWh8SmPll-LFzPYJeI4OudIZ1q2rdcZ_NmytsM=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview%26utm_content=260204_primary/2/0100019c2864d4ef-978d52b0-ce8f-4d32-b7a4-5e88b8213c69-000000/CnjsMVWh8SmPll-LFzPYJeI4OudIZ1q2rdcZ_NmytsM=443", "authors": ["TLDR Newsletter"], "title": "Unblocked is AI code review with the taste and judgement of your best engineer", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview%26utm_content=260204_primary/2/0100019c2864d4ef-978d52b0-ce8f-4d32-b7a4-5e88b8213c69-000000/CnjsMVWh8SmPll-LFzPYJeI4OudIZ1q2rdcZ_NmytsM=443", "summary": "Unblocked is AI code review with the taste and judgement of your best engineer (Sponsor) Most AI code review tools analyze the diff. Sometimes the file, occasionally the repo.That's not how experienced engineers work.Instead, they remember the Slack thread that explains this database pattern. They know David on the platform team has strong opinions about error handling. They've internalized dozens of unwritten conventions.Unblocked is the only AI code review tool that uses codebase, docs, and...", "source": "tldr", "AI": {"tldr": "Unblocked\u662f\u4e00\u6b3eAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u901a\u8fc7\u6574\u5408\u4ee3\u7801\u5e93\u3001\u6587\u6863\u548c\u56e2\u961f\u77e5\u8bc6\u6765\u6a21\u62df\u8d44\u6df1\u5de5\u7a0b\u5e08\u7684\u5ba1\u67e5\u65b9\u5f0f", "motivation": "\u73b0\u6709AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u901a\u5e38\u53ea\u5206\u6790\u4ee3\u7801\u5dee\u5f02\u3001\u6587\u4ef6\u6216\u4ed3\u5e93\uff0c\u4f46\u7f3a\u4e4f\u8d44\u6df1\u5de5\u7a0b\u5e08\u6240\u5177\u5907\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff08\u5982Slack\u8ba8\u8bba\u3001\u56e2\u961f\u60ef\u4f8b\u3001\u540c\u4e8b\u504f\u597d\u7b49\uff09\uff0c\u5bfc\u81f4\u5ba1\u67e5\u8d28\u91cf\u6709\u9650", "method": "\u6574\u5408\u4ee3\u7801\u5e93\u3001\u6587\u6863\u548c\u56e2\u961f\u77e5\u8bc6\uff08\u5305\u62ec\u8ba8\u8bba\u8bb0\u5f55\u3001\u540c\u4e8b\u504f\u597d\u3001\u672a\u6210\u6587\u60ef\u4f8b\u7b49\uff09\uff0c\u6784\u5efa\u66f4\u5168\u9762\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u7cfb\u7edf", "result": "\u58f0\u79f0\u662f\u552f\u4e00\u80fd\u591f\u4f7f\u7528\u4ee3\u7801\u5e93\u3001\u6587\u6863\u548c\u56e2\u961f\u77e5\u8bc6\u8fdb\u884cAI\u4ee3\u7801\u5ba1\u67e5\u7684\u5de5\u5177\uff0c\u80fd\u63d0\u4f9b\u66f4\u63a5\u8fd1\u8d44\u6df1\u5de5\u7a0b\u5e08\u7684\u5ba1\u67e5\u8d28\u91cf", "conclusion": "\u901a\u8fc7\u6574\u5408\u66f4\u5e7f\u6cdb\u7684\u56e2\u961f\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\uff0cAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5ba1\u67e5\u8d28\u91cf\uff0c\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8d44\u6df1\u5de5\u7a0b\u5e08\u7684\u6c34\u5e73", "topic": "code agent"}}
{"id": "tldr.2602.9115e9e9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fapple-xcode-claude-agent-sdk%3Futm_source=tldrnewsletter/1/0100019c2864d4ef-978d52b0-ce8f-4d32-b7a4-5e88b8213c69-000000/KPbnrrJQqAjEWY5QwuOy9ftSZPHFWnAAZJ_OLMOzl1U=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fapple-xcode-claude-agent-sdk%3Futm_source=tldrnewsletter/1/0100019c2864d4ef-978d52b0-ce8f-4d32-b7a4-5e88b8213c69-000000/KPbnrrJQqAjEWY5QwuOy9ftSZPHFWnAAZJ_OLMOzl1U=443", "authors": ["TLDR Newsletter"], "title": "Apple's Xcode now supports the Claude Agent SDK", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fapple-xcode-claude-agent-sdk%3Futm_source=tldrnewsletter/1/0100019c2864d4ef-978d52b0-ce8f-4d32-b7a4-5e88b8213c69-000000/KPbnrrJQqAjEWY5QwuOy9ftSZPHFWnAAZJ_OLMOzl1U=443", "summary": "Apple's Xcode now supports the Claude Agent SDK (2 minute read) Xcode 26.3 introduces a native integration with the Claude Agent SDK. Developers now have the full power of Claude Code directly in Xcode without having to leave the IDE. Claude can work autonomously on sophisticated, long-running tasks. The integration supports visual verification with Previews, reasoning across projects, autonomous task execution, and interfacing through the Model Context Protocol. Xcode 26.3 is now available a...", "source": "tldr", "AI": {"tldr": "Xcode 26.3 \u539f\u751f\u96c6\u6210\u4e86 Claude Agent SDK\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u5728 IDE \u5185\u76f4\u63a5\u4f7f\u7528 Claude Code \u7684\u5b8c\u6574\u80fd\u529b\uff0c\u652f\u6301\u81ea\u4e3b\u5904\u7406\u590d\u6742\u957f\u671f\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u5f00\u53d1\u8005\u65e0\u9700\u79bb\u5f00 Xcode IDE \u5c31\u80fd\u5229\u7528 Claude Code \u7684\u5f3a\u5927\u529f\u80fd\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u7279\u522b\u662f\u5904\u7406\u590d\u6742\u3001\u957f\u671f\u8fd0\u884c\u7684\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u539f\u751f\u96c6\u6210 Claude Agent SDK \u5230 Xcode 26.3 \u4e2d\uff0c\u652f\u6301\u89c6\u89c9\u9a8c\u8bc1\u9884\u89c8\u3001\u8de8\u9879\u76ee\u63a8\u7406\u3001\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u8fdb\u884c\u63a5\u53e3\u4ea4\u4e92\u3002", "result": "Xcode 26.3 \u73b0\u5df2\u53ef\u7528\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u5728 Xcode \u4e2d\u76f4\u63a5\u4f7f\u7528 Claude Code \u7684\u5b8c\u6574\u80fd\u529b\uff0c\u5305\u62ec\u81ea\u4e3b\u5904\u7406\u590d\u6742\u4efb\u52a1\u3001\u89c6\u89c9\u9a8c\u8bc1\u7b49\u529f\u80fd\u3002", "conclusion": "Xcode \u4e0e Claude Agent SDK \u7684\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u4f53\u9a8c\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u5728 IDE \u5185\u65e0\u7f1d\u4f7f\u7528 AI \u8f85\u52a9\u7f16\u7a0b\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2602.ed00e37a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260204%26utm_content=std/1/0100019c288bf99c-d67fcde5-a5dc-4dfe-a9f2-93352617770c-000000/DV7Zy8w7lCXR5TUetRlC_-cjX7IhAZ47SkwxNZ9IW7o=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260204%26utm_content=std/1/0100019c288bf99c-d67fcde5-a5dc-4dfe-a9f2-93352617770c-000000/DV7Zy8w7lCXR5TUetRlC_-cjX7IhAZ47SkwxNZ9IW7o=443", "authors": ["TLDR Newsletter"], "title": "Cut your dev loop from hours to seconds", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260204%26utm_content=std/1/0100019c288bf99c-d67fcde5-a5dc-4dfe-a9f2-93352617770c-000000/DV7Zy8w7lCXR5TUetRlC_-cjX7IhAZ47SkwxNZ9IW7o=443", "summary": "Cut your dev loop from hours to seconds (Sponsor) mirrord (4.9k GitHub stars) lets you run your microservice locally with access to everything in the cloud, speeding up development, improving code quality, and reducing cloud costs. It's used by companies like monday.com, which reduced dev cycle time by 70%. Learn more about mirrord.", "source": "tldr", "AI": {"tldr": "mirrord\u5de5\u5177\u8ba9\u5f00\u53d1\u8005\u80fd\u5728\u672c\u5730\u8fd0\u884c\u5fae\u670d\u52a1\u65f6\u8bbf\u95ee\u4e91\u7aef\u6240\u6709\u8d44\u6e90\uff0c\u5c06\u5f00\u53d1\u5468\u671f\u4ece\u5c0f\u65f6\u7ea7\u7f29\u77ed\u5230\u79d2\u7ea7\uff0c\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u5e76\u964d\u4f4e\u4e91\u6210\u672c", "motivation": "\u89e3\u51b3\u5fae\u670d\u52a1\u5f00\u53d1\u4e2d\u672c\u5730\u73af\u5883\u4e0e\u4e91\u7aef\u73af\u5883\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u5f00\u53d1\u5468\u671f\u65f6\u95f4\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "method": "\u901a\u8fc7mirrord\u5de5\u5177\u5728\u672c\u5730\u8fd0\u884c\u5fae\u670d\u52a1\u65f6\u63d0\u4f9b\u5bf9\u4e91\u7aef\u6240\u6709\u8d44\u6e90\u7684\u8bbf\u95ee\u80fd\u529b", "result": "monday.com\u7b49\u516c\u53f8\u4f7f\u7528\u540e\u5f00\u53d1\u5468\u671f\u65f6\u95f4\u51cf\u5c1170%\uff0cGitHub\u661f\u68074.9k", "conclusion": "mirrord\u80fd\u663e\u8457\u63d0\u5347\u5fae\u670d\u52a1\u5f00\u53d1\u6548\u7387\uff0c\u7f29\u77ed\u5f00\u53d1\u5468\u671f\uff0c\u964d\u4f4e\u6210\u672c", "topic": "swe application"}}
{"id": "tldr.2602.6025d439", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zolkos.com%2F2026%2F02%2F04%2Fdeep-dive-how-claude-codes-insights-command-works.html%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/GKLxS-H2vOEbee8SooEQ19pvDnZjhvFlzvdsYUjY29o=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zolkos.com%2F2026%2F02%2F04%2Fdeep-dive-how-claude-codes-insights-command-works.html%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/GKLxS-H2vOEbee8SooEQ19pvDnZjhvFlzvdsYUjY29o=443", "authors": ["TLDR Newsletter"], "title": "Deep Dive: How Claude Code's /insights Command Works", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zolkos.com%2F2026%2F02%2F04%2Fdeep-dive-how-claude-codes-insights-command-works.html%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/GKLxS-H2vOEbee8SooEQ19pvDnZjhvFlzvdsYUjY29o=443", "summary": "Deep Dive: How Claude Code's /insights Command Works (14 minute read) The `/insights` command in Claude Code generates an HTML report that analyzes users' interaction patterns across all their sessions. This process involves filtering session logs, extracting metadata, and using an LLM to perform qualitative \"facet extraction\" on session transcripts. The aggregated data and LLM insights then identify interaction styles, successful workflows, and friction points, and proposes actionable sugges...", "source": "tldr", "AI": {"tldr": "Claude Code\u7684/insights\u547d\u4ee4\u901a\u8fc7\u5206\u6790\u7528\u6237\u4f1a\u8bdd\u65e5\u5fd7\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u751f\u6210HTML\u62a5\u544a\u6765\u8bc6\u522b\u7528\u6237\u4ea4\u4e92\u6a21\u5f0f\u3001\u6210\u529f\u5de5\u4f5c\u6d41\u7a0b\u548c\u6469\u64e6\u70b9\uff0c\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u5efa\u8bae\u3002", "motivation": "\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u81ea\u5df1\u5728\u4ee3\u7801\u5f00\u53d1\u4e2d\u7684\u4ea4\u4e92\u6a21\u5f0f\uff0c\u8bc6\u522b\u9ad8\u6548\u5de5\u4f5c\u6d41\u7a0b\u548c\u6f5c\u5728\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u8fc7\u6ee4\u4f1a\u8bdd\u65e5\u5fd7\u3001\u63d0\u53d6\u5143\u6570\u636e\uff0c\u4f7f\u7528LLM\u5bf9\u4f1a\u8bdd\u8bb0\u5f55\u8fdb\u884c\"\u9762\u63d0\u53d6\"\u5b9a\u6027\u5206\u6790\uff0c\u805a\u5408\u6570\u636e\u5e76\u751f\u6210\u6d1e\u5bdf\u62a5\u544a\u3002", "result": "\u751f\u6210\u5305\u542b\u7528\u6237\u4ea4\u4e92\u98ce\u683c\u3001\u6210\u529f\u5de5\u4f5c\u6d41\u7a0b\u3001\u6469\u64e6\u70b9\u548c\u53ef\u64cd\u4f5c\u5efa\u8bae\u7684HTML\u62a5\u544a\uff0c\u5e2e\u52a9\u7528\u6237\u4f18\u5316\u5f00\u53d1\u8fc7\u7a0b\u3002", "conclusion": "/insights\u547d\u4ee4\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u4f1a\u8bdd\u5206\u6790\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5f00\u53d1\u8fc7\u7a0b\u6d1e\u5bdf\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4ee3\u7801\u5f00\u53d1\u6548\u7387\u548c\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "tldr.2602.5e000e6f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fsubagents%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/e0y67qVPRfe3bginhey17YksMQT4zLvwjN3pk7XcIoY=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fsubagents%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/e0y67qVPRfe3bginhey17YksMQT4zLvwjN3pk7XcIoY=443", "authors": ["TLDR Newsletter"], "title": "Subagents: When and How to Use Them", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fsubagents%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/e0y67qVPRfe3bginhey17YksMQT4zLvwjN3pk7XcIoY=443", "summary": "Subagents: When and How to Use Them (10 minute read) Subagents are specialist agents that your main AI agent can spawn to handle focused tasks in their own clean context, keeping your main conversation readable instead of cluttered with noise. The two useful patterns are chaining them sequentially (like scout files \u2192 implement \u2192 verify) or running them in parallel for exploration.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u5b50\u4ee3\u7406\u7684\u6982\u5ff5\u53ca\u5176\u4f7f\u7528\u6a21\u5f0f\uff0c\u5b50\u4ee3\u7406\u662f\u4e3bAI\u4ee3\u7406\u53ef\u4ee5\u751f\u6210\u7684\u4e13\u95e8\u4ee3\u7406\uff0c\u7528\u4e8e\u5728\u72ec\u7acb\u4e0a\u4e0b\u6587\u4e2d\u5904\u7406\u7279\u5b9a\u4efb\u52a1\uff0c\u4fdd\u6301\u4e3b\u5bf9\u8bdd\u6e05\u6670\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u4e3b\u5bf9\u8bdd\u5bb9\u6613\u53d8\u5f97\u6742\u4e71\u65e0\u7ae0\uff0c\u5305\u542b\u5927\u91cf\u4e2d\u95f4\u6b65\u9aa4\u548c\u566a\u97f3\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4fdd\u6301\u4e3b\u5bf9\u8bdd\u7684\u6574\u6d01\u6027\uff0c\u540c\u65f6\u9ad8\u6548\u5904\u7406\u4e13\u95e8\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u5b50\u4ee3\u7406\u6a21\u5f0f\uff1a1\uff09\u987a\u5e8f\u94fe\u5f0f\u6a21\u5f0f\uff08\u5982\u4fa6\u5bdf\u6587\u4ef6\u2192\u5b9e\u65bd\u2192\u9a8c\u8bc1\uff09\uff0c2\uff09\u5e76\u884c\u63a2\u7d22\u6a21\u5f0f\u3002\u5b50\u4ee3\u7406\u4f5c\u4e3a\u4e13\u95e8\u4ee3\u7406\u5728\u72ec\u7acb\u4e0a\u4e0b\u6587\u4e2d\u8fd0\u884c\uff0c\u5b8c\u6210\u4efb\u52a1\u540e\u8fd4\u56de\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u5b50\u4ee3\u7406\u6a21\u5f0f\uff0c\u53ef\u4ee5\u4fdd\u6301\u4e3b\u5bf9\u8bdd\u7684\u6e05\u6670\u5ea6\uff0c\u63d0\u9ad8\u4efb\u52a1\u5904\u7406\u7684\u6a21\u5757\u5316\u548c\u6548\u7387\uff0c\u540c\u65f6\u652f\u6301\u590d\u6742\u7684\u4efb\u52a1\u5206\u89e3\u548c\u5e76\u884c\u5904\u7406\u3002", "conclusion": "\u5b50\u4ee3\u7406\u662f\u6709\u6548\u7684AI\u4ee3\u7406\u67b6\u6784\u6a21\u5f0f\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u4e13\u95e8\u5316\u5904\u7406\uff0c\u65e2\u80fd\u4fdd\u6301\u4e3b\u5bf9\u8bdd\u7684\u6574\u6d01\u6027\uff0c\u53c8\u80fd\u63d0\u9ad8\u590d\u6742\u4efb\u52a1\u7684\u5904\u7406\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.b7395dd8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.apple.com%2Fnewsroom%2F2026%2F02%2Fxcode-26-point-3-unlocks-the-power-of-agentic-coding%2F%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/B-KiB4Ckcta9EVyF37-bjnxjGs2gWxsKpcidXLkftV4=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.apple.com%2Fnewsroom%2F2026%2F02%2Fxcode-26-point-3-unlocks-the-power-of-agentic-coding%2F%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/B-KiB4Ckcta9EVyF37-bjnxjGs2gWxsKpcidXLkftV4=443", "authors": ["TLDR Newsletter"], "title": "Xcode 26.3 unlocks the power of agentic coding", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.apple.com%2Fnewsroom%2F2026%2F02%2Fxcode-26-point-3-unlocks-the-power-of-agentic-coding%2F%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/B-KiB4Ckcta9EVyF37-bjnxjGs2gWxsKpcidXLkftV4=443", "summary": "Xcode 26.3 unlocks the power of agentic coding (6 minute read) Xcode 26.3 introduces agentic coding, allowing developers to integrate AI agents directly into their app development workflow. This new capability enables agents like Anthropic's Claude Agent and OpenAI's Codex to autonomously tackle complex tasks, interact with project architecture, and visually verify their work.", "source": "tldr", "AI": {"tldr": "Xcode 26.3\u5f15\u5165\u4ee3\u7406\u5f0f\u7f16\u7801\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u5c06AI\u4ee3\u7406\u76f4\u63a5\u96c6\u6210\u5230\u5e94\u7528\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f7f\u4ee3\u7406\u80fd\u81ea\u4e3b\u5904\u7406\u590d\u6742\u4efb\u52a1\u3001\u4e0e\u9879\u76ee\u67b6\u6784\u4ea4\u4e92\u5e76\u53ef\u89c6\u5316\u9a8c\u8bc1\u5de5\u4f5c", "motivation": "\u4f20\u7edf\u5f00\u53d1\u5de5\u5177\u7f3a\u4e4f\u4e0eAI\u4ee3\u7406\u7684\u6df1\u5ea6\u96c6\u6210\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528AI\u5728\u4ee3\u7801\u751f\u6210\u3001\u67b6\u6784\u4ea4\u4e92\u548c\u53ef\u89c6\u5316\u9a8c\u8bc1\u65b9\u9762\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5f00\u53d1\u6548\u7387\u548c\u81ea\u52a8\u5316\u6c34\u5e73", "method": "\u5728Xcode 26.3\u4e2d\u96c6\u6210\u4ee3\u7406\u5f0f\u7f16\u7801\u529f\u80fd\uff0c\u652f\u6301Anthropic\u7684Claude Agent\u548cOpenAI\u7684Codex\u7b49AI\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u76f4\u63a5\u4e0e\u5f00\u53d1\u73af\u5883\u4ea4\u4e92\uff0c\u5904\u7406\u590d\u6742\u5f00\u53d1\u4efb\u52a1", "result": "\u5f00\u53d1\u8005\u73b0\u5728\u53ef\u4ee5\u5c06AI\u4ee3\u7406\u65e0\u7f1d\u96c6\u6210\u5230Xcode\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4ee3\u7406\u80fd\u591f\u81ea\u4e3b\u5904\u7406\u590d\u6742\u4efb\u52a1\u3001\u4e0e\u9879\u76ee\u67b6\u6784\u4ea4\u4e92\u5e76\u8fdb\u884c\u53ef\u89c6\u5316\u9a8c\u8bc1\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387", "conclusion": "Xcode 26.3\u7684\u4ee3\u7406\u5f0f\u7f16\u7801\u529f\u80fd\u4ee3\u8868\u4e86\u5f00\u53d1\u5de5\u5177\u4e0eAI\u4ee3\u7406\u6df1\u5ea6\u96c6\u6210\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u3001\u81ea\u52a8\u5316\u7684\u5f00\u53d1\u4f53\u9a8c", "topic": "swe application"}}
{"id": "tldr.2602.381f3a17", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.wordpress.com%2F2026%2F02%2F03%2Funless-that-claw-is-the-famous-openclaw%2F%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/Yno4AnQMFHOv3hvVGe32Nk4Zj_cmxiLdpZH8dP8Edoo=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.wordpress.com%2F2026%2F02%2F03%2Funless-that-claw-is-the-famous-openclaw%2F%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/Yno4AnQMFHOv3hvVGe32Nk4Zj_cmxiLdpZH8dP8Edoo=443", "authors": ["TLDR Newsletter"], "title": "Unless That Claw Is The Famous OpenClaw", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.wordpress.com%2F2026%2F02%2F03%2Funless-that-claw-is-the-famous-openclaw%2F%3Futm_source=tldrdev/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/Yno4AnQMFHOv3hvVGe32Nk4Zj_cmxiLdpZH8dP8Edoo=443", "summary": "Unless That Claw Is The Famous OpenClaw (8 minute read) OpenClaw, an autonomous AI agent with full shell access and browser control, has security risks, including prompt injection and session theft, so developers must use dedicated hardware and burner accounts to keep their main accounts safe.", "source": "tldr", "AI": {"tldr": "OpenClaw\u662f\u4e00\u4e2a\u5177\u6709\u5b8c\u6574shell\u8bbf\u95ee\u548c\u6d4f\u89c8\u5668\u63a7\u5236\u529f\u80fd\u7684\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5f00\u53d1\u8005\u9700\u8981\u4f7f\u7528\u4e13\u7528\u786c\u4ef6\u548c\u4e34\u65f6\u8d26\u6237\u6765\u4fdd\u62a4\u4e3b\u8d26\u6237\u5b89\u5168\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63ed\u793aOpenClaw\u8fd9\u7c7b\u81ea\u4e3bAI\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u7279\u522b\u662f\u63d0\u793a\u6ce8\u5165\u548c\u4f1a\u8bdd\u7a83\u53d6\u7b49\u6f0f\u6d1e\uff0c\u63d0\u9192\u5f00\u53d1\u8005\u91c7\u53d6\u9632\u62a4\u63aa\u65bd\u3002", "method": "\u901a\u8fc7\u5206\u6790OpenClaw\u7684\u529f\u80fd\u7279\u6027\uff08\u5b8c\u6574shell\u8bbf\u95ee\u548c\u6d4f\u89c8\u5668\u63a7\u5236\uff09\uff0c\u8bc6\u522b\u5176\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u5b89\u5168\u9632\u62a4\u5efa\u8bae\u3002", "result": "\u8bc6\u522b\u51faOpenClaw\u5b58\u5728\u591a\u79cd\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3001\u4f1a\u8bdd\u7a83\u53d6\u7b49\uff0c\u8fd9\u4e9b\u6f0f\u6d1e\u53ef\u80fd\u88ab\u6076\u610f\u5229\u7528\u3002", "conclusion": "\u5f00\u53d1\u8005\u5728\u4f7f\u7528OpenClaw\u7b49\u81ea\u4e3bAI\u4ee3\u7406\u65f6\u5fc5\u987b\u91c7\u53d6\u4e25\u683c\u7684\u5b89\u5168\u63aa\u65bd\uff0c\u5305\u62ec\u4f7f\u7528\u4e13\u7528\u786c\u4ef6\u548c\u4e34\u65f6\u8d26\u6237\uff0c\u4ee5\u4fdd\u62a4\u4e3b\u8d26\u6237\u548c\u7cfb\u7edf\u5b89\u5168\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.22c1256e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/f8fsopq2_gv0GeeAH_eGudFjymkLJg6kc8mmlSXSlSE=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/f8fsopq2_gv0GeeAH_eGudFjymkLJg6kc8mmlSXSlSE=443", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/f8fsopq2_gv0GeeAH_eGudFjymkLJg6kc8mmlSXSlSE=443", "summary": "Unless That Claw Is The Famous OpenClaw (8 minute read) OpenClaw, an autonomous AI agent with full shell access and browser control, has security risks, including prompt injection and session theft, so developers must use dedicated hardware and burner accounts to keep their main accounts safe.", "source": "tldr", "AI": {"tldr": "OpenClaw\u662f\u4e00\u4e2a\u5177\u6709\u5b8c\u6574shell\u8bbf\u95ee\u548c\u6d4f\u89c8\u5668\u63a7\u5236\u529f\u80fd\u7684\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5f00\u53d1\u8005\u9700\u4f7f\u7528\u4e13\u7528\u786c\u4ef6\u548c\u4e34\u65f6\u8d26\u6237\u6765\u4fdd\u62a4\u4e3b\u8d26\u6237\u5b89\u5168\u3002", "motivation": "OpenClaw\u4f5c\u4e3a\u5177\u6709\u5b8c\u6574\u7cfb\u7edf\u8bbf\u95ee\u6743\u9650\u7684AI\u4ee3\u7406\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u7814\u7a76\u5176\u5b89\u5168\u98ce\u9669\u5e76\u63d0\u51fa\u9632\u62a4\u63aa\u65bd\u3002", "method": "\u5206\u6790OpenClaw\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u63d0\u793a\u6ce8\u5165\u548c\u4f1a\u8bdd\u7a83\u53d6\u7b49\u653b\u51fb\u5411\u91cf\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u4e13\u7528\u786c\u4ef6\u548c\u4e34\u65f6\u8d26\u6237\u7684\u5b89\u5168\u5b9e\u8df5\u3002", "result": "\u8bc6\u522b\u4e86OpenClaw\u7684\u591a\u9879\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u9632\u62a4\u5efa\u8bae\uff0c\u5f3a\u8c03\u5f00\u53d1\u8005\u9700\u8981\u91c7\u53d6\u989d\u5916\u5b89\u5168\u63aa\u65bd\u6765\u4fdd\u62a4\u4e3b\u8d26\u6237\u3002", "conclusion": "OpenClaw\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5b89\u5168\u98ce\u9669\uff0c\u5f00\u53d1\u8005\u5fc5\u987b\u91c7\u53d6\u4e25\u683c\u7684\u9694\u79bb\u63aa\u65bd\u6765\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.68fa32ec", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/LvCNiU_jTLVPvHOTbJC-ZVBHtqi1fwcSI4PxpzP6_HQ=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/LvCNiU_jTLVPvHOTbJC-ZVBHtqi1fwcSI4PxpzP6_HQ=443", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/LvCNiU_jTLVPvHOTbJC-ZVBHtqi1fwcSI4PxpzP6_HQ=443", "summary": "Unless That Claw Is The Famous OpenClaw (8 minute read) OpenClaw, an autonomous AI agent with full shell access and browser control, has security risks, including prompt injection and session theft, so developers must use dedicated hardware and burner accounts to keep their main accounts safe.", "source": "tldr", "AI": {"tldr": "OpenClaw\u662f\u4e00\u4e2a\u5177\u6709\u5b8c\u6574shell\u8bbf\u95ee\u548c\u6d4f\u89c8\u5668\u63a7\u5236\u80fd\u529b\u7684\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u63d0\u793a\u6ce8\u5165\u548c\u4f1a\u8bdd\u7a83\u53d6\uff0c\u5f00\u53d1\u8005\u9700\u8981\u4f7f\u7528\u4e13\u7528\u786c\u4ef6\u548c\u4e00\u6b21\u6027\u8d26\u6237\u6765\u4fdd\u62a4\u4e3b\u8d26\u6237\u5b89\u5168\u3002", "motivation": "\u5206\u6790OpenClaw\u8fd9\u7c7b\u5177\u6709\u7cfb\u7edf\u7ea7\u8bbf\u95ee\u6743\u9650\u7684AI\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5b89\u5168\u4f7f\u7528\u6307\u5357\u3002", "method": "\u901a\u8fc7\u5206\u6790OpenClaw\u7684\u529f\u80fd\u7279\u6027\uff08\u5b8c\u6574shell\u8bbf\u95ee\u3001\u6d4f\u89c8\u5668\u63a7\u5236\uff09\u8bc6\u522b\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u51fa\u5177\u4f53\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "result": "\u8bc6\u522b\u51faOpenClaw\u5b58\u5728\u63d0\u793a\u6ce8\u5165\u548c\u4f1a\u8bdd\u7a83\u53d6\u7b49\u5b89\u5168\u98ce\u9669\uff0c\u5efa\u8bae\u4f7f\u7528\u4e13\u7528\u786c\u4ef6\u548c\u4e00\u6b21\u6027\u8d26\u6237\u4f5c\u4e3a\u9632\u62a4\u63aa\u65bd\u3002", "conclusion": "\u5177\u6709\u7cfb\u7edf\u7ea7\u6743\u9650\u7684AI\u4ee3\u7406\u5b58\u5728\u663e\u8457\u5b89\u5168\u98ce\u9669\uff0c\u5f00\u53d1\u8005\u9700\u8981\u91c7\u53d6\u9694\u79bb\u63aa\u65bd\u6765\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u548c\u8d26\u6237\u5b89\u5168\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.58e16f62", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/o1ydGx2CiD6Ho0RpOLNtTiDNHAuTBpSffWjl_j760wg=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/o1ydGx2CiD6Ho0RpOLNtTiDNHAuTBpSffWjl_j760wg=443", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c28a3ab93-e4aacd5c-ede2-433c-8167-1771fcc3c925-000000/o1ydGx2CiD6Ho0RpOLNtTiDNHAuTBpSffWjl_j760wg=443", "summary": "Unless That Claw Is The Famous OpenClaw (8 minute read) OpenClaw, an autonomous AI agent with full shell access and browser control, has security risks, including prompt injection and session theft, so developers must use dedicated hardware and burner accounts to keep their main accounts safe.", "source": "tldr", "AI": {"tldr": "OpenClaw\u662f\u4e00\u4e2a\u5177\u6709\u5b8c\u6574shell\u8bbf\u95ee\u548c\u6d4f\u89c8\u5668\u63a7\u5236\u80fd\u529b\u7684\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5f00\u53d1\u8005\u9700\u8981\u4f7f\u7528\u4e13\u7528\u786c\u4ef6\u548c\u4e34\u65f6\u8d26\u6237\u6765\u4fdd\u62a4\u4e3b\u8d26\u6237\u5b89\u5168\u3002", "motivation": "\u5206\u6790OpenClaw\u8fd9\u7c7b\u81ea\u4e3bAI\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u7279\u522b\u662f\u63d0\u793a\u6ce8\u5165\u548c\u4f1a\u8bdd\u7a83\u53d6\u7b49\u5a01\u80c1\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5b89\u5168\u4f7f\u7528\u5efa\u8bae\u3002", "method": "\u901a\u8fc7\u5206\u6790OpenClaw\u7684\u529f\u80fd\u7279\u6027\uff08\u5b8c\u6574shell\u8bbf\u95ee\u3001\u6d4f\u89c8\u5668\u63a7\u5236\uff09\uff0c\u8bc6\u522b\u5176\u6f5c\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "result": "\u8bc6\u522b\u51faOpenClaw\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u548c\u4f1a\u8bdd\u7a83\u53d6\uff0c\u5efa\u8bae\u5f00\u53d1\u8005\u4f7f\u7528\u4e13\u7528\u786c\u4ef6\u548c\u4e34\u65f6\u8d26\u6237\u6765\u964d\u4f4e\u98ce\u9669\u3002", "conclusion": "\u81ea\u4e3bAI\u4ee3\u7406\u5982OpenClaw\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5b89\u5168\u5a01\u80c1\uff0c\u5fc5\u987b\u91c7\u53d6\u4e25\u683c\u7684\u5b89\u5168\u63aa\u65bd\u624d\u80fd\u5b89\u5168\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.7d43a25f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2141%26utm_source=tldrdesign/1/0100019c28c31fad-81f5f70e-8920-4d87-934e-f6327eda577e-000000/0gq15hYN3XrSrYyWkL49UUJ81KIECRt0vE0F-hcM-Qg=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2141%26utm_source=tldrdesign/1/0100019c28c31fad-81f5f70e-8920-4d87-934e-f6327eda577e-000000/0gq15hYN3XrSrYyWkL49UUJ81KIECRt0vE0F-hcM-Qg=443", "authors": ["TLDR Newsletter"], "title": "Agent Orchestration UI", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2141%26utm_source=tldrdesign/1/0100019c28c31fad-81f5f70e-8920-4d87-934e-f6327eda577e-000000/0gq15hYN3XrSrYyWkL49UUJ81KIECRt0vE0F-hcM-Qg=443", "summary": "Agent Orchestration UI (3 minute read) AI products have evolved from behind-the-scenes models to chat interfaces to agents, and now to agent orchestration, where multiple agents coordinate on unified tasks. Intent by Augment introduces a new UI approach for agent orchestration that focuses on workspaces, context management, and specialized agents working in parallel. The platform uses isolated spaces with living specifications that allow coordinator, implementer, and verifier agents to work t...", "source": "tldr", "AI": {"tldr": "Agent Orchestration UI\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u6237\u754c\u9762\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\uff0c\u901a\u8fc7\u5de5\u4f5c\u7a7a\u95f4\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u5e76\u884c\u4e13\u4e1a\u667a\u80fd\u4f53\u6765\u534f\u8c03\u7edf\u4e00\u4efb\u52a1\u3002", "motivation": "\u968f\u7740AI\u4ea7\u54c1\u4ece\u540e\u53f0\u6a21\u578b\u53d1\u5c55\u5230\u804a\u5929\u754c\u9762\u518d\u5230\u667a\u80fd\u4f53\uff0c\u73b0\u5728\u9700\u8981\u667a\u80fd\u4f53\u7f16\u6392\u6280\u672f\u6765\u8ba9\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u8c03\u5b8c\u6210\u7edf\u4e00\u4efb\u52a1\u3002\u73b0\u6709\u7684\u754c\u9762\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u652f\u6301\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\u3002", "method": "Augment\u7684Intent\u5e73\u53f0\u91c7\u7528\u5de5\u4f5c\u7a7a\u95f4\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u5e76\u884c\u4e13\u4e1a\u667a\u80fd\u4f53\u7684UI\u65b9\u6cd5\u3002\u5e73\u53f0\u4f7f\u7528\u9694\u79bb\u7a7a\u95f4\u548c\u52a8\u6001\u89c4\u8303\uff0c\u8ba9\u534f\u8c03\u8005\u3001\u5b9e\u65bd\u8005\u548c\u9a8c\u8bc1\u8005\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u8be5\u5e73\u53f0\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u5728\u7edf\u4e00\u4efb\u52a1\u4e0a\u7684\u534f\u8c03\u5de5\u4f5c\uff0c\u901a\u8fc7\u4e13\u95e8\u7684UI\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u534f\u540c\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "conclusion": "\u667a\u80fd\u4f53\u7f16\u6392\u662fAI\u4ea7\u54c1\u53d1\u5c55\u7684\u4e0b\u4e00\u4e2a\u9636\u6bb5\uff0c\u9700\u8981\u4e13\u95e8\u7684UI\u8bbe\u8ba1\u6765\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\uff0cIntent\u5e73\u53f0\u4e3a\u6b64\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
