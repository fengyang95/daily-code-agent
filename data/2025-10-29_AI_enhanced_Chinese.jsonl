{"id": "2510.23642", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23642", "abs": "https://arxiv.org/abs/2510.23642", "authors": ["Yuansheng Ni", "Songcheng Cai", "Xiangchao Chen", "Jiarong Liang", "Zhiheng Lyu", "Jiaqi Deng", "Kai Zou", "Ping Nie", "Fei Yuan", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "comment": null, "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86VisCode-Multi-679K\u6570\u636e\u96c6\u3001VisPlotBench\u57fa\u51c6\u548cVisCoder2\u6a21\u578b\uff0c\u7528\u4e8e\u6539\u8fdb\u53ef\u89c6\u5316\u7f16\u7801\u4ee3\u7406\u7684\u591a\u8bed\u8a00\u652f\u6301\u548c\u8fed\u4ee3\u8c03\u8bd5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u7f16\u7801\u4ee3\u7406\u5728\u5b9e\u9645\u5de5\u4f5c\u6d41\u4e2d\u5b58\u5728\u8bed\u8a00\u8986\u76d6\u6709\u9650\u3001\u6267\u884c\u4e0d\u53ef\u9760\u548c\u7f3a\u4e4f\u8fed\u4ee3\u4fee\u6b63\u673a\u5236\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6570\u636e\u96c6\u548c\u57fa\u51c6\u8fc7\u4e8e\u72ed\u7a84\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b679K\u9a8c\u8bc1\u6837\u672c\u7684\u591a\u8bed\u8a00\u53ef\u89c6\u5316\u6570\u636e\u96c6VisCode-Multi-679K\uff0c\u5f00\u53d1\u4e86VisPlotBench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u8bad\u7ec3\u4e86VisCoder2\u7cfb\u5217\u591a\u8bed\u8a00\u53ef\u89c6\u5316\u6a21\u578b\u3002", "result": "VisCoder2\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u63a5\u8fd1GPT-4.1\u6027\u80fd\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u8c03\u8bd5\u572832B\u89c4\u6a21\u8fbe\u523082.4%\u6267\u884c\u901a\u8fc7\u7387\uff0c\u5728\u7b26\u53f7\u6216\u7f16\u8bd1\u5668\u4f9d\u8d56\u8bed\u8a00\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d44\u6e90\u548c\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u89c6\u5316\u7f16\u7801\u4ee3\u7406\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u591a\u8bed\u8a00\u53ef\u89c6\u5316\u751f\u6210\u548c\u8c03\u8bd5\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "2510.23664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23664", "abs": "https://arxiv.org/abs/2510.23664", "authors": ["Eranga Bandara", "Ross Gore", "Xueping Liang", "Sachini Rajapakse", "Isurunima Kularathne", "Pramoda Karunarathna", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Amin Hass", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Agentsway -- Software Development Methodology for AI Agents-based Teams", "comment": null, "summary": "The emergence of Agentic AI is fundamentally transforming how software is\ndesigned, developed, and maintained. Traditional software development\nmethodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for\nhuman-centric teams and are increasingly inadequate in environments where\nautonomous AI agents contribute to planning, coding, testing, and continuous\nlearning. To address this methodological gap, we present \"Agentsway\" a novel\nsoftware development framework designed for ecosystems where AI agents operate\nas first-class collaborators. Agentsway introduces a structured lifecycle\ncentered on human orchestration, and privacy-preserving collaboration among\nspecialized AI agents. The framework defines distinct roles for planning,\nprompting, coding, testing, and fine-tuning agents, each contributing to\niterative improvement and adaptive learning throughout the development process.\nBy integrating fine-tuned LLMs that leverage outputs and feedback from\ndifferent agents throughout the development cycle as part of a retrospective\nlearning process, Agentsway enhances domain-specific reasoning, and explainable\ndecision-making across the entire software development lifecycle. Responsible\nAI principles are further embedded across the agents through the coordinated\nuse of multiple fine-tuned LLMs and advanced reasoning models, ensuring\nbalanced, transparent, and accountable decision-making. This work advances\nsoftware engineering by formalizing agent-centric collaboration, integrating\nprivacy-by-design principles, and defining measurable metrics for productivity\nand trust. Agentsway represents a foundational step toward the next generation\nof AI-native, self-improving software development methodologies. To the best of\nour knowledge, this is the first research effort to introduce a dedicated\nmethodology explicitly designed for AI agent-based software engineering teams.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAgentsway\u7684\u65b0\u578b\u8f6f\u4ef6\u5f00\u53d1\u6846\u67b6\uff0c\u4e13\u95e8\u4e3aAI\u4ee3\u7406\u4f5c\u4e3a\u4e00\u7b49\u534f\u4f5c\u8005\u7684\u73af\u5883\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u547d\u5468\u671f\u548c\u4e13\u4e1a\u5316\u4ee3\u7406\u89d2\u8272\u6765\u6539\u8fdb\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u5982Agile\u3001Kanban\u7b49\u662f\u4e3a\u4eba\u7c7b\u56e2\u961f\u8bbe\u8ba1\u7684\uff0c\u5728AI\u4ee3\u7406\u53c2\u4e0e\u89c4\u5212\u3001\u7f16\u7801\u3001\u6d4b\u8bd5\u548c\u6301\u7eed\u5b66\u4e60\u7684\u73af\u5883\u4e2d\u8d8a\u6765\u8d8a\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u8bba\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "Agentsway\u6846\u67b6\u56f4\u7ed5\u4eba\u7c7b\u7f16\u6392\u548c\u9690\u79c1\u4fdd\u62a4\u534f\u4f5c\u6784\u5efa\uff0c\u5b9a\u4e49\u4e86\u89c4\u5212\u3001\u63d0\u793a\u3001\u7f16\u7801\u3001\u6d4b\u8bd5\u548c\u5fae\u8c03\u7b49\u4e13\u4e1a\u5316\u4ee3\u7406\u89d2\u8272\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u6574\u5408\u4e0d\u540c\u4ee3\u7406\u7684\u8f93\u51fa\u548c\u53cd\u9988\u8fdb\u884c\u56de\u987e\u6027\u5b66\u4e60\u3002", "result": "\u8be5\u6846\u67b6\u589e\u5f3a\u4e86\u9886\u57df\u7279\u5b9a\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\uff0c\u901a\u8fc7\u534f\u8c03\u4f7f\u7528\u591a\u4e2a\u5fae\u8c03LLM\u548c\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u5d4c\u5165\u8d1f\u8d23\u4efbAI\u539f\u5219\uff0c\u786e\u4fdd\u5e73\u8861\u3001\u900f\u660e\u548c\u8d1f\u8d23\u4efb\u7684\u51b3\u7b56\u3002", "conclusion": "Agentsway\u901a\u8fc7\u5f62\u5f0f\u5316\u4ee3\u7406\u4e2d\u5fc3\u534f\u4f5c\u3001\u6574\u5408\u9690\u79c1\u8bbe\u8ba1\u539f\u5219\u548c\u5b9a\u4e49\u53ef\u6d4b\u91cf\u6307\u6807\uff0c\u63a8\u8fdb\u4e86\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u4ee3\u8868\u4e86\u8fc8\u5411AI\u539f\u751f\u3001\u81ea\u6211\u6539\u8fdb\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u7684\u57fa\u7840\u6027\u6b65\u9aa4\u3002", "topic": "swe application"}}
{"id": "2510.23674", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23674", "abs": "https://arxiv.org/abs/2510.23674", "authors": ["Bin Wang", "Hui Li", "AoFan Liu", "BoTao Yang", "Ao Yang", "YiLu Zhong", "Weixiang Huang", "Yanping Zhang", "Runhuai Huang", "Weimin Zeng"], "title": "RefleXGen:The unexamined code is not worth using", "comment": null, "summary": "Security in code generation remains a pivotal challenge when applying large\nlanguage models (LLMs). This paper introduces RefleXGen, an innovative method\nthat significantly enhances code security by integrating Retrieval-Augmented\nGeneration (RAG) techniques with guided self-reflection mechanisms inherent in\nLLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing\nspecialized secure code datasets - processes that can be resource-intensive -\nRefleXGen iteratively optimizes the code generation process through\nself-assessment and reflection without the need for extensive resources. Within\nthis framework, the model continuously accumulates and refines its knowledge\nbase, thereby progressively improving the security of the generated code.\nExperimental results demonstrate that RefleXGen substantially enhances code\nsecurity across multiple models, achieving a 13.6% improvement with GPT-3.5\nTurbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a\n5.8% improvement with Gemini. Our findings highlight that improving the quality\nof model self-reflection constitutes an effective and practical strategy for\nstrengthening the security of AI-generated code.", "AI": {"tldr": "RefleXGen\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5f15\u5bfc\u5f0f\u81ea\u53cd\u601d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\uff0c\u65e0\u9700\u5927\u91cf\u8d44\u6e90\u6295\u5165\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b89\u5168\u6027\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8d44\u6e90\u8fdb\u884c\u5fae\u8c03\u6216\u6784\u5efa\u5b89\u5168\u4ee3\u7801\u6570\u636e\u96c6\u3002", "method": "\u96c6\u6210RAG\u6280\u672f\u548cLLM\u7684\u5f15\u5bfc\u5f0f\u81ea\u53cd\u601d\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u6211\u8bc4\u4f30\u548c\u53cd\u601d\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u663e\u8457\u63d0\u5347\u591a\u4e2a\u6a21\u578b\u7684\u4ee3\u7801\u5b89\u5168\u6027\uff1aGPT-3.5 Turbo\u63d0\u534713.6%\uff0cGPT-4o\u63d0\u53476.7%\uff0cCodeQwen\u63d0\u53474.5%\uff0cGemini\u63d0\u53475.8%\u3002", "conclusion": "\u63d0\u5347\u6a21\u578b\u81ea\u53cd\u601d\u8d28\u91cf\u662f\u589e\u5f3aAI\u751f\u6210\u4ee3\u7801\u5b89\u5168\u6027\u7684\u6709\u6548\u5b9e\u7528\u7b56\u7565\u3002", "topic": "code agent"}}
{"id": "2510.23730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23730", "abs": "https://arxiv.org/abs/2510.23730", "authors": ["Alessandra Terranova", "Bj\u00f6rn Ross", "Alexandra Birch"], "title": "Evaluating Long-Term Memory for Long-Context Question Answering", "comment": "14 pages including appendix, 3 figures. Submitted to October ARR and\n  to Metacognition in Generative AI EurIPS workshop (under review for both)", "summary": "In order for large language models to achieve true conversational continuity\nand benefit from experiential learning, they need memory. While research has\nfocused on the development of complex memory systems, it remains unclear which\ntypes of memory are most effective for long-context conversational tasks. We\npresent a systematic evaluation of memory-augmented methods using LoCoMo, a\nbenchmark of synthetic long-context dialogues annotated for question-answering\ntasks that require diverse reasoning strategies. We analyse full-context\nprompting, semantic memory through retrieval-augmented generation and agentic\nmemory, episodic memory through in-context learning, and procedural memory\nthrough prompt optimization. Our findings show that memory-augmented approaches\nreduce token usage by over 90% while maintaining competitive accuracy. Memory\narchitecture complexity should scale with model capability, with small\nfoundation models benefitting most from RAG, and strong instruction-tuned\nreasoning model gaining from episodic learning through reflections and more\ncomplex agentic semantic memory. In particular, episodic memory can help LLMs\nrecognise the limits of their own knowledge.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u7c7b\u578b\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u80fd\u51cf\u5c1190%\u4ee5\u4e0a\u7684token\u4f7f\u7528\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u771f\u6b63\u7684\u5bf9\u8bdd\u8fde\u7eed\u6027\u548c\u4ece\u7ecf\u9a8c\u5b66\u4e60\u4e2d\u53d7\u76ca\uff0c\u9700\u8981\u6709\u6548\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4f46\u76ee\u524d\u4e0d\u6e05\u695a\u54ea\u79cd\u8bb0\u5fc6\u7c7b\u578b\u5bf9\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4efb\u52a1\u6700\u6709\u6548\u3002", "method": "\u4f7f\u7528LoCoMo\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u5b8c\u6574\u4e0a\u4e0b\u6587\u63d0\u793a\u3001\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u8bed\u4e49\u8bb0\u5fc6\u3001\u4ee3\u7406\u8bb0\u5fc6\u3001\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7247\u6bb5\u8bb0\u5fc6\uff0c\u4ee5\u53ca\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u7684\u7a0b\u5e8f\u8bb0\u5fc6\u3002", "result": "\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u51cf\u5c11token\u4f7f\u7528\u8d85\u8fc790%\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u7387\u3002\u8bb0\u5fc6\u67b6\u6784\u590d\u6742\u5ea6\u5e94\u4e0e\u6a21\u578b\u80fd\u529b\u76f8\u79f0\uff0c\u5c0f\u578b\u57fa\u7840\u6a21\u578b\u4eceRAG\u83b7\u76ca\u6700\u591a\uff0c\u5f3a\u6307\u4ee4\u8c03\u4f18\u63a8\u7406\u6a21\u578b\u4ece\u7247\u6bb5\u5b66\u4e60\u548c\u590d\u6742\u4ee3\u7406\u8bed\u4e49\u8bb0\u5fc6\u83b7\u76ca\u3002", "conclusion": "\u7247\u6bb5\u8bb0\u5fc6\u80fd\u5e2e\u52a9LLM\u8bc6\u522b\u81ea\u8eab\u77e5\u8bc6\u7684\u5c40\u9650\u6027\uff0c\u8bb0\u5fc6\u67b6\u6784\u5e94\u6839\u636e\u6a21\u578b\u80fd\u529b\u8fdb\u884c\u9002\u5f53\u7f29\u653e\u3002", "topic": "agent analysis"}}
{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS\u662f\u4e00\u4e2a\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u952e\u76d8\u9f20\u6807\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u8de8\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u548c\u6a21\u62df\u6e38\u620f\u7684\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709API\u6216GUI\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8de8\u5f02\u6784\u9886\u57df\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\u8303\u5f0f\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u952e\u76d8\u9f20\u6807\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4f7f\u7528500B+ token\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5305\u542b\u8870\u51cf\u6301\u7eed\u635f\u5931\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\u548c\u7a00\u758f\u601d\u8003\u7b56\u7565\u5e73\u8861\u63a8\u7406\u6df1\u5ea6\u4e0e\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u662f\u4e4b\u524dSOTA\u76842\u500d\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7f51\u98753D\u6e38\u620f\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u5728FPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\u3001Gemini-2.5-Pro\u548cClaude-4-Sonnet\u3002", "conclusion": "\u7b80\u5355\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u8868\u793a\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e3a\u5f00\u53d1\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.23761", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23761", "abs": "https://arxiv.org/abs/2510.23761", "authors": ["Kevin Han", "Siddharth Maddikayala", "Tim Knappe", "Om Patel", "Austen Liao", "Amir Barati Farimani"], "title": "TDFlow: Agentic Workflows for Test Driven Software Engineering", "comment": null, "summary": "We introduce TDFlow, a novel test-driven agentic workflow that frames\nrepository-scale software engineering as a test-resolution task, specifically\ndesigned to solve human-written tests. Given a set of tests, TDFlow repeatedly\nproposes, revises, and debugs repository-scale patches using precisely\nengineered sub-agents and tightly constrained tools. The workflow decomposes\nsoftware engineering program repair into four components governed by respective\nsub-agents. This simple, forced decoupling of patch proposing, debugging, patch\nrevision, and optional test generation (1) reduces long-context burden on any\nindividual sub-agent, (2) focuses each sub-agent on specific, pre-defined\nsub-tasks, and (3) allows for specialized performance improvement on specific\nsub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on\nSWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and\n94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within\nSWE-Bench Lite and Verified uncover only 7 instances of test hacking, which\nwere subsequently counted as failures. Furthermore, we show that the primary\nobstacle to human-level software engineering performance lies within writing\nsuccessful reproduction tests. We envision a human-LLM interactive system\npowered by TDFlow where human developers write tests solved by LLM systems.\nTogether, these results indicate that modern LLMs, when embedded in a narrowly\nengineered, test-driven workflow, already achieve human-level test resolution\n-- with the final frontier for fully autonomous repository repair being the\naccurate generation of valid reproduction tests.", "AI": {"tldr": "TDFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d4b\u8bd5\u9a71\u52a8\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5c06\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u91cd\u6784\u4e3a\u6d4b\u8bd5\u89e3\u51b3\u4efb\u52a1\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u5b50\u4ee3\u7406\u548c\u53d7\u9650\u5de5\u5177\u6765\u53cd\u590d\u63d0\u51fa\u3001\u4fee\u8ba2\u548c\u8c03\u8bd5\u4ee3\u7801\u8865\u4e01\uff0c\u5728SWE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u4fee\u590d\u7684\u590d\u6742\u6027\uff0c\u901a\u8fc7\u6d4b\u8bd5\u9a71\u52a8\u65b9\u6cd5\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u51cf\u8f7b\u5355\u4e2a\u4ee3\u7406\u7684\u957f\u4e0a\u4e0b\u6587\u8d1f\u62c5\uff0c\u5e76\u4e13\u6ce8\u4e8e\u7279\u5b9a\u5b50\u4efb\u52a1\u7684\u6027\u80fd\u4f18\u5316\u3002", "method": "\u5c06\u8f6f\u4ef6\u5de5\u7a0b\u7a0b\u5e8f\u4fee\u590d\u5206\u89e3\u4e3a\u56db\u4e2a\u7ec4\u4ef6\uff1a\u8865\u4e01\u63d0\u51fa\u3001\u8c03\u8bd5\u3001\u8865\u4e01\u4fee\u8ba2\u548c\u53ef\u9009\u6d4b\u8bd5\u751f\u6210\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u7531\u4e13\u95e8\u7684\u5b50\u4ee3\u7406\u7ba1\u7406\uff0c\u4f7f\u7528\u7cbe\u786e\u8bbe\u8ba1\u7684\u5b50\u4ee3\u7406\u548c\u4e25\u683c\u53d7\u9650\u7684\u5de5\u5177\u3002", "result": "\u5728SWE-Bench Lite\u4e0a\u8fbe\u523088.8%\u7684\u901a\u8fc7\u7387\uff08\u6bd4\u6b21\u4f18\u7cfb\u7edf\u63d0\u534727.8%\uff09\uff0c\u5728SWE-Bench Verified\u4e0a\u8fbe\u523094.3%\u7684\u901a\u8fc7\u7387\uff0c800\u6b21\u8fd0\u884c\u4e2d\u4ec5\u53d1\u73b07\u6b21\u6d4b\u8bd5\u4f5c\u5f0a\u3002", "conclusion": "\u73b0\u4ee3LLM\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u9a71\u52a8\u5de5\u4f5c\u6d41\u4e2d\u5df2\u80fd\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u6d4b\u8bd5\u89e3\u51b3\u80fd\u529b\uff0c\u5b8c\u5168\u81ea\u4e3b\u4ed3\u5e93\u4fee\u590d\u7684\u6700\u7ec8\u6311\u6218\u5728\u4e8e\u751f\u6210\u6709\u6548\u7684\u590d\u73b0\u6d4b\u8bd5\u3002", "topic": "swe application"}}
{"id": "2510.24019", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24019", "abs": "https://arxiv.org/abs/2510.24019", "authors": ["Xing Xing", "Wei Wang", "Lipeng Ma", "Weidong Yang", "Junjie Zheng"], "title": "Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs", "comment": null, "summary": "Recent progress in large language models (LLMs) has advanced automatic code\ngeneration, yet most approaches rely on direct, single-step translation from\nproblem descriptions to code, disregarding structured software engineering\npractices. We introduce a lifecycle-aware framework that systematically\nincorporates intermediate artifacts such as requirements analysis, state\nmachine modeling, and pseudocode into both the training and inference stages.\nThis design aligns code generation with standard software development phases\nand enables more structured reasoning. Experiments show that lifecycle-level\nfine-tuning improves code correctness by up to 75% over the same model before\nfine-tuning, with performance gains compounding across intermediate stages.\nMulti-step inference consistently surpasses single-step generation,\ndemonstrating the effectiveness of intermediate scaffolding. Notably,\nopen-source LLMs, once fine-tuned under our framework, match or slightly\noutperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our\nframework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and\n22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,\nrespectively. Our pipeline also proves robust with up to 80\\% less training\ndata, confirming its resilience. Ablation studies further reveal that each\nintermediate artifact contributes distinctly to final code quality, with state\nmachine modeling yielding the most substantial impact. Our source code and\ndetailed experimental data are available at\nhttps://anonymous.4open.science/r/Lifecycle-Aware-3CCB.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u547d\u5468\u671f\u611f\u77e5\u7684\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u9700\u6c42\u5206\u6790\u3001\u72b6\u6001\u673a\u5efa\u6a21\u548c\u4f2a\u4ee3\u7801\u7b49\u4e2d\u95f4\u4ea7\u7269\uff0c\u5c06\u4ee3\u7801\u751f\u6210\u4e0e\u6807\u51c6\u8f6f\u4ef6\u5f00\u53d1\u9636\u6bb5\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u6b63\u786e\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5927\u591a\u91c7\u7528\u4ece\u95ee\u9898\u63cf\u8ff0\u5230\u4ee3\u7801\u7684\u76f4\u63a5\u5355\u6b65\u7ffb\u8bd1\uff0c\u5ffd\u89c6\u4e86\u7ed3\u6784\u5316\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u3002\u9700\u8981\u5c06\u4ee3\u7801\u751f\u6210\u4e0e\u6807\u51c6\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u76f8\u7ed3\u5408\u3002", "method": "\u8bbe\u8ba1\u751f\u547d\u5468\u671f\u611f\u77e5\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7cfb\u7edf\u6027\u5730\u5f15\u5165\u4e2d\u95f4\u4ea7\u7269\uff08\u9700\u6c42\u5206\u6790\u3001\u72b6\u6001\u673a\u5efa\u6a21\u3001\u4f2a\u4ee3\u7801\uff09\uff0c\u91c7\u7528\u591a\u6b65\u63a8\u7406\u800c\u975e\u5355\u6b65\u751f\u6210\u3002", "result": "\u751f\u547d\u5468\u671f\u7ea7\u5fae\u8c03\u4f7f\u4ee3\u7801\u6b63\u786e\u6027\u63d0\u5347\u9ad8\u8fbe75%\uff1b\u591a\u6b65\u63a8\u7406\u59cb\u7ec8\u4f18\u4e8e\u5355\u6b65\u751f\u6210\uff1b\u5f00\u6e90LLM\u7ecf\u5fae\u8c03\u540e\u6027\u80fd\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u9884\u8bad\u7ec3\u4ee3\u7801\u6a21\u578b\uff1b\u5728DeepSeek-Coder-1.3B\u4e0a\u76f8\u5bf9CodeBLEU\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u4e2d\u95f4\u4ea7\u7269\u5bf9\u6700\u7ec8\u4ee3\u7801\u8d28\u91cf\u6709\u663e\u8457\u8d21\u732e\uff0c\u5176\u4e2d\u72b6\u6001\u673a\u5efa\u6a21\u5f71\u54cd\u6700\u5927\uff1b\u8be5\u6846\u67b6\u5728\u8f83\u5c11\u8bad\u7ec3\u6570\u636e\u4e0b\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "topic": "code agent"}}
{"id": "2510.23853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23853", "abs": "https://arxiv.org/abs/2510.23853", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Kazem Faghih", "Parsa Hosseini", "Wenxiao Wang", "Soheil Feizi"], "title": "Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception", "comment": "preliminary work in progress", "summary": "Large language model agents are increasingly used in multi-turn\nconversational settings to interact with and execute tasks in dynamic\nenvironments. However, a key limitation is their temporal blindness: they, by\ndefault, operate with a stationary context, failing to account for the\nreal-world time elapsed between messages. This becomes a critical liability\nwhen an agent must decide whether to invoke a tool based on how much time has\npassed since the last observation. Without temporal awareness, agents often\neither over-rely on previous context (skipping necessary tool calls), or\nunder-rely on it (unnecessarily repeating tool calls). To study this challenge,\nwe introduce TicToc-v1, a test set of multi-turn user-agent trajectories across\n34 scenarios with varying time sensitivity. Each trajectory ends with a user\nquestion, where the need for a tool call depends on the amount of time elapsed\nsince the last message. To give LLMs temporal context, we augment dialogue\nmessages with explicit timestamps, bridging the gap between static dialogue and\nevolving environments. We then collected human preferences for these samples,\ncreating two subsets: one where humans preferred relying on the previous\nobservation (prefer-noTool), and another where they preferred a new tool call\n(prefer-Tool). We evaluated how well LLM tool-calling decisions align with\nhuman preferences under varying time intervals on TicToc-v1. Our analysis show\nthat without time information, most models perform only slightly better than\nrandom, with the top alignment rate being just over 60%. While adding\ntimestamps leads to a slight improvement, particularly for larger models, the\nimprovement is modest, peaking at around 65%. We also show that naive,\nprompt-based alignment have limited effectiveness. Our findings highlight the\nneed for specific post-training alignment to align multi-turn LLM tool use with\nhuman temporal perception.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u7f3a\u4e4f\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u800c\u5bfc\u81f4\u7684\u5de5\u5177\u8c03\u7528\u95ee\u9898\uff0c\u63d0\u51fa\u4e86TicToc-v1\u6d4b\u8bd5\u96c6\u6765\u8bc4\u4f30\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u65f6\u95f4\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u7684\u540e\u8bad\u7ec3\u5bf9\u9f50\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u591a\u8f6e\u5bf9\u8bdd\u73af\u5883\u4e2d\u5b58\u5728\u65f6\u95f4\u76f2\u70b9\uff0c\u65e0\u6cd5\u611f\u77e5\u6d88\u606f\u95f4\u7684\u65f6\u95f4\u6d41\u901d\uff0c\u5bfc\u81f4\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0d\u5f53\u2014\u2014\u8981\u4e48\u8fc7\u5ea6\u4f9d\u8d56\u65e7\u4e0a\u4e0b\u6587\u800c\u8df3\u8fc7\u5fc5\u8981\u8c03\u7528\uff0c\u8981\u4e48\u4e0d\u5fc5\u8981\u5730\u91cd\u590d\u8c03\u7528\u3002", "method": "\u5f15\u5165TicToc-v1\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b34\u4e2a\u65f6\u95f4\u654f\u611f\u573a\u666f\u7684\u591a\u8f6e\u5bf9\u8bdd\u8f68\u8ff9\uff1b\u5728\u5bf9\u8bdd\u6d88\u606f\u4e2d\u6dfb\u52a0\u65f6\u95f4\u6233\uff1b\u6536\u96c6\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u5206\u4e3a\u504f\u597d\u4e0d\u8c03\u7528\u5de5\u5177\u548c\u504f\u597d\u8c03\u7528\u5de5\u5177\u4e24\u4e2a\u5b50\u96c6\uff1b\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u65f6\u95f4\u95f4\u9694\u4e0b\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002", "result": "\u65e0\u65f6\u95f4\u4fe1\u606f\u65f6\uff0c\u5927\u591a\u6570\u6a21\u578b\u8868\u73b0\u4ec5\u7565\u4f18\u4e8e\u968f\u673a\u731c\u6d4b\uff0c\u6700\u9ad8\u5bf9\u9f50\u7387\u7ea660%\uff1b\u6dfb\u52a0\u65f6\u95f4\u6233\u540e\u7565\u6709\u6539\u5584\uff0c\u7279\u522b\u662f\u5bf9\u5927\u6a21\u578b\uff0c\u4f46\u6539\u5584\u6709\u9650\uff0c\u5cf0\u503c\u7ea665%\uff1b\u57fa\u4e8e\u63d0\u793a\u7684\u5bf9\u9f50\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u9700\u8981\u4e13\u95e8\u7684\u540e\u8bad\u7ec3\u5bf9\u9f50\u6765\u4f7f\u591a\u8f6eLLM\u5de5\u5177\u4f7f\u7528\u4e0e\u4eba\u7c7b\u65f6\u95f4\u611f\u77e5\u4fdd\u6301\u4e00\u81f4\uff0c\u5f53\u524d\u6a21\u578b\u5728\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2510.23629", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23629", "abs": "https://arxiv.org/abs/2510.23629", "authors": ["Nuo Chen", "Zehua Li", "Keqin Bao", "Junyang Lin", "Dayiheng Liu"], "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "comment": null, "summary": "Building robust and general reasoning ability is a central goal in the\ndevelopment of large language models (LLMs). Recent efforts increasingly turn\nto code as a rich training source, given its inherent logical structure and\ndiverse reasoning paradigms such as divide-and-conquer, topological ordering,\nand enumeration. However, reasoning in code is often expressed implicitly and\nentangled with syntactic or implementation noise, making direct training on raw\ncode suboptimal.To address this, we introduce TracePile, a large-scale corpus\nof 2.6 million samples that transforms code execution into explicit,\nstep-by-step chain-of-thought-style rationales, which we call Chain of\nExecution (CoE). The corpus spans domains including mathematics, classical\nalgorithms and algorithmic competition, and is enriched with variable-tracing\nquestions and code rewritings to enhance logical granularity and code\ndiversity. We evaluate TracePile using three training setups:\ncontinue-pretraining, instruction tuning after pretraining, and two-stage\nfinetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,\nand Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and\nalgorithms demonstrate consistent improvements. Notably, TracePile boosts\nLLaMA3.1-8B by 7.1\\% on average across nine math datasets and delivers clear\ngains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.", "AI": {"tldr": "TracePile\u662f\u4e00\u4e2a\u5305\u542b260\u4e07\u6837\u672c\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff0c\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u663e\u5f0f\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff08Chain of Execution\uff09\uff0c\u901a\u8fc7\u7ee7\u7eed\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u5728\u539f\u59cb\u4ee3\u7801\u4e0a\u8bad\u7ec3\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u4ee3\u7801\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\u901a\u5e38\u662f\u9690\u5f0f\u7684\uff0c\u5e76\u4e0e\u8bed\u6cd5\u6216\u5b9e\u73b0\u566a\u58f0\u7ea0\u7f20\u5728\u4e00\u8d77\u3002\u9700\u8981\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u663e\u5f0f\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u6765\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efaTracePile\u8bed\u6599\u5e93\uff0c\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3aChain of Execution\u98ce\u683c\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u5305\u542b\u53d8\u91cf\u8ffd\u8e2a\u95ee\u9898\u548c\u4ee3\u7801\u91cd\u5199\uff0c\u91c7\u7528\u7ee7\u7eed\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u4e09\u79cd\u8bad\u7ec3\u8bbe\u7f6e\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u7840\u6a21\u578b\uff08LLaMA 3\u3001LLaMA 3.1\u3001Qwen-2.5\u548cQwen-2.5 Coder\uff09\u548c20\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u663e\u793a\u51fa\u4e00\u81f4\u6539\u8fdb\uff0cLLaMA3.1-8B\u5728\u4e5d\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53477.1%\uff0c\u5728LiveCodeBench\u3001CRUX\u548cMMLU\u4e0a\u4e5f\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "TracePile\u901a\u8fc7\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "2510.24188", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24188", "abs": "https://arxiv.org/abs/2510.24188", "authors": ["C\u00e9sar Santos", "Ermeson Andrade", "Roberto Natella"], "title": "Investigating Software Aging in LLM-Generated Software Systems", "comment": "Presented at the 17th International Workshop on Software Aging and\n  Rejuvenation (WoSAR), 2025", "summary": "Automatically generated software, especially code produced by Large Language\nModels (LLMs), is increasingly adopted to accelerate development and reduce\nmanual effort. However, little is known about the long-term reliability of such\nsystems under sustained execution. In this paper, we experimentally investigate\nthe phenomenon of software aging in applications generated by LLM-based tools.\nUsing the Bolt platform and standardized prompts from Baxbench, we generated\nfour service-oriented applications and subjected them to 50-hour load tests.\nResource usage, response time, and throughput were continuously monitored to\ndetect degradation patterns. The results reveal significant evidence of\nsoftware aging, including progressive memory growth, increased response time,\nand performance instability across all applications. Statistical analyzes\nconfirm these trends and highlight variability in the severity of aging\naccording to the type of application. Our findings show the need to consider\naging in automatically generated software and provide a foundation for future\nstudies on mitigation strategies and long-term reliability evaluation.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u8c03\u67e5\u4e86LLM\u751f\u6210\u8f6f\u4ef6\u4e2d\u7684\u8f6f\u4ef6\u8001\u5316\u73b0\u8c61\uff0c\u53d1\u73b0\u6240\u6709\u5e94\u7528\u90fd\u5b58\u5728\u663e\u8457\u7684\u5185\u5b58\u589e\u957f\u3001\u54cd\u5e94\u65f6\u95f4\u589e\u52a0\u548c\u6027\u80fd\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u9700\u8981\u4e86\u89e3\u8fd9\u7c7b\u7cfb\u7edf\u5728\u6301\u7eed\u6267\u884c\u4e0b\u7684\u957f\u671f\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u8f6f\u4ef6\u8001\u5316\u95ee\u9898\u3002", "method": "\u4f7f\u7528Bolt\u5e73\u53f0\u548cBaxbench\u6807\u51c6\u5316\u63d0\u793a\u751f\u6210\u56db\u4e2a\u9762\u5411\u670d\u52a1\u7684\u5e94\u7528\uff0c\u8fdb\u884c50\u5c0f\u65f6\u8d1f\u8f7d\u6d4b\u8bd5\uff0c\u6301\u7eed\u76d1\u63a7\u8d44\u6e90\u4f7f\u7528\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u541e\u5410\u91cf\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6240\u6709\u5e94\u7528\u90fd\u5b58\u5728\u663e\u8457\u7684\u8f6f\u4ef6\u8001\u5316\u8bc1\u636e\uff0c\u5305\u62ec\u6e10\u8fdb\u5f0f\u5185\u5b58\u589e\u957f\u3001\u54cd\u5e94\u65f6\u95f4\u589e\u52a0\u548c\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u7edf\u8ba1\u5206\u6790\u786e\u8ba4\u4e86\u8fd9\u4e9b\u8d8b\u52bf\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u9700\u8981\u5728\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u4e2d\u8003\u8651\u8001\u5316\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7f13\u89e3\u7b56\u7565\u548c\u957f\u671f\u53ef\u9760\u6027\u8bc4\u4f30\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2510.23854", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23854", "abs": "https://arxiv.org/abs/2510.23854", "authors": ["Jyotika Singh", "Weiyi Sun", "Amit Agarwal", "Viji Krishnamurthy", "Yassine Benajiba", "Sujith Ravi", "Dan Roth"], "title": "Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs", "comment": "Accepted at EMNLP 2025", "summary": "In modern industry systems like multi-turn chat agents, Text-to-SQL\ntechnology bridges natural language (NL) questions and database (DB) querying.\nThe conversion of tabular DB results into NL representations (NLRs) enables the\nchat-based interaction. Currently, NLR generation is typically handled by large\nlanguage models (LLMs), but information loss or errors in presenting tabular\nresults in NL remains largely unexplored. This paper introduces a novel\nevaluation method - Combo-Eval - for judgment of LLM-generated NLRs that\ncombines the benefits of multiple existing methods, optimizing evaluation\nfidelity and achieving a significant reduction in LLM calls by 25-61%.\nAccompanying our method is NLR-BIRD, the first dedicated dataset for NLR\nbenchmarking. Through human evaluations, we demonstrate the superior alignment\nof Combo-Eval with human judgments, applicable across scenarios with and\nwithout ground truth references.", "AI": {"tldr": "\u63d0\u51faCombo-Eval\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684\u8868\u683c\u7ed3\u679c\u81ea\u7136\u8bed\u8a00\u8868\u793a\uff0c\u663e\u8457\u51cf\u5c1125-61%\u7684LLM\u8c03\u7528\uff0c\u5e76\u53d1\u5e03\u9996\u4e2a\u4e13\u7528\u6570\u636e\u96c6NLR-BIRD\u3002", "motivation": "\u5728\u591a\u8f6e\u5bf9\u8bdd\u4ee3\u7406\u7b49\u73b0\u4ee3\u5de5\u4e1a\u7cfb\u7edf\u4e2d\uff0cText-to-SQL\u6280\u672f\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u4e0e\u6570\u636e\u5e93\u67e5\u8be2\uff0c\u4f46LLM\u751f\u6210\u7684\u8868\u683c\u7ed3\u679c\u81ea\u7136\u8bed\u8a00\u8868\u793a\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u6216\u9519\u8bef\u95ee\u9898\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCombo-Eval\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4f18\u5316\u8bc4\u4f30\u4fdd\u771f\u5ea6\uff0c\u5e76\u521b\u5efa\u9996\u4e2a\u4e13\u7528\u6570\u636e\u96c6NLR-BIRD\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "Combo-Eval\u663e\u8457\u51cf\u5c1125-61%\u7684LLM\u8c03\u7528\uff0c\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u8bc1\u660e\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u5177\u6709\u66f4\u597d\u7684\u5bf9\u9f50\u5ea6\uff0c\u9002\u7528\u4e8e\u6709\u548c\u65e0\u53c2\u8003\u57fa\u51c6\u7684\u573a\u666f\u3002", "conclusion": "Combo-Eval\u65b9\u6cd5\u5728\u8bc4\u4f30LLM\u751f\u6210\u7684\u8868\u683c\u7ed3\u679c\u81ea\u7136\u8bed\u8a00\u8868\u793a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u80fd\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2510.24241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24241", "abs": "https://arxiv.org/abs/2510.24241", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "MAGNET: A Multi-Graph Attentional Network for Code Clone Detection", "comment": null, "summary": "Code clone detection is a fundamental task in software engineering that\nunderpins refactoring, debugging, plagiarism detection, and vulnerability\nanalysis. Existing methods often rely on singular representations such as\nabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs\n(DFGs), which capture only partial aspects of code semantics. Hybrid approaches\nhave emerged, but their fusion strategies are typically handcrafted and\nineffective. In this study, we propose MAGNET, a multi-graph attentional\nframework that jointly leverages AST, CFG, and DFG representations to capture\nsyntactic and semantic features of source code. MAGNET integrates residual\ngraph neural networks with node-level self-attention to learn both local and\nlong-range dependencies, introduces a gated cross-attention mechanism for\nfine-grained inter-graph interactions, and employs Set2Set pooling to fuse\nmulti-graph embeddings into unified program-level representations. Extensive\nexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNET\nachieves state-of-the-art performance with an overall F1 score of 96.5\\% and\n99.2\\% on the two datasets, respectively. Ablation studies confirm the critical\ncontributions of multi-graph fusion and each attentional component. Our code is\navailable at https://github.com/ZixianReid/Multigraph_match", "AI": {"tldr": "\u63d0\u51fa\u4e86MAGNET\u591a\u56fe\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528AST\u3001CFG\u548cDFG\u8868\u793a\u6765\u6355\u83b7\u6e90\u4ee3\u7801\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u8868\u793a\uff08\u5982AST\u3001CFG\u3001DFG\uff09\uff0c\u53ea\u80fd\u6355\u83b7\u4ee3\u7801\u8bed\u4e49\u7684\u90e8\u5206\u65b9\u9762\uff0c\u800c\u6df7\u5408\u65b9\u6cd5\u878d\u5408\u7b56\u7565\u901a\u5e38\u662f\u624b\u5de5\u8bbe\u8ba1\u4e14\u6548\u679c\u4e0d\u4f73\u3002", "method": "MAGNET\u96c6\u6210\u6b8b\u5dee\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u8282\u70b9\u7ea7\u81ea\u6ce8\u610f\u529b\u6765\u5b66\u4e60\u5c40\u90e8\u548c\u957f\u7a0b\u4f9d\u8d56\uff0c\u5f15\u5165\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u7ec6\u7c92\u5ea6\u56fe\u95f4\u4ea4\u4e92\uff0c\u4f7f\u7528Set2Set\u6c60\u5316\u5c06\u591a\u56fe\u5d4c\u5165\u878d\u5408\u4e3a\u7edf\u4e00\u7a0b\u5e8f\u7ea7\u8868\u793a\u3002", "result": "\u5728BigCloneBench\u548cGoogle Code Jam\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523096.5%\u548c99.2%\u7684F1\u5206\u6570\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u591a\u56fe\u878d\u5408\u548c\u5404\u6ce8\u610f\u529b\u7ec4\u4ef6\u7684\u5173\u952e\u8d21\u732e\u3002", "conclusion": "MAGNET\u901a\u8fc7\u591a\u56fe\u6ce8\u610f\u529b\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u5229\u7528\u591a\u79cd\u4ee3\u7801\u8868\u793a\u7684\u91cd\u8981\u6027\u3002", "topic": "swe application"}}
{"id": "2510.23870", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23870", "abs": "https://arxiv.org/abs/2510.23870", "authors": ["Marianne Menglin Liu", "Sai Ashish Somayajula", "Syed Fahad Allam Shah", "Sujith Ravi", "Dan Roth"], "title": "OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning", "comment": null, "summary": "We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge\n2025, a bilingual benchmark requiring complex reasoning such as arithmetic,\ncommonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding\nthe second-best system by more than 6% in execution accuracy (EX), with 55.0%\nin English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).\nOur system follows an agentic framework with two components: Planner agent that\ngenerates stepwise natural language plans, and SQL agent that converts these\nplans into executable SQL. Since SQL agent reliably adheres to the plan, our\nrefinements focus on the planner. Unlike prior methods that rely on multiple\nsub-agents for planning and suffer from orchestration overhead, we introduce a\nfeedback-guided meta-prompting strategy to refine a single planner. Failure\ncases from a held-out set are clustered with human input, and an LLM distills\nthem into corrective guidelines that are integrated into the planner's system\nprompt, improving generalization without added complexity. For the multilingual\nscenario, to address transliteration and entity mismatch issues, we incorporate\nentity-linking guidelines that generate alternative surface forms for entities\nand explicitly include them in the plan. Finally, we enhance reliability\nthrough plan diversification: multiple candidate plans are generated for each\nquery, with the SQL agent producing a query for each plan, and final output\nselected via majority voting over their executions.", "AI": {"tldr": "OraPlan-SQL\u662f\u4e00\u4e2a\u7528\u4e8eArcher NL2SQL\u8bc4\u4f30\u6311\u6218\u8d5b\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u91c7\u7528\u53cc\u4ee3\u7406\u6846\u67b6\uff08\u89c4\u5212\u5668\u548cSQL\u4ee3\u7406\uff09\uff0c\u901a\u8fc7\u53cd\u9988\u5f15\u5bfc\u7684\u5143\u63d0\u793a\u7b56\u7565\u548c\u8ba1\u5212\u591a\u6837\u5316\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u5728\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u63a8\u7406\uff08\u5982\u7b97\u672f\u3001\u5e38\u8bc6\u548c\u5047\u8bbe\u63a8\u7406\uff09\u5728\u53cc\u8bedNL2SQL\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u540c\u65f6\u514b\u670d\u4f20\u7edf\u591a\u5b50\u4ee3\u7406\u65b9\u6cd5\u5728\u534f\u8c03\u65b9\u9762\u7684\u5f00\u9500\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53cc\u4ee3\u7406\u6846\u67b6\uff1a\u89c4\u5212\u5668\u4ee3\u7406\u751f\u6210\u9010\u6b65\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\uff0cSQL\u4ee3\u7406\u5c06\u8ba1\u5212\u8f6c\u6362\u4e3a\u53ef\u6267\u884cSQL\u3002\u5f15\u5165\u53cd\u9988\u5f15\u5bfc\u7684\u5143\u63d0\u793a\u7b56\u7565\u6765\u7cbe\u70bc\u5355\u4e2a\u89c4\u5212\u5668\uff0c\u5e76\u91c7\u7528\u8ba1\u5212\u591a\u6837\u5316\uff08\u751f\u6210\u591a\u4e2a\u5019\u9009\u8ba1\u5212\u5e76\u901a\u8fc7\u591a\u6570\u6295\u7968\u9009\u62e9\u6700\u7ec8\u8f93\u51fa\uff09\u3002", "result": "\u5728Archer NL2SQL\u8bc4\u4f30\u6311\u6218\u8d5b2025\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u6267\u884c\u51c6\u786e\u7387\u8d85\u8fc7\u7b2c\u4e8c\u540d6%\u4ee5\u4e0a\uff08\u82f1\u658755.0%\uff0c\u4e2d\u658756.7%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8fc799%\u7684SQL\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u53cd\u9988\u5f15\u5bfc\u7684\u5143\u63d0\u793a\u7b56\u7565\u548c\u8ba1\u5212\u591a\u6837\u5316\uff0cOraPlan-SQL\u5728\u4fdd\u6301\u9ad8SQL\u6709\u6548\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cc\u8bedNL2SQL\u4efb\u52a1\u7684\u6267\u884c\u51c6\u786e\u7387\u3002", "topic": "code agent"}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u95ee\u9898\uff0c\u667a\u80fd\u4f53\u57fa\u4e8e\u73af\u5883\u8868\u793a\u72ec\u7acb\u751f\u6210\u76ee\u6807\u504f\u597d\uff0c\u901a\u8fc7\u56fa\u5b9a\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\u8fdb\u884c\u5206\u914d\uff0c\u65e0\u9700\u534f\u5546\u3002LLM\u667a\u80fd\u4f53\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u4e0b\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u6761\u4ef6\u4e0b\u591a\u667a\u80fd\u4f53\u5728\u5171\u4eab\u73af\u5883\u4e2d\u7684\u534f\u8c03\u6311\u6218\uff0c\u63a2\u7d22LLM\u5728\u76ee\u6807\u5206\u914d\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u667a\u80fd\u4f53\u57fa\u4e8e\u7f51\u683c\u53ef\u89c6\u5316\u548c\u573a\u666f\u6570\u636e\u72ec\u7acb\u751f\u6210\u76ee\u6807\u504f\u597d\u6392\u5e8f\uff0c\u901a\u8fc7\u56fa\u5b9a\u786e\u5b9a\u6027\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\uff08\u5982\u667a\u80fd\u4f53\u7d22\u5f15\u6392\u5e8f\uff09\u8fdb\u884c\u76ee\u6807\u5206\u914d\uff0c\u65e0\u9700\u534f\u5546\u3002\u6bd4\u8f83\u4e86\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548cLLM\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "result": "LLM\u667a\u80fd\u4f53\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u5b9a\u91cf\u4fe1\u606f\u4e0b\uff0c\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5b8c\u5de5\u65f6\u95f4\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4fe1\u606f\u7ed3\u6784\u5728\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "IBM\u5f00\u53d1\u4e86\u901a\u7528\u8ba1\u7b97\u673a\u4ee3\u7406CUGA\uff0c\u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u67b6\u6784\uff0c\u5728AppWorld\u548cWebArena\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u4f01\u4e1a\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u83b7\u53d6\u9886\u57df\u8fdb\u884c\u4e86\u8bd5\u70b9\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u4ece\u539f\u578b\u5230\u90e8\u7f72\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u514b\u670d\u6846\u67b6\u788e\u7247\u5316\u3001\u5f00\u53d1\u7f13\u6162\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u901a\u7528\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u67b6\u6784\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5206\u6790\u57fa\u7840\uff0c\u5e76\u5f15\u5165BPO-TA\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b26\u4e2a\u4efb\u52a1\u548c13\u4e2a\u5206\u6790\u7aef\u70b9\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "CUGA\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u4f01\u4e1a\u8bd5\u70b9\u4e2d\u63a5\u8fd1\u4e13\u4e1a\u4ee3\u7406\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u793a\u51fa\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u6210\u672c\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u901a\u7528\u4ee3\u7406\u5728\u4f01\u4e1a\u89c4\u6a21\u8fd0\u884c\u7684\u65e9\u671f\u8bc1\u636e\uff0c\u5e76\u603b\u7ed3\u4e86\u6280\u672f\u548c\u7ec4\u7ec7\u7ecf\u9a8c\uff0c\u4e3a\u5c06\u7814\u7a76\u7ea7\u67b6\u6784\u53d1\u5c55\u4e3a\u5065\u58ee\u7684\u4f01\u4e1a\u5c31\u7eea\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.24358", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24358", "abs": "https://arxiv.org/abs/2510.24358", "authors": ["Lingyue Fu", "Bolun Zhang", "Hao Guan", "Yaoming Zhu", "Lin Qiu", "Weiwen Liu", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang", "Yong Yu"], "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation", "comment": null, "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86PRDBench\u57fa\u51c6\uff0c\u901a\u8fc7\u4ee3\u7406\u9a71\u52a8\u7684\u7ba1\u9053\u6784\u5efa\u5305\u542b50\u4e2a\u771f\u5b9ePython\u9879\u76ee\u7684\u57fa\u51c6\uff0c\u89e3\u51b3\u73b0\u6709\u4ee3\u7801\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u7684\u9ad8\u6807\u6ce8\u6210\u672c\u548c\u8bc4\u4f30\u6307\u6807\u50f5\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u9ad8\u6807\u6ce8\u6210\u672c\u548c\u4e13\u4e1a\u77e5\u8bc6\u8981\u6c42\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u4e3b\u8981\u4f9d\u8d56\u5355\u5143\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u9a71\u52a8\u7684\u57fa\u51c6\u6784\u5efa\u7ba1\u9053\uff0c\u7ed3\u5408\u4eba\u5de5\u76d1\u7763\u751f\u6210\u591a\u6837\u5316\u7684\u9879\u76ee\u7ea7\u4efb\u52a1\uff0c\u5e76\u5f15\u5165Agent-as-a-Judge\u8303\u5f0f\u8fdb\u884c\u8bc4\u5206\u3002", "result": "PRDBench\u5305\u542b50\u4e2a\u771f\u5b9ePython\u9879\u76ee\uff0c\u6db5\u76d620\u4e2a\u9886\u57df\uff0c\u5177\u6709\u7ed3\u6784\u5316PRD\u9700\u6c42\u3001\u5168\u9762\u8bc4\u4f30\u6807\u51c6\u548c\u53c2\u8003\u5b9e\u73b0\uff0c\u80fd\u6709\u6548\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u548c\u8bc4\u4f30\u4ee3\u7406\u7684\u80fd\u529b\u3002", "conclusion": "PRDBench\u4e3a\u4ee3\u7801\u4ee3\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u652f\u6301\u8d85\u8d8a\u5355\u5143\u6d4b\u8bd5\u7684\u591a\u79cd\u6d4b\u8bd5\u7c7b\u578b\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2510.24367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24367", "abs": "https://arxiv.org/abs/2510.24367", "authors": ["Junda He", "Jieke Shi", "Terry Yue Zhuo", "Christoph Treude", "Jiamou Sun", "Zhenchang Xing", "Xiaoning Du", "David Lo"], "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff08LLM-as-a-Judge\uff09\u6765\u81ea\u52a8\u8bc4\u4f30LLM\u751f\u6210\u7684\u8f6f\u4ef6\u5de5\u4ef6\u7684\u5fc5\u8981\u6027\uff0c\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u52302030\u5e74\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4ea7\u751f\u4e86\u5927\u91cf\u8f6f\u4ef6\u5de5\u4ef6\uff0c\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u81ea\u52a8\u6307\u6807\u65e0\u6cd5\u6355\u6349\u8d28\u91cf\u7ec6\u8282\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u5206\u6790\u73b0\u6709SE\u7814\u7a76\uff0c\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u5236\u5b9a\u8be6\u7ec6\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "result": "LLM-as-a-Judge\u65b9\u6cd5\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u5728\u81ea\u52a8\u5316\u89c4\u6a21\u4e0a\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7ec6\u5fae\u8bc4\u4f30\u7684\u9014\u5f84\u3002", "conclusion": "\u52302030\u5e74\uff0c\u8fd9\u4e9b\u6846\u67b6\u6709\u671b\u6210\u4e3a\u53ef\u9760\u3001\u7a33\u5065\u3001\u53ef\u6269\u5c55\u7684\u4eba\u7c7b\u66ff\u4ee3\u54c1\uff0c\u80fd\u591f\u8fdb\u884c\u4e00\u81f4\u3001\u591a\u65b9\u9762\u7684\u5de5\u4ef6\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2510.24428", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24428", "abs": "https://arxiv.org/abs/2510.24428", "authors": ["Nguyen Hoang Anh", "Minh Le-Anh", "Bach Le", "Nghi D. Q. Bui"], "title": "CodeWiki: Automated Repository-Level Documentation at Scale", "comment": null, "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.", "AI": {"tldr": "CodeWiki\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u5168\u6808\u4ed3\u5e93\u7ea7\u6587\u6863\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u89e3\u3001\u9012\u5f52\u4ee3\u7406\u5904\u7406\u548c\u6587\u672c-\u89c6\u89c9\u5408\u6210\uff0c\u57287\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4ed3\u5e93\u7ea7\u6587\u6863\u751f\u6210\u3002", "motivation": "\u5f00\u53d1\u4eba\u545858%\u7684\u65f6\u95f4\u7528\u4e8e\u7406\u89e3\u4ee3\u7801\u5e93\uff0c\u4f46\u73b0\u6709LLM\u53ea\u80fd\u5904\u7406\u51fd\u6570\u7ea7\u6587\u6863\uff0c\u65e0\u6cd5\u6355\u6349\u4ed3\u5e93\u7ea7\u7684\u67b6\u6784\u6a21\u5f0f\u548c\u8de8\u6a21\u5757\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u521b\u65b0\uff1a\u5206\u5c42\u5206\u89e3\u4fdd\u6301\u67b6\u6784\u4e0a\u4e0b\u6587\u3001\u9012\u5f52\u4ee3\u7406\u5904\u7406\u4e0e\u52a8\u6001\u59d4\u6258\u3001\u6587\u672c\u548c\u89c6\u89c9\u5de5\u4ef6\u7684\u5408\u6210\uff08\u5305\u62ec\u67b6\u6784\u56fe\u548c\u6570\u636e\u6d41\uff09\u3002", "result": "CodeWiki\u5728\u4e13\u6709\u6a21\u578b\u4e0a\u83b7\u5f9768.79%\u8d28\u91cf\u5206\uff0c\u5f00\u6e90\u6a21\u578b\u4e0a\u83b7\u5f9764.80%\uff0c\u4f18\u4e8e\u73b0\u6709\u95ed\u6e90\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u51c6\u786e\u6587\u6863\u751f\u6210\u80fd\u529b\u3002", "conclusion": "CodeWiki\u6846\u67b6\u80fd\u591f\u4e3a\u771f\u5b9e\u4e16\u754c\u4ed3\u5e93\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u51c6\u786e\u7684\u4ed3\u5e93\u7ea7\u6587\u6863\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4ed3\u5e93\u7ea7\u6587\u6863\u751f\u6210\u4e0a\u7684\u5c40\u9650\u6027\u3002", "topic": "swe application"}}
{"id": "2510.23924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23924", "abs": "https://arxiv.org/abs/2510.23924", "authors": ["Dina Pisarevskaya", "Arkaitz Zubiaga"], "title": "Agent-based Automated Claim Matching with Instruction-following LLMs", "comment": "Accepted for the International Joint Conference on Natural Language\n  Processing & Asia-Pacific Chapter of the Association for Computational\n  Linguistics (2025) Findings", "summary": "We present a novel agent-based approach for the automated claim matching task\nwith instruction-following LLMs. We propose a two-step pipeline that first\ngenerates prompts with LLMs, to then perform claim matching as a binary\nclassification task with LLMs. We demonstrate that LLM-generated prompts can\noutperform SOTA with human-generated prompts, and that smaller LLMs can do as\nwell as larger ones in the generation process, allowing to save computational\nresources. We also demonstrate the effectiveness of using different LLMs for\neach step of the pipeline, i.e. using an LLM for prompt generation, and another\nfor claim matching. Our investigation into the prompt generation process in\nturn reveals insights into the LLMs' understanding of claim matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u81ea\u52a8\u5316\u7d22\u8d54\u5339\u914d\u65b9\u6cd5\uff0c\u4f7f\u7528\u6307\u4ee4\u9075\u5faa\u7684LLMs\u6784\u5efa\u4e24\u6b65\u6d41\u6c34\u7ebf\uff0c\u9996\u5148\u751f\u6210\u63d0\u793a\uff0c\u7136\u540e\u8fdb\u884c\u7d22\u8d54\u5339\u914d\u7684\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u81ea\u52a8\u5316\u7d22\u8d54\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u901a\u8fc7\u667a\u80fd\u4f53\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6d41\u6c34\u7ebf\uff1a1\uff09\u4f7f\u7528LLMs\u751f\u6210\u63d0\u793a\uff1b2\uff09\u4f7f\u7528LLMs\u8fdb\u884c\u7d22\u8d54\u5339\u914d\u7684\u4e8c\u5143\u5206\u7c7b\u3002\u7814\u7a76\u4e86\u4e0d\u540cLLMs\u5728\u6d41\u6c34\u7ebf\u5404\u6b65\u9aa4\u4e2d\u7684\u7ec4\u5408\u4f7f\u7528\u3002", "result": "LLM\u751f\u6210\u7684\u63d0\u793a\u4f18\u4e8e\u4eba\u7c7b\u751f\u6210\u7684SOTA\u63d0\u793a\uff1b\u8f83\u5c0f\u7684LLMs\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u8868\u73b0\u4e0e\u8f83\u5927\u6a21\u578b\u76f8\u5f53\uff0c\u53ef\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\uff1b\u4e0d\u540cLLMs\u7ec4\u5408\u4f7f\u7528\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86LLMs\u5728\u7d22\u8d54\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7406\u89e3LLMs\u5bf9\u7d22\u8d54\u5339\u914d\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u6d1e\u5bdf\uff0c\u5e76\u5c55\u793a\u4e86\u8d44\u6e90\u4f18\u5316\u7684\u53ef\u80fd\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u5e26\u6765\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bc4\u4f30\u65b9\u6cd5\u548c\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u667a\u80fdAI\u7cfb\u7edf\u5728\u7f51\u9875\u3001\u8f6f\u4ef6\u548c\u7269\u7406\u73af\u5883\u4e2d\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\u521b\u9020\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u8fd9\u4e9b\u98ce\u9669\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684AI\u5b89\u5168\u548c\u8f6f\u4ef6\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u5206\u6790\uff0c\u6784\u5efa\u4e86\u667a\u80fd\u4ee3\u7406\u7279\u5b9a\u7684\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u4e86\u6700\u8fd1\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4ece\u6280\u672f\u548c\u6cbb\u7406\u89d2\u5ea6\u8ba8\u8bba\u4e86\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u4e86\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u9632\u5fa1\u65b9\u6848\u3002", "conclusion": "\u5f53\u524d\u7814\u7a76\u4e3a\u5f00\u53d1\u5b89\u5168\u8bbe\u8ba1\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4f46\u4ecd\u5b58\u5728\u8bb8\u591a\u5f00\u653e\u6311\u6218\u9700\u8981\u89e3\u51b3\u3002", "topic": "agent analysis"}}
{"id": "2510.23682", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.SE", "I.2.11; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.23682", "abs": "https://arxiv.org/abs/2510.23682", "authors": ["Gokturk Aytug Akarlar"], "title": "Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents", "comment": "35 pages, 15 figures, 2 tables. Keywords: Large Language Models,\n  Autonomous Agents, Neuro-Symbolic AI, Causal Inference, Formal Verification,\n  Multi-Objective Optimization. Open-source code and interactive demo available", "summary": "Large language models show promise as autonomous decision-making agents, yet\ntheir deployment in high-stakes domains remains fraught with risk. Without\narchitectural safeguards, LLM agents exhibit catastrophic brittleness:\nidentical capabilities produce wildly different outcomes depending solely on\nprompt framing. We present Chimera, a neuro-symbolic-causal architecture that\nintegrates three complementary components - an LLM strategist, a formally\nverified symbolic constraint engine, and a causal inference module for\ncounterfactual reasoning. We benchmark Chimera against baseline architectures\n(LLM-only, LLM with symbolic constraints) across 52-week simulations in a\nrealistic e-commerce environment featuring price elasticity, trust dynamics,\nand seasonal demand. Under organizational biases toward either volume or margin\noptimization, LLM-only agents fail catastrophically (total loss of \\$99K in\nvolume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding\nsymbolic constraints prevents disasters but achieves only 43-87% of Chimera's\nprofit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M\nrespectively, some cases +\\$2.2M) while improving brand trust (+1.8% and\n+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+\nformal verification proves zero constraint violations across all scenarios.\nThese results establish that architectural design not prompt engineering\ndetermines the reliability of autonomous agents in production environments. We\nprovide open-source implementations and interactive demonstrations for\nreproducibility.", "AI": {"tldr": "\u63d0\u51faChimera\u795e\u7ecf-\u7b26\u53f7-\u56e0\u679c\u67b6\u6784\uff0c\u96c6\u6210LLM\u7b56\u7565\u5e08\u3001\u5f62\u5f0f\u9a8c\u8bc1\u7b26\u53f7\u7ea6\u675f\u5f15\u64ce\u548c\u56e0\u679c\u63a8\u7406\u6a21\u5757\uff0c\u5728\u7535\u5546\u73af\u5883\u4e2d\u76f8\u6bd4\u7eafLLM\u548cLLM+\u7b26\u53f7\u7ea6\u675f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5229\u6da6\u548c\u54c1\u724c\u4fe1\u4efb\u5ea6\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u5b58\u5728\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u76f8\u540c\u80fd\u529b\u56e0\u63d0\u793a\u6846\u67b6\u4e0d\u540c\u800c\u4ea7\u751f\u622a\u7136\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u9700\u8981\u67b6\u6784\u7ea7\u5b89\u5168\u4fdd\u969c\u3002", "method": "\u5f00\u53d1Chimera\u4e09\u7ec4\u4ef6\u67b6\u6784\uff1aLLM\u7b56\u7565\u5e08\u8d1f\u8d23\u51b3\u7b56\uff0c\u5f62\u5f0f\u9a8c\u8bc1\u7b26\u53f7\u7ea6\u675f\u5f15\u64ce\u786e\u4fdd\u5b89\u5168\u7ea6\u675f\uff0c\u56e0\u679c\u63a8\u7406\u6a21\u5757\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u3002\u5728\u5305\u542b\u4ef7\u683c\u5f39\u6027\u3001\u4fe1\u4efb\u52a8\u6001\u548c\u5b63\u8282\u6027\u9700\u6c42\u768452\u5468\u7535\u5546\u6a21\u62df\u4e2d\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7eafLLM\u4ee3\u7406\u5728\u91cf\u4f18\u5316\u573a\u666f\u4e2d\u9020\u621099K\u7f8e\u5143\u603b\u635f\u5931\uff0c\u5728\u5229\u6da6\u4f18\u5316\u573a\u666f\u4e2d\u54c1\u724c\u4fe1\u4efb\u4e0b\u964d48.6%\u3002\u6dfb\u52a0\u7b26\u53f7\u7ea6\u675f\u53ef\u9632\u6b62\u707e\u96be\u4f46\u4ec5\u8fbe\u5230Chimera\u5229\u6da6\u768443-87%\u3002Chimera\u5b9e\u73b0\u6700\u9ad8\u56de\u62a5\uff08152\u4e07\u548c196\u4e07\u7f8e\u5143\uff0c\u90e8\u5206\u6848\u4f8b+220\u4e07\u7f8e\u5143\uff09\u5e76\u63d0\u5347\u54c1\u724c\u4fe1\u4efb\uff08+1.8%\u548c+10.8%\uff0c\u90e8\u5206\u6848\u4f8b+20.86%\uff09\u3002", "conclusion": "\u67b6\u6784\u8bbe\u8ba1\u800c\u975e\u63d0\u793a\u5de5\u7a0b\u51b3\u5b9a\u4e86\u81ea\u4e3b\u4ee3\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002TLA+\u5f62\u5f0f\u9a8c\u8bc1\u8bc1\u660e\u6240\u6709\u573a\u666f\u4e2d\u96f6\u7ea6\u675f\u8fdd\u89c4\u3002", "topic": "agent analysis"}}
{"id": "2510.23966", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23966", "abs": "https://arxiv.org/abs/2510.23966", "authors": ["Scott Emmons", "Roland S. Zimmermann", "David K. Elson", "Rohin Shah"], "title": "A Pragmatic Way to Measure Chain-of-Thought Monitorability", "comment": "The first two authors contributed equally", "summary": "While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI\nsafety, this opportunity could be lost through shifts in training practices or\nmodel architecture. To help preserve monitorability, we propose a pragmatic way\nto measure two components of it: legibility (whether the reasoning can be\nfollowed by a human) and coverage (whether the CoT contains all the reasoning\nneeded for a human to also produce the final output). We implement these\nmetrics with an autorater prompt that enables any capable LLM to compute the\nlegibility and coverage of existing CoTs. After sanity-checking our prompted\nautorater with synthetic CoT degradations, we apply it to several frontier\nmodels on challenging benchmarks, finding that they exhibit high\nmonitorability. We present these metrics, including our complete autorater\nprompt, as a tool for developers to track how design decisions impact\nmonitorability. While the exact prompt we share is still a preliminary version\nunder ongoing development, we are sharing it now in the hopes that others in\nthe community will find it useful. Our method helps measure the default\nmonitorability of CoT - it should be seen as a complement, not a replacement,\nfor the adversarial stress-testing needed to test robustness against\ndeliberately evasive models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u91cf\u601d\u7ef4\u94fe\u53ef\u76d1\u63a7\u6027\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5305\u62ec\u53ef\u8bfb\u6027\u548c\u8986\u76d6\u5ea6\u4e24\u4e2a\u6307\u6807\uff0c\u5e76\u4f7f\u7528\u81ea\u52a8\u8bc4\u5206\u5668\u63d0\u793a\u6765\u8bc4\u4f30\u73b0\u6709CoT\u7684\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u8bad\u7ec3\u5b9e\u8df5\u548c\u6a21\u578b\u67b6\u6784\u7684\u53d8\u5316\uff0c\u601d\u7ef4\u94fe\u76d1\u63a7\u8fd9\u4e00AI\u5b89\u5168\u673a\u4f1a\u53ef\u80fd\u4e27\u5931\uff0c\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u6765\u4fdd\u6301\u53ef\u76d1\u63a7\u6027\u3002", "method": "\u8bbe\u8ba1\u53ef\u8bfb\u6027\u548c\u8986\u76d6\u5ea6\u6307\u6807\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc4\u5206\u5668\u63d0\u793a\u8ba9LLM\u8ba1\u7b97\u73b0\u6709CoT\u7684\u8fd9\u4e24\u4e2a\u6307\u6807\uff0c\u5e76\u5728\u5408\u6210CoT\u9000\u5316\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u591a\u4e2a\u524d\u6cbf\u6a21\u578b\u548c\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d1\u73b0\u5b83\u4eec\u8868\u73b0\u51fa\u9ad8\u53ef\u76d1\u63a7\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u6d4b\u91cfCoT\u7684\u9ed8\u8ba4\u53ef\u76d1\u63a7\u6027\uff0c\u662f\u5bf9\u6297\u6027\u538b\u529b\u6d4b\u8bd5\u7684\u8865\u5145\u800c\u975e\u66ff\u4ee3\u3002", "topic": "agent analysis"}}
{"id": "2510.24459", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24459", "abs": "https://arxiv.org/abs/2510.24459", "authors": ["Habtom Kahsay Gidey", "Niklas Huber", "Alexander Lenz", "Alois Knoll"], "title": "Affordance Representation and Recognition for Autonomous Agents", "comment": null, "summary": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u67b6\u6784\u6a21\u5f0f\uff1aDOM\u8f6c\u6362\u6a21\u5f0f\u548c\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\uff0c\u7528\u4e8e\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff0c\u89e3\u51b3\u7f51\u9875\u590d\u6742\u6027\u548c\u670d\u52a1\u52a8\u6001\u9002\u5e94\u6027\u95ee\u9898\u3002", "motivation": "\u8f6f\u4ef6\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u4f9d\u8d56\u4e8e\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u53ef\u64cd\u4f5c\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4f46\u539f\u59cbHTML\u7684\u5197\u957f\u6027\u548c\u9759\u6001API\u96c6\u6210\u7684\u9650\u5236\u963b\u788d\u4e86\u4ee3\u7406\u7684\u9002\u5e94\u6027\u3002", "method": "DOM\u8f6c\u6362\u6a21\u5f0f\u5c06\u5197\u957f\u7684\u539f\u59cbDOM\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u8868\u793a\uff1b\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u901a\u8fc7\u89e3\u6790\u8bed\u4e49\u63cf\u8ff0\u52a8\u6001\u53d1\u73b0\u548c\u96c6\u6210\u672a\u77e5Web\u670d\u52a1\u7684\u80fd\u529b\u3002", "result": "\u8fd9\u4e24\u79cd\u6a21\u5f0f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u6846\u67b6\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u5f0f\u652f\u6301\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4e92\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\uff0c\u9002\u7528\u4e8eWeb\u53ca\u5176\u6269\u5c55\u8d44\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2510.24020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24020", "abs": "https://arxiv.org/abs/2510.24020", "authors": ["Hao An", "Yang Xu"], "title": "Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward", "comment": "23pages, 4figures", "summary": "Mitigating hallucinations in Large Language Models (LLMs) is critical for\ntheir reliable deployment. Existing methods typically fine-tune LLMs to abstain\nfrom answering questions beyond their knowledge scope. However, these methods\noften rely on coarse-grained signals to guide LLMs to abstain, such as overall\nconfidence or uncertainty scores on multiple sampled answers, which may result\nin an imprecise awareness of the model's own knowledge boundaries. To this end,\nwe propose a novel reinforcement learning framework built on\n$\\textbf{\\underline{Fi}ne-grained \\underline{S}emantic \\underline{Co}nfidence\n\\underline{Re}ward (\\Ours)}$, which guides LLMs to abstain via sample-specific\nconfidence. Specifically, our method operates by sampling multiple candidate\nanswers and conducting semantic clustering, then training the LLM to retain\nanswers within high-confidence clusters and discard those within low-confidence\nones, thereby promoting accurate post-hoc abstention. Additionally, we propose\na new metric for evaluating the reliability of abstention fine-tuning tasks\nmore comprehensively. Our method significantly enhances reliability in both\nin-domain and out-of-distribution benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u5f15\u5bfcLLM\u5728\u77e5\u8bc6\u8fb9\u754c\u5916\u7684\u95ee\u9898\u4e0a\u62d2\u7edd\u56de\u7b54\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u7c97\u7c92\u5ea6\u4fe1\u53f7\uff08\u5982\u6574\u4f53\u7f6e\u4fe1\u5ea6\u6216\u4e0d\u786e\u5b9a\u6027\u5206\u6570\uff09\u6765\u6307\u5bfcLLM\u62d2\u7edd\u56de\u7b54\u8d85\u51fa\u77e5\u8bc6\u8303\u56f4\u7684\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5bf9\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u8ba4\u77e5\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u591a\u4e2a\u5019\u9009\u7b54\u6848\u5e76\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\uff0c\u8bad\u7ec3LLM\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u7c07\u4e2d\u7684\u7b54\u6848\uff0c\u4e22\u5f03\u4f4e\u7f6e\u4fe1\u5ea6\u7c07\u4e2d\u7684\u7b54\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u5956\u52b1\u65b9\u6cd5\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u5f15\u5bfcLLM\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.24051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24051", "abs": "https://arxiv.org/abs/2510.24051", "authors": ["In Gim", "Zhiyao Ma", "Seung-seob Lee", "Lin Zhong"], "title": "Pie: A Programmable Serving System for Emerging LLM Applications", "comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie", "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.", "AI": {"tldr": "Pie\u662f\u4e00\u4e2a\u53ef\u7f16\u7a0b\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u4f20\u7edf\u751f\u6210\u5faa\u73af\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u670d\u52a1\u5904\u7406\u5668\uff0c\u5e76\u4f7f\u7528WebAssembly\u6267\u884c\u7528\u6237\u63d0\u4f9b\u7684inferlet\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u9ad8\u6548\u7684LLM\u63a8\u7406\u670d\u52a1\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u57fa\u4e8e\u5355\u4e00\u4ee4\u724c\u751f\u6210\u5faa\u73af\uff0c\u96be\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u63a8\u7406\u7b56\u7565\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u670d\u52a1\u67b6\u6784\u3002", "method": "\u5c06\u4f20\u7edf\u751f\u6210\u5faa\u73af\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u670d\u52a1\u5904\u7406\u5668\uff0c\u901a\u8fc7API\u66b4\u9732\uff1b\u4f7f\u7528\u7528\u6237\u63d0\u4f9b\u7684inferlet\u7a0b\u5e8f\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\uff1b\u91c7\u7528WebAssembly\u8fdb\u884c\u8f7b\u91cf\u7ea7\u6c99\u7bb1\u6267\u884c\u3002", "result": "\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\uff083-12%\u5ef6\u8fdf\u5f00\u9500\uff09\uff0c\u5728\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u63d0\u9ad81.3x-3.4x\uff09\u3002", "conclusion": "Pie\u7cfb\u7edf\u901a\u8fc7\u53ef\u7f16\u7a0b\u67b6\u6784\u5b9e\u73b0\u4e86LLM\u670d\u52a1\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u573a\u666f\u3002", "topic": "agent analysis"}}
{"id": "2510.23658", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23658", "abs": "https://arxiv.org/abs/2510.23658", "authors": ["Vaibhav Jindal", "Hejian Sang", "Chun-Mao Lai", "Yanning Chen", "Zhipeng Wang"], "title": "Aligning Diffusion Language Models via Unpaired Preference Optimization", "comment": null, "summary": "Diffusion language models (dLLMs) are an emerging alternative to\nautoregressive (AR) generators, but aligning them to human preferences is\nchallenging because sequence log-likelihoods are intractable and pairwise\npreference data are costly to collect. We introduce ELBO-KTO, which combines an\nELBO surrogate for diffusion log-likelihoods with a prospect-theoretic,\nunpaired preference objective (Kahneman Tversky Optimization, KTO). We analyze\nthe bias and variance induced by the ELBO substitution and employ\nvariance-reduction practices that stabilize gradients during training. Applied\nto LLaDA-8B-Instruct, ELBO-KTO yields \\textbf{65.9\\%} and \\textbf{62.3\\%}\nadjusted win rates on kto-mix-14k and UltraFeedback-Binary, respectively,\nversus the base model under an automatic LLM judge. Across downstream tasks,\nincluding GSM8K, MMLU, and additional reasoning/knowledge benchmarks, ELBO-KTO\ntrained on UltraFeedback-Binary performs on par with or better than the base\nmodel under identical decoding. This establishes unpaired preference\noptimization as a viable alternative to pairwise alignment in diffusion LLMs.", "AI": {"tldr": "ELBO-KTO\u65b9\u6cd5\u7ed3\u5408ELBO\u66ff\u4ee3\u6269\u6563\u5bf9\u6570\u4f3c\u7136\u4e0e\u65e0\u914d\u5bf9\u504f\u597d\u76ee\u6807\uff0c\u5728\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u9762\u4e34\u5e8f\u5217\u5bf9\u6570\u4f3c\u7136\u96be\u4ee5\u8ba1\u7b97\u548c\u914d\u5bf9\u504f\u597d\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faELBO-KTO\u65b9\u6cd5\uff0c\u4f7f\u7528ELBO\u66ff\u4ee3\u6269\u6563\u5bf9\u6570\u4f3c\u7136\uff0c\u7ed3\u5408\u57fa\u4e8e\u524d\u666f\u7406\u8bba\u7684\u975e\u914d\u5bf9\u504f\u597d\u4f18\u5316\u76ee\u6807\uff0c\u5e76\u91c7\u7528\u65b9\u5dee\u51cf\u5c11\u6280\u672f\u7a33\u5b9a\u8bad\u7ec3\u68af\u5ea6\u3002", "result": "\u5728LLaDA-8B-Instruct\u4e0a\uff0cELBO-KTO\u5728kto-mix-14k\u548cUltraFeedback-Binary\u4e0a\u5206\u522b\u83b7\u5f9765.9%\u548c62.3%\u7684\u8c03\u6574\u80dc\u7387\uff0c\u5728GSM8K\u3001MMLU\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u57fa\u51c6\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u975e\u914d\u5bf9\u504f\u597d\u4f18\u5316\u662f\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u914d\u5bf9\u5bf9\u9f50\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "\u63d0\u51fa\u4e86MGA\uff08Memory-Driven GUI Agent\uff09\uff0c\u901a\u8fc7\"\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\"\u539f\u5219\u91cd\u6784GUI\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u5386\u53f2\u8f68\u8ff9\u7684\u4f9d\u8d56\u548c\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5bf9\u5386\u53f2\u8f68\u8ff9\u7684\u4f9d\u8d56\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u653e\u5927\uff0c\u4ee5\u53ca\"\u5148\u51b3\u7b56\u540e\u89c2\u5bdf\"\u673a\u5236\u5ffd\u89c6\u5173\u952e\u754c\u9762\u7ebf\u7d22\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u6cdb\u5316\u7684GUI\u4ea4\u4e92\u65b9\u6cd5\u3002", "method": "MGA\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5efa\u6a21\u4e3a\u72ec\u7acb\u7684\u73af\u5883\u72b6\u6001\uff0c\u5305\u542b\u4e09\u4e2a\u8981\u7d20\uff1a\u5f53\u524d\u5c4f\u5e55\u622a\u56fe\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7a7a\u95f4\u4fe1\u606f\u3001\u52a8\u6001\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\u3002\u91c7\u7528\"\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\"\u539f\u5219\u3002", "result": "\u5728OSworld\u57fa\u51c6\u6d4b\u8bd5\u3001\u771f\u5b9e\u684c\u9762\u5e94\u7528\uff08Chrome\u3001VSCode\u3001VLC\uff09\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b9e\u9a8c\u4e2d\uff0cMGA\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MGA\u901a\u8fc7\u91cd\u6784GUI\u4ea4\u4e92\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aGUI\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.24284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24284", "abs": "https://arxiv.org/abs/2510.24284", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "AI": {"tldr": "MCP-Flow\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7f51\u7edc\u4ee3\u7406\u9a71\u52a8\u7ba1\u9053\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u670d\u52a1\u5668\u53d1\u73b0\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728MCP\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MCP\u7814\u7a76\u8986\u76d6\u670d\u52a1\u5668\u5c11\u3001\u4f9d\u8d56\u6602\u8d35\u4eba\u5de5\u6574\u7406\u4e14\u7f3a\u4e4f\u8bad\u7ec3\u652f\u6301\uff0c\u9650\u5236\u4e86LLM\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u7f51\u7edc\u4ee3\u7406\u9a71\u52a8\u7ba1\u9053\uff0c\u4ece1166\u4e2a\u670d\u52a1\u5668\u548c11536\u4e2a\u5de5\u5177\u4e2d\u6536\u96c6\u7b5b\u9009\u6570\u636e\uff0c\u751f\u621068733\u4e2a\u9ad8\u8d28\u91cf\u6307\u4ee4-\u51fd\u6570\u8c03\u7528\u5bf9\u548c6439\u6761\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMCP-Flow\u5728\u5de5\u5177\u9009\u62e9\u3001\u51fd\u6570\u8c03\u7528\u751f\u6210\u548c\u4ee3\u7406\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8fdc\u8d85\u5148\u524d\u5de5\u4f5c\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "conclusion": "MCP-Flow\u4e3a\u63d0\u5347LLM\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u719f\u7ec3\u5ea6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.24126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24126", "abs": "https://arxiv.org/abs/2510.24126", "authors": ["Vivek Kalyan", "Martin Andrews"], "title": "Reinforcement Learning for Long-Horizon Multi-Turn Search Agents", "comment": "4 pages plus references and appendices. Accepted into the First\n  Workshop on Multi-Turn Interactions in Large Language Models at NeurIPS 2025", "summary": "Large Language Model (LLM) agents can leverage multiple turns and tools to\nsolve complex tasks, with prompt-based approaches achieving strong performance.\nThis work demonstrates that Reinforcement Learning (RL) can push capabilities\nsignificantly further by learning from experience. Through experiments on a\nlegal document search benchmark, we show that our RL-trained 14 Billion\nparameter model outperforms frontier class models (85% vs 78% accuracy). In\naddition, we explore turn-restricted regimes, during training and at test-time,\nthat show these agents achieve better results if allowed to operate over longer\nmulti-turn horizons.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684140\u4ebf\u53c2\u6570\u6a21\u578b\u5728\u6cd5\u5f8b\u6587\u6863\u641c\u7d22\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u524d\u6cbf\u6a21\u578b\uff0c\u51c6\u786e\u7387\u8fbe\u523085% vs 78%\uff0c\u5e76\u8bc1\u660e\u591a\u8f6e\u4ea4\u4e92\u80fd\u63d0\u5347\u6027\u80fd", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u901a\u8fc7\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u6765\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u80fd\u529b\uff0c\u8d85\u8d8a\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5", "method": "\u5728\u6cd5\u5f8b\u6587\u6863\u641c\u7d22\u57fa\u51c6\u4e0a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3140\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u7684\u8f6e\u6b21\u9650\u5236\u5bf9\u6027\u80fd\u7684\u5f71\u54cd", "result": "RL\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u8d85\u8d8a\u524d\u6cbf\u6a21\u578b\uff0885% vs 78%\uff09\uff0c\u4e14\u591a\u8f6e\u4ea4\u4e92\u80fd\u5e26\u6765\u66f4\u597d\u7684\u7ed3\u679c", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u591a\u8f6e\u4ea4\u4e92\u5bf9\u4efb\u52a1\u5b8c\u6210\u81f3\u5173\u91cd\u8981", "topic": "agentic reinforcement learning"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u884c\u4e3a\u7684\u76f8\u5173\u77e9\u9635\u79e9\u6765\u8bc4\u4f30\u63a8\u7406\u8def\u5f84\u53ef\u4fe1\u5ea6\u7684Self-Indicator\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u5373\u53ef\u6709\u6548\u68c0\u6d4b\u63a8\u7406\u9519\u8bef", "motivation": "\u73b0\u6709\u68c0\u67e5\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\uff08\u5982\u8bad\u7ec3\u9a8c\u8bc1\u5668\u6216\u590d\u6742\u63d0\u793a\uff09\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u901a\u7528\u7684LLM\u8f93\u51fa\u9a8c\u8bc1\u65b9\u6cd5", "method": "\u901a\u8fc7\u8ba1\u7b97\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u77e9\u9635\u79e9\u4f5c\u4e3a\u63a8\u7406\u6b63\u786e\u6027\u7684\u6307\u6807\uff0c\u8bbe\u8ba1Self-Indicator\u65b9\u6cd5\u5bf9\u5019\u9009\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u91cd\u52a0\u6743", "result": "\u5728\u591a\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u4e0a\u5b9e\u9a8c\uff0c\u8be5\u65b9\u6cd5\u80fd\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u8def\u5f84\u7684\u51c6\u786e\u7387\u8d85\u8fc775%\uff0c\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc78%", "conclusion": "LLM\u7684\u5185\u90e8\u884c\u4e3a\u5df2\u7ecf\u9690\u542b\u4e86\u5176\u63a8\u7406\u8def\u5f84\u7684\u53ef\u4fe1\u5ea6\u4fe1\u606f\uff0cSelf-Indicator\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e", "topic": "agent analysis"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u9884\u6d4b\u6027-\u53ef\u8ba1\u7b97\u6027-\u7a33\u5b9a\u6027(PCS)\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347LLM\u9a71\u52a8\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u63a8\u7406\uff0c\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u539f\u5219\u6307\u5bfc\uff0c\u5728\u5904\u7406\u590d\u6742\u73b0\u5b9e\u6570\u636e\u96c6\u65f6\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8ePCS\u539f\u5219\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u5904\u7406\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u8d1f\u8d23\uff0c\u7ed3\u5408\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u57289\u4e2a\u4e0d\u540c\u7279\u5f81\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528DeepSeek-V3\u548cGPT-4o\u4f5c\u4e3a\u540e\u7aef\uff0cVDSAgents\u6301\u7eed\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u7b49\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u3002", "conclusion": "\u5c06PCS\u539f\u5219\u5d4c\u5165LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u662f\u53ef\u884c\u7684\uff0c\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.24208", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24208", "abs": "https://arxiv.org/abs/2510.24208", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment", "comment": "an early-stage version", "summary": "Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u503c\u800c\u975e\u5c42\u53c2\u6570\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u89c4\u6a21LLM\u4e4b\u95f4\u7684\u795e\u7ecf\u4e0d\u517c\u5bb9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u91cd\u7528\u5c42\u53c2\u6570\u5728\u4e0d\u540c\u89c4\u6a21LLM\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\u5b58\u5728\u4e25\u91cd\u9650\u5236\uff0c\u4e3b\u8981\u7531\u4e8e\u795e\u7ecf\u4e0d\u517c\u5bb9\u6027\uff08\u67b6\u6784\u548c\u53c2\u6570\u5dee\u5f02\uff09\u3002\u9700\u8981\u627e\u5230\u66f4\u6709\u6548\u7684\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u66f4\u5927\u7684\u7075\u6d3b\u6027\u548c\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002", "method": "\u5c06\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\u4f5c\u4e3a\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u7684\u57fa\u7840\u524d\u63d0\uff0c\u4f7f\u7528\u6fc0\u6d3b\u503c\u4f5c\u4e3a\u5c42\u95f4\u77e5\u8bc6\u8f6c\u79fb\u7684\u5a92\u4ecb\uff0c\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u5b9e\u73b0\u7b80\u5355\u6709\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u66f4\u597d\u5730\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e4b\u95f4\u5bf9\u9f50\u6a21\u578b\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u7f13\u89e3\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u4e3a\u6f5c\u5728\u8bed\u4e49\u5bf9\u9f50\u7684\u672c\u8d28\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u4e3aLLM\u77e5\u8bc6\u8f6c\u79fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.24383", "categories": ["cs.AI", "cs.CY", "cs.MA", "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.24383", "abs": "https://arxiv.org/abs/2510.24383", "authors": ["Juraj Mavra\u010di\u0107"], "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents", "comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796", "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.", "AI": {"tldr": "Policy Cards\u662f\u4e00\u79cd\u673a\u5668\u53ef\u8bfb\u7684\u90e8\u7f72\u5c42\u6807\u51c6\uff0c\u7528\u4e8e\u8868\u8fbeAI\u4ee3\u7406\u7684\u64cd\u4f5c\u3001\u76d1\u7ba1\u548c\u4f26\u7406\u7ea6\u675f\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u9075\u5faa\u8981\u6c42\u3002", "motivation": "\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5408\u89c4\u6027\u673a\u5236\uff0c\u5c06\u9ad8\u7ea7\u6cbb\u7406\u4e0e\u5de5\u7a0b\u5b9e\u8df5\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u7684\u53ef\u95ee\u8d23\u81ea\u4e3b\u6027\u3002", "method": "\u5b9a\u4e49\u89c4\u8303\u5c42\u7f16\u7801\u5141\u8bb8/\u62d2\u7edd\u89c4\u5219\u3001\u4e49\u52a1\u3001\u8bc1\u636e\u8981\u6c42\u548c\u4e0eNIST AI RMF\u3001ISO/IEC 42001\u3001\u6b27\u76dfAI\u6cd5\u6848\u7b49\u4fdd\u8bc1\u6846\u67b6\u7684\u6620\u5c04\u5173\u7cfb\u3002", "result": "\u6bcf\u4e2aPolicy Card\u53ef\u4ee5\u81ea\u52a8\u9a8c\u8bc1\u3001\u7248\u672c\u63a7\u5236\uff0c\u5e76\u4e0e\u8fd0\u884c\u65f6\u6267\u884c\u6216\u6301\u7eed\u5ba1\u8ba1\u7ba1\u9053\u94fe\u63a5\u3002", "conclusion": "Policy Cards\u4e3a\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5408\u89c4\u6027\uff0c\u5f62\u6210\u591a\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u5206\u5e03\u5f0f\u4fdd\u8bc1\u7684\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.24397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24397", "abs": "https://arxiv.org/abs/2510.24397", "authors": ["Jiarui Qin", "Yunjia Xi", "Junjie Huang", "Renting Rui", "Di Yin", "Weiwen Liu", "Yong Yu", "Weinan Zhang", "Xing Sun"], "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "comment": "46 pages", "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.", "AI": {"tldr": "APTBench \u662f\u4e00\u4e2a\u65b0\u7684\u9884\u8bad\u7ec3\u57fa\u51c6\u6846\u67b6\uff0c\u5c06\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u4efb\u52a1\u8f6c\u6362\u4e3a\u9002\u5408\u57fa\u7840\u6a21\u578b\u7684\u591a\u9009\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u667a\u80fd\u4f53\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u9759\u6001\u6280\u80fd\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff1b\u800c\u667a\u80fd\u4f53\u57fa\u51c6\u901a\u5e38\u9488\u5bf9\u540e\u8bad\u7ec3\u6a21\u578b\uff0c\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u652f\u6301\u591a\u8f6e\u4efb\u52a1\u6267\u884c\u3002\u9700\u8981\u80fd\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u8bc4\u4f30\u667a\u80fd\u4f53\u6f5c\u529b\u7684\u57fa\u51c6\u3002", "method": "\u5c06\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u4efb\u52a1\u548c\u6210\u529f\u8f68\u8ff9\u8f6c\u6362\u4e3a\u591a\u9009\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u805a\u7126\u89c4\u5212\u548c\u884c\u52a8\u7b49\u6838\u5fc3\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u8986\u76d6\u8f6f\u4ef6\u5de5\u7a0b\u548c\u6df1\u5ea6\u7814\u7a76\u7b49\u5173\u952e\u573a\u666f\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u901a\u7528\u57fa\u51c6\uff0cAPTBench \u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u6a21\u578b\u5728\u4e0b\u6e38\u4f5c\u4e3a\u667a\u80fd\u4f53\u7684\u8868\u73b0\uff0c\u540c\u65f6\u6bd4\u540e\u8bad\u7ec3\u7684\u5168\u89c4\u6a21\u7aef\u5230\u7aef\u8bc4\u4f30\u66f4\u8f7b\u91cf\u3001\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u3002", "conclusion": "APTBench \u586b\u8865\u4e86\u9884\u8bad\u7ec3\u9636\u6bb5\u667a\u80fd\u4f53\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u6307\u5bfc\u3002", "topic": "swe benchmark"}}
{"id": "2510.24411", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24411", "abs": "https://arxiv.org/abs/2510.24411", "authors": ["Qiushi Sun", "Mukai Li", "Zhoumianze Liu", "Zhihui Xie", "Fangzhi Xu", "Zhangyue Yin", "Kanzhi Cheng", "Zehao Li", "Zichen Ding", "Qi Liu", "Zhiyong Wu", "Zhuosheng Zhang", "Ben Kao", "Lingpeng Kong"], "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows", "comment": "work in progress", "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548c\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\uff0c\u4ee5\u53caOS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u79fb\u52a8\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u79fb\u52a8\u5e73\u53f0\u7b49\u6570\u5b57\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u7c7b\u4eba\u80fd\u529b\uff0c\u5176\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\uff08\u5982\u7cfb\u7edf\u7834\u574f\u548c\u9690\u79c1\u6cc4\u9732\uff09\u5f15\u53d1\u4e86\u91cd\u5927\u62c5\u5fe7\u3002\u5728\u79fb\u52a8\u73af\u5883\u5e7f\u9614\u590d\u6742\u7684\u64cd\u4f5c\u7a7a\u95f4\u4e2d\u68c0\u6d4b\u8fd9\u4e9b\u5b89\u5168\u95ee\u9898\u662f\u4e00\u4e2a\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u5f15\u5165MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548c\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\uff0c\u5305\u542b\u5e26\u6709\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684\u771f\u5b9e\u8f68\u8ff9\u3002\u63d0\u51faOS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u9a8c\u8bc1\u5668\u68c0\u6d4b\u663e\u5f0f\u7cfb\u7edf\u7ea7\u8fdd\u89c4\u548c\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\u5668\u8bc4\u4f30\u4e0a\u4e0b\u6587\u98ce\u9669\u548c\u4ee3\u7406\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOS-Sentinel\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8610%-30%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u79fb\u52a8\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u4fc3\u8fdb\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u7684\u5173\u952e\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u6bd4\u8f83\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u57fa\u51c6\u8fdb\u884c\u5bf9\u6bd4", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u5f88\u91cd\u8981\uff0c\u9700\u8981\u4e86\u89e3\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\u3001\u8fdb\u884c\u63a8\u7406\u5e76\u4ee5\u903b\u8f91\u6709\u6548\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u8bba", "method": "\u4f7f\u75288\u4e2a\u5b9a\u5236\u8bbe\u8ba1\u7684\u63a8\u7406\u95ee\u9898\u6d4b\u8bd5GPT\u3001Claude\u3001DeepSeek\u3001Gemini\u3001Grok\u3001Llama\u3001Mistral\u3001Perplexity\u548cSabi'a\u7b49LLM\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u5bf9\u6bd4", "result": "\u63ed\u793a\u4e86\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660eLLM\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be", "conclusion": "LLM\u5728\u903b\u8f91\u63a8\u7406\u80fd\u529b\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb", "topic": "agent analysis"}}
{"id": "2510.24302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24302", "abs": "https://arxiv.org/abs/2510.24302", "authors": ["Shangyu Xing", "Siyuan Wang", "Chenyuan Yang", "Xinyu Dai", "Xiang Ren"], "title": "Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly with\nalgorithms like Group Relative Policy Optimization (GRPO), has proven highly\neffective in enhancing the reasoning capabilities of large language models.\nHowever, a critical bottleneck in current pipelines lies in the limited\ndiversity of sampled trajectories during group rollouts. Homogeneous\ntrajectories and their associated rewards would diminish the return signals for\npolicy updates, thereby hindering effective policy learning. This lack of\ndiversity stems primarily from token-level stochastic sampling, where local\nvariations are likely to collapse into near-identical reasoning paths. To\naddress this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a\nnovel rollout strategy designed to explicitly promotes trajectory-level\ndiversity by enforcing branching into different candidate tokens likely to\nyield distinct continuations. Specifically, LATR iteratively operates in three\nstages: (1) branching at high-uncertainty generation steps, (2) performing\nlookahead simulation for each new branch, and (3) pruning branches that\nexhibits prolonged similarity during simulation. Compared with stochastic\nSampling, LATR accelerates policy learning by 131% on average and improves\nfinal pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy\nOptimization (DAPO) algorithms across different reasoning tasks. Our code and\ndata are publicly available at https://github.com/starreeze/latr.", "AI": {"tldr": "\u63d0\u51faLATR\u65b9\u6cd5\u89e3\u51b3RLVR\u4e2d\u8f68\u8ff9\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u524d\u77bb\u6811\u641c\u7d22\u4fc3\u8fdb\u5206\u652f\u591a\u6837\u6027\uff0c\u76f8\u6bd4\u968f\u673a\u91c7\u6837\u52a0\u901f\u7b56\u7565\u5b66\u4e60131%\uff0c\u63d0\u5347\u6700\u7ec8\u6027\u80fd4.2%\u3002", "motivation": "\u5f53\u524dRLVR\u65b9\u6cd5\u5728\u7fa4\u4f53rollout\u8fc7\u7a0b\u4e2d\u91c7\u6837\u8f68\u8ff9\u591a\u6837\u6027\u6709\u9650\uff0c\u540c\u8d28\u5316\u8f68\u8ff9\u548c\u5956\u52b1\u4f1a\u524a\u5f31\u7b56\u7565\u66f4\u65b0\u7684\u56de\u62a5\u4fe1\u53f7\uff0c\u963b\u788d\u6709\u6548\u7b56\u7565\u5b66\u4e60\u3002", "method": "LATR\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a(1)\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u6b65\u9aa4\u8fdb\u884c\u5206\u652f\uff1b(2)\u5bf9\u6bcf\u4e2a\u65b0\u5206\u652f\u6267\u884c\u524d\u77bb\u6a21\u62df\uff1b(3)\u5728\u6a21\u62df\u8fc7\u7a0b\u4e2d\u526a\u9664\u6301\u7eed\u76f8\u4f3c\u7684\u5206\u652f\u3002", "result": "\u76f8\u6bd4\u968f\u673a\u91c7\u6837\uff0cLATR\u5e73\u5747\u52a0\u901f\u7b56\u7565\u5b66\u4e60131%\uff0c\u5728GRPO\u548cDAPO\u7b97\u6cd5\u4e0a\u63d0\u5347\u6700\u7ec8pass@1\u6027\u80fd4.2%\u3002", "conclusion": "LATR\u901a\u8fc7\u663e\u5f0f\u4fc3\u8fdb\u8f68\u8ff9\u7ea7\u591a\u6837\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u7684\u8f68\u8ff9\u540c\u8d28\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.24320", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24320", "abs": "https://arxiv.org/abs/2510.24320", "authors": ["Zhiheng Xi", "Jixuan Huang", "Xin Guo", "Boyang Hong", "Dingwen Yang", "Xiaoran Fan", "Shuo Li", "Zehui Chen", "Junjie Ye", "Siyu Yuan", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning", "comment": "Preprint, 25 pages, 9 figures. Code:\n  https://github.com/WooooDyy/Critique-RL", "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", "AI": {"tldr": "\u63d0\u51faCritique-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8bc4\u8bba\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u5f3a\u76d1\u7763\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u63d0\u5347\u8bc4\u8bba\u8005\u7684\u5224\u522b\u80fd\u529b\u548c\u5e2e\u52a9\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5f3a\u76d1\u7763\u6807\u6ce8\u8bc4\u8bba\u6570\u636e\uff0c\u9650\u5236\u4e86\u8bc4\u8bba\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u53cc\u73a9\u5bb6\u8303\u5f0f\uff1a\u6f14\u5458\u751f\u6210\u54cd\u5e94\uff0c\u8bc4\u8bba\u8005\u63d0\u4f9b\u53cd\u9988\uff0c\u6f14\u5458\u636e\u6b64\u4f18\u5316\u54cd\u5e94\u3002\u4f7f\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u89c4\u5219\u5956\u52b1\u589e\u5f3a\u5224\u522b\u80fd\u529b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u95f4\u63a5\u5956\u52b1\u63d0\u5347\u5e2e\u52a9\u6027\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5982Qwen2.5-7B\u5728\u57df\u5185\u4efb\u52a1\u63d0\u53479.02%\uff0c\u57df\u5916\u4efb\u52a1\u63d0\u53475.70%\u3002", "conclusion": "Critique-RL\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8bc4\u8bba\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\u548c\u5f15\u5165\u7279\u6743\u6307\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86SNN\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u80fd\u91cf\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u8bad\u7ec3\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8109\u51b2\u795e\u7ecf\u5143\u7684\u4e0d\u53ef\u5fae\u5206\u6027\u548c\u72b6\u6001\u52a8\u6001\u9700\u8981\u5e8f\u5217\u8bad\u7ec3\uff0c\u8fd9\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d7\u5230\u65e9\u671f\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u9650\u5236\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\uff0c\u53d1\u73b0\u8f83\u6d45\u659c\u7387\u80fd\u589e\u52a0\u6df1\u5c42\u68af\u5ea6\u5e45\u5ea6\u4f46\u964d\u4f4e\u4e0e\u771f\u5b9e\u68af\u5ea6\u7684\u5bf9\u9f50\uff1b\u63d0\u51fa\u4f7f\u7528\u7279\u6743\u6307\u5bfc\u7b56\u7565\u6765\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u5229\u7528\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\uff1b\u7ed3\u5408\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u65b9\u6cd5\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u8f83\u6d45\u659c\u7387\u6216\u8c03\u5ea6\u659c\u7387\u4f7f\u8bad\u7ec3\u548c\u6700\u7ec8\u90e8\u7f72\u6027\u80fd\u63d0\u53472.1\u500d\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u56de\u62a5\u8fbe\u5230400\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\u548cTD3BC\u7b49\u73b0\u6709\u6280\u672f\uff08\u6700\u591a-200\u5206\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5bf9SNN\u4e2d\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u4e3a\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "FunReason-MT\u662f\u4e00\u4e2a\u7528\u4e8e\u5408\u6210\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u6570\u636e\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u5883-API\u56fe\u4ea4\u4e92\u3001\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u548c\u5f15\u5bfc\u8fed\u4ee3\u94fe\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u73af\u5883\u91c7\u6837\u6216\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\uff09\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u65e0\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u5b58\u5728\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u3001\u5de5\u5177\u67b6\u6784\u9694\u79bb\u548c\u591a\u8f6e\u903b\u8f91\u4f9d\u8d56\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "\u91c7\u7528\u73af\u5883-API\u56fe\u4ea4\u4e92\u6536\u96c6\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u7b80\u5316\u56f0\u96be\u67e5\u8be2\u6784\u5efa\uff0c\u5f15\u5bfc\u8fed\u4ee3\u94fe\u751f\u6210\u590d\u6742\u601d\u7ef4\u94fe\u3002", "result": "\u5728Berkeley Function-Calling Leaderboard (BFCLv3)\u4e0a\uff0c\u57fa\u4e8eFunReason-MT\u751f\u6210\u6570\u636e\u6784\u5efa\u76844B\u6a21\u578b\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f18\u4e8e\u5927\u591a\u6570\u95ed\u6e90\u6a21\u578b\u3002\u5728BFCLv4\u4e0a\u7684\u8fdb\u4e00\u6b65\u6027\u80fd\u63d0\u5347\u8bc1\u5b9e\u4e86\u5176\u53ef\u9760\u6027\u3002", "conclusion": "FunReason-MT\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9c81\u68d2\u7684\u6570\u636e\u6e90\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u6570\u636e\u7684\u590d\u6742\u6027\u969c\u788d\u3002", "topic": "agent analysis"}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "OrchDAG\u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u5177\u6709\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u6709\u5411\u65e0\u73af\u56fe\uff0c\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\u5e76\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u5ffd\u89c6\u4e86\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u548c\u8bc4\u4f30\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u3002", "method": "\u5f15\u5165OrchDAG\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u673a\u5236\uff0c\u5e76\u4e0eGRPO\u98ce\u683c\u7b97\u6cd5\u7ed3\u5408\u8fdb\u884cRLVR\u8bad\u7ec3\u3002", "result": "\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4f46\u53ef\u89e3\u51b3\u7684\u57fa\u51c6\uff0c\u63d0\u51fa\u7684\u5956\u52b1\u673a\u5236\u5728\u4e0eGRPO\u98ce\u683c\u7b97\u6cd5\u7ed3\u5408\u65f6\u8868\u73b0\u6709\u6548\u3002", "conclusion": "\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\uff0c\u5229\u7528\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u5ea6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2510.24438", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24438", "abs": "https://arxiv.org/abs/2510.24438", "authors": ["Abdullah Mushtaq", "Rafay Naeem", "Ezieddin Elmahjub", "Ibrahim Ghaznavi", "Shawqi Al-Maliki", "Mohamed Abdallah", "Ala Al-Fuqaha", "Junaid Qadir"], "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: 5th Muslims in Machine Learning (MusIML) Workshop", "summary": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.", "AI": {"tldr": "\u8bc4\u4f30GPT-4o\u3001Ansari AI\u548cFanar\u5728\u4f0a\u65af\u5170\u6307\u5bfc\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u53cc\u4ee3\u7406\u6846\u67b6\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u51c6\u786e\u5f15\u7528\u4f0a\u65af\u5170\u6587\u672c\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u4f0a\u65af\u5170\u6307\u5bfc\uff0c\u4f46\u5b58\u5728\u8bef\u5f15\u6587\u672c\u3001\u8bef\u7528\u6cd5\u7406\u5b66\u6216\u4ea7\u751f\u6587\u5316\u4e0d\u4e00\u81f4\u56de\u7b54\u7684\u98ce\u9669\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u4f0a\u65af\u5170\u77e5\u8bc6\u9886\u57df\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u53cc\u4ee3\u7406\u6846\u67b6\uff1a\u5b9a\u91cf\u4ee3\u7406\u8fdb\u884c\u5f15\u7528\u9a8c\u8bc1\u548c\u516d\u7ef4\u5ea6\u8bc4\u5206\uff08\u7ed3\u6784\u3001\u4f0a\u65af\u5170\u4e00\u81f4\u6027\u3001\u5f15\u7528\u7b49\uff09\uff0c\u5b9a\u6027\u4ee3\u7406\u8fdb\u884c\u4e94\u7ef4\u5ea6\u5e76\u6392\u6bd4\u8f83\uff08\u8bed\u8c03\u3001\u6df1\u5ea6\u3001\u539f\u521b\u6027\u7b49\uff09\u3002", "result": "GPT-4o\u5728\u4f0a\u65af\u5170\u51c6\u786e\u6027\uff083.93\uff09\u548c\u5f15\u7528\uff083.38\uff09\u65b9\u9762\u5f97\u5206\u6700\u9ad8\uff0cAnsari AI\u6b21\u4e4b\uff083.68, 3.32\uff09\uff0cFanar\u843d\u540e\uff082.76, 1.82\uff09\u3002GPT-4o\u5b9a\u91cf\u5e73\u5747\u5206\u6700\u9ad8\uff083.90/5\uff09\uff0cAnsari AI\u5728\u5b9a\u6027\u6bd4\u8f83\u4e2d\u83b7\u80dc\u6700\u591a\uff08116/200\uff09\u3002", "conclusion": "\u5c3d\u7ba1\u8868\u73b0\u76f8\u5bf9\u8f83\u597d\uff0c\u6a21\u578b\u5728\u53ef\u9760\u751f\u6210\u51c6\u786e\u7684\u4f0a\u65af\u5170\u5185\u5bb9\u548c\u5f15\u7528\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u4ee5\u7a46\u65af\u6797\u89c6\u89d2\u4e3a\u4e2d\u5fc3\u7684\u793e\u533a\u9a71\u52a8\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2510.24446", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24446", "abs": "https://arxiv.org/abs/2510.24446", "authors": ["Viktoriia Zinkovich", "Anton Antonov", "Andrei Spiridonov", "Denis Shepelev", "Andrey Moskalenko", "Daria Pugacheva", "Elena Tutubalina", "Andrey Kuznetsov", "Vlad Shakhuro"], "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u6027\u6539\u5199\u4efb\u52a1\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u4e49\u7b49\u4ef7\u4f46\u8bed\u6cd5\u6b63\u786e\u7684\u6587\u672c\u6539\u5199\u6765\u964d\u4f4e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e86SPARTA\u65b9\u6cd5\u6765\u6709\u6548\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u8f93\u5165\u7684\u6270\u52a8\uff0c\u800c\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7528\u6237\u4f1a\u7528\u591a\u79cd\u65b9\u5f0f\u8868\u8fbe\u76f8\u540c\u610f\u56fe\uff0c\u8bed\u4e49\u7b49\u4ef7\u7684\u6587\u672c\u6539\u5199\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86SPARTA\u65b9\u6cd5\u2014\u2014\u4e00\u79cd\u9ed1\u76d2\u3001\u53e5\u5b50\u7ea7\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u81ea\u7f16\u7801\u5668\u7684\u4f4e\u7ef4\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6307\u5bfc\u751f\u6210\u5bf9\u6297\u6027\u6539\u5199\u3002", "result": "SPARTA\u5728ReasonSeg\u548cLLMSeg-40k\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa\u6700\u591a2\u500d\uff0c\u6210\u529f\u63ed\u793a\u4e86\u5148\u8fdb\u63a8\u7406\u5206\u5272\u6a21\u578b\u5bf9\u5bf9\u6297\u6027\u6539\u5199\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u5373\u4f7f\u53d7\u5230\u4e25\u683c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u7ea6\u675f\uff0c\u5148\u8fdb\u7684\u63a8\u7406\u5206\u5272\u6a21\u578b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u6539\u5199\u7684\u653b\u51fb\uff0c\u8fd9\u51f8\u663e\u4e86\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5fc5\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.24476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24476", "abs": "https://arxiv.org/abs/2510.24476", "authors": ["Yihan Li", "Xiyuan Fu", "Ghanshyam Verma", "Paul Buitelaar", "Mingming Liu"], "title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems", "comment": "25 pages, 7 figures, 3 tables", "summary": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86RAG\u548c\u63a8\u7406\u589e\u5f3a\u5728\u7f13\u89e3LLM\u5e7b\u89c9\u65b9\u9762\u7684\u534f\u540c\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u548c\u903b\u8f91\u7684\u5e7b\u89c9\u5206\u7c7b\uff0c\u5e76\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5e7b\u89c9\u662fLLM\u53ef\u9760\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76RAG\u548c\u63a8\u7406\u589e\u5f3a\u8fd9\u4e24\u79cd\u6700\u6709\u6548\u65b9\u6cd5\u7684\u534f\u540c\u673a\u5236\u3002", "method": "\u91c7\u7528\u5e94\u7528\u5bfc\u5411\u7684\u80fd\u529b\u589e\u5f3a\u89c6\u89d2\uff0c\u5206\u6790RAG\u3001\u63a8\u7406\u589e\u5f3a\u53ca\u5176\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6574\u5408\u5982\u4f55\u7f13\u89e3\u5e7b\u89c9\uff0c\u63d0\u51fa\u5e7b\u89c9\u5206\u7c7b\u5b66\u5e76\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u533a\u5206\u77e5\u8bc6\u578b\u548c\u903b\u8f91\u578b\u5e7b\u89c9\u7684\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u68c0\u9a8c\u4e86RAG\u548c\u63a8\u7406\u5982\u4f55\u5206\u522b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e94\u7528\u3001\u8bc4\u4f30\u548c\u57fa\u51c6\u652f\u6301\u3002", "conclusion": "RAG\u548c\u63a8\u7406\u589e\u5f3a\u5728\u7f13\u89e3LLM\u5e7b\u89c9\u65b9\u9762\u5177\u6709\u91cd\u8981\u534f\u540c\u6f5c\u529b\uff0c\u9700\u8981\u7cfb\u7edf\u6574\u5408\u4ee5\u5e73\u8861\u521b\u9020\u6027\u548c\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.24505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24505", "abs": "https://arxiv.org/abs/2510.24505", "authors": ["Qing Zong", "Jiayu Liu", "Tianshi Zheng", "Chunyang Li", "Baixuan Xu", "Haochen Shi", "Weiqi Wang", "Zhaowei Wang", "Chunkit Chan", "Yangqiu Song"], "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?", "comment": null, "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u901a\u8fc7\u5206\u6790\u6279\u8bc4\u5185\u5bb9\u548c\u65b9\u5f0f\uff0c\u5f00\u53d1\u4e86Self-Critique\u548cCritiCal\u4e24\u79cd\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6027\u80fd\u3002", "motivation": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u6240\u9700\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u800c\u7cbe\u786e\u7684\u9ec4\u91d1\u7f6e\u4fe1\u5ea6\u6807\u7b7e\u96be\u4ee5\u83b7\u53d6\uff0c\u9700\u8981\u591a\u6b21\u751f\u6210\u3002", "method": "\u7814\u7a76\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u5982\u4f55\u589e\u5f3a\u7f6e\u4fe1\u5ea6\u8868\u8fbe\uff1a(1)\u5206\u6790\u6279\u8bc4\u5185\u5bb9\uff1a\u4e0d\u786e\u5b9a\u6027\uff08\u95ee\u9898\u5bfc\u5411\uff09vs\u7f6e\u4fe1\u5ea6\uff08\u7b54\u6848\u5bfc\u5411\uff09\uff1b(2)\u63d0\u51faSelf-Critique\u65b9\u6cd5\u8ba9LLM\u81ea\u6211\u6279\u8bc4\u4f18\u5316\u7f6e\u4fe1\u5ea6\uff0c\u4ee5\u53caCritiCal\u6279\u8bc4\u6821\u51c6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u6539\u8fdb\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCritiCal\u663e\u8457\u4f18\u4e8eSelf-Critique\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u751a\u81f3\u8d85\u8d8a\u4e86\u5176\u6559\u5e08\u6a21\u578bGPT-4o\uff0c\u5e76\u5728\u5206\u5e03\u5916\u8bbe\u7f6e\u4e2d\u5c55\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u662f\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u6709\u6548\u65b9\u6cd5\uff0cCritiCal\u65b9\u6cd5\u901a\u8fc7\u8d85\u8d8a\u76f4\u63a5\u6570\u503c\u4f18\u5316\u7684\u65b9\u5f0f\uff0c\u63d0\u5347\u4e86LLM\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.23948", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23948", "abs": "https://arxiv.org/abs/2510.23948", "authors": ["Qianfeng Wen", "Zhenwei Tang", "Ashton Anderson"], "title": "ChessQA: Evaluating Large Language Models for Chess Understanding", "comment": "33 pages,8 figures", "summary": "Chess provides an ideal testbed for evaluating the reasoning, modeling, and\nabstraction capabilities of large language models (LLMs), as it has\nwell-defined structure and objective ground truth while admitting a wide\nspectrum of skill levels. However, existing evaluations of LLM ability in chess\nare ad hoc and narrow in scope, making it difficult to accurately measure LLM\nchess understanding and how it varies with scale, post-training methodologies,\nor architecture choices. We present ChessQA, a comprehensive benchmark that\nassesses LLM chess understanding across five task categories (Structural,\nMotifs, Short Tactics, Position Judgment, and Semantic), which approximately\ncorrespond to the ascending abstractions that players master as they accumulate\nchess knowledge, from understanding basic rules and learning tactical motifs to\ncorrectly calculating tactics, evaluating positions, and semantically\ndescribing high-level concepts. In this way, ChessQA captures a more\ncomprehensive picture of chess ability and understanding, going significantly\nbeyond the simple move quality evaluations done previously, and offers a\ncontrolled, consistent setting for diagnosis and comparison. Furthermore,\nChessQA is inherently dynamic, with prompts, answer keys, and construction\nscripts that can evolve as models improve. Evaluating a range of contemporary\nLLMs, we find persistent weaknesses across all five categories and provide\nresults and error analyses by category. We will release the code, periodically\nrefreshed datasets, and a public leaderboard to support further research.", "AI": {"tldr": "ChessQA\u662f\u4e00\u4e2a\u5168\u9762\u7684\u56fd\u9645\u8c61\u68cb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u5728\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\u4e2d\u7684\u56fd\u9645\u8c61\u68cb\u7406\u89e3\u80fd\u529b\uff0c\u4ece\u57fa\u672c\u89c4\u5219\u5230\u9ad8\u7ea7\u6982\u5ff5\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u8d70\u5b50\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u5bf9LLM\u5728\u56fd\u9645\u8c61\u68cb\u4e2d\u80fd\u529b\u7684\u8bc4\u4f30\u662f\u4e34\u65f6\u6027\u7684\u4e14\u8303\u56f4\u72ed\u7a84\uff0c\u96be\u4ee5\u51c6\u786e\u8861\u91cfLLM\u7684\u56fd\u9645\u8c61\u68cb\u7406\u89e3\u80fd\u529b\u53ca\u5176\u968f\u89c4\u6a21\u3001\u8bad\u7ec3\u540e\u65b9\u6cd5\u6216\u67b6\u6784\u9009\u62e9\u7684\u53d8\u5316\u3002", "method": "\u521b\u5efaChessQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\uff1a\u7ed3\u6784\u3001\u6a21\u5f0f\u3001\u77ed\u6218\u672f\u3001\u4f4d\u7f6e\u5224\u65ad\u548c\u8bed\u4e49\uff0c\u5bf9\u5e94\u73a9\u5bb6\u79ef\u7d2f\u56fd\u9645\u8c61\u68cb\u77e5\u8bc6\u65f6\u638c\u63e1\u7684\u9012\u8fdb\u62bd\u8c61\u5c42\u6b21\u3002", "result": "\u8bc4\u4f30\u5f53\u4ee3LLM\u53d1\u73b0\u6240\u6709\u4e94\u4e2a\u7c7b\u522b\u90fd\u5b58\u5728\u6301\u7eed\u5f31\u70b9\uff0c\u5e76\u901a\u8fc7\u7c7b\u522b\u63d0\u4f9b\u7ed3\u679c\u548c\u9519\u8bef\u5206\u6790\u3002", "conclusion": "ChessQA\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u56fd\u9645\u8c61\u68cb\u80fd\u529b\u8bc4\u4f30\uff0c\u4e3a\u8bca\u65ad\u548c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u53d7\u63a7\u4e00\u81f4\u7684\u73af\u5883\uff0c\u5e76\u5c06\u53d1\u5e03\u4ee3\u7801\u3001\u5b9a\u671f\u66f4\u65b0\u7684\u6570\u636e\u96c6\u548c\u516c\u5171\u6392\u884c\u699c\u3002", "topic": "agent analysis"}}
{"id": "2510.24636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24636", "abs": "https://arxiv.org/abs/2510.24636", "authors": ["Ziyou Hu", "Zhengliang Shi", "Minghang Zhu", "Haitao Li", "Teng Sun", "Pengjie Ren", "Suzan Verberne", "Zhaochun Ren"], "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning", "comment": null, "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.", "AI": {"tldr": "OpenRM\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u957f\u6587\u672c\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u8c03\u7528\u5916\u90e8\u5de5\u5177\u6536\u96c6\u8bc1\u636e\u6765\u7cfb\u7edf\u6027\u8bc4\u4f30\u5f00\u653e\u5f0f\u56de\u7b54\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u548c\u957f\u6587\u672c\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8bc4\u4f30\u6b63\u786e\u6027\u9700\u8981\u8d85\u51fa\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5916\u90e8\u8bc1\u636e\u65f6\u96be\u4ee5\u53ef\u9760\u533a\u5206\u7ec6\u5fae\u8d28\u91cf\u5dee\u5f02\u3002", "method": "\u4f7f\u7528Group Relative Policy Optimization (GRPO)\u5728\u8d85\u8fc727K\u5408\u6210\u6210\u5bf9\u793a\u4f8b\u4e0a\u8bad\u7ec3OpenRM\uff0c\u8bad\u7ec3\u76ee\u6807\u8054\u5408\u76d1\u7763\u4e2d\u95f4\u5de5\u5177\u4f7f\u7528\u548c\u6700\u7ec8\u7ed3\u679c\u51c6\u786e\u6027\uff0c\u5b66\u4e60\u57fa\u4e8e\u8bc1\u636e\u7684\u5224\u65ad\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u65b0\u6536\u96c6\u7684\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cOpenRM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u5728\u4e0b\u6e38LLM\u5bf9\u9f50\u4efb\u52a1\u4e2d\u5e26\u6765\u4e00\u81f4\u589e\u76ca\u3002", "conclusion": "\u5de5\u5177\u589e\u5f3a\u7684\u5956\u52b1\u6a21\u578b\u5177\u6709\u6269\u5c55\u53ef\u9760\u957f\u6587\u672c\u8bc4\u4f30\u7684\u6f5c\u529b\uff0c\u53ef\u6709\u6548\u5e94\u7528\u4e8e\u63a8\u7406\u65f6\u56de\u7b54\u9009\u62e9\u548c\u8bad\u7ec3\u65f6\u6570\u636e\u9009\u62e9\u3002", "topic": "agent analysis"}}
{"id": "2510.24677", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24677", "abs": "https://arxiv.org/abs/2510.24677", "authors": ["Xun Liang", "Huayi Lai", "Hanyu Wang", "Wentao Zhang", "Linfeng Zhang", "Yanfang Chen", "Feiyu Xiong", "Zhiyu Li"], "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation", "comment": "15 pages, 9 figures", "summary": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u89d2\u8272\u626e\u6f14(PBRP)\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u533b\u5b66\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u89d2\u8272\u63d0\u793a\u4e3b\u8981\u5f71\u54cd\u8bed\u8a00\u98ce\u683c\u800c\u975e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30\u89d2\u8272\u63d0\u793a\u662f\u5426\u80fd\u8bf1\u5bfcLLMs\u4ea7\u751f\u89d2\u8272\u7279\u5b9a\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u6539\u53d8\u8bed\u8a00\u98ce\u683c\u3002", "method": "\u4f7f\u7528RP-Neuron-Activated\u8bc4\u4f30\u6846\u67b6(RPNA)\uff0c\u5728\u4e09\u4e2a\u533b\u5b66QA\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u795e\u7ecf\u5143\u6d88\u878d\u548c\u8868\u793a\u5206\u6790\u6280\u672f\u3002", "result": "\u89d2\u8272\u63d0\u793a\u4e0d\u4f1a\u663e\u8457\u589e\u5f3aLLMs\u7684\u533b\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e3b\u8981\u5f71\u54cd\u8868\u9762\u8bed\u8a00\u7279\u5f81\uff0c\u6ca1\u6709\u53d1\u73b0\u4e0d\u540c\u4e34\u5e8a\u89d2\u8272\u95f4\u7684\u63a8\u7406\u8def\u5f84\u5dee\u5f02\u3002", "conclusion": "\u5f53\u524dPBRP\u65b9\u6cd5\u65e0\u6cd5\u590d\u5236\u771f\u5b9e\u533b\u5b66\u5b9e\u8df5\u4e2d\u7684\u8ba4\u77e5\u590d\u6742\u6027\uff0c\u9700\u8981\u6a21\u62df\u771f\u6b63\u8ba4\u77e5\u8fc7\u7a0b\u800c\u975e\u8bed\u8a00\u6a21\u4eff\u7684\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2510.24694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24694", "abs": "https://arxiv.org/abs/2510.24694", "authors": ["Yida Zhao", "Kuan Li", "Xixi Wu", "Liwen Zhang", "Dingchu Zhang", "Baixuan Li", "Maojia Song", "Zhuo Chen", "Chenxi Wang", "Xinyu Wang", "Kewei Tu", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision", "comment": null, "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86E-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5b9e\u4f53\u4fe1\u606f\u6765\u6539\u8fdb\u641c\u7d22\u4ee3\u7406\u7684\u8bad\u7ec3\uff0c\u76f8\u6bd4\u4f20\u7edfGRPO\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4ece\"\u63a5\u8fd1\u6b63\u786e\"\u7684\u6837\u672c\u4e2d\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982GRPO\uff09\u4e22\u5f03\u4e86\u4e30\u5bcc\u7684\u5b9e\u4f53\u4fe1\u606f\uff0c\u4ec5\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\uff0c\u65e0\u6cd5\u533a\u5206\u5177\u6709\u6b63\u786e\u63a8\u7406\u4f46\u6700\u7ec8\u7b54\u6848\u9519\u8bef\u7684\"\u63a5\u8fd1\u6b63\u786e\"\u6837\u672c\u4e0e\u5b8c\u5168\u5931\u8d25\u7684\u6837\u672c\uff0c\u6d6a\u8d39\u4e86\u6709\u4ef7\u503c\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "method": "\u63d0\u51faEntity-aware Group Relative Policy Optimization (E-GRPO)\u6846\u67b6\uff0c\u6784\u5efa\u5bc6\u96c6\u7684\u5b9e\u4f53\u611f\u77e5\u5956\u52b1\u51fd\u6570\uff0c\u6839\u636e\u5b9e\u4f53\u5339\u914d\u7387\u4e3a\u9519\u8bef\u6837\u672c\u5206\u914d\u90e8\u5206\u5956\u52b1\uff0c\u4f7f\u6a21\u578b\u80fd\u4ece\"\u63a5\u8fd1\u6b63\u786e\"\u7684\u6837\u672c\u4e2d\u6709\u6548\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u95ee\u7b54\u548c\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cE-GRPO\u59cb\u7ec8\u663e\u8457\u4f18\u4e8eGRPO\u57fa\u7ebf\uff0c\u4e0d\u4ec5\u8fbe\u5230\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u8fd8\u8bf1\u5bfc\u51fa\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565\uff0c\u9700\u8981\u66f4\u5c11\u7684\u5de5\u5177\u8c03\u7528\u3002", "conclusion": "E-GRPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u548c\u6837\u672c\u9ad8\u6548\u7684\u641c\u7d22\u4ee3\u7406\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5b9e\u4f53\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2510.24695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24695", "abs": "https://arxiv.org/abs/2510.24695", "authors": ["Xuanzhong Chen", "Zile Qiao", "Guoxin Chen", "Liangcai Su", "Zhen Zhang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Jingren Zhou", "Yong Jiang"], "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6700\u8fd1\u53d1\u5c55\u533a\u7406\u8bba\u7684AgentFrontier\u5f15\u64ce\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u9053\u5408\u6210\u9ad8\u8d28\u91cf\u591a\u5b66\u79d1\u6570\u636e\uff0c\u8bad\u7ec3\u51fa\u5728Humanity's Last Exam\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u768430B\u53c2\u6570\u6a21\u578b\u3002", "motivation": "\u5728LLM\u80fd\u529b\u8fb9\u754c\u4e0a\u8bad\u7ec3\u667a\u80fd\u4f53\u662f\u89e3\u9501\u9ad8\u7ea7\u63a8\u7406\u7684\u5173\u952e\uff0c\u9700\u8981\u627e\u5230\u6a21\u578b\u65e0\u6cd5\u72ec\u7acb\u89e3\u51b3\u4f46\u80fd\u5728\u6307\u5bfc\u4e0b\u638c\u63e1\u7684\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u6700\u8fd1\u53d1\u5c55\u533a\u7406\u8bba\u6784\u5efaAgentFrontier\u5f15\u64ce\uff0c\u81ea\u52a8\u5316\u5408\u6210\u9ad8\u8d28\u91cf\u591a\u5b66\u79d1\u6570\u636e\uff0c\u652f\u6301\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u9488\u5bf9\u6027\u540e\u8bad\u7ec3\u3002", "result": "\u8bad\u7ec3\u7684AgentFrontier-30B-A3B\u6a21\u578b\u5728Humanity's Last Exam\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u751a\u81f3\u8d85\u8fc7\u90e8\u5206\u9886\u5148\u7684\u4e13\u6709\u667a\u80fd\u4f53\u3002", "conclusion": "ZPD\u6307\u5bfc\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.24697", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24697", "abs": "https://arxiv.org/abs/2510.24697", "authors": ["Zhengwei Tao", "Haiyang Shen", "Baixuan Li", "Wenbiao Yin", "Jialong Wu", "Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Rui Ye", "Liwen Zhang", "Xinyu Wang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking", "comment": null, "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86WebLeaper\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8986\u76d6\u7387\u7684\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u548c\u751f\u6210\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u8f68\u8ff9\uff0c\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u4fe1\u606f\u641c\u7d22\u4e2d\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4fe1\u606f\u641c\u7d22\u667a\u80fd\u4f53\u5b58\u5728\u641c\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\u3002\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u4efb\u52a1\u4e2d\u76ee\u6807\u5b9e\u4f53\u7684\u7a00\u758f\u6027\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u5b66\u4e60\u548c\u6cdb\u5316\u9ad8\u6548\u641c\u7d22\u884c\u4e3a\u7684\u673a\u4f1a\u3002", "method": "\u5c06\u4fe1\u606f\u641c\u7d22\u5efa\u6a21\u4e3a\u6811\u72b6\u7ed3\u6784\u63a8\u7406\u95ee\u9898\uff0c\u5728\u53d7\u9650\u4e0a\u4e0b\u6587\u4e2d\u5d4c\u5165\u66f4\u591a\u76ee\u6807\u5b9e\u4f53\u3002\u5229\u7528\u7ef4\u57fa\u767e\u79d1\u8868\u683c\u63d0\u51fa\u4e09\u79cd\u4efb\u52a1\u5408\u6210\u53d8\u4f53\uff1aBasic\u3001Union\u548cReverse-Union\uff0c\u7cfb\u7edf\u63d0\u5347\u641c\u7d22\u6548\u7387\u548c\u6548\u679c\u3002\u901a\u8fc7\u7b5b\u9009\u540c\u65f6\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u8f68\u8ff9\u6765\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u4e94\u4e2a\u4fe1\u606f\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\uff08BrowserComp\u3001GAIA\u3001xbench-DeepSearch\u3001WideSearch\u548cSeal-0\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u672c\u548c\u7efc\u5408\u8bbe\u7f6e\u4e0b\u90fd\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u6548\u679c\u548c\u6548\u7387\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "WebLeaper\u6846\u67b6\u901a\u8fc7\u9ad8\u8986\u76d6\u7387\u4efb\u52a1\u6784\u5efa\u548c\u9ad8\u6548\u8f68\u8ff9\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u7684\u4fe1\u606f\u641c\u7d22\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.24699", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24699", "abs": "https://arxiv.org/abs/2510.24699", "authors": ["Rui Ye", "Zhongwang Zhang", "Kuan Li", "Huifeng Yin", "Zhengwei Tao", "Yida Zhao", "Liangcai Su", "Liwen Zhang", "Zile Qiao", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Siheng Chen", "Jingren Zhou", "Yong Jiang"], "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management", "comment": "26 pages, 9 figures", "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.", "AI": {"tldr": "AgentFold\u662f\u4e00\u79cd\u65b0\u578b\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u901a\u8fc7\u4e3b\u52a8\u4e0a\u4e0b\u6587\u7ba1\u7406\u89e3\u51b3\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u9971\u548c\u95ee\u9898\uff0c\u5728\u591a\u5c3a\u5ea6\u4e0a\u7ba1\u7406\u5386\u53f2\u8f68\u8ff9\uff0c\u5728BrowseComp\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8eReAct\u7684\u667a\u80fd\u4f53\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e0a\u4e0b\u6587\u9971\u548c\u95ee\u9898\uff0c\u4ee5\u53ca\u56fa\u5b9a\u603b\u7ed3\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u5173\u952e\u7ec6\u8282\u4e22\u5931\u7684\u7f3a\u9677\u3002", "method": "\u5f15\u5165AgentFold\u8303\u5f0f\uff0c\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u52a8\u6001\u8ba4\u77e5\u5de5\u4f5c\u7a7a\u95f4\uff0c\u901a\u8fc7'\u6298\u53e0'\u64cd\u4f5c\u5728\u591a\u5c3a\u5ea6\u4e0a\u7ba1\u7406\u5386\u53f2\u8f68\u8ff9\uff0c\u5305\u62ec\u7ec6\u7c92\u5ea6\u538b\u7f29\u548c\u6df1\u5ea6\u6574\u5408\u3002", "result": "AgentFold-30B-A3B\u5728BrowseComp\u4e0a\u8fbe\u523036.2%\uff0c\u5728BrowseComp-ZH\u4e0a\u8fbe\u523047.3%\uff0c\u8d85\u8d8a\u4e86\u66f4\u5927\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u548c\u9886\u5148\u7684\u4e13\u6709\u667a\u80fd\u4f53\u3002", "conclusion": "AgentFold\u901a\u8fc7\u4e3b\u52a8\u4e0a\u4e0b\u6587\u7ba1\u7406\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u6311\u6218\uff0c\u5c55\u793a\u4e86\u7b80\u5355\u76d1\u7763\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.24701", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24701", "abs": "https://arxiv.org/abs/2510.24701", "authors": ["Tongyi DeepResearch Team", "Baixuan Li", "Bo Zhang", "Dingchu Zhang", "Fei Huang", "Guangyu Li", "Guoxin Chen", "Huifeng Yin", "Jialong Wu", "Jingren Zhou", "Kuan Li", "Liangcai Su", "Litu Ou", "Liwen Zhang", "Pengjun Xie", "Rui Ye", "Wenbiao Yin", "Xinmiao Yu", "Xinyu Wang", "Xixi Wu", "Xuanzhong Chen", "Yida Zhao", "Zhen Zhang", "Zhengwei Tao", "Zhongwang Zhang", "Zile Qiao", "Chenxi Wang", "Donglei Yu", "Gang Fu", "Haiyang Shen", "Jiayin Yang", "Jun Lin", "Junkai Zhang", "Kui Zeng", "Li Yang", "Hailong Yin", "Maojia Song", "Ming Yan", "Peng Xia", "Qian Xiao", "Rui Min", "Ruixue Ding", "Runnan Fang", "Shaowei Chen", "Shen Huang", "Shihang Wang", "Shihao Cai", "Weizhou Shen", "Xiaobin Wang", "Xin Guan", "Xinyu Geng", "Yingcheng Shi", "Yuning Wu", "Zhuo Chen", "Zijian Li", "Yong Jiang"], "title": "Tongyi DeepResearch Technical Report", "comment": "https://tongyi-agent.github.io/blog", "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.", "AI": {"tldr": "Tongyi DeepResearch\u662f\u4e00\u4e2a\u4e13\u4e3a\u957f\u5468\u671f\u6df1\u5ea6\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u4efb\u52a1\u8bbe\u8ba1\u7684\u667a\u80fd\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u63a8\u7406\u548c\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u957f\u5468\u671f\u3001\u6df1\u5ea6\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u4efb\u52a1\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u8fdb\u884c\u6df1\u5ea6\u7814\u7a76\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4ee3\u7406\u4e2d\u95f4\u8bad\u7ec3\u548c\u4ee3\u7406\u540e\u8bad\u7ec3\uff0c\u8bbe\u8ba1\u4e86\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u9636\u6bb5\u6784\u5efa\u5b9a\u5236\u5316\u73af\u5883\u3002", "result": "Tongyi DeepResearch\u62e5\u6709305\u4ebf\u603b\u53c2\u6570\uff0c\u6bcftoken\u4ec5\u6fc0\u6d3b33\u4ebf\u53c2\u6570\uff0c\u5728\u591a\u4e2a\u4ee3\u7406\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4ee3\u7406\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f5c\u8005\u5f00\u6e90\u4e86\u6a21\u578b\u3001\u6846\u67b6\u548c\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u4ee5\u8d4b\u80fd\u793e\u533a\u3002", "topic": "agent analysis"}}
{"id": "2510.24702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24702", "abs": "https://arxiv.org/abs/2510.24702", "authors": ["Yueqi Song", "Ketan Ramaneti", "Zaid Sheikh", "Ziru Chen", "Boyu Gou", "Tianbao Xie", "Yiheng Xu", "Danyang Zhang", "Apurva Gandhi", "Fan Yang", "Joseph Liu", "Tianyue Ou", "Zhihao Yuan", "Frank Xu", "Shuyan Zhou", "Xingyao Wang", "Xiang Yue", "Tao Yu", "Huan Sun", "Yu Su", "Graham Neubig"], "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents", "comment": null, "summary": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.", "AI": {"tldr": "\u63d0\u51fa\u4e86agent\u6570\u636e\u534f\u8bae(ADP)\uff0c\u4f5c\u4e3a\u5f02\u6784\u4ee3\u7406\u6570\u636e\u96c6\u4e0e\u7edf\u4e00\u8bad\u7ec3\u7ba1\u9053\u4e4b\u95f4\u7684\u4e2d\u95f4\u8868\u793a\u8bed\u8a00\uff0c\u901a\u8fc7\u6807\u51c6\u531613\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u83b7\u5f97\u7ea620%\u6027\u80fd\u63d0\u5347", "motivation": "\u5927\u89c4\u6a21\u4ee3\u7406\u76d1\u7763\u5fae\u8c03\u7684\u7814\u7a76\u6210\u679c\u8f83\u5c11\uff0c\u4e3b\u8981\u56e0\u4e3a\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u9762\u4e34\u5f02\u6784\u683c\u5f0f\u3001\u5de5\u5177\u548c\u63a5\u53e3\u7684\u788e\u7247\u5316\u6311\u6218", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7ADP\u8868\u793a\u8bed\u8a00\u4f5c\u4e3a\u4e2d\u95f4\u8bed\u8a00\uff0c\u7edf\u4e0013\u4e2a\u73b0\u6709\u4ee3\u7406\u6570\u636e\u96c6\u683c\u5f0f\uff0c\u8f6c\u6362\u4e3a\u591a\u4e2a\u4ee3\u7406\u6846\u67b6\u7684\u8bad\u7ec3\u5c31\u7eea\u683c\u5f0f\u8fdb\u884cSFT", "result": "\u5728\u6807\u51c6\u5316ADP\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u5347\u7ea620%\uff0c\u5728\u7f16\u7801\u3001\u6d4f\u89c8\u3001\u5de5\u5177\u4f7f\u7528\u548c\u7814\u7a76\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6216\u63a5\u8fd1SOTA\u6c34\u5e73", "conclusion": "ADP\u6709\u52a9\u4e8e\u964d\u4f4e\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55\u548c\u53ef\u590d\u73b0\u4ee3\u7406\u8bad\u7ec3\u7684\u95e8\u69db\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00", "topic": "agent analysis"}}
{"id": "2510.24235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24235", "abs": "https://arxiv.org/abs/2510.24235", "authors": ["Ai Jian", "Jingqing Ruan", "Xing Ma", "Dailin Li", "QianLin Zhou", "Ke Zeng", "Xunliang Cai"], "title": "PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling", "comment": null, "summary": "Reward models (RMs) are central to reinforcement learning from human feedback\n(RLHF), providing the critical supervision signals that align large language\nmodels (LLMs) with human preferences. While generative reward models (GRMs)\noffer greater interpretability than traditional scalar RMs, current training\nparadigms remain limited. Pair-wise methods rely on binary good-versus-bad\nlabels, which cause mismatches for point-wise inference and necessitate complex\npairing strategies for effective application in RLHF. On the other hand,\npoint-wise methods require more elaborate absolute labeling with rubric-driven\ncriteria, resulting in poor adaptability and high annotation costs. In this\nwork, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a\nunified framework that integrates a preference-aware reward (PAR) mechanism\nwith dynamic rubric adaptation. PaTaRM leverages relative preference\ninformation from pairwise data to construct robust point-wise training signals,\neliminating the need for explicit point-wise labels. Simultaneously, it employs\na task-adaptive rubric system that flexibly generates evaluation criteria for\nboth global task consistency and instance-specific fine-grained reasoning. This\ndesign enables efficient, generalizable, and interpretable reward modeling for\nRLHF. Extensive experiments show that PaTaRM achieves an average relative\nimprovement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B\nmodels. Furthermore, PaTaRM boosts downstream RLHF performance, with an average\nimprovement of 13.6% across IFEval and InFoBench benchmarks, confirming its\neffectiveness and robustness. Our code is available at\nhttps://github.com/JaneEyre0530/PaTaRM.", "AI": {"tldr": "\u63d0\u51fa\u4e86PaTaRM\u6846\u67b6\uff0c\u5c06\u504f\u597d\u611f\u77e5\u5956\u52b1\u673a\u5236\u4e0e\u52a8\u6001\u6807\u51c6\u9002\u5e94\u76f8\u7ed3\u5408\uff0c\u65e0\u9700\u663e\u5f0f\u9010\u70b9\u6807\u6ce8\u5373\u53ef\u6784\u5efa\u7a33\u5065\u7684\u9010\u70b9\u8bad\u7ec3\u4fe1\u53f7\uff0c\u5728RLHF\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff1a\u6210\u5bf9\u65b9\u6cd5\u4f9d\u8d56\u4e8c\u5143\u6807\u7b7e\u5bfc\u81f4\u63a8\u7406\u4e0d\u5339\u914d\uff0c\u9010\u70b9\u65b9\u6cd5\u9700\u8981\u590d\u6742\u6807\u6ce8\u4e14\u9002\u5e94\u6027\u5dee\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "PaTaRM\u6574\u5408\u504f\u597d\u611f\u77e5\u5956\u52b1\u673a\u5236\u548c\u52a8\u6001\u6807\u51c6\u9002\u5e94\uff0c\u5229\u7528\u6210\u5bf9\u6570\u636e\u7684\u76f8\u5bf9\u504f\u597d\u4fe1\u606f\u6784\u5efa\u9010\u70b9\u8bad\u7ec3\u4fe1\u53f7\uff0c\u540c\u65f6\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u6807\u51c6\u7cfb\u7edf\u7075\u6d3b\u751f\u6210\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5728RewardBench\u548cRMBench\u4e0a\u5e73\u5747\u76f8\u5bf9\u63d0\u53474.7%\uff0c\u5728IFEval\u548cInFoBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0b\u6e38RLHF\u6027\u80fd\u5e73\u5747\u63d0\u534713.6%\u3002", "conclusion": "PaTaRM\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3aRLHF\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.24356", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24356", "abs": "https://arxiv.org/abs/2510.24356", "authors": ["Suman Sanyal"], "title": "Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning", "comment": null, "summary": "We introduce Perception Learning (PeL), a paradigm that optimizes an agent's\nsensory interface $f_\\phi:\\mathcal{X}\\to\\mathcal{Z}$ using task-agnostic\nsignals, decoupled from downstream decision learning\n$g_\\theta:\\mathcal{Z}\\to\\mathcal{Y}$. PeL directly targets label-free\nperceptual properties, such as stability to nuisances, informativeness without\ncollapse, and controlled geometry, assessed via objective\nrepresentation-invariant metrics. We formalize the separation of perception and\ndecision, define perceptual properties independent of objectives or\nreparameterizations, and prove that PeL updates preserving sufficient\ninvariants are orthogonal to Bayes task-risk gradients. Additionally, we\nprovide a suite of task-agnostic evaluation metrics to certify perceptual\nquality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u611f\u77e5\u5b66\u4e60(PeL)\u8303\u5f0f\uff0c\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u4fe1\u53f7\u4f18\u5316\u667a\u80fd\u4f53\u7684\u611f\u5b98\u63a5\u53e3\uff0c\u4e0e\u4e0b\u6e38\u51b3\u7b56\u5b66\u4e60\u89e3\u8026\uff0c\u76f4\u63a5\u9488\u5bf9\u65e0\u6807\u7b7e\u7684\u611f\u77e5\u5c5e\u6027\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u611f\u77e5\u548c\u51b3\u7b56\u8026\u5408\u5728\u4e00\u8d77\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u611f\u77e5\u6a21\u5757\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002PeL\u65e8\u5728\u5206\u79bb\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\uff0c\u4f7f\u611f\u77e5\u5b66\u4e60\u80fd\u591f\u4e13\u6ce8\u4e8e\u611f\u77e5\u8d28\u91cf\u672c\u8eab\u3002", "method": "\u5b9a\u4e49\u611f\u77e5\u5c5e\u6027\uff08\u5982\u5bf9\u6270\u52a8\u7684\u7a33\u5b9a\u6027\u3001\u4fe1\u606f\u6027\u3001\u51e0\u4f55\u63a7\u5236\uff09\uff0c\u4f7f\u7528\u8868\u793a\u4e0d\u53d8\u6027\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u8bc1\u660ePeL\u66f4\u65b0\u4e0e\u8d1d\u53f6\u65af\u4efb\u52a1\u98ce\u9669\u68af\u5ea6\u6b63\u4ea4\u3002", "result": "\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u7684\u611f\u77e5-\u51b3\u7b56\u5206\u79bb\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u72ec\u7acb\u4e8e\u76ee\u6807\u4efb\u52a1\u548c\u53c2\u6570\u5316\u7684\u611f\u77e5\u5c5e\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4efb\u52a1\u65e0\u5173\u7684\u8bc4\u4f30\u6307\u6807\u6765\u9a8c\u8bc1\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "PeL\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u8bba\uff0c\u4f7f\u611f\u77e5\u5b66\u4e60\u80fd\u591f\u72ec\u7acb\u4e8e\u5177\u4f53\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u3002", "topic": "agent analysis"}}
{"id": "2510.24432", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24432", "abs": "https://arxiv.org/abs/2510.24432", "authors": ["Seyed Mahdi Basiri Azad", "Joschka Boedecker"], "title": "Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings", "comment": null, "summary": "Reinforcement learning (RL) in sparse-reward environments remains a\nsignificant challenge due to the lack of informative feedback. We propose a\nsimple yet effective method that uses a small number of successful\ndemonstrations to initialize the value function of an RL agent. By precomputing\nvalue estimates from offline demonstrations and using them as targets for early\nlearning, our approach provides the agent with a useful prior over promising\nactions. The agent then refines these estimates through standard online\ninteraction. This hybrid offline-to-online paradigm significantly reduces the\nexploration burden and improves sample efficiency in sparse-reward settings.\nExperiments on benchmark tasks demonstrate that our method accelerates\nconvergence and outperforms standard baselines, even with minimal or suboptimal\ndemonstration data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5c11\u91cf\u6210\u529f\u6f14\u793a\u521d\u59cb\u5316\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u4ef7\u503c\u51fd\u6570\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u9884\u8ba1\u7b97\u79bb\u7ebf\u6f14\u793a\u7684\u4ef7\u503c\u4f30\u8ba1\u4f5c\u4e3a\u65e9\u671f\u5b66\u4e60\u76ee\u6807\uff0c\u663e\u8457\u51cf\u5c11\u63a2\u7d22\u8d1f\u62c5\u5e76\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "motivation": "\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u7531\u4e8e\u7f3a\u4e4f\u4fe1\u606f\u53cd\u9988\u800c\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u79bb\u7ebf\u5230\u5728\u7ebf\u8303\u5f0f\uff1a\u9996\u5148\u4ece\u5c11\u91cf\u79bb\u7ebf\u6f14\u793a\u4e2d\u9884\u8ba1\u7b97\u4ef7\u503c\u4f30\u8ba1\uff0c\u4f5c\u4e3a\u65e9\u671f\u5b66\u4e60\u76ee\u6807\uff1b\u7136\u540e\u901a\u8fc7\u6807\u51c6\u5728\u7ebf\u4ea4\u4e92\u8fdb\u884c\u7cbe\u70bc\u3002", "result": "\u5728\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u52a0\u901f\u4e86\u6536\u655b\u8fc7\u7a0b\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u5c11\u6216\u6b21\u4f18\u7684\u6f14\u793a\u6570\u636e\uff0c\u4e5f\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u79bb\u7ebf\u6f14\u793a\u7684\u4ef7\u503c\u4f30\u8ba1\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.24482", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24482", "abs": "https://arxiv.org/abs/2510.24482", "authors": ["Klemens Iten", "Lenart Treven", "Bhavya Sukhija", "Florian D\u00f6rfler", "Andreas Krause"], "title": "Sample-efficient and Scalable Exploration in Continuous-Time RL", "comment": "26 pages, 6 figures, 6 tables", "summary": "Reinforcement learning algorithms are typically designed for discrete-time\ndynamics, even though the underlying real-world control systems are often\ncontinuous in time. In this paper, we study the problem of continuous-time\nreinforcement learning, where the unknown system dynamics are represented using\nnonlinear ordinary differential equations (ODEs). We leverage probabilistic\nmodels, such as Gaussian processes and Bayesian neural networks, to learn an\nuncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily\nmaximizes a weighted sum of the extrinsic reward and model epistemic\nuncertainty. This yields a scalable and sample-efficient approach to\ncontinuous-time model-based RL. We show that COMBRL achieves sublinear regret\nin the reward-driven setting, and in the unsupervised RL setting (i.e., without\nextrinsic rewards), we provide a sample complexity bound. In our experiments,\nwe evaluate COMBRL in both standard and unsupervised RL settings and\ndemonstrate that it scales better, is more sample-efficient than prior methods,\nand outperforms baselines across several deep RL tasks.", "AI": {"tldr": "\u63d0\u51faCOMBRL\u7b97\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6982\u7387\u6a21\u578b\u5b66\u4e60\u975e\u7ebf\u6027ODE\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u5916\u90e8\u5956\u52b1\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u8d2a\u5a6a\u4f18\u5316\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u6837\u672c\u9ad8\u6548\u7684\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u63a7\u5236\u7cfb\u7edf\u901a\u5e38\u662f\u8fde\u7eed\u65f6\u95f4\u7684\uff0c\u4f46\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u9488\u5bf9\u79bb\u6563\u65f6\u95f4\u52a8\u6001\u8bbe\u8ba1\uff0c\u9700\u8981\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u7b49\u6982\u7387\u6a21\u578b\u5b66\u4e60\u975e\u7ebf\u6027ODE\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u63d0\u51faCOMBRL\u7b97\u6cd5\u8d2a\u5a6a\u6700\u5927\u5316\u5916\u90e8\u5956\u52b1\u548c\u6a21\u578b\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u52a0\u6743\u548c\u3002", "result": "COMBRL\u5728\u5956\u52b1\u9a71\u52a8\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u5728\u65e0\u76d1\u7763RL\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "conclusion": "COMBRL\u4e3a\u8fde\u7eed\u65f6\u95f4\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6df1\u5ea6RL\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.24700", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24700", "abs": "https://arxiv.org/abs/2510.24700", "authors": ["Di Wu", "Chengshuai Shi", "Jing Yang", "Cong Shen"], "title": "Greedy Sampling Is Provably Efficient for RLHF", "comment": "NeurIPS 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a key\ntechnique for post-training large language models. Despite its empirical\nsuccess, the theoretical understanding of RLHF is still limited, as learning\nthe KL-regularized target with only preference feedback poses additional\nchallenges compared with canonical RL. Existing works mostly study the\nreward-based Bradley-Terry (BT) preference model, and extend classical designs\nutilizing optimism or pessimism. This work, instead, considers the general\npreference model (whose practical relevance has been observed recently) and\nobtains performance guarantees with major, order-wise improvements over\nexisting ones. Surprisingly, these results are derived from algorithms that\ndirectly use the empirical estimates (i.e., greedy sampling), as opposed to\nconstructing optimistic or pessimistic estimates in previous works. This\ninsight has a deep root in the unique structural property of the optimal policy\nclass under the KL-regularized target, and we further specialize it to the BT\nmodel, highlighting the surprising sufficiency of greedy sampling in RLHF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u901a\u7528\u504f\u597d\u6a21\u578b\u4e0bRLHF\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8d2a\u5a6a\u91c7\u6837\u800c\u975e\u4e50\u89c2/\u60b2\u89c2\u4f30\u8ba1\u83b7\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u63ed\u793a\u4e86KL\u6b63\u5219\u5316\u76ee\u6807\u4e0b\u6700\u4f18\u7b56\u7565\u7c7b\u7684\u72ec\u7279\u7ed3\u6784\u7279\u6027\u3002", "motivation": "RLHF\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u7406\u8bba\u7406\u89e3\u4ecd\u6709\u9650\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u5956\u52b1\u7684Bradley-Terry\u504f\u597d\u6a21\u578b\uff0c\u800c\u672c\u6587\u65e8\u5728\u5206\u6790\u66f4\u901a\u7528\u7684\u504f\u597d\u6a21\u578b\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528\u76f4\u63a5\u4f7f\u7528\u7ecf\u9a8c\u4f30\u8ba1\uff08\u8d2a\u5a6a\u91c7\u6837\uff09\u7684\u65b9\u6cd5\uff0c\u800c\u975e\u6784\u5efa\u4e50\u89c2\u6216\u60b2\u89c2\u4f30\u8ba1\u3002\u5206\u6790\u4e86KL\u6b63\u5219\u5316\u76ee\u6807\u4e0b\u6700\u4f18\u7b56\u7565\u7c7b\u7684\u7ed3\u6784\u7279\u6027\uff0c\u5e76\u5c06\u5176\u4e13\u95e8\u5316\u5230BT\u6a21\u578b\u3002", "result": "\u5728\u901a\u7528\u504f\u597d\u6a21\u578b\u4e0b\u83b7\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u6539\u8fdb\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u8bc1\u660e\u4e86\u8d2a\u5a6a\u91c7\u6837\u5728RLHF\u4e2d\u7684\u5145\u5206\u6027\u3002", "conclusion": "\u8d2a\u5a6a\u91c7\u6837\u5728RLHF\u4e2d\u662f\u5145\u5206\u7684\uff0c\u8fd9\u6e90\u4e8eKL\u6b63\u5219\u5316\u76ee\u6807\u4e0b\u6700\u4f18\u7b56\u7565\u7c7b\u7684\u72ec\u7279\u7ed3\u6784\u7279\u6027\uff0c\u4e3aRLHF\u7684\u7406\u8bba\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.087d8323", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fhow-cloudflares-client-side-security-made-the-npm-supply-chain-attack-a-non%2F%3Futm_source=tldrinfosec/1/0100019a25c8af65-d9fa5641-2669-408a-8358-61b883501c54-000000/FLNGez16bpnUtQZKoKYv4NrI1mf9A52s4eYbBrBaLOY=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fhow-cloudflares-client-side-security-made-the-npm-supply-chain-attack-a-non%2F%3Futm_source=tldrinfosec/1/0100019a25c8af65-d9fa5641-2669-408a-8358-61b883501c54-000000/FLNGez16bpnUtQZKoKYv4NrI1mf9A52s4eYbBrBaLOY=428", "authors": ["TLDR Newsletter"], "title": "How Cloudflare's client-side security made the npm supply chain attack a non-event", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fhow-cloudflares-client-side-security-made-the-npm-supply-chain-attack-a-non%2F%3Futm_source=tldrinfosec/1/0100019a25c8af65-d9fa5641-2669-408a-8358-61b883501c54-000000/FLNGez16bpnUtQZKoKYv4NrI1mf9A52s4eYbBrBaLOY=428", "summary": "How Cloudflare's client-side security made the npm supply chain attack a non-event (4 minute read) Attackers used phishing to compromise npm maintainer accounts in September, injecting malicious code into 18 popular packages with over 2 billion weekly downloads to steal cryptocurrency and credentials from CI/CD pipelines. Cloudflare's Page Shield, using ML-based analysis of 3.5 billion scripts daily, detected all compromised packages with 98% precision and 90% recall by identifying obfuscated...", "source": "tldr", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "tldr.2510.bd786e78", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fblog%2Fintroducing-pytorch-native-agentic-stack%2F%3Futm_source=tldrai/1/0100019a25d2873b-230fc112-8543-482e-838d-f6d461d242fc-000000/gBrV2yBzRuP1hOE2cQkMT57PbHhSyLZtvcJC0lVEqAk=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fblog%2Fintroducing-pytorch-native-agentic-stack%2F%3Futm_source=tldrai/1/0100019a25d2873b-230fc112-8543-482e-838d-f6d461d242fc-000000/gBrV2yBzRuP1hOE2cQkMT57PbHhSyLZtvcJC0lVEqAk=428", "authors": ["TLDR Newsletter"], "title": "Building Blocks of Agentic AI from Meta", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fblog%2Fintroducing-pytorch-native-agentic-stack%2F%3Futm_source=tldrai/1/0100019a25d2873b-230fc112-8543-482e-838d-f6d461d242fc-000000/gBrV2yBzRuP1hOE2cQkMT57PbHhSyLZtvcJC0lVEqAk=428", "summary": "Building Blocks of Agentic AI from Meta (14 minute read) A PyTorch-native stack of open-source tools for agentic AI, including kernel development, distributed systems, and reinforcement learning, with support for massive GPU workloads and edge deployments.", "source": "tldr", "AI": {"tldr": "Meta\u53d1\u5e03\u4e86\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5f00\u6e90\u5de5\u5177\u6808\uff0c\u7528\u4e8e\u6784\u5efa\u667a\u80fd\u4ee3\u7406AI\uff0c\u652f\u6301\u5185\u6838\u5f00\u53d1\u3001\u5206\u5e03\u5f0f\u7cfb\u7edf\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21GPU\u5de5\u4f5c\u8d1f\u8f7d\u548c\u8fb9\u7f18\u90e8\u7f72\u3002", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e00\u5957\u5b8c\u6574\u7684\u3001\u57fa\u4e8ePyTorch\u7684\u667a\u80fd\u4ee3\u7406AI\u5f00\u53d1\u5de5\u5177\uff0c\u89e3\u51b3\u5927\u89c4\u6a21GPU\u5de5\u4f5c\u8d1f\u8f7d\u548c\u8fb9\u7f18\u90e8\u7f72\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aPyTorch\u539f\u751f\u7684\u5f00\u6e90\u5de5\u5177\u6808\uff0c\u5305\u542b\u5185\u6838\u5f00\u53d1\u3001\u5206\u5e03\u5f0f\u7cfb\u7edf\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u7ec4\u4ef6\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u5927\u89c4\u6a21GPU\u5de5\u4f5c\u8d1f\u8f7d\u548c\u8fb9\u7f18\u90e8\u7f72\u7684\u667a\u80fd\u4ee3\u7406AI\u5f00\u53d1\u5e73\u53f0\u3002", "conclusion": "\u8be5\u5de5\u5177\u6808\u4e3a\u667a\u80fd\u4ee3\u7406AI\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.8b1f242d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrai/1/0100019a25d2873b-230fc112-8543-482e-838d-f6d461d242fc-000000/7UawMihjpyfm8XXi8WA_G8K1m3IcfWxfnzMua1IMh8s=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrai/1/0100019a25d2873b-230fc112-8543-482e-838d-f6d461d242fc-000000/7UawMihjpyfm8XXi8WA_G8K1m3IcfWxfnzMua1IMh8s=428", "authors": ["TLDR Newsletter"], "title": "Code like a surgeon", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrai/1/0100019a25d2873b-230fc112-8543-482e-838d-f6d461d242fc-000000/7UawMihjpyfm8XXi8WA_G8K1m3IcfWxfnzMua1IMh8s=428", "summary": "Code like a surgeon (2 minute read) The goal isn't delegating your core work and then becoming an auditor or editor. It should be offloading secondary tasks so you can focus on what really matters. Tasks like writing documentation, fixing TypeScript errors, and spiking out changes are the kind of grunt work AI agents excel at that can run async in the background.", "source": "tldr", "AI": {"tldr": "AI agents should\u88ab\u7528\u4e8e\u5904\u7406\u6b21\u8981\u4efb\u52a1\uff0c\u5982\u7f16\u5199\u6587\u6863\u3001\u4fee\u590dTypeScript\u9519\u8bef\u548c\u5feb\u901f\u5c1d\u8bd5\u53d8\u66f4\uff0c\u4ee5\u4fbf\u5f00\u53d1\u8005\u80fd\u591f\u4e13\u6ce8\u4e8e\u6838\u5fc3\u5de5\u4f5c\u3002", "motivation": "\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u901a\u8fc7\u5c06\u7e41\u7410\u7684\u6b21\u8981\u4efb\u52a1\u59d4\u6258\u7ed9AI\u4ee3\u7406\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u96c6\u4e2d\u7cbe\u529b\u5728\u771f\u6b63\u91cd\u8981\u7684\u6838\u5fc3\u5de5\u4f5c\u4e0a\u3002", "method": "\u5229\u7528AI\u4ee3\u7406\u5f02\u6b65\u5904\u7406\u540e\u53f0\u4efb\u52a1\uff0c\u5982\u6587\u6863\u7f16\u5199\u3001\u9519\u8bef\u4fee\u590d\u548c\u53d8\u66f4\u5c1d\u8bd5\u3002", "result": "\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u4e13\u6ce8\u4e8e\u6838\u5fc3\u5f00\u53d1\u4efb\u52a1\uff0c\u63d0\u9ad8\u6574\u4f53\u5de5\u4f5c\u6548\u7387\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e94\u88ab\u7528\u4e8e\u5904\u7406\u6b21\u8981\u4efb\u52a1\uff0c\u4ee5\u4f18\u5316\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2510.3485ccc4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarma.dev%2Farticles%2F2025%2Fwhen-perfect-code-fails%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/PRd0SQPz2rHmdpECNzZoWM8DIMHCqZgzY-2lHy1Y84E=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarma.dev%2Farticles%2F2025%2Fwhen-perfect-code-fails%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/PRd0SQPz2rHmdpECNzZoWM8DIMHCqZgzY-2lHy1Y84E=429", "authors": ["TLDR Newsletter"], "title": "When 'perfect' code fails", "comment": "Source: TLDR Newsletter, Date: 2025-10-28, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarma.dev%2Farticles%2F2025%2Fwhen-perfect-code-fails%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/PRd0SQPz2rHmdpECNzZoWM8DIMHCqZgzY-2lHy1Y84E=429", "summary": "When 'perfect' code fails (6 minute read) A critical security bug found in this dev's Next.js application was caused by a seemingly perfect one-line equality check. Due to Next.js's server function implementation, the synchronous function was unexpectedly converted to an asynchronous function returning a Promise. In JavaScript, a Promise evaluates to true in an `if` statement, effectively bypassing the security check and granting access to everyone.", "source": "tldr", "AI": {"tldr": "Next.js\u5e94\u7528\u4e2d\u4e00\u4e2a\u770b\u4f3c\u5b8c\u7f8e\u7684\u5355\u884c\u76f8\u7b49\u6027\u68c0\u67e5\u5bfc\u81f4\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u539f\u56e0\u662f\u670d\u52a1\u5668\u51fd\u6570\u5b9e\u73b0\u5c06\u540c\u6b65\u51fd\u6570\u610f\u5916\u8f6c\u6362\u4e3a\u5f02\u6b65\u51fd\u6570\uff0c\u8fd4\u56de\u7684Promise\u5728if\u8bed\u53e5\u4e2d\u88ab\u8bc4\u4f30\u4e3atrue\uff0c\u4ece\u800c\u7ed5\u8fc7\u5b89\u5168\u68c0\u67e5\u3002", "motivation": "\u63ed\u793aNext.js\u6846\u67b6\u4e2d\u670d\u52a1\u5668\u51fd\u6570\u5b9e\u73b0\u53ef\u80fd\u5bfc\u81f4\u7684\u610f\u5916\u884c\u4e3a\uff0c\u7279\u522b\u662f\u540c\u6b65\u51fd\u6570\u88ab\u8f6c\u6362\u4e3a\u5f02\u6b65\u51fd\u6570\u65f6\u5f15\u53d1\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b9e\u9645\u6848\u4f8b\uff0c\u7814\u7a76Next.js\u670d\u52a1\u5668\u51fd\u6570\u5982\u4f55\u5c06\u540c\u6b65\u51fd\u6570\u8f6c\u6362\u4e3a\u8fd4\u56dePromise\u7684\u5f02\u6b65\u51fd\u6570\uff0c\u4ee5\u53caJavaScript\u4e2dPromise\u5728\u5e03\u5c14\u4e0a\u4e0b\u6587\u4e2d\u7684\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u540c\u6b65\u51fd\u6570\u5728Next.js\u670d\u52a1\u5668\u73af\u5883\u4e2d\u88ab\u610f\u5916\u8f6c\u6362\u4e3a\u5f02\u6b65\u51fd\u6570\uff0c\u5bfc\u81f4\u8fd4\u56de\u7684Promise\u5728if\u8bed\u53e5\u4e2d\u59cb\u7ec8\u8bc4\u4f30\u4e3atrue\uff0c\u4ece\u800c\u5b8c\u5168\u7ed5\u8fc7\u5b89\u5168\u68c0\u67e5\u673a\u5236\u3002", "conclusion": "\u5f00\u53d1\u4eba\u5458\u9700\u8981\u8b66\u60d5\u6846\u67b6\u7684\u9690\u5f0f\u884c\u4e3a\u8f6c\u6362\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u4ee3\u7801\u4e2d\uff0c\u5e94\u660e\u786e\u5904\u7406\u5f02\u6b65\u64cd\u4f5c\u548cPromise\u8fd4\u56de\u503c\u3002", "topic": "swe application"}}
{"id": "tldr.2510.627a5618", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.joemag.dev%2F2025%2F10%2Fthe-new-calculus-of-ai-based-coding.html%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/Ir7bGfo3VfXoEeHooqaSU_mXXkZN6ua8EirqIrRiWFo=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.joemag.dev%2F2025%2F10%2Fthe-new-calculus-of-ai-based-coding.html%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/Ir7bGfo3VfXoEeHooqaSU_mXXkZN6ua8EirqIrRiWFo=429", "authors": ["TLDR Newsletter"], "title": "The New Calculus of AI-based Coding", "comment": "Source: TLDR Newsletter, Date: 2025-10-28, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.joemag.dev%2F2025%2F10%2Fthe-new-calculus-of-ai-based-coding.html%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/Ir7bGfo3VfXoEeHooqaSU_mXXkZN6ua8EirqIrRiWFo=429", "summary": "The New Calculus of AI-based Coding (12 minute read) This team used AI agents to write code, resulting in a 10x increase in coding throughput. This agentic coding requires a shift in traditional software development practices, especially in testing, deployment, and team coordination, to avoid increased bugs and bottlenecks. AI agents can also be used to reduce the costs of implementing testing approaches and improving the infrastructure needed to sustain high velocity.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528AI\u4ee3\u7406\u7f16\u5199\u4ee3\u7801\u53ef\u5b9e\u73b010\u500d\u7f16\u7801\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4f46\u9700\u8981\u6539\u53d8\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u4ee5\u907f\u514d\u9519\u8bef\u548c\u74f6\u9888\u589e\u52a0", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u5728\u7f16\u7801\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u540c\u65f6\u5e94\u5bf9\u7531\u6b64\u5e26\u6765\u7684\u6d4b\u8bd5\u3001\u90e8\u7f72\u548c\u56e2\u961f\u534f\u4f5c\u6311\u6218", "method": "\u4f7f\u7528AI\u4ee3\u7406\u8fdb\u884c\u4ee3\u7801\u7f16\u5199\uff0c\u8c03\u6574\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u6d4b\u8bd5\u3001\u90e8\u7f72\u548c\u56e2\u961f\u534f\u8c03\u65b9\u6cd5", "result": "\u7f16\u7801\u541e\u5410\u91cf\u63d0\u534710\u500d\uff0c\u4f46\u9700\u8981\u76f8\u5e94\u6539\u8fdb\u6d4b\u8bd5\u65b9\u6cd5\u548c\u57fa\u7840\u8bbe\u65bd\u6765\u7ef4\u6301\u9ad8\u901f\u5ea6", "conclusion": "AI\u4ee3\u7406\u7f16\u7801\u9700\u8981\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u7684\u8f6c\u53d8\uff0c\u7279\u522b\u662f\u6d4b\u8bd5\u3001\u90e8\u7f72\u548c\u56e2\u961f\u534f\u8c03\u65b9\u9762\uff0c\u4ee5\u5145\u5206\u5229\u7528\u5176\u6548\u7387\u4f18\u52bf", "topic": "code agent"}}
{"id": "tldr.2510.4dbb790b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbytesauna.com%2Fpost%2Fcoding-vs-software-engineering%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/JJk3WDaK_K58ragKiwjj6xYfTo4tnPl4B9YO9yqSZ-I=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbytesauna.com%2Fpost%2Fcoding-vs-software-engineering%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/JJk3WDaK_K58ragKiwjj6xYfTo4tnPl4B9YO9yqSZ-I=429", "authors": ["TLDR Newsletter"], "title": "AI can code, but it can't build software", "comment": "Source: TLDR Newsletter, Date: 2025-10-28, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbytesauna.com%2Fpost%2Fcoding-vs-software-engineering%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/JJk3WDaK_K58ragKiwjj6xYfTo4tnPl4B9YO9yqSZ-I=429", "summary": "AI can code, but it can't build software (3 minute read) An increase in requests for technical cofounders suggests a gap in what AI can currently accomplish. While AI is great at coding specific, isolated tasks, it struggles with the complexities of building and maintaining production-ready software. Software engineering is different from mere coding, as it involves integration, expansion, and long-term maintainability, which AI currently doesn't have.", "source": "tldr", "AI": {"tldr": "AI can code but cannot build production-ready software due to limitations in integration, expansion, and long-term maintainability.", "motivation": "To highlight the gap between AI's coding capabilities and the complexities of software engineering, as evidenced by increased demand for technical cofounders.", "method": "Analysis of AI's current limitations in software development based on industry trends and technical requirements.", "result": "AI excels at isolated coding tasks but fails in building and maintaining complex software systems.", "conclusion": "There is a significant gap between AI's coding abilities and the comprehensive skills required for software engineering.", "topic": "agent analysis"}}
{"id": "tldr.2510.4614e118", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.expo.dev%2Feas%2Fai%2Fmcp%2F%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/DYalQLFvlav7p9zHw0gC_BK-O13FmjoiT3ddElwhEKQ=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.expo.dev%2Feas%2Fai%2Fmcp%2F%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/DYalQLFvlav7p9zHw0gC_BK-O13FmjoiT3ddElwhEKQ=429", "authors": ["TLDR Newsletter"], "title": "Using Model Context Protocol with Expo", "comment": "Source: TLDR Newsletter, Date: 2025-10-28, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.expo.dev%2Feas%2Fai%2Fmcp%2F%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/DYalQLFvlav7p9zHw0gC_BK-O13FmjoiT3ddElwhEKQ=429", "summary": "Using Model Context Protocol (MCP) with Expo (7 minute read) The Expo MCP Server connects AI coding agents to your Expo projects so they can access docs, automate workflows, and verify your app visually. It integrates with tools like Claude Code, Cursor, and VS Code, teaching them about the Expo SDK and enabling interaction with mobile simulators and React Native DevTools. Some local capabilities include taking screenshots, opening DevTools, and automation features like tapping views and find...", "source": "tldr", "AI": {"tldr": "Expo MCP Server \u5c06AI\u7f16\u7a0b\u4ee3\u7406\u8fde\u63a5\u5230Expo\u9879\u76ee\uff0c\u4f7f\u5176\u80fd\u591f\u8bbf\u95ee\u6587\u6863\u3001\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u5e76\u89c6\u89c9\u9a8c\u8bc1\u5e94\u7528\uff0c\u652f\u6301\u4e0eClaude Code\u3001Cursor\u548cVS Code\u7b49\u5de5\u5177\u96c6\u6210\u3002", "motivation": "\u4e3aAI\u7f16\u7a0b\u4ee3\u7406\u63d0\u4f9b\u4e0eExpo\u9879\u76ee\u7684\u6df1\u5ea6\u96c6\u6210\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u64cd\u4f5c\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u73af\u5883\uff0c\u63d0\u5347\u81ea\u52a8\u5316\u5f00\u53d1\u6548\u7387\u3002", "method": "\u4f7f\u7528Model Context Protocol (MCP)\u6784\u5efaExpo MCP Server\uff0c\u901a\u8fc7\u672c\u5730\u80fd\u529b\u5982\u622a\u56fe\u3001\u6253\u5f00DevTools\u3001\u81ea\u52a8\u5316\u70b9\u51fb\u89c6\u56fe\u7b49\u529f\u80fd\uff0c\u5c06AI\u4ee3\u7406\u4e0eExpo SDK\u548c\u79fb\u52a8\u6a21\u62df\u5668\u8fde\u63a5\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86AI\u7f16\u7a0b\u4ee3\u7406\u4e0eExpo\u9879\u76ee\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u8bbf\u95ee\u6587\u6863\u3001\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u5e76\u8fdb\u884c\u89c6\u89c9\u9a8c\u8bc1\u3002", "conclusion": "Expo MCP Server\u4e3aAI\u9a71\u52a8\u7684\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u81ea\u52a8\u5316\u548c\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "wechat.2510.73e50da7", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483783&idx=1&sn=b9888a0af4953dc180bd38c35acf7815&chksm=e917aa049437df7d5e35586798442bfc586b138ccfc4789afd3108560906b2a037895867e37e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483783&idx=1&sn=b9888a0af4953dc180bd38c35acf7815&chksm=e917aa049437df7d5e35586798442bfc586b138ccfc4789afd3108560906b2a037895867e37e#rd", "authors": ["CodeAgent\u4ee3\u7801\u667a\u80fd"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em>\u00b7Code LLMs\u00b7\u521d\u521b\u52a8\u6001\uff0810.29\uff09", "comment": "Source: WeChat, Published: 2025-10-29 12:43:17", "summary": "\u4ea7\u54c1\u7279\u70b9\uff1a\u670d\u52a1\u63d0\u4f9b\u6301\u4e45\u8bb0\u5fc6\u5c42\uff0c\u53ef\u7528\u4e09\u884c\u4ee3\u7801\u63a5\u5165\uff0c\u5728\u4f1a\u8bdd\u95f4\u5b58\u50a8\u3001\u9057\u5fd8\u4e0e\u56de\u5fc6\u91cd\u8981\u4fe1\u606f\uff1bAWS \u9009\u62e9 Mem0 \u4e3a Agent SDK \u7684\u552f\u4e00\u5185\u5b58\u63d0\u4f9b\u5546\u3010356931508696392L570-L572\u3011\u3002", "AI": {"tldr": "\u4ea7\u54c1\u7279\u70b9\uff1a\u670d\u52a1\u63d0\u4f9b\u6301\u4e45\u8bb0\u5fc6\u5c42\uff0c\u53ef\u7528\u4e09\u884c\u4ee3\u7801\u63a5\u5165\uff0c\u5728\u4f1a\u8bdd\u95f4\u5b58\u50a8\u3001\u9057\u5fd8\u4e0e\u56de\u5fc6\u91cd\u8981\u4fe1\u606f\uff1bAWS \u9009\u62e9 Mem0 \u4e3a Agent SDK \u7684\u552f\u4e00\u5185\u5b58\u63d0\u4f9b\u5546\u3010356931508696392L570-L572\u3011\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.46654d7c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNjYzNTIwNw==&mid=2247493565&idx=1&sn=9f55d6b98eebaa413e4063c556cb2233&chksm=c343d1c62c0f170017690543bb09b0e6837a48343630d9d370b9e7bf5ad527a19f2bf266592e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNjYzNTIwNw==&mid=2247493565&idx=1&sn=9f55d6b98eebaa413e4063c556cb2233&chksm=c343d1c62c0f170017690543bb09b0e6837a48343630d9d370b9e7bf5ad527a19f2bf266592e#rd", "authors": ["CIOCDO"], "title": "\u544a\u522b\u201c\u6551\u706b\u201d\uff01\u201c<em class=\"highlight\">\u4ee3\u7406</em>\u5f0f\u4eba\u5de5\u667a\u80fd\u201d\uff08<em class=\"highlight\">Agentic</em> AI\uff09\u6b63\u6380\u8d77\u4e00\u573aIT\u8fd0\u7ef4\u9769\u547d", "comment": "Source: WeChat, Published: 2025-10-29 13:39:19", "summary": "\u4e3b\u89d2\u5c31\u662f\u2014\u2014\u201c\u4ee3\u7406\u5f0f\u4eba\u5de5\u667a\u80fd\u201d\uff08Agentic AI\uff09\u3002\u5b83\u4e0e\u4f20\u7edf\u81ea\u52a8\u5316\uff08RPA\u6216\u811a\u672c\uff09\u7684\u6839\u672c\u533a\u522b\u5728\u4e8e\uff1a\u5b83\u4e0d\u518d\u662f\u201c\u6267\u884c\u201d\u4f60\u8bbe\u5b9a\u7684\u91cd\u590d\u4efb\u52a1\uff0c\u800c\u662f\u80fd\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\uff0c\u5728\u51e0\u4e4e\u65e0\u9700\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u3001\u5e76\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002", "AI": {"tldr": "\u4e3b\u89d2\u5c31\u662f\u2014\u2014\u201c\u4ee3\u7406\u5f0f\u4eba\u5de5\u667a\u80fd\u201d\uff08Agentic AI\uff09\u3002\u5b83\u4e0e\u4f20\u7edf\u81ea\u52a8\u5316\uff08RPA\u6216\u811a\u672c\uff09\u7684\u6839\u672c\u533a\u522b\u5728\u4e8e\uff1a\u5b83\u4e0d\u518d\u662f\u201c\u6267\u884c\u201d\u4f60\u8bbe\u5b9a\u7684\u91cd\u590d\u4efb\u52a1\uff0c\u800c\u662f\u80fd\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\uff0c\u5728\u51e0\u4e4e\u65e0\u9700\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u3001\u5e76\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.cb40f2ab", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MDY2MzcwMg==&mid=2651194115&idx=1&sn=433d2fa1dbd68f3c24bf4beb62dac157&chksm=859cef478bdcf7ef1d76d0e964d9f76d15071d77ebe2eb061df4f440ee675d4b9d21d55f991e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MDY2MzcwMg==&mid=2651194115&idx=1&sn=433d2fa1dbd68f3c24bf4beb62dac157&chksm=859cef478bdcf7ef1d76d0e964d9f76d15071d77ebe2eb061df4f440ee675d4b9d21d55f991e#rd", "authors": ["IT\u804c\u573a\u659c\u6760\u9752\u5e74"], "title": "\u4e00\u5e74\u540e\u7684<em class=\"highlight\">Agentic</em> AI\uff1a\u4f01\u4e1a\u4ece\u63a2\u7d22\u5230\u4ef7\u503c\u5151\u73b0\u7684\u516d\u4e2a\u5173\u952e\u6559\u8bad\u548c\u542f\u793a\u2014-\u9ea6\u80af\u9521\u62a5\u544a\u89e3\u8bfb", "comment": "Source: WeChat, Published: 2025-10-29 11:55:28", "summary": "2024\u5e74\uff0c\u662f\u201cAgentic AI\u201d\uff08\u5177\u5907\u81ea\u4e3b\u51b3\u7b56\u4e0e\u884c\u52a8\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff09\u4ece\u6982\u5ff5\u8d70\u5411\u843d\u5730\u7684\u4e00\u5e74\u3002\u4e00\u5e74\u8fc7\u53bb\uff0c\u9ea6\u80af\u9521\u6700\u65b0\u7814\u7a76\u53d1\u73b0\uff1a\u591a\u6570\u4f01\u4e1a\u4ece\u201c\u5b9e\u9a8c\u6027\u90e8\u7f72\u201d\u8fdb\u5165\u201c\u4ef7\u503c\u5316\u5e94\u7528\u201d\u9636\u6bb5\uff0c\u4f46\u4e5f\u66b4\u9732\u51fa\u7ec4\u7ec7\u3001\u6d41\u7a0b\u3001\u6280\u672f\u6574\u5408\u7b49\u6df1\u5c42\u6311\u6218\u3002", "AI": {"tldr": "2024\u5e74\uff0c\u662f\u201cAgentic AI\u201d\uff08\u5177\u5907\u81ea\u4e3b\u51b3\u7b56\u4e0e\u884c\u52a8\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff09\u4ece\u6982\u5ff5\u8d70\u5411\u843d\u5730\u7684\u4e00\u5e74\u3002\u4e00\u5e74\u8fc7\u53bb\uff0c\u9ea6\u80af\u9521\u6700\u65b0\u7814\u7a76\u53d1\u73b0\uff1a\u591a\u6570\u4f01\u4e1a\u4ece\u201c\u5b9e\u9a8c\u6027\u90e8\u7f72\u201d\u8fdb\u5165\u201c\u4ef7\u503c\u5316\u5e94\u7528\u201d\u9636\u6bb5\uff0c\u4f46\u4e5f\u66b4\u9732\u51fa\u7ec4\u7ec7\u3001\u6d41\u7a0b\u3001\u6280\u672f\u6574\u5408\u7b49\u6df1\u5c42\u6311\u6218\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.ec574699", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzI1NjU4OA==&mid=2247483916&idx=1&sn=f44a3cbfb6aafcc80dc0218900724eb9&chksm=fef57fad84d4a1c6506a66346b5707ead8311c43b9cdb0c0db132867d1e519348400711689ad#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzI1NjU4OA==&mid=2247483916&idx=1&sn=f44a3cbfb6aafcc80dc0218900724eb9&chksm=fef57fad84d4a1c6506a66346b5707ead8311c43b9cdb0c0db132867d1e519348400711689ad#rd", "authors": ["\u865a\u821f\u672d\u8bb0"], "title": "<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u4eba\u5de5\u667a\u80fd\uff08<em class=\"highlight\">Agentic</em> AI\uff09\u53ef\u80fd\u5f15\u53d1\u8d23\u4efb\u95ee\u9898", "comment": "Source: WeChat, Published: 2025-10-29 11:31:45", "summary": "\u652f\u4ed8\u516c\u53f8\u6b63\u8bbe\u6cd5\u89e3\u51b3 AI \u667a\u80fd\u4f53\uff08AI agents\uff09\u4e3a\u6d88\u8d39\u8005\u8d2d\u7269\u65f6\u51fa\u73b0\u5931\u8bef\u7684\u76f8\u5173\u95ee\u9898\u3002\u94f6\u884c\u548c\u652f\u4ed8\u516c\u53f8\u5728\u4e3a\u667a\u80fd\u4f53\u5546\u52a1\uff08agentic commerce\uff09\u505a\u51c6\u5907\u65f6\uff0c\u4e0d\u5f97\u4e0d\u89e3\u51b3\u5931\u8bef\u4e0e\u8d23\u4efb\u5f52\u5c5e\u95ee\u9898\u3002", "AI": {"tldr": "\u652f\u4ed8\u516c\u53f8\u6b63\u8bbe\u6cd5\u89e3\u51b3 AI \u667a\u80fd\u4f53\uff08AI agents\uff09\u4e3a\u6d88\u8d39\u8005\u8d2d\u7269\u65f6\u51fa\u73b0\u5931\u8bef\u7684\u76f8\u5173\u95ee\u9898\u3002\u94f6\u884c\u548c\u652f\u4ed8\u516c\u53f8\u5728\u4e3a\u667a\u80fd\u4f53\u5546\u52a1\uff08agentic commerce\uff09\u505a\u51c6\u5907\u65f6\uff0c\u4e0d\u5f97\u4e0d\u89e3\u51b3\u5931\u8bef\u4e0e\u8d23\u4efb\u5f52\u5c5e\u95ee\u9898\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.9c675475", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyMzYzODM4MA==&mid=2247502747&idx=1&sn=622c85d6c28610b1943cee86f083edec&chksm=c009957c00011f8264ccc40adce1c5d5fa5e6daefab95a189268dc14345b57aaa8aa1e1260ad#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyMzYzODM4MA==&mid=2247502747&idx=1&sn=622c85d6c28610b1943cee86f083edec&chksm=c009957c00011f8264ccc40adce1c5d5fa5e6daefab95a189268dc14345b57aaa8aa1e1260ad#rd", "authors": ["\u963f\u91cc\u4e91\u57fa\u7840\u8bbe\u65bd"], "title": "\u5bb9\u5668\u670d\u52a1 - <em class=\"highlight\">Agentic</em> AI \u65f6\u4ee3\u4e91\u539f\u751f\u5e95\u5ea7", "comment": "Source: WeChat, Published: 2025-10-29 10:18:04", "summary": "\u8fce\u63a5 agentic ai\u5e94\u7528\u53d8\u9769 40% \u65b0\u8d1f\u8f7d 95% \u65b0\u5e73\u53f0 54% \u65b0\u7b97\u529b\u3002genai\u3002 laas\u5e02\u573a\u5728\u672a\u6765\u4e94\u5e74\u5c06\u4ee554.0% \u7684\u590d\u5408\u589e\u957f\u7387\u9ad8\u901f\u589e\u957f\uff0c\u9884\u8ba12025\u5168\u5e74 \u63a8\u7406\u7b97\u529b\u652f\u51fa\u63a5\u8fd1\u8bad\u7ec3\u7b97\u529b\u652f\u51fa -idc", "AI": {"tldr": "\u8fce\u63a5 agentic ai\u5e94\u7528\u53d8\u9769 40% \u65b0\u8d1f\u8f7d 95% \u65b0\u5e73\u53f0 54% \u65b0\u7b97\u529b\u3002genai\u3002 laas\u5e02\u573a\u5728\u672a\u6765\u4e94\u5e74\u5c06\u4ee554.0% \u7684\u590d\u5408\u589e\u957f\u7387\u9ad8\u901f\u589e\u957f\uff0c\u9884\u8ba12025\u5168\u5e74 \u63a8\u7406\u7b97\u529b\u652f\u51fa\u63a5\u8fd1\u8bad\u7ec3\u7b97\u529b\u652f\u51fa -idc", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.d24a53ff", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyOTgzODIxOQ==&mid=2247483719&idx=1&sn=c39ab095fd90032e5b8abd8bd8947e58&chksm=fb87c74f330121669239ce7d1f25e58d3d1b9f9931b6d08191207a08c2fab39209f2af5b4708#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyOTgzODIxOQ==&mid=2247483719&idx=1&sn=c39ab095fd90032e5b8abd8bd8947e58&chksm=fb87c74f330121669239ce7d1f25e58d3d1b9f9931b6d08191207a08c2fab39209f2af5b4708#rd", "authors": ["\u73a9\u5f00\u6e90"], "title": "\u65af\u5766\u798f\u8bba\u6587\u843d\u5730\uff1a\u5f00\u6e90\u7248 <em class=\"highlight\">Agentic</em> Context Engineering \u6765\u4e86", "comment": "Source: WeChat, Published: 2025-10-29 07:19:00", "summary": "\u65af\u5766\u798f\u5927\u5b66\u53bb\u5e74\u63d0\u51fa\u7684\u300cAgentic Context Engineering\u300d\u6846\u67b6\uff08\u8bba\u6587\u94fe\u63a5\uff09\uff0c\u73b0\u5728\u6709\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5b9e\u73b0\u7248\u672c\uff0c\u800c\u4e14\u96c6\u6210\u8d77\u6765\u6bd4\u60f3\u8c61\u4e2d\u7b80\u5355\u5f97\u591a\u3002kayba\u667a\u80fd\u4f53\u7684'\u8bb0\u5fc6\u8fdb\u5316\u5668'", "AI": {"tldr": "\u65af\u5766\u798f\u5927\u5b66\u53bb\u5e74\u63d0\u51fa\u7684\u300cAgentic Context Engineering\u300d\u6846\u67b6\uff08\u8bba\u6587\u94fe\u63a5\uff09\uff0c\u73b0\u5728\u6709\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5b9e\u73b0\u7248\u672c\uff0c\u800c\u4e14\u96c6\u6210\u8d77\u6765\u6bd4\u60f3\u8c61\u4e2d\u7b80\u5355\u5f97\u591a\u3002kayba\u667a\u80fd\u4f53\u7684'\u8bb0\u5fc6\u8fdb\u5316\u5668'", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.c154cab3", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2MzYzNzE1OQ==&mid=2247486117&idx=1&sn=0a48680c43b0e729781dfb010fe8497a&chksm=fdfe21abc2e056d1be16fd61024d013f1842cb0069607709b134163aae19422878d09b639bca#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2MzYzNzE1OQ==&mid=2247486117&idx=1&sn=0a48680c43b0e729781dfb010fe8497a&chksm=fdfe21abc2e056d1be16fd61024d013f1842cb0069607709b134163aae19422878d09b639bca#rd", "authors": ["HCL\u8f6f\u4ef6"], "title": "HCL BigFix AEX v12 \u9707\u64bc\u53d1\u5e03\uff1a\u4f01\u4e1a\u7ea7 <em class=\"highlight\">Agentic</em> AI \u5e73\u53f0", "comment": "Source: WeChat, Published: 2025-10-29 04:00:38", "summary": "Agentic \u6838\u5fc3\u80fd\u529bAI Agent Builder\uff08\u96f6\u4ee3\u7801/\u4f4e\u4ee3\u7801\u62d6\u62fd\uff09\u2192 \u5feb\u901f\u6784\u5efa\u81ea\u4e3b\u4ee3\u7406\u7edf\u4e00\u5de5\u5177\u76ee\u5f55 \u2192 50+ \u9884\u96c6\u6210\u8fde\u63a5\u5668\uff0c\u53ef\u6269\u5c55 IT/HR/\u8d22\u52a1\u7b49\u4efb\u610f\u7cfb\u7edf\u591a\u4ee3\u7406\u534f\u540c \u2192 \u590d\u6742\u6d41\u7a0b\uff08\u5165\u804c\u3001\u5ba1\u6279\u3001\u4e8b\u4ef6\u54cd\u5e94\uff09\u7531\u591a\u4e2a\u4ee3\u7406\u5206\u5de5\u5b8c\u6210", "AI": {"tldr": "Agentic \u6838\u5fc3\u80fd\u529bAI Agent Builder\uff08\u96f6\u4ee3\u7801/\u4f4e\u4ee3\u7801\u62d6\u62fd\uff09\u2192 \u5feb\u901f\u6784\u5efa\u81ea\u4e3b\u4ee3\u7406\u7edf\u4e00\u5de5\u5177\u76ee\u5f55 \u2192 50+ \u9884\u96c6\u6210\u8fde\u63a5\u5668\uff0c\u53ef\u6269\u5c55 IT/HR/\u8d22\u52a1\u7b49\u4efb\u610f\u7cfb\u7edf\u591a\u4ee3\u7406\u534f\u540c \u2192 \u590d\u6742\u6d41\u7a0b\uff08\u5165\u804c\u3001\u5ba1\u6279\u3001\u4e8b\u4ef6\u54cd\u5e94\uff09\u7531\u591a\u4e2a\u4ee3\u7406\u5206\u5de5\u5b8c\u6210", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.8a28747f", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNjYwMTAxNg==&mid=2247501067&idx=1&sn=b4792cccea38c8eb5a6af5357b4e012f&chksm=c3c2c0e0c344d5a5ba5405c4b12ec64f4d38cda6b32bea7e30b450918d31ce1e7c08578850d3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNjYwMTAxNg==&mid=2247501067&idx=1&sn=b4792cccea38c8eb5a6af5357b4e012f&chksm=c3c2c0e0c344d5a5ba5405c4b12ec64f4d38cda6b32bea7e30b450918d31ce1e7c08578850d3#rd", "authors": ["\u65e0\u95ee\u82af\u7a79"], "title": "\u65e0\u95ee\u82af\u7a79\u590f\u7acb\u96ea\uff1a\u4ece\u89c6Agent\u4e3a\u5de5\u5177\uff0c\u5230\u89c6Agent\u4e3a\u534f\u4f5c\u8005\uff0c<em class=\"highlight\">Agentic</em> AI\u662f\u4e00\u573a\u57fa\u7840\u8bbe\u65bd\u5fc5\u987b\u8ddf\u4e0a\u7684\u7cfb\u7edf\u6027\u9769\u547d", "comment": "Source: WeChat, Published: 2025-10-29 03:02:21", "summary": "\u8fc8\u5411 Agentic Infra \u9636\u6bb5\uff0c\u5728\u8ba9\u667a\u80fd\u4f53\u6df1\u5ea6\u53c2\u4e0e\u534f\u4f5c\u7684\u5c1d\u8bd5\u4e2d\uff0c\u65e0\u95ee\u82af\u7a79\u642d\u5efa\u4e86\u57fa\u7840\u8bbe\u65bd\u667a\u80fd\u4f53\u8702\u7fa4\u4f53\u7cfb\uff0c\u628a\u4f20\u7edf\u5206\u6563\u5728\u5f00\u53d1\u3001\u8fd0\u7ef4\u3001\u8fd0\u8425\u56e2\u961f\u7684\u5272\u88c2\u6d41\u7a0b\uff0c\u7edf\u4e00\u5728\u4e00\u4e2a\u667a\u80fd\u5316\u7cfb\u7edf\u4e2d\uff0c\u8ba9\u7b97\u529b\u4ef7\u503c\u66f4\u52a0\u81ea\u52a8\u5316\u5730\u4f20\u9012\u7ed9\u5e73\u53f0\u7ec8\u7aef\u7528\u6237\u3002", "AI": {"tldr": "\u8fc8\u5411 Agentic Infra \u9636\u6bb5\uff0c\u5728\u8ba9\u667a\u80fd\u4f53\u6df1\u5ea6\u53c2\u4e0e\u534f\u4f5c\u7684\u5c1d\u8bd5\u4e2d\uff0c\u65e0\u95ee\u82af\u7a79\u642d\u5efa\u4e86\u57fa\u7840\u8bbe\u65bd\u667a\u80fd\u4f53\u8702\u7fa4\u4f53\u7cfb\uff0c\u628a\u4f20\u7edf\u5206\u6563\u5728\u5f00\u53d1\u3001\u8fd0\u7ef4\u3001\u8fd0\u8425\u56e2\u961f\u7684\u5272\u88c2\u6d41\u7a0b\uff0c\u7edf\u4e00\u5728\u4e00\u4e2a\u667a\u80fd\u5316\u7cfb\u7edf\u4e2d\uff0c\u8ba9\u7b97\u529b\u4ef7\u503c\u66f4\u52a0\u81ea\u52a8\u5316\u5730\u4f20\u9012\u7ed9\u5e73\u53f0\u7ec8\u7aef\u7528\u6237\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.920b992e", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5ODI5NDYxNA==&mid=2649772745&idx=1&sn=8a6dd4b7d543742fa6fde4a5d73af4c1&chksm=892a095c3b748ad344f32c3ac2f358dcc559f03a735b7591b680bd02d23083c7cd2429c41be8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5ODI5NDYxNA==&mid=2649772745&idx=1&sn=8a6dd4b7d543742fa6fde4a5d73af4c1&chksm=892a095c3b748ad344f32c3ac2f358dcc559f03a735b7591b680bd02d23083c7cd2429c41be8#rd", "authors": ["\u96e8\u6768\u7f51\u5fd7"], "title": "\u6b22\u8fce\u6765\u5230 AI \u81ea\u4e3b\u8d2d\u7269\u65f6\u4ee3\uff1a\u4e00\u6587\u8bfb\u61c2<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7535\u5546 (<em class=\"highlight\">Agentic</em> Commerce)", "comment": "Source: WeChat, Published: 2025-10-29 01:56:33", "summary": "\u5b83\u53eb\u201c\u667a\u80fd\u4f53\u7535\u5546\u201d\uff08Agentic Commerce\uff09\uff0c\u800c\u4e14\u5df2\u7ecf\u5f00\u59cb\u53d1\u751f\u4e86\u3002\u4ec0\u4e48\u662f\u667a\u80fd\u4f53\u7535\u5546\uff1f\u2014\u2014 \u5b83\u4e0d\u662f\u66f4\u806a\u660e\u7684\u804a\u5929\u673a\u5668\u4eba\u9996\u5148\u8981\u6e05\u695a\uff1a\u667a\u80fd\u4f53\u7535\u5546\u548c\u4f60\u73b0\u5728\u7528\u7684\u8d2d\u7269\u5de5\u5177\u6709\u70b9\u4e0d\u4e00\u6837\u3002", "AI": {"tldr": "\u5b83\u53eb\u201c\u667a\u80fd\u4f53\u7535\u5546\u201d\uff08Agentic Commerce\uff09\uff0c\u800c\u4e14\u5df2\u7ecf\u5f00\u59cb\u53d1\u751f\u4e86\u3002\u4ec0\u4e48\u662f\u667a\u80fd\u4f53\u7535\u5546\uff1f\u2014\u2014 \u5b83\u4e0d\u662f\u66f4\u806a\u660e\u7684\u804a\u5929\u673a\u5668\u4eba\u9996\u5148\u8981\u6e05\u695a\uff1a\u667a\u80fd\u4f53\u7535\u5546\u548c\u4f60\u73b0\u5728\u7528\u7684\u8d2d\u7269\u5de5\u5177\u6709\u70b9\u4e0d\u4e00\u6837\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.e0d6bd52", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5OTEyNzE0NQ==&mid=2247485757&idx=1&sn=22062ef48eeb76df6ce84d2ffa08b750&chksm=97424e26355ddf54d66c72d509ad093aceb24323823a2b3af748817f93f3a5c63b4d71dcdbee#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5OTEyNzE0NQ==&mid=2247485757&idx=1&sn=22062ef48eeb76df6ce84d2ffa08b750&chksm=97424e26355ddf54d66c72d509ad093aceb24323823a2b3af748817f93f3a5c63b4d71dcdbee#rd", "authors": ["AI Daily Papers"], "title": "<em class=\"highlight\">Agentic</em> AI\u4e0e\u81ea\u4e3b\u6027\uff1a2025\u5e74\u901a\u4fe1\u8fd0\u8425\u5546\u6218\u7565\u62a5\u544a", "comment": "Source: WeChat, Published: 2025-10-28 16:00:25", "summary": "\u62a5\u544a\u539f\u6587\u5730\u5740\uff1ahttps\uff1a//inform.tmforum.org/research-and-analysis/reports/agentic-ai-and-autonomy-csps-set-out-their-strategies\u62a5\u544a\u6982\u8ff0\u672c\u62a5\u544a\u7531TM Forum\u4e8e2025\u5e749\u6708\u53d1\u5e03\uff0c\u6807\u9898\u4e3a\u201cAgentic AI and autonomy\uff1a CSPs set out their strategies\u201d\uff0c\u65e8\u5728\u6df1\u5165\u5206\u6790\u5168\u7403\u901a\u4fe1\u670d\u52a1", "AI": {"tldr": "\u62a5\u544a\u539f\u6587\u5730\u5740\uff1ahttps\uff1a//inform.tmforum.org/research-and-analysis/reports/agentic-ai-and-autonomy-csps-set-out-their-strategies\u62a5\u544a\u6982\u8ff0\u672c\u62a5\u544a\u7531TM Forum\u4e8e2025\u5e749\u6708\u53d1\u5e03\uff0c\u6807\u9898\u4e3a\u201cAgentic AI and autonomy\uff1a CSPs set out their strategi...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.98d1eb59", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5ODU3NjY5OA==&mid=2247484886&idx=1&sn=9df6cab2db12dcf335946180ce4b9138&chksm=ffb5ac2ba418e3046a7bcac3f276d839a842cb53bb5051d06ccc5295c1570ff3500ad0a5906c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5ODU3NjY5OA==&mid=2247484886&idx=1&sn=9df6cab2db12dcf335946180ce4b9138&chksm=ffb5ac2ba418e3046a7bcac3f276d839a842cb53bb5051d06ccc5295c1570ff3500ad0a5906c#rd", "authors": ["ByteLink"], "title": "<em class=\"highlight\">Agentic</em> AI \u5b66\u4e60\u7b14\u8bb0", "comment": "Source: WeChat, Published: 2025-10-28 15:45:22", "summary": "\u8fd9\u4e2a\u8bfe\u7a0b\u4f7f\u7528\u7684\u8bcd\u662f Agentic\uff0c\u66f4\u4e2d\u6027\u7684\u5f62\u5bb9\u8bcd\uff0c\u800c\u4e0d\u662f\u786e\u5b9a\u6027\u7684\u540d\u8bcd agent\uff0c\u7528\u6765\u8868\u793a\u4e0d\u540c\u7a0b\u5ea6\u7684 Agent\uff0c\u6709\u4e00\u4e9b\u7684\u81ea\u4e3b\u6027\u6bd4\u8f83\u4f4e\uff0c\u53e6\u4e00\u4e9b\u7684\u81ea\u4e3b\u6027\u8f83\u9ad8\u3002", "AI": {"tldr": "\u8fd9\u4e2a\u8bfe\u7a0b\u4f7f\u7528\u7684\u8bcd\u662f Agentic\uff0c\u66f4\u4e2d\u6027\u7684\u5f62\u5bb9\u8bcd\uff0c\u800c\u4e0d\u662f\u786e\u5b9a\u6027\u7684\u540d\u8bcd agent\uff0c\u7528\u6765\u8868\u793a\u4e0d\u540c\u7a0b\u5ea6\u7684 Agent\uff0c\u6709\u4e00\u4e9b\u7684\u81ea\u4e3b\u6027\u6bd4\u8f83\u4f4e\uff0c\u53e6\u4e00\u4e9b\u7684\u81ea\u4e3b\u6027\u8f83\u9ad8\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.4bde9bfe", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2ODUyMTQyNA==&mid=2247499438&idx=1&sn=1efc7aa410b8092f57ad5212773f4471&chksm=eba00b5ef2c8147f5a6d7f4df6972106ea723118062fda8509a77d93b57b26ed59fea1dabce6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2ODUyMTQyNA==&mid=2247499438&idx=1&sn=1efc7aa410b8092f57ad5212773f4471&chksm=eba00b5ef2c8147f5a6d7f4df6972106ea723118062fda8509a77d93b57b26ed59fea1dabce6#rd", "authors": ["PyTorch\u7814\u4e60\u793e"], "title": "\u6784\u5efa\u5177\u5907\u6df1\u5ea6\u601d\u8003\u80fd\u529b\u7684 <em class=\"highlight\">Agentic</em> RAG \u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u67e5\u8be2", "comment": "Source: WeChat, Published: 2025-10-28 14:38:08", "summary": "\u5728\u4e00\u4e2a agentic \u7cfb\u7edf\u91cc\uff0c\u5de5\u4f5c\u6d41\u590d\u6742\u4e14\u5faa\u73af\uff0ctracing \u5e76\u975e\u53ef\u6709\u53ef\u65e0\uff0c\u800c\u662f\u5f88\u91cd\u8981\u3002\u5b83\u5e2e\u52a9\u4f60\u53ef\u89c6\u5316\u5185\u90e8\u8fc7\u7a0b\uff0c\u66f4\u5bb9\u6613\u8c03\u8bd5 agent \u7684\u601d\u8003\u8def\u5f84\u3002\u77e5\u8bc6\u5e93\u6765\u6e90\u4e00\u4e2a\u751f\u4ea7\u7ea7 RAG \u7cfb\u7edf\u9700\u8981\u65e2\u590d\u6742\u53c8\u6709\u6311\u6218\u6027\u7684\u77e5\u8bc6\u5e93\uff0c\u624d\u80fd\u771f\u6b63\u4f53\u73b0\u5176\u6709\u6548\u6027\u3002", "AI": {"tldr": "\u5728\u4e00\u4e2a agentic \u7cfb\u7edf\u91cc\uff0c\u5de5\u4f5c\u6d41\u590d\u6742\u4e14\u5faa\u73af\uff0ctracing \u5e76\u975e\u53ef\u6709\u53ef\u65e0\uff0c\u800c\u662f\u5f88\u91cd\u8981\u3002\u5b83\u5e2e\u52a9\u4f60\u53ef\u89c6\u5316\u5185\u90e8\u8fc7\u7a0b\uff0c\u66f4\u5bb9\u6613\u8c03\u8bd5 agent \u7684\u601d\u8003\u8def\u5f84\u3002\u77e5\u8bc6\u5e93\u6765\u6e90\u4e00\u4e2a\u751f\u4ea7\u7ea7 RAG \u7cfb\u7edf\u9700\u8981\u65e2\u590d\u6742\u53c8\u6709\u6311\u6218\u6027\u7684\u77e5\u8bc6\u5e93\uff0c\u624d\u80fd\u771f\u6b63\u4f53\u73b0\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.27772a4a", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwNDQxNTE0OA==&mid=2247491330&idx=1&sn=e6b8f1595a1f155b8d92d5c432aea9c2&chksm=c10ad5185f3e3357f3c88726e040950abc7caa9cee05a3a42b94b6b305831fd2a34e99baf408#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwNDQxNTE0OA==&mid=2247491330&idx=1&sn=e6b8f1595a1f155b8d92d5c432aea9c2&chksm=c10ad5185f3e3357f3c88726e040950abc7caa9cee05a3a42b94b6b305831fd2a34e99baf408#rd", "authors": ["Edison\u670b\u53cb\u5708"], "title": "\u4f01\u4e1a\u7ea7<em class=\"highlight\">\u5927\u6a21\u578b</em>\u79c1\u6709\u5316\u90e8\u7f72\u63a2\u8ba8", "comment": "Source: WeChat, Published: 2025-10-29 11:11:52", "summary": "\u672c\u6587\u8ba8\u8bba\u4f01\u4e1a\u7ea7AI \u5927\u6a21\u578b\u79c1\u6709\u5316\u90e8\u7f72\u65b9\u6848\u53caAI Agent\u7684\u5f00\u53d1\u8bbe\u8ba1\u6d41\u7a0b\u3002\u672c\u6587\u7531Eddie\u4ee5\u53ca\u5408\u521b\u793e\u533a\u7684\u5b66\u5458\u63d0\u4f9b\u6574\u4f53\u601d\u8def\uff0c\u6982\u8ff0\u6d41\u7a0b\u5982\u4e0b\uff1a\u7acb\u9879 \u2192 \u9700\u6c42 \u2192 \u65b9\u6848 \u2192 \u786c\u4ef6\u4e0e\u6570\u636e \u2192 \u5f00\u53d1 \u2192 \u6d4b\u8bd5 \u2192 \u90e8\u7f72 \u2192 \u8fd0\u8425", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4f01\u4e1a\u7ea7AI \u5927\u6a21\u578b\u79c1\u6709\u5316\u90e8\u7f72\u65b9\u6848\u53caAI Agent\u7684\u5f00\u53d1\u8bbe\u8ba1\u6d41\u7a0b\u3002\u672c\u6587\u7531Eddie\u4ee5\u53ca\u5408\u521b\u793e\u533a\u7684\u5b66\u5458\u63d0\u4f9b\u6574\u4f53\u601d\u8def\uff0c\u6982\u8ff0\u6d41\u7a0b\u5982\u4e0b\uff1a\u7acb\u9879 \u2192 \u9700\u6c42 \u2192 \u65b9\u6848 \u2192 \u786c\u4ef6\u4e0e\u6570\u636e \u2192 \u5f00\u53d1 \u2192 \u6d4b\u8bd5 \u2192 \u90e8\u7f72 \u2192 \u8fd0\u8425", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.ed46f15c", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0NDcxNTcxNA==&mid=2247499259&idx=2&sn=398d4d6e36a24a0aeae4e31bba2c8ba9&chksm=e872895ebca96e4ac54b52cd53d6fff2d7ca81aa9123105f7469cac1b8a507c67c67f187252e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0NDcxNTcxNA==&mid=2247499259&idx=2&sn=398d4d6e36a24a0aeae4e31bba2c8ba9&chksm=e872895ebca96e4ac54b52cd53d6fff2d7ca81aa9123105f7469cac1b8a507c67c67f187252e#rd", "authors": ["\u6570\u878d\u5496\u5561"], "title": "\u667a\u80fd\u4f53\u7efc\u8ff0\uff1a\u63a2\u7d22\u57fa\u4e8e\u5927\u578b\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>\u7684\u667a\u80fd\u4f53\uff1a\u5b9a\u4e49\u3001\u65b9\u6cd5\u4e0e\u524d\u666f", "comment": "Source: WeChat, Published: 2025-10-29 10:32:49", "summary": "\u9c81\u68d2\u6027\u5f3a\uff0c\u9002\u5e94\u52a8\u6001\u73af\u5883 \u96be\u8fbe\u5168\u5c40\u6700\u4f18\uff0c\u901a\u4fe1\u5f00\u9500\u5927 \u901a\u4fe1\u673a\u5236\u4e0e\u6548\u7387\u63d0\u5347 \u4fe1\u606f\u4ea4\u6362\u65b9\u5f0f\uff1a\u65e0\u901a\u4fe1\uff08\u4ec5\u4f9d\u8d56\u672c\u5730\u4fe1\u606f\uff09\u3001\u6709\u901a\u4fe1\uff08\u6d88\u606f\u4f20\u9012\uff09\u3001\u5171\u4eab\u5185\u5b58\uff08\u4e2d\u592e\u77e5\u8bc6\u5e93\u5982MetaGPT\u7684\u5168\u5c40\u5185\u5b58\u6c60\u3001\u5171\u4eab\u53c2\u6570\uff09\u3002", "AI": {"tldr": "\u9c81\u68d2\u6027\u5f3a\uff0c\u9002\u5e94\u52a8\u6001\u73af\u5883 \u96be\u8fbe\u5168\u5c40\u6700\u4f18\uff0c\u901a\u4fe1\u5f00\u9500\u5927 \u901a\u4fe1\u673a\u5236\u4e0e\u6548\u7387\u63d0\u5347 \u4fe1\u606f\u4ea4\u6362\u65b9\u5f0f\uff1a\u65e0\u901a\u4fe1\uff08\u4ec5\u4f9d\u8d56\u672c\u5730\u4fe1\u606f\uff09\u3001\u6709\u901a\u4fe1\uff08\u6d88\u606f\u4f20\u9012\uff09\u3001\u5171\u4eab\u5185\u5b58\uff08\u4e2d\u592e\u77e5\u8bc6\u5e93\u5982MetaGPT\u7684\u5168\u5c40\u5185\u5b58\u6c60\u3001\u5171\u4eab\u53c2\u6570\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.9f869d9b", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498236&idx=1&sn=d23c85cf4300270852466f11043662b1&chksm=fd5e95eb4cfc5b3f0a3647a141448fb2a6cc737f87794e0daef50a3bc1865c47f1c2b82a2c4d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498236&idx=1&sn=d23c85cf4300270852466f11043662b1&chksm=fd5e95eb4cfc5b3f0a3647a141448fb2a6cc737f87794e0daef50a3bc1865c47f1c2b82a2c4d#rd", "authors": ["\u4e30\u519c\u4fe1\u606f"], "title": "\u79d1\u6280\u65e5\u62a5\uff1a\u8f7b\u91cf\u5316+\u591a\u667a\u80fd\u4f53\uff0c\u795e\u519c<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8ba9\u519c\u4e1aAI\u597d\u7528\u666e\u60e0", "comment": "Source: WeChat, Published: 2025-10-29 10:15:15", "summary": "eSIM\u5546\u7528\u843d\u5730\uff0c\u624b\u673a\u8fdb\u5165\u300c\u65e0\u5361\u65f6\u4ee3\u300d\uff1f2025 \u4e1c\u6e56\u8bba\u575b\u5728\u6b66\u6c49\u4e3e\u884c \u53e3\u3002\u4e2a\u7701\u533a\u5e02\u540c\u6b65\u5f00\u901aeSIM\u624b\u673a\u4e1a\u52a1\u3002\u5728\u56fd\u5185\u5373\u5c06\u6b63\u5f0f\u4ece\u7269\u8054\u7f51\uff0c\u667a\u80fd\u7a7f\u6234\u9886\u57df\u5411\u624b\u673a\u7aef\u5ef6\u4f38\uff0c\u79d1\u6280\u65e5\u62a5\u6b66\u6c4910\u670814\u65e5\u7535\uff08\u8bb0 \u4f4d\u9662\u58eb\u4e3a2025\u5e74\u5ea6\u5168\u56fd\u9752\u5c11\u5e74\u521b\u00b7\u9020 \u6960\uff091", "AI": {"tldr": "eSIM\u5546\u7528\u843d\u5730\uff0c\u624b\u673a\u8fdb\u5165\u300c\u65e0\u5361\u65f6\u4ee3\u300d\uff1f2025 \u4e1c\u6e56\u8bba\u575b\u5728\u6b66\u6c49\u4e3e\u884c \u53e3\u3002\u4e2a\u7701\u533a\u5e02\u540c\u6b65\u5f00\u901aeSIM\u624b\u673a\u4e1a\u52a1\u3002\u5728\u56fd\u5185\u5373\u5c06\u6b63\u5f0f\u4ece\u7269\u8054\u7f51\uff0c\u667a\u80fd\u7a7f\u6234\u9886\u57df\u5411\u624b\u673a\u7aef\u5ef6\u4f38\uff0c\u79d1\u6280\u65e5\u62a5\u6b66\u6c4910\u670814\u65e5\u7535\uff08\u8bb0 \u4f4d\u9662\u58eb\u4e3a2025\u5e74\u5ea6\u5168\u56fd\u9752\u5c11\u5e74\u521b\u00b7\u9020 \u6960\uff091", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.478c3784", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNjI2MjQ3Nw==&mid=2247494219&idx=1&sn=44c1520e4401d5458ea15c97bd2d8fe0&chksm=c0c59dcd1968a83389d699fcc6e0e607bc0f852a8a4c5d2b0e2e476e22cc0151f12ff22651f2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNjI2MjQ3Nw==&mid=2247494219&idx=1&sn=44c1520e4401d5458ea15c97bd2d8fe0&chksm=c0c59dcd1968a83389d699fcc6e0e607bc0f852a8a4c5d2b0e2e476e22cc0151f12ff22651f2#rd", "authors": ["BanTech\u667a\u5e93"], "title": "\u9762\u5411<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u573a\u666f\u7684\u4ea4\u4e92\u80fd\u529b\u8bc4\u6d4b", "comment": "Source: WeChat, Published: 2025-10-29 08:57:00", "summary": "\u5927\u6a21\u578b\u5e94\u7528\u7cfb\u7edf\u4ee5\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4e3a\u6838\u5fc3\uff0c\u7528\u6237\u8f93\u5165\u4e0d\u518d\u53d7\u9650\u4e8e\u9884\u8bbe\u7684\u56fa\u5b9a\u8def\u5f84\u6216\u683c\u5f0f\u3002\uff082\uff09\u8f93\u5165\u8868\u8fbe\u591a\u6837\u6027\u3002\u5bf9\u4e8e\u540c\u4e00\u7528\u6237\u610f\u56fe\uff0c\u5b58\u5728\u6210\u5343\u4e0a\u4e07\u79cd\u4e0d\u540c\u7684\u8868\u8fbe\u65b9\u5f0f\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u5e94\u7528\u7cfb\u7edf\u4ee5\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4e3a\u6838\u5fc3\uff0c\u7528\u6237\u8f93\u5165\u4e0d\u518d\u53d7\u9650\u4e8e\u9884\u8bbe\u7684\u56fa\u5b9a\u8def\u5f84\u6216\u683c\u5f0f\u3002\uff082\uff09\u8f93\u5165\u8868\u8fbe\u591a\u6837\u6027\u3002\u5bf9\u4e8e\u540c\u4e00\u7528\u6237\u610f\u56fe\uff0c\u5b58\u5728\u6210\u5343\u4e0a\u4e07\u79cd\u4e0d\u540c\u7684\u8868\u8fbe\u65b9\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.ab3fa193", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5NzA2MjUxMA==&mid=2651524629&idx=1&sn=2220631b4215d5b5a76476de17741da8&chksm=8ac1ea108661f88695d653cdd2b4b1e7a53930e8ba4933cf63e453d3ca2c79c96eaf9bc8016e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5NzA2MjUxMA==&mid=2651524629&idx=1&sn=2220631b4215d5b5a76476de17741da8&chksm=8ac1ea108661f88695d653cdd2b4b1e7a53930e8ba4933cf63e453d3ca2c79c96eaf9bc8016e#rd", "authors": ["\u529b\u7ef4\u667a\u8054"], "title": "\u4e1a\u754c\u8363\u8a89 | \u529b\u7ef4\u667a\u8054\u5165\u9009\u4e2d\u56fd\u4fe1\u901a\u9662\u300a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e00\u4f53\u673a\u4ea7\u4e1a\u56fe\u8c31\u300b\uff0c\u52a9\u529b\u901a\u7528<em class=\"highlight\">\u5927\u6a21\u578b</em>\u843d\u5730", "comment": "Source: WeChat, Published: 2025-10-29 08:42:13", "summary": "\u8fd1\u65e5\uff0c\u7531\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u53d1\u5c55\u8054\u76df\u4e0e\u4e2d\u56fd\u4fe1\u606f\u901a\u4fe1\u7814\u7a76\u9662\u5171\u540c\u7f16\u5236\u7684\u300a\u5927\u6a21\u578b\u4e00\u4f53\u673a\u4ea7\u4e1a\u56fe\u8c31\u300b\u6b63\u5f0f\u53d1\u5e03\u3002\u529b\u7ef4\u667a\u8054\u51ed\u501f\u5176\u81ea\u7814\u7684Sentosa_USL\u7edf\u4e00\u8bed\u4e49\u5c42\u5e73\u53f0\u4e0eDataAgent\u6570\u636e\u5206\u6790\u667a\u80fd\u4f53\uff0c\u5206\u522b\u5165\u9009\u4ea7\u4e1a\u56fe\u8c31\u4e2d\u7684\u201c\u6a21\u578b\u4f9b\u5e94\u5546\u201d\u4e0e\u201c", "AI": {"tldr": "\u8fd1\u65e5\uff0c\u7531\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u53d1\u5c55\u8054\u76df\u4e0e\u4e2d\u56fd\u4fe1\u606f\u901a\u4fe1\u7814\u7a76\u9662\u5171\u540c\u7f16\u5236\u7684\u300a\u5927\u6a21\u578b\u4e00\u4f53\u673a\u4ea7\u4e1a\u56fe\u8c31\u300b\u6b63\u5f0f\u53d1\u5e03\u3002\u529b\u7ef4\u667a\u8054\u51ed\u501f\u5176\u81ea\u7814\u7684Sentosa_USL\u7edf\u4e00\u8bed\u4e49\u5c42\u5e73\u53f0\u4e0eDataAgent\u6570\u636e\u5206\u6790\u667a\u80fd\u4f53\uff0c\u5206\u522b\u5165\u9009\u4ea7\u4e1a\u56fe\u8c31\u4e2d\u7684\u201c\u6a21\u578b\u4f9b\u5e94\u5546\u201d\u4e0e\u201c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.9c871afb", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247514326&idx=3&sn=2c5991d0fead98eec3e0633bdb186239&chksm=e9ac25cbd72fc4646759f9037b0b21ac1bbe6b16a5bdb03c26f5edb424a5526f57eb3e35d3e2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247514326&idx=3&sn=2c5991d0fead98eec3e0633bdb186239&chksm=e9ac25cbd72fc4646759f9037b0b21ac1bbe6b16a5bdb03c26f5edb424a5526f57eb3e35d3e2#rd", "authors": ["\u5f00\u6e90\u62a5\u544a"], "title": "2025\u5e74<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\u5728\u4f4e\u78b3\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63a2\u7d22\u62a5\u544a-\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\uff08\u8d75\u4fca\u534e\uff09", "comment": "Source: WeChat, Published: 2025-10-29 07:38:59", "summary": "\u300a\u5927\u6a21\u578b\u667a\u80fd\u4f53\u5728\u4f4e\u78b3\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63a2\u7d22\u62a5\u544a\u300b\u7531\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\uff08\u6df1\u5733\uff09\u8d75\u4fca\u534e\u56e2\u961f\u53d1\u5e03\uff0c\u7cfb\u7edf\u9610\u8ff0\u4e86\u5927\u6a21\u578b\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u80fd\u529b\u3001\u5728\u7535\u529b\u7cfb\u7edf\u7684\u5e94\u7528\u573a\u666f\u4e0e\u5b9e\u8df5\u6210\u679c\uff0c\u4e3a\u4f4e\u78b3\u7535\u529b\u7cfb\u7edf\u7684\u667a\u80fd\u5316\u5347\u7ea7\u63d0\u4f9b\u4e86\u5168\u9762\u6307\u5f15\u3002", "AI": {"tldr": "\u300a\u5927\u6a21\u578b\u667a\u80fd\u4f53\u5728\u4f4e\u78b3\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63a2\u7d22\u62a5\u544a\u300b\u7531\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\uff08\u6df1\u5733\uff09\u8d75\u4fca\u534e\u56e2\u961f\u53d1\u5e03\uff0c\u7cfb\u7edf\u9610\u8ff0\u4e86\u5927\u6a21\u578b\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u80fd\u529b\u3001\u5728\u7535\u529b\u7cfb\u7edf\u7684\u5e94\u7528\u573a\u666f\u4e0e\u5b9e\u8df5\u6210\u679c\uff0c\u4e3a\u4f4e\u78b3\u7535\u529b\u7cfb\u7edf\u7684\u667a\u80fd\u5316\u5347\u7ea7\u63d0\u4f9b\u4e86\u5168\u9762\u6307\u5f15\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.9f30056f", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNDI1NjI4NA==&mid=2247485094&idx=1&sn=6cacf3141e955b4f3282964f9b811a35&chksm=c0b2d94c0b3853949fbd7169c3f75c083d7225424e759c3b539bb6cf8c6b0bf768fa7c54e4ac#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNDI1NjI4NA==&mid=2247485094&idx=1&sn=6cacf3141e955b4f3282964f9b811a35&chksm=c0b2d94c0b3853949fbd7169c3f75c083d7225424e759c3b539bb6cf8c6b0bf768fa7c54e4ac#rd", "authors": ["\u8f7b\u677e\u5b66AI\u5927\u6a21\u578b"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u5f00\u53d1\u5165\u95e8\u5168\u653b\u7565\uff0c\u770b\u61c2\u8fd9\u4e00\u7bc7\u5c31\u591f\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-10-29 05:44:22", "summary": "\u578b\u5e94\u7528\u5f00\u53d1\uff0c\u5e26\u9886\u5927\u5bb6\u5165\u95e8\uff0c\u5e26\u9886\u5927\u5bb6\u4e86\u89e3AI\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u7684\u5168\u653b\u7565\u3002\u5c3d\u7ba1\u5e02\u9762\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u79cd\u7c7b\u7e41\u591a\uff0c\u4f46\u5927\u5bb6\u5728\u4f7f\u7528\u65f6\u5176\u5b9e\u90fd\u662f\u901a\u8fc7API\u6765\u4e0e\u5927\u6a21\u578b\u4ea4\u4e92\u7684\u3002", "AI": {"tldr": "\u578b\u5e94\u7528\u5f00\u53d1\uff0c\u5e26\u9886\u5927\u5bb6\u5165\u95e8\uff0c\u5e26\u9886\u5927\u5bb6\u4e86\u89e3AI\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u7684\u5168\u653b\u7565\u3002\u5c3d\u7ba1\u5e02\u9762\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u79cd\u7c7b\u7e41\u591a\uff0c\u4f46\u5927\u5bb6\u5728\u4f7f\u7528\u65f6\u5176\u5b9e\u90fd\u662f\u901a\u8fc7API\u6765\u4e0e\u5927\u6a21\u578b\u4ea4\u4e92\u7684\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.f8a9b192", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0NzE2OTU4Nw==&mid=2247516240&idx=3&sn=a508d4aa0e115ec63a96d260eb88c58e&chksm=c2c637905e17ad499954021aaebf08a9933273e327459ba6d2614b80ff28ffdcf98120ac3ace#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0NzE2OTU4Nw==&mid=2247516240&idx=3&sn=a508d4aa0e115ec63a96d260eb88c58e&chksm=c2c637905e17ad499954021aaebf08a9933273e327459ba6d2614b80ff28ffdcf98120ac3ace#rd", "authors": ["\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66\u4fe1\u606f\u5b66\u9662"], "title": "\u5b66\u672f\u901f\u9012\u4e28\u8303\u4e3e\u6559\u6388\u56e2\u961f\u7814\u53d1\u7684\u9762\u5411\u6570\u636e\u79d1\u5b66\u7684\u00a0Agentic\u00a0<em class=\"highlight\">\u5927\u6a21\u578b</em>\u2014\u2014DeepAnalyze\u6b63\u5f0f\u53d1\u5e03", "comment": "Source: WeChat, Published: 2025-10-29 00:06:11", "summary": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u4ee5\u201c\u4ece\u5355\u4e00\u80fd\u529b\u5230\u590d\u5408\u80fd\u529b\u201d\u7684\u65b9\u5f0f\u5bf9\u5927\u6a21\u578b\u8fdb\u884c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff0c\u9010\u6b65\u63d0\u5347\u5927\u6a21\u578b\u5404\u9879\u80fd\u529b\u3002\u6b64\u5916\uff0c\u56e2\u961f\u8fd8\u63d0\u51fa\u4e86\u9762\u5411\u6570\u636e\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u6784\u5efa\u8d85\u8fc750\u4e07\u6761\u6570\u636e\u79d1\u5b66\u63a8\u7406\u4e0e\u73af\u5883\u4ea4\u4e92\u6570\u636e\uff0c\u5728\u5e9e\u5927\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d", "AI": {"tldr": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u4ee5\u201c\u4ece\u5355\u4e00\u80fd\u529b\u5230\u590d\u5408\u80fd\u529b\u201d\u7684\u65b9\u5f0f\u5bf9\u5927\u6a21\u578b\u8fdb\u884c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff0c\u9010\u6b65\u63d0\u5347\u5927\u6a21\u578b\u5404\u9879\u80fd\u529b\u3002\u6b64\u5916\uff0c\u56e2\u961f\u8fd8\u63d0\u51fa\u4e86\u9762\u5411\u6570\u636e\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u6784\u5efa\u8d85\u8fc750\u4e07\u6761\u6570\u636e\u79d1\u5b66\u63a8\u7406\u4e0e\u73af\u5883\u4ea4\u4e92\u6570\u636e\uff0c\u5728\u5e9e\u5927\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.15558ca5", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzODIzNzE0NQ==&mid=2654455972&idx=1&sn=a23867284d1eec14c85482b0fff9d55b&chksm=f306b15451a5cd317aafe7e75167bcecd225ad0cc4569b93c9fc21b8e31470da52ca6491c5fb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzODIzNzE0NQ==&mid=2654455972&idx=1&sn=a23867284d1eec14c85482b0fff9d55b&chksm=f306b15451a5cd317aafe7e75167bcecd225ad0cc4569b93c9fc21b8e31470da52ca6491c5fb#rd", "authors": ["\u7384\u59d0\u804aAGI"], "title": "\u4f01\u4e1a\u7ea7<em class=\"highlight\">\u5927\u6a21\u578b</em> AI \u5e94\u7528\u6784\u5efa\u6307\u5357", "comment": "Source: WeChat, Published: 2025-10-29 00:01:04", "summary": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u98de\u901f\u53d1\u5c55\uff0c\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u4e0e\u591a\u6a21\u6001\u80fd\u529b\u6301\u7eed\u7a81\u7834\uff0cAI \u5e94\u7528\u65e9\u5df2\u544a\u522b\u7b80\u5355\u5bf9\u8bdd\u95ee\u7b54\u7684\u521d\u7ea7\u9636\u6bb5\uff0c\u8fc8\u5165\u878d\u5408 RAG\u3001\u5de5\u4f5c\u6d41\u3001Agent \u7684\u590d\u6742\u7814\u53d1\u8303\u5f0f\u3002", "AI": {"tldr": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u98de\u901f\u53d1\u5c55\uff0c\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u4e0e\u591a\u6a21\u6001\u80fd\u529b\u6301\u7eed\u7a81\u7834\uff0cAI \u5e94\u7528\u65e9\u5df2\u544a\u522b\u7b80\u5355\u5bf9\u8bdd\u95ee\u7b54\u7684\u521d\u7ea7\u9636\u6bb5\uff0c\u8fc8\u5165\u878d\u5408 RAG\u3001\u5de5\u4f5c\u6d41\u3001Agent \u7684\u590d\u6742\u7814\u53d1\u8303\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.4e63ac0d", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk4ODgxOTExNA==&mid=2247485788&idx=1&sn=f3356ce1ef07bd2b4fb6951f13ec2762&chksm=c4614d3197f08ca8e0ad63bcbcd726a7fb3ca42f8b87ea173d3faa8d60b6e47a1efa065e6fd3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk4ODgxOTExNA==&mid=2247485788&idx=1&sn=f3356ce1ef07bd2b4fb6951f13ec2762&chksm=c4614d3197f08ca8e0ad63bcbcd726a7fb3ca42f8b87ea173d3faa8d60b6e47a1efa065e6fd3#rd", "authors": ["\u89c9\u9192AI\u65b0\u7eaa\u5143"], "title": "\u5f7b\u5e95\u641e\u61c2\u4e86\uff01\u4ec0\u4e48\u662f<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5fae\u8c03\uff1f<em class=\"highlight\">\u5927\u6a21\u578b</em>\u548c\u5fae\u8c03\u7684\u5173\u7cfb\uff1f", "comment": "Source: WeChat, Published: 2025-10-29 00:01:02", "summary": "\u4ec0\u4e48\u662f\u5fae\u8c03\uff1f\u5927\u6a21\u578b\u5fae\u8c03 \u6bcf\u5929\u62c6\u89e3\u4e00\u4e2aai\u77e5\u8bc6\u3002\u5982\u679c\u4f60\u662f\u4e00\u4e2a\u5f00\u53d1\u8005\uff0c\u624b\u91cc\u6709\u4e00\u4e2a\u5f3a\u5927\u7684\u8bed\u8a00\u6a21 \u578b\uff08llm\uff09\uff0c\u60f3\u7528\u5b83\u6765\u505a\u70b9\u5389\u5bb3\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982\u6587 \u672c\u5206\u7c7b\u3001\u667a\u80fd\u95ee\u7b54\uff0c\u6216\u8005\u8bc6\u522b\u6587\u672c\u91cc\u7684\u5173\u952e\u4fe1 \u606f\u3002", "AI": {"tldr": "\u4ec0\u4e48\u662f\u5fae\u8c03\uff1f\u5927\u6a21\u578b\u5fae\u8c03 \u6bcf\u5929\u62c6\u89e3\u4e00\u4e2aai\u77e5\u8bc6\u3002\u5982\u679c\u4f60\u662f\u4e00\u4e2a\u5f00\u53d1\u8005\uff0c\u624b\u91cc\u6709\u4e00\u4e2a\u5f3a\u5927\u7684\u8bed\u8a00\u6a21 \u578b\uff08llm\uff09\uff0c\u60f3\u7528\u5b83\u6765\u505a\u70b9\u5389\u5bb3\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982\u6587 \u672c\u5206\u7c7b\u3001\u667a\u80fd\u95ee\u7b54\uff0c\u6216\u8005\u8bc6\u522b\u6587\u672c\u91cc\u7684\u5173\u952e\u4fe1 \u606f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.5b6cb792", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2MzA4MzYzMQ==&mid=2650775338&idx=1&sn=f34f3dcd917d07da8a3a14e44fab1cc9&chksm=f3d938a4d74401120d81644a6328fc46cf347be39cb9d3b503dfd2eabaca0019582de9b80afc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2MzA4MzYzMQ==&mid=2650775338&idx=1&sn=f34f3dcd917d07da8a3a14e44fab1cc9&chksm=f3d938a4d74401120d81644a6328fc46cf347be39cb9d3b503dfd2eabaca0019582de9b80afc#rd", "authors": ["\u884c\u8005\u5434\u6c5f"], "title": "\u4e24\u5927\u8d5b\u573a\u540c\u65f6\u51fa\u51fb \u4e2d\u56fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8ba9\u5168\u7403\u6295\u8d44\u4eba\u6298\u670d\uff01", "comment": "Source: WeChat, Published: 2025-10-28 16:02:53", "summary": "\u4e2d\u56fd\u7387\u5148\u51fa\u53f0\u300a\u91d1\u878d\u5927\u6a21\u578b\u5e94\u7528\u6307\u5f15\u300b\uff0c\u8981\u6c42\u6a21\u578b\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u8fbe85%\u4ee5\u4e0a\u3002\u8fd9\u79cd\u76d1\u7ba1\u5012\u903c\u673a\u5236\u4fc3\u4f7fDeepSeek\u5f00\u53d1\u51fa\"\u51b3\u7b56\u6eaf\u6e90\"\u529f\u80fd\uff0c\u6bcf\u7b14\u4ea4\u6613\u53ef\u751f\u6210\u5305\u542b12\u4e2a\u56e0\u5b50\u7684\u5206\u6790\u62a5\u544a\u3002", "AI": {"tldr": "\u4e2d\u56fd\u7387\u5148\u51fa\u53f0\u300a\u91d1\u878d\u5927\u6a21\u578b\u5e94\u7528\u6307\u5f15\u300b\uff0c\u8981\u6c42\u6a21\u578b\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u8fbe85%\u4ee5\u4e0a\u3002\u8fd9\u79cd\u76d1\u7ba1\u5012\u903c\u673a\u5236\u4fc3\u4f7fDeepSeek\u5f00\u53d1\u51fa\"\u51b3\u7b56\u6eaf\u6e90\"\u529f\u80fd\uff0c\u6bcf\u7b14\u4ea4\u6613\u53ef\u751f\u6210\u5305\u542b12\u4e2a\u56e0\u5b50\u7684\u5206\u6790\u62a5\u544a\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.6f71b6a0", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNTEyODQwMw==&mid=2247489243&idx=1&sn=dcf7658921bd188964225c28dd4db68a&chksm=964edabdc71e1a55648997485d57a0a50dec994547979a1880103de6f98a2d8da90f97bf55bb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNTEyODQwMw==&mid=2247489243&idx=1&sn=dcf7658921bd188964225c28dd4db68a&chksm=964edabdc71e1a55648997485d57a0a50dec994547979a1880103de6f98a2d8da90f97bf55bb#rd", "authors": ["\u96ea\u5cf0\u5927\u6570\u636e"], "title": "\u4ece\u201c\u6572\u4ee3\u7801\u201d\u5230\u201c\u8bf4\u4eba\u8bdd\u201d\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0eText-to-SQL\u91cd\u5851\u516c\u5b89\u6570\u636e\u4fa6\u67e5\u5206\u6790\u601d\u8def\u63a2\u7d22", "comment": "Source: WeChat, Published: 2025-10-28 14:56:45", "summary": "\u5927\u6a21\u578b\uff08llm\uff09\u7684\u52a0\u5165\uff0c\u8ba9\u5b83\u201c\u6d3b\u201d\u4e86\u2014\u2014\u9760\u7684\u662f\u4e09\u5927\u5173\u952e\u80fd\u529b\u30021. \u542c\u5f97\u61c2\u201c\u9ed1\u8bdd\u201d\uff1a\u5f3a\u5927\u7684\u610f\u56fe\u7406\u89e3\uff08nlu\uff09DeepSeek\u3001Qwen\u7b49\u5927\u6a21\u578b\u7ecf\u8fc7\u6d77\u91cf\u8bed\u6599\u8bad\u7ec3\uff0c\u5177\u5907\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\uff08llm\uff09\u7684\u52a0\u5165\uff0c\u8ba9\u5b83\u201c\u6d3b\u201d\u4e86\u2014\u2014\u9760\u7684\u662f\u4e09\u5927\u5173\u952e\u80fd\u529b\u30021. \u542c\u5f97\u61c2\u201c\u9ed1\u8bdd\u201d\uff1a\u5f3a\u5927\u7684\u610f\u56fe\u7406\u89e3\uff08nlu\uff09DeepSeek\u3001Qwen\u7b49\u5927\u6a21\u578b\u7ecf\u8fc7\u6d77\u91cf\u8bed\u6599\u8bad\u7ec3\uff0c\u5177\u5907\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.d0f370fc", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDY3Mzk3MA==&mid=2247484403&idx=1&sn=60fb54ed8f037599dea430471b0ca94c&chksm=c5ca5ae889355026e1541045510ab9c4a4c6d2e033d4a35c77114570ca309e78351128f115a5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDY3Mzk3MA==&mid=2247484403&idx=1&sn=60fb54ed8f037599dea430471b0ca94c&chksm=c5ca5ae889355026e1541045510ab9c4a4c6d2e033d4a35c77114570ca309e78351128f115a5#rd", "authors": ["\u6570\u667a\u7b51\u57ce"], "title": "2025\u5fc5\u6536\u85cf\uff01\u4e3b\u6d41 AI <em class=\"highlight\">\u5927\u6a21\u578b</em>\u5168\u89e3\u6790", "comment": "Source: WeChat, Published: 2025-10-28 14:16:43", "summary": "2.\u91c7\u7528\u7684\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u6a21\u578b\u5b8c\u5168\u5f00\u6e90\uff0c\u9002\u914d\u4e0d\u540c\u7b97\u529b\u9700\u6c42\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86AI\u5e94\u7528\u95e8\u69db\uff0c\u8d4b\u80fd\u5f00\u6e90\u793e\u533a\u53d1\u5c55\u3002", "AI": {"tldr": "2.\u91c7\u7528\u7684\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u6a21\u578b\u5b8c\u5168\u5f00\u6e90\uff0c\u9002\u914d\u4e0d\u540c\u7b97\u529b\u9700\u6c42\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86AI\u5e94\u7528\u95e8\u69db\uff0c\u8d4b\u80fd\u5f00\u6e90\u793e\u533a\u53d1\u5c55\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.a882bf85", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247712859&idx=4&sn=05b7b46d1b06ab2bbed167a7644af786&chksm=ed0a03b2f2e33d4499e88ff6b4315db03e9079fd8ea9c122a32d9d819300a46ad632dc5dcae1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247712859&idx=4&sn=05b7b46d1b06ab2bbed167a7644af786&chksm=ed0a03b2f2e33d4499e88ff6b4315db03e9079fd8ea9c122a32d9d819300a46ad632dc5dcae1#rd", "authors": ["\u6781\u5e02\u5e73\u53f0"], "title": "OpenAI\u524dCTO Mira Murati\u56e2\u961f\u53c8\u653e\u5927\u62db\uff0c\u8ba9<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8bad\u7ec3\u6210\u672c\u66b4\u964d10\u500d", "comment": "Source: WeChat, Published: 2025-10-28 14:00:34", "summary": "\u4e00\u79cd\u80fd\u4ee5 1/10 \u6210\u672c\u8fbe\u5230\u5f3a\u5316\u5b66\u4e60\u540c\u7b49\u6548\u679c\u7684\u5927\u6a21\u578b\u540e\u8bad\u7ec3\u65b0\u65b9\u6cd5\u3002mira murati @miramurati \u00b7 14h combining the benefits of rl and sft with on-policy distillation\uff0c a promising approach for training small models for domain performance and continual learning.\u3002", "AI": {"tldr": "\u4e00\u79cd\u80fd\u4ee5 1/10 \u6210\u672c\u8fbe\u5230\u5f3a\u5316\u5b66\u4e60\u540c\u7b49\u6548\u679c\u7684\u5927\u6a21\u578b\u540e\u8bad\u7ec3\u65b0\u65b9\u6cd5\u3002mira murati @miramurati \u00b7 14h combining the benefits of rl and sft with on-policy distillation\uff0c a promising approach for training small models for domain performance a...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
