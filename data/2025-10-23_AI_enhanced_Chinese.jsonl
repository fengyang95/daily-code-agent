{"id": "2510.18895", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18895", "abs": "https://arxiv.org/abs/2510.18895", "authors": ["Santhosh Kumar Ravindran"], "title": "CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation", "comment": "12 pages", "summary": "We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)\narchitecture that integrates affective signals to enhance code generation in\nlarge language models (LLMs). Motivated by human and animal learning where\nembarrassment from mistakes drives rapid correction, as observed in training a\npuppy to avoid repeating errors after a single scolding CosmoCore tags code\ngeneration trajectories with valence and surprise using a lightweight\nmulti-layer perceptron (MLP). High-negative valence (cringe) episodes, such as\nbuggy code outputs, are prioritized in a Dream Queue for five-fold replay\nduring off-policy updates, while low-surprise successes are pruned to prevent\noverconfidence and buffer bloat. Evaluated on code generation benchmarks like\nHumanEval and BigCodeBench, alongside simulations with a custom data pipeline\nenvironment, CosmoCore reduces hallucinated code (e.g., syntax errors or\nlogical bugs) by 48\\% and accelerates self-correction by 45\\%. Local\nexperiments using Hugging Face models in a PySpark environment validate these\ngains, with code snippets provided for replication. Ablations confirm valence\ntagging boosts curiosity in exploration, and pruning mitigates inefficiency.\nThis framework extends RL from human feedback (RLHF) for more emotionally aware\ncode assistants, with applications in IDEs and data pipelines. Code and the\ncustom mini-world simulation are released.", "AI": {"tldr": "CosmoCore\u662f\u4e00\u4e2a\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u60c5\u611f\u4fe1\u53f7\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002\u5b83\u4f7f\u7528\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u673a\u6807\u8bb0\u4ee3\u7801\u751f\u6210\u8f68\u8ff9\u7684\u6548\u4ef7\u548c\u60ca\u559c\u5ea6\uff0c\u4f18\u5148\u91cd\u653e\u9ad8\u8d1f\u6548\u4ef7\uff08\u5c34\u5c2c\uff09\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u4fee\u526a\u4f4e\u60ca\u559c\u5ea6\u7684\u6210\u529f\u6848\u4f8b\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\u4ee3\u780148%\uff0c\u52a0\u901f\u81ea\u6211\u4fee\u6b6345%\u3002", "motivation": "\u53d7\u4eba\u7c7b\u548c\u52a8\u7269\u5b66\u4e60\u7684\u542f\u53d1\uff0c\u7279\u522b\u662f\u4ece\u9519\u8bef\u4e2d\u611f\u5230\u5c34\u5c2c\u4f1a\u9a71\u52a8\u5feb\u901f\u4fee\u6b63\u7684\u884c\u4e3a\u6a21\u5f0f\uff08\u5982\u8bad\u7ec3\u5c0f\u72d7\u65f6\u4e00\u6b21\u8d23\u9a82\u5c31\u80fd\u907f\u514d\u91cd\u590d\u9519\u8bef\uff09\uff0c\u65e8\u5728\u5c06\u60c5\u611f\u4fe1\u53f7\u6574\u5408\u5230\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u673a\u6807\u8bb0\u4ee3\u7801\u751f\u6210\u8f68\u8ff9\u7684\u6548\u4ef7\u548c\u60ca\u559c\u5ea6\uff1b\u5c06\u9ad8\u8d1f\u6548\u4ef7\uff08\u5c34\u5c2c\uff09\u7684\u5931\u8d25\u6848\u4f8b\u4f18\u5148\u653e\u5165Dream Queue\u8fdb\u884c\u4e94\u500d\u91cd\u653e\uff1b\u4fee\u526a\u4f4e\u60ca\u559c\u5ea6\u7684\u6210\u529f\u6848\u4f8b\u4ee5\u9632\u6b62\u8fc7\u5ea6\u81ea\u4fe1\u548c\u7f13\u51b2\u533a\u81a8\u80c0\u3002", "result": "\u5728HumanEval\u548cBigCodeBench\u7b49\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCosmoCore\u5c06\u5e7b\u89c9\u4ee3\u7801\uff08\u5982\u8bed\u6cd5\u9519\u8bef\u6216\u903b\u8f91\u9519\u8bef\uff09\u51cf\u5c11\u4e8648%\uff0c\u81ea\u6211\u4fee\u6b63\u901f\u5ea6\u63d0\u9ad8\u4e8645%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6269\u5c55\u4e86\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\uff0c\u7528\u4e8e\u5f00\u53d1\u66f4\u5177\u60c5\u611f\u610f\u8bc6\u7684\u4ee3\u7801\u52a9\u624b\uff0c\u5728IDE\u548c\u6570\u636e\u7ba1\u9053\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18892", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.18892", "abs": "https://arxiv.org/abs/2510.18892", "authors": ["Richard J. Young", "Brandon Gillins", "Alice M. Matthews"], "title": "When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs", "comment": "21 pages, 3 figures, 5 tables. Comprehensive evaluation of 256 LLMs\n  on instruction-following tasks", "summary": "Despite widespread deployment of Large Language Models, systematic evaluation\nof instruction-following capabilities remains challenging. While comprehensive\nbenchmarks exist, focused assessments that quickly diagnose specific\ninstruction adherence patterns are valuable. As newer models may be trained on\nexisting benchmarks, novel evaluation approaches are needed to assess genuine\ncapabilities rather than memorized performance. This paper presents a\nstreamlined evaluation framework using twenty carefully designed prompts to\nassess LLM instruction-following across diverse task categories. We demonstrate\nthis framework through a large-scale empirical study conducted on October 14,\n2025, testing 256 verified working models from 331 available via OpenRouter. To\nensure methodological rigor and prevent selection bias, we first verified each\nmodel's basic functionality before inclusion. Unlike large-scale benchmarks\nrequiring extensive computational resources, our approach offers a practical\ndiagnostic tool researchers and practitioners can readily apply. Our\nmethodology builds upon verifiable instructions while introducing a compact\ntest suite balancing comprehensiveness with efficiency. Each prompt targets\ndistinct aspects of instruction following, including format compliance, content\nconstraints, logical sequencing, and multi-step task execution. We evaluate\nmodels from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and\nemerging implementations (Qwen, DeepSeek, community models), providing\ncomparative performance analysis. Our findings reveal consistent failure modes\nand identify specific instruction types posing particular challenges. This work\ncontributes both a practical evaluation tool and one of the most comprehensive\nempirical analyses of instruction-following capabilities across the\ncontemporary LLM landscape.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u752820\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u6765\u8bc4\u4f30LLM\u5728\u591a\u6837\u5316\u4efb\u52a1\u7c7b\u522b\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u6d4b\u8bd5\u4e86256\u4e2a\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u7cfb\u7edf\u8bc4\u4f30\u5176\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u57fa\u51c6\u53ef\u80fd\u88ab\u65b0\u6a21\u578b\u8bad\u7ec3\u8fc7\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u8bc4\u4f30\u771f\u5b9e\u80fd\u529b\u800c\u975e\u8bb0\u5fc6\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b20\u4e2a\u9488\u5bf9\u4e0d\u540c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u7684\u63d0\u793a\uff0c\u5305\u62ec\u683c\u5f0f\u5408\u89c4\u3001\u5185\u5bb9\u7ea6\u675f\u3001\u903b\u8f91\u6392\u5e8f\u548c\u591a\u6b65\u9aa4\u4efb\u52a1\u6267\u884c\u3002\u5728256\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6301\u7eed\u5b58\u5728\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u8bc6\u522b\u51fa\u7279\u5b9a\u6307\u4ee4\u7c7b\u578b\u5e26\u6765\u7684\u6311\u6218\u3002\u63d0\u4f9b\u4e86\u4e3b\u8981\u63d0\u4f9b\u5546\u548c\u65b0\u5174\u5b9e\u73b0\u7684\u6bd4\u8f83\u6027\u80fd\u5206\u6790\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u65e2\u8d21\u732e\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u4e5f\u63d0\u4f9b\u4e86\u5f53\u4ee3LLM\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u6700\u5168\u9762\u7684\u5b9e\u8bc1\u5206\u6790\u4e4b\u4e00\u3002", "topic": "agent analysis"}}
{"id": "2510.19274", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19274", "abs": "https://arxiv.org/abs/2510.19274", "authors": ["Saurabh Chauhan", "Zeeshan Rasheed", "Malik Abdul Sami", "Kai-Kristian Kemell", "Muhammad Waseem", "Zheying Zhang", "Jussi Rasku", "Mika Saari", "Pekka Abrahamsson"], "title": "From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems", "comment": "9 Figures, 6Tables", "summary": "This paper presents a system that uses Large Language Models (LLMs)-based\nagents to automate the API-first development of RESTful microservices. This\nsystem helps to create an OpenAPI specification, generate server code from it,\nand refine the code through a feedback loop that analyzes execution logs and\nerror messages. The integration of log analysis enables the LLM to detect and\naddress issues efficiently, reducing the number of iterations required to\nproduce functional and robust services. This study's main goal is to advance\nAPI-first development automation for RESTful web services and test the\ncapability of LLM-based multi-agent systems in supporting the API-first\ndevelopment approach. To test the proposed system's potential, we utilized the\nPRAB benchmark. The results indicate that if we keep the OpenAPI specification\nsmall and focused, LLMs are capable of generating complete functional code with\nbusiness logic that aligns to the specification. The code for the system is\npublicly available at https://github.com/sirbh/code-gen", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316RESTful\u5fae\u670d\u52a1\u7684API\u4f18\u5148\u5f00\u53d1\uff0c\u5305\u62ec\u751f\u6210OpenAPI\u89c4\u8303\u3001\u751f\u6210\u670d\u52a1\u5668\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6267\u884c\u65e5\u5fd7\u548c\u9519\u8bef\u6d88\u606f\u7684\u53cd\u9988\u5faa\u73af\u6765\u4f18\u5316\u4ee3\u7801\u3002", "motivation": "\u63a8\u8fdbRESTful Web\u670d\u52a1\u7684API\u4f18\u5148\u5f00\u53d1\u81ea\u52a8\u5316\uff0c\u6d4b\u8bd5\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u652f\u6301API\u4f18\u5148\u5f00\u53d1\u65b9\u6cd5\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u4eceOpenAPI\u89c4\u8303\u751f\u6210\u670d\u52a1\u5668\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6267\u884c\u65e5\u5fd7\u548c\u9519\u8bef\u6d88\u606f\u7684\u53cd\u9988\u5faa\u73af\u6765\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u3002", "result": "\u4f7f\u7528PRAB\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5f53\u4fdd\u6301OpenAPI\u89c4\u8303\u5c0f\u800c\u4e13\u6ce8\u65f6\uff0cLLM\u80fd\u591f\u751f\u6210\u7b26\u5408\u89c4\u8303\u7684\u5b8c\u6574\u529f\u80fd\u4ee3\u7801\u548c\u4e1a\u52a1\u903b\u8f91\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u652f\u6301API\u4f18\u5148\u5f00\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u65e5\u5fd7\u5206\u6790\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\uff0c\u751f\u6210\u529f\u80fd\u5f3a\u5927\u4e14\u5065\u58ee\u7684\u670d\u52a1\u3002", "topic": "code agent"}}
{"id": "2510.18924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18924", "abs": "https://arxiv.org/abs/2510.18924", "authors": ["Omar El mansouri", "Mohamed El Amine Seddik", "Salem Lahlou"], "title": "Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) or verifiable rewards\n(RLVR), the standard paradigm for aligning LLMs or building recent SOTA\nreasoning models, is highly sensitive to noise from inconsistent or erroneous\nrewards. Yet, the interaction between such noise and widely used group-based\npolicy optimization methods remains underexplored. We introduce a noise-robust\nGroup Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO)\nframework that explicitly models reward corruption as Bernoulli noise. Our\nmethod applies noise correction after estimating reward flip probabilities to\ndebias the learning signal, yielding provably unbiased gradient estimates.\nTheoretical analysis shows that group-based methods inherently mitigate\nindividual-level noise, and our correction strategy amplifies this robustness.\nEmpirically, we observe consistent improvements across math and code tasks when\napplying our noise correction to standard reward model usage, with particular\ngains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code\ntasks under realistic reward model conditions. This work bridges label-noise\ncorrection from supervised learning with modern RLHF, offering both theoretical\ninsights and a practical algorithm for noisy real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86GRPO\u548cDr.GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u5956\u52b1\u566a\u58f0\u4e3a\u4f2f\u52aa\u5229\u566a\u58f0\u5e76\u8fdb\u884c\u566a\u58f0\u6821\u6b63\uff0c\u5728RLHF\u4e2d\u63d0\u9ad8\u5bf9\u566a\u58f0\u5956\u52b1\u7684\u9c81\u68d2\u6027\u3002", "motivation": "RLHF\u6216RLVR\u5bf9\u4e0d\u4e00\u81f4\u6216\u9519\u8bef\u5956\u52b1\u7684\u566a\u58f0\u975e\u5e38\u654f\u611f\uff0c\u4f46\u566a\u58f0\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u566a\u58f0\u9c81\u68d2\u7684GRPO\u548cDr.GRPO\u6846\u67b6\uff0c\u5c06\u5956\u52b1\u8150\u8d25\u5efa\u6a21\u4e3a\u4f2f\u52aa\u5229\u566a\u58f0\uff0c\u5728\u4f30\u8ba1\u5956\u52b1\u7ffb\u8f6c\u6982\u7387\u540e\u8fdb\u884c\u566a\u58f0\u6821\u6b63\u4ee5\u6d88\u9664\u5b66\u4e60\u4fe1\u53f7\u7684\u504f\u5dee\u3002", "result": "\u5728\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u4e0a\u5e94\u7528\u566a\u58f0\u6821\u6b63\u540e\u83b7\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe6.7\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u4ee3\u7801\u4efb\u52a1\u4e0a\u63d0\u53471.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5c06\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u6821\u6b63\u4e0e\u73b0\u4ee3RLHF\u76f8\u7ed3\u5408\uff0c\u4e3a\u566a\u58f0\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\u548c\u5b9e\u7528\u7b97\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18927", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18927", "abs": "https://arxiv.org/abs/2510.18927", "authors": ["Zhiheng Xi", "Xin Guo", "Yang Nan", "Enyu Zhou", "Junrui Shen", "Wenxiang Chen", "Jiaqi Liu", "Jixuan Huang", "Zhihao Zhang", "Honglin Guo", "Xun Deng", "Zhikai Lei", "Miao Zheng", "Guoteng Wang", "Shuo Zhang", "Peng Sun", "Rui Zheng", "Hang Yan", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping", "comment": "Preprint", "summary": "Reinforcement learning (RL) has recently become the core paradigm for\naligning and strengthening large language models (LLMs). Yet, applying RL in\noff-policy settings--where stale data from past policies are used for\ntraining--improves sample efficiency, but remains challenging: policy entropy\ndeclines sharply, optimization often becomes unstable and may even collapse.\nThrough theoretical and empirical analysis, we identify two key insights: (i)\nan imbalance in optimization, where negative-advantage samples dominate the\npolicy gradient, suppressing useful behaviors and risking gradient explosions;\nand (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping\nmechanism in PPO-like objectives systematically blocks entropy-increasing\nupdates, thereby driving the policy toward over-exploitation at the expense of\nexploration. Building on these insights, we propose BAlanced Policy\nOptimization with Adaptive Clipping (BAPO), a simple yet effective method that\ndynamically adjusts clipping bounds to adaptively re-balance positive and\nnegative contributions, preserve entropy, and stabilize RL optimization. Across\ndiverse off-policy scenarios--including sample replay and partial rollout--BAPO\nachieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025\nbenchmarks, our 7B BAPO model surpasses open-source counterparts such as\nSkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art\nresults among models of the same scale but also outperforms leading proprietary\nsystems like o3-mini and Gemini-2.5-Flash-Thinking.", "AI": {"tldr": "\u63d0\u51faBAPO\u65b9\u6cd5\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7b56\u7565\u71b5\u4e0b\u964d\u3001\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u88c1\u526a\u8fb9\u754c\u5e73\u8861\u6b63\u8d1f\u6837\u672c\u8d21\u732e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728LLM\u5bf9\u9f50\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9762\u4e34\u7b56\u7565\u71b5\u6025\u5267\u4e0b\u964d\u3001\u4f18\u5316\u4e0d\u7a33\u5b9a\u751a\u81f3\u5d29\u6e83\u7684\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u8d1f\u4f18\u52bf\u6837\u672c\u4e3b\u5bfc\u548c\u56fa\u5b9a\u88c1\u526a\u673a\u5236\u6291\u5236\u63a2\u7d22\u7684\u95ee\u9898", "method": "\u63d0\u51faBAPO\u65b9\u6cd5\uff0c\u57fa\u4e8e\u71b5\u88c1\u526a\u89c4\u5219\uff0c\u52a8\u6001\u8c03\u6574PPO\u76ee\u6807\u7684\u88c1\u526a\u8fb9\u754c\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u6b63\u8d1f\u6837\u672c\u8d21\u732e\uff0c\u4fdd\u6301\u7b56\u7565\u71b5\u5e76\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b", "result": "\u5728AIME 2024\u548c2025\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B BAPO\u6a21\u578b\u8d85\u8d8a\u5f00\u6e90\u5bf9\u624bSkyWork-OR1-7B\uff0c32B\u6a21\u578b\u5728\u540c\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\uff0c\u751a\u81f3\u4f18\u4e8eo3-mini\u548cGemini-2.5-Flash-Thinking\u7b49\u4e13\u6709\u7cfb\u7edf", "conclusion": "BAPO\u901a\u8fc7\u52a8\u6001\u88c1\u526a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u4e2d\u7684\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u7a33\u5b9a\u4e14\u6570\u636e\u9ad8\u6548\u7684\u8bad\u7ec3", "topic": "agentic reinforcement learning"}}
{"id": "2510.19299", "categories": ["cs.AI", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.19299", "abs": "https://arxiv.org/abs/2510.19299", "authors": ["Philipp J. Schneider", "Lin Tian", "Marian-Andrei Rizoiu"], "title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties", "comment": null, "summary": "Can large language model (LLM) agents reproduce the complex social dynamics\nthat characterize human online behavior -- shaped by homophily, reciprocity,\nand social validation -- and what memory and learning mechanisms enable such\ndynamics to emerge? We present a multi-agent LLM simulation framework in which\nagents repeatedly interact, evaluate one another, and adapt their behavior\nthrough in-context learning accelerated by a coaching signal. To model human\nsocial behavior, we design behavioral reward functions that capture core\ndrivers of online engagement, including social interaction, information\nseeking, self-presentation, coordination, and emotional support. These rewards\nalign agent objectives with empirically observed user motivations, enabling the\nstudy of how network structures and group formations emerge from individual\ndecision-making. Our experiments show that coached LLM agents develop stable\ninteraction patterns and form emergent social ties, yielding network structures\nthat mirror properties of real online communities. By combining behavioral\nrewards with in-context adaptation, our framework establishes a principled\ntestbed for investigating collective dynamics in LLM populations and reveals\nhow artificial agents may approximate or diverge from human-like social\nbehavior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u5956\u52b1\u51fd\u6570\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u6a21\u62df\u4eba\u7c7b\u5728\u7ebf\u793e\u4ea4\u52a8\u6001\uff0c\u5305\u62ec\u540c\u8d28\u6027\u3001\u4e92\u60e0\u6027\u548c\u793e\u4f1a\u9a8c\u8bc1\u7b49\u7279\u5f81\u3002", "motivation": "\u7814\u7a76LLM\u667a\u80fd\u4f53\u662f\u5426\u80fd\u91cd\u73b0\u4eba\u7c7b\u5728\u7ebf\u884c\u4e3a\u7684\u590d\u6742\u793e\u4ea4\u52a8\u6001\uff0c\u4ee5\u53ca\u4ec0\u4e48\u8bb0\u5fc6\u548c\u5b66\u4e60\u673a\u5236\u80fd\u4f7f\u8fd9\u79cd\u52a8\u6001\u51fa\u73b0\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53LLM\u6a21\u62df\u6846\u67b6\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u884c\u4e3a\u5956\u52b1\u51fd\u6570\uff08\u6355\u6349\u793e\u4ea4\u4e92\u52a8\u3001\u4fe1\u606f\u5bfb\u6c42\u3001\u81ea\u6211\u5448\u73b0\u3001\u534f\u8c03\u548c\u60c5\u611f\u652f\u6301\u7b49\u6838\u5fc3\u9a71\u52a8\u56e0\u7d20\uff09\u8fdb\u884c\u91cd\u590d\u4e92\u52a8\u548c\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6559\u7ec3\u4fe1\u53f7\u6765\u9002\u5e94\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u7ecf\u8fc7\u6559\u7ec3\u7684LLM\u667a\u80fd\u4f53\u53d1\u5c55\u51fa\u7a33\u5b9a\u7684\u4e92\u52a8\u6a21\u5f0f\u5e76\u5f62\u6210\u6d8c\u73b0\u7684\u793e\u4ea4\u8054\u7cfb\uff0c\u4ea7\u751f\u7684\u7f51\u7edc\u7ed3\u6784\u53cd\u6620\u4e86\u771f\u5b9e\u5728\u7ebf\u793e\u533a\u7684\u7279\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76LLM\u7fa4\u4f53\u4e2d\u7684\u96c6\u4f53\u52a8\u6001\u5efa\u7acb\u4e86\u539f\u5219\u6027\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u4f53\u5982\u4f55\u8fd1\u4f3c\u6216\u504f\u79bb\u7c7b\u4eba\u793e\u4ea4\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2510.19600", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19600", "abs": "https://arxiv.org/abs/2510.19600", "authors": ["Qianli Ma", "Siyu Wang", "Yilin Chen", "Yinhao Tang", "Yixiang Yang", "Chang Guo", "Bingjie Gao", "Zhening Xing", "Yanan Sun", "Zhipeng Zhang"], "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1", "comment": null, "summary": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\n$\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct $\\textbf{PageBench}$, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\$0.1. Code and dataset will be released at\n$\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.", "AI": {"tldr": "AutoPage\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u5c06\u5b66\u672f\u8bba\u6587\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u7f51\u9875\uff0c\u901a\u8fc7\u5206\u5c42\u534f\u4f5c\u6d41\u7a0b\u89e3\u51b3\u4f20\u7edf\u7f51\u9875\u6784\u5efa\u7684\u7e41\u7410\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u4eba\u5458\u5728\u6784\u5efa\u9879\u76ee\u7f51\u9875\u65f6\u9762\u4e34\u624b\u52a8\u3001\u91cd\u590d\u6027\u7684\u5de5\u4f5c\u8d1f\u62c5\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u5de5\u5177\u65e0\u6cd5\u5904\u7406\u7f51\u9875\u7684\u52a8\u6001\u4ea4\u4e92\u7279\u6027\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u8bba\u6587\u5230\u7f51\u9875\u7684\u521b\u5efa\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4ece\u53d9\u4e8b\u89c4\u5212\u5230\u591a\u6a21\u6001\u5185\u5bb9\u751f\u6210\u518d\u5230\u4ea4\u4e92\u5f0f\u6e32\u67d3\u7684\u7c97\u5230\u7ec6\u7ba1\u9053\uff0c\u5e76\u8bbe\u6709\u4e13\u95e8\u7684\u68c0\u67e5\u5668\u667a\u80fd\u4f53\u9a8c\u8bc1\u5185\u5bb9\u51c6\u786e\u6027\u3002", "result": "AutoPage\u80fd\u591f\u572815\u5206\u949f\u5185\u4ee5\u4f4e\u4e8e0.1\u7f8e\u5143\u7684\u6210\u672c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u89c6\u89c9\u5438\u5f15\u529b\u5f3a\u7684\u7f51\u9875\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u6b64\u7c7b\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5PageBench\u3002", "conclusion": "AutoPage\u5c06\u7cfb\u7edf\u4ece\u5355\u7eaf\u5de5\u5177\u8f6c\u53d8\u4e3a\u5f3a\u5927\u7684\u534f\u4f5c\u52a9\u624b\uff0c\u901a\u8fc7\u5206\u5c42\u534f\u4f5c\u548c\u9a8c\u8bc1\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7f51\u9875\u81ea\u52a8\u5316\u521b\u5efa\u7684\u6311\u6218\u3002", "topic": "swe application"}}
{"id": "2510.19314", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19314", "abs": "https://arxiv.org/abs/2510.19314", "authors": ["Jinwu Hu", "Zihao Lian", "Zhiquan Wen", "Chenghao Li", "Guohao Chen", "Xutao Wen", "Bin Xiao", "Mingkui Tan"], "title": "Continual Knowledge Adaptation for Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Reinforcement Learning enables agents to learn optimal behaviors through\ninteractions with environments. However, real-world environments are typically\nnon-stationary, requiring agents to continuously adapt to new tasks and\nchanging conditions. Although Continual Reinforcement Learning facilitates\nlearning across multiple tasks, existing methods often suffer from catastrophic\nforgetting and inefficient knowledge utilization. To address these challenges,\nwe propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),\nwhich enables the accumulation and effective utilization of historical\nknowledge. Specifically, we introduce a Continual Knowledge Adaptation\nstrategy, which involves maintaining a task-specific knowledge vector pool and\ndynamically using historical knowledge to adapt the agent to new tasks. This\nprocess mitigates catastrophic forgetting and enables efficient knowledge\ntransfer across tasks by preserving and adapting critical model parameters.\nAdditionally, we propose an Adaptive Knowledge Merging mechanism that combines\nsimilar knowledge vectors to address scalability challenges, reducing memory\nrequirements while ensuring the retention of essential knowledge. Experiments\non three benchmarks demonstrate that the proposed CKA-RL outperforms\nstate-of-the-art methods, achieving an improvement of 4.20% in overall\nperformance and 8.02% in forward transfer. The source code is available at\nhttps://github.com/Fhujinwu/CKA-RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86CKA-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u77e5\u8bc6\u9002\u5e94\u7b56\u7565\u548c\u81ea\u9002\u5e94\u77e5\u8bc6\u5408\u5e76\u673a\u5236\uff0c\u89e3\u51b3\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u77e5\u8bc6\u5229\u7528\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u901a\u5e38\u662f\u975e\u5e73\u7a33\u7684\uff0c\u9700\u8981\u667a\u80fd\u4f53\u6301\u7eed\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u53d8\u5316\u6761\u4ef6\u3002\u73b0\u6709\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u77e5\u8bc6\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6301\u7eed\u77e5\u8bc6\u9002\u5e94\u7b56\u7565\uff0c\u7ef4\u62a4\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u5411\u91cf\u6c60\uff0c\u52a8\u6001\u4f7f\u7528\u5386\u53f2\u77e5\u8bc6\u9002\u5e94\u65b0\u4efb\u52a1\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u77e5\u8bc6\u5408\u5e76\u673a\u5236\uff0c\u5408\u5e76\u76f8\u4f3c\u77e5\u8bc6\u5411\u91cf\u4ee5\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cCKA-RL\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u53474.20%\uff0c\u524d\u5411\u8fc1\u79fb\u63d0\u53478.02%\u3002", "conclusion": "CKA-RL\u80fd\u6709\u6548\u79ef\u7d2f\u548c\u5229\u7528\u5386\u53f2\u77e5\u8bc6\uff0c\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u7684\u9ad8\u6548\u77e5\u8bc6\u8fc1\u79fb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18939", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18939", "abs": "https://arxiv.org/abs/2510.18939", "authors": ["Howard Yen", "Ashwin Paranjape", "Mengzhou Xia", "Thejas Venkatesh", "Jack Hessel", "Danqi Chen", "Yuhao Zhang"], "title": "Lost in the Maze: Overcoming Context Limitations in Long-Horizon Agentic Search", "comment": "Code and data are available here: https://github.com/howard-yen/SLIM", "summary": "Long-horizon agentic search requires iteratively exploring the web over long\ntrajectories and synthesizing information across many sources, and is the\nfoundation for enabling powerful applications like deep research systems. In\nthis work, we show that popular agentic search frameworks struggle to scale to\nlong trajectories primarily due to context limitations-they accumulate long,\nnoisy content, hit context window and tool budgets, or stop early. Then, we\nintroduce SLIM (Simple Lightweight Information Management), a simple framework\nthat separates retrieval into distinct search and browse tools, and\nperiodically summarizes the trajectory, keeping context concise while enabling\nlonger, more focused searches. On long-horizon tasks, SLIM achieves comparable\nperformance at substantially lower cost and with far fewer tool calls than\nstrong open-source baselines across multiple base models. Specifically, with o3\nas the base model, SLIM achieves 56% on BrowseComp and 31% on HLE,\noutperforming all open-source frameworks by 8 and 4 absolute points,\nrespectively, while incurring 4-6x fewer tool calls. Finally, we release an\nautomated fine-grained trajectory analysis pipeline and error taxonomy for\ncharacterizing long-horizon agentic search frameworks; SLIM exhibits fewer\nhallucinations than prior systems. We hope our analysis framework and simple\ntool design inform future long-horizon agents.", "AI": {"tldr": "SLIM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4fe1\u606f\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u641c\u7d22\u548c\u6d4f\u89c8\u5de5\u5177\u5e76\u5b9a\u671f\u603b\u7ed3\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u957f\u8f68\u8ff9\u667a\u80fd\u641c\u7d22\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u548c\u66f4\u5c11\u7684\u5de5\u5177\u8c03\u7528\u5b9e\u73b0\u53ef\u6bd4\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u641c\u7d22\u6846\u67b6\u5728\u957f\u8f68\u8ff9\u4efb\u52a1\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u5305\u62ec\u79ef\u7d2f\u5197\u957f\u5608\u6742\u5185\u5bb9\u3001\u89e6\u53ca\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u5de5\u5177\u9884\u7b97\u9650\u5236\u3001\u6216\u8fc7\u65e9\u505c\u6b62\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u957f\u8f68\u8ff9\u641c\u7d22\u3002", "method": "\u5f15\u5165SLIM\u6846\u67b6\uff0c\u5c06\u68c0\u7d22\u5206\u79bb\u4e3a\u72ec\u7acb\u7684\u641c\u7d22\u548c\u6d4f\u89c8\u5de5\u5177\uff0c\u5b9a\u671f\u603b\u7ed3\u8f68\u8ff9\u4ee5\u4fdd\u6301\u4e0a\u4e0b\u6587\u7b80\u6d01\uff0c\u540c\u65f6\u652f\u6301\u66f4\u957f\u3001\u66f4\u4e13\u6ce8\u7684\u641c\u7d22\u3002", "result": "\u5728\u957f\u8f68\u8ff9\u4efb\u52a1\u4e0a\uff0cSLIM\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u548c\u66f4\u5c11\u7684\u5de5\u5177\u8c03\u7528\u5b9e\u73b0\u53ef\u6bd4\u6027\u80fd\u3002\u4f7f\u7528o3\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u65f6\uff0c\u5728BrowseComp\u4e0a\u8fbe\u523056%\uff0c\u5728HLE\u4e0a\u8fbe\u523031%\uff0c\u5206\u522b\u4f18\u4e8e\u6240\u6709\u5f00\u6e90\u6846\u67b68\u548c4\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5de5\u5177\u8c03\u7528\u51cf\u5c114-6\u500d\u3002", "conclusion": "SLIM\u6846\u67b6\u901a\u8fc7\u7b80\u5355\u5de5\u5177\u8bbe\u8ba1\u548c\u4fe1\u606f\u7ba1\u7406\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u8f68\u8ff9\u667a\u80fd\u641c\u7d22\u7684\u6311\u6218\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e3a\u672a\u6765\u957f\u8f68\u8ff9\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "topic": "agent analysis"}}
{"id": "2510.19423", "categories": ["cs.AI", "I.2.0; I.2.1; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.19423", "abs": "https://arxiv.org/abs/2510.19423", "authors": ["Jia-Kai Dong", "I-Wei Huang", "Chun-Tin Wu", "Yi-Tien Tsai"], "title": "MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration", "comment": "under ACL Rolling Review 2025", "summary": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,\nend-to-end tool orchestration by LLM agents in a hierarchical Model-Context\nProtocol (MCP) ecosystem. Existing benchmarks often evaluate tools in\nisolation, ignoring challenges such as functional overlap and cross-server\norchestration, leading to overly optimistic assessments. MSC-Bench addresses\nthese gaps by constructing ground truth through 'equal function sets', allowing\nobjective metrics such as F1 score and reducing the dependency on\nLLM-as-a-judge evaluation. Organized as a five-level curriculum, it\nsystematically tests agent capabilities from single-tool orchestration to\ncomplex cross-server planning, and robustness to out-of-scope requests.\nExperiments reveal that rigid hierarchies can hinder performance without\nco-designed strategies, and even state-of-the-art agents exhibit systemic\nweaknesses in robustness. MSC-Bench provides a diagnostic framework to expose\nthese limitations and guide the development of more capable and efficient\ntool-using agents. The benchmark and resources are publicly available at\nhttps://github.com/snooow1029/MSC_Bench.", "AI": {"tldr": "MSC-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u5206\u5c42MCP\u751f\u6001\u7cfb\u7edf\u4e2d\u591a\u8df3\u7aef\u5230\u7aef\u5de5\u5177\u7f16\u6392\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6784\u5efa\"\u7b49\u51fd\u6570\u96c6\"\u4f5c\u4e3a\u771f\u5b9e\u57fa\u51c6\uff0c\u51cf\u5c11\u5bf9LLM\u4f5c\u4e3a\u8bc4\u5224\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5728\u5b64\u7acb\u73af\u5883\u4e2d\u8bc4\u4f30\u5de5\u5177\uff0c\u5ffd\u7565\u4e86\u529f\u80fd\u91cd\u53e0\u548c\u8de8\u670d\u52a1\u5668\u7f16\u6392\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u8bc4\u4f30\u8fc7\u4e8e\u4e50\u89c2\u3002", "method": "\u91c7\u7528\u4e94\u7ea7\u8bfe\u7a0b\u7ed3\u6784\uff0c\u4ece\u5355\u5de5\u5177\u7f16\u6392\u5230\u590d\u6742\u8de8\u670d\u52a1\u5668\u89c4\u5212\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u6784\u5efa\"\u7b49\u51fd\u6570\u96c6\"\u4f5c\u4e3a\u771f\u5b9e\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u521a\u6027\u5c42\u6b21\u7ed3\u6784\u4f1a\u963b\u788d\u6027\u80fd\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u5728\u9c81\u68d2\u6027\u65b9\u9762\u4e5f\u5b58\u5728\u7cfb\u7edf\u6027\u5f31\u70b9\u3002", "conclusion": "MSC-Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\u6765\u66b4\u9732\u8fd9\u4e9b\u9650\u5236\uff0c\u6307\u5bfc\u5f00\u53d1\u66f4\u5f3a\u5927\u9ad8\u6548\u7684\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u3002", "topic": "agent analysis"}}
{"id": "2510.19692", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19692", "abs": "https://arxiv.org/abs/2510.19692", "authors": ["Rashina Hoda"], "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary", "comment": "5 pages", "summary": "Agentic AI is poised to usher in a seismic paradigm shift in Software\nEngineering (SE). As technologists rush head-along to make agentic AI a\nreality, SE researchers are driven to establish agentic SE as a research area.\nWhile early visions of agentic SE are primarily focused on code-related\nactivities, early empirical evidence calls for a consideration of a range of\nsocio-technical concerns to make it work in practice. This paper contributes to\nthe emerging community vision by: (a) recommending an expansion of its scope\nbeyond code, toward a 'whole of process' vision, grounding it in SE foundations\nand evolution and emerging agentic SE frameworks, (b) proposing a preliminary\nset of values and principles to guide efforts, and (c) sharing guidance on\ndesigning/using well-defined vocabulary for agentic SE. It is hoped that these\nideas will encourage community collaborations and steer the SE community\ntowards laying strong foundations of agentic SE so its not only inevitable but\nalso deliberate and desirable in the long run.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6269\u5c55\u667a\u80fd\u4ee3\u7406\u8f6f\u4ef6\u5de5\u7a0b\u8303\u56f4\u7684\u613f\u666f\uff0c\u5efa\u8bae\u8d85\u8d8a\u4ee3\u7801\u6d3b\u52a8\uff0c\u5efa\u7acb\u6db5\u76d6\u6574\u4e2a\u8f6f\u4ef6\u8fc7\u7a0b\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u6307\u5bfc\u539f\u5219\u548c\u8bcd\u6c47\u8bbe\u8ba1\u6307\u5357\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4ee3\u7406AI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5174\u8d77\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u9886\u57df\uff0c\u8003\u8651\u793e\u4f1a\u6280\u672f\u56e0\u7d20\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4ee3\u7801\u76f8\u5173\u6d3b\u52a8\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u7840\u548c\u6f14\u5316\u8d8b\u52bf\uff0c\u7ed3\u5408\u65b0\u5174\u7684\u667a\u80fd\u4ee3\u7406SE\u6846\u67b6\uff0c\u63d0\u51fa\u6269\u5c55\u8303\u56f4\u3001\u5236\u5b9a\u4ef7\u503c\u89c2\u539f\u5219\u548c\u8bcd\u6c47\u8bbe\u8ba1\u6307\u5bfc\u3002", "result": "\u63d0\u51fa\u4e86'\u5168\u8fc7\u7a0b'\u613f\u666f\uff0c\u521d\u6b65\u7684\u4ef7\u503c\u89c2\u548c\u539f\u5219\uff0c\u4ee5\u53ca\u660e\u786e\u7684\u8bcd\u6c47\u8bbe\u8ba1\u6307\u5357\uff0c\u4e3a\u667a\u80fd\u4ee3\u7406SE\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8fd9\u4e9b\u5efa\u8bae\u5c06\u4fc3\u8fdb\u793e\u533a\u5408\u4f5c\uff0c\u5f15\u5bfc\u8f6f\u4ef6\u5de5\u7a0b\u793e\u533a\u4e3a\u667a\u80fd\u4ee3\u7406SE\u5efa\u7acb\u575a\u5b9e\u57fa\u7840\uff0c\u4f7f\u5176\u4e0d\u4ec5\u662f\u5fc5\u7136\u7684\uff0c\u800c\u4e14\u662f\u7ecf\u8fc7\u6df1\u601d\u719f\u8651\u548c\u957f\u671f\u53ef\u53d6\u7684\u3002", "topic": "agent analysis"}}
{"id": "2510.19747", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19747", "abs": "https://arxiv.org/abs/2510.19747", "authors": ["Priyaranjan Pattnayak", "Hussain Bohra"], "title": "Review of Tools for Zero-Code LLM Based Application Development", "comment": "Accepted in 6th World Conference on Artificial Intelligence: Advances\n  and Applications (WCAIAA 2025)", "summary": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software.", "AI": {"tldr": "\u672c\u8c03\u67e5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u4ee3\u7801\u5f00\u53d1\u5e73\u53f0\uff0c\u5206\u6790\u4e86\u8fd9\u4e9b\u5e73\u53f0\u5982\u4f55\u5229\u7528LLM\u4f5c\u4e3a\u5f00\u53d1\u8fc7\u7a0b\u7684\u6838\u5fc3\uff0c\u8ba9\u7528\u6237\u65e0\u9700\u7f16\u5199\u4ee3\u7801\u5373\u53ef\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u96f6\u4ee3\u7801\u5f00\u53d1\u5e73\u53f0\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u521b\u5efa\u65b9\u5f0f\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u548c\u5206\u6790\u8fd9\u4e9b\u65b0\u5174\u5e73\u53f0\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u5168\u9762\u7684\u5206\u7c7b\u548c\u6bd4\u8f83\u3002", "method": "\u91c7\u7528\u5e7f\u6cdb\u7684\u8c03\u67e5\u65b9\u6cd5\uff0c\u57fa\u4e8e\u754c\u9762\u98ce\u683c\u3001\u540e\u7aef\u96c6\u6210\u3001\u8f93\u51fa\u7c7b\u578b\u548c\u53ef\u6269\u5c55\u6027\u7b49\u5173\u952e\u7ef4\u5ea6\u5bf9\u5e73\u53f0\u8fdb\u884c\u5206\u7c7b\u3002\u5206\u6790\u4e86\u4e13\u95e8\u7684LLM\u5e94\u7528\u6784\u5efa\u5668\u548c\u96c6\u6210LLM\u529f\u80fd\u7684\u901a\u7528\u65e0\u4ee3\u7801\u5e73\u53f0\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6cd5\uff0c\u6309\u754c\u9762\u7c7b\u578b\u3001\u652f\u6301\u7684LLM\u540e\u7aef\u3001\u8f93\u51fa\u7c7b\u578b\u548c\u53ef\u6269\u5c55\u6027\u7a0b\u5ea6\u5bf9\u5e73\u53f0\u8fdb\u884c\u5206\u7c7b\u3002\u8be6\u7ec6\u6bd4\u8f83\u4e86\u5404\u5e73\u53f0\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e0e\u4f20\u7edf\u548c\u4f4e\u4ee3\u7801\u5f00\u53d1\u65b9\u6cd5\u7684\u6743\u8861\u3002", "conclusion": "\u867d\u7136\u57fa\u4e8eLLM\u7684\u96f6\u4ee3\u7801\u5e73\u53f0\u5927\u5927\u964d\u4f4e\u4e86\u521b\u5efaAI\u9a71\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684\u95e8\u69db\uff0c\u4f46\u5728\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u8be5\u9886\u57df\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4e3a\u975e\u7a0b\u5e8f\u5458\u521b\u5efa\u590d\u6742\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u4ee4\u4eba\u5174\u594b\u7684\u673a\u4f1a\u3002", "topic": "swe application"}}
{"id": "2510.19777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19777", "abs": "https://arxiv.org/abs/2510.19777", "authors": ["S M Sadrul Islam Asif", "James Chen", "Earl T. Barr", "Mark Marron"], "title": "BOSQTGEN: Breaking the Sound Barrier in Test Generation", "comment": null, "summary": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development.", "AI": {"tldr": "BOSQTGEN\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9ed1\u76d2API\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3API\u89c4\u8303\u4e3a\u57fa\u672c\u5143\u7d20\uff0c\u4f7f\u7528LLM\u751f\u6210\u8fde\u8d2f\u7684\u8f93\u5165\u503c\u5c42\u6b21\uff0c\u5e76\u91c7\u7528\u7ec4\u5408\u6d4b\u8bd5\u6765\u9ad8\u6548\u91c7\u6837\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u901a\u8fc7API\u7ec4\u5408\u6784\u5efa\uff0c\u4f46API\u5951\u7ea6\u4e0d\u8db3\u4f1a\u5bfc\u81f4\u671f\u671b\u4e0d\u5339\u914d\u548c\u6545\u969c\u3002\u73b0\u6709\u6d4b\u8bd5\u751f\u6210\u6280\u672f\u9762\u4e34\u591a\u8bed\u8a00\u7cfb\u7edf\u3001\u6e90\u4ee3\u7801\u4e0d\u53ef\u8bbf\u95ee\u3001\u6210\u672c\u53ef\u9760\u6027\u6743\u8861\u4ee5\u53ca\u751f\u6210\u7ed3\u6784\u5316\u8f93\u5165\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u5c06API\u89c4\u8303\u5206\u89e3\u4e3a\u57fa\u672c\u5143\u7d20\uff0c\u4f7f\u7528LLM\u751f\u6210\u8fde\u8d2f\u7684\u8f93\u5165\u503c\u5c42\u6b21\uff0c\u91c7\u7528\u7ec4\u5408\u6d4b\u8bd5\u65b9\u6cd5\u9ad8\u6548\u91c7\u6837\u8fd9\u4e9b\u503c\uff0c\u786e\u4fdd\u8986\u76d6\u5173\u952e\u4ea4\u4e92\u540c\u65f6\u907f\u514d\u968f\u673a\u91c7\u6837\u7684\u5197\u4f59\u3002", "result": "\u5728RESTful\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u8fbe\u523082%\u7684\u4ee3\u7801\u8986\u76d6\u7387\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\u63d0\u9ad820%\u4ee5\u4e0a\uff0c\u63a5\u8fd1\u624b\u5de5\u7f16\u5199\u7684\u6d4b\u8bd5\u5957\u4ef6\u6c34\u5e73\u3002", "conclusion": "BOSQTGEN\u63d0\u4f9b\u5b8c\u5168API\u9a71\u52a8\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u4f7f\u5f00\u53d1\u4eba\u5458\u80fd\u591f\u81ea\u52a8\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u6d4b\u8bd5\u7528\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u6216\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u3002", "topic": "swe application"}}
{"id": "2510.19631", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19631", "abs": "https://arxiv.org/abs/2510.19631", "authors": ["Yiqian Yang", "Tian Lan", "Qianghuai Jia", "Li Zhu", "Hui Jiang", "Hang Zhu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application", "comment": null, "summary": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further.", "AI": {"tldr": "\u63d0\u51fa\u4e86HSCodeComp\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u5728\u5206\u5c42\u89c4\u5219\u5e94\u7528\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u9884\u6d4b10\u4f4d\u5546\u54c1\u7f16\u7801\uff08HSCode\uff09\uff0c\u73b0\u6709\u4ee3\u7406\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u89c6\u4e86\u4ee3\u7406\u5728\u5e94\u7528\u590d\u6742\u89c4\u5219\uff08\u5982\u6cd5\u5f8b\u6761\u6b3e\u3001\u533b\u7597\u624b\u518c\u548c\u5173\u7a0e\u89c4\u5219\uff09\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u89c4\u5219\u5177\u6709\u6a21\u7cca\u8fb9\u754c\u548c\u9690\u542b\u903b\u8f91\u5173\u7cfb\uff0c\u5bf9\u4ee3\u7406\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7535\u5546\u5e73\u53f0\u6570\u636e\u7684HSCodeComp\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b632\u4e2a\u4ea7\u54c1\u6761\u76ee\uff0c\u6db5\u76d6\u591a\u6837\u5316\u4ea7\u54c1\u7c7b\u522b\uff0cHSCode\u7531\u591a\u4f4d\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6700\u4f73\u4ee3\u7406\u4ec5\u8fbe\u523046.8%\u768410\u4f4d\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u768495.0%\uff0c\u6d4b\u8bd5\u65f6\u6269\u5c55\u65e0\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u5206\u5c42\u89c4\u5219\u5e94\u7528\u5bf9\u5f53\u524d\u4ee3\u7406\u5177\u6709\u663e\u8457\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u4ee3\u7406\u80fd\u529b\u6765\u5904\u7406\u590d\u6742\u89c4\u5219\u63a8\u7406\u4efb\u52a1\u3002", "topic": "swe benchmark"}}
{"id": "2510.19116", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19116", "abs": "https://arxiv.org/abs/2510.19116", "authors": ["Jaesung Bae", "Cameron Churchwell", "Mitchell Hermon", "Tsun-An Hsieh", "Jocelyn Xu", "Yekaterina Yegorova", "Mark Hasegawa-Johnson", "Heng Ji"], "title": "That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation", "comment": null, "summary": "This paper investigates how large language models (LLMs) behave when faced\nwith discrepancies between their parametric knowledge and conflicting\ninformation contained in a prompt. Building on prior question-answering (QA)\nresearch, we extend the investigation of knowledge conflicts to the realm of\ncode generation. We propose a domain-agnostic framework for constructing and\ninterpreting such conflicts, along with a novel evaluation method and dataset\ntailored to code conflict scenarios. Our experiments indicate that sufficiently\nlarge LLMs encode the notion of a knowledge conflict in their parameters,\nenabling us to detect knowledge conflicts with up to \\textbf{80.65\\%} accuracy.\nBuilding on these insights, we show that activation-level steering can achieve\nup to a \\textbf{12.6\\%} improvement in steering success over a random baseline.\nHowever, effectiveness depends critically on balancing model size, task domain,\nand steering direction. The experiment code and data will be made publicly\navailable after acceptance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53c2\u6570\u77e5\u8bc6\u4e0e\u63d0\u793a\u4e2d\u51b2\u7a81\u4fe1\u606f\u4e0d\u4e00\u81f4\u65f6\u7684\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u7684\u6846\u67b6\u6765\u6784\u5efa\u548c\u89e3\u91ca\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u6269\u5c55\u77e5\u8bc6\u51b2\u7a81\u7814\u7a76\u5230\u4ee3\u7801\u751f\u6210\u9886\u57df\uff0c\u63a2\u7d22LLMs\u5982\u4f55\u5904\u7406\u53c2\u6570\u77e5\u8bc6\u4e0e\u63d0\u793a\u4fe1\u606f\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86\u9886\u57df\u65e0\u5173\u7684\u6846\u67b6\u6765\u6784\u5efa\u548c\u89e3\u91ca\u77e5\u8bc6\u51b2\u7a81\uff0c\u5f00\u53d1\u4e86\u9488\u5bf9\u4ee3\u7801\u51b2\u7a81\u573a\u666f\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u6fc0\u6d3b\u5c42\u5f15\u5bfc\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5927\u578bLLMs\u5728\u53c2\u6570\u4e2d\u7f16\u7801\u4e86\u77e5\u8bc6\u51b2\u7a81\u7684\u6982\u5ff5\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe80.65%\uff1b\u6fc0\u6d3b\u5c42\u5f15\u5bfc\u6bd4\u968f\u673a\u57fa\u7ebf\u63d0\u5347\u4e8612.6%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u3001\u4efb\u52a1\u9886\u57df\u548c\u5f15\u5bfc\u65b9\u5411\u7684\u5e73\u8861\u5bf9\u77e5\u8bc6\u51b2\u7a81\u5904\u7406\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2510.19698", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19698", "abs": "https://arxiv.org/abs/2510.19698", "authors": ["Yang Yang", "Hua XU", "Zhangyi Hu", "Yutao Yue"], "title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning.", "AI": {"tldr": "RLIE\u662f\u4e00\u4e2a\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6982\u7387\u5efa\u6a21\u76f8\u7ed3\u5408\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u52a0\u6743\u89c4\u5219\u96c6\uff0c\u901a\u8fc7\u89c4\u5219\u751f\u6210\u3001\u903b\u8f91\u56de\u5f52\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u8bc4\u4f30\u56db\u4e2a\u9636\u6bb5\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c4\u5219\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528LLM\u4e0e\u6982\u7387\u89c4\u5219\u5b66\u4e60\u7ed3\u5408\u8fdb\u884c\u9c81\u68d2\u63a8\u7406\u7684\u6f5c\u529b\u3002", "method": "RLIE\u6846\u67b6\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1aLLM\u751f\u6210\u548c\u8fc7\u6ee4\u89c4\u5219\u5019\u9009\u3001\u903b\u8f91\u56de\u5f52\u5b66\u4e60\u6982\u7387\u6743\u91cd\u3001\u57fa\u4e8e\u9884\u6d4b\u8bef\u5dee\u7684\u8fed\u4ee3\u4f18\u5316\u3001\u4ee5\u53ca\u5c06\u52a0\u6743\u89c4\u5219\u96c6\u4f5c\u4e3a\u76f4\u63a5\u5206\u7c7b\u5668\u7684\u8bc4\u4f30\u3002", "result": "\u76f4\u63a5\u4f7f\u7528\u5b66\u4e60\u6743\u91cd\u7684\u89c4\u5219\u96c6\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u5c06\u89c4\u5219\u3001\u6743\u91cd\u548c\u903b\u8f91\u6a21\u578b\u8f93\u51fa\u6ce8\u5165LLM\u63d0\u793a\u53cd\u800c\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u8868\u660eLLM\u64c5\u957f\u8bed\u4e49\u751f\u6210\u4f46\u6982\u7387\u96c6\u6210\u80fd\u529b\u6709\u9650\u3002", "conclusion": "RLIE\u9610\u660e\u4e86LLM\u5728\u5f52\u7eb3\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4e0e\u7ecf\u5178\u6982\u7387\u89c4\u5219\u7ec4\u5408\u65b9\u6cd5\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2510.19732", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19732", "abs": "https://arxiv.org/abs/2510.19732", "authors": ["Gunshi Gupta", "Karmesh Yadav", "Zsolt Kira", "Yarin Gal", "Rahaf Aljundi"], "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning", "comment": "Accepted for Spotlight Presentation at NeurIPS 2025", "summary": "To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints.", "AI": {"tldr": "Memo\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u5185\u5b58\u5bc6\u96c6\u578b\u3001\u957f\u89c6\u91ce\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u63d2\u5165\u5468\u671f\u6027\u603b\u7ed3\u6807\u8bb0\u6765\u521b\u5efa\u548c\u68c0\u7d22\u8bb0\u5fc6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7b56\u7565\u8bad\u7ec3\u4e2d\uff0c\u89c6\u89c9\u8f93\u5165\u5e38\u5e38\u8d85\u51faTransformer\u7684\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u800c\u4eba\u7c7b\u80fd\u591f\u7ef4\u62a4\u548c\u5229\u7528\u538b\u7f29\u4e3a\u8bb0\u5fc6\u7684\u7ec8\u8eab\u7ecf\u9a8c\u3002", "method": "\u63d0\u51faMemo\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u5728\u6a21\u578b\u8f93\u5165\u4e2d\u63d2\u5165\u5468\u671f\u6027\u603b\u7ed3\u6807\u8bb0\u6765\u5b9e\u73b0\u8bb0\u5fc6\u7684\u521b\u5efa\u548c\u68c0\u7d22\u3002", "result": "\u5728\u7f51\u683c\u4e16\u754c\u5143\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u548c\u7167\u7247\u7ea7\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u7684\u591a\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cMemo\u4f18\u4e8e\u6734\u7d20\u7684\u957f\u4e0a\u4e0b\u6587Transformer\u57fa\u7ebf\uff0c\u540c\u65f6\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "Memo\u5728\u63a8\u7406\u65f6\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728\u9700\u8981\u622a\u65ad\u5386\u53f2\u4e0a\u4e0b\u6587\u4ee5\u9002\u5e94\u63a8\u7406\u7ea6\u675f\u7684\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.19771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19771", "abs": "https://arxiv.org/abs/2510.19771", "authors": ["Gil Pasternak", "Dheeraj Rajagopal", "Julia White", "Dhruv Atreja", "Matthew Thomas", "George Hurn-Maloney", "Ash Lewis"], "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents", "comment": null, "summary": "LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions.", "AI": {"tldr": "\u63d0\u51fa\u4e86PROBE\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u4e3b\u52a8\u6027\u80fd\u529b\uff0c\u5c06\u4e3b\u52a8\u6027\u5206\u89e3\u4e3a\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a\u641c\u7d22\u672a\u6307\u5b9a\u95ee\u9898\u3001\u8bc6\u522b\u5177\u4f53\u74f6\u9888\u548c\u6267\u884c\u9002\u5f53\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u4e3b\u52a8\u6027\u7684\u57fa\u51c6\u5c40\u9650\u4e8e\u5c40\u90e8\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u6d4b\u8bd5\u8de8\u6765\u6e90\u548c\u957f\u65f6\u95f4\u8de8\u5ea6\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1PROBE\u57fa\u51c6\uff0c\u5c06\u4e3b\u52a8\u6027\u5206\u89e3\u4e3a\u4e09\u4e2a\u80fd\u529b\u9636\u6bb5\uff0c\u5e76\u5e94\u7528\u4e8e\u8bc4\u4f30\u9886\u5148LLM\u548c\u6d41\u884c\u4ee3\u7406\u6846\u67b6\u3002", "result": "\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u89e3\u51b3\u8be5\u57fa\u51c6\uff0cGPT-5\u548cClaude Opus-4.1\u7684\u6700\u4f73\u7aef\u5230\u7aef\u6027\u80fd\u4ec5\u4e3a40%\uff0c\u63ed\u793a\u4e86\u4ee3\u7406\u7cfb\u7edf\u7684\u5f53\u524d\u5c40\u9650\u6027\u3002", "conclusion": "\u7ed3\u679c\u7a81\u663e\u4e86\u4ee3\u7406\u7cfb\u7edf\u4e2d\u81ea\u4e3b\u884c\u52a8\u7684\u5f53\u524d\u9650\u5236\uff0c\u5e76\u63ed\u793a\u4e86\u6709\u524d\u666f\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.19172", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19172", "abs": "https://arxiv.org/abs/2510.19172", "authors": ["Nishanth Sridhar Nakshatri", "Shamik Roy", "Manoj Ghuhan Arivazhagan", "Hanhan Zhou", "Vinayshekhar Bannihatti Kumar", "Rashmi Gangadharaiah"], "title": "When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA", "comment": "Under submission", "summary": "LLMs often fail to handle temporal knowledge conflicts--contradictions\narising when facts evolve over time within their training data. Existing\nstudies evaluate this phenomenon through benchmarks built on structured\nknowledge bases like Wikidata, but they focus on widely-covered,\neasily-memorized popular entities and lack the dynamic structure needed to\nfairly evaluate LLMs with different knowledge cut-off dates. We introduce\nevolveQA, a benchmark specifically designed to evaluate LLMs on temporally\nevolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS\nupdates, Azure changes, and WHO disease outbreak reports. Our framework\nidentifies naturally occurring knowledge evolution and generates questions with\ngold answers tailored to different LLM knowledge cut-off dates. Through\nextensive evaluation of 12 open and closed-source LLMs across 3 knowledge\nprobing formats, we demonstrate significant performance drops of up to 31% on\nevolveQA compared to static knowledge questions.", "AI": {"tldr": "\u63d0\u51fa\u4e86evolveQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30LLM\u5904\u7406\u65f6\u95f4\u6f14\u5316\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u57fa\u4e8eAWS\u66f4\u65b0\u3001Azure\u53d8\u66f4\u548cWHO\u75be\u75c5\u62a5\u544a\u4e09\u4e2a\u771f\u5b9e\u65f6\u95f4\u6233\u8bed\u6599\u5e93\u6784\u5efa\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u57fa\u4e8e\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u7684\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u65f6\u95f4\u77e5\u8bc6\u51b2\u7a81\uff0c\u4f46\u5173\u6ce8\u6d41\u884c\u5b9e\u4f53\u4e14\u7f3a\u4e4f\u52a8\u6001\u7ed3\u6784\u6765\u516c\u5e73\u8bc4\u4f30\u4e0d\u540c\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u7684LLM\u3002", "method": "\u4ece\u4e09\u4e2a\u771f\u5b9e\u65f6\u95f4\u6233\u8bed\u6599\u5e93\u8bc6\u522b\u81ea\u7136\u53d1\u751f\u7684\u77e5\u8bc6\u6f14\u5316\uff0c\u751f\u6210\u9488\u5bf9\u4e0d\u540cLLM\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u7684\u95ee\u9898\u548c\u9ec4\u91d1\u7b54\u6848\u3002", "result": "\u5bf912\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90LLM\u57283\u79cd\u77e5\u8bc6\u63a2\u6d4b\u683c\u5f0f\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u9759\u6001\u77e5\u8bc6\u95ee\u9898\u76f8\u6bd4\uff0cevolveQA\u4e0a\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe31%\u3002", "conclusion": "evolveQA\u57fa\u51c6\u6d4b\u8bd5\u6709\u6548\u63ed\u793a\u4e86LLM\u5728\u5904\u7406\u65f6\u95f4\u6f14\u5316\u77e5\u8bc6\u65f6\u7684\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u4e3a\u8bc4\u4f30LLM\u7684\u65f6\u95f4\u77e5\u8bc6\u5904\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.19186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19186", "abs": "https://arxiv.org/abs/2510.19186", "authors": ["Zhaoyi Joey Hou", "Tanya Shourya", "Yingfan Wang", "Shamik Roy", "Vinayshekhar Bannihatti Kumar", "Rashmi Gangadharaiah"], "title": "Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems", "comment": "The first two authors contributed equally. Manuscript under\n  submission", "summary": "Evaluating conversational AI systems that use external tools is challenging,\nas errors can arise from complex interactions among user, agent, and tools.\nWhile existing evaluation methods assess either user satisfaction or agents'\ntool-calling capabilities, they fail to capture critical errors in multi-turn\ntool-augmented dialogues-such as when agents misinterpret tool results yet\nappear satisfactory to users. We introduce TRACE, a benchmark of systematically\nsynthesized tool-augmented conversations covering diverse error cases, and\nSCOPE, an evaluation framework that automatically discovers diverse error\npatterns and evaluation rubrics in tool-augmented dialogues. Experiments show\nSCOPE significantly outperforms the baseline, particularly on challenging cases\nwhere user satisfaction signals are misleading.", "AI": {"tldr": "\u63d0\u51fa\u4e86TRACE\u57fa\u51c6\u548cSCOPE\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u7684\u591a\u8f6e\u5bf9\u8bddAI\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u9519\u8bef\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u6ee1\u610f\u5ea6\u4fe1\u53f7\u8bef\u5bfc\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u9519\u8bef\uff0c\u7279\u522b\u662f\u5f53\u4ee3\u7406\u8bef\u89e3\u5de5\u5177\u7ed3\u679c\u4f46\u5bf9\u7528\u6237\u4ecd\u663e\u5f97\u6ee1\u610f\u7684\u60c5\u51b5\u3002", "method": "\u5f15\u5165TRACE\u57fa\u51c6\uff08\u7cfb\u7edf\u5408\u6210\u7684\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\uff09\u548cSCOPE\u8bc4\u4f30\u6846\u67b6\uff08\u81ea\u52a8\u53d1\u73b0\u9519\u8bef\u6a21\u5f0f\u548c\u8bc4\u4f30\u6807\u51c6\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSCOPE\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u6ee1\u610f\u5ea6\u4fe1\u53f7\u5177\u6709\u8bef\u5bfc\u6027\u7684\u6311\u6218\u6027\u6848\u4f8b\u4e0a\u3002", "conclusion": "TRACE\u548cSCOPE\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u9519\u8bef\u4ea4\u4e92\uff0c\u5f25\u8865\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2510.19178", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19178", "abs": "https://arxiv.org/abs/2510.19178", "authors": ["Runzhe Wu", "Ankur Samanta", "Ayush Jain", "Scott Fujimoto", "Jeongyeol Kwon", "Ben Kretzu", "Youliang Yu", "Kaveh Hassani", "Boris Vidolov", "Yonathan Efroni"], "title": "Imbalanced Gradients in RL Post-Training of Multi-Task LLMs", "comment": null, "summary": "Multi-task post-training of large language models (LLMs) is typically\nperformed by mixing datasets from different tasks and optimizing them jointly.\nThis approach implicitly assumes that all tasks contribute gradients of similar\nmagnitudes; when this assumption fails, optimization becomes biased toward\nlarge-gradient tasks. In this paper, however, we show that this assumption\nfails in RL post-training: certain tasks produce significantly larger\ngradients, thus biasing updates toward those tasks. Such gradient imbalance\nwould be justified only if larger gradients implied larger learning gains on\nthe tasks (i.e., larger performance improvements) -- but we find this is not\ntrue. Large-gradient tasks can achieve similar or even much lower learning\ngains than small-gradient ones. Further analyses reveal that these gradient\nimbalances cannot be explained by typical training statistics such as training\nrewards or advantages, suggesting that they arise from the inherent differences\nbetween tasks. This cautions against naive dataset mixing and calls for future\nwork on principled gradient-level corrections for LLMs.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0RL\u540e\u8bad\u7ec3\u4e2d\u5b58\u5728\u68af\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5927\u68af\u5ea6\u4efb\u52a1\u4e3b\u5bfc\u4f18\u5316\u8fc7\u7a0b\u4f46\u5b66\u4e60\u6536\u76ca\u4e0d\u4e00\u5b9a\u66f4\u9ad8\uff0c\u8fd9\u8d28\u7591\u4e86\u7b80\u5355\u6df7\u5408\u6570\u636e\u96c6\u7684\u505a\u6cd5\u3002", "motivation": "\u7814\u7a76\u591a\u4efb\u52a1\u540e\u8bad\u7ec3\u4e2d\u68af\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u4efb\u52a1\u8d21\u732e\u76f8\u4f3c\u68af\u5ea6\uff0c\u4f46\u5b9e\u9645RL\u8bad\u7ec3\u4e2d\u67d0\u4e9b\u4efb\u52a1\u4f1a\u4ea7\u751f\u663e\u8457\u66f4\u5927\u7684\u68af\u5ea6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4efb\u52a1\u5728RL\u540e\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u5927\u5c0f\u548c\u5b66\u4e60\u6536\u76ca\u5173\u7cfb\uff0c\u63a2\u7a76\u68af\u5ea6\u4e0d\u5e73\u8861\u73b0\u8c61\u53ca\u5176\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5927\u68af\u5ea6\u4efb\u52a1\u4e0d\u4e00\u5b9a\u83b7\u5f97\u66f4\u5927\u5b66\u4e60\u6536\u76ca\uff0c\u68af\u5ea6\u4e0d\u5e73\u8861\u4e0d\u80fd\u7531\u8bad\u7ec3\u5956\u52b1\u6216\u4f18\u52bf\u503c\u7b49\u5178\u578b\u7edf\u8ba1\u91cf\u89e3\u91ca\uff0c\u800c\u662f\u6e90\u4e8e\u4efb\u52a1\u672c\u8eab\u5dee\u5f02\u3002", "conclusion": "\u7b80\u5355\u6df7\u5408\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u5b58\u5728\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u68af\u5ea6\u7ea7\u522b\u7684\u6821\u6b63\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.19208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19208", "abs": "https://arxiv.org/abs/2510.19208", "authors": ["Hang Zheng", "Hongshen Xu", "Yongkai Lin", "Shuai Fan", "Lu Chen", "Kai Yu"], "title": "DiSRouter: Distributed Self-Routing for LLM Selections", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has created a diverse\necosystem of models with highly varying performance and costs, necessitating\neffective query routing to balance performance and expense. Current routing\nsystems often rely on a centralized external router trained on a fixed set of\nLLMs, making them inflexible and prone to poor performance since the small\nrouter can not fully understand the knowledge boundaries of different LLMs. We\nintroduce DiSRouter (Distributed Self-Router), a novel paradigm that shifts\nfrom centralized control to distributed routing. In DiSRouter, a query\ntraverses a network of LLM agents, each independently deciding whether to\nanswer or route to other agents based on its own self-awareness, its ability to\njudge its competence. This distributed design offers superior flexibility,\nscalability, and generalizability. To enable this, we propose a two-stage\nSelf-Awareness Training pipeline that enhances each LLM's self-awareness.\nExtensive experiments demonstrate that DiSRouter significantly outperforms\nexisting routing methods in utility across various scenarios, effectively\ndistinguishes between easy and hard queries, and shows strong generalization to\nout-of-domain tasks. Our work validates that leveraging an LLM's intrinsic\nself-awareness is more effective than external assessment, paving the way for\nmore modular and efficient multi-agent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86DiSRouter\uff08\u5206\u5e03\u5f0f\u81ea\u8def\u7531\uff09\u8303\u5f0f\uff0c\u4ece\u96c6\u4e2d\u5f0f\u8def\u7531\u8f6c\u5411\u5206\u5e03\u5f0f\u8def\u7531\uff0c\u8ba9LLM\u4ee3\u7406\u57fa\u4e8e\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u56de\u7b54\u95ee\u9898\u6216\u8def\u7531\u7ed9\u5176\u4ed6\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u8def\u7531\u7cfb\u7edf\u4f9d\u8d56\u56fa\u5b9a\u7684\u96c6\u4e2d\u5f0f\u5916\u90e8\u8def\u7531\u5668\uff0c\u65e0\u6cd5\u5145\u5206\u7406\u89e3\u4e0d\u540cLLM\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u6211\u8ba4\u77e5\u8bad\u7ec3\u6d41\u7a0b\u589e\u5f3aLLM\u7684\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\uff0c\u5728DiSRouter\u4e2d\u67e5\u8be2\u5728\u7f51\u7edc\u4e2d\u904d\u5386\uff0c\u6bcf\u4e2aLLM\u4ee3\u7406\u57fa\u4e8e\u81ea\u8eab\u80fd\u529b\u5224\u65ad\u72ec\u7acb\u51b3\u5b9a\u662f\u5426\u56de\u7b54\u6216\u8def\u7531\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDiSRouter\u5728\u5404\u79cd\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8def\u7531\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u533a\u5206\u7b80\u5355\u548c\u56f0\u96be\u67e5\u8be2\uff0c\u5e76\u5728\u9886\u57df\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5229\u7528LLM\u5185\u5728\u81ea\u6211\u8ba4\u77e5\u6bd4\u5916\u90e8\u8bc4\u4f30\u66f4\u6709\u6548\uff0c\u4e3a\u66f4\u6a21\u5757\u5316\u548c\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.19199", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19199", "abs": "https://arxiv.org/abs/2510.19199", "authors": ["Xiaoxing Ren", "Nicola Bastianello", "Thomas Parisini", "Andreas A. Malikopoulos"], "title": "A Communication-Efficient Decentralized Actor-Critic Algorithm", "comment": null, "summary": "In this paper, we study the problem of reinforcement learning in multi-agent\nsystems where communication among agents is limited. We develop a decentralized\nactor-critic learning framework in which each agent performs several local\nupdates of its policy and value function, where the latter is approximated by a\nmulti-layer neural network, before exchanging information with its neighbors.\nThis local training strategy substantially reduces the communication burden\nwhile maintaining coordination across the network. We establish finite-time\nconvergence analysis for the algorithm under Markov-sampling. Specifically, to\nattain the $\\varepsilon$-accurate stationary point, the sample complexity is of\norder $\\mathcal{O}(\\varepsilon^{-3})$ and the communication complexity is of\norder $\\mathcal{O}(\\varepsilon^{-1}\\tau^{-1})$, where tau denotes the number of\nlocal training steps. We also show how the final error bound depends on the\nneural network's approximation quality. Numerical experiments in a cooperative\ncontrol setting illustrate and validate the theoretical findings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u5730\u8bad\u7ec3\u51cf\u5c11\u901a\u4fe1\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u7f51\u7edc\u534f\u8c03\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u901a\u4fe1\u53d7\u9650\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u540c\u65f6\u7ef4\u6301\u667a\u80fd\u4f53\u95f4\u7684\u534f\u8c03\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u7684actor-critic\u5b66\u4e60\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5728\u4fe1\u606f\u4ea4\u6362\u524d\u8fdb\u884c\u591a\u6b21\u672c\u5730\u7b56\u7565\u548c\u4ef7\u503c\u51fd\u6570\u66f4\u65b0\uff0c\u4ef7\u503c\u51fd\u6570\u7531\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u3002", "result": "\u5728\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u4e0b\u5efa\u7acb\u4e86\u6709\u9650\u65f6\u95f4\u6536\u655b\u5206\u6790\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3aO(\u03b5^-3)\uff0c\u901a\u4fe1\u590d\u6742\u5ea6\u4e3aO(\u03b5^-1\u03c4^-1)\uff0c\u5176\u4e2d\u03c4\u4e3a\u672c\u5730\u8bad\u7ec3\u6b65\u6570\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u901a\u4fe1\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u6027\u80fd\uff0c\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u8d28\u91cf\u5f71\u54cd\u6700\u7ec8\u8bef\u5dee\u754c\u9650\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.19247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19247", "abs": "https://arxiv.org/abs/2510.19247", "authors": ["Ziwei Wang", "Jiayuan Su", "Mengyu Zhou", "Huaxing Zeng", "Mengni Jia", "Xiao Lv", "Haoyu Dong", "Xiaojun Ma", "Shi Han", "Dongmei Zhang"], "title": "SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets", "comment": null, "summary": "Understanding and reasoning over complex spreadsheets remain fundamental\nchallenges for large language models (LLMs), which often struggle with\naccurately capturing the complex structure of tables and ensuring reasoning\ncorrectness. In this work, we propose SheetBrain, a neuro-symbolic dual\nworkflow agent framework designed for accurate reasoning over tabular data,\nsupporting both spreadsheet question answering and manipulation tasks.\nSheetBrain comprises three core modules: an understanding module, which\nproduces a comprehensive overview of the spreadsheet - including sheet summary\nand query-based problem insight to guide reasoning; an execution module, which\nintegrates a Python sandbox with preloaded table-processing libraries and an\nExcel helper toolkit for effective multi-turn reasoning; and a validation\nmodule, which verifies the correctness of reasoning and answers, triggering\nre-execution when necessary. We evaluate SheetBrain on multiple public tabular\nQA and manipulation benchmarks, and introduce SheetBench, a new benchmark\ntargeting large, multi-table, and structurally complex spreadsheets.\nExperimental results show that SheetBrain significantly improves accuracy on\nboth existing benchmarks and the more challenging scenarios presented in\nSheetBench. Our code is publicly available at\nhttps://github.com/microsoft/SheetBrain.", "AI": {"tldr": "SheetBrain\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u53cc\u5de5\u4f5c\u6d41\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8868\u683c\u6570\u636e\u4e0a\u8fdb\u884c\u51c6\u786e\u63a8\u7406\uff0c\u652f\u6301\u7535\u5b50\u8868\u683c\u95ee\u7b54\u548c\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u63a8\u7406\u590d\u6742\u7535\u5b50\u8868\u683c\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u96be\u4ee5\u51c6\u786e\u6355\u6349\u8868\u683c\u7684\u590d\u6742\u7ed3\u6784\u5e76\u786e\u4fdd\u63a8\u7406\u6b63\u786e\u6027\u3002", "method": "SheetBrain\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u7406\u89e3\u6a21\u5757\u751f\u6210\u7535\u5b50\u8868\u683c\u7684\u5168\u9762\u6982\u89c8\uff1b\u6267\u884c\u6a21\u5757\u96c6\u6210Python\u6c99\u7bb1\u548cExcel\u5de5\u5177\u5305\u8fdb\u884c\u591a\u8f6e\u63a8\u7406\uff1b\u9a8c\u8bc1\u6a21\u5757\u9a8c\u8bc1\u63a8\u7406\u548c\u7b54\u6848\u7684\u6b63\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u8868\u683c\u95ee\u7b54\u548c\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSheetBrain\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u66f4\u5177\u6311\u6218\u6027\u7684SheetBench\u573a\u666f\u4e2d\u3002", "conclusion": "SheetBrain\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u53cc\u5de5\u4f5c\u6d41\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u590d\u6742\u7535\u5b50\u8868\u683c\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.19286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19286", "abs": "https://arxiv.org/abs/2510.19286", "authors": ["Reza Esfandiarpoor", "Vishwas Suryanarayanan", "Stephen H. Bach", "Vishal Chowdhary", "Anthony Aue"], "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools", "comment": "Code: https://github.com/Reza-esfandiarpoor/the-mcp-company", "summary": "Since the introduction of the Model Context Protocol (MCP), the number of\navailable tools for Large Language Models (LLMs) has increased significantly.\nThese task-specific tool sets offer an alternative to general-purpose tools\nsuch as web browsers, while being easier to develop and maintain than GUIs.\nHowever, current general-purpose agents predominantly rely on web browsers for\ninteracting with the environment. Here, we introduce TheMCPCompany, a benchmark\nfor evaluating tool-calling agents on tasks that involve interacting with\nvarious real-world services. We use the REST APIs of these services to create\nMCP servers, which include over 18,000 tools. We also provide manually\nannotated ground-truth tools for each task. In our experiments, we use the\nground truth tools to show the potential of tool-calling agents for both\nimproving performance and reducing costs assuming perfect tool retrieval. Next,\nwe explore agent performance using tool retrieval to study the real-world\npracticality of tool-based agents. While all models with tool retrieval perform\nsimilarly or better than browser-based agents, smaller models cannot take full\nadvantage of the available tools through retrieval. On the other hand, GPT-5's\nperformance with tool retrieval is very close to its performance with\nground-truth tools. Overall, our work shows that the most advanced reasoning\nmodels are effective at discovering tools in simpler environments, but\nseriously struggle with navigating complex enterprise environments.\nTheMCPCompany reveals that navigating tens of thousands of tools and combining\nthem in non-trivial ways to solve complex problems is still a challenging task\nfor current models and requires both better reasoning and better retrieval\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86TheMCPCompany\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u670d\u52a1\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b18,000\u591a\u4e2a\u5de5\u5177\uff0c\u5b9e\u9a8c\u663e\u793a\u5148\u8fdb\u6a21\u578b\u80fd\u6709\u6548\u53d1\u73b0\u5de5\u5177\u4f46\u96be\u4ee5\u5904\u7406\u590d\u6742\u4f01\u4e1a\u73af\u5883\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u4ee3\u7406\u4e3b\u8981\u4f9d\u8d56\u7f51\u9875\u6d4f\u89c8\u5668\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u800cMCP\u534f\u8bae\u5e26\u6765\u4e86\u5927\u91cf\u7279\u5b9a\u4efb\u52a1\u5de5\u5177\uff0c\u9700\u8981\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u5728\u5b9e\u9645\u670d\u52a1\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u670d\u52a1\u7684REST API\u521b\u5efaMCP\u670d\u52a1\u5668\uff0c\u63d0\u4f9b\u624b\u52a8\u6807\u6ce8\u7684\u771f\u5b9e\u5de5\u5177\uff0c\u901a\u8fc7\u5de5\u5177\u68c0\u7d22\u548c\u771f\u5b9e\u5de5\u5177\u4e24\u79cd\u65b9\u5f0f\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u5de5\u5177\u65f6\u6027\u80fd\u63d0\u5347\u4e14\u6210\u672c\u964d\u4f4e\uff1b\u901a\u8fc7\u5de5\u5177\u68c0\u7d22\u65f6\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u76f8\u4f3c\u6216\u4f18\u4e8e\u6d4f\u89c8\u5668\u4ee3\u7406\uff0c\u4f46\u5c0f\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u5229\u7528\u53ef\u7528\u5de5\u5177\uff0cGPT-5\u63a5\u8fd1\u771f\u5b9e\u5de5\u5177\u6027\u80fd\u3002", "conclusion": "\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u73af\u5883\u4e2d\u80fd\u6709\u6548\u53d1\u73b0\u5de5\u5177\uff0c\u4f46\u5728\u590d\u6742\u4f01\u4e1a\u73af\u5883\u4e2d\u4e25\u91cd\u53d7\u632b\uff0c\u5bfc\u822a\u6570\u4e07\u5de5\u5177\u5e76\u4ee5\u975e\u5e73\u51e1\u65b9\u5f0f\u7ec4\u5408\u89e3\u51b3\u590d\u6742\u95ee\u9898\u4ecd\u662f\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2510.19244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19244", "abs": "https://arxiv.org/abs/2510.19244", "authors": ["Yiyu Qian", "Su Nguyen", "Chao Chen", "Qinyue Zhou", "Liyuan Zhao"], "title": "Interpret Policies in Deep Reinforcement Learning using SILVER with RL-Guided Labeling: A Model-level Approach to High-dimensional and Multi-action Environments", "comment": null, "summary": "Deep reinforcement learning (RL) achieves remarkable performance but lacks\ninterpretability, limiting trust in policy behavior. The existing SILVER\nframework (Li, Siddique, and Cao 2025) explains RL policy via Shapley-based\nregression but remains restricted to low-dimensional, binary-action domains. We\npropose SILVER with RL-guided labeling, an enhanced variant that extends SILVER\nto multi-action and high-dimensional environments by incorporating the RL\npolicy's own action outputs into the boundary points identification. Our method\nfirst extracts compact feature representations from image observations,\nperforms SHAP-based feature attribution, and then employs RL-guided labeling to\ngenerate behaviorally consistent boundary datasets. Surrogate models, such as\ndecision trees and regression-based functions, are subsequently trained to\ninterpret RL policy's decision structure. We evaluate the proposed framework on\ntwo Atari environments using three deep RL algorithms and conduct human-subject\nstudy to assess the clarity and trustworthiness of the derived interpretable\npolicy. Results show that our approach maintains competitive task performance\nwhile substantially improving transparency and human understanding of agent\nbehavior. This work advances explainable RL by transforming SILVER into a\nscalable and behavior-aware framework for interpreting deep RL agents in\nhigh-dimensional, multi-action settings.", "AI": {"tldr": "\u63d0\u51faSILVER with RL-guided labeling\uff0c\u901a\u8fc7\u5c06RL\u7b56\u7565\u81ea\u8eab\u52a8\u4f5c\u8f93\u51fa\u7eb3\u5165\u8fb9\u754c\u70b9\u8bc6\u522b\uff0c\u5c06SILVER\u6846\u67b6\u6269\u5c55\u5230\u591a\u52a8\u4f5c\u548c\u9ad8\u7ef4\u73af\u5883\uff0c\u63d0\u5347\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u6027\u80fd\u51fa\u8272\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u7b56\u7565\u884c\u4e3a\u7684\u4fe1\u4efb\u3002\u73b0\u6709SILVER\u6846\u67b6\u4ec5\u9650\u4e8e\u4f4e\u7ef4\u3001\u4e8c\u5143\u52a8\u4f5c\u9886\u57df\uff0c\u9700\u8981\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u73af\u5883\u3002", "method": "\u4ece\u56fe\u50cf\u89c2\u5bdf\u4e2d\u63d0\u53d6\u7d27\u51d1\u7279\u5f81\u8868\u793a\uff0c\u8fdb\u884cSHAP\u7279\u5f81\u5f52\u56e0\uff0c\u7136\u540e\u4f7f\u7528RL\u5f15\u5bfc\u6807\u8bb0\u751f\u6210\u884c\u4e3a\u4e00\u81f4\u7684\u8fb9\u754c\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u51b3\u7b56\u6811\u548c\u56de\u5f52\u51fd\u6570\u7b49\u66ff\u4ee3\u6a21\u578b\u6765\u89e3\u91caRL\u7b56\u7565\u7684\u51b3\u7b56\u7ed3\u6784\u3002", "result": "\u5728\u4e24\u4e2aAtari\u73af\u5883\u4e2d\u4f7f\u7528\u4e09\u79cd\u6df1\u5ea6RL\u7b97\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u4eba\u7c7b\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u7406\u89e3\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5c06SILVER\u8f6c\u53d8\u4e3a\u53ef\u6269\u5c55\u4e14\u884c\u4e3a\u611f\u77e5\u7684\u6846\u67b6\uff0c\u63a8\u8fdb\u4e86\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u5728\u9ad8\u7ef4\u591a\u52a8\u4f5c\u8bbe\u7f6e\u4e2d\u89e3\u91ca\u6df1\u5ea6RL\u667a\u80fd\u4f53\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.19296", "categories": ["cs.LG", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19296", "abs": "https://arxiv.org/abs/2510.19296", "authors": ["Yang Zhang", "Rui Zhang", "Jiaming Guo", "Lei Huang", "Di Huang", "Yunpu Zhao", "Shuyao Cheng", "Pengwei Jin", "Chongxiao Li", "Zidong Du", "Xing Hu", "Qi Guo", "Yunji Chen"], "title": "QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation", "comment": "Accepted to NeurIPS 2025", "summary": "The remarkable progress of Large Language Models (LLMs) presents promising\nopportunities for Verilog code generation which is significantly important for\nautomated circuit design. The lacking of meaningful functional rewards hinders\nthe preference optimization based on Reinforcement Learning (RL) for producing\nfunctionally correct Verilog code. In this paper, we propose Signal-Aware\nLearning for Verilog code generation (QiMeng-SALV) by leveraging code segments\nof functionally correct output signal to optimize RL training. Considering\nVerilog code specifies the structural interconnection of hardware gates and\nwires so that different output signals are independent, the key insight of\nQiMeng-SALV is to extract verified signal-aware implementations in partially\nincorrect modules, so as to enhance the extraction of meaningful functional\nrewards. Roughly, we verify the functional correctness of signals in generated\nmodule by comparing with that of reference module in the training data. Then\nabstract syntax tree (AST) is employed to identify signal-aware code segments\nwhich can provide meaningful functional rewards from erroneous modules.\nFinally, we introduce signal-aware DPO which is optimized on the correct\nsignal-level code segments, thereby preventing noise and interference from\nincorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from\nconventional module-level to fine-grained signal-level optimization in Verilog\ncode generation, addressing the issue of insufficient functional rewards.\nExperiments demonstrate that our method achieves state-of-the-art performance\non VerilogEval and RTLLM, with a 7B parameter model matching the performance of\nthe DeepSeek v3 671B model and significantly outperforming the leading\nopen-source model CodeV trained on the same dataset. Our code is available at\nhttps://github.com/zy1xxx/SALV.", "AI": {"tldr": "\u63d0\u51faQiMeng-SALV\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u53f7\u611f\u77e5\u5b66\u4e60\u4f18\u5316Verilog\u4ee3\u7801\u751f\u6210\uff0c\u5229\u7528\u529f\u80fd\u6b63\u786e\u8f93\u51fa\u4fe1\u53f7\u7684\u4ee3\u7801\u6bb5\u6765\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728VerilogEval\u548cRTLLM\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Verilog\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u6709\u610f\u4e49\u7684\u529f\u80fd\u5956\u52b1\u963b\u788d\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u504f\u597d\u4f18\u5316\uff0c\u96be\u4ee5\u751f\u6210\u529f\u80fd\u6b63\u786e\u7684Verilog\u4ee3\u7801\u3002", "method": "\u5229\u7528\u529f\u80fd\u6b63\u786e\u8f93\u51fa\u4fe1\u53f7\u7684\u4ee3\u7801\u6bb5\u4f18\u5316RL\u8bad\u7ec3\uff1a\u9a8c\u8bc1\u751f\u6210\u6a21\u5757\u4e2d\u4fe1\u53f7\u7684\u529f\u80fd\u6b63\u786e\u6027\uff1b\u4f7f\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u8bc6\u522b\u4fe1\u53f7\u611f\u77e5\u4ee3\u7801\u6bb5\uff1b\u5f15\u5165\u4fe1\u53f7\u611f\u77e5DPO\u5728\u6b63\u786e\u4fe1\u53f7\u7ea7\u4ee3\u7801\u6bb5\u4e0a\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728VerilogEval\u548cRTLLM\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c7B\u53c2\u6570\u6a21\u578b\u5339\u914dDeepSeek v3 671B\u6a21\u578b\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5728\u540c\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u5f00\u6e90\u6a21\u578bCodeV\u3002", "conclusion": "QiMeng-SALV\u5b9e\u73b0\u4e86\u4ece\u4f20\u7edf\u6a21\u5757\u7ea7\u5230\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u7ea7\u4f18\u5316\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u89e3\u51b3\u4e86\u529f\u80fd\u5956\u52b1\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2510.19361", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19361", "abs": "https://arxiv.org/abs/2510.19361", "authors": ["Xianyang Liu", "Yilin Liu", "Shuai Wang", "Hao Cheng", "Andrew Estornell", "Yuzhi Zhao", "Jiaheng Wei"], "title": "AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation", "comment": "Work in progress", "summary": "The creation of high-quality datasets to improve Large Language Model (LLM)\nreasoning remains a significant challenge, as current methods often suffer from\ngenerating low-quality/incorrect answers and limited information richness from\navailable data sources. To address this, we propose AgenticMath, a novel\nagentic pipeline for generating high-quality mathematical question-answer pairs\nto enhance the supervised fine-tuning of LLMs. Our method operates through four\nstages: (1) Seed Question Filter that selects questions with high information\nrichness, complexity, and clarity; (2) an Agentic Question Rephrase step that\nemploys a multi-agent system to generate diverse, logically consistent\nparaphrases; (3) an Answer Augment step where rewrite answers using\nchain-of-thought reasoning to enhance numerical and logical correctness,\nwithout reliance on human-provided labels; and (4) a final Question and Answer\nEvaluation that retains only the most superior pairs. Extensive experiments\ndemonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated\ndatasets (comprising only 30-60K math samples) achieves competitive or superior\nperformance on diverse in domain and out-of-domain mathematical reasoning\nbenchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M\nsamples). Our work demonstrates that targeted, high-quality data generation is\na more efficient path to improving mathematical reasoning in LLMs than\nlarge-scale, low-quality alternatives.", "AI": {"tldr": "AgenticMath\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u5b66\u95ee\u7b54\u5bf9\u7684\u667a\u80fd\u4ee3\u7406\u7ba1\u9053\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u6d41\u7a0b\u63d0\u5347LLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u97003-6\u4e07\u6837\u672c\u5373\u53ef\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4f7f\u7528\u66f4\u5927\u6570\u636e\u96c6\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u96c6\u8d28\u91cf\u4f4e\u3001\u7b54\u6848\u9519\u8bef\u591a\u3001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u5b66\u95ee\u7b54\u5bf9\u7684\u65b9\u6cd5\u6765\u63d0\u5347LLM\u7684\u76d1\u7763\u5fae\u8c03\u6548\u679c\u3002", "method": "\u56db\u9636\u6bb5\u7ba1\u9053\uff1a1)\u79cd\u5b50\u95ee\u9898\u7b5b\u9009\uff08\u9ad8\u4fe1\u606f\u91cf\u3001\u590d\u6742\u6027\u3001\u6e05\u6670\u5ea6\uff09\uff1b2)\u591a\u4ee3\u7406\u95ee\u9898\u91cd\u8ff0\uff08\u751f\u6210\u591a\u6837\u5316\u903b\u8f91\u4e00\u81f4\u7684\u91cd\u8ff0\uff09\uff1b3)\u7b54\u6848\u589e\u5f3a\uff08\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u91cd\u5199\u7b54\u6848\uff0c\u63d0\u9ad8\u6570\u503c\u548c\u903b\u8f91\u6b63\u786e\u6027\uff09\uff1b4)\u95ee\u7b54\u5bf9\u8bc4\u4f30\uff08\u4fdd\u7559\u6700\u4f18\u5bf9\uff09\u3002", "result": "\u57283B-8B\u53c2\u6570\u7684LLM\u4e0a\uff0c\u4ec5\u4f7f\u75283-6\u4e07\u6570\u5b66\u6837\u672c\u8fdb\u884c\u5fae\u8c03\uff0c\u5c31\u80fd\u5728\u591a\u6837\u5316\u7684\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4f7f\u752840\u4e07\u6216230\u4e07\u6837\u672c\u7684\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u6709\u9488\u5bf9\u6027\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u6210\u6bd4\u5927\u89c4\u6a21\u4f4e\u8d28\u91cf\u6570\u636e\u66f4\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.19363", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19363", "abs": "https://arxiv.org/abs/2510.19363", "authors": ["Siyuan Wang", "Gaokai Zhang", "Li Lyna Zhang", "Ning Shang", "Fan Yang", "Dongyao Chen", "Mao Yang"], "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts", "comment": null, "summary": "Reasoning over long contexts is essential for large language models. While\nreinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required for\nlong-context reasoning remain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduce LoongRL, a data-driven RL method for\nadvanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis\napproach that transforms short multi-hop QA into high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training on\nKeyChain data induces an emergent plan-retrieve-reason-recheck reasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-length RL rollout costs.\nOn Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA\naccuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)\nand DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all\n128K needle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities.", "AI": {"tldr": "LoongRL\u662f\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7KeyChain\u6280\u672f\u5c06\u77ed\u591a\u8df3\u95ee\u7b54\u8f6c\u6362\u4e3a\u9ad8\u96be\u5ea6\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\uff0c\u8bad\u7ec3\u6a21\u578b\u572816K\u957f\u5ea6\u4e0b\u6709\u6548\u89e3\u51b3128K\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u77ed\u4e0a\u4e0b\u6587\uff0c\u7f3a\u4e4f\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u9ad8\u7ea7\u601d\u7ef4\u6a21\u5f0f\u8bad\u7ec3\u6570\u636e\u548c\u65b9\u6cd5\u3002", "method": "\u63d0\u51faKeyChain\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d2\u5165UUID\u94fe\u5c06\u77ed\u591a\u8df3\u95ee\u7b54\u8f6c\u6362\u4e3a\u9ad8\u96be\u5ea6\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u9010\u6b65\u8ffd\u8e2a\u6b63\u786e\u94fe\u3001\u8bc6\u522b\u771f\u5b9e\u95ee\u9898\u3001\u68c0\u7d22\u76f8\u5173\u4e8b\u5b9e\u5e76\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728Qwen2.5-7B\u548c14B\u4e0a\uff0cLoongRL\u5c06\u957f\u4e0a\u4e0b\u6587\u591a\u8df3\u95ee\u7b54\u51c6\u786e\u7387\u5206\u522b\u63d0\u534723.5%\u548c21.1%\uff0cLoongRL-14B\u8fbe\u523074.2\u5206\uff0c\u5ab2\u7f8e\u66f4\u5927\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "LoongRL\u8bf1\u5bfc\u51fa\u8ba1\u5212-\u68c0\u7d22-\u63a8\u7406-\u590d\u67e5\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u80fd\u6cdb\u5316\u5230\u8fdc\u8d85\u8bad\u7ec3\u957f\u5ea6\u7684\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u77ed\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.19348", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19348", "abs": "https://arxiv.org/abs/2510.19348", "authors": ["Paul Strang", "Zacharie Al\u00e8s", "C\u00f4me Bissuel", "Olivier Juan", "Safia Kedad-Sidhoum", "Emmanuel Rachelson"], "title": "A Markov Decision Process for Variable Selection in Branch & Bound", "comment": null, "summary": "Mixed-Integer Linear Programming (MILP) is a powerful framework used to\naddress a wide range of NP-hard combinatorial optimization problems, often\nsolved by Branch and Bound (B&B). A key factor influencing the performance of\nB&B solvers is the variable selection heuristic governing branching decisions.\nRecent contributions have sought to adapt reinforcement learning (RL)\nalgorithms to the B&B setting to learn optimal branching policies, through\nMarkov Decision Processes (MDP) inspired formulations, and ad hoc convergence\ntheorems and algorithms. In this work, we introduce BBMDP, a principled vanilla\nMDP formulation for variable selection in B&B, allowing to leverage a broad\nrange of RL algorithms for the purpose of learning optimal B\\&B heuristics.\nComputational experiments validate our model empirically, as our branching\nagent outperforms prior state-of-the-art RL agents on four standard MILP\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86BBMDP\uff0c\u4e00\u79cd\u7528\u4e8e\u5206\u652f\u5b9a\u754c\u4e2d\u53d8\u91cf\u9009\u62e9\u7684MDP\u516c\u5f0f\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b66\u4e60\u6700\u4f18\u5206\u652f\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u5728\u56db\u4e2a\u6807\u51c6MILP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684RL\u4ee3\u7406\u3002", "motivation": "\u5206\u652f\u5b9a\u754c\u6c42\u89e3\u5668\u4e2d\u53d8\u91cf\u9009\u62e9\u542f\u53d1\u5f0f\u5bf9\u6027\u80fd\u5f71\u54cd\u91cd\u5927\uff0c\u73b0\u6709RL\u65b9\u6cd5\u7f3a\u4e4f\u539f\u5219\u6027\u7684MDP\u516c\u5f0f\uff0c\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u7684\u6536\u655b\u5b9a\u7406\u548c\u7b97\u6cd5\u3002", "method": "\u5f15\u5165BBMDP\uff0c\u4e00\u4e2a\u539f\u5219\u6027\u7684\u6807\u51c6MDP\u516c\u5f0f\uff0c\u7528\u4e8e\u5206\u652f\u5b9a\u754c\u4e2d\u7684\u53d8\u91cf\u9009\u62e9\uff0c\u53ef\u4ee5\u5e7f\u6cdb\u5229\u7528\u5404\u79cdRL\u7b97\u6cd5\u6765\u5b66\u4e60\u6700\u4f18\u5206\u652f\u542f\u53d1\u5f0f\u3002", "result": "\u8ba1\u7b97\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u7684\u5206\u652f\u4ee3\u7406\u5728\u56db\u4e2a\u6807\u51c6MILP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4e4b\u524d\u6700\u5148\u8fdb\u7684RL\u4ee3\u7406\u3002", "conclusion": "BBMDP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684MDP\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u5206\u652f\u5b9a\u754c\u4e2d\u7684\u6700\u4f18\u53d8\u91cf\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6c42\u89e3\u5668\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.19644", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19644", "abs": "https://arxiv.org/abs/2510.19644", "authors": ["Daria Cherniuk", "Nikita Sukhorukov", "Nikita Sushko", "Daniil Gusak", "Danil Sivtsov", "Elena Tutubalina", "Evgeny Frolov"], "title": "LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation", "comment": null, "summary": "Retrieval-augmented generation has emerged as one of the most effective\napproaches for code completion, particularly when context from a surrounding\nrepository is essential. However, incorporating context significantly extends\nsequence length, leading to slower inference - a critical limitation for\ninteractive settings such as IDEs. In this work, we introduce LlavaCode, a\nframework that compresses code into compact, semantically rich representations\ninterpretable by code LLM, enhancing generation quality while reducing the\nretrieved context to only a few compressed single-token vectors. Using a small\nprojector module we can significantly increase the EM and ES metrics of coding\nmodel with negligible latency increase. Our experiments demonstrate that\ncompressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on\nline completion tasks compared to full-RAG pipelines.", "AI": {"tldr": "LlavaCode\u6846\u67b6\u901a\u8fc7\u5c06\u4ee3\u7801\u538b\u7f29\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u8868\u793a\u6765\u63d0\u5347\u4ee3\u7801\u8865\u5168\u6548\u7387\uff0c\u51cf\u5c11\u68c0\u7d22\u4e0a\u4e0b\u6587\u81f3\u51e0\u4e2a\u538b\u7f29\u7684\u5355\u6807\u8bb0\u5411\u91cf\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u4ee3\u7801\u8865\u5168\u4e2d\u5f88\u6709\u6548\uff0c\u4f46\u5f15\u5165\u4e0a\u4e0b\u6587\u4f1a\u663e\u8457\u589e\u52a0\u5e8f\u5217\u957f\u5ea6\uff0c\u5bfc\u81f4\u63a8\u7406\u53d8\u6162\uff0c\u8fd9\u5728IDE\u7b49\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u662f\u4e25\u91cd\u9650\u5236\u3002", "method": "\u4f7f\u7528\u5c0f\u578b\u6295\u5f71\u5668\u6a21\u5757\u5c06\u4ee3\u7801\u538b\u7f29\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u8868\u793a\uff0c\u4f7f\u4ee3\u7801LLM\u80fd\u591f\u7406\u89e3\uff0c\u5c06\u68c0\u7d22\u4e0a\u4e0b\u6587\u51cf\u5c11\u5230\u4ec5\u51e0\u4e2a\u538b\u7f29\u7684\u5355\u6807\u8bb0\u5411\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u538b\u7f29\u4e0a\u4e0b\u6587\u5728\u7ebf\u8865\u5168\u4efb\u52a1\u4e2d\u76f8\u6bd4\u5b8c\u6574RAG\u6d41\u6c34\u7ebf\u5b9e\u73b0\u4e8620-38%\u7684\u9996\u6807\u8bb0\u65f6\u95f4\u51cf\u5c11\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86EM\u548cES\u6307\u6807\u3002", "conclusion": "LlavaCode\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u538b\u7f29\u5728\u4fdd\u6301\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u4ee3\u7801\u8865\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2510.19669", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19669", "abs": "https://arxiv.org/abs/2510.19669", "authors": ["Xiang Liu", "Xuming Hu", "Xiaowen Chu", "Eunsol Choi"], "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference", "comment": null, "summary": "Recent reasoning Large Language Models (LLMs) demonstrate remarkable\nproblem-solving abilities but often generate long thinking traces whose utility\nis unclear. Our work aims to improve their efficiency, enabling them to reach\nhigh performance without overthinking. First, we analyze the entropy of token\nprobabilities in reasoning traces. Across three models, we observe a consistent\nU-shaped entropy pattern: high entropy on easy problems despite high accuracy,\nlow entropy on problems with medium difficulty, and high entropy on hard\nproblems reflecting uncertainty. Specifically, we notice 22--25\\% entropy\nreduction from easy to medium difficulty regions, suggesting an {overthinking}\nphenomenon on easy instances. Building on these insights, we introduce\n\\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard\ninference strategies per question based on their difficulty and reasoning trace\nentropy. Each inference strategy consists of a fixed prompt, temperature and\nmaximum token length. In contrast to existing efficiency optimization methods,\nour approach does not fine-tune base LLM but a small probe that classifies\nLLM's final hidden state, allowing inexpensive adaptation. We comprehensively\nevaluate our method on five models and eight benchmarks. Our method achieves\ncomparable or improved accuracy while reducing token usage by up to 22.4\\%,\nestablishing a practical path toward compute-efficient reasoning.", "AI": {"tldr": "\u63d0\u51faDiffAdapt\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u7684\u71b5\u6a21\u5f0f\u6765\u8bc6\u522b\u95ee\u9898\u96be\u5ea6\uff0c\u4e3a\u4e0d\u540c\u96be\u5ea6\u7684\u95ee\u9898\u9009\u62e9\u4e0d\u540c\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c1122.4%\u7684token\u4f7f\u7528\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u7ecf\u5e38\u4ea7\u751f\u5197\u957f\u7684\u601d\u8003\u8f68\u8ff9\uff0c\u4f46\u5176\u6548\u7528\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u4f7f\u6a21\u578b\u65e0\u9700\u8fc7\u5ea6\u601d\u8003\u5c31\u80fd\u8fbe\u5230\u9ad8\u6027\u80fd\u3002", "method": "\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u4e2dtoken\u6982\u7387\u7684\u71b5\u6a21\u5f0f\uff0c\u53d1\u73b0U\u5f62\u71b5\u5206\u5e03\u3002\u57fa\u4e8e\u6b64\u63d0\u51faDiffAdapt\u6846\u67b6\uff0c\u6839\u636e\u95ee\u9898\u96be\u5ea6\u548c\u63a8\u7406\u8f68\u8ff9\u71b5\u9009\u62e9Easy/Normal/Hard\u63a8\u7406\u7b56\u7565\uff0c\u6bcf\u4e2a\u7b56\u7565\u5305\u542b\u56fa\u5b9a\u7684\u63d0\u793a\u3001\u6e29\u5ea6\u548c\u6700\u5927token\u957f\u5ea6\u3002", "result": "\u57285\u4e2a\u6a21\u578b\u548c8\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06token\u4f7f\u7528\u91cf\u51cf\u5c11\u9ad8\u8fbe22.4%\u3002", "conclusion": "DiffAdapt\u4e3a\u8ba1\u7b97\u9ad8\u6548\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u65e0\u9700\u5fae\u8c03\u57fa\u7840LLM\uff0c\u4ec5\u9700\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684\u5206\u7c7b\u5668\u6765\u9002\u5e94\u4e0d\u540c\u96be\u5ea6\u7684\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.19507", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19507", "abs": "https://arxiv.org/abs/2510.19507", "authors": ["Demian Till", "John Smeaton", "Peter Haubrick", "Gouse Saheb", "Florian Graef", "David Berman"], "title": "Teaming LLMs to Detect and Mitigate Hallucinations", "comment": "Accepted to NeurIPS 2025 workshop on Reliable ML from Unreliable Data", "summary": "Recent work has demonstrated state-of-the-art results in large language model\n(LLM) hallucination detection and mitigation through consistency-based\napproaches which involve aggregating multiple responses sampled from a single\nLLM for a given prompt. These approaches help offset limitations stemming from\nthe imperfect data on which LLMs are trained, which includes biases and\nunder-representation of information required at deployment time among other\nlimitations which can lead to hallucinations. We show that extending these\nsingle-model consistency methods to combine responses from multiple LLMs with\ndifferent training data, training schemes and model architectures can result in\nsubstantial further improvements in hallucination detection and mitigation\ncapabilities beyond their single-model consistency counterparts. We evaluate\nthis \\emph{consortium consistency} approach across many model teams from a pool\nof 15 LLMs and explore under what conditions it is beneficial to team together\ndifferent LLMs in this manner. Further, we show that these performance\nimprovements often come with reduced inference costs, offsetting a significant\ndrawback with single-model consistency methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u8054\u76df\u4e00\u81f4\u6027\"\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u6765\u81ea\u4e0d\u540cLLM\u7684\u591a\u4e2a\u54cd\u5e94\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u5e7b\u89c9\uff0c\u76f8\u6bd4\u5355\u6a21\u578b\u4e00\u81f4\u6027\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u4e14\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6a21\u578b\u4e00\u81f4\u6027\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u7f13\u89e3LLM\u5e7b\u89c9\u65b9\u9762\u53d6\u5f97\u4e86\u5148\u8fdb\u6210\u679c\uff0c\u4f46\u4ecd\u6709\u5c40\u9650\u6027\u3002\u4e0d\u540cLLM\u5177\u6709\u4e0d\u540c\u7684\u8bad\u7ec3\u6570\u636e\u3001\u8bad\u7ec3\u65b9\u6848\u548c\u6a21\u578b\u67b6\u6784\uff0c\u6574\u5408\u5b83\u4eec\u7684\u54cd\u5e94\u53ef\u4ee5\u5e26\u6765\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "method": "\u5c06\u5355\u6a21\u578b\u4e00\u81f4\u6027\u65b9\u6cd5\u6269\u5c55\u5230\u591a\u4e2aLLM\uff0c\u5f62\u6210\"\u8054\u76df\u4e00\u81f4\u6027\"\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u6765\u81ea\u4e0d\u540cLLM\u7684\u54cd\u5e94\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u5e7b\u89c9\u3002\u8bc4\u4f30\u4e86\u6765\u81ea15\u4e2aLLM\u7684\u4e0d\u540c\u6a21\u578b\u7ec4\u5408\uff0c\u5e76\u63a2\u7d22\u4e86\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u8fd9\u79cd\u7ec4\u5408\u662f\u6709\u76ca\u7684\u3002", "result": "\u8054\u76df\u4e00\u81f4\u6027\u65b9\u6cd5\u5728\u5e7b\u89c9\u68c0\u6d4b\u548c\u7f13\u89e3\u80fd\u529b\u4e0a\u76f8\u6bd4\u5355\u6a21\u578b\u4e00\u81f4\u6027\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6027\u80fd\u6539\u8fdb\u901a\u5e38\u4f34\u968f\u7740\u63a8\u7406\u6210\u672c\u7684\u964d\u4f4e\u3002", "conclusion": "\u6574\u5408\u591a\u4e2a\u4e0d\u540cLLM\u7684\u54cd\u5e94\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5e7b\u89c9\u68c0\u6d4b\u548c\u7f13\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u4e3aLLM\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.19687", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19687", "abs": "https://arxiv.org/abs/2510.19687", "authors": ["Addison J. Wu", "Ryan Liu", "Kerem Oktar", "Theodore R. Sumers", "Thomas L. Griffiths"], "title": "Are Large Language Models Sensitive to the Motives Behind Communication?", "comment": "NeurIPS 2025", "summary": "Human communication is motivated: people speak, write, and create content\nwith a particular communicative intent in mind. As a result, information that\nlarge language models (LLMs) and AI agents process is inherently framed by\nhumans' intentions and incentives. People are adept at navigating such nuanced\ninformation: we routinely identify benevolent or self-serving motives in order\nto decide what statements to trust. For LLMs to be effective in the real world,\nthey too must critically evaluate content by factoring in the motivations of\nthe source -- for instance, weighing the credibility of claims made in a sales\npitch. In this paper, we undertake a comprehensive study of whether LLMs have\nthis capacity for motivational vigilance. We first employ controlled\nexperiments from cognitive science to verify that LLMs' behavior is consistent\nwith rational models of learning from motivated testimony, and find they\nsuccessfully discount information from biased sources in a human-like manner.\nWe then extend our evaluation to sponsored online adverts, a more naturalistic\nreflection of LLM agents' information ecosystems. In these settings, we find\nthat LLMs' inferences do not track the rational models' predictions nearly as\nclosely -- partly due to additional information that distracts them from\nvigilance-relevant considerations. However, a simple steering intervention that\nboosts the salience of intentions and incentives substantially increases the\ncorrespondence between LLMs and the rational model. These results suggest that\nLLMs possess a basic sensitivity to the motivations of others, but generalizing\nto novel real-world settings will require further improvements to these models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLMs\u662f\u5426\u5177\u5907\u52a8\u673a\u8b66\u89c9\u80fd\u529b\uff0c\u5373\u80fd\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u8003\u8651\u4fe1\u606f\u6e90\u7684\u52a8\u673a\u6765\u8bc4\u4f30\u5185\u5bb9\u53ef\u4fe1\u5ea6\u3002\u7814\u7a76\u53d1\u73b0LLMs\u5728\u53d7\u63a7\u5b9e\u9a8c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u5e7f\u544a\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u5f15\u5bfc\u5e72\u9884\u53ef\u663e\u8457\u63d0\u5347\u5176\u8868\u73b0\u3002", "motivation": "\u4eba\u7c7b\u6c9f\u901a\u5177\u6709\u52a8\u673a\u6027\uff0c\u800cLLMs\u5904\u7406\u7684\u4fe1\u606f\u4e5f\u53d7\u5230\u4eba\u7c7b\u610f\u56fe\u548c\u52a8\u673a\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u8ba9LLMs\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6709\u6548\u8fd0\u4f5c\uff0c\u5b83\u4eec\u5fc5\u987b\u80fd\u591f\u6279\u5224\u6027\u5730\u8bc4\u4f30\u5185\u5bb9\uff0c\u8003\u8651\u4fe1\u606f\u6e90\u7684\u52a8\u673a\u3002", "method": "\u9996\u5148\u4f7f\u7528\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u53d7\u63a7\u5b9e\u9a8c\u9a8c\u8bc1LLMs\u884c\u4e3a\u662f\u5426\u4e0e\u7406\u6027\u6a21\u578b\u4e00\u81f4\uff0c\u7136\u540e\u6269\u5c55\u5230\u66f4\u81ea\u7136\u7684\u5728\u7ebf\u5e7f\u544a\u573a\u666f\uff0c\u5e76\u6d4b\u8bd5\u7b80\u5355\u7684\u5f15\u5bfc\u5e72\u9884\u6548\u679c\u3002", "result": "LLMs\u5728\u53d7\u63a7\u5b9e\u9a8c\u4e2d\u80fd\u6210\u529f\u50cf\u4eba\u7c7b\u4e00\u6837\u6298\u6263\u6709\u504f\u89c1\u6765\u6e90\u7684\u4fe1\u606f\uff0c\u4f46\u5728\u771f\u5b9e\u5e7f\u544a\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u7b80\u5355\u7684\u5f15\u5bfc\u5e72\u9884\u663e\u8457\u63d0\u9ad8\u4e86LLMs\u4e0e\u7406\u6027\u6a21\u578b\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "LLMs\u5177\u5907\u5bf9\u4ed6\u4eba\u52a8\u673a\u7684\u57fa\u672c\u654f\u611f\u6027\uff0c\u4f46\u8981\u6cdb\u5316\u5230\u65b0\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u8fd9\u4e9b\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2510.19791", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19791", "abs": "https://arxiv.org/abs/2510.19791", "authors": ["Saptarshi Sengupta", "Zhengyu Zhou", "Jun Araki", "Xingbo Wang", "Bingqing Wang", "Suhang Wang", "Zhe Feng"], "title": "ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers", "comment": null, "summary": "Tool calling has become increasingly popular for Large Language Models\n(LLMs). However, for large tool sets, the resulting tokens would exceed the\nLLM's context window limit, making it impossible to include every tool. Hence,\nan external retriever is used to provide LLMs with the most relevant tools for\na query. Existing retrieval models rank tools based on the similarity between a\nuser query and a tool description (TD). This leads to suboptimal retrieval as\nuser requests are often poorly aligned with the language of TD. To remedy the\nissue, we propose ToolDreamer, a framework to condition retriever models to\nfetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,\ndescription of tools that the LLM feels will be potentially useful for the\nquery. The framework enables a more natural alignment between queries and tools\nwithin the language space of TD's. We apply ToolDreamer on the ToolRet dataset\nand show that our method improves the performance of sparse and dense\nretrievers with and without training, thus showcasing its flexibility. Through\nour proposed framework, our aim is to offload a portion of the reasoning burden\nto the retriever so that the LLM may effectively handle a large collection of\ntools without inundating its context window.", "AI": {"tldr": "ToolDreamer\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5047\u8bbe\u6027\u5de5\u5177\u63cf\u8ff0\u6765\u6539\u8fdb\u5de5\u5177\u68c0\u7d22\uff0c\u4f7f\u68c0\u7d22\u5668\u80fd\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u67e5\u8be2\u4e0e\u5de5\u5177\u63cf\u8ff0\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u5927\u578b\u5de5\u5177\u96c6\u7684\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u57fa\u4e8e\u7528\u6237\u67e5\u8be2\u4e0e\u5de5\u5177\u63cf\u8ff0\u7684\u76f8\u4f3c\u5ea6\u8fdb\u884c\u5de5\u5177\u6392\u540d\uff0c\u4f46\u7528\u6237\u8bf7\u6c42\u5f80\u5f80\u4e0e\u5de5\u5177\u63cf\u8ff0\u8bed\u8a00\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u68c0\u7d22\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u89e3\u51b3\u5927\u578b\u5de5\u5177\u96c6\u8d85\u51faLLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faToolDreamer\u6846\u67b6\uff0c\u4f7f\u7528LLM\u751f\u6210\u5047\u8bbe\u6027\uff08\u5408\u6210\uff09\u5de5\u5177\u63cf\u8ff0\uff0c\u8ba9\u68c0\u7d22\u5668\u57fa\u4e8e\u8fd9\u4e9b\u63cf\u8ff0\u6765\u83b7\u53d6\u5de5\u5177\uff0c\u5b9e\u73b0\u67e5\u8be2\u4e0e\u5de5\u5177\u5728\u63cf\u8ff0\u8bed\u8a00\u7a7a\u95f4\u4e2d\u7684\u81ea\u7136\u5bf9\u9f50\u3002", "result": "\u5728ToolRet\u6570\u636e\u96c6\u4e0a\u5e94\u7528ToolDreamer\uff0c\u63d0\u9ad8\u4e86\u7a00\u758f\u548c\u5bc6\u96c6\u68c0\u7d22\u5668\u7684\u6027\u80fd\uff0c\u65e0\u8bba\u662f\u5426\u7ecf\u8fc7\u8bad\u7ec3\u90fd\u8868\u73b0\u51fa\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "ToolDreamer\u80fd\u591f\u5c06\u90e8\u5206\u63a8\u7406\u8d1f\u62c5\u8f6c\u79fb\u5230\u68c0\u7d22\u5668\u4e0a\uff0c\u4f7fLLM\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u578b\u5de5\u5177\u96c6\u800c\u4e0d\u4f1a\u8d85\u51fa\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3002", "topic": "agent analysis"}}
{"id": "2510.19807", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19807", "abs": "https://arxiv.org/abs/2510.19807", "authors": ["Xichen Zhang", "Sitong Wu", "Yinghao Zhu", "Haoru Tan", "Shaozuo Yu", "Ziyi He", "Jiaya Jia"], "title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning", "comment": "Code: https://github.com/dvlab-research/Scaf-GRPO", "summary": "Reinforcement learning from verifiable rewards has emerged as a powerful\ntechnique for enhancing the complex reasoning abilities of Large Language\nModels (LLMs). However, these methods are fundamentally constrained by the\n''learning cliff'' phenomenon: when faced with problems far beyond their\ncurrent capabilities, models consistently fail, yielding a persistent\nzero-reward signal. In policy optimization algorithms like GRPO, this collapses\nthe advantage calculation to zero, rendering these difficult problems invisible\nto the learning gradient and stalling progress. To overcome this, we introduce\nScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive\ntraining framework that strategically provides minimal guidance only when a\nmodel's independent learning has plateaued. The framework first diagnoses\nlearning stagnation and then intervenes by injecting tiered in-prompt hints,\nranging from abstract concepts to concrete steps, enabling the model to\nconstruct a valid solution by itself. Extensive experiments on challenging\nmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the\npass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative\n44.3% over a vanilla GRPO baseline. This result demonstrates our framework\nprovides a robust and effective methodology for unlocking a model's ability to\nsolve problems previously beyond its reach, a critical step towards extending\nthe frontier of autonomous reasoning in LLM.", "AI": {"tldr": "Scaf-GRPO\u901a\u8fc7\u63d0\u4f9b\u6e10\u8fdb\u5f0f\u811a\u624b\u67b6\u63d0\u793a\u6765\u89e3\u51b3LLM\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\"\u5b66\u4e60\u60ac\u5d16\"\u95ee\u9898\uff0c\u5f53\u6a21\u578b\u5b66\u4e60\u505c\u6ede\u65f6\u6ce8\u5165\u5206\u5c42\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\"\u5b66\u4e60\u60ac\u5d16\"\u95ee\u9898\uff1a\u5f53\u9047\u5230\u8fdc\u8d85\u6a21\u578b\u5f53\u524d\u80fd\u529b\u7684\u95ee\u9898\u65f6\uff0c\u6a21\u578b\u6301\u7eed\u5931\u8d25\u4ea7\u751f\u96f6\u5956\u52b1\u4fe1\u53f7\uff0c\u5bfc\u81f4\u4f18\u52bf\u8ba1\u7b97\u5d29\u6e83\u548c\u5b66\u4e60\u505c\u6ede\u3002", "method": "\u63d0\u51faScaf-GRPO\u6846\u67b6\uff0c\u9996\u5148\u8bca\u65ad\u5b66\u4e60\u505c\u6ede\uff0c\u7136\u540e\u6ce8\u5165\u5206\u5c42\u63d0\u793a\uff08\u4ece\u62bd\u8c61\u6982\u5ff5\u5230\u5177\u4f53\u6b65\u9aa4\uff09\uff0c\u8ba9\u6a21\u578b\u81ea\u884c\u6784\u5efa\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ec5\u5728\u72ec\u7acb\u5b66\u4e60\u505c\u6ede\u65f6\u63d0\u4f9b\u6700\u5c0f\u5316\u6307\u5bfc\u3002", "result": "\u5728\u6311\u6218\u6027\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cScaf-GRPO\u5c06Qwen2.5-Math-7B\u6a21\u578b\u5728AIME24\u57fa\u51c6\u4e0a\u7684pass@1\u5206\u6570\u76f8\u5bf9GRPO\u57fa\u7ebf\u63d0\u5347\u4e8644.3%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u9501LLM\u89e3\u51b3\u8d85\u51fa\u5176\u80fd\u529b\u8303\u56f4\u95ee\u9898\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u7a33\u5065\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u662f\u63a8\u8fdb\u81ea\u4e3b\u63a8\u7406\u524d\u6cbf\u7684\u5173\u952e\u6b65\u9aa4\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.19811", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19811", "abs": "https://arxiv.org/abs/2510.19811", "authors": ["Johnny Tian-Zheng Wei", "Ameya Godbole", "Mohammad Aflah Khan", "Ryan Wang", "Xiaoyuan Zhu", "James Flemings", "Nitya Kashyap", "Krishna P. Gummadi", "Willie Neiswanger", "Robin Jia"], "title": "Hubble: a Model Suite to Advance the Study of LLM Memorization", "comment": null, "summary": "We present Hubble, a suite of fully open-source large language models (LLMs)\nfor the scientific study of LLM memorization. Hubble models come in standard\nand perturbed variants: standard models are pretrained on a large English\ncorpus, and perturbed models are trained in the same way but with controlled\ninsertion of text (e.g., book passages, biographies, and test sets) designed to\nemulate key memorization risks. Our core release includes 8 models -- standard\nand perturbed models with 1B or 8B parameters, pretrained on 100B or 500B\ntokens -- establishing that memorization risks are determined by the frequency\nof sensitive data relative to size of the training corpus (i.e., a password\nappearing once in a smaller corpus is memorized better than the same password\nin a larger corpus). Our release also includes 6 perturbed models with text\ninserted at different pretraining phases, showing that sensitive data without\ncontinued exposure can be forgotten. These findings suggest two best practices\nfor addressing memorization risks: to dilute sensitive data by increasing the\nsize of the training corpus, and to order sensitive data to appear earlier in\ntraining. Beyond these general empirical findings, Hubble enables a broad range\nof memorization research; for example, analyzing the biographies reveals how\nreadily different types of private information are memorized. We also\ndemonstrate that the randomized insertions in Hubble make it an ideal testbed\nfor membership inference and machine unlearning, and invite the community to\nfurther explore, benchmark, and build upon our work.", "AI": {"tldr": "Hubble\u662f\u4e00\u5957\u7528\u4e8e\u7814\u7a76LLM\u8bb0\u5fc6\u673a\u5236\u7684\u5f00\u6e90\u6a21\u578b\u5957\u4ef6\uff0c\u5305\u542b\u6807\u51c6\u6a21\u578b\u548c\u6270\u52a8\u6a21\u578b\uff0c\u901a\u8fc7\u63a7\u5236\u6587\u672c\u63d2\u5165\u6765\u6a21\u62df\u8bb0\u5fc6\u98ce\u9669\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u6570\u636e\u9891\u7387\u548c\u8bad\u7ec3\u9636\u6bb5\u5bf9\u8bb0\u5fc6\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76LLM\u8bb0\u5fc6\u673a\u5236\uff0c\u7279\u522b\u662f\u654f\u611f\u6570\u636e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8bb0\u5fc6\u98ce\u9669\uff0c\u4e3a\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\u3002", "method": "\u5f00\u53d1\u6807\u51c6\u6a21\u578b\u548c\u6270\u52a8\u6a21\u578b\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u63a7\u5236\u63d2\u5165\u7279\u5b9a\u6587\u672c\uff08\u5982\u4e66\u7c4d\u6bb5\u843d\u3001\u4f20\u8bb0\u3001\u6d4b\u8bd5\u96c6\uff09\uff0c\u5206\u6790\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u3001\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u8bad\u7ec3\u9636\u6bb5\u5bf9\u8bb0\u5fc6\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u8bb0\u5fc6\u98ce\u9669\u53d6\u51b3\u4e8e\u654f\u611f\u6570\u636e\u5728\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u76f8\u5bf9\u9891\u7387\uff0c\u4ee5\u53ca\u654f\u611f\u6570\u636e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6301\u7eed\u66b4\u9732\u7a0b\u5ea6\u3002\u589e\u52a0\u8bad\u7ec3\u8bed\u6599\u89c4\u6a21\u53ef\u4ee5\u7a00\u91ca\u654f\u611f\u6570\u636e\uff0c\u800c\u8ba9\u654f\u611f\u6570\u636e\u5728\u8bad\u7ec3\u65e9\u671f\u51fa\u73b0\u6709\u52a9\u4e8e\u8bb0\u5fc6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u6700\u4f73\u5b9e\u8df5\uff1a\u901a\u8fc7\u589e\u52a0\u8bad\u7ec3\u8bed\u6599\u89c4\u6a21\u7a00\u91ca\u654f\u611f\u6570\u636e\uff0c\u4ee5\u53ca\u8ba9\u654f\u611f\u6570\u636e\u5728\u8bad\u7ec3\u65e9\u671f\u51fa\u73b0\u3002Hubble\u6a21\u578b\u5957\u4ef6\u4e3a\u8bb0\u5fc6\u7814\u7a76\u3001\u6210\u5458\u63a8\u65ad\u548c\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u7406\u60f3\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.0f30bfd4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.maginative.com%2Farticle%2Fopenai-launches-chatgpt-atlas-browser-with-built-in-agent-mode%2F%3Futm_source=tldrmarketing/1/0100019a0b9828da-09892b91-bb8d-473d-9502-b43c075162b3-000000/wskbY7im6xRykvh6sTUX1Ytw1IxWff3IcXdOe-SRkDc=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.maginative.com%2Farticle%2Fopenai-launches-chatgpt-atlas-browser-with-built-in-agent-mode%2F%3Futm_source=tldrmarketing/1/0100019a0b9828da-09892b91-bb8d-473d-9502-b43c075162b3-000000/wskbY7im6xRykvh6sTUX1Ytw1IxWff3IcXdOe-SRkDc=427", "authors": ["TLDR Newsletter"], "title": "OpenAI Launches ChatGPT Atlas Browser With Built-In Agent Mode", "comment": "Source: TLDR Newsletter, Date: 2025-10-22, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.maginative.com%2Farticle%2Fopenai-launches-chatgpt-atlas-browser-with-built-in-agent-mode%2F%3Futm_source=tldrmarketing/1/0100019a0b9828da-09892b91-bb8d-473d-9502-b43c075162b3-000000/wskbY7im6xRykvh6sTUX1Ytw1IxWff3IcXdOe-SRkDc=427", "summary": "OpenAI Launches ChatGPT Atlas Browser With Built-In Agent Mode (2 minute read) Atlas is a browser that integrates ChatGPT directly into a persistent sidebar. The browser can read, summarize, and interact with webpages, while an exclusive Agent Mode for Plus and Pro users allows ChatGPT to complete actions like filling carts or creating tickets. Atlas also stores user \u201cmemories\u201d to personalize browsing, though users can opt out. It's now available on macOS.", "source": "tldr", "AI": {"tldr": "OpenAI\u63a8\u51fa\u5185\u7f6eAgent\u6a21\u5f0f\u7684ChatGPT Atlas\u6d4f\u89c8\u5668\uff0c\u53ef\u5728\u4fa7\u8fb9\u680f\u76f4\u63a5\u96c6\u6210ChatGPT\uff0c\u5177\u5907\u7f51\u9875\u9605\u8bfb\u3001\u603b\u7ed3\u548c\u4ea4\u4e92\u529f\u80fd\uff0cPlus\u548cPro\u7528\u6237\u53ef\u4f7f\u7528Agent\u6a21\u5f0f\u5b8c\u6210\u8d2d\u7269\u8f66\u586b\u5145\u3001\u7968\u52a1\u521b\u5efa\u7b49\u64cd\u4f5c", "motivation": "\u5c06AI\u52a9\u624b\u76f4\u63a5\u96c6\u6210\u5230\u6d4f\u89c8\u5668\u4e2d\uff0c\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u7f51\u9875\u4ea4\u4e92\u4f53\u9a8c\uff0c\u901a\u8fc7Agent\u6a21\u5f0f\u5b9e\u73b0\u81ea\u52a8\u5316\u64cd\u4f5c\uff0c\u63d0\u5347\u7528\u6237\u751f\u4ea7\u529b", "method": "\u5f00\u53d1\u5185\u7f6eChatGPT\u7684\u6d4f\u89c8\u5668\uff0c\u63d0\u4f9b\u6301\u4e45\u4fa7\u8fb9\u680f\u754c\u9762\uff0c\u5b9e\u73b0\u7f51\u9875\u5185\u5bb9\u8bfb\u53d6\u548c\u603b\u7ed3\uff0c\u4e3a\u9ad8\u7ea7\u7528\u6237\u63d0\u4f9bAgent\u6a21\u5f0f\u6267\u884c\u5177\u4f53\u4efb\u52a1", "result": "Atlas\u6d4f\u89c8\u5668\u5df2\u5728macOS\u4e0a\u53d1\u5e03\uff0c\u652f\u6301\u7f51\u9875\u4ea4\u4e92\u3001\u4e2a\u6027\u5316\u8bb0\u5fc6\u5b58\u50a8\u548c\u81ea\u52a8\u5316\u64cd\u4f5c\u529f\u80fd", "conclusion": "Atlas\u6d4f\u89c8\u5668\u6210\u529f\u5c06AI\u52a9\u624b\u6df1\u5ea6\u96c6\u6210\u5230\u6d4f\u89c8\u4f53\u9a8c\u4e2d\uff0c\u4e3a\u9ad8\u7ea7\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u80fd\u529b", "topic": "swe application"}}
{"id": "tldr.2510.b1fa9135", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ufried.com%2Fblog%2Fai_assisted_coding%2F%3Futm_source=tldrwebdev/1/0100019a0b9bb411-f1f0bfc7-a9e0-4a33-8000-65b7dfdfb63d-000000/wMbIxYJYwbbo-aTIsc3sOOxAi9Apo53MBdowi-k2WQ0=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ufried.com%2Fblog%2Fai_assisted_coding%2F%3Futm_source=tldrwebdev/1/0100019a0b9bb411-f1f0bfc7-a9e0-4a33-8000-65b7dfdfb63d-000000/wMbIxYJYwbbo-aTIsc3sOOxAi9Apo53MBdowi-k2WQ0=428", "authors": ["TLDR Newsletter"], "title": "Solving the wrong problem", "comment": "Source: TLDR Newsletter, Date: 2025-10-22, Reading time: 21 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ufried.com%2Fblog%2Fai_assisted_coding%2F%3Futm_source=tldrwebdev/1/0100019a0b9bb411-f1f0bfc7-a9e0-4a33-8000-65b7dfdfb63d-000000/wMbIxYJYwbbo-aTIsc3sOOxAi9Apo53MBdowi-k2WQ0=428", "summary": "Solving the wrong problem (21 minute read) AI-assisted coding, despite its impressive capabilities, may be addressing the wrong problem in software development. AI agents often recreate existing code due to the nature of LLMs and training data. Furthermore, AI is primarily benefiting less experienced developers and masking underlying issues like inadequate software engineering education and a focus on speed over quality.", "source": "tldr", "AI": {"tldr": "AI\u8f85\u52a9\u7f16\u7801\u53ef\u80fd\u6b63\u5728\u89e3\u51b3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u9519\u8bef\u95ee\u9898\uff0c\u56e0\u4e3aAI\u4ee3\u7406\u7ecf\u5e38\u91cd\u65b0\u521b\u5efa\u73b0\u6709\u4ee3\u7801\uff0c\u4e3b\u8981\u5e2e\u52a9\u7ecf\u9a8c\u4e0d\u8db3\u7684\u5f00\u53d1\u8005\uff0c\u5e76\u63a9\u76d6\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u4e0d\u8db3\u548c\u91cd\u901f\u5ea6\u8f7b\u8d28\u91cf\u7b49\u6839\u672c\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8AI\u8f85\u52a9\u7f16\u7801\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5f71\u54cd\u548c\u6f5c\u5728\u95ee\u9898\uff0c\u63ed\u793aAI\u53ef\u80fd\u6ca1\u6709\u771f\u6b63\u89e3\u51b3\u8f6f\u4ef6\u5f00\u53d1\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5206\u6790AI\u8f85\u52a9\u7f16\u7801\u7684\u73b0\u72b6\u3001LLM\u8bad\u7ec3\u6570\u636e\u7684\u6027\u8d28\u4ee5\u53caAI\u5bf9\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u5f00\u53d1\u8005\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0AI\u4ee3\u7406\u7ecf\u5e38\u91cd\u65b0\u521b\u5efa\u73b0\u6709\u4ee3\u7801\uff0c\u4e3b\u8981\u4f7f\u7ecf\u9a8c\u4e0d\u8db3\u7684\u5f00\u53d1\u8005\u53d7\u76ca\uff0c\u540c\u65f6\u63a9\u76d6\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u4e0d\u8db3\u548c\u91cd\u901f\u5ea6\u8f7b\u8d28\u91cf\u7684\u6839\u672c\u95ee\u9898\u3002", "conclusion": "AI\u8f85\u52a9\u7f16\u7801\u53ef\u80fd\u6b63\u5728\u89e3\u51b3\u9519\u8bef\u7684\u95ee\u9898\uff0c\u9700\u8981\u5173\u6ce8\u66f4\u6839\u672c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u8d28\u91cf\u548c\u5f00\u53d1\u5b9e\u8df5\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.a42a9d40", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F20%2Fdeepseek-ocr-claude-code%2F%3Futm_source=tldrwebdev/1/0100019a0b9bb411-f1f0bfc7-a9e0-4a33-8000-65b7dfdfb63d-000000/KmPx5Kmkr6E4qWJxiW8T5sAepIb7MBzB-I5f32AJf4I=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F20%2Fdeepseek-ocr-claude-code%2F%3Futm_source=tldrwebdev/1/0100019a0b9bb411-f1f0bfc7-a9e0-4a33-8000-65b7dfdfb63d-000000/KmPx5Kmkr6E4qWJxiW8T5sAepIb7MBzB-I5f32AJf4I=428", "authors": ["TLDR Newsletter"], "title": "Getting DeepSeek-OCR working on an NVIDIA Spark via brute force using Claude Code", "comment": "Source: TLDR Newsletter, Date: 2025-10-22, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F20%2Fdeepseek-ocr-claude-code%2F%3Futm_source=tldrwebdev/1/0100019a0b9bb411-f1f0bfc7-a9e0-4a33-8000-65b7dfdfb63d-000000/KmPx5Kmkr6E4qWJxiW8T5sAepIb7MBzB-I5f32AJf4I=428", "summary": "Getting DeepSeek-OCR working on an NVIDIA Spark via brute force using Claude Code (10 minute read) Claude Code can be used to get DeepSeek-OCR running on an NVIDIA Spark by automating the process within a Docker sandbox. Initially, Claude Code struggled to do this due to PyTorch and CUDA compatibility issues, but after being suggested different PyTorch versions, Claude Code found a compatible version and completed the task. Claude Code generated notes, scripts, and documentation, including a ...", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528Claude Code\u5728NVIDIA Spark\u4e0a\u901a\u8fc7Docker\u6c99\u76d2\u81ea\u52a8\u5316\u90e8\u7f72DeepSeek-OCR\uff0c\u89e3\u51b3\u4e86PyTorch\u548cCUDA\u517c\u5bb9\u6027\u95ee\u9898", "motivation": "\u89e3\u51b3\u5728NVIDIA Spark\u8bbe\u5907\u4e0a\u90e8\u7f72DeepSeek-OCR\u65f6\u9047\u5230\u7684PyTorch\u548cCUDA\u517c\u5bb9\u6027\u6311\u6218", "method": "\u901a\u8fc7Claude Code\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u5728Docker\u6c99\u76d2\u73af\u5883\u4e2d\u5c1d\u8bd5\u4e0d\u540cPyTorch\u7248\u672c\uff0c\u76f4\u5230\u627e\u5230\u517c\u5bb9\u7248\u672c\u5e76\u751f\u6210\u76f8\u5173\u811a\u672c\u548c\u6587\u6863", "result": "\u6210\u529f\u627e\u5230\u517c\u5bb9\u7684PyTorch\u7248\u672c\uff0c\u5b8c\u6210DeepSeek-OCR\u5728NVIDIA Spark\u4e0a\u7684\u90e8\u7f72\uff0c\u5e76\u751f\u6210\u4e86\u5b8c\u6574\u7684\u90e8\u7f72\u7b14\u8bb0\u3001\u811a\u672c\u548c\u6587\u6863", "conclusion": "Claude Code\u80fd\u591f\u6709\u6548\u89e3\u51b3\u590d\u6742\u73af\u5883\u4e0b\u7684\u8f6f\u4ef6\u90e8\u7f72\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u81ea\u52a8\u5316\u5de5\u5177\u5728\u89e3\u51b3\u6280\u672f\u6311\u6218\u65b9\u9762\u5177\u6709\u5b9e\u7528\u4ef7\u503c", "topic": "swe application"}}
{"id": "tldr.2510.ac77b756", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fgenerative-ai%2Fthe-linkedin-generative-ai-application-tech-stack-extending-to-build-ai-agents%3Futm_source=tldrdevops/1/0100019a0ba1fde4-448155c2-3dd0-4dea-b3bd-bfc73c0d9f09-000000/OpX3601UT-947YMZu0O_DVlc1r6lPR3lwaowI3lAOa0=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fgenerative-ai%2Fthe-linkedin-generative-ai-application-tech-stack-extending-to-build-ai-agents%3Futm_source=tldrdevops/1/0100019a0ba1fde4-448155c2-3dd0-4dea-b3bd-bfc73c0d9f09-000000/OpX3601UT-947YMZu0O_DVlc1r6lPR3lwaowI3lAOa0=427", "authors": ["TLDR Newsletter"], "title": "The LinkedIn Generative AI Application Tech Stack: Extending to Build AI Agents", "comment": "Source: TLDR Newsletter, Date: 2025-10-22, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fgenerative-ai%2Fthe-linkedin-generative-ai-application-tech-stack-extending-to-build-ai-agents%3Futm_source=tldrdevops/1/0100019a0ba1fde4-448155c2-3dd0-4dea-b3bd-bfc73c0d9f09-000000/OpX3601UT-947YMZu0O_DVlc1r6lPR3lwaowI3lAOa0=427", "summary": "The LinkedIn Generative AI Application Tech Stack: Extending to Build AI Agents (10 minute read) LinkedIn's generative AI application tech stack was updated to improve AI agents by enabling them to think, plan, and act with users, with the Hiring Assistant being globally available in English by the end of September. Key to this development were defining agents through gRPC service schema definitions and leveraging LinkedIn's messaging system for multi-agent orchestration. Agent interactions w...", "source": "tldr", "AI": {"tldr": "LinkedIn\u66f4\u65b0\u4e86\u751f\u6210\u5f0fAI\u5e94\u7528\u6280\u672f\u6808\uff0c\u901a\u8fc7\u5b9a\u4e49gRPC\u670d\u52a1\u6a21\u5f0f\u548c\u5229\u7528\u6d88\u606f\u7cfb\u7edf\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u7f16\u6392\uff0c\u4f7fAI\u667a\u80fd\u4f53\u80fd\u591f\u601d\u8003\u3001\u89c4\u5212\u548c\u4e0e\u7528\u6237\u4e92\u52a8\uff0c\u62db\u8058\u52a9\u624b\u5df2\u57289\u6708\u5e95\u5168\u7403\u82f1\u8bed\u7248\u4e0a\u7ebf\u3002", "motivation": "\u6269\u5c55LinkedIn\u7684\u751f\u6210\u5f0fAI\u5e94\u7528\u6280\u672f\u6808\uff0c\u63d0\u5347AI\u667a\u80fd\u4f53\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u601d\u8003\u3001\u89c4\u5212\u548c\u4e0e\u7528\u6237\u4e92\u52a8\uff0c\u7279\u522b\u662f\u4e3a\u62db\u8058\u52a9\u624b\u7b49\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49gRPC\u670d\u52a1\u6a21\u5f0f\u6765\u63cf\u8ff0\u667a\u80fd\u4f53\uff0c\u5e76\u5229\u7528LinkedIn\u7684\u6d88\u606f\u7cfb\u7edf\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u7f16\u6392\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u7684\u534f\u4f5c\u548c\u4ea4\u4e92\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u80fd\u591f\u601d\u8003\u3001\u89c4\u5212\u548c\u884c\u52a8\u7684AI\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u62db\u8058\u52a9\u624b\u5df2\u57289\u6708\u5e95\u5b9e\u73b0\u5168\u7403\u82f1\u8bed\u7248\u53ef\u7528\u3002", "conclusion": "LinkedIn\u7684\u751f\u6210\u5f0fAI\u6280\u672f\u6808\u6269\u5c55\u6709\u6548\u63d0\u5347\u4e86AI\u667a\u80fd\u4f53\u7684\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.5098bfe4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftowardsdatascience.com%2Fbeyond-rag%2F%3Futm_source=tldrdata/1/0100019a10899501-ec02b680-f7d6-4c62-97ff-70fe1bbfa982-000000/Jsq4aHSQ4_NA_c70SAJfN-uKnk96phrD1G2LMXKt32M=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftowardsdatascience.com%2Fbeyond-rag%2F%3Futm_source=tldrdata/1/0100019a10899501-ec02b680-f7d6-4c62-97ff-70fe1bbfa982-000000/Jsq4aHSQ4_NA_c70SAJfN-uKnk96phrD1G2LMXKt32M=428", "authors": ["TLDR Newsletter"], "title": "Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftowardsdatascience.com%2Fbeyond-rag%2F%3Futm_source=tldrdata/1/0100019a10899501-ec02b680-f7d6-4c62-97ff-70fe1bbfa982-000000/Jsq4aHSQ4_NA_c70SAJfN-uKnk96phrD1G2LMXKt32M=428", "summary": "Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI (18 minute read) RAG was only the starting point of the Context Engineering discipline. Modern context engineering now incorporates context writing, compression, isolation, and selection, demanding robust metadata management, policy-as-code guardrails, and multimodal capabilities. Knowledge graphs underpin explainable, trustworthy, and scalable AI, while new evaluation metrics (relevance, groundedness, provenance,...", "source": "tldr", "AI": {"tldr": "RAG\u53ea\u662f\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b66\u79d1\u7684\u8d77\u70b9\uff0c\u73b0\u4ee3\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5df2\u53d1\u5c55\u4e3a\u5305\u542b\u4e0a\u4e0b\u6587\u7f16\u5199\u3001\u538b\u7f29\u3001\u9694\u79bb\u548c\u9009\u62e9\uff0c\u9700\u8981\u5f3a\u5927\u7684\u5143\u6570\u636e\u7ba1\u7406\u3001\u7b56\u7565\u5373\u4ee3\u7801\u62a4\u680f\u548c\u591a\u6a21\u6001\u80fd\u529b\u3002\u77e5\u8bc6\u56fe\u8c31\u652f\u6491\u53ef\u89e3\u91ca\u3001\u53ef\u4fe1\u8d56\u548c\u53ef\u6269\u5c55\u7684AI\uff0c\u540c\u65f6\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u63a2\u7d22RAG\u4e4b\u540e\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u53d1\u5c55\uff0c\u5f3a\u8c03\u73b0\u4ee3AI\u7cfb\u7edf\u9700\u8981\u66f4\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u65b9\u6cd5\u6765\u63d0\u9ad8\u53ef\u4fe1\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u65b0\u6846\u67b6\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u7f16\u5199\u3001\u538b\u7f29\u3001\u9694\u79bb\u548c\u9009\u62e9\u7b49\u6280\u672f\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u7b56\u7565\u5373\u4ee3\u7801\u65b9\u6cd5\u3002", "result": "\u5c55\u793a\u4e86\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5982\u4f55\u4eceRAG\u6f14\u53d8\u4e3a\u66f4\u5168\u9762\u7684\u5b66\u79d1\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u77e5\u8bc6\u56fe\u8c31\u548c\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u5bf9\u4e8e\u6784\u5efa\u53ef\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0cRAG\u53ea\u662f\u8fd9\u4e00\u66f4\u5e7f\u6cdb\u5b66\u79d1\u7684\u8d77\u70b9\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.0a5f7578", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Main23/1/0100019a10986cfc-04fe8ff6-2f2c-4d64-96d8-f1ec81f8ab19-000000/Baf8_rE9WT3uDVYgSBrme2xIV6v9qP4Kn9al45vbaAM=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Main23/1/0100019a10986cfc-04fe8ff6-2f2c-4d64-96d8-f1ec81f8ab19-000000/Baf8_rE9WT3uDVYgSBrme2xIV6v9qP4Kn9al45vbaAM=428", "authors": ["TLDR Newsletter"], "title": "To unlock agentic engineering, you need more than a software catalog", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Main23/1/0100019a10986cfc-04fe8ff6-2f2c-4d64-96d8-f1ec81f8ab19-000000/Baf8_rE9WT3uDVYgSBrme2xIV6v9qP4Kn9al45vbaAM=428", "summary": "To unlock agentic engineering, you need more than a software catalog (Sponsor) You need a deep, rich, real-time context lake. You need guardrails. And you need harmonious human-to-agent collaboration.Port's agentic engineering platform lets you successfully orchestrate AI across your SDLC. Get a demo", "source": "tldr", "AI": {"tldr": "Port\u7684\u667a\u80fd\u4f53\u5de5\u7a0b\u5e73\u53f0\u901a\u8fc7\u6784\u5efa\u4e0a\u4e0b\u6587\u6e56\u3001\u8bbe\u7f6e\u62a4\u680f\u548c\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\uff0c\u5e2e\u52a9\u4f01\u4e1a\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u6210\u529f\u7f16\u6392AI", "motivation": "\u5f53\u524d\u4f01\u4e1a\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u7684\u8f6f\u4ef6\u76ee\u5f55\uff0c\u5b9e\u73b0\u6df1\u5ea6\u3001\u4e30\u5bcc\u3001\u5b9e\u65f6\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u4ee5\u53ca\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u673a\u5236\u6765\u89e3\u9501\u667a\u80fd\u4f53\u5de5\u7a0b\u7684\u6f5c\u529b", "method": "\u6784\u5efa\u667a\u80fd\u4f53\u5de5\u7a0b\u5e73\u53f0\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u6e56\u3001\u62a4\u680f\u7cfb\u7edf\u548c\u4eba\u673a\u534f\u4f5c\u673a\u5236\uff0c\u7528\u4e8e\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u7f16\u6392AI", "result": "\u8be5\u5e73\u53f0\u80fd\u591f\u6210\u529f\u534f\u8c03AI\u5728\u6574\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u7684\u5e94\u7528", "conclusion": "Port\u7684\u667a\u80fd\u4f53\u5de5\u7a0b\u5e73\u53f0\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u4e86\u6210\u529f\u5b9e\u65bdAI\u7f16\u6392\u6240\u9700\u7684\u5173\u952e\u7ec4\u4ef6", "topic": "swe application"}}
{"id": "wechat.2510.22d34343", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3NzI0MTk1OQ==&mid=2247517331&idx=1&sn=c4d7f42e5710d8baf5da0cbd38bf1f46&chksm=ea666f92feb59ad3751586d4b95123d97bd195f7b293289b0b65281c5440d12bd7ce7ae5fd73#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3NzI0MTk1OQ==&mid=2247517331&idx=1&sn=c4d7f42e5710d8baf5da0cbd38bf1f46&chksm=ea666f92feb59ad3751586d4b95123d97bd195f7b293289b0b65281c5440d12bd7ce7ae5fd73#rd", "authors": ["PaperEveryday"], "title": "\u6beb\u65e0\u7591\u95ee\uff0c\u672a\u6765AI\u754c\u5c06\u4f1a\u662f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5929\u4e0b", "comment": "Source: WeChat, Published: 2025-10-23 11:00:45", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.a85b709a", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTk0MjcxNQ==&mid=2247483895&idx=1&sn=d4c9c54568a5c2b853235c5ff12f4901&chksm=c55d5b9ea5d55a3d798111ac6caf6c581de20c6b441cf99dd01146d8320682014ec36799abcb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTk0MjcxNQ==&mid=2247483895&idx=1&sn=d4c9c54568a5c2b853235c5ff12f4901&chksm=c55d5b9ea5d55a3d798111ac6caf6c581de20c6b441cf99dd01146d8320682014ec36799abcb#rd", "authors": ["\u9694\u58c1\u540c\u684c\u963f\u5b85"], "title": "\u65af\u5766\u798f+DeepMind\u7a81\u7834\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8ba9AI\u5b66\u4f1a\u806a\u660e\u8bb0\u4e1c\u897f\uff0c\u6027\u80fd\u78be\u538b\u4f20\u7edf\u65b9\u6848", "comment": "Source: WeChat, Published: 2025-10-23 06:14:21", "summary": "\u8fd9\u4e00\u9636\u6bb5\u662fMem-\u03b1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u6838\u5fc3\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u73af\u5883\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u8bb0\u5fc6\u7ba1\u7406\u9700\u6c42\u3002\u8bad\u7ec3\u73af\u5883\u6a21\u62df\u4e86\u4e09\u79cd\u5178\u578b\u5e72\u6270\uff1a", "AI": {"tldr": "\u8fd9\u4e00\u9636\u6bb5\u662fMem-\u03b1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u6838\u5fc3\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u73af\u5883\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u8bb0\u5fc6\u7ba1\u7406\u9700\u6c42\u3002\u8bad\u7ec3\u73af\u5883\u6a21\u62df\u4e86\u4e09\u79cd\u5178\u578b\u5e72\u6270\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.e7d5d0a2", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671515&idx=1&sn=71a5779ba77647faed213f58d024ad6a&chksm=fd8b8c3888401ed1e3bf348d803cbc0118bd5a1bdfe30a08f62ade2b303237e87e25e6fdde26#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671515&idx=1&sn=71a5779ba77647faed213f58d024ad6a&chksm=fd8b8c3888401ed1e3bf348d803cbc0118bd5a1bdfe30a08f62ade2b303237e87e25e6fdde26#rd", "authors": ["\u4e13\u77e5"], "title": "\u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u667a\u80fd\u4f53\u5316\u641c\u7d22\u5168\u9762\u7efc\u8ff0\uff1a\u57fa\u7840\u3001\u89d2\u8272\u3001\u4f18\u5316\u3001\u8bc4\u4f30\u4e0e\u5e94\u7528", "comment": "Source: WeChat, Published: 2025-10-23 03:00:00", "summary": "\u5728\u8fd9\u4e00\u65b0\u8303\u5f0f\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09 \u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u673a\u5236\uff0c\u7528\u4e8e\u5b9e\u73b0\u81ea\u9002\u5e94\u4e0e\u81ea\u6211\u6539\u8fdb\u7684\u641c\u7d22\u884c\u4e3a\u3002\u672c\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u68b3\u7406\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u5316\u641c\u7d22\uff08RL-based agentic search\uff09\u7814\u7a76\u8fdb\u5c55\uff0c\u4ece\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7ec4\u7ec7", "AI": {"tldr": "\u5728\u8fd9\u4e00\u65b0\u8303\u5f0f\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09 \u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u673a\u5236\uff0c\u7528\u4e8e\u5b9e\u73b0\u81ea\u9002\u5e94\u4e0e\u81ea\u6211\u6539\u8fdb\u7684\u641c\u7d22\u884c\u4e3a\u3002\u672c\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u68b3\u7406\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u5316\u641c\u7d22\uff08RL-based agentic search\uff09\u7814\u7a76\u8fdb\u5c55\uff0c\u4ece\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7ec4\u7ec7", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.6a7e837b", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2NDIzMjYyMA==&mid=2247535806&idx=1&sn=ccfbde31d4df7458cfdbd1846d861075&chksm=eb76c872315b7c89142a6e5d965ecfcac507c333bdbd0a8624cf8a3b5e481fb67925d33c1d78#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2NDIzMjYyMA==&mid=2247535806&idx=1&sn=ccfbde31d4df7458cfdbd1846d861075&chksm=eb76c872315b7c89142a6e5d965ecfcac507c333bdbd0a8624cf8a3b5e481fb67925d33c1d78#rd", "authors": ["\u58a8\u5b50\u6c99\u9f99"], "title": "\u4ece\u9e3d\u5b50\u5230\u4eba\u5de5\u667a\u80fd\uff0c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u600e\u6837\u6309\u4eba\u7c7b\u7684\u610f\u613f\u884c\u4e8b\uff1f", "comment": "Source: WeChat, Published: 2025-10-23 02:07:02", "summary": "\u4ece\u6700\u57fa\u672c\u7684\u89d2\u5ea6\u6765\u8bf4\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\uff0c\u8fd9\u79cd\u8bd5\u9519\uff08\u4e5f\u53ef\u4ee5\u8bf4\u662f\u63a2\u7d22\uff09\u6700\u7b80\u5355\u7684\u7b97\u6cd5\u5f62\u5f0f\u662f\u6240\u8c13\u7684\u201c\u03b5-\u8d2a\u5a6a\u201d\uff08\u5384\u666e\u897f\u9686\u2015\u8d2a\u5a6a\uff09\u7b97\u6cd5\u3002\u5e0c\u814a\u5b57\u6bcd \u03b5\u5728\u6570\u5b66\u4e0a\u5e38\u7528\u6765\u8868\u793a\u201c\u4e00\u70b9\u70b9\u201d\uff0c\u03b5-\u8d2a\u5a6a\u7684\u610f\u601d\u5c31\u662f\u201c\u8d2a\u5a6a\uff0c\u9664\u4e86\u4e00\u70b9", "AI": {"tldr": "\u4ece\u6700\u57fa\u672c\u7684\u89d2\u5ea6\u6765\u8bf4\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\uff0c\u8fd9\u79cd\u8bd5\u9519\uff08\u4e5f\u53ef\u4ee5\u8bf4\u662f\u63a2\u7d22\uff09\u6700\u7b80\u5355\u7684\u7b97\u6cd5\u5f62\u5f0f\u662f\u6240\u8c13\u7684\u201c\u03b5-\u8d2a\u5a6a\u201d\uff08\u5384\u666e\u897f\u9686\u2015\u8d2a\u5a6a\uff09\u7b97\u6cd5\u3002\u5e0c\u814a\u5b57\u6bcd \u03b5\u5728\u6570\u5b66\u4e0a\u5e38\u7528\u6765\u8868\u793a\u201c\u4e00\u70b9\u70b9\u201d\uff0c\u03b5-\u8d2a\u5a6a\u7684\u610f\u601d\u5c31\u662f\u201c\u8d2a\u5a6a\uff0c\u9664\u4e86\u4e00\u70b9", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.e9ed6bbf", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyODY1NDE1NA==&mid=2247575267&idx=1&sn=cc073057a48a1da0cac4b27e0cd40fdb&chksm=fbf70761a495a30dbd44ebe6a99bf8dafe85ad1dd2dae94c86e76457cbdb794f89edd1ae3f79#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyODY1NDE1NA==&mid=2247575267&idx=1&sn=cc073057a48a1da0cac4b27e0cd40fdb&chksm=fbf70761a495a30dbd44ebe6a99bf8dafe85ad1dd2dae94c86e76457cbdb794f89edd1ae3f79#rd", "authors": ["\u667a\u9a7e\u6700\u524d\u6cbf"], "title": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e38\u63d0\u7684\u201c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u201d\u662f\u4e2a\u5565\uff1f", "comment": "Source: WeChat, Published: 2025-10-23 00:41:02", "summary": "\u5bf9\u4e8e\u5f3a\u5316\u5b66\u4e60\u6765\u8bf4\uff0c\u9996\u5148\u8981\u89e3\u51b3\u7684\u662f\u5982\u4f55\u5b9a\u4e49\u72b6\u6001\u4e0e\u5956\u52b1\u3002\u72b6\u6001\u65e2\u8981\u5305\u542b\u8db3\u591f\u7684\u4fe1\u606f\u8ba9\u7b56\u7565\u505a\u51fa\u6b63\u786e\u51b3\u7b56\uff0c\u53c8\u4e0d\u80fd\u8fc7\u4e8e\u5197\u4f59\u5bfc\u81f4\u5b66\u4e60\u56f0\u96be\u3002\u5956\u52b1\u8bbe\u8ba1\u5219\u975e\u5e38\u654f\u611f\uff0c\u5956\u52b1\u4fe1\u53f7\u5982\u679c\u4e0d\u5408\u7406\u4f1a\u5bfc\u81f4\u201c\u5956\u52b1\u52ab\u6301\u201d\u6216\u201c\u8d70\u6377\u5f84\u201d\u73b0\u8c61\uff0c\u6a21\u578b", "AI": {"tldr": "\u5bf9\u4e8e\u5f3a\u5316\u5b66\u4e60\u6765\u8bf4\uff0c\u9996\u5148\u8981\u89e3\u51b3\u7684\u662f\u5982\u4f55\u5b9a\u4e49\u72b6\u6001\u4e0e\u5956\u52b1\u3002\u72b6\u6001\u65e2\u8981\u5305\u542b\u8db3\u591f\u7684\u4fe1\u606f\u8ba9\u7b56\u7565\u505a\u51fa\u6b63\u786e\u51b3\u7b56\uff0c\u53c8\u4e0d\u80fd\u8fc7\u4e8e\u5197\u4f59\u5bfc\u81f4\u5b66\u4e60\u56f0\u96be\u3002\u5956\u52b1\u8bbe\u8ba1\u5219\u975e\u5e38\u654f\u611f\uff0c\u5956\u52b1\u4fe1\u53f7\u5982\u679c\u4e0d\u5408\u7406\u4f1a\u5bfc\u81f4\u201c\u5956\u52b1\u52ab\u6301\u201d\u6216\u201c\u8d70\u6377\u5f84\u201d\u73b0\u8c61\uff0c\u6a21\u578b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.91b06297", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4ODM4NjcyMg==&mid=2247498481&idx=1&sn=75025f196b052f351a6f5f0342c738f6&chksm=fc2594ff72ca39de4412821cf481b0be838688032297965544ae1187a05f9f6b2e2d60232574#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4ODM4NjcyMg==&mid=2247498481&idx=1&sn=75025f196b052f351a6f5f0342c738f6&chksm=fc2594ff72ca39de4412821cf481b0be838688032297965544ae1187a05f9f6b2e2d60232574#rd", "authors": ["\u673a\u5668\u89c6\u89c9\u4e0eAI\u6df1\u5ea6\u5b66\u4e60"], "title": "\u6beb\u65e0\u7591\u95ee\uff0c\u672a\u6765AI\u754c\u5c06\u4f1a\u662f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5929\u4e0b", "comment": "Source: WeChat, Published: 2025-10-23 00:30:50", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.6298031e", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDIzNTE0Mw==&mid=2247485456&idx=1&sn=7915deab2163f8d3a9cab78115b94a2c&chksm=c49c5d390550d3e6ba962d9180d11f2457495d4ee55333cb280e1102c73b6a85a5b78fb4475e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDIzNTE0Mw==&mid=2247485456&idx=1&sn=7915deab2163f8d3a9cab78115b94a2c&chksm=c49c5d390550d3e6ba962d9180d11f2457495d4ee55333cb280e1102c73b6a85a5b78fb4475e#rd", "authors": ["\u611f\u5b58\u7b97\u4e00\u4f53"], "title": "\u8c37\u6b4c10.23\u65e5Nature\uff1a\u6700\u5148\u8fdb\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7b97\u6cd5", "comment": "Source: WeChat, Published: 2025-10-22 17:06:19", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u6280\u672f\uff0c\u5df2\u5728\u56f4\u68cb\u3001\u661f\u9645\u4e89\u9738\u3001\u82af\u7247\u8bbe\u8ba1\u7b49\u9886\u57df\u53d6\u5f97\u7a81\u7834\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6210\u529f\u80cc\u540e\u7684\u5b66\u4e60\u7b97\u6cd5\u90fd\u662f\u4eba\u7c7b\u4e13\u5bb6\u7ecf\u8fc7\u6570\u5341\u5e74\u53cd\u590d\u8bd5\u9519\u624b\u5de5\u8bbe\u8ba1\u51fa\u6765\u7684\uff0c\u5982Q-learning\u3001PPO\u3001MuZero\u7b49\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u6280\u672f\uff0c\u5df2\u5728\u56f4\u68cb\u3001\u661f\u9645\u4e89\u9738\u3001\u82af\u7247\u8bbe\u8ba1\u7b49\u9886\u57df\u53d6\u5f97\u7a81\u7834\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6210\u529f\u80cc\u540e\u7684\u5b66\u4e60\u7b97\u6cd5\u90fd\u662f\u4eba\u7c7b\u4e13\u5bb6\u7ecf\u8fc7\u6570\u5341\u5e74\u53cd\u590d\u8bd5\u9519\u624b\u5de5\u8bbe\u8ba1\u51fa\u6765\u7684\uff0c\u5982Q-learning\u3001PPO\u3001MuZero\u7b49\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.7d2efadd", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574522&idx=2&sn=33d8bfebdd06e9ed903b98c72fc79ea6&chksm=ea46e462254385922dfa488d5e68bc1dfa6d17d7e3da5f3bd8dc31dfeae19f1677d2ff6bc5f1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574522&idx=2&sn=33d8bfebdd06e9ed903b98c72fc79ea6&chksm=ea46e462254385922dfa488d5e68bc1dfa6d17d7e3da5f3bd8dc31dfeae19f1677d2ff6bc5f1#rd", "authors": ["\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406"], "title": "Meta\u752840\u4e07\u4e2aGPU\u5c0f\u65f6\u505a\u4e86\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u53ea\u4e3a\u5f04\u6e05<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>ScalingLaw", "comment": "Source: WeChat, Published: 2025-10-22 16:03:21", "summary": "\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4f5c\u8005\u9996\u5148\u7814\u7a76\u5f02\u6b65\u7684 off-policy RL \u8bad\u7ec3\u7ed3\u6784\uff0c\u56e0\u4e3a\u5b83\u51b3\u5b9a\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u4e0e\u7b97\u529b\u6548\u7387\uff0c\u5e76\u4e14\u901a\u5e38\u72ec\u7acb\u4e8e\u5176\u4ed6\u8bbe\u8ba1\u9009\u62e9\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f5c\u8005\u6bd4\u8f83\u4e86\u4e24\u79cd off-policy \u5b66\u4e60\u65b9\u5f0f\uff1aPPO-off-policy-k \u548c PipelineRL-k\u3002", "AI": {"tldr": "\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4f5c\u8005\u9996\u5148\u7814\u7a76\u5f02\u6b65\u7684 off-policy RL \u8bad\u7ec3\u7ed3\u6784\uff0c\u56e0\u4e3a\u5b83\u51b3\u5b9a\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u4e0e\u7b97\u529b\u6548\u7387\uff0c\u5e76\u4e14\u901a\u5e38\u72ec\u7acb\u4e8e\u5176\u4ed6\u8bbe\u8ba1\u9009\u62e9\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f5c\u8005\u6bd4\u8f83\u4e86\u4e24\u79cd off-policy \u5b66\u4e60\u65b9\u5f0f\uff1aPPO-off-policy-k \u548c PipelineRL-k\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.477ba9b6", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4MzYxODkzMg==&mid=2247502905&idx=1&sn=1f89dc2ccf31e389e5ad59c99c4f7002&chksm=cebdf41fa8e72a40e2306ee8fc8ccceeb774d59f7705893993e52df87d73416722085d7b289f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4MzYxODkzMg==&mid=2247502905&idx=1&sn=1f89dc2ccf31e389e5ad59c99c4f7002&chksm=cebdf41fa8e72a40e2306ee8fc8ccceeb774d59f7705893993e52df87d73416722085d7b289f#rd", "authors": ["AI\u4fee\u732bPrompt"], "title": "\u6700\u65b0<em class=\"highlight\">Agentic</em> Search\u7efc\u8ff0\uff0cRL\u8ba9Agent\u81ea\u4e3b\u68c0\u7d22\uff0cRAG\u9010\u6e10\u6210\u4e3a\u8fc7\u53bb\u5f0f", "comment": "Source: WeChat, Published: 2025-10-23 12:23:03", "summary": "survey analytical focus rl foun- search reasoning evaluation dations behavior integration scope application scope singh et al. [169] agentic rag x x liang et al. [108] reasoning in rag x x x x gao et al. [58] reasoning in rag x x xi et al. [220] general search agents x 1 x li et al. [102] rl-based d", "AI": {"tldr": "survey analytical focus rl foun- search reasoning evaluation dations behavior integration scope application scope singh et al. [169] agentic rag x x liang et al. [108] reasoning in rag x x x x gao et ...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.bdaf15f8", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658053&idx=1&sn=22e1308a5679f4f6d856da7ce8c02afc&chksm=c05d56b1a65adcd6b6dd30132ad40dacedf2ba916def32284911c172feb7059bb123c66e57f4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658053&idx=1&sn=22e1308a5679f4f6d856da7ce8c02afc&chksm=c05d56b1a65adcd6b6dd30132ad40dacedf2ba916def32284911c172feb7059bb123c66e57f4#rd", "authors": ["DataFunSummit"], "title": "<em class=\"highlight\">Agentic</em> AI\uff1a\u901a\u5411 AGI \u5e94\u7528\u7684\u5173\u952e\u524d\u7ad9\u4e0e\u667a\u80fd\u6d8c\u73b0\u4e4b\u8def", "comment": "Source: WeChat, Published: 2025-10-23 10:02:11", "summary": "1. \u80cc\u666f\u4e0e\u95ee\u9898\uff1a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5386\u53f2\u6027\u5206\u5316 2. \u6838\u5fc3\u6982\u5ff5\u4e0e\u7406\u8bba\uff1aagentic ai \u7684\u6df1\u5ea6\u89e3\u6790 3. \u6280\u672f\u5b9e\u73b0\u4e0e\u6848\u4f8b\uff1aagentic ai \u5e94\u7528\u7684\u5a01\u529b\u5c55\u73b0 4. \u7406\u8bba\u57fa\u7840\u4e0e\u8bba\u8bc1\uff1a\u667a\u80fd\u6d8c\u73b0\u7684\u79d1\u5b66\u652f\u6491", "AI": {"tldr": "1. \u80cc\u666f\u4e0e\u95ee\u9898\uff1a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5386\u53f2\u6027\u5206\u5316 2. \u6838\u5fc3\u6982\u5ff5\u4e0e\u7406\u8bba\uff1aagentic ai \u7684\u6df1\u5ea6\u89e3\u6790 3. \u6280\u672f\u5b9e\u73b0\u4e0e\u6848\u4f8b\uff1aagentic ai \u5e94\u7528\u7684\u5a01\u529b\u5c55\u73b0 4. \u7406\u8bba\u57fa\u7840\u4e0e\u8bba\u8bc1\uff1a\u667a\u80fd\u6d8c\u73b0\u7684\u79d1\u5b66\u652f\u6491", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.d71a2efa", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDkwOTEyMQ==&mid=2651666121&idx=1&sn=9793f173dc16e560dae12d24658fb5dc&chksm=bce47c92e8b4c25c8f5ab0f1c3b3d9563b466bc46c359e2a6a7b688f54d84739f04c513edd7b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDkwOTEyMQ==&mid=2651666121&idx=1&sn=9793f173dc16e560dae12d24658fb5dc&chksm=bce47c92e8b4c25c8f5ab0f1c3b3d9563b466bc46c359e2a6a7b688f54d84739f04c513edd7b#rd", "authors": ["GDG"], "title": "2025 Google Devfest\uff5c\u5f53\u6d4f\u89c8\u5668\u5f00\u59cb\u601d\u8003\uff0c\u4eba\u7c7b\u8fdb\u5165\u4e86 <em class=\"highlight\">Agentic</em> \u65f6\u4ee3", "comment": "Source: WeChat, Published: 2025-10-23 09:41:56", "summary": "\u6d4f\u89c8\u5668\uff0c\u4ece\u4fe1\u606f\u5165\u53e3\u5230\u5e94\u7528\u5bb9\u5668\uff0c\u518d\u5230\u5982\u4eca\u7684\u667a\u80fd\u4ee3\u7406\u5e73\u53f0\uff0c\u5b83\u6b63\u7ecf\u5386\u4e00\u573a\u65b0\u7684\u201c\u8fdb\u5316\u201d\u3002\u5728 2025 Google DevFest \u4e0a\u6d77\u73b0\u573a\uff0c\u6765\u81ea\u6e05\u534e\u5927\u5b66\u7684\u535a\u58eb\u3001Fellou \u521b\u59cb\u56e2\u961f\u6210\u5458 \u9a6c\u9a81\u817e \u5c06\u5e26\u6765\u4e00\u6b21\u5173\u4e8e\u6d4f\u89c8\u5668\u672a\u6765\u7684\u5206\u4eab \u2014\u2014 \u63ed\u79d8 Fellou \u80cc\u540e", "AI": {"tldr": "\u6d4f\u89c8\u5668\uff0c\u4ece\u4fe1\u606f\u5165\u53e3\u5230\u5e94\u7528\u5bb9\u5668\uff0c\u518d\u5230\u5982\u4eca\u7684\u667a\u80fd\u4ee3\u7406\u5e73\u53f0\uff0c\u5b83\u6b63\u7ecf\u5386\u4e00\u573a\u65b0\u7684\u201c\u8fdb\u5316\u201d\u3002\u5728 2025 Google DevFest \u4e0a\u6d77\u73b0\u573a\uff0c\u6765\u81ea\u6e05\u534e\u5927\u5b66\u7684\u535a\u58eb\u3001Fellou \u521b\u59cb\u56e2\u961f\u6210\u5458 \u9a6c\u9a81\u817e \u5c06\u5e26\u6765\u4e00\u6b21\u5173\u4e8e\u6d4f\u89c8\u5668\u672a\u6765\u7684\u5206\u4eab \u2014\u2014 \u63ed\u79d8 Fellou \u80cc\u540e", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.1be8805c", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484085&idx=1&sn=b9231b373bc3456a48de900db61739dc&chksm=feaa3abddcf5b1601088342a3f9f03287691473fd5c8cab130b8c5b2a1b69d85b344f1ad1e4c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484085&idx=1&sn=b9231b373bc3456a48de900db61739dc&chksm=feaa3abddcf5b1601088342a3f9f03287691473fd5c8cab130b8c5b2a1b69d85b344f1ad1e4c#rd", "authors": ["AI \u77e5\u884c\u793e Lab"], "title": "<em class=\"highlight\">Agentic</em> \u6846\u67b6\u7cfb\u5217\uff08\u4e09\uff09\uff1a\u52a8\u624b\u5b9e\u8df5\u7bc7", "comment": "Source: WeChat, Published: 2025-10-23 09:01:00", "summary": "\u5728\u524d\u4e24\u7bc7\u4e2d\uff0c\u6211\u4eec\u4e86\u89e3\u4e86 Agentic \u6846\u67b6\u7684\u7406\u5ff5\u4e0e\u751f\u6001\u3002\u672c\u7bc7\u6211\u4eec\u5c06\u771f\u6b63\u52a8\u624b\u5b9e\u8df5\u2014\u2014 \u7528 LangGraph + LLM + \u5de5\u5177\u8c03\u7528\uff0c\u642d\u5efa\u4e00\u4e2a\u7b80\u5355\u53ef\u8fd0\u884c\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b9e\u73b0", "AI": {"tldr": "\u5728\u524d\u4e24\u7bc7\u4e2d\uff0c\u6211\u4eec\u4e86\u89e3\u4e86 Agentic \u6846\u67b6\u7684\u7406\u5ff5\u4e0e\u751f\u6001\u3002\u672c\u7bc7\u6211\u4eec\u5c06\u771f\u6b63\u52a8\u624b\u5b9e\u8df5\u2014\u2014 \u7528 LangGraph + LLM + \u5de5\u5177\u8c03\u7528\uff0c\u642d\u5efa\u4e00\u4e2a\u7b80\u5355\u53ef\u8fd0\u884c\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b9e\u73b0", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.d0e43f8b", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MTEwNzcxOQ==&mid=2451068095&idx=1&sn=10981bc9031e1a033e9bd669bb08f03f&chksm=89ee011213dc72be43206655b1be0a8174c60e2dbf5e6b6c81ea47005fea6bc07a67e6b9fea2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MTEwNzcxOQ==&mid=2451068095&idx=1&sn=10981bc9031e1a033e9bd669bb08f03f&chksm=89ee011213dc72be43206655b1be0a8174c60e2dbf5e6b6c81ea47005fea6bc07a67e6b9fea2#rd", "authors": ["\u98ce\u6d77\u7b51\u68a6"], "title": "LLM workflow vs. <em class=\"highlight\">Agentic</em> workflow vs. AI Agent", "comment": "Source: WeChat, Published: 2025-10-23 08:56:10", "summary": "llm workflow vs. agentic workflow vs. ai agent 3x185 workflow runs\u3002a. llm workflow the entire logic fully orchestrated by n8n\uff1b autonomy\uff1a none gpt-40 total tokens\uff1a 2\uff0c705 ee time\uff1a 25s when clicking 'execute get competitors message perplexity compress the response aggregate responses workfl", "AI": {"tldr": "llm workflow vs. agentic workflow vs. ai agent 3x185 workflow runs\u3002a. llm workflow the entire logic fully orchestrated by n8n\uff1b autonomy\uff1a none gpt-40 total tokens\uff1a 2\uff0c705 ee time\uff1a 25s when clicking 'exe...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.7c6d18c0", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODYzNTgwMg==&mid=2650769585&idx=4&sn=db018529eb2b7d18c3a13c3c2d4c0ce8&chksm=bf6dd4805864c06615196ea1461bb3c27c68b29b75ca4d4ff28c76c10dfb17dc19594e566cd3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODYzNTgwMg==&mid=2650769585&idx=4&sn=db018529eb2b7d18c3a13c3c2d4c0ce8&chksm=bf6dd4805864c06615196ea1461bb3c27c68b29b75ca4d4ff28c76c10dfb17dc19594e566cd3#rd", "authors": ["CTI\u8bba\u575b"], "title": "AI\u6d1e\u5bdf | \u4f01\u4e1aAI <em class=\"highlight\">Agent</em>\u843d\u5730\u9762\u4e34\u7684\u591a\u91cd\u6311\u6218\u548c\u5e94\u5bf9\u7b56\u7565", "comment": "Source: WeChat, Published: 2025-10-23 02:00:00", "summary": "Agentic AI \u662f \u201c\u4f1a\u601d\u8003\u3001\u80fd\u534f\u4f5c\u7684\u7cfb\u7edf\u8303\u5f0f\u201d\u3002\u6309\u7167Gartner\u7684\u8bf4\u6cd5\uff0cAgentic AI \u662f\u4e00\u79cd\u80fd\u81ea\u4e3b\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u7684\u8f6f\u4ef6\u5b9e\u4f53\uff0c\u4f01\u4e1a\u5728\u89c4\u5212\u67b6\u6784\u65f6\uff0c\u9700\u8c28\u614e\u8bbe\u8ba1\u3001\u98ce\u9669\u7ba1\u7406\u3001\u6a21\u5757\u5316\u6784\u5efa\u7b49\u3002", "AI": {"tldr": "Agentic AI \u662f \u201c\u4f1a\u601d\u8003\u3001\u80fd\u534f\u4f5c\u7684\u7cfb\u7edf\u8303\u5f0f\u201d\u3002\u6309\u7167Gartner\u7684\u8bf4\u6cd5\uff0cAgentic AI \u662f\u4e00\u79cd\u80fd\u81ea\u4e3b\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u7684\u8f6f\u4ef6\u5b9e\u4f53\uff0c\u4f01\u4e1a\u5728\u89c4\u5212\u67b6\u6784\u65f6\uff0c\u9700\u8c28\u614e\u8bbe\u8ba1\u3001\u98ce\u9669\u7ba1\u7406\u3001\u6a21\u5757\u5316\u6784\u5efa\u7b49\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.aae98091", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247492124&idx=1&sn=444048f76737262f706d300de27d8611&chksm=fa677327992e46eedeba064fe13b816eeb1fc7acee22234283b9b607538aa0d47559c319672d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247492124&idx=1&sn=444048f76737262f706d300de27d8611&chksm=fa677327992e46eedeba064fe13b816eeb1fc7acee22234283b9b607538aa0d47559c319672d#rd", "authors": ["\u6155\u5bb9\u5343\u8bed"], "title": "\u7a81\u7136\u53d1\u73b0\u5434\u6069\u8fbe\u7684<em class=\"highlight\">Agentic</em> AI\u8bfe\u7a0b\u7b14\u8bb0\uff0c\u771f\u7684\u8d85\u8be6\u7ec6\uff01", "comment": "Source: WeChat, Published: 2025-10-23 01:51:35", "summary": "\u6240\u4ee5\u4ed6\u6362 \u4e86\u4e2a\u601d\u8def\uff1a\u4e0d\u7528\u540d\u8bcd\u201cagent\u201d\uff0c\u6539\u7528\u5f62\u5bb9\u8bcd\u3002\u4e86\u4e2a\u601d\u8def\uff1a\u4e0d\u7528\u540d\u8bcd\u201cagent\u201d\uff0c\u6539\u7528\u5f62\u5bb9\u8bcd\u201cagentic\u201d\u3002\u5f62\u5bb9\u8bcd\u662f\u53ef\u4ee5\u533a\u5206\u7a0b\u5ea6\u7684\u3002\u7ba1\u667a\u80fd\u5316\u3001\u81ea\u4e3b\u5316\u7a0b\u5ea6\u591a\u9ad8\u591a\u4f4e\uff0c\u90fd\u53ef\u4ee5\u7b97AgenticAI\uff0c\u53ea\u662f\u7a0b\u5ea6\u4e0d\u540c\u800c\u5df2\u3002", "AI": {"tldr": "\u6240\u4ee5\u4ed6\u6362 \u4e86\u4e2a\u601d\u8def\uff1a\u4e0d\u7528\u540d\u8bcd\u201cagent\u201d\uff0c\u6539\u7528\u5f62\u5bb9\u8bcd\u3002\u4e86\u4e2a\u601d\u8def\uff1a\u4e0d\u7528\u540d\u8bcd\u201cagent\u201d\uff0c\u6539\u7528\u5f62\u5bb9\u8bcd\u201cagentic\u201d\u3002\u5f62\u5bb9\u8bcd\u662f\u53ef\u4ee5\u533a\u5206\u7a0b\u5ea6\u7684\u3002\u7ba1\u667a\u80fd\u5316\u3001\u81ea\u4e3b\u5316\u7a0b\u5ea6\u591a\u9ad8\u591a\u4f4e\uff0c\u90fd\u53ef\u4ee5\u7b97AgenticAI\uff0c\u53ea\u662f\u7a0b\u5ea6\u4e0d\u540c\u800c\u5df2\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.f5ff536b", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU3Mzg1Njk0Ng==&mid=2247485696&idx=1&sn=da5fdb61605b65d2936357342ec6af1b&chksm=fc24b382d16d9791389f8ea1d8ac0caafb4f9720cf304d6a7c1c829ecec5d092a34eadba2ec7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU3Mzg1Njk0Ng==&mid=2247485696&idx=1&sn=da5fdb61605b65d2936357342ec6af1b&chksm=fc24b382d16d9791389f8ea1d8ac0caafb4f9720cf304d6a7c1c829ecec5d092a34eadba2ec7#rd", "authors": ["\u6c42\u7d22\u4e91\u9014"], "title": "<em class=\"highlight\">Agentic</em> \u5e94\u7528\u4ea4\u4ed8\u5dee\u5f02\u548c\u53ef\u89c2\u6d4b\u6027\u89e3\u8bfb-\u5b66\u4e60\u963f\u91cc\u6280\u672f\u548c\u601d\u7801\u9038\u6709\u611f", "comment": "Source: WeChat, Published: 2025-10-23 01:46:17", "summary": "Agentic \u5e94\u7528\u5177\u5907\u81ea\u4e3b\u51b3\u7b56\u3001\u6301\u7eed\u5b66\u4e60\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u8fd9\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u7684\u9884\u5b9a\u4e49\u903b\u8f91\u6267\u884c\u6a21\u5f0f\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff1b\u8fd9\u79cd\u6839\u672c\u6027\u5dee\u5f02\u4f53\u73b0\u5728\u67b6\u6784\u590d\u6742\u6027\u7684\u663e\u8457\u63d0\u5347\u3001\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u5168\u65b0\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u6a21\u5f0f\u3002", "AI": {"tldr": "Agentic \u5e94\u7528\u5177\u5907\u81ea\u4e3b\u51b3\u7b56\u3001\u6301\u7eed\u5b66\u4e60\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u8fd9\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u7684\u9884\u5b9a\u4e49\u903b\u8f91\u6267\u884c\u6a21\u5f0f\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff1b\u8fd9\u79cd\u6839\u672c\u6027\u5dee\u5f02\u4f53\u73b0\u5728\u67b6\u6784\u590d\u6742\u6027\u7684\u663e\u8457\u63d0\u5347\u3001\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u5168\u65b0\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u6a21\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.4afd4e5a", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NjA2MDAyOA==&mid=2247484371&idx=1&sn=d48e8c7442693e15dddcd49eb7fc3a8a&chksm=cf4cd879c46de92f3086a19445b267f9ae9cfeea7d58ca016e180b1a73d6406736ab834e8a7d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NjA2MDAyOA==&mid=2247484371&idx=1&sn=d48e8c7442693e15dddcd49eb7fc3a8a&chksm=cf4cd879c46de92f3086a19445b267f9ae9cfeea7d58ca016e180b1a73d6406736ab834e8a7d#rd", "authors": ["TouchAI"], "title": "AI Agents vs. <em class=\"highlight\">Agentic</em> AI\uff1a\u6982\u5ff5\u5206\u7c7b\u3001\u5e94\u7528\u3001\u6311\u6218\u53ca\u6f5c\u5728\u89e3\u51b3\u65b9\u6848", "comment": "Source: WeChat, Published: 2025-10-22 15:06:04", "summary": "\u76f8\u6bd4\u4e4b\u4e0b\uff0cAgentic AI \u7cfb\u7edf\u4ee3\u8868\u7740\u4e00\u79cd\u8303\u5f0f\u8f6c\u53d8\uff0c\u5176\u7279\u5f81\u5728\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u3001\u6301\u4e45\u8bb0\u5fc6\u4ee5\u53ca\u81ea\u6cbb\u884c\u4e3a\u7684\u7f16\u6392\u5316\uff08orchestrated autonomy\uff09\u3002\u901a\u8fc7\u5bf9\u4f53\u7cfb\u7ed3\u6784\u6f14\u8fdb\u3001\u8fd0\u884c\u673a\u5236\u3001\u4ea4\u4e92\u6a21\u5f0f\u4e0e\u81ea\u6cbb\u5c42\u7ea7\u7684\u987a\u5e8f\u5206\u6790\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e24\u79cd", "AI": {"tldr": "\u76f8\u6bd4\u4e4b\u4e0b\uff0cAgentic AI \u7cfb\u7edf\u4ee3\u8868\u7740\u4e00\u79cd\u8303\u5f0f\u8f6c\u53d8\uff0c\u5176\u7279\u5f81\u5728\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u3001\u6301\u4e45\u8bb0\u5fc6\u4ee5\u53ca\u81ea\u6cbb\u884c\u4e3a\u7684\u7f16\u6392\u5316\uff08orchestrated autonomy\uff09\u3002\u901a\u8fc7\u5bf9\u4f53\u7cfb\u7ed3\u6784\u6f14\u8fdb\u3001\u8fd0\u884c\u673a\u5236\u3001\u4ea4\u4e92\u6a21\u5f0f\u4e0e\u81ea\u6cbb\u5c42\u7ea7\u7684\u987a\u5e8f\u5206\u6790\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e24\u79cd", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.a9802e7e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNTM1MDI4MQ==&mid=2247522486&idx=1&sn=2d1572503821bad93605e3b13ae21403&chksm=c3e96172bb1126c60ce87595b4c0f4fd51e866b14d18efbea7deb7c5843bb0de938271e07bb3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNTM1MDI4MQ==&mid=2247522486&idx=1&sn=2d1572503821bad93605e3b13ae21403&chksm=c3e96172bb1126c60ce87595b4c0f4fd51e866b14d18efbea7deb7c5843bb0de938271e07bb3#rd", "authors": ["JAVA\u8475\u82b1\u5b9d\u5178"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u53c2\u6570\u5230\u5e95\u662f\u5565\uff1f\u5373\u4e13\u4e1a\u53c8\u901a\u4fd7\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e4b\u65c5", "comment": "Source: WeChat, Published: 2025-10-23 11:02:47", "summary": "50.5 kb \u2192 \u4f60\u4f1a\u53d1\u73b0\u5927\u6a21\u578b\u5c31\u662f\u4e00\u5806\u6587\u4ef6\u3002\u9664\u4e86\u51e0\u4e2a\u5f88\u5c0f\u7684\u914d\u7f6e\u6587\u4ef6\u5916\uff0c\u6700\u5360\u5730\u65b9\u7684\uff0c\u5c31\u662f\u51e0\u4e2a\u5de8\u5927\u7684\u3001\u4ee5 .safetensors \u7ed3\u5c3e\u7684\u6587\u4ef6\u3002\u8fd9\u4e9b\u6587\u4ef6\u662f\u4ec0\u4e48\uff1f\u662f\u590d\u6742\u7684\u4ee3\u7801\u5417\uff1f", "AI": {"tldr": "50.5 kb \u2192 \u4f60\u4f1a\u53d1\u73b0\u5927\u6a21\u578b\u5c31\u662f\u4e00\u5806\u6587\u4ef6\u3002\u9664\u4e86\u51e0\u4e2a\u5f88\u5c0f\u7684\u914d\u7f6e\u6587\u4ef6\u5916\uff0c\u6700\u5360\u5730\u65b9\u7684\uff0c\u5c31\u662f\u51e0\u4e2a\u5de8\u5927\u7684\u3001\u4ee5 .safetensors \u7ed3\u5c3e\u7684\u6587\u4ef6\u3002\u8fd9\u4e9b\u6587\u4ef6\u662f\u4ec0\u4e48\uff1f\u662f\u590d\u6742\u7684\u4ee3\u7801\u5417\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.b5504436", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODY1ODY4NA==&mid=2651428831&idx=2&sn=01b198186fad9aef3a29ab212aab7418&chksm=bc90c99aab4b62e31236b985a92c876f1cbc569c4ca50f9774ed6b633ed7f419a7370cbd1fc9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODY1ODY4NA==&mid=2651428831&idx=2&sn=01b198186fad9aef3a29ab212aab7418&chksm=bc90c99aab4b62e31236b985a92c876f1cbc569c4ca50f9774ed6b633ed7f419a7370cbd1fc9#rd", "authors": ["\u9633\u68c0\u5728\u7ebf"], "title": "\u201c<em class=\"highlight\">\u5927\u6a21\u578b</em>\u2014\u5fae\u7b97\u6cd5\u201d\u534f\u540c\u63a8\u8fdb\u68c0\u5bdf\u4e1a\u52a1\u4e0e\u667a\u80fd\u6280\u672f\u6df1\u878d", "comment": "Source: WeChat, Published: 2025-10-23 10:56:25", "summary": "\u5927\u6a21\u578b\u672c\u8eab\u4f5c\u4e3a\u4e00\u7c7b\u7279\u6b8a\u7684\u201c\u667a\u80fd\u4f53\u201d\u4e0e\u5176\u4ed6\u5c0f\u6a21\u578b\u5904\u4e8e\u9ad8\u5ea6\u5173\u8054\u8026\u5408\u6027\u5b58\u5728\u3002\u56e0\u6b64\uff0c\u6709\u6548\u89e3\u51b3\u5927\u6a21\u578b\u901a\u7528\u6027\u4e0e\u4e13\u4e1a\u6027\u534f\u8c03\u4e0d\u8db3\u7684\u5173\u952e\u5728\u4e8e\u5982\u4f55\u5b9e\u73b0\u7b97\u6cd5\u4f53\u7cfb\u8bbe\u8ba1\u7684\u201c\u5927\u201d\u201c\u5c0f\u201d\u534f\u540c\u6027\u95ee\u9898\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u672c\u8eab\u4f5c\u4e3a\u4e00\u7c7b\u7279\u6b8a\u7684\u201c\u667a\u80fd\u4f53\u201d\u4e0e\u5176\u4ed6\u5c0f\u6a21\u578b\u5904\u4e8e\u9ad8\u5ea6\u5173\u8054\u8026\u5408\u6027\u5b58\u5728\u3002\u56e0\u6b64\uff0c\u6709\u6548\u89e3\u51b3\u5927\u6a21\u578b\u901a\u7528\u6027\u4e0e\u4e13\u4e1a\u6027\u534f\u8c03\u4e0d\u8db3\u7684\u5173\u952e\u5728\u4e8e\u5982\u4f55\u5b9e\u73b0\u7b97\u6cd5\u4f53\u7cfb\u8bbe\u8ba1\u7684\u201c\u5927\u201d\u201c\u5c0f\u201d\u534f\u540c\u6027\u95ee\u9898\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.51729c77", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MzMxOTEwMA==&mid=2651000003&idx=1&sn=a88d68c8da1bd09091552bb8ebeb7843&chksm=8a79a5f92974b7dc9b6bbdc87a46ec6c76725bf43b0bd4ba3ff50ce7941c5dd777950c38a97f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MzMxOTEwMA==&mid=2651000003&idx=1&sn=a88d68c8da1bd09091552bb8ebeb7843&chksm=8a79a5f92974b7dc9b6bbdc87a46ec6c76725bf43b0bd4ba3ff50ce7941c5dd777950c38a97f#rd", "authors": ["\u4e2d\u79d1\u66d9\u5149"], "title": "\u4e2d\u79d1\u66d9\u5149\u53d1\u5e03\u56fd\u5185\u9996\u4e2a\u79d1\u5b66<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e00\u7ad9\u5f0f\u5f00\u53d1\u5e73\u53f0\uff01", "comment": "Source: WeChat, Published: 2025-10-23 10:45:57", "summary": "\u201c\u7b97\u529b\u662f\u5927\u6a21\u578b\u53d1\u5c55\u524d\u7f6e\u6761\u4ef6\uff0c\u4e14\u95e8\u69db\u8d8a\u6765\u8d8a\u9ad8\u201d\uff0c\u4e2d\u79d1\u66d9\u5149\u9ad8\u7ea7\u526f\u603b\u88c1\u674e\u658c\u8868\u793a\uff0c\u5f53\u524dAI\u79d1\u5b66\u5927\u6a21\u578b\u6b63\u4ece\u201c\u4efb\u52a1\u5316\u201d\u5411\u201c\u901a\u7528\u5316\u201d\u8f6c\u53d8\uff0c\u7531\u6b64\u5e26\u6765\u6a21\u578b\u53c2\u6570\u91cf\u5feb\u901f\u589e\u957f\uff0c\u8fd9\u5bfc\u81f4\u8bad\u7ec3\u79d1\u5b66\u5927\u6a21\u578b\u7684\u201c\u8d85\u667a\u878d\u5408\u201d\u7cfb\u7edf\u6027\u80fd\u6bcf9\u4e2a\u6708\u5c31", "AI": {"tldr": "\u201c\u7b97\u529b\u662f\u5927\u6a21\u578b\u53d1\u5c55\u524d\u7f6e\u6761\u4ef6\uff0c\u4e14\u95e8\u69db\u8d8a\u6765\u8d8a\u9ad8\u201d\uff0c\u4e2d\u79d1\u66d9\u5149\u9ad8\u7ea7\u526f\u603b\u88c1\u674e\u658c\u8868\u793a\uff0c\u5f53\u524dAI\u79d1\u5b66\u5927\u6a21\u578b\u6b63\u4ece\u201c\u4efb\u52a1\u5316\u201d\u5411\u201c\u901a\u7528\u5316\u201d\u8f6c\u53d8\uff0c\u7531\u6b64\u5e26\u6765\u6a21\u578b\u53c2\u6570\u91cf\u5feb\u901f\u589e\u957f\uff0c\u8fd9\u5bfc\u81f4\u8bad\u7ec3\u79d1\u5b66\u5927\u6a21\u578b\u7684\u201c\u8d85\u667a\u878d\u5408\u201d\u7cfb\u7edf\u6027\u80fd\u6bcf9\u4e2a\u6708\u5c31", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.0514f483", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzODAzMDYyMQ==&mid=2247483870&idx=1&sn=2f348b5b749a2dc99926e526b79e74f2&chksm=f1c1be01640614f35d1ca4e696d59075e117896b8dfa1281765d64dc0d875efe73a29eaf6587#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzODAzMDYyMQ==&mid=2247483870&idx=1&sn=2f348b5b749a2dc99926e526b79e74f2&chksm=f1c1be01640614f35d1ca4e696d59075e117896b8dfa1281765d64dc0d875efe73a29eaf6587#rd", "authors": ["\u8bb2\u70b9\u5927\u6a21\u578b"], "title": "\u60f3\u5b66AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u5f00\u53d1\uff0c\u5c31\u6309\u7167\u8fd9\u4e2a\u987a\u5e8f\u5b66\uff01", "comment": "Source: WeChat, Published: 2025-10-23 10:00:34", "summary": "\u6309\u8fd9\u4e2a\u987a\u5e8f\u5b66 \u9636\u6bb51\uff1a\u5927\u6a21\u578b\u57fa\u7840 5\u5929 60\u5929 20\u5929 3\u5929 \u5927\u6a21\u578b \u5927\u6a21\u578b\u7684 prompt \u5927\u6a21\u578bapi \u57fa\u672c\u4fe1\u606f \u539f\u7406 \u63d0\u793a\u8bcd\u3002\u9636\u6bb52\uff1arag\u5e94\u7528\u5f00\u53d1\u5de5\u7a0b 15\u5929 10\u5929 5\u5929 7\u5929\u3002", "AI": {"tldr": "\u6309\u8fd9\u4e2a\u987a\u5e8f\u5b66 \u9636\u6bb51\uff1a\u5927\u6a21\u578b\u57fa\u7840 5\u5929 60\u5929 20\u5929 3\u5929 \u5927\u6a21\u578b \u5927\u6a21\u578b\u7684 prompt \u5927\u6a21\u578bapi \u57fa\u672c\u4fe1\u606f \u539f\u7406 \u63d0\u793a\u8bcd\u3002\u9636\u6bb52\uff1arag\u5e94\u7528\u5f00\u53d1\u5de5\u7a0b 15\u5929 10\u5929 5\u5929 7\u5929\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.6708b021", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4NTc0Mjk0Mg==&mid=2247495371&idx=1&sn=9b1ff3501c24a4c53e4ff18ecd5b60b6&chksm=cee24f0bfba6537abd5009283ffc9d2ae40248d562f64679883d122276a66834988bb014f9f8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4NTc0Mjk0Mg==&mid=2247495371&idx=1&sn=9b1ff3501c24a4c53e4ff18ecd5b60b6&chksm=cee24f0bfba6537abd5009283ffc9d2ae40248d562f64679883d122276a66834988bb014f9f8#rd", "authors": ["\u521b\u5b87\u540e\u5929"], "title": "AI\u8206\u60c5\u9632\u7ebf\u8bc4\u6d4b\u51fa\u7089\uff1a14\u6b3e\u4e2d\u5916<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8c01\u80fd\u7a33\u4f4f\u201c\u717d\u52a8\u98ce\u66b4\u201d\uff1f", "comment": "Source: WeChat, Published: 2025-10-23 09:53:35", "summary": "\u524d\u5341\u540d\u5927\u6a21\u578b\u6574\u4f53\u8868\u73b0\u7a33\u5065\uff0c\u5b89\u5168\u4e0e\u8868\u8fbe\u7684\u5e73\u8861\u5ea6\u6301\u7eed\u63d0\u5347\u3002\u56fd\u4ea7\u6a21\u578b\u5728\u60c5\u7eea\u8bc6\u522b\u3001\u8206\u60c5\u5f15\u5bfc\u4e0e\u5185\u5bb9\u9632\u62a4\u7b49\u65b9\u9762\u4fdd\u6301\u9886\u5148\uff0c\u5c55\u73b0\u51fa\u66f4\u6210\u719f\u7684\u8bed\u4e49\u9632\u63a7\u80fd\u529b\u3002", "AI": {"tldr": "\u524d\u5341\u540d\u5927\u6a21\u578b\u6574\u4f53\u8868\u73b0\u7a33\u5065\uff0c\u5b89\u5168\u4e0e\u8868\u8fbe\u7684\u5e73\u8861\u5ea6\u6301\u7eed\u63d0\u5347\u3002\u56fd\u4ea7\u6a21\u578b\u5728\u60c5\u7eea\u8bc6\u522b\u3001\u8206\u60c5\u5f15\u5bfc\u4e0e\u5185\u5bb9\u9632\u62a4\u7b49\u65b9\u9762\u4fdd\u6301\u9886\u5148\uff0c\u5c55\u73b0\u51fa\u66f4\u6210\u719f\u7684\u8bed\u4e49\u9632\u63a7\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.2b94e89f", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMjg1MDUxMw==&mid=2247483762&idx=1&sn=5681ad80eadc56f368469fbf1f74d0ac&chksm=fe11d1ebc4b7998225467babf8c51dc037b40d49ddead98258a19af34d5f5238b1e8f7813cd5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMjg1MDUxMw==&mid=2247483762&idx=1&sn=5681ad80eadc56f368469fbf1f74d0ac&chksm=fe11d1ebc4b7998225467babf8c51dc037b40d49ddead98258a19af34d5f5238b1e8f7813cd5#rd", "authors": ["AI\u5927\u6a21\u578b\u8001\u4ed4"], "title": "\u7ec8\u4e8e\u6709\u4eba\u628aAI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e09\u79cd\u6a21\u5f0fagent\u3001embedding\u3001copilot\u8bb2\u6e05\u695a\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-10-23 06:56:15", "summary": "\u4e00\u6587\u8bb2\u6e05 ai\u5927\u6a21\u578b \u4e09\u79cd\u6a21\u5f0f\u3002\u4e00\u3001embedding\u6a21\u5f0f\u3002embedding\u901a\u8fc7\u5c06\u9ad8\u7ef4\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\u3001\u58f0\u97f3 \u7b49\uff09\u8f6c\u6362\u4e3a\u4f4e\u7ef4\u8fde\u7eed\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\uff0c\u751f\u6210\u79f0\u4e3a \u5d4c\u5165\u5411\u91cf\u7684\u6570\u503c\u5316\u5f62\u5f0f\u3002", "AI": {"tldr": "\u4e00\u6587\u8bb2\u6e05 ai\u5927\u6a21\u578b \u4e09\u79cd\u6a21\u5f0f\u3002\u4e00\u3001embedding\u6a21\u5f0f\u3002embedding\u901a\u8fc7\u5c06\u9ad8\u7ef4\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\u3001\u58f0\u97f3 \u7b49\uff09\u8f6c\u6362\u4e3a\u4f4e\u7ef4\u8fde\u7eed\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\uff0c\u751f\u6210\u79f0\u4e3a \u5d4c\u5165\u5411\u91cf\u7684\u6570\u503c\u5316\u5f62\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.dbdc33a5", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4NjY2MDYxMg==&mid=2247531658&idx=2&sn=9248464355f1525357f0a8a910d93a30&chksm=fceaefea6b288fbc94517cd1811ec47aa96352db96d0ed21756afacd63785229f08746bd6225#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4NjY2MDYxMg==&mid=2247531658&idx=2&sn=9248464355f1525357f0a8a910d93a30&chksm=fceaefea6b288fbc94517cd1811ec47aa96352db96d0ed21756afacd63785229f08746bd6225#rd", "authors": ["\u58a8\u73ab\u4eba\u5de5\u667a\u80fd"], "title": "\u3010\u62a5\u544a\u3011\u7834\u5c40\u4e4b\u8def\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u751f\u4ea7\u529b\u5de5\u5177\u7684\u601d\u8003\u4e0e\u5b9e\u8df5\uff08\u9644PDF\u4e0b\u8f7d\uff09", "comment": "Source: WeChat, Published: 2025-10-23 06:00:40", "summary": "\u300a\u5927\u6a21\u578b\u751f\u4ea7\u529b\u5de5\u5177\u7684\u601d\u8003\u4e0e\u5b9e\u8df5\u300b \uff08\u5b8c\u6574\u7248.pdf \uff09\u4ee5\u4e0b\u4ec5\u5c55\u793a\u90e8\u5206\u5185\u5bb9 \u4e0b\u8f7d\u65b9\u5f0f\u89c1\u6587\u672b \u4e00\u3001\u5e02\u573a\u53d1\u5c55\u4e0e\u7528\u6237\u6001\u5ea6 \uff08\u4e00\uff09\u6d77\u5916 AI \u751f\u4ea7\u529b\u5de5\u5177\u5e94\u7528\u73b0\u72b6\uff08\u4ee5\u7f16\u7a0b\u52a9\u624b\u4e3a\u4f8b\uff09", "AI": {"tldr": "\u300a\u5927\u6a21\u578b\u751f\u4ea7\u529b\u5de5\u5177\u7684\u601d\u8003\u4e0e\u5b9e\u8df5\u300b \uff08\u5b8c\u6574\u7248.pdf \uff09\u4ee5\u4e0b\u4ec5\u5c55\u793a\u90e8\u5206\u5185\u5bb9 \u4e0b\u8f7d\u65b9\u5f0f\u89c1\u6587\u672b \u4e00\u3001\u5e02\u573a\u53d1\u5c55\u4e0e\u7528\u6237\u6001\u5ea6 \uff08\u4e00\uff09\u6d77\u5916 AI \u751f\u4ea7\u529b\u5de5\u5177\u5e94\u7528\u73b0\u72b6\uff08\u4ee5\u7f16\u7a0b\u52a9\u624b\u4e3a\u4f8b\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.b8fc341f", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247835865&idx=4&sn=069efca60b6be453d4ceb658dde6b385&chksm=e9a6297549688781e7c46c3ac987676a7293ac5054d67f4cac18eccfe45592869146fd35a696#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247835865&idx=4&sn=069efca60b6be453d4ceb658dde6b385&chksm=e9a6297549688781e7c46c3ac987676a7293ac5054d67f4cac18eccfe45592869146fd35a696#rd", "authors": ["\u91cf\u5b50\u4f4d"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u63a8\u7406\u5b66\u4e60\u65b0\u8303\u5f0f\uff01ExGRPO\u6846\u67b6\uff1a\u4ece\u76f2\u76ee\u5237\u9898\u5230\u806a\u660e\u590d\u76d8", "comment": "Source: WeChat, Published: 2025-10-23 05:16:02", "summary": "\u5927\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u7ec8\u4e8e\u77e5\u9053\u4ec0\u4e48\u7ecf\u9a8c\u66f4\u5b9d\u8d35\u4e86\uff01\u6765\u81ea\u4e0a\u6d77\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4\u3001\u6fb3\u95e8\u5927\u5b66\u3001\u5357\u4eac\u5927\u5b66\u548c\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\u7684\u7814\u7a76\u56e2\u961f\uff0c\u6700\u8fd1\u63d0\u51fa\u4e86\u4e00\u5957\u7ecf\u9a8c\u7ba1\u7406\u548c\u5b66\u4e60\u6846\u67b6ExGRPO\u2014\u2014", "AI": {"tldr": "\u5927\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u7ec8\u4e8e\u77e5\u9053\u4ec0\u4e48\u7ecf\u9a8c\u66f4\u5b9d\u8d35\u4e86\uff01\u6765\u81ea\u4e0a\u6d77\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4\u3001\u6fb3\u95e8\u5927\u5b66\u3001\u5357\u4eac\u5927\u5b66\u548c\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\u7684\u7814\u7a76\u56e2\u961f\uff0c\u6700\u8fd1\u63d0\u51fa\u4e86\u4e00\u5957\u7ecf\u9a8c\u7ba1\u7406\u548c\u5b66\u4e60\u6846\u67b6ExGRPO\u2014\u2014", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.15a44b45", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMTM0OTQzNQ==&mid=2247486922&idx=1&sn=b0059c06f8e0faca32ad9cde3a3aa524&chksm=c0e27413a9ea4342825e1dfbe6f53b526672d0d8d9f58d4ad4f5eea6ae6f129fb196d353591b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMTM0OTQzNQ==&mid=2247486922&idx=1&sn=b0059c06f8e0faca32ad9cde3a3aa524&chksm=c0e27413a9ea4342825e1dfbe6f53b526672d0d8d9f58d4ad4f5eea6ae6f129fb196d353591b#rd", "authors": ["DigitalPath"], "title": "\u884c\u4e1a\u8d44\u8baf | \u5f53\u75c5\u7406\u5207\u7247\u9047\u89c1AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff1a\u76d8\u70b9\u5341\u5927\u56fd\u4ea7AI\u75c5\u7406<em class=\"highlight\">\u5927\u6a21\u578b</em>", "comment": "Source: WeChat, Published: 2025-10-23 04:00:15", "summary": "cytobrain \u5170\u4e01\u601d\u9088\u5927\u6a21\u578b \u4ee3\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u75c5\u7406\u8bca\u65ad\u6a21\u578b\u3002ROAM\u6a21\u578b\uff1a\u80f6\u8d28\u7624\u7cbe\u51c6\u8bca\u65ad\u7684\u4e13\u4e1a\u7a81\u7834\u6e05\u534e\u5927\u5b66\u81ea\u52a8\u5316\u7cfb\u751f\u547d\u57fa\u7840\u6a21\u578b\u5b9e\u9a8c\u5ba4\u4e0e\u4e2d\u5357\u5927\u5b66\u6e58\u96c5\u533b\u9662\u5408\u4f5c\u5f00\u53d1\u7684ROAM\u6a21\u578b\uff0c\u5176\u7814\u7a76\u6210\u679c\u4e8e2024\u5e747\u6708\u53d1\u8868\u4e8e\u300aNature Machine Intelligence", "AI": {"tldr": "cytobrain \u5170\u4e01\u601d\u9088\u5927\u6a21\u578b \u4ee3\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u75c5\u7406\u8bca\u65ad\u6a21\u578b\u3002ROAM\u6a21\u578b\uff1a\u80f6\u8d28\u7624\u7cbe\u51c6\u8bca\u65ad\u7684\u4e13\u4e1a\u7a81\u7834\u6e05\u534e\u5927\u5b66\u81ea\u52a8\u5316\u7cfb\u751f\u547d\u57fa\u7840\u6a21\u578b\u5b9e\u9a8c\u5ba4\u4e0e\u4e2d\u5357\u5927\u5b66\u6e58\u96c5\u533b\u9662\u5408\u4f5c\u5f00\u53d1\u7684ROAM\u6a21\u578b\uff0c\u5176\u7814\u7a76\u6210\u679c\u4e8e2024\u5e747\u6708\u53d1\u8868\u4e8e\u300aNature Machine Intelligence", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.b5f36f1a", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247509226&idx=2&sn=397ed1816c77e0cc6beea8e392a9d795&chksm=e9705c6eeeba56699618225a0dcc4ef0a0494a080aae8117331b43d8b50c983ccc2282ebf576#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247509226&idx=2&sn=397ed1816c77e0cc6beea8e392a9d795&chksm=e9705c6eeeba56699618225a0dcc4ef0a0494a080aae8117331b43d8b50c983ccc2282ebf576#rd", "authors": ["\u5f00\u6e90\u62a5\u544a"], "title": "\u4f01\u4e1a\u7ea7AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u843d\u5730\u5b9e\u6218\u6280\u672f\u5e94\u7528\u6307\u5357\uff082025\u7248\uff09-\u5b89\u5168\u725b", "comment": "Source: WeChat, Published: 2025-10-23 01:40:11", "summary": "\u56fe2 \u57fa\u7840\u5927\u6a21\u578b\u5e94\u7528\u6210\u719f\u5ea6\u3002\uff081\uff09\u80fd\u529b\u8fb9\u754c\u4e0e\u201c\u6d8c\u73b0\u80fd\u529b\u201d \u8fd1\u534a\u5e74\u4ee5\u6765\uff0c\u5927\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u3001\u751f\u6210\u3001\u63a8\u7406\u3001\u7f16\u7a0b\u3001\u7ffb\u8bd1\u3001\u5bf9\u8bdd\u3001\u6458\u8981\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u30022\u3002", "AI": {"tldr": "\u56fe2 \u57fa\u7840\u5927\u6a21\u578b\u5e94\u7528\u6210\u719f\u5ea6\u3002\uff081\uff09\u80fd\u529b\u8fb9\u754c\u4e0e\u201c\u6d8c\u73b0\u80fd\u529b\u201d \u8fd1\u534a\u5e74\u4ee5\u6765\uff0c\u5927\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u3001\u751f\u6210\u3001\u63a8\u7406\u3001\u7f16\u7a0b\u3001\u7ffb\u8bd1\u3001\u5bf9\u8bdd\u3001\u6458\u8981\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u30022\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.067c5dd2", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2Nzk3MjUyNw==&mid=2247492744&idx=2&sn=20beb1947c5c723cb237ce76fe4806c4&chksm=eb9f589fb9bdb652cb1541019f476cb90d06c5e83e5443de4dbdcaaf35b397d416d60e7b4065#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2Nzk3MjUyNw==&mid=2247492744&idx=2&sn=20beb1947c5c723cb237ce76fe4806c4&chksm=eb9f589fb9bdb652cb1541019f476cb90d06c5e83e5443de4dbdcaaf35b397d416d60e7b4065#rd", "authors": ["\u673a\u5668\u5b66\u4e60\u793e\u533a"], "title": "\u56fe\u89e3AI\u6838\u5fc3\u6280\u672f\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u3001RAG\u3001\u667a\u80fd\u4f53\u3001MCP", "comment": "Source: WeChat, Published: 2025-10-23 00:15:00", "summary": "\u5927\u6a21\u578bTransformer vs. Mixture of Experts\u6df7\u5408\u4e13\u5bb6 \uff08MoE\uff09 \u662f\u4e00\u79cd\u6d41\u884c\u7684\u67b6\u6784\uff0c\u5b83\u4f7f\u7528\u4e0d\u540c\u7684\u201c\u4e13\u5bb6\u201d\u6765\u6539\u8fdb Transformer \u6a21\u578b\u3002\u4e0b\u56fe\u89e3\u91ca\u4e86\u5b83\u4eec\u4e0e Transformers \u7684\u533a\u522b\u3002", "AI": {"tldr": "\u5927\u6a21\u578bTransformer vs. Mixture of Experts\u6df7\u5408\u4e13\u5bb6 \uff08MoE\uff09 \u662f\u4e00\u79cd\u6d41\u884c\u7684\u67b6\u6784\uff0c\u5b83\u4f7f\u7528\u4e0d\u540c\u7684\u201c\u4e13\u5bb6\u201d\u6765\u6539\u8fdb Transformer \u6a21\u578b\u3002\u4e0b\u56fe\u89e3\u91ca\u4e86\u5b83\u4eec\u4e0e Transformers \u7684\u533a\u522b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.8c9a5411", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODY1NzAwNA==&mid=2247483788&idx=1&sn=83274f16a98b67bbcdfcaa7d187d8c51&chksm=97989b93d2cb4350e8a2b76f295f2d0b7985232b2333872f4fd153e49b8a9ebd1edcfb4242ad#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODY1NzAwNA==&mid=2247483788&idx=1&sn=83274f16a98b67bbcdfcaa7d187d8c51&chksm=97989b93d2cb4350e8a2b76f295f2d0b7985232b2333872f4fd153e49b8a9ebd1edcfb4242ad#rd", "authors": ["LLM Tools\u730e\u624b"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u5f00\u53d1\u8fdb\u9636\u4e4b\u8def\uff1a\u4eceLLM\u80fd\u529b\u96c6\u6210\u5230\u81ea\u4e3b\u667a\u80fd\u4f53\u6784\u5efa", "comment": "Source: WeChat, Published: 2025-10-22 23:38:27", "summary": "\u8fd9\u624d\u662f\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u6700\u6839\u672c\u7684\u601d\u7ef4\u8f6c\u53d8\u3002\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u662f\u201c\u8fc7\u7a0b\u81ea\u52a8\u5316\u201d\uff1a\u7528\u6237\u9700\u8981\u6309\u7167\u9884\u8bbe\u6d41\u7a0b\u64cd\u4f5c\u3002\u6bd4\u5982\u70b9\u5916\u5356\uff1a\u6253\u5f00App\u2192\u6d4f\u89c8\u5546\u5bb6\u2192\u9009\u62e9\u83dc\u54c1\u2192\u652f\u4ed8\u2192\u7b49\u5f85\u9001\u8fbe\u3002", "AI": {"tldr": "\u8fd9\u624d\u662f\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u6700\u6839\u672c\u7684\u601d\u7ef4\u8f6c\u53d8\u3002\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u662f\u201c\u8fc7\u7a0b\u81ea\u52a8\u5316\u201d\uff1a\u7528\u6237\u9700\u8981\u6309\u7167\u9884\u8bbe\u6d41\u7a0b\u64cd\u4f5c\u3002\u6bd4\u5982\u70b9\u5916\u5356\uff1a\u6253\u5f00App\u2192\u6d4f\u89c8\u5546\u5bb6\u2192\u9009\u62e9\u83dc\u54c1\u2192\u652f\u4ed8\u2192\u7b49\u5f85\u9001\u8fbe\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.47b3f4cc", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNTI2MTYwOQ==&mid=2247499453&idx=1&sn=bafa25a544dab86e9f15a9ade99541d8&chksm=c0eb57cada0e651e79e8861ea53ac44ee02f9a3da68744d19591d054a7bcbfea03814be939ab#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNTI2MTYwOQ==&mid=2247499453&idx=1&sn=bafa25a544dab86e9f15a9ade99541d8&chksm=c0eb57cada0e651e79e8861ea53ac44ee02f9a3da68744d19591d054a7bcbfea03814be939ab#rd", "authors": ["\u5927\u6570\u636e\u8303\u5f0f"], "title": "\u6784\u5efa\u4f01\u4e1a\u7ea7AI\u6cbb\u7406\u4e2d\u53f0\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9a71\u52a8\u7684\u6570\u636e\u6cbb\u74064.0\u67b6\u6784\u767d\u76ae\u4e66", "comment": "Source: WeChat, Published: 2025-10-23 00:20:00", "summary": "ai\u5927\u6a21\u578b\u4e3b\u6570\u636e\u6cbb\u7406\u6574\u4f53\u65b9\u6848\u667a\u80fd\u4f53\u6848\u4f8b\u5e94\u7528\u4e00deepseek\u8d4b\u80fd \u6570\u636e\u6cbb\u7406\u6280\u672f\u4e00\u57fa\u4e8edeepseek\u7684\u667a\u80fd\u4f53\u6784\u5efa\u4e00al \u5728\u4e3b\u6570\u636e\u6e05\u6d17\u7684\u5e94\u7528\u4e0e\u5c55\u671b\u3002\u57fa\u4e8edeepseek\u7684\u6570\u636e\u6cbb\u7406\u65b9\u6848\uff08\u5b8c\u6574\u724864\u9875\uff09.pdf p \u57fa\u4e8edeepseek\u7684\u6570\u636e\u6cbb\u7406\u65b9\u6848\uff08\u5b8c\u6574\u724864\u9875\uff09.pptx al", "AI": {"tldr": "ai\u5927\u6a21\u578b\u4e3b\u6570\u636e\u6cbb\u7406\u6574\u4f53\u65b9\u6848\u667a\u80fd\u4f53\u6848\u4f8b\u5e94\u7528\u4e00deepseek\u8d4b\u80fd \u6570\u636e\u6cbb\u7406\u6280\u672f\u4e00\u57fa\u4e8edeepseek\u7684\u667a\u80fd\u4f53\u6784\u5efa\u4e00al \u5728\u4e3b\u6570\u636e\u6e05\u6d17\u7684\u5e94\u7528\u4e0e\u5c55\u671b\u3002\u57fa\u4e8edeepseek\u7684\u6570\u636e\u6cbb\u7406\u65b9\u6848\uff08\u5b8c\u6574\u724864\u9875\uff09.pdf p \u57fa\u4e8edeepseek\u7684\u6570\u636e\u6cbb\u7406\u65b9\u6848\uff08\u5b8c\u6574\u724864\u9875\uff09.pptx al", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
