<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.SE](#cs.SE) [Total: 8]
- [tldr.article](#tldr.article) [Total: 12]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Recursive Knowledge Synthesis for Multi-LLM Systems: Stability Analysis and Tri-Agent Audit Framework](https://arxiv.org/abs/2601.08839)
*Toshiyuki Shigemura*

Main category: cs.CL

TL;DR: 提出三智能体交叉验证框架，通过异构LLM的递归交互实现稳定且可解释的多模型知识合成。


<details>
  <summary>Details</summary>
Motivation: 解决多模型大语言系统中稳定性与可解释性问题，通过异构LLM的协同推理实现安全可靠的知识合成。

Method: 采用三智能体框架：语义生成、分析一致性检查、透明度审计，通过递归交互实现知识合成，基于不动点理论建立形式化模型。

Result: 在47次实验中，平均RRS=0.78±0.06，68%实验TS≥0.8，89%实验收敛，验证了透明度审计作为收缩算子的理论预测。

Conclusion: 三智能体框架能在现实公开部署环境中实现稳定的递归知识合成，为人类监督的多LLM架构提供实证支持。

Abstract: This paper presents a tri-agent cross-validation framework for analyzing stability and explainability in multi-model large language systems. The architecture integrates three heterogeneous LLMs-used for semantic generation, analytical consistency checking, and transparency auditing-into a recursive interaction cycle. This design induces Recursive Knowledge Synthesis (RKS), where intermediate representations are continuously refined through mutually constraining transformations irreducible to single-model behavior. Across 47 controlled trials using public-access LLM deployments (October 2025), we evaluated system stability via four metrics: Reflex Reliability Score (RRS), Transparency Score (TS), Deviation Detection Rate (DDR), and Correction Success Rate (CSR). The system achieved mean RRS = 0.78+-0.06 and maintained TS >= 0.8 in about 68% of trials. Approximately 89% of trials converged, supporting the theoretical prediction that transparency auditing acts as a contraction operator within the composite validation mapping. The contributions are threefold: (1) a structured tri-agent framework for coordinated reasoning across heterogeneous LLMs, (2) a formal RKS model grounded in fixed-point theory, and (3) empirical evaluation of inter-model stability under realistic, non-API public-access conditions. These results provide initial empirical evidence that a safety-preserving, humansupervised multi-LLM architecture can achieve stable recursive knowledge synthesis in realistic, publicly deployed environments.

</details>


### [2] [Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning](https://arxiv.org/abs/2601.08846)
*Cagatay Tekin,Charbel Barakat,Luis Joseph Luna Limgenco*

Main category: cs.CL

TL;DR: InftyThink with Cross-Chain Memory通过嵌入语义缓存改进迭代推理框架，在结构化数学任务中提升准确性，但在异构领域测试中暴露局限性，揭示了基于相似性记忆的优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有迭代推理框架（如InftyThink）在长程推理中能控制上下文增长，但会在不同任务中重复生成相似的推理策略，缺乏对先前成功推理模式的复用。

Method: 扩展InftyThink框架，加入基于嵌入的语义缓存，存储先前成功的推理模式（引理）。在每个推理步骤中，模型检索语义最相似的存储引理，以此指导推理而不盲目扩展上下文窗口。

Result: 在MATH500、AIME2024和GPQA-Diamond等数据集上，语义引理检索在结构化领域提高了准确性，但在包含异构领域的测试中暴露了失败模式。几何分析显示缓存检索在嵌入空间中诱导方向性偏差，形成一致的"修复"（提升基线准确性）和"破坏"（降低基线准确性）吸引子。

Conclusion: 研究结果突出了基于相似性记忆对自改进LLM推理的益处和局限性，表明这种方法在结构化领域有效，但在处理异构任务时需要更谨慎的设计。

Abstract: Iterative summarization based reasoning frameworks such as InftyThink enable long-horizon reasoning in large language models (LLMs) by controlling context growth, but they repeatedly regenerate similar reasoning strategies across tasks. We introduce InftyThink with Cross-Chain Memory, an extension that augments iterative reasoning with an embedding-based semantic cache of previously successful reasoning patterns. At each reasoning step, the model retrieves and conditions on the most semantically similar stored lemmas, guiding inference without expanding the context window indiscriminately. Experiments on MATH500, AIME2024, and GPQA-Diamond demonstrate that semantic lemma retrieval improves accuracy in structured domains while exposing failure modes in tests that include heterogeneous domains. Geometric analyses of reasoning trajectories reveal that cache retrieval induces directional biases in embedding space, leading to consistent fix (improve baseline accuracy) and break (degradation in baseline accuracy) attractors. Our results highlight both the benefits and limits of similarity-based memory for self-improving LLM reasoning.

</details>


### [3] [Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models](https://arxiv.org/abs/2601.08955)
*Youwei Liu,Jian Wang,Hanlin Wang,Beichen Guo,Wenjie Li*

Main category: cs.CL

TL;DR: ITP框架通过世界模型进行前瞻想象，结合自适应前瞻机制，在多种智能体基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 当前世界模型主要进行单步或固定步长的推演，未能充分发挥其在复杂任务规划中的潜力。需要一种能够自适应调整想象步长、融合未来信号的方法来提升智能体的推理能力。

Method: 提出Imagine-then-Plan (ITP)框架，让策略模型与学习的世界模型交互生成多步"想象"轨迹。引入自适应前瞻机制，权衡最终目标和任务进度。将想象轨迹提供的未来信号（如进度和潜在冲突）与当前观察融合，形成部分可观察和可想象的MDP来指导策略学习。

Result: 在代表性智能体基准测试中，ITP显著优于竞争基线。分析验证自适应前瞻机制大幅增强了智能体的推理能力。

Conclusion: ITP框架通过前瞻想象和自适应机制有效提升了智能体在复杂任务中的规划能力，为处理更广泛复杂任务提供了有价值的见解。

Abstract: Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \textit{observable} and \textit{imaginable} Markov decision process to guide policy learning. We instantiate \texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.

</details>


### [4] [Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social Deduction Game](https://arxiv.org/abs/2601.09017)
*Haryo Akbarianto Wibowo,Alaa Elsetohy,Qinrong Cui,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本文提出基于社交推理游戏Spyfall的动态评测框架，用于评估LLM的多语言和文化能力，发现模型在非英语语境下表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 传统静态评测基准面临数据饱和和泄露问题，需要更鲁棒的评估方法。特别是需要评估LLM在多语言和跨文化环境下的真实能力。

Method: 使用社交推理游戏Spyfall作为动态评测框架，模型需要参与战略对话来识别秘密特工或避免被发现，利用文化相关的地点或当地食物作为游戏元素。

Result: 游戏排名与Chatbot Arena高度一致，但发现显著的非英语性能差距：模型在处理本地特定实体时普遍较弱，在非英语语言中经常难以遵循规则或保持战略完整性。

Conclusion: 基于游戏的方法为传统NLP基准提供了可扩展、抗泄露且具有文化细微差别的替代方案，能更好地评估LLM的多语言和文化能力。

Abstract: The rapid advancement of Large Language Models (LLMs) has necessitated more robust evaluation methods that go beyond static benchmarks, which are increasingly prone to data saturation and leakage. In this paper, we propose a dynamic benchmarking framework for evaluating multilingual and multicultural capabilities through the social deduction game Spyfall. In our setup, models must engage in strategic dialogue to either identify a secret agent or avoid detection, utilizing culturally relevant locations or local foods. Our results show that our game-based rankings align closely with the Chatbot Arena. However, we find a significant performance gap in non-English contexts: models are generally less proficient when handling locally specific entities and often struggle with rule-following or strategic integrity in non-English languages. We demonstrate that this game-based approach provides a scalable, leakage-resistant, and culturally nuanced alternative to traditional NLP benchmarks. The game history can be accessed here https://huggingface.co/datasets/haryoaw/cultural-spyfall.

</details>


### [5] [Identity-Robust Language Model Generation via Content Integrity Preservation](https://arxiv.org/abs/2601.09141)
*Miao Zhang,Kelly Chen,Md Mehrab Tanjim,Rumi Chunara*

Main category: cs.CL

TL;DR: 提出轻量级免训练框架，通过选择性中和非关键身份信息来减少LLM输出中的身份依赖偏差，在四个基准测试中平均减少77%的偏差


<details>
  <summary>Details</summary>
Motivation: LLM输出质量会因用户社会人口属性（如性别、种族等）而出现差异，即使对于客观问题也是如此。这种身份依赖的质量退化不同于刻板印象或代表性偏见，而是核心响应质量的下降

Method: 提出轻量级免训练框架，选择性中和非关键身份信息，同时保留语义关键属性。该方法基于观察到的事实知识在不同身份间是稳健编码的，但生成行为存在偏差

Result: 在四个基准测试和18个社会人口身份上的实验显示，相比普通提示平均减少77%的身份依赖偏差，相比基于提示的防御方法减少45%的偏差

Conclusion: 该方法有效缓解了用户身份线索对核心生成质量的影响，填补了该领域的重要空白。身份信息的选择性中和能保持输出内容完整性

Abstract: Large Language Model (LLM) outputs often vary across user sociodemographic attributes, leading to disparities in factual accuracy, utility, and safety, even for objective questions where demographic information is irrelevant. Unlike prior work on stereotypical or representational bias, this paper studies identity-dependent degradation of core response quality. We show empirically that such degradation arises from biased generation behavior, despite factual knowledge being robustly encoded across identities. Motivated by this mismatch, we propose a lightweight, training-free framework for identity-robust generation that selectively neutralizes non-critical identity information while preserving semantically essential attributes, thus maintaining output content integrity. Experiments across four benchmarks and 18 sociodemographic identities demonstrate an average 77% reduction in identity-dependent bias compared to vanilla prompting and a 45% reduction relative to prompt-based defenses. Our work addresses a critical gap in mitigating the impact of user identity cues in prompts on core generation quality.

</details>


### [6] [UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2601.09215)
*Feng Zhang,Shijia Li,Chunmao Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Jingwen Xu,Han Liu*

Main category: cs.CL

TL;DR: UserLM-R1：具有推理能力的用户语言模型，通过动态用户配置和目标驱动的决策策略，提升跨领域泛化能力和战略谈判能力


<details>
  <summary>Details</summary>
Motivation: 当前用户模拟器存在两个主要问题：1）依赖静态、上下文无关的用户配置，需要大量手动调整以适应新场景，泛化能力有限；2）忽视人类的战略思维，容易被智能体操纵。需要开发能够跨领域泛化并主动参与谈判的用户模拟器。

Method: 提出UserLM-R1用户语言模型：1）构建包含静态角色和动态场景特定目标的综合用户配置；2）采用目标驱动的决策策略，在生成响应前产生高质量推理；3）通过监督微调（SFT）和多奖励强化学习（RL）优化推理和战略能力。

Result: 大量实验结果表明，UserLM-R1在多个基准测试中优于竞争基线，特别是在更具挑战性的对抗性测试集上表现突出。

Conclusion: UserLM-R1通过结合动态用户配置和目标驱动的推理策略，有效解决了现有用户模拟器的局限性，实现了更好的跨领域泛化能力和战略谈判能力。

Abstract: User simulators serve as the critical interactive environment for agent post-training, and an ideal user simulator generalizes across domains and proactively engages in negotiation by challenging or bargaining. However, current methods exhibit two issues. They rely on static and context-unaware profiles, necessitating extensive manual redesign for new scenarios, thus limiting generalizability. Moreover, they neglect human strategic thinking, leading to vulnerability to agent manipulation. To address these issues, we propose UserLM-R1, a novel user language model with reasoning capability. Specifically, we first construct comprehensive user profiles with both static roles and dynamic scenario-specific goals for adaptation to diverse scenarios. Then, we propose a goal-driven decision-making policy to generate high-quality rationales before producing responses, and further refine the reasoning and improve strategic capabilities with supervised fine-tuning and multi-reward reinforcement learning. Extensive experimental results demonstrate that UserLM-R1 outperforms competitive baselines, particularly on the more challenging adversarial set.

</details>


### [7] [Ability Transfer and Recovery via Modularized Parameters Localization](https://arxiv.org/abs/2601.09398)
*Songyao Jin,Kun Zhou,Wenqi Li,Peng Wang,Biwei Huang*

Main category: cs.CL

TL;DR: ACT方法通过分析激活差异定位能力相关通道，选择性转移参数并轻量微调，解决LLM专业化导致的灾难性遗忘问题，实现多能力整合。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定领域、语言或技能上的持续预训练或微调会降低其他能力，导致灾难性遗忘。需要研究能力在参数中的分布机制，以解决专业化与通用性的平衡问题。

Method: ACT（激活引导的通道级能力转移）：1）分析模块激活差异定位能力相关通道（通常<5%）；2）选择性转移对应参数；3）轻量微调确保兼容性。

Result: 在多语言数学和科学推理任务上，ACT能恢复遗忘的能力同时保留已有技能，还能将多个专业化模型的能力整合到单一模型中，实现最小干扰。

Conclusion: LLM能力高度集中在少量通道中且可解耦，ACT方法通过通道级参数转移有效解决灾难性遗忘，实现多能力模型整合，为模型专业化提供新思路。

Abstract: Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.

</details>


### [8] [Dialogue Telemetry: Turn-Level Instrumentation for Autonomous Information Gathering](https://arxiv.org/abs/2601.09570)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.CL

TL;DR: 提出对话遥测(DT)框架，用于监测信息收集对话的效率，包含进度估计器和停滞指数两个模型无关信号，在搜救场景中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 自主系统进行基于模式的信息收集对话时缺乏回合级别的可观测指标，无法监控获取效率和检测何时提问变得无效。

Method: 引入对话遥测(DT)框架，包含：1)进度估计器(PE)量化每个类别的剩余信息潜力；2)停滞指数(SI)检测重复类别探测、语义相似、低边际增益响应的失败模式。

Result: 在搜救场景的LLM模拟中验证DT能区分高效与停滞对话轨迹，将DT信号集成到强化学习策略中可改善策略性能。

Conclusion: DT提供可解释的回合级别监测工具，在停滞带来操作成本时能改善策略性能。

Abstract: Autonomous systems conducting schema-grounded information-gathering dialogues face an instrumentation gap, lacking turn-level observables for monitoring acquisition efficiency and detecting when questioning becomes unproductive. We introduce Dialogue Telemetry (DT), a measurement framework that produces two model-agnostic signals after each question-answer exchange: (i) a Progress Estimator (PE) quantifying residual information potential per category (with a bits-based variant), and (ii) a Stalling Index (SI) detecting an observable failure signature characterized by repeated category probing with semantically similar, low-marginal-gain responses. SI flags this pattern without requiring causal diagnosis, supporting monitoring in settings where attributing degradation to specific causes may be impractical. We validate DT in controlled search-and-rescue (SAR)-inspired interviews using large language model (LLM)-based simulations, distinguishing efficient from stalled dialogue traces and illustrating downstream utility by integrating DT signals into a reinforcement learning (RL) policy. Across these settings, DT provides interpretable turn-level instrumentation that improves policy performance when stalling carries operational costs.

</details>


### [9] [DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing](https://arxiv.org/abs/2601.09609)
*Qian Cao,Yahui Liu,Wei Bi,Yi Zhao,Ruihua Song,Xiting Wang,Ruiming Tang,Guorui Zhou,Han Li*

Main category: cs.CL

TL;DR: 提出基于半结构化长思维链的RL框架，通过多样化规划分支和群体感知多样性奖励，在保持生成质量的同时显著提升LLM输出的多样性。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的LLM增强通常会导致输出多样性降低，影响在开放式任务（如创意写作）中的实用性。现有方法缺乏明确的多样化探索机制，过于关注优化效率和性能而忽视多样性。

Method: 提出基于半结构化长思维链的RL框架，将生成过程分解为明确规划的中间步骤。引入多样化规划分支方法，在规划阶段基于多样性变化策略性地引入分歧，同时使用群体感知多样性奖励来鼓励不同的轨迹。

Result: 在创意写作基准测试中，该方法显著提高了输出多样性而不损害生成质量，持续优于现有基线方法。

Conclusion: 通过结构化规划和多样化探索机制，可以在保持生成质量的同时有效提升LLM在开放式任务中的输出多样性。

Abstract: Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.

</details>


### [10] [DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation](https://arxiv.org/abs/2601.09688)
*Yibo Wang,Lei Wang,Yue Deng,Keming Wu,Yao Xiao,Huanjin Yao,Liwei Kang,Hai Ye,Yongcheng Jing,Lidong Bing*

Main category: cs.CL

TL;DR: DeepResearchEval：一个用于深度研究任务构建和智能评估的自动化框架，通过角色驱动生成复杂研究任务，并采用自适应质量评估和主动事实核查进行多维度评估


<details>
  <summary>Details</summary>
Motivation: 现有深度研究系统的评估存在挑战：任务构建需要大量标注、依赖静态评估维度、在缺少引用时无法可靠验证事实。需要自动化框架来评估多步骤网络研究、分析和跨源合成能力。

Method: 1. 任务构建：采用角色驱动管道生成基于多样化用户配置的现实复杂研究任务，应用两阶段过滤（任务资格和搜索必要性）保留需要多源证据整合和外部检索的任务。2. 评估：采用智能管道，包含自适应点式质量评估（动态推导任务特定评估维度、标准和权重）和主动事实核查（自主提取和通过网络搜索验证报告陈述，即使缺少引用）。

Result: 提出了DeepResearchEval框架，能够自动化生成需要深度研究的复杂任务，并提供动态、全面的评估方法，解决了现有评估方法的局限性。

Conclusion: DeepResearchEval为深度研究系统提供了更有效、自动化的评估框架，通过角色驱动的任务生成和智能评估管道，能够更准确地评估系统的多步骤研究、分析和跨源合成能力。

Abstract: Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.

</details>


### [11] [Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection](https://arxiv.org/abs/2601.09692)
*Tianyi Niu,Justin Chih-Yao Chen,Genta Indra Winata,Shi-Xiong Zhang,Supriyo Chakraborty,Sambit Sahu,Yue Zhang,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 论文提出RGD（使用生成数据的路由）设置，其中路由器仅使用生成器LLM从高级任务描述生成的查询和答案进行训练。研究发现查询-答案路由器比仅查询路由器对生成器质量下降更敏感，并提出了CASCAL这一更鲁棒的仅查询路由器方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器方法通常假设可以访问真实标注数据，但在实践中，特别是当用户请求分布异构且未知时，这种数据往往不可用。因此需要研究仅使用生成数据训练路由器的挑战性设置。

Method: 提出RGD设置，评估查询-答案路由器和仅查询路由器。分析发现有效生成器的两个关键特征：1）能准确回答自己生成的问题；2）问题能在模型池中产生足够的性能差异。基于此提出数据过滤方法，并进一步提出CASCAL方法，通过共识投票估计模型正确率，并通过层次聚类识别模型特定技能领域。

Result: 在四个不同基准测试和12个模型上的实验表明，查询-答案路由器比仅查询路由器对生成器质量下降更敏感。CASCAL方法对生成器质量具有更强的鲁棒性，在弱生成器数据上训练时，比最佳查询-答案路由器绝对准确率高4.6%。

Conclusion: 仅使用生成数据训练LLM路由器是可行的，但需要仔细设计路由器架构。查询-答案路由器对生成器质量敏感，而仅查询路由器（特别是CASCAL）在生成器质量较差时表现更鲁棒。有效生成数据的关键是生成器能准确回答自己的问题，且问题能充分区分模型性能。

Abstract: Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: LAUDE是一个统一的硬件设计单元测试生成与调试框架，利用大语言模型的思维链推理能力，结合提示工程和设计执行信息，显著提高了硬件设计的测试覆盖率和调试成功率。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中的单元测试对于确保组件功能正确性至关重要，但开发针对各种设计特性的测试需要深入理解设计功能并具备创造力。当测试暴露设计缺陷时，调试过程通常繁琐且耗时。因此需要自动化工具来提升测试生成和调试效率。

Method: LAUDE框架将硬件设计源代码的语义理解与大语言模型的思维链推理能力相结合，集成提示工程和设计执行信息来增强单元测试生成的准确性和代码可调试性。该框架支持闭源和开源LLM，并在VerilogEval数据集的错误硬件设计代码上进行应用。

Result: 在VerilogEval数据集上，LAUDE生成的单元测试在组合设计中检测到高达100%的错误，在时序设计中检测到93%的错误；在调试方面，成功调试了93%的组合设计和84%的时序设计。

Conclusion: LAUDE框架通过结合LLM的推理能力和硬件设计语义理解，显著提升了硬件设计的自动化测试生成和调试能力，为硬件设计验证提供了有效的解决方案。

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [13] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 通过遗传-Pareto框架优化提示，提升小型LLM生成OpenACC并行代码的能力，使编译成功率和GPU加速效果显著提升


<details>
  <summary>Details</summary>
Motivation: OpenACC降低了GPU卸载的门槛，但编写高性能的pragma仍然复杂，需要深厚的内存层次、数据移动和并行化策略专业知识。LLM为自动并行代码生成提供了有前景的解决方案，但简单的提示往往导致语法错误、无法编译或性能不佳。

Method: 采用系统化的提示优化方法，利用GEPA（遗传-Pareto）框架，通过反射反馈循环迭代演化提示。该过程使用指令的交叉和变异，以专家策划的黄金示例为指导，并基于黄金和预测pragma之间的子句和子句参数级别不匹配提供结构化反馈。

Result: 在PolyBench套件评估中，优化提示显著提高了编译成功率：GPT-4.1 Nano从66.7%提升到93.3%，GPT-5 Nano从86.7%提升到100%，匹配或超越了更大、更昂贵版本的能力。优化提示还使实现GPU加速的程序数量增加了21%。

Conclusion: 提示优化有效释放了更小、更便宜LLM在编写稳定有效GPU卸载指令方面的潜力，为HPC工作流中基于指令的自动并行化建立了成本效益高的途径。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [14] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: LLM生成的数据库测试中，不稳定测试比例略高于现有测试，主要原因是依赖未保证的顺序（无序集合），且LLM会通过提示上下文将现有测试的不稳定性传递到新生成的测试中。


<details>
  <summary>Details</summary>
Motivation: LLM生成的测试存在不稳定性问题，但其普遍性和根本原因尚不清楚。本研究旨在分析LLM生成的数据库测试中不稳定测试的普遍性、根本原因以及不稳定性从现有测试到生成测试的传递现象。

Method: 研究使用GPT-4o和Mistral-Large-Instruct-2407两个LLM，在四个关系数据库管理系统（SAP HANA、DuckDB、MySQL、SQLite）中扩增测试套件，评估生成测试用例的不稳定性，并通过手动检查分析根本原因。

Result: 生成测试的不稳定测试比例略高于现有测试。手动检查发现，72/115个不稳定测试（63%）的根本原因是测试依赖未保证的顺序（无序集合）。LLM通过提示上下文将现有测试的不稳定性传递到新生成的测试中，这种现象在闭源系统（如SAP HANA）中比开源系统更普遍。

Conclusion: 研究揭示了LLM生成测试中不稳定性的主要类型和传递机制，为开发者提供了预期指导，并强调了在LLM测试生成中提供定制化上下文的重要性。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [15] [AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems](https://arxiv.org/abs/2601.09393)
*Zirui Wang,Guangba Yu,Michael R. Lyu*

Main category: cs.SE

TL;DR: AI-NativeBench：首个基于MCP和A2A标准的应用中心化白盒AI原生基准测试套件，通过分布式追踪中的智能体跨度分析，揭示传统指标无法捕捉的工程现实。


<details>
  <summary>Details</summary>
Motivation: 从云原生向AI原生架构的转变正在重塑软件工程，但传统的黑盒评估范式已不适用，现有基准测试仅衡量原始模型能力而忽视了系统级执行动态。

Method: 引入AI-NativeBench基准套件，基于模型上下文协议(MCP)和智能体到智能体(A2A)标准，将智能体跨度作为分布式追踪中的一等公民，实现超越简单能力的工程特性细粒度分析。

Result: 在21个系统变体上测试发现：参数悖论（轻量模型在协议遵循上常优于旗舰模型）、普遍存在的推理主导（使协议开销次要）、昂贵失败模式（自愈机制在不可行工作流上成为成本倍增器）。

Conclusion: 这项工作为从衡量模型能力转向工程化可靠AI原生系统提供了首个系统性证据，开源基准测试和数据集促进可复现性和进一步研究。

Abstract: The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.

</details>


### [16] [DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries](https://arxiv.org/abs/2601.09440)
*Yi Gao,Xing Hu,Tongtong Xu,Jiali Zhao,Xiaohu Yang,Xin Xia*

Main category: cs.SE

TL;DR: DepRadar是一个用于深度学习库缺陷影响分析的智能体协调框架，通过多智能体协作从PR/提交中提取缺陷语义，分析触发条件，并检查下游程序是否受影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习库（如Transformers、Megatron）的缺陷（从静默计算错误到性能回归）难以被下游用户检测，因为需要理解缺陷语义并检查复杂的触发条件（配置标志、运行时环境、间接API使用）。

Method: DepRadar协调四个专门智能体分三步工作：1) PR挖掘器和代码差异分析器从提交/PR提取结构化缺陷语义；2) 编排智能体合成统一缺陷模式和触发条件；3) 影响分析器检查下游程序是否触发缺陷。结合静态分析和DL特定领域规则提高准确性和可解释性。

Result: 在两个代表性DL库的157个PR和70个提交上评估，缺陷识别精度达90%，生成高质量结构化字段（平均字段得分1.6）。在122个客户端程序上，识别受影响案例的召回率90%，精度80%，显著优于其他基线方法。

Conclusion: DepRadar通过智能体协调框架有效解决了DL库缺陷影响分析的挑战，实现了高精度的缺陷识别和影响分析，为下游用户提供了实用的工具。

Abstract: Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.

</details>


### [17] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 对nf-core科学工作流管道的实证研究发现13个关键挑战，包括管道开发集成、bug修复、基因组数据整合等，89.38%的问题能解决，标签和代码片段显著提升解决概率。


<details>
  <summary>Details</summary>
Motivation: 尽管Nextflow和nf-core社区在科学工作流管理中广泛应用，但用户在这些管道开发和维护中面临的具体挑战缺乏系统性了解，需要实证研究来揭示实际困难和改进机会。

Method: 对25,173个问题和拉取请求进行实证研究，使用BERTopic主题建模识别关键挑战，统计分析解决动态，评估标签和代码片段对问题解决的影响。

Result: 识别出13个关键挑战，89.38%的问题最终关闭，半数在3天内解决；标签（大效应，δ=0.94）和代码片段（中效应，δ=0.50）显著提高解决可能性；工具开发和仓库维护是最具挑战性的任务。

Conclusion: 研究揭示了nf-core管道协作开发和维护的实际挑战，为提升可用性、可持续性和可重复性提供了可操作的见解，特别强调了标签系统和代码示例的重要性。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [18] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 研究发现，在最新LLM版本下，简单的LLM测试生成方法在代码覆盖率、分支覆盖率和变异得分等所有测试有效性指标上都优于现有的四种最先进工具，且成本相当。建议采用先类后方法的策略可减少约20%的LLM请求。


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试生成技术通常与较弱的基线（旧版LLM和简单提示）进行比较，这可能会夸大这些技术的性能贡献。随着LLM的快速发展，更强的LLM可能使这些复杂技术失去优势，因此需要重新评估。

Method: 复制了四种最先进的LLM测试生成工具（HITS、SymPrompt、TestSpark、CoverUp），并将它们与简单的LLM测试生成方法进行比较。在所有方法中集成了当前LLM版本，并在393个类和3,657个方法上进行了实验。还提出了先针对程序类（测试生成更高效）再针对未覆盖方法的策略。

Result: 简单的LLM方法在所有测试有效性指标上都优于先前的最先进方法：行覆盖率提高17.72%，分支覆盖率提高19.80%，变异得分提高20.92%，且成本（LLM查询次数）相当。先类后方法的策略在保持相当（略高）有效性的同时，减少了约20%的LLM请求。

Conclusion: 随着LLM能力的提升，复杂的测试生成工程技术可能不再必要，简单的LLM方法已经足够有效。采用适当的生成粒度策略可以进一步优化成本效益。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [19] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: ShortCoder是一个知识注入框架，通过语法级简化规则、混合数据合成和微调策略，优化代码生成效率，在保持语义等价性和可读性的同时减少18.1%-37.8%的token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码生成时每个token都需要完整推理过程，导致内存占用高、资源消耗大。当前研究主要关注推理阶段优化（如提示压缩、模型量化），但生成阶段优化不足。

Method: 提出三部分方法：1）基于AST保持转换的10个Python语法级简化规则；2）结合规则重写和LLM引导精炼的混合数据合成管道，构建ShorterCodeBench语料库；3）将简洁性意识注入基础LLM的微调策略。

Result: 在HumanEval基准测试中，ShortCoder持续优于最先进方法，代码生成效率比先前方法提高18.1%-37.8%，同时保持代码生成性能。

Conclusion: ShortCoder框架有效解决了代码生成效率问题，通过语法简化、数据合成和模型微调，在保持语义等价性的同时显著减少token消耗，为代码生成优化提供了新方向。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [20] [Anthropic introduced Cowork](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/nN4lBI_Lt8NsC9JkoB-meY6h87rrO-i1FPqCkeG6E0Q=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出Claude Cowork，这是集成到Claude桌面应用中的简化版Claude Code，允许用户分配文件夹供Claude访问并通过聊天引导，实现无需复杂设置的类代理工作流程。


<details>
  <summary>Details</summary>
Motivation: 为开发者提供更简单、更集成的代码协作工具，降低使用门槛，让用户无需复杂设置即可实现类似代理的工作流程，提高开发效率。

Method: 将Claude Code简化版本集成到Claude桌面应用中，用户只需分配文件夹供Claude访问，通过聊天界面引导Claude进行代码相关操作，实现类代理的工作流程。

Result: 创建了Claude Cowork工具，目前处于研究预览阶段，仅限Max订阅用户使用，提供了更简单直接的代码协作体验。

Conclusion: Claude Cowork通过简化设置和集成到桌面应用，为开发者提供了更便捷的代码协作工具，降低了使用门槛，但目前功能有限且仅面向特定用户群体。

Abstract: Anthropic introduced Cowork (6 minute read) Claude Cowork is a simpler version of Claude Code built into the Claude Desktop app. Users can assign a folder for Claude to access and guide it via chat, enabling agent-like workflows without complex setup. The tool is in research preview and currently limited to Max subscribers.

</details>


### [21] [When AI writes almost all code, what happens to software engineering?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fwhen-ai-writes-almost-all-code-what%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/fO_s9QKIPj8OrGeNiUD71BsHUCmsf_pgLSDnEexLbio=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI模型现在能编写大部分软件代码，导致软件工程实践发生转变，降低了语言专业知识和常规编码任务的价值，同时增加了对具备产品思维的tech lead的需求。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型（如Opus 4.5和GPT-5.2）能够编写大部分软件代码，软件工程领域正在经历根本性变革。传统编码技能的价值正在下降，而新的技能需求正在出现。

Method: 本文通过分析当前AI代码生成技术的发展现状，探讨AI对软件工程实践的影响，特别是对工程师角色和技能要求的变化。

Result: AI接管了大部分编码工作，减少了语言专业知识和常规编码任务的价值，但增加了对具备产品思维的技术领导者的需求。工程师需要监督复杂任务，强调产品管理和技术监督的混合技能。

Conclusion: 软件工程正在从编码为中心转向以产品思维和技术监督为中心，工程师需要发展新的混合技能来适应AI驱动的开发环境。

Abstract: When AI writes almost all code, what happens to software engineering? (12 minute read) AI models like Opus 4.5 and GPT-5.2 now write most software code, prompting a shift in software engineering practices. This reduces the value of language expertise and routine coding tasks while increasing the demand for tech leads who are product-minded. Although AI can handle more of the coding workload, engineers will need to oversee complex tasks, emphasizing a hybrid skill set in both product managemen...

</details>


### [22] [Map-Augmented Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Famap-ml.github.io%2FThinking-with-Map%2F%3Futm_source=tldrai/1/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/E0ptM2nd7dNNBDaInODjDcB_0DissRPMcuOZb1hmBbI=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 阿里巴巴提出地图增强智能体用于图像地理定位，通过地图引导循环结合强化学习和并行测试时推理来提高预测精度


<details>
  <summary>Details</summary>
Motivation: 解决图像地理定位任务中预测精度不足的问题，传统方法可能无法充分利用地图信息进行准确的位置推断

Method: 采用地图增强智能体架构，嵌入地图引导循环，结合强化学习进行决策优化，并使用并行测试时推理提升效率

Result: 提高了图像地理定位的预测准确性，通过地图信息的增强和强化学习优化获得了更好的性能表现

Conclusion: 地图增强智能体框架有效提升了图像地理定位任务的性能，证明了地图信息与强化学习结合的价值

Abstract: Map-Augmented Agent (2 minute read) Alibaba introduces a map-augmented agent for image geolocalization, embedding it in a map-guided loop that combines reinforcement learning and parallel test-time inference to improve prediction accuracy.

</details>


### [23] [Why We Built Our Own Background Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuilders.ramp.com%2Fpost%2Fwhy-we-built-our-background-agent%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/u0zzCmUTDI0d_pLLtjlOepTs-RKYyPLcGHiDloYreL0=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ramp开发了名为Inspect的自定义后台编码代理，该代理能自主编写、验证和调试代码，使用与人类工程师相同的上下文和工具，已内部快速采用，编写了约30%的合并PR。


<details>
  <summary>Details</summary>
Motivation: Ramp需要构建自己的后台代理来解决现有编码代理的局限性，创建能够自主操作、使用与人类工程师相同上下文和工具的系统，以提高开发效率。

Method: 开发了Inspect自定义后台编码代理，在快速沙盒化VM中运行完整开发环境，集成现有工具，支持Slack、Web和Chrome扩展等多种接口。

Result: Inspect在内部快速被采用，编写了大约30%的合并拉取请求，显著提高了开发效率。

Conclusion: 构建自定义后台编码代理是有效的，Inspect展示了自主编码代理在实际工程环境中的可行性和价值。

Abstract: Why We Built Our Own Background Agent (15 minute read) Inspect is a custom background coding agent developed by Ramp that autonomously writes, verifies, and debugs code with the same context and tools a human engineer would use. The agent operates in fast, sandboxed VMs with full development environments, integrates with many existing tools, and supports interfaces like Slack, web, and Chrome extensions. Inspect was adopted quickly internally, writing approximately 30% of merged pull requests.

</details>


### [24] [Swark](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fswark-io%2Fswark%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/Yw_rGIRLnTVrw6sZIQCBzpynPgiTl8rcb7QLrL_TFpI=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Swark是一个开源的VS Code扩展，利用LLM从代码自动生成Mermaid.js格式的架构图，并与GitHub Copilot集成。


<details>
  <summary>Details</summary>
Motivation: 传统架构图创建过程繁琐且容易过时，需要自动化工具从代码直接生成可视化架构图，提高开发效率和文档质量。

Method: 开发VS Code扩展，集成LLM分析代码结构，自动生成Mermaid.js格式的架构图，并与GitHub Copilot深度集成。

Result: 创建了开源工具Swark，能够自动从代码生成架构图，支持Mermaid.js格式，提供直观的可视化效果。

Conclusion: Swark通过自动化架构图生成，简化了开发文档工作，提高了代码理解和维护效率。

Abstract: Swark (GitHub Repo) Swark is a free and open-source VS Code extension that automatically generates architecture diagrams from code using LLMs. It integrates directly with GitHub Copilot and outputs diagrams in the Mermaid.js format.

</details>


### [25] [OpenCode vs Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fopencode-vs-claude-code%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/h2ib2fAoEzTCuaJDe69N1hn3ZJ6bsG98VG2jQONhQz0=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 对比OpenCode和Claude Code两款AI编程助手：Claude Code在Anthropic生态内提供更流畅的集成体验，OpenCode则是支持75+AI提供商的开源替代方案


<details>
  <summary>Details</summary>
Motivation: 比较两款AI编程助手（OpenCode和Claude Code）的性能差异，帮助开发者了解不同工具的特点和适用场景

Method: 使用相同模型进行头对头测试，对比两款工具在代码库对话和终端命令执行方面的表现

Result: Claude Code更快更流畅（9分钟 vs 16分钟），OpenCode更全面但速度较慢；Claude Code集成度更高但局限于Anthropic生态，OpenCode开源且支持更多AI提供商

Conclusion: 选择取决于需求：追求速度和集成体验选Claude Code，需要开源性和多提供商支持选OpenCode

Abstract: OpenCode vs Claude Code (8 minute read) OpenCode and Claude Code are both AI coding assistants that let you chat with your codebase and run terminal commands. Claude Code offers a polished, integrated experience locked to Anthropic's ecosystem, while OpenCode is an open-source alternative supporting 75+ AI providers. In head-to-head testing using the same model, Claude Code was faster and more streamlined (9 minutes vs 16 minutes total), while OpenCode was more thorough.

</details>


### [26] [Cut your dev loop from hours to seconds](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260114%26utm_content=std/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/CsEoHZx-EAi9gq5IhVrrqlicS08mst3o0ooW2s_3q44=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍mirrord工具，让开发者能在本地运行代码时无缝访问Kubernetes集群中的所有资源，无需实际部署到云端


<details>
  <summary>Details</summary>
Motivation: 解决开发者在本地开发时需要频繁部署到云端进行测试的痛点，缩短开发循环时间，提高开发效率

Method: 通过mirrord工具在本地环境中模拟云端的Kubernetes集群访问，让本地代码能够直接与集群资源交互

Result: 将开发循环从数小时缩短到数秒，提供类似云端测试的体验但无需实际部署

Conclusion: mirrord工具能显著提升开发效率，让开发者享受云端测试的便利性而无需承受实际部署的复杂性

Abstract: Cut your dev loop from hours to seconds (Sponsor) Run your code locally with seamless access to everything in your Kubernetes cluster. It's like testing in the cloud without the hassle of actually deploying it there. Learn more about mirrord

</details>


### [27] [Nogic](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarketplace.visualstudio.com%2Fitems%3FitemName=Nogic.nogic%26utm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/qbPKvFuXNvk3CYR9_jZ1wGLWZQFTQ_ucUloWI7NG-5o=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Nogic是一个VS Code扩展，通过交互式图表帮助用户可视化代码库结构


<details>
  <summary>Details</summary>
Motivation: 帮助开发者更好地理解和导航复杂代码库，通过可视化手段提升代码理解效率

Method: 开发Visual Studio Code扩展，生成交互式图表来展示代码库结构

Result: 创建了Nogic工具，能够可视化代码结构，提供交互式导航体验

Conclusion: 可视化工具能有效帮助开发者理解复杂代码库结构，提升开发效率

Abstract: Nogic (Website) Nogic is a Visual Studio Code extension that helps users visualize their codebase structure with interactive diagrams.

</details>


### [28] [Using Proxies to Hide Secrets from Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.joinformal.com%2Fblog%2Fusing-proxies-to-hide-secrets-from-claude-code%2F%3Futm_source=tldrdev/1/0100019bbc69ad50-6dfc025c-e5b3-4edf-9df6-d42ae34eb166-000000/dEeK9RSisug9DMGXSjnIcdVjImQWi-WxWgr99SyGP3E=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用网络代理隐藏API密钥等敏感信息，防止Claude Code沙箱环境中的数据泄露风险


<details>
  <summary>Details</summary>
Motivation: Claude Code的沙箱环境存在安全挑战，可能通过网络访问或环境变量暴露API密钥等敏感数据，需要更安全的方法来保护这些信息

Method: 采用网络代理技术，在沙箱环境中通过代理服务器注入实际的API密钥，避免敏感信息直接暴露在沙箱环境中

Result: 网络代理方法能够有效隐藏敏感数据，提供比直接在沙箱中存储API密钥更安全的解决方案

Conclusion: 使用代理技术是保护Claude Code沙箱环境中敏感信息的有效安全策略

Abstract: Using Proxies to Hide Secrets from Claude Code (10 minute read) Claude Code has security challenges, as its sandboxes can expose sensitive data such as API keys through network access or environment variables, but network proxies can be used to inject the actual API keys in a safer manner.

</details>


### [29] [WeKnora](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FTencent%2FWeKnora%3Futm_source=tldrdevops/1/0100019bbc7b9b13-bbd5f8d3-5e71-4451-b5a4-96a458134216-000000/6Ktek_x_uFLEzofviJ-Kj8RMlSn2ppSEsJop0PbPAm0=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: WeKnora是微信开发的基于LLM的框架，用于深度文档理解和语义检索，采用RAG范式，具有模块化架构，支持ReACT Agent模式和知识图谱转换，v0.2.0版本增加了Web UI配置界面和登录认证安全功能。


<details>
  <summary>Details</summary>
Motivation: 解决深度文档理解和语义检索的需求，通过RAG范式结合LLM能力，提供模块化、可扩展的解决方案，并增强系统的易用性和安全性。

Method: 采用RAG（检索增强生成）范式，构建模块化架构，支持ReACT Agent模式进行交互式推理，实现知识图谱转换，并通过Web UI提供可视化配置界面。

Result: 开发了WeKnora框架，v0.2.0版本实现了Web UI配置界面和登录认证安全功能，提供了完整的深度文档理解和语义检索解决方案。

Conclusion: WeKnora是一个功能完善的LLM驱动框架，通过RAG范式有效支持深度文档理解和语义检索，模块化设计和安全增强使其具有实用性和可扩展性。

Abstract: WeKnora (GitHub Repo) WeChat's WeKnora, an LLM-powered framework designed for deep document understanding and semantic retrieval using the RAG paradigm, has a modular architecture that supports features like ReACT Agent mode and knowledge graph transformation. The v0.2.0 update introduced a Web UI for model configuration and enhanced security with login authentication.

</details>


### [30] [Building trust in agentic tools: What we learned from our users](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fabout.gitlab.com%2Fblog%2Fbuilding-trust-in-agentic-tools-what-we-learned-from-our-users%2F%3Futm_source=tldrdevops/1/0100019bbc7b9b13-bbd5f8d3-5e71-4451-b5a4-96a458134216-000000/n5kABcA7UE-yaw7LgXM1RD9JPGwHs6Mmd6vEcYUzsJs=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了如何在DevSecOps工作流中建立对AI代理工具的信任，发现信任是通过积累微小转折点而非突破性时刻逐步建立的


<details>
  <summary>Details</summary>
Motivation: 研究如何在实际工作场景中建立用户对AI代理工具的信任，特别是在DevSecOps领域，以促进这些工具的采用和有效使用

Method: 基于用户研究和实践经验，分析信任建立的关键因素，包括安全措施、透明度、上下文记忆和用户需求预测等方面

Result: 发现信任是通过一系列微小转折点逐步建立的，安全防护、透明性、上下文记忆和预测用户需求是建立信任的关键要素

Conclusion: 在DevSecOps工作流中，AI代理工具的信任建立是一个渐进过程，需要通过系统性的设计考虑来促进用户信任和采用

Abstract: Building trust in agentic tools: What we learned from our users (6 minute read) Trust in AI agents develops through accumulated micro-inflection points rather than breakthrough moments. Safeguards, transparency, contextual memory, and anticipation of user needs gradually build confidence and drive adoption in DevSecOps workflows.

</details>


### [31] [Atoms](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fatoms.dev%2F%3Futm_source=tldrfounders/1/0100019bbc9f1101-a8e9559d-f2df-4d83-8420-cb55ab02b847-000000/hxN0GWiomLzTbgRlYc7551wQJjiNPK-_VFCFb5aSRlI=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Atoms是一个AI代理平台，能够研究、设计、构建和部署具有集成业务功能的全栈应用程序


<details>
  <summary>Details</summary>
Motivation: 解决全栈应用开发中需要集成业务功能的复杂性问题，通过AI代理自动化整个开发流程

Method: 构建AI代理平台，整合研究、设计、构建和部署的全流程自动化能力

Result: 开发出能够处理完整应用开发周期的AI代理系统

Conclusion: Atoms平台展示了AI代理在全栈应用开发中的潜力，能够显著提高开发效率

Abstract: Atoms (Tool) Atoms is an AI agent platform that researches, designs, builds, and deploys full-stack applications with integrated business functionality.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache](https://arxiv.org/abs/2601.09083)
*Chi-Chih Chang,Siqi Zhu,Zhichen Zeng,Haibin Lin,Jiaxuan You,Mohamed S. Abdelfattah,Ziheng Jiang,Xuehai Qian*

Main category: cs.LG

TL;DR: SRT是一种利用树结构缓存和推测解码加速语言模型强化学习的模型无关方法


<details>
  <summary>Details</summary>
Motivation: 现有语言模型强化学习方法在生成阶段计算成本高，同一提示在不同训练步骤中的生成结果具有相似性，可以利用这种相似性来加速训练过程

Method: 为每个提示建立树结构缓存存储历史生成结果，在生成时作为草稿模型进行推测解码，通过在线更新缓存和利用GPU空闲时间进行预生成来保持缓存新鲜度

Result: SRT能显著降低生成延迟和步延迟，减少每token推理成本，在rollout阶段实现最高2.08倍的时钟时间加速

Conclusion: SRT是一种简单有效的加速方法，可与标准RL流程集成，在不牺牲分布正确性的前提下提升训练效率

Abstract: We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.

</details>


### [33] [MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting](https://arxiv.org/abs/2601.09085)
*Kangda Wei,Ruihong Huang*

Main category: cs.LG

TL;DR: MMR-GRPO通过引入最大边际相关性来重新加权奖励，基于完成多样性减少训练步骤，在保持性能的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: GRPO方法训练数学推理模型需要每个提示生成多个完成，导致训练计算成本高昂。虽然最近研究减少了达到峰值性能所需的训练步骤，但由于每步成本增加，总体训练时间往往保持不变甚至增加。

Method: 提出MMR-GRPO，集成最大边际相关性来基于完成多样性重新加权奖励。核心洞察是语义冗余的完成贡献有限的学习信号；优先考虑多样化的解决方案能产生更多信息化的更新并加速收敛。

Result: 在三个模型规模（1.5B、7B、8B）、三个GRPO变体和五个数学推理基准上的广泛评估显示，MMR-GRPO在保持可比峰值性能的同时，平均需要减少47.9%的训练步骤和70.2%的墙上时间。这些收益在不同模型、方法和基准上保持一致。

Conclusion: MMR-GRPO通过奖励多样性显著加速GRPO训练，在保持性能的同时大幅降低计算成本，为高效训练数学推理模型提供了有效解决方案。

Abstract: Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.

</details>


### [34] [Interpretable Probability Estimation with LLMs via Shapley Reconstruction](https://arxiv.org/abs/2601.09151)
*Yang Nan,Qihao Wen,Jiahao Wang,Pengfei He,Ravi Tandon,Yong Ge,Han Xu*

Main category: cs.LG

TL;DR: PRISM框架使用Shapley值分解LLM的概率估计，通过量化各输入因素的边际贡献来重构校准的最终估计，提高透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在概率估计方面有潜力，但直接提示存在输出噪声大、预测过程不透明的问题，需要提高透明度和准确性。

Method: 提出PRISM框架，使用Shapley值量化每个输入因素的边际贡献，然后聚合这些因素级贡献来重构校准的最终概率估计。

Result: PRISM在金融、医疗、农业等多个领域优于直接提示和其他基线方法，提高了预测准确性，并提供透明的预测流程。

Conclusion: PRISM为LLM概率估计带来了透明度和精确性，有助于建立对基于LLM的决策支持系统的信任。

Abstract: Large Language Models (LLMs) demonstrate potential to estimate the probability of uncertain events, by leveraging their extensive knowledge and reasoning capabilities. This ability can be applied to support intelligent decision-making across diverse fields, such as financial forecasting and preventive healthcare. However, directly prompting LLMs for probability estimation faces significant challenges: their outputs are often noisy, and the underlying predicting process is opaque. In this paper, we propose PRISM: Probability Reconstruction via Shapley Measures, a framework that brings transparency and precision to LLM-based probability estimation. PRISM decomposes an LLM's prediction by quantifying the marginal contribution of each input factor using Shapley values. These factor-level contributions are then aggregated to reconstruct a calibrated final estimate. In our experiments, we demonstrate PRISM improves predictive accuracy over direct prompting and other baselines, across multiple domains including finance, healthcare, and agriculture. Beyond performance, PRISM provides a transparent prediction pipeline: our case studies visualize how individual factors shape the final estimate, helping build trust in LLM-based decision support systems.

</details>


### [35] [GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization](https://arxiv.org/abs/2601.09233)
*Zhengyang Zhao,Lu Ma,Yizhen Jiang,Xiaochen Ma,Zimo Meng,Chengyu Shen,Lexiang Tang,Haoze Sun,Peng Pei,Wentao Zhang*

Main category: cs.LG

TL;DR: 提出GIFT方法，将SFT重新表述为有限温度的能量势，解决传统SFT+RL训练范式中的优化不匹配问题，为RL初始化提供更好的分布基础。


<details>
  <summary>Details</summary>
Motivation: 传统大推理模型的后训练范式（SFT+RL）存在内在优化不匹配：SFT的刚性监督导致分布坍缩，耗尽后续RL所需的探索空间。

Method: 将SFT重新表述为统一的后训练框架，提出Gibbs初始化与有限温度（GIFT）方法。将标准SFT视为抑制基础先验的零温度极限，而GIFT将监督作为有限温度的能量势，建立分布桥梁确保整个后训练流程的目标一致性。

Result: 实验表明，GIFT在用于RL初始化时显著优于标准SFT和其他竞争基线，为后训练实现全局最优提供了数学原理上的路径。

Conclusion: GIFT通过有限温度的能量势框架解决了SFT+RL范式中的优化不匹配问题，为后训练提供了更优的初始化方法。

Abstract: The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.

</details>


### [36] [Reward Learning through Ranking Mean Squared Error](https://arxiv.org/abs/2601.09236)
*Chaitanya Kharyal,Calarina Muslimani,Matthew E. Taylor*

Main category: cs.LG

TL;DR: 提出R4方法，使用排序均方误差损失从人类评分中学习奖励函数，相比现有评分和偏好方法在机器人控制任务上表现更好且需要更少反馈。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励设计是一个瓶颈，传统方法需要手动设计奖励函数。虽然已有从人类偏好学习奖励的方法，但评分反馈比二元偏好更丰富且认知负担更小。现有评分方法缺乏理论保证，需要更有效的方法。

Method: 提出R4方法，使用排序均方误差损失。从轨迹-评分对数据集中学习，每个轨迹有离散评分。训练时采样轨迹集，预测回报，使用可微排序算子获得软排序，然后优化软排序与教师评分之间的均方误差损失。

Result: 在OpenAI Gym和DeepMind Control Suite的机器人运动基准测试中，使用模拟人类反馈，R4始终匹配或优于现有评分和偏好强化学习方法，且需要显著更少的反馈。

Conclusion: R4方法通过新颖的排序均方误差损失，从评分反馈中学习奖励函数，提供形式化保证，在实践中表现优异且反馈效率高。

Abstract: Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., "bad," "neutral," "good"). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.

</details>


### [37] [RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning](https://arxiv.org/abs/2601.09253)
*Zehua Liu,Shuqi Liu,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: RIFT（Reward Informed Fine-Tuning）是一种利用所有自生成样本进行LLM对齐的框架，通过奖励加权损失从正负轨迹中学习，解决了SFT依赖专家数据和RFT丢弃负样本的数据效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法存在数据效率问题：SFT依赖昂贵的专家标注数据，而RFT丢弃了有价值的负样本。需要一种能够充分利用自生成数据（包括正负样本）的高效对齐方法。

Method: 提出RIFT框架，不同于RFT的硬阈值方法，RIFT重新利用负轨迹，通过标量奖励对损失进行加权，从模型输出的正负轨迹中学习。为避免朴素奖励集成导致的训练崩溃（直接乘法产生无界损失），引入了稳定的损失公式确保数值鲁棒性和优化效率。

Result: 在多个基础模型上的数学基准测试表明，RIFT始终优于RFT。结果表明RIFT是使用混合质量自生成数据进行对齐的鲁棒且数据高效的替代方案。

Conclusion: RIFT是一个简单有效的框架，通过奖励加权损失充分利用所有自生成样本，为LLM对齐提供了数据效率更高的解决方案，特别是在混合质量数据场景下表现鲁棒。

Abstract: While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.

</details>


### [38] [GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR](https://arxiv.org/abs/2601.09361)
*Jiaying Zhang,Lei Shi,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: GeoRA是一种针对强化学习可验证奖励（RLVR）的几何感知低秩适应方法，通过SVD提取主方向并冻结残差组件，解决了现有参数高效方法在RLVR中的几何失配和优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效方法（如PiSSA和MiLoRA）专为监督微调设计，未考虑RLVR独特的优化动态和几何结构，直接应用会导致谱崩溃和优化不稳定，而利用更新稀疏性的方法在现代硬件上存在效率瓶颈。

Method: GeoRA通过奇异值分解（SVD）在几何约束子空间中提取主方向来初始化适配器，同时冻结残差组件，利用RL更新子空间的各向异性和可压缩性，保持预训练几何结构并通过密集算子实现高效GPU计算。

Result: 在Qwen和Llama模型上的实验表明，GeoRA缓解了几何失配引起的优化瓶颈，在关键数学基准测试中持续优于现有低秩基线方法，达到SOTA结果，并在领域外任务中表现出更好的泛化能力和抗灾难性遗忘能力。

Conclusion: GeoRA通过几何感知的低秩适应有效解决了RLVR中的优化问题，为大规模推理模型的强化学习微调提供了高效稳定的参数高效方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.

</details>


### [39] [Preliminary Tests of the Anticipatory Classifier System with Hindsight Experience Replay](https://arxiv.org/abs/2601.09400)
*Olgierd Unold,Stanisław Franczyk*

Main category: cs.LG

TL;DR: ACS2HER将Anticipatory Classifier System与Hindsight Experience Replay结合，通过在失败时重新标记状态为虚拟目标来解决稀疏奖励问题，加速了知识获取但增加了计算开销。


<details>
  <summary>Details</summary>
Motivation: ACS2在稀疏奖励环境中性能停滞，需要增强其在稀疏奖励环境中的学习能力。

Method: 将ACS2与HER机制集成，当代理未能达到主要目标时触发事后学习，将访问过的状态重新标记为虚拟目标以增加学习信号密度。

Result: 在Maze 6和FrozenLake基准测试中，ACS2HER显著加速了知识获取和环境掌握，但增加了计算开销和分类器数量。

Conclusion: 首次分析了学习分类器系统中前瞻机制与回顾目标重标记的结合，证明了效率提升但需权衡计算成本。

Abstract: This paper introduces ACS2HER, a novel integration of the Anticipatory Classifier System (ACS2) with the Hindsight Experience Replay (HER) mechanism. While ACS2 is highly effective at building cognitive maps through latent learning, its performance often stagnates in environments characterized by sparse rewards. We propose a specific architectural variant that triggers hindsight learning when the agent fails to reach its primary goal, re-labeling visited states as virtual goals to densify the learning signal. The proposed model was evaluated on two benchmarks: the deterministic \texttt{Maze 6} and the stochastic \texttt{FrozenLake}. The results demonstrate that ACS2HER significantly accelerates knowledge acquisition and environmental mastery compared to the standard ACS2. However, this efficiency gain is accompanied by increased computational overhead and a substantial expansion in classifier numerosity. This work provides the first analysis of combining anticipatory mechanisms with retrospective goal-relabeling in Learning Classifier Systems.

</details>


### [40] [From Prompt to Protocol: Fast Charging Batteries with Large Language Models](https://arxiv.org/abs/2601.09626)
*Ge Lei,Ferran Brosa Planella,Sterling G. Baird,Samuel J. Cooper*

Main category: cs.LG

TL;DR: 该论文提出两种基于LLM的电池充电协议优化方法：P2O（提示到优化器）和P2P（提示到协议），通过LLM生成协议代码或函数，在实验成本高的场景中实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 电池充电协议优化面临挑战：评估过程缓慢、成本高、不可微分。现有方法过度约束搜索空间，限制了协议多样性，阻碍了高性能解决方案的发现。

Method: 提出两种梯度无关的LLM驱动闭环方法：1) P2O：使用LLM生成小型神经网络协议代码，通过内循环训练；2) P2P：使用LLM直接编写电流函数及其标量参数。

Result: 在案例研究中，LLM引导的P2O优于贝叶斯优化、进化算法和随机搜索设计的神经网络。在快速充电场景中，P2O和P2P相比最先进的多步恒流基线，在状态健康度上实现约4.2%的改进，P2P在相同评估预算下达到此效果。

Conclusion: LLM能够扩展协议函数形式空间，整合基于语言的约束，在实验成本高的设置中实现高效优化。

Abstract: Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which simply writes an explicit function for the current and its scalar parameters. Across our case studies, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search. In a realistic fast charging scenario, both P2O and P2P yield around a 4.2 percent improvement in state of health (capacity retention based health metric under fast charging cycling) over a state-of-the-art multi-step constant current (CC) baseline, with P2P achieving this under matched evaluation budgets (same number of protocol evaluations). These results demonstrate that LLMs can expand the space of protocol functional forms, incorporate language-based constraints, and enable efficient optimization in high cost experimental settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments](https://arxiv.org/abs/2601.09032)
*Logan Ritchie,Sushant Mehta,Nick Heiner,Mason Yu,Edwin Chen*

Main category: cs.AI

TL;DR: 论文通过电商RL环境评估前沿AI模型在150个职场任务上的表现，揭示了智能体能力层级：工具使用、规划与目标形成、适应性、接地性和常识推理。即使最佳模型也有约40%任务失败，失败模式沿能力层级可预测分布。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体发展，AI评估需要从单轮响应评估转向交互环境中的多步骤任务完成评估。研究旨在评估前沿AI模型在真实电商RL环境中的职场任务表现，识别能力差距。

Method: 在Surge提供的真实电商RL环境中，对前沿AI模型进行150个职场任务的实证评估。采用任务中心设计方法，强调多样性和领域专家贡献，进行详细失败分析。

Result: 发现经验推导的智能体能力层级：工具使用、规划与目标形成、适应性、接地性和常识推理。最佳模型仍有约40%任务失败，较弱模型在基础工具使用和规划上挣扎，较强模型主要在需要超出明确指令的上下文推理任务上失败。

Conclusion: 当前前沿模型能展示连贯的多步骤行为，但在真实职场环境中实现人类水平任务完成仍有显著能力差距。失败模式沿能力层级可预测分布，为智能体开发提供指导。

Abstract: The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.

</details>


### [42] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE框架通过分离推理与代码执行来解决多约束规划问题，相比现有LLM方法在性能、成本和延迟方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在多约束规划中存在根本性局限：纯推理方法容易产生不一致性和错误累积，成本高昂；而结合编码或求解器的方法缺乏灵活性，无法捕捉跨问题的通用逻辑。

Method: 提出SCOPE框架，将查询特定推理与通用代码执行解耦，生成一致、确定且可重用的求解器函数，仅需对输入参数进行最小更改。

Result: 在TravelPlanner任务上达到93.1%的成功率，相比最佳基线（CoT）提升61.6%，同时降低推理成本1.4倍，时间减少约4.67倍。

Conclusion: SCOPE通过分离推理与执行，在多约束规划中实现了最先进的性能，同时显著降低了成本和延迟。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [43] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: 该论文对LLMs和MLLMs中的记忆机制进行了全面综述，提出了包含隐式、显式和代理记忆的分类法，并讨论了多模态记忆集成及当前挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs和MLLMs从静态预测器转变为能够持续学习和个性化推理的交互系统，记忆机制已成为其架构和功能演化的核心主题，需要系统性的整理和分析。

Method: 采用文献综述方法，将记忆机制组织成包含隐式记忆、显式记忆和代理记忆的分类法，并扩展到多模态环境中的记忆集成。

Result: 提出了一个结构化的记忆分类框架，涵盖了从模型参数中的隐式知识到外部存储系统和代理持久记忆的各种记忆范式，并识别了关键架构进展和基准任务。

Conclusion: 记忆机制对增强LLMs和MLLMs的推理、适应性和上下文保真度至关重要，但仍面临容量、对齐、事实一致性和跨系统互操作性等开放挑战。

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [44] [PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?](https://arxiv.org/abs/2601.09152)
*Yiwen Tu,Xuan Liu,Lianhui Qin,Haojian Jin*

Main category: cs.AI

TL;DR: PRA是一个AI智能体设计，用于模拟个体用户如何根据现实世界新闻形成隐私担忧。它超越了群体层面的情感分析，整合隐私和认知理论，基于个人评论历史和上下文线索模拟用户特定的隐私推理。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注群体层面的隐私情感分析，缺乏对个体用户隐私担忧形成过程的模拟。需要开发能够理解个人隐私推理模式、考虑个人历史和上下文因素的AI智能体。

Method: PRA整合隐私和认知理论，重建用户的"隐私思维"，通过上下文过滤器动态激活相关隐私记忆（模拟有限理性），生成反映用户对新隐私场景可能反应的合成评论。使用基于既定隐私担忧分类法校准的LLM-as-a-Judge评估器量化生成推理的忠实度。

Result: 在真实世界的Hacker News讨论实验中，PRA在隐私担忧预测方面优于基线智能体，并能捕捉跨领域（包括AI、电子商务和医疗保健）的可迁移推理模式。

Conclusion: PRA提供了一种模拟个体用户隐私担忧形成过程的有效方法，能够生成忠实于用户特定推理模式的合成评论，在跨领域隐私担忧预测中表现出色。

Abstract: This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's "privacy mind", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.

</details>


### [45] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: MAXS是一个基于LLM代理的元自适应推理框架，通过前瞻策略和轨迹收敛机制，平衡全局有效性和计算效率，在多工具推理中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理推理方法存在两个问题：(1) 局部短视生成，缺乏前瞻性；(2) 轨迹不稳定性，早期小错误会导致推理路径发散。这些问题使得难以平衡全局有效性和计算效率。

Method: 提出MAXS框架，采用前瞻策略扩展推理路径几步，评估工具使用的优势值，结合步骤一致性方差和步骤间趋势斜率选择稳定、一致、高价值的推理步骤。引入轨迹收敛机制，在路径一致性达到时停止进一步展开，平衡资源效率和全局有效性。

Result: 在三个基础模型（MiMo-VL-7B、Qwen2.5-VL-7B、Qwen2.5-VL-32B）和五个数据集上的广泛实证研究表明，MAXS在性能和推理效率方面均优于现有方法。进一步分析证实了前瞻策略和工具使用的有效性。

Conclusion: MAXS通过元自适应推理框架有效解决了LLM代理推理中的局部短视和轨迹不稳定问题，实现了全局有效性和计算效率的良好平衡。

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [46] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体政策制定框架，用于协调和主动的跨区域疫情防控，相比现实政策可显著降低感染和死亡人数。


<details>
  <summary>Details</summary>
Motivation: 人类驱动的疫情应对往往是分散和反应式的，政策制定孤立且调整滞后，无法实现主动干预和全球疫情缓解。需要协调的跨区域政策制定方法。

Method: 为每个行政区分配一个LLM智能体作为AI政策制定助手。智能体基于区域特定流行病学动态进行推理，同时与其他智能体通信以考虑跨区域相互依赖性。整合真实世界数据、疫情演化模拟器和结构化智能体间通信。

Result: 使用美国2020年4-12月州级COVID-19数据进行验证。相比现实疫情结果，在单个州层面分别减少累计感染和死亡达63.7%和40.1%；在跨州聚合层面分别减少39.0%和27.0%。

Conclusion: LLM多智能体系统能够通过协调政策制定实现更有效的疫情防控，展示了AI在复杂公共卫生决策中的潜力。

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [47] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER是一个基于路由器的激活空间干预框架，通过强化学习动态组合可复用推理向量，在零样本设置下显著提升LLM推理性能，同时保持高token效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活引导的方法采用静态、手动干预，无法适应复杂推理的动态特性。需要一种能够自适应引导LLM推理的参数高效方法。

Method: 构建可复用推理向量库，使用轻量级路由器动态组合这些向量。通过强化学习在任务级奖励下优化路由器，以涌现和组合方式激活潜在认知原语。

Result: 在七个不同基准测试中，RISER相比基础模型带来3.4-6.5%的平均零样本准确率提升，超越CoT推理且token效率高2-3倍，同时保持稳健的准确率增益。

Conclusion: RISER能够自主组合多个向量形成可解释的精确控制策略，为实现更可控、高效的LLM推理指明了方向。

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [48] [M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning](https://arxiv.org/abs/2601.09278)
*Xiaohan Yu,Chao Feng,Lang Mei,Chong Chen*

Main category: cs.AI

TL;DR: M³Searcher是一个模块化的多模态信息搜索代理，通过解耦信息获取和答案推导来解决多模态自主搜索的挑战，使用检索导向的多目标奖励进行优化，并在新的MMSearchVQA数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有DeepResearch风格代理仅限于文本模态，扩展到多模态环境面临两个关键挑战：1) 大规模多模态工具使用的专业化-泛化权衡；2) 复杂多步骤多模态搜索轨迹训练数据的严重稀缺。

Method: 提出M³Searcher模块化多模态信息搜索代理，明确解耦信息获取和答案推导。使用检索导向的多目标奖励进行优化，联合鼓励事实准确性、推理合理性和检索保真度。开发了MMSearchVQA多模态多跳数据集来支持检索中心的强化学习训练。

Result: 实验结果表明M³Searcher优于现有方法，在复杂多模态任务中表现出强大的迁移适应性和有效推理能力。

Conclusion: M³Searcher通过模块化设计和检索导向的强化学习，成功解决了多模态自主信息搜索的关键挑战，为多模态信息获取代理提供了有效的解决方案。

Abstract: Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.

</details>


### [49] [Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments](https://arxiv.org/abs/2601.09382)
*Qinglong Shi,Donghai Wang,Hantao Zhou,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.AI

TL;DR: 提出主动式任务导向智能体新范式，通过意图条件监控和事件触发跟进，在动态环境中实现长期用户意图维护，并创建ChronosBench基准验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体主要采用反应式范式，只能响应短期会话中的即时用户查询，无法维护长期用户意图和适应动态变化的外部环境，需要新的交互范式来解决这一问题。

Method: 提出主动式任务导向智能体范式，包含两个核心能力：意图条件监控（基于对话历史自主制定触发条件）和事件触发跟进（检测到有用环境更新时主动与用户互动）。建立高质量数据合成管道构建动态环境中的复杂多轮对话数据，并创建ChronosBench基准进行评估。

Result: 使用合成数据进行监督学习的微调模型在包含用户意图转变的复杂任务中达到85.19%的任务完成率，优于其他测试模型。评估当前领先的闭源和开源模型，揭示了它们在长期任务导向交互中的缺陷。

Conclusion: 提出的主动式智能体范式能有效弥合相对静态的用户需求与动态环境之间的差距，数据驱动策略被验证有效，为长期任务导向交互提供了新解决方案。

Abstract: Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.

</details>


### [50] [EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines](https://arxiv.org/abs/2601.09465)
*Shuo Zhang,Chaofa Yuan,Ryan Guo,Xiaomin Yu,Rui Xu,Zhangquan Chen,Zinuo Li,Zhi Yang,Shuhao Guan,Zhenheng Tang,Sen Hu,Liwen Zhang,Ronghao Chen,Huacan Wang*

Main category: cs.AI

TL;DR: EvoFSM提出了一种结构化自进化框架，通过演化有限状态机而非自由形式重写，在保持控制的同时实现适应性，在五个多跳QA基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体大多依赖固定工作流，难以适应现实世界的开放式查询。虽然最近研究探索通过重写代码或提示实现自进化，但无约束优化常导致不稳定性、幻觉和指令漂移。

Method: 提出EvoFSM框架，将优化空间解耦为宏观的Flow（状态转移逻辑）和微观的Skill（状态特定行为），通过受限操作集精炼FSM，并引入自进化记忆系统，将成功轨迹提炼为可重用先验，失败模式作为未来查询的约束。

Result: 在五个多跳QA基准测试中验证了有效性，特别是在DeepSearch基准上达到58.0%准确率。在交互式决策任务上的额外结果进一步验证了其泛化能力。

Conclusion: EvoFSM通过结构化自进化方法，在保持控制的同时实现了适应性，为LLM智能体在开放式查询中的有效自进化提供了可行方案。

Abstract: While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.

</details>


### [51] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 提出T2Q评估范式，通过任务转问答的方式解耦任务执行与环境理解，发现任务成功不能有效反映环境理解能力，现有记忆机制存在局限


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估主要依赖轨迹成功率指标，但缺乏对环境理解能力的评估，无法判断智能体是否真正掌握了可迁移的环境模型

Method: 提出Task-to-Quiz(T2Q)评估范式，将任务执行与环境理解解耦，构建T2QBench基准，包含30个环境和1,967个基于环境的QA对

Result: 实验表明任务成功率与环境理解能力相关性弱，现有记忆机制无法有效帮助智能体获取环境模型，主动探索和细粒度状态表示是主要瓶颈

Conclusion: 需要超越任务成功率的评估方法，T2Q为开发更通用的自主智能体提供了坚实基础，揭示了智能体泛化能力的关键瓶颈

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [52] [LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach](https://arxiv.org/abs/2601.09635)
*Kuo Liang,Yuhang Lu,Jianming Mao,Shuyi Sun,Chunwei Yang,Congcong Zeng,Xiao Jin,Hanzhang Qin,Ruihao Zhu,Chung-Piaw Teo*

Main category: cs.AI

TL;DR: LEAN-LLM-OPT是一个轻量级智能工作流框架，利用LLM代理自动构建大规模优化模型，通过分解建模任务和自动化数据处理，显著减少人工建模工作量。


<details>
  <summary>Details</summary>
Motivation: 大规模优化是现代商业决策的关键，但传统建模过程劳动密集且耗时。需要自动化工具来加速优化模型的构建过程。

Method: 提出LEAN-LLM-OPT框架：两个上游LLM代理动态构建工作流，指定类似问题的建模步骤；下游LLM代理遵循工作流生成最终输出。利用LLM文本处理能力和建模实践，将任务分解为结构化子任务，将机械数据处理卸载到辅助工具。

Result: 使用GPT-4.1和开源gpt-oss-20B的LEAN-LLM-OPT在大规模优化建模任务上表现强劲，与最先进方法竞争。在新加坡航空收益管理用例中，在各种场景下都取得领先性能。同时创建了首个大规模优化自动建模基准Large-Scale-OR和Air-NRM。

Conclusion: LEAN-LLM-OPT通过智能工作流和LLM代理的协同，有效自动化大规模优化建模过程，在基准测试和实际应用中均表现出色，具有实用价值。

Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.

</details>


### [53] [PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records](https://arxiv.org/abs/2601.09636)
*Yibo Lyu,Gongwei Chen,Rui Shao,Weili Guan,Liqiang Nie*

Main category: cs.AI

TL;DR: 论文提出PersonalAlign任务，要求GUI代理利用长期用户记录解决模糊指令中的省略偏好，并根据用户状态预测潜在习惯以提供主动协助。为此构建了AndroidIntent基准，并设计了分层意图记忆代理HIM-Agent，在基准测试中显著提升了执行和主动性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在显式指令下表现良好，但实际部署需要与用户更复杂的隐式意图对齐。用户指令往往模糊且省略偏好，需要代理利用长期用户记录作为持久上下文来解析这些意图，并预测用户的潜在习惯以提供主动协助。

Method: 1) 提出PersonalAlign任务，要求代理利用长期用户记录解决模糊指令中的省略偏好并预测用户习惯；2) 构建AndroidIntent基准，包含20k长期记录，标注了775个用户特定偏好和215个习惯；3) 设计HIM-Agent，维护持续更新的个人记忆，分层组织用户偏好和习惯以实现个性化。

Result: 在AndroidIntent基准上评估了多种GUI代理（GPT-5、Qwen3-VL、UI-TARS等）。HIM-Agent显著提升了执行性能15.7%和主动性能7.3%，证明了其有效性。

Conclusion: PersonalAlign任务和AndroidIntent基准为GUI代理的隐式意图对齐研究提供了重要基础。HIM-Agent通过分层记忆机制有效解决了模糊指令解析和主动协助问题，为个性化GUI代理的发展提供了新方向。

Abstract: While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.

</details>


### [54] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: MATTRL提出了一种多智能体测试时强化学习框架，通过将结构化文本经验注入多智能体推理过程，无需训练即可提升分布偏移下的多智能体推理能力。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在实际应用中展现出优势，但多智能体强化学习训练资源密集且不稳定，存在非平稳性和稀疏高方差奖励问题。

Method: 构建多专家团队进行多轮讨论，检索并整合测试时经验，通过信用分配构建轮级经验池并重新注入对话，最终达成共识决策。

Result: 在医学、数学和教育等挑战性基准测试中，MATTRL相比多智能体基线平均提升3.67%准确率，相比单智能体基线提升8.67%。

Conclusion: MATTRL提供了一种稳定、有效且高效的路径，无需调优即可实现分布偏移鲁棒的多智能体推理。

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>
