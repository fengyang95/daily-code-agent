<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 10]
- [wechat.article](#wechat.article) [Total: 21]
- [tldr.article](#tldr.article) [Total: 16]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Beyond Majority Voting: Towards Fine-grained and More Reliable Reward Signal for Test-Time Reinforcement Learning](https://arxiv.org/abs/2512.15146)
*Weiqin Wang,Yile Wang,Kehao Chen,Hui Huang*

Main category: cs.CL

TL;DR: SCOPE框架通过置信度加权伪标签估计和动态子群划分，解决了测试时强化学习中多数投票策略的确认偏差和稀疏奖励问题，显著提升了LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 测试时强化学习使用多数投票结果作为伪标签来减少对标注数据的依赖，但现有投票策略存在确认偏差和稀疏奖励问题，限制了性能提升。

Method: 提出SCOPE框架：1）引入逐步置信度加权伪标签推导，优先考虑高质量推理路径而非简单频率计数；2）动态划分候选输出池为独立子群，平衡推理质量与探索多样性；3）通过重复采样为每个子群获取局部共识，提供多样化监督目标。

Result: 在多个模型和基准测试中，SCOPE始终优于现有基线方法，在AIME 2025上相对提升13.1%，在AMC上相对提升8.1%。

Conclusion: SCOPE通过置信度加权和动态子群划分有效解决了测试时强化学习中的确认偏差和稀疏奖励问题，为提升LLM推理能力提供了有效框架。

Abstract: Test-time reinforcement learning mitigates the reliance on annotated data by using majority voting results as pseudo-labels, emerging as a complementary direction to reinforcement learning with verifiable rewards (RLVR) for improving reasoning ability of large language models (LLMs). However, this voting strategy often induces confirmation bias and suffers from sparse rewards, limiting the overall performance. In this work, we propose subgroup-specific step-wise confidence-weighted pseudo-label estimation (SCOPE), a framework integrating model confidence and dynamic subgroup partitioning to address these issues. Specifically, SCOPE integrates the proposed step-wise confidence into pseudo label deduction, prioritizing high-quality reasoning paths over simple frequency count. Furthermore, it dynamically partitions the candidate outputs pool into independent subgroups by balancing reasoning quality against exploration diversity. By deriving local consensus via repeat sampling for each sub group, SCOPE provides diverse supervision targets to encourage broader exploration. We conduct experiments across various models and benchmarks, experimental results show that SCOPE consistently outperforms recent baselines. Notably, SCOPE achieving relative improvements of 13.1\% on challenging AIME 2025 and 8.1\% on AMC. The code is released at \href{https://github.com/szu-tera/SCOPE}{https://github.com/szu-tera/SCOPE}.

</details>


### [2] [MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers](https://arxiv.org/abs/2512.15163)
*Xuanjun Zong,Zhiqi Shen,Lei Wang,Yunshi Lan,Chao Yang*

Main category: cs.CL

TL;DR: MCP-SafetyBench：首个基于真实MCP服务器的综合性安全基准，评估LLM在多服务器工作流中的安全风险，涵盖5个领域20种攻击类型


<details>
  <summary>Details</summary>
Motivation: MCP作为连接LLM与异构工具的关键协议，其开放性和多服务器工作流引入了新的安全风险，现有基准无法捕捉这些现实威胁

Method: 构建基于真实MCP服务器的基准，支持多轮评估，涵盖浏览器自动化、金融分析、位置导航、仓库管理和网络搜索5个领域，包含20种攻击类型的统一分类法

Result: 系统评估领先的开源和闭源LLM，发现安全性能存在巨大差异，随着任务范围和服务器交互增加，漏洞呈升级趋势

Conclusion: MCP-SafetyBench为诊断和缓解真实MCP部署中的安全风险奠定了基础，突显了加强防御的紧迫需求

Abstract: Large language models (LLMs) are evolving into agentic systems that reason, plan, and operate external tools. The Model Context Protocol (MCP) is a key enabler of this transition, offering a standardized interface for connecting LLMs with heterogeneous tools and services. Yet MCP's openness and multi-server workflows introduce new safety risks that existing benchmarks fail to capture, as they focus on isolated attacks or lack real-world coverage. We present MCP-SafetyBench, a comprehensive benchmark built on real MCP servers that supports realistic multi-turn evaluation across five domains: browser automation, financial analysis, location navigation, repository management, and web search. It incorporates a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and includes tasks requiring multi-step reasoning and cross-server coordination under uncertainty. Using MCP-SafetyBench, we systematically evaluate leading open- and closed-source LLMs, revealing large disparities in safety performance and escalating vulnerabilities as task horizons and server interactions grow. Our results highlight the urgent need for stronger defenses and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.

</details>


### [3] [Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning](https://arxiv.org/abs/2512.15274)
*Yiliu Sun,Zicheng Zhao,Yang Wei,Yanfang Zhang,Chen Gong*

Main category: cs.CL

TL;DR: PPPO是一种新的RLVR方法，专注于优化LLM推理输出的前缀token，通过渐进式前缀保留和延续累积奖励策略，显著提升训练效率和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法对所有生成token进行统一训练，忽视了不同token对推理贡献的差异。特别是前缀token对后续推理过程有重要影响，但现有方法未能针对性优化，导致训练效率低下。

Method: 提出PPPO方法，基于"路径依赖"理论和"起始锁定效应"，专注于优化LLM推理的前缀过程。采用两种训练策略：1) 渐进式前缀保留：逐步增加训练中保留的前缀token比例；2) 延续累积奖励：对同一前缀序列采样多个延续，累积其得分作为奖励信号。

Result: 在多种推理任务上的实验表明，PPPO显著优于现有RLVR方法，仅使用26.17%的训练token就实现了18.02%的准确率提升。

Conclusion: PPPO通过针对性优化推理过程的前缀token，有效提升了LLM的推理能力和训练效率，验证了"起始锁定效应"在LLM推理中的重要性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.

</details>


### [4] [Towards Proactive Personalization through Profile Customization for Individual Users in Dialogues](https://arxiv.org/abs/2512.15302)
*Xiaotian Zhang,Yuan Wang,Ruizhe Chen,Zeya Wang,Runchen Hou,Zuozhu Liu*

Main category: cs.CL

TL;DR: PersonalAgent是一个面向用户终身学习的对话代理，通过分解对话为单轮交互并构建动态用户画像，持续推断和适应用户偏好，解决了长期个性化需求和冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对齐技术主要关注通用人类价值观或静态单轮偏好，无法满足长期个性化需求和用户冷启动问题，需要开发能够持续适应用户偏好的系统。

Method: 将对话分解为单轮交互，构建统一用户画像并动态优化，将偏好推断建模为序列决策任务，实现终身学习和个性化适应。

Result: 在理想和嘈杂对话环境中均优于强提示基线和策略优化基线，保持跨会话偏好一致性，人工评估显示能自然连贯地捕捉用户偏好。

Conclusion: 终身个性化对于开发更具包容性和适应性的对话代理至关重要，PersonalAgent展示了持续适应用户偏好的有效性。

Abstract: The deployment of Large Language Models (LLMs) in interactive systems necessitates a deep alignment with the nuanced and dynamic preferences of individual users. Current alignment techniques predominantly address universal human values or static, single-turn preferences, thereby failing to address the critical needs of long-term personalization and the initial user cold-start problem. To bridge this gap, we propose PersonalAgent, a novel user-centric lifelong agent designed to continuously infer and adapt to user preferences. PersonalAgent constructs and dynamically refines a unified user profile by decomposing dialogues into single-turn interactions, framing preference inference as a sequential decision-making task. Experiments show that PersonalAgent achieves superior performance over strong prompt-based and policy optimization baselines, not only in idealized but also in noisy conversational contexts, while preserving cross-session preference consistency. Furthermore, human evaluation confirms that PersonalAgent excels at capturing user preferences naturally and coherently. Our findings underscore the importance of lifelong personalization for developing more inclusive and adaptive conversational agents. Our code is available here.

</details>


### [5] [Evaluating Metrics for Safety with LLM-as-Judges](https://arxiv.org/abs/2512.15617)
*Kester Clegg,Richard Hawkins,Ibrahim Habli,Tom Lawton*

Main category: cs.CL

TL;DR: 该论文探讨如何在安全关键的信息处理流程中安全可靠地引入LLM，提出通过加权指标组合、上下文敏感的错误严重性定义以及置信度阈值来降低LLM-as-Judge评估中的风险。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于文本处理流程，可能替代人力瓶颈，但LLM会犯错且某些处理角色是安全关键的。如何在之前由人类执行的关键信息流中安全引入LLM？

Method: 主张安全论证应聚焦于LLM流程中评估点提供的证据类型，特别是在使用LLM-as-Judge评估器的框架中。采用加权指标组合来降低评估错误风险，使用上下文敏感性定义错误严重性，设计置信度阈值在评估者一致性低时触发人工审查。

Result: 论文提出了一种方法论框架，通过多指标评估、错误严重性分级和置信度触发机制，可以在无法获得确定性评估的自然语言处理任务中提高LLM在安全关键应用中的可靠性。

Conclusion: 虽然无法从许多自然语言处理任务中获得确定性评估，但通过采用加权指标组合、上下文敏感的错误严重性定义和置信度阈值，可以降低LLM-as-Judge评估中的风险，为安全关键应用中引入LLM提供可行路径。

Abstract: LLMs (Large Language Models) are increasingly used in text processing pipelines to intelligently respond to a variety of inputs and generation tasks. This raises the possibility of replacing human roles that bottleneck existing information flows, either due to insufficient staff or process complexity. However, LLMs make mistakes and some processing roles are safety critical. For example, triaging post-operative care to patients based on hospital referral letters, or updating site access schedules in nuclear facilities for work crews. If we want to introduce LLMs into critical information flows that were previously performed by humans, how can we make them safe and reliable? Rather than make performative claims about augmented generation frameworks or graph-based techniques, this paper argues that the safety argument should focus on the type of evidence we get from evaluation points in LLM processes, particularly in frameworks that employ LLM-as-Judges (LaJ) evaluators. This paper argues that although we cannot get deterministic evaluations from many natural language processing tasks, by adopting a basket of weighted metrics it may be possible to lower the risk of errors within an evaluation, use context sensitivity to define error severity and design confidence thresholds that trigger human review of critical LaJ judgments when concordance across evaluators is low.

</details>


### [6] [Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers](https://arxiv.org/abs/2512.15674)
*Adam Karvonen,James Chua,Clément Dumas,Kit Fraser-Taliente,Subhash Kantamneni,Julian Minder,Euan Ong,Arnab Sen Sharma,Daniel Wen,Owain Evans,Samuel Marks*

Main category: cs.CL

TL;DR: 本文提出了一种通用的激活预言机方法，通过多样化训练让LLM能够直接读取其他LLM的激活状态并回答自然语言问题，在多个下游任务中超越了传统白盒方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM激活理解方法通常复杂且专用，而LatentQA方法虽然简单但局限于狭窄任务。本文旨在探索通用化的激活理解能力，研究训练数据多样性如何影响性能。

Method: 采用LatentQA方法训练激活预言机，让LLM能够接受其他LLM的激活状态作为输入并回答自然语言问题。通过多样化训练数据集（包括分类任务和自监督上下文预测任务）提升模型泛化能力。

Result: 激活预言机能够恢复微调模型中未出现在输入文本的信息（如传记知识或恶意倾向）。在四个下游任务评估中，最佳AO模型在所有任务上匹配或超越了传统白盒基线，在3/4任务中表现最佳。

Conclusion: 多样化训练使LLM获得了通用的激活状态语言化能力，能够有效理解并描述其他LLM的内部表示，为模型解释提供了更简单有效的途径。

Abstract: Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge](https://arxiv.org/abs/2512.14766)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Jiaoyan Chen,Steffen Staab,Yuan He,Evgeny Kharlamov*

Main category: cs.AI

TL;DR: 论文提出了一种在知识图谱不完整情况下构建基准测试的方法论，并开发了自适应图推理智能体（GR-Agent）来解决不完整知识图谱上的问答问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答基准测试大多假设知识图谱是完整的，这忽略了现实世界中知识图谱通常不完整的事实。当直接支持三元组缺失时，需要从现有事实中推理答案，而现有方法在这种情况下的推理能力有限。

Method: 1) 提出构建不完整知识图谱基准测试的方法论，移除直接支持三元组但保留替代推理路径；2) 开发自适应图推理智能体（GR-Agent），将知识图谱构建为交互环境，将问答形式化为智能体与环境交互，使用图推理工具作为动作空间，并维护潜在支持推理证据的记忆。

Result: 实验表明，现有方法在不完整知识图谱下性能显著下降，而GR-Agent在完整和不完整设置下均优于非训练基线，与基于训练的方法性能相当。

Conclusion: 论文强调了评估知识图谱问答系统在不完整知识图谱下的重要性，提出的GR-Agent展示了通过智能体框架增强推理能力的有效性。

Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.

</details>


### [8] [IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection](https://arxiv.org/abs/2512.14792)
*Roman Nekrasov,Stefano Fossati,Indika Kumara,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.AI

TL;DR: 该研究通过结构化配置知识注入技术，显著提升了LLM生成基础设施即代码（IaC）的正确率，从基线27.1%提升至62.6%，但发现LLM在意图对齐方面存在瓶颈，形成了"正确性-一致性差距"。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成正确且意图对齐的基础设施即代码（特别是Terraform）方面成功率较低，需要系统性的方法来提升LLM的IaC生成能力。

Method: 1) 增强IaC-Eval基准测试，加入云仿真和自动化错误分析；2) 开发LLM辅助IaC代码生成的错误分类法；3) 实现从朴素检索增强生成到更复杂的图RAG方法的知识注入技术，包括图组件的语义丰富化和资源间依赖关系建模。

Result: 基线LLM性能较差（总体成功率27.1%），注入结构化配置知识后，技术验证成功率提升至75.3%，总体成功率提升至62.6%。但意图对齐方面出现平台期，揭示了"正确性-一致性差距"。

Conclusion: 结构化知识注入能显著提升LLM生成IaC的技术正确性，但LLM在理解复杂用户意图方面仍有局限，可能成为熟练的"编码者"但仍是有限的"架构师"。

Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.

</details>


### [9] [AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally](https://arxiv.org/abs/2512.14910)
*Nadine Angela Cantonjos,Arpita Biswas*

Main category: cs.AI

TL;DR: AgroAskAI是一个用于农业气候适应决策支持的多智能体推理系统，采用模块化角色专业化架构，通过责任链方法协调自主智能体，集成实时工具和数据集，支持多语言交互，为脆弱农村社区提供可操作、接地气且包容性的气候适应策略。


<details>
  <summary>Details</summary>
Motivation: 农村农业地区面临干旱、强降雨和天气模式变化等气候相关风险的损害。现有研究呼吁适应性风险管理解决方案和决策策略。虽然过去系统依赖单智能体模型或仅将多智能体框架用于静态功能，但需要支持动态协作推理和上下文感知输出的架构。

Method: 提出AgroAskAI多智能体推理系统，采用模块化、角色专业化的架构，使用责任链方法协调自主智能体，集成实时工具和数据集。系统内置治理机制减轻幻觉，支持内部反馈以产生连贯、本地相关的策略，并支持多语言交互。

Result: 在常见农业气候适应查询上的实验表明，通过额外工具和提示优化，AgroAskAI能够提供更可操作、接地气和包容性的输出。实验结果突显了智能体AI在农业气候适应中可持续和负责任决策支持的潜力。

Conclusion: AgroAskAI展示了智能体AI在农业气候适应决策支持中的潜力，为脆弱农村社区提供动态协作推理和上下文感知的解决方案，支持可持续和负责任的农业实践。

Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.

</details>


### [10] [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)
*Jinwu Hu,Dongjin Yang,Langyu Bian,Zhiquan Wen,Yufeng Wang,Yaofo Chen,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.AI

TL;DR: CogER是一个受人类分层推理启发的框架，通过强化学习训练代理动态选择最适合每个查询的推理策略，平衡推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理策略主要依赖模型自身的快慢模式，难以在不同难度查询间平衡推理效率和准确性。需要一种能根据查询复杂度动态选择策略的方法。

Method: CogER首先评估查询复杂度并分配到预定义级别，每个级别对应定制处理策略。将策略选择建模为马尔可夫决策过程，用强化学习训练CogER-Agent，奖励函数平衡解质量和计算成本。对于需要外部工具的查询，引入认知工具辅助推理，让LLM在思维链中自主调用外部工具。

Result: CogER在领域内任务上相对现有测试时缩放方法至少提升13%的平均精确匹配率，在领域外任务上获得8%的相对增益。

Conclusion: CogER通过动态策略选择有效平衡了推理效率和准确性，在多个任务上显著优于现有方法，展示了分层推理框架的有效性。

Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.

</details>


### [11] [Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis](https://arxiv.org/abs/2512.15295)
*Toshihide Ubukata,Enhong Mu,Takuto Yamauchi,Mingyue Zhang,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: 本文提出GCRL方法，通过图神经网络增强强化学习在控制器合成中的探索策略，相比现有方法在多数基准领域表现出更优的学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 控制器合成中探索策略的效率至关重要，现有方法通常依赖固定规则或仅考虑有限当前特征的强化学习策略，无法充分利用历史探索信息。

Method: 提出GCRL方法，将LTS探索历史编码为图结构，使用图神经网络捕捉更广泛的非当前上下文信息，增强强化学习探索策略。

Result: 在五个基准领域的比较实验中，GCRL在四个领域表现出优于现有方法的学习效率和泛化能力，但在具有高对称性和严格局部交互特性的特定领域表现不佳。

Conclusion: GCRL通过图神经网络整合历史探索信息，有效提升了控制器合成中探索策略的性能，但在特定对称性强的领域仍有改进空间。

Abstract: Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.

</details>


### [12] [SCOPE: Prompt Evolution for Enhancing Agent Effectiveness](https://arxiv.org/abs/2512.15374)
*Zehua Pei,Hui-Ling Zhen,Shixiong Kai,Sinno Jialin Pan,Yunhe Wang,Mingxuan Yuan,Bei Yu*

Main category: cs.AI

TL;DR: SCOPE通过自动演化提示来优化LLM代理的上下文管理，将上下文管理视为在线优化问题，使用双流机制平衡战术特定性和战略通用性，在HLE基准上将任务成功率从14.23%提升至38.64%。


<details>
  <summary>Details</summary>
Motivation: LLM代理在动态环境中面临上下文管理瓶颈，静态提示缺乏有效管理机制，导致频繁的纠正和增强失败。需要解决代理能力差距，使其能有效处理大规模动态上下文。

Method: 提出SCOPE框架，将上下文管理视为在线优化问题，从执行轨迹中合成指导原则来自动演化代理提示。采用双流机制平衡战术特定性（解决即时错误）和战略通用性（演化长期原则），并引入视角驱动探索最大化策略覆盖。

Result: 在HLE基准测试中，SCOPE将任务成功率从14.23%显著提升至38.64%，且无需人工干预。代码已公开在GitHub上。

Conclusion: SCOPE通过自动演化提示有效解决了LLM代理的上下文管理问题，显著提升了在动态环境中的任务成功率，为代理的自我优化提供了新思路。

Abstract: Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\% to 38.64\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.

</details>


### [13] [A Decision-Theoretic Approach for Managing Misalignment](https://arxiv.org/abs/2512.15584)
*Daniel A. Herrmann,Abinav Chari,Isabelle Qian,Sree Sharvesh,B. A. Levinstein*

Main category: cs.AI

TL;DR: 论文提出了一个决策理论框架，用于在不确定性下确定何时将决策委托给AI系统，强调需要平衡价值对齐、认知准确性和行动范围，并区分了通用委托和上下文特定委托的不同要求。


<details>
  <summary>Details</summary>
Motivation: 当前价值对齐文献主要关注如何塑造AI的价值观，但较少研究在不确定性下如何确定不完美的对齐何时足够好以证明委托的合理性。需要建立一个原则性的方法来决定何时AI在特定上下文中足够对齐，从而将重点从实现完美对齐转向管理委托的风险和回报。

Method: 引入了一个正式的决策理论框架，精确分析价值对齐、认知准确性和行动范围之间的权衡。开发了一个新颖的评分框架来量化事前决策，区分通用委托和上下文特定委托两种场景。

Result: 分析揭示了两种委托场景的明显区别：通用委托需要近乎完美的价值对齐和完全的认知信任（实践中很少满足），而上下文特定委托即使在显著不对齐的情况下也可能是最优的。AI的优越准确性或扩展的行动范围可能提供更好的整体决策问题，使得委托在期望上是合理的。

Conclusion: 该工作提供了一个原则性的方法来确定AI在给定上下文中是否足够对齐，将重点从实现完美对齐转向管理不确定性下的委托风险和回报。上下文特定的委托可以在存在显著不对齐的情况下仍然是最优的。

Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [FrontierCS: Evolving Challenges for Evolving Intelligence](https://arxiv.org/abs/2512.15699)
*Qiuyang Mang,Wenhao Chai,Zhifei Li,Huanzhi Mao,Shang Zhou,Alexander Du,Hanchen Li,Shu Liu,Edwin Chen,Yichuan Wang,Xieting Chu,Zerui Cheng,Yuan Xu,Tian Xia,Zirui Wang,Tianneng Shi,Jianzhu Yao,Yilong Zhao,Qizheng Zhang,Charlie Ruan,Zeyu Shen,Kaiyuan Liu,Runyuan He,Dong Xing,Zerui Li,Zirong Zeng,Yige Jiang,Lufeng Cheng,Ziyi Zhao,Youran Sun,Wesley Zheng,Meiyuwang Zhang,Ruyi Ji,Xuechang Tu,Zihan Zheng,Zexing Chen,Kangyang Zhou,Zhaozi Wang,Jingbang Chen,Aleksandra Korolova,Peter Henderson,Pramod Viswanath,Vijay Ganesh,Saining Xie,Zhuang Liu,Dawn Song,Sewon Min,Ion Stoica,Joseph E. Gonzalez,Jingbo Shang,Alvin Cheung*

Main category: cs.LG

TL;DR: FrontierCS是一个包含156个开放式计算机科学问题的基准测试，涵盖算法和研究问题，特点是问题的最优解未知但可客观评估解决方案质量，模型通过实现可执行程序而非直接输出答案来解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注已知最优解的任务，缺乏针对最优解未知但可客观评估解决方案质量的开放式问题的评估框架。需要创建一个能够衡量模型在计算机科学前沿问题解决能力的基准。

Method: 创建包含156个开放式问题的基准，涵盖算法问题（通常是NP难问题的变体）和研究问题。每个问题提供专家参考解决方案和自动评估器。问题由CS博士、顶级竞赛编程参与者和出题者设计和评审。

Result: 前沿推理模型在算法和研究赛道上仍远落后于人类专家；仅增加推理预算无法缩小这一差距；模型往往过度优化生成仅能工作的代码，而非发现高质量算法和系统设计。

Conclusion: FrontierCS为评估模型在计算机科学前沿问题上的解决能力提供了基准，揭示了当前模型在开放式问题解决方面的局限性，表明需要超越代码生成的新方法来实现真正的算法创新。

Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.

</details>


### [15] [Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections](https://arxiv.org/abs/2512.14895)
*Niklas Lauffer,Xiang Deng,Srivatsa Kundurthy,Brad Kenstler,Jeff Da*

Main category: cs.LG

TL;DR: 提出OEC方法解决多轮LLM代理训练中的协变量偏移问题，通过在学生模型轨迹中切换专家模型生成部分在线数据，在软件工程任务中相比传统模仿学习提升14%性能


<details>
  <summary>Details</summary>
Motivation: 传统基于模仿学习的LM代理训练在多轮交互中存在协变量偏移问题：学生策略偏离专家行为时，会遇到训练数据中未出现的状态，降低微调效果

Method: 提出在线专家修正(OEC)数据生成方法：从学生模型开始生成轨迹，中途切换到专家模型完成剩余部分，生成部分在线数据。在软件工程任务中，结合拒绝采样和监督微调技术进行训练

Result: 在SWE-bench验证集上，OEC轨迹相比传统模仿学习在7b和32b模型上分别实现14%和13%的相对改进，证明结合专家演示和在线数据对多轮LM代理训练的重要性

Conclusion: 多轮LM代理训练需要结合专家演示和在线数据，OEC方法有效解决了协变量偏移问题，在软件工程任务中显著提升了代理性能

Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.

</details>


### [16] [DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding](https://arxiv.org/abs/2512.15000)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-Code提出了一种针对代码的过程奖励模型，通过将函数作为推理步骤并使用链式函数提示策略来诱导模块化代码生成，同时引入元学习校正机制来减少标签噪声，在LiveCodeBench上实现了80.9%的pass@1率。


<details>
  <summary>Details</summary>
Motivation: 当前过程奖励模型在代码任务中效果有限，主要因为代码缺乏有意义的步骤分解，以及蒙特卡洛生成的中间标签存在噪声问题。

Method: 1. 将函数视为推理步骤，采用链式函数提示策略诱导模块化代码生成；2. 引入基于元学习的校正机制，利用干净的最终解决方案单元测试标签，通过双层优化来精炼中间标签。

Result: 在LiveCodeBench上实现了80.9%的pass@1率，超越了OpenAI o4-mini，达到了最先进的性能水平。

Conclusion: DreamPRM-Code通过创新的函数级步骤分解和标签校正机制，显著提升了过程奖励模型在代码任务中的效果，为代码生成的质量评估提供了新方法。

Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.

</details>


### [17] [Spectral Representation-based Reinforcement Learning](https://arxiv.org/abs/2512.15036)
*Chenxiao Gao,Haotian Sun,Na Li,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TL;DR: 该论文提出了一种基于谱表示的新视角来解决强化学习中函数逼近的挑战，通过谱分解转移算子来抽象系统动态，并在DeepMind Control Suite的20多个任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在大型状态和动作空间的实际应用中，强化学习通常使用函数逼近来表示策略、价值函数和动态模型等核心组件。虽然神经网络等强大逼近器具有很好的表达能力，但存在理论模糊性、优化不稳定、探索困难以及计算成本高等问题。

Method: 引入谱表示视角，基于转移算子的谱分解构建框架。展示了如何为具有隐变量结构或基于能量结构的转移算子构建谱表示，并提出了从数据中提取谱表示的不同学习方法。还将该谱视角扩展到部分可观测MDPs。

Result: 在DeepMind Control Suite的20多个挑战性任务上验证了算法，性能达到或超过了当前最先进的模型无关和基于模型的基线方法。

Conclusion: 谱表示框架为解决强化学习中函数逼近的困难提供了有效的解决方案，既能有效抽象系统动态用于策略优化，又能提供清晰的理论特征。

Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.

</details>


### [18] [The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems](https://arxiv.org/abs/2512.15068)
*Debu Sinha*

Main category: cs.LG

TL;DR: 该论文揭示了基于嵌入的RAG幻觉检测方法存在严重缺陷，提出了"语义幻觉"概念，并证明GPT-4作为LLM法官能显著降低误报率。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG系统基于检索证据，但仍容易产生幻觉。当前基于语义相似度和自然语言推理的检测方法存在根本性局限，但尚未被严格表征。

Method: 应用符合性预测进行幻觉检测，提供有限样本覆盖保证。在多个真实幻觉基准上测试了基于嵌入的方法（包括最先进的OpenAI text-embedding-3-large和交叉编码器模型）和GPT-4作为LLM法官。

Result: 在合成幻觉上达到94%覆盖率和0%误报率，但在三个真实幻觉基准上，基于嵌入的方法误报率极高（HaluEval 100%、RAGTruth 88%、WikiBio 50%）。GPT-4作为LLM法官仅7%误报率，证明任务可通过推理解决。

Conclusion: 提出了"语义幻觉"概念：语义上合理的幻觉保持与源文档的相似性，同时引入嵌入无法检测的事实错误。基于嵌入的检测方法在多种架构和任务类型中持续存在局限，不适合生产级RAG部署。

Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.

</details>


### [19] [DEER: Draft with Diffusion, Verify with Autoregressive Models](https://arxiv.org/abs/2512.15176)
*Zicong Cheng,Guo-Wei Yang,Jia Li,Zhijie Deng,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.LG

TL;DR: DEER提出使用扩散大语言模型作为草稿器，通过并行解码生成长草稿段，显著提升推测解码效率，在HumanEval上达到5.54倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法依赖自回归草稿模型，存在两个根本问题：1）逐步不确定性累积导致目标模型与草稿器之间信任逐渐崩溃；2）自回归草稿器的固有顺序解码。这些问题限制了加速效果。

Method: 提出DEER框架，使用扩散大语言模型作为草稿器，采用两阶段训练流程对齐扩散草稿器与目标自回归模型，并采用单步解码生成长草稿段。

Result: DEER达到32个token的草稿接受长度，远超EAGLE-3的10个token。在HumanEval上使用Qwen3-30B-A3B达到5.54倍加速，而EAGLE-3仅2.41倍。

Conclusion: 扩散大语言模型草稿器能自然克服自回归草稿器的根本问题，通过并行解码策略实现更高效的推测解码，显著提升LLM驱动系统的推理效率。

Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/

</details>


### [20] [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](https://arxiv.org/abs/2512.15405)
*Jianfei Ma,Wee Sun Lee*

Main category: cs.LG

TL;DR: 提出EUBRL算法，利用认知不确定性指导探索，在无限时域折扣MDP中实现接近极小极大最优的遗憾和样本复杂度


<details>
  <summary>Details</summary>
Motivation: 智能体在已知与未知边界面临探索-利用困境，认知不确定性反映了知识有限导致的系统性不确定性。需要一种能够利用认知指导实现原则性探索的强化学习算法。

Method: 提出贝叶斯强化学习算法EUBRL，利用认知不确定性指导探索，自适应减少由估计误差引起的每步遗憾。算法适用于具有充分表达能力先验的无限时域折扣MDP。

Result: 理论分析表明EUBRL在无限时域折扣MDP中实现了接近极小极大最优的遗憾和样本复杂度保证。实验在稀疏奖励、长时域和随机性任务中验证了算法的优越样本效率、可扩展性和一致性。

Conclusion: EUBRL通过认知不确定性指导实现了原则性探索，在理论和实验上都表现出色，为解决强化学习中的探索-利用困境提供了有效方法。

Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.

</details>


### [21] [FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments](https://arxiv.org/abs/2512.15430)
*Quanxi Zhou,Wencan Mao,Manabu Tsukada,John C. S. Lui,Yusheng Ji*

Main category: cs.LG

TL;DR: 提出FM-EAC算法，结合基于模型和无模型强化学习的优势，通过特征模型和增强的actor-critic框架提升多任务控制中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习方法在跨任务和场景的有效迁移性方面仍然存在困难，尽管基于模型和无模型强化学习在Dyna-Q中有所融合，但仍需改进泛化能力。

Method: 提出FM-EAC（特征模型增强actor-critic）算法，整合规划、行动和学习，使用新颖的特征模型和增强的actor-critic框架，支持根据用户需求定制子网络。

Result: 在城市和农业应用仿真中，FM-EAC持续优于多种最先进的基于模型和无模型强化学习方法。

Conclusion: FM-EAC成功结合了基于模型和无模型强化学习的优势，通过特征模型和增强的actor-critic框架显著提升了多任务控制的泛化能力。

Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.

</details>


### [22] [Corrective Diffusion Language Models](https://arxiv.org/abs/2512.15596)
*Shuibai Zhang,Fred Zhangzhi Peng,Yiheng Zhang,Jin Pan,Grigorios G. Chrysos*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散语言模型的校正导向后训练方法，通过显式监督可见的错误标记来提升错误定位和迭代修正能力，并在代码修订任务上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在结构上适合迭代错误修正，但标准的掩码扩散语言模型训练无法可靠地诱导这种校正行为，因为模型常常无法识别完整输入中的不可靠标记，导致置信度引导的修正效果不佳。

Method: 提出了校正导向的后训练原则，显式监督可见的错误标记，使模型能够进行错误感知的置信度评估和针对性修正。同时引入了代码修订基准（CRB），这是一个可控且可执行的基准，用于评估错误定位和原地修正能力。

Result: 实验表明，使用该方法训练的模型在校正场景中显著优于标准掩码扩散语言模型，同时也在纯完成任务上提升了性能。

Conclusion: 校正导向的训练方法能够有效提升扩散语言模型的错误修正能力，使其能够可靠地识别和修正错误标记，同时保持正确内容不变。

Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.

</details>


### [23] [Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605)
*Mathieu Blondel,Michael E. Sander,Germain Vivier-Ardisson,Tianlin Liu,Vincent Roulet*

Main category: cs.LG

TL;DR: 该论文建立了自回归模型（ARMs）与能量模型（EBMs）之间的统一理论框架，证明两者在函数空间存在显式双射关系，并推导了监督学习的等价性以及EBMs蒸馏到ARMs的理论误差界。


<details>
  <summary>Details</summary>
Motivation: 自回归模型是当前大语言模型的主流范式，而能量模型虽然在LLM开发中较少使用，但天然表征了后训练对齐中的最优策略。作者希望建立这两类模型之间的统一理论框架，以深入理解ARMs的规划能力。

Method: 从概率链式法则出发，在函数空间中建立ARMs与EBMs之间的显式双射关系，证明该关系对应最大熵强化学习中的软贝尔曼方程特例。在此基础上推导监督学习的等价性，并分析EBMs蒸馏到ARMs的理论误差界。

Result: 建立了ARMs与EBMs之间的统一理论框架，证明了监督学习的等价性，提供了EBMs蒸馏到ARMs的理论误差界，为理解ARMs基于下一个token预测范式却具备规划能力提供了理论见解。

Conclusion: 该研究为自回归模型和能量模型提供了统一的理论视角，揭示了它们之间的深层联系，为理解ARMs的规划能力提供了理论基础，并为模型蒸馏提供了理论指导。

Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [24] [扩展<em class="highlight">强化学习</em>永远不会通向通用人工智能](http://mp.weixin.qq.com/s?__biz=MzY0MDI1NjI1OA==&mid=2247489450&idx=1&sn=cb39a526bf081c475f79b4b3f361d49b&chksm=f13f109ce95f40b2e2a4df5521c89a197ab0a7ef5cf73838f45b29c21417ed003fb6d555fdd9#rd)
*和通AI*

Main category: wechat.article

TL;DR: 强化学习（以及其他后期训练方法）同理。它无法将智能嵌入系统内部，至多只能提取智能（本文将剖析为何它在这方面实际上也做得很糟糕）。题外话一则趣闻：阿希什·瓦斯瓦尼（是的，Transformer的创造者）回来了。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（以及其他后期训练方法）同理。它无法将智能嵌入系统内部，至多只能提取智能（本文将剖析为何它在这方面实际上也做得很糟糕）。题外话一则趣闻：阿希什·瓦斯瓦尼（是的，Transformer的创造者）回来了。

</details>


### [25] [《指挥和控制<em class="highlight">强化学习</em>智能体的对抗性攻击》](http://mp.weixin.qq.com/s?__biz=MzU2NzkxMTQ5NQ==&mid=2247557808&idx=5&sn=335bf5f89523a4e38901afdc496365fe&chksm=fdb1a095077da24720d4dbd45394e397b3d309713f55b82e929ee1c56f89f448fee88c4c91b9#rd)
*专知智能防务*

Main category: wechat.article

TL;DR: 深度强化学习（DRL）已被成功用于训练《星际争霸》和《DoTAdota》等几款战术和即时战略游戏中的智能体，这些游戏涉及复杂的规划和决策。这些智能体通过自我博弈、模仿学习等技术，熟练地提出了与经验丰富的人类玩家不相


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 深度强化学习（DRL）已被成功用于训练《星际争霸》和《DoTAdota》等几款战术和即时战略游戏中的智能体，这些游戏涉及复杂的规划和决策。这些智能体通过自我博弈、模仿学习等技术，熟练地提出了与经验丰富的人类玩家不相

</details>


### [26] [首个证实在线<em class="highlight">强化学习</em>有效性的VLA框架......](http://mp.weixin.qq.com/s?__biz=MzkwNTgxMTgxNQ==&mid=2247484598&idx=1&sn=9216033949e9083a177b560af834e1f5&chksm=c1ac56c904b3ca8bf9510415f0a2c1a5ff57fcd49ae0f03e783c3a84e28944adb5cb058360ea#rd)
*智核工场*

Main category: wechat.article

TL;DR: 在线强化学习通过试错学习为解决这些问题提供了一条极具潜力的途径。然而，将在线强化学习应用于自动驾驶视觉-语言-动作模型时，面临着连续动作空间中探索效率低下的难题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在线强化学习通过试错学习为解决这些问题提供了一条极具潜力的途径。然而，将在线强化学习应用于自动驾驶视觉-语言-动作模型时，面临着连续动作空间中探索效率低下的难题。

</details>


### [27] [大模型<em class="highlight">强化学习</em>前沿技术综述：从理论到实践的系统性洞察](http://mp.weixin.qq.com/s?__biz=MzIyMjE5Njk1Mw==&mid=2651252573&idx=1&sn=7c24da3fb3d659740f0acbaacbf323fb&chksm=f2bbfc038283d27dde3d0737274fa5d8c9502244b547d7e30859fbf625c3030937026d80108d#rd)
*Machi*

Main category: wechat.article

TL;DR: 本文系统梳理2025年大模型强化学习领域的重要进展，涵盖训练稳定性、数据效率、算法创新等核心议题。1. 训练稳定性：RLVR的核心挑战1.1 GRPO训练崩溃的根本原因


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文系统梳理2025年大模型强化学习领域的重要进展，涵盖训练稳定性、数据效率、算法创新等核心议题。1. 训练稳定性：RLVR的核心挑战1.1 GRPO训练崩溃的根本原因

</details>


### [28] [如何评测一个<em class="highlight">Agentic</em> SOC?](http://mp.weixin.qq.com/s?__biz=MzU5NjYyOTAyNg==&mid=2247483912&idx=1&sn=2e220a1f9ad35e8711dc45fcc64f1bf2&chksm=ff9ece0b85fa027828f1abaaaee42cb350b4d7fc0f89123d385241d5f43a3df2505894dd8af4#rd)
*安全深度观察*

Main category: wechat.article

TL;DR: 用户视角的Agentic AI用于安全运营的POC 验收清单整理了一份Agentic AI在安全运营领域POC 项目的验收清单， 基于愿意埋单付钱的用户视角，分解成四个维度： 技术、运营、风险、组织 中最核心的内容


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 用户视角的Agentic AI用于安全运营的POC 验收清单整理了一份Agentic AI在安全运营领域POC 项目的验收清单， 基于愿意埋单付钱的用户视角，分解成四个维度： 技术、运营、风险、组织 中最核心的内容

</details>


### [29] [【AI安全】OWASP <em class="highlight">Agentic</em> AI Top 10 攻击预警!](http://mp.weixin.qq.com/s?__biz=MzkxNzU2NDgxNQ==&mid=2247484291&idx=1&sn=8de27988b1d03830afe4e8965fcc95a8&chksm=c0e2652936fbdb0d53cb9f9d46e8b2d848f10f35256da104d5cf838adb0697b38118b4d7ce1b#rd)
*Oxo Security*

Main category: wechat.article

TL;DR: Agentic AI 则完全不同。智能体不再仅仅是生成内容，它们具备了自主性（Autonomy）。它们可以： 规划（Plan）：将一个模糊的高级目标拆解为具体的执行步骤。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 则完全不同。智能体不再仅仅是生成内容，它们具备了自主性（Autonomy）。它们可以： 规划（Plan）：将一个模糊的高级目标拆解为具体的执行步骤。

</details>


### [30] [2025-2026 麦肯锡 AI 战略回顾与展望：<em class="highlight">Agentic</em> AI 时代的崛起](http://mp.weixin.qq.com/s?__biz=MzA4MzEzMTA0NQ==&mid=2457472221&idx=1&sn=9105aaeedcbe9cd1c01a785cf1c7d7f6&chksm=89f8bebd8d7f664ea9e713bfd0fdfd5f7f0f8c4aff5e89a6371a4de5b9c32235e647ce29bb83#rd)
*制造AI战略伙伴*

Main category: wechat.article

TL;DR: Agentic应用： 多代理系统协同工作，一个负责数据提取，一个负责风险评分，一个负责起草报告，人类仅需进行最终战略审核。成效： 生产力提升 20-60%，信贷周转速度提升 30%。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic应用： 多代理系统协同工作，一个负责数据提取，一个负责风险评分，一个负责起草报告，人类仅需进行最终战略审核。成效： 生产力提升 20-60%，信贷周转速度提升 30%。

</details>


### [31] [人大 x 清华 | DeepAnalyze：如何打造数据分析的“<em class="highlight">Agentic</em> AI”？](http://mp.weixin.qq.com/s?__biz=Mzg2MzAwNzM0NQ==&mid=2247492708&idx=1&sn=0e0779d59cb678a7f04e26a39b6c4955&chksm=cf628e382b7d280402469018fc8e1e751699bb2193f48c0a3acf2b8868bb32b96af34195f700#rd)
*QuantML*

Main category: wechat.article

TL;DR: 今天给大家介绍的这篇人大与清华团队的论文 《DeepAnalyze： Agentic Large Language Models for Autonomous Data Science》，搞出了一个 80亿参数（8B） 的模型，在全自动数据科学任务上，取得了相当不错的结果 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 今天给大家介绍的这篇人大与清华团队的论文 《DeepAnalyze： Agentic Large Language Models for Autonomous Data Science》，搞出了一个 80亿参数（8B） 的模型，在全自动数据科学任务上，取得了相当不错的结果 。

</details>


### [32] [巴适得板！<em class="highlight">Agentic</em> AI从概念到生产，这些坑必须避开！](http://mp.weixin.qq.com/s?__biz=MzkyOTQyMjkzOA==&mid=2247501277&idx=1&sn=e0b9de2d1de74f5300abfa767bcd240b&chksm=c343ecd246ea804132472b9901cb3d9065f8ec0d52d14bd5f167d3f649f4b54b33263046b0dd#rd)
*大话数智*

Main category: wechat.article

TL;DR: 概念阶段 vs 生产环境：一道90%的Agentic AI项目过不去的鸿沟。概念阶段： Demo跑得飞快 投资人很满意 团队信心爆棚生产环境： Agent决策逻辑混乱，自主性变成"随机性"


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 概念阶段 vs 生产环境：一道90%的Agentic AI项目过不去的鸿沟。概念阶段： Demo跑得飞快 投资人很满意 团队信心爆棚生产环境： Agent决策逻辑混乱，自主性变成"随机性"

</details>


### [33] [<em class="highlight">Agentic</em> AI实践指南｜秘籍三：构建Agent记忆模块](http://mp.weixin.qq.com/s?__biz=Mzg4NjU5NDUxNg==&mid=2247603892&idx=1&sn=850216656f5f968cdffc5c053b730aa0&chksm=ceabde69cf7fc52f5aa6d0a6fd254d86c1b1a1d9cb22a5ef6562fb0c740dc12f5af15f40bd2f#rd)
*亚马逊云开发者*

Main category: wechat.article

TL;DR: 本系列文章基于在多个项目中积累的Agent应用构建经验，分享Agentic AI基础设施实践经验内容，帮助您全面深入地掌握Agent构建的基本环节。上篇文章介绍了专用沙盒环境的必要性与实践方案。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本系列文章基于在多个项目中积累的Agent应用构建经验，分享Agentic AI基础设施实践经验内容，帮助您全面深入地掌握Agent构建的基本环节。上篇文章介绍了专用沙盒环境的必要性与实践方案。

</details>


### [34] [一文彻底厘清：AI Agent、<em class="highlight">Agentic</em> Workflow与<em class="highlight">Agentic</em> AI（附6篇核心论文）](http://mp.weixin.qq.com/s?__biz=MzA5NjMzODEwNQ==&mid=2650595987&idx=1&sn=107eff46209427706e48e6003c7c0b11&chksm=89cf451dcddfcc4596753e3b7b92307c03635f64e72bbfffb1e766fd27c7a04a24f8991667b3#rd)
*王吉伟*

Main category: wechat.article

TL;DR: Agentic WorkflowAgentic Workflow是基于一个或多个AI Agent构建的结构化任务执行框架，核心是通过任务拆解、角色分工、流程编排，将复杂目标转化为可落地的分步执行路径。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic WorkflowAgentic Workflow是基于一个或多个AI Agent构建的结构化任务执行框架，核心是通过任务拆解、角色分工、流程编排，将复杂目标转化为可落地的分步执行路径。

</details>


### [35] [<em class="highlight">Agentic</em>设计模式（5）：工具使用（函数调用）](http://mp.weixin.qq.com/s?__biz=MzkyMDYxNDA5Nw==&mid=2247484839&idx=1&sn=406d11b3409288eb8fd57e76c1d0e2f2&chksm=c0bc042499526a2ccb1b0f8b131cc390b5b1ae93fa79f960b79e765fdf020ab2431bfb06a0ff#rd)
*叶行宽*

Main category: wechat.article

TL;DR: 观察/结果：工具执的输出或结果返回给智能体。大模型处理：大语言模型接收工具的输出作为上下文，并用它来生成对用户的最终回复，或决定工作流的下一步（可能涉及调用另一个工具、进行反思或提供最终答案）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 观察/结果：工具执的输出或结果返回给智能体。大模型处理：大语言模型接收工具的输出作为上下文，并用它来生成对用户的最终回复，或决定工作流的下一步（可能涉及调用另一个工具、进行反思或提供最终答案）

</details>


### [36] [突发！OpenAI大神姚顺雨，任腾讯首席AI科学家](http://mp.weixin.qq.com/s?__biz=MzkzNzcwNzE1MA==&mid=2247582178&idx=3&sn=c1b57529b2444f33dc20cc5f603ef806&chksm=c3aab382e1ad6687c712c4633ee8857dc58cf145b7087b1fc8b75a993366be329753c297f688#rd)
*扩展迷AIGC*

Main category: wechat.article

TL;DR: 而OpenAI最新研究也印证了这一观点：评测方法是影响模型幻觉的关键因素，优化评测手段可进一步释放大模型的潜力。论文地址：https：//arxiv.org/pdf/2509.04664


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而OpenAI最新研究也印证了这一观点：评测方法是影响模型幻觉的关键因素，优化评测手段可进一步释放大模型的潜力。论文地址：https：//arxiv.org/pdf/2509.04664

</details>


### [37] [豆包<em class="highlight">大模型</em>日均 tokens 使用量超 50 万亿！同比去年增长超十倍](http://mp.weixin.qq.com/s?__biz=Mzg3NDkyMTQ5Mw==&mid=2247499266&idx=1&sn=49b3de5059ee0380688ed012ce7355de&chksm=cf34470c6ce996a165273af80d1111948a545918c03340654a63f2a6274e8b5d428847076a7e#rd)
*有新Newin*

Main category: wechat.article

TL;DR: 图：豆包大模型1.8测试表现 在多项公开评测中，豆包1.8展现出极具竞争力的全面表现：在视觉推理、通用视觉问答、空间理解及视频理解等任务中，均获得最佳或接近最佳成绩；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 图：豆包大模型1.8测试表现 在多项公开评测中，豆包1.8展现出极具竞争力的全面表现：在视觉推理、通用视觉问答、空间理解及视频理解等任务中，均获得最佳或接近最佳成绩；

</details>


### [38] [产业规模超500亿元！AI企业为何扎堆黄埔？](http://mp.weixin.qq.com/s?__biz=MzkzOTg3NTYxNw==&mid=2247497524&idx=3&sn=f37fea06ef76d33f7373db0d6f5319a7&chksm=c3e863616dabf401529a60717e1f888738c5bc5622f7db60532d283586491f45437271af11df#rd)
*广州市黄埔区社会工作联合会*

Main category: wechat.article

TL;DR: △南方电网数字电网研究院股份有限公司开发上线全国首个自主可控电力大模型“大瓦特” 图/南方电网 在算力方面，黄埔区布局了一系列数字新基建，包括建成粤港澳大湾区智能算力中心、落地全省首个“东数西算”结对子合


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: △南方电网数字电网研究院股份有限公司开发上线全国首个自主可控电力大模型“大瓦特” 图/南方电网 在算力方面，黄埔区布局了一系列数字新基建，包括建成粤港澳大湾区智能算力中心、落地全省首个“东数西算”结对子合

</details>


### [39] [【报告】智能体专题四：2025年从<em class="highlight">大模型</em>到智能体——人工智能场景的投资展望（附PDF下载）](http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247568138&idx=4&sn=b57641769a5d95e0bad9aec9af441804&chksm=fcefd6c384430f7c2fd98b74d2ef0d11ca39ad418537114eb6d1db74316f91f81aad6cc46c47#rd)
*人工智能产业链union*

Main category: wechat.article

TL;DR: 报告指出，大模型实现了 “大数据 + 大算力 + 强算法” 的融合，而智能体作为 “大模型 + 工具 + 场景” 的集成形态，是人工智能从 “技术突破” 走向 “产业落地” 的关键载体，将在金融、农业、矿山、医疗等领域重塑生产


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 报告指出，大模型实现了 “大数据 + 大算力 + 强算法” 的融合，而智能体作为 “大模型 + 工具 + 场景” 的集成形态，是人工智能从 “技术突破” 走向 “产业落地” 的关键载体，将在金融、农业、矿山、医疗等领域重塑生产

</details>


### [40] [亚信科技输了！1050万元，AI<em class="highlight">大模型</em>大单：联通云中标](http://mp.weixin.qq.com/s?__biz=MzU0NDEyODkzMQ==&mid=2247562022&idx=3&sn=d518db2cf6af6b4fe256a5e721e6a8b1&chksm=fada97722572fa8358dd416e9cb31121a964f9c5fe909d83c8304661a2f81d3a3d4044a97ba0#rd)
*云技术*

Main category: wechat.article

TL;DR: 在集团范围内，人工智能场景实施等工作前景巨大，本项目就是在集团公司大模型为基础的人工智能应用需求的基础上形成的。旨在为中国海油基于大模型的人工智能工作提供技术支持服务，承接基于大模型的场景化开发服务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在集团范围内，人工智能场景实施等工作前景巨大，本项目就是在集团公司大模型为基础的人工智能应用需求的基础上形成的。旨在为中国海油基于大模型的人工智能工作提供技术支持服务，承接基于大模型的场景化开发服务。

</details>


### [41] [豆包<em class="highlight">大模型</em>1.8发布：三大能力显著增强 媲美全球顶尖模型](http://mp.weixin.qq.com/s?__biz=Mzg4MTc1MjY2Mw==&mid=2247497549&idx=3&sn=9139180da768050430c4e667f3e7223a&chksm=cec9ef0ad4d07edaef1b27c74754ccd82bc835735ddf7d7937b708030cdee43c599f945d175a#rd)
*科技情报*

Main category: wechat.article

TL;DR: 快科技12月18日消息，在今天召开的火山引擎Force原动力大会上，豆包大模型1.8正式发布，多模态Agent能力媲美全球顶尖模型。据了解，豆包大模型1.8面向多模态Agent场景进行了定向优化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 快科技12月18日消息，在今天召开的火山引擎Force原动力大会上，豆包大模型1.8正式发布，多模态Agent能力媲美全球顶尖模型。据了解，豆包大模型1.8面向多模态Agent场景进行了定向优化。

</details>


### [42] [<em class="highlight">大模型</em>+RAG：构建高适配性测试智能体的核心实践](http://mp.weixin.qq.com/s?__biz=MjM5MzA3MzAzOQ==&mid=2655589292&idx=2&sn=241bc6a01b9cf25d8271e569a385c046&chksm=bcef3857deb8dc6883e3dbd5d2240a048d0b62fb1f53a9b78232abdae9582baeec5b411bd090#rd)
*金融电子化*

Main category: wechat.article

TL;DR: 1. 纯大模型测试智能体的三大致命局限 知识时效性与版本脱节：通用大模型训练数据存在固定时间窗口，无法同步测试项目的动态迭代需求。例如某电商项目V2.1版新增“信创适配测试规范”（含麒麟系统兼容性要求），纯大模


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1. 纯大模型测试智能体的三大致命局限 知识时效性与版本脱节：通用大模型训练数据存在固定时间窗口，无法同步测试项目的动态迭代需求。例如某电商项目V2.1版新增“信创适配测试规范”（含麒麟系统兼容性要求），纯大模

</details>


### [43] [豆包<em class="highlight">大模型</em>1.8发布，Seedance模型同步升级](http://mp.weixin.qq.com/s?__biz=MzkzMTY2MzMzMQ==&mid=2247485842&idx=1&sn=a121df5259c5e62f83797e568f804d7f&chksm=c38c4191af4c4c8daa7ed3b5710ac4f6942bdf6b4e1f41ac0ee1deedf9960704b01d33b8980e#rd)
*豆包*

Main category: wechat.article

TL;DR: 豆包大模型1.8多模态Agent能力媲美全球顶尖模型豆包大模型1.8（Doubao-Seed-1.8）面向多模态Agent场景进行了定向优化。其工具调用能力、复杂指令遵循能力及OS Agent能力显著增强，大幅提升了模型在处理复杂任务时的规划与执行水平


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 豆包大模型1.8多模态Agent能力媲美全球顶尖模型豆包大模型1.8（Doubao-Seed-1.8）面向多模态Agent场景进行了定向优化。其工具调用能力、复杂指令遵循能力及OS Agent能力显著增强，大幅提升了模型在处理复杂任务时的规划与执行水平

</details>


### [44] [豆包<em class="highlight">大模型</em>1.8发布，Seedance模型同步升级](http://mp.weixin.qq.com/s?__biz=MzU1MDgyNzk5Ng==&mid=2247492666&idx=1&sn=c59cbcc42fd874bc628c405adf357d26&chksm=faf3048d286b7779f31a557afdb4d0d21cb7eeffa84d1b4e93cf1147f0f1bbcbef51432232bf#rd)
*字节跳动*

Main category: wechat.article

TL;DR: 豆包大模型1.8多模态Agent能力媲美全球顶尖模型豆包大模型1.8（Doubao-Seed-1.8）面向多模态Agent场景进行了定向优化。其工具调用能力、复杂指令遵循能力及OS Agent能力显著增强，大幅提升了模型在处理复杂任务时的规划与执行水平


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 豆包大模型1.8多模态Agent能力媲美全球顶尖模型豆包大模型1.8（Doubao-Seed-1.8）面向多模态Agent场景进行了定向优化。其工具调用能力、复杂指令遵循能力及OS Agent能力显著增强，大幅提升了模型在处理复杂任务时的规划与执行水平

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [45] [Anthropic preparing new Agentic Tasks Mode for Claude](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fanthropic-testing-new-agentic-tasks-mode-for-claude%2F%3Futm_source=tldrai/1/0100019b278508dd-bc1490d3-afb7-429a-91e5-642c70b93734-000000/oc3GK8TYK9HiUUxE5Xorzpo0j1FhTSypXnB1D649nr8=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic正在为Claude的Agent模式测试新的任务界面，增加了研究、分析、写作和构建等新模式，并提供经典聊天与Agent模式之间的切换功能。


<details>
  <summary>Details</summary>
Motivation: Anthropic希望改进Claude的Agent模式用户体验，通过更直观的界面设计和专门的任务模式，让用户能更高效地使用Claude进行不同类型的任务处理。

Method: 开发新的任务界面，引入研究、分析、写作、构建等专门模式，并添加经典聊天与Agent模式之间的切换功能，通过界面截图展示新设计。

Result: 新的Agentic Tasks Mode界面正在测试中，提供了更直观的任务切换和模式选择功能，改善了用户与Claude Agent的交互体验。

Conclusion: Anthropic通过界面改进增强了Claude Agent模式的功能性和易用性，为用户提供了更专业化的任务处理工具。

Abstract: Anthropic preparing new Agentic Tasks Mode for Claude (2 minute read) Anthropic is testing a new interface for tasks in Claude's Agent mode. It is also introducing new modes for research, analysis, writing, and building. The updated interface introduces a toggle that allows users to switch between classic chat and agent modes. Screenshots of the new interface are available in the article.

</details>


### [46] [Self Improving Agent with Dynamic Context and Continuous Learning](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ashpreetbedi.com%2Farticles%2Fsql-agent%3Futm_source=tldrai/1/0100019b278508dd-bc1490d3-afb7-429a-91e5-642c70b93734-000000/P2vRZqAsbDazM2meKY4LJOV_8L_0WCijnTPVb36LeK8=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 构建一个使用动态上下文和"穷人版持续学习"的自我改进Text-to-SQL代理，该代理通过检索知识库回答问题，并从成功运行中学习以扩展知识库，形成自我改进的反馈循环。


<details>
  <summary>Details</summary>
Motivation: 传统Text-to-SQL代理通常缺乏从经验中学习的能力，每次查询都是独立的。本文旨在创建一个能够从成功执行中学习并持续改进的智能代理，通过构建自我改进的反馈循环来提升性能。

Method: 采用动态上下文机制和"穷人版持续学习"方法。代理通过检索知识库来回答SQL查询问题，当成功执行查询后，将相关上下文和解决方案添加到知识库中，形成持续学习的循环。这种方法不需要复杂的强化学习框架，而是通过简单的知识积累实现自我改进。

Result: 成功构建了一个能够自我改进的Text-to-SQL代理系统。代理能够从历史成功案例中学习，随着使用时间的增加，其回答准确性和效率会逐步提升。文章提供了演示视频展示代理的实际运行效果。

Conclusion: 通过动态上下文和简单的持续学习方法，可以构建有效的自我改进Text-to-SQL代理。这种"穷人版"方法虽然简单，但能够实现代理性能的持续提升，为构建更智能的数据库查询代理提供了实用方案。

Abstract: Self Improving Agent with Dynamic Context and Continuous Learning (11 minute read) This guide walks readers through how to build a self-improving Text-to-SQL agent using dynamic context and 'poor-man's continuous learning'. The agent answers questions by retrieving data from a knowledge base. It learns from successful runs by adding to the knowledge base, creating a self-improving feedback loop. A video that shows the agent in action is available at the end of the article.

</details>


### [47] [200k Tokens Is Plenty](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F200k-tokens-is-plenty%3Futm_source=tldrai/1/0100019b278508dd-bc1490d3-afb7-429a-91e5-642c70b93734-000000/03KXkMBBzWF85mzkEnCcRDha2osuQhyLu6rMFPy2uoA=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Opus 4.5的20万token上下文窗口对编码任务足够，因为最佳对话线程应该简短且只包含必要上下文，避免上下文窗口被无用信息填满。


<details>
  <summary>Details</summary>
Motivation: 针对有人认为20万token上下文窗口不够大的观点，作者认为对于编码任务来说这已经足够，关键在于如何有效使用上下文窗口。

Method: 通过分析Claude Opus 4.5在编码任务中的实际表现，强调简短对话线程的重要性，建议只提供完成任务所需的必要上下文。

Result: 20万token的上下文窗口对于编码任务来说是充足的，关键在于保持对话线程简短，只包含相关上下文，避免上下文窗口被无用信息占用。

Conclusion: 对于开发者来说，Claude Opus 4.5的20万token上下文窗口完全足够，关键在于使用简短、有针对性的对话线程，只提供必要的上下文信息。

Abstract: 200k Tokens Is Plenty (6 minute read) Claude Opus 4.5 is considered the best model for coding, and it only has a context window of roughly 200,000 tokens. While some people feel like that is not a lot, that can be plenty for developers who use short threads. The best threads are short, with just the right amount of context. The longer the conversation, the more an agent's context window gets filled up with junk. You only need to give agents the context they need to get the job done, and no more.

</details>


### [48] [Kimi CLI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2Fkimi-cli%3Futm_source=tldrdevops/1/0100019b2c33d7af-ee3622ca-1307-4c46-8d32-a21f41192fea-000000/X5JpfyYnBeG039vhkFAMlip-hMxYr0uuLoG9EZAivH0=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Kimi CLI是一个新的命令行界面代理，用于软件开发和终端操作，可作为编码代理和shell使用，支持ACP兼容编辑器集成。


<details>
  <summary>Details</summary>
Motivation: 开发一个集成的命令行工具，将AI编码助手功能与终端操作相结合，提高开发者在软件开发和日常终端任务中的效率。

Method: 开发Python包作为命令行界面代理，提供编码代理和shell功能，集成ACP兼容编辑器（如Zed和JetBrains）以及Zsh。

Result: 发布了Kimi CLI技术预览版，可通过uv安装，为开发者提供了统一的AI辅助开发和终端操作体验。

Conclusion: Kimi CLI成功创建了一个集成的开发工具，将AI编码助手与终端shell功能结合，有望提升开发工作流程的效率。

Abstract: Kimi CLI (GitHub Repo) Kimi CLI, a new command-line interface agent for software development and terminal operations, has been released in technical preview. This Python package, installable via `uv`, functions as both a coding agent and a shell, offering integrations with ACP-compatible editors like Zed and JetBrains, as well as Zsh.

</details>


### [49] [Highlights from AWS re:Invent: Supercharging Kiro with Docker Sandboxes and MCP Catalog](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Faws-reinvent-kiro-docker-sandboxes-mcp-catalog%2F%3Futm_source=tldrdevops/1/0100019b2c33d7af-ee3622ca-1307-4c46-8d32-a21f41192fea-000000/xAFCer5Ehib3T1kOQfIhk_Tmei3_Thu8e1XBu3m9emQ=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AWS re:Invent展示了如何利用Docker沙箱和MCP工具包来安全运行Kiro等AI代理，通过容器隔离保护主机文件与凭证，同时支持安全的代码修改、测试和工具使用。


<details>
  <summary>Details</summary>
Motivation: AI代理在软件开发中越来越重要，但直接运行存在安全风险，可能访问主机敏感文件和凭证。需要一种安全机制来隔离AI代理，同时保持其功能完整性。

Method: 采用Docker沙箱技术创建隔离容器环境，结合MCP（Model Context Protocol）工具包，为AI代理提供安全的运行环境，限制对主机系统的访问权限。

Result: 成功实现了Kiro等AI代理在隔离容器中的安全运行，防止了对主机文件和凭证的访问，同时支持代码修改、测试和工具使用的完整功能。

Conclusion: Docker沙箱与MCP工具包的结合为AI代理提供了安全可靠的运行环境，是AI辅助软件开发的重要基础设施改进。

Abstract: Highlights from AWS re:Invent: Supercharging Kiro with Docker Sandboxes and MCP Catalog (5 minute read) Docker Sandboxes and the MCP Toolkit can run AI agents like Kiro in isolated containers, preventing access to host files and credentials while enabling safe code changes, testing, and tool use.

</details>


### [50] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b2c33d7af-ee3622ca-1307-4c46-8d32-a21f41192fea-000000/tnTGvuyGkqPd7XRdrihuXWCZ7jX9DfYKksGh-sqI-04=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AWS re:Invent展示的Docker沙箱和MCP工具包能让Kiro等AI代理在隔离容器中运行，保护主机文件安全的同时支持安全代码修改、测试和工具使用。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在运行时的安全问题，防止其访问主机敏感文件和凭证，同时提供安全的代码修改和测试环境。

Method: 使用Docker沙箱技术创建隔离容器环境，结合MCP（Model Context Protocol）工具包来管理AI代理的运行环境。

Result: 成功实现了Kiro等AI代理在隔离容器中的安全运行，既能执行代码修改和测试，又能防止对主机系统的安全威胁。

Conclusion: Docker沙箱和MCP工具包为AI代理提供了安全可靠的运行环境，是AI代理部署的重要技术方案。

Abstract: Highlights from AWS re:Invent: Supercharging Kiro with Docker Sandboxes and MCP Catalog (5 minute read) Docker Sandboxes and the MCP Toolkit can run AI agents like Kiro in isolated containers, preventing access to host files and credentials while enabling safe code changes, testing, and tool use.

</details>


### [51] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b2c33d7af-ee3622ca-1307-4c46-8d32-a21f41192fea-000000/a3Yi6tKpEN7Ks-GS5FF9TVpYjHy407GAJNtUQlW_BQI=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AWS re:Invent展示了如何利用Docker沙箱和MCP工具包安全运行Kiro等AI代理，通过容器隔离保护主机文件与凭证，同时支持安全代码修改、测试和工具使用。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在运行时的安全问题，防止其对主机系统造成潜在风险，同时保持其功能完整性。

Method: 采用Docker容器技术创建隔离的沙箱环境，结合MCP（Model Context Protocol）工具包，为AI代理提供安全的运行环境。

Result: 成功实现了Kiro等AI代理在隔离容器中的安全运行，既能防止对主机系统的访问，又能支持代码修改、测试和工具使用等核心功能。

Conclusion: Docker沙箱与MCP工具包的结合为AI代理提供了安全可靠的运行环境，是AI应用部署的重要技术方案。

Abstract: Highlights from AWS re:Invent: Supercharging Kiro with Docker Sandboxes and MCP Catalog (5 minute read) Docker Sandboxes and the MCP Toolkit can run AI agents like Kiro in isolated containers, preventing access to host files and credentials while enabling safe code changes, testing, and tool use.

</details>


### [52] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b2c33d7af-ee3622ca-1307-4c46-8d32-a21f41192fea-000000/VIYZloFpxgfh4pdFvA0XUf4O6DV2RTzv9Iyh0SPWf7c=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AWS re:Invent展示如何通过Docker沙箱和MCP工具包增强Kiro AI代理，实现安全隔离运行


<details>
  <summary>Details</summary>
Motivation: AI代理在运行代码和使用工具时存在安全风险，需要防止对主机文件和凭证的访问，同时支持安全的代码修改和测试

Method: 使用Docker沙箱提供隔离容器环境，结合MCP（Model Context Protocol）工具包，让AI代理在受控环境中运行

Result: Kiro等AI代理能够在隔离容器中安全运行，防止对主机系统的访问，同时支持代码修改、测试和工具使用

Conclusion: Docker沙箱和MCP工具包为AI代理提供了安全可靠的运行环境，是AI代理部署的重要技术方案

Abstract: Highlights from AWS re:Invent: Supercharging Kiro with Docker Sandboxes and MCP Catalog (5 minute read) Docker Sandboxes and the MCP Toolkit can run AI agents like Kiro in isolated containers, preventing access to host files and credentials while enabling safe code changes, testing, and tool use.

</details>


### [53] [How to prompt v0](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fhow-to-prompt-v0%3Futm_source=tldrdev/1/0100019b2c4a2be1-17170461-eff7-4d37-a495-0e5db7e593ef-000000/iS8cnyVbYpFyEao25lxuaPZAvyeFFWx6EahqWhvZ5J0=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了如何有效提示v0构建Web应用或组件的三个核心方法：指定产品服务、定义使用上下文、以及概述约束和偏好，以提升生成速度、UX决策和代码质量。


<details>
  <summary>Details</summary>
Motivation: 为了帮助用户更有效地使用v0工具生成Web应用和组件，提高生成效率、改善用户体验决策并产出更清晰可维护的代码。

Method: 提出了三个核心提示方法：1) 指定产品服务（组件、数据和操作）；2) 定义使用上下文（用户、时机和预期结果）；3) 概述约束和偏好（样式、平台和布局）。

Result: 通过应用这些具体的提示方法，v0能够更快地生成代码，做出更智能的UX决策，并创建更清晰、更易维护的代码。

Conclusion: 有效的提示策略对于优化v0生成Web应用和组件的性能至关重要，三个核心方法提供了系统化的指导框架。

Abstract: How to prompt v0 (4 minute read) When prompting v0 to build web apps or components, it's important to know three core methods: specify the product service (components, data, and actions), define the context of use (who, when, and for what outcome), and outline constraints and taste (style, platform, and layout). By providing these specific details, v0 can generate faster, make smarter UX decisions, and create cleaner, more maintainable code.

</details>


### [54] [Optimizing Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmays.co%2Foptimizing-claude-code%3Futm_source=tldrdev/1/0100019b2c4a2be1-17170461-eff7-4d37-a495-0e5db7e593ef-000000/0xx7Gh_qbQnE2LmkaPoYhphDhEcmrGb6Hv_wxXNTOko=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 一篇关于如何优化Claude代码助手的指南，通过定制技能、插件、命令和配置文件来匹配用户的工作流程和风格


<details>
  <summary>Details</summary>
Motivation: 默认的Claude代码助手虽然功能强大，但通过定制化可以使其更贴合特定工作流程，就像从雇佣通才转变为雇佣熟悉公司内部流程的专家

Method: 通过定制技能、插件、命令和配置文件来优化Claude代码助手，使其适应特定的工作风格和需求

Result: 优化后的Claude代码助手能够更好地匹配用户的工作流程，减少重复性指令，提高工作效率

Conclusion: 虽然优化Claude需要投入时间，但长期来看能够显著提升工作效率和用户体验

Abstract: Optimizing Claude Code (12 minute read) Claude Code can be customized with skills, plugins, commands, and configuration files. While the defaults are remarkably capable, the system can be adjusted to fit any workflow - the difference is like hiring a talented generalist versus hiring someone who's worked at your company for years. This guide teaches readers how to optimize the assistant so it matches their style. It can take some time to optimize Claude, but doing so means you won't have to c...

</details>


### [55] [Skyramp: Deterministic testing, powered by AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.skyramp.dev%2F%3Futm_campaign=Beta1225%26utm_source=tldrdev%26utm_medium=newsletter/1/0100019b2c4a2be1-17170461-eff7-4d37-a495-0e5db7e593ef-000000/HbJCPwczgMoWdpqt1Cz5xvla9iS9HDnBLPzWR8tQUes=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Skyramp是一个AI驱动的确定性测试工具，通过自然语言生成生产就绪的测试用例


<details>
  <summary>Details</summary>
Motivation: 解决传统测试方法需要大量手动编写测试代码的问题，通过AI自动化测试生成，提高测试效率和覆盖率

Method: 结合AI编排和规范驱动设计，从自然语言描述自动生成生产就绪的测试用例

Result: 提供在Cursor和VS Code中的beta版本，实现对话式、智能化和确定性的自动化测试

Conclusion: Skyramp通过AI技术革新软件测试流程，使测试生成更加智能化和高效

Abstract: Skyramp: Deterministic testing, powered by AI (Sponsor) Automated testing that's conversational, intelligent, and deterministic. Skyramp blends AI orchestration with specification-driven design to generate production-ready tests from natural language. Join the beta in Cursor or VS Code

</details>


### [56] [A2UI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa2ui.org%2F%3Futm_source=tldrdev/1/0100019b2c4a2be1-17170461-eff7-4d37-a495-0e5db7e593ef-000000/GkSxGAvmLoAsfWIoFVAGKAxQcVn13J8y_J9Fy9mnN1Y=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: A2UI是一个协议，让AI代理通过发送声明式组件描述而非可执行代码来为Web、移动和桌面应用生成交互式用户界面。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理生成UI通常需要生成可执行代码，这存在复杂性、安全性和跨平台兼容性问题。A2UI旨在通过声明式描述简化AI代理生成交互式界面的过程。

Method: 提出A2UI协议，定义了一种声明式组件描述语言，AI代理可以通过发送结构化描述来指定UI组件，而不是生成具体代码。该协议支持Web、移动和桌面应用的跨平台界面生成。

Result: A2UI协议能够使AI代理更高效地生成交互式用户界面，减少了代码生成复杂性，提高了跨平台兼容性，并增强了安全性。

Conclusion: A2UI为AI代理生成用户界面提供了一种更简单、更安全、更跨平台的解决方案，通过声明式描述而非可执行代码的方式改进了现有方法。

Abstract: A2UI (Website) A2UI is a protocol that lets AI agents generate interactive user interfaces for web, mobile, and desktop applications by sending declarative component descriptions rather than executable code.

</details>


### [57] [Letta Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.letta.com%2Fblog%2Fletta-code%3Futm_source=tldrdev/1/0100019b2c4a2be1-17170461-eff7-4d37-a495-0e5db7e593ef-000000/tbIu4TLzja2wr4Wl1LUrXAY41SOvPL2h1gOWaiV2EIM=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Letta Code是一个面向长期运行代理的以内存优先的编码代理系统，支持跨独立会话的持续学习和改进


<details>
  <summary>Details</summary>
Motivation: 当前编码代理通常缺乏长期记忆能力，无法在多个独立会话中积累知识和经验，限制了代理的持续改进能力

Method: 采用内存优先架构，为编码代理设计专门的记忆系统，支持跨会话的知识保留和经验积累

Result: 开发了Letta Code系统，实现了长期运行的编码代理，能够在多个独立会话中持续学习和改进

Conclusion: 内存优先的架构对于构建长期运行的编码代理至关重要，能够显著提升代理的持续学习能力和性能

Abstract: Letta Code (Website) Letta Code is a memory-first coding agent designed for long-lived agents that continually learn and improve across independent sessions.

</details>


### [58] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b2c4a2be1-17170461-eff7-4d37-a495-0e5db7e593ef-000000/axscqZhoZLWW3TfM6NK0bNbEgGQ9qzIl_uS2E-02JMk=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Letta Code是一个基于记忆优先的编码智能体，专为长期运行、跨独立会话持续学习和改进的智能体设计。


<details>
  <summary>Details</summary>
Motivation: 当前编码智能体通常缺乏长期记忆能力，无法在多个独立会话中积累知识和经验。这限制了智能体在长期任务中的持续改进能力，特别是在需要跨会话学习和知识积累的场景中。

Method: 采用记忆优先的设计理念，构建具有长期记忆能力的编码智能体。系统能够跨独立会话存储和检索编码经验、解决方案和最佳实践，实现知识的持续积累和重用。

Result: Letta Code能够通过长期记忆机制在多个会话中持续改进编码能力，提高代码生成质量和效率，特别适合需要长期学习和知识积累的编码任务。

Conclusion: 记忆优先的编码智能体设计是实现长期学习和持续改进的关键，Letta Code为构建具有长期记忆能力的编码助手提供了有效的解决方案。

Abstract: Letta Code (Website) Letta Code is a memory-first coding agent designed for long-lived agents that continually learn and improve across independent sessions.

</details>


### [59] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b2c4a2be1-17170461-eff7-4d37-a495-0e5db7e593ef-000000/HqgFnPWL2Mj-QMQI2qeOsV0ERj0OAr83sx8ZBSIbENw=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Letta Code是一个基于记忆优先的编码代理，专为长期运行、持续学习和跨独立会话改进的智能体设计


<details>
  <summary>Details</summary>
Motivation: 现有编码代理通常缺乏长期记忆能力，无法在多个独立会话中持续学习和改进。为了解决这一问题，作者提出了一个记忆优先的架构，使智能体能够积累知识并随时间提升性能。

Method: 采用记忆优先的设计理念，构建长期记忆系统，使编码代理能够在不同会话间保持和利用历史经验。系统可能包括知识库、经验存储和检索机制，支持持续学习和改进。

Result: Letta Code作为一个长期运行的编码代理，能够在多个独立会话中持续学习和改进，相比传统一次性代理具有更好的知识积累和性能提升能力。

Conclusion: 记忆优先的架构对于构建长期运行的编码代理至关重要，Letta Code展示了通过持续学习和跨会话知识积累来提升智能体性能的有效途径。

Abstract: Letta Code (Website) Letta Code is a memory-first coding agent designed for long-lived agents that continually learn and improve across independent sessions.

</details>


### [60] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b2c4a2be1-17170461-eff7-4d37-a495-0e5db7e593ef-000000/BRyDJEv3WZhhI6S8GIWJ6yNLeeo3aEaoqkpevpr0tQY=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Letta Code是一个面向长期运行代理的以内存优先的编码代理系统，旨在支持跨独立会话的持续学习和改进


<details>
  <summary>Details</summary>
Motivation: 现有的编码代理通常在单次会话中运行，缺乏跨会话的持续学习和改进能力。对于需要长期运行、不断积累知识和经验的编码代理来说，需要一种能够记忆先前经验并在后续任务中利用这些知识的机制。

Method: 采用"内存优先"的设计理念，构建一个能够跨会话存储和检索编码经验、解决方案和知识的系统。代理通过记忆机制保存历史交互、代码片段和问题解决策略，在后续任务中利用这些记忆来提高效率和代码质量。

Result: Letta Code能够实现跨会话的知识积累和复用，长期运行的代理能够随着时间推移提高编码效率和代码质量，减少重复工作，并在类似任务中表现出更好的性能。

Conclusion: 内存优先的设计为编码代理的长期运行和持续改进提供了有效框架，使代理能够从历史经验中学习并不断提升性能，这对于实际应用中的编码代理系统具有重要意义。

Abstract: Letta Code (Website) Letta Code is a memory-first coding agent designed for long-lived agents that continually learn and improve across independent sessions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [61] [Revisiting the Reliability of Language Models in Instruction-Following](https://arxiv.org/abs/2512.14754)
*Jianshuo Dong,Yutong Zhang,Yan Liu,Zhenyu Zhong,Tao Wei,Chao Zhang,Han Qiu*

Main category: cs.SE

TL;DR: 该论文研究了LLM在细微差别导向的可靠性问题，发现即使模型在基准测试中表现优异，在面对表达相同意图但措辞略有不同的"cousin prompts"时，性能可能大幅下降达61.8%。


<details>
  <summary>Details</summary>
Motivation: 尽管先进LLM在IFEval等基准测试中取得了接近天花板的指令跟随准确率，但这些高分不一定能转化为现实世界中的可靠服务，因为用户经常变化措辞、上下文框架和任务表述方式。

Method: 引入新的可靠度指标reliable@k，开发自动化流水线通过数据增强生成高质量的cousin prompts，构建IFEval++进行系统评估，测试了20个专有和26个开源LLM。

Result: 当前模型在细微差别导向的可靠性方面存在严重不足，性能可能因细微的提示修改而下降高达61.8%。论文还描述了这一现象并探索了三种潜在的改进方法。

Conclusion: 细微差别导向的可靠性是实现更可靠、更可信LLM行为的关键但尚未充分探索的下一步，论文的代码和基准测试已开源。

Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.

</details>


### [62] [Workflows vs Agents for Code Translation](https://arxiv.org/abs/2512.14762)
*Henry Gray,Tom Yotam,Octavian Udrea*

Main category: cs.SE

TL;DR: 比较两种基于LLM的MATLAB到HDL语法修复方法：结构化专家流程与自主代理方法，发现代理方法在解决语法错误方面更有效，尤其能提升中小规模模型的性能。


<details>
  <summary>Details</summary>
Motivation: 将MATLAB算法转换为硬件描述语言(HDL)是FPGA/ASIC部署的必要步骤，但LLM在HDL代码训练有限，导致端到端转换容易产生语法错误。需要研究有效的语法修复方法。

Method: 比较两种LLM驱动的语法修复方法：1) 结构化专家设计流程（固定操作序列）；2) 自主代理方法（使用MCP动态选择工具）。研究42个MATLAB信号处理函数，隔离语法修复阶段，测试三种模型规模。

Result: 代理方法在解决初始语法错误方面更有效，能解锁更多候选代码通过流程。上游改进带来下游可测量的提升，特别是中等规模模型，仿真成功率提高超过20个百分点。条件检索在8B和30B模型中有效，235B模型最终成功率提升较小。

Conclusion: 当设计恰当时，这些代理框架能有效补偿中小规模模型的能力限制。代理方法的优势来自短提示、积极上下文管理和条件工具使用。

Abstract: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.

</details>


### [63] [Evaluating Code Reasoning Abilities of Large Language Models Under Real-World Settings](https://arxiv.org/abs/2512.14917)
*Changshu Liu,Alireza Ghazanfari,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: RE2-Bench是一个包含1101个代码推理问题的基准测试，其中195个来自真实项目，通过程序分析处理复杂类型，并使用9个复杂度指标将问题分为Easy和Hard两类，揭示了LLM在真实复杂代码推理中的性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准测试使用简单程序，无法反映真实世界代码的复杂性（如过程间依赖、API调用、嵌套结构、复杂类型等），导致对LLM泛化能力的评估存在偏差。

Method: 提出RE2-Bench基准：1）包含1101个推理问题（195个来自真实项目）；2）使用静态和动态程序分析自动序列化/反序列化复杂类型；3）通过9个可解释代码复杂度指标的多数投票机制将问题分类为Easy或Hard。

Result: 在输入预测和输出预测两个任务上评估6个LLM，发现从Easy到Hard问题性能显著下降（输入预测下降51.50%，输出预测下降42.15%），证实先前评估高估了LLM的推理能力。

Conclusion: RE2-Bench提供了更真实的代码推理评估框架，揭示了LLM在复杂代码推理中的局限性，先前基于简单程序的评估显著高估了LLM的实际推理能力。

Abstract: Code reasoning tasks are becoming prevalent in large language model (LLM) assessments. Existing benchmarks involve simple programs, failing to represent real-world complexities such as inter- or intra-procedural dependencies, core or third-party API calls, highly nested constructs, and non-primitive complex types. Evaluating LLMs under such a simplistic setting poses a significant threat to assumptions about their generalizability in practice. To enable a more realistic evaluation of code reasoning, this paper proposes RE2-Bench, a benchmark of 1,101 reasoning problems, including 195 drawn from mature real-world projects. RE2-Bench leverages static and dynamic program analysis to automatically serialize and deserialize compound, complex, and custom types in real-world code, going far beyond the primitive-only settings used in prior work.
  A key feature of RE2-Bench is categorizing each reasoning problem as Easy or Hard via a principled majority-vote mechanism over nine interpretable code complexity metrics, resulting in two well-separated and semantically meaningful difficulty categories suitable for precise calibration of LLM reasoning ability. A comprehensive evaluation of six general-purpose and reasoning-oriented LLMs on two widely used code reasoning tasks -- input prediction and output prediction -- using RE2-Bench reveals a significant performance drop from Easy to Hard problems (51.50\% for input prediction and 42.15\% for output prediction), confirming that prior evaluations substantially overestimate the reasoning capabilities of LLMs.

</details>


### [64] [Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent](https://arxiv.org/abs/2512.14990)
*Mehil B Shah,Mohammad Masudur Rahman,Foutse Khomh*

Main category: cs.SE

TL;DR: RepGen：一种自动化智能方法，用于复现深度学习bug，通过构建学习增强上下文、制定复现计划、采用迭代生成-验证-精炼机制，使用LLM生成复现代码，在106个真实bug上达到80.19%的复现率。


<details>
  <summary>Details</summary>
Motivation: 深度学习应用存在大量bug、故障和漏洞，但由于DL模型的固有非确定性和与软硬件环境的紧密耦合，复现这些bug极具挑战性。研究表明仅有约3%的DL bug可以通过手动方法可靠复现。

Method: 1) 从项目中构建学习增强上下文；2) 制定全面的bug复现计划；3) 采用迭代的生成-验证-精炼机制；4) 使用LLM生成能够复现bug的代码。

Result: 在106个真实深度学习bug上评估，达到80.19%的复现率，比最先进方法提升19.81%。开发者研究显示：复现成功率提高23.35%，复现时间减少56.8%，认知负荷降低。

Conclusion: RepGen是首个自动化智能DL bug复现方法，显著提高复现率和效率，降低开发者负担，为DL系统可靠性提供重要支持。

Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.

</details>


### [65] [An Exploratory Study of Bayesian Prompt Optimization for Test-Driven Code Generation with Large Language Models](https://arxiv.org/abs/2512.15076)
*Shlok Tomar,Aryan Deshwal,Ethan Villalovoz,Mattia Fazzini,Haipeng Cai,Janardhan Rao Doppa*

Main category: cs.SE

TL;DR: BODE-GEN使用贝叶斯优化在连续嵌入空间中搜索最优提示，通过辅助LLM连接离散提示空间，提高代码生成准确性


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码正确性受提示影响，手动提示工程耗时且效果有限，需要自动化的提示搜索方法

Method: 提出BODE-GEN框架：1) 使用辅助LLM将离散提示映射到连续嵌入空间；2) 采用随机投影和维度缩放先验构建高斯过程代理模型；3) 在嵌入空间中进行贝叶斯优化搜索最优提示

Result: 在HumanEval+基准测试中，BODE-GEN相比固定提示和手动提示工程能提高代码生成准确性，且样本效率高，只需较少BO迭代就能改善性能

Conclusion: BODE-GEN通过将提示搜索问题转化为连续嵌入空间中的贝叶斯优化问题，有效提高了LLM代码生成的准确性，为自动化提示工程提供了新思路

Abstract: We consider the task of generating functionally correct code using large language models (LLMs). The correctness of generated code is influenced by the prompt used to query the given base LLM. We formulate the problem of finding the appropriate prompt as combinatorial search process and propose a Bayesian optimization (BO) approach referred to as {\em BO for Code GENeration (BODE-GEN)}. BODE-GEN performs an adaptive data-driven search over prompts guided by training data in the form of prompts tried and the functional accuracy of the generated code over a set of given test cases. The key insight is to perform BO in continuous embedding space by using an auxiliary LLM to bridge the gap between discrete prompt space and continuous embedding space. We leverage two synergistic ideas, namely, random projections and dimensionality scaled priors, to build effective Gaussian process based surrogate models over the high-dimensional embedding space. Our experiments on the HumanEval+ benchmark using multiple base LLMs show that BODE-GEN can improve performance in terms of code generation accuracy compared to fixed prompts and manual prompt engineering. Additionally, we demonstrate that BODE-GEN is sample-efficient, requiring relatively few iterations of BO to demonstrate improvements in code accuracy.

</details>


### [66] [Aligning Academia with Industry: An Empirical Study of Industrial Needs and Academic Capabilities in AI-Driven Software Engineering](https://arxiv.org/abs/2512.15148)
*Hang Yu,Yuzhou Lai,Li Zhang,Xiaoli Lian,Fang Liu,Yanrui Dong,Ting Zhang,Zhi Jin,David Lo*

Main category: cs.SE

TL;DR: 该研究通过系统分析1367篇顶级软件工程会议论文（2022-2025）和17个组织的282份工业调查，揭示了学术研究与工业需求之间的差距，并提出了七个关键启示来指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在重塑软件工程领域，但学术研究的进展与工业实际需求之间的对齐程度不明确。研究旨在通过系统分析学术论文和工业调查来弥合这一差距。

Method: 1. 对FSE、ASE和ICSE会议在2022-2025年间发表的1367篇论文进行系统分析，识别研究主题、常用基准、工业相关性和开源可用性；2. 在17个组织中进行实证调查，收集282份关于六个关键主题（程序分析、自动化测试、代码生成/补全、问题解决、预训练代码模型、依赖管理）的结构化问卷反馈。

Result: 通过对比学术能力与工业反馈，发现了七个关键启示：软件需求和架构方面的挑战未得到充分解决、智能软件工程方法的可靠性和可解释性问题、学术研究的输入假设、实际评估中的矛盾以及伦理考虑。

Conclusion: 研究旨在重新聚焦学术界对这些重要但未充分探索问题的关注，并指导未来软件工程研究朝着更大的工业影响力方向发展。

Abstract: The rapid advancement of large language models (LLMs) is fundamentally reshaping software engineering (SE), driving a paradigm shift in both academic research and industrial practice. While top-tier SE venues continue to show sustained or emerging focus on areas like automated testing and program repair, with researchers worldwide reporting continuous performance gains, the alignment of these academic advances with real industrial needs remains unclear. To bridge this gap, we first conduct a systematic analysis of 1,367 papers published in FSE, ASE, and ICSE between 2022 and 2025, identifying key research topics, commonly used benchmarks, industrial relevance, and open-source availability. We then carry out an empirical survey across 17 organizations, collecting 282 responses on six prominent topics, i.e., program analysis, automated testing, code generation/completion, issue resolution, pre-trained code models, and dependency management, through structured questionnaires. By contrasting academic capabilities with industrial feedback, we derive seven critical implications, highlighting under-addressed challenges in software requirements and architecture, the reliability and explainability of intelligent SE approaches, input assumptions in academic research, practical evaluation tensions, and ethical considerations. This study aims to refocus academic attention on these important yet under-explored problems and to guide future SE research toward greater industrial impact.

</details>


### [67] [On Assessing the Relevance of Code Reviews Authored by Generative Models](https://arxiv.org/abs/2512.15466)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 提出基于多主观排名的代码审查评估方法，发现ChatGPT生成的评论质量显著优于人类评论，甚至超过StackExchange的采纳答案


<details>
  <summary>Details</summary>
Motivation: 现有代码审查生成评估方法存在局限：要么依赖与单一标准答案的自动比较，无法捕捉人类观点的多样性；要么依赖"有用性"的主观评估，这个概念过于模糊

Method: 提出多主观排名评估方法，使用CodeReview StackExchange的280个独立代码审查请求和对应评论数据集，让多位人类评委对ChatGPT生成的评论和平台上的顶级人类回答进行质量排名

Result: ChatGPT的评论排名显著优于人类评论，甚至超过了StackExchange的采纳答案

Conclusion: 提出的方法能够对生成式AI在代码审查中的性能进行更有意义的评估，同时提高对无限制集成到审查流程中潜在风险的认识

Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.

</details>


### [68] [How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?](https://arxiv.org/abs/2512.15468)
*Hua Yang,Alejandro Velasco,Thanh Le-Cong,Md Nazmul Haque,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 研究发现语义等价的代码转换技术（特别是变量重命名）可以显著降低成员推断检测的有效性，暴露了大型代码语言模型训练中许可证合规性执行的漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练使用大量代码数据（包括开源和私有代码），引发了知识产权合规问题。虽然已有成员推断技术来检测未经授权的代码使用，但这些技术可能被语义等价的代码转换技术规避。

Method: 系统研究语义等价代码转换规则是否可用于规避成员推断检测，评估不同转换规则对模型准确性和MI检测成功率的影响，并进行因果分析验证。

Result: 1. 每个转换规则在最坏情况下仅使模型准确率下降1.5%，转换后的数据集可有效用于微调；2. RenameVariable规则使MI成功率降低10.19%；3. 因果分析确认变量重命名对破坏MI检测具有最强因果效应；4. 组合多个转换不会进一步降低MI有效性。

Conclusion: 语义等价代码转换（特别是变量重命名）可显著削弱成员推断检测，暴露了代码语言模型训练中许可证合规性执行的重大漏洞，转换式混淆技术可能被滥用以规避检测。

Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.

</details>
