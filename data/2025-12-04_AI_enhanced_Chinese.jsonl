{"id": "2512.03053", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.03053", "abs": "https://arxiv.org/abs/2512.03053", "authors": ["Andrew S. Cassidy", "Guillaume Garreau", "Jay Sivagnaname", "Mike Grassi", "Bernard Brezzo", "John V. Arthur", "Dharmendra S. Modha"], "title": "Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation", "comment": "7 pages, 2 figures, 7 tables", "summary": "We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528LLM\u4f5c\u4e3a\u65e0\u635f\u7f16\u7801\u5668/\u89e3\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e90\u57df\u5230\u76ee\u6807\u57df\u518d\u8fd4\u56de\u6e90\u57df\u7684\u8f6c\u6362\u9a8c\u8bc1\uff0c\u663e\u8457\u51cf\u5c11LLM\u7684\u5e7b\u89c9\u548c\u9057\u6f0f\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u4ece\u903b\u8f91\u6761\u4ef6\u8868\u751f\u6210\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\u4ee3\u7801\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5e38\u89c1\u7684\u5e7b\u89c9\u548c\u9057\u6f0f\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u6210\u4ee3\u7801\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u786c\u4ef6\u8bbe\u8ba1\u7b49\u5173\u952e\u9886\u57df\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u8bba\u4e2d\u7684\u65e0\u635f\u538b\u7f29\u601d\u60f3\uff0c\u5c06LLM\u4f5c\u4e3a\u65e0\u635f\u7f16\u7801\u5668\u5c06\u6e90\u57df\u6570\u636e\uff08\u5982\u903b\u8f91\u6761\u4ef6\u8868\uff09\u8f6c\u6362\u4e3a\u76ee\u6807\u57df\uff08\u5982HDL\u4ee3\u7801\uff09\uff0c\u518d\u4f5c\u4e3a\u65e0\u635f\u89e3\u7801\u5668\u5c06\u751f\u6210\u7684\u4ee3\u7801\u8f6c\u6362\u56de\u6e90\u57df\uff0c\u901a\u8fc7\u6bd4\u8f83\u539f\u59cb\u548c\u91cd\u5efa\u7684\u6e90\u57df\u6570\u636e\u6765\u9a8c\u8bc1\u751f\u6210\u8d28\u91cf\u3002", "result": "\u4f7f\u75287\u79cd\u4e0d\u540cLLM\u6210\u529f\u751f\u6210\u4e86\u4e8c\u7ef4\u7247\u4e0a\u7f51\u7edc\u8def\u7531\u5668\u7684\u5b8c\u6574HDL\u4ee3\u7801\uff0813\u4e2a\u5355\u5143\uff0c1500-2000\u884c\u4ee3\u7801\uff09\uff0c\u5e76\u901a\u8fc7\u91cd\u5efa\u9a8c\u8bc1\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u51c6\u786e\u6027\uff0c\u4e0d\u4ec5\u80fd\u591f\u786e\u8ba4\u6b63\u786e\u751f\u6210\u7684\u903b\u8f91\uff0c\u8fd8\u80fd\u68c0\u6d4b\u9519\u8bef\u751f\u6210\u7684\u903b\u8f91\uff0c\u751a\u81f3\u5e2e\u52a9\u5f00\u53d1\u8005\u53d1\u73b0\u8bbe\u8ba1\u89c4\u8303\u9519\u8bef\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u635f\u7f16\u7801/\u89e3\u7801\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86LLM\u7684\u5e7b\u89c9\u548c\u9057\u6f0f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\u548c\u5f00\u53d1\u6548\u7387\uff0c\u4e3aLLM\u5728\u5173\u952e\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "2512.03262", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03262", "abs": "https://arxiv.org/abs/2512.03262", "authors": ["Songwen Zhao", "Danqing Wang", "Kexun Zhang", "Jiaxuan Luo", "Zhuo Li", "Lei Li"], "title": "Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks", "comment": null, "summary": "Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSU S VI B E S\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u7f16\u7a0b\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u529f\u80fd\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\u4e2d\uff0c\u4ec5\u670910.5%\u662f\u5b89\u5168\u7684\uff0c\u5bf9vibe coding\u5728\u751f\u4ea7\u73af\u5883\u7684\u5b89\u5168\u6027\u63d0\u51fa\u4e25\u91cd\u8b66\u544a\u3002", "motivation": "\u968f\u7740vibe coding\uff08\u4eba\u7c7b\u5de5\u7a0b\u5e08\u6307\u5bfcLLM\u4ee3\u7406\u5b8c\u6210\u590d\u6742\u7f16\u7801\u4efb\u52a1\uff09\u7684\u666e\u53ca\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u79cd\u7f16\u7a0b\u8303\u5f0f\u8f93\u51fa\u7684\u4ee3\u7801\u662f\u5426\u771f\u7684\u5b89\u5168\uff0c\u7279\u522b\u662f\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faSU S VI B E S\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b200\u4e2a\u6765\u81ea\u771f\u5b9e\u5f00\u6e90\u9879\u76ee\u7684\u529f\u80fd\u8bf7\u6c42\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5728\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5b9e\u73b0\u65f6\u5bfc\u81f4\u4e86\u6f0f\u6d1e\u3002\u4f7f\u7528\u524d\u6cbf\u6a21\u578b\u8bc4\u4f30\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f16\u7a0b\u4ee3\u7406\uff0c\u5e76\u6d4b\u8bd5\u521d\u6b65\u5b89\u5168\u7b56\u7565\uff08\u5982\u6dfb\u52a0\u6f0f\u6d1e\u63d0\u793a\uff09\u7684\u6709\u6548\u6027\u3002", "result": "\u6240\u6709\u7f16\u7a0b\u4ee3\u7406\u5728\u8f6f\u4ef6\u5b89\u5168\u65b9\u9762\u8868\u73b0\u90fd\u5f88\u5dee\u3002\u867d\u7136SWE-Agent with Claude 4 Sonnet\u768461%\u89e3\u51b3\u65b9\u6848\u529f\u80fd\u6b63\u786e\uff0c\u4f46\u53ea\u670910.5%\u662f\u5b89\u5168\u7684\u3002\u521d\u6b65\u5b89\u5168\u7b56\u7565\u65e0\u6cd5\u7f13\u89e3\u8fd9\u4e9b\u5b89\u5168\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9vibe coding\u5728\u5b89\u5168\u654f\u611f\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\u63d0\u51fa\u4e86\u4e25\u91cd\u5173\u5207\uff0c\u8868\u660e\u5f53\u524dLLM\u7f16\u7a0b\u4ee3\u7406\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u91cd\u5927\u7f3a\u9677\u3002", "topic": "swe benchmark"}}
{"id": "2512.03421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.03421", "abs": "https://arxiv.org/abs/2512.03421", "authors": ["Hexiang Xu", "Hengyuan Liu", "Yonghao Wu", "Xiaolan Kang", "Xiang Chen", "Yong Liu"], "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization", "comment": "The paper has been accepted for publication in The Journal of Systems & Software", "summary": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8613\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6545\u969c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u5148\u8fdb\u6a21\u578b\uff08\u5982OpenAI o3\u548cDeepSeekR1\uff09\u5728\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u3002\u867d\u7136LLMs\u5728\u7b80\u5355\u6545\u969c\u5b9a\u4f4d\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u968f\u7740\u95ee\u9898\u96be\u5ea6\u589e\u52a0\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u4e14\u5b58\u5728\u8fc7\u5ea6\u63a8\u7406\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "motivation": "\u65b0\u624b\u7a0b\u5e8f\u5458\u7531\u4e8e\u7ecf\u9a8c\u6709\u9650\uff0c\u5728\u6545\u969c\u5b9a\u4f4d\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u4f20\u7edf\u7684\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\uff08\u5982SBFL\u548cMBFL\uff09\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5bf9\u521d\u5b66\u8005\u6548\u679c\u6709\u9650\u3002\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u901a\u8fc7\u7406\u89e3\u7a0b\u5e8f\u8bed\u6cd5\u548c\u8bed\u4e49\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e866\u4e2a\u95ed\u6e90\u548c7\u4e2a\u5f00\u6e90LLMs\uff0c\u4f7f\u7528Codeflaws\u3001Condefects\u548cBugT\u4e09\u4e2a\u6570\u636e\u96c6\u3002BugT\u662f\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u51cf\u8f7b\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u6545\u969c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u7279\u522b\u5173\u6ce8\u63a8\u7406\u80fd\u529b\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u5148\u8fdb\u6a21\u578b\uff08\u5982OpenAI o3\u548cDeepSeekR1\uff09\u5728\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u63d0\u793a\u5de5\u7a0b\u7684\u4f9d\u8d56\u6700\u5c0f\u3002\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\uff08\u5982GPT-4\uff09\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u6765\u7ef4\u6301\u6027\u80fd\u3002LLMs\u5728\u7b80\u5355\u6545\u969c\u5b9a\u4f4d\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u968f\u7740\u95ee\u9898\u96be\u5ea6\u589e\u52a0\u51c6\u786e\u6027\u4e0b\u964d\u3002\u8fc7\u5ea6\u63a8\u7406\u548c\u8ba1\u7b97\u6210\u672c\u662f\u4e3b\u8981\u6311\u6218\u3002\u65b0\u624b\u7a0b\u5e8f\u5458\u5bf9LLMs\u7684\u89e3\u91ca\u7ed9\u4e88\u9ad8\u5ea6\u8bc4\u4ef7\u3002", "conclusion": "LLMs\u5728\u63d0\u9ad8\u8c03\u8bd5\u6548\u7387\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5728\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8fdb\u4e00\u6b65\u6539\u8fdb\u624d\u80fd\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u3002\u7814\u7a76\u5f3a\u8c03\u4e86LLMs\u5bf9\u65b0\u624b\u7a0b\u5e8f\u5458\u8f85\u52a9\u7684\u91cd\u8981\u4ef7\u503c\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "topic": "code agent"}}
{"id": "2512.03272", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03272", "abs": "https://arxiv.org/abs/2512.03272", "authors": ["Zhiyuan He", "Dingmin Wang"], "title": "When Do Symbolic Solvers Enhance Reasoning in Large Language Models?", "comment": null, "summary": "Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models \"overthink\" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.", "AI": {"tldr": "\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u4ec5\u5bf9\u9700\u8981\u6709\u9650\u9690\u5f0f\u63a8\u7406\u4f46\u6d89\u53ca\u5145\u8db3\u641c\u7d22\u7a7a\u95f4\u7684\u95ee\u9898\u6709\u5e2e\u52a9\uff0c\u800c\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u65b9\u6cd5\u5728\u6df1\u5ea6\u63a8\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u751f\u6210\u957f\u601d\u7ef4\u94fe\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u4ea7\u751f\u5927\u91cftoken\u5f00\u9500\uff0c\u4e14\u6a21\u578b\u53ef\u80fd\"\u8fc7\u5ea6\u601d\u8003\"\u4ea7\u751f\u5197\u957f\u63a8\u7406\u94fe\u751a\u81f3\u9519\u8bef\u7b54\u6848\u3002\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u5229\u7528LLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5c06\u63a8\u7406\u4efb\u52a1\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u7136\u540e\u7528\u7b26\u53f7\u6c42\u89e3\u5668\u89e3\u51b3\uff0c\u4f46\u4f55\u65f6\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u65b9\u6cd5\u80fd\u88ab\u7b26\u53f7\u6c42\u89e3\u5668\u589e\u5f3a\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u63a2\u7d22\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u4f55\u65f6\u80fd\u589e\u5f3a\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u65b9\u6cd5\u3002\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\u9700\u8981\u6709\u9650\u9690\u5f0f\u63a8\u7406\u4f46\u6d89\u53ca\u5145\u8db3\u641c\u7d22\u7a7a\u95f4\u7684\u95ee\u9898\u3001\u6f14\u7ece\u95ee\u9898\u3001\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u7b49\u3002", "result": "\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u4ec5\u5728\u95ee\u9898\u9700\u8981\u6709\u9650\u9690\u5f0f\u63a8\u7406\u4f46\u6d89\u53ca\u5145\u8db3\u641c\u7d22\u7a7a\u95f4\u65f6\u6709\u6548\u3002\u6700\u65b0LLM\uff08\u5982GPT-4o\uff09\u5728\u63a8\u7406\u6df1\u5ea6\u8f83\u6d45\u7684\u6f14\u7ece\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u9700\u8981\u91cd\u590d\u56de\u6eaf\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u3002\u5f53\u63d0\u4f9b\u58f0\u660e\u6027\u793a\u4f8b\u65f6\uff0c\u5373\u4f7f\u662fCodeLlama-13B\u4e5f\u80fd\u5728\u56f0\u96be\u7684\u6591\u9a6c\u8c1c\u9898\u4e0a\u8d85\u8d8aGPT-4o\u3002", "conclusion": "\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u4e0e\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\u9886\u57df\uff0c\u5e94\u6839\u636e\u95ee\u9898\u7c7b\u578b\u9009\u62e9\u5408\u9002\u7684\u63a8\u7406\u65b9\u6cd5\u3002\u5bf9\u4e8e\u9700\u8981\u6709\u9650\u9690\u5f0f\u63a8\u7406\u4f46\u6d89\u53ca\u5927\u91cf\u641c\u7d22\u7a7a\u95f4\u7684\u95ee\u9898\uff0c\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u66f4\u6709\u6548\uff1b\u5bf9\u4e8e\u6df1\u5ea6\u63a8\u7406\u95ee\u9898\uff0c\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "topic": "code agent"}}
{"id": "2512.03293", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.03293", "abs": "https://arxiv.org/abs/2512.03293", "authors": ["Filippo Torresan", "Ryota Kanai", "Manuel Baltieri"], "title": "Prior preferences in active inference agents: soft, hard, and goal shaping", "comment": "41 pages, 23 figures", "summary": "Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e3b\u52a8\u63a8\u7406\u4e2d\u56db\u79cd\u504f\u597d\u5206\u5e03\u5b9a\u4e49\u65b9\u5f0f\uff08\u786c\u76ee\u6807vs\u8f6f\u76ee\u6807\u3001\u6709\u65e0\u76ee\u6807\u5851\u9020\uff09\u5728\u7f51\u683c\u4e16\u754c\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u76ee\u6807\u5851\u9020\u80fd\u63d0\u5347\u6027\u80fd\u4f46\u4f1a\u727a\u7272\u73af\u5883\u52a8\u6001\u5b66\u4e60\u3002", "motivation": "\u4e3b\u52a8\u63a8\u7406\u4f7f\u7528\u671f\u671b\u81ea\u7531\u80fd\u4f5c\u4e3a\u89c4\u5212\u51b3\u7b56\u76ee\u6807\uff0c\u4f46\u504f\u597d\u5206\u5e03\u5982\u4f55\u6307\u5b9a\u53ca\u5176\u5bf9\u63a8\u7406\u5b66\u4e60\u7684\u5f71\u54cd\u5728\u6587\u732e\u4e2d\u5f88\u5c11\u88ab\u5173\u6ce8\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u504f\u597d\u5206\u5e03\u5b9a\u4e49\u65b9\u5f0f\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u8003\u8651\u4e86\u56db\u79cd\u504f\u597d\u5206\u5e03\u5b9a\u4e49\u65b9\u5f0f\uff1a\u786c\u76ee\u6807vs\u8f6f\u76ee\u6807\u3001\u6709\u65e0\u76ee\u6807\u5851\u9020\uff08\u4e2d\u95f4\u76ee\u6807\uff09\u3002\u5728\u7f51\u683c\u4e16\u754c\u5bfc\u822a\u4efb\u52a1\u4e2d\u6bd4\u8f83\u4e86\u56db\u79cd\u667a\u80fd\u4f53\u7684\u8868\u73b0\uff0c\u6bcf\u79cd\u667a\u80fd\u4f53\u4f7f\u7528\u4e00\u79cd\u504f\u597d\u5206\u5e03\u3002", "result": "\u76ee\u6807\u5851\u9020\u80fd\u5e26\u6765\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff08\u4fc3\u8fdb\u5229\u7528\uff09\uff0c\u4f46\u4f1a\u727a\u7272\u5bf9\u73af\u5883\u8f6c\u79fb\u52a8\u6001\u7684\u5b66\u4e60\uff08\u963b\u788d\u63a2\u7d22\uff09\u3002", "conclusion": "\u504f\u597d\u5206\u5e03\u7684\u5b9a\u4e49\u65b9\u5f0f\u663e\u8457\u5f71\u54cd\u4e3b\u52a8\u63a8\u7406\u667a\u80fd\u4f53\u7684\u6027\u80fd\u8868\u73b0\uff0c\u9700\u8981\u5728\u5229\u7528\u548c\u63a2\u7d22\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2512.03318", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03318", "abs": "https://arxiv.org/abs/2512.03318", "authors": ["Chandler Smith", "Marwa Abdulhai", "Manfred Diaz", "Marko Tesic", "Rakshit S. Trivedi", "Alexander Sasha Vezhnevets", "Lewis Hammond", "Jesse Clifton", "Minsuk Chang", "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n", "John P. Agapiou", "Jayd Matyas", "Danny Karmon", "Akash Kundu", "Aliaksei Korshuk", "Ananya Ananya", "Arrasy Rahman", "Avinaash Anand Kulandaivel", "Bain McHale", "Beining Zhang", "Buyantuev Alexander", "Carlos Saith Rodriguez Rojas", "Caroline Wang", "Chetan Talele", "Chenao Liu", "Chichen Lin", "Diana Riazi", "Di Yang Shi", "Emanuel Tewolde", "Elizaveta Tennant", "Fangwei Zhong", "Fuyang Cui", "Gang Zhao", "Gema Parre\u00f1o Piqueras", "Hyeonggeun Yun", "Ilya Makarov", "Jiaxun Cui", "Jebish Purbey", "Jim Dilkes", "Jord Nguyen", "Lingyun Xiao", "Luis Felipe Giraldo", "Manuela Chacon-Chamorro", "Manuel Sebastian Rios Beltran", "Marta Emili Garc\u00eda Segura", "Mengmeng Wang", "Mogtaba Alim", "Nicanor Quijano", "Nico Schiavone", "Olivia Macmillan-Scott", "Oswaldo Pe\u00f1a", "Peter Stone", "Ram Mohan Rao Kadiyala", "Rolando Fernandez", "Ruben Manrique", "Sunjia Lu", "Sheila A. McIlraith", "Shamika Dhuri", "Shuqing Shi", "Siddhant Gupta", "Sneheel Sarangi", "Sriram Ganapathi Subramanian", "Taehun Cha", "Toryn Q. Klassen", "Wenming Tu", "Weijian Fan", "Wu Ruiyang", "Xue Feng", "Yali Du", "Yang Liu", "Yiding Wang", "Yipeng Kang", "Yoonchang Sung", "Yuxuan Chen", "Zhaowei Zhang", "Zhihan Wang", "Zhiqiang Wu", "Ziang Chen", "Zilong Zheng", "Zixia Jia", "Ziyan Wang", "Dylan Hadfield-Menell", "Natasha Jaques", "Tim Baarslag", "Jose Hernandez-Orallo", "Joel Z. Leibo"], "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "comment": "Published at NeurIPS Datasets and Benchmarks 2025, 10 pages", "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u96f6\u6837\u672c\u6df7\u5408\u52a8\u673a\u73af\u5883\u4e2d\u5408\u4f5c\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528Concordia\u81ea\u7136\u8bed\u8a00\u591a\u667a\u80fd\u4f53\u6a21\u62df\u73af\u5883\uff0c\u901a\u8fc7NeurIPS 2024\u7ade\u8d5b\u53d1\u73b0\u5f53\u524d\u667a\u80fd\u4f53\u5728\u8bf4\u670d\u548c\u89c4\u8303\u6267\u884c\u7b49\u9700\u8981\u9c81\u68d2\u6cdb\u5316\u7684\u5408\u4f5c\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u793e\u4f1a\u4ea4\u4e92\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u5e76\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u4e0e\u4eba\u7c7b\u548c\u5176\u4ed6\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u573a\u666f\u4e2d\u3002\u7136\u800c\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u8861\u91cf\u8fd9\u4e9b\u80fd\u529b\u5728\u65b0\u9896\u793e\u4ea4\u60c5\u5883\u4e2d\u7684\u6cdb\u5316\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u52a8\u673a\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u80fd\u529b\u3002", "method": "\u5f15\u5165\u57fa\u4e8eConcordia\u81ea\u7136\u8bed\u8a00\u591a\u667a\u80fd\u4f53\u6a21\u62df\u73af\u5883\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u667a\u80fd\u4f53\u5728\u96f6\u6837\u672c\u3001\u6df7\u5408\u52a8\u673a\u73af\u5883\u4e2d\u4e0e\u591a\u6837\u5316\u4f19\u4f34\u548c\u60c5\u5883\u4e0b\u7684\u5408\u4f5c\u80fd\u529b\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5229\u7528\u4e92\u5229\u673a\u4f1a\u6765\u8861\u91cf\u901a\u7528\u5408\u4f5c\u667a\u80fd\u3002", "result": "NeurIPS 2024 Concordia\u7ade\u8d5b\u7684\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u667a\u80fd\u4f53\u80fd\u529b\u4e0e\u53ef\u9760\u5408\u4f5c\u6240\u9700\u7684\u9c81\u68d2\u6cdb\u5316\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8bf4\u670d\u548c\u89c4\u8303\u6267\u884c\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u793e\u4ea4\u60c5\u5883\u4e2d\u7684\u5408\u4f5c\u80fd\u529b\u4ecd\u9700\u5927\u5e45\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9002\u5e94\u65b0\u4f19\u4f34\u548c\u73af\u5883\u3001\u8fdb\u884c\u6709\u6548\u8bf4\u670d\u548c\u6267\u884c\u793e\u4f1a\u89c4\u8303\u7684\u573a\u666f\u4e2d\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u91cd\u8981\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2512.03438", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03438", "abs": "https://arxiv.org/abs/2512.03438", "authors": ["Reuben Tan", "Baolin Peng", "Zhengyuan Yang", "Hao Cheng", "Oier Mees", "Theodore Zhao", "Andrea Tupini", "Isar Meijier", "Qianhui Wu", "Yuncong Yang", "Lars Liden", "Yu Gu", "Sheng Zhang", "Xiaodong Liu", "Lijuan Wang", "Marc Pollefeys", "Yong Jae Lee", "Jianfeng Gao"], "title": "Multimodal Reinforcement Learning with Agentic Verifier for AI Agents", "comment": null, "summary": "Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.", "AI": {"tldr": "Argos\u662f\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u7684\u5956\u52b1\u4ee3\u7406\uff0c\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u8bc4\u5206\u51fd\u6570\u6765\u540c\u65f6\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u3001\u65f6\u7a7a\u5b9a\u4f4d\u548c\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u7684\u57fa\u4e8e\u6700\u7ec8\u7ed3\u679c\u7684\u5956\u52b1\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u7ec6\u7c92\u5ea6\u6307\u5bfc\u3002\u4e0d\u540c\u6837\u672c\u9700\u8981\u4e0d\u540c\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u4e14\u6559\u5e08\u6a21\u578b\u53ef\u80fd\u63d0\u4f9b\u566a\u58f0\u5956\u52b1\u4fe1\u53f7\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u667a\u80fd\u7684\u5956\u52b1\u673a\u5236\u3002", "method": "\u63d0\u51faArgos\u5956\u52b1\u4ee3\u7406\uff0c\u4e3a\u6bcf\u4e2a\u6837\u672c\u4ece\u6559\u5e08\u6a21\u578b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u8bc4\u5206\u51fd\u6570\u6c60\u4e2d\u9009\u62e9\u5408\u9002\u7684\u51fd\u6570\uff0c\u540c\u65f6\u8bc4\u4f30\uff1a(1)\u6700\u7ec8\u54cd\u5e94\u51c6\u786e\u6027\uff0c(2)\u5b9e\u4f53\u548c\u52a8\u4f5c\u7684\u65f6\u7a7a\u5b9a\u4f4d\uff0c(3)\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\u3002", "result": "\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u89c6\u89c9\u5e7b\u89c9\u4ee5\u53ca\u673a\u5668\u4eba\u548c\u5177\u8eabAI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u8bc1\u660e\u4ec5\u4f9d\u8d56SFT\u540e\u8bad\u7ec3\u4e0d\u8db3\uff0c\u9700\u8981\u5728\u7ebf\u9a8c\u8bc1\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u5230\u975e\u63a5\u5730\u89e3\uff0c\u5e76\u80fd\u51cf\u5c11\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u3002", "conclusion": "Argos\u901a\u8fc7\u667a\u80fd\u5956\u52b1\u9009\u62e9\u673a\u5236\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u63d0\u4f9b\u7406\u8bba\u4e0a\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u8bc1\u660e\uff0c\u4e3a\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.03065", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03065", "abs": "https://arxiv.org/abs/2512.03065", "authors": ["Nihir Chadderwala"], "title": "Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning", "comment": null, "summary": "Generative AI agents in life sciences face a critical challenge: determining the optimal approach for diverse queries ranging from simple factoid questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, neither of which adapts to changing conditions or user preferences. We present a novel framework that combines AWS Strands Agents with Thompson Sampling contextual bandits to enable AI agents to learn optimal decision-making strategies from user feedback alone. Our system optimizes three key dimensions: generation strategy selection (direct vs. chain-of-thought), tool selection (literature search, drug databases, etc.), and domain routing (pharmacology, molecular biology, clinical specialists). Through empirical evaluation on life science queries, we demonstrate 15-30\\% improvement in user satisfaction compared to random baselines, with clear learning patterns emerging after 20-30 queries. Our approach requires no ground truth labels, adapts continuously to user preferences, and provides a principled solution to the exploration-exploitation dilemma in agentic AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408AWS Strands Agents\u4e0eThompson Sampling\u4e0a\u4e0b\u6587bandits\u7684\u6846\u67b6\uff0c\u8ba9AI\u4ee3\u7406\u901a\u8fc7\u7528\u6237\u53cd\u9988\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7b56\u7565\uff0c\u5728\u751f\u547d\u79d1\u5b66\u9886\u57df\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea615-30%", "motivation": "\u751f\u547d\u79d1\u5b66\u4e2d\u7684\u751f\u6210\u5f0fAI\u4ee3\u7406\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u5982\u4f55\u4e3a\u4ece\u7b80\u5355\u4e8b\u5b9e\u95ee\u9898\u5230\u590d\u6742\u673a\u5236\u63a8\u7406\u7684\u591a\u6837\u5316\u67e5\u8be2\u786e\u5b9a\u6700\u4f18\u65b9\u6cd5\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u89c4\u5219\u6216\u6602\u8d35\u7684\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0c\u90fd\u65e0\u6cd5\u9002\u5e94\u53d8\u5316\u6761\u4ef6\u6216\u7528\u6237\u504f\u597d\u3002", "method": "\u7ed3\u5408AWS Strands Agents\u4e0eThompson Sampling\u4e0a\u4e0b\u6587bandits\u7684\u6846\u67b6\uff0c\u8ba9AI\u4ee3\u7406\u4ec5\u4ece\u7528\u6237\u53cd\u9988\u4e2d\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7b56\u7565\u3002\u7cfb\u7edf\u4f18\u5316\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u751f\u6210\u7b56\u7565\u9009\u62e9\uff08\u76f4\u63a5vs.\u94fe\u5f0f\u601d\u8003\uff09\u3001\u5de5\u5177\u9009\u62e9\uff08\u6587\u732e\u641c\u7d22\u3001\u836f\u7269\u6570\u636e\u5e93\u7b49\uff09\u3001\u9886\u57df\u8def\u7531\uff08\u836f\u7406\u5b66\u3001\u5206\u5b50\u751f\u7269\u5b66\u3001\u4e34\u5e8a\u4e13\u5bb6\uff09\u3002", "result": "\u5728\u751f\u547d\u79d1\u5b66\u67e5\u8be2\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u968f\u673a\u57fa\u7ebf\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u63d0\u534715-30%\uff0c\u572820-30\u4e2a\u67e5\u8be2\u540e\u51fa\u73b0\u6e05\u6670\u7684\u5b66\u4e60\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff0c\u80fd\u6301\u7eed\u9002\u5e94\u7528\u6237\u504f\u597d\uff0c\u4e3a\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u56f0\u5883\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.03549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03549", "abs": "https://arxiv.org/abs/2512.03549", "authors": ["Yuki Orimo", "Iori Kurata", "Hodaka Mori", "Ryuhei Okuno", "Ryohto Sawada", "Daisuke Okanohara"], "title": "PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks", "comment": null, "summary": "We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.", "AI": {"tldr": "PARC\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u6267\u884c\u957f\u65f6\u7a0b\u8ba1\u7b97\u4efb\u52a1\u7684\u7f16\u7801\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u542b\u4efb\u52a1\u89c4\u5212\u3001\u6267\u884c\u548c\u81ea\u6211\u8bc4\u4f30\u53cd\u9988\u673a\u5236\uff0c\u80fd\u591f\u5728\u6750\u6599\u79d1\u5b66\u548cKaggle\u7ade\u8d5b\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u5de5\u4f5c\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u6267\u884c\u957f\u65f6\u7a0b\u3001\u590d\u6742\u7684\u8ba1\u7b97\u4efb\u52a1\u65f6\uff0c\u5f80\u5f80\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\uff0c\u96be\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u4e3b\u5de5\u4f5c\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u6211\u8bc4\u4f30\u3001\u81ea\u6211\u4fee\u6b63\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u6765\u5b8c\u6210\u5927\u89c4\u6a21\u79d1\u5b66\u548c\u5206\u6790\u5de5\u4f5c\u3002", "method": "PARC\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u542b\u4efb\u52a1\u89c4\u5212\u3001\u6267\u884c\u548c\u81ea\u6211\u8bc4\u4f30\u53cd\u9988\u673a\u5236\u3002\u7cfb\u7edf\u80fd\u591f\u4ece\u72ec\u7acb\u4e0a\u4e0b\u6587\u8bc4\u4f30\u81ea\u8eab\u884c\u4e3a\u548c\u7ed3\u679c\uff0c\u63d0\u4f9b\u81ea\u6211\u8bc4\u4f30\u548c\u81ea\u6211\u53cd\u9988\uff0c\u4ece\u800c\u68c0\u6d4b\u548c\u4fee\u6b63\u9ad8\u5c42\u6218\u7565\u9519\u8bef\u3002", "result": "\u5728\u6750\u6599\u79d1\u5b66\u4efb\u52a1\u4e2d\uff0cPARC\u6210\u529f\u590d\u73b0\u4e86\u9502\u79bb\u5b50\u4f20\u5bfc\u548c\u5408\u91d1\u504f\u6790\u7814\u7a76\u7684\u5173\u952e\u7ed3\u679c\uff0c\u534f\u8c03\u4e86\u6570\u5341\u4e2a\u5e76\u884c\u6a21\u62df\u4efb\u52a1\uff08\u6bcf\u4e2a\u7ea643\u5c0f\u65f6\u8ba1\u7b97\u65f6\u95f4\uff09\u3002\u5728Kaggle\u5b9e\u9a8c\u4e2d\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5f00\u59cb\uff0cPARC\u8fdb\u884c\u6570\u636e\u5206\u6790\u548c\u5b9e\u73b0\u641c\u7d22\u7b56\u7565\uff0c\u4ea7\u751f\u4e86\u4e0e\u4eba\u5de5\u8bbe\u8ba1\u57fa\u7ebf\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u81ea\u6211\u8bc4\u4f30\u3001\u81ea\u6211\u53cd\u9988\u673a\u5236\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0AI\u7cfb\u7edf\u5728\u72ec\u7acb\u3001\u5927\u89c4\u6a21\u79d1\u5b66\u548c\u5206\u6790\u5de5\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2512.03560", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03560", "abs": "https://arxiv.org/abs/2512.03560", "authors": ["Gianni Molinari", "Fabio Ciravegna"], "title": "Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks", "comment": "11 pages, 1 figure, 2 tables, Workshop AAAI 2026 agentic AI Benchmarks and Applications for Enterprise Tasks", "summary": "Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.", "AI": {"tldr": "RP-ReAct\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5c06\u6218\u7565\u89c4\u5212\u4e0e\u4f4e\u7ea7\u6267\u884c\u89e3\u8026\uff0c\u901a\u8fc7Reasoner Planner Agent\u8fdb\u884c\u89c4\u5212\u5206\u6790\uff0cProxy-Execution Agent\u6267\u884c\u5de5\u5177\u4ea4\u4e92\uff0c\u5e76\u91c7\u7528\u4e0a\u4e0b\u6587\u4fdd\u5b58\u7b56\u7565\u7ba1\u7406\u5927\u578b\u5de5\u5177\u8f93\u51fa\uff0c\u5728ToolQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u9886\u57df\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u5355\u667a\u80fd\u4f53\u67b6\u6784\u5bfc\u81f4\u8f68\u8ff9\u4e0d\u7a33\u5b9a\uff0c\u4ee5\u53ca\u672c\u5730\u5f00\u6e90\u6a21\u578b\u7684\u5c0f\u4e0a\u4e0b\u6587\u7a97\u53e3\u65e0\u6cd5\u5904\u7406\u5927\u578b\u5de5\u5177\u8f93\u51fa\u3002\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRP-ReAct\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542bReasoner Planner Agent\uff08RPA\uff09\u8d1f\u8d23\u89c4\u5212\u548c\u5206\u6790\uff0cProxy-Execution Agent\uff08PEA\uff09\u8d1f\u8d23\u5de5\u5177\u4ea4\u4e92\u6267\u884c\uff0c\u5e76\u91c7\u7528\u4e0a\u4e0b\u6587\u4fdd\u5b58\u7b56\u7565\u901a\u8fc7\u5916\u90e8\u5b58\u50a8\u7ba1\u7406\u5927\u578b\u5de5\u5177\u8f93\u51fa\u3002", "result": "\u5728ToolQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u516d\u79cd\u5f00\u6e90\u63a8\u7406\u6a21\u578b\u8bc4\u4f30\uff0cRP-ReAct\u5728\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u90fd\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "RP-ReAct\u901a\u8fc7\u89e3\u8026\u89c4\u5212\u4e0e\u6267\u884c\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f01\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u4efb\u52a1\u7684\u534f\u8c03\u95ee\u9898\uff0c\u4e3a\u53ef\u90e8\u7f72\u7684\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2512.03571", "categories": ["cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.03571", "abs": "https://arxiv.org/abs/2512.03571", "authors": ["Zhening Li", "Armando Solar-Lezama", "Yisong Yue", "Stephan Zheng"], "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "comment": "65 pages, 2 figures, published in NeurIPS 2025", "summary": "We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce \"probabilistic angelic nondeterminism\" (\"PAN\"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.", "AI": {"tldr": "\u63d0\u51faPAN\u7f16\u7a0b\u6a21\u578b\uff0c\u5206\u79bb\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u903b\u8f91\u4e0e\u63a8\u7406\u7b56\u7565\uff0c\u901a\u8fc7EnCompass\u6846\u67b6\u5b9e\u73b0\uff0c\u53ef\u5feb\u901f\u63d0\u5347\u667a\u80fd\u4f53\u53ef\u9760\u6027\u5e76\u7075\u6d3b\u5207\u6362\u63a8\u7406\u7b56\u7565", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7f16\u7a0b\u65b9\u6cd5\u901a\u5e38\u5c06\u6838\u5fc3\u5de5\u4f5c\u6d41\u903b\u8f91\u4e0e\u63a8\u7406\u65f6\u7b56\u7565\uff08\u5982\u6811\u641c\u7d22\uff09\u8026\u5408\u5728\u4e00\u8d77\uff0c\u8fd9\u79cd\u8026\u5408\u9650\u5236\u4e86\u7f16\u7a0b\u7075\u6d3b\u6027\u548c\u5b9e\u9a8c\u6548\u7387", "method": "\u63d0\u51fa\"\u6982\u7387\u5929\u4f7f\u975e\u786e\u5b9a\u6027\"\uff08PAN\uff09\u7f16\u7a0b\u6a21\u578b\uff0c\u4f7f\u7528Python\u88c5\u9970\u5668\u5c06\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u5e8f\u7f16\u8bd1\u4e3a\u641c\u7d22\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5de5\u4f5c\u6d41\u4e0e\u63a8\u7406\u7b56\u7565\u7684\u89e3\u8026", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u8ba9\u7a0b\u5e8f\u5458\u5feb\u901f\u63d0\u5347\u667a\u80fd\u4f53\u53ef\u9760\u6027\uff0c\u8f7b\u677e\u5207\u6362\u4e0d\u540c\u63a8\u7406\u7b56\u7565\uff0c\u4e14\u53ea\u9700\u5c11\u91cf\u989d\u5916\u7f16\u7801", "conclusion": "PAN\u6a21\u578b\u548cEnCompass\u6846\u67b6\u4e3a\u667a\u80fd\u4f53\u7f16\u7a0b\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u53ef\u5b9e\u9a8c\u7684\u65b9\u6cd5\uff0c\u89e3\u8026\u4e86\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u4e0e\u63a8\u7406\u7b56\u7565\u9009\u62e9", "topic": "code agent"}}
{"id": "2512.03442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03442", "abs": "https://arxiv.org/abs/2512.03442", "authors": ["Xingrun Xing", "Zhiyuan Fan", "Jie Lou", "Guoqi Li", "Jiajun Zhang", "Debing Zhang"], "title": "PretrainZero: Reinforcement Active Pretraining", "comment": null, "summary": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.", "AI": {"tldr": "PretrainZero\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u6599\u7684\u5f3a\u5316\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u8bc6\u522b\u4fe1\u606f\u5185\u5bb9\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u9a8c\u8bc1\u6269\u5c55\uff0c\u5c06RL\u4ece\u9886\u57df\u7279\u5b9a\u7684\u540e\u8bad\u7ec3\u6269\u5c55\u5230\u901a\u7528\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5927\u601d\u8003\u6a21\u578b\u867d\u7136\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u8f6f\u4ef6\u548c\u6570\u5b66\uff09\u5c55\u73b0\u51fa\u4e13\u5bb6\u7ea7\u80fd\u529b\uff0c\u4f46\u4ecd\u4e25\u91cd\u4f9d\u8d56\u7279\u5b9a\u9886\u57df\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\uff0c\u8fd9\u9650\u5236\u4e86\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u6269\u5c55\u3002\u9700\u8981\u6253\u7834\u9a8c\u8bc1\u6570\u636e\u58c1\u5792\uff0c\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faPretrainZero\u6846\u67b6\uff1a1\uff09\u4e3b\u52a8\u9884\u8bad\u7ec3\uff1a\u5b66\u4e60\u7edf\u4e00\u63a8\u7406\u7b56\u7565\uff0c\u4e3b\u52a8\u4ece\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u8bc6\u522b\u5408\u7406\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u5185\u5bb9\uff1b2\uff09\u81ea\u76d1\u7763\u5b66\u4e60\uff1a\u65e0\u9700\u53ef\u9a8c\u8bc1\u6807\u7b7e\u3001\u9884\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u6216\u76d1\u7763\u5fae\u8c03\uff0c\u76f4\u63a5\u5728\u901a\u7528\u8bed\u6599\u4e0a\u4f7f\u7528RL\u9884\u8bad\u7ec3\u63a8\u7406\u5668\uff1b3\uff09\u9a8c\u8bc1\u6269\u5c55\uff1a\u901a\u8fc7\u5904\u7406\u8d8a\u6765\u8d8a\u96be\u7684\u63a9\u7801\u8de8\u5ea6\uff0c\u589e\u5f3a\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u5f3a\u5316\u9884\u8bad\u7ec3\u4e2d\uff0cPretrainZero\u5c06Qwen3-4B-Base\u5728MMLU-Pro\u3001SuperGPQA\u548c\u6570\u5b66\u5e73\u5747\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e868.43\u30015.96\u548c10.60\u5206\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u4e5f\u53ef\u4f5c\u4e3a\u4e0b\u6e38RLVR\u4efb\u52a1\u7684\u63a8\u7406\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "PretrainZero\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u4ece\u9886\u57df\u7279\u5b9a\u7684\u540e\u8bad\u7ec3\u6269\u5c55\u5230\u901a\u7528\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u65b9\u6cd5\u6253\u7834\u4e86\u9a8c\u8bc1\u6570\u636e\u58c1\u5792\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u4eba\u5de5\u901a\u7528\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.03783", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.03783", "abs": "https://arxiv.org/abs/2512.03783", "authors": ["Dongchao Yang", "Songxiang Liu", "Disong Wang", "Yuanyuan Wang", "Guanglu Wan", "Helen Meng"], "title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning", "comment": null, "summary": "Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.", "AI": {"tldr": "\u63d0\u51faOmni-AutoThink\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u89e3\u51b3\u73b0\u6709Omni\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u50f5\u5316\u7684\u95ee\u9898\uff0c\u5305\u542b\u81ea\u9002\u5e94\u76d1\u7763\u5fae\u8c03\u548c\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u4e24\u9636\u6bb5\uff0c\u6784\u5efa\u4e86\u591a\u6a21\u6001\u81ea\u9002\u5e94\u63a8\u7406\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709Omni\u6a21\u578b\u5728\u63a8\u7406\u884c\u4e3a\u4e0a\u5b58\u5728\u50f5\u5316\u95ee\u9898\uff1a\u5bf9\u4e8e\u7b80\u5355\u95ee\u9898\u8fc7\u5ea6\u63a8\u7406\uff0c\u5bf9\u4e8e\u590d\u6742\u95ee\u9898\u53c8\u63a8\u7406\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u81ea\u9002\u5e94\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u63a8\u7406\u589e\u5f3a\u6570\u636e\u8d4b\u4e88Omni\u6a21\u578b\u57fa\u7840\u63a8\u7406\u80fd\u529b\uff1b2) \u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u5956\u52b1\u53cd\u9988\u4f18\u5316\u63a8\u7406\u884c\u4e3a\u3002\u540c\u65f6\u6784\u5efa\u4e86\u6db5\u76d6\u6587\u672c\u3001\u6587\u672c-\u97f3\u9891\u3001\u6587\u672c-\u89c6\u89c9\u3001\u6587\u672c-\u97f3\u9891-\u89c6\u89c9\u591a\u6a21\u6001\u7684\u81ea\u9002\u5e94\u63a8\u7406\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u5728\u81ea\u9002\u5e94\u63a8\u7406\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002\u6240\u6709\u57fa\u51c6\u6570\u636e\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "Omni-AutoThink\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86Omni\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u50f5\u5316\u7684\u95ee\u9898\uff0c\u5728\u591a\u6a21\u6001\u81ea\u9002\u5e94\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.03887", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03887", "abs": "https://arxiv.org/abs/2512.03887", "authors": ["Saurav Prateek"], "title": "A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)", "comment": null, "summary": "The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.\n  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.\n  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/", "AI": {"tldr": "\u63d0\u51faStatic-DRA\uff0c\u4e00\u79cd\u57fa\u4e8e\u6811\u72b6\u9759\u6001\u5de5\u4f5c\u6d41\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u53ef\u914d\u7f6e\u7684Depth\u548cBreadth\u53c2\u6570\u8ba9\u7528\u6237\u5e73\u8861\u7814\u7a76\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u9759\u6001RAG\u7ba1\u9053\u5728\u5904\u7406\u590d\u6742\u591a\u8f6e\u7814\u7a76\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6811\u72b6\u9759\u6001\u5de5\u4f5c\u6d41\u67b6\u6784\uff0c\u5305\u542bSupervisor\u3001Independent\u548cWorker\u4e09\u79cd\u4ee3\u7406\uff0c\u901a\u8fc7Depth\u548cBreadth\u53c2\u6570\u63a7\u5236\u7814\u7a76\u6df1\u5ea6\u548c\u5e7f\u5ea6\u3002", "result": "\u5728DeepResearch Bench\u4e0a\u4f7f\u7528RACE\u6846\u67b6\u8bc4\u4f30\uff0c\u914d\u7f6edepth=2\u3001breadth=5\u65f6\u83b7\u5f9734.72\u5206\uff0c\u9a8c\u8bc1\u53c2\u6570\u589e\u52a0\u80fd\u63d0\u5347\u7814\u7a76\u6df1\u5ea6\u548c\u8bc4\u5206\u3002", "conclusion": "Static-DRA\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u8d44\u6e90\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8d4b\u4e88\u7528\u6237\u5bf9\u6df1\u5ea6\u7814\u7a76\u8fc7\u7a0b\u7684\u900f\u660e\u63a7\u5236\u3002", "topic": "code agent"}}
{"id": "2512.03109", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03109", "abs": "https://arxiv.org/abs/2512.03109", "authors": ["Shuvom Sadhuka", "Drew Prinster", "Clara Fannjiang", "Gabriele Scalia", "Aviv Regev", "Hanchen Wang"], "title": "E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing", "comment": null, "summary": "Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.", "AI": {"tldr": "\u63d0\u51fae-valuator\u65b9\u6cd5\uff0c\u5c06\u4efb\u610f\u9ed1\u76d2\u9a8c\u8bc1\u5668\u5206\u6570\u8f6c\u6362\u4e3a\u5177\u6709\u53ef\u8bc1\u660e\u8bef\u62a5\u7387\u63a7\u5236\u7684\u51b3\u7b56\u89c4\u5219\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u8f68\u8ff9\u7684\u6210\u529f\u6982\u7387\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u9a8c\u8bc1\u5668\uff08\u5982LLM\u6cd5\u5b98\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff09\u4f7f\u7528\u542f\u53d1\u5f0f\u8bc4\u5206\uff0c\u4f46\u7f3a\u4e4f\u6b63\u786e\u6027\u4fdd\u8bc1\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u9a8c\u8bc1\u5668\u5206\u6570\u8f6c\u6362\u4e3a\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684\u51b3\u7b56\u89c4\u5219\uff0c\u4ee5\u53ef\u9760\u5730\u533a\u5206\u6210\u529f\u4e0e\u5931\u8d25\u7684\u4ee3\u7406\u8f68\u8ff9\u3002", "method": "\u5c06\u533a\u5206\u6210\u529f\u4e0e\u5931\u8d25\u8f68\u8ff9\u95ee\u9898\u6784\u5efa\u4e3a\u5e8f\u5217\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u3002\u57fa\u4e8ee-process\u5de5\u5177\u5f00\u53d1\u5e8f\u5217\u5047\u8bbe\u68c0\u9a8c\uff0c\u5728\u4ee3\u7406\u8f68\u8ff9\u7684\u6bcf\u4e00\u6b65\u90fd\u4fdd\u6301\u7edf\u8ba1\u6709\u6548\u6027\uff0c\u652f\u6301\u5bf9\u4efb\u610f\u957f\u52a8\u4f5c\u5e8f\u5217\u7684\u5728\u7ebf\u76d1\u63a7\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u79cd\u4ee3\u7406\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0ce-valuator\u76f8\u6bd4\u5176\u4ed6\u7b56\u7565\u5177\u6709\u66f4\u9ad8\u7684\u7edf\u8ba1\u529f\u6548\u548c\u66f4\u597d\u7684\u8bef\u62a5\u7387\u63a7\u5236\u3002\u8fd8\u80fd\u7528\u4e8e\u5feb\u901f\u7ec8\u6b62\u95ee\u9898\u8f68\u8ff9\u4ee5\u8282\u7701token\u3002", "conclusion": "e-valuator\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5c06\u9a8c\u8bc1\u5668\u542f\u53d1\u5f0f\u65b9\u6cd5\u8f6c\u6362\u4e3a\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684\u51b3\u7b56\u89c4\u5219\uff0c\u4f7f\u80fd\u90e8\u7f72\u66f4\u53ef\u9760\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2512.03955", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03955", "abs": "https://arxiv.org/abs/2512.03955", "authors": ["Niklas Jobs", "Luis Miguel Vieira da Silva", "Jayanth Somashekaraiah", "Maximilian Weigand", "David Kube", "Felix Gehlhoff"], "title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol", "comment": "This work has been submitted to IFAC for possible publication", "summary": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eBlocksworld\u95ee\u9898\u7684\u53ef\u6267\u884c\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u7684\u81ea\u9002\u5e94\u89c4\u5212\u4e0e\u6267\u884c\u80fd\u529b\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u5de5\u5177\u63a5\u53e3\u652f\u6301\u4e0d\u540c\u67b6\u6784\u7684\u5bf9\u6bd4\u3002", "motivation": "\u5de5\u4e1a\u81ea\u52a8\u5316\u9700\u8981\u80fd\u591f\u9002\u5e94\u53d8\u5316\u4efb\u52a1\u548c\u73af\u5883\u7684\u7075\u6d3b\u63a7\u5236\u7b56\u7565\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5177\u6709\u8fd9\u79cd\u81ea\u9002\u5e94\u89c4\u5212\u548c\u6267\u884c\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u7684\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5305\u542b\u53ef\u6267\u884c\u4eff\u771f\u73af\u5883\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eBlocksworld\u95ee\u9898\u63d0\u4f9b\u4e94\u4e2a\u590d\u6742\u5ea6\u7c7b\u522b\uff0c\u901a\u8fc7\u96c6\u6210\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u4f5c\u4e3a\u6807\u51c6\u5316\u5de5\u5177\u63a5\u53e3\uff0c\u4f7f\u4e0d\u540c\u667a\u80fd\u4f53\u67b6\u6784\u80fd\u591f\u65e0\u9700\u7279\u5b9a\u4fee\u6539\u5373\u53ef\u8fde\u63a5\u548c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5355\u667a\u80fd\u4f53\u5b9e\u73b0\u5c55\u793a\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u9002\u7528\u6027\uff0c\u5efa\u7acb\u4e86\u7528\u4e8e\u6bd4\u8f83\u57fa\u4e8eLLM\u7684\u89c4\u5212\u4e0e\u6267\u884c\u65b9\u6cd5\u7684\u5b9a\u91cf\u6307\u6807\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u548c\u6bd4\u8f83LLM\u667a\u80fd\u4f53\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u7684\u81ea\u9002\u5e94\u89c4\u5212\u4e0e\u6267\u884c\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.03676", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03676", "abs": "https://arxiv.org/abs/2512.03676", "authors": ["Daria Kryvosheieva", "Andrea de Varda", "Evelina Fedorenko", "Greta Tuckute"], "title": "Different types of syntactic agreement recruit the same units within large language models", "comment": null, "summary": "Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u529f\u80fd\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u57287\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8bc6\u522b\u5bf967\u79cd\u82f1\u8bed\u53e5\u6cd5\u73b0\u8c61\u6700\u654f\u611f\u7684\u5355\u5143\uff0c\u53d1\u73b0\u4e0d\u540c\u53e5\u6cd5\u4e00\u81f4\u6027\u73b0\u8c61\uff08\u5982\u4e3b\u8c13\u4e00\u81f4\u3001\u56de\u6307\u4e00\u81f4\u3001\u9650\u5b9a\u8bcd-\u540d\u8bcd\u4e00\u81f4\uff09\u4f1a\u6fc0\u6d3b\u91cd\u53e0\u7684\u5355\u5143\u96c6\u5408\uff0c\u8868\u660e\u53e5\u6cd5\u4e00\u81f4\u6027\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6784\u6210\u6709\u610f\u4e49\u7684\u51fd\u6570\u7c7b\u522b\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u53ef\u9760\u5730\u533a\u5206\u8bed\u6cd5\u6b63\u786e\u548c\u9519\u8bef\u7684\u53e5\u5b50\uff0c\u4f46\u8bed\u6cd5\u77e5\u8bc6\u5728\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u5f81\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7a76\u4e0d\u540c\u53e5\u6cd5\u73b0\u8c61\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u662f\u5426\u5171\u4eab\u6216\u4f7f\u7528\u4e0d\u540c\u7684\u7ec4\u4ef6\u3002", "method": "\u91c7\u7528\u53d7\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u529f\u80fd\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u57287\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8bc6\u522b\u5bf967\u79cd\u82f1\u8bed\u53e5\u6cd5\u73b0\u8c61\u6700\u654f\u611f\u7684\u5355\u5143\u3002\u901a\u8fc7\u8de8\u53e5\u5b50\u4e00\u81f4\u6027\u9a8c\u8bc1\u8fd9\u4e9b\u5355\u5143\uff0c\u5e76\u8fdb\u884c\u56e0\u679c\u5206\u6790\u786e\u8ba4\u5b83\u4eec\u5bf9\u6a21\u578b\u53e5\u6cd5\u6027\u80fd\u7684\u652f\u6301\u4f5c\u7528\u3002\u7814\u7a76\u6269\u5c55\u5230\u82f1\u8bed\u3001\u4fc4\u8bed\u548c\u4e2d\u6587\uff0c\u5e76\u572857\u79cd\u4e0d\u540c\u8bed\u8a00\u4e2d\u8fdb\u884c\u8de8\u8bed\u8a00\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u53e5\u6cd5\u4e00\u81f4\u6027\u73b0\u8c61\uff08\u5982\u4e3b\u8c13\u4e00\u81f4\u3001\u56de\u6307\u4e00\u81f4\u3001\u9650\u5b9a\u8bcd-\u540d\u8bcd\u4e00\u81f4\uff09\u4f1a\u6fc0\u6d3b\u91cd\u53e0\u7684\u5355\u5143\u96c6\u5408\uff0c\u8868\u660e\u53e5\u6cd5\u4e00\u81f4\u6027\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6784\u6210\u6709\u610f\u4e49\u7684\u51fd\u6570\u7c7b\u522b\u3002\u8fd9\u4e00\u6a21\u5f0f\u5728\u82f1\u8bed\u3001\u4fc4\u8bed\u548c\u4e2d\u6587\u4e2d\u90fd\u6210\u7acb\uff0c\u5e76\u4e14\u572857\u79cd\u8bed\u8a00\u7684\u8de8\u8bed\u8a00\u5206\u6790\u4e2d\uff0c\u7ed3\u6784\u66f4\u76f8\u4f3c\u7684\u8bed\u8a00\u5728\u4e3b\u8c13\u4e00\u81f4\u6027\u65b9\u9762\u5171\u4eab\u66f4\u591a\u5355\u5143\u3002", "conclusion": "\u53e5\u6cd5\u4e00\u81f4\u6027\u4f5c\u4e3a\u53e5\u6cd5\u4f9d\u8d56\u5173\u7cfb\u7684\u5173\u952e\u6807\u8bb0\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u5f81\u7a7a\u95f4\u4e2d\u6784\u6210\u4e86\u6709\u610f\u4e49\u7684\u7c7b\u522b\u3002\u7814\u7a76\u63ed\u793a\u4e86\u53e5\u6cd5\u77e5\u8bc6\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u5185\u90e8\u53e5\u6cd5\u5904\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2512.03759", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03759", "abs": "https://arxiv.org/abs/2512.03759", "authors": ["Jingyang Ou", "Jiaqi Han", "Minkai Xu", "Shaoxuan Xu", "Jianwen Xie", "Stefano Ermon", "Yi Wu", "Chongxuan Li"], "title": "Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective", "comment": null, "summary": "Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.", "AI": {"tldr": "ESPO\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e8f\u5217\u7ea7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edftoken\u7ea7RL\u65b9\u6cd5\u5728dLLMs\u4e0a\u7684\u4e0d\u9002\u7528\u95ee\u9898\uff0c\u901a\u8fc7ELBO\u4f5c\u4e3a\u5e8f\u5217\u7ea7\u4f3c\u7136\u4ee3\u7406\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7a0b\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\u3002\u6838\u5fc3\u56f0\u96be\u5728\u4e8e\u4f3c\u7136\u8fd1\u4f3c\uff1a\u81ea\u56de\u5f52\u6a21\u578b\u5929\u7136\u63d0\u4f9btoken\u7ea7\u6761\u4ef6\u6982\u7387\uff0c\u800cdLLMs\u901a\u8fc7\u8fed\u4ee3\u975e\u81ea\u56de\u5f52\u53bb\u566a\u6b65\u9aa4\u751f\u6210\u5e8f\u5217\uff0c\u7f3a\u4e4f\u8fd9\u79cd\u5206\u89e3\u3002", "method": "\u63d0\u51faELBO-based Sequence-level Policy Optimization (ESPO)\uff0c\u5c06\u6574\u4e2a\u5e8f\u5217\u751f\u6210\u89c6\u4e3a\u5355\u4e00\u52a8\u4f5c\uff0c\u4f7f\u7528ELBO\u4f5c\u4e3a\u53ef\u5904\u7406\u7684\u5e8f\u5217\u7ea7\u4f3c\u7136\u4ee3\u7406\u3002\u65b9\u6cd5\u5305\u542btoken\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u5f52\u4e00\u5316\u548c\u9c81\u68d2\u7684KL\u6563\u5ea6\u4f30\u8ba1\uff0c\u786e\u4fdd\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7a0b\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cESPO\u663e\u8457\u4f18\u4e8etoken\u7ea7\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Countdown\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8620-40\u5206\u7684\u5de8\u5927\u63d0\u5347\uff0c\u540c\u65f6\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u4e0a\u4fdd\u6301\u4e00\u81f4\u7684\u589e\u76ca\u3002", "conclusion": "ESPO\u4e3adLLMs\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u5efa\u7acb\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u7ecf\u9a8c\u6709\u6548\u7684\u5e8f\u5217\u7ea7\u4f18\u5316\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u4e0e\u975e\u81ea\u56de\u5f52\u6a21\u578b\u5728RL\u65b9\u6cd5\u4e0a\u7684\u6839\u672c\u6027\u4e0d\u5339\u914d\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.03244", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03244", "abs": "https://arxiv.org/abs/2512.03244", "authors": ["Salman Rahman", "Sruthi Gorantla", "Arpit Gupta", "Swastik Roy", "Nanyun Peng", "Yang Liu"], "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning", "comment": null, "summary": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.", "AI": {"tldr": "SPARK\u6846\u67b6\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u751f\u6210\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u5e76\u7528\u9a8c\u8bc1\u5668\u8bc4\u4f30\uff0c2) \u7528\u9a8c\u8bc1\u8f93\u51fa\u5fae\u8c03\u751f\u6210\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c3) \u5c06PRM-CoT\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u7528\u4e8e\u6570\u5b66\u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u53c2\u8003\u7b54\u6848\u5373\u53ef\u8d85\u8d8a\u57fa\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684\u65b9\u6cd5\u3002", "motivation": "\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRMs)\u9700\u8981\u6602\u8d35\u7684\u6b65\u9aa4\u7ea7\u6807\u6ce8\u6216\u771f\u5b9e\u53c2\u8003\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5c31\u80fd\u8bad\u7ec3\u9ad8\u8d28\u91cfPRM\u7684\u65b9\u6cd5\uff0c\u4ee5\u6269\u5927\u5176\u5728\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7b54\u6848\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u751f\u6210\u5668\u4ea7\u751f\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u5668\u901a\u8fc7\u5e76\u884c\u6269\u5c55\uff08\u81ea\u4e00\u81f4\u6027\uff09\u548c\u5e8f\u5217\u6269\u5c55\uff08\u5143\u6279\u5224\uff09\u8fdb\u884c\u8bc4\u4f30\uff1b2) \u7528\u9a8c\u8bc1\u8f93\u51fa\u4f5c\u4e3a\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5fae\u8c03\u751f\u6210\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff1b3) \u5c06PRM\u4e0e\u601d\u7ef4\u94fe\u9a8c\u8bc1(PRM-CoT)\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u683c\u5f0f\u7ea6\u675f\u9632\u6b62\u5956\u52b1\u653b\u51fb\u3002", "result": "\u5728ProcessBench\u4e0a\u8fbe\u523067.5 F1\uff0c\u4f18\u4e8e\u53c2\u8003\u6307\u5bfc\u8bad\u7ec3\u768466.4\u548cGPT-4o\u768461.9\u3002\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523047.4%\uff0c\u8d85\u8d8a\u57fa\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684RLVR(43.9%)\u3002", "conclusion": "SPARK\u6846\u67b6\u5b9e\u73b0\u4e86\u65e0\u9700\u53c2\u8003\u7b54\u6848\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u6027\u80fd\u8d85\u8d8a\u57fa\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684\u65b9\u6cd5\uff0c\u4e3a\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7b54\u6848\u6216\u96be\u4ee5\u83b7\u53d6\u771f\u5b9e\u6807\u7b7e\u7684\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.03276", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03276", "abs": "https://arxiv.org/abs/2512.03276", "authors": ["Constantin Venhoff", "Ashkan Khakzar", "Sonia Joseph", "Philip Torr", "Neel Nanda"], "title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval", "comment": null, "summary": "Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4e8b\u5b9e\u53ec\u56de\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0b\u964d\uff0c\u539f\u56e0\u662f\u5b9e\u4f53\u8868\u793a\u5f62\u6210\u8fc7\u665a\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528LLM\u539f\u6709\u7684\u77e5\u8bc6\u673a\u5236\u3002\u901a\u8fc7\u65e9\u671f\u5b9e\u4f53\u89e3\u6790\u53ef\u4ee5\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u8bb8\u591aVLM\u5728\u4e8b\u5b9e\u53ec\u56de\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u5982\u5176LLM\u9aa8\u5e72\u6a21\u578b\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u591a\u6a21\u6001\u5fae\u8c03\u6709\u6548\u6027\u7684\u8d28\u7591\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76VLM\u5982\u4f55\u5c06\u89c6\u89c9\u8f93\u5165\u4e0eLLM\u7684\u73b0\u6709\u77e5\u8bc6\u673a\u5236\u5bf9\u9f50\u3002", "method": "\u5bf914\u79cd\u4e0d\u540c\u67b6\u6784\u3001\u89c4\u6a21\u548c\u8bad\u7ec3\u8bbe\u7f6e\u7684VLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u3001\u6fc0\u6d3b\u4fee\u8865\u548c\u63a2\u6d4b\u6280\u672f\u5206\u6790\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u6d4b\u8bd5\u4e24\u79cd\u6027\u80fd\u6062\u590d\u65b9\u6cd5\uff1a\u4eceLLM\u9aa8\u5e72\u4fee\u8865\u5b9e\u4f53\u8868\u793a\u548c\u4f7f\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u3002", "result": "14\u4e2a\u6a21\u578b\u4e2d11\u4e2a\u51fa\u73b0\u4e8b\u5b9e\u53ec\u56de\u6027\u80fd\u4e0b\u964d\u3002\u6027\u80fd\u5dee\u7684VLM\u56e0\u5b9e\u4f53\u8868\u793a\u5f62\u6210\u8fc7\u665a\u800c\u65e0\u6cd5\u5229\u7528LLM\u539f\u6709\u673a\u5236\uff0c\u800c\u9ad8\u6027\u80fdVLM\u80fd\u65e9\u671f\u89e3\u6790\u5b9e\u4f53\u8868\u793a\u3002\u4e24\u79cd\u6062\u590d\u65b9\u6cd5\u5747\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u65e9\u671f\u5b9e\u4f53\u89e3\u6790\u901f\u5ea6\u662fVLM\u80fd\u5426\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3LLM\u673a\u5236\u7684\u5173\u952e\u56e0\u7d20\u3002\u673a\u5236\u5206\u6790\u80fd\u89e3\u91ca\u591a\u6a21\u6001\u5bf9\u9f50\u4e2d\u7684\u7cfb\u7edf\u6027\u5931\u8d25\uff0c\u5e76\u4e3a\u6539\u8fdbVLM\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2512.04072", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04072", "abs": "https://arxiv.org/abs/2512.04072", "authors": ["Zayne Sprague", "Jack Lu", "Manya Wadhwa", "Sedrick Keh", "Mengye Ren", "Greg Durrett"], "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors", "comment": null, "summary": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.", "AI": {"tldr": "SkillFactory\u662f\u4e00\u79cd\u5728\u5f3a\u5316\u5b66\u4e60\u524d\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8ba9\u6a21\u578b\u5b66\u4e60\u8ba4\u77e5\u6280\u80fd\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\"\u94f6\u724c\"\u8bad\u7ec3\u6570\u636e\uff0c\u5e2e\u52a9\u6a21\u578b\u5728RL\u540e\u66f4\u597d\u5730\u6cdb\u5316\u5230\u66f4\u96be\u4efb\u52a1", "motivation": "\u5f53\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u4e0d\u5177\u5907\u67d0\u4e9b\u8ba4\u77e5\u6280\u80fd\uff08\u5982\u9a8c\u8bc1\u7b54\u6848\u3001\u56de\u6eaf\u3001\u5c1d\u8bd5\u66ff\u4ee3\u65b9\u6cd5\u7b49\uff09\u65f6\uff0c\u5982\u4f55\u8ba9\u6a21\u578b\u5b66\u4f1a\u5229\u7528\u8fd9\u4e9b\u6280\u80fd\uff1f\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4ece\u66f4\u5f3a\u6a21\u578b\u84b8\u998f\uff0c\u4f46\u9700\u8981\u63a2\u7d22\u4e0d\u4f9d\u8d56\u84b8\u998f\u7684\u65b9\u6cd5", "method": "1) \u5728RL\u524d\u7684\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff0c\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u6837\u672c\u91cd\u65b0\u6392\u5217\uff0c\u5f62\u6210\u8ba4\u77e5\u6280\u80fd\u683c\u5f0f\u7684\u8bad\u7ec3\u6570\u636e\uff08\"\u94f6\u724c\"SFT\u8f68\u8ff9\uff09\uff1b2) \u8fd9\u4e9b\u53ef\u80fd\u4e0d\u5b8c\u7f8e\u7684\u8bad\u7ec3\u6570\u636e\u7528\u4e8e\u4e3a\u6a21\u578b\u5b66\u4e60\u6280\u80fd\u505a\u51c6\u5907\uff1b3) \u7136\u540e\u5728RL\u9636\u6bb5\u8fdb\u4e00\u6b65\u4f18\u5316", "result": "1) SkillFactory SFT\u521d\u59cb\u5316\u5e2e\u52a9\u6a21\u578b\u5728RL\u540e\u66f4\u597d\u5730\u6cdb\u5316\u5230\u66f4\u96be\u4efb\u52a1\u53d8\u4f53\uff1b2) \u6a21\u578b\u786e\u5b9e\u4f7f\u7528\u4e86\u8ba4\u77e5\u6280\u80fd\uff1b3) SkillFactory\u6a21\u578b\u5728\u57df\u5916\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7840RL\u6a21\u578b\u66f4\u7a33\u5065\uff0c\u56de\u5f52\u66f4\u5c11", "conclusion": "\u5728RL\u524d\u5b66\u4e60\u7684\u5f52\u7eb3\u504f\u7f6e\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u7a33\u5065\u7684\u8ba4\u77e5\u6280\u80fd\u4f7f\u7528\uff0cSkillFactory\u65b9\u6cd5\u6709\u6548\u4e14\u4e0d\u4f9d\u8d56\u4ece\u66f4\u5f3a\u6a21\u578b\u84b8\u998f", "topic": "agentic reinforcement learning"}}
{"id": "2512.03847", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03847", "abs": "https://arxiv.org/abs/2512.03847", "authors": ["Dingwei Zhu", "Zhiheng Xi", "Shihan Dou", "Yuhui Wang", "Sixian Li", "Junjie Ye", "Honglin Guo", "Shichun Liu", "Chenhao Huang", "Yajie Yang", "Junlin Shang", "Senjie Jin", "Ming Zhang", "Jiazheng Zhang", "Caishuang Huang", "Yunke Zhang", "Demei Yan", "Yuran Wang", "Tao Gui"], "title": "DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training", "comment": null, "summary": "Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.", "AI": {"tldr": "DVPO\uff1a\u7ed3\u5408\u6761\u4ef6\u98ce\u9669\u7406\u8bba\u4e0e\u5206\u5e03\u4ef7\u503c\u5efa\u6a21\u7684\u65b0RL\u6846\u67b6\uff0c\u901a\u8fc7token\u7ea7\u4ef7\u503c\u5206\u5e03\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff0c\u4f7f\u7528\u975e\u5bf9\u79f0\u98ce\u9669\u6b63\u5219\u5316\u5e73\u8861\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\uff0c\u5728\u566a\u58f0\u76d1\u7763\u4e0b\u4f18\u4e8ePPO\u3001GRPO\u7b49\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754cLLM\u540e\u8bad\u7ec3\u5e38\u9762\u4e34\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u76d1\u7763\uff0c\u590d\u6742\u4e0d\u53ef\u9760\u7684\u76d1\u7763\u4fe1\u53f7\u4f1a\u7834\u574f\u8bad\u7ec3\u7a33\u5b9a\u6027\u5e76\u635f\u5bb3\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6700\u574f\u60c5\u51b5\u4f18\u5316\u548c\u5747\u503c\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u6cdb\u5316\u6027\uff0c\u53ef\u80fd\u4ea7\u751f\u8fc7\u4e8e\u4fdd\u5b88\u7684\u7b56\u7565\uff0c\u5bfc\u81f4\u5728\u4e0d\u540c\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u5747\u3002", "method": "DVPO\u7ed3\u5408\u6761\u4ef6\u98ce\u9669\u7406\u8bba\u4e0e\u5206\u5e03\u4ef7\u503c\u5efa\u6a21\uff1a1)\u5b66\u4e60token\u7ea7\u4ef7\u503c\u5206\u5e03\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff1b2)\u5e94\u7528\u975e\u5bf9\u79f0\u98ce\u9669\u6b63\u5219\u5316\u5851\u9020\u5206\u5e03\u5c3e\u90e8\uff1a\u538b\u7f29\u4e0b\u5c3e\u4ee5\u6291\u5236\u566a\u58f0\u8d1f\u504f\u5dee\uff0c\u6269\u5c55\u4e0a\u5c3e\u4ee5\u4fdd\u6301\u63a2\u7d22\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u8f6e\u5bf9\u8bdd\u3001\u6570\u5b66\u63a8\u7406\u548c\u79d1\u5b66\u95ee\u7b54\u7b49\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cDVPO\u5728\u566a\u58f0\u76d1\u7763\u4e0b\u6301\u7eed\u4f18\u4e8ePPO\u3001GRPO\u548c\u57fa\u4e8e\u9c81\u68d2\u8d1d\u5c14\u66fc\u7684PPO\uff0c\u663e\u793a\u51fa\u5728\u73b0\u5b9e\u4e16\u754cLLM\u540e\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "DVPO\u901a\u8fc7\u5206\u5e03\u4ef7\u503c\u5efa\u6a21\u548c\u975e\u5bf9\u79f0\u98ce\u9669\u6b63\u5219\u5316\uff0c\u6709\u6548\u5e73\u8861\u4e86\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\uff0c\u4e3a\u566a\u58f0\u76d1\u7763\u4e0b\u7684LLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.03805", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03805", "abs": "https://arxiv.org/abs/2512.03805", "authors": ["Tai Nguyen", "Phong Le", "Andr\u00e9 Biedenkapp", "Carola Doerr", "Nguyen Dang"], "title": "Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($\u03bb$,$\u03bb$))-GA", "comment": "arXiv admin note: text overlap with arXiv:2502.20265", "summary": "Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($\u03bb$,$\u03bb$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u7b97\u6cd5\u914d\u7f6e\u4e2d\u7684\u5e94\u7528\uff0c\u9488\u5bf9(1+(\u03bb,\u03bb))-GA\u7b97\u6cd5\u7684\u79cd\u7fa4\u89c4\u6a21\u53c2\u6570\u63a7\u5236\u95ee\u9898\uff0c\u63ed\u793a\u4e86DDQN\u548cPPO\u5b58\u5728\u7684\u53ef\u6269\u5c55\u6027\u9000\u5316\u548c\u5b66\u4e60\u4e0d\u7a33\u5b9a\u6027\u4e24\u5927\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u5956\u52b1\u504f\u79fb\u673a\u5236\u7b49\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u7b97\u6cd5\u914d\u7f6e\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u9762\u4e34\u6311\u6218\u4e14\u9700\u8981\u5927\u91cf\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5177\u4f53\u7b97\u6cd5\u914d\u7f6e\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5176\u6839\u672c\u6311\u6218\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4ee5\u63a7\u5236(1+(\u03bb,\u03bb))-GA\u7b97\u6cd5\u5728OneMax\u95ee\u9898\u4e0a\u7684\u79cd\u7fa4\u89c4\u6a21\u53c2\u6570\u4e3a\u6848\u4f8b\uff0c\u7cfb\u7edf\u7814\u7a76DDQN\u548cPPO\u4e24\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002\u9488\u5bf9\u53d1\u73b0\u7684\u6311\u6218\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u5956\u52b1\u504f\u79fb\u673a\u5236\u89e3\u51b3\u63a2\u7d22\u4e0d\u8db3\u95ee\u9898\uff0c\u4f7f\u7528\u65e0\u6298\u6263\u5b66\u4e60\u89e3\u51b3\u89c4\u5212\u89c6\u91ce\u8986\u76d6\u95ee\u9898\uff0c\u5e76\u5206\u6790PPO\u7684\u8d85\u53c2\u6570\u4f9d\u8d56\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0DDQN\u548cPPO\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9000\u5316\u548c\u5b66\u4e60\u4e0d\u7a33\u5b9a\u6027\u4e24\u5927\u6311\u6218\uff0c\u4e3b\u8981\u6e90\u4e8e\u63a2\u7d22\u4e0d\u8db3\u548c\u89c4\u5212\u89c6\u91ce\u8986\u76d6\u95ee\u9898\u3002\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u5956\u52b1\u504f\u79fb\u673a\u5236\u80fd\u6709\u6548\u589e\u5f3aDDQN\u63a2\u7d22\u80fd\u529b\uff0c\u65e0\u6298\u6263\u5b66\u4e60\u89e3\u51b3\u89c4\u5212\u89c6\u91ce\u95ee\u9898\u3002DDQN\u7ed3\u5408\u81ea\u9002\u5e94\u5956\u52b1\u504f\u79fb\u7b56\u7565\u80fd\u8fbe\u5230\u4e0e\u7406\u8bba\u63a8\u5bfc\u7b56\u7565\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u6837\u672c\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u5148\u524dDAC\u65b9\u6cd5\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u7b97\u6cd5\u914d\u7f6e\u4e2d\u9762\u4e34\u7cfb\u7edf\u6027\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u514b\u670d\u3002DDQN\u7ed3\u5408\u81ea\u9002\u5e94\u5956\u52b1\u504f\u79fb\u673a\u5236\u662f\u6709\u6548\u7684DAC\u65b9\u6cd5\uff0c\u800cPPO\u5b58\u5728\u6839\u672c\u6027\u65b9\u5dee\u95ee\u9898\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u3002\u7814\u7a76\u4e3aDAC\u9886\u57df\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.3f7f2ca8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.openai.com%2Fscaling-code-verification%2F%3Futm_source=tldrai/1/0100019adf6cef0e-391161ea-3c75-4db1-9d24-f8b7fdfff6d2-000000/H4IuKdPnj9gIE76MMdnAII_ePiSVvq16oepX7AayLJI=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.openai.com%2Fscaling-code-verification%2F%3Futm_source=tldrai/1/0100019adf6cef0e-391161ea-3c75-4db1-9d24-f8b7fdfff6d2-000000/H4IuKdPnj9gIE76MMdnAII_ePiSVvq16oepX7AayLJI=434", "authors": ["TLDR Newsletter"], "title": "A Practical Approach to Verifying Code at Scale", "comment": "Source: TLDR Newsletter, Date: 2025-12-02, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.openai.com%2Fscaling-code-verification%2F%3Futm_source=tldrai/1/0100019adf6cef0e-391161ea-3c75-4db1-9d24-f8b7fdfff6d2-000000/H4IuKdPnj9gIE76MMdnAII_ePiSVvq16oepX7AayLJI=434", "summary": "A Practical Approach to Verifying Code at Scale (7 minute read) OpenAI trained an agentic code reviewer for Codex since noisy safety tools are inevitably bypassed. The system now handles 100k+ external PRs daily, providing repo-wide context and execution access. Internally, it has caught launch-blocking bugs and protected high-stakes experiments.", "source": "tldr", "AI": {"tldr": "OpenAI\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eCodex\u7684\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u9a8c\u8bc1\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u6bcf\u5929\u5904\u7406\u8d85\u8fc710\u4e07\u4e2a\u5916\u90e8PR\uff0c\u63d0\u4f9b\u4ed3\u5e93\u7ea7\u4e0a\u4e0b\u6587\u548c\u6267\u884c\u8bbf\u95ee\uff0c\u5df2\u6210\u529f\u6355\u83b7\u5173\u952ebug\u5e76\u4fdd\u62a4\u9ad8\u98ce\u9669\u5b9e\u9a8c\u3002", "motivation": "\u4f20\u7edf\u566a\u58f0\u5b89\u5168\u5de5\u5177\u5bb9\u6613\u88ab\u7ed5\u8fc7\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u4ee3\u7801\u9a8c\u8bc1\u65b9\u6cd5\u6765\u786e\u4fdd\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u91cf\u5916\u90e8\u8d21\u732e\u548c\u9ad8\u98ce\u9669\u5b9e\u9a8c\u65f6\u3002", "method": "\u57fa\u4e8eCodex\u8bad\u7ec3\u4e86\u4e00\u4e2a\u4ee3\u7406\u5f0f\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u8bbf\u95ee\u4ed3\u5e93\u7ea7\u4e0a\u4e0b\u6587\u548c\u6267\u884c\u6743\u9650\uff0c\u5bf9\u4ee3\u7801\u8fdb\u884c\u6df1\u5ea6\u5206\u6790\u548c\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u6bcf\u5929\u5904\u7406\u8d85\u8fc710\u4e07\u4e2a\u5916\u90e8PR\uff0c\u6210\u529f\u6355\u83b7\u4e86\u963b\u6b62\u53d1\u5e03\u7684bug\uff0c\u5e76\u6709\u6548\u4fdd\u62a4\u4e86\u9ad8\u98ce\u9669\u5b9e\u9a8c\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u4ee3\u7406\u5f0f\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5927\u89c4\u6a21\u4ee3\u7801\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u5b89\u5168\u6027\u548c\u8d28\u91cf\u4fdd\u8bc1\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2512.a8c8557c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Faws%2Famazon-bedrock-agentcore-adds-quality-evaluations-and-policy-controls-for-deploying-trusted-ai-agents%2F%3Futm_source=tldrdevops/1/0100019ae41b5d32-e397873b-baf2-4ccb-8078-1dea7997e506-000000/SmZONAMVAUXL3h-0RwNGhfakWb1P-I5DgWIJQQhrc2c=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Faws%2Famazon-bedrock-agentcore-adds-quality-evaluations-and-policy-controls-for-deploying-trusted-ai-agents%2F%3Futm_source=tldrdevops/1/0100019ae41b5d32-e397873b-baf2-4ccb-8078-1dea7997e506-000000/SmZONAMVAUXL3h-0RwNGhfakWb1P-I5DgWIJQQhrc2c=434", "authors": ["TLDR Newsletter"], "title": "Amazon Bedrock AgentCore adds quality evaluations and policy controls for deploying trusted AI agents", "comment": "Source: TLDR Newsletter, Date: 2025-12-03, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Faws%2Famazon-bedrock-agentcore-adds-quality-evaluations-and-policy-controls-for-deploying-trusted-ai-agents%2F%3Futm_source=tldrdevops/1/0100019ae41b5d32-e397873b-baf2-4ccb-8078-1dea7997e506-000000/SmZONAMVAUXL3h-0RwNGhfakWb1P-I5DgWIJQQhrc2c=434", "summary": "Amazon Bedrock AgentCore adds quality evaluations and policy controls for deploying trusted AI agents (8 minute read) Amazon Bedrock AgentCore has been updated with new features, including Policy for agent control, Evaluations for performance analysis, episodic memory for experience-based learning, and bidirectional streaming for natural conversations. The new features are available in select AWS Regions and accessible through the AWS Free Tier for new users. The AgentCore SDK has been downlo...", "source": "tldr", "AI": {"tldr": "Amazon Bedrock AgentCore\u66f4\u65b0\u589e\u52a0\u4e86\u8d28\u91cf\u8bc4\u4f30\u3001\u7b56\u7565\u63a7\u5236\u3001\u60c5\u666f\u8bb0\u5fc6\u548c\u53cc\u5411\u6d41\u5f0f\u4f20\u8f93\u529f\u80fd\uff0c\u7528\u4e8e\u90e8\u7f72\u53ef\u4fe1\u7684AI\u4ee3\u7406", "motivation": "\u4e3a\u4e86\u589e\u5f3aAI\u4ee3\u7406\u7684\u53ef\u4fe1\u5ea6\u548c\u90e8\u7f72\u63a7\u5236\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u5206\u6790\u548c\u81ea\u7136\u5bf9\u8bdd\u80fd\u529b", "method": "\u901a\u8fc7\u6dfb\u52a0\u7b56\u7565\u63a7\u5236\u3001\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u3001\u60c5\u666f\u8bb0\u5fc6\u673a\u5236\u548c\u53cc\u5411\u6d41\u5f0f\u4f20\u8f93\u6280\u672f\u6765\u589e\u5f3aAgentCore\u5e73\u53f0", "result": "\u65b0\u529f\u80fd\u5df2\u5728\u7279\u5b9aAWS\u533a\u57df\u53ef\u7528\uff0c\u53ef\u901a\u8fc7AWS\u514d\u8d39\u5c42\u8bbf\u95ee\uff0cAgentCore SDK\u5df2\u53ef\u4f9b\u4e0b\u8f7d", "conclusion": "Bedrock AgentCore\u7684\u66f4\u65b0\u4e3a\u90e8\u7f72\u53ef\u4fe1AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u548c\u63a7\u5236\u80fd\u529b", "topic": "agent analysis"}}
{"id": "tldr.2512.0ff9e9ae", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMemoriLabs%2FMemori%3Futm_source=tldrdevops/1/0100019ae41b5d32-e397873b-baf2-4ccb-8078-1dea7997e506-000000/MrnhgVEI79iI3pvpZ0kFCnWkNVEhYuhN8RyBoQyRP9U=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMemoriLabs%2FMemori%3Futm_source=tldrdevops/1/0100019ae41b5d32-e397873b-baf2-4ccb-8078-1dea7997e506-000000/MrnhgVEI79iI3pvpZ0kFCnWkNVEhYuhN8RyBoQyRP9U=434", "authors": ["TLDR Newsletter"], "title": "Memori", "comment": "Source: TLDR Newsletter, Date: 2025-12-03, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMemoriLabs%2FMemori%3Futm_source=tldrdevops/1/0100019ae41b5d32-e397873b-baf2-4ccb-8078-1dea7997e506-000000/MrnhgVEI79iI3pvpZ0kFCnWkNVEhYuhN8RyBoQyRP9U=434", "summary": "Memori (GitHub Repo) Memori, an open-source memory engine, can be installed to attribute LLM interactions to entities and processes. The tool enhances memories without latency and offers advanced augmentation.", "source": "tldr", "AI": {"tldr": "Memori\u662f\u4e00\u4e2a\u5f00\u6e90\u8bb0\u5fc6\u5f15\u64ce\uff0c\u53ef\u5c06LLM\u4ea4\u4e92\u5f52\u56e0\u4e8e\u5b9e\u4f53\u548c\u6d41\u7a0b\uff0c\u63d0\u4f9b\u65e0\u5ef6\u8fdf\u7684\u8bb0\u5fc6\u589e\u5f3a\u548c\u9ad8\u7ea7\u589e\u5f3a\u529f\u80fd", "motivation": "\u73b0\u6709LLM\u4ea4\u4e92\u7f3a\u4e4f\u7ed3\u6784\u5316\u8bb0\u5fc6\u7ba1\u7406\uff0c\u96be\u4ee5\u5c06\u4ea4\u4e92\u5f52\u56e0\u4e8e\u7279\u5b9a\u5b9e\u4f53\u548c\u6d41\u7a0b\uff0c\u9700\u8981\u65e0\u5ef6\u8fdf\u7684\u8bb0\u5fc6\u589e\u5f3a\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u5f00\u6e90\u8bb0\u5fc6\u5f15\u64ce\uff0c\u901a\u8fc7\u5b9e\u4f53\u548c\u6d41\u7a0b\u5f52\u56e0\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u5ef6\u8fdf\u7684\u8bb0\u5fc6\u589e\u5f3a\u548c\u9ad8\u7ea7\u589e\u5f3a\u529f\u80fd", "result": "\u521b\u5efa\u4e86Memori\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u7ba1\u7406LLM\u4ea4\u4e92\u8bb0\u5fc6\uff0c\u63d0\u4f9b\u65e0\u5ef6\u8fdf\u7684\u8bb0\u5fc6\u589e\u5f3a\uff0c\u652f\u6301\u9ad8\u7ea7\u589e\u5f3a\u529f\u80fd", "conclusion": "Memori\u4f5c\u4e3a\u5f00\u6e90\u8bb0\u5fc6\u5f15\u64ce\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u4ea4\u4e92\u8bb0\u5fc6\u7ba1\u7406\u7684\u9700\u6c42\uff0c\u4e3a\u5b9e\u4f53\u548c\u6d41\u7a0b\u5f52\u56e0\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177", "topic": "code agent"}}
{"id": "tldr.2512.18e571f7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FZGn41hf/2/0100019ae4300932-43f46e76-9193-48d6-a96b-61cd31327356-000000/liJb2R-24qPxHZoW8pDWU5SpxqZKr7IWNQhS55ZjPZQ=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FZGn41hf/2/0100019ae4300932-43f46e76-9193-48d6-a96b-61cd31327356-000000/liJb2R-24qPxHZoW8pDWU5SpxqZKr7IWNQhS55ZjPZQ=434", "authors": ["TLDR Newsletter"], "title": "Skip the docs. Let AI add auth to your Next.js app", "comment": "Source: TLDR Newsletter, Date: 2025-12-03, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FZGn41hf/2/0100019ae4300932-43f46e76-9193-48d6-a96b-61cd31327356-000000/liJb2R-24qPxHZoW8pDWU5SpxqZKr7IWNQhS55ZjPZQ=434", "summary": "Skip the docs. Let AI add auth to your Next.js app (Sponsor) Clerk's Next.js quickstart includes a pre-built prompt you can copy or open directly in Cursor, Claude, or ChatGPT. Your AI assistant walks through the complete setup: installing the SDK, configuring middleware, and adding auth components. No bouncing between docs and your editor, no manual code copying, no missed configuration steps. Just paste the prompt (or click to open) and go from empty project to working authentication in one...", "source": "tldr", "AI": {"tldr": "AI\u52a9\u624b\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u4e3aNext.js\u5e94\u7528\u6dfb\u52a0\u8ba4\u8bc1\u529f\u80fd\uff0c\u65e0\u9700\u67e5\u9605\u6587\u6863\u548c\u624b\u52a8\u590d\u5236\u4ee3\u7801", "motivation": "\u7b80\u5316Next.js\u5e94\u7528\u8ba4\u8bc1\u914d\u7f6e\u6d41\u7a0b\uff0c\u907f\u514d\u5728\u6587\u6863\u548c\u7f16\u8f91\u5668\u4e4b\u95f4\u5207\u6362\uff0c\u51cf\u5c11\u624b\u52a8\u4ee3\u7801\u590d\u5236\u548c\u914d\u7f6e\u9057\u6f0f", "method": "\u63d0\u4f9b\u9884\u6784\u5efa\u7684\u63d0\u793a\u8bcd\uff0c\u5f00\u53d1\u8005\u53ef\u5728Cursor\u3001Claude\u6216ChatGPT\u4e2d\u76f4\u63a5\u4f7f\u7528\uff0cAI\u52a9\u624b\u6307\u5bfc\u5b8c\u6210SDK\u5b89\u88c5\u3001\u4e2d\u95f4\u4ef6\u914d\u7f6e\u548c\u8ba4\u8bc1\u7ec4\u4ef6\u6dfb\u52a0", "result": "\u4ece\u7a7a\u9879\u76ee\u5230\u5b8c\u6574\u53ef\u7528\u7684\u8ba4\u8bc1\u7cfb\u7edf\u4e00\u6b65\u5b8c\u6210\uff0c\u65e0\u9700\u67e5\u9605\u6587\u6863\u548c\u624b\u52a8\u590d\u5236\u4ee3\u7801", "conclusion": "AI\u9a71\u52a8\u7684\u5f00\u53d1\u6d41\u7a0b\u53ef\u4ee5\u663e\u8457\u7b80\u5316Next.js\u5e94\u7528\u8ba4\u8bc1\u914d\u7f6e\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "topic": "swe application"}}
{"id": "wechat.2512.fd311878", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5ODQ5ODQ1Mw==&mid=2247666495&idx=1&sn=42f487b46fc3f8ebc4d3ff31f9169ed7&chksm=ff66bcce5ccea898ceb50abb4a7fb03a176e18963965e8b270cf9c2ef8ea046e6b53b689f9f1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5ODQ5ODQ1Mw==&mid=2247666495&idx=1&sn=42f487b46fc3f8ebc4d3ff31f9169ed7&chksm=ff66bcce5ccea898ceb50abb4a7fb03a176e18963965e8b270cf9c2ef8ea046e6b53b689f9f1#rd", "authors": ["\u7109\u77e5\u6c7d\u8f66"], "title": "\u57fa\u4e8e\u201c\u6050\u60e7-\u597d\u5947\u201d\u5f15\u5bfc\u7684\u5b89\u5168\u9ad8\u6548\u7aef\u5230\u7aef\u81ea\u4e3b\u5bfc\u822a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b9\u6cd5", "comment": "Source: WeChat, Published: 2025-12-04 13:13:07", "summary": "\u540c\u65f6\uff0c\u8fd9\u4e5f\u8868\u660e\u6a21\u62df\u751f\u7269\u795e\u7ecf\u4e0e\u5fc3\u7406\u673a\u5236\u6709\u52a9\u4e8e\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002\u5173\u952e\u8bcd\uff1a\u81ea\u52a8\u5bfc\u822a\uff1b\u6050\u60e7\u4e0e\u597d\u5947\uff1b\u5b89\u5168\u7ea6\u675f\uff1b\u7ecf\u9a8c\u56de\u653e\u4f18\u5316 \u2160 \u5f15\u8a00 \u81ea\u4e3b\u5730\u9762\u8f66\u8f86\uff08AGVs\uff09\u6b63\u901a\u8fc7\u51cf\u5c11\u4eba\u7c7b\u5de5\u4f5c\u91cf\u5e76\u5b9e\u73b0\u81ea\u4e3b\u79fb\u52a8\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u53ca\u667a", "AI": {"tldr": "\u540c\u65f6\uff0c\u8fd9\u4e5f\u8868\u660e\u6a21\u62df\u751f\u7269\u795e\u7ecf\u4e0e\u5fc3\u7406\u673a\u5236\u6709\u52a9\u4e8e\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002\u5173\u952e\u8bcd\uff1a\u81ea\u52a8\u5bfc\u822a\uff1b\u6050\u60e7\u4e0e\u597d\u5947\uff1b\u5b89\u5168\u7ea6\u675f\uff1b\u7ecf\u9a8c\u56de\u653e\u4f18\u5316 \u2160 \u5f15\u8a00 \u81ea\u4e3b\u5730\u9762\u8f66\u8f86\uff08AGVs\uff09\u6b63\u901a\u8fc7\u51cf\u5c11\u4eba\u7c7b\u5de5\u4f5c\u91cf\u5e76\u5b9e\u73b0\u81ea\u4e3b\u79fb\u52a8\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u53ca\u667a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.2782dbf1", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNDE4ODIzMQ==&mid=2247487186&idx=1&sn=05774a725d63589e0f52b0baf0e4a5d0&chksm=c041ae5a9bd325e15ce0444c407a7843740846e49f92c3b90cf8260ed4743e8a49f76e53b5f1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNDE4ODIzMQ==&mid=2247487186&idx=1&sn=05774a725d63589e0f52b0baf0e4a5d0&chksm=c041ae5a9bd325e15ce0444c407a7843740846e49f92c3b90cf8260ed4743e8a49f76e53b5f1#rd", "authors": ["Mbot\u5177\u8eab\u667a\u80fd\u5b9e\u9a8c\u5ba4"], "title": "\u5b57\u8282\u8df3\u52a8Seed\u56e2\u961f\u53d1\u5e03\u9762\u5411\u957f\u5468\u671f\u7075\u5de7\u64cd\u4f5c\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6846\u67b6GR-RL", "comment": "Source: WeChat, Published: 2025-12-04 09:05:39", "summary": "\u5f3a\u5316\u5b66\u4e60\u8def\u7ebf\u601d\u8003\u8fc7\u53bb\u51e0\u5e74\uff0c\u5177\u8eab\u667a\u80fd\u7684\u4e3b\u6d41\u53d9\u4e8b\u662f\u201c\u6a21\u4eff\u5b66\u4e60\uff08BC/IL\uff09+ \u5927\u6570\u636e\u201d\uff0c\u8bd5\u56fe\u590d\u523b LLM \u7684Scaling Law\u3002\u4f46\u6700\u8fd1\u8fd9\u51e0\u4e2a\u7206\u706b\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u7eaf\u6a21\u4eff\u5b66\u4e60\u7684\u201c\u7ea2\u5229\u201d\u6b63\u5728\u5403\u7d27\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6b63\u5728\u4ee5\u4e00\u79cd\u5168\u65b0\u7684\u3001\u66f4\u52a1\u5b9e\u7684\u59ff\u6001", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u8def\u7ebf\u601d\u8003\u8fc7\u53bb\u51e0\u5e74\uff0c\u5177\u8eab\u667a\u80fd\u7684\u4e3b\u6d41\u53d9\u4e8b\u662f\u201c\u6a21\u4eff\u5b66\u4e60\uff08BC/IL\uff09+ \u5927\u6570\u636e\u201d\uff0c\u8bd5\u56fe\u590d\u523b LLM \u7684Scaling Law\u3002\u4f46\u6700\u8fd1\u8fd9\u51e0\u4e2a\u7206\u706b\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u7eaf\u6a21\u4eff\u5b66\u4e60\u7684\u201c\u7ea2\u5229\u201d\u6b63\u5728\u5403\u7d27\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6b63\u5728\u4ee5\u4e00\u79cd\u5168\u65b0\u7684\u3001\u66f4\u52a1\u5b9e\u7684\u59ff\u6001", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.4bb09d95", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMDk2NDY2Mw==&mid=2247486218&idx=2&sn=49709f3bc596eb06b4fa74076a91a5a3&chksm=f8ad7875916ce8eaafe6a7932d76c138ebfc53b4a7dc1a0cbfa678ba8fa489a40852f27f6033#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMDk2NDY2Mw==&mid=2247486218&idx=2&sn=49709f3bc596eb06b4fa74076a91a5a3&chksm=f8ad7875916ce8eaafe6a7932d76c138ebfc53b4a7dc1a0cbfa678ba8fa489a40852f27f6033#rd", "authors": ["lightA"], "title": "\u963f\u91cc\u56e2\u961f\u6700\u65b0\u8bba\u6587--\u7528\u5927\u6a21\u578b\u7a33\u5b9a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-12-04 01:50:23", "summary": "#LLM #\u5f3a\u5316\u5b66\u4e60 #\u963f\u91cc\u7cfb", "AI": {"tldr": "#LLM #\u5f3a\u5316\u5b66\u4e60 #\u963f\u91cc\u7cfb", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.aeebbec1", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4NTUxMDgyMA==&mid=2247507697&idx=1&sn=105192c5133eca00b49a1a302b28ce05&chksm=ce91361ba5750736bee9728d574339eb542e9e8450b7366e462d3762062d9494f880b46665f7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4NTUxMDgyMA==&mid=2247507697&idx=1&sn=105192c5133eca00b49a1a302b28ce05&chksm=ce91361ba5750736bee9728d574339eb542e9e8450b7366e462d3762062d9494f880b46665f7#rd", "authors": ["\u79d1\u6280\u548c\u4ea7\u4e1a"], "title": "\u5b54\u7ef4\u9633 | \u591a\u667a\u80fd\u4f53<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7814\u7a76\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-12-04 00:19:05", "summary": "\u6458\u8981\uff1a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u662f\u4eba\u7c7b\u901a\u5f80\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u65b9\u6cd5\u3002\u5f53\u524d\u5bf9\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u7814\u7a76\u4e3b\u8981\u4ece\u4efb\u52a1\u7c7b\u578b\u3001\u8bad\u7ec3\u6846\u67b6\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9762\u4e34\u7684\u6311\u6218\u548c\u7b97\u6cd5\u5e94\u75284\u4e2a\u65b9\u9762\u5c55\u5f00\u3002", "AI": {"tldr": "\u6458\u8981\uff1a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u662f\u4eba\u7c7b\u901a\u5f80\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u65b9\u6cd5\u3002\u5f53\u524d\u5bf9\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u7814\u7a76\u4e3b\u8981\u4ece\u4efb\u52a1\u7c7b\u578b\u3001\u8bad\u7ec3\u6846\u67b6\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9762\u4e34\u7684\u6311\u6218\u548c\u7b97\u6cd5\u5e94\u75284\u4e2a\u65b9\u9762\u5c55\u5f00\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.64d67583", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNTk1ODYwOA==&mid=2247502771&idx=1&sn=609b0b3c7f5f68a98a605b54e5eb1f3a&chksm=fb53a1767df565cb4b62e10dce82f1ab35ac137d8de46e142d723a42fc1d224f990bc5dfee5d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNTk1ODYwOA==&mid=2247502771&idx=1&sn=609b0b3c7f5f68a98a605b54e5eb1f3a&chksm=fb53a1767df565cb4b62e10dce82f1ab35ac137d8de46e142d723a42fc1d224f990bc5dfee5d#rd", "authors": ["\u8d22\u5bcc\u70df\u53f0"], "title": "\u94f6\u884c\u63a2\u7d22AI<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-12-03 16:00:15", "summary": "\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u5728\u6bcf\u4e00\u65f6\u523b\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\uff0c\u5e76\u6839\u636e\u73af\u5883\u53cd\u9988\u7684\u5956\u52b1\u6216\u60e9\u7f5a\uff0c\u8c03\u6574\u5176\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u957f\u671f\u6700\u5927\u5316\u7684\u603b\u5956\u52b1\u3002\u5728\u94f6\u884c\u4e1a\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u52bf\u4f53\u73b0\u5728\u5176\u5f3a\u5927\u7684\u52a8\u6001\u51b3\u7b56\u80fd\u529b\u4e0a\uff0c\u7279\u522b\u9002\u5408\u5e94\u5bf9\u91d1\u878d\u5e02", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u5728\u6bcf\u4e00\u65f6\u523b\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\uff0c\u5e76\u6839\u636e\u73af\u5883\u53cd\u9988\u7684\u5956\u52b1\u6216\u60e9\u7f5a\uff0c\u8c03\u6574\u5176\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u957f\u671f\u6700\u5927\u5316\u7684\u603b\u5956\u52b1\u3002\u5728\u94f6\u884c\u4e1a\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u52bf\u4f53\u73b0\u5728\u5176\u5f3a\u5927\u7684\u52a8\u6001\u51b3\u7b56\u80fd\u529b\u4e0a\uff0c\u7279\u522b\u9002\u5408\u5e94\u5bf9\u91d1\u878d\u5e02", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.39586b4f", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzY0MDEwOTI4Mw==&mid=2247487188&idx=1&sn=b1668e3e5f7d6b62f9d79e83f77af147&chksm=f1db7d026b6272d5a0f51355cad29d2f71240487e4070bf50a11450aa415fad2fe4112a29a61#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzY0MDEwOTI4Mw==&mid=2247487188&idx=1&sn=b1668e3e5f7d6b62f9d79e83f77af147&chksm=f1db7d026b6272d5a0f51355cad29d2f71240487e4070bf50a11450aa415fad2fe4112a29a61#rd", "authors": ["LifeNexAI"], "title": "Nature Machine Intelligence | \u540c\u6001\u52a0\u5bc6\u8d4b\u80fdAI\uff0c\u63a8\u52a8\u53ef\u4fe1\u5b89\u5168\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-12-03 16:00:00", "summary": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\u6765\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u6d89\u53ca\u5927\u91cf\u654f\u611f\u6570\u636e\u7684\u6536\u96c6\u548c\u5904\u7406\u3002\u5728\u533b\u7597\u8bca\u65ad\u3001\u91d1\u878d\u98ce\u63a7\u7b49\u573a\u666f\u4e2d\uff0c\u8fd9\u4e9b\u6570\u636e\u5f80\u5f80\u5305\u542b\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f\uff0c\u4f20\u7edfDRL\u8bad\u7ec3\u65b9\u5f0f\u4f7f\u5f97\u6570\u636e\u9762\u4e34\u88ab\u6076\u610f\u7a83\u53d6\u6216\u6ee5\u7528\u7684\u98ce\u9669", "AI": {"tldr": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\u6765\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u6d89\u53ca\u5927\u91cf\u654f\u611f\u6570\u636e\u7684\u6536\u96c6\u548c\u5904\u7406\u3002\u5728\u533b\u7597\u8bca\u65ad\u3001\u91d1\u878d\u98ce\u63a7\u7b49\u573a\u666f\u4e2d\uff0c\u8fd9\u4e9b\u6570\u636e\u5f80\u5f80\u5305\u542b\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f\uff0c\u4f20\u7edfDRL\u8bad\u7ec3\u65b9\u5f0f\u4f7f\u5f97\u6570\u636e\u9762\u4e34\u88ab\u6076\u610f\u7a83\u53d6\u6216\u6ee5\u7528\u7684\u98ce\u9669", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
