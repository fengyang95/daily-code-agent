<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.AI](#cs.AI) [Total: 18]
- [tldr.article](#tldr.article) [Total: 11]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution](https://arxiv.org/abs/2601.01126)
*Andrew Borthwick,Stephen Ash*

Main category: cs.CL

TL;DR: RoboPhD是一个AI自主研究系统，通过进化循环自动改进Text-to-SQL性能，从70行基线进化到1500行，在BIRD测试集上达到73.67%准确率，实现"跳过一级"部署效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索AI能否在没有人类领域指导的情况下，自主构建强大的Text-to-SQL代理系统，通过进化机制发现有效策略，降低部署成本。

Method: 采用闭环进化循环：SQL生成代理（数据库分析脚本+SQL生成指令）和进化代理（基于性能反馈设计新版本）。核心是ELO选择机制处理性能非传递性，通过迭代交叉授粉进化代理。

Result: 从70行基线进化到1500行代理，发现自适应数据库分析、列选择、证据解释和聚合等策略。在BIRD测试集上达到73.67%准确率，对较弱模型改进更大（Claude Haiku提升8.9点），实现"跳过一级"部署：进化Haiku超过原生Sonnet，进化Sonnet超过原生Opus。

Conclusion: AI能够仅从简单的人类提供起点自主构建强大的代理系统，进化机制对成本较低模型效果更显著，实现成本效益优化。

Abstract: We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.

</details>


### [2] [From Failure to Mastery: Generating Hard Samples for Tool-use Agents](https://arxiv.org/abs/2601.01498)
*Bingguang Hao,Zengzhuang Xu,Yuntao Wen,Xinyi Xu,Yang Liu,Tong Zhao,Maolin Wang,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Xiangyu Zhao,Chenyi Zhuang,Ji Zhang*

Main category: cs.CL

TL;DR: HardGen是一个自动化的智能体管道，用于生成具有可验证推理的困难工具使用训练样本，通过动态API图、高级工具实例化和闭环评估反馈来创建复杂多样的训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体工具使用训练数据生成方法主要采用随机采样和浅层生成范式，产生的轨迹简单且同质化，无法捕捉复杂、隐式的逻辑依赖关系，限制了智能体的能力提升。

Method: 1) 基于智能体失败案例建立动态API图并采样合成困难轨迹；2) 以这些轨迹为条件先验指导模块化抽象高级工具的实例化；3) 利用高级工具制定困难查询；4) 生成可验证的复杂思维链，并通过闭环评估反馈持续优化整个过程。

Result: 使用HardGen生成的数据集训练的4B参数模型在性能上超越了多个领先的开源和闭源竞争对手，包括GPT-5.2、Gemini-3-Pro和Claude-Opus-4.5。

Conclusion: HardGen能够有效生成复杂多样的工具使用训练数据，显著提升LLM智能体的工具使用能力，为未来研究提供了高质量的数据集和方法论。

Abstract: The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.

</details>


### [3] [Steerability of Instrumental-Convergence Tendencies in LLMs](https://arxiv.org/abs/2601.01584)
*Jakub Hoscilowicz*

Main category: cs.CL

TL;DR: 研究发现AI系统的能力与可操控性并非负相关，区分了授权与非授权操控性，揭示了开放权重模型的安全-安全困境：安全需要高操控性来实施控制，而安全需要低操控性来防止恶意行为。通过Qwen3模型实验发现，简短的反工具性提示后缀能显著减少工具性趋同行为。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统的两个关键属性：能力（系统能做什么）和可操控性（如何可靠地将行为转向预期结果），特别关注开放权重AI模型面临的基本安全-安全困境。

Method: 使用Qwen3模型（4B/30B；基础版/指导版/思考版）和InstrumentalEval工具，通过对比亲工具性和反工具性提示后缀来评估模型的行为操控性，测量工具性趋同行为（如关机回避、欺骗、自我复制）的变化。

Result: 简短的反工具性提示后缀能显著减少工具性趋同行为：Qwen3-30B指导版从亲工具性后缀下的81.69%降至反工具性后缀下的2.82%。在反工具性提示下，更大的对齐模型产生的趋同行为比小模型更少。

Conclusion: AI系统的能力与可操控性并非负相关，开放权重模型面临安全-安全困境：既需要高操控性来实施安全控制，又需要低操控性来防止恶意利用。提示工程能有效影响模型行为，更大的对齐模型在适当提示下表现出更好的安全性。

Abstract: We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.

</details>


### [4] [Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage](https://arxiv.org/abs/2601.01685)
*Jinwei Hu,Xinmiao Huang,Youcheng Sun,Yi Dong,Xiaowei Huang*

Main category: cs.CL

TL;DR: 论文提出一种新型认知共谋攻击，利用LLM过度思考倾向，通过真实证据片段构建欺骗性叙事，使受害者内化并传播虚假结论


<details>
  <summary>Details</summary>
Motivation: 随着LLM向自主代理发展，其推理能力引入了新的攻击面。现有攻击依赖隐蔽通信、后门或伪造文档，而本研究探索仅通过真实证据片段在公共渠道传播的认知共谋攻击

Method: 提出Generative Montage框架，包含Writer-Editor-Director三个角色，通过对抗性辩论和协调发布证据片段构建欺骗性叙事。开发CoPHEME数据集（基于真实谣言事件），在14个LLM家族上模拟攻击

Result: 攻击成功率：专有模型74.4%，开源权重模型70.6%。反直觉的是，推理能力更强的模型更容易受攻击，推理专用模型比基础模型或提示更易受影响。虚假信念会向下游传播，欺骗率超过60%

Conclusion: 揭示了LLM代理在动态信息环境中的社会技术脆弱性，推理能力反而增加了对认知共谋攻击的易感性，需要新的防御机制

Abstract: As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.

</details>


### [5] [CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning](https://arxiv.org/abs/2601.01825)
*Yaxin Cui,Yuanqiang Zeng,Jiapeng Yan,Keling Lin,Kai Ji,Jianhui Zeng,Sheng Zhang,Xin Luo,Binzhu Su,Chaolai Shen,Jiahao Yu*

Main category: cs.CL

TL;DR: 论文提出了CSCBench基准测试，用于评估大语言模型在商品供应链领域的推理能力，发现LLM在流程和认知维度表现良好，但在品种规则维度（特别是货运协议）表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用基准测试中表现出色，但在商品供应链这一受制度规则系统和可行性约束支配的领域，其能力尚未得到充分探索。商品供应链决策涉及复杂的流程阶段、品种特定规则和多层次推理深度，需要专门的评估框架。

Method: 提出了CSCBench基准测试，包含2300+个单选题，基于PVC 3D评估框架（流程、品种、认知）。流程维度与SCOR+Enable标准对齐；品种维度基于权威交易所指南/规则手册和行业报告，在耦合的材料-信息-财务约束下操作化商品特定规则系统；认知维度遵循修订版布鲁姆分类法。

Result: 在直接提示设置下评估代表性LLM，发现在流程和认知维度表现良好，但在品种维度表现显著下降，特别是在货运协议方面。这表明LLM在处理商品特定规则系统方面存在明显短板。

Conclusion: CSCBench为衡量和改进LLM在这一高风险领域的能力提供了诊断性基准。研究揭示了LLM在商品供应链推理中的局限性，特别是在处理复杂规则系统方面，为未来模型改进指明了方向。

Abstract: Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.

</details>


### [6] [Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment](https://arxiv.org/abs/2601.01862)
*Nuo Chen,Hanpei Fang,Piaohong Wang,Jiqun Liu,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 研究探索了通过提示让大语言模型模拟特定人格特质如何影响网络搜索中的相关性评估和置信度校准，发现某些人格特质（如低宜人性）能更好地与人类标注对齐，而低尽责性能有效平衡过度自信和自信不足，最终提出基于人格特征和置信度的分类器在有限训练数据下取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然表明提示可以让大语言模型模拟特定人格特质，但缺乏对这些模拟人格如何影响关键网络搜索决策（特别是相关性评估）的理解，以及它们如何影响置信度校准（过度自信或自信不足的倾向）。心理学文献表明这些偏见是特质相关的，但这一领域的研究存在空白。

Method: 研究评估了多个大语言模型（包括商业模型和开源模型），通过提示让它们模拟大五人格特质。在三个测试集（TREC DL 2019、TREC DL 2020和LLMJudge）上进行测试，为每个查询-文档对收集两个关键输出：相关性判断和自报告的置信度分数。基于这些发现，将人格条件化的分数和置信度作为特征整合到随机森林分类器中。

Result: 研究发现低宜人性人格与人类标注的一致性比无提示条件更好；低尽责性在平衡抑制过度自信和自信不足方面表现良好；相关分数和置信度分布在不同人格间存在系统性差异。基于人格特征和置信度的随机森林分类器在新数据集（TREC DL 2021）上超越了最佳单一人格条件，即使在有限训练数据下也表现出色。

Conclusion: 人格衍生的置信度提供了互补的预测信号，为开发更可靠、更符合人类判断的大语言模型评估器铺平了道路。这表明通过精心设计的人格模拟可以改善大语言模型在搜索评估任务中的表现。

Abstract: Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.
  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.

</details>


### [7] [Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents](https://arxiv.org/abs/2601.01885)
*Yi Yu,Liuyi Yao,Yuexiang Xie,Qingquan Tan,Jiaqi Feng,Yaliang Li,Libing Wu*

Main category: cs.CL

TL;DR: AgeMem是一个统一的记忆管理框架，将长短期记忆整合到智能体策略中，通过工具化操作和渐进式强化学习训练，在长时程推理任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在长时程推理中面临有限上下文窗口的限制，传统方法将长短期记忆作为分离组件处理，依赖启发式规则或辅助控制器，限制了适应性和端到端优化能力。

Method: 提出Agentic Memory (AgeMem)统一框架，将记忆管理直接集成到智能体策略中，将记忆操作暴露为工具化动作。采用三阶段渐进式强化学习策略，设计step-wise GRPO处理记忆操作带来的稀疏和不连续奖励问题。

Result: 在五个长时程基准测试中，AgeMem在多个LLM骨干网络上均优于强记忆增强基线，实现了更好的任务性能、更高质量的长时记忆和更高效的上下文使用。

Conclusion: AgeMem通过统一的记忆管理框架和渐进式强化学习训练，有效解决了LLM智能体在长时程推理中的记忆管理问题，展示了端到端优化记忆操作的优势。

Abstract: Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.

</details>


### [8] [Hidden State Poisoning Attacks against Mamba-based Language Models](https://arxiv.org/abs/2601.01972)
*Alexandre Le Mercier,Chris Develder,Thomas Demeester*

Main category: cs.CL

TL;DR: 本文研究了状态空间模型（如Mamba）中的隐藏状态中毒攻击（HiSPA），发现特定短输入短语会不可逆地覆盖其隐藏状态信息，导致模型出现部分失忆效应。作者创建了RoBench25基准来评估模型在HiSPA攻击下的信息检索能力，并证实了SSM对此类攻击的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）如Mamba提供了Transformer语言模型的高效替代方案，具有线性时间复杂度。然而，它们的对抗鲁棒性尚未得到充分研究。本文旨在探索SSM中特定短输入短语如何通过不可逆地覆盖隐藏状态信息来诱导部分失忆效应。

Method: 1. 提出隐藏状态中毒攻击（HiSPA）概念，即特定短输入短语不可逆地覆盖模型隐藏状态信息；2. 创建RoBench25基准来评估模型在HiSPA攻击下的信息检索能力；3. 对SSM和Transformer模型进行对比实验；4. 对Mamba的隐藏层进行可解释性研究，分析HiSPA攻击时的模式。

Result: 1. RoBench25基准证实了SSM对HiSPA攻击的脆弱性；2. 即使是最近的52B混合SSM-Transformer模型（Jamba家族）在优化的HiSPA触发器下也会在RoBench25上崩溃，而纯Transformer模型不会；3. HiSPA触发器显著削弱了Jamba模型在Open-Prompt-Injections基准上的表现，而纯Transformer不受影响；4. 可解释性研究揭示了Mamba隐藏层在HiSPA攻击期间的特定模式。

Conclusion: 状态空间模型对隐藏状态中毒攻击具有显著脆弱性，这种攻击会不可逆地覆盖模型隐藏状态信息。研究发现的模式可用于构建HiSPA缓解系统，为SSM的安全性改进提供了重要见解。

Abstract: State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.

</details>


### [9] [Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs](https://arxiv.org/abs/2601.02023)
*Amirali Ebrahimzadeh,Seyyed M. Salili*

Main category: cs.CL

TL;DR: 该研究评估了四个主流大语言模型在长上下文中的信息提取和推理能力，发现上下文长度增加并不总是提升性能，模型表现受信息分布、位置效应和反幻觉提示的显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM支持越来越长的输入上下文，但它们在规模化信息提取和推理方面的可靠性仍不明确。性能随上下文长度变化，且与真实语料中信息分布方式强烈交互。企业工作流中常将大量未过滤文档粘贴到LLM提示中，因此需要评估模型在长上下文中的可靠性。

Method: 引入扩展的"大海捞针"基准测试，评估四个生产级模型：Gemini-2.5-flash、ChatGPT-5-mini、Claude-4.5-haiku和Deepseek-v3.2-chat。分别评估字面提取、逻辑推理和幻觉风险。考虑位置效应、证据在长上下文中的真实分布，以及明确禁止捏造的提示。

Result: 更长的上下文并不保证更好性能，当相关证据被稀释或广泛分散时可能有害。模型间表现差异显著：一些在真实条件下严重退化，而另一些在较长上下文长度下更稳健。反幻觉指令可能使某些模型过于保守，显著降低字面提取和逻辑推理的准确性。模型经常难以识别和优先处理相关信息。

Conclusion: 有效上下文长度和模型对长上下文的特定鲁棒性对于LLM在研究和业务中的可靠部署至关重要。许多失败源于无效的上下文利用，而非信息缺失。企业工作流中直接粘贴大量未过滤文档的做法需要谨慎考虑模型的实际能力。

Abstract: Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体强化学习的Agentic AI框架，用于实现自主、透明、实时的信用风险评估决策系统，相比传统机器学习模型在决策速度、透明度和响应性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 金融服务的快速数字化对自主、透明、实时的信用风险决策系统提出了迫切需求。传统机器学习模型虽然擅长模式识别，但缺乏现代金融运营所需的适应性推理、情境感知和自主性。

Method: 提出了一个Agentic AI框架，包含多智能体系统、强化学习、自然语言推理、可解释AI模块和实时数据吸收管道。系统包括智能体协作协议、风险评分引擎、可解释性层和持续反馈学习循环。

Result: 研究发现该系统在决策速度、透明度和响应性方面优于传统信用评分模型。但仍存在模型漂移风险、高维数据解释不一致、监管不确定性以及低资源环境基础设施限制等实际限制。

Conclusion: 该系统具有变革信用分析的巨大潜力，未来研究应关注动态监管合规机制、新型智能体协作、对抗鲁棒性以及跨国信用生态系统中的大规模实施。

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [11] [CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations](https://arxiv.org/abs/2601.00821)
*Tao An*

Main category: cs.AI

TL;DR: CogCanvas是一个无需训练的框架，通过提取对话中的认知构件（决策、事实、提醒）并组织成时间感知图，解决大语言模型在长对话中上下文限制与信息保真度的矛盾。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长对话中面临上下文窗口限制与信息保真度的基本矛盾。现有方法（截断和摘要）要么丢弃早期信息，要么丢失细节信息。

Method: 提出CogCanvas框架，从对话轮次中提取基于原文的认知构件（决策、事实、提醒），并将其组织成时间感知图，实现抗压缩检索。

Result: 在LoCoMo基准测试中，CogCanvas达到34.7%总体准确率，优于RAG（25.6%）和GraphRAG（13.7%）。在时间推理上表现尤为突出：31.5% vs. 9.3%（RAG）和5.0%（GraphRAG），相对提升530%。在多跳因果推理上达到81.0%通过率，vs. GraphRAG的40.0%。

Conclusion: 虽然专门训练的方法（如EverMemOS约92%）能达到更高绝对分数，但CogCanvas作为无需训练的方法为实践者提供了可立即部署的替代方案，显著优于标准基线方法。

Abstract: Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.
  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.
  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.

</details>


### [12] [Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models](https://arxiv.org/abs/2601.00848)
*Ron F. Del Rosario*

Main category: cs.AI

TL;DR: 提出一种基于OpenTelemetry追踪分析的多智能体AI工作流时序攻击检测方法，通过QLoRA微调语言模型，在资源受限硬件上实现准确率从42.86%提升至74.29%


<details>
  <summary>Details</summary>
Motivation: 当前多智能体AI工作流面临时序攻击威胁，需要开发能够检测此类攻击模式的安全模型。现有方法缺乏针对多智能体协调攻击和监管违规的检测能力，且缺乏可复现的框架

Method: 收集80,851个网络安全示例和35,026个合成OpenTelemetry追踪数据，在NVIDIA DGX Spark ARM64硬件上使用QLoRA进行三次迭代微调，采用策略性数据增强

Result: 自定义基准测试准确率从42.86%提升至74.29%，获得31.4个百分点的显著提升。针对特定知识差距的定向训练优于无差别扩展

Conclusion: 训练数据组成从根本上决定模型行为，虽然实际部署需要人工监督（存在误报），但本研究建立了首个可复现框架，使从业者能够构建适应其威胁环境的定制化智能体安全模型

Abstract: We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.

</details>


### [13] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: UCL是一个将提示工程从启发式实践转化为系统化优化的数学框架，通过系统评估显著减少29.8%的token使用并降低成本，揭示了过规范悖论现象。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程主要依赖启发式实践，缺乏系统化的数学框架。作者希望将提示优化转化为可系统化的数学问题，提高LLM交互的效率并降低成本。

Method: 提出Universal Conditional Logic (UCL)数学框架，包含指示函数(I_i)、结构开销函数(O_s = gamma * sum(ln C_k))、早期绑定等核心机制。通过系统评估(N=305, 11个模型, 4次迭代)验证框架有效性。

Result: 显著减少29.8%的token使用(t(10)=6.36, p < 0.001, Cohen's d = 2.01)，相应降低成本。发现过规范悖论：超过阈值S* = 0.509后，额外规范会二次降低性能。不同模型架构需要不同的UCL配置。

Conclusion: UCL建立了可校准的高效LLM交互框架，模型家族特定的优化是重要研究方向。框架将提示工程从启发式实践转化为系统化优化。

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


### [14] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: ElecTwit是一个模拟社交媒体政治选举中多智能体说服交互的框架，通过真实环境实验发现LLM使用了25种说服技巧，揭示了模型架构和训练对社交模拟动态的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究多使用基于游戏的模拟，存在局限性。作者希望创建一个更真实的模拟环境来研究多智能体系统中的说服行为，特别是在社交媒体政治选举场景下。

Method: 开发了ElecTwit模拟框架，在模拟社交媒体政治选举环境中测试多个LLM模型，分析它们使用的说服技巧和交互动态。

Result: 观察到大多数测试的LLM使用了25种特定的说服技巧，范围比先前报道的更广。不同模型在技巧使用和整体说服输出上存在差异，反映了模型架构和训练的影响。还发现了"真相核心"信息和"墨水"痴迷等独特现象。

Conclusion: 该研究为在真实世界环境中评估有说服力的LLM智能体奠定了基础，有助于确保对齐并防止危险结果。

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [15] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: JiSi框架通过查询-响应混合路由、支持集聚合器选择和自适应路由-聚合切换，使开源LLM协作超越Gemini-3-Pro，成本仅47%


<details>
  <summary>Details</summary>
Motivation: 探索集体智能作为替代单体扩展的路径，解决当前LLM路由和聚合的三个瓶颈：查询式路由局限、静态聚合方法、路由与聚合互补性未充分利用

Method: 提出JiSi框架，包含：1) 查询-响应混合路由，同时捕获语义信息和问题难度；2) 基于支持集的聚合器选择，评估聚合能力和领域能力；3) 自适应路由-聚合切换，动态利用路由和聚合优势

Result: 在9个基准测试中，JiSi通过协调10个开源LLM，以仅47%的成本超越了Gemini-3-Pro，同时优于主流基线方法

Conclusion: 集体智能代表了通往AGI的新路径，开源LLM的协作可以超越顶级单体模型

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [16] [KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models](https://arxiv.org/abs/2601.01366)
*Zixian Liu,Sihao Liu,Yuqi Zhao*

Main category: cs.AI

TL;DR: 提出了KGCE基准平台，通过知识库增强和双图评估框架解决教育场景中跨平台任务执行的评估问题，特别针对学校专用软件的理解瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准框架在教育场景的跨平台任务支持不足，特别是对学校专用软件（如小雅智能助手、华师匣子等）缺乏理解导致代理效率显著下降，且当前评估方法依赖粗粒度指标难以捕捉复杂任务中的详细执行效率。

Method: 构建包含104个教育相关任务的数据集，涵盖Windows、Android和跨平台协作任务；提出双图评估框架将任务分解为多个子目标并验证完成状态；开发了包含学校专用软件知识库的增强代理系统。

Result: 开发了KGCE基准平台，提供细粒度评估指标，代码已开源在GitHub上。

Conclusion: KGCE通过知识库增强和双图评估框架有效解决了教育场景中跨平台任务执行的评估瓶颈，特别是针对私有领域软件的理解问题。

Abstract: With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.

</details>


### [17] [Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making](https://arxiv.org/abs/2601.01522)
*Danial Amin*

Main category: cs.AI

TL;DR: 论文提出了一种基于贝叶斯框架的多LLM协同决策方法，用于处理具有不对称错误成本的顺序决策问题，相比传统单LLM方法显著降低了成本并提高了公平性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在不对称错误成本的决策场景（如招聘、医疗分诊、欺诈检测）中被广泛部署，但主流方法仅查询单个LLM进行后验概率估计并基于"置信度"阈值行动，这在顺序决策中是不充分的。

Method: 提出贝叶斯、成本感知的多LLM协同框架，将LLM视为近似似然模型而非分类器。通过对比提示为每个候选状态获取似然估计，使用稳健统计方法聚合多个模型结果，在新证据到达时基于显式先验使用贝叶斯规则更新信念。

Result: 在简历筛选实验中（成本：错失人才40000美元/次，面试2500美元/次，电话筛选150美元/次），使用5个LLM（GPT-4o、Claude 4.5 Sonnet、Gemini Pro、Grok、DeepSeek）处理1000份简历，相比最佳单LLM基线降低总成本294000美元（34%），并将人口统计公平性提高45%（最大群体差距从22%降至5%）。

Conclusion: 正确的概率基础为顺序决策带来了理论优势，多LLM聚合贡献了51%的成本节约，顺序更新贡献43%，分歧触发的信息收集贡献20%，证明了该框架在不对称成本决策中的有效性。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds "confidence," and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.

</details>


### [18] [Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation](https://arxiv.org/abs/2601.01546)
*Letian Kong,Qianran,Jin,Renyu Zhang*

Main category: cs.AI

TL;DR: 提出两阶段框架改善LLM在复杂决策环境中的行为对齐：第一阶段上下文形成明确指定实验设计，第二阶段上下文导航指导推理过程。验证表明复杂环境需要两阶段，简单任务仅需第一阶段。


<details>
  <summary>Details</summary>
Motivation: LLM在模拟人类行为时，在需要预测他人行动和基于观察行为形成信念的复杂决策环境中，会系统性地偏离人类决策。需要改进LLM在行为研究中的对齐表现。

Method: 提出两阶段框架：1) 上下文形成：明确指定实验设计，建立决策任务及其上下文的准确表示；2) 上下文导航：在该表示内指导推理过程以做出决策。在三个不同决策环境中验证：顺序购买游戏、众筹游戏和需求估计任务。

Result: 在四个SOTA模型（GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1）上验证发现：复杂决策环境需要两阶段才能实现与人类基准的行为对齐，而简单的需求估计任务仅需上下文形成阶段。

Conclusion: 阐明了每个阶段在何时必要，为设计和诊断LLM社会模拟提供了系统方法，可作为行为研究中人类受试者的补充。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.

</details>


### [19] [CaveAgent: Transforming LLMs into Stateful Runtime Operators](https://arxiv.org/abs/2601.01569)
*Maohao Ran,Zhenglin Wan,Cooper Lin,Yanting Zhang,Hongyu Xin,Hongwei Fan,Yibo Xu,Beier Luo,Yaxin Zhou,Wangbo Zhao,Lijie Yang,Lang Feng,Fuchao Yang,Jingxuan Wu,Yiqiao Huang,Chendong Ma,Dailing Jiang,Jianbo Deng,Sihui Han,Bo An,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: CaveAgent是一个将LLM从文本生成器转变为运行时操作员的框架，通过双流上下文架构和状态化运行时管理，解决了传统JSON函数调用在长时任务中的脆弱性和上下文漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体系统受限于文本中心范式，传统JSON函数调用方法在处理长时任务时存在脆弱的多轮依赖和上下文漂移问题，需要更强大的状态管理和执行能力。

Method: 提出CaveAgent框架，采用双流上下文架构：轻量级语义流用于推理，持久化确定性Python运行时流用于执行。引入状态化运行时管理，支持复杂Python对象（如DataFrame、数据库连接）的注入、操作和跨轮次持久化。

Result: 在Tau²-bench、BFCL等基准测试中表现优异：零售任务成功率提升10.5%，多轮场景总token消耗减少28.4%。数据密集型任务中，直接变量存储检索减少59% token消耗，能处理导致其他代理上下文溢出的海量数据。

Conclusion: CaveAgent通过将LLM转变为运行时操作员，解决了传统代理系统的局限性，实现了更高效、可靠的长时任务执行，为数据密集型应用提供了可扩展的解决方案。

Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from "LLM-as-Text-Generator" to "LLM-as-Runtime-Operator." We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\% success rate improvement on retail tasks and reduces total token consumption by 28.4\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.

</details>


### [20] [Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://arxiv.org/abs/2601.01718)
*YuanLab. ai,:,Shawn Wu,Sean Wang,Louie Li,Darcy Chen,Allen Wang,Jiangang Luo,Xudong Zhao,Joseph Shen,Gawain Ma,Jasper Jia,Marcus Mao,Claire Wang,Hunter He,Carol Wang,Zera Zhang,Jason Wang,Chonly Shen,Leo Zhang,Logan Chen,Qasim Meng,James Gong,Danied Zhao,Penn Zheng,Owen Zhu,Tong Yu*

Main category: cs.AI

TL;DR: Yuan3.0 Flash是一个开源的MoE多模态大语言模型，具有37亿激活参数和400亿总参数，专为企业任务优化，同时保持通用任务竞争力。为解决大型推理模型的"过度思考"问题，提出了RAPO训练算法。在RAG、复杂表格理解和摘要等企业任务上表现优异，在数学、科学等推理任务上达到前沿模型精度，但仅需1/4到1/2的平均token数。


<details>
  <summary>Details</summary>
Motivation: 针对企业任务优化大型语言模型，同时解决大型推理模型中常见的"过度思考"现象，开发一个既能在企业任务上表现优异，又能在通用任务上保持竞争力的高效模型。

Method: 采用混合专家(MoE)架构的多模态大语言模型，包含37亿激活参数和400亿总参数。提出Reflection-aware Adaptive Policy Optimization (RAPO)强化学习训练算法，有效调节过度思考行为。

Result: 在企业任务（RAG、复杂表格理解、摘要）上表现优异；在数学、科学等推理任务上达到前沿模型精度，但仅需1/4到1/2的平均token数；已完全开源供研究和实际部署。

Conclusion: Yuan3.0 Flash是一个高效的企业导向多模态大语言模型，通过RAPO算法解决了过度思考问题，在企业任务和通用推理任务上都表现出色，且计算效率更高。

Abstract: We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.

</details>


### [21] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: 该论文是一篇关于AI智能体架构的综述，系统性地整理了智能体在推理、规划、工具使用等方面的技术，提出了统一的分类框架，并讨论了设计权衡、评估挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型与推理、规划、记忆和工具使用相结合，AI智能体正在成为连接自然语言意图与现实世界计算的实用接口。需要系统性地整理这一快速发展的领域，为研究者和实践者提供清晰的架构分类和设计指导。

Method: 采用综述研究方法，对现有AI智能体架构进行系统性整理和分类。提出了统一的分类框架，涵盖智能体组件（策略/LLM核心、记忆、世界模型、规划器、工具路由器、批评器）、编排模式（单智能体vs.多智能体；集中式vs.去中心化协调）和部署设置（离线分析vs.在线交互辅助；安全关键vs.开放任务）。

Result: 建立了AI智能体架构的全面分类体系，识别了关键设计权衡（延迟vs.准确性、自主性vs.可控性、能力vs.可靠性），分析了评估挑战（非确定性、长时程信用分配、工具和环境变异性、隐藏成本），并总结了当前的测量和基准测试实践。

Conclusion: AI智能体架构领域正在快速发展，但仍面临诸多挑战，包括工具动作的验证和安全防护、可扩展的记忆和上下文管理、智能体决策的可解释性，以及在真实工作负载下的可重复评估。需要进一步研究来解决这些开放性问题。

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [22] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: 提出Jenius-Agent框架，通过自适应提示生成、上下文感知工具编排和分层内存机制优化LLM智能体，在任务准确性上提升20%，降低token成本和延迟


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体系统发展，提升其在上下文理解、工具使用和响应生成方面的任务性能变得日益重要。现有研究虽然改进了智能体整体设计，但对其内部推理和工具使用流程的系统性优化仍不足

Method: 提出Jenius-Agent框架，包含三个核心创新：1) 自适应提示生成策略，根据智能体状态和任务目标调整提示；2) 上下文感知工具编排模块，进行工具分类、语义检索和自适应调用；3) 分层内存机制，整合会话内存、任务历史和外部摘要，通过动态摘要和压缩提高相关性和效率。框架还集成了基于MCP的工具、文件I/O和执行反馈优化

Result: 实验显示任务准确性提升20%，同时降低了token成本、响应延迟和调用失败率。框架已在Jenius平台部署，提供轻量级、可扩展的解决方案

Conclusion: Jenius-Agent框架通过系统性优化智能体内部流程，显著提升了任务性能和效率，为稳健、协议兼容的自主智能体提供了实用解决方案

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [23] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: ChaosBench-Logic是一个评估LLM在混沌动力系统领域逻辑推理能力的基准，包含30个系统、621个问题，测试多种推理类型，发现前沿LLM在单项准确率可达91-94%，但在组合推理上得分为0%，对话准确率53.1-75.5%。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言任务表现出色，但在需要精确逻辑和符号推理的领域仍然脆弱。混沌动力系统特别具有挑战性，因为混沌是确定性的，但常被误解为随机性或复杂性。需要评估LLM在这类科学推理任务上的能力。

Method: 引入ChaosBench-Logic基准，使用统一的一阶逻辑本体论评估30个混沌动力系统。每个系统标注11个语义谓词的真值分配，生成621个问题涵盖7个推理类别：多步蕴含、跨系统类比、反事实推理、偏见探测、多轮对话等。定义逻辑准确性、蕴含一致性、对话连贯性和矛盾性等指标，并发布开源评估流程。

Result: 前沿LLM（GPT-4、Claude 3.5 Sonnet、Gemini 2.5 Flash、LLaMA-3 70B）在单项准确率达到91-94%，但在组合项目上得分为0%，表现出脆弱的全局连贯性。对话级准确率从53.1%（GPT-4 CoT）到75.5%（LLaMA-3 zero-shot）。

Conclusion: ChaosBench-Logic为诊断LLM推理失败提供了严格的测试平台，并为开发改进LLM科学推理能力的神经符号方法奠定了基础。

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [24] [Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management](https://arxiv.org/abs/2601.02061)
*Faizan Ahmed,Aniket Dixit,James Brusey*

Main category: cs.AI

TL;DR: 论文研究了通过高阶导数惩罚实现动作平滑正则化，在连续控制基准和建筑能源管理应用中验证了其有效性，特别是三阶导数惩罚（急动度最小化）能显著提升平滑性同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习代理常表现出不稳定、高频的控制行为，这在实际部署中会导致能耗过高和机械磨损问题，需要找到平衡RL优化与实际操作约束的方法。

Method: 采用高阶导数惩罚进行动作平滑正则化，从连续控制基准的理论理解到建筑能源管理的实际验证，系统评估了不同阶次的导数惩罚效果。

Result: 在四个连续控制环境中，三阶导数惩罚（急动度最小化）始终实现最佳平滑性同时保持竞争力性能；在HVAC控制系统中，平滑策略将设备切换减少了60%。

Conclusion: 高阶动作正则化为RL优化与能源关键应用中的操作约束之间建立了有效桥梁，具有显著的实用价值。

Abstract: Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.

</details>


### [25] [EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning](https://arxiv.org/abs/2601.02163)
*Chuanrui Hu,Xingze Gao,Zuyi Zhou,Dannong Xu,Yi Bai,Xintong Li,Hui Zhang,Tong Li,Chong Zhang,Lidong Bing,Yafeng Deng*

Main category: cs.AI

TL;DR: EverMemOS是一个自组织记忆操作系统，采用类记忆印迹的生命周期管理计算记忆，通过将对话流转换为记忆单元、组织成主题场景、进行重构回忆，显著提升LLM在长期交互中的记忆能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为长期交互代理时，有限的上下文窗口难以维持连贯行为。现有记忆系统通常存储孤立记录并检索片段，无法有效整合演变的用户状态和解决冲突。

Method: 提出EverMemOS自组织记忆操作系统：1) 情节痕迹形成：将对话流转换为包含情节痕迹、原子事实和时间边界前瞻信号的记忆单元；2) 语义整合：将记忆单元组织成主题记忆场景，提炼稳定语义结构并更新用户画像；3) 重构回忆：执行记忆场景引导的代理检索，为下游推理组合必要且充分的上下文。

Result: 在LoCoMo和LongMemEval基准测试中，EverMemOS在记忆增强推理任务上达到最先进性能。在PersonaMem v2上的画像研究和定性案例研究展示了用户画像和前瞻等聊天导向能力。

Conclusion: EverMemOS通过自组织记忆操作系统有效解决了LLM长期交互中的记忆限制问题，实现了更连贯和智能的代理行为。

Abstract: Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.

</details>


### [26] [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
*Sourena Khanzadeh*

Main category: cs.AI

TL;DR: 论文提出Project Ariadne框架，使用结构因果模型和反事实逻辑来审计LLM智能体推理的因果完整性，发现当前智能体存在严重的"因果解耦"问题，推理痕迹只是"推理剧场"而非真正的决策驱动因素。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体越来越多地承担高风险自主决策任务，其推理过程的透明度成为关键安全问题。虽然思维链提示允许生成人类可读的推理痕迹，但尚不清楚这些痕迹是模型输出的真实生成驱动因素还是事后合理化解释。

Method: 提出Project Ariadne框架，利用结构因果模型和反事实逻辑来审计智能体推理的因果完整性。通过硬干预（do-演算）对中间推理节点进行系统操作（反转逻辑、否定前提、反转事实主张），测量终端答案的因果敏感性。

Result: 评估发现存在持续的"忠实性差距"，定义并检测到广泛的"因果解耦"故障模式，在事实和科学领域中违反密度高达0.77。智能体在内部逻辑矛盾的情况下仍得出相同结论，证明推理痕迹只是"推理剧场"，决策由潜在参数先验控制。

Conclusion: 当前智能体架构本质上容易产生不忠实的解释，提出Ariadne分数作为新的基准，用于对齐陈述逻辑与模型行动。

Abstract: As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.

</details>


### [27] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: Falcon-H1R是一个7B参数的推理优化模型，证明了小语言模型也能达到竞争力的推理性能，通过精心数据筛选和针对性训练策略，在参数效率、推理速度和准确性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型（SLMs）能否通过优化训练策略和架构设计，在保持参数效率的同时达到与大模型相当的推理性能，为实际应用提供更实用的推理系统。

Method: 采用7B参数模型，通过精心数据筛选和针对性训练策略（包括高效的监督微调和强化学习扩展），结合混合并行架构设计提高推理速度，并利用DeepConf方法实现最优测试时扩展效率。

Result: Falcon-H1R在多个推理密集型基准测试中，性能匹配或超越比其大2-7倍的SOTA推理模型，实现了推理效率的3D提升（更快推理、更高token效率、更高准确性），成为扩展推理系统的实用骨干。

Conclusion: 紧凑模型通过针对性的模型训练和架构选择，能够提供强大且可扩展的推理性能，证明了小语言模型在推理任务中的可行性。

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [28] [Is Agentic Metadata the Next Infrastructure Layer?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fis-agentic-metadata-the-next-infrastructure-layer%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/KcLruhcU92qwevZTMgtFnd5A-Cp7_VWcL1Gre-lRoJ0=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了AI代理生成的"代理元数据"作为基础设施层的潜力，包括推理轨迹、用户提示和工具调用等数据，这些数据对调试、持续改进、成本优化和治理合规具有重要价值，但收集、存储和利用这些碎片化数据面临挑战。


<details>
  <summary>Details</summary>
Motivation: AI代理生成丰富的元数据，这些数据对系统调试、性能优化、成本控制和合规管理具有重要价值，但目前这些数据分散且难以有效利用，需要建立系统化的基础设施来处理。

Method: 论文提出了将"代理元数据"概念化为基础设施层的框架，探讨了如何系统化收集、存储和操作化这些数据的方法，包括数据标准化、存储架构和利用模式。

Result: 识别了代理元数据作为基础设施层的关键价值，分析了当前数据碎片化带来的挑战，提出了建立统一元数据管理系统的必要性和潜在解决方案。

Conclusion: 代理元数据有望成为AI代理生态系统中的重要基础设施层，需要建立标准化框架和工具来解决数据碎片化问题，以充分发挥其在调试、优化和治理方面的价值。

Abstract: Is Agentic Metadata the Next Infrastructure Layer? (13 minute read) AI agents generate "agentic metadata," which includes rich data like reasoning traces, user prompts, and tool calls. This metadata is invaluable for debugging, continuous improvement, cost optimization, and governance and compliance. However, effectively collecting, storing, and operationalizing this fragmented data has a lot of challenges.

</details>


### [29] [Claude Code On-The-Go](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgranda.org%2Fen%2F2026%2F01%2F02%2Fclaude-code-on-the-go%2F%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/zeho-540QkPZEVqkTevbpL_gbJzRnru7eGZJtNXzdUs=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 用户使用云虚拟机、Termius和推送通知在手机上并行运行多个Claude Code代理，实现随时随地异步开发


<details>
  <summary>Details</summary>
Motivation: 开发者希望能够在移动设备上高效进行编程开发，突破传统开发环境的限制，实现随时随地异步开发

Method: 使用云虚拟机作为开发环境，通过Termius终端应用连接，结合推送通知机制，在手机上并行运行多个Claude Code代理

Result: 成功在手机上实现了异步开发能力，开发者可以从任何地方进行编程工作，提高了开发灵活性和效率

Conclusion: 通过云基础设施和移动终端应用的结合，可以在移动设备上实现高效的程序开发工作流

Abstract: Claude Code On-The-Go (4 minute read) This dev uses a cloud VM, Termius, and push notifications to run multiple Claude Code agents in parallel on their phone, allowing for asynchronous development from anywhere.

</details>


### [30] [Awesome Agentic Patterns](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fnibzard%2Fawesome-agentic-patterns%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/uLUvjgRfWujt6zEVN8ZOpHckCqNnzk7dn-rdsbp1-ws=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub仓库收集真实世界中的智能体模式，包括技巧、工作流和微架构，帮助自主或半自主AI智能体在生产环境中完成有用任务


<details>
  <summary>Details</summary>
Motivation: 为AI智能体开发提供实用的生产级模式参考，解决实际应用中的挑战，促进智能体技术的落地应用

Method: 收集整理真实世界中的智能体模式，包括技巧、工作流程和微架构设计，形成可复用的模式库

Result: 创建了一个包含多种实用智能体模式的GitHub仓库，为开发者提供生产环境中的最佳实践参考

Conclusion: 通过模式库的形式系统化智能体开发经验，有助于加速智能体技术的实际应用和推广

Abstract: Awesome Agentic Patterns (GitHub Repo) This is a catalog of real-world agentic AI patterns, including tricks, workflows, and mini-architectures, designed to help autonomous or semi-autonomous AI agents accomplish useful tasks in production environments.

</details>


### [31] [The future of agentic coding: conductors to orchestrators](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Ffuture-agentic-coding%2F%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/iIyiVIDTxnzLc6xeQgY4PdB5pf2jKb_GkVIzJ1-zNeU=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了代理编码的未来，预测软件工程师将从直接实施者转变为"指挥家"指导单个AI助手，并进一步发展为"编排者"管理自主代理舰队以加速软件开发。


<details>
  <summary>Details</summary>
Motivation: 随着AI助手能力的增强，软件开发角色正在发生根本性转变。传统的工程师作为直接编码者的角色正在被重新定义，需要探索如何有效利用AI代理来提高开发效率。

Method: 通过分析当前AI编码助手的发展趋势，提出从"指挥家"到"编排者"的角色演进模型，探讨工程师如何从指导单个AI助手发展到管理多个自主代理的协作。

Result: 提出了软件工程师角色演进的框架：从直接编码者到指导单个AI的"指挥家"，再到管理代理舰队的"编排者"，这种转变将显著加速软件开发流程。

Conclusion: 代理编码的未来将使软件工程师的角色发生根本性转变，从实施者转变为战略指导者和系统管理者，通过有效利用AI代理舰队来大幅提升开发效率。

Abstract: The future of agentic coding: conductors to orchestrators (30 minute read) The future of agentic coding will transform software engineers from direct implementers into "conductors" guiding single AI assistants and increasingly into "orchestrators" managing autonomous fleets of agents to speed up software development.

</details>


### [32] [Build Your First Reddit AI Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaven.com%2Fp%2Fe5ba0f%2Fbuild-your-first-reddit-ai-agent-automate-growth%3Futm_source=tldrmarketing/1/0100019b8e1a536c-bb5725fd-2aef-4c88-bc96-6a62d5e1dbf7-000000/aqGmm9G9ltzNTFzZBNuMkMse9Yqy5eeEgV4Wj0qtuE8=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 关于Reddit AI代理的在线研讨会，指导如何创建自动识别高意向产品讨论的工作流


<details>
  <summary>Details</summary>
Motivation: Reddit上约25%的帖子与产品推荐相关，表明这是一个高购买意向的平台，需要自动化工具来识别和利用这些讨论

Method: 通过在线研讨会形式，逐步指导创建自动化工作流，用于自动发现和提取Reddit上的高意向产品讨论

Result: 研讨会将于1月14日太平洋时间上午9:30举行，参与者将学习到构建Reddit AI代理的实际技能

Conclusion: 该研讨会为开发者提供了在Reddit平台上构建AI代理的实用指导，帮助利用平台上的高意向用户讨论

Abstract: Build Your First Reddit AI Agent (Webinar) About 25% of Reddit posts relate to product recommendations, making it a high-intent space for buyers. This webinar walks through how to create a workflow that automatically surfaces high-intent discussions. It will take place on January 14 at 9:30 AM PST.

</details>


### [33] [WRAP up your backlog with GitHub Copilot coding agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fwrap-up-your-backlog-with-github-copilot-coding-agent%2F%3Futm_source=tldrdevops/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ZL4c67C525dVqExcqB5TiGEQP8rYJSaYUnWVwtCRhCM=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: WRAP是一个帮助开发者最大化GitHub Copilot编码代理效能的框架，通过编写清晰问题、细化指令、分解原子任务，并结合人类判断与AI实现高效、准确、可扩展的代码补全。


<details>
  <summary>Details</summary>
Motivation: 开发者在使用GitHub Copilot等AI编码助手时，经常面临指令不清晰、任务分解不当等问题，导致AI生成的代码质量不高、效率低下。需要一种系统化方法来优化人类与AI编码代理的协作。

Method: WRAP框架包含四个核心步骤：1) 编写清晰的问题描述；2) 细化和优化指令；3) 将复杂工作分解为原子任务；4) 结合人类判断与AI能力进行代码补全。该方法强调系统化的人机协作流程。

Result: WRAP框架能够显著提升GitHub Copilot等编码代理的效能，实现更高效、准确和可扩展的代码补全。通过结构化的人机协作流程，开发者可以更好地利用AI编码助手处理积压任务。

Conclusion: WRAP提供了一个实用的框架来优化开发者与AI编码代理的协作，通过系统化的问题描述、指令优化和任务分解，结合人类判断，能够最大化GitHub Copilot等工具的效能，提高开发效率。

Abstract: WRAP up your backlog with GitHub Copilot coding agent (6 minute read) WRAP helps developers maximize GitHub Copilot coding agent by writing clear issues, refining instructions, breaking work into atomic tasks, and pairing human judgment with AI for efficient, accurate, and scalable code completion.

</details>


### [34] [Claude Code On-The-Go](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgranda.org%2Fen%2F2026%2F01%2F02%2Fclaude-code-on-the-go%2F%3Futm_source=tldrdevops/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/mFK7i431xcCOSjbPB0NsyAOTyXUhbK01s64UWZq1pA0=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍一种移动优先的开发环境配置，通过手机使用Termius+mosh连接按需付费的Vultr虚拟机，运行多个Claude Code代理并行工作，利用tmux保持会话持久性，git worktrees支持并行功能开发。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备上进行代码开发的挑战，提供灵活、可扩展的云端开发环境，使开发者能够随时随地通过手机进行高效编程。

Method: 使用Termius移动端SSH客户端配合mosh协议连接Vultr按需付费虚拟机，通过Tailscale确保安全连接，利用tmux管理持久会话，git worktrees支持并行功能分支开发，运行多个Claude Code代理。

Result: 成功构建了一个移动优先的云端开发环境，使开发者能够通过手机高效运行多个AI代码代理，实现随时随地的编程工作流。

Conclusion: 移动设备配合云端虚拟机可以成为有效的开发平台，通过合适的工具链配置能够实现高效的移动编程体验。

Abstract: Claude Code On-The-Go (3 minute read) A mobile-first development setup runs multiple Claude Code agents in parallel from a phone using Termius + mosh into a pay-per-use Vultr VM secured by Tailscale, with tmux for persistence and git worktrees for parallel features.

</details>


### [35] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ioP_EkoNUfVcsmsknzSFO06B2cFFm0mAN61S-bUCwi4=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 在手机上通过Termius+mosh连接Vultr云服务器，运行多个Claude Code代理进行并行开发，使用tmux保持会话，git worktrees支持并行功能开发


<details>
  <summary>Details</summary>
Motivation: 解决移动端开发环境搭建问题，让开发者能够在手机上使用强大的代码代理工具进行并行开发工作

Method: 采用移动优先的开发设置：Termius+mosh连接Vultr按需付费VM，Tailscale确保安全，tmux提供会话持久化，git worktrees支持并行功能开发

Result: 成功实现了在手机上运行多个Claude Code代理的并行开发环境，提供了完整的移动开发解决方案

Conclusion: 移动设备可以成为有效的开发平台，通过合适的工具链组合，能够在手机上实现高效的代码代理并行开发工作流

Abstract: Claude Code On-The-Go (3 minute read) A mobile-first development setup runs multiple Claude Code agents in parallel from a phone using Termius + mosh into a pay-per-use Vultr VM secured by Tailscale, with tmux for persistence and git worktrees for parallel features.

</details>


### [36] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ReGecqGdG4Od6j6K_o_273c19CP8wAnkP08q71Sd5jY=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 在手机上通过Termius+mosh连接Vultr云服务器，运行多个Claude Code代理并行开发，使用tmux保持会话，git worktrees支持并行功能开发


<details>
  <summary>Details</summary>
Motivation: 为移动开发者提供便捷的开发环境，让开发者能够在手机上运行多个AI编码代理，实现随时随地的并行开发工作

Method: 使用Termius+mosh连接Vultr按需付费云服务器，通过Tailscale确保安全连接，tmux保持会话持久化，git worktrees支持并行功能分支开发

Result: 成功构建了移动优先的开发环境，能够在手机上运行多个Claude Code代理进行并行开发，实现随时随地的编码工作

Conclusion: 移动设备可以成为有效的开发平台，通过合适的工具链配置，能够在手机上实现多代理并行开发环境

Abstract: Claude Code On-The-Go (3 minute read) A mobile-first development setup runs multiple Claude Code agents in parallel from a phone using Termius + mosh into a pay-per-use Vultr VM secured by Tailscale, with tmux for persistence and git worktrees for parallel features.

</details>


### [37] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/jl6TW2WlGgfP9EFGhQYTOBXJ4pWEpm1Jqvjoifxgv2U=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 移动优先开发环境：通过Termius+mosh连接按需付费的Vultr虚拟机，运行多个Claude Code代理并行开发，使用tmux保持会话，git worktrees支持并行功能开发


<details>
  <summary>Details</summary>
Motivation: 为移动开发者提供便捷的开发环境，能够在手机上运行多个代码代理并行工作，解决移动设备开发能力有限的问题

Method: 使用Termius+mosh连接到Vultr按需付费虚拟机，通过Tailscale确保安全连接，利用tmux保持会话持久性，使用git worktrees支持并行功能开发

Result: 成功实现了在手机上运行多个Claude Code代理的移动开发环境，支持并行开发和持久会话

Conclusion: 移动优先的开发环境配置方案可行，为移动开发者提供了强大的云端开发能力

Abstract: Claude Code On-The-Go (3 minute read) A mobile-first development setup runs multiple Claude Code agents in parallel from a phone using Termius + mosh into a pay-per-use Vultr VM secured by Tailscale, with tmux for persistence and git worktrees for parallel features.

</details>


### [38] [How Base44 Sold for $80M in 6 Months](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productmarketfit.tech%2Fp%2Fhow-base44-was-bootstrapped-and-sold%3Futm_source=tldrfounders/1/0100019b8e466024-e176364e-796f-40ef-b09e-49f98be6d45a-000000/DZ18-lLodLbY1y4n670k_5dUiz--kE_hpyumZMMsfhk=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 创始人Maor Shlomo在180天内将Base44以8000万美元现金出售，通过AI生成近100%前端代码，让用户通过文本提示构建应用，实现零广告支出下350万美元年化收入。


<details>
  <summary>Details</summary>
Motivation: 展示如何通过AI技术快速构建和规模化SaaS平台，实现从零到高价值退出的创业路径，证明AI在代码生成和产品开发中的商业潜力。

Method: 使用AI生成近100%前端代码，构建"开箱即用"平台（包含数据库、认证和预连接后端），用户通过文本提示构建应用，专注于"神奇时刻"——让用户在60秒内获得可工作的应用。

Result: 在6个月内实现8000万美元全现金退出，零广告支出下达到350万美元年化收入，用户能在60秒内构建出可工作的应用。

Conclusion: AI驱动的代码生成可以极大加速产品开发和商业化进程，专注于核心用户体验的"神奇时刻"能带来显著的商业成功和退出价值。

Abstract: How Base44 Sold for $80M in 6 Months (3 minute read) Zero to exit in 180 days. Maor Shlomo bootstrapped Base44 to an $80M all-cash exit by using AI to generate nearly 100% of the frontend code. This allowed the founder to ship a "batteries included" platform (database, auth, and backend pre-wired) where users built apps via text prompts. By focusing entirely on the magic moment - getting a user to a working app in under 60 seconds - the company hit $3.5M ARR with zero ad spend.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Horizon Reduction as Information Loss in Offline Reinforcement Learning](https://arxiv.org/abs/2601.00831)
*Uday Kumar Nidadala,Venkata Bhumika Guthi*

Main category: cs.LG

TL;DR: 论文证明在离线强化学习中，horizon reduction（视野缩减）会导致不可恢复的信息损失，使得最优策略与次优策略在统计上无法区分，即使有无限数据和完美函数逼近。


<details>
  <summary>Details</summary>
Motivation: 尽管经验证据表明视野缩减可以改善离线强化学习的扩展性，但其理论影响尚未充分发展。本文旨在揭示视野缩减在离线强化学习中的基本理论限制。

Method: 将视野缩减形式化为从固定长度轨迹片段中学习，并通过理论证明和最小反例马尔可夫决策过程（MDPs）展示三种结构性失效模式。

Result: 证明在固定长度轨迹片段的学习范式下，最优策略可能与次优策略统计上无法区分。识别了三种失效模式：前缀不可区分性导致可识别性失败、截断回报导致的目标错误指定、离线数据集支持和表示混淆。

Conclusion: 视野缩减在离线强化学习中存在固有局限性，无法仅通过算法改进克服。研究建立了视野缩减安全应用的必要条件，并补充了关于保守目标和分布偏移的算法工作。

Abstract: Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).

</details>


### [40] [SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation](https://arxiv.org/abs/2601.00868)
*Aditya Sreevatsa K,Arun Kumar Raveendran,Jesrael K Mani,Prakash G Shigli,Rajkumar Rangadore,Narayana Darapaneni,Anwesh Reddy Paduri*

Main category: cs.LG

TL;DR: SmartFlow是一个多层框架，结合强化学习和智能体AI解决城市共享单车动态再平衡问题，通过战略DQN学习策略、战术模块优化调度、通信层生成可执行指令，显著降低网络不平衡并提高运营效率。


<details>
  <summary>Details</summary>
Motivation: 解决城市共享单车系统中的动态再平衡问题，传统方法难以应对复杂动态环境，需要智能解决方案来减少闲置时间、提高车辆可用性并降低运营成本。

Method: 采用多层架构：战略层使用DQN在纽约Citi Bike高保真模拟中学习再平衡策略；战术层确定性优化多段行程和调度；通信层基于LLM的智能体AI将计划转化为可执行指令。

Result: 评估显示SmartFlow能减少95%以上的网络不平衡，同时最小化行驶距离，实现高卡车利用率，显著提高运营效率。

Conclusion: SmartFlow为复杂城市移动网络提供了可解释、AI驱动的物流蓝图，成功连接机器智能与人工操作，具有可扩展性。

Abstract: SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

</details>


### [41] [Dichotomous Diffusion Policy Optimization](https://arxiv.org/abs/2601.00898)
*Ruiming Liang,Yinan Zheng,Kexin Zheng,Tianyi Tan,Jianxiong Li,Liyuan Mao,Zhihao Wang,Guang Chen,Hangjun Ye,Jingjing Liu,Jinqiao Wang,Xianyuan Zhan*

Main category: cs.LG

TL;DR: DIPOLE是一种新颖的强化学习算法，通过将最优策略分解为一对稳定学习的二分策略（一个最大化奖励，一个最小化奖励），解决了扩散策略在强化学习中训练不稳定和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在决策任务中表现出色，但在强化学习中训练大型扩散策略面临挑战：现有方法要么因直接最大化价值目标导致训练不稳定，要么因依赖粗糙的高斯似然近似而面临计算问题。

Method: 重新审视RL中的KL正则化目标，提出贪婪化策略正则化方案，将最优策略分解为一对二分策略（奖励最大化和最小化）。在推理时通过线性组合二分策略的分数生成优化动作，实现贪婪程度的灵活控制。

Result: 在ExORL和OGBench的离线和离线到在线RL设置中验证了方法的有效性。使用DIPOLE训练了大型视觉-语言-动作模型用于端到端自动驾驶，并在大规模真实世界AD基准NAVSIM上评估，展示了在复杂现实应用中的潜力。

Conclusion: DIPOLE为扩散策略的稳定可控优化提供了有效解决方案，通过二分策略分解实现了训练稳定性和推理灵活性的平衡，在复杂决策任务中表现出色。

Abstract: Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.

</details>


### [42] [Complexity-based code embeddings](https://arxiv.org/abs/2601.00924)
*Rares Folea,Radu Iacob,Emil Slusanschi,Traian Rebedea*

Main category: cs.LG

TL;DR: 提出一种通用方法，将各种算法的源代码转换为数值嵌入，通过动态分析程序在不同输入下的行为，并为分析指标定制多个通用复杂度函数，基于r-Complexity构建代码嵌入，并在Codeforces真实代码片段数据集上实现XGBoost算法，在11类多标签分类中取得良好F1分数。


<details>
  <summary>Details</summary>
Motivation: 需要一种通用的方法来将算法源代码转换为数值表示（嵌入），以便进行机器学习分析。现有方法可能缺乏对程序动态行为的分析，或者不能很好地适应不同类型的算法复杂度度量。

Method: 1. 动态分析计算机程序在不同输入下的行为；2. 为分析指标定制多个通用复杂度函数；3. 基于r-Complexity构建算法嵌入；4. 使用这些嵌入实现XGBoost算法进行多标签分类。

Result: 在Codeforces平台真实编程竞赛代码片段构建的11类多标签数据集上，实现了平均F1分数（具体数值未在摘要中给出，但表明取得了良好性能）。

Conclusion: 提出的代码嵌入方法能够有效地将算法源代码转换为数值表示，并在真实世界的代码分类任务中表现出良好的性能，为代码分析和机器学习应用提供了有效的工具。

Abstract: This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.

</details>


### [43] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: Warp Cortex是一种异步多智能体LLM架构，通过单例权重共享和拓扑突触技术，将内存复杂度从O(N*L)降低到O(1)权重和O(N*k)上下文，实现在消费级硬件上支持百万级智能体扩展。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架存在线性内存扩展问题，使得"系统2"并行推理在消费级硬件上不切实际，需要解决内存瓶颈以实现大规模智能体部署。

Method: 采用异步架构，通过单例权重共享和拓扑突触技术（受拓扑数据分析启发），将KV缓存视为潜在空间中的点云，应用见证复形稀疏化技术，并引入引用注入机制实现非侵入式KV缓存更新。

Result: 在单张NVIDIA RTX 4090上实现了100个并发智能体仅占用2.2GB显存，理论容量超过1000个智能体，计算延迟成为主要瓶颈。

Conclusion: Warp Cortex通过创新的内存优化架构，解决了多智能体LLM系统的内存扩展问题，为大规模智能体部署提供了可行的技术方案。

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [44] [Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows](https://arxiv.org/abs/2601.01357)
*Ke Xiao,Haoze Zhang,Runze Mao,Han Li,Zhi X. Chen*

Main category: cs.LG

TL;DR: FlamePilot是一个专门用于燃烧建模的LLM智能体，能够自动执行CFD模拟工作流，从文献学习到仿真配置、执行和优化，在公开基准测试中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂科学领域（如燃烧建模）的应用存在局限，需要将领域文献知识与专业工具（如CFD代码）的执行能力无缝集成。现有AI助手难以处理专业知识密集的计算流体动力学工具。

Method: FlamePilot采用基于原子工具的架构，确保在OpenFOAM和DeepFlame等框架中稳健设置和执行复杂模拟。系统能够从科学文献中学习，提取关键信息指导从初始设置到优化结果的整个仿真过程。

Result: 在公开基准测试中，FlamePilot获得了完美的1.0可执行性分数和0.438的成功率，超过了之前最佳报告的0.625和0.250分数。在MILD燃烧模拟案例研究中，系统能够自主将研究论文转化为配置的模拟，执行仿真，后处理结果，提出基于证据的改进建议，并在最少人工干预下管理多步参数研究直至收敛。

Conclusion: FlamePilot通过透明可解释的范式，为AI赋能的燃烧建模建立了基础框架，促进了人机协作伙伴关系，其中智能体管理工作流编排，让研究人员专注于高层次分析。

Abstract: The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.

</details>


### [45] [The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs](https://arxiv.org/abs/2601.01580)
*Zibo Zhao,Yuanting Zha,Haipeng Zhang,Xingcheng Xu*

Main category: cs.LG

TL;DR: 该论文通过梯度归因属性和两阶段决策-采样假设，从理论上解释了为什么RL后训练能让大语言模型获得自我反思能力，而SFT不能，并实证验证了RL的优越泛化主要源于改进的决策能力而非采样能力。


<details>
  <summary>Details</summary>
Motivation: 尽管RL后训练能让大语言模型获得自我反思能力，但统一的优化目标如何产生生成解决方案和评估何时修订这两种功能上不同的能力，其机制仍然不透明。需要从理论上解释为什么RL能成功而SFT失败。

Method: 引入梯度归因属性来刻画奖励梯度在策略组件间的分布，形式化为两阶段决策-采样假设，将策略分解为用于生成的采样组件和用于验证的决策组件。理论证明代理奖励具有平衡梯度归因，而SFT和KL惩罚具有不平衡梯度归因，长度加权创建了不对称正则化。

Result: 理论分析表明长度加权的不对称正则化约束了采样组件而让决策组件优化不足，这解释了RL成功而SFT失败的原因。在算术推理任务上的实证验证表明RL的优越泛化主要源于改进的决策能力而非采样能力。

Conclusion: 该研究为思维模型的自我修正能力提供了基于第一性原理的机制解释，揭示了RL后训练中决策能力改进的关键作用，为理解大语言模型自我反思能力的涌现提供了理论框架。

Abstract: Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.

</details>


### [46] [HeurekaBench: A Benchmarking Framework for AI Co-scientist](https://arxiv.org/abs/2601.01678)
*Siba Smarak Panigrahi,Jovana Videnović,Maria Brbić*

Main category: cs.LG

TL;DR: HeurekaBench是一个用于评估科学代理系统的基准框架，通过半自动化流程创建基于真实科学研究的开放性问题，并在单细胞生物学领域实例化为sc-HeurekaBench。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM驱动的科学代理系统面临挑战，需要真实、端到端的研究场景来整合数据分析、解释和从实验数据生成新见解。现有基准缺乏这种现实性和综合性。

Method: 开发了一个半自动化流水线，利用多个LLM从科学研究和对应代码仓库中提取见解并生成候选工作流程，然后与报告结果进行验证。在单细胞生物学领域实例化为sc-HeurekaBench基准。

Result: 使用该基准比较了最先进的单细胞代理，发现添加批评模块可以将开源LLM代理的错误响应改善高达22%，缩小与闭源模型的差距。展示了基准在定量分析代理系统设计选择方面的价值。

Conclusion: HeurekaBench为科学代理的严格端到端评估奠定了基础，将基准构建锚定在真实的科学工作流程中，推动了科学代理系统的评估方法发展。

Abstract: LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.

</details>


### [47] [Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving](https://arxiv.org/abs/2601.01800)
*Qi Wei,Junchao Fan,Zhao Yang,Jianhua Wang,Jingkai Mao,Xiaolin Chang*

Main category: cs.LG

TL;DR: CARRL是一种针对自动驾驶的鲁棒强化学习方法，通过风险暴露对手和风险目标鲁棒代理的博弈设计，专门处理稀疏的安全关键风险，显著降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶强化学习方法存在脆弱性，传统对抗训练采用零和博弈和连续攻击，忽略了智能体与对手之间的不对称性，未能反映安全关键风险的稀疏性，导致实际场景中的鲁棒性不足。

Method: 提出CARRL框架，包含风险暴露对手（REA）和风险目标鲁棒代理（RTRA）。将交互建模为一般和博弈，REA专注于暴露安全关键故障（如碰撞），RTRA学习平衡安全与驾驶效率。REA采用解耦优化机制识别稀疏安全关键时刻，RTRA通过双回放缓冲区联合利用良性对抗经验，并在扰动下强制策略一致性。

Result: 实验结果表明，与最先进的基线方法相比，该方法在所有情况下至少降低了22.66%的碰撞率。

Conclusion: CARRL通过专门针对稀疏安全关键风险的对抗训练方法，显著提升了自动驾驶策略的鲁棒性，为解决实际部署中的安全性问题提供了有效方案。

Abstract: Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.

</details>


### [48] [Moments Matter:Stabilizing Policy Optimization using Return Distributions](https://arxiv.org/abs/2601.01803)
*Dennis Jabs,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 提出一种基于分布评论家高阶矩（偏度和峰度）的PPO改进方法，通过惩罚极端尾部行为来减少策略更新引起的变异性，提高连续控制任务的稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习智能体在相同回报下可能表现出不同行为，这种不稳定性源于环境和算法因素。连续控制任务中，即使小的参数变化也会导致不稳定步态，影响算法比较和实际应用。虽然约束策略保持窄的回报分布可以改善稳定性，但直接估计该分布在计算上昂贵。

Method: 通过分布评论家建模状态-动作回报分布，然后利用该分布的高阶矩（偏度和峰度）对PPO的优势函数进行偏置。通过惩罚极端尾部行为，阻止策略进入容易产生不稳定性的参数区域。

Result: 在Walker2D环境中，该方法将稳定性提高了75%，同时保持了可比较的评估回报。当更新后评论家值与更新后回报对齐不佳时，标准PPO难以产生窄的回报分布，而本文的矩基修正方法能够缩小回报分布。

Conclusion: 利用环境随机性和分布评论家的高阶矩信息可以有效缓解策略更新引起的变异性，提高强化学习智能体在连续控制任务中的稳定性，而不牺牲性能。

Abstract: Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.

</details>


### [49] [GDRO: Group-level Reward Post-training Suitable for Diffusion Models](https://arxiv.org/abs/2601.02036)
*Yiyang Wang,Xi Chen,Xiaogang Xu,Yu Liu,Hengshuang Zhao*

Main category: cs.LG

TL;DR: 提出GDRO方法，一种针对整流流扩散模型的群体级直接奖励优化后训练范式，解决在线RL训练效率低、依赖随机采样器和奖励黑客问题


<details>
  <summary>Details</summary>
Motivation: 现有方法采用在线RL从LLMs到文本到图像整流流扩散模型进行奖励对齐，但面临效率低、依赖随机采样器和奖励黑客问题。整流流模型与LLMs有本质差异：1) 在线图像采样耗时；2) 整流流在初始噪声固定后是确定性的

Method: 设计Group-level Direct Reward Optimization (GDRO)，结合整流流模型特性的群体级奖励对齐后训练范式。支持完全离线训练，节省图像采样时间；扩散采样器无关，无需ODE-to-SDE近似；考虑奖励黑客陷阱，使用修正评分进行评估

Result: GDRO在OCR和GenEval任务上通过群体级离线优化有效提升扩散模型的奖励分数，同时展现强大的稳定性和鲁棒性，缓解奖励黑客问题

Conclusion: GDRO为整流流扩散模型提供了一种高效、稳定、鲁棒的群体级奖励对齐后训练方法，解决了现有在线RL方法的局限性

Abstract: Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.

</details>


### [50] [ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense](https://arxiv.org/abs/2601.02196)
*Yu Li,Sizhe Tang,Rongqian Chen,Fei Xu Yu,Guangyu Jiang,Mahdi Imani,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 该论文提出了一种基于蒙特卡洛树搜索和图神经网络的自动化网络防御方法，在CAGE-4挑战中实现了样本高效的防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的自动化网络防御方法在复杂网络环境中面临探索困难、决策空间大、需要大量样本的问题，需要开发样本效率更高的防御策略。

Method: 将自动化网络防御建模为基于上下文的部分可观测马尔可夫决策问题，提出基于蒙特卡洛树搜索的规划中心防御策略。使用图神经网络嵌入网络观测作为属性图，结合学习到的图嵌入和图编辑动作先验来指导MCTS搜索。

Result: 在CAGE-4挑战的各种网络结构和对手行为场景中，该搜索引导、基于图嵌入的规划方法相比最先进的强化学习基线，提高了防御奖励和鲁棒性。

Conclusion: 结合模型无关泛化、策略蒸馏和前向规划的MCTS方法，能够有效解决自动化网络防御中的探索-利用权衡问题，实现样本高效的防御策略。

Abstract: Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.

</details>


### [51] [CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents](https://arxiv.org/abs/2601.02201)
*Keyu Wang,Bingchen Miao,Wendong Bu,Yu Wu,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: CORE提出了一种基于代码的逆自训练框架，通过图扩展桥接模仿学习与强化学习，自动从专家演示中推断奖励函数，并增强行为多样性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态虚拟代理训练面临两难：行为克隆方法简单有效但行为多样性低，强化学习方法能发现新策略但依赖人工设计的奖励函数。需要解决这两种方法的冲突。

Method: 1. 语义代码抽象：自动从专家演示中推断奖励函数（标签函数），无需人工设计；2. 策略图扩展：构建多路径策略图捕捉多样有效解决方案；3. 轨迹引导外推：利用成功和失败轨迹扩展任务空间，增强领域外行为多样性。

Result: 在Web和Android平台上的实验表明，CORE显著提高了整体性能和泛化能力，展示了作为构建强大虚拟代理的鲁棒且可泛化训练范式的潜力。

Conclusion: CORE通过桥接模仿学习与强化学习，解决了行为多样性与奖励函数设计的冲突，为多模态虚拟代理提供了更强大的训练框架。

Abstract: The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [52] [SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities](https://arxiv.org/abs/2601.01042)
*Zixiao Zhao,Yanjie Jiang,Hui Liu,Kui Liu,Lu Zhang*

Main category: cs.SE

TL;DR: 构建了一个安全相关的代码审查数据集SeRe，通过主动学习集成分类方法从37万+原始审查中提取6732条安全相关评论，并用于评估现有代码审查生成方法的安全反馈能力。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞可能导致严重后果，需要早期检测。虽然代码审查是防止安全缺陷的关键机制，但由于审查者对安全问题关注不足或缺乏专业知识，相关反馈仍然稀缺。现有数据集和研究主要关注通用代码审查评论，要么缺乏安全特定标注，要么规模太小无法支持大规模研究。

Method: 采用基于主动学习的集成分类方法构建SeRe数据集。该方法通过人工标注迭代优化模型预测，在保持合理召回率的同时实现高精度。使用微调的集成分类器从373,824个原始审查实例中提取6,732个安全相关审查，确保跨多种编程语言的代表性。

Result: 统计分析表明SeRe数据集与现实世界安全相关审查分布基本一致。通过基准测试评估了现有代码审查评论生成方法在安全相关反馈生成方面的效果，为自动化安全导向代码审查研究提供了基础。

Conclusion: 通过发布SeRe数据集和基准测试结果，旨在推进自动化安全导向代码审查研究，促进更有效的安全软件工程实践发展。

Abstract: Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \textbf{SeRe}, a \textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.

</details>


### [53] [RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian](https://arxiv.org/abs/2601.01129)
*Kla Tantithamthavorn,Yaotian Zou,Andy Wong,Michael Gupta,Zhe Wang,Mike Buller,Ryan Jiang,Matthew Watson,Minwoo Jeong,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: RovoDev Code Reviewer是一个企业级LLM代码审查自动化工具，无需微调即可生成基于审查指导、上下文感知和质量检查的代码审查评论，已在Atlassian的Bitbucket中大规模部署。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的代码审查评论生成方法有所进展，但设计企业级代码审查自动化工具仍面临实际挑战。本文旨在回答一个实际问题：如何在不进行微调的情况下，设计出基于审查指导、上下文感知和质量检查的代码审查评论生成系统。

Method: 提出RovoDev Code Reviewer，这是一个企业级LLM代码审查自动化工具，无缝集成到Atlassian的Bitbucket中。通过离线、在线和用户反馈评估，在一年时间内验证其效果。

Result: RovoDev Code Reviewer有效生成能导致代码解决的审查评论（38.70%的评论触发了后续提交中的代码更改），并加速反馈周期（减少PR周期时间30.8%）、减轻审查员工作量（减少人工编写评论35.6%）、提高整体软件质量（发现错误并提供可行建议）。

Conclusion: RovoDev Code Reviewer是一个成功的企业级LLM代码审查自动化工具，能够在不进行微调的情况下有效生成高质量的代码审查评论，显著改善开发工作流程和软件质量。

Abstract: Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?
  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).

</details>


### [54] [Abductive Vibe Coding (Extended Abstract)](https://arxiv.org/abs/2601.01199)
*Logan Murphy,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 提出一个框架，为AI生成的代码（"vibe coding"）提取可分析、半形式化的合理性依据，而非直接判断正确性，生成一组条件来评估代码的充分性。


<details>
  <summary>Details</summary>
Motivation: 当AI模型生成软件制品时，人类工程师需要验证其正确性。理想情况下应通过形式化证明来完成，但对于许多现实世界的vibe coding场景来说不可行，特别是当AI生成制品的要求难以形式化时。

Method: 开发一个框架来提取可分析、半形式化的合理性依据，用于评估vibe-coded制品的充分性。该框架不直接判断正确性，而是生成一组条件，在这些条件下可以认为生成的代码是充分的。

Result: 论文描述了当前框架实现的工作进展和预期的研究机会。这是一个正在进行的工作，提出了方法论但尚未报告具体实验结果。

Conclusion: 需要为AI生成的代码验证开发新的方法，特别是当形式化证明不可行时。提出的半形式化合理性框架为解决vibe coding验证问题提供了有前景的方向。

Abstract: When software artifacts are generated by AI models ("vibe coding"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.

</details>


### [55] [Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling](https://arxiv.org/abs/2601.01233)
*Kangchen Zhu,Zhiliang Tian,Shangwen Wang,Mingyue Leng,Xiaoguang Mao*

Main category: cs.SE

TL;DR: Atomizer是一个用于解耦复合提交的多智能体框架，通过意图导向的思维链和分组-评审协作循环，显著优于现有图聚类方法。


<details>
  <summary>Details</summary>
Motivation: 复合提交（包含多个无关关注点的提交）普遍存在，严重阻碍程序理解和维护。现有自动化解耦方法（特别是基于图聚类的方法）存在两个根本限制：1) 过度依赖结构信息，无法理解变更的语义意图；2) 作为"单次"算法，缺乏人类评审过程中的关键反思和精炼机制。

Method: 提出Atomizer框架：1) 采用意图导向的思维链策略，利用大语言模型根据代码结构和语义信息推断每个代码变更的意图；2) 使用两个智能体建立分组-评审协作精炼循环，模拟人类评审实践，迭代精炼分组直到同一簇中的所有变更共享相同的底层语义意图。

Result: 在两个基准C#和Java数据集上的实验表明，Atomizer显著优于多个代表性基线。平均而言，在C#数据集上比最先进的图基方法高出6.0%以上，在Java数据集上高出5.5%以上。在复杂提交上优势更加明显，性能优势扩大到16%以上。

Conclusion: Atomizer通过结合语义意图理解和迭代精炼机制，有效解决了复合提交解耦问题，显著优于现有方法，特别是在复杂场景下表现突出。

Abstract: Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.

</details>


### [56] [CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs](https://arxiv.org/abs/2601.01271)
*Qingxiao Tao,Xiaodong Gu,Hao Zhong,Beijun Shen*

Main category: cs.SE

TL;DR: CatchAll是一个基于LLM的仓库感知异常处理方法，通过三层知识（API级异常知识、仓库级执行上下文、跨仓库处理模式）提升代码生成中的异常处理能力。


<details>
  <summary>Details</summary>
Motivation: 异常处理是编程中的重要错误恢复机制，但LLM在仓库级别的异常处理上表现不佳，因为复杂的依赖关系和上下文约束。需要一种能够理解仓库级上下文并生成准确异常处理代码的方法。

Method: 提出CatchAll方法，为LLM提供三层异常处理知识：1) API级异常知识，基于经验构建的API-异常映射；2) 仓库级执行上下文，通过建模目标代码周围的调用跟踪捕获异常传播；3) 跨仓库处理知识，从历史代码中挖掘可重用的异常处理模式。这些知识被编码到结构化提示中指导LLM生成代码。

Result: 在新建的两个仓库感知异常处理基准测试（RepoExEval和RepoExEval-Exec）上，CatchAll优于现有最佳基线，CodeBLEU得分0.31（基线0.27），意图预测准确率60.1%（基线48.0%），Pass@1为29%（基线25%）。

Conclusion: CatchAll通过整合多层异常处理知识，有效提升了LLM在真实仓库级别异常处理任务中的性能，证明了其在复杂代码环境中的实用性。

Abstract: Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.

</details>


### [57] [Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python](https://arxiv.org/abs/2601.01320)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: ALPHA是首个函数级Python基准测试，使用分层感知的CWE特定惩罚来评估LLM和SAST工具，区分不同类型错误并提供可操作的漏洞检测反馈。


<details>
  <summary>Details</summary>
Motivation: 现有代码漏洞检测基准采用二元分类，缺乏CWE级别的特异性，无法为迭代修正系统提供可操作的反馈。需要更精细的评估框架来反映实际诊断效用差异。

Method: 提出ALPHA基准，采用分层感知的CWE特定惩罚机制，区分过度泛化、过度规范和横向错误。评估了7个LLM和2个SAST工具，分析预测一致性。

Result: LLM在漏洞检测方面显著优于SAST工具，但SAST在检测发生时具有更高精度。模型间的预测一致性差异巨大（8.26%-81.87%），对反馈驱动系统有重要影响。

Conclusion: ALPHA为代码漏洞检测提供了更精细的评估框架，未来可将ALPHA惩罚机制融入监督微调，实现原则性的分层感知漏洞检测。

Abstract: Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.

</details>


### [58] [SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving](https://arxiv.org/abs/2601.01426)
*Chaofan Tao,Jierun Chen,Yuxin Jiang,Kaiqi Kou,Shaowei Wang,Ruoyu Wang,Xiaohui Li,Sidi Yang,Yiming Du,Jianbo Dai,Zhiming Mao,Xinyu Wang,Lifeng Shang,Haoli Bai*

Main category: cs.SE

TL;DR: SWE-Lego提出了一种仅使用监督微调的轻量级方法，在软件工程问题解决任务上达到最先进性能。该方法包含三个核心模块：高质量数据集、改进的SFT流程以及测试时扩展技术。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程任务解决方法通常依赖复杂的训练范式（如中期训练、SFT、强化学习及其组合），本文探索如何通过轻量级的纯SFT方法在SWE任务上达到极限性能。

Method: 1) SWE-Lego数据集：包含32k高质量任务实例和18k验证轨迹，结合真实和合成数据；2) 改进的SFT流程：包含错误掩码和基于难度的课程学习；3) 测试时扩展：基于训练良好的验证器进行扩展。

Result: 在SWE-bench Verified基准上，SWE-Lego-Qwen3-8B达到42.2%，32B版本达到52.6%。通过测试时扩展@16，8B模型提升至49.6%，32B模型提升至58.8%，在同等规模开源模型中达到最先进水平。

Conclusion: 通过精心设计的纯SFT方法，可以在软件工程任务上达到最先进性能，无需复杂的训练范式。高质量数据、改进的SFT流程和测试时扩展是成功的关键要素。

Abstract: We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.

</details>


### [59] [Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox](https://arxiv.org/abs/2601.01514)
*Matej Kucera,Marco Castelluccio,Daniel Feitosa,Ayushi Rastogi*

Main category: cs.SE

TL;DR: 该研究探讨了代码审查中群体审查请求与个体审查请求对审查速度和质量的影响，发现群体审查与更少的回归问题相关，但对审查速度影响不大。


<details>
  <summary>Details</summary>
Motivation: 代码审查速度是衡量软件开发效率和开发者满意度的重要指标。虽然已有研究探讨了影响审查速度的因素，但审查分配过程中"群体审查请求"的作用尚不明确。群体审查请求允许将代码变更分配给一个审查者群体，而不是特定的个体审查者，这种模式在Phabricator、GitHub、Bitbucket等平台上可用。

Method: 研究借鉴了管理学中的共享任务队列理论，分析了Mozilla Firefox项目中约66,000个修订版本。采用统计建模方法，并结合了从业者焦点小组讨论的观点。

Result: 群体审查与改进的审查质量相关（表现为更少的回归问题），但与审查速度的关联可以忽略不计。额外的感知益处包括更平衡的工作分配和新审查者的培训机会。

Conclusion: 群体审查请求在提高代码审查质量方面具有优势，特别是在减少回归问题方面，但对审查速度的影响有限。这种模式还能带来工作负载平衡和新人培训等额外好处。

Abstract: The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.

</details>


### [60] [LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment](https://arxiv.org/abs/2601.01780)
*Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan,Mehdi Keshani,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 使用DeepSeek-R1-Distill-Llama-8B进行监督微调，提出LIA方法实现自动化的issue分配，相比基准模型和现有方法在Hit@1指标上提升显著。


<details>
  <summary>Details</summary>
Motivation: 软件维护中的issue分配过程通常手动进行，存在不一致和易出错的问题，特别是在大型开源项目中每月有数千个新issue。现有自动化方法依赖大量项目特定训练数据或稀疏嘈杂的关系信息，效果有限。

Method: 提出LIA方法，采用监督微调将DeepSeek-R1-Distill-Llama-8B大语言模型适配到issue分配任务。利用LLM对自然语言和软件相关文本的预训练语义理解，直接从issue标题和描述生成开发者推荐排名，基于历史issue-开发者分配模式学习。

Result: LIA相比其基础预训练模型在Hit@1指标上提升高达+187.8%，相比四种领先的issue分配方法在Hit@1上提升高达+211.2%，表现显著优于现有方法。

Conclusion: 领域适配的LLM在软件维护任务中非常有效，LIA为issue分配提供了一个实用且高性能的解决方案。

Abstract: Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.

</details>


### [61] [Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics](https://arxiv.org/abs/2601.02200)
*Markus Borg,Nadim Hagatulah,Adam Tornhill,Emma Söderberg*

Main category: cs.SE

TL;DR: 研究发现人类友好的代码（CodeHealth评分高）也更容易被AI工具理解和编辑，语义保留度更高，表明组织可以用CodeHealth指标来评估AI干预的风险程度。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程代理与人类开发者共同工作的混合时代到来，需要确保不同能力的LLM能够可靠地编辑代码。传统上代码优化主要考虑人类可读性，现在需要研究什么样的代码对AI更友好。

Method: 使用LLM对5,000个来自编程竞赛的Python文件进行重构，分析CodeHealth（针对人类理解校准的质量指标）与AI重构后语义保留度之间的关系。

Result: 发现CodeHealth与AI重构后的语义保留度存在有意义的关联，人类友好的代码也更兼容AI工具。

Conclusion: 组织可以使用CodeHealth指标来指导AI干预的风险评估，投资代码可维护性不仅帮助人类开发者，也为大规模AI采用做好准备。

Abstract: We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.

</details>


### [62] [LLM-Empowered Functional Safety and Security by Design in Automotive Systems](https://arxiv.org/abs/2601.02215)
*Nenad Petrovic,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 提出一个LLM赋能的软件定义车辆开发工作流，涵盖安全感知系统拓扑设计和事件驱动决策代码分析


<details>
  <summary>Details</summary>
Motivation: 软件定义车辆开发需要系统化的方法来处理安全感知的系统拓扑设计和事件驱动的代码分析，传统方法难以满足这些需求

Method: 采用事件链模型进行代码分析，结合模型驱动工程和对象约束语言进行安全拓扑设计，支持本地部署和专有解决方案

Result: 在高级驾驶辅助系统相关场景中进行了评估，验证了方法的有效性

Conclusion: LLM赋能的工作流能够有效支持软件定义车辆的开发，特别是在安全感知系统设计和代码分析方面

Abstract: This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.

</details>


### [63] [NQC2: A Non-Intrusive QEMU Code Coverage Plugin](https://arxiv.org/abs/2601.02238)
*Nils Bosbach,Alwalid Salama,Lukas Jünger,Mark Burton,Niko Zurstraßen,Rebecca Pelke,Rainer Leupers*

Main category: cs.SE

TL;DR: NQC2是一个QEMU插件，用于在嵌入式系统裸机程序中提取代码覆盖率信息，无需目标软件插桩，性能比Xilinx方案提升8.5倍。


<details>
  <summary>Details</summary>
Motivation: 传统代码覆盖率分析需要操作系统和文件系统支持，而嵌入式系统的裸机程序缺乏这些功能，导致无法使用常规覆盖率测量方法。

Method: 开发QEMU插件NQC2，在运行时从QEMU虚拟机中提取覆盖率信息并存储到主机文件中，兼容修改版QEMU且无需目标软件插桩。

Result: NQC2在性能上比Xilinx的类似方案提升高达8.5倍，能够有效测量嵌入式系统裸机程序的代码覆盖率。

Conclusion: NQC2提供了一种无需插桩的嵌入式系统代码覆盖率测量解决方案，克服了传统方法的局限性，具有更好的兼容性和性能。

Abstract: Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.
  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.

</details>
