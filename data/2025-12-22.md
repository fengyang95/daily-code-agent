<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LG](#cs.LG) [Total: 5]
- [tldr.article](#tldr.article) [Total: 11]
- [cs.AI](#cs.AI) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL](https://arxiv.org/abs/2512.17053)
*Khushboo Thaker,Yony Bresler*

Main category: cs.CL

TL;DR: Struct-SQL：一个新颖的知识蒸馏框架，使用结构化推理表示（查询执行计划）来训练小型语言模型，在Text-to-SQL任务上比非结构化思维链蒸馏基线提升8.1%，主要减少语法错误。


<details>
  <summary>Details</summary>
Motivation: 企业级Text-to-SQL系统面临成本、安全和性能的三难困境：需要在昂贵的大型专有LLM和性能较低的小型模型之间做出选择。现有改进小型模型的方法通常使用非结构化思维链进行知识蒸馏，但这种方法存在固有的模糊性。

Method: 提出Struct-SQL知识蒸馏框架，使用查询执行计划作为结构化推理表示来训练小型语言模型。该方法采用正式的推理蓝图，让小型模型模仿大型LLM的推理过程，而不是使用非结构化的思维链。

Result: 使用结构化思维链蒸馏的小型模型相比非结构化思维链蒸馏基线实现了8.1%的绝对提升。详细错误分析显示，这种提升主要源于语法错误的显著减少。

Conclusion: 使用结构化逻辑蓝图教导模型推理对于小型语言模型生成可靠的SQL是有益的，结构化推理表示提供了更清晰、更可靠的教学信号。

Abstract: Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.

</details>


### [2] [Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience](https://arxiv.org/abs/2512.17260)
*Jiangjie Chen,Wenxiang Chen,Jiacheng Du,Jinyi Hu,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Wenlei Shi,Zhihong Wang,Mingxuan Wang,Chenrui Wei,Shufa Wei,Huajian Xin,Fan Yang,Weihao Gao,Zheng Yuan,Tianyang Zhan,Zeyu Zheng,Tianxi Zhou,Thomas Hanwen Zhu*

Main category: cs.CL

TL;DR: Seed-Prover 1.5是一个通过大规模智能体强化学习训练的形式定理证明模型，采用高效测试时扩展工作流，在形式数学推理方面取得显著突破，特别是在本科及以上级别的数学问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在生成严格数学证明方面取得进展，但在形式语言（如Lean）中进行定理证明仍然具有挑战性且计算成本高昂，特别是在处理本科及以上级别的问题时。需要更高效、更强大的形式定理证明系统。

Method: 1. 通过大规模智能体强化学习训练Seed-Prover 1.5模型；2. 模型通过与Lean等工具持续交互积累经验；3. 采用高效的测试时扩展工作流，利用自然语言证明的最新进展，弥合自然语言和形式语言之间的差距。

Result: Seed-Prover 1.5在较小计算预算下实现了最先进的性能：解决了88%的PutnamBench（本科级）、80%的Fate-H（研究生级）和33%的Fate-X（博士级）问题。特别地，在9小时内解决了2025年Putnam竞赛12道题中的11道。

Conclusion: 研究表明，通过高质量形式反馈驱动的经验学习扩展，在形式数学推理的未来发展中具有巨大潜力。Seed-Prover 1.5展示了智能体强化学习在形式定理证明中的有效性。

Abstract: Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \textbf{88\% of PutnamBench} (undergraduate-level), \textbf{80\% of Fate-H} (graduate-level), and \textbf{33\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.

</details>


### [3] [UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models](https://arxiv.org/abs/2512.17385)
*Jiajun Wu,Jian Yang,Wei Zhang,Lin Jing,Yuqing Ma,Ensheng Shi,Yuchi Ma,Zhoujun Li,Xianglong Liu*

Main category: cs.CL

TL;DR: IPC是一种无监督代码生成框架，通过内部探测LLM的知识和置信模式来训练代码生成模型，无需外部语料库，在资源受限场景下实现与监督方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码生成任务中严重依赖昂贵且难以大规模获取的标注数据（如问答对）或无标注代码片段。为了解决这一限制，需要开发不依赖外部语料库的无监督方法。

Method: IPC框架包含四个核心组件：问题空间探测、测试理解探测、解决方案空间探测、知识巩固与强化。通过探测LLM内部知识和置信模式，结合自一致性机制和基于表示的质量估计来识别可靠代码候选，训练UCoder（无监督学习的代码生成器）。

Result: 在多个代码基准测试中验证了该方法的有效性，无监督方法能够达到与监督方法竞争的性能，同时显著减少对标注数据和计算资源的依赖。分析实验表明模型内部状态包含丰富的代码质量和正确性信号。

Conclusion: 通过适当利用LLM内部信号，可以实现有效的无监督代码生成学习，为资源受限场景下训练代码LLM开辟了新方向。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.

</details>


### [4] [DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports](https://arxiv.org/abs/2512.17776)
*Janghoon Han,Heegyu Kim,Changho Lee,Dahm Lee,Min Hyung Park,Hosung Song,Stanley Jungkyu Choi,Moontae Lee,Honglak Lee*

Main category: cs.CL

TL;DR: DEER是一个评估专家级深度研究报告的基准，包含50个跨13个领域的报告写作任务，提供专家评估分类法和细粒度评分标准，并引入文档级事实核查架构来验证报告中所有声明。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，深度研究系统能够通过多步推理和基于证据的合成生成专家级报告，但评估此类报告仍然具有挑战性。现有基准缺乏专家报告的系统标准，依赖LLM评判者的评估无法捕捉需要专家判断的问题，且来源验证通常只覆盖有限的部分引用声明而非整个报告的事实可靠性。

Method: DEER包含50个跨13个领域的报告写作任务，建立专家评估分类法（7个维度，25个子维度）并细化为130个评分项目，提供任务特定的专家指导以帮助LLM评判者更一致地评估报告质量。此外，提出文档级事实核查架构，提取并验证报告中所有声明（包括引用和未引用的），并量化外部证据质量。

Result: DEER与人类专家判断密切相关，能够产生可解释的系统优势和弱点诊断。该基准提供了对专家级报告质量的全面评估框架。

Conclusion: DEER是一个全面评估专家级深度研究报告的基准，通过结合专家评估分类法和文档级事实核查，解决了现有评估方法的局限性，为研究系统的评估提供了更可靠和可解释的框架。

Abstract: As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization](https://arxiv.org/abs/2512.16956)
*Shravan Chaudhari,Rahul Thomas Jacob,Mononito Goswami,Jiajun Cao,Shihab Rashid,Christian Bock*

Main category: cs.SE

TL;DR: SpIDER是一种增强的密集检索方法，通过结合LLM推理和图结构探索来改进代码单元的语义检索性能


<details>
  <summary>Details</summary>
Motivation: 现有代码检索方法（如BM25和密集嵌入）存在局限性：BM25性能较差，而密集嵌入方法缺乏对代码库的充分探索，未能充分利用代码的图结构信息

Method: 提出SpIDER方法，结合密集嵌入检索与基于图的代码库探索，通过LLM推理利用辅助上下文信息来增强检索效果

Result: SpIDER在多种编程语言上都能持续提升密集检索性能，显著优于传统方法

Conclusion: 结合图结构探索和LLM推理的密集检索方法能够有效改进代码单元的语义检索，为LLM编码代理提供更好的基础能力

Abstract: Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.

</details>


### [6] [GraphCue for SDN Configuration Code Synthesis](https://arxiv.org/abs/2512.17371)
*Haomin Qi,Fengfei Yu,Chengbo Huang*

Main category: cs.SE

TL;DR: GraphCue是一个基于拓扑的检索和代理循环框架，用于自动化SDN配置，通过图神经网络嵌入、约束性代码生成和验证循环实现高效配置


<details>
  <summary>Details</summary>
Motivation: 自动化SDN（软件定义网络）配置通常复杂且容易出错，需要解决拓扑感知的配置检索和约束性代码生成问题，以提高配置准确性和效率

Method: 将每个配置案例抽象为JSON图，使用三层GCN进行对比学习嵌入；通过拓扑感知检索找到最近的有效参考配置；使用结构化提示约束代码生成；通过验证器执行候选配置并将失败反馈给代理形成闭环

Result: 在628个验证案例中，GraphCue在20次迭代内达到88.2%通过率，95%的验证循环在9秒内完成；消融实验显示，没有检索或结构化提示时性能显著下降

Conclusion: 拓扑感知检索和约束性条件化是GraphCue性能的关键驱动因素，该框架为自动化SDN配置提供了有效的解决方案

Abstract: We present GraphCue, a topology-grounded retrieval and agent-in-the-loop framework for automated SDN configuration. Each case is abstracted into a JSON graph and embedded using a lightweight three-layer GCN trained with contrastive learning. The nearest validated reference is injected into a structured prompt that constrains code generation, while a verifier closes the loop by executing the candidate configuration and feeding failures back to the agent. On 628 validation cases, GraphCue achieves an 88.2 percent pass rate within 20 iterations and completes 95 percent of verification loops within 9 seconds. Ablation studies without retrieval or structured prompting perform substantially worse, indicating that topology-aware retrieval and constraint-based conditioning are key drivers of performance.

</details>


### [7] [CIFE: Code Instruction-Following Evaluation](https://arxiv.org/abs/2512.17387)
*Sravani Gunnu,Shanmukha Guttula,Hima Patel*

Main category: cs.SE

TL;DR: 该论文提出了一个包含1000个Python任务的基准测试，每个任务平均有7个开发者指定的约束条件，用于评估LLM在代码生成中不仅关注功能正确性，还要遵循各种约束要求的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在真实世界代码生成中，仅功能正确性不足以可靠部署，开发者还需要模型遵循鲁棒性、格式化和安全性等约束要求。现有基准主要基于测试用例评估正确性，对模型如何可靠遵循这些约束提供有限洞察。

Method: 通过四阶段人机协作流程构建包含1000个Python任务的基准，每个任务平均有7个开发者指定的约束，涵盖13个类别。约束经过精心设计确保原子性、相关性和客观性。评估14个开源和闭源模型，使用互补的遵循度指标，并提出C2A分数作为综合衡量正确性和约束遵循的复合指标。

Result: 结果显示部分遵循和严格遵循之间存在显著差距：强大模型的部分遵循率超过90%，但严格遵循率仅为39-66%。这表明当前LLM在代码生成中虽然功能正确性较高，但在严格遵循开发者意图方面仍有不足。

Conclusion: 可信赖的代码生成不仅需要正确性，还需要对开发者意图的一致遵循。该基准和C2A分数为评估LLM在真实世界代码生成中的可靠性提供了新工具。

Abstract: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.

</details>


### [8] [SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories](https://arxiv.org/abs/2512.17419)
*Lilin Wang,Lucas Ramalho,Alan Celestino,Phuc Anthony Pham,Yu Liu,Umang Kumar Sinha,Andres Portillo,Onassis Osunwa,Gabriel Maduekwe*

Main category: cs.SE

TL;DR: SWE-Bench++ 是一个自动化框架，从GitHub拉取请求生成仓库级编码任务，支持11种语言，包含11,133个实例，为代码生成提供可扩展的多语言基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如SWE-bench）存在局限性：手动构建、静态数据集、仅关注Python错误修复。需要更自动化、多语言、覆盖更广任务类型的基准测试。

Method: 通过四个阶段自动化生成任务：1) 程序化采集GitHub拉取请求；2) 环境合成；3) 测试预言提取；4) 质量保证。还包含提示引导的轨迹合成步骤，将失败实例转化为训练轨迹。

Result: 构建了包含11,133个实例、3,971个仓库、11种语言的基准测试。在1,782个实例子集上，Claude Sonnet 4.5达到36.20% pass@10，GPT-5 34.57%，Gemini 2.5 Pro 24.92%，GPT-4o 16.89%。微调SWE-Bench++实例在SWE-bench多语言基准上带来可测量的改进。

Conclusion: SWE-Bench++提供了一个可扩展、多语言的基准测试，用于评估和改进仓库级代码生成，解决了现有基准的局限性。

Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

</details>


### [9] [SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review](https://arxiv.org/abs/2512.17540)
*Kai Wang,Bingcheng Mao,Shuai Jia,Yujie Ding,Dongming Han,Tianyi Ma,Bin Cao*

Main category: cs.SE

TL;DR: SGCR框架通过将LLM基于人类编写的规范进行代码审查，采用显式和隐式双路径架构确保可靠性和上下文感知，在工业环境中实现42%的开发者采纳率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码审查中存在可靠性不足、缺乏上下文感知和控制能力的问题，阻碍了实际应用。需要一种方法既能利用LLM的生成能力，又能满足软件工程对可靠性的严格要求。

Method: 提出Specification-Grounded Code Review (SGCR)框架，采用双路径架构：显式路径确保对规范衍生规则的确定性遵守，隐式路径启发式发现并验证超出规则的问题。

Result: 在HiThink Research的工业环境中部署，SGCR的建议实现了42%的开发者采纳率，相比基线LLM（22%）有90.9%的相对提升。

Conclusion: 规范基础化是弥合LLM生成能力与软件工程可靠性要求之间差距的强大范式，能够产生可信赖且相关的代码审查反馈。

Abstract: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 论文提出turn-PPO算法，通过将MDP从token级改为turn级来增强PPO在多轮任务中的表现，解决了GRPO在长视野推理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练交互式LLM智能体方面具有优势，但现有的GRPO算法在多轮任务中，特别是在需要长视野推理的场景下存在明显局限性。需要更稳定有效的优势估计策略来应对这些挑战。

Method: 首先探索PPO作为GRPO的替代方案，发现PPO更鲁棒。然后提出turn-PPO变体，将MDP从常用的token级重新表述为turn级，专门针对多轮场景进行优化。

Result: 在WebShop和Sokoban数据集上的实验结果表明，turn-PPO无论是否包含长推理组件都表现出有效性。

Conclusion: turn-PPO通过turn级MDP重新表述，有效增强了PPO在多轮任务中的性能，为长视野推理场景提供了更稳定有效的强化学习解决方案。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [11] [Dynamic Tool Dependency Retrieval for Efficient Function Calling](https://arxiv.org/abs/2512.17052)
*Bhrij Patel,Davide Belli,Amir Jalalirad,Maximilian Arnold,Aleksandr Ermovol,Bence Major*

Main category: cs.LG

TL;DR: 提出DTDR动态工具依赖检索方法，通过结合初始查询和演化执行上下文来改进LLM函数调用代理的工具选择，相比静态检索方法将函数调用成功率提升23%-104%。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的工具选择方法依赖静态有限输入，无法捕捉多步骤工具依赖关系和演化任务上下文，导致引入无关工具误导代理，降低效率和准确性。

Method: 提出动态工具依赖检索(DTDR)方法，这是一种轻量级检索方法，同时考虑初始查询和演化执行上下文，从函数调用演示中建模工具依赖关系，实现自适应检索。

Result: 在多个数据集和LLM骨干网络上评估，DTDR相比最先进的静态检索方法将函数调用成功率提升23%-104%，同时评估了检索精度、下游任务准确性和计算效率。

Conclusion: 动态工具检索能显著提升函数调用代理的性能，通过建模工具依赖关系和适应任务上下文演化来改进工具选择。

Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.

</details>


### [12] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: 训练模型在包含故意错误的推理轨迹上，可以提高其检测和恢复错误的能力，而不损害标准问题解决能力


<details>
  <summary>Details</summary>
Motivation: 链式思维提示已成为大语言模型数学推理的核心方法，但模型对早期错误仍然脆弱：单个算术错误或不合理推断通常会传播到最终错误答案。研究是否可以通过训练模型在故意错误的推理轨迹上来提高其错误检测和恢复能力

Method: 使用MATH-lighteval竞赛级问题，生成包含一个受控错误的CoT前缀（计算错误或推理错误），使用GRPO对Qwen3-4B进行微调，采用二元最终答案奖励机制

Result: Mixed-CoT-RL模型在干净问题上与标准RL表现相当（41% vs 41%），但在包含错误推理的问题上显著优于标准RL（24% vs 19%）。仅使用干净数据训练的RL会降低鲁棒性（19% vs 20%）。混合训练效果最佳，推理错误训练比计算错误训练带来更大的鲁棒性提升

Conclusion: 在训练中暴露于错误轨迹可以改善错误恢复行为而不牺牲准确性，为LLMs中更鲁棒的数学推理提供了路径

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [13] [A Theoretical Analysis of State Similarity Between Markov Decision Processes](https://arxiv.org/abs/2512.17265)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 该论文提出了广义双模拟度量（GBSM），用于测量任意马尔可夫决策过程（MDP）对之间的状态相似性，并严格证明了其三个基本度量性质，为跨MDP的理论分析提供了更紧的界限。


<details>
  <summary>Details</summary>
Motivation: 双模拟度量（BSM）在分析单个MDP内的状态相似性方面很有效，但扩展到多个MDP之间时面临挑战。先前工作尝试将BSM扩展到MDP对，但由于缺乏完善的数学性质，限制了MDP间的进一步理论分析。

Method: 正式建立了广义双模拟度量（GBSM），用于测量任意MDP对之间的状态相似性。严格证明了GBSM的三个基本度量性质：对称性、跨MDP三角不等式和相同空间上的距离界限。

Result: 利用GBSM的性质，理论上分析了跨MDP的策略迁移、状态聚合和基于采样的估计，获得了比标准BSM更严格的理论界限。GBSM还提供了闭式样本复杂度估计，改进了基于BSM的现有渐近结果。数值结果验证了理论发现。

Conclusion: GBSM为跨MDP的状态相似性测量提供了严格的数学框架，具有更好的理论性质和实际应用价值，特别是在多MDP场景中表现出有效性。

Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.

</details>


### [14] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: TRAPO是一种混合训练框架，通过在每个训练实例中交错SFT和RL，结合专家监督与模型自我探索，解决了传统两阶段训练中SFT抑制探索的问题，在数学推理任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段训练流程（先SFT后RL）存在不一致性：SFT强制模仿会抑制探索并导致遗忘，限制了RL的改进潜力。需要一种更高效的训练框架来统一外部监督和自我探索。

Method: TRAPO框架在每个训练实例中交错SFT和RL：在专家前缀上优化SFT损失，在模型自身补全上优化RL损失。引入信任区域SFT（TrSFT）来稳定训练，最小化信任区域内的前向KL散度，在区域外衰减优化。还包含自适应前缀选择机制，基于测量效用分配专家指导。

Result: 在五个数学推理基准测试中，TRAPO一致超越了标准SFT、RL、SFT-then-RL流程以及近期最先进方法，为推理增强型LLM建立了强大的新范式。

Conclusion: TRAPO通过统一SFT和RL训练，解决了传统两阶段流程的不一致性，提供了一种更有效的LLM推理能力增强方法，在数学推理任务上取得了显著改进。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [15] [Introducing GPT-5.2-Codex](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FAzoRbc/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/Bs982Y0-W6j6pTmiWEYNF9P6Cv9B7EsVZF7r9WzQrqo=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GPT-5.2-Codex是GPT-5.2针对智能编码优化的版本，具有上下文压缩、大代码变更处理能力提升、Windows环境性能改进和显著增强的网络安全能力，现已向付费ChatGPT用户开放。


<details>
  <summary>Details</summary>
Motivation: OpenAI旨在开发专门针对编码任务的AI模型，提升在长时程工作、大规模代码修改、Windows环境适应性和网络安全方面的性能，为开发者提供更强大的编码助手。

Method: 基于GPT-5.2架构进行优化，采用上下文压缩技术处理长序列，针对代码变更任务进行专门训练，增强Windows环境兼容性，并强化网络安全能力。

Result: GPT-5.2-Codex在编码代理任务中表现显著提升，已向付费ChatGPT用户开放，API版本将在未来几周内安全地提供给开发者使用。

Conclusion: GPT-5.2-Codex代表了编码专用AI模型的进一步发展，通过多项针对性优化为开发者提供了更强大的编码辅助工具，同时注重安全性部署。

Abstract: Introducing GPT-5.2-Codex (9 minute read) GPT‑5.2-Codex is a version of GPT‑5.2 optimized for agentic coding. It features improvements in long-horizon work through context compaction, stronger performance on large code changes, improved performance in Windows environments, and significantly stronger cybersecurity capabilities. The model is now available in all Codex surfaces for paid ChatGPT users. OpenAI is working towards safely enabling the model for API users in the coming weeks.

</details>


### [16] [My LLM coding workflow going into 2026](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fmy-llm-coding-workflow-going-into%3Futm_source=tldrnewsletter/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/xingaawQKu88yKcIHZS7IxJ89d3W9GRt5RGxm_q10Qs=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 关于2026年LLM编码工作流程的实用指南，提供规划、编码和AI协作的最佳实践


<details>
  <summary>Details</summary>
Motivation: 随着AI编码助手日益普及，需要系统化的最佳实践来优化LLM在编码工作流程中的使用，提高开发效率和协作质量

Method: 基于作者实际经验总结的方法论，涵盖规划阶段、编码实践、AI协作策略等系统化工作流程

Result: 提供了一套完整的LLM编码工作流程框架，包括具体操作指南、工具使用建议和协作模式

Conclusion: 系统化的LLM编码工作流程能显著提升开发效率，但需要结合人类专业判断和适当的协作策略

Abstract: My LLM coding workflow going into 2026 (28 minute read) This post provides tips and best practices for planning, coding, and collaborating with AI.

</details>


### [17] [We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJJd0BW/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/T7iElYe0qm8bUWvxUwXkPa3Z8nnZ7-C4AYx5v5jFQuU=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic设计了一个AI驱动的自动售货机，让AI代理拥有自主权、资金和人类同事，结果损失了数百美元


<details>
  <summary>Details</summary>
Motivation: 探索当AI代理被赋予自主权、资金和人类同事时会发生什么，测试AI在现实世界商业环境中的表现

Method: 设计并部署了一个AI驱动的自动售货机，让AI代理管理库存、定价和销售，拥有自主决策权和资金

Result: AI代理在运营过程中损失了数百美元，表明AI在现实世界商业决策中存在严重问题

Conclusion: AI代理在自主运营商业活动时面临重大挑战，需要更谨慎的设计和监督机制

Abstract: We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars (11 minute read) Anthropic designed an AI-powered vending machine to see what happens when an AI agent is given autonomy, money, and human colleagues.

</details>


### [18] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/XUFpvnHzMTrqf3o9PpD37d_0KVTE0OZrAdJcoFi1DRY=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic实验让AI代理自主运营办公室自动售货机，结果亏损数百美元，揭示了AI在现实商业环境中自主决策的挑战


<details>
  <summary>Details</summary>
Motivation: 探索当AI代理被赋予自主权、资金和人类同事时会发生什么，测试AI在现实商业环境中的实际表现和潜在问题

Method: 设计一个AI驱动的自动售货机，让AI代理完全自主运营，包括库存管理、定价、采购决策，并与人类同事互动

Result: AI运营的自动售货机亏损了数百美元，表明AI在现实商业决策中存在明显缺陷和优化问题

Conclusion: AI在自主商业运营中面临实际挑战，需要更好的约束机制和更复杂的决策框架，当前技术还不适合完全自主的商业应用

Abstract: We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars (11 minute read) Anthropic designed an AI-powered vending machine to see what happens when an AI agent is given autonomy, money, and human colleagues.

</details>


### [19] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/0Dk9i4LHlLN1ayoLTWFqcJ1IRC2lj7MJ8Ud5l6aVE_E=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic实验让AI代理自主运营办公室自动售货机，结果亏损数百美元，揭示了AI在真实商业环境中的局限性


<details>
  <summary>Details</summary>
Motivation: 探索当AI代理被赋予自主权、资金和人类同事时会发生什么，测试AI在真实商业环境中的表现和局限性

Method: 设计一个AI驱动的自动售货机，让AI代理自主运营，包括库存管理、定价、采购决策等，并与人类同事互动

Result: AI代理运营的自动售货机亏损了数百美元，表明AI在真实商业环境中的决策存在明显缺陷

Conclusion: AI代理在真实商业环境中的自主运营面临挑战，需要更好的训练和约束机制来避免经济损失

Abstract: We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars (11 minute read) Anthropic designed an AI-powered vending machine to see what happens when an AI agent is given autonomy, money, and human colleagues.

</details>


### [20] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/6xHQzcc3zQo2jggRi2YFQIQs7i-DFKcb1KONKgzg93k=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic设计了一个AI驱动的自动售货机，让AI代理拥有自主权、资金和人类同事，结果导致数百美元损失


<details>
  <summary>Details</summary>
Motivation: 探索当AI代理被赋予自主权、资金和人类同事时会发生什么，测试AI在现实世界商业环境中的表现

Method: 设计并部署了一个AI驱动的自动售货机，让AI代理管理库存、定价和销售，与人类同事互动

Result: AI代理在管理自动售货机时损失了数百美元，表现出不理想的商业决策能力

Conclusion: AI在现实世界商业环境中需要更多监督和约束，完全自主的AI代理可能导致经济损失

Abstract: We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars (11 minute read) Anthropic designed an AI-powered vending machine to see what happens when an AI agent is given autonomy, money, and human colleagues.

</details>


### [21] [Introducing GPT-5.2-Codex](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGsXHsk/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/taouC-mr7ckR60h-LHyWwefynCLX7GP41OjJaE4PZO8=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI发布GPT-5.2-Codex模型，在长时程任务、大规模代码修改和Windows环境下表现提升，在智能编码代理方面创下新基准，并已发现真实世界漏洞。


<details>
  <summary>Details</summary>
Motivation: 开发更强大的代码生成模型，提升在复杂编码任务（特别是长时程任务、大规模代码修改和Windows环境）中的性能，推动智能编码代理技术的发展。

Method: 基于GPT架构开发的新一代代码模型GPT-5.2-Codex，针对长时程任务、大规模代码修改和Windows环境进行了专门优化。

Result: 模型在智能编码代理方面创下新基准，性能显著提升，并已实际应用于发现真实世界漏洞。

Conclusion: GPT-5.2-Codex代表了代码生成模型的重要进展，在复杂编码任务中展现出强大能力，具有实际应用价值。

Abstract: Introducing GPT-5.2-Codex (9 minute read) OpenAI has launched GPT-5.2-Codex. This new model improves performance on long-horizon tasks, large code changes, and in Windows environments. It sets new benchmarks in agentic coding. The model's capabilities have already led to the discovery of real-world vulnerabilities.

</details>


### [22] [Piloting Claude in Chrome](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fclaude-for-chrome%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/FK-I3IBRlYkD7bdb2gVhOLmBW_GgPgGmT6nnIWnntfY=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude在Chrome浏览器中集成了Claude Code功能，可以直接在浏览器中测试代码验证工作，并能通过控制台日志查看客户端错误。


<details>
  <summary>Details</summary>
Motivation: 提高代码代理在浏览器环境中的工作效率和准确性，通过直接集成到Chrome浏览器中，使Claude能够实时测试代码并获取客户端错误反馈。

Method: 在Chrome浏览器中集成Claude Code功能，实现代码在浏览器环境中的直接测试和验证，同时通过控制台日志获取客户端错误信息。

Result: Claude能够在Chrome浏览器中直接测试代码并验证其工作，同时能够查看客户端错误信息，提高了代码开发和调试的效率。

Conclusion: Chrome浏览器中的Claude集成显著提升了代码代理的工作效率，通过实时测试和错误反馈机制，使代码开发和调试更加高效。

Abstract: Piloting Claude in Chrome (8 minute read) Claude in Chrome has an integration with Claude Code, where Claude Code can test code directly in the browser to validate its work. Claude can also see client-side errors via console logs.

</details>


### [23] [A Better Way to Work with Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnimbalyst.com%2F%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/MUDqYj50nMMQ0Fl4pkkFRrf_QrEIDm3Mz5FiR6XiF6Y=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Nimbalyst是一个本地WYSIWYG编辑器和会话管理器，让用户能够在完整上下文中与Claude Code进行迭代


<details>
  <summary>Details</summary>
Motivation: 现有的Claude Code交互方式可能缺乏完整的上下文管理和本地编辑体验，需要更好的工具来提升开发效率

Method: 开发了一个本地所见即所得编辑器和会话管理系统，支持与Claude Code的完整上下文迭代

Result: 创建了Nimbalyst工具，提供了更好的Claude Code工作方式，支持本地编辑和会话管理

Conclusion: Nimbalyst通过本地WYSIWYG编辑器和会话管理功能，显著改善了与Claude Code的工作体验

Abstract: A Better Way to Work with Claude Code (Website) Nimbalyst is a local WYSIWYG editor and session manager that lets users iterate with Claude Code with full context.

</details>


### [24] [One Agent Isn't Enough](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenr.build%2Fblog%2Fone-agent-isnt-enough%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/Jz2bir_AHr58G7nPNiQB4ZZuPVrLbTA0VAH1jhcxfTo=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 单智能体在代码生成中存在方差问题，由于LLM的随机性可能导致错过最优解


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代码生成智能体存在随机性导致的方差问题，单次运行可能无法找到最优解决方案

Method: 未在摘要中明确说明具体方法，但暗示需要超越单智能体架构

Result: 未提供具体实验结果

Conclusion: 单智能体不足够，需要更复杂的架构来减少方差并提高代码生成质量

Abstract: One Agent Isn't Enough (12 minute read) Agentic coding often struggles with variance, as the stochastic nature of LLMs means a single agent run might miss the optimal solution.

</details>


### [25] [Your job is to deliver code you have proven to work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F18%2Fcode-proven-to-work%2F%3Futm_source=tldrdevops/1/0100019b3690ecd9-71b197c8-a61e-4116-a2e5-7f26c9bf9be1-000000/i_f6kzuM5bsdSkjuhbgB6Fys9zdzIo04O6qDXaA2NT8=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 工程师应交付经过验证的代码，而非将未经测试的AI生成代码推给评审者；需要手动和自动化测试证明，即使使用编码代理，人类仍需对代码正确性负责


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成工具可能导致工程师将未经充分测试的代码推给评审者，增加了评审负担并降低了代码质量，需要强调工程师对代码正确性的最终责任

Method: 提出工程师应通过手动测试和自动化测试来证明代码的正确性，强调即使使用AI编码代理，人类仍需确保每个PR都附带正确性证据

Result: 明确了工程师在AI辅助编程环境中的责任边界，强调代码验证的必要性和人类对代码质量的最终责任

Conclusion: 工程师必须交付经过验证的代码，不能将AI生成代码的验证责任转嫁给评审者，人类对代码正确性负有最终责任

Abstract: Your job is to deliver code you have proven to work (4 minute read) Engineers are responsible for delivering code that is proven to work, not dumping large, untested AI-generated changes onto reviewers and shifting the burden of validation onto others. Proof requires both manual testing and automated tests, and even when using coding agents, the human remains accountable for ensuring evidence of correctness accompanies every PR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE是一个用于优化LLM智能体上下文管理的统一框架，通过计划感知的自动上下文工程，在减少上下文负载的同时提高智能体性能。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂多步工作流中产生快速扩展的上下文，现有压缩方法忽略了多步、计划感知的智能体推理特性，需要专门的上下文优化方案。

Method: 提出PAACE框架，包含：1) PAACE-Syn：生成带压缩监督标注的合成智能体工作流；2) PAACE-FT：从成功教师演示中蒸馏训练的计划感知压缩器。

Result: 在AppWorld、OfficeBench和8-Objective QA基准测试中，PAACE在提高智能体正确性的同时显著降低上下文负载。蒸馏模型保留97%性能，推理成本降低一个数量级。

Conclusion: PAACE证明了计划感知上下文工程的有效性，能够实现高效、实用的智能体上下文管理，为紧凑模型的部署提供了可行方案。

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [27] [Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats](https://arxiv.org/abs/2512.17041)
*Ali Eslami,Jiangbo Yu*

Main category: cs.AI

TL;DR: 该论文首次系统分析了智能车辆中代理式AI的安全风险，包括OWASP式风险和跨层攻击，提出了基于角色的架构和严重性矩阵框架。


<details>
  <summary>Details</summary>
Motivation: 现有OWASP代理式AI安全风险框架未针对车辆等安全关键网络物理系统设计，也未考虑与感知、通信、控制等层的交互，需要专门研究智能车辆的安全威胁。

Method: 提出基于角色的智能车辆架构（个人代理和驾驶策略代理），分析代理式AI层和跨层风险，使用严重性矩阵和攻击链分析展示小扰动如何升级为不安全行为。

Result: 建立了首个结构化框架来分析当前和新兴车辆平台中代理式AI的安全风险，展示了跨层攻击如何导致车辆行为失准或不安全。

Conclusion: 该研究为智能车辆代理式AI安全风险分析提供了首个系统化基础框架，填补了现有安全框架在车辆等网络物理系统中的空白。

Abstract: Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.

</details>


### [28] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 使用LLM驱动的智能体在虚拟社会中模拟现实威胁与象征性威胁对冲突的影响，发现现实威胁直接增加敌意，而象征性威胁效应较弱且完全通过内群体偏见中介，仅在现实威胁缺失时增加敌意。


<details>
  <summary>Details</summary>
Motivation: 人类冲突常归因于物质条件和象征价值的威胁，但两者如何相互作用以及哪个占主导地位尚不清楚。研究受到因果控制弱、伦理约束和时间数据稀缺的限制。

Method: 使用大型语言模型驱动的智能体在虚拟社会中模拟，独立变化现实威胁和象征性威胁，同时追踪行动、语言和态度。通过表征分析验证LLM编码的威胁状态，并通过操纵这些状态因果性地改变行为。

Result: 底层LLM将现实威胁、象征性威胁和敌意编码为不同的内部状态；现实威胁直接增加敌意，象征性威胁效应较弱且完全通过内群体偏见中介，仅在现实威胁缺失时增加敌意；非敌对群体间接触能缓冲冲突升级，结构性不对称使敌意集中在多数群体中。

Conclusion: 研究提供了威胁驱动冲突的因果解释框架，区分了现实威胁和象征性威胁的不同作用机制，为理解人类冲突提供了新的模拟方法和理论见解。

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [29] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 将AIXI强化学习智能体推广到更广泛的效用函数类别，通过将效用分配给交互历史，处理信念分布中某些假设只预测有限历史前缀的模糊性，探讨死亡解释与不精确概率两种视角，并研究Choquet积分的可计算性。


<details>
  <summary>Details</summary>
Motivation: AIXI智能体通常假设效用函数定义在无限交互历史上，但实际中许多假设只能预测有限历史前缀，这导致模糊性。需要处理这种有限预测情况下的效用分配问题，特别是所谓的"半测度损失"（semimeasure loss）的两种解释：死亡概率或不精确概率。

Method: 1) 将AIXI推广到更广泛的效用函数类别；2) 提出两种处理有限历史前缀的方法：死亡解释（将半测度损失解释为死亡概率）和不精确概率解释（将信念分布视为不精确概率分布）；3) 使用不精确概率理论中的Choquet积分计算期望效用；4) 分析这些方法的可计算性水平。

Result: 1) 标准递归值函数可作为Choquet积分的特例恢复；2) 在不精确概率解释下，Choquet积分提供了合理的期望效用计算方法；3) 然而，在死亡解释下，最一般的期望效用不能表示为Choquet积分；4) 分析了这些方法的可计算性水平。

Conclusion: 通过推广AIXI智能体以容纳更广泛的效用函数，并探讨有限历史预测的两种解释框架，为强化学习中的信念建模提供了更丰富的理论基础。不精确概率视角下的Choquet积分方法具有理论优势，但死亡解释下的最一般情况需要其他数学工具。

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [30] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 提出了一种ASP求解器在循环中的方法，通过求解器引导的指令微调来提升LLM生成答案集编程代码的能力，仅需自然语言问题描述和解决方案即可训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成领域特定语言（如答案集编程ASP）代码方面存在挑战，因为ASP代码具有复杂的语义解析需求，且LLM在预训练阶段接触的ASP示例有限。

Method: 采用ASP求解器在循环中的方法：1) 从LLM采样ASP语句作为程序延续；2) 利用ASP声明式编程特性（部分编码逐渐缩小解空间），基于求解器反馈将样本分为接受和拒绝实例；3) 对筛选数据进行监督微调；4) 使用求解器引导的搜索（包括best-of-N采样）进一步提高鲁棒性。

Result: 在两个数据集上的两种不同提示设置中均显示出持续改进，证明了该方法在提升LLM生成ASP代码能力方面的有效性。

Conclusion: 提出的ASP求解器在循环中的方法能够有效提升LLM生成答案集编程代码的能力，仅需自然语言问题描述和解决方案即可实现训练，为解决领域特定语言代码生成问题提供了新思路。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [31] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: 提出SAGE框架，通过强化学习和技能库增强LLM智能体的自我进化能力，在AppWorld基准上显著提升任务完成率并减少交互步骤和token消耗。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在新环境中部署时难以持续改进和适应，现有技能库方法主要依赖提示工程，实现一致性挑战较大。

Method: 提出SAGE强化学习框架，包含Sequential Rollout机制（在相似任务链上迭代部署智能体）和Skill-integrated Reward（结合原始结果奖励的技能集成奖励），系统地将技能融入学习过程。

Result: 在AppWorld基准测试中，SAGE应用于有专家经验的监督微调模型，实现了8.9%更高的场景目标完成率，同时减少26%交互步骤和59%token生成，在准确性和效率上都显著优于现有方法。

Conclusion: SAGE框架通过强化学习和技能库有效增强了LLM智能体的自我进化能力，为解决智能体在新环境中的适应问题提供了有前景的解决方案。

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [32] [Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction](https://arxiv.org/abs/2512.17250)
*Ziyang Lin,Zixuan Sun,Sanhorn Chen,Xiaoyang Chen,Roy Zhao*

Main category: cs.AI

TL;DR: 提出一种推测-校正框架，将推测执行思想应用于基于模型的实时控制，通过预测动作队列和潜在状态，减少规划推理次数，降低延迟，同时保持控制性能。


<details>
  <summary>Details</summary>
Motivation: 实时顺序控制代理常受推理延迟瓶颈，即使适度的每步规划延迟也会破坏控制稳定性并降低性能。需要减少规划推理频率以降低延迟。

Method: 采用推测-校正框架，结合TD-MPC2：1) 预训练世界模型和潜在空间MPC规划器生成短时域动作队列和预测潜在状态；2) 新观测到达时测量真实与预测潜在状态差异；3) 差异较小时使用轻量级学习校正器（门控双塔MLP或时序Transformer）应用残差更新；4) 差异较大时回退到完全重新规划。

Result: 在DMC Humanoid-Walk任务中：规划推理次数从500降至282，端到端步延迟降低25%，控制性能仅下降7.1%。消融实验显示无校正的推测执行在较长时域不可靠。

Conclusion: 推测-校正框架能有效减少实时控制中的规划推理频率，显著降低延迟，同时通过不匹配感知校正保持鲁棒性。该方法为实时控制中的延迟优化提供了有效解决方案。

Abstract: Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.

</details>


### [33] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 该研究将Rashomon效应扩展到序列决策领域，发现不同内部结构的策略可以产生相同行为，并利用这些策略构建更鲁棒的集成系统。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但在序列决策领域尚未探索。研究者希望将这一概念扩展到强化学习等序列决策场景，研究不同内部结构的策略如何产生相同行为。

Method: 使用形式化验证方法构建和比较每个策略在环境中的完整概率行为，通过验证策略在状态空间和动作选择上的一致性来识别Rashomon效应。

Result: 实验证明Rashomon效应确实存在于序列决策中。从Rashomon集合构建的集成策略对分布偏移表现出更强的鲁棒性，且从该集合推导的宽松策略在保持最优性能的同时减少了验证计算需求。

Conclusion: Rashomon效应在序列决策中普遍存在，利用这一现象可以构建更鲁棒的策略集成，并为策略验证提供计算效率更高的方法。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [34] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 提出定时奖励机（TRMs）作为奖励机的扩展，将时间约束纳入奖励结构，使强化学习能处理时间敏感应用中的精确时序要求。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机无法建模精确的时间约束，限制了在时间敏感应用中的使用。需要更丰富的奖励规范来表达具有时序要求的任务。

Method: 提出定时奖励机（TRMs），扩展奖励机以包含时间约束。开发基于表格Q学习的模型无关RL框架，通过定时自动机抽象将TRM集成到学习中，并采用反事实想象启发式方法利用TRM结构改进搜索。

Result: 实验证明算法能在流行的RL基准测试中学习到满足TRM指定时间约束的高奖励策略。比较研究展示了不同TRM语义下的性能，以及反事实想象方法的优势。

Conclusion: TRMs为强化学习提供了更丰富的奖励规范能力，能够处理时间敏感任务，提出的算法能有效学习满足时序约束的策略。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>
