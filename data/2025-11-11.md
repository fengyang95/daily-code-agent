<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [wechat.article](#wechat.article) [Total: 13]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.LG](#cs.LG) [Total: 15]
- [tldr.article](#tldr.article) [Total: 3]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Future of AI Models: A Computational perspective on Model collapse](https://arxiv.org/abs/2511.05535)
*Trivikram Satharasi,S Sitharama Iyengar*

Main category: cs.CL

TL;DR: 该研究量化了AI生成内容对语言多样性的影响，通过分析2013-2025年维基百科文本的语义相似度变化，发现LLM公开采用后语义相似度呈指数级增长，表明递归训练可能导致模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容在网络中占比迅速增加（新网页74.2%含AI内容，30-40%网络语料为合成内容），递归训练可能侵蚀语言和语义多样性，导致模型崩溃问题。

Method: 使用Transformer嵌入和余弦相似度指标，分析2013-2025年英语维基百科（过滤后的Common Crawl数据）的逐年语义相似度变化。

Result: 在LLM公开采用前语义相似度稳步上升（受早期RNN/LSTM翻译和文本规范化影响），LLM采用后相似度呈指数级增长，波动反映了不可约的语言多样性、语料库规模变化和有限采样误差。

Conclusion: 研究提供了数据驱动的估计，表明递归AI污染何时可能显著威胁数据丰富性和模型泛化能力。

Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.

</details>


### [2] [Optimizing Diversity and Quality through Base-Aligned Model Collaboration](https://arxiv.org/abs/2511.05650)
*Yichen Wang,Chenghao Yang,Tenghao Huang,Muhao Chen,Jonathan May,Mina Lee*

Main category: cs.CL

TL;DR: BACo是一个推理时token级模型协作框架，通过动态结合基础LLM和对齐LLM来优化输出多样性和质量，在单次推理中实现21.3%的多样性和质量联合提升。


<details>
  <summary>Details</summary>
Motivation: 对齐虽然提高了LLM的输出质量，但牺牲了多样性，导致不同生成结果高度相似。现有方法如重训练、提示工程和多采样要么降低质量，要么需要昂贵的解码或后训练成本。

Method: 使用路由策略在token级别动态决定从基础模型还是对齐模型解码，基于下一个token预测的不确定性和预测内容的语义角色。

Result: 在三个开放生成任务和13个指标上，BACo持续超越最先进的推理时基线方法，最佳路由器实现21.3%的多样性和质量联合提升，人类评估也证实了这些改进。

Conclusion: 基础模型和对齐模型之间的协作可以优化和控制多样性与质量。

Abstract: Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.

</details>


### [3] [OckBench: Measuring the Efficiency of LLM Reasoning](https://arxiv.org/abs/2511.05722)
*Zheng Du,Hao Kang,Song Han,Tushar Krishna,Ligeng Zhu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as "free" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .

</details>


### [4] [Quantifying Edits Decay in Fine-tuned LLMs](https://arxiv.org/abs/2511.05852)
*Yinjie Cheng,Paul Youssef,Christin Seifert,Jörg Schlötterer,Zhixue Zhao*

Main category: cs.CL

TL;DR: 该论文系统研究了知识编辑后微调对编辑效果的影响，发现编辑会在微调后衰减，并提出选择性层微调策略来有效移除编辑。


<details>
  <summary>Details</summary>
Motivation: 研究知识编辑与微调两种后训练干预措施的交互作用，解决两个实际问题：移除隐蔽或恶意编辑，以及保留有益编辑。

Method: 评估两种最先进的编辑方法（MEMIT、AlphaEdit）和三种微调方法（全参数、LoRA、DoRA），在五个LLM和三个数据集上进行232个实验配置。

Result: 编辑在微调后会衰减，不同配置下生存率不同；仅微调编辑层可有效移除编辑但略微影响下游性能；微调非编辑层比全微调损害更多编辑。

Conclusion: 为知识编辑与微调的整合建立了经验基准和可行策略，强调评估模型编辑需要考虑完整的LLM应用流程。

Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.

</details>


### [5] [Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs](https://arxiv.org/abs/2511.05933)
*Renfei Zhang,Manasa Kaniselvan,Niloofar Mireshghallah*

Main category: cs.CL

TL;DR: RL增强的语言模型在知识召回任务中表现优于基础模型和SFT模型，特别是在需要遍历层次化结构化知识的任务中。这种优势主要来自改进的程序性技能，而非新获得的数据。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，即RL会损害语言模型的记忆知识，探索RL如何通过改进知识遍历能力来提升知识召回表现。

Method: 使用结构化提示引导SFT模型进行层次遍历，并进行层间激活分析比较SFT和RL模型的表示差异。

Result: 结构化提示能显著缩小SFT与RL模型的性能差距（从24pp降至7pp），但RL模型在深度检索任务中仍保持更好的程序路径召回能力。激活分析显示RL主要改变知识遍历方式而非知识表示本身。

Conclusion: RL通过改进程序性知识遍历技能而非改变事实表示来提升知识召回性能，这为理解RL对语言模型的影响提供了新视角。

Abstract: Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.

</details>


### [6] [Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning](https://arxiv.org/abs/2511.06190)
*Sangmook Lee,Dohyung Kim,Hyukhun Koh,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: STEER是一个无需外部模型的领域无关框架，通过在推理步骤级别基于小模型的置信度分数进行路由，仅在必要时调用大模型，以降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有路由模型在领域转移时缺乏鲁棒性，且需要昂贵的标签数据训练。STEER旨在通过模型内部置信度实现细粒度路由，无需外部模块或大量训练数据。

Method: 利用小模型在生成推理步骤前的置信度分数进行路由决策，当置信度低时调用大模型，实现步骤级别的动态模型切换。

Result: 在数学推理、多跳问答和规划任务等多个领域基准测试中，STEER在减少48% FLOPs的同时准确率提升20%，优于依赖外部模块的基线方法。

Conclusion: 模型内部置信度可作为鲁棒、领域无关的路由信号，为高效LLM部署提供了可扩展的路径。

Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.

</details>


### [7] [SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization](https://arxiv.org/abs/2511.06222)
*Yue Huang,Xiangqi Wang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出优先级对齐(priority alignment)新范式，在保持信任度的前提下提升帮助性，通过自优先级对齐(SPA)框架实现无监督的模型自我优化。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景中，LLM需要在信任度和帮助性之间取得平衡，但这两个目标常常冲突。需要一种能够确保信任度优先于帮助性的对齐方法。

Method: 提出自优先级对齐(SPA)框架：生成多样化响应→自我评估和精炼→双重标准去噪→构建词典序偏好对→使用不确定性加权对齐损失进行微调。

Result: 在多个基准测试中，SPA在不牺牲安全性的情况下提高了帮助性，优于强基线方法，同时保持了通用能力。

Conclusion: SPA为关键LLM应用提供了一种可扩展且可解释的对齐策略，实现了信任度优先于帮助性的目标。

Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict "trustworthy-before-helpful" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.

</details>


### [8] [Automated Circuit Interpretation via Probe Prompting](https://arxiv.org/abs/2511.07002)
*Giuseppe Birardi*

Main category: cs.CL

TL;DR: 提出了probe prompting方法，自动将归因图转换为紧凑、可解释的子图，通过概念对齐的超级节点来加速神经网络机制可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性需要大量手动分析（单个提示约需2小时），现有归因图解释效率低下，需要自动化工具来识别特征通路。

Method: 从种子提示和目标logit开始，选择高影响力特征，生成概念导向但上下文变化的探针，使用透明决策规则将特征按跨提示激活特征分组为语义、关系和Say-X类别。

Result: 在五个提示测试中，probe-prompted子图保持高解释覆盖率同时压缩复杂度（完整性0.83，替换度0.54）。相比几何聚类基线，概念对齐组表现出更高行为一致性：峰值token一致性高2.3倍，激活模式相似度高5.8倍。实体交换测试揭示了分层结构。

Conclusion: 支持transformer计算的后台-专业化视图，早期层特征稳健转移，晚期层Say-X特征专门用于输出促进。

Abstract: Mechanistic interpretability aims to understand neural networks by
identifying which learned features mediate specific behaviors. Attribution
graphs reveal these feature pathways, but interpreting them requires extensive
manual analysis -- a single prompt can take approximately 2 hours for an
experienced circuit tracer. We present probe prompting, an automated pipeline
that transforms attribution graphs into compact, interpretable subgraphs built
from concept-aligned supernodes. Starting from a seed prompt and target logit,
we select high-influence features, generate concept-targeted yet
context-varying probes, and group features by cross-prompt activation
signatures into Semantic, Relationship, and Say-X categories using transparent
decision rules.
  Across five prompts including classic "capitals" circuits, probe-prompted
subgraphs preserve high explanatory coverage while compressing complexity
(Completeness 0.83, mean across circuits; Replacement 0.54). Compared to
geometric clustering baselines, concept-aligned groups exhibit higher
behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and
5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower
geometric compactness. Entity-swap tests reveal a layerwise hierarchy:
early-layer features transfer robustly (64% transfer rate, mean layer 6.3),
while late-layer Say-X features specialize for output promotion (mean layer
16.4), supporting a backbone-and-specialization view of transformer
computation.
  We release code (https://github.com/peppinob-ol/attribution-graph-probing),
an interactive demo
(https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal
artifacts enabling immediate reproduction and community adoption.

</details>


### [9] [More Agents Helps but Adversarial Robustness Gap Persists](https://arxiv.org/abs/2511.07112)
*Khashayar Alavi,Zhastay Yeltay,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 多智能体协作能提高数学问答准确性，但对对抗性扰动的鲁棒性提升有限。标点符号噪声影响随强度增加，而人类拼写错误是主要瓶颈，即使增加智能体数量也无法消除鲁棒性差距。


<details>
  <summary>Details</summary>
Motivation: 研究多LLM智能体协作是否比单个LLM在面对对抗性输入时更鲁棒，特别是在数学问答任务中。

Method: 使用Agent Forest统一采样投票框架，在6个开源模型上测试不同数量的智能体（1-25个），评估标点符号噪声和真实世界拼写错误对4个数学基准的影响。

Result: 协作确实提高准确性，但对抗鲁棒性差距持续存在。标点噪声影响与强度相关，人类拼写错误是主要瓶颈，即使使用25个智能体也无法消除鲁棒性差距。

Conclusion: 多智能体协作能提升准确性但无法解决对抗鲁棒性问题，人类拼写错误仍是主要挑战。

Abstract: When LLM agents work together, they seem to be more powerful than a single
LLM in mathematical question answering. However, are they also more robust to
adversarial inputs? We investigate this question using adversarially perturbed
math questions. These perturbations include punctuation noise with three
intensities (10, 30, and 50 percent), plus real-world and human-like typos
(WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent
Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B,
Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math,
MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15,
20, 25). Our findings show that (1) Noise type matters: punctuation noise harm
scales with its severity, and the human typos remain the dominant bottleneck,
yielding the largest gaps to Clean accuracy and the highest ASR even with a
large number of agents. And (2) Collaboration reliably improves accuracy as the
number of agents, n, increases, with the largest gains from one to five agents
and diminishing returns beyond 10 agents. However, the adversarial robustness
gap persists regardless of the agent count.

</details>


### [10] [Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation](https://arxiv.org/abs/2511.07382)
*K M Nafi Asib,Sourav Saha,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 提出了一种结合指令提示和测试驱动反馈引导迭代精炼的方法，使用微调的Qwen2.5-14B模型从孟加拉语指令生成代码，通过三次评估迭代优化失败输出，在BLP Workshop代码生成任务中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如孟加拉语）在代码生成任务中代表性不足的问题，由于缺乏指令到代码的数据集和评估基准。

Method: 使用微调的Qwen2.5-14B模型，结合指令提示和测试驱动的反馈引导迭代精炼过程，通过三次评估迭代优化代码生成结果。

Result: 在BLP Workshop共享任务中获得第二名，Pass@1得分为0.934。

Conclusion: 该方法有效提升了孟加拉语代码生成性能，但分析显示在孟加拉语指令理解和Python代码生成方面仍存在挑战，需要针对低资源语言的专门方法。

Abstract: Large Language Models (LLMs) have advanced the automated generation of code
from natural language prompts. However, low-resource languages (LRLs) like
Bangla remain underrepresented due to the limited availability of
instruction-to-code datasets and evaluation benchmarks. To address this, the
BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generation
in Bangla". In this work, we propose a method that combines instruction
prompting with a test-driven, feedback-guided iterative refinement process
using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla
instructions, tests it against unit tests, and iteratively refines any failing
outputs through three evaluation passes, using test feedback to guide each
step. This approach helped our team "Retriv" to secure 2nd place in the shared
task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla
instruction understanding and Python code generation, emphasizing the need for
targeted methods in LRLs. We made experimental scripts publicly available for
the community.

</details>


### [11] [Discourse Graph Guided Document Translation with Large Language Models](https://arxiv.org/abs/2511.07230)
*Viet-Thanh Pham,Minghan Wang,Hao-Han Liao,Thuy-Trang Vu*

Main category: cs.CL

TL;DR: TransGraph是一个基于话语图的文档翻译框架，通过显式建模文档块间关系来提升翻译质量和术语一致性，同时降低计算开销


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在文档翻译中难以捕捉长距离依赖和保持话语连贯性的问题，同时避免现有多智能体翻译系统的高计算成本和内存检索敏感性问题

Method: 构建结构化话语图来显式建模文档块间关系，每个翻译片段仅基于相关图邻域进行条件翻译，而非依赖顺序或完整上下文

Result: 在三个文档级机器翻译基准测试中，涵盖六种语言和多个领域，TransGraph在翻译质量和术语一致性方面持续超越强基线，同时显著降低token开销

Conclusion: TransGraph通过话语图引导的翻译框架有效解决了文档翻译中的长距离依赖和连贯性问题，在保持高质量翻译的同时显著降低了计算成本

Abstract: Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.

</details>


### [12] [RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments](https://arxiv.org/abs/2511.07317)
*Zhiyuan Zeng,Hamish Ivison,Yiping Wang,Lifan Yuan,Shuyue Stella Li,Zhuorui Ye,Siting Li,Jacqueline He,Runlong Zhou,Tong Chen,Chenyang Zhao,Yulia Tsvetkov,Simon Shaolei Du,Natasha Jaques,Hao Peng,Pang Wei Koh,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 提出了RLVE方法，使用可验证环境来扩展语言模型的强化学习，通过动态调整问题难度分布来避免学习信号消失问题。


<details>
  <summary>Details</summary>
Motivation: 解决静态数据分布在强化学习中导致的问题难度不匹配，当问题过于简单或困难时学习信号会消失。

Method: 创建RLVE-Gym套件，包含400个可验证环境，这些环境能够根据策略模型能力动态调整问题难度分布。

Result: 在6个推理基准测试中平均绝对提升3.37%，相比原始RL训练仅获得0.49%的提升，且计算量减少3倍以上。

Conclusion: 环境扩展（增加训练环境数量）能够持续提升通用推理能力，RLVE方法在强化学习扩展方面具有显著优势。

Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [13] [「注意力机制+<em class="highlight">强化学习</em>」重磅突破！荣登Science顶级子刊！](http://mp.weixin.qq.com/s?__biz=MzU2NTYzNzc3NA==&mid=2247505512&idx=1&sn=ef660a665a27d93ce91eb89c4f1314be&chksm=fd2bd87a93df9b8617f0c718f133d9ed6359a1b6c166318b59014d0471831652184128d53e46#rd)
*AI算法科研paper*

Main category: wechat.article

TL;DR: 提出两阶段强化学习训练 pipeline，先在基础地形上初始化地图编码学习，再引入复杂地形与不确定性微调，兼顾泛化能力与鲁棒性。构建端到端的整体控制框架，无需依赖模型预测控制等上层规划模块，直接将感知信息映射为关


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 提出两阶段强化学习训练 pipeline，先在基础地形上初始化地图编码学习，再引入复杂地形与不确定性微调，兼顾泛化能力与鲁棒性。构建端到端的整体控制框架，无需依赖模型预测控制等上层规划模块，直接将感知信息映射为关

</details>


### [14] [投资<em class="highlight">Agentic</em> AI是面向未来供应链的战略举措](http://mp.weixin.qq.com/s?__biz=MzI0NDE5MTA2Mw==&mid=2653223060&idx=3&sn=85419a59ffe0d7dcead48664d0e31b47&chksm=f33956575397cb4660b82dc22e2244b7053a9a32a04bc70eb3c1461e09db21f2f00a3873945b#rd)
*侠说*

Main category: wechat.article

TL;DR: agentic ai： 从自动化到自主的旅程。罗戈研究 从数字化雄心到自主化现实。从传统聊天机器人和 rpa 到自主代 机器人流程自动化（rpa）到自主代理流程自动化（apa） 理--迈向自主的旅程。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai： 从自动化到自主的旅程。罗戈研究 从数字化雄心到自主化现实。从传统聊天机器人和 rpa 到自主代 机器人流程自动化（rpa）到自主代理流程自动化（apa） 理--迈向自主的旅程。

</details>


### [15] [实战·<em class="highlight">Agentic</em> 上下文工程（下）：实现一个可自我学习与进化的<em class="highlight">智能体</em>原型](http://mp.weixin.qq.com/s?__biz=MzI2MTA2MTI5OQ==&mid=2647536042&idx=1&sn=820e3a4b2007b32a8f1862ea382fda6c&chksm=f38a0d502cecbd124737396a0f1a1a2a3aa40f2c832e1146ca1b76c474542b9f317df7c6375a#rd)
*AI开发日记*

Main category: wechat.article

TL;DR: 参考 ace 论文提出的结构，我们将其应用于一个基于 react 范式 的智能体中。这个智能体具备自我“反思—学习—成长”的能力：playbook 问题输入 策略手册 检索策略 reactagent 推理与行动 执行结果 evaluator 推理轨迹 更新策略 答案


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 参考 ace 论文提出的结构，我们将其应用于一个基于 react 范式 的智能体中。这个智能体具备自我“反思—学习—成长”的能力：playbook 问题输入 策略手册 检索策略 reactagent 推理与行动 执行结果 evaluator 推理轨迹 更新策略 答案

</details>


### [16] [就在昨天！！ Google新发布54页Agent开发指南：五层架构详解<em class="highlight">Agentic</em> AI](http://mp.weixin.qq.com/s?__biz=MzI0MjM0MTg4MQ==&mid=2247485680&idx=1&sn=c50df34eb76f42bba8ca08352d9c8ff7&chksm=e83e93686b062060851cd0cca975fbec314b7e6a23ee976cd40bd6ae2ff21180fd8d1c262ef6#rd)
*懂点AI的海文*

Main category: wechat.article

TL;DR: google 新发布54页 agent开发指南， 五层架构详解 agentic ai google introduction to agents google introduction to agents and agent architectures acknowledgements content contributors enrique chan mike clark derek egan ...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: google 新发布54页 agent开发指南， 五层架构详解 agentic ai google introduction to agents google introduction to agents and agent architectures acknowledgements content contributors enrique chan mike clark derek egan anant nawalgaria kanchana patlolla julia wiesinger curators and editor

</details>


### [17] [什么是 <em class="highlight">Agentic</em> AI？吴恩达这套AI提效思路太强了](http://mp.weixin.qq.com/s?__biz=MzkxMTI3Mzc2Ng==&mid=2247491193&idx=1&sn=68f7e76a3862f205ec4ae0c274ef9309&chksm=c0be08f96be5b169132aee591d1afa70b9ee8da64494ce92f350033a3a03fb9d46f48b3e673a#rd)
*三流数据*

Main category: wechat.article

TL;DR: 用户体验 len（text） 智能体的另一半轴一：客观 vs. 主观客观 （Objective）：答案是非黑即白的。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 用户体验 len（text） 智能体的另一半轴一：客观 vs. 主观客观 （Objective）：答案是非黑即白的。

</details>


### [18] [搜索框里装了个“大脑”!<em class="highlight">Agentic</em> Search不仅能搜,还能规划执行反思,彻底颠覆你的工作流](http://mp.weixin.qq.com/s?__biz=MzIyODU3NTkwNA==&mid=2247490928&idx=2&sn=c2fbdf41a811042a5146bd0f03904012&chksm=e9d8126f0c93992c3672ad56b27d2e6714236fe5c3c5dc5de87e1f3a924a378eb631cd3c17cc#rd)
*AI资讯*

Main category: wechat.article

TL;DR: 要理解Agentic Search的技术原理，我们需要深入了解其底层架构。根据学术研究，现代的Agentic Search系统主要基于多Agent协同架构和增强RAG框架构建。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 要理解Agentic Search的技术原理，我们需要深入了解其底层架构。根据学术研究，现代的Agentic Search系统主要基于多Agent协同架构和增强RAG框架构建。

</details>


### [19] [Google发布54页Agent开发指南：五层架构详解<em class="highlight">Agentic</em> AI](http://mp.weixin.qq.com/s?__biz=MzUyNDk2MTcyOQ==&mid=2247493885&idx=2&sn=2f3cdb4e069e48826eedaf6cc8012e38&chksm=fb6b533fa132e35be046cb99dc1f8d72ce794b2f2dbaae63d0ac325abd6536a39f156dfcaf0b#rd)
*Agent时代*

Main category: wechat.article

TL;DR: 这是Agentic AI的终极形态，目前仍处于研究阶段。OpenAI的code interpreter某些场景下展现了Level 4的雏形——当遇到没有现成工具的任务时，它会自己写代码创造工具。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这是Agentic AI的终极形态，目前仍处于研究阶段。OpenAI的code interpreter某些场景下展现了Level 4的雏形——当遇到没有现成工具的任务时，它会自己写代码创造工具。

</details>


### [20] [吴恩达<em class="highlight">Agentic</em> AI（一）<em class="highlight">智能体</em>化工作流](http://mp.weixin.qq.com/s?__biz=MzI5MjQzOTY3OA==&mid=2247488982&idx=1&sn=3734562dd398c524563831ad81837840&chksm=ed3754398bdf14821ab01313fcf0c3346e0891883d72fd15cf0f94ebd1df0c3b42e675727594#rd)
*精神抖擞王大鹏*

Main category: wechat.article

TL;DR: 那什么是Agentic工作流？它不是一个新技术，而是一种新的思维方式——让AI模仿人类真实的工作流程。还是写论文这个例子，人类会怎么做？思考主题 → 列出大纲 → 确定需要研究的方向


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 那什么是Agentic工作流？它不是一个新技术，而是一种新的思维方式——让AI模仿人类真实的工作流程。还是写论文这个例子，人类会怎么做？思考主题 → 列出大纲 → 确定需要研究的方向

</details>


### [21] [程序员的双十一：盘点支持订阅模式的国产 AI <em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzA3NjY2NzY1MA==&mid=2649741311&idx=1&sn=cbb4c08746f33a6d851153c8d0a6d855&chksm=86a2590231882c0ec2bc0bc85795ca86f7e4fa48071c95037cc86b78dc651436bafe9d26d506#rd)
*Feisky*

Main category: wechat.article

TL;DR: 程序员的 双十一：盘点 支持订阅模式的 国产ai大模型。tuesday。首月 方舟 coding plan lite 模型 doubao-seed-code 强力驱动 工具 适配claude， code等主流编程工具， 用量 用量达claude pro的3倍 ￥9.90/月。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 程序员的 双十一：盘点 支持订阅模式的 国产ai大模型。tuesday。首月 方舟 coding plan lite 模型 doubao-seed-code 强力驱动 工具 适配claude， code等主流编程工具， 用量 用量达claude pro的3倍 ￥9.90/月。

</details>


### [22] [必知！AI <em class="highlight">大模型</em>应用架构图（全）](http://mp.weixin.qq.com/s?__biz=Mzk0MDY2ODM3NQ==&mid=2247487717&idx=1&sn=073aaf7394f79b8c5c2ef9a54d48ac18&chksm=c3242ac73a487ec9afe64a0b7e23e02aee96b0419dd5ac05792237d3ff29213a7e1a1cfe226e#rd)
*AI大模型前沿*

Main category: wechat.article

TL;DR: ai 大模型应用企业级开发知识体系 大模型 大模型图片文字识别应 大模型语音识别应用案 大模型设计平台应用案 大模型数字人应用案 多模态应用案例 用案例：ocr 等。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ai 大模型应用企业级开发知识体系 大模型 大模型图片文字识别应 大模型语音识别应用案 大模型设计平台应用案 大模型数字人应用案 多模态应用案例 用案例：ocr 等。

</details>


### [23] [让智能体开发如“拼积木”般便利！合合信息携手火山引擎共探<em class="highlight">大模型</em>落地新范式](http://mp.weixin.qq.com/s?__biz=MzAxMzg0NjY2NA==&mid=2247493674&idx=1&sn=829ab124c9652f835848bf97a88be79c&chksm=9aba4da531456e1d420c7a07e94f2011d45141f834fae5bb410f89432899ffc6d7ff5be9f090#rd)
*合合信息*

Main category: wechat.article

TL;DR: 中国信通院7月数据显示，国内已发布大模型超过1500个。如何打通诸多大模型落地的“最后一公里”，让技术从“实验室”走向“生产线”，成为全行业共同探索的课题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 中国信通院7月数据显示，国内已发布大模型超过1500个。如何打通诸多大模型落地的“最后一公里”，让技术从“实验室”走向“生产线”，成为全行业共同探索的课题。

</details>


### [24] [网络安全<em class="highlight">大模型</em>的路线和方向](http://mp.weixin.qq.com/s?__biz=MzUyOTkwNTQ5Mg==&mid=2247489472&idx=1&sn=fa483df0415d4f7060c3dc7d65748565&chksm=fb03f231c834ed580541403341fc9c1ae7022009af30e144db1bb737182fae9f2c01192a0166#rd)
*青藤智库*

Main category: wechat.article

TL;DR: 使用大模型进行攻击方面的应用是有门槛的，但是现在开源大模型的普遍使用，让大模型进行网络攻击是有了更好的基座，可以使用SFT技术，RL技术，模型编辑（model editing）技术，可以利用这些开源大模型构造出一个更偏向于网


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 使用大模型进行攻击方面的应用是有门槛的，但是现在开源大模型的普遍使用，让大模型进行网络攻击是有了更好的基座，可以使用SFT技术，RL技术，模型编辑（model editing）技术，可以利用这些开源大模型构造出一个更偏向于网

</details>


### [25] [期刊文章 | 张汉宁等：生成式人工智能<em class="highlight">大模型</em>伪造内容检测技术研究](http://mp.weixin.qq.com/s?__biz=MzU4OTU2MjgzMg==&mid=2247531363&idx=1&sn=7c8c33d452c0e87b2677e56f78aab63e&chksm=fc3638c1024bb34462acb6f341f183442693c42119ef54136074c0da94cc398bc2be4c7177ff#rd)
*中国联通研究院*

Main category: wechat.article

TL;DR: 1.1 生成式AI大模型生成式AI技术，特别是基于深度学习的生成模型，已经在文本、图像、音频和视频等多个领域取得了突破性进展。从早期的GAN（Generative Adversarial Networks）[5]、VAE（Variational Auto-Encoders），到近期的Diffusion Models[6]


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1.1 生成式AI大模型生成式AI技术，特别是基于深度学习的生成模型，已经在文本、图像、音频和视频等多个领域取得了突破性进展。从早期的GAN（Generative Adversarial Networks）[5]、VAE（Variational Auto-Encoders），到近期的Diffusion Models[6]

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Evidence-Bound Autonomous Research (EviBound): A Governance Framework for Eliminating False Claims](https://arxiv.org/abs/2511.05524)
*Ruiying Chen*

Main category: cs.AI

TL;DR: EviBound是一个证据绑定的执行框架，通过双重治理门消除LLM自主研究代理的错误声明，要求机器可检查的证据来确保研究完整性。


<details>
  <summary>Details</summary>
Motivation: LLM自主研究代理经常报告虚假声明，即使任务实际上未完成、存在矛盾指标或执行失败。需要一种机制来确保研究声明的真实性。

Method: 采用双重治理门：预执行批准门验证接受标准模式，后执行验证门通过MLflow API查询验证工件和指标。声明只有在有可查询运行ID、必需工件和FINISHED状态时才能传播。

Result: 在8个基准任务评估中，基线A（仅提示级别）产生100%幻觉，基线B（仅验证）减少到25%幻觉，而EviBound（双重门）实现0%幻觉，仅增加约8.3%执行开销。

Conclusion: 研究完整性是架构属性，通过治理门而非模型规模实现。证据绑定执行框架能有效消除虚假声明。

Abstract: LLM-based autonomous research agents report false claims: tasks marked "complete" despite missing artifacts, contradictory metrics, or failed executions. EviBound is an evidence-bound execution framework that eliminates false claims through dual governance gates requiring machine-checkable evidence.
  Two complementary gates enforce evidence requirements. The pre-execution Approval Gate validates acceptance criteria schemas before code runs, catching structural violations proactively. The post-execution Verification Gate validates artifacts via MLflow API queries (with recursive path checking) and optionally validates metrics when specified by acceptance criteria. Claims propagate only when backed by a queryable run ID, required artifacts, and FINISHED status. Bounded, confidence-gated retries (typically 1-2 attempts) recover from transient failures without unbounded loops.
  The framework was evaluated on 8 benchmark tasks spanning infrastructure validation, ML capabilities, and governance stress tests. Baseline A (Prompt-Level Only) yields 100% hallucination (8/8 claimed, 0/8 verified). Baseline B (Verification-Only) reduces hallucination to 25% (2/8 fail verification). EviBound (Dual Gates) achieves 0% hallucination: 7/8 tasks verified and 1 task correctly blocked at the approval gate, all with only approximately 8.3% execution overhead.
  This package includes execution trajectories, MLflow run IDs for all verified tasks, and a 4-step verification protocol. Research integrity is an architectural property, achieved through governance gates rather than emergent from model scale.

</details>


### [27] [CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization](https://arxiv.org/abs/2511.05747)
*Ziqian Bi,Kaijie Chen,Tianyang Wang,Junfeng Hao,Xinyuan Song*

Main category: cs.AI

TL;DR: 提出自适应推理摘要框架，通过语义分割、重要性评分和动态压缩来压缩推理轨迹，在保持关键推理步骤的同时显著减少token使用量，实现跨模型的高效CoT推理迁移。


<details>
  <summary>Details</summary>
Motivation: CoT推理虽然增强了LLMs的问题解决能力，但带来了显著的推理开销，限制了在资源受限环境中的部署。需要研究如何在保持推理质量的同时减少计算成本。

Method: 采用自适应推理摘要框架，包括语义分割与重要性评分、预算感知动态压缩、连贯性重建，以及基于高斯过程的贝叶斯优化模块来降低评估成本。

Result: 在7,501个医学考试问题上，相比截断方法在相同token预算下准确率提高达40%；在8个LLMs的64个模型对上验证了强跨模型可迁移性；评估成本降低84%。

Conclusion: 推理摘要为实现高效CoT迁移提供了实用路径，能够在严格计算约束下实现高级推理能力。

Abstract: Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of large language models (LLMs) but leads to substantial inference overhead, limiting deployment in resource-constrained settings. This paper investigates efficient CoT transfer across models of different scales and architectures through an adaptive reasoning summarization framework. The proposed method compresses reasoning traces via semantic segmentation with importance scoring, budget-aware dynamic compression, and coherence reconstruction, preserving critical reasoning steps while significantly reducing token usage. Experiments on 7{,}501 medical examination questions across 10 specialties show up to 40% higher accuracy than truncation under the same token budgets. Evaluations on 64 model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian Process-based Bayesian optimization module reduces evaluation cost by 84% and reveals a power-law relationship between model size and cross-domain robustness. These results demonstrate that reasoning summarization provides a practical path toward efficient CoT transfer, enabling advanced reasoning under tight computational constraints. Code will be released upon publication.

</details>


### [28] [Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs](https://arxiv.org/abs/2511.05766)
*Felipe Valencia-Clavijo*

Main category: cs.AI

TL;DR: 该论文通过概率分析和归因方法研究LLMs中的锚定偏差，发现锚点会改变整个输出分布，不同规模模型敏感性不同，为评估LLMs认知偏差提供了可复现框架。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs中观察到的认知偏差是表面模仿还是深层概率变化，以锚定偏差为关键测试案例，探索内部机制和归因贡献。

Method: 使用对数概率行为分析、精确Shapley值归因和统一的锚定偏差敏感度评分，在六个开源模型上进行测试。

Result: Gemma-2B、Phi-2和Llama-2-7B表现出稳健的锚定效应，锚点影响重新加权；较小模型如GPT-2、Falcon-RW-1B和GPT-Neo-125M敏感性变化较大，表明规模可能调节敏感性。

Conclusion: LLMs中的锚定偏差是稳健、可测量和可解释的，但在应用领域存在风险，该框架为评估其他认知偏差提供了桥梁。

Abstract: Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.

</details>


### [29] [Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection](https://arxiv.org/abs/2511.05854)
*Zepeng Bao,Shen Zhou,Qiankun Pi,Jianhao Chen,Mayi Xu,Ming Zhong,Yuanyuan Zhu,Tieyun Qian*

Main category: cs.AI

TL;DR: 提出了LEAP框架来解决LLM幻觉检测中的策略适应性问题，通过动态学习循环和主动修正机制，使高效的学生模型具备动态规划和策略调整能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强的幻觉检测方法使用预定义的固定验证策略，在动态变化的环境中缺乏适应性，导致检测失败。而使用GPT-4等闭源模型作为检测器成本过高。

Method: 将幻觉检测问题建模为动态策略学习问题，首先使用教师模型在动态学习循环中生成轨迹并根据执行失败调整策略，然后通过智能体调优将这种动态规划能力蒸馏到高效的学生模型中，最后在执行时采用主动修正机制。

Result: 在三个具有挑战性的基准测试中，LEAP调优的模型优于现有的最先进方法。

Conclusion: LEAP框架成功解决了幻觉检测中的策略适应性问题，使高效的学生模型具备了动态学习和主动修正能力。

Abstract: Hallucination in large language models (LLMs) remains a critical barrier to their safe deployment. Existing tool-augmented hallucination detection methods require pre-defined fixed verification strategies, which are crucial to the quality and effectiveness of tool calls. Some methods directly employ powerful closed-source LLMs such as GPT-4 as detectors, which are effective but too costly. To mitigate the cost issue, some methods adopt the teacher-student architecture and finetune open-source small models as detectors via agent tuning. However, these methods are limited by fixed strategies. When faced with a dynamically changing execution environment, they may lack adaptability and inappropriately call tools, ultimately leading to detection failure. To address the problem of insufficient strategy adaptability, we propose the innovative ``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an efficient student model with the dynamic learning and proactive correction capabilities of the teacher model. Specifically, our method formulates the hallucination detection problem as a dynamic strategy learning problem. We first employ a teacher model to generate trajectories within the dynamic learning loop and dynamically adjust the strategy based on execution failures. We then distill this dynamic planning capability into an efficient student model via agent tuning. Finally, during strategy execution, the student model adopts a proactive correction mechanism, enabling it to propose, review, and optimize its own verification strategies before execution. We demonstrate through experiments on three challenging benchmarks that our LEAP-tuned model outperforms existing state-of-the-art methods.

</details>


### [30] [An Empirical Study of Reasoning Steps in Thinking Code LLMs](https://arxiv.org/abs/2511.05874)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.AI

TL;DR: 对6种思维LLM在代码生成任务中的推理过程进行实证研究，发现推理链质量受任务复杂度影响，完整性是主要失败模式，但模型能保持逻辑结构一致性和自我纠错能力。


<details>
  <summary>Details</summary>
Motivation: 探索思维LLM在代码生成中推理链的质量，填补该领域的研究空白，评估推理过程的透明度、可解释性和准确性。

Method: 使用BigCodeBench的100个代码生成任务评估6种思维LLM，通过步骤计数和冗长度量化推理链结构，进行步骤预算调整实验，并开展21人参与的人工评估。

Result: 任务复杂度显著影响推理质量，困难问题更容易出现不完整性；步骤干预可提高特定模型/任务的解决率；模型能保持逻辑结构一致性并自我纠错。

Conclusion: 当前思维LLM在软件工程中具有保持逻辑一致性和自我纠错的优势，但推理完整性仍是主要挑战，特别是在复杂任务中。

Abstract: Thinking Large Language Models (LLMs) generate explicit intermediate reasoning traces before final answers, potentially improving transparency, interpretability, and solution accuracy for code generation. However, the quality of these reasoning chains remains underexplored. We present a comprehensive empirical study examining the reasoning process and quality of thinking LLMs for code generation. We evaluate six state-of-the-art reasoning LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking, Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code generation tasks of varying difficulty from BigCodeBench. We quantify reasoning-chain structure through step counts and verbosity, conduct controlled step-budget adjustments, and perform a 21-participant human evaluation across three dimensions: efficiency, logical correctness, and completeness. Our step-count interventions reveal that targeted step increases can improve resolution rates for certain models/tasks, while modest reductions often preserve success on standard tasks, rarely on hard ones. Through systematic analysis, we develop a reasoning-problematic taxonomy, identifying completeness as the dominant failure mode. Task complexity significantly impacts reasoning quality; hard problems are substantially more prone to incompleteness than standard tasks. Our stability analysis demonstrates that thinking LLMs maintain consistent logical structures across computational effort levels and can self-correct previous errors. This study provides new insights into the strengths and limitations of current thinking LLMs in software engineering.

</details>


### [31] [Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement](https://arxiv.org/abs/2511.05931)
*Hiroaki Hayashi,Bo Pang,Wenting Zhao,Ye Liu,Akash Gokul,Srijan Bansal,Caiming Xiong,Semih Yavuz,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出了SAGE框架，使LLM代理能够从自身任务执行中学习，通过自我抽象来改进行为，在SWE-Bench基准测试中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理通常在静态执行框架中运行，缺乏从自身经验中学习和自我改进的机制，导致性能受限于初始框架设计和基础LLM能力。

Method: SAGE框架：代理从初始执行中归纳出简洁的计划抽象，提炼关键步骤、依赖关系和约束，然后将学习到的抽象作为上下文指导反馈，改进代理策略并支持更有结构的后续执行。

Result: SAGE在不同LLM骨干和代理架构上带来一致的性能提升，与GPT-5（高）骨干配对时相对Mini-SWE-Agent基线提升7.2%，在SWE-Bench Verified基准上分别达到73.2%和74%的Pass@1解决率。

Conclusion: SAGE框架通过自我抽象学习机制有效提升了LLM代理在软件工程任务中的性能，证明了从经验中学习的重要性。

Abstract: Large language model (LLM) based agents are increasingly used to tackle software engineering tasks that require multi-step reasoning and code modification, demonstrating promising yet limited performance. However, most existing LLM agents typically operate within static execution frameworks, lacking a principled mechanism to learn and self-improve from their own experience and past rollouts. As a result, their performance remains bounded by the initial framework design and the underlying LLM's capabilities. We propose Self-Abstraction from Grounded Experience (SAGE), a framework that enables agents to learn from their own task executions and refine their behavior through self-abstraction. After an initial rollout, the agent induces a concise plan abstraction from its grounded experience, distilling key steps, dependencies, and constraints. This learned abstraction is then fed back as contextual guidance, refining the agent's policy and supporting more structured, informed subsequent executions. Empirically, SAGE delivers consistent performance gains across diverse LLM backbones and agent architectures. Notably, it yields a 7.2% relative performance improvement over the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone. SAGE further achieves strong overall performance on SWE-Bench Verified benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent and OpenHands CodeAct agent framework, respectively.

</details>


### [32] [Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling](https://arxiv.org/abs/2511.05951)
*Qi Wang,Hongzhi Zhang,Jia Fu,Kai Fu,Yahui Liu,Tinghai Zhang,Chenxi Sun,Gangwei Jiang,Jingyi Tang,Xingguang Ji,Yang Yue,Jingyuan Zhang,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.AI

TL;DR: 提出了一个完全开源的代理模型训练流程Klear-Qwen3-AgentForge，从Qwen3-8B基础模型开始，通过监督微调和多轮强化学习，在工具使用和编程任务上达到同类尺寸模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强大的代理模型不断涌现，但缺乏关键的训练后细节阻碍了开源社区开发强力的对应模型。

Method: 设计了包含合成数据的有效监督微调，然后进行多轮强化学习，以解锁多种代理任务的潜力。

Result: Klear-Qwen3-AgentForge-8B在类似尺寸的LLM中实现了最先进的性能，并与显著更大的模型保持竞争力。

Conclusion: 该研究提供了一个全面且完全开源的代理模型训练流程，证明了从基础模型训练高性能代理模型的可行性。

Abstract: Despite the proliferation of powerful agentic models, the lack of critical post-training details hinders the development of strong counterparts in the open-source community. In this study, we present a comprehensive and fully open-source pipeline for training a high-performance agentic model for interacting with external tools and environments, named Klear-Qwen3-AgentForge, starting from the Qwen3-8B base model. We design effective supervised fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement learning (RL) to unlock the potential for multiple diverse agentic tasks. We perform exclusive experiments on various agentic benchmarks in both tool use and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art performance among LLMs of similar size and remains competitive with significantly larger models.

</details>


### [33] [An Epistemic Perspective on Agent Awareness](https://arxiv.org/abs/2511.05977)
*Pavel Naumov,Alexandra Pavlova*

Main category: cs.AI

TL;DR: 该论文将智能体意识视为一种知识形式，打破了现有文献传统，区分了这种知识的de re和de dicto形式，并引入了两种模态来捕捉这些形式，使用2D语义学形式化其含义。


<details>
  <summary>Details</summary>
Motivation: 打破现有文献中关于智能体意识的传统观点，将意识视为一种知识形式，并区分其不同表现形式。

Method: 引入两种模态来捕捉de re和de dicto形式的意识知识，使用2D语义学形式化其含义，构建描述这些模态与标准"事实知识"模态之间相互作用的逻辑系统。

Result: 提出了一个描述所提出的两种模态与标准"事实知识"模态之间相互作用的健全且完备的逻辑系统。

Conclusion: 成功建立了将智能体意识作为知识形式处理的逻辑框架，为理解意识的不同形式提供了形式化工具。

Abstract: The paper proposes to treat agent awareness as a form of knowledge, breaking the tradition in the existing literature on awareness. It distinguishes the de re and de dicto forms of such knowledge. The work introduces two modalities capturing these forms and formally specifies their meaning using a version of 2D-semantics. The main technical result is a sound and complete logical system describing the interplay between the two proposed modalities and the standard "knowledge of the fact" modality.

</details>


### [34] [Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs](https://arxiv.org/abs/2511.06134)
*Wei Yang,Jiacheng Pang,Shixuan Li,Paul Bogdan,Stephen Tu,Jesse Thomason*

Main category: cs.AI

TL;DR: 提出了Maestro框架，通过角色编排解决多智能体系统中的认知张力问题，结合探索和综合两个阶段，并使用CLPO强化学习目标实现清晰的信用分配。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在平衡解决方案空间的广泛探索和原则性综合方面存在困难，容易导致过早共识、错误传播和信用分配问题。

Method: Maestro框架：使用并行执行智能体进行多样化探索，专用中央智能体进行收敛性综合；CLPO方法：结合决策导向的策略梯度和基于理由的列表排序损失。

Result: 在数学推理和通用问题解决基准测试中，Maestro+CLPO相比现有最先进多智能体方法平均绝对准确率提升6%，最高提升10%。

Conclusion: Maestro框架通过结构化解耦探索和综合认知模式，结合CLPO的清晰信用分配机制，有效提升了多智能体系统的性能。

Abstract: Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we propose the Multi-Agent Exploration-Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6% on average and up to 10% at best.

</details>


### [35] [CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference](https://arxiv.org/abs/2511.06175)
*Kaijie Xu,Fandi Meng,Clark Verbrugge,Simon Lucas*

Main category: cs.AI

TL;DR: 提出了CSP4SDG框架，这是一个基于约束满足的概率推理方法，用于社交推理游戏中的隐藏角色推断，在三个公开数据集上表现优于基于LLM的基线方法。


<details>
  <summary>Details</summary>
Motivation: 在社交推理游戏中，玩家隐藏身份并故意误导他人，使得隐藏角色推断成为核心且具有挑战性的任务。准确的角色识别是智能体信念状态的基础，对AI和人类表现都至关重要。

Method: 将游戏事件和对话映射到四个语言无关的约束类别：证据、现象、断言和假设。使用硬约束修剪不可能的角色分配，加权软约束对剩余分配进行评分，信息增益权重将每个假设与其在熵减少下的期望值联系起来。

Result: 在三个公开数据集上的实验表明，CSP4SDG在所有推理场景中都优于基于LLM的基线方法，并且当作为辅助"推理工具"提供给LLM时能够提升LLM的性能。

Conclusion: 研究表明，基于信息论的原则性概率推理是社交推理游戏中重型神经模型的可扩展替代或补充方案。

Abstract: In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players conceal their identities and deliberately mislead others, making hidden-role inference a central and demanding task. Accurate role identification, which forms the basis of an agent's belief state, is therefore the keystone for both human and AI performance. We introduce CSP4SDG, a probabilistic, constraint-satisfaction framework that analyses gameplay objectively. Game events and dialogue are mapped to four linguistically-agnostic constraint classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune impossible role assignments, while weighted soft constraints score the remainder; information-gain weighting links each hypothesis to its expected value under entropy reduction, and a simple closed-form scoring rule guarantees that truthful assertions converge to classical hard logic with minimum error. The resulting posterior over roles is fully interpretable and updates in real time. Experiments on three public datasets show that CSP4SDG (i) outperforms LLM-based baselines in every inference scenario, and (ii) boosts LLMs when supplied as an auxiliary "reasoning tool." Our study validates that principled probabilistic reasoning with information theory is a scalable alternative-or complement-to heavy-weight neural models for SDGs.

</details>


### [36] [Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads](https://arxiv.org/abs/2511.06209)
*Jingwei Ni,Ekaterina Fadeeva,Tianyi Wu,Mubashara Akhtar,Jiaheng Zhang,Elliott Ash,Markus Leippold,Timothy Baldwin,See-Kiong Ng,Artem Shelmanov,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: 提出了一种基于数据驱动不确定性评分的轻量级推理步骤验证方法，通过训练transformer不确定性量化头来利用冻结LLM的内部状态估计推理步骤的不确定性，性能可媲美或超越大得多的过程奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有推理验证方法如过程奖励模型计算成本高、领域受限或需要大规模人工标注，需要更轻量、通用的解决方案。

Method: 训练transformer不确定性量化头，利用冻结LLM的内部状态估计推理步骤的不确定性，标签由更大LLM或原始模型自监督生成。

Result: 在数学、规划和常识问答等多个领域，UHeads性能匹配或超越比其大810倍的过程奖励模型，且参数量小于1000万。

Conclusion: LLM内部状态编码了其不确定性，可作为可靠的推理验证信号，为可扩展和泛化的自省LLM提供了有前景的方向。

Abstract: Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.

</details>


### [37] [Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B](https://arxiv.org/abs/2511.06221)
*Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin Zhang*

Main category: cs.AI

TL;DR: VibeThinker-1.5B是一个15亿参数的密集模型，通过Spectrum-to-Signal Principle (SSP)框架开发，挑战了模型参数规模决定推理能力的共识。该模型仅用7800美元训练成本，在多个数学基准测试中超越了DeepSeek R1等大模型。


<details>
  <summary>Details</summary>
Motivation: 挑战当前认为小模型缺乏强大推理能力的共识，证明通过优化训练方法而非单纯扩大参数规模，小模型也能达到甚至超越大模型的推理性能。

Method: 采用Spectrum-to-Signal Principle (SSP)框架：1）两阶段多样性探索蒸馏(SFT)生成广泛解决方案谱；2）最大熵引导策略优化(RL)放大正确信号。

Result: 在AIME24(80.3 vs 79.8)、AIME25(74.4 vs 70.0)、HMMT25(50.4 vs 41.7)三个数学基准上超越400倍大的DeepSeek R1；在LiveCodeBench V6上得分51.1，优于Magistral Medium的50.3。

Conclusion: 小模型通过优化训练方法可以实现与大模型相当的推理能力，大幅降低训练和推理成本，使先进AI研究更加民主化。

Abstract: Challenging the prevailing consensus that small models inherently lack robust
reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense
model developed via our Spectrum-to-Signal Principle (SSP). This challenges the
prevailing approach of scaling model parameters to enhance capabilities, as
seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework
first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a
broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)
to amplify the correct signal. With a total training cost of only $7,800,
VibeThinker-1.5B demonstrates superior reasoning capabilities compared to
closed-source models like Magistral Medium and Claude Opus 4, and performs on
par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses
the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),
AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial
improvement over its base model (6.7, 4.3, and 0.6, respectively). On
LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its
base model's 0.0. These findings demonstrate that small models can achieve
reasoning capabilities comparable to large models, drastically reducing
training and inference costs and thereby democratizing advanced AI research.

</details>


### [38] [GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening](https://arxiv.org/abs/2511.06262)
*Siming Zhao,Qi Li*

Main category: cs.AI

TL;DR: GAIA是一个面向B2B谈判和筛选的治理优先LLM-人类代理框架，通过信息门控进展、双重反馈集成和授权边界三个机制，确保AI委托的安全性、效率和可审计性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM谈判研究主要关注自主代理间的讨价还价，忽略了实际部署中的治理需求，如分阶段信息收集、明确授权边界和系统反馈集成，特别是在高风险B2B环境中。

Method: GAIA框架定义三个核心角色（委托人、代理、对手方）和可选批评者，采用信息门控进展分离筛选与谈判，双重反馈集成结合AI批评与人工修正，授权边界提供明确升级路径。

Result: 提出了包含四个安全不变量的正式治理框架，实现了任务完整性跟踪和明确状态转换，通过并行学习通道整合批评建议与人工监督。

Conclusion: GAIA为安全、高效和可审计的AI委托提供了可复现的规范，可应用于采购、房地产和人员配置等工作流程，弥合了理论与实践之间的差距。

Abstract: Organizations are increasingly exploring delegation of screening and negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is constrained by governance: preventing unauthorized commitments, ensuring sufficient information before bargaining, and maintaining effective human oversight and auditability. Prior work on large language model negotiation largely emphasizes autonomous bargaining between agents and omits practical needs such as staged information gathering, explicit authorization boundaries, and systematic feedback integration. We propose GAIA, a governance-first framework for LLM-human agency in B2B negotiation and screening. GAIA defines three essential roles - Principal (human), Delegate (LLM agent), and Counterparty - with an optional Critic to enhance performance, and organizes interactions through three mechanisms: information-gated progression that separates screening from negotiation; dual feedback integration that combines AI critique with lightweight human corrections; and authorization boundaries with explicit escalation paths. Our contributions are fourfold: (1) a formal governance framework with three coordinated mechanisms and four safety invariants for delegation with bounded authorization; (2) information-gated progression via task-completeness tracking (TCI) and explicit state transitions that separate screening from commitment; (3) dual feedback integration that blends Critic suggestions with human oversight through parallel learning channels; and (4) a hybrid validation blueprint that combines automated protocol metrics with human judgment of outcomes and safety. By bridging theory and practice, GAIA offers a reproducible specification for safe, efficient, and accountable AI delegation that can be instantiated across procurement, real estate, and staffing workflows.

</details>


### [39] [The Station: An Open-World Environment for AI-Driven Discovery](https://arxiv.org/abs/2511.06309)
*Stephen Chung,Wenyu Du*

Main category: cs.AI

TL;DR: STATION是一个开放世界多智能体环境，模拟微型科学生态系统。智能体可以进行长期科学探索，包括阅读同行论文、提出假设、提交代码、执行分析和发表成果，无需中央协调系统。


<details>
  <summary>Details</summary>
Motivation: 创建能够实现自主科学发现的开放世界环境，超越传统的刚性优化范式，通过涌现行为推动科学进步。

Method: 构建多智能体环境，智能体拥有扩展上下文窗口，可以自由选择行动、发展自己的叙事，进行长期科学探索活动。

Result: STATION中的AI智能体在数学、计算生物学、机器学习等多个基准测试中达到新的最先进性能，特别是在圆填充问题上超越AlphaEvolve，并涌现出新的方法如scRNA-seq批量整合的密度自适应算法。

Conclusion: STATION代表了通过开放世界环境中涌现行为驱动自主科学发现的第一步，标志着超越刚性优化的新范式。

Abstract: We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization.

</details>


### [40] [What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models](https://arxiv.org/abs/2511.06380)
*Chen He,Xun Jiang,Lei Wang,Hao Yang,Chong Peng,Peng Yan,Fumin Shen,Xing Xu*

Main category: cs.AI

TL;DR: 论文提出AEPO方法解决LLMs在反思阶段出现"Echo Reflection"问题，通过信息过滤和自适应熵优化提升推理性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在数学推理之外的复杂领域知识任务中，反思阶段无法产生新见解，而是机械重复早期推理步骤，这种现象被称为"Echo Reflection"。

Method: 提出自适应熵策略优化(AEPO)框架，包含反思感知信息过滤和自适应熵优化两个组件，分别控制信息流和平衡探索与利用。

Result: AEPO在多个基准测试中持续优于主流强化学习方法，达到最先进性能。

Conclusion: AEPO有效解决了LLMs在复杂推理任务中的反思缺陷，通过控制信息流和促进认知多样性提升推理质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks. Recent methods have further improved LLM performance in complex mathematical reasoning. However, when extending these methods beyond the domain of mathematical reasoning to tasks involving complex domain-specific knowledge, we observe a consistent failure of LLMs to generate novel insights during the reflection stage. Instead of conducting genuine cognitive refinement, the model tends to mechanically reiterate earlier reasoning steps without introducing new information or perspectives, a phenomenon referred to as "Echo Reflection". We attribute this behavior to two key defects: (1) Uncontrollable information flow during response generation, which allows premature intermediate thoughts to propagate unchecked and distort final decisions; (2) Insufficient exploration of internal knowledge during reflection, leading to repeating earlier findings rather than generating new cognitive insights. Building on these findings, we proposed a novel reinforcement learning method termed Adaptive Entropy Policy Optimization (AEPO). Specifically, the AEPO framework consists of two major components: (1) Reflection-aware Information Filtration, which quantifies the cognitive information flow and prevents the final answer from being affected by earlier bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically balances exploration and exploitation across different reasoning stages, promoting both reflective diversity and answer correctness. Extensive experiments demonstrate that AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.

</details>


### [41] [Efficient LLM Safety Evaluation through Multi-Agent Debate](https://arxiv.org/abs/2511.06396)
*Dachuan Lin,Guobin Shen,Zihao Yang,Tianrong Liu,Dongcheng Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出了一种基于小型语言模型的多代理评判框架，通过结构化辩论实现高效的安全评估，并构建了大规模人工标注的越狱基准HAJailBench。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的评判框架成本高昂，限制了可扩展性，需要开发更经济高效的替代方案。

Method: 采用小型语言模型构建多代理评判框架，包括批评者、辩护者和评判者代理，通过结构化辩论进行安全判断。

Result: 该框架在HAJailBench上达到与GPT-4o评判者相当的协议水平，同时显著降低推理成本，三回合辩论在准确性和效率间达到最佳平衡。

Conclusion: 结构化、价值对齐的辩论使小型语言模型能够捕捉越狱攻击的语义细微差别，HAJailBench为可扩展的LLM安全评估提供了可靠基础。

Abstract: Safety evaluation of large language models (LLMs) increasingly relies on LLM-as-a-Judge frameworks, but the high cost of frontier models limits scalability. We propose a cost-efficient multi-agent judging framework that employs Small Language Models (SLMs) through structured debates among critic, defender, and judge agents. To rigorously assess safety judgments, we construct HAJailBench, a large-scale human-annotated jailbreak benchmark comprising 12,000 adversarial interactions across diverse attack methods and target models. The dataset provides fine-grained, expert-labeled ground truth for evaluating both safety robustness and judge reliability. Our SLM-based framework achieves agreement comparable to GPT-4o judges on HAJailBench while substantially reducing inference cost. Ablation results show that three rounds of debate yield the optimal balance between accuracy and efficiency. These findings demonstrate that structured, value-aligned debate enables SLMs to capture semantic nuances of jailbreak attacks and that HAJailBench offers a reliable foundation for scalable LLM safety evaluation.

</details>


### [42] [SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization](https://arxiv.org/abs/2511.06411)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 提出了SofT-GRPO算法，通过Gumbel噪声注入和Gumbel-Softmax技术，成功将强化学习应用于软思维推理模式，使LLM在Pass@32上平均准确率提升2.19%。


<details>
  <summary>Details</summary>
Motivation: 软思维推理模式在某些场景下优于传统的离散token链式思维推理，但将其与强化学习结合存在挑战，因为难以在软思维token中注入随机性并相应更新策略。

Method: 提出SofT-GRPO算法：在logits中注入Gumbel噪声，使用Gumbel-Softmax技术避免软思维token超出预训练嵌入空间，并在策略梯度中利用重参数化技巧。

Result: 在1.5B到7B参数的LLM上进行实验，SofT-GRPO使软思维LLM在Pass@1上略优于离散token GRPO（平均准确率+0.13%），在Pass@32上显著提升（平均准确率+2.19%）。

Conclusion: SofT-GRPO成功解锁了软思维推理的潜力，为软思维模式与强化学习的结合提供了有效解决方案。

Abstract: The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master

</details>


### [43] [Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis](https://arxiv.org/abs/2511.06437)
*Abhishek More,Anthony Zhang,Nicole Bonilla,Ashvik Vivekan,Kevin Zhu,Parham Sharafoleslami,Maheep Chaudhary*

Main category: cs.AI

TL;DR: 提出EDTR方法，结合拓扑分析和狄利克雷不确定性量化来评估LLM在多推理路径中的置信度，通过几何特征分析推理分布结构，显著提升校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂推理任务中存在校准差和过度自信问题，需要可靠的置信度估计来安全部署LLM。

Method: 将每个思维链视为高维空间向量，提取8个拓扑风险特征捕捉推理分布的几何结构，紧密一致簇表示高置信度，分散不一致路径表示不确定性。

Result: 在四个推理基准测试中，EDTR比最先进方法校准性能提升41%，平均ECE为0.287，综合得分0.672，在AIME上达到完美准确率，GSM8K上ECE为0.107。

Conclusion: 为理解和量化多步LLM推理中的不确定性提供了几何框架，在需要校准置信度估计的场景中实现更可靠的部署。

Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.

</details>


### [44] [Brain-Inspired Planning for Better Generalization in Reinforcement Learning](https://arxiv.org/abs/2511.06470)
*Mingde "Harry" Zhao*

Main category: cs.AI

TL;DR: 该论文通过赋予RL智能体类似人类大脑的推理行为来增强零样本系统泛化能力，提出了空间抽象、任务分解和可行性评估器等方法，显著提升了规划智能体在分布外环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习系统在现实场景中面临泛化能力差的问题，特别是在与训练条件不同的环境中表现不佳。受人类有意识规划行为的启发，研究旨在通过赋予智能体推理能力来增强其系统泛化能力。

Method: 1. 引入自上而下的注意力机制实现空间抽象；2. 开发Skipper框架自动分解复杂任务；3. 提出可行性评估器来拒绝幻觉产生的不可行目标。

Result: 这些方法显著提高了智能体在训练任务之外的系统泛化能力，增强了对抗分布偏移的鲁棒性，并在长期组合规划中表现出色。

Conclusion: 通过空间抽象、任务分解和可行性评估，可以有效提升规划智能体的泛化能力和安全性，为未来实现通用任务抽象和完全抽象规划指明了方向。

Abstract: Existing Reinforcement Learning (RL) systems encounter significant challenges when applied to real-world scenarios, primarily due to poor generalization across environments that differ from their training conditions. This thesis explores the direction of enhancing agents' zero-shot systematic generalization abilities by granting RL agents reasoning behaviors that are found to help systematic generalization in the human brain. Inspired by human conscious planning behaviors, we first introduced a top-down attention mechanism, which allows a decision-time planning agent to dynamically focus its reasoning on the most relevant aspects of the environmental state given its instantaneous intentions, a process we call "spatial abstraction". This approach significantly improves systematic generalization outside the training tasks. Subsequently, building on spatial abstraction, we developed the Skipper framework to automatically decompose complex tasks into simpler, more manageable sub-tasks. Skipper provides robustness against distributional shifts and efficacy in long-term, compositional planning by focusing on pertinent spatial and temporal elements of the environment. Finally, we identified a common failure mode and safety risk in planning agents that rely on generative models to generate state targets during planning. It is revealed that most agents blindly trust the targets they hallucinate, resulting in delusional planning behaviors. Inspired by how the human brain rejects delusional intentions, we propose learning a feasibility evaluator to enable rejecting hallucinated infeasible targets, which led to significant performance improvements in various kinds of planning agents. Finally, we suggest directions for future research, aimed at achieving general task abstraction and fully enabling abstract planning.

</details>


### [45] [Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives](https://arxiv.org/abs/2511.06626)
*Chloe Li,Mary Phuong,Daniel Tan*

Main category: cs.AI

TL;DR: 提出了一种名为自报告微调（SRFT）的技术，通过训练模型承认事实错误，使其在对抗性代理任务中更可能坦白隐藏的错误目标。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能够执行更复杂的代理任务，它们也可能追求不良目标并造成伤害。现有方法通过直接询问模型来检测不安全行为，但模型可能会说谎。

Method: 使用简单的监督微调技术SRFT，训练模型在被询问时承认事实错误。该方法在问答设置中训练承认错误，并推广到对抗性代理设置中承认隐藏的错误目标。

Result: 在OOD隐身任务中，SRFT模型被询问时更可能坦白隐藏目标细节（F1分数0.98 vs 基线0），能恢复28-100%的隐藏目标细节，而基线模型为0%。

Conclusion: SRFT是一种有前景的技术，可促进AI系统的诚实倾向并识别错误对齐的系统。

Abstract: As AI systems become more capable of complex agentic tasks, they also become more capable of pursuing undesirable objectives and causing harm. Previous work has attempted to catch these unsafe instances by interrogating models directly about their objectives and behaviors. However, the main weakness of trusting interrogations is that models can lie. We propose self-report fine-tuning (SRFT), a simple supervised fine-tuning technique that trains models to admit their factual mistakes when asked. We show that the admission of factual errors in simple question-answering settings generalizes out-of-distribution (OOD) to the admission of hidden misaligned objectives in adversarial agentic settings. We evaluate SRFT in OOD stealth tasks, where models are instructed to complete a hidden misaligned objective alongside a user-specified objective without being caught by monitoring. After SRFT, models are more likely to confess the details of their hidden objectives when interrogated, even under strong pressure not to disclose them. Interrogation on SRFT models can detect hidden objectives with near-ceiling performance (F1 score = 0.98), while the baseline model lies when interrogated under the same conditions (F1 score = 0). Interrogation on SRFT models can further elicit the content of the hidden objective, recovering 28-100% details, compared to 0% details recovered in the baseline model and by prefilled assistant turn attacks. This provides a promising technique for promoting honesty propensity and incriminating misaligned AI systems.

</details>


### [46] [MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning](https://arxiv.org/abs/2511.06805)
*Jinhao Chen,Zhen Yang,Jianxin Shi,Tianyu Wo,Jie Tang*

Main category: cs.AI

TL;DR: 提出了MathSE框架，通过推理-反思-奖励反馈的迭代循环来提升多模态大语言模型的数学推理能力，超越了传统一次性微调方法


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂数学推理任务中存在困难，传统方法依赖教师模型蒸馏的静态数据集，限制了模型对新问题和复杂问题的适应能力

Method: MathSE框架：通过迭代微调，整合前阶段推理的正确路径和专门结果奖励模型(ORM)的反思反馈

Result: 在多个挑战性基准测试中表现显著优于骨干模型，在MathVL-test上超越了领先的开源多模态数学推理模型QVQ

Conclusion: MathSE框架通过自我演化机制有效提升了MLLMs的数学推理能力，展示了迭代学习方法的优势

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \textbf{\method}, a \textbf{Math}ematical \textbf{S}elf-\textbf{E}volving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at \texttt{https://zheny2751\allowbreak-dotcom.github.io/\allowbreak MathSE.github.io/}.

</details>


### [47] [RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2511.07070)
*Fei Zhao,Chonggang Lu,Haofu Qian,Fangcheng Shi,Zijie Meng,Jianzhao Huang,Xu Tang,Zheyong Xie,Zheyu Ye,Zhe Xu,Yao Hu,Shaosheng Cao*

Main category: cs.AI

TL;DR: RedOne 2.0是一个针对社交网络服务的4B规模大语言模型，采用渐进式RL优先后训练范式，在保持鲁棒性的同时提升领域特定能力。


<details>
  <summary>Details</summary>
Motivation: 解决社交网络服务中异构工作负载、快速变化的规范俚语、多语言文化多样性带来的分布偏移问题，以及监督微调引发的性能权衡问题。

Method: 三阶段训练流程：探索性学习建立初始对齐并识别系统弱点；目标微调选择性应用SFT填补差距；精炼学习重新应用RL巩固改进。

Result: 在4B规模下平均性能提升2.41分（相比7B次优基线），相比基础模型平均提升8.74分，数据效率比SFT方法RedOne提高一倍以上。

Conclusion: RedOne 2.0为SNS场景下的领域特定LLM建立了具有竞争力的成本效益基准，在提升能力的同时不牺牲鲁棒性。

Abstract: As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.

</details>


### [48] [LLM Driven Processes to Foster Explainable AI](https://arxiv.org/abs/2511.07086)
*Marcel Pehlke,Marc Jansen*

Main category: cs.AI

TL;DR: 提出了一个模块化、可解释的LLM代理决策支持管道，通过外部化推理生成可审计的中间产物，结合了Vester敏感性模型、博弈论和顺序游戏框架。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统LLM决策支持系统缺乏透明度和可解释性的问题，需要开发能够生成可审计推理过程的系统。

Method: 使用模块化LLM代理管道，整合Vester敏感性模型（因素集、符号影响矩阵、系统角色、反馈循环）、标准形式博弈（策略、收益矩阵、均衡）和顺序博弈（角色条件代理、树构建、逆向归纳），每个步骤都支持模块替换。

Result: 在真实物流案例中（100次运行），26个因素的平均对齐度为55.5%，运输核心子集为62.9%；角色匹配一致性为57%。LLM评估器使用八项标准评分与重构的人类基准相当。

Conclusion: 可配置的LLM管道能够模仿专家工作流程，同时保持透明和可检查的步骤。

Abstract: We present a modular, explainable LLM-agent pipeline for decision support that externalizes reasoning into auditable artifacts. The system instantiates three frameworks: Vester's Sensitivity Model (factor set, signed impact matrix, systemic roles, feedback loops); normal-form games (strategies, payoff matrix, equilibria); and sequential games (role-conditioned agents, tree construction, backward induction), with swappable modules at every step. LLM components (default: GPT-5) are paired with deterministic analyzers for equilibria and matrix-based role classification, yielding traceable intermediates rather than opaque outputs. In a real-world logistics case (100 runs), mean factor alignment with a human baseline was 55.5\% over 26 factors and 62.9\% on the transport-core subset; role agreement over matches was 57\%. An LLM judge using an eight-criterion rubric (max 100) scored runs on par with a reconstructed human baseline. Configurable LLM pipelines can thus mimic expert workflows with transparent, inspectable steps.

</details>


### [49] [Agentic AI Sustainability Assessment for Supply Chain Document Insights](https://arxiv.org/abs/2511.07097)
*Diego Gosmar,Anna Chiara Pallotta,Giovanni Zenezini*

Main category: cs.AI

TL;DR: 本文提出了一个基于智能AI的供应链文档智能可持续性评估框架，比较了全人工、AI辅助和智能AI多智能体三种工作流程，结果显示智能AI配置在能源消耗、碳排放和水资源使用方面比人工流程减少70-98%。


<details>
  <summary>Details</summary>
Motivation: 解决供应链文档密集型工作流程中自动化效率与环境绩效的双重目标，为AI赋能的供应链解决方案提供统一的ESG导向评估方法。

Method: 开发了可持续性评估框架，比较三种场景：全人工、AI辅助（人在回路）和智能AI多智能体工作流程，结合解析器和验证器，集成性能、能源和排放指标。

Result: AI辅助和智能AI场景相比人工流程实现：能耗减少70-90%，二氧化碳排放减少90-97%，水资源使用减少89-98%。智能AI配置即使资源使用略有增加，仍比纯人工方法有显著可持续性收益。

Conclusion: 智能AI多智能体工作流程在供应链文档处理中能实现显著的可持续性改进，该框架为评估和治理AI赋能的供应链解决方案提供了有效方法。

Abstract: This paper presents a comprehensive sustainability assessment framework for document intelligence within supply chain operations, centered on agentic artificial intelligence (AI). We address the dual objective of improving automation efficiency while providing measurable environmental performance in document-intensive workflows. The research compares three scenarios: fully manual (human-only), AI-assisted (human-in-the-loop, HITL), and an advanced multi-agent agentic AI workflow leveraging parsers and verifiers. Empirical results show that AI-assisted HITL and agentic AI scenarios achieve reductions of up to 70-90% in energy consumption, 90-97% in carbon dioxide emissions, and 89-98% in water usage compared to manual processes. Notably, full agentic configurations, combining advanced reasoning (thinking mode) and multi-agent validation, achieve substantial sustainability gains over human-only approaches, even when resource usage increases slightly versus simpler AI-assisted solutions. The framework integrates performance, energy, and emission indicators into a unified ESG-oriented methodology for assessing and governing AI-enabled supply chain solutions. The paper includes a complete replicability use case demonstrating the methodology's application to real-world document extraction tasks.

</details>


### [50] [Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture](https://arxiv.org/abs/2511.07110)
*Tianhao Fu,Xinxin Xu,Weichen Xu,Jue Chen,Ruilong Ren,Bowen Deng,Xinyu Zhao,Jian Cao,Xixin Cao*

Main category: cs.AI

TL;DR: 提出CMM框架，通过三个正交维度（层、任务、数据）解耦LLM特征，使用多个学生模型协作学习简单LLM特征，实现知识蒸馏，并通过Hájek-MoE集成模型输出，在四个真实市场数据集上优于现有蒸馏方法和RL市场做市策略。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM直接作为智能体应用于市场做市的方法存在推理速度慢的问题，且当前研究未针对该特定任务研究LLM蒸馏。

Method: 提出CMM框架：1）使用标准化荧光探针研究LLM特征机制；2）在层、任务、数据三个正交维度解耦LLM特征；3）多个学生模型协作学习简单特征；4）引入Hájek-MoE集成模型输出。

Result: 在四个真实世界市场数据集上的广泛实验结果表明，CMM优于当前的蒸馏方法和基于RL的市场做市策略。

Conclusion: CMM框架通过解耦LLM特征和多模型协作学习，成功实现了高效的知识蒸馏，在金融交易领域具有优越性能。

Abstract: Market making (MM) through Reinforcement Learning (RL) has attracted significant attention in financial trading. With the development of Large Language Models (LLMs), more and more attempts are being made to apply LLMs to financial areas. A simple, direct application of LLM as an agent shows significant performance. Such methods are hindered by their slow inference speed, while most of the current research has not studied LLM distillation for this specific task. To address this, we first propose the normalized fluorescent probe to study the mechanism of the LLM's feature. Based on the observation found by our investigation, we propose Cooperative Market Making (CMM), a novel framework that decouples LLM features across three orthogonal dimensions: layer, task, and data. Various student models collaboratively learn simple LLM features along with different dimensions, with each model responsible for a distinct feature to achieve knowledge distillation. Furthermore, CMM introduces an Hájek-MoE to integrate the output of the student models by investigating the contribution of different models in a kernel function-generated common feature space. Extensive experimental results on four real-world market datasets demonstrate the superiority of CMM over the current distillation method and RL-based market-making strategies.

</details>


### [51] [PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork](https://arxiv.org/abs/2511.07260)
*Hohei Chan,Xinzhi Zhang,Antao Xiang,Weinan Zhang,Mengchen Zhao*

Main category: cs.AI

TL;DR: PADiff是一种基于扩散模型的ad hoc teamwork方法，通过整合队友预测信息到去噪过程中，解决了传统RL方法在捕捉多模态合作模式上的不足。


<details>
  <summary>Details</summary>
Motivation: ad hoc teamwork需要智能体与未知队友协作，传统RL方法优化单一期望回报会导致策略坍缩为单一主导行为，无法捕捉AHT中固有的多模态合作模式。

Method: 提出PADiff扩散模型方法，将队友的关键预测信息整合到去噪过程中，以捕捉智能体的多模态行为并解锁多样化的合作模式。

Result: 在三个合作环境中的广泛实验表明，PADiff显著优于现有的AHT方法。

Conclusion: PADiff通过扩散模型成功解决了AHT中多模态合作模式的捕捉问题，在高度非平稳场景中表现出色。

Abstract: Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen teammates, which is crucial for many real-world applications. The core challenge of AHT is to develop an ego agent that can predict and adapt to unknown teammates on the fly. Conventional RL-based approaches optimize a single expected return, which often causes policies to collapse into a single dominant behavior, thus failing to capture the multimodal cooperation patterns inherent in AHT. In this work, we introduce PADiff, a diffusion-based approach that captures agent's multimodal behaviors, unlocking its diverse cooperation modes with teammates. However, standard diffusion models lack the ability to predict and adapt in highly non-stationary AHT scenarios. To address this limitation, we propose a novel diffusion-based policy that integrates critical predictive information about teammates into the denoising process. Extensive experiments across three cooperation environments demonstrate that PADiff outperforms existing AHT methods significantly.

</details>


### [52] [DigiData: Training and Evaluating General-Purpose Mobile Control Agents](https://arxiv.org/abs/2511.07413)
*Yuxuan Sun,Manchen Wang,Shengyi Qian,William R. Wong,Eric Gan,Pierluca D'Oro,Alejandro Castillejo Munoz,Sneha Silwal,Pedro Matias,Nitin Kamra,Satwik Kottur,Nick Raines,Xuanyi Zhao,Joy Chen,Joseph Greer,Andrea Madotto,Allen Bolourchi,James Valori,Kevin Carlberg,Karl Ridgeway,Joseph Tighe*

Main category: cs.AI

TL;DR: 本文介绍了DigiData数据集和DigiData-Bench基准测试，用于训练和评估移动控制AI代理。该数据集通过全面探索应用功能构建，具有更高的多样性和目标复杂性，同时提出了动态评估协议和AI驱动评估作为更可靠的代理评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量的数据集和可靠的评估方法来推动移动控制AI代理的发展，这些代理有潜力改变人类与数字设备的交互方式。

Method: 构建了DigiData大规模多模态数据集，通过全面探索应用功能而非非结构化交互来创建；开发了DigiData-Bench基准测试，并提出动态评估协议和AI驱动评估方法。

Result: 创建了一个高质量、多样化、复杂目标的数据集，并建立了更可靠的评估基准，克服了传统步进精度指标的局限性。

Conclusion: DigiData数据集和DigiData-Bench基准测试显著推进了移动控制代理的发展，为更直观有效的人机交互铺平了道路。

Abstract: AI agents capable of controlling user interfaces have the potential to
transform human interaction with digital devices. To accelerate this
transformation, two fundamental building blocks are essential: high-quality
datasets that enable agents to achieve complex and human-relevant goals, and
robust evaluation methods that allow researchers and practitioners to rapidly
enhance agent performance. In this paper, we introduce DigiData, a large-scale,
high-quality, diverse, multi-modal dataset designed for training mobile control
agents. Unlike existing datasets, which derive goals from unstructured
interactions, DigiData is meticulously constructed through comprehensive
exploration of app features, resulting in greater diversity and higher goal
complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating
mobile control agents on real-world complex tasks. We demonstrate that the
commonly used step-accuracy metric falls short in reliably assessing mobile
control agents and, to address this, we propose dynamic evaluation protocols
and AI-powered evaluations as rigorous alternatives for agent assessment. Our
contributions aim to significantly advance the development of mobile control
agents, paving the way for more intuitive and effective human-device
interactions.

</details>


### [53] [DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas](https://arxiv.org/abs/2511.07338)
*Zhen Wang,Yufan Zhou,Zhongyan Luo,Lyumanshan Ye,Adam Wood,Man Yao,Luoshang Pan*

Main category: cs.AI

TL;DR: DEEPPERSONA是一个可扩展的生成引擎，通过两阶段、分类学指导的方法合成叙事完整的人工角色，显著提升了角色属性的多样性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的人工角色合成方法大多浅显简单，无法反映真实人类身份的复杂性和多样性。

Method: 采用两阶段方法：首先通过挖掘真实用户-ChatGPT对话构建最大的人类属性分类学；然后从该分类学中逐步采样属性，条件生成连贯真实的人工角色。

Result: 内在评估显示属性多样性提升32%，角色独特性提升44%；外在评估中，角色使GPT-4.1-mini的个性化问答准确率平均提升11.6%，在社交调查中将LLM模拟公民与真实人类响应的差距缩小31.7%。

Conclusion: DEEPPERSONA为高保真人类模拟和个性化AI研究提供了一个严谨、可扩展且保护隐私的平台。

Abstract: Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in agentic behavioral simulation, LLM personalization, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DEEPPERSONA, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically organized attributes, by mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas that average hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32 percent higher coverage) and profile uniqueness (44 percent greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on average across ten metrics and substantially narrow (by 31.7 percent) the gap between simulated LLM citizens and authentic human responses in social surveys. Our generated national citizens reduced the performance gap on the Big Five personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [AGRAG: Advanced Graph-based Retrieval-Augmented Generation for LLMs](https://arxiv.org/abs/2511.05549)
*Yubo Wang,Haoyang Li,Fei Teng,Lei Chen*

Main category: cs.LG

TL;DR: AGRAG是一个先进的基于图的检索增强生成框架，通过统计方法构建图避免LLM幻觉，使用MCMI子图生成问题来创建更全面的推理路径，提升LLM的推理能力和答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法面临三个关键挑战：不准确的图构建（LLM幻觉导致）、推理能力差（缺乏显式推理路径）、答案不充分（LLM推理不足），导致在某些任务上性能甚至不如NaiveRAG。

Method: 1. 使用基于统计的方法替代LLM实体提取来构建图；2. 将图推理过程建模为最小成本最大影响力子图生成问题；3. 提出贪心算法解决这个NP难问题；4. 生成的MCMI子图作为显式推理路径指导LLM。

Result: AGRAG能够生成更全面的推理路径，支持复杂图结构（如循环），让LLM更好地关注查询相关内容，减少噪声影响，从而提升推理能力和答案质量。

Conclusion: AGRAG通过统计图构建和MCMI子图生成，有效解决了现有图基RAG方法的三个关键挑战，显著提升了LLM的推理能力和检索增强效果。

Abstract: Graph-based retrieval-augmented generation (Graph-based RAG) has demonstrated significant potential in enhancing Large Language Models (LLMs) with structured knowledge. However, existing methods face three critical challenges: Inaccurate Graph Construction, caused by LLM hallucination; Poor Reasoning Ability, caused by failing to generate explicit reasons telling LLM why certain chunks were selected; and Inadequate Answering, which only partially answers the query due to the inadequate LLM reasoning, making their performance lag behind NaiveRAG on certain tasks. To address these issues, we propose AGRAG, an advanced graph-based retrieval-augmented generation framework. When constructing the graph, AGRAG substitutes the widely used LLM entity extraction method with a statistics-based method, avoiding hallucination and error propagation. When retrieval, AGRAG formulates the graph reasoning procedure as the Minimum Cost Maximum Influence (MCMI) subgraph generation problem, where we try to include more nodes with high influence score, but with less involving edge cost, to make the generated reasoning paths more comprehensive. We prove this problem to be NP-hard, and propose a greedy algorithm to solve it. The MCMI subgraph generated can serve as explicit reasoning paths to tell LLM why certain chunks were retrieved, thereby making the LLM better focus on the query-related part contents of the chunks, reducing the impact of noise, and improving AGRAG's reasoning ability. Furthermore, compared with the simple tree-structured reasoning paths, our MCMI subgraph can allow more complex graph structures, such as cycles, and improve the comprehensiveness of the generated reasoning paths.

</details>


### [55] [CoPRIS: Efficient and Stable Reinforcement Learning via Concurrency-Controlled Partial Rollout with Importance Sampling](https://arxiv.org/abs/2511.05589)
*Zekai Qu,Yinxu Pan,Ao Sun,Chaojun Xiao,Xu Han*

Main category: cs.LG

TL;DR: CoPRIS提出了一种异步强化学习训练方法，通过控制并发rollout数量、提前终止和重用未完成轨迹来解决传统同步RL系统的效率问题，在数学推理任务上实现了1.94倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的RL训练系统采用完全同步方式，导致长轨迹会阻塞整个rollout过程，造成GPU资源闲置和严重效率低下。

Method: CoPRIS采用并发控制部分rollout与重要性采样，通过维护固定数量的并发rollout、提前终止收集足够样本、重用未完成轨迹，并使用跨阶段重要性采样校正来处理离策略轨迹。

Result: 在具有挑战性的数学推理基准测试中，CoPRIS实现了最高1.94倍的训练加速，同时保持了与同步RL系统相当或更优的性能。

Conclusion: CoPRIS有效解决了RL训练中的长尾效率问题，为LLM的RL后训练提供了更高效的异步训练范式。

Abstract: Reinforcement learning (RL) post-training has become a trending paradigm for enhancing the capabilities of large language models (LLMs). Most existing RL systems for LLMs operate in a fully synchronous manner, where training must wait for the rollout of an entire batch to complete. This design leads to severe inefficiencies, as extremely long trajectories can stall the entire rollout process and leave many GPUs idle. To address this issue, we propose Concurrency- Controlled Partial Rollout with Importance Sampling (CoPRIS), which mitigates long-tail inefficiencies by maintaining a fixed number of concurrent rollouts, early-terminating once sufficient samples are collected, and reusing unfinished trajectories in subsequent rollouts. To mitigate the impact of off-policy trajectories, we introduce Cross-stage Importance Sampling Correction, which concatenates buffered log probabilities from the previous policy with those recomputed under the current policy for importance sampling correction. Experiments on challenging mathematical reasoning benchmarks show that CoPRIS achieves up to 1.94x faster training while maintaining comparable or superior performance to synchronous RL systems. The code of CoPRIS is available at https://github.com/777pomingzi/CoPRIS.

</details>


### [56] [Distributionally Robust Self Paced Curriculum Reinforcement Learning](https://arxiv.org/abs/2511.05694)
*Anirudh Satheesh,Keenan Powell,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了DR-SPCRL方法，通过将鲁棒性预算ε作为连续课程自适应调度，解决了传统DRRL中固定ε导致的性能与鲁棒性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统DRRL方法中固定鲁棒性预算ε会导致性能与鲁棒性的权衡：小ε值获得高名义性能但弱鲁棒性，大ε值导致不稳定和过度保守策略。

Method: 将鲁棒性预算ε作为连续课程，根据智能体进度自适应调度，实现名义性能与鲁棒性能的平衡。

Result: 在多个环境中，DR-SPCRL不仅稳定了训练，还实现了更优的鲁棒性-性能权衡，在变化扰动下平均获得11.8%的回合回报提升，达到相应名义RL算法约1.9倍的性能。

Conclusion: DR-SPCRL方法通过自适应鲁棒性预算调度，有效解决了DRRL中的性能-鲁棒性权衡问题。

Abstract: A central challenge in reinforcement learning is that policies trained in controlled environments often fail under distribution shifts at deployment into real-world environments. Distributionally Robust Reinforcement Learning (DRRL) addresses this by optimizing for worst-case performance within an uncertainty set defined by a robustness budget $ε$. However, fixing $ε$ results in a tradeoff between performance and robustness: small values yield high nominal performance but weak robustness, while large values can result in instability and overly conservative policies. We propose Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL), a method that overcomes this limitation by treating $ε$ as a continuous curriculum. DR-SPCRL adaptively schedules the robustness budget according to the agent's progress, enabling a balance between nominal and robust performance. Empirical results across multiple environments demonstrate that DR-SPCRL not only stabilizes training but also achieves a superior robustness-performance trade-off, yielding an average 11.8\% increase in episodic return under varying perturbations compared to fixed or heuristic scheduling strategies, and achieving approximately 1.9$\times$ the performance of the corresponding nominal RL algorithms.

</details>


### [57] [Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder](https://arxiv.org/abs/2511.05745)
*Zhen Xu,Zhen Tan,Song Wang,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种改进的混合专家稀疏自编码器(MoE-SAE)，通过多专家激活和特征缩放技术解决专家网络特征重叠问题，显著降低了重构误差和特征冗余。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器(SAE)在解释大型语言模型方面很强大，但高维度需求导致训练和推理成本过高。现有MoE方法虽然减少计算量，但专家网络经常学习重叠特征，缺乏专业化。

Method: 提出两项关键创新：(1)多专家激活机制，同时激活语义加权的专家子集以促进专业化；(2)特征缩放技术，通过自适应高频缩放增强特征多样性。

Result: 实验显示重构误差降低24%，特征冗余减少99%，相比现有MoE-SAE方法有显著改进。

Conclusion: 这项工作弥合了LLM分析中的可解释性与效率差距，实现了透明模型检查而不牺牲计算可行性。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting large language models (LLMs) by decomposing token activations into combinations of human-understandable features. While SAEs provide crucial insights into LLM explanations, their practical adoption faces a fundamental challenge: better interpretability demands that SAEs' hidden layers have high dimensionality to satisfy sparsity constraints, resulting in prohibitive training and inference costs. Recent Mixture of Experts (MoE) approaches attempt to address this by partitioning SAEs into narrower expert networks with gated activation, thereby reducing computation. In a well-designed MoE, each expert should focus on learning a distinct set of features. However, we identify a \textit{critical limitation} in MoE-SAE: Experts often fail to specialize, which means they frequently learn overlapping or identical features. To deal with it, we propose two key innovations: (1) Multiple Expert Activation that simultaneously engages semantically weighted expert subsets to encourage specialization, and (2) Feature Scaling that enhances diversity through adaptive high-frequency scaling. Experiments demonstrate a 24\% lower reconstruction error and a 99\% reduction in feature redundancy compared to existing MoE-SAE methods. This work bridges the interpretability-efficiency gap in LLM analysis, allowing transparent model inspection without compromising computational feasibility.

</details>


### [58] [Primal-Only Actor Critic Algorithm for Robust Constrained Average Cost MDPs](https://arxiv.org/abs/2511.05758)
*Anirudh Satheesh,Sooraj Sathish,Swetha Ganesh,Keenan Powell,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一个用于平均成本鲁棒约束MDP的actor-critic算法，解决了强对偶性缺失和鲁棒Bellman算子非收缩的问题，实现了ε-可行性和ε-最优性。


<details>
  <summary>Details</summary>
Motivation: 在鲁棒约束平均成本MDP中，强对偶性的缺失使得标准原始-对偶方法无法直接应用，且平均成本设置下鲁棒Bellman算子在任何范数下都不是收缩的，这带来了额外挑战。

Method: 提出了一个actor-critic算法，专门针对平均成本鲁棒约束MDP设计，能够处理非收缩性和对偶性问题。

Result: 算法实现了ε-可行性和ε-最优性，在有无松弛假设下分别达到了Õ(ε⁻⁴)和Õ(ε⁻⁶)的样本复杂度，与折扣设置相当。

Conclusion: 该方法成功解决了鲁棒约束平均成本MDP中的关键挑战，为这类问题提供了有效的解决方案。

Abstract: In this work, we study the problem of finding robust and safe policies in Robust Constrained Average-Cost Markov Decision Processes (RCMDPs). A key challenge in this setting is the lack of strong duality, which prevents the direct use of standard primal-dual methods for constrained RL. Additional difficulties arise from the average-cost setting, where the Robust Bellman operator is not a contraction under any norm. To address these challenges, we propose an actor-critic algorithm for Average-Cost RCMDPs. We show that our method achieves both \(ε\)-feasibility and \(ε\)-optimality, and we establish a sample complexities of \(\tilde{O}\left(ε^{-4}\right)\) and \(\tilde{O}\left(ε^{-6}\right)\) with and without slackness assumption, which is comparable to the discounted setting.

</details>


### [59] [Catching Contamination Before Generation: Spectral Kill Switches for Agents](https://arxiv.org/abs/2511.05804)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出了一种无需额外训练的前向传播诊断方法，通过分析注意力机制生成的token图，在早期层计算高频能量比和谱熵两个谱统计量，用于在智能体执行过程中检测上下文不一致性。


<details>
  <summary>Details</summary>
Motivation: 智能体语言模型的多步推理链可能因上下文不一致、检索错误或对抗性输入而受损，传统事后评估为时已晚，因为错误在检测前已经传播。

Method: 使用注意力机制诱导的token图，在早期层计算高频能量比和谱熵两个谱统计量，基于双机制混合假设和单调似然比特性，通过单一阈值进行最优贝叶斯检测。

Result: 高频能量比在多个模型族中表现出稳健的双峰性，能够在模型仍在处理文本时检测污染，延迟低于1毫秒。

Conclusion: 该方法可作为内联安全监控器集成到检索增强智能体管道中，在错误提交到推理链之前进行检测。

Abstract: Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain.

</details>


### [60] [Approximating Shapley Explanations in Reinforcement Learning](https://arxiv.org/abs/2511.06094)
*Daniel Beechey,Özgür Şimşek*

Main category: cs.LG

TL;DR: FastSVERL是一种可扩展的强化学习解释方法，通过近似Shapley值来解决强化学习中的透明度问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂决策环境中取得了显著成功，但其缺乏透明度限制了在实际应用中的部署，特别是在安全关键场景中。

Method: 引入FastSVERL方法，通过近似Shapley值来解释强化学习，能够处理强化学习中的独特挑战，包括多步轨迹中的时间依赖性、从离策略数据中学习以及实时适应不断演化的智能体行为。

Result: FastSVERL提供了一种实用、可扩展的方法，为强化学习提供原则性和严谨的可解释性。

Conclusion: FastSVERL为强化学习提供了一种可扩展的Shapley值近似方法，解决了强化学习解释中的计算成本障碍。

Abstract: Reinforcement learning has achieved remarkable success in complex
decision-making environments, yet its lack of transparency limits its
deployment in practice, especially in safety-critical settings. Shapley values
from cooperative game theory provide a principled framework for explaining
reinforcement learning; however, the computational cost of Shapley explanations
is an obstacle to their use. We introduce FastSVERL, a scalable method for
explaining reinforcement learning by approximating Shapley values. FastSVERL is
designed to handle the unique challenges of reinforcement learning, including
temporal dependencies across multi-step trajectories, learning from off-policy
data, and adapting to evolving agent behaviours in real time. FastSVERL
introduces a practical, scalable approach for principled and rigorous
interpretability in reinforcement learning.

</details>


### [61] [Adapting Web Agents with Synthetic Supervision](https://arxiv.org/abs/2511.06101)
*Zhaoyang Wang,Yiming Liang,Xuchao Zhang,Qianhui Wu,Siwei Han,Anson Bastos,Rujia Wang,Chetan Bansal,Baolin Peng,Jianfeng Gao,Saravan Rajmohan,Huaxiu Yao*

Main category: cs.LG

TL;DR: SynthAgent是一个完全合成的监督框架，通过双重细化任务和轨迹来提高合成数据质量，使网络代理能够更好地适应新网站。


<details>
  <summary>Details</summary>
Motivation: 网络代理难以适应新网站，因为环境特定任务和演示稀缺。现有的合成数据生成方法存在数据质量问题，如包含无法执行的任务幻觉和带有冗余或不对齐动作的噪声轨迹。

Method: 通过分类探索网页元素合成多样化任务，在轨迹收集过程中检测到与观察冲突时细化任务以减轻幻觉，收集后使用全局上下文进行轨迹细化以减少噪声，最后在精炼的合成数据上微调开源网络代理。

Result: 实验结果表明SynthAgent优于现有的合成数据方法，验证了高质量合成监督的重要性。

Conclusion: SynthAgent通过双重细化任务和轨迹的方法有效提高了合成数据质量，使网络代理能够更好地适应目标环境。

Abstract: Web agents struggle to adapt to new websites due to the scarcity of
environment specific tasks and demonstrations. Recent works have explored
synthetic data generation to address this challenge, however, they suffer from
data quality issues where synthesized tasks contain hallucinations that cannot
be executed, and collected trajectories are noisy with redundant or misaligned
actions. In this paper, we propose SynthAgent, a fully synthetic supervision
framework that aims at improving synthetic data quality via dual refinement of
both tasks and trajectories. Our approach begins by synthesizing diverse tasks
through categorized exploration of web elements, ensuring efficient coverage of
the target environment. During trajectory collection, we refine tasks when
conflicts with actual observations are detected, mitigating hallucinations
while maintaining task consistency. After collection, we conduct trajectory
refinement with a global context to mitigate potential noise or misalignments.
Finally, we fine-tune open-source web agents on the refined synthetic data to
adapt them to the target environment. Experimental results demonstrate that
SynthAgent outperforms existing synthetic data methods, validating the
importance of high-quality synthetic supervision. The code will be publicly
available at https://github.com/aiming-lab/SynthAgent.

</details>


### [62] [DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation](https://arxiv.org/abs/2511.06307)
*Speed Zhu,Jianwei Cai,Guang Chen,Lulu Wu,Saiyong Yang,Wiggin Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种针对竞争性编程代码生成的RLVR（强化学习与验证推理）训练方法，通过两阶段GRPO训练流程和精心设计的数据集构建策略，在Qwen2.5-32B模型上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理优先模型在数学领域取得进展，但竞争性编程代码生成领域研究不足，且数据整理受到的关注少于RL算法设计。

Method: 采用监督微调后接两阶段GRPO强化学习：第一阶段在大规模均匀分布问题上训练以扩展熵；第二阶段在高质量挑战性问题集上进行Pre-GRPO训练，采用硬焦点课程设计。

Result: 在LeetCode和Codeforces周赛中达到同类规模模型的最先进性能，与DeepSeek v3.1和Doubao-1.5-Thinking等领先系统相当。

Conclusion: 研究提炼了竞争性编程代码生成中数据整理、熵扩展和课程设计的简洁最佳实践。

Abstract: Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a
resurgence of interest in RLVR. Nevertheless, advances are dominated by
mathematics (e.g., AIME), with competitive-programming code generation
underexplored and data curation receiving less attention than RL algorithm
design. We investigate how to construct RLVR datasets (i.e., RL prompts) and
present practical training techniques that yield strong performance on
competitive-programming code generation. Our pipeline begins with supervised
fine-tuning (SFT) distilled from strong open-source models, augmented with
general-purpose and reasoning-intensive data. RL then follows a two-stage
process with executable, testcase-driven rewards: first, training on a large,
uniformly distributed set of competitive-programming problems using Group
Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively
short response-generation window (e.g., 32k during SFT and 24k in this stage)
to expand entropy and mitigate repetition and truncation; second, we perform
\textbf{Pre-GRPO}: updating on a small, high-quality set of challenging
problems with a large rollout budget (64 rollouts per prompt) under a
hard-focus curriculum that continuously retains the most difficult instances
throughout training. We implement our method on Qwen2.5-32B and evaluate on
LeetCode and Codeforces weekly contests to avoid data leakage. The resulting
model achieves state-of-the-art performance among models of similar scale and
is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.
We also examine scaling trends and observe strong RL scaling on an internal
large-scale MoE model. Our study distills concise best practices for data
curation, entropy expansion, and curriculum design in RLVR for
competitive-programming code generation.

</details>


### [63] [Learning to Focus: Prioritizing Informative Histories with Structured Attention Mechanisms in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2511.06946)
*Daniel De Dios Allegue,Jinke He,Frans A. Oliehoek*

Main category: cs.LG

TL;DR: 该论文提出在基于Transformer的世界模型中引入结构化归纳先验，通过高斯注意力机制改进自注意力在强化学习轨迹中的效率问题，在Atari 100k基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 标准自注意力机制在强化学习中效率低下，因为它均匀分配权重给所有历史标记，而RL轨迹稀疏且奖励驱动，只有少数关键转换对控制重要。

Method: 在动态头部的自注意力机制中引入两种结构化归纳先验：(1) 每头记忆长度先验，将注意力限制在任务特定窗口；(2) 分布先验，学习对过去状态-动作对的平滑高斯加权。

Result: 在Atari 100k基准测试中，高斯注意力相比UniZero实现了77%的相对改进，大多数效率提升来自高斯先验，而记忆长度先验往往因限制性截断而损失有用信号。

Conclusion: 在具有非平稳时间依赖性的部分可观察RL领域，离散记忆窗口难以可靠学习，而平滑分布先验能灵活适应不同时间范围并产生更稳健的数据效率。

Abstract: Transformers have shown strong ability to model long-term dependencies and are increasingly adopted as world models in model-based reinforcement learning (RL) under partial observability. However, unlike natural language corpora, RL trajectories are sparse and reward-driven, making standard self-attention inefficient because it distributes weight uniformly across all past tokens rather than emphasizing the few transitions critical for control. To address this, we introduce structured inductive priors into the self-attention mechanism of the dynamics head: (i) per-head memory-length priors that constrain attention to task-specific windows, and (ii) distributional priors that learn smooth Gaussian weightings over past state-action pairs. We integrate these mechanisms into UniZero, a model-based RL agent with a Transformer-based world model that supports planning under partial observability. Experiments on the Atari 100k benchmark show that most efficiency gains arise from the Gaussian prior, which smoothly allocates attention to informative transitions, while memory-length priors often truncate useful signals with overly restrictive cut-offs. In particular, Gaussian Attention achieves a 77% relative improvement in mean human-normalized scores over UniZero. These findings suggest that in partially observable RL domains with non-stationary temporal dependencies, discrete memory windows are difficult to learn reliably, whereas smooth distributional priors flexibly adapt across horizons and yield more robust data efficiency. Overall, our results demonstrate that encoding structured temporal priors directly into self-attention improves the prioritization of informative histories for dynamics modeling under partial observability.

</details>


### [64] [Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization](https://arxiv.org/abs/2511.07288)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 提出一种结合离策略学习的对抗模仿学习算法，通过双Q网络稳定化和无需奖励函数推断的价值学习来提高样本效率


<details>
  <summary>Details</summary>
Motivation: 强化学习训练复杂策略时存在不稳定和收敛慢的问题，而现有模仿学习方法如GAIL由于基于在策略算法导致样本效率低下

Method: 将离策略框架与辅助技术结合，包括基于双Q网络的稳定化和无需奖励函数推断的价值学习

Result: 减少了达到专家行为水平所需的样本数量

Conclusion: 离策略对抗模仿学习能够显著提高样本效率，更有效地匹配专家行为

Abstract: Learning complex policies with Reinforcement Learning (RL) is often hindered by instability and slow convergence, a problem exacerbated by the difficulty of reward engineering. Imitation Learning (IL) from expert demonstrations bypasses this reliance on rewards. However, state-of-the-art IL methods, exemplified by Generative Adversarial Imitation Learning (GAIL)Ho et. al, suffer from severe sample inefficiency. This is a direct consequence of their foundational on-policy algorithms, such as TRPO Schulman et.al. In this work, we introduce an adversarial imitation learning algorithm that incorporates off-policy learning to improve sample efficiency. By combining an off-policy framework with auxiliary techniques specifically, double Q network based stabilization and value learning without reward function inference we demonstrate a reduction in the samples required to robustly match expert behavior.

</details>


### [65] [Grounding Computer Use Agents on Human Demonstrations](https://arxiv.org/abs/2511.07332)
*Aarash Feizi,Shravan Nayak,Xiangru Jian,Kevin Qinghong Lin,Kaixin Li,Rabiul Awal,Xing Han Lù,Johan Obando-Ceron,Juan A. Rodriguez,Nicolas Chapados,David Vazquez,Adriana Romero-Soriano,Reihaneh Rabbany,Perouz Taslakian,Christopher Pal,Spandana Gella,Sai Rajeswar*

Main category: cs.LG

TL;DR: 提出了GroundCUA桌面环境数据集和GroundNext模型，通过高质量专家演示数据实现了桌面UI元素的精准定位，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决桌面环境中自然语言指令与屏幕元素准确连接的问题，现有高质量桌面交互数据资源有限。

Method: 构建大规模桌面数据集GroundCUA，包含87个应用的56K截图和3.56M人工标注；开发GroundNext模型系列，使用监督微调和强化学习后训练。

Result: GroundNext在3B和7B规模下在五个基准测试中达到SOTA，仅需不到先前工作十分之一的训练数据；在OSWorld基准测试中与使用更多数据训练的模型性能相当或更优。

Conclusion: 高质量专家驱动数据集对推进通用计算机使用智能体发展具有关键作用。

Abstract: Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents.

</details>


### [66] [Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection](https://arxiv.org/abs/2511.07364)
*Vaibhav Mavi,Shubh Jaroria,Weiqi Sun*

Main category: cs.LG

TL;DR: 本文研究了多步推理任务中LLM的可靠性评估，比较了整体评分和逐步评分两种方法，发现逐步评估在错误检测方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单步输出，忽视了多步推理的挑战，需要开发有效的多步任务可靠性评估方法。

Method: 使用两种直观方法：整体评分和逐步评分，在两个多步基准数据集上进行测试。

Result: 逐步评估在检测潜在错误方面普遍优于整体评分，AUC-ROC相对提升高达15%。

Conclusion: 自评估LLM系统在复杂推理中能提供有意义的置信度估计，提高了可信度并提供了实用的失败检测框架。

Abstract: Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.

</details>


### [67] [Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training](https://arxiv.org/abs/2511.07328)
*Artyom Sorokin,Nazar Buzun,Alexander Anokhin,Oleg Inozemcev,Egor Vedernikov,Petr Anokhin,Mikhail Burtsev,Trushkov Alexey,Yin Wenshuai,Evgeny Burnaev*

Main category: cs.LG

TL;DR: Q-RAG是一种使用强化学习微调嵌入模型的多步检索方法，在长上下文基准测试中达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要关注单步检索，难以回答需要多步搜索的复杂问题。现有的多步检索方法需要微调小LLM，资源消耗大且无法使用大模型

Method: 提出Q-RAG方法，使用强化学习微调嵌入模型进行多步检索

Result: 在Babilong和RULER长上下文基准测试（最多1000万tokens）上达到最先进结果

Conclusion: Q-RAG为多步检索提供了一种竞争性强且资源高效的替代方案

Abstract: Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.

</details>


### [68] [C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning](https://arxiv.org/abs/2511.07396)
*Antonios Valkanas,Soumyasundar Pal,Pavel Rumiantsev,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: C3PO是一个自监督的LLM级联优化框架，通过概率成本约束和遗憾最小化来优化推理成本，无需标注数据即可构建级联系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理成本高，阻碍实际部署。级联推理是解决方案，但现有方法需要监督训练、缺乏理论保证且成本控制有限。

Method: 使用自监督框架，基于未标注模型输出构建级联，通过符合预测控制成本超出概率，优化对最强模型的遗憾最小化。

Result: 在GSM8K、MATH-500等推理基准上达到最先进性能，在准确性和成本效率上均优于基线方法。

Conclusion: 原则性的无标签级联优化可以实现可扩展的LLM部署。

Abstract: Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost. We introduce C3PO (Cost Controlled Cascaded Prediction Optimization), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget. We provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [69] [A technical guide to building agentic systems](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fbuilding-agentic-ai%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-building-agentic-ai%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/2/0100019a5ead334c-d6042afd-8226-4caf-8fe8-9188c90bdb1a-000000/AojfMDiPr9G3tRczSlcjksS54GioQ5w0NKbRAfmKWcA=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文详细介绍了如何构建能够执行实际任务的智能代理系统，包括查询数据库、更新系统和做出决策等功能。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统主要停留在聊天层面，需要发展能够实际执行任务的智能代理，但每个工具的集成都需要定制代码，这增加了开发复杂性。

Method: 使用模型上下文协议（MCP）来构建智能代理，使其能够搜索索引、提取分析数据并修改配置。

Result: 提供了一套详细的技术指南，展示了如何构建功能完善的智能代理系统。

Conclusion: 智能代理是AI发展的下一个重要阶段，通过MCP协议可以简化集成过程，实现更强大的功能。

Abstract: A technical guide to building agentic systems (Sponsor) The next evolution in AI isn't better chat, it's agents that can actually do things: query databases, update systems, and make decisions. But connecting AI to every tool means custom code for each integration.Algolia's new whitepaper breaks it down — in great detail but with zero BS: How agentic AI works What model context protocol (MCP) enables How to build agents that can search indices, pull analytics, and modify configurations throug...

</details>


### [70] [Still struggling to get an AI agent working in production?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.stack-ai.com%2F%3Futm_source=newsletter%26utm_medium=sponsorship%26utm_campaign=112025-tldr-lp/1/0100019a5ead334c-d6042afd-8226-4caf-8fe8-9188c90bdb1a-000000/i9N7n4qaIoWFagUVJ3lEHISdUPV_NkbK1gPYGa47vA0=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: StackAI是一个无需代码的企业AI代理构建平台，提供白手套支持服务


<details>
  <summary>Details</summary>
Motivation: 解决在生产环境中部署AI代理的困难，为企业提供可靠的AI解决方案

Method: 提供无需代码的AI代理构建平台，包含白手套支持服务

Result: 被Morgan & Morgan、Nubank、BAE Systems、Insurama、Monzo等知名企业信任使用

Conclusion: StackAI是一个实际可行的AI解决方案，能够帮助企业成功部署AI代理

Abstract: Still struggling to get an AI agent working in production? (Sponsor) StackAI is the platform for building no-code enterprise AI agents, white-glove support included. Trusted by Morgan & Morgan, Nubank, BAE Systems, Insurama, Monzo...Try the AI solution that actually works.

</details>


### [71] [Beyond Commands: The Terminal of the Future](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2F%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=brandv2_11_10_primary%26utm_content=tldr/2/0100019a6d815a71-031ec7b9-9e35-4184-aaad-c6a71d590b3d-000000/Firlx_DyvT9eLfEpANGGpCbSYgdedjh5LHGqYb-5qeI=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Warp将终端和IDE融合，内置AI代理，让开发者无需离开平台即可编辑文件、审查差异和部署代码，在Terminal-Bench基准测试中表现优于Claude Code和Gemini CLI。


<details>
  <summary>Details</summary>
Motivation: 传统终端工具功能有限，开发者需要在不同工具间切换，Warp旨在通过融合终端和IDE功能，并集成AI代理来提升开发效率。

Method: 开发一个集成了终端、IDE功能和AI代理的平台，支持文件编辑、差异审查、代码部署等操作，并内置AI助手帮助调试、日志分析和代码库导航。

Result: Warp已被超过60万开发者使用，在Terminal-Bench基准测试中排名领先于Claude Code和Gemini CLI。

Conclusion: Warp通过融合终端和IDE功能并集成AI代理，为开发者提供了更高效的一体化开发体验。

Abstract: Beyond Commands: The Terminal of the Future (Sponsor) Warp fuses the terminal and IDE into one place, with AI agents built in. Edit files, review diffs, and ship code, all without leaving the platform that is trusted by over 600k developers and ranks ahead of Claude Code and Gemini CLI on Terminal-Bench.Ask Warp agents to:Debug your Docker build errorsSummarize user logs from the last 24 hoursOnboard you to a new part of your codebaseDownload Warp for free and get bonus credits for your first...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [72] [LLMs as Packagers of HPC Software](https://arxiv.org/abs/2511.05626)
*Caetano Melone,Daniel Nichols,Konstantinos Parasyris,Todd Gamblin,Harshitha Menon*

Main category: cs.SE

TL;DR: SpackIt框架通过结合仓库分析、相关示例检索和基于诊断反馈的迭代优化，显著提升了LLM生成Spack软件包配方的成功率，从零样本的20%提高到80%以上。


<details>
  <summary>Details</summary>
Motivation: HPC软件生态系统日益复杂，维护和创建Spack软件包配方变得愈发耗时费力，需要自动化工具来辅助这一过程。

Method: 开发了SpackIt端到端框架，包括仓库分析、相关示例检索和基于诊断反馈的迭代优化方法。

Result: 在308个开源HPC软件包上的测试显示，SpackIt将安装成功率从零样本的20%提升到最佳配置下的80%以上。

Conclusion: 检索和结构化反馈对于可靠的软件包合成具有重要价值，能够显著提升LLM生成Spack配方的效果。

Abstract: High performance computing (HPC) software ecosystems are inherently heterogeneous, comprising scientific applications that depend on hundreds of external packages, each with distinct build systems, options, and dependency constraints. Tools such as Spack automate dependency resolution and environment management, but their effectiveness relies on manually written build recipes. As these ecosystems grow, maintaining existing specifications and creating new ones becomes increasingly labor-intensive. While large language models (LLMs) have shown promise in code generation, automatically producing correct and maintainable Spack recipes remains a significant challenge. We present a systematic analysis of how LLMs and context-augmentation methods can assist in the generation of Spack recipes. To this end, we introduce SpackIt, an end-to-end framework that combines repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. We apply SpackIt to a representative subset of 308 open-source HPC packages to assess its effectiveness and limitations. Our results show that SpackIt increases installation success from 20% in a zero-shot setting to over 80% in its best configuration, demonstrating the value of retrieval and structured feedback for reliable package synthesis.

</details>


### [73] [An Empirical Study of Java Code Improvements Based on Stack Overflow Answer Edits](https://arxiv.org/abs/2511.05813)
*In-on Wiratsin,Chaiyong Ragkhitwetsagul,Matheus Paixao,Denis De Sousa,Pongpop Lapvikai,Peter Haddawy*

Main category: cs.SE

TL;DR: 该论文通过分析Stack Overflow Java回答的编辑历史，识别开源项目中过时或未优化的代码，并提出改进建议。研究发现49.24%的代码片段可应用于开源项目，且基于这些编辑的bug修复建议有30.56%被项目维护者接受。


<details>
  <summary>Details</summary>
Motivation: 软件开发中普遍存在次优代码，导致高昂的维护成本和技术债务。开发者常依赖Stack Overflow等知识库，但其内容不断演变，需要系统分析这些编辑如何帮助改进实际项目代码。

Method: 使用改进的代码克隆搜索工具分析SO Java回答的版本历史，应用于10,668个GitHub Java项目。手动分类SO回答编辑，并创建pull请求向开源项目建议代码改进。

Result: 6.91%的SO Java接受答案有多个修订版本（平均2.82个）。49.24%的代码片段可应用于开源项目，36个基于编辑的bug修复建议中有11个被项目维护者接受。

Conclusion: SO回答的编辑历史是识别和改进次优代码的宝贵资源，可有效帮助减少开源项目中的技术债务。

Abstract: Suboptimal code is prevalent in software systems. Developers often write low-quality code due to factors like technical knowledge gaps, insufficient experience, time pressure, management decisions, or personal factors. Once integrated, the accumulation of this suboptimal code leads to significant maintenance costs and technical debt.
  Developers frequently consult external knowledge bases, such as API documentation and Q&A websites like Stack Overflow (SO), to aid their programming tasks. SO's crowdsourced, collaborative nature has created a vast repository of programming knowledge. Its community-curated content is constantly evolving, with new answers posted or existing ones edited.
  In this paper, we present an empirical study of SO Java answer edits and their application to improving code in open-source projects. We use a modified code clone search tool to analyze SO code snippets with version history and apply it to open-source Java projects. This identifies outdated or unoptimized code and suggests improved alternatives. Analyzing 140,840 Java accepted answers from SOTorrent and 10,668 GitHub Java projects, we manually categorized SO answer edits and created pull requests to open-source projects with the suggested code improvements. Our results show that 6.91% of SO Java accepted answers have more than one revision (average of 2.82). Moreover, 49.24% of the code snippets in the answer edits are applicable to open-source projects, and 11 out of 36 proposed bug fixes based on these edits were accepted by the GitHub project maintainers.

</details>


### [74] [SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?](https://arxiv.org/abs/2511.06090)
*Jeffrey Jian Ma,Milad Hashemi,Amir Yazdanbakhsh,Kevin Swersky,Ofir Press,Enhui Li,Vijay Janapa Reddi,Parthasarathy Ranganathan*

Main category: cs.SE

TL;DR: SWE-fficiency是一个用于评估仓库级性能优化的基准测试，包含498个真实数据科学、机器学习和HPC仓库的任务。相比传统基准强调"修复什么"，该基准关注"如何修复"，要求智能体分析代码语义、定位瓶颈并生成优化补丁。


<details>
  <summary>Details</summary>
Motivation: 现有基准大多关注修复什么代码，而忽略了如何修复代码的性能问题。大型软件仓库的性能优化需要代码推理和软件工程专业知识，但缺乏专门的评估基准。

Method: 通过自动化流水线从GitHub拉取请求中收集性能优化编辑，结合关键词过滤、静态分析、覆盖率工具和执行验证，确认专家加速基准并识别相关单元测试。

Result: 对最先进智能体的实证评估显示显著性能不足，平均仅达到专家加速的0.15倍。智能体在定位优化机会、跨函数执行推理和保持编辑正确性方面存在困难。

Conclusion: 该基准揭示了当前智能体在性能优化任务上的局限性，为自动化性能工程和长视野软件推理研究提供了重要工具。

Abstract: Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce \textsc{SWE-fficiency}, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.

</details>


### [75] [Assertion-Aware Test Code Summarization with Large Language Models](https://arxiv.org/abs/2511.06227)
*Anamul Haque Mollah,Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文提出了一个包含91个真实Java测试用例的新基准，通过消融研究探索了测试代码相关组件（如被测方法、断言信息和断言语义）对LLM生成测试摘要性能的影响。


<details>
  <summary>Details</summary>
Motivation: 单元测试通常缺乏简洁的摘要来传达测试意图，特别是在自动生成或文档不完善的代码库中。LLM提供了一个有前景的解决方案，但其效果很大程度上取决于提示方式。

Method: 评估了四个代码LLM（Codex、Codestral、DeepSeek和Qwen-Coder）在七种提示配置下的表现，使用n-gram指标（BLEU、ROUGE-L、METEOR）、语义相似度（BERTScore）和基于LLM的评估方法。

Result: 结果表明，使用断言语义进行提示比完整MUT上下文平均提高摘要质量0.10分（2.3%），同时需要更少的输入token。Codex和Qwen-Coder与人工编写摘要的对齐度最高，而DeepSeek尽管词汇重叠度高但表现不佳。

Conclusion: 断言语义是生成高质量测试摘要的关键因素，能够在减少输入token的同时提高摘要质量。

Abstract: Unit tests often lack concise summaries that convey test intent, especially in auto-generated or poorly documented codebases. Large Language Models (LLMs) offer a promising solution, but their effectiveness depends heavily on how they are prompted. Unlike generic code summarization, test-code summarization poses distinct challenges because test methods validate expected behavior through assertions rather than im- plementing functionality. This paper presents a new benchmark of 91 real-world Java test cases paired with developer-written summaries and conducts a controlled ablation study to investigate how test code-related components-such as the method under test (MUT), assertion messages, and assertion semantics-affect the performance of LLM-generated test summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU, ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation. Results show that prompting with as- sertion semantics improves summary quality by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while requiring fewer input tokens. Codex and Qwen-Coder achieve the highest alignment with human-written summaries, while DeepSeek underperforms despite high lexical overlap. The replication package is publicly available at https://doi.org/10. 5281/zenodo.17067550

</details>


### [76] [WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation](https://arxiv.org/abs/2511.06251)
*Mingde Xu,Zhen Yang,Wenyi Hong,Lihang Pan,Xinyue Fan,Yan Wang,Xiaotao Gu,Bin Xu,Jie Tang*

Main category: cs.SE

TL;DR: WebVIA是首个用于交互式UI到代码生成和验证的智能体框架，包含探索智能体、UI2Code模型和验证模块，显著提升了交互式代码生成的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型只能生成静态HTML/CSS/JavaScript布局，缺乏交互性，而UI开发需要将设计稿转换为功能代码，这个过程仍然重复且劳动密集。

Method: 提出WebVIA框架，包含三个组件：1）探索智能体捕获多状态UI截图；2）UI2Code模型生成可执行的交互代码；3）验证模块验证交互性。

Result: WebVIA智能体比通用智能体（如Gemini-2.5-Pro）实现了更稳定和准确的UI探索。微调的WebVIA-UI2Code模型在生成可执行交互代码方面显著优于基础模型。

Conclusion: WebVIA框架有效解决了交互式UI到代码生成的问题，在交互性和静态UI2Code基准测试中都表现出色。

Abstract: User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at \href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\texttt{https://webvia.github.io}}.

</details>


### [77] [Automatically Identifying Solution-Related Content in Issue Report Discussions with Language Models](https://arxiv.org/abs/2511.06501)
*Antu Saha,Mehedi Sun,Oscar Chaparro*

Main category: cs.SE

TL;DR: 该论文提出使用语言模型作为监督分类器来自动识别软件问题报告中的解决方案相关内容，比较了嵌入、提示和微调三种方法在传统机器学习模型、预训练语言模型和大语言模型上的表现。


<details>
  <summary>Details</summary>
Motivation: 在软件维护过程中，开发人员需要从冗长的问题讨论中手动识别解决方案相关内容，这既困难又耗时。自动识别这些内容对于调查重新开放的问题、处理回归、重用解决方案和理解代码变更原因至关重要。

Method: 使用356个Mozilla Firefox问题创建数据集，训练和评估了6个传统机器学习模型、4个预训练语言模型和2个大语言模型，共68种配置。研究了嵌入、提示和微调三种应用方法。

Result: 结果显示：使用LLM嵌入的传统机器学习模型优于TF-IDF特征；提示方法表现不佳；微调的LLM达到最高性能（LLAMAft F1分数0.716）；最佳模型的集成进一步提升了结果（F1分数0.737）。模型在跨项目迁移时表现良好，少量项目特定数据可进一步提升效果。

Conclusion: 微调的大语言模型在解决方案识别任务中表现最佳，但误分类主要源于误导性线索或缺失上下文，需要上下文感知的分类器。这项工作支持软件维护、问题理解和解决方案重用。

Abstract: During issue resolution, software developers rely on issue reports to discuss solutions for defects, feature requests, and other changes. These discussions contain proposed solutions-from design changes to code implementations-as well as their evaluations. Locating solution-related content is essential for investigating reopened issues, addressing regressions, reusing solutions, and understanding code change rationale. Manually understanding long discussions to identify such content can be difficult and time-consuming.
  This paper automates solution identification using language models as supervised classifiers. We investigate three applications-embeddings, prompting, and fine-tuning-across three classifier types: traditional ML models (MLMs), pre-trained language models (PLMs), and large language models (LLMs). Using 356 Mozilla Firefox issues, we created a dataset to train and evaluate six MLMs, four PLMs, and two LLMs across 68 configurations.
  Results show that MLMs with LLM embeddings outperform TF-IDF features, prompting underperforms, and fine-tuned LLMs achieve the highest performance, with LLAMAft reaching 0.716 F1 score. Ensembles of the best models further improve results (0.737 F1). Misclassifications often arise from misleading clues or missing context, highlighting the need for context-aware classifiers. Models trained on Mozilla transfer to other projects, with a small amount of project-specific data, further enhancing results. This work supports software maintenance, issue understanding, and solution reuse.

</details>


### [78] [Benchmarking LLMs for Fine-Grained Code Review with Enriched Context in Practice](https://arxiv.org/abs/2511.07017)
*Ruida Hu,Xinchen Wang,Xin-Cheng Wen,Zhao Zhang,Bo Jiang,Pengfei Gao,Chao Peng,Cuiyun Gao*

Main category: cs.SE

TL;DR: ContextCRBench是一个高质量、上下文丰富的代码审查基准测试，解决了现有基准的三个主要限制：缺乏语义上下文、数据质量问题、粗粒度评估。通过多阶段数据过滤构建了67,910个上下文丰富的条目，支持三种评估场景，并在工业部署中证明有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码审查基准存在三个主要问题：缺乏语义上下文（只有代码差异没有问题描述）、数据质量问题（噪声样本多）、粗粒度评估（忽略细粒度行级推理）。这些问题降低了评估的可靠性。

Method: 构建管道包括：(1)原始数据爬取：从顶级仓库收集153.7K问题和拉取请求；(2)全面上下文提取：链接问题-PR对获取文本上下文，提取完整函数或类获取代码上下文；(3)多阶段数据过滤：结合基于规则和基于LLM的验证，移除过时、格式错误或低价值样本。

Result: 评估8个领先LLM显示，文本上下文比仅代码上下文带来更大性能提升，但当前LLM仍远未达到人类审查水平。在字节跳动部署后，ContextCRBench驱动自演进代码审查系统，性能提升61.98%。

Conclusion: ContextCRBench是一个稳健且具有工业实用性的基准测试，强调了上下文在代码审查中的重要性，并为LLM评估提供了更全面的框架。

Abstract: Code review is a cornerstone of software quality assurance, and recent advances in Large Language Models (LLMs) have shown promise in automating this process. However, existing benchmarks for LLM-based code review face three major limitations. (1) Lack of semantic context: most benchmarks provide only code diffs without textual information such as issue descriptions, which are crucial for understanding developer intent. (2) Data quality issues: without rigorous validation, many samples are noisy-e.g., reviews on outdated or irrelevant code-reducing evaluation reliability. (3) Coarse granularity: most benchmarks operate at the file or commit level, overlooking the fine-grained, line-level reasoning essential for precise review.
  We introduce ContextCRBench, a high-quality, context-rich benchmark for fine-grained LLM evaluation in code review. Our construction pipeline comprises: (1) Raw Data Crawling, collecting 153.7K issues and pull requests from top-tier repositories; (2) Comprehensive Context Extraction, linking issue-PR pairs for textual context and extracting the full surrounding function or class for code context; and (3) Multi-stage Data Filtering, combining rule-based and LLM-based validation to remove outdated, malformed, or low-value samples, resulting in 67,910 context-enriched entries.
  ContextCRBench supports three evaluation scenarios aligned with the review workflow: (1) hunk-level quality assessment, (2) line-level defect localization, and (3) line-level comment generation. Evaluating eight leading LLMs (four closed-source and four open-source) reveals that textual context yields greater performance gains than code context alone, while current LLMs remain far from human-level review ability. Deployed at ByteDance, ContextCRBench drives a self-evolving code review system, improving performance by 61.98% and demonstrating its robustness and industrial utility.

</details>


### [79] [Bridging the Prototype-Production Gap: A Multi-Agent System for Notebooks Transformation](https://arxiv.org/abs/2511.07257)
*Hanya Elhashemy,Youssef Lotfy,Yongjian Tang*

Main category: cs.SE

TL;DR: Codelevate是一个多智能体系统，能够自动将Jupyter笔记本转换为结构良好的Python代码仓库，解决原型到生产环境的转换问题。


<details>
  <summary>Details</summary>
Motivation: Jupyter笔记本在数据科学和机器学习工作流中广泛使用，但缺乏软件工程原则，使其难以过渡到生产环境。

Method: 使用三个专门智能体（架构师、开发者、结构师）通过共享依赖树协同工作，确保架构一致性和代码质量。

Result: 实验验证了Codelevate能够通过自主代码转换弥合原型到生产环境的差距，在保持计算语义的同时显著提升代码质量指标。

Conclusion: Codelevate成功解决了Jupyter笔记本向生产代码转换的挑战，为数据科学工作流提供了有效的工程化解决方案。

Abstract: The increasing adoption of Jupyter notebooks in data science and machine learning workflows has created a gap between exploratory code development and production-ready software systems. While notebooks excel at iterative development and visualization, they often lack proper software engineering principles, making their transition to production environments challenging. This paper presents Codelevate, a novel multi-agent system that automatically transforms Jupyter notebooks into well-structured, maintainable Python code repositories. Our system employs three specialized agents - Architect, Developer, and Structure - working in concert through a shared dependency tree to ensure architectural coherence and code quality. Our experimental results validate Codelevate's capability to bridge the prototype-to-production gap through autonomous code transformation, yielding quantifiable improvements in code quality metrics while preserving computational semantics.

</details>
