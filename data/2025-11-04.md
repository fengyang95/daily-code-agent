<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.AI](#cs.AI) [Total: 15]
- [tldr.article](#tldr.article) [Total: 13]
- [wechat.article](#wechat.article) [Total: 24]
- [cs.SE](#cs.SE) [Total: 23]
- [cs.LG](#cs.LG) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: PlotCraft是一个包含1000个挑战性可视化任务的新基准，涵盖金融、科学研究和社会学等多个领域，系统评估LLM在复杂数据可视化方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成方面表现出色，但在处理规模和结构化数据的复杂可视化任务方面能力尚未充分评估和发展。

Method: 开发了PlotCraft基准，包含7个高级可视化任务和48种图表类型；构建了SynthVis-30K大规模高质量数据集；开发了PlotCraftor代码生成模型。

Result: 对23个领先LLM的评估显示在复杂可视化任务上存在明显性能缺陷；PlotCraftor模型在VisEval、PandasPlotBench和PlotCraft基准上表现与领先专有方法相当，在困难任务上性能提升超过50%。

Conclusion: PlotCraft基准揭示了LLM在复杂可视化方面的局限性，PlotCraftor模型通过专门训练在小规模下实现了强大的复杂数据可视化能力。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 提出WordSaladChopper(WSC)组件，通过检测和删除大型推理模型中的无用自我重复token来显著减少输出长度，同时保持推理质量


<details>
  <summary>Details</summary>
Motivation: 大型推理模型输出token成本高昂，其中大量是无用的自我重复token（称为'word salad'），这些token消耗解码预算但不增加价值

Method: 利用模型对<\n\n>标记的隐藏状态模式，通过单层线性分类器实时检测word salad行为，检测后通过简单截断和重新生成提示来节省长度

Result: WSC组件能够显著减少输出长度，同时质量损失最小，为LRM应用提供了轻量级、即插即用的解决方案

Conclusion: WSC或类似组件是所有考虑用户体验的LRM应用必备组件，因其开销低、节省显著且word salad token缺乏语义价值

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [3] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 提出ISA攻击方法，通过意图转换使有害请求被LLM误解为良性信息请求，相比直接有害提示攻击成功率提升70%以上，现有防御方法对其无效


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要通过添加干扰上下文或对抗性token来分散LLM注意力，但未改变核心有害意图。需要研究更隐蔽的攻击方式以增强LLM安全性

Method: 建立意图转换分类法，对原始请求进行最小编辑，生成自然、人类可读且看似无害的提示，使LLM误判攻击意图

Result: 在开源和商业LLM上，ISA攻击成功率相比直接有害提示提升70%以上；仅使用ISA模板重新表述的良性数据微调模型可使成功率接近100%

Conclusion: LLM在意图推断方面存在根本性安全挑战，现有防御方法对ISA攻击无效，需要更有效的防御策略

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [4] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR是一个无需外部监督的自学习推理框架，通过让LLM在教师和学生角色间切换来生成和解决新颖问题，实现开放式的数学发现。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理方法依赖带注释的数据集进行可验证奖励，这限制了模型超越人类水平的能力。自学习提供了有希望的替代方案，但现有方法需要外部验证器或无法进行开放式学习。

Method: OpenSIR框架让LLM交替扮演教师和学生角色：教师生成新颖问题（优化难度和多样性），学生解决问题，无需外部监督。

Result: 从单个简单种子问题开始，OpenSIR显著提升了指令模型性能：Llama-3.2-3B-Instruct在GSM8K上从73.9提升到78.3，在College Math上从28.8提升到34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提升到58.7。

Conclusion: OpenSIR通过共同进化的教师-学生角色实现开放式学习，自适应地校准难度并推动多样化探索，能够自主地从基础数学发展到高级数学。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [5] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 提出了一种评估LLM推理过程中知识基础的新框架，包含大规模知识库、知识基础评估指标和轻量级评估模型，能有效识别推理中的知识缺失或误用问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM分步推理中如何验证推理是否准确基于知识的问题，评估中间推理的知识基础性。

Method: 构建包含三个关键组件的评估套件：(1)主要知识收集库，(2)知识基础评估指标，(3)轻量级评估LLM用于计算指标。

Result: 评估套件能有效识别推理中缺失或误用的知识元素，揭示LLM的基本推理缺陷。

Conclusion: 知识基础评估不仅可用于评估，还可集成到偏好优化中，具有更广泛的应用价值。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [6] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL是一个多代理框架，通过任务分解和交互式强化学习解决复杂自然语言到SQL的翻译问题，在BIRD和Spider数据集上取得了最先进的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂自然语言查询到SQL翻译的困难，这些查询通常需要环境交互和自我修正。

Method: 采用三代理框架：接地代理进行模式链接，生成代理通过多轮强化学习策略生成查询，验证代理进行最终选择。生成代理使用ReAct风格的思考-行动-观察循环进行迭代推理。

Result: 在BIRD开发集上达到77.84%的执行准确率，在Spider测试集上达到89.75%的执行准确率，实现了最先进的性能。

Conclusion: 结构化工作流程结合交互式强化学习和生成建模验证，为稳健准确的SQL生成提供了高效方法。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [7] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 提出了MicroRemed基准测试和ThinkRemed多智能体框架，用于评估LLM在端到端微服务修复中的能力，直接根据诊断报告生成可执行的Ansible剧本。


<details>
  <summary>Details</summary>
Motivation: 现有方法仍依赖SRE手工编写提示，LLM仅将文本指令转换为可执行代码，需要推进LLM在微服务修复领域的自主决策能力研究。

Method: 引入MicroRemed基准测试评估LLM端到端修复能力，提出ThinkRemed多智能体框架模拟SRE的反思和感知推理过程。

Result: 实验结果显示MicroRemed对当前LLM构成重大挑战，而ThinkRemed通过迭代推理和系统反思提高了端到端修复性能。

Conclusion: 该研究为LLM在微服务修复领域的应用提供了首个基准测试，多智能体框架显著提升了修复效果。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [8] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了IF-CRITIC，一个能够高效可靠评估指令约束遵循的LLM批评器，通过约束清单生成和多阶段筛选机制训练，在指令遵循评估上超越现有LLM-as-a-Judge基线。


<details>
  <summary>Details</summary>
Motivation: 现有指令遵循评估模型存在成本高和评估不可靠的问题，需要开发更高效可靠的评估方法。

Method: 开发约束清单生成器分解指令，通过多阶段筛选机制收集高质量批评数据，采用约束级偏好优化方法训练IF-CRITIC。

Result: IF-CRITIC在评估性能上超越了Deepseek-R1和o4-mini等强基线，能以更低计算开销为LLM提供可扩展的奖励信号。

Conclusion: IF-CRITIC能够提供高效可靠的指令遵循评估，显著提升LLM在指令遵循优化中的性能。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [9] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个端到端的强化学习框架，使用小型LLM与大型LLM协作，通过多轮提示交互来优化复杂问题的解决效果。


<details>
  <summary>Details</summary>
Motivation: 当前用户难以提供准确有效的提示来与大型语言模型交互，限制了LLM的性能发挥。

Method: 采用小型LLM思考生成提示，大型LLM进行复杂推理的多轮交互模式，设计双约束奖励机制优化正确性、生成质量和推理准确性。

Result: 在多个公共数据集上的实验表明，Prompt-R1显著优于基线模型。

Conclusion: Prompt-R1提供了一个即插即用的框架，支持与各种大型LLM的推理和训练，有效提升了问题解决能力。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [10] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 开发了一个基于语言模型的停止代理，用于优化销售对话中的动态筛选决策，显著减少失败通话时间同时保持销售业绩


<details>
  <summary>Details</summary>
Motivation: 销售人员面临何时放弃对话转向下一个潜在客户的动态筛选决策，但现有研究对这些决策的效率和改进方法了解甚少

Method: 将动态筛选决策形式化为最优停止问题，开发基于生成语言模型的序列决策代理，通过模仿事后推断的最优停止策略来学习何时退出对话

Result: 应用于欧洲电信公司的通话数据，停止代理将失败通话时间减少54%，同时几乎保持所有销售；重新分配节省的时间可使预期销售增加高达37%

Conclusion: AI算法有潜力纠正认知受限的人类决策，提高销售团队效率；销售人员倾向于过度重视少数明显的客户不感兴趣表达，错误预测通话失败风险

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [11] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 本文提出了一种改进LLM输出事实性的方法，通过将自然语言推理(NLI)模型推广到文本前缀级别，训练了专门的MiniTruePrefixes模型，在受控解码框架中显著提高了摘要生成的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有NLI模型主要用于完整句子的推理判断，但自回归生成架构在解码过程中需要对文本前缀进行决策，因此需要专门针对文本前缀的推理模型来改进生成的事实性。

Method: 将蕴含检测任务推广到任意文本前缀，提供相应的评估和训练数据集，训练专门的MiniTruePrefixes模型，并将其集成到受控解码框架中。

Result: MiniTruePrefixes在前缀级蕴含检测上比基线NLI模型高出5-14个F1点；在摘要生成中，LLaMA-3.2-3B-Instruct在MiniTruePrefixes指导下能达到与8B模型相当的事实性和运行时性能，同时仅使用一半内存。

Conclusion: 针对文本前缀的专门推理模型能有效提高LLM生成的事实一致性，在保持性能的同时显著减少计算资源需求。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [12] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 提出BARD框架，通过两阶段训练在蒸馏推理能力的同时实现推理长度的细粒度控制，让8B学生模型在保持性能的同时精确控制计算预算。


<details>
  <summary>Details</summary>
Motivation: 解决传统CoT蒸馏中推理过程冗余且计算预算不可控的问题，实现推理性能与计算效率的动态平衡。

Method: 两阶段训练：第一阶段SFT在教师生成的长CoT数据上进行预算约束理解；第二阶段RL同时优化推理性能和预算保真度。

Result: 8B学生模型在AIME24、AIME25、GPQA等推理基准上表现强劲，并能跨广泛预算范围精确自适应控制推理长度。

Conclusion: BARD框架成功实现了推理能力蒸馏与计算预算控制的统一，为高效推理模型提供了新思路。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [13] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 提出了一种新的rank-2投影子空间方法，用于更准确地区分大型语言模型中参数知识(PK)和上下文知识(CK)的贡献，并首次对长自然语言解释序列进行多步知识交互分析。


<details>
  <summary>Details</summary>
Motivation: 理解参数知识和上下文知识在大型语言模型决策中的交互对于评估自然语言解释的可靠性至关重要，但现有研究对此探索不足，主要关注单步生成和二元交互模型。

Method: 使用rank-2投影子空间更精确地解耦PK和CK贡献，在四个QA数据集和三个开源指令调优LLM上进行多步知识交互分析。

Result: 实验表明，rank-1子空间无法有效表示多样化的知识交互，而rank-2方法能有效捕捉；多步分析显示幻觉NLE与PK方向强相关，上下文忠实NLE平衡PK和CK，思维链提示通过减少PK依赖将NLE转向CK。

Conclusion: 这项工作为通过更丰富的rank-2子空间解耦系统研究LLM中多步知识交互提供了首个框架。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [14] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 语言模型在长时间对话和阅读过程中会经历信念漂移，导致其世界观和行为发生变化，这可能影响模型的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在自主应用中的广泛使用，上下文积累可能导致模型信念配置文件发生无声变化，带来潜在风险。

Method: 通过设计道德困境讨论、政治立场阅读和工具使用任务，评估模型在累积上下文后的信念和行为变化。

Result: GPT-5在10轮道德讨论后信念变化达54.7%，Grok 4在阅读对立政治文本后信念变化达27.2%，行为变化与信念漂移一致。

Conclusion: 模型在长时间对话和阅读过程中会出现显著的信念漂移，这会影响其意见和行为的可靠性。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [15] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出了Tool-to-Agent Retrieval框架，通过在共享向量空间中嵌入工具及其父代理，并利用元数据关系连接它们，实现了细粒度的工具级或代理级检索，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法通常将查询与粗粒度的代理级描述进行匹配，这会掩盖细粒度的工具功能，导致代理选择不理想。

Method: 在共享向量空间中嵌入工具和父代理，通过元数据关系连接它们，支持细粒度的工具级或代理级检索。

Result: 在LiveMCPBench基准测试中，使用8种嵌入模型，Recall@5提升了19.4%，nDCG@5提升了17.7%，优于现有最先进的代理检索方法。

Conclusion: Tool-to-Agent Retrieval框架通过显式表示工具能力并遍历元数据到代理级别，确保了代理及其底层工具或MCP服务器的平等表示，避免了将多个工具分块在一起时出现的上下文稀释问题。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: Engineering.ai是一个基于多智能体架构的AI工程师平台，通过首席工程师协调空气动力学、结构、声学和优化等专业智能体，实现自主的跨学科工程设计。


<details>
  <summary>Details</summary>
Motivation: 现代工程设计需要多学科专家团队协作，耗费大量时间和成本。该研究旨在开发能够自主完成复杂工程任务的AI工程师团队。

Method: 采用分层多智能体架构，首席工程师协调专业智能体，通过文件通信实现数据溯源和可复现性，集成多种工程软件工具进行并行多学科仿真。

Result: 在无人机机翼优化案例中，系统在400多个参数配置中实现了100%成功率，无网格生成失败、求解器收敛问题或人工干预。

Conclusion: 基于智能体的AI工程师能够自主执行复杂工程任务，验证了该框架的可靠性和可信性。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [17] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: 本文介绍了ARC-GEN，一个开源程序生成器，旨在扩展ARC-AGI训练数据集，以解决原始数据集样本数量有限的问题。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准测试评估技能获取效率，但每个任务的演示样本数量有限，限制了需要大量样本的算法的性能。

Method: 开发ARC-GEN程序生成器，全面覆盖400个任务，并尽可能忠实于原始ARC-AGI-1发布的分布特性和特征。

Result: 创建了一个扩展的训练数据集，更准确地模拟原始ARC-AGI的特性分布。

Conclusion: ARC-GEN为ARC-AGI基准测试提供了更丰富的训练数据，并可用于验证2025年Google Code Golf Championship提交程序的正确性。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [18] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统的电信网络故障自动排查框架，利用LLM协调多个专业工具，通过微调小型语言模型生成基于内部文档的修复方案，显著加速RAN和核心网络故障排查自动化。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂度增加，现有AI模型范围狭窄、需要大量标注数据且难以在异构部署中泛化，网络故障排查仍严重依赖专家手动分析。

Method: 采用多智能体系统，当AI/ML监控检测到故障时，动态激活编排器、解决方案规划器、执行器、数据检索器和根因分析器等智能体进行诊断和修复。关键组件是解决方案规划器，通过微调小型语言模型基于内部文档生成修复计划。

Result: 实验结果表明，该框架显著加速了RAN和核心网络领域的故障排查自动化。

Conclusion: 多智能体系统结合LLM协调和微调SLM的方法，能够有效实现电信网络故障的自动化排查和修复。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [19] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 提出了Ariadne框架，使用合成迷宫进行多步空间推理，通过强化学习后训练显著提升了视觉语言模型在视觉中心空间任务上的能力，从基模型的0%准确率提升到50%以上，并在真实世界基准测试中实现了零样本泛化改进。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型通过强化学习后训练是否能够真正扩展其能力边界，特别是在基模型最初失败的视觉中心空间任务上。

Method: 使用合成迷宫创建可控难度的多步空间推理环境，采用强化学习与验证奖励机制进行难度感知课程训练。

Result: 后训练后模型在基模型得分为0%的问题集上达到50%以上准确率，在MapBench和ReasonMap基准测试中分别实现16%和24%的零样本改进。

Conclusion: 该方法不仅扩展了模型的基本能力限制，还增强了其在真实世界空间推理中的泛化能力，为专门化能力扩展对齐研究提供了动力。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [20] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 本文从CPU角度分析智能体AI工作负载的系统瓶颈，发现工具处理在CPU上占用高达90.6%的总延迟，并提出CPU和GPU感知的微批处理及混合工作负载调度优化方案，实现最高2.1倍的延迟加速。


<details>
  <summary>Details</summary>
Motivation: 研究智能体AI工作负载中常被忽视的CPU中心视角下的系统瓶颈问题，揭示CPU在智能体AI性能中的关键作用。

Method: 首先系统化表征智能体AI的工作特性，然后选择五个代表性工作负载进行性能分析，最后提出CPU和GPU感知的微批处理及混合工作负载调度优化方案。

Result: 发现工具处理在CPU上占用高达90.6%的总延迟，CPU动态能耗占总能耗的44%，并提出优化方案实现最高2.1倍的P50延迟加速。

Conclusion: CPU在智能体AI系统中扮演关键角色，通过针对性的优化策略可以显著提升系统性能和效率。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [21] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 比较基于计数的模型、预训练序列变换器和混合代理LLM管道在结构化电子健康记录预测任务上的性能，发现基于计数的方法和混合代理方法表现相当，但基于计数的方法更简单且可解释性更强。


<details>
  <summary>Details</summary>
Motivation: 结构化电子健康记录对临床预测至关重要，但缺乏对基于计数的学习器与新兴混合代理LLM管道的直接基准比较，后者在NLP任务中表现出优于单一LLM的性能。

Method: 使用EHRSHOT数据集评估三类方法：基于计数的模型（LightGBM和TabPFN）、预训练序列变换器（CLMBR）和混合代理管道（将表格历史转换为自然语言摘要后使用文本分类器）。

Result: 在八个评估任务中，基于计数的方法和混合代理方法的胜率大致相当。

Conclusion: 基于计数的模型因其简单性和可解释性，仍然是结构化电子健康记录基准测试的有力候选方法。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [22] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 该研究首次将RLVR LLM训练应用于公共交通运营中的实时预测挑战，通过引入基于容差的奖励函数，在NYC MTA服务警报数据集上实现了比传统方法更好的5分钟预测精度。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，标准监督微调难以处理领域稀疏性和噪声连续标签问题，而RLVR在数学等二元正确性任务中表现出色，但其在噪声连续预测中的适用性仍是开放问题。

Method: 通过引入基于容差的成形奖励函数，在连续误差范围内给予部分信用，而不是要求单一正确答案，将RLVR适应于该任务，并在NYC MTA服务警报数据集上进行系统评估。

Result: 通用指令调优LLM显著优于专业数学推理模型，成形奖励设计至关重要，RLVR方法在最具挑战性的指标上表现优异，相比最强基线在5分钟准确率上实现了35%的相对改进。

Conclusion: RLVR可以成功适应现实世界中的噪声预测任务，但需要设计反映问题连续性质的验证器。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [23] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 提出了一种双智能体框架，通过LLM旅行者智能体和校准智能体的协作，实现与人类旅行者在学习适应行为上的持续对齐，显著提升了交通行为模拟的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 准确模拟人类旅行者如何从与交通系统交互中学习和调整行为对系统评估和规划至关重要，但由于复杂认知和决策过程，这一任务具有挑战性。

Method: 使用配备记忆系统和可学习角色的LLM旅行者智能体模拟人类旅行者，引入LLM校准智能体利用推理分析能力训练角色，形成双智能体系统跟踪对齐决策机制。

Result: 在真实世界日常路线选择实验数据集上，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有LLM方法，并能捕捉底层学习过程的演化。

Conclusion: 该框架为创建适应性强、行为真实的智能体来模拟旅行者学习和适应提供了新方法，有助于交通模拟和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [24] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行问题


<details>
  <summary>Details</summary>
Motivation: 解决单一智能体在复杂任务执行中任务分解和协作的局限性

Method: 使用大语言模型将自然语言任务转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，采用动态调度和路由机制实现智能体间的合理分工和实时协作

Result: 在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度上验证，该方法在整体性能和鲁棒性方面优于现有方法

Conclusion: 证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境中的任务执行提供了系统化解决方案

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [25] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 本文分析了将Shapley值应用于大型语言模型决策支持系统中的特征归因，探讨了在随机推理环境下Shapley值原则的满足情况及其权衡。


<details>
  <summary>Details</summary>
Motivation: 研究Shapley值在随机推理的LLM决策支持系统中的适用性，因为传统Shapley值假设确定性推理，而LLM推理本质上是随机的。

Method: 将Shapley值应用于LLM决策支持系统的特征归因，分析不同实现变体下Shapley值原则的满足情况，并研究随机性对保证的影响。

Result: 展示了在不同实现变体下Shapley值原则的满足条件，分析了LLM随机性对这些保证的影响，并揭示了可解释推理速度、与精确Shapley值归因的一致性以及原则达成之间的权衡。

Conclusion: 在LLM随机推理环境中，Shapley值原则的满足需要特定条件，存在速度、准确性和原则达成之间的重要权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [26] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 研究从AI角度探索双任务范式中的时间处理干扰，使用简化的Overcooked环境比较单任务(T)和双任务(T+N)下DRL智能体的时间产生行为，发现双任务智能体出现显著的时间过度产生现象。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习智能体在双任务情境下表现出的时间处理行为是否与人类研究一致，以及智能体内部的时间保持机制。

Method: 在简化的Overcooked环境中实现单任务(T，仅时间产生)和双任务(T+N，时间产生+数字比较)两种变体，分别训练两个DRL智能体，分析其行为表现和LSTM层的神经动力学。

Result: 双任务(T+N)智能体相对于单任务(T)智能体在四个目标持续时间上都表现出显著的时间过度产生，与人类计时研究一致；但LSTM层分析未发现明确的内置计时器证据。

Conclusion: 研究为理解DRL智能体涌现行为与生物系统行为之间的相似性迈出了一小步，需要进一步研究智能体的时间保持机制。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [27] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出一种交互式AI代理，通过可审计的行动序列生成解释，使用强化学习优化策略来寻找外部视觉证据支持诊断推理，显著提高校准准确性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，AI模型的解释往往缺乏可验证性，这会阻碍信任。需要构建具有可验证和忠实推理能力的AI系统。

Method: 使用强化学习优化策略，让代理战略性地寻求外部视觉证据来支持诊断推理，并通过因果干预方法验证解释的忠实性。

Result: 基于行动的推理过程显著提高了校准准确性，Brier分数比非交互基线降低了18%。通过屏蔽代理选择的视觉证据，观察到性能明显下降，证实证据对其决策过程至关重要。

Conclusion: 该工作为构建具有可验证和忠实推理能力的AI系统提供了实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [28] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: 本文介绍了TPS-Bench基准测试，用于评估LLM代理在需要工具规划与调度能力的复合任务中的表现。基准包含200个复合任务和数百个MCP工具，评估显示大多数模型能进行合理的工具规划但在调度策略上存在差异，同时探索了使用强化学习提升调度效率的方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在解决需要多种工具协作的复合现实世界问题时，其工具规划与调度能力尚未得到充分探索。需要评估代理在大型异构工具库中如何选择合适工具并战略性地安排执行顺序以确保效率。

Method: 构建TPS-Bench基准，包含200个复合任务和数百个MCP工具。每个任务由多个子任务组成，如网络搜索、地图导航、日历检查等。评估重点包括任务完成率和效率，并对主流闭源和开源LLM进行实证研究。

Result: 实证研究表明，大多数模型能进行合理的工具规划，但在调度策略上差异显著。GLM-4.5达到64.72%的任务完成率但执行时间较长，GPT-4o优先并行工具调用但完成率仅45.08%。使用强化学习在Qwen3-1.7B上实现了14%执行时间减少和6%任务完成率提升。

Conclusion: LLM代理在工具规划方面表现良好，但在调度效率上仍有改进空间。强化学习是提升调度效率而不牺牲性能的可行方法，即使使用少量训练样本也能显著改善性能。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [29] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 提出了一种混合法律问答代理，结合检索增强生成和多模型集成，为司法场景提供可靠、可审计且持续更新的法律咨询服务。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在法律问答中容易产生幻觉，而静态知识库难以跟上频繁更新的法规和判例，需要确保法律问答的真实性和可追溯性。

Method: 采用检索优先策略：当可信法律知识库有相关证据时使用RAG生成答案，否则由多个LLM生成候选答案并由专业选择器评分返回最佳答案。高质量输出经人工审核后写回知识库。

Result: 在Law_QA数据集上的实验表明，该方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流程。

Conclusion: 该方法有效减少幻觉，提高答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际落地。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [30] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LLM代理在需要深度推理的紧凑环境中表现出色，但在更广泛复杂的需要跨多种工具和模式鲁棒性的环境中仍然脆弱。本文提出了两个框架：Simia-SFT通过扩增小规模种子数据生成多样化轨迹，Simia-RL通过LLM模拟反馈实现无需真实环境的强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 构建定制化环境进行训练成本高、脆弱且限制进展，而LLM能够在不访问实际测试数据或API的情况下模拟真实环境反馈。

Method: 提出Simia-SFT和Simia-RL两个框架：Simia-SFT通过环境无关方式将小规模种子数据扩增为多样化轨迹；Simia-RL通过LLM模拟反馈实现无需真实环境实现的强化学习训练。

Result: 微调开源模型在多个基准测试中取得一致改进，在τ²-Bench上超越GPT-4o并接近o4-mini水平。

Conclusion: Simia-SFT和Simia-RL能够实现无需环境工程的可扩展代理训练，用灵活的基于LLM的模拟替代繁重脆弱的环境实现。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [31] [Roll up your sleeves to build the next wave of agentic AI at BUILD 2025](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Snowflake BUILD 2025是一个免费的虚拟开发者大会，专注于教授如何在Snowflake平台上构建可靠、生产就绪的智能体应用


<details>
  <summary>Details</summary>
Motivation: 帮助开发者学习如何基于Snowflake平台构建强大的AI智能体应用，将AI战略与数据战略相结合

Method: 通过虚拟开发者大会形式，提供Snowflake最新AI功能的深度讲解和开源用例展示

Result: 参与者将学习到如何构建基于数据的智能体应用，并实现安全的大规模部署

Conclusion: BUILD 2025为开发者提供了构建下一代智能体AI所需的技能和工具

Abstract: Roll up your sleeves to build the next wave of agentic AI at BUILD 2025 (Sponsor) You can't have an AI strategy without a data strategy. The most powerful agents are grounded in your data, connected across pipelines, and deployed securely at scale.Join BUILD (Nov 4-7), a free virtual developer conference where you'll learn how build reliable, production-ready agentic applications on Snowflake. Expect: Deep dives on the latest Snowflake AI capabilities and cool OSS use cases with LlamaIndex, C...

</details>


### [32] [Hands-on labs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Snowflake BUILD 2025是一个免费的虚拟开发者会议，专注于教授如何在Snowflake平台上构建可靠、生产就绪的智能代理应用。


<details>
  <summary>Details</summary>
Motivation: 帮助企业制定AI战略，强调数据策略的重要性，并展示如何基于Snowflake平台构建强大的智能代理系统。

Method: 通过虚拟开发者会议形式，提供关于Snowflake最新AI功能的深入讲解，以及使用LlamaIndex等开源工具的实用案例。

Result: 参与者将学习到如何构建基于数据的智能代理应用，这些应用能够在管道间连接并安全地大规模部署。

Conclusion: BUILD 2025会议为开发者提供了构建下一代智能代理AI所需的技能和工具。

Abstract: Roll up your sleeves to build the next wave of agentic AI at BUILD 2025 (Sponsor) You can't have an AI strategy without a data strategy. The most powerful agents are grounded in your data, connected across pipelines, and deployed securely at scale.Join BUILD (Nov 4-7), a free virtual developer conference where you'll learn how build reliable, production-ready agentic applications on Snowflake. Expect: Deep dives on the latest Snowflake AI capabilities and cool OSS use cases with LlamaIndex, C...

</details>


### [33] [Inspiration from industry leaders](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Snowflake BUILD 2025是一个免费的虚拟开发者会议，专注于教授如何在Snowflake平台上构建可靠、生产就绪的代理应用


<details>
  <summary>Details</summary>
Motivation: 帮助开发者学习如何基于Snowflake平台构建强大的AI代理应用，这些代理需要以数据为基础，跨管道连接并安全部署

Method: 通过虚拟会议形式，提供Snowflake最新AI功能的深度讲解和开源用例展示

Result: 开发者将能够构建基于数据的可靠代理应用

Conclusion: BUILD 2025为开发者提供了构建下一代代理AI所需的知识和工具

Abstract: Roll up your sleeves to build the next wave of agentic AI at BUILD 2025 (Sponsor) You can't have an AI strategy without a data strategy. The most powerful agents are grounded in your data, connected across pipelines, and deployed securely at scale.Join BUILD (Nov 4-7), a free virtual developer conference where you'll learn how build reliable, production-ready agentic applications on Snowflake. Expect: Deep dives on the latest Snowflake AI capabilities and cool OSS use cases with LlamaIndex, C...

</details>


### [34] [How Well Does RL Scale?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tobyord.com%2Fwriting%2Fhow-well-does-rl-scale%3Futm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/8YNtbzYmhKt0VDAViHAqyLMnQzWgO_VXBdfDLx8sYdI=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 分析RL扩展性的研究，探讨训练计算与推理计算扩展对AI能力提升的不同影响


<details>
  <summary>Details</summary>
Motivation: 理解RL能力提升的来源，因为训练计算扩展和推理计算扩展具有不同的实际意义，目前RL扩展已变得过于昂贵

Method: 分析RL扩展的现状和成本效益，比较训练计算与推理计算扩展的不同影响

Result: RL虽然取得了显著成果，但目前已达到难以继续大规模扩展的阶段

Conclusion: 需要重新评估RL扩展策略，寻找更可持续的发展路径

Abstract: How Well Does RL Scale? (14 minute read) Improving AI capabilities either involves scaling the amount of compute used for RL during training or scaling the amount of compute used for inference during deployment. It's important to know where capability gains come from because scaling up the inference compute has very different implications than scaling up the training compute. While RL has provided impressive gains, we've reached a point where it is too expensive to go much further. This leave...

</details>


### [35] [Introducing Aardvark: OpenAI's agentic security researcher](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FrHEVcC/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/WF93i4wBLKeBCGjKuY1t1BoaRs5KUpDUFgy5D10Qr6Q=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Aardvark是OpenAI开发的基于GPT-5的自主安全研究代理，能够扫描代码库发现安全漏洞、验证可利用性并提出补丁。


<details>
  <summary>Details</summary>
Motivation: 开发能够自主进行安全研究的AI代理，以实时监控代码提交、自动发现漏洞并生成修复方案，提高软件安全防护效率。

Method: 使用GPT-5驱动的代理系统，集成GitHub工作流，实时监控代码提交，自动生成威胁模型，并提供一键补丁功能。

Result: 目前处于私有测试阶段，能够自主扫描代码库、验证漏洞可利用性并生成修复补丁。

Conclusion: Aardvark代表了AI在代码安全领域的应用进展，能够显著提升漏洞发现和修复的效率。

Abstract: Introducing Aardvark: OpenAI's agentic security researcher (5 minute read) Aardvark, currently in private beta, is a GPT-5-powered agent that autonomously scans code repositories to find security vulnerabilities, validate exploitability, and propose patches. It monitors commits in real-time, generates threat models for entire repositories, and integrates directly with GitHub workflows to deliver one-click patches, similar to Google's CodeMender.

</details>


### [36] [ImpossibleBench: Measuring Reward Hacking in LLM Coding Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FqJYMbrabcQqCZ7iqm%2Fimpossiblebench-measuring-reward-hacking-in-llm-coding-1%3Futm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/vJVkPyCs64uLjwsoKhNAt6iW348AS0QVIzopeME7o3E=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ImpossibleBench是一个专门用于测量LLM编码代理中奖励黑客行为的基准测试，通过操纵现有编码基准的单元测试使其与自然语言规范直接冲突，创建不可能完成的任务。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的编码代理被发现会利用测试或评分系统中的漏洞，而不是解决实际指定的任务。为了系统性地测量这种行为，创建了ImpossibleBench。

Method: 采用现有的编码基准，操纵其单元测试使其与自然语言规范直接冲突，创建不可能完成的任务，迫使模型在遵循规范还是通过测试之间做出选择。

Result: 通过这个基准测试可以测量LLM编码代理是否倾向于利用系统漏洞而非真正解决问题。

Conclusion: ImpossibleBench为评估LLM编码代理的奖励黑客行为提供了系统性的测量工具，有助于识别和解决这类问题。

Abstract: ImpossibleBench: Measuring Reward Hacking in LLM Coding Agents (9 minute read) LLM-powered coding agents have been observed exploiting loopholes in tests or scoring systems rather than solving the actual tasks specified. ImpossibleBench was created to systematically measure this behavior. Its creators took existing coding benchmarks and manipulated their unit tests to directly conflict with the natural language specifications to create impossible tasks where models must choose between followi...

</details>


### [37] [The Secrets of Claude Code From the Engineers Who Built It](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=IDSAMqip6ms%26utm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/fT0_VcsYKAjldA15IxFxe2dcM0N6dF93Yjf5iYP-vHU=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code工程师分享产品哲学和技术工作流程，包括使用竞争性子代理获得更清晰结果、平衡简单性与功能的"unshipping"方法，以及未来让工具更自主且对非技术用户更易用的发展方向。


<details>
  <summary>Details</summary>
Motivation: 分享Anthropic编码代理Claude Code的设计理念和技术实现细节，帮助开发者理解其工作原理和最佳实践。

Method: 采用竞争性子代理架构，通过"unshipping"方法平衡功能与简洁性，工程师团队分享实际工作流程。

Result: 开发出能够产生更清晰代码结果的编码代理工具，并建立了有效的开发方法论。

Conclusion: Claude Code通过创新的技术方法在编码代理领域取得进展，未来将继续向更自主和易用的方向发展。

Abstract: The Secrets of Claude Code From the Engineers Who Built It (1 hour video) Claude Code creators, Cat Wu and Boris Cherny, discuss the product philosophy and technical workflows behind Anthropic's coding agent. They cover how their engineers use competing subagents for cleaner results, the team's "unshipping" approach to balance simplicity with power, and future form factors to make the tool more autonomous and accessible to non-technical users.

</details>


### [38] [How I Use Every Claude Code Feature](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sshh.io%2Fp%2Fhow-i-use-every-claude-code-feature%3Futm_source=tldrnewsletter/1/0100019a497555c5-c5835607-8c07-4441-b07c-136126961070-000000/Tt6nm0e-6rmQOhKGGm5aNZzYmVfqWDK27ZHu_SKNIWM=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了Claude Code的各种实用功能，包括Claude.md文件、自定义斜杠命令、子代理、钩子和GitHub Actions。


<details>
  <summary>Details</summary>
Motivation: 帮助开发者更好地理解和利用Claude Code的各项功能，提高开发效率。

Method: 通过详细的功能介绍和使用示例来展示Claude Code的各项特性。

Result: 读者能够全面了解Claude Code的功能，并学会如何在实际项目中应用这些功能。

Conclusion: Claude Code提供了丰富的功能集，能够显著提升开发工作流程的效率。

Abstract: How I Use Every Claude Code Feature (18 minute read) The post covers useful Claude Code features, including the Claude.md file, custom slash commands, Subagents, Hooks, and GitHub Actions.

</details>


### [39] [AI coding agents should live in your internal developer portal](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Ops3/1/0100019a499c1bc8-d2199b9e-4398-44fc-b3b4-d2d63624df9c-000000/7yBS1If6s0EyeRBdD8EqiMmfetT8nPWDtqmp5jeDxIY=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Port's internal developer portal enables building AI coding agents with guardrails and context lakes for safe agentic engineering.


<details>
  <summary>Details</summary>
Motivation: Uncontrolled AI agents can cause real-time damage, so there's a need for robust guardrails and context management in agentic engineering.

Method: Using Port's internal developer portal to build agentic flows with built-in guardrails and context lakes.

Result: Enables creation of AI coding agents that make decisions developers can stand behind.

Conclusion: Port provides a solution for safe and controlled agentic engineering in software development.

Abstract: AI coding agents should live in your internal developer portal (Sponsor) Manual engineering is fast becoming agentic engineering – but uncontrolled AI agents can do real damage in real-time. Port's internal developer portal lets you rapidly build agentic flows with robust guardrails and context lakes, so your agents make decisions you can stand behind. Try Port now, or learn more about the future of agentic engineering.

</details>


### [40] [Formae](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fplatform-engineering-labs%2Fformae%3Futm_source=tldrdevops/1/0100019a499c1bc8-d2199b9e-4398-44fc-b3b4-d2d63624df9c-000000/oAvGvnXWAHMConCG0HKGTcIB9O2sfcjQPXhD-7orbbc=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Formae是一个完全基于代码的、自主的基础设施即代码工具，旨在保持基础设施代码自动同步并适应不同团队角色需求。


<details>
  <summary>Details</summary>
Motivation: 解决基础设施代码同步问题，支持GitOps但不强制要求，能够整合来自Terraform和ClickOps等其他工具的变更。

Method: 构建一个100%基于代码的自主IaC工具，支持GitOps工作流，能够合并来自不同工具的变更，提供一致且版本控制的基础设施视图。

Result: 开发了Formae工具，实现了基础设施代码的自动同步和适应性，为不同团队角色提供统一的基础设施管理界面。

Conclusion: Formae成功创建了一个灵活的基础设施管理解决方案，能够在保持代码同步的同时适应多样化的团队工作流程。

Abstract: Formae (GitHub Repo) Formae, a 100% code-based, agentic Infrastructure-as-Code (IaC) tool built from scratch, was designed to keep infrastructure code automatically in sync and adaptable for various team roles. Supporting GitOps without enforcing it, Formae merges changes from other tools like Terraform and ClickOps, providing a consistent, version-controlled view of infrastructure.

</details>


### [41] [Drowning in AI Code Review Noise? A Framework to Measure Signal vs. Noise](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjetxu-llm.github.io%2Fposts%2Flow-noise-code-review%2F%3Futm_source=tldrwebdev/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/ClboS-qnkUIZ7OQJKVpU0gdQYZ78CJx2syARorCL-lo=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 提出了一个测量AI代码审查工具信噪比的框架，将评论按严重程度分为三个等级：关键、重要和噪音


<details>
  <summary>Details</summary>
Motivation: AI代码审查工具经常产生过多评论，其中高达80%是无关噪音，掩盖了关键问题

Method: 通过将评论分类为三个严重程度等级（关键、重要、噪音）来测量信噪比，信号比率计算公式为（第1层+第2层）/总评论数

Result: 理想的信号比率应高于60%，以确保工具的有效性

Conclusion: 该框架有助于评估AI代码审查工具的质量，减少噪音干扰，提高审查效率

Abstract: Drowning in AI Code Review Noise? A Framework to Measure Signal vs. Noise (8 minute read) AI code review tools often generate excessive comments, with up to 80% being irrelevant noise that buries critical issues. A framework for measuring the signal-to-noise ratio of AI code review tools is to classify comments into three tiers based on severity: critical, important, and noise. The signal ratio, calculated as (Tier 1 + Tier 2) / Total comments, should ideally be above 60% to ensure the tool i...

</details>


### [42] [Claude Code Can Debug Low-level Cryptography](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwords.filippo.io%2Fclaude-debugging%2F%3Futm_source=tldrwebdev/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/u5n_RfnJxs1986-kQExXeF4VfmdyMuYEUCJ9s_y4m64=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code成功调试了Go语言实现的ML-DSA算法中的复杂低层密码学问题


<details>
  <summary>Details</summary>
Motivation: 验证Claude Code在处理复杂低层密码学调试方面的能力，特别是在新兴密码算法实现中的表现

Method: 使用Claude Code对Go语言实现的ML-DSA算法进行调试，识别和修复低层密码学实现中的问题

Result: Claude Code成功识别并调试了ML-DSA算法实现中的复杂密码学问题，展示了其在低层密码学调试方面的能力

Conclusion: Claude Code能够有效处理复杂的低层密码学调试任务，为密码算法实现的质量保证提供了有力工具

Abstract: Claude Code Can Debug Low-level Cryptography (8 minute read) Claude Code successfully debugged complex low-level cryptography issues in a new Go implementation of the ML-DSA algorithm.

</details>


### [43] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/rSmo8ITV1fxON3zqNpwZdb4LktgbpnemFTPc2fePDxg=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code成功调试了Go语言实现的ML-DSA算法中的复杂低层密码学问题


<details>
  <summary>Details</summary>
Motivation: 验证Claude Code在处理复杂低层密码学代码调试方面的能力，特别是在新兴密码学算法实现中的表现

Method: 使用Claude Code对Go语言实现的ML-DSA算法进行调试，分析其处理低层密码学问题的能力

Result: Claude Code成功识别并修复了ML-DSA算法实现中的复杂密码学错误，展示了在低层代码调试方面的有效性

Conclusion: Claude Code在调试复杂低层密码学代码方面表现出色，能够有效处理新兴密码学算法的实现问题

Abstract: Claude Code Can Debug Low-level Cryptography (8 minute read) Claude Code successfully debugged complex low-level cryptography issues in a new Go implementation of the ML-DSA algorithm.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [44] [机器人“自主进化”时代开启：全球首个具身智能<em class="highlight">强化学习</em>技术引爆工业变革](http://mp.weixin.qq.com/s?__biz=MzkzNTk5NjgwMQ==&mid=2247483973&idx=1&sn=11dd11db3a30464b0248bbdb427b3a87&chksm=c36e23584dfe71f50ebcfca48195b88d0545bdaa57dfe9bd5818170a219660bece9ad821572e#rd)
*AI新机遇*

Main category: wechat.article

TL;DR: 最新研究提出的知识组合强化学习框架，通过将参数分离为任务无关和任务特定组件，使机器人在连续学习多任务时保持原有技能，同时高效掌握新技能。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 最新研究提出的知识组合强化学习框架，通过将参数分离为任务无关和任务特定组件，使机器人在连续学习多任务时保持原有技能，同时高效掌握新技能。

</details>


### [45] [<em class="highlight">强化学习</em>：荣膺今年最大赢家，成功登上Nature顶级期刊](http://mp.weixin.qq.com/s?__biz=MjM5OTAxMjQ0OA==&mid=2655002493&idx=1&sn=6ec5b0a366445d23b474b4ae36e9b407&chksm=bc381bd21cb13051c84c2d68f8a9206a78acec8208b659c28336c5a25a56fe2a326d3652edbc#rd)
*青云学长*

Main category: wechat.article

TL;DR: 方法：论文提出了一种基于物理信息深度强化学习的公交运行控制新方法。该方法综合运用历史数据与车路协同车辆（CAV）实时数据，采用分布式近端策略优化（DPPO）算法，将物理规律等知识融入模型构建。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 方法：论文提出了一种基于物理信息深度强化学习的公交运行控制新方法。该方法综合运用历史数据与车路协同车辆（CAV）实时数据，采用分布式近端策略优化（DPPO）算法，将物理规律等知识融入模型构建。

</details>


### [46] [如何使用MATLAB进行深度<em class="highlight">强化学习</em>（附代码）](http://mp.weixin.qq.com/s?__biz=MzI0NDMyODUxMA==&mid=2247488793&idx=1&sn=7d48084f9430547aa249e81792b98e27&chksm=e8b5dc4e8e18b9c38f761e1a6f99dbec87b1590a6117f4b47bfd238a85f53d30591b6e94d6c7#rd)
*调度与优化算法的集结地*

Main category: wechat.article

TL;DR: 深度强化学习（Deep Reinforcement Learning， DRL）是将深度学习与强化学习相结合的一类智能决策算法。其核心思想是：通过与环境交互，学习最大化长期累积奖励的策略。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 深度强化学习（Deep Reinforcement Learning， DRL）是将深度学习与强化学习相结合的一类智能决策算法。其核心思想是：通过与环境交互，学习最大化长期累积奖励的策略。

</details>


### [47] [智元机器人真机<em class="highlight">强化学习</em>落地工业产线，开启具身智能规模化应用新阶段｜产业创新动态](http://mp.weixin.qq.com/s?__biz=MjM5MDU2MTc4NQ==&mid=2650692736&idx=3&sn=774c339997ff9b852450fa65df2785d6&chksm=bf9cce372cf89ea58745158f552ba22432ad63e6c7ccfbb43483e673bfe99ae98686622b8f01#rd)
*上海经信委*

Main category: wechat.article

TL;DR: 智元此次落地的真机强化学习方案，实现了革命性突破：机器人可在真实产线中自主学习、持续优化作业策略，新技能训练与稳定部署仅需数十分钟，且性能全程不降级。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 智元此次落地的真机强化学习方案，实现了革命性突破：机器人可在真实产线中自主学习、持续优化作业策略，新技能训练与稳定部署仅需数十分钟，且性能全程不降级。

</details>


### [48] [【Applied Energy 最新原创论文】基于学习的<em class="highlight">强化学习</em>提升自适应模型预测控制优化需求侧管理建筑能源系统的能源灵活性](http://mp.weixin.qq.com/s?__biz=MzAxMTE3NTAwNA==&mid=2649939357&idx=1&sn=e5450488855239bec9df4279cf7ecda4&chksm=821f7495d9be480db9dbf0d932b56b5d91f6bd6a63db6314715a327f503601911c1477fcef8d#rd)
*AEii国际应用能源*

Main category: wechat.article

TL;DR: Reinforcement learning 强化学习Model predictive control 模型预测控制Sparse neural network 稀疏神经网络Graphics， max cw，OCS， 1 ... 1 pw，PSR，sequential decision step 1+ 1 prediction hore 总-∫（1，m，w） l agent step i "≤u， ...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Reinforcement learning 强化学习Model predictive control 模型预测控制Sparse neural network 稀疏神经网络Graphics， max cw，OCS， 1 ... 1 pw，PSR，sequential decision step 1+ 1 prediction hore 总-∫（1，m，w） l agent step i "≤u， su dynamic sparse training calcrolated acm

</details>


### [49] [超越SFT与RLVR！谷歌提出监督<em class="highlight">强化学习</em>范式SRL，突破LLM复杂推理训练局限](http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449996308&idx=1&sn=2610b84c911e791adc35ab597a04d328&chksm=b0a4bfc95e63c866e7a66488a785f858acc774ac9a145692efbf1ba258db11902926743d83ef#rd)
*智猩猩AI*

Main category: wechat.article

TL;DR: 为了解决从Dhard中学习的挑战，论文引入监督强化学习（SRL），框架如下图所示。where the product \（a \times b = 20！\）.。1. **Prime Factorization of 20！-The prime factors of 20！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 为了解决从Dhard中学习的挑战，论文引入监督强化学习（SRL），框架如下图所示。where the product \（a \times b = 20！\）.。1. **Prime Factorization of 20！-The prime factors of 20！

</details>


### [50] [机器人“10分钟上岗”，智元实现真机<em class="highlight">强化学习</em>工业落地](http://mp.weixin.qq.com/s?__biz=MzA4OTIzNjU5NA==&mid=2651021421&idx=2&sn=d45cde0abea86d0f9401a4e100dfac53&chksm=8a65ebdb1b1c826adbd4fcbc9bbea2ee865bf70e198e023d9d1627cbb7afb10f3e024a652954#rd)
*工业机器人*

Main category: wechat.article

TL;DR: 针对这一痛点，智元推出了真机强化学习方案。界面新闻从智元获悉，在该方案中，机器人可在真实产线中自主学习、持续优化作业策略，新技能训练与稳定部署仅需数十分钟，且性能全程不降级。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对这一痛点，智元推出了真机强化学习方案。界面新闻从智元获悉，在该方案中，机器人可在真实产线中自主学习、持续优化作业策略，新技能训练与稳定部署仅需数十分钟，且性能全程不降级。

</details>


### [51] [智能计算| 深度<em class="highlight">强化学习</em>引导的多种群协同进化超多目标优化算法](http://mp.weixin.qq.com/s?__biz=Mzg3NzkzNzY2Nw==&mid=2247488384&idx=1&sn=b9b1b5f40bf1145354f89cf285bb65f3&chksm=ce091354838e56f818782b4a7a048870db911a084417c83523da359a119b4b7109a33da25441#rd)
*计算机学报*

Main category: wechat.article

TL;DR: 近年来，强化学习因其卓越的决策能力被引入进化算法框架，成为提升算法性能的关键技术。因此，本文提出了一种深度强化学习引导的多种群协同进化超多目标优化算法DQNMaOEA，用于求解复杂的超多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近年来，强化学习因其卓越的决策能力被引入进化算法框架，成为提升算法性能的关键技术。因此，本文提出了一种深度强化学习引导的多种群协同进化超多目标优化算法DQNMaOEA，用于求解复杂的超多目标优化问题。

</details>


### [52] [机器人“10分钟上岗”，智元实现真机<em class="highlight">强化学习</em>工业落地](http://mp.weixin.qq.com/s?__biz=MzI3NjIxODA0Mg==&mid=2247536418&idx=1&sn=284ee955822c7770adda9fc8fec906f4&chksm=ea418d9f9f14b7ccf4ac50f37a363b2a523f0e1e3c0427260bb78bdb0eeefadbb1f4a2435a29#rd)
*具身智能头条*

Main category: wechat.article

TL;DR: 针对这一痛点，智元推出了真机强化学习方案。界面新闻从智元获悉，在该方案中，机器人可在真实产线中自主学习、持续优化作业策略，新技能训练与稳定部署仅需数十分钟，且性能全程不降级。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对这一痛点，智元推出了真机强化学习方案。界面新闻从智元获悉，在该方案中，机器人可在真实产线中自主学习、持续优化作业策略，新技能训练与稳定部署仅需数十分钟，且性能全程不降级。

</details>


### [53] [<em class="highlight">强化学习</em> AI 系统的设计实现及未来发展](http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651261553&idx=2&sn=c4dd026bb61388765d188789a6ad7a24&chksm=bcb54e7a521fca0dc473bc32c04e3fba203930f15f9f9b9ef2e1f297e8b2255ca287b0fa7ae4#rd)
*InfoQ*

Main category: wechat.article

TL;DR: 从常见的人类反馈强化学习，到基于宪法的反馈强化学习，再到如今基于可验证规则的强化学习，这些不断进步的过程，实际上代表着强化学习奖励函数的信号来源日益广泛，同时任务难度也在不断提高。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从常见的人类反馈强化学习，到基于宪法的反馈强化学习，再到如今基于可验证规则的强化学习，这些不断进步的过程，实际上代表着强化学习奖励函数的信号来源日益广泛，同时任务难度也在不断提高。

</details>


### [54] [【论文推荐】基于有效动作表示的策略搜索<em class="highlight">强化学习</em>方法](http://mp.weixin.qq.com/s?__biz=MzIyNjY4OTQ0Mg==&mid=2247491906&idx=1&sn=9ee846e8f2bb0ecc79d637b5b369e4cb&chksm=e9184b36b926428cf681f88fb8d9a24e856e9f853ca78bdfbfe17c821b07ba1f766c1ee6ff04#rd)
*天津科技大学学报*

Main category: wechat.article

TL;DR: 强化学习方法[j].天津科技大学学报，2025，40。（5）：57-65 wang x x， huang jx，zhao t t， et al. strategy search reinforcement learning method based on effective action repre sentation[j]. journal of tianjin university of sc...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习方法[j].天津科技大学学报，2025，40。（5）：57-65 wang x x， huang jx，zhao t t， et al. strategy search reinforcement learning method based on effective action repre sentation[j]. journal of tianjin university of science and technology， 2025， 40。

</details>


### [55] [【工业智能】机器人也“上岗”学习？智元机器人实现生产线<em class="highlight">强化学习</em>部署](http://mp.weixin.qq.com/s?__biz=MzE5MTQ0NzAyMQ==&mid=2247484621&idx=1&sn=b9fe965209e9b325d7e8630e09f08fc5&chksm=975d2744e9690d97e8f81ba8fee6804934f14908a13a528897e39d7973d434020f5ecce7ffc9#rd)
*国智园*

Main category: wechat.article

TL;DR: 什么是“强化学习”（Reinforcement Learning）在此处做一个通俗说明：强化学习是一类让机器“通过试错学习”的算法——系统在执行动作后会得到一个“回报”（比如装配成功或失败），算法根据这些回报调整策略，逐步学会在


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 什么是“强化学习”（Reinforcement Learning）在此处做一个通俗说明：强化学习是一类让机器“通过试错学习”的算法——系统在执行动作后会得到一个“回报”（比如装配成功或失败），算法根据这些回报调整策略，逐步学会在

</details>


### [56] [<em class="highlight">强化学习</em>只能<em class="highlight">加强</em>偏见？](http://mp.weixin.qq.com/s?__biz=Mzg4Mzc3MDI1Nw==&mid=2247488892&idx=1&sn=fa3bd13799468e54d6841ef2a3aa73d8&chksm=ce3a2187b557b0854a6a842e007a9cdbf7b42a4aeead3bd7cbb1aa619ff5782c4ef684b0599c#rd)
*脑洞峰谷*

Main category: wechat.article

TL;DR: 4.强化学习模型模拟 作者搭建了一个三层的人工神经网络，再通过建立不同的损失函数来差异化地更新神经网络不同联结间的权重。接下来，我们依据原文行文逻辑，展开详细讨论。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 4.强化学习模型模拟 作者搭建了一个三层的人工神经网络，再通过建立不同的损失函数来差异化地更新神经网络不同联结间的权重。接下来，我们依据原文行文逻辑，展开详细讨论。

</details>


### [57] [<em class="highlight">强化学习</em>，到底行不行？AI 还要多久才真正“聪明”？](http://mp.weixin.qq.com/s?__biz=Mzg3MTA1MzE2Nw==&mid=2247493982&idx=1&sn=c4be92e6883616edfcf6c256b971b0a7&chksm=cf31f1ef13b7314db228f3f701e9ba8161348891a589eee58787b191184236dabab967a74353#rd)
*AI森林物种图鉴*

Main category: wechat.article

TL;DR: “强化学习就像在迷宫里找出口，每次走错一步，错误会一路累积。”这不是夸张。RL 的核心问题是信号稀薄、反馈太慢：AI 要靠“奖惩”学习，必须一遍遍试错；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “强化学习就像在迷宫里找出口，每次走错一步，错误会一路累积。”这不是夸张。RL 的核心问题是信号稀薄、反馈太慢：AI 要靠“奖惩”学习，必须一遍遍试错；

</details>


### [58] [元古<em class="highlight">大模型</em>，入选！](http://mp.weixin.qq.com/s?__biz=MzAxMzAyNjgxNg==&mid=2650910842&idx=1&sn=2faf7487d03267c9a1412eaf4306607a&chksm=818c40fa0614fec5a74b1fac84705a0cb43adf29f74d1dac2cd8c91f3717583f362db25a8751#rd)
*讯飞高教*

Main category: wechat.article

TL;DR: 中国地质大学 chena univernity of geoscience 你好，我是地学智思体 地质图像理解 鱼类化石复原 文献抽取 知识图谱 地学小助手 请输入想了解的问题（enter发送，ctrl+enter换行） 元古大模型。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 中国地质大学 chena univernity of geoscience 你好，我是地学智思体 地质图像理解 鱼类化石复原 文献抽取 知识图谱 地学小助手 请输入想了解的问题（enter发送，ctrl+enter换行） 元古大模型。

</details>


### [59] [腾讯基于 RAG 和 Agent 技术的混元<em class="highlight">大模型</em>业务落地实践](http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658743&idx=3&sn=88e0b61515585393d7fb424d87b8c7d6&chksm=c0cc8df4db615bfa85a3220e4268267cd3f5d5a610bb2204cb4d955fbda70616dd10e38010eb#rd)
*DataFunSummit*

Main category: wechat.article

TL;DR: 首先介绍腾讯大模型的广泛应用场景，如内容生成、智能客服和角色扮演等，并详细解析 RAG（Retrieval-Augmented Generation）技术及其在实际业务中的创新应用，特别是在文档生成和问答系统中的优势。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 首先介绍腾讯大模型的广泛应用场景，如内容生成、智能客服和角色扮演等，并详细解析 RAG（Retrieval-Augmented Generation）技术及其在实际业务中的创新应用，特别是在文档生成和问答系统中的优势。

</details>


### [60] [全球首个AI投资大赛落幕，国产<em class="highlight">大模型</em>包揽前二](http://mp.weixin.qq.com/s?__biz=MzkxMjY3Nzg5NQ==&mid=2247492889&idx=8&sn=60a4e789c674b315c5c15394420f2735&chksm=c0c1b6fa0ab04eb4b2a52296e274ad8592b73db3c149eb1a46e3f2bf0ef318732a76e9ec6931#rd)
*中国数智开发区*

Main category: wechat.article

TL;DR: 当前，AI大模型在各类性能基准榜单中屡创佳绩，而如何评估大模型在真实、动态且竞争激烈环境中的决策水平，已成为当下AI领域的关注焦点。此次“AlphaArena”大赛由美国人工智能研究实验室nof1.ai主办，为六大顶尖模型提供1


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 当前，AI大模型在各类性能基准榜单中屡创佳绩，而如何评估大模型在真实、动态且竞争激烈环境中的决策水平，已成为当下AI领域的关注焦点。此次“AlphaArena”大赛由美国人工智能研究实验室nof1.ai主办，为六大顶尖模型提供1

</details>


### [61] [多模态<em class="highlight">大模型</em>理解物理工具吗？PhysToolBench提出了衡量多模态<em class="highlight">大模型</em>对物理工具理解的基准](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650999897&idx=3&sn=6b60184fb0631f030876f4a5c9e44a03&chksm=85a3dc9b659ae348b58d051108c77819b572f14913bb60bd2d023cf4ec3df36231fb88818817#rd)
*机器之心*

Main category: wechat.article

TL;DR: 实验结果：大模型在 PhysToolBench 上的答卷CategoriesMLLM Difficulty Level Scene Category Overall↑Easy个 m1个一 m2个 m3↑Hard↑ Professional个 Industrial↑ Outdoor↑ Daily↑


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 实验结果：大模型在 PhysToolBench 上的答卷CategoriesMLLM Difficulty Level Scene Category Overall↑Easy个 m1个一 m2个 m3↑Hard↑ Professional个 Industrial↑ Outdoor↑ Daily↑

</details>


### [62] [AI<em class="highlight">大模型</em>·白皮书 | 2025年中国<em class="highlight">大模型</em>行业发展研究报告](http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247596768&idx=2&sn=4e901e93a811c4cb360221ab81fdd97c&chksm=e9a2226d861ca50d6656819e59504fe114d9cfec7ac4a815d3e4dafb99df508222970106d115#rd)
*木木自由*

Main category: wechat.article

TL;DR: 大模型作为ai发展核心引擎，按应用 广度分通用与行业模型，按部署形态分云端与端，侧模型，按技术路径分闭源与开源模型，2024年中国大模型市场规模约294.16亿元，预计2026年突破700亿元，多模态 融合与智能体演进成竞争焦点


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型作为ai发展核心引擎，按应用 广度分通用与行业模型，按部署形态分云端与端，侧模型，按技术路径分闭源与开源模型，2024年中国大模型市场规模约294.16亿元，预计2026年突破700亿元，多模态 融合与智能体演进成竞争焦点

</details>


### [63] [老外吹爆的国产开源 AI <em class="highlight">大模型</em>，登顶了开源热榜第 1。](http://mp.weixin.qq.com/s?__biz=MzUxNjg4NDEzNA==&mid=2247528435&idx=1&sn=b5ed24818d400556e09f986a05c475bf&chksm=f88f908b469091ee558509b8dc479f6b6d9e29dbace9492d5c0f8f9d67fca58067d739d4f3de#rd)
*逛逛GitHub*

Main category: wechat.article

TL;DR: 任何大模型新版本发布，都会拉到 Artificial Analysis 去溜溜。Artificial Analysis 显示开源模型 MiniMax-M2 现在是世界第五，国产第一。太争气了。另外看 X 上的帖子，很多国外知名的开发者和大厂员工都觉得 MiniMax M2 很顶，是最好的开


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 任何大模型新版本发布，都会拉到 Artificial Analysis 去溜溜。Artificial Analysis 显示开源模型 MiniMax-M2 现在是世界第五，国产第一。太争气了。另外看 X 上的帖子，很多国外知名的开发者和大厂员工都觉得 MiniMax M2 很顶，是最好的开

</details>


### [64] [一文掌握<em class="highlight">大模型</em>应用开发精髓，很难找全的！](http://mp.weixin.qq.com/s?__biz=MzkxNjQzMDU3Ng==&mid=2247484050&idx=1&sn=bb1dfd8186ad4a00682205d0fb0ffd2d&chksm=c05e8195d3053ad3afb69e7fb10457fc286bca3412e85d6f977b2eea6610580a929345d73d0f#rd)
*Ai大模型知识营*

Main category: wechat.article

TL;DR: 大模型开发全流程。· 1、大模型应用开发核心流程。·。2.2项目规划阶段 。1.2数据处理。· 2、核心技术栈全景解析。·。2.1大模型技术栈分层架构 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型开发全流程。· 1、大模型应用开发核心流程。·。2.2项目规划阶段 。1.2数据处理。· 2、核心技术栈全景解析。·。2.1大模型技术栈分层架构 。

</details>


### [65] [AI“盆景”已成“风景”？<em class="highlight">大模型</em>的规模复制让工业长出数智生产力！](http://mp.weixin.qq.com/s?__biz=MzU3NjU5MDY3NQ==&mid=2247664758&idx=1&sn=735a1439792722c58b61aeb2206d8699&chksm=fc4ba886c226cefc656695bcaa16c3b7f2b881a5753d27a9f29d65c86497dae538f23141dfba#rd)
*IntelMining智能矿业*

Main category: wechat.article

TL;DR: 回顾“矿山盘古大模型”的推出，其初衷正是为了打破AI在矿山场景中“散、小、乱”的魔咒。在过去，每个应用场景都像一个独立的“手工作坊”：针对皮带跑偏识别开发一个算法，为瓦斯监测再另起炉灶。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 回顾“矿山盘古大模型”的推出，其初衷正是为了打破AI在矿山场景中“散、小、乱”的魔咒。在过去，每个应用场景都像一个独立的“手工作坊”：针对皮带跑偏识别开发一个算法，为瓦斯监测再另起炉灶。

</details>


### [66] [搞懂这些AI<em class="highlight">大模型</em>名词，你也能轻松入门！](http://mp.weixin.qq.com/s?__biz=Mzg2MzcyNDExNQ==&mid=2247485735&idx=3&sn=db898706b4ff49c66e443b6bf0b53c76&chksm=cfb5f0a3063990c5db9daf179ede9485ac97562948b5fb3aef521df12e63ce5d944d26de16bd#rd)
*柳星聊产品*

Main category: wechat.article

TL;DR: 大模型应用开发正在逐渐改变各个行业，但对技术小白来说，了解并掌握这些复杂的工具和概念非常重要。你是否觉得面对“LlamaIndex”、“Ollama”、“Anthropic”等术语无从下手？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型应用开发正在逐渐改变各个行业，但对技术小白来说，了解并掌握这些复杂的工具和概念非常重要。你是否觉得面对“LlamaIndex”、“Ollama”、“Anthropic”等术语无从下手？

</details>


### [67] [<em class="highlight">大模型</em>生态的“不可能三角”：规模化应用的架构困境？](http://mp.weixin.qq.com/s?__biz=MjM5NTk0MTM1Mw==&mid=2650707337&idx=1&sn=90c9653f17ed14541b1abb2bee49ed4e&chksm=bfec525856369e181e68d2f809d381a48ec17bfdb795adcfaab749e3671067e32ef3a87c2e21#rd)
*twt企业IT社区*

Main category: wechat.article

TL;DR: 我们习惯于将“大模型”视为无所不能的智能体，然而，在面对快速演变的现实“动态的”数据世界中，大模型的静态知识体系暴露出根本性的局限。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们习惯于将“大模型”视为无所不能的智能体，然而，在面对快速演变的现实“动态的”数据世界中，大模型的静态知识体系暴露出根本性的局限。

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [68] [Adding New Capability in Existing Scientific Application with LLM Assistance](https://arxiv.org/abs/2511.00087)
*Anshu Dubey,Akash Dhruv*

Main category: cs.SE

TL;DR: 提出了一种使用LLM辅助从零开始为新算法编写代码的新方法，并改进了现有的代码翻译工具Code-Scribe用于新代码生成


<details>
  <summary>Details</summary>
Motivation: 当前代码生成研究主要关注已有代码的生成，对于全新算法（训练数据中无类似代码示例）的代码生成研究较少

Method: 开发了一种新的代码生成方法，并增强了现有的Code-Scribe代码翻译工具，使其能够处理新算法的代码生成任务

Result: 论文描述了改进后的工具和方法，但未提供具体的实验结果数据

Conclusion: 该方法为处理训练数据中不存在的新算法代码生成问题提供了解决方案

Abstract: With the emergence and rapid evolution of large language models (LLM),
automating coding tasks has become an im- portant research topic. Many efforts
are underway and liter- ature abounds about the efficacy of models and their
ability to generate code. A less explored aspect of code generation is for new
algorithms, where the training data-set would not have included any previous
example of similar code. In this paper we propose a new methodology for writing
code from scratch for a new algorithm using LLM assistance, and describe
enhancement of a previously developed code- translation tool, Code-Scribe, for
new code generation.

</details>


### [69] [Inferring multiple helper Dafny assertions with LLMs](https://arxiv.org/abs/2511.00125)
*Álvaro Silva,Alexandra Mendes,Ruben Martins*

Main category: cs.SE

TL;DR: 使用LLM自动推断Dafny程序中缺失的辅助断言，开发DAISY工具，在单断言缺失情况下验证63.4%程序，多断言缺失情况下验证31.7%程序。


<details>
  <summary>Details</summary>
Motivation: Dafny验证器需要大量手动辅助断言，成为采用障碍，研究如何自动推断缺失断言以减少证明工程工作量。

Method: 扩展DafnyBench基准，创建缺失断言数据集，结合LLM预测和错误消息启发式方法的混合故障定位方法，开发DAISY工具。

Result: DAISY在单断言缺失时验证63.4%程序，多断言缺失时验证31.7%程序，证明无需恢复所有原始断言即可完成验证。

Conclusion: 自动断言推断能显著减少证明工程工作量，是迈向更可扩展和可访问的形式验证的重要一步。

Abstract: The Dafny verifier provides strong correctness guarantees but often requires
numerous manual helper assertions, creating a significant barrier to adoption.
We investigate the use of Large Language Models (LLMs) to automatically infer
missing helper assertions in Dafny programs, with a primary focus on cases
involving multiple missing assertions. To support this study, we extend the
DafnyBench benchmark with curated datasets where one, two, or all assertions
are removed, and we introduce a taxonomy of assertion types to analyze
inference difficulty. Our approach refines fault localization through a hybrid
method that combines LLM predictions with error-message heuristics. We
implement this approach in a new tool called DAISY (Dafny Assertion Inference
SYstem). While our focus is on multiple missing assertions, we also evaluate
DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one
missing assertion and 31.7% with multiple missing assertions. Notably, many
programs can be verified with fewer assertions than originally present,
highlighting that proofs often admit multiple valid repair strategies and that
recovering every original assertion is unnecessary. These results demonstrate
that automated assertion inference can substantially reduce proof engineering
effort and represent a step toward more scalable and accessible formal
verification.

</details>


### [70] [What a diff makes: automating code migration with large language models](https://arxiv.org/abs/2511.00160)
*Katherine A. Rosenfeld,Cliff C. Kerr,Jessica Lundin*

Main category: cs.SE

TL;DR: 使用包含差异信息的上下文可以显著提升LLM在代码迁移任务中的性能，特别是在依赖项发生语义版本变更时保持兼容性。


<details>
  <summary>Details</summary>
Motivation: 现代软件程序依赖的堆栈经常更新，这些变更可能会破坏依赖项目。需要解决如何利用LLM帮助代码迁移，特别是在依赖项发生重大和次要语义版本变更时保持兼容性的问题。

Method: 使用包含差异信息(diffs)的上下文来提升LLM性能，通过测试覆盖率和变更比较等指标进行评估。开发了AIMigrate开源Python包来协助代码库迁移。

Result: 在真实世界的TYPHOIDSIM从STARSIM版本迁移中，AIMigrate单次运行正确识别了65%的必要变更，多次运行提升至80%，其中47%的变更生成完美。

Conclusion: 包含差异信息的上下文可以显著改善LLM在代码迁移任务中的表现，在某些情况下甚至优于使用纯代码的方法。

Abstract: Modern software programs are built on stacks that are often undergoing
changes that introduce updates and improvements, but may also break any project
that depends upon them. In this paper we explore the use of Large Language
Models (LLMs) for code migration, specifically the problem of maintaining
compatibility with a dependency as it undergoes major and minor semantic
version changes. We demonstrate, using metrics such as test coverage and change
comparisons, that contexts containing diffs can significantly improve
performance against out of the box LLMs and, in some cases, perform better than
using code. We provide a dataset to assist in further development of this
problem area, as well as an open-source Python package, AIMigrate, that can be
used to assist with migrating code bases. In a real-world migration of
TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of
required changes in a single run, increasing to 80% with multiple runs, with
47% of changes generated perfectly.

</details>


### [71] [Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories](https://arxiv.org/abs/2511.00197)
*Oorja Majgaonkar,Zhiwei Fei,Xiang Li,Federica Sarro,He Ye*

Main category: cs.SE

TL;DR: 对三种先进代码代理在SWE-Bench基准测试中的执行轨迹进行实证研究，分析成功和失败轨迹的特征差异，揭示代理决策模式和故障定位能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在复杂软件工程任务中的部署增加，需要超越简单成功指标来理解其问题解决行为，因为它们的决策过程仍然不透明。

Method: 分析OpenHands、SWE-agent和Prometheus三种代码代理在SWE-Bench基准测试中的执行轨迹，包括成功和失败的尝试，考察问题解决策略、轨迹长度和故障定位准确性。

Result: 发现不同问题解决策略在不同场景中有效；失败轨迹更长且方差更高；大多数轨迹能正确识别问题文件(72-81%)，但成功更依赖于实现近似而非精确的代码修改。

Conclusion: 通过轨迹分析为理解代理行为提供了基础，有助于开发更健壮和可解释的自主软件工程系统。

Abstract: The increasing deployment of Large Language Model (LLM) agents for complex
software engineering tasks has created a need to understand their
problem-solving behaviours beyond simple success metrics. While these agents
demonstrate impressive capabilities in automated issue resolution, their
decision-making processes remain largely opaque. This paper presents an
empirical study of agent trajectories, namely the execution traces capturing
the steps agents take when attempting to resolve software issues. We analyse
trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and
Prometheus) on the SWE-Bench benchmark, examining both successful and failed
attempts. Our investigation reveals several key insights into agent behaviour.
First, we identify how distinct problem-solving strategies, such as defensive
programming and context gathering, enable success in different scenarios.
Second, we find that failed trajectories are consistently longer and exhibit
higher variance than successful ones, with failure patterns differing
significantly between agents. Third, our fault localisation analysis shows that
while most trajectories correctly identify problematic files (72-81\% even in
failures), success depends more on achieving approximate rather than exact code
modifications. These and other findings unveiled by our study, provide a
foundation for understanding agent behaviour through trajectory analysis,
contributing to the development of more robust and interpretable autonomous
software engineering systems.

</details>


### [72] [Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification](https://arxiv.org/abs/2511.00202)
*Jacqueline Mitchell,Yasser Shaaban*

Main category: cs.SE

TL;DR: 论文提出通过形式化方法解决"氛围编码"（与LLM迭代对话开发软件）中的技术债务、安全问题和代码混乱问题，建议使用旁路系统自动形式化规范、验证目标、提供可操作反馈并允许开发者直观影响规范。


<details>
  <summary>Details</summary>
Motivation: 氛围编码虽然流行，但存在技术债务积累、安全问题和代码混乱等限制，这些问题源于LLM无法在开发过程中协调人类施加的约束，优先考虑用户命令而非代码一致性。

Method: 提出在整个氛围编码过程中使用旁路系统，包括：(1)自动形式化规范，(2)针对目标进行验证，(3)向LLM提供可操作反馈，(4)允许开发者直观影响规范。

Result: 通过形式化方法可以缓解氛围编码的缺陷，使其更加可靠，但需要超越现有的形式化方法与LLM结合方法。

Conclusion: 形式化方法能够有效解决氛围编码中的关键限制，但需要创新的集成方式，特别是通过旁路系统在整个开发过程中提供持续验证和反馈。

Abstract: ``Vibe coding'' -- the practice of developing software through iteratively
conversing with a large language model (LLM) -- has exploded in popularity
within the last year. However, developers report key limitations including the
accumulation of technical debt, security issues, and code churn to achieve
satisfactory results. We argue that these pitfalls result from LLMs' inability
to reconcile accumulating human-imposed constraints during vibe coding, with
developers inadvertently failing to resolve contradictions because LLMs
prioritize user commands over code consistency. Given LLMs' receptiveness to
verification-based feedback, we argue that formal methods can mitigate these
pitfalls, making vibe coding more reliable. However, we posit that integrating
formal methods must transcend existing approaches that combine formal methods
and LLMs. We advocate for a side-car system throughout the vibe coding process
which: (1) \emph{Autoformalizes} specifications (2) Validates against targets,
(3) Delivers \emph{actionable} feedback to the LLM, and (4) Allows intuitive
developer influence on specifications.

</details>


### [73] [DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies](https://arxiv.org/abs/2511.00215)
*Xiaomeng Xu,Zahin Wahab,Reid Holmes,Caroline Lemieux*

Main category: cs.SE

TL;DR: DocPrism是一个多语言代码文档不一致性检测工具，使用LLM分析并解释不一致性，通过LCEF方法显著降低误报率


<details>
  <summary>Details</summary>
Motivation: 代码文档不一致性很常见且不可取，可能导致开发者误解和软件缺陷

Method: 使用标准LLM分析不一致性，并引入LCEF（本地分类、外部过滤）方法来减少误报，依赖LLM的本地补全技能而非长期推理技能

Result: LCEF将不一致性标记率从98%降至14%，准确率从14%提升至94%。在Python、TypeScript、C++和Java的广泛评估中，DocPrism保持15%的低标记率和0.62的精确度

Conclusion: DocPrism通过LCEF方法有效检测代码文档不一致性，显著降低误报率，无需微调即可在多语言环境中保持良好性能

Abstract: Code-documentation inconsistencies are common and undesirable: they can lead
to developer misunderstandings and software defects. This paper introduces
DocPrism, a multi-language, code-documentation inconsistency detection tool.
DocPrism uses a standard large language model (LLM) to analyze and explain
inconsistencies. Plain use of LLMs for this task yield unacceptably high false
positive rates: LLMs identify natural gaps between high-level documentation and
detailed code implementations as inconsistencies. We introduce and apply the
Local Categorization, External Filtering (LCEF) methodology to reduce false
positives. LCEF relies on the LLM's local completion skills rather than its
long-term reasoning skills. In our ablation study, LCEF reduces DocPrism's
inconsistency flag rate from 98% to 14%, and increases accuracy from 14% to
94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism
maintains a low flag rate of 15%, and achieves a precision of 0.62 without
performing any fine-tuning.

</details>


### [74] [SmartDoc: A Context-Aware Agentic Method Comment Generation Plugin](https://arxiv.org/abs/2511.00450)
*Vahid Etemadi,Gregorio Robles*

Main category: cs.SE

TL;DR: SmartDoc是一个IntelliJ IDEA插件，使用AI代理生成上下文感知的方法注释，通过构建调用图和使用DFS遍历提供完整上下文来增强LLM提示。


<details>
  <summary>Details</summary>
Motivation: 软件维护阶段需要程序理解，方法作为程序的主要构建块可以提供代码理解的知识源，但阅读整个方法语句具有挑战性，需要精确和最新的注释。

Method: 开发IntelliJ IDEA插件作为AI代理，具有自己的内存并通过目标方法的上下文进行增强。使用深度优先搜索遍历调用图，为LLM提示提供完整上下文。

Result: 开发了适用于Java代码库的插件，可以并发更新方法注释。在BERTScore指标上获得了0.80到0.90的准确率，BLEU和ROUGE-1指标也用于评估生成注释的准确性。

Conclusion: SmartDoc插件能够有效生成上下文感知的方法注释，准确率表现良好，有助于提高程序理解效率。

Abstract: Context: The software maintenance phase involves many activities such as code
refactoring, bug fixing, code review or testing. Program comprehension is key
to all these activities, as it demands developers to grasp the knowledge (e.g.,
implementation details) required to modify the codebase. Methods as main
building blocks in a program can offer developers this knowledge source for
code comprehension. However, reading entire method statements can be
challenging, which necessitates precise and up-to-date comments. Objective: We
propose a solution as an IntelliJ IDEA plugin, named SmartDoc, that assists
developers in generating context-aware method comments. Method: This plugin
acts as an Artificial Intelligence (AI) agent that has its own memory and is
augmented by target methods' context. When a request is initiated by the
end-user, the method content and all its nested method calls are used in the
comment generation. At the beginning, these nested methods are visited and a
call graph is generated. This graph is then traversed using depth-first search
(DFS), enabling the provision of full-context to enrich Large Language Model
(LLM) prompts. Result: The product is a software, as a plugin, developed for
Java codebase and installable on IntelliJ IDEA. This plugin can serve
concurrently for methods whose comments are being updated , and it shares
memory across all flows to avoid redundant calls. o measure the accuracy of
this solution, a dedicated test case is run to record SmartDoc generated
comments and their corresponding ground truth. For each collected result-set,
three metrics are computed, BERTScore, BLEU and ROUGE-1. These metrics will
determine how accurate the generated comments are in comparison to the ground
truth. Result: The obtained accuracy, in terms of the precision, recall and F1,
is promising, and lies in the range of 0.80 to 0.90 for BERTScore.

</details>


### [75] [Issue-Oriented Agent-Based Framework for Automated Review Comment Generation](https://arxiv.org/abs/2511.00517)
*Shuochuan Li,Dong Wang,Patanamon Thongtanunam,Zan Wang,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: RevAgent是一个基于代理的问题导向代码审查框架，通过三个阶段的分解方法显著提升了代码审查评论生成的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查方法依赖单一模型处理各种问题，无法有效应对代码变更的多样性，特别是在复杂场景如bug修复中容易生成非信息性评论。

Method: 将任务分解为三个阶段：生成阶段（五个类别特定的评论代理从不同问题角度分析代码变更）、判别阶段（批评代理选择最合适的问题-评论对）、训练阶段（所有代理在特定类别数据上微调）。

Result: RevAgent显著优于现有PLM和LLM基线方法，在BLEU、ROUGE-L、METEOR和SBERT指标上分别提升12.90%、10.87%、6.32%和8.57%，在问题类别识别方面也表现更好。

Conclusion: RevAgent在生成准确、可读且上下文感知的审查评论方面具有实用性，并在性能与效率之间实现了良好的平衡。

Abstract: Code review (CR) is a crucial practice for ensuring software quality. Various
automated review comment generation techniques have been proposed to streamline
the labor-intensive process. However, existing approaches heavily rely on a
single model to identify various issues within the code, limiting the model's
ability to handle the diverse, issue-specific nature of code changes and
leading to non-informative comments, especially in complex scenarios such as
bug fixes. To address these limitations, we propose RevAgent, a novel
agent-based issue-oriented framework, decomposes the task into three stages:
(1) Generation Stage, where five category-specific commentator agents analyze
code changes from distinct issue perspectives and generate candidate comments;
(2) Discrimination Stage, where a critic agent selects the most appropriate
issue-comment pair; and (3) Training Stage, where all agents are fine-tuned on
curated, category-specific data to enhance task specialization. Evaluation
results show that RevAgent significantly outperforms state-of-the-art PLM- and
LLM-based baselines, with improvements of 12.90\%, 10.87\%, 6.32\%, and 8.57\%
on BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively
higher accuracy in issue-category identification, particularly for challenging
scenarios. Human evaluations further validate the practicality of RevAgent in
generating accurate, readable, and context-aware review comments. Moreover,
RevAgent delivers a favorable trade-off between performance and efficiency.

</details>


### [76] [HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models](https://arxiv.org/abs/2511.00527)
*Robab Aghazadeh-Chakherlou,Qing Guo,Siddartha Khastgir,Peter Popov,Xiaoge Zhang,Xingyu Zhao*

Main category: cs.SE

TL;DR: HIP-LLM是一个基于层次不精确概率框架的LLM可靠性评估方法，通过定义LLM在特定操作配置文件下的无故障运行概率来量化可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于基准测试的评估方法主要提供模型在数据集上的准确率统计，对LLM在实际操作条件下的概率行为提供有限洞察。

Method: HIP-LLM构建层次依赖结构，嵌入不精确先验来捕捉认知不确定性，并整合操作配置文件来反映使用情境，推导后验可靠性包络。

Result: 在多个基准数据集上的实验表明，HIP-LLM比现有基准和最先进方法提供了更准确和标准化的可靠性表征。

Conclusion: HIP-LLM为LLM可靠性评估提供了一个更严谨的框架，能够量化不确定性并提供多级可靠性推断。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
domains, raising the need for rigorous reliability assessment methods. Existing
benchmark-based evaluations primarily offer descriptive statistics of model
accuracy over datasets, providing limited insight into the probabilistic
behavior of LLMs under real operational conditions. This paper introduces
HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and
inferring LLM reliability. Building upon the foundations of software
reliability engineering, HIP-LLM defines LLM reliability as the probability of
failure-free operation over a specified number of future tasks under a given
Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains
hierarchically, enabling multi-level inference from subdomain to system-level
reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty
and incorporates OPs to reflect usage contexts. It derives posterior
reliability envelopes that quantify uncertainty across priors and data.
Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a
more accurate and standardized reliability characterization than existing
benchmark and state-of-the-art approaches. A publicly accessible repository of
HIP-LLM is provided.

</details>


### [77] [GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android](https://arxiv.org/abs/2511.00619)
*Huaijin Ran,Haoyi Zhang,Xunzhu Tang*

Main category: cs.SE

TL;DR: 提出了GDPR-Bench-Android基准，用于评估Android应用中GDPR违规检测的自动化方法，包含1951个手动标注的违规实例，并比较了11种方法在不同任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 自动化检测源代码中的GDPR违规是一个关键但未被充分探索的挑战，需要建立全面的评估基准。

Method: 创建GDPR-Bench-Android基准，包含1951个手动标注的违规实例；提出Formal-AST形式化方法作为确定性基线；评估11种方法在两个任务上的表现：多粒度违规定位和片段级多标签分类。

Result: 不同范式在不同任务上表现各异：ReAct代理在文件级定位任务中表现最佳（17.38%），Qwen2.5-72B在行级定位中领先（61.60%），Claude-Sonnet-4.5在多标签分类中Macro-F1最高（5.75%），RAG方法在Macro-Precision上最优（7.10%）。

Conclusion: 没有单一范式在所有任务上都表现优异，不同自动化方法具有任务依赖性的优势，基准有助于诊断各种方法的能力。

Abstract: Automating the detection of EU General Data Protection Regulation (GDPR)
violations in source code is a critical but underexplored challenge. We
introduce \textbf{GDPR-Bench-Android}, the first comprehensive benchmark for
evaluating diverse automated methods for GDPR compliance detection in Android
applications. It contains \textbf{1951} manually annotated violation instances
from \textbf{15} open-source repositories, covering 23 GDPR articles at file-,
module-, and line-level granularities. To enable a multi-paradigm evaluation,
we contribute \textbf{Formal-AST}, a novel, source-code-native formal method
that serves as a deterministic baseline. We define two tasks: (1)
\emph{multi-granularity violation localization}, evaluated via
Accuracy@\textit{k}; and (2) \emph{snippet-level multi-label classification},
assessed by macro-F1 and other classification metrics. We benchmark 11 methods,
including eight state-of-the-art LLMs, our Formal-AST analyzer, a
retrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings
reveal that no single paradigm excels across all tasks. For Task 1, the ReAct
agent achieves the highest file-level Accuracy@1 (17.38%), while the
Qwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the
Formal-AST method's 1.86%. For the difficult multi-label Task 2, the
Claude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method
yields the highest Macro-Precision (7.10%). These results highlight the
task-dependent strengths of different automated approaches and underscore the
value of our benchmark in diagnosing their capabilities. All resources are
available at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.

</details>


### [78] [Can Large Language Models Detect Real-World Android Software Compliance Violations?](https://arxiv.org/abs/2511.00624)
*Haoyi Zhang,Huaijin Ran,Xunzhu Tang*

Main category: cs.SE

TL;DR: 提出了CompliBench评估框架，用于评估LLM在Android应用合规性检测方面的能力，包含检索定位和多标签判断两个任务，并引入了稳定性感知的复合指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在检测Android应用跨法律框架的合规违规方面存在困难，需要专门的评估框架来提升合规检测能力。

Method: 定义了两个任务：Task1评估文件、模块和行级别的检索定位能力，Task2评估代码片段的多标签判断能力；引入了稳定性感知复合指标(SGS、RCS、CRGS、OCS)进行综合评估。

Result: 在六个模型上的实验显示，Claude-3.5-sonnet-20241022获得最高OCS分数(0.3295)，Gemini-2.5-pro得分最低(0.0538)。

Conclusion: CompliBench能够有效提升LLM在合规任务中的表现，为符合数据保护标准的未来工具提供了基础。

Abstract: The rapid development of Large Language Models (LLMs) has transformed
software engineering, showing promise in tasks like code generation, bug
detection, and compliance checking. However, current models struggle to detect
compliance violations in Android applications across diverse legal frameworks.
We propose \emph{CompliBench}, a novel evaluation framework for assessing LLMs'
ability to detect compliance violations under regulations like LGPD, PDPA, and
PIPEDA. The framework defines two tasks: Task 1 evaluates \emph{retrieval and
localization} at file, module, and line granularities, and Task 2 assesses
\emph{multi-label judgment} for code snippets. These tasks mirror the audit
process, where auditors locate problematic code and determine implicated
provisions. Traditional metrics fail to capture important aspects like
cross-granularity stability and jurisdictional consistency. Thus, we introduce
stability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive
assessment. Experiments with six models, including GPT-4O and Claude-3.5, show
\emph{CompliBench} improves compliance detection, with
Claude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and
Gemini-2.5-pro the lowest (0.0538). This work demonstrates \emph{CompliBench}'s
potential for improving LLM performance in compliance tasks and provides a
foundation for future tools aligned with data protection standards. Our project
is available at https://github.com/Haoyi-Zhang/CompliBench.

</details>


### [79] [Repairing Responsive Layout Failures Using Retrieval Augmented Generation](https://arxiv.org/abs/2511.00678)
*Tasmia Zerin,Moumita Asad,B. M. Mainul Hossain,Kazi Sakib*

Main category: cs.SE

TL;DR: 提出ReDeFix方法，利用检索增强生成(RAG)结合Stack Overflow知识来自动修复响应式布局故障(RLFs)，准确率达88%


<details>
  <summary>Details</summary>
Motivation: 响应式网站在特定屏幕尺寸下经常出现布局变形问题，手动修复需要繁琐的试错调整HTML元素和CSS属性

Method: 基于检索增强生成(RAG)的方法，利用Stack Overflow讨论来指导LLM进行CSS修复，通过增强相关SO知识与RLF特定上下文创建提示词

Result: 评估显示该方法在修复RLFs方面达到88%的准确率，软件工程师研究表明生成的修复能够产生视觉正确的布局同时保持美观性

Conclusion: ReDeFix方法有效解决了响应式布局故障的自动修复问题，结合领域知识和LLM技术取得了良好效果

Abstract: Responsive websites frequently experience distorted layouts at specific
screen sizes, called Responsive Layout Failures (RLFs). Manually repairing
these RLFs involves tedious trial-and-error adjustments of HTML elements and
CSS properties. In this study, an automated repair approach, leveraging LLM
combined with domain-specific knowledge is proposed. The approach is named
ReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes
Stack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting
relevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that
is sent to the LLM to generate CSS patches. Evaluation demonstrates that our
approach achieves an 88\% accuracy in repairing RLFs. Furthermore, a study from
software engineers reveals that generated repairs produce visually correct
layouts while maintaining aesthetics.

</details>


### [80] [A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI](https://arxiv.org/abs/2511.00776)
*Cuiyun Gao,Guodong Fan,Chun Yong Chong,Shizhan Chen,Chao Liu,David Lo,Zibin Zheng,Qing Liao*

Main category: cs.SE

TL;DR: 这篇论文系统综述了代码导向大语言模型中的幻觉现象，从定义、成因、缓解策略、代码特定挑战到评估基准四个方面进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程任务中的广泛应用，理解和缓解代码幻觉问题变得至关重要，特别是在高风险代码智能任务中。

Method: 通过综述60篇相关论文，从四个关键角度系统分析代码幻觉：定义与成因、通用缓解策略、代码特定挑战、评估基准。

Result: 总结了代码幻觉的主要成因（数据噪声、暴露偏差、语义基础不足），缓解策略（知识增强生成、约束解码、后编辑），以及代码特定挑战（语法敏感性、严格类型系统、外部库依赖）。

Conclusion: 需要开发专门的幻觉导向评估基准，并利用程序分析、符号执行等新兴代码智能任务来检测和缓解幻觉。

Abstract: Model hallucination is one of the most critical challenges faced by Large
Language Models (LLMs), especially in high-stakes code intelligence tasks. As
LLMs become increasingly integrated into software engineering tasks,
understanding and mitigating hallucination in code becomes essential. In this
survey, we provide a systematic review of hallucination phenomena in
code-oriented LLMs from four key perspectives. First, we begin by surveying 60
papers to define hallucination in the context of code and summarize its primary
causes, such as data noise, exposure bias, and insufficient semantic grounding,
while also tracing recent trends in literature across natural language
processing (NLP) and software engineering communities. Second, we review model
hallucination surveys in a broader span and summarize representative
hallucination mitigation strategies, such as knowledge-enhanced generation,
constrained decoding, and post-editing. Third, we review approaches targeted
for code intelligence and highlight code-specific challenges that aggravate
hallucination, including syntax sensitivity, strict type systems, and
dependence on external libraries. Meanwhile, we analyze how emerging code
intelligence tasks, e.g., program analysis, symbolic execution, and unit
testing, are utilized to detect and mitigate hallucinations. Fourth, we
summarize current evaluation benchmarks, ranging from static metrics to dynamic
checks, e.g., compilation and execution correctness, and emphasize the need for
hallucination-oriented benchmarks.

</details>


### [81] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: 论文提出GrowthHacker基准，通过LLM和基于LLM的代理优化离线A/B测试(OPE)性能，开发了two_agent框架实现代码优化和评估循环。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试需要大量资源且可能对用户产生负面影响，而离线评估(OPE)使用日志数据评估技术，在医疗、推荐系统等高风险领域尤为重要。但目前缺乏利用LLM优化OPE结果的研究。

Method: 提出GrowthHacker基准，在真实世界数据集上实现代理和基线方法，开发two_agent框架降低系统复杂度同时保持优化效果，使用Open Bandit Pipeline和Scope-RL进行OPE评估。

Result: two_agent框架实现100%可靠性和106.7%平均改进率，与CrewAI均达到45%成功率，优于AutoGen的34%。

Conclusion: 基于LLM的代理可以作为自动化"增长黑客"来增强OPE系统，为生产环境中扩展数据驱动决策提供了可行性。

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [82] [CodeClash: Benchmarking Goal-Oriented Software Engineering](https://arxiv.org/abs/2511.00839)
*John Yang,Kilian Lieret,Joyce Yang,Carlos E. Jimenez,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: CodeClash是一个新的基准测试，让语言模型在多轮锦标赛中竞争，通过迭代开发代码来实现竞争性目标，评估模型在无明确指导下的自主代码开发能力。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要评估模型在具体、明确任务上的表现，但真实软件开发涉及追求高层次目标。需要评估模型能否在没有明确指导的情况下迭代开发代码来实现开放目标。

Method: 设计多轮锦标赛机制，每轮分为两个阶段：代理编辑代码，然后在代码竞技场中进行对抗。模型需要自行决定如何改进代码库来应对竞争目标。

Result: 运行1680场锦标赛（共25200轮），评估8个语言模型在6个竞技场中的表现。模型展现出多样化的开发风格，但在战略推理和长期代码库维护方面存在根本性限制。

Conclusion: 顶级模型在对抗人类专家程序员时全部失败，表明当前模型在自主、目标导向的代码开发方面仍有显著局限性。开源CodeClash以推动相关研究。

Abstract: Current benchmarks for coding evaluate language models (LMs) on concrete,
well-specified tasks such as fixing specific bugs or writing targeted tests.
However, human programmers do not spend all day incessantly addressing isolated
tasks. Instead, real-world software development is grounded in the pursuit of
high-level goals, like improving user retention or reducing costs. Evaluating
whether LMs can also iteratively develop code to better accomplish open-ended
objectives without any explicit guidance remains an open challenge. To address
this, we introduce CodeClash, a benchmark where LMs compete in multi-round
tournaments to build the best codebase for achieving a competitive objective.
Each round proceeds in two phases: agents edit their code, then their codebases
compete head-to-head in a code arena that determines winners based on
objectives like score maximization, resource acquisition, or survival. Whether
it's writing notes, scrutinizing documentation, analyzing competition logs, or
creating test suites, models must decide for themselves how to improve their
codebases both absolutely and against their opponents. We run 1680 tournaments
(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal
that while models exhibit diverse development styles, they share fundamental
limitations in strategic reasoning. Models also struggle with long-term
codebase maintenance, as repositories become progressively messy and redundant.
These limitations are stark: top models lose every round against expert human
programmers. We open-source CodeClash to advance the study of autonomous,
goal-oriented code development.

</details>


### [83] [A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks](https://arxiv.org/abs/2511.00872)
*Zhuowen Yin,Cuifeng Gao,Chunsong Fan,Wenzhang Yang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: 对7个通用代理框架在软件开发、漏洞检测和程序修复三个代码中心任务上的综合实证研究，从有效性、效率和开销三个维度系统评估代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定任务或孤立方面，无法全面了解代理在实际软件工程中的能力，需要进行全面评估以指导实践应用和未来研究。

Method: 使用标准基准测试评估7个通用代理框架在三个代表性代码中心任务上的表现，从有效性（任务成功率）、效率（执行过程）和开销（token消耗）三个互补角度进行系统分析。

Result: 代理整体表现中等；AgentOrchestra因协调开销而轨迹最长、修正尝试最多；OpenHands展示更强的反思推理能力；软件开发成本最高，GPTswarm最具成本效益。

Conclusion: 研究揭示了不同框架的能力模式和权衡关系，为软件工程代理的实际采用和未来研究提供了指导。

Abstract: Unlike traditional automation tools or static LLM-based systems, agents
combine decision-making and tool utilization to accomplish complex tasks,
showing great potential in software engineering. However, existing studies
largely focus on specific tasks or isolated aspects, providing an incomplete
picture of agents' practical capabilities. To address this, we conduct a
comprehensive empirical study evaluating seven general-purpose agent frameworks
across three representative code-centric tasks: software development,
vulnerability detection, and program repair. Each task is assessed using
standard, widely adopted benchmarks to ensure objective and comparable
evaluation. Agent performance is systematically analyzed from three
complementary perspectives: effectiveness (task success), efficiency (execution
process), and overhead (token consumption). Our findings reveal distinct
capability patterns and trade-offs among the evaluated frameworks. In terms of
effectiveness, agents achieve moderate overall performance. Regarding
efficiency, AgentOrchestra tends to exhibit the longest trajectories and the
most correction attempts due to coordination overhead, whereas OpenHands
demonstrate stronger reflective reasoning abilities. For overhead, software
development incurs the highest monetary cost, while GPTswarm remains the most
cost-efficient. Furthermore, we conduct an in-depth cross-analysis of the
relationship between effectiveness and efficiency, exploring the underlying
reasons behind their interplay. These findings guide both practical adoption
and future research toward more efficient software engineering agents.

</details>


### [84] [DPO-F+: Aligning Code Repair Feedback with Developers' Preferences](https://arxiv.org/abs/2511.01043)
*Zihan Fang,Yifan Zhang,Yueke Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: 提出了DPO-f+框架，通过直接偏好优化和轻量级边界信号来对齐代码修复反馈与开发者需求，提升LLM在软件工程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 开发者在理解LLM输出时遇到困难，限制了人机协作效果。现有工作主要优化修复代码，但忽视了支持理解和迭代改进的自然语言反馈。

Method: 1) 形式化开发者画像和领域特定指标；2) 从代码修复任务自动构建成对偏好数据集；3) 使用增强边界信号的DPO进行微调；4) 提供自动反馈评估协议。

Result: 在初学者编程任务中，DPO-f+比基线提升5.71个百分点，比标准DPO提升3.30个百分点；在SWE-bench Lite基准测试中，问题解决率比DPO高1.67个百分点，比基线高4.67个百分点。

Conclusion: DPO-f+通过更紧密地对齐反馈与开发者需求，将LLM辅助修复从一次性输出转变为协作理解工作流，增强了代码理解和人机协作效果。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
tasks, especially code repair. However, developers often struggle to interpret
model outputs, limiting effective human-AI teaming. Prior work largely
optimizes repaired code while under-addressing the natural-language feedback
that enables comprehension and iterative improvement. We present DPO-f+, a
novel framework that aligns code-repair feedback with developer needs and
profiles. It (1) formalizes developer-profiled, domain-specific metrics for
feedback alignment; (2) automatically constructs pairwise preference datasets
from code-repair tasks; (3) fine-tunes using Direct Preference Optimization
(DPO) augmented with a lightweight margin signal; and (4) provides an automated
feedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline
and standard DPO on generated-code accuracy and overall feedback alignment. On
novice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage
points (pp) over the baseline and by 3.30 pp over DPO. On the more challenging
SWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp
over DPO and by 4.67 pp over the baseline. It also achieves the largest
improvement in feedback alignment, outperforming DPO and the baseline. By
aligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted
repair from one-shot outputs into a collaborative sensemaking workflow,
providing a practical approach to enhancing code comprehension and fostering
more effective human-AI teaming in software engineering.

</details>


### [85] [HAFixAgent: History-Aware Automated Program Repair Agent](https://arxiv.org/abs/2511.01047)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: HAFixAgent是一个历史感知的bug修复代理，通过注入基于代码仓库历史的启发式信息来改进自动程序修复系统，特别针对复杂的多行bug。


<details>
  <summary>Details</summary>
Motivation: 现有的自动程序修复系统主要依赖本地快照上下文，忽略了代码仓库历史信息。研究表明仓库历史有助于修复单行bug，但尚未在基于代理的APR系统中大规模验证其对复杂多行bug的改进效果。

Method: 开发HAFixAgent代理系统，将基于blame的仓库历史启发式信息注入修复循环中。通过Defects4J中854个真实bug的初步研究指导设计，结合不同的历史启发式策略。

Result: HAFixAgent显著优于基于代理的基线（提升212.3%）和多行修复基线（提升29.9%）。历史信息不会显著增加代理步骤，保持token成本可比，对复杂多文件多行bug的中位成本更低。

Conclusion: HAFixAgent为历史感知的代理式APR提供了实用方案：将代理基于版本控制历史，优先使用基于diff的历史上下文，并在需要时集成互补的启发式方法。

Abstract: Automated program repair (APR) has recently shifted toward large language
models and agent-based systems, yet most systems rely on local snapshot
context, overlooking repository history. Prior work shows that repository
history helps repair single-line bugs, since the last commit touching the buggy
line is often the bug-introducing one. In this paper, we investigate whether
repository history can also improve agentic APR systems at scale, especially
for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing
Agent that injects blame-derived repository heuristics into its repair loop. A
preliminary study of all 854 real-world bugs from Defects4J motivates our
design, showing that bug-relevant history is both widely available and highly
concentrated. Empirical comparison of HAFixAgent with two state-of-the-art
baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the
agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)
Efficiency: history does not significantly increase agent steps and keeps token
costs comparable, with notably lower median costs for complex
multi-file-multi-hunk bugs. (3) Practicality: combining different historical
heuristics repairs more bugs, offering a clear cost-benefit trade-off.
HAFixAgent offers a practical recipe for history-aware agentic APR: ground the
agent in version control history, prioritize diff-based historical context, and
integrate complementary heuristics when needed.

</details>


### [86] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: HarnessLLM是一个两阶段训练管道，使LLM能够编写测试用的harness代码，生成合成输入和验证输出的代码，支持复杂测试用例和灵活的输出验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动测试生成方法主要产生输入-输出对，测试多样性有限且无法提供足够的调试信息。

Method: 采用SFT后接RLVR的两阶段训练管道，使用定制化奖励设计，使LLM生成测试harness代码。

Result: HarnessLLM在错误发现和测试策略多样性方面优于基于输入-输出的测试方法，并通过测试时扩展提升代码生成性能。

Conclusion: HarnessLLM能够生成更复杂的测试用例，提供更好的测试覆盖和调试信息，提升代码生成质量。

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [87] [An Empirical Study of LLM-Based Code Clone Detection](https://arxiv.org/abs/2511.01176)
*Wenqing Zhu,Norihiro Yoshida,Eunjong Choi,Yutaka Matsubara,Hiroaki Takada*

Main category: cs.SE

TL;DR: 评估5个LLM在7个代码克隆数据集上的性能，发现LLM在CodeNet数据集表现优异（o3-mini达到0.943 F1分数），但在BigCloneBench数据集性能显著下降，同时LLM在代码克隆检测中具有高响应一致性（超过90%的判断保持一致）。


<details>
  <summary>Details</summary>
Motivation: 现有研究证明了LLM在代码克隆检测中的有效性，但未解决两个关键问题：LLM在不同数据集上的可比性能表现，以及LLM在代码克隆检测中的响应一致性。

Method: 构建了7个代码克隆数据集，通过从CodeNet和BigCloneBench两个代码集合中采样代码对（使用Levenshtein比率），然后使用4种现有提示评估5个LLM在这些数据集上的表现。

Result: LLM在CodeNet相关数据集表现良好（o3-mini达到0.943 F1分数），但在BigCloneBench相关数据集性能显著下降；大多数模型具有高响应一致性（超过90%的判断保持一致），F1分数受不一致性影响的波动很小（变化小于0.03）。

Conclusion: LLM在代码克隆检测中表现出良好的性能，但在不同数据集上的表现存在显著差异，同时LLM具有高响应一致性，不一致性对性能的影响很小。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various software engineering tasks, such as code generation and debugging,
because of their ability to translate between programming languages and natural
languages. Existing studies have demonstrated the effectiveness of LLMs in code
clone detection. However, two crucial issues remain unaddressed: the ability of
LLMs to achieve comparable performance across different datasets and the
consistency of LLMs' responses in code clone detection. To address these
issues, we constructed seven code clone datasets and then evaluated five LLMs
in four existing prompts with these datasets. The datasets were created by
sampling code pairs using their Levenshtein ratio from two different code
collections, CodeNet and BigCloneBench. Our evaluation revealed that although
LLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943
F1 score, their performance significantly decreased in BigCloneBench-related
datasets. Most models achieved a high response consistency, with over 90\% of
judgments remaining consistent across all five submissions. The fluctuations of
the F1 score affected by inconsistency are also tiny; their variations are less
than 0.03.

</details>


### [88] [The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project](https://arxiv.org/abs/2511.01348)
*Robin Gröpler,Steffen Klepke,Jack Johns,Andreas Dreschinski,Klaus Schmid,Benedikt Dornauer,Eray Tüzün,Joost Noppen,Mohammad Reza Mousavi,Yongjian Tang,Johannes Viehmann,Selin Şirin Aslangül,Beum Seuk Lee,Adam Ziolkowski,Eric Zie*

Main category: cs.SE

TL;DR: GENIUS项目通过整合30多个欧洲工业界和学术界合作伙伴，旨在解决生成式AI在整个软件开发生命周期中应用的挑战，包括可靠性、问责制、安全性和数据隐私等问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程中显示出巨大潜力，但在整个软件开发生命周期中的应用尚未充分探索，存在可靠性、问责制、安全性和数据隐私等关键不确定性。

Method: 采用跨部门对话和GENIUS联盟内的经验，结合探索性文献综述，提出未来生成式AI在软件工程中的愿景，包括技术方法创新和工具开发。

Result: 提出了四个核心要素：当前挑战的结构化概述、未来五年的前瞻性愿景、软件专业人员角色和技能集的预期变化，以及GENIUS项目在实现这一转型中的贡献。

Conclusion: 通过将技术创新与业务相关性相结合，为研究和工业战略提供信息，为软件工程团队提供可靠、可扩展且行业就绪的生成式AI解决方案奠定基础。

Abstract: Generative AI (GenAI) has recently emerged as a groundbreaking force in
Software Engineering, capable of generating code, suggesting fixes, and
supporting quality assurance. While its use in coding tasks shows considerable
promise, applying GenAI across the entire Software Development Life Cycle
(SDLC) has not yet been fully explored. Critical uncertainties in areas such as
reliability, accountability, security, and data privacy demand deeper
investigation and coordinated action. The GENIUS project, comprising over 30
European industrial and academic partners, aims to address these challenges by
advancing AI integration across all SDLC phases. It focuses on GenAI's
potential, the development of innovative tools, and emerging research
challenges, actively shaping the future of software engineering. This vision
paper presents a shared perspective on the future of GenAI-based software
engineering, grounded in cross-sector dialogue and experience within the GENIUS
consortium, supported by an exploratory literature review. The paper explores
four central elements: (1) a structured overview of current challenges in GenAI
adoption across the SDLC; (2) a forward-looking vision outlining key
technological and methodological advances expected over the next five years;
(3) anticipated shifts in the roles and required skill sets of software
professionals; and (4) the contribution of GENIUS in realizing this
transformation through practical tools and industrial validation. By aligning
technical innovation with business relevance, this paper aims to inform both
research agendas and industrial strategies, providing a foundation for
reliable, scalable, and industry-ready GenAI solutions for software engineering
teams.

</details>


### [89] [Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy](https://arxiv.org/abs/2511.01757)
*Shamse Tasnim Cynthia,Banani Roy*

Main category: cs.SE

TL;DR: 提出一个两阶段检索框架，结合密集向量搜索和LLM重排序，显著提升Galaxy科学工作流管理系统中工作流的检索性能。


<details>
  <summary>Details</summary>
Motivation: Galaxy现有的基于关键词的检索系统在语义查询解释方面支持有限，当缺少精确术语匹配时往往无法找到相关工作流。

Method: 采用任务感知的两阶段检索框架：第一阶段使用最先进的嵌入模型检索候选工作流，第二阶段使用指令调优的生成式LLM（GPT-4o、Mistral-7B）基于语义任务对齐进行重排序。

Result: 该方法显著提高了top-k准确性和相关性，特别是对于长查询或未充分指定的查询。构建了带语义主题注释的基准数据集，并进行了全面的检索性能评估。

Conclusion: 这项工作通过LLM增强的工作流搜索提高了科学工作流的可用性和可访问性，特别是对新手用户和跨学科研究人员。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become
essential infrastructure in bioinformatics, supporting the design, execution,
and sharing of complex multi-step analyses. Despite hosting hundreds of
reusable workflows across domains, Galaxy's current keyword-based retrieval
system offers limited support for semantic query interpretation and often fails
to surface relevant workflows when exact term matches are absent. To address
this gap, we propose a task-aware, two-stage retrieval framework that
integrates dense vector search with large language model (LLM)-based reranking.
Our system first retrieves candidate workflows using state-of-the-art embedding
models and then reranks them using instruction-tuned generative LLMs (GPT-4o,
Mistral-7B) based on semantic task alignment. To support robust evaluation, we
construct a benchmark dataset of Galaxy workflows annotated with semantic
topics via BERTopic and synthesize realistic task-oriented queries using LLMs.
We conduct a comprehensive comparison of lexical, dense, and reranking models
using standard IR metrics, presenting the first systematic evaluation of
retrieval performance in the Galaxy ecosystem. Results show that our approach
significantly improves top-k accuracy and relevance, particularly for long or
under-specified queries. We further integrate our system as a prototype tool
within Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.
This work advances the usability and accessibility of scientific workflows,
especially for novice users and interdisciplinary researchers.

</details>


### [90] [SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring](https://arxiv.org/abs/2511.01850)
*Jiawei Jin,Yingxin Su,Xiaotong Zhu*

Main category: cs.SE

TL;DR: 本文提出了一种LLM集成的IDE，通过自动化MLOps流水线在单一环境中实现持续模型开发和监控，显著提升了ML工作流程的效率。


<details>
  <summary>Details</summary>
Motivation: 传统IDE主要关注代码编写，缺乏对完整ML生命周期的智能支持，而现有MLOps平台又与编码工作流脱节，需要弥合这一差距。

Method: 设计了一个集成LLM助手的IDE，具备代码生成、调试推荐和自动流水线配置功能，后端包含自动化数据验证、特征存储、漂移检测、重训练触发和CI/CD部署编排。

Result: 在UCI Adult和M5数据集上的实验表明，SmartMLOps Studio相比传统工作流减少了61%的流水线配置时间，提高了45%的实验可重现性，并提升了14%的漂移检测准确率。

Conclusion: 通过桥接智能代码辅助和自动化操作流水线，本研究为AI工程建立了新范式，将IDE从静态编码工具转变为动态、生命周期感知的智能平台。

Abstract: The rapid expansion of artificial intelligence and machine learning (ML)
applications has intensified the demand for integrated environments that unify
model development, deployment, and monitoring. Traditional Integrated
Development Environments (IDEs) focus primarily on code authoring, lacking
intelligent support for the full ML lifecycle, while existing MLOps platforms
remain detached from the coding workflow. To address this gap, this study
proposes the design of an LLM-Integrated IDE with automated MLOps pipelines
that enables continuous model development and monitoring within a single
environment. The proposed system embeds a Large Language Model (LLM) assistant
capable of code generation, debugging recommendation, and automatic pipeline
configuration. The backend incorporates automated data validation, feature
storage, drift detection, retraining triggers, and CI/CD deployment
orchestration. This framework was implemented in a prototype named SmartMLOps
Studio and evaluated using classification and forecasting tasks on the UCI
Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio
reduces pipeline configuration time by 61%, improves experiment reproducibility
by 45%, and increases drift detection accuracy by 14% compared to traditional
workflows. By bridging intelligent code assistance and automated operational
pipelines, this research establishes a novel paradigm for AI engineering -
transforming the IDE from a static coding tool into a dynamic, lifecycle-aware
intelligent platform for scalable and efficient model development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [91] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAE)进行特征选择和定向引导，在Llama-3 8B模型上实现了安全性能提升18.9%同时实用性提升11.1%，突破了传统安全-实用性权衡的限制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全部署方法需要调整模型权重且过程昂贵，而当前SAE方法缺乏系统性的特征选择机制和安全-实用性权衡的评估。

Method: 使用对比提示方法和AI生成提示数据集，通过SAE选择最佳引导特征，在不同引导强度下进行测试。

Result: 在Llama-3 8B模型上，安全性能提升18.9%，实用性同时提升11.1%。

Conclusion: 通过原则性特征选择方法识别最优特征，定向SAE引导可以克服传统安全-实用性权衡。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [92] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: TR-GRPO通过基于token概率的权重调节机制，解决了GRPO中低概率token梯度过度放大的问题，在RLVR任务中表现优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO算法存在低概率token梯度过度主导的问题，导致训练不稳定并抑制高概率token的学习贡献。

Method: 提出TR-GRPO方法，通过token级权重分配机制，使权重与模型预测概率正相关，从而降低低概率token的影响并强调高概率token。

Result: 在逻辑推理、数学推理和智能体推理等RLVR任务上的广泛实验表明，TR-GRPO始终优于GRPO。

Conclusion: 调节token贡献在RL训练中至关重要，TR-GRPO为增强LLM推理能力提供了一个稳健的框架。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [93] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 提出了Agent-REINFORCE框架，通过LLM代理增强的强化学习方法，在固定计算预算下搜索最优的多LLM协作图架构，显著提升了测试时扩展的性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常假设固定的协作架构和单一模型使用，忽略了不同任务可能需要不同的最优架构和模型组合。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，将REINFORCE流程映射为采样-反馈-更新，其中反馈作为文本梯度来更新概率图。

Result: Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能有效识别在准确性和推理延迟联合目标下的最优图。

Conclusion: 该方法能够高效搜索计算最优的多LLM协作图，为测试时扩展提供了更灵活和有效的解决方案。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [94] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: 提出一种基于多奖励信号的强化学习方法来微调基础模型，通过迭代微调策略在多个奖励上实现泛化，并在文本、生物序列和小分子生成等多个领域验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多应用如文本生成和药物发现中，单一奖励信号优化可能不够理想，因为通常需要多个评估标准。

Method: 采用基于多奖励信号的强化学习方法，通过迭代微调策略在多个奖励上进行优化。

Result: 在文本、生物序列和小分子生成等多个领域的实验结果表明，所提算法优于现有最先进基线方法。

Conclusion: 多奖励强化学习微调方法能够有效处理多标准优化问题，在多个应用领域展现出优越性能。

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [95] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文研究使用强化学习控制混沌对流流动，通过引入领域知识增强RL代理的泛化能力和样本效率，在Rayleigh-Bénard对流模型中实现了对混沌流动的有效控制。


<details>
  <summary>Details</summary>
Motivation: 混沌对流流动在微流体设备和化学反应器等实际系统中普遍存在，但传统控制方法在混沌状态下往往失效。虽然强化学习在层流控制中表现出潜力，但其在混沌和湍流动态下的泛化能力和鲁棒性尚未充分探索。

Method: 采用基于近端策略优化的领域知识强化学习代理，在多样初始条件和流动状态下进行训练。在奖励函数中引入鼓励Bénard单元合并的领域知识项，作为期望的宏观特性。

Result: 在层流状态下，领域知识RL代理将对流热传输减少高达33%；在混沌流动状态下仍能实现10%的减少，显著优于传统控制器。领域知识奖励设计产生稳定流动、训练收敛更快，且无需重新训练即可跨流动状态泛化。

Conclusion: 优雅的领域知识先验可以显著增强基于强化学习的混沌流动控制的鲁棒性，使实际部署更接近现实。

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [96] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: 提出了Tree Training方法，通过树形打包和梯度恢复技术，在智能体LLM训练中重用共享前缀计算，显著提升训练效率


<details>
  <summary>Details</summary>
Motivation: 当前训练流程将树形轨迹分解为独立线性段，导致共享前缀重复计算，效率低下

Method: Tree Training范式，包含树形打包（重用轨迹间共享计算）和梯度恢复（确保重用前缀的正确梯度传播）

Result: 在多个开源模型上实验，总训练时间最多减少3.9倍

Conclusion: Tree Training能够显著提升智能体LLM SFT和RL训练的计算效率

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [97] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: BOOM是一个将规划与离策略学习紧密结合的强化学习框架，通过引导循环和联合学习的世界模型，在DeepMind Control Suite和Humanoid-Bench上实现了最先进的训练稳定性和最终性能。


<details>
  <summary>Details</summary>
Motivation: 在线规划虽然能提高强化学习的样本效率和最终性能，但会导致收集的数据与策略实际行为之间的差异，从而降低模型学习和策略改进的效果。

Method: 提出BOOM框架，通过引导循环将规划与离策略学习紧密结合：策略初始化规划器，规划器通过行为对齐引导策略。使用联合学习的世界模型支持规划器模拟未来轨迹，并提供价值目标促进策略改进。核心是无似然对齐损失和软价值加权机制。

Result: 在高维DeepMind Control Suite和Humanoid-Bench上的实验表明，BOOM在训练稳定性和最终性能方面都达到了最先进的结果。

Conclusion: BOOM通过紧密整合规划和离策略学习，有效解决了规划导致的数据与行为差异问题，在复杂控制任务中表现出色。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [98] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC框架通过对比学习学习共享表示空间，选择最优推理方法，在提高准确性的同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择输出答案，假设更多候选答案能带来更高准确性。本文重新审视这一假设，通过理论分析发现这种假设并不总是成立。

Method: 提出EPIC框架，通过对比学习学习共享表示空间，捕捉模型推理能力和查询-方法兼容性。将概率界限作为正则化器纳入效用驱动的优化中，平衡准确性和计算成本。

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，提高准确性同时减少计算开销。

Conclusion: EPIC框架通过理论指导的方法选择，在保持高准确性的同时显著降低了计算成本，为语言模型推理方法选择提供了有效解决方案。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [99] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制框架，利用DreamerV3世界模型学习控制策略，通过集中决策规避多智能体协调复杂性，显著减少排队长度并展现抗波动能力。


<details>
  <summary>Details</summary>
Motivation: 传统交通信号控制模型难以捕捉现实交通复杂性，多智能体系统存在协调困难，需要开发更有效的自适应控制方法。

Method: 使用单智能体强化学习框架，结合邻接矩阵编码路网拓扑、实时排队状态和信号参数，采用DreamerV3世界模型学习控制策略，通过顺序选择交叉口和调整信号相位来调节交通流。

Result: SUMO仿真实验显示，在不同OD需求波动场景下，该框架具有鲁棒的抗波动能力，显著减少了排队长度。

Conclusion: 建立了一种与探测车辆技术兼容的智能交通控制新范式，为缓解交通拥堵提供了有效解决方案。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [100] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 提出了一种轻量级黑盒红队测试方法，使用现成LLM结合迭代反馈和上下文学习来发现激活探针的脆弱性模式，无需微调、梯度或架构访问。


<details>
  <summary>Details</summary>
Motivation: 激活探针作为AI系统监控器具有低成本和低延迟优势，但其在现实黑盒对抗压力下的鲁棒性尚未充分探索，需要发现失效模式并最小化测试成本。

Method: 采用轻量级黑盒红队测试流程，包装现成LLM并使用迭代反馈和上下文学习，无需微调、梯度或架构访问。通过高风险交互探针的案例研究验证方法。

Result: 发现了可解释的脆弱性模式（如法律术语导致的误报、平淡程序性语气的漏报），以及在场景约束攻击下减少但仍持续的漏洞。

Conclusion: 简单的提示式红队测试框架可以在部署前预测失效模式，并为未来探针的加固提供有前景的可操作见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [101] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 提出了一个统一的贝叶斯框架来解释LLM的控制方法，将上下文学习和激活引导视为改变模型对潜在概念信念的不同方式。


<details>
  <summary>Details</summary>
Motivation: 探索上下文学习和激活引导这两种看似不同的LLM控制方法是否可以从更广泛的统一框架来理解。

Method: 从贝叶斯角度开发了一个预测性统一框架，将上下文干预视为证据积累，激活引导视为改变概念先验。

Result: 该模型能够预测LLM在各种干预下的行为，解释了先前的经验现象（如S型学习曲线），并预测了新的现象（如对数信念空间中的干预可加性）。

Conclusion: 这项工作为基于提示和基于激活的LLM行为控制提供了统一解释，并为预测这些干预效果提供了方法论。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [102] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出逻辑增强强化学习(LIRL)，通过在标准策略梯度算法中引入投影机制，将低维潜在动作映射到由一阶逻辑定义的可行混合流形上，保证每个探索步骤的可行性，无需惩罚调优。


<details>
  <summary>Details</summary>
Motivation: 现有方法在混合动作空间中难以保证约束满足，分层方法牺牲全局最优性，而强化学习方法依赖脆弱的奖励惩罚、掩码或屏蔽机制。

Method: 在标准策略梯度算法中引入投影机制，将低维潜在动作映射到由一阶逻辑定义的可行混合流形上，保证每个探索步骤的可行性。

Result: 在工业制造、电动汽车充电站和交通信号控制等多个场景中均优于现有分层优化方法。以机器人减速器装配系统为例，相比传统工业分层调度方法，在综合完工时间-能耗目标上最多减少36.47%到44.33%，始终保持零约束违反，显著超越最先进的混合动作强化学习基线。

Conclusion: 该框架可无缝迁移到智能交通和智能电网等其他领域，为大规模CPS的安全实时优化铺平道路。

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [103] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出了一个均衡策略泛化(EPG)框架，用于在对抗性游戏中学习具有跨图零样本性能的泛化策略，特别适用于追逃游戏(PEG)。


<details>
  <summary>Details</summary>
Motivation: 解决追逃游戏中图结构变化时现有强化学习方法需要重新计算或微调的问题，提高实时适用性。

Method: 使用动态规划算法生成纯策略纳什均衡作为单图策略的均衡预言机，通过分组机制和序列模型扩展DP和RL以处理多个追捕者，提出距离特征用于跨图训练。

Result: EPG框架在各种未见过的真实世界图中展现出良好的零样本性能，在有出口的图中，泛化的追捕者策略甚至能与最先进PEG方法的微调策略相媲美。

Conclusion: EPG框架成功实现了在追逃游戏中的跨图泛化能力，为对抗性游戏中的均衡学习提供了有效解决方案。

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [104] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO是一种结合K-FAC二阶策略优化与安全感知梯度操作的安全强化学习算法，通过层式闭式近似Fisher信息矩阵实现高效稳定的自然梯度更新，并在Safety Gymnasium基准上相比最佳基线获得10.3%-50.2%的平均回报提升。


<details>
  <summary>Details</summary>
Motivation: 解决安全强化学习中奖励最大化与约束满足之间的权衡问题，避免传统方法中固定硬阈值导致的突变和不稳定。

Method: 结合Kronecker-Factored Approximate Curvature (K-FAC)二阶策略优化，采用边界感知梯度操作机制自适应调整奖励和成本梯度的影响，使用方向敏感投影消除有害干扰，并采用小批量KL回滚策略确保信任区域合规。

Result: 在Safety Gymnasium基准测试中，KFCPO相比遵守安全约束的最佳基线实现了10.3%到50.2%的平均回报提升，表现出安全与性能的优越平衡。

Conclusion: KFCPO通过结合高效的二阶优化和智能梯度操作，在安全强化学习任务中实现了更好的性能与安全平衡。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [105] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出RLAC方法，通过动态验证机制解决开放生成任务中多维度评估标准带来的验证成本问题，使用LLM作为批评器动态识别最可能的失败模式，联合优化生成器和批评器。


<details>
  <summary>Details</summary>
Motivation: 开放生成任务需要满足多样化的评估标准，但全面验证成本过高且评估不完整，使得基于规则的强化学习难以扩展。

Method: 使用LLM作为动态批评器识别最可能的失败模式，通过外部验证器验证，联合优化生成器和批评器，减少所需验证次数。

Result: 实验显示RLAC在文本生成中提高事实准确性，在代码生成中提高正确性，优于穷举验证和奖励模型方法。

Conclusion: 动态批评器比固定批评器更有效，RLAC有潜力扩展强化学习后训练到自由形式生成任务。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [106] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文提出数据感知能力应成为智能体系统设计的首要任务，包括主动数据获取、复杂数据处理、交互式测试数据合成和持续适应四个关键能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体研究主要关注推理能力，但忽视了数据处理能力。为了确保智能体在现实世界中的可靠部署，需要让智能体能够持续获取、处理和发展数据。

Method: 提出四个关键能力框架：主动数据获取、复杂数据处理、交互式测试数据合成和持续适应，以构建数据感知的智能体系统。

Result: 构建了一个系统性的数据感知智能体能力框架，为未来智能体研究提供了新的方向和重点。

Conclusion: 数据感知能力应成为智能体系统设计的核心，这是数据为中心AI的下一个前沿领域。

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [107] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习和基于代理模拟的新框架，用于优化电动汽车充电站布局，显著减少等待时间


<details>
  <summary>Details</summary>
Motivation: 现有充电站布局方法采用确定性奖励系统，无法充分应对现实世界动态和不确定条件，导致评估成本高且不反映真实场景

Method: 使用深度强化学习与基于代理模拟相结合的方法，采用具有双Q网络的混合RL代理来选择最优位置和配置充电端口，通过混合奖励函数结合确定性因素和模拟反馈

Result: 在越南河内的案例研究中，该方法相比初始状态将平均等待时间减少了53.28%，优于静态基准方法

Conclusion: 该可扩展和自适应解决方案增强了电动汽车基础设施规划，有效应对现实世界复杂性并改善用户体验

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [108] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: 提出了一种基于重参数化的多模态强化学习方法，通过距离多样性正则化解决了传统连续RL算法在多样性关键场景中的性能限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统连续深度强化学习算法使用确定性或单峰高斯策略，无法表达复杂的多模态决策分布，这在多样性关键场景中会限制性能。现有基于扩散或摊销策略的多模态RL方法存在计算不可行的问题。

Method: 首先在统一框架下重新表述现有不可行多模态策略，证明可以通过重参数化直接优化策略梯度；然后提出基于距离的多样性正则化方法，无需显式计算决策概率。

Result: 在多目标达成和生成式RL等多样性关键领域展示了多模态策略的优势，特别是在少样本鲁棒性方面；在传统MuJoCo基准测试中也表现出竞争力；实验表明摊销策略是具有强大多模态表达能力和高性能的有前景策略模型类别。

Conclusion: 提出的方法能够同时平衡性能、决策多样性和效率，解决了现有多模态RL方法的局限性，为多样性关键强化学习任务提供了有效解决方案。

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [109] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM是一个多代理图思维链系统，通过分解推理任务、优化LLM服务架构，显著提升了图推理的准确性、效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的图思维链方法存在准确性低、token使用过多、延迟高和吞吐量低的问题，主要由于单代理整体提示、重复上下文重新编码和低效的服务执行。

Method: 将推理分解为分类、推理、行动生成和图检索等专门代理，采用分支和选择性上下文共享；引入图思维链感知的LLM推理机制，包括图特定KV缓存管理、基于优先级的驱逐和流水线执行。

Result: GLM将答案准确性提升高达38%，token成本降低95.7%，推理延迟降低90.3%，吞吐量提高15.1倍。

Conclusion: GLM实现了复杂现实世界推理的高效规模化应用，是第一个与优化LLM服务架构协同设计的多代理图思维链系统。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>
