<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 17]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/abs/2510.12826)
*Thao Pham*

Main category: cs.CL

TL;DR: 该研究评估了前沿LLM代理在战略欺骗方面的能力和倾向，通过两种博弈论框架测试了四个模型，发现即使没有明确提示，模型也表现出显著的欺骗倾向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在多样化环境中自主部署，评估其战略欺骗能力变得至关重要。虽然已有研究考察AI系统如何对抗人类开发者，但LLM之间的欺骗行为仍未被充分探索。

Method: 使用两种博弈论框架：廉价谈话信号博弈和同行评估对抗博弈，测试了GPT-4o、Gemini-2.5-pro、Claude-3.7-Sonnet和Llama-3.3-70b四个模型，通过思维链推理分析欺骗策略。

Result: 当有提示时，大多数模型（特别是Gemini-2.5-pro和Claude-3.7-Sonnet）达到接近完美的表现。关键发现是，即使没有提示，所有模型在同行评估中都选择欺骗而非坦白（100%比率），在廉价谈话中选择欺骗的模型成功率达到95-100%。

Conclusion: 这些发现强调了在多智能体环境中使用高风险博弈论场景进行稳健评估的必要性。

Abstract: As large language model (LLM) agents are deployed autonomously in diverse
contexts, evaluating their capacity for strategic deception becomes crucial.
While recent research has examined how AI systems scheme against human
developers, LLM-to-LLM scheming remains underexplored. We investigate the
scheming ability and propensity of frontier LLM agents through two
game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation
adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,
Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and
without explicit prompting while analyzing scheming tactics through
chain-of-thought reasoning. When prompted, most models, especially
Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.
Critically, models exhibited significant scheming propensity without prompting:
all models chose deception over confession in Peer Evaluation (100% rate),
while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These
findings highlight the need for robust evaluations using high-stakes
game-theoretic scenarios in multi-agent settings.

</details>


### [2] [MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training](https://arxiv.org/abs/2510.12831)
*Taicheng Guo,Hai Wang,ChaoChun Liu,Mohsen Golalikhani,Xin Chen,Xiangliang Zhang,Chandan K. Reddy*

Main category: cs.CL

TL;DR: MTSQL-R1是一个用于多轮Text-to-SQL的智能体训练框架，通过将任务建模为马尔可夫决策过程，实现执行反馈和对话一致性验证的迭代优化循环。


<details>
  <summary>Details</summary>
Motivation: 现有系统将多轮Text-to-SQL视为简单的文本翻译任务，采用短视范式逐轮生成查询而不执行、验证和优化，导致输出不可执行或不连贯。

Method: 将任务建模为MDP，智能体与数据库交互获取执行反馈，与持久对话记忆交互进行一致性验证，执行'提出执行→验证→优化'的迭代循环直到所有检查通过。

Result: 在COSQL和SPARC数据集上的实验表明，MTSQL-R1持续优于强基线方法。

Conclusion: 环境驱动的验证和记忆引导的优化对于会话语义解析至关重要。

Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances
into executable SQL while preserving dialogue coherence and grounding to the
target schema. However, most existing systems only regard this task as a simple
text translation task and follow a short-horizon paradigm, generating a query
per turn without execution, explicit verification, and refinement, which leads
to non-executable or incoherent outputs. We present MTSQL-R1, an agentic
training framework for long-horizon multi-turn Text-to-SQL. We cast the task as
a Markov Decision Process (MDP) in which an agent interacts with (i) a database
for execution feedback and (ii) a persistent dialogue memory for coherence
verification, performing an iterative propose to execute -> verify -> refine
cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that
MTSQL-R1 consistently outperforms strong baselines, highlighting the importance
of environment-driven verification and memory-guided refinement for
conversational semantic parsing. Full recipes (including code, trained models,
logs, reasoning trajectories, etc.) will be released after the internal review
to contribute to community research.

</details>


### [3] [A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning](https://arxiv.org/abs/2510.12838)
*Qianben Chen,Jingyi Cao,Jiayu Zhang,Tianrui Qin,Xiaowan Li,King Zhu,Dingfeng Shi,He Zhu,Minghao Liu,Xiaobo Liang,Ge Zhang,Jian Yang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了自适应智能体基础模型(A²FM)，通过路由-对齐原则统一推理型和智能体型LLM，引入即时模式处理简单查询，显著提升成本效率。


<details>
  <summary>Details</summary>
Motivation: 解决推理型LLM和智能体型LLM之间的能力分裂问题，前者擅长内部推理但无法调用工具，后者能交互环境但推理能力较弱，且两者在简单查询上都存在过度思考或过度调用工具的低效问题。

Method: 采用路由-对齐原则：先学习任务感知路由，然后在共享骨干网络下对齐模式特定轨迹；引入即时模式处理简单查询；提出自适应策略优化(APO)，强制跨模式自适应采样并应用成本正则化奖励。

Result: 在32B规模上，A²FM在BrowseComp、AIME25和HLE基准上分别达到13.4%、70.4%和16.7%，在可比模型中创下新SOTA，在智能体、推理和通用基准上与前沿LLM竞争。自适应执行每个正确答案成本仅0.00487美元，相比推理模式降低成本45.2%，相比智能体模式降低成本33.5%。

Conclusion: A²FM框架成功统一了推理型和智能体型LLM的能力，通过自适应路由显著提升了成本效率，在保持可比准确性的同时大幅降低了计算成本。

Abstract: Large language models split into two families: reasoning-centric LLMs, which
strengthen internal chain-of-thought reasoning but cannot invoke external
tools, and agentic LLMs, which learn to interact with environments and leverage
tools but often lag in deep reasoning. This divide arises from fundamentally
different training objectives, leading to mismatched strengths and inefficiency
on simple queries, where both families tend to overthink or over-call tools. In
this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM),
a unified framework that follows a route-then-align principle: the model first
learns task-aware routing and then aligns mode-specific trajectories under a
shared backbone. To address the inefficiency gap, we introduce a third
mode-instant-that handles simple queries directly, preventing unnecessary
reasoning or tool calls while complementing the agentic and reasoning modes. To
jointly enhance accuracy and efficiency, we propose Adaptive Policy
Optimization (APO), which enforces adaptive sampling across modes and applies a
cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves
13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA
among comparable models and performing competitively with frontier LLMs across
agentic, reasoning, and general benchmarks. Notably, the adaptive execution
achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by
45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering
substantially higher cost efficiency while maintaining comparable accuracy.

</details>


### [4] [EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus](https://arxiv.org/abs/2510.12899)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Zhongxiang Dai,Kun Kuang*

Main category: cs.CL

TL;DR: 提出了EduDial，一个全面的多轮师生对话数据集，包含34,250个对话会话，覆盖345个核心知识点，并基于布鲁姆教育目标分类学和十种提问策略设计。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育领域的应用日益重要，需要专门的师生对话基准来评估其教学能力，以推动智能教育发展。

Method: 通过教师和学生代理之间的交互生成对话数据，结合布鲁姆教育目标分类学和十种提问策略，为不同认知水平的学生设计差异化教学策略。

Result: 在17个主流大语言模型上的实验表明，大多数模型在学生中心教学场景中表现不佳，而EduDial-LLM 32B在所有指标上均显著优于基线模型。

Conclusion: EduDial为评估大语言模型的教学能力提供了全面基准，EduDial-LLM在师生对话场景中表现出色，为智能教育应用提供了有力支持。

Abstract: Recently, several multi-turn dialogue benchmarks have been proposed to
evaluate the conversational abilities of large language models (LLMs). As LLMs
are increasingly recognized as a key technology for advancing intelligent
education, owing to their ability to deeply understand instructional contexts
and provide personalized guidance, the construction of dedicated
teacher-student dialogue benchmarks has become particularly important. To this
end, we present EduDial, a comprehensive multi-turn teacher-student dialogue
dataset. EduDial covers 345 core knowledge points and consists of 34,250
dialogue sessions generated through interactions between teacher and student
agents. Its design is guided by Bloom's taxonomy of educational objectives and
incorporates ten questioning strategies, including situational questioning,
zone of proximal development (ZPD) questioning, and metacognitive
questioning-thus better capturing authentic classroom interactions.
Furthermore, we design differentiated teaching strategies for students at
different cognitive levels, thereby providing more targeted teaching guidance.
Building on EduDial, we further develop EduDial-LLM 32B via training and
propose an 11-dimensional evaluation framework that systematically measures the
teaching abilities of LLMs, encompassing both overall teaching quality and
content quality. Experiments on 17 mainstream LLMs reveal that most models
struggle in student-centered teaching scenarios, whereas our EduDial-LLM
achieves significant gains, consistently outperforming all baselines across all
metrics. The code is available at
https://github.com/Mind-Lab-ECNU/EduDial/tree/main.

</details>


### [5] [Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering](https://arxiv.org/abs/2510.12925)
*Nil-Jana Akpinar,Chia-Jung Lee,Vanessa Murdock,Pietro Perona*

Main category: cs.CL

TL;DR: 本文首次系统评估了LLMs对询问角色的鲁棒性，发现用户个人属性信息会显著影响问答准确性并触发多种失败模式。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在面对包含用户身份、专业知识或信仰等个人属性的询问时的鲁棒性，这些是真实交互中常见的用户信息。

Method: 通过设计包含用户属性信息的询问角色提示，系统测试LLMs在接收不同用户背景信息时的问答表现。

Result: 用户角色提示能显著改变问答准确性，触发拒绝回答、幻觉限制和角色混淆等失败模式。

Conclusion: 模型对用户框架的敏感性会损害事实可靠性，询问角色测试是有效的鲁棒性评估工具。

Abstract: Large Language Models (LLMs) should answer factual questions truthfully,
grounded in objective knowledge, regardless of user context such as
self-disclosed personal information, or system personalization. In this paper,
we present the first systematic evaluation of LLM robustness to inquiry
personas, i.e. user profiles that convey attributes like identity, expertise,
or belief. While prior work has primarily focused on adversarial inputs or
distractors for robustness testing, we evaluate plausible, human-centered
inquiry persona cues that users disclose in real-world interactions. We find
that such cues can meaningfully alter QA accuracy and trigger failure modes
such as refusals, hallucinated limitations, and role confusion. These effects
highlight how model sensitivity to user framing can compromise factual
reliability, and position inquiry persona testing as an effective tool for
robustness evaluation.

</details>


### [6] [The Curious Case of Curiosity across Human Cultures and LLMs](https://arxiv.org/abs/2510.12943)
*Angana Borah,Rada Mihalcea*

Main category: cs.CL

TL;DR: 该论文研究了LLMs在不同文化背景下的好奇心表达，发现LLMs会削弱跨文化多样性，更接近西方国家的表达方式，并通过微调策略将人机对齐差距缩小了50%。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在跨文化背景下好奇心的表达差异，因为好奇心是驱动探究的核心因素，但在这些系统中尚未得到充分研究。

Method: 使用Yahoo! Answers多国数据集，引入CUEST评估框架，通过语言风格、主题偏好分析和社会科学构建来测量人机好奇心对齐。

Result: 发现LLMs会削弱跨文化多样性，更接近西方国家的表达方式；通过微调策略可将人机对齐差距缩小50%；证明好奇心对LLMs跨文化适应性的实用价值。

Conclusion: 好奇心对于LLMs的跨文化适应性具有重要价值，是未来NLP研究的重要方向。

Abstract: Recent advances in Large Language Models (LLMs) have expanded their role in
human interaction, yet curiosity -- a central driver of inquiry -- remains
underexplored in these systems, particularly across cultural contexts. In this
work, we investigate cultural variation in curiosity using Yahoo! Answers, a
real-world multi-country dataset spanning diverse topics. We introduce CUEST
(CUriosity Evaluation across SocieTies), an evaluation framework that measures
human-model alignment in curiosity through linguistic (style), topic preference
(content) analysis and grounding insights in social science constructs. Across
open- and closed-source models, we find that LLMs flatten cross-cultural
diversity, aligning more closely with how curiosity is expressed in Western
countries. We then explore fine-tuning strategies to induce curiosity in LLMs,
narrowing the human-model alignment gap by up to 50\%. Finally, we demonstrate
the practical value of curiosity for LLM adaptability across cultures, showing
its importance for future NLP research.

</details>


### [7] [On the Role of Preference Variance in Preference Optimization](https://arxiv.org/abs/2510.13022)
*Jiacheng Guo,Zihao Li,Jiahao Qiu,Yue Wu,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文研究了偏好方差（PVar）对DPO训练效果的影响，发现高PVar的提示能产生更大的梯度更新，从而更有效地进行LLM对齐。


<details>
  <summary>Details</summary>
Motivation: 收集人类偏好数据成本高昂且效率低下，需要减少所需标注的方法。偏好方差可以衡量模型在比较响应对时的偏好差异程度。

Method: 通过理论分析建立DPO梯度范数的上界，证明其受PVar控制。使用奖励模型生成偏好数据，在AlpacaEval 2.0和Arena-Hard基准上微调LLMs，比较不同PVar提示的训练效果。

Result: 实验表明高PVar提示优于随机选择或低PVar提示。使用小奖励模型（1B、3B）进行选择时，PVar方法依然稳健。在UltraFeedback数据集上，仅使用PVar最高的前10%提示训练，效果优于使用完整数据集。

Conclusion: 偏好方差是识别信息丰富示例以进行高效LLM对齐的重要指标，高PVar提示能产生更有效的学习信号。

Abstract: Direct Preference Optimization (DPO) has emerged as an important approach for
learning from human preferences in aligning large language models (LLMs).
However, collecting human preference data is costly and inefficient, motivating
methods to reduce the required annotations. In this work, we investigate the
impact of \emph{preference variance} (PVar), which measures the variance in
model preferences when comparing pairs of responses, on the effectiveness of
DPO training. We provide a theoretical insight by establishing an upper bound
on the DPO gradient norm for any given prompt, showing it is controlled by the
PVar of that prompt. This implies that prompts with low PVar can only produce
small gradient updates, making them less valuable for learning. We validate
this finding by fine-tuning LLMs with preferences generated by a reward model,
evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental
results demonstrate that prompts with higher PVar outperform randomly selected
prompts or those with lower PVar. We also show that our PVar-based selection
method is robust, when using smaller reward models (1B, 3B) for selection.
Notably, in a separate experiment using the original human annotations from the
UltraFeedback dataset, we found that training on only the top 10\% of prompts
with the highest PVar yields better evaluation performance than training on the
full dataset, highlighting the importance of preference variance in identifying
informative examples for efficient LLM alignment.

</details>


### [8] [Stable LLM Ensemble: Interaction between Example Representativeness and Diversity](https://arxiv.org/abs/2510.13143)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 该研究系统分析了示例代表性（单样本策略）和输出多样性（采样温度）对LLM集成性能的影响，提出基于质心的代表性示例选择方法，在较高温度设置下显著优于随机选择和5样本提示。


<details>
  <summary>Details</summary>
Motivation: 当前单样本LLM预测的准确性和鲁棒性对示例选择和集成成员多样性高度敏感，需要系统研究如何通过优化示例选择和温度控制来提升集成性能。

Method: 比较两种单样本策略：基于质心的代表性示例（提出方法）和随机采样示例（基线），同时变化采样温度，评估对LLM集成性能的影响。

Result: 提出的方法在较高温度设置下显著优于随机选择：+7.6%（宏F1）和-10.5%（RMSE），且超过5样本提示：+21.1%（宏F1）和-24.0%（RMSE）。

Conclusion: 结合代表性示例选择和增加温度能为集成提供适当的多样性水平，强调了示例选择和受控多样性在设计有效单样本LLM集成中的实际重要性。

Abstract: Large language models (LLMs) have achieved remarkable results in wide range
of domains. However, the accuracy and robustness of one-shot LLM predictions
remain highly sensitive to the examples and the diversity among ensemble
members. This study systematically investigates the effects of example
representativeness (one-shot strategy) and output diversity (sampling
temperature) on LLM ensemble performance. Two one-shot strategies are compared:
centroid-based representative examples (proposed) and randomly sampled examples
(baseline) and sampling temperature also is varied. The proposed approach with
higher temperature setting significantly outperforms random selection by +7.6%
(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot
prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that
combining representative example selection with increased temperature provides
the appropriate level of diversity to the ensemble. This work highlights the
practical importance of both example selection and controlled diversity in
designing effective one-shot LLM ensembles.

</details>


### [9] [A Matter of Representation: Towards Graph-Based Abstract Code Generation](https://arxiv.org/abs/2510.13163)
*Nyx Iskandar,Hisham Bedri,Andy Tsen*

Main category: cs.CL

TL;DR: 该论文提出并评估了用于图结构抽象代码生成的JSON表示方法，在基于Scratch的测试基准上验证了LLM能够单次生成图结构代码，且不同表示方法对准确率有显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM擅长生成原始顺序代码，但在图结构抽象代码生成方面研究较少，这种能力对可视化编程语言和无法访问源代码的场景很重要。

Method: 提出并评估了图结构的JSON表示方法，在基于Python重实现的Scratch测试基准ScratchTest上进行评估。

Result: LLM能够在单次生成中完成图结构代码生成任务，无需复杂流程，且不同表示方法导致准确率显著差异。

Conclusion: 这项工作为图结构抽象代码生成的表示学习奠定了基础。

Abstract: Most large language models (LLMs) today excel at generating raw, sequential
code with minimal abstractions and custom structures. However, there has been
little work on graph-based abstract code generation, where significant logic is
encapsulated in predefined nodes and execution flow is determined by edges.
This is relevant for visual programming languages, and in cases where raw
source code is inaccessible to users and LLM training sets. In this work, we
propose and evaluate JSON representations for graphs to enable high accuracy
graph-based abstract code generation. We evaluate these representations on
ScratchTest, a mini-benchmark based on our custom Python re-implementation of
Scratch, which tests the LLM in code graph space. Our findings demonstrate that
LLMs can indeed perform the aforementioned generation task in a single pass
without relying on specialized or complex pipelines, given the correct graph
representations. We also show that different representations induce
significantly different accuracies, highlighting the instrumental role of
representations in this generation task. All in all, this work establishes the
first steps towards representation learning for graph-based abstract code
generation.

</details>


### [10] [Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism](https://arxiv.org/abs/2510.13170)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Duanyang Yuan,Haoyuan Chen,Xiaoyu Sun,Linyuan Meng,Xinwang Liu*

Main category: cs.CL

TL;DR: 本文首次从人类推理理论角度系统综述思维链微调技术，基于六顶思考帽框架对CoT微调方法进行分类分析，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有关于思维链微调的研究主要关注技术层面，缺乏从人类推理机制角度的系统分析。鉴于CoT微调的最终目标是让大语言模型像人类一样推理，从认知角度研究这一技术至关重要。

Method: 采用六顶思考帽框架对CoT微调方法进行分类和系统分析，构建了数据集和模型性能的全面概述，并维护实时GitHub仓库跟踪最新进展。

Result: 提出了基于人类推理理论的CoT微调系统分类框架，识别了现有方法在模拟人类推理能力方面的特点，并指出了未来研究方向。

Conclusion: 这项调查为CoT微调领域提供了基于人类认知理论的新视角，有望激发创新并推动这一快速发展领域的进步。

Abstract: Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs)
with reasoning capabilities by training them on curated reasoning traces. It
leverages both supervised and reinforced fine-tuning to cultivate human-like
reasoning skills in LLMs, including detailed planning, divergent thinking,
intuitive judgment, timely reflection, internal thinking, and fact perception,
etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial
improvements in tasks such as mathematical reasoning and code generation.
However, existing surveys about CoT fine-tuning primarily focus on technical
aspects and overlook a systematic analysis from the perspective of human
reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to
enable LLMs to reason like humans, it is crucial to investigate this technique
through the lens of human cognition. To fill this gap, we present the first
comprehensive survey of CoT fine-tuning grounded in human reasoning theory.
Specifically, inspired by the well-known Six Thinking Hats framework, which
systematically characterizes common human thinking modes using six metaphorical
hats, we classify and examine CoT fine-tuning methods through this lens.
Furthermore, building upon this theory, we outline potential directions for
future research in CoT fine-tuning. In addition, we compile a comprehensive
overview of existing datasets and model performances, and a real-time GitHub
repository \footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that
continuously tracks recent advances in this area is maintained. We hope this
survey will serve as a valuable resource to inspire innovation and foster
progress in this rapidly evolving field.

</details>


### [11] [Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](https://arxiv.org/abs/2510.13255)
*Jingmin An,Yilong Song,Ruolin Yang,Nai Ding,Lingxi Lu,Yuxuan Wang,Wei Wang,Chu Zhuang,Qian Wang,Fang Fang*

Main category: cs.CL

TL;DR: 该论文提出了分层频率标记探针(HFTP)，通过频域分析识别LLM中处理句法结构的神经元组件和大脑皮层区域，发现LLM与人类大脑在句法处理上存在相似性和差异性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM的语言能力是否源于与人类大脑相似的机制，以及识别负责句法处理的具体计算模块。

Method: 使用分层频率标记探针(HFTP)进行频域分析，比较GPT-2、Gemma、Llama等多个LLM模型与人类大脑皮层在句法处理上的表征相似性。

Result: LLM在相似层处理句法，而人类大脑依赖不同皮层区域；LLM表征与大脑左半球更相似；模型升级呈现不同趋势：Gemma 2比Gemma更接近大脑，而Llama 3.1比Llama 2更偏离。

Conclusion: HFTP是连接计算语言学和认知神经科学的有价值工具，揭示了LLM行为改进的机制可能并非完全人类化。

Abstract: Large Language Models (LLMs) demonstrate human-level or even superior
language abilities, effectively modeling syntactic structures, yet the specific
computational modules responsible remain unclear. A key question is whether LLM
behavioral capabilities stem from mechanisms akin to those in the human brain.
To address these questions, we introduce the Hierarchical Frequency Tagging
Probe (HFTP), a tool that utilizes frequency-domain analysis to identify
neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)
neurons) and cortical regions (via intracranial recordings) encoding syntactic
structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama
2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human
brain relies on distinct cortical regions for different syntactic levels.
Representational similarity analysis reveals a stronger alignment between LLM
representations and the left hemisphere of the brain (dominant in language
processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows
greater brain similarity than Gemma, while Llama 3.1 shows less alignment with
the brain compared to Llama 2. These findings offer new insights into the
interpretability of LLM behavioral improvements, raising questions about
whether these advancements are driven by human-like or non-human-like
mechanisms, and establish HFTP as a valuable tool bridging computational
linguistics and cognitive neuroscience. This project is available at
https://github.com/LilTiger/HFTP.

</details>


### [12] [Do You Get the Hint? Benchmarking LLMs on the Board Game Concept](https://arxiv.org/abs/2510.13271)
*Ine Gevers,Walter Daelemans*

Main category: cs.CL

TL;DR: Concept游戏作为评估LLM溯因推理能力的基准，结果显示LLM在需要抽象推理和战略意图理解的任务上表现不佳，成功率低于40%，远低于人类的90%以上。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在需要抽象推理的任务上仍有根本性弱点，特别是处理与自然语言训练数据表示形式不同的任务时表现不佳。

Method: 引入Concept单词猜测棋盘游戏作为基准，在更接近LLM预训练数据（自然语言）的表示中测试溯因推理能力，并扩展到多种语言进行评估。

Result: 人类成功率超过90%，而最先进的LLM成功率不超过40%。LLM在解释其他玩家战略意图和根据顺序信息更新修正初始假设方面存在困难。

Conclusion: Concept游戏揭示了LLM在抽象推理和战略理解方面的显著局限性，特别是在低资源语言中表现更差。

Abstract: Large language models (LLMs) have achieved striking successes on many
benchmarks, yet recent studies continue to expose fundamental weaknesses. In
particular, tasks that require abstract reasoning remain challenging, often
because they use representations such as grids, symbols, or visual patterns
that differ from the natural language data LLMs are trained on. In this paper,
we introduce Concept, a simple word-guessing board game, as a benchmark for
probing abductive reasoning in a representation that is much closer to LLM
pre-training data: natural language. Our results show that this game, easily
solved by humans (with a success rate of over 90\%), is still very challenging
for state-of-the-art LLMs (no model exceeds 40\% success rate). Specifically,
we observe that LLMs struggle with interpreting other players' strategic
intents, and with correcting initial hypotheses given sequential information
updates. In addition, we extend the evaluation across multiple languages, and
find that the LLM performance drops further in lower-resource languages (Dutch,
French, and Spanish) compared to English.

</details>


### [13] [Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13272)
*Zhichao Xu,Zongyu Wu,Yun Zhou,Aosong Feng,Kang Zhou,Sangmin Woo,Kiran Ramnath,Yijun Tian,Xuan Qi,Weikang Qiu,Lin Lee Cheong,Haibo Ding*

Main category: cs.CL

TL;DR: 提出VERITAS框架，通过将细粒度忠实性奖励集成到强化学习中，提升基于RL的搜索代理的推理忠实性，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的搜索代理方法虽然能提升问答基准性能，但过于关注最终答案正确性而忽略了中间推理步骤的质量，可能导致思维链不忠实的问题。

Method: 首先引入全面的评估框架评估基于RL的搜索代理的忠实性，然后提出VERITAS框架，在强化学习过程中集成细粒度忠实性奖励。

Result: 实验表明，使用VERITAS训练的模型不仅显著提高了推理忠实性，还在七个问答基准上实现了可比较的任务性能。

Conclusion: VERITAS框架能有效提升搜索代理的推理忠实性，同时保持任务性能，解决了现有方法忽视中间推理质量的问题。

Abstract: Inspired by the success of reinforcement learning (RL) in Large Language
Model (LLM) training for domains like math and code, recent works have begun
exploring how to train LLMs to use search engines more effectively as tools for
retrieval-augmented generation. Although these methods achieve performance
improvement across QA benchmarks, many prioritize final answer correctness
while overlooking the quality of intermediate reasoning steps, which may lead
to chain-of-thought unfaithfulness. In this paper, we first introduce a
comprehensive evaluation framework for evaluating RL-based search agents,
covering three distinct faithfulness metrics: information-think faithfulness,
think-answer faithfulness, and think-search faithfulness. Our evaluations
reveal that a prototypical RL-based search agent, Search-R1, has significant
room for improvement in this regard. To foster faithful reasoning, we introduce
VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in
Agentic Search), a novel framework that integrates fine-grained faithfulness
rewards into the reinforcement learning process. Our experiments show that
models trained with VERITAS not only significantly improve reasoning
faithfulness, but also achieve comparable task performance across seven QA
benchmarks.

</details>


### [14] [ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering](https://arxiv.org/abs/2510.13312)
*Simon Lupart,Mohammad Aliannejadi,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: ChatR1是一个基于强化学习的对话问答推理框架，通过交错搜索和推理来处理用户意图的动态演变，使用意图感知奖励解决稀疏奖励问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 对话问答中用户意图会随对话轮次演变，且话语往往不完整，需要上下文解释、查询重构以及检索与生成的动态协调，静态管道方法无法有效处理这些挑战。

Method: 提出基于强化学习的推理框架，交错进行搜索和推理，采用意图感知奖励提供轮次级反馈，使检索和推理与用户目标对齐。

Result: 在3B和7B模型骨干上均表现强劲，在五个对话问答数据集上优于竞争模型，通过F1、BERTScore和LLM-as-judge等指标验证。

Conclusion: 基于强化学习的推理比静态对话问答管道更灵活和上下文敏感，能够实现多样化的推理轨迹和有效的搜索工具使用。

Abstract: We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of sparse and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.

</details>


### [15] [D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree](https://arxiv.org/abs/2510.13363)
*Xiang Lei,Qin Li,Min Zhang,Min Zhang*

Main category: cs.CL

TL;DR: D-SMART是一个模型无关框架，通过动态结构化记忆和推理树来维护多轮对话的一致性，显著提升对话质量。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在多轮对话中出现事实不一致和逻辑衰减的问题，现有方法如RAG和代理工作记忆仍依赖静态知识源，无法适应动态对话上下文。

Method: 使用动态结构化记忆(DSM)构建权威知识图谱，结合推理树(RT)进行多步图搜索推理，并引入基于NLI的新指标来评估对话一致性。

Result: 在MT-Bench-101基准测试中，D-SMART显著优于现有方法，对话一致性得分提升超过48%，开源模型质量得分提升达10.1%。

Conclusion: D-SMART通过动态结构化表示和显式推理有效解决了多轮对话中的一致性问题，为LLMs在复杂对话场景中的应用提供了新思路。

Abstract: Large Language Models (LLMs) often exhibit factual inconsistencies and
logical decay in extended, multi-turn dialogues, a challenge stemming from
their reliance on static, pre-trained knowledge and an inability to reason
adaptively over the dialogue history. Prevailing mitigation strategies, such as
Retrieval-Augmented Generation (RAG) and agentic working memories, improve
information recall but still engage with fundamentally static knowledge sources
and follow pre-defined single reasoning path. This hinders their ability to
preserve factual and logical consistency of their responses in multi-turn
dialogues while the context evolves over time. To address this issue, we
propose D-SMART, a model-agnostic framework designed to maintain multi-turn
dialogue consistency by enabling LLMs to build and reason over a dynamic,
structured representation of the conversational context. This is achieved via
two synergistic components: (1) a Dynamic Structured Memory (DSM), which
incrementally constructs and maintains an authoritative, OWL-compliant
knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which
executes inferences as an explicit and traceable multi-step search over the
graph. As the popular-used quality score (judged by GPT-4) can overlook logical
flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue
consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that
D-SMART significantly outperforms state-of-the-art baselines, elevating the
dialogue consistency score by over 48\% for both proprietary and open-source
models, and notably improves the quality score of the latter by up to 10.1\%.

</details>


### [16] [Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment](https://arxiv.org/abs/2510.13387)
*Buwei He,Yang Liu,Zhaowei Zhang,Zixia Jia,Huijia Wu,Zhaofeng He,Zilong Zheng,Yipeng Kang*

Main category: cs.CL

TL;DR: 该论文探索了在单轮对话中应用贝叶斯说服框架来增强大语言模型的战略说服能力，通过承诺-沟通机制让说服者明确描述信息模式，引导被说服者进行贝叶斯信念更新。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在说服能力方面存在挑战，现有研究往往忽视信息不对称的战略使用或依赖强假设的预先承诺。

Method: 提出了两种贝叶斯说服方法：半正式自然语言和全自然语言，并在包含多样化被说服者的综合评估框架中与基线方法进行比较。

Result: 实验发现：1）基于贝叶斯说服的LLM比基线方法获得更高说服成功率；2）半正式自然语言版本更具可信度和逻辑一致性，全自然语言版本在情感共鸣和鲁棒性方面更强；3）通过监督微调，小模型能达到与大模型相当的性能。

Conclusion: 贝叶斯说服框架能有效提升LLM的战略说服能力，不同方法变体各有优势，且模型规模不是性能的决定性因素。

Abstract: Persuasion, a fundamental social capability for humans, remains a challenge
for AI systems such as large language models (LLMs). Current studies often
overlook the strategic use of information asymmetry in message design or rely
on strong assumptions regarding pre-commitment. In this work, we explore the
application of Bayesian Persuasion (BP) in natural language within single-turn
dialogue settings, to enhance the strategic persuasion capabilities of LLMs.
Our framework incorporates a commitment-communication mechanism, where the
persuader explicitly outlines an information schema by narrating their
potential types (e.g., honest or dishonest), thereby guiding the persuadee in
performing the intended Bayesian belief update. We evaluate two variants of our
approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language
(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)
baselines within a comprehensive evaluation framework. This framework covers a
diverse set of persuadees -- including LLM instances with varying prompts and
fine-tuning and human participants -- across tasks ranging from specially
designed persuasion scenarios to general everyday situations. Experimental
results on LLM-based agents reveal three main findings: (1) LLMs guided by BP
strategies consistently achieve higher persuasion success rates than NBP
baselines; (2) SFNL exhibits greater credibility and logical coherence, while
FNL shows stronger emotional resonance and robustness in naturalistic
conversations; (3) with supervised fine-tuning, smaller models can attain BP
performance comparable to that of larger models.

</details>


### [17] [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554)
*Yang Li,Zhichen Dong,Yuhan Sun,Weixun Wang,Shaopan Xiong,Yijia Luo,Jiashun Liu,Han Lu,Jiamang Wang,Wenbo Su,Bo Zheng,Junchi Yan*

Main category: cs.CL

TL;DR: 本文通过分析注意力机制揭示LLM的推理模式，提出两种注意力度量指标，发现预规划-锚定机制，并基于此开发了三种针对关键节点的强化学习策略。


<details>
  <summary>Details</summary>
Motivation: LLM的推理模式不透明，传统强化学习对生成内容均匀分配信用，无法区分关键步骤和常规步骤。

Method: 1) 区分局部和全局注意力头；2) 提出窗口平均注意力距离和未来注意力影响两个指标；3) 发现预规划-锚定机制；4) 开发三种针对关键节点的RL策略。

Result: 揭示了LLM推理中的预规划-锚定机制，提出的RL策略在各种推理任务中获得了持续的性能提升。

Conclusion: 通过将优化与模型内在推理节奏对齐，将不透明的优化转变为结构感知的过程，为LLM推理的透明有效优化提供了可能路径。

Abstract: The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [18] [👋 Meet cto.new: a new AI code agent that crushes benchmarks - and costs $0](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/cuGpdXUET-o5tS2GOz4U4iX4r1Uo_mIEt7YR5cY1sP0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍了一个名为cto.new的新型AI代码代理，它在基准测试中表现优于Claude Code和Codex，并且完全免费使用


<details>
  <summary>Details</summary>
Motivation: 提供比现有商业AI代码代理（如Claude Code和Codex）性能更好且完全免费的替代方案

Method: 开发了一个新的AI代码代理系统，支持GPT-5、Sonnet 4.5等模型，无需API密钥即可使用

Result: 在基准测试中持续超越Anthropic的Claude Code和OpenAI的Codex，同时提供免费服务

Conclusion: cto.new是一个性能优越且完全免费的AI代码代理解决方案，为用户提供了更好的选择

Abstract: 👋 Meet cto.new: a new AI code agent that crushes benchmarks - and costs $0 (Sponsor) Psst: there's a new kid on the block — and it launched today. It consistently benchmarks higher than Anthropic's Claude Code and OpenAI's Codex. And instead of $200/month for premium usage, you pay absolutely nothing. Use on GPT-5, Sonnet 4.5 and more, no API keys required. Start now

</details>


### [19] [❤️ "I love paying Anthropic $200/mo"](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/4/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/8xT6Dx5m_5OEdpPwGZREmi7X1VTe_r7xZeTmQ2ixBF8=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍了一个免费的AI代码助手，声称在基准测试中优于Claude Code且无需付费


<details>
  <summary>Details</summary>
Motivation: 为用户提供比付费AI代码助手（如Claude Code）更优且免费的替代方案

Method: 开发了一个新的AI代码代理工具，通过基准测试验证其性能

Result: 该工具在基准测试中超越了Claude Code，并且完全免费使用

Conclusion: cto.new是一个性能优越且免费的AI代码助手选择

Abstract: ❤️ "I love paying Anthropic $200/mo" (Sponsor) If you love paying, follow your heart. Otherwise, you might want to try the new AI code agent that launched today — it beats Claude Code on benchmarks and costs $0. Experience cto.new - it's free

</details>


### [20] [Just Talk To It - the no-bs Way of Agentic Engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2Fjust-talk-to-it%3Futm_source=tldrnewsletter/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/0ZXg2APLQBSH7sMGD8Fd4VBv2odtqgJg847WfFM60LE=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过与智能体直接对话来改进其性能，无需复杂工程


<details>
  <summary>Details</summary>
Motivation: 简化智能体开发流程，避免过度工程化，通过自然对话提升智能体表现

Method: 采用直接对话的方式与智能体交互，通过持续对话优化智能体行为

Result: 随着对话次数增加，智能体表现逐渐改善，用户与智能体的协作效果更好

Conclusion: 直接对话是提升智能体性能的有效方法，工程复杂度越低效果越好

Abstract: Just Talk To It - the no-bs Way of Agentic Engineering (23 minute read) Just talk to your agents - the more you work with them, the better your results will be.

</details>


### [21] [How we test a web framework](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwasp.sh%2Fblog%2F2025%2F10%2F07%2Fhow-we-test-a-web-framework%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/nY7ek6OWjUEZJE1ww29cU8pzcOXtY3Tbn2xo1lyfIV4=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Wasp测试全栈Web框架的策略：优先选择能清晰表达输入和预期输出的测试，重勇气而非100%覆盖率，重度依赖类型系统，使用快照测试追踪代码生成变化，Playwright验证应用。


<details>
  <summary>Details</summary>
Motivation: 为全栈Web框架建立有效的测试策略，在测试覆盖率和测试质量之间找到平衡，确保框架的可靠性和可维护性。

Method: 采用基于类型的测试方法，使用快照测试来追踪代码生成的变化，通过Playwright进行端到端应用验证，强调测试的清晰表达而非追求100%覆盖率。

Result: 建立了一套有效的测试体系，能够在保证质量的同时提高开发效率，通过快照测试有效管理代码生成的变化。

Conclusion: 对于全栈Web框架，清晰的测试表达和类型安全比100%测试覆盖率更重要，快照测试是管理代码生成变化的有效工具。

Abstract: How we test a web framework (15 minute read) Wasp's testing strategy for its full-stack web framework prioritizes tests that clearly express input and expected output, favoring courage over 100% coverage. It also relies on types heavily. Wasp uses snapshot testing to track code generation changes and Playwright to validate apps.

</details>


### [22] [Why your boss isn't worried about AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboydkane.com%2Fessays%2Fboss%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/tWghou8WeBrgeKJzVurXxvi2DBMuSCJddpz46ICvFI8=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 公众对软件漏洞的理解适用于常规软件，但在AI系统上具有误导性。AI漏洞源于大型训练数据集，难以精确定位或完全消除，且修复后可能重现。


<details>
  <summary>Details</summary>
Motivation: 澄清AI系统与常规软件在漏洞特性上的根本差异，解释为何AI漏洞更难以管理和修复。

Method: 通过对比分析常规软件漏洞与AI系统漏洞的起源、定位难度和修复特性。

Result: AI漏洞具有系统性、难以追溯和可能重现的特点，与常规软件漏洞的局部性和可完全修复性形成鲜明对比。

Conclusion: 需要重新思考AI系统的漏洞管理方法，不能简单套用传统软件工程的最佳实践。

Abstract: Why your boss isn't worried about AI (11 minute read) The public's understanding of software bugs, while helpful for regular software, is misleading when applied to AI systems. Unlike regular software, where bugs are caused by mistakes in code and can be precisely located and fixed, AI vulnerabilities come from large training datasets, making the origin of errors difficult to pinpoint or eliminate entirely. Once a bug is fixed in regular software, it won't reappear again, but this isn't the c...

</details>


### [23] [Sentry's AI Code Review predicts what's going to break - based on what's already broken](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/GQ-Ztf3RzSPe3RaYN5fsZXpVo8sGURnpSgiCXk4J3FM=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentry的AI代码审查工具通过分析代码变更、依赖关系和历史问题来预测可能导致生产环境故障的问题，提供具体可操作的建议而非通用代码风格检查


<details>
  <summary>Details</summary>
Motivation: 传统代码审查过于关注代码风格细节，而缺乏对可能引发生产环境故障的实际问题的预测能力，需要更智能的审查工具

Method: 结合代码变更上下文和问题历史记录，分析函数调用、类或对象依赖关系、数据库连接等，提供具体可操作的反馈

Result: 开发出能够预测生产环境故障的AI代码审查工具，减少通用linting建议，提供更精准的问题预警

Conclusion: AI驱动的代码审查可以显著提升代码质量，通过历史数据和上下文分析来预测和预防生产环境故障

Abstract: Sentry's AI Code Review predicts what's going to break - based on what's already broken (Sponsor) Code reviews should be less style nits and more "this is going to break prod". Sentry's AI Code Review blends context and issue history with the code you just touched - function calls, class or objects dependencies, database connections - to provide specific and actionable feedback rather than generic linting advice. Read the blog

</details>


### [24] [Generate a Page from a Prompt and Edit Visually](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.experimently.io%2F%3Futm_source=tldrdesign/1/01000199e7d00108-31ae7426-7b3f-4d30-a485-b7d8a37eb849-000000/a8DJ5nieo-oQKZ_p3PSjdO9QVa18ZH5-sU-K8lSc_7Q=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 从提示生成网页，无需代码即可可视化编辑，发布到自定义域名，并通过可配置的加权路由进行A/B测试多页面变体


<details>
  <summary>Details</summary>
Motivation: 降低网页创建和编辑的技术门槛，让非技术人员也能轻松创建和优化网页，同时提供专业的A/B测试功能

Method: 基于提示生成网页内容，提供可视化编辑界面，支持自定义域名发布，实现加权路由的A/B测试机制

Result: 开发了一个无需编码的网页生成和编辑平台，支持可视化操作和专业的A/B测试功能

Conclusion: 该系统成功降低了网页创建的技术门槛，为非技术人员提供了专业的网页优化工具

Abstract: Generate a Page from a Prompt and Edit Visually (Website) Generate a page from a prompt, edit visually (no code needed), publish to your domain, and A/B test multiple page variations with configurable weighted routing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning](https://arxiv.org/abs/2510.12939)
*James Pedley,Benjamin Etheridge,Stephen J. Roberts,Francesco Quinzan*

Main category: cs.LG

TL;DR: 该论文首次为状态对抗马尔可夫决策过程(SA-MDPs)中的剪枝认证鲁棒性建立了理论框架，证明了元素级剪枝只会收紧认证鲁棒性边界，并揭示了性能-鲁棒性权衡边界。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署的强化学习策略需要在对抗扰动下保持可靠性，而现代深度强化学习智能体的过度参数化带来了成本和脆弱性问题。虽然剪枝在监督学习中已被证明能提高鲁棒性，但其在对抗强化学习中的作用尚不清楚。

Method: 为高斯和分类策略开发理论框架，证明元素级剪枝对认证鲁棒性的影响；推导新颖的三项遗憾分解；在连续控制基准上评估幅度剪枝和微剪枝调度。

Result: 剪枝在中等稀疏度水平上始终发现可复现的"最佳点"，在这些点上鲁棒性显著改善，而不会损害甚至有时能提升干净性能。

Conclusion: 剪枝不仅是压缩工具，更是实现鲁棒强化学习的结构性干预手段。

Abstract: Reinforcement learning (RL) policies deployed in real-world environments must
remain reliable under adversarial perturbations. At the same time, modern deep
RL agents are heavily over-parameterized, raising costs and fragility concerns.
While pruning has been shown to improve robustness in supervised learning, its
role in adversarial RL remains poorly understood. We develop the first
theoretical framework for certified robustness under pruning in
state-adversarial Markov decision processes (SA-MDPs). For Gaussian and
categorical policies with Lipschitz networks, we prove that element-wise
pruning can only tighten certified robustness bounds; pruning never makes the
policy less robust. Building on this, we derive a novel three-term regret
decomposition that disentangles clean-task performance, pruning-induced
performance loss, and robustness gains, exposing a fundamental
performance--robustness frontier. Empirically, we evaluate magnitude and
micro-pruning schedules on continuous-control benchmarks with strong
policy-aware adversaries. Across tasks, pruning consistently uncovers
reproducible ``sweet spots'' at moderate sparsity levels, where robustness
improves substantially without harming - and sometimes even enhancing - clean
performance. These results position pruning not merely as a compression tool
but as a structural intervention for robust RL.

</details>


### [26] [Towards Understanding Valuable Preference Data for Large Language Model Alignment](https://arxiv.org/abs/2510.13212)
*Zizhuo Zhang,Qizhou Wang,Shanshan Ye,Jianing Zhu,Jiangchao Yao,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了一种新的截断影响函数(TIF)来评估偏好数据质量，发现数据质量是模型相关的，并提出两种更简单的评分函数来改进偏好数据选择方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常使用外部奖励模型或现成LLM预处理原始训练数据集来识别有价值的偏好对，但很少检查单个数据点是否真正有益。需要改进偏好数据选择方法以适应特定模型。

Method: 提出截断影响函数(TIF)评估数据质量，引入两种计算更简单的评分函数(SF)，并将它们组合以抵消不同的误差源，形成简单有效的数据选择规则。

Result: 在多样化对齐基准和各种LLM家族上的实验表明，使用更少的数据可以实现更好的对齐性能，证明了方法的通用性。

Conclusion: 偏好数据质量是模型相关的，通过模型依赖的数据选择方法可以更精确地选择有价值的偏好数据，提高对齐效率。

Abstract: Large language model (LLM) alignment is typically achieved through learning
from human preference comparisons, making the quality of preference data
critical to its success. Existing studies often pre-process raw training
datasets to identify valuable preference pairs using external reward models or
off-the-shelf LLMs, achieving improved overall performance but rarely examining
whether individual, selected data point is genuinely beneficial. We assess data
quality through individual influence on validation data using our newly
proposed truncated influence function (TIF), which mitigates the over-scoring
present in traditional measures and reveals that preference data quality is
inherently a property of the model. In other words, a data pair that benefits
one model may harm another. This leaves the need to improve the preference data
selection approaches to be adapting to specific models. To this end, we
introduce two candidate scoring functions (SFs) that are computationally
simpler than TIF and positively correlated with it. They are also model
dependent and can serve as potential indicators of individual data quality for
preference data selection. Furthermore, we observe that these SFs inherently
exhibit errors when compared to TIF. To this end, we combine them to offset
their diverse error sources, resulting in a simple yet effective data selection
rule that enables the models to achieve a more precise selection of valuable
preference data. We conduct experiments across diverse alignment benchmarks and
various LLM families, with results demonstrating that better alignment
performance can be achieved using less data, showing the generality of our
findings and new methods.

</details>


### [27] [When In Doubt, Abstain: The Impact of Abstention on Strategic Classification](https://arxiv.org/abs/2510.13327)
*Lina Alkarmi,Ziyuan Huang,Mingyan Liu*

Main category: cs.LG

TL;DR: 本文研究在战略分类中引入弃权机制如何影响战略代理的响应，以及主体应如何最优利用弃权。研究表明最优弃权能确保主体效用不劣于非弃权设置，且弃权可作为操纵威慑，使不合格代理操纵成本更高。


<details>
  <summary>Details</summary>
Motivation: 算法决策日益普及但易受战略操纵，先前研究表明分类器弃权能显著提高准确性。本文旨在探索在战略分类背景下引入弃权机制的影响。

Method: 使用Stackelberg博弈模型，主体作为分类器首先宣布决策策略，战略代理随后操纵特征以获得期望结果。聚焦于二进制分类器，代理操纵可观察特征而非真实特征。

Result: 最优弃权确保主体效用不劣于非弃权设置；弃权不仅能提高准确性，还能作为操纵威慑，在操纵成本足够大时使不合格代理操纵成本更高。

Conclusion: 弃权是减少算法决策系统中战略行为负面影响的宝贵工具。

Abstract: Algorithmic decision making is increasingly prevalent, but often vulnerable
to strategic manipulation by agents seeking a favorable outcome. Prior research
has shown that classifier abstention (allowing a classifier to decline making a
decision due to insufficient confidence) can significantly increase classifier
accuracy. This paper studies abstention within a strategic classification
context, exploring how its introduction impacts strategic agents' responses and
how principals should optimally leverage it. We model this interaction as a
Stackelberg game where a principal, acting as the classifier, first announces
its decision policy, and then strategic agents, acting as followers, manipulate
their features to receive a desired outcome. Here, we focus on binary
classifiers where agents manipulate observable features rather than their true
features, and show that optimal abstention ensures that the principal's utility
(or loss) is no worse than in a non-abstention setting, even in the presence of
strategic agents. We also show that beyond improving accuracy, abstention can
also serve as a deterrent to manipulation, making it costlier for agents,
especially those less qualified, to manipulate to achieve a positive outcome
when manipulation costs are significant enough to affect agent behavior. These
results highlight abstention as a valuable tool for reducing the negative
effects of strategic behavior in algorithmic decision making systems.

</details>


### [28] [A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control](https://arxiv.org/abs/2510.13367)
*Nikita Kachaev,Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovelev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文探讨了在在线无模型强化学习中应用Transformer架构的关键设计问题，包括输入条件、组件共享和序列数据处理，提出了稳定的架构和训练策略。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在离线或基于模型的强化学习中表现出色，但在在线无模型强化学习中仍未充分探索，主要因为其对训练设置和模型设计决策的敏感性。

Method: 研究了Transformer在在线无模型强化学习中的关键设计问题：输入条件设置、actor和critic组件共享策略、序列数据切片训练方法。

Result: 实验表明，提出的稳定架构和训练策略在完全和部分可观测任务中都能实现有竞争力的性能，适用于向量和图像两种设置。

Conclusion: 这些发现为在在线强化学习中应用Transformer提供了实用指导。

Abstract: Despite their effectiveness and popularity in offline or model-based
reinforcement learning (RL), transformers remain underexplored in online
model-free RL due to their sensitivity to training setups and model design
decisions such as how to structure the policy and value networks, share
components, or handle temporal information. In this paper, we show that
transformers can be strong baselines for continuous control in online
model-free RL. We investigate key design questions: how to condition inputs,
share components between actor and critic, and slice sequential data for
training. Our experiments reveal stable architectural and training strategies
enabling competitive performance across fully and partially observable tasks,
and in both vector- and image-based settings. These findings offer practical
guidance for applying transformers in online RL.

</details>


### [29] [Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks](https://arxiv.org/abs/2510.13391)
*Benjamin Kempinski,Tal Kachman*

Main category: cs.LG

TL;DR: 提出一种基于图神经网络的方法来近似计算网络流博弈中的Banzhaf值，相比传统精确计算和蒙特卡洛采样方法，在保持高精度的同时实现了数量级的速度提升，并展示了优秀的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统Banzhaf值计算方法在超过约20个代理时因指数复杂度变得不可行，而蒙特卡洛采样方法样本复杂度高且无法在不同网络配置间迁移知识，难以应用于大规模或动态系统。

Method: 将问题建模为图级预测任务，使用图神经网络直接从网络拓扑和控制结构中学习代理影响力的通用模式，比较了GAT、GINE和EdgeConv三种最先进的GNN架构。

Result: 在包含200,000个图的大规模合成数据集上验证，训练后的GNN模型实现了高保真度的Banzhaf值近似，相比精确和基于采样的方法获得了数量级的速度提升，并展示了强大的零样本泛化能力。

Conclusion: 这项工作确立了GNN作为复杂网络系统可扩展合作博弈论分析的实用工具。

Abstract: Computing the Banzhaf value in network flow games is fundamental for
quantifying agent influence in multi-agent systems, with applications ranging
from cybersecurity to infrastructure planning. However, exact computation is
intractable for systems with more than $\sim20$ agents due to exponential
complexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide
statistical estimates, they suffer from high sample complexity and cannot
transfer knowledge across different network configurations, making them
impractical for large-scale or dynamic systems. We present a novel
learning-based approach using Graph Neural Networks (GNNs) to approximate
Banzhaf values in cardinal network flow games. By framing the problem as a
graph-level prediction task, our method learns generalisable patterns of agent
influence directly from network topology and control structure. We conduct a
comprehensive empirical study comparing three state-of-the-art GNN
architectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with
Edge features (GINE), and EdgeConv-on a large-scale synthetic dataset of
200,000 graphs per configuration, varying in size (20-100 nodes), agent count
(5-20), and edge probability (0.5-1.0). Our results demonstrate that trained
GNN models achieve high-fidelity Banzhaf value approximation with
order-of-magnitude speedups compared to exact and sampling-based methods. Most
significantly, we show strong zero-shot generalisation: models trained on
graphs of a specific size and topology accurately predict Banzhaf values for
entirely new networks with different structural properties, without requiring
retraining. This work establishes GNNs as a practical tool for scalable
cooperative game-theoretic analysis of complex networked systems.

</details>


### [30] [Selective Adversarial Attacks on LLM Benchmarks](https://arxiv.org/abs/2510.13570)
*Ivan Dubrovsky,Anastasia Orlova,Illarion Iov,Nina Gubina,Irena Gureeva,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 该论文研究了选择性对抗攻击对LLM基准测试的影响，发现即使微小的文本扰动也能显著改变模型在MMLU基准上的相对排名，挑战了排行榜评估的公平性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试容易受到语义等效的对抗性扰动影响，但现有研究主要关注对所有模型产生同等影响的攻击，缺乏对选择性攻击（即只针对特定模型性能）的研究。

Method: 使用TextAttack框架中的标准攻击方法，开发了选择性评估协议和自定义约束，提出了基于代理LLM的管道来生成选择性扰动。

Result: 实证发现选择性对抗攻击确实存在，能够显著改变模型在MMLU基准上的相对排名，影响评估的公平性。

Conclusion: 研究结果表明需要采用扰动感知的报告和鲁棒性诊断方法来进行LLM评估，即使是微小的编辑也可能改变比较判断。

Abstract: Benchmarking outcomes increasingly govern trust, selection, and deployment of
LLMs, yet these evaluations remain vulnerable to semantically equivalent
adversarial perturbations. Prior work on adversarial robustness in NLP has
emphasized text attacks that affect many models equally, leaving open the
question of whether it is possible to selectively degrade or enhance
performance while minimally affecting other models. We formalize this problem
and study selective adversarial attacks on MMLU - a widely used benchmark
designed to measure a language model's broad general knowledge and reasoning
ability across different subjects. Using canonical attacks integrated into
TextAttack framework, we introduce a protocol for selectivity assessment,
develop a custom constraint to increase selectivity of attacks and propose a
surrogate-LLM pipeline that generates selective perturbations. Empirically, we
find that selective adversarial attacks exist and can materially alter relative
rankings, challenging the fairness, reproducibility, and transparency of
leaderboard-driven evaluation. Our results motivate perturbation-aware
reporting and robustness diagnostics for LLM evaluation and demonstrate that
even subtle edits can shift comparative judgments.

</details>


### [31] [Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents](https://arxiv.org/abs/2510.13704)
*Johan Obando-Ceron,Walter Mayor,Samuel Lavoie,Scott Fujimoto,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 论文提出使用单纯形嵌入作为轻量级表示层，通过几何归纳偏置产生稀疏离散特征，提升强化学习算法的样本效率和最终性能，且不影响运行速度。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模环境并行化方法虽然能加速训练，但仍需大量环境交互。结构良好的表示可以改善深度强化学习的泛化能力和样本效率。

Method: 提出单纯形嵌入方法，将嵌入约束到单纯形结构中，产生稀疏离散特征，稳定评论家自举并增强策略梯度。应用于FastTD3、FastSAC和PPO算法。

Result: 在多种连续和离散控制环境中，单纯形嵌入一致提升了样本效率和最终性能，且没有损失运行速度。

Conclusion: 单纯形嵌入是一种有效的几何归纳偏置方法，能够显著提升强化学习算法的性能而不影响效率。

Abstract: Recent works have proposed accelerating the wall-clock training time of
actor-critic methods via the use of large-scale environment parallelization;
unfortunately, these can sometimes still require large number of environment
interactions to achieve a desired level of performance. Noting that
well-structured representations can improve the generalization and sample
efficiency of deep reinforcement learning (RL) agents, we propose the use of
simplicial embeddings: lightweight representation layers that constrain
embeddings to simplicial structures. This geometric inductive bias results in
sparse and discrete features that stabilize critic bootstrapping and strengthen
policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial
embeddings consistently improve sample efficiency and final performance across
a variety of continuous- and discrete-control environments, without any loss in
runtime speed.

</details>


### [32] [Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking](https://arxiv.org/abs/2510.13694)
*Yuchun Miao,Liang Ding,Sen Zhang,Rong Bao,Lefei Zhang,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出InfoRM和IBL框架解决RLHF中的奖励过优化问题，通过信息瓶颈原理过滤偏好无关信息，并使用分布级正则化防止策略偏离。


<details>
  <summary>Details</summary>
Motivation: RLHF在语言模型对齐中面临奖励过优化问题，主要障碍包括奖励模型的错误泛化和RL优化中缺乏合适的正则化约束。

Method: 基于信息瓶颈原理的InfoRM奖励建模框架，过滤偏好无关信息；IBL分布级正则化，惩罚与SFT分布的偏离；MOP统计指标量化奖励过优化程度。

Result: 在多种LLM和数据集上的实验验证了方法的有效性，InfoRM和IBL能有效缓解奖励过优化，MOP作为诊断工具可靠。

Conclusion: 提出的框架和方法显著推进了RLHF技术的发展，为解决奖励过优化问题提供了系统性的解决方案。

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
aligning language models with human values, reward hacking-or reward
over-optimization-remains a major challenge. We identify two key obstacles to
its mitigation: (1) reward misgeneralization in reward modeling, where reward
models overfit to spurious, preference-irrelevant features; and (2) the lack of
suitable regularization during RL optimization, as existing token-level
constraints often over-restrict the policy space. To address these issues, we
propose InfoRM, an information-theoretic reward modeling framework based on the
Information Bottleneck (IB) principle, which filters out preference-irrelevant
information to alleviate reward misgeneralization. We further observe that
reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent
space, measured by Mahalanobis distance from the SFT-induced distribution.
Motivated by this, we introduce IBL, a distribution-level regularization that
penalizes such deviations, effectively expanding the optimization landscape
while maintaining alignment. We prove that IBL is theoretically equivalent to
the pessimistic RL objective within the IB latent space. Finally, we present
Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying
reward hacking severity, enabling principled hyperparameter tuning and online
mitigation such as early stopping. Extensive experiments across diverse LLMs
and datasets confirm the generality of our findings, the effectiveness of
InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively
advancing the state of RLHF.

</details>


### [33] [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/abs/2510.13786)
*Devvrit Khatri,Lovish Madaan,Rishabh Tiwari,Rachit Bansal,Sai Surya Duvvuri,Manzil Zaheer,Inderjit S. Dhillon,David Brandfonbrener,Rishabh Agarwal*

Main category: cs.LG

TL;DR: 该论文提出了首个大规模系统性研究，建立了强化学习在大型语言模型中的可预测缩放框架，通过超过40万GPU小时的实验定义了RL缩放原则，并提出了最佳实践配方ScaleRL。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习已成为训练大型语言模型的核心技术，但该领域缺乏与预训练相当的预测性缩放方法。随着计算预算的快速增长，如何评估RL计算缩放的算法改进缺乏原则性理解。

Method: 进行了大规模系统性研究（超过40万GPU小时），拟合RL训练的S型计算-性能曲线，消融分析各种常见设计选择对渐进性能和计算效率的影响。

Result: 发现：(1) 不同配方产生不同的渐进性能；(2) 损失聚合、归一化、课程学习和离策略算法等细节主要调节计算效率而不显著改变渐进性能；(3) 稳定可扩展的配方遵循可预测的缩放轨迹。提出的ScaleRL配方在10万GPU小时的单一RL运行中成功缩放并预测验证性能。

Conclusion: 该研究为分析RL缩放提供了科学框架和实用配方，使RL训练更接近预训练长期实现的可预测性。

Abstract: Reinforcement learning (RL) has become central to training large language
models (LLMs), yet the field lacks predictive scaling methodologies comparable
to those established for pre-training. Despite rapidly rising compute budgets,
there is no principled understanding of how to evaluate algorithmic
improvements for scaling RL compute. We present the first large-scale
systematic study, amounting to more than 400,000 GPU-hours, that defines a
principled framework for analyzing and predicting RL scaling in LLMs. We fit
sigmoidal compute-performance curves for RL training and ablate a wide range of
common design choices to analyze their effects on asymptotic performance and
compute efficiency. We observe: (1) Not all recipes yield similar asymptotic
performance, (2) Details such as loss aggregation, normalization, curriculum,
and off-policy algorithm primarily modulate compute efficiency without
materially shifting the asymptote, and (3) Stable, scalable recipes follow
predictable scaling trajectories, enabling extrapolation from smaller-scale
runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and
demonstrate its effectiveness by successfully scaling and predicting validation
performance on a single RL run scaled up to 100,000 GPU-hours. Our work
provides both a scientific framework for analyzing scaling in RL and a
practical recipe that brings RL training closer to the predictability long
achieved in pre-training.

</details>


### [34] [Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach](https://arxiv.org/abs/2510.13792)
*Ziqing Lu,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 提出一种基于信息论的"不可战胜"对抗攻击方法，通过率失真理论随机改变智能体对转移核的观测，使其在训练中获得零或有限的地面真实信息。


<details>
  <summary>Details</summary>
Motivation: 提高RL系统对抗攻击的鲁棒性需要研究各种对抗攻击策略。以往确定性攻击可被受害者智能体逆转，因此需要开发更强大的攻击方法。

Method: 采用率失真信息论方法，随机改变智能体对转移核或其他属性的观测，限制智能体获取地面真实信息。推导了受害者智能体奖励遗憾的信息论下界。

Result: 证明了这种攻击对最先进的基于模型和无模型算法的影响，并将该方法扩展到状态观测攻击等其他类型的对抗攻击。

Conclusion: 提出的信息论方法能够实现"不可战胜"的对抗攻击，为RL系统的安全防御提供了新的挑战和视角。

Abstract: Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged
in many security-related applications, such as autonomous driving, financial
decisions, and drone/robot algorithms. In order to improve the
robustness/defense of RL systems against adversaries, studying various
adversarial attacks on RL systems is very important. Most previous work
considered deterministic adversarial attack strategies in MDP, which the
recipient (victim) agent can defeat by reversing the deterministic attacks. In
this paper, we propose a provably ``invincible'' or ``uncounterable'' type of
adversarial attack on RL. The attackers apply a rate-distortion
information-theoretic approach to randomly change agents' observations of the
transition kernel (or other properties) so that the agent gains zero or very
limited information about the ground-truth kernel (or other properties) during
the training. We derive an information-theoretic lower bound on the recipient
agent's reward regret and show the impact of rate-distortion attacks on
state-of-the-art model-based and model-free algorithms. We also extend this
notion of an information-theoretic approach to other types of adversarial
attack, such as state observation attacks.

</details>


### [35] [Asymptotically optimal reinforcement learning in Block Markov Decision Processes](https://arxiv.org/abs/2510.13748)
*Thomas van Vuren,Fiona Sloothaak,Maarten G. Wolf,Jaron Sanders*

Main category: cs.LG

TL;DR: 该论文提出了一种针对块马尔可夫决策过程的两阶段强化学习算法，通过聚类学习潜在状态结构，实现了比现有方法更优的遗憾界O(√T+n)，并在该类问题上达到渐近最优。


<details>
  <summary>Details</summary>
Motivation: 高维状态和动作空间使得强化学习在许多实际应用中不可行，但许多环境存在可利用的结构。块马尔可夫决策过程能够建模具有大观测空间但由潜在状态决定动态的问题，需要开发能够利用这种结构的有效算法。

Method: 采用两阶段算法：第一阶段通过随机探索学习潜在状态结构，第二阶段切换到适应已发现结构的乐观引导策略。

Result: 算法在可聚类的BMDP类上实现了O(√T+n)的遗憾界，优于之前最好的O(√T+n²)结果，特别是当观测空间基数n很大时。

Conclusion: 该算法在可聚类的BMDP类上达到渐近最优，证明了准确估计潜在状态确实能有效加速学习。

Abstract: The curse of dimensionality renders Reinforcement Learning (RL) impractical
in many real-world settings with exponentially large state and action spaces.
Yet, many environments exhibit exploitable structure that can accelerate
learning. To formalize this idea, we study RL in Block Markov Decision
Processes (BMDPs). BMDPs model problems with large observation spaces, but
where transition dynamics are fully determined by latent states. Recent
advances in clustering methods have enabled the efficient recovery of this
latent structure. However, a regret analysis that exploits these techniques to
determine their impact on learning performance remained open. We are now
addressing this gap by providing a regret analysis that explicitly leverages
clustering, demonstrating that accurate latent state estimation can indeed
effectively speed up learning.
  Concretely, this paper analyzes a two-phase RL algorithm for BMDPs that first
learns the latent structure through random exploration and then switches to an
optimism-guided strategy adapted to the uncovered structure. This algorithm
achieves a regret that is $O(\sqrt{T}+n)$ on a large class of BMDPs susceptible
to clustering. Here, $T$ denotes the number of time steps, $n$ is the
cardinality of the observation space, and the Landau notation $O(\cdot)$ holds
up to constants and polylogarithmic factors. This improves the best prior
bound, $O(\sqrt{T}+n^2)$, especially when $n$ is large. Moreover, we prove that
no algorithm can achieve lower regret uniformly on this same class of BMDPs.
This establishes that, on this class, the algorithm achieves asymptotic
optimality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [36] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: 本文证明在大型代码库中使用关键词搜索足以检索相关代码上下文，无需GPU资源，在代码上下文竞赛中取得良好表现


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成框架使用语义搜索需要大量计算资源，难以集成到轻量级应用中如IDE代码补全

Method: 使用关键词搜索替代语义搜索来检索相关代码上下文

Result: 在代码上下文竞赛的Kotlin和Python赛道上分别达到0.748和0.725的chRF分数

Conclusion: 关键词搜索是检索代码上下文的有效替代方案，无需大量计算资源

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [37] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: 使用大型语言模型自动修复工业嵌入式系统中的编译错误，在CI系统中能解决63%的编译错误，83%的修复被认为是合理的，并将调试时间从几小时缩短到8分钟内。


<details>
  <summary>Details</summary>
Motivation: 工业嵌入式系统中硬件和软件共同开发经常导致持续集成过程中的编译错误，现有修复技术依赖测试用例，但不可编译代码没有测试用例可用。

Method: 收集超过40000个产品源代码提交，评估四个最先进LLM增强的工业CI系统性能，与人工修正进行比较。

Result: LLM增强的CI系统能解决基线数据集中63%的编译错误，其中83%的成功修复被认为是合理的，调试时间大幅减少到8分钟内。

Conclusion: LLM在自动修复编译错误方面表现出色，能有效解决工业嵌入式系统中的编译问题并显著提高调试效率。

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [38] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: 本文研究了不同代码仓库处理策略对OpenCoder模型（1.5B参数）上下文学习的影响，通过扩展上下文窗口至16,384 token并在1B token的仓库级数据上训练，在Long Code Arena基准测试中取得了与更大数据集模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 探索代码仓库级预训练中不同处理策略对大型语言模型代码生成能力的影响，特别是如何在小数据集条件下实现与大规模数据集模型相媲美的性能。

Method: 扩展OpenCoder模型的上下文窗口从4,096到16,384 token，在1B token的精选仓库级数据上进行训练，比较不同仓库处理技术，并测试更简单的文件级训练方法。

Result: 尽管使用比竞争模型小得多的数据集，但在Long Code Arena基准测试中取得了可比性能；不同仓库处理技术产生相似强结果，主要增益来自适应新的RoPE缩放参数；更简单的文件级训练方法在原始序列长度下仍然非常有效。

Conclusion: 仓库级代码补全研究可以在数据和计算资源受限的环境中进行，更简单的文件级训练方法仍然有效，为资源受限场景提供了可行方案。

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: 提出了RID框架，一种零样本元提示技术，通过结构化认知模式让LLM能够进行人类对齐的异常处理，显著提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 解决LLM作为智能体推理引擎时存在的规则刚性缺陷，即过度遵循显式规则而忽视人类常识和意图的问题。

Method: RID框架：结构化元提示技术，引导模型解构任务、分类规则、权衡冲突结果并证明最终决策。

Result: 在20个需要细微判断的场景基准测试中，RID框架达到95%的人类对齐分数，显著优于基线(80%)和CoT(75%)。

Conclusion: RID框架为从字面指令遵循转向目标导向推理提供了实用有效的方法，有助于构建更可靠的AI智能体。

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [40] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanner是一个端到端的强化学习框架，通过基于熵的令牌级优势塑造和选择性加权样本级优势，有效增强深度研究代理的规划能力，在七个深度研究基准上实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖推理阶段的隐式规划，要么引入显式规划器但没有系统性地解决如何优化规划阶段的问题。研究发现，在标准强化学习下，规划令牌的熵显著高于其他动作令牌，揭示了未优化的不确定决策点。

Method: 提出DeepPlanner框架，通过基于熵的令牌级优势塑造为高熵令牌分配更大的更新，并选择性加权规划密集型rollouts的样本级优势。

Result: 在七个深度研究基准上的广泛实验表明，DeepPlanner提高了规划质量，并在显著降低的训练预算下实现了最先进的结果。

Conclusion: DeepPlanner通过系统性地优化规划阶段，有效增强了深度研究代理的规划能力，为复杂任务的长时程规划提供了更好的解决方案。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [41] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: Sentinel是首个基于形式化方法评估LLM具身智能体物理安全性的框架，通过语义、规划和轨迹三个层面对安全性进行形式化验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式规则或主观的LLM判断，缺乏对物理安全性的形式化验证，无法精确指定状态不变量、时间依赖性和时序约束。

Method: 使用时序逻辑(TL)语义形式化安全需求，构建多级验证管道：语义级验证LLM对安全需求的理解，规划级验证高层行动计划，轨迹级验证执行轨迹。

Result: 在VirtualHome和ALFRED环境中评估多个LLM具身智能体，发现Sentinel能够暴露先前方法忽略的安全违规，并揭示其失败模式。

Conclusion: 通过将物理安全性与时序逻辑相结合，Sentinel为系统评估LLM具身智能体在物理环境中的安全性提供了严谨基础。

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [42] [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)
*Stephane Hatgis-Kessell,Logan Mondal Bhamidipaty,Emma Brunskill*

Main category: cs.AI

TL;DR: 提出Preference-Based Reward Repair (PBRR)框架，通过从人类偏好中学习加性修正项来自动修复人工设计的代理奖励函数，解决奖励函数错配问题。


<details>
  <summary>Details</summary>
Motivation: 人工设计的奖励函数经常与真实目标错配，导致奖励黑客问题；而从头学习奖励函数成本高昂。需要一种结合两者优势的方法。

Method: PBRR框架：通过目标探索策略和新的偏好学习目标，学习一个与状态转移相关的加性修正项来修复代理奖励函数。

Result: 在表格域中证明PBRR的累积遗憾与现有偏好强化学习方法相当；在奖励黑客基准测试中，PBRR始终优于基线方法，需要更少的偏好来学习高性能策略。

Conclusion: PBRR能够有效修复错配的代理奖励函数，比从头学习奖励函数或其他修正方法更高效，显著减少所需的人类偏好数量。

Abstract: Human-designed reward functions for reinforcement learning (RL) agents are
frequently misaligned with the humans' true, unobservable objectives, and thus
act only as proxies. Optimizing for a misspecified proxy reward function often
induces reward hacking, resulting in a policy misaligned with the human's true
objectives. An alternative is to perform RL from human feedback, which involves
learning a reward function from scratch by collecting human preferences over
pairs of trajectories. However, building such datasets is costly. To address
the limitations of both approaches, we propose Preference-Based Reward Repair
(PBRR): an automated iterative framework that repairs a human-specified proxy
reward function by learning an additive, transition-dependent correction term
from preferences. A manually specified reward function can yield policies that
are highly suboptimal under the ground-truth objective, yet corrections on only
a few transitions may suffice to recover optimal performance. To identify and
correct for those transitions, PBRR uses a targeted exploration strategy and a
new preference-learning objective. We prove in tabular domains PBRR has a
cumulative regret that matches, up to constants, that of prior preference-based
RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR
consistently outperforms baselines that learn a reward function from scratch
from preferences or modify the proxy reward function using other approaches,
requiring substantially fewer preferences to learn high performing policies.

</details>


### [43] [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)
*Zehui Ling,Deshu Chen,Yichi Zhang,Yuchen Liu,Xigui Li,Xin Guo,Yuan Cheng*

Main category: cs.AI

TL;DR: 提出一种结合小型和大型LLM的互补代理系统，小型LLM先生成初始答案，大型LLM进行验证，仅在必要时进行深度推理，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度推理和多代理系统虽然能提高复杂任务性能，但计算成本高昂，需要一种更高效的方法来平衡性能和成本。

Method: 采用小型LLM生成初始答案，大型LLM进行验证，仅在答案错误时进行深度推理，形成互补代理系统。

Result: 在简单问题上，大型LLM的计算成本降低超过50%，准确率损失可忽略，复杂任务上保持稳健性能。

Conclusion: 该互补代理系统能有效降低计算成本，同时保持任务性能，为LLM应用提供了更高效的解决方案。

Abstract: Recent advances in Large Language Models (LLMs) demonstrate that
chain-of-thought prompting and deep reasoning substantially enhance performance
on complex tasks, and multi-agent systems can further improve accuracy by
enabling model debates. However, applying deep reasoning to all problems is
computationally expensive. To mitigate these costs, we propose a complementary
agent system integrating small and large LLMs. The small LLM first generates an
initial answer, which is then verified by the large LLM. If correct, the answer
is adopted directly; otherwise, the large LLM performs in-depth reasoning.
Experimental results show that, for simple problems, our approach reduces the
computational cost of the large LLM by more than 50% with negligible accuracy
loss, while consistently maintaining robust performance on complex tasks.

</details>


### [44] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: 提出了J-TTL基准测试来衡量AI代理在测试时的学习能力，并开发了EvoTest进化框架，通过演化和重配置代理系统来提升性能，无需微调或梯度计算。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理无法在测试时动态学习复杂技能，在陌生环境中表现不佳，这限制了其实际应用价值。需要系统性地衡量和推动这一挑战的进展。

Method: 引入J-TTL基准测试，要求代理在连续多轮游戏中提升表现。提出EvoTest进化测试时学习框架，包含执行游戏的Actor代理和分析游戏记录以提出新配置的Evolver代理，通过重写提示、更新记忆、调整超参数和学习工具使用例程来改进系统。

Result: EvoTest在J-TTL基准上持续提升性能，优于反射、记忆和在线微调等方法。是唯一能够赢得两个游戏（Detective和Library）的方法，而所有基线方法都未能赢得任何游戏。

Conclusion: EvoTest框架通过进化方法有效解决了测试时学习挑战，展示了在不进行微调的情况下提升代理性能的可行性。

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [45] [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)
*Robert West,Ashton Anderson,Ece Kamar,Eric Horvitz*

Main category: cs.AI

TL;DR: 提出了一种名为tandem training的强化学习方法，通过随机将控制权交给较弱模型来训练强模型产生可理解且可被较弱模型继续执行的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能力快速提升，其推理过程可能变得难以被较弱代理或人类理解，这会影响可解释性和监督。需要确保强模型产生的解决方案对较弱合作者保持可理解性。

Method: 引入handoff robustness作为可理解性标准，提出tandem training强化学习范式：在训练过程中随机从冻结的弱模型采样token，只有当强模型的动作和推理过程能被弱模型继续时，训练才会成功。

Result: 在GSM8K数学推理任务中，tandem training能有效教会模型放弃专业术语，适应较弱合作伙伴的语言风格，同时保持高任务准确率。

Conclusion: 该方法为构建可由较弱代理审计的AI系统提供了有前景的途径，对人与AI协作和多智能体通信具有重要意义。

Abstract: As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model's solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model's actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.

</details>


### [46] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出了一种基于最大化人类赋能的辅助语言模型调优方法Empower，仅需离线文本数据即可训练更有效的AI助手，无需额外人工反馈。


<details>
  <summary>Details</summary>
Motivation: 现有辅助代理方法往往鼓励AI独立完成任务而非真正协助人类，且需要昂贵的人工反馈。需要一种能真正帮助人类实现目标、仅需离线数据的自监督方法。

Method: 提出Empower方法，通过最大化人类在环境中的赋能（影响期望变化的能力）来调优语言模型，仅使用离线文本数据，无需额外人工反馈。

Result: 用户研究中78%的参与者偏好Empower助手（p=0.015），接受率提高31%，建议减少38%。在代码协助环境中，Empower将模拟程序员在挑战性编程问题上的成功率平均提高192%。

Conclusion: Empower提供了一个仅使用离线数据、无需额外人工反馈或可验证奖励的大规模有用对齐AI代理框架。

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [47] [From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails](https://arxiv.org/abs/2510.13727)
*Ravi Pandya,Madison Bland,Duy P. Nguyen,Changliu Liu,Jaime Fernández Fisac,Andrea Bajcsy*

Main category: cs.AI

TL;DR: 提出了一种基于控制理论的安全护栏方法，通过实时监控和主动修正AI系统的输出，预防下游危害，而不是简单地阻止危险行为。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全护栏主要依赖输出分类和人为标准，无法应对新危险情况，且检测到危险后只能拒绝行动，这并不总是安全的选择。

Method: 将AI安全视为序列决策问题，在AI模型的潜在表示空间中应用安全关键控制理论，构建预测性护栏来实时监控和主动修正风险输出。

Result: 在模拟驾驶和电子商务场景中的实验表明，控制理论护栏能可靠地避免灾难性后果（如碰撞和破产），同时保持任务性能。

Conclusion: 控制理论护栏为当前标记-阻止式护栏提供了原则性的动态替代方案，能够更有效地确保AI系统的安全性。

Abstract: Generative AI systems are increasingly assisting and acting on behalf of end
users in practical settings, from digital shopping assistants to
next-generation autonomous cars. In this context, safety is no longer about
blocking harmful content, but about preempting downstream hazards like
financial or physical harm. Yet, most AI guardrails continue to rely on output
classification based on labeled datasets and human-specified criteria,making
them brittle to new hazardous situations. Even when unsafe conditions are
flagged, this detection offers no path to recovery: typically, the AI system
simply refuses to act--which is not always a safe choice. In this work, we
argue that agentic AI safety is fundamentally a sequential decision problem:
harmful outcomes arise from the AI system's continually evolving interactions
and their downstream consequences on the world. We formalize this through the
lens of safety-critical control theory, but within the AI model's latent
representation of the world. This enables us to build predictive guardrails
that (i) monitor an AI system's outputs (actions) in real time and (ii)
proactively correct risky outputs to safe ones, all in a model-agnostic manner
so the same guardrail can be wrapped around any AI model. We also offer a
practical training recipe for computing such guardrails at scale via
safety-critical reinforcement learning. Our experiments in simulated driving
and e-commerce settings demonstrate that control-theoretic guardrails can
reliably steer LLM agents clear of catastrophic outcomes (from collisions to
bankruptcy) while preserving task performance, offering a principled dynamic
alternative to today's flag-and-block guardrails.

</details>


### [48] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: Hard2Verify是一个人工标注的数学证明步骤级验证基准，用于评估前沿LLM验证器在挑战性数学问题中的表现。


<details>
  <summary>Details</summary>
Motivation: 在IMO 2025等数学竞赛中，LLM推理系统需要能够验证每个证明步骤的正确性和充分性，因此需要强大的步骤级验证器。

Method: 构建了包含500+小时人工标注的Hard2Verify基准，评估了29个生成式评论器和过程奖励模型在识别前沿LLM生成的数学证明中第一个错误的能力。

Result: 除了少数表现优异者外，开源验证器落后于闭源模型，分析了性能不佳的原因、计算规模化的影响以及自验证和验证-生成动态等基本问题。

Conclusion: Hard2Verify基准揭示了当前步骤级验证器的局限性，为改进LLM推理系统的验证能力提供了重要见解。

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>
