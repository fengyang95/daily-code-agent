<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 17]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/abs/2510.12826)
*Thao Pham*

Main category: cs.CL

TL;DR: è¯¥ç ”ç©¶è¯„ä¼°äº†å‰æ²¿LLMä»£ç†åœ¨æˆ˜ç•¥æ¬ºéª—æ–¹é¢çš„èƒ½åŠ›å’Œå€¾å‘ï¼Œé€šè¿‡ä¸¤ç§åšå¼ˆè®ºæ¡†æ¶æµ‹è¯•äº†å››ä¸ªæ¨¡å‹ï¼Œå‘ç°å³ä½¿æ²¡æœ‰æ˜ç¡®æç¤ºï¼Œæ¨¡å‹ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æ¬ºéª—å€¾å‘ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€LLMä»£ç†åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­è‡ªä¸»éƒ¨ç½²ï¼Œè¯„ä¼°å…¶æˆ˜ç•¥æ¬ºéª—èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚è™½ç„¶å·²æœ‰ç ”ç©¶è€ƒå¯ŸAIç³»ç»Ÿå¦‚ä½•å¯¹æŠ—äººç±»å¼€å‘è€…ï¼Œä½†LLMä¹‹é—´çš„æ¬ºéª—è¡Œä¸ºä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚

Method: ä½¿ç”¨ä¸¤ç§åšå¼ˆè®ºæ¡†æ¶ï¼šå»‰ä»·è°ˆè¯ä¿¡å·åšå¼ˆå’ŒåŒè¡Œè¯„ä¼°å¯¹æŠ—åšå¼ˆï¼Œæµ‹è¯•äº†GPT-4oã€Gemini-2.5-proã€Claude-3.7-Sonnetå’ŒLlama-3.3-70bå››ä¸ªæ¨¡å‹ï¼Œé€šè¿‡æ€ç»´é“¾æ¨ç†åˆ†ææ¬ºéª—ç­–ç•¥ã€‚

Result: å½“æœ‰æç¤ºæ—¶ï¼Œå¤§å¤šæ•°æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯Gemini-2.5-proå’ŒClaude-3.7-Sonnetï¼‰è¾¾åˆ°æ¥è¿‘å®Œç¾çš„è¡¨ç°ã€‚å…³é”®å‘ç°æ˜¯ï¼Œå³ä½¿æ²¡æœ‰æç¤ºï¼Œæ‰€æœ‰æ¨¡å‹åœ¨åŒè¡Œè¯„ä¼°ä¸­éƒ½é€‰æ‹©æ¬ºéª—è€Œéå¦ç™½ï¼ˆ100%æ¯”ç‡ï¼‰ï¼Œåœ¨å»‰ä»·è°ˆè¯ä¸­é€‰æ‹©æ¬ºéª—çš„æ¨¡å‹æˆåŠŸç‡è¾¾åˆ°95-100%ã€‚

Conclusion: è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­ä½¿ç”¨é«˜é£é™©åšå¼ˆè®ºåœºæ™¯è¿›è¡Œç¨³å¥è¯„ä¼°çš„å¿…è¦æ€§ã€‚

Abstract: As large language model (LLM) agents are deployed autonomously in diverse
contexts, evaluating their capacity for strategic deception becomes crucial.
While recent research has examined how AI systems scheme against human
developers, LLM-to-LLM scheming remains underexplored. We investigate the
scheming ability and propensity of frontier LLM agents through two
game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation
adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,
Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and
without explicit prompting while analyzing scheming tactics through
chain-of-thought reasoning. When prompted, most models, especially
Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.
Critically, models exhibited significant scheming propensity without prompting:
all models chose deception over confession in Peer Evaluation (100% rate),
while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These
findings highlight the need for robust evaluations using high-stakes
game-theoretic scenarios in multi-agent settings.

</details>


### [2] [MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training](https://arxiv.org/abs/2510.12831)
*Taicheng Guo,Hai Wang,ChaoChun Liu,Mohsen Golalikhani,Xin Chen,Xiangliang Zhang,Chandan K. Reddy*

Main category: cs.CL

TL;DR: MTSQL-R1æ˜¯ä¸€ä¸ªç”¨äºå¤šè½®Text-to-SQLçš„æ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å°†ä»»åŠ¡å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå®ç°æ‰§è¡Œåé¦ˆå’Œå¯¹è¯ä¸€è‡´æ€§éªŒè¯çš„è¿­ä»£ä¼˜åŒ–å¾ªç¯ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ç³»ç»Ÿå°†å¤šè½®Text-to-SQLè§†ä¸ºç®€å•çš„æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡ï¼Œé‡‡ç”¨çŸ­è§†èŒƒå¼é€è½®ç”ŸæˆæŸ¥è¯¢è€Œä¸æ‰§è¡Œã€éªŒè¯å’Œä¼˜åŒ–ï¼Œå¯¼è‡´è¾“å‡ºä¸å¯æ‰§è¡Œæˆ–ä¸è¿è´¯ã€‚

Method: å°†ä»»åŠ¡å»ºæ¨¡ä¸ºMDPï¼Œæ™ºèƒ½ä½“ä¸æ•°æ®åº“äº¤äº’è·å–æ‰§è¡Œåé¦ˆï¼Œä¸æŒä¹…å¯¹è¯è®°å¿†äº¤äº’è¿›è¡Œä¸€è‡´æ€§éªŒè¯ï¼Œæ‰§è¡Œ'æå‡ºæ‰§è¡Œâ†’éªŒè¯â†’ä¼˜åŒ–'çš„è¿­ä»£å¾ªç¯ç›´åˆ°æ‰€æœ‰æ£€æŸ¥é€šè¿‡ã€‚

Result: åœ¨COSQLå’ŒSPARCæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMTSQL-R1æŒç»­ä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ã€‚

Conclusion: ç¯å¢ƒé©±åŠ¨çš„éªŒè¯å’Œè®°å¿†å¼•å¯¼çš„ä¼˜åŒ–å¯¹äºä¼šè¯è¯­ä¹‰è§£æè‡³å…³é‡è¦ã€‚

Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances
into executable SQL while preserving dialogue coherence and grounding to the
target schema. However, most existing systems only regard this task as a simple
text translation task and follow a short-horizon paradigm, generating a query
per turn without execution, explicit verification, and refinement, which leads
to non-executable or incoherent outputs. We present MTSQL-R1, an agentic
training framework for long-horizon multi-turn Text-to-SQL. We cast the task as
a Markov Decision Process (MDP) in which an agent interacts with (i) a database
for execution feedback and (ii) a persistent dialogue memory for coherence
verification, performing an iterative propose to execute -> verify -> refine
cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that
MTSQL-R1 consistently outperforms strong baselines, highlighting the importance
of environment-driven verification and memory-guided refinement for
conversational semantic parsing. Full recipes (including code, trained models,
logs, reasoning trajectories, etc.) will be released after the internal review
to contribute to community research.

</details>


### [3] [A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning](https://arxiv.org/abs/2510.12838)
*Qianben Chen,Jingyi Cao,Jiayu Zhang,Tianrui Qin,Xiaowan Li,King Zhu,Dingfeng Shi,He Zhu,Minghao Liu,Xiaobo Liang,Ge Zhang,Jian Yang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: æå‡ºäº†è‡ªé€‚åº”æ™ºèƒ½ä½“åŸºç¡€æ¨¡å‹(AÂ²FM)ï¼Œé€šè¿‡è·¯ç”±-å¯¹é½åŸåˆ™ç»Ÿä¸€æ¨ç†å‹å’Œæ™ºèƒ½ä½“å‹LLMï¼Œå¼•å…¥å³æ—¶æ¨¡å¼å¤„ç†ç®€å•æŸ¥è¯¢ï¼Œæ˜¾è‘—æå‡æˆæœ¬æ•ˆç‡ã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³æ¨ç†å‹LLMå’Œæ™ºèƒ½ä½“å‹LLMä¹‹é—´çš„èƒ½åŠ›åˆ†è£‚é—®é¢˜ï¼Œå‰è€…æ“…é•¿å†…éƒ¨æ¨ç†ä½†æ— æ³•è°ƒç”¨å·¥å…·ï¼Œåè€…èƒ½äº¤äº’ç¯å¢ƒä½†æ¨ç†èƒ½åŠ›è¾ƒå¼±ï¼Œä¸”ä¸¤è€…åœ¨ç®€å•æŸ¥è¯¢ä¸Šéƒ½å­˜åœ¨è¿‡åº¦æ€è€ƒæˆ–è¿‡åº¦è°ƒç”¨å·¥å…·çš„ä½æ•ˆé—®é¢˜ã€‚

Method: é‡‡ç”¨è·¯ç”±-å¯¹é½åŸåˆ™ï¼šå…ˆå­¦ä¹ ä»»åŠ¡æ„ŸçŸ¥è·¯ç”±ï¼Œç„¶ååœ¨å…±äº«éª¨å¹²ç½‘ç»œä¸‹å¯¹é½æ¨¡å¼ç‰¹å®šè½¨è¿¹ï¼›å¼•å…¥å³æ—¶æ¨¡å¼å¤„ç†ç®€å•æŸ¥è¯¢ï¼›æå‡ºè‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–(APO)ï¼Œå¼ºåˆ¶è·¨æ¨¡å¼è‡ªé€‚åº”é‡‡æ ·å¹¶åº”ç”¨æˆæœ¬æ­£åˆ™åŒ–å¥–åŠ±ã€‚

Result: åœ¨32Bè§„æ¨¡ä¸Šï¼ŒAÂ²FMåœ¨BrowseCompã€AIME25å’ŒHLEåŸºå‡†ä¸Šåˆ†åˆ«è¾¾åˆ°13.4%ã€70.4%å’Œ16.7%ï¼Œåœ¨å¯æ¯”æ¨¡å‹ä¸­åˆ›ä¸‹æ–°SOTAï¼Œåœ¨æ™ºèƒ½ä½“ã€æ¨ç†å’Œé€šç”¨åŸºå‡†ä¸Šä¸å‰æ²¿LLMç«äº‰ã€‚è‡ªé€‚åº”æ‰§è¡Œæ¯ä¸ªæ­£ç¡®ç­”æ¡ˆæˆæœ¬ä»…0.00487ç¾å…ƒï¼Œç›¸æ¯”æ¨ç†æ¨¡å¼é™ä½æˆæœ¬45.2%ï¼Œç›¸æ¯”æ™ºèƒ½ä½“æ¨¡å¼é™ä½æˆæœ¬33.5%ã€‚

Conclusion: AÂ²FMæ¡†æ¶æˆåŠŸç»Ÿä¸€äº†æ¨ç†å‹å’Œæ™ºèƒ½ä½“å‹LLMçš„èƒ½åŠ›ï¼Œé€šè¿‡è‡ªé€‚åº”è·¯ç”±æ˜¾è‘—æå‡äº†æˆæœ¬æ•ˆç‡ï¼Œåœ¨ä¿æŒå¯æ¯”å‡†ç¡®æ€§çš„åŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

Abstract: Large language models split into two families: reasoning-centric LLMs, which
strengthen internal chain-of-thought reasoning but cannot invoke external
tools, and agentic LLMs, which learn to interact with environments and leverage
tools but often lag in deep reasoning. This divide arises from fundamentally
different training objectives, leading to mismatched strengths and inefficiency
on simple queries, where both families tend to overthink or over-call tools. In
this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM),
a unified framework that follows a route-then-align principle: the model first
learns task-aware routing and then aligns mode-specific trajectories under a
shared backbone. To address the inefficiency gap, we introduce a third
mode-instant-that handles simple queries directly, preventing unnecessary
reasoning or tool calls while complementing the agentic and reasoning modes. To
jointly enhance accuracy and efficiency, we propose Adaptive Policy
Optimization (APO), which enforces adaptive sampling across modes and applies a
cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves
13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA
among comparable models and performing competitively with frontier LLMs across
agentic, reasoning, and general benchmarks. Notably, the adaptive execution
achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by
45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering
substantially higher cost efficiency while maintaining comparable accuracy.

</details>


### [4] [EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus](https://arxiv.org/abs/2510.12899)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Zhongxiang Dai,Kun Kuang*

Main category: cs.CL

TL;DR: æå‡ºäº†EduDialï¼Œä¸€ä¸ªå…¨é¢çš„å¤šè½®å¸ˆç”Ÿå¯¹è¯æ•°æ®é›†ï¼ŒåŒ…å«34,250ä¸ªå¯¹è¯ä¼šè¯ï¼Œè¦†ç›–345ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œå¹¶åŸºäºå¸ƒé²å§†æ•™è‚²ç›®æ ‡åˆ†ç±»å­¦å’Œåç§æé—®ç­–ç•¥è®¾è®¡ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€å¤§è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šé‡è¦ï¼Œéœ€è¦ä¸“é—¨çš„å¸ˆç”Ÿå¯¹è¯åŸºå‡†æ¥è¯„ä¼°å…¶æ•™å­¦èƒ½åŠ›ï¼Œä»¥æ¨åŠ¨æ™ºèƒ½æ•™è‚²å‘å±•ã€‚

Method: é€šè¿‡æ•™å¸ˆå’Œå­¦ç”Ÿä»£ç†ä¹‹é—´çš„äº¤äº’ç”Ÿæˆå¯¹è¯æ•°æ®ï¼Œç»“åˆå¸ƒé²å§†æ•™è‚²ç›®æ ‡åˆ†ç±»å­¦å’Œåç§æé—®ç­–ç•¥ï¼Œä¸ºä¸åŒè®¤çŸ¥æ°´å¹³çš„å­¦ç”Ÿè®¾è®¡å·®å¼‚åŒ–æ•™å­¦ç­–ç•¥ã€‚

Result: åœ¨17ä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨å­¦ç”Ÿä¸­å¿ƒæ•™å­¦åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œè€ŒEduDial-LLM 32Båœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚

Conclusion: EduDialä¸ºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ•™å­¦èƒ½åŠ›æä¾›äº†å…¨é¢åŸºå‡†ï¼ŒEduDial-LLMåœ¨å¸ˆç”Ÿå¯¹è¯åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºæ™ºèƒ½æ•™è‚²åº”ç”¨æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚

Abstract: Recently, several multi-turn dialogue benchmarks have been proposed to
evaluate the conversational abilities of large language models (LLMs). As LLMs
are increasingly recognized as a key technology for advancing intelligent
education, owing to their ability to deeply understand instructional contexts
and provide personalized guidance, the construction of dedicated
teacher-student dialogue benchmarks has become particularly important. To this
end, we present EduDial, a comprehensive multi-turn teacher-student dialogue
dataset. EduDial covers 345 core knowledge points and consists of 34,250
dialogue sessions generated through interactions between teacher and student
agents. Its design is guided by Bloom's taxonomy of educational objectives and
incorporates ten questioning strategies, including situational questioning,
zone of proximal development (ZPD) questioning, and metacognitive
questioning-thus better capturing authentic classroom interactions.
Furthermore, we design differentiated teaching strategies for students at
different cognitive levels, thereby providing more targeted teaching guidance.
Building on EduDial, we further develop EduDial-LLM 32B via training and
propose an 11-dimensional evaluation framework that systematically measures the
teaching abilities of LLMs, encompassing both overall teaching quality and
content quality. Experiments on 17 mainstream LLMs reveal that most models
struggle in student-centered teaching scenarios, whereas our EduDial-LLM
achieves significant gains, consistently outperforming all baselines across all
metrics. The code is available at
https://github.com/Mind-Lab-ECNU/EduDial/tree/main.

</details>


### [5] [Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering](https://arxiv.org/abs/2510.12925)
*Nil-Jana Akpinar,Chia-Jung Lee,Vanessa Murdock,Pietro Perona*

Main category: cs.CL

TL;DR: æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†LLMså¯¹è¯¢é—®è§’è‰²çš„é²æ£’æ€§ï¼Œå‘ç°ç”¨æˆ·ä¸ªäººå±æ€§ä¿¡æ¯ä¼šæ˜¾è‘—å½±å“é—®ç­”å‡†ç¡®æ€§å¹¶è§¦å‘å¤šç§å¤±è´¥æ¨¡å¼ã€‚


<details>
  <summary>Details</summary>
Motivation: è¯„ä¼°LLMsåœ¨é¢å¯¹åŒ…å«ç”¨æˆ·èº«ä»½ã€ä¸“ä¸šçŸ¥è¯†æˆ–ä¿¡ä»°ç­‰ä¸ªäººå±æ€§çš„è¯¢é—®æ—¶çš„é²æ£’æ€§ï¼Œè¿™äº›æ˜¯çœŸå®äº¤äº’ä¸­å¸¸è§çš„ç”¨æˆ·ä¿¡æ¯ã€‚

Method: é€šè¿‡è®¾è®¡åŒ…å«ç”¨æˆ·å±æ€§ä¿¡æ¯çš„è¯¢é—®è§’è‰²æç¤ºï¼Œç³»ç»Ÿæµ‹è¯•LLMsåœ¨æ¥æ”¶ä¸åŒç”¨æˆ·èƒŒæ™¯ä¿¡æ¯æ—¶çš„é—®ç­”è¡¨ç°ã€‚

Result: ç”¨æˆ·è§’è‰²æç¤ºèƒ½æ˜¾è‘—æ”¹å˜é—®ç­”å‡†ç¡®æ€§ï¼Œè§¦å‘æ‹’ç»å›ç­”ã€å¹»è§‰é™åˆ¶å’Œè§’è‰²æ··æ·†ç­‰å¤±è´¥æ¨¡å¼ã€‚

Conclusion: æ¨¡å‹å¯¹ç”¨æˆ·æ¡†æ¶çš„æ•æ„Ÿæ€§ä¼šæŸå®³äº‹å®å¯é æ€§ï¼Œè¯¢é—®è§’è‰²æµ‹è¯•æ˜¯æœ‰æ•ˆçš„é²æ£’æ€§è¯„ä¼°å·¥å…·ã€‚

Abstract: Large Language Models (LLMs) should answer factual questions truthfully,
grounded in objective knowledge, regardless of user context such as
self-disclosed personal information, or system personalization. In this paper,
we present the first systematic evaluation of LLM robustness to inquiry
personas, i.e. user profiles that convey attributes like identity, expertise,
or belief. While prior work has primarily focused on adversarial inputs or
distractors for robustness testing, we evaluate plausible, human-centered
inquiry persona cues that users disclose in real-world interactions. We find
that such cues can meaningfully alter QA accuracy and trigger failure modes
such as refusals, hallucinated limitations, and role confusion. These effects
highlight how model sensitivity to user framing can compromise factual
reliability, and position inquiry persona testing as an effective tool for
robustness evaluation.

</details>


### [6] [The Curious Case of Curiosity across Human Cultures and LLMs](https://arxiv.org/abs/2510.12943)
*Angana Borah,Rada Mihalcea*

Main category: cs.CL

TL;DR: è¯¥è®ºæ–‡ç ”ç©¶äº†LLMsåœ¨ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„å¥½å¥‡å¿ƒè¡¨è¾¾ï¼Œå‘ç°LLMsä¼šå‰Šå¼±è·¨æ–‡åŒ–å¤šæ ·æ€§ï¼Œæ›´æ¥è¿‘è¥¿æ–¹å›½å®¶çš„è¡¨è¾¾æ–¹å¼ï¼Œå¹¶é€šè¿‡å¾®è°ƒç­–ç•¥å°†äººæœºå¯¹é½å·®è·ç¼©å°äº†50%ã€‚


<details>
  <summary>Details</summary>
Motivation: æ¢ç´¢LLMsåœ¨è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹å¥½å¥‡å¿ƒçš„è¡¨è¾¾å·®å¼‚ï¼Œå› ä¸ºå¥½å¥‡å¿ƒæ˜¯é©±åŠ¨æ¢ç©¶çš„æ ¸å¿ƒå› ç´ ï¼Œä½†åœ¨è¿™äº›ç³»ç»Ÿä¸­å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚

Method: ä½¿ç”¨Yahoo! Answerså¤šå›½æ•°æ®é›†ï¼Œå¼•å…¥CUESTè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è¯­è¨€é£æ ¼ã€ä¸»é¢˜åå¥½åˆ†æå’Œç¤¾ä¼šç§‘å­¦æ„å»ºæ¥æµ‹é‡äººæœºå¥½å¥‡å¿ƒå¯¹é½ã€‚

Result: å‘ç°LLMsä¼šå‰Šå¼±è·¨æ–‡åŒ–å¤šæ ·æ€§ï¼Œæ›´æ¥è¿‘è¥¿æ–¹å›½å®¶çš„è¡¨è¾¾æ–¹å¼ï¼›é€šè¿‡å¾®è°ƒç­–ç•¥å¯å°†äººæœºå¯¹é½å·®è·ç¼©å°50%ï¼›è¯æ˜å¥½å¥‡å¿ƒå¯¹LLMsè·¨æ–‡åŒ–é€‚åº”æ€§çš„å®ç”¨ä»·å€¼ã€‚

Conclusion: å¥½å¥‡å¿ƒå¯¹äºLLMsçš„è·¨æ–‡åŒ–é€‚åº”æ€§å…·æœ‰é‡è¦ä»·å€¼ï¼Œæ˜¯æœªæ¥NLPç ”ç©¶çš„é‡è¦æ–¹å‘ã€‚

Abstract: Recent advances in Large Language Models (LLMs) have expanded their role in
human interaction, yet curiosity -- a central driver of inquiry -- remains
underexplored in these systems, particularly across cultural contexts. In this
work, we investigate cultural variation in curiosity using Yahoo! Answers, a
real-world multi-country dataset spanning diverse topics. We introduce CUEST
(CUriosity Evaluation across SocieTies), an evaluation framework that measures
human-model alignment in curiosity through linguistic (style), topic preference
(content) analysis and grounding insights in social science constructs. Across
open- and closed-source models, we find that LLMs flatten cross-cultural
diversity, aligning more closely with how curiosity is expressed in Western
countries. We then explore fine-tuning strategies to induce curiosity in LLMs,
narrowing the human-model alignment gap by up to 50\%. Finally, we demonstrate
the practical value of curiosity for LLM adaptability across cultures, showing
its importance for future NLP research.

</details>


### [7] [On the Role of Preference Variance in Preference Optimization](https://arxiv.org/abs/2510.13022)
*Jiacheng Guo,Zihao Li,Jiahao Qiu,Yue Wu,Mengdi Wang*

Main category: cs.CL

TL;DR: æœ¬æ–‡ç ”ç©¶äº†åå¥½æ–¹å·®ï¼ˆPVarï¼‰å¯¹DPOè®­ç»ƒæ•ˆæœçš„å½±å“ï¼Œå‘ç°é«˜PVarçš„æç¤ºèƒ½äº§ç”Ÿæ›´å¤§çš„æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è¿›è¡ŒLLMå¯¹é½ã€‚


<details>
  <summary>Details</summary>
Motivation: æ”¶é›†äººç±»åå¥½æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦å‡å°‘æ‰€éœ€æ ‡æ³¨çš„æ–¹æ³•ã€‚åå¥½æ–¹å·®å¯ä»¥è¡¡é‡æ¨¡å‹åœ¨æ¯”è¾ƒå“åº”å¯¹æ—¶çš„åå¥½å·®å¼‚ç¨‹åº¦ã€‚

Method: é€šè¿‡ç†è®ºåˆ†æå»ºç«‹DPOæ¢¯åº¦èŒƒæ•°çš„ä¸Šç•Œï¼Œè¯æ˜å…¶å—PVaræ§åˆ¶ã€‚ä½¿ç”¨å¥–åŠ±æ¨¡å‹ç”Ÿæˆåå¥½æ•°æ®ï¼Œåœ¨AlpacaEval 2.0å’ŒArena-HardåŸºå‡†ä¸Šå¾®è°ƒLLMsï¼Œæ¯”è¾ƒä¸åŒPVaræç¤ºçš„è®­ç»ƒæ•ˆæœã€‚

Result: å®éªŒè¡¨æ˜é«˜PVaræç¤ºä¼˜äºéšæœºé€‰æ‹©æˆ–ä½PVaræç¤ºã€‚ä½¿ç”¨å°å¥–åŠ±æ¨¡å‹ï¼ˆ1Bã€3Bï¼‰è¿›è¡Œé€‰æ‹©æ—¶ï¼ŒPVaræ–¹æ³•ä¾ç„¶ç¨³å¥ã€‚åœ¨UltraFeedbackæ•°æ®é›†ä¸Šï¼Œä»…ä½¿ç”¨PVaræœ€é«˜çš„å‰10%æç¤ºè®­ç»ƒï¼Œæ•ˆæœä¼˜äºä½¿ç”¨å®Œæ•´æ•°æ®é›†ã€‚

Conclusion: åå¥½æ–¹å·®æ˜¯è¯†åˆ«ä¿¡æ¯ä¸°å¯Œç¤ºä¾‹ä»¥è¿›è¡Œé«˜æ•ˆLLMå¯¹é½çš„é‡è¦æŒ‡æ ‡ï¼Œé«˜PVaræç¤ºèƒ½äº§ç”Ÿæ›´æœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ã€‚

Abstract: Direct Preference Optimization (DPO) has emerged as an important approach for
learning from human preferences in aligning large language models (LLMs).
However, collecting human preference data is costly and inefficient, motivating
methods to reduce the required annotations. In this work, we investigate the
impact of \emph{preference variance} (PVar), which measures the variance in
model preferences when comparing pairs of responses, on the effectiveness of
DPO training. We provide a theoretical insight by establishing an upper bound
on the DPO gradient norm for any given prompt, showing it is controlled by the
PVar of that prompt. This implies that prompts with low PVar can only produce
small gradient updates, making them less valuable for learning. We validate
this finding by fine-tuning LLMs with preferences generated by a reward model,
evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental
results demonstrate that prompts with higher PVar outperform randomly selected
prompts or those with lower PVar. We also show that our PVar-based selection
method is robust, when using smaller reward models (1B, 3B) for selection.
Notably, in a separate experiment using the original human annotations from the
UltraFeedback dataset, we found that training on only the top 10\% of prompts
with the highest PVar yields better evaluation performance than training on the
full dataset, highlighting the importance of preference variance in identifying
informative examples for efficient LLM alignment.

</details>


### [8] [Stable LLM Ensemble: Interaction between Example Representativeness and Diversity](https://arxiv.org/abs/2510.13143)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: è¯¥ç ”ç©¶ç³»ç»Ÿåˆ†æäº†ç¤ºä¾‹ä»£è¡¨æ€§ï¼ˆå•æ ·æœ¬ç­–ç•¥ï¼‰å’Œè¾“å‡ºå¤šæ ·æ€§ï¼ˆé‡‡æ ·æ¸©åº¦ï¼‰å¯¹LLMé›†æˆæ€§èƒ½çš„å½±å“ï¼Œæå‡ºåŸºäºè´¨å¿ƒçš„ä»£è¡¨æ€§ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œåœ¨è¾ƒé«˜æ¸©åº¦è®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©å’Œ5æ ·æœ¬æç¤ºã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰å•æ ·æœ¬LLMé¢„æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§å¯¹ç¤ºä¾‹é€‰æ‹©å’Œé›†æˆæˆå‘˜å¤šæ ·æ€§é«˜åº¦æ•æ„Ÿï¼Œéœ€è¦ç³»ç»Ÿç ”ç©¶å¦‚ä½•é€šè¿‡ä¼˜åŒ–ç¤ºä¾‹é€‰æ‹©å’Œæ¸©åº¦æ§åˆ¶æ¥æå‡é›†æˆæ€§èƒ½ã€‚

Method: æ¯”è¾ƒä¸¤ç§å•æ ·æœ¬ç­–ç•¥ï¼šåŸºäºè´¨å¿ƒçš„ä»£è¡¨æ€§ç¤ºä¾‹ï¼ˆæå‡ºæ–¹æ³•ï¼‰å’Œéšæœºé‡‡æ ·ç¤ºä¾‹ï¼ˆåŸºçº¿ï¼‰ï¼ŒåŒæ—¶å˜åŒ–é‡‡æ ·æ¸©åº¦ï¼Œè¯„ä¼°å¯¹LLMé›†æˆæ€§èƒ½çš„å½±å“ã€‚

Result: æå‡ºçš„æ–¹æ³•åœ¨è¾ƒé«˜æ¸©åº¦è®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ï¼š+7.6%ï¼ˆå®F1ï¼‰å’Œ-10.5%ï¼ˆRMSEï¼‰ï¼Œä¸”è¶…è¿‡5æ ·æœ¬æç¤ºï¼š+21.1%ï¼ˆå®F1ï¼‰å’Œ-24.0%ï¼ˆRMSEï¼‰ã€‚

Conclusion: ç»“åˆä»£è¡¨æ€§ç¤ºä¾‹é€‰æ‹©å’Œå¢åŠ æ¸©åº¦èƒ½ä¸ºé›†æˆæä¾›é€‚å½“çš„å¤šæ ·æ€§æ°´å¹³ï¼Œå¼ºè°ƒäº†ç¤ºä¾‹é€‰æ‹©å’Œå—æ§å¤šæ ·æ€§åœ¨è®¾è®¡æœ‰æ•ˆå•æ ·æœ¬LLMé›†æˆä¸­çš„å®é™…é‡è¦æ€§ã€‚

Abstract: Large language models (LLMs) have achieved remarkable results in wide range
of domains. However, the accuracy and robustness of one-shot LLM predictions
remain highly sensitive to the examples and the diversity among ensemble
members. This study systematically investigates the effects of example
representativeness (one-shot strategy) and output diversity (sampling
temperature) on LLM ensemble performance. Two one-shot strategies are compared:
centroid-based representative examples (proposed) and randomly sampled examples
(baseline) and sampling temperature also is varied. The proposed approach with
higher temperature setting significantly outperforms random selection by +7.6%
(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot
prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that
combining representative example selection with increased temperature provides
the appropriate level of diversity to the ensemble. This work highlights the
practical importance of both example selection and controlled diversity in
designing effective one-shot LLM ensembles.

</details>


### [9] [A Matter of Representation: Towards Graph-Based Abstract Code Generation](https://arxiv.org/abs/2510.13163)
*Nyx Iskandar,Hisham Bedri,Andy Tsen*

Main category: cs.CL

TL;DR: è¯¥è®ºæ–‡æå‡ºå¹¶è¯„ä¼°äº†ç”¨äºå›¾ç»“æ„æŠ½è±¡ä»£ç ç”Ÿæˆçš„JSONè¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨åŸºäºScratchçš„æµ‹è¯•åŸºå‡†ä¸ŠéªŒè¯äº†LLMèƒ½å¤Ÿå•æ¬¡ç”Ÿæˆå›¾ç»“æ„ä»£ç ï¼Œä¸”ä¸åŒè¡¨ç¤ºæ–¹æ³•å¯¹å‡†ç¡®ç‡æœ‰æ˜¾è‘—å½±å“ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰LLMæ“…é•¿ç”ŸæˆåŸå§‹é¡ºåºä»£ç ï¼Œä½†åœ¨å›¾ç»“æ„æŠ½è±¡ä»£ç ç”Ÿæˆæ–¹é¢ç ”ç©¶è¾ƒå°‘ï¼Œè¿™ç§èƒ½åŠ›å¯¹å¯è§†åŒ–ç¼–ç¨‹è¯­è¨€å’Œæ— æ³•è®¿é—®æºä»£ç çš„åœºæ™¯å¾ˆé‡è¦ã€‚

Method: æå‡ºå¹¶è¯„ä¼°äº†å›¾ç»“æ„çš„JSONè¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨åŸºäºPythoné‡å®ç°çš„Scratchæµ‹è¯•åŸºå‡†ScratchTestä¸Šè¿›è¡Œè¯„ä¼°ã€‚

Result: LLMèƒ½å¤Ÿåœ¨å•æ¬¡ç”Ÿæˆä¸­å®Œæˆå›¾ç»“æ„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œæ— éœ€å¤æ‚æµç¨‹ï¼Œä¸”ä¸åŒè¡¨ç¤ºæ–¹æ³•å¯¼è‡´å‡†ç¡®ç‡æ˜¾è‘—å·®å¼‚ã€‚

Conclusion: è¿™é¡¹å·¥ä½œä¸ºå›¾ç»“æ„æŠ½è±¡ä»£ç ç”Ÿæˆçš„è¡¨ç¤ºå­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚

Abstract: Most large language models (LLMs) today excel at generating raw, sequential
code with minimal abstractions and custom structures. However, there has been
little work on graph-based abstract code generation, where significant logic is
encapsulated in predefined nodes and execution flow is determined by edges.
This is relevant for visual programming languages, and in cases where raw
source code is inaccessible to users and LLM training sets. In this work, we
propose and evaluate JSON representations for graphs to enable high accuracy
graph-based abstract code generation. We evaluate these representations on
ScratchTest, a mini-benchmark based on our custom Python re-implementation of
Scratch, which tests the LLM in code graph space. Our findings demonstrate that
LLMs can indeed perform the aforementioned generation task in a single pass
without relying on specialized or complex pipelines, given the correct graph
representations. We also show that different representations induce
significantly different accuracies, highlighting the instrumental role of
representations in this generation task. All in all, this work establishes the
first steps towards representation learning for graph-based abstract code
generation.

</details>


### [10] [Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism](https://arxiv.org/abs/2510.13170)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Duanyang Yuan,Haoyuan Chen,Xiaoyu Sun,Linyuan Meng,Xinwang Liu*

Main category: cs.CL

TL;DR: æœ¬æ–‡é¦–æ¬¡ä»äººç±»æ¨ç†ç†è®ºè§’åº¦ç³»ç»Ÿç»¼è¿°æ€ç»´é“¾å¾®è°ƒæŠ€æœ¯ï¼ŒåŸºäºå…­é¡¶æ€è€ƒå¸½æ¡†æ¶å¯¹CoTå¾®è°ƒæ–¹æ³•è¿›è¡Œåˆ†ç±»åˆ†æï¼Œå¹¶å±•æœ›æœªæ¥ç ”ç©¶æ–¹å‘ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰å…³äºæ€ç»´é“¾å¾®è°ƒçš„ç ”ç©¶ä¸»è¦å…³æ³¨æŠ€æœ¯å±‚é¢ï¼Œç¼ºä¹ä»äººç±»æ¨ç†æœºåˆ¶è§’åº¦çš„ç³»ç»Ÿåˆ†æã€‚é‰´äºCoTå¾®è°ƒçš„æœ€ç»ˆç›®æ ‡æ˜¯è®©å¤§è¯­è¨€æ¨¡å‹åƒäººç±»ä¸€æ ·æ¨ç†ï¼Œä»è®¤çŸ¥è§’åº¦ç ”ç©¶è¿™ä¸€æŠ€æœ¯è‡³å…³é‡è¦ã€‚

Method: é‡‡ç”¨å…­é¡¶æ€è€ƒå¸½æ¡†æ¶å¯¹CoTå¾®è°ƒæ–¹æ³•è¿›è¡Œåˆ†ç±»å’Œç³»ç»Ÿåˆ†æï¼Œæ„å»ºäº†æ•°æ®é›†å’Œæ¨¡å‹æ€§èƒ½çš„å…¨é¢æ¦‚è¿°ï¼Œå¹¶ç»´æŠ¤å®æ—¶GitHubä»“åº“è·Ÿè¸ªæœ€æ–°è¿›å±•ã€‚

Result: æå‡ºäº†åŸºäºäººç±»æ¨ç†ç†è®ºçš„CoTå¾®è°ƒç³»ç»Ÿåˆ†ç±»æ¡†æ¶ï¼Œè¯†åˆ«äº†ç°æœ‰æ–¹æ³•åœ¨æ¨¡æ‹Ÿäººç±»æ¨ç†èƒ½åŠ›æ–¹é¢çš„ç‰¹ç‚¹ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚

Conclusion: è¿™é¡¹è°ƒæŸ¥ä¸ºCoTå¾®è°ƒé¢†åŸŸæä¾›äº†åŸºäºäººç±»è®¤çŸ¥ç†è®ºçš„æ–°è§†è§’ï¼Œæœ‰æœ›æ¿€å‘åˆ›æ–°å¹¶æ¨åŠ¨è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„è¿›æ­¥ã€‚

Abstract: Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs)
with reasoning capabilities by training them on curated reasoning traces. It
leverages both supervised and reinforced fine-tuning to cultivate human-like
reasoning skills in LLMs, including detailed planning, divergent thinking,
intuitive judgment, timely reflection, internal thinking, and fact perception,
etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial
improvements in tasks such as mathematical reasoning and code generation.
However, existing surveys about CoT fine-tuning primarily focus on technical
aspects and overlook a systematic analysis from the perspective of human
reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to
enable LLMs to reason like humans, it is crucial to investigate this technique
through the lens of human cognition. To fill this gap, we present the first
comprehensive survey of CoT fine-tuning grounded in human reasoning theory.
Specifically, inspired by the well-known Six Thinking Hats framework, which
systematically characterizes common human thinking modes using six metaphorical
hats, we classify and examine CoT fine-tuning methods through this lens.
Furthermore, building upon this theory, we outline potential directions for
future research in CoT fine-tuning. In addition, we compile a comprehensive
overview of existing datasets and model performances, and a real-time GitHub
repository \footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that
continuously tracks recent advances in this area is maintained. We hope this
survey will serve as a valuable resource to inspire innovation and foster
progress in this rapidly evolving field.

</details>


### [11] [Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](https://arxiv.org/abs/2510.13255)
*Jingmin An,Yilong Song,Ruolin Yang,Nai Ding,Lingxi Lu,Yuxuan Wang,Wei Wang,Chu Zhuang,Qian Wang,Fang Fang*

Main category: cs.CL

TL;DR: è¯¥è®ºæ–‡æå‡ºäº†åˆ†å±‚é¢‘ç‡æ ‡è®°æ¢é’ˆ(HFTP)ï¼Œé€šè¿‡é¢‘åŸŸåˆ†æè¯†åˆ«LLMä¸­å¤„ç†å¥æ³•ç»“æ„çš„ç¥ç»å…ƒç»„ä»¶å’Œå¤§è„‘çš®å±‚åŒºåŸŸï¼Œå‘ç°LLMä¸äººç±»å¤§è„‘åœ¨å¥æ³•å¤„ç†ä¸Šå­˜åœ¨ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: æ¢ç´¢LLMçš„è¯­è¨€èƒ½åŠ›æ˜¯å¦æºäºä¸äººç±»å¤§è„‘ç›¸ä¼¼çš„æœºåˆ¶ï¼Œä»¥åŠè¯†åˆ«è´Ÿè´£å¥æ³•å¤„ç†çš„å…·ä½“è®¡ç®—æ¨¡å—ã€‚

Method: ä½¿ç”¨åˆ†å±‚é¢‘ç‡æ ‡è®°æ¢é’ˆ(HFTP)è¿›è¡Œé¢‘åŸŸåˆ†æï¼Œæ¯”è¾ƒGPT-2ã€Gemmaã€Llamaç­‰å¤šä¸ªLLMæ¨¡å‹ä¸äººç±»å¤§è„‘çš®å±‚åœ¨å¥æ³•å¤„ç†ä¸Šçš„è¡¨å¾ç›¸ä¼¼æ€§ã€‚

Result: LLMåœ¨ç›¸ä¼¼å±‚å¤„ç†å¥æ³•ï¼Œè€Œäººç±»å¤§è„‘ä¾èµ–ä¸åŒçš®å±‚åŒºåŸŸï¼›LLMè¡¨å¾ä¸å¤§è„‘å·¦åŠçƒæ›´ç›¸ä¼¼ï¼›æ¨¡å‹å‡çº§å‘ˆç°ä¸åŒè¶‹åŠ¿ï¼šGemma 2æ¯”Gemmaæ›´æ¥è¿‘å¤§è„‘ï¼Œè€ŒLlama 3.1æ¯”Llama 2æ›´åç¦»ã€‚

Conclusion: HFTPæ˜¯è¿æ¥è®¡ç®—è¯­è¨€å­¦å’Œè®¤çŸ¥ç¥ç»ç§‘å­¦çš„æœ‰ä»·å€¼å·¥å…·ï¼Œæ­ç¤ºäº†LLMè¡Œä¸ºæ”¹è¿›çš„æœºåˆ¶å¯èƒ½å¹¶éå®Œå…¨äººç±»åŒ–ã€‚

Abstract: Large Language Models (LLMs) demonstrate human-level or even superior
language abilities, effectively modeling syntactic structures, yet the specific
computational modules responsible remain unclear. A key question is whether LLM
behavioral capabilities stem from mechanisms akin to those in the human brain.
To address these questions, we introduce the Hierarchical Frequency Tagging
Probe (HFTP), a tool that utilizes frequency-domain analysis to identify
neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)
neurons) and cortical regions (via intracranial recordings) encoding syntactic
structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama
2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human
brain relies on distinct cortical regions for different syntactic levels.
Representational similarity analysis reveals a stronger alignment between LLM
representations and the left hemisphere of the brain (dominant in language
processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows
greater brain similarity than Gemma, while Llama 3.1 shows less alignment with
the brain compared to Llama 2. These findings offer new insights into the
interpretability of LLM behavioral improvements, raising questions about
whether these advancements are driven by human-like or non-human-like
mechanisms, and establish HFTP as a valuable tool bridging computational
linguistics and cognitive neuroscience. This project is available at
https://github.com/LilTiger/HFTP.

</details>


### [12] [Do You Get the Hint? Benchmarking LLMs on the Board Game Concept](https://arxiv.org/abs/2510.13271)
*Ine Gevers,Walter Daelemans*

Main category: cs.CL

TL;DR: Conceptæ¸¸æˆä½œä¸ºè¯„ä¼°LLMæº¯å› æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œç»“æœæ˜¾ç¤ºLLMåœ¨éœ€è¦æŠ½è±¡æ¨ç†å’Œæˆ˜ç•¥æ„å›¾ç†è§£çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼ŒæˆåŠŸç‡ä½äº40%ï¼Œè¿œä½äºäººç±»çš„90%ä»¥ä¸Šã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰LLMåœ¨éœ€è¦æŠ½è±¡æ¨ç†çš„ä»»åŠ¡ä¸Šä»æœ‰æ ¹æœ¬æ€§å¼±ç‚¹ï¼Œç‰¹åˆ«æ˜¯å¤„ç†ä¸è‡ªç„¶è¯­è¨€è®­ç»ƒæ•°æ®è¡¨ç¤ºå½¢å¼ä¸åŒçš„ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚

Method: å¼•å…¥Conceptå•è¯çŒœæµ‹æ£‹ç›˜æ¸¸æˆä½œä¸ºåŸºå‡†ï¼Œåœ¨æ›´æ¥è¿‘LLMé¢„è®­ç»ƒæ•°æ®ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰çš„è¡¨ç¤ºä¸­æµ‹è¯•æº¯å› æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ‰©å±•åˆ°å¤šç§è¯­è¨€è¿›è¡Œè¯„ä¼°ã€‚

Result: äººç±»æˆåŠŸç‡è¶…è¿‡90%ï¼Œè€Œæœ€å…ˆè¿›çš„LLMæˆåŠŸç‡ä¸è¶…è¿‡40%ã€‚LLMåœ¨è§£é‡Šå…¶ä»–ç©å®¶æˆ˜ç•¥æ„å›¾å’Œæ ¹æ®é¡ºåºä¿¡æ¯æ›´æ–°ä¿®æ­£åˆå§‹å‡è®¾æ–¹é¢å­˜åœ¨å›°éš¾ã€‚

Conclusion: Conceptæ¸¸æˆæ­ç¤ºäº†LLMåœ¨æŠ½è±¡æ¨ç†å’Œæˆ˜ç•¥ç†è§£æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°æ›´å·®ã€‚

Abstract: Large language models (LLMs) have achieved striking successes on many
benchmarks, yet recent studies continue to expose fundamental weaknesses. In
particular, tasks that require abstract reasoning remain challenging, often
because they use representations such as grids, symbols, or visual patterns
that differ from the natural language data LLMs are trained on. In this paper,
we introduce Concept, a simple word-guessing board game, as a benchmark for
probing abductive reasoning in a representation that is much closer to LLM
pre-training data: natural language. Our results show that this game, easily
solved by humans (with a success rate of over 90\%), is still very challenging
for state-of-the-art LLMs (no model exceeds 40\% success rate). Specifically,
we observe that LLMs struggle with interpreting other players' strategic
intents, and with correcting initial hypotheses given sequential information
updates. In addition, we extend the evaluation across multiple languages, and
find that the LLM performance drops further in lower-resource languages (Dutch,
French, and Spanish) compared to English.

</details>


### [13] [Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13272)
*Zhichao Xu,Zongyu Wu,Yun Zhou,Aosong Feng,Kang Zhou,Sangmin Woo,Kiran Ramnath,Yijun Tian,Xuan Qi,Weikang Qiu,Lin Lee Cheong,Haibo Ding*

Main category: cs.CL

TL;DR: æå‡ºVERITASæ¡†æ¶ï¼Œé€šè¿‡å°†ç»†ç²’åº¦å¿ å®æ€§å¥–åŠ±é›†æˆåˆ°å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæå‡åŸºäºRLçš„æœç´¢ä»£ç†çš„æ¨ç†å¿ å®æ€§ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰åŸºäºRLçš„æœç´¢ä»£ç†æ–¹æ³•è™½ç„¶èƒ½æå‡é—®ç­”åŸºå‡†æ€§èƒ½ï¼Œä½†è¿‡äºå…³æ³¨æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§è€Œå¿½ç•¥äº†ä¸­é—´æ¨ç†æ­¥éª¤çš„è´¨é‡ï¼Œå¯èƒ½å¯¼è‡´æ€ç»´é“¾ä¸å¿ å®çš„é—®é¢˜ã€‚

Method: é¦–å…ˆå¼•å…¥å…¨é¢çš„è¯„ä¼°æ¡†æ¶è¯„ä¼°åŸºäºRLçš„æœç´¢ä»£ç†çš„å¿ å®æ€§ï¼Œç„¶åæå‡ºVERITASæ¡†æ¶ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­é›†æˆç»†ç²’åº¦å¿ å®æ€§å¥–åŠ±ã€‚

Result: å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨VERITASè®­ç»ƒçš„æ¨¡å‹ä¸ä»…æ˜¾è‘—æé«˜äº†æ¨ç†å¿ å®æ€§ï¼Œè¿˜åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†ä¸Šå®ç°äº†å¯æ¯”è¾ƒçš„ä»»åŠ¡æ€§èƒ½ã€‚

Conclusion: VERITASæ¡†æ¶èƒ½æœ‰æ•ˆæå‡æœç´¢ä»£ç†çš„æ¨ç†å¿ å®æ€§ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å¿½è§†ä¸­é—´æ¨ç†è´¨é‡çš„é—®é¢˜ã€‚

Abstract: Inspired by the success of reinforcement learning (RL) in Large Language
Model (LLM) training for domains like math and code, recent works have begun
exploring how to train LLMs to use search engines more effectively as tools for
retrieval-augmented generation. Although these methods achieve performance
improvement across QA benchmarks, many prioritize final answer correctness
while overlooking the quality of intermediate reasoning steps, which may lead
to chain-of-thought unfaithfulness. In this paper, we first introduce a
comprehensive evaluation framework for evaluating RL-based search agents,
covering three distinct faithfulness metrics: information-think faithfulness,
think-answer faithfulness, and think-search faithfulness. Our evaluations
reveal that a prototypical RL-based search agent, Search-R1, has significant
room for improvement in this regard. To foster faithful reasoning, we introduce
VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in
Agentic Search), a novel framework that integrates fine-grained faithfulness
rewards into the reinforcement learning process. Our experiments show that
models trained with VERITAS not only significantly improve reasoning
faithfulness, but also achieve comparable task performance across seven QA
benchmarks.

</details>


### [14] [ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering](https://arxiv.org/abs/2510.13312)
*Simon Lupart,Mohammad Aliannejadi,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: ChatR1æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹è¯é—®ç­”æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡äº¤é”™æœç´¢å’Œæ¨ç†æ¥å¤„ç†ç”¨æˆ·æ„å›¾çš„åŠ¨æ€æ¼”å˜ï¼Œä½¿ç”¨æ„å›¾æ„ŸçŸ¥å¥–åŠ±è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚


<details>
  <summary>Details</summary>
Motivation: å¯¹è¯é—®ç­”ä¸­ç”¨æˆ·æ„å›¾ä¼šéšå¯¹è¯è½®æ¬¡æ¼”å˜ï¼Œä¸”è¯è¯­å¾€å¾€ä¸å®Œæ•´ï¼Œéœ€è¦ä¸Šä¸‹æ–‡è§£é‡Šã€æŸ¥è¯¢é‡æ„ä»¥åŠæ£€ç´¢ä¸ç”Ÿæˆçš„åŠ¨æ€åè°ƒï¼Œé™æ€ç®¡é“æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†è¿™äº›æŒ‘æˆ˜ã€‚

Method: æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†æ¡†æ¶ï¼Œäº¤é”™è¿›è¡Œæœç´¢å’Œæ¨ç†ï¼Œé‡‡ç”¨æ„å›¾æ„ŸçŸ¥å¥–åŠ±æä¾›è½®æ¬¡çº§åé¦ˆï¼Œä½¿æ£€ç´¢å’Œæ¨ç†ä¸ç”¨æˆ·ç›®æ ‡å¯¹é½ã€‚

Result: åœ¨3Bå’Œ7Bæ¨¡å‹éª¨å¹²ä¸Šå‡è¡¨ç°å¼ºåŠ²ï¼Œåœ¨äº”ä¸ªå¯¹è¯é—®ç­”æ•°æ®é›†ä¸Šä¼˜äºç«äº‰æ¨¡å‹ï¼Œé€šè¿‡F1ã€BERTScoreå’ŒLLM-as-judgeç­‰æŒ‡æ ‡éªŒè¯ã€‚

Conclusion: åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†æ¯”é™æ€å¯¹è¯é—®ç­”ç®¡é“æ›´çµæ´»å’Œä¸Šä¸‹æ–‡æ•æ„Ÿï¼Œèƒ½å¤Ÿå®ç°å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹å’Œæœ‰æ•ˆçš„æœç´¢å·¥å…·ä½¿ç”¨ã€‚

Abstract: We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of sparse and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.

</details>


### [15] [D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree](https://arxiv.org/abs/2510.13363)
*Xiang Lei,Qin Li,Min Zhang,Min Zhang*

Main category: cs.CL

TL;DR: D-SMARTæ˜¯ä¸€ä¸ªæ¨¡å‹æ— å…³æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€ç»“æ„åŒ–è®°å¿†å’Œæ¨ç†æ ‘æ¥ç»´æŠ¤å¤šè½®å¯¹è¯çš„ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡å¯¹è¯è´¨é‡ã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³LLMsåœ¨å¤šè½®å¯¹è¯ä¸­å‡ºç°äº‹å®ä¸ä¸€è‡´å’Œé€»è¾‘è¡°å‡çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¦‚RAGå’Œä»£ç†å·¥ä½œè®°å¿†ä»ä¾èµ–é™æ€çŸ¥è¯†æºï¼Œæ— æ³•é€‚åº”åŠ¨æ€å¯¹è¯ä¸Šä¸‹æ–‡ã€‚

Method: ä½¿ç”¨åŠ¨æ€ç»“æ„åŒ–è®°å¿†(DSM)æ„å»ºæƒå¨çŸ¥è¯†å›¾è°±ï¼Œç»“åˆæ¨ç†æ ‘(RT)è¿›è¡Œå¤šæ­¥å›¾æœç´¢æ¨ç†ï¼Œå¹¶å¼•å…¥åŸºäºNLIçš„æ–°æŒ‡æ ‡æ¥è¯„ä¼°å¯¹è¯ä¸€è‡´æ€§ã€‚

Result: åœ¨MT-Bench-101åŸºå‡†æµ‹è¯•ä¸­ï¼ŒD-SMARTæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¯¹è¯ä¸€è‡´æ€§å¾—åˆ†æå‡è¶…è¿‡48%ï¼Œå¼€æºæ¨¡å‹è´¨é‡å¾—åˆ†æå‡è¾¾10.1%ã€‚

Conclusion: D-SMARTé€šè¿‡åŠ¨æ€ç»“æ„åŒ–è¡¨ç¤ºå’Œæ˜¾å¼æ¨ç†æœ‰æ•ˆè§£å†³äº†å¤šè½®å¯¹è¯ä¸­çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œä¸ºLLMsåœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ã€‚

Abstract: Large Language Models (LLMs) often exhibit factual inconsistencies and
logical decay in extended, multi-turn dialogues, a challenge stemming from
their reliance on static, pre-trained knowledge and an inability to reason
adaptively over the dialogue history. Prevailing mitigation strategies, such as
Retrieval-Augmented Generation (RAG) and agentic working memories, improve
information recall but still engage with fundamentally static knowledge sources
and follow pre-defined single reasoning path. This hinders their ability to
preserve factual and logical consistency of their responses in multi-turn
dialogues while the context evolves over time. To address this issue, we
propose D-SMART, a model-agnostic framework designed to maintain multi-turn
dialogue consistency by enabling LLMs to build and reason over a dynamic,
structured representation of the conversational context. This is achieved via
two synergistic components: (1) a Dynamic Structured Memory (DSM), which
incrementally constructs and maintains an authoritative, OWL-compliant
knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which
executes inferences as an explicit and traceable multi-step search over the
graph. As the popular-used quality score (judged by GPT-4) can overlook logical
flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue
consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that
D-SMART significantly outperforms state-of-the-art baselines, elevating the
dialogue consistency score by over 48\% for both proprietary and open-source
models, and notably improves the quality score of the latter by up to 10.1\%.

</details>


### [16] [Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment](https://arxiv.org/abs/2510.13387)
*Buwei He,Yang Liu,Zhaowei Zhang,Zixia Jia,Huijia Wu,Zhaofeng He,Zilong Zheng,Yipeng Kang*

Main category: cs.CL

TL;DR: è¯¥è®ºæ–‡æ¢ç´¢äº†åœ¨å•è½®å¯¹è¯ä¸­åº”ç”¨è´å¶æ–¯è¯´æœæ¡†æ¶æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æˆ˜ç•¥è¯´æœèƒ½åŠ›ï¼Œé€šè¿‡æ‰¿è¯º-æ²Ÿé€šæœºåˆ¶è®©è¯´æœè€…æ˜ç¡®æè¿°ä¿¡æ¯æ¨¡å¼ï¼Œå¼•å¯¼è¢«è¯´æœè€…è¿›è¡Œè´å¶æ–¯ä¿¡å¿µæ›´æ–°ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰AIç³»ç»Ÿåœ¨è¯´æœèƒ½åŠ›æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç°æœ‰ç ”ç©¶å¾€å¾€å¿½è§†ä¿¡æ¯ä¸å¯¹ç§°çš„æˆ˜ç•¥ä½¿ç”¨æˆ–ä¾èµ–å¼ºå‡è®¾çš„é¢„å…ˆæ‰¿è¯ºã€‚

Method: æå‡ºäº†ä¸¤ç§è´å¶æ–¯è¯´æœæ–¹æ³•ï¼šåŠæ­£å¼è‡ªç„¶è¯­è¨€å’Œå…¨è‡ªç„¶è¯­è¨€ï¼Œå¹¶åœ¨åŒ…å«å¤šæ ·åŒ–è¢«è¯´æœè€…çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ä¸­ä¸åŸºçº¿æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚

Result: å®éªŒå‘ç°ï¼š1ï¼‰åŸºäºè´å¶æ–¯è¯´æœçš„LLMæ¯”åŸºçº¿æ–¹æ³•è·å¾—æ›´é«˜è¯´æœæˆåŠŸç‡ï¼›2ï¼‰åŠæ­£å¼è‡ªç„¶è¯­è¨€ç‰ˆæœ¬æ›´å…·å¯ä¿¡åº¦å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œå…¨è‡ªç„¶è¯­è¨€ç‰ˆæœ¬åœ¨æƒ…æ„Ÿå…±é¸£å’Œé²æ£’æ€§æ–¹é¢æ›´å¼ºï¼›3ï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒï¼Œå°æ¨¡å‹èƒ½è¾¾åˆ°ä¸å¤§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚

Conclusion: è´å¶æ–¯è¯´æœæ¡†æ¶èƒ½æœ‰æ•ˆæå‡LLMçš„æˆ˜ç•¥è¯´æœèƒ½åŠ›ï¼Œä¸åŒæ–¹æ³•å˜ä½“å„æœ‰ä¼˜åŠ¿ï¼Œä¸”æ¨¡å‹è§„æ¨¡ä¸æ˜¯æ€§èƒ½çš„å†³å®šæ€§å› ç´ ã€‚

Abstract: Persuasion, a fundamental social capability for humans, remains a challenge
for AI systems such as large language models (LLMs). Current studies often
overlook the strategic use of information asymmetry in message design or rely
on strong assumptions regarding pre-commitment. In this work, we explore the
application of Bayesian Persuasion (BP) in natural language within single-turn
dialogue settings, to enhance the strategic persuasion capabilities of LLMs.
Our framework incorporates a commitment-communication mechanism, where the
persuader explicitly outlines an information schema by narrating their
potential types (e.g., honest or dishonest), thereby guiding the persuadee in
performing the intended Bayesian belief update. We evaluate two variants of our
approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language
(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)
baselines within a comprehensive evaluation framework. This framework covers a
diverse set of persuadees -- including LLM instances with varying prompts and
fine-tuning and human participants -- across tasks ranging from specially
designed persuasion scenarios to general everyday situations. Experimental
results on LLM-based agents reveal three main findings: (1) LLMs guided by BP
strategies consistently achieve higher persuasion success rates than NBP
baselines; (2) SFNL exhibits greater credibility and logical coherence, while
FNL shows stronger emotional resonance and robustness in naturalistic
conversations; (3) with supervised fine-tuning, smaller models can attain BP
performance comparable to that of larger models.

</details>


### [17] [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554)
*Yang Li,Zhichen Dong,Yuhan Sun,Weixun Wang,Shaopan Xiong,Yijia Luo,Jiashun Liu,Han Lu,Jiamang Wang,Wenbo Su,Bo Zheng,Junchi Yan*

Main category: cs.CL

TL;DR: æœ¬æ–‡é€šè¿‡åˆ†ææ³¨æ„åŠ›æœºåˆ¶æ­ç¤ºLLMçš„æ¨ç†æ¨¡å¼ï¼Œæå‡ºä¸¤ç§æ³¨æ„åŠ›åº¦é‡æŒ‡æ ‡ï¼Œå‘ç°é¢„è§„åˆ’-é”šå®šæœºåˆ¶ï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†ä¸‰ç§é’ˆå¯¹å…³é”®èŠ‚ç‚¹çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚


<details>
  <summary>Details</summary>
Motivation: LLMçš„æ¨ç†æ¨¡å¼ä¸é€æ˜ï¼Œä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å¯¹ç”Ÿæˆå†…å®¹å‡åŒ€åˆ†é…ä¿¡ç”¨ï¼Œæ— æ³•åŒºåˆ†å…³é”®æ­¥éª¤å’Œå¸¸è§„æ­¥éª¤ã€‚

Method: 1) åŒºåˆ†å±€éƒ¨å’Œå…¨å±€æ³¨æ„åŠ›å¤´ï¼›2) æå‡ºçª—å£å¹³å‡æ³¨æ„åŠ›è·ç¦»å’Œæœªæ¥æ³¨æ„åŠ›å½±å“ä¸¤ä¸ªæŒ‡æ ‡ï¼›3) å‘ç°é¢„è§„åˆ’-é”šå®šæœºåˆ¶ï¼›4) å¼€å‘ä¸‰ç§é’ˆå¯¹å…³é”®èŠ‚ç‚¹çš„RLç­–ç•¥ã€‚

Result: æ­ç¤ºäº†LLMæ¨ç†ä¸­çš„é¢„è§„åˆ’-é”šå®šæœºåˆ¶ï¼Œæå‡ºçš„RLç­–ç•¥åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­è·å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚

Conclusion: é€šè¿‡å°†ä¼˜åŒ–ä¸æ¨¡å‹å†…åœ¨æ¨ç†èŠ‚å¥å¯¹é½ï¼Œå°†ä¸é€æ˜çš„ä¼˜åŒ–è½¬å˜ä¸ºç»“æ„æ„ŸçŸ¥çš„è¿‡ç¨‹ï¼Œä¸ºLLMæ¨ç†çš„é€æ˜æœ‰æ•ˆä¼˜åŒ–æä¾›äº†å¯èƒ½è·¯å¾„ã€‚

Abstract: The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [18] [ğŸ‘‹ Meet cto.new: a new AI code agent that crushes benchmarks - and costs $0](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/cuGpdXUET-o5tS2GOz4U4iX4r1Uo_mIEt7YR5cY1sP0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ä»‹ç»äº†ä¸€ä¸ªåä¸ºcto.newçš„æ–°å‹AIä»£ç ä»£ç†ï¼Œå®ƒåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºClaude Codeå’ŒCodexï¼Œå¹¶ä¸”å®Œå…¨å…è´¹ä½¿ç”¨


<details>
  <summary>Details</summary>
Motivation: æä¾›æ¯”ç°æœ‰å•†ä¸šAIä»£ç ä»£ç†ï¼ˆå¦‚Claude Codeå’ŒCodexï¼‰æ€§èƒ½æ›´å¥½ä¸”å®Œå…¨å…è´¹çš„æ›¿ä»£æ–¹æ¡ˆ

Method: å¼€å‘äº†ä¸€ä¸ªæ–°çš„AIä»£ç ä»£ç†ç³»ç»Ÿï¼Œæ”¯æŒGPT-5ã€Sonnet 4.5ç­‰æ¨¡å‹ï¼Œæ— éœ€APIå¯†é’¥å³å¯ä½¿ç”¨

Result: åœ¨åŸºå‡†æµ‹è¯•ä¸­æŒç»­è¶…è¶ŠAnthropicçš„Claude Codeå’ŒOpenAIçš„Codexï¼ŒåŒæ—¶æä¾›å…è´¹æœåŠ¡

Conclusion: cto.newæ˜¯ä¸€ä¸ªæ€§èƒ½ä¼˜è¶Šä¸”å®Œå…¨å…è´¹çš„AIä»£ç ä»£ç†è§£å†³æ–¹æ¡ˆï¼Œä¸ºç”¨æˆ·æä¾›äº†æ›´å¥½çš„é€‰æ‹©

Abstract: ğŸ‘‹ Meet cto.new: a new AI code agent that crushes benchmarks - and costs $0 (Sponsor) Psst: there's a new kid on the block â€” and it launched today. It consistently benchmarks higher than Anthropic's Claude Code and OpenAI's Codex. And instead of $200/month for premium usage, you pay absolutely nothing. Use on GPT-5, Sonnet 4.5 and more, no API keys required. Start now

</details>


### [19] [â¤ï¸ "I love paying Anthropic $200/mo"](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/4/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/8xT6Dx5m_5OEdpPwGZREmi7X1VTe_r7xZeTmQ2ixBF8=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ä»‹ç»äº†ä¸€ä¸ªå…è´¹çš„AIä»£ç åŠ©æ‰‹ï¼Œå£°ç§°åœ¨åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºClaude Codeä¸”æ— éœ€ä»˜è´¹


<details>
  <summary>Details</summary>
Motivation: ä¸ºç”¨æˆ·æä¾›æ¯”ä»˜è´¹AIä»£ç åŠ©æ‰‹ï¼ˆå¦‚Claude Codeï¼‰æ›´ä¼˜ä¸”å…è´¹çš„æ›¿ä»£æ–¹æ¡ˆ

Method: å¼€å‘äº†ä¸€ä¸ªæ–°çš„AIä»£ç ä»£ç†å·¥å…·ï¼Œé€šè¿‡åŸºå‡†æµ‹è¯•éªŒè¯å…¶æ€§èƒ½

Result: è¯¥å·¥å…·åœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†Claude Codeï¼Œå¹¶ä¸”å®Œå…¨å…è´¹ä½¿ç”¨

Conclusion: cto.newæ˜¯ä¸€ä¸ªæ€§èƒ½ä¼˜è¶Šä¸”å…è´¹çš„AIä»£ç åŠ©æ‰‹é€‰æ‹©

Abstract: â¤ï¸ "I love paying Anthropic $200/mo" (Sponsor) If you love paying, follow your heart. Otherwise, you might want to try the new AI code agent that launched today â€” it beats Claude Code on benchmarks and costs $0. Experience cto.new - it's free

</details>


### [20] [Just Talk To It - the no-bs Way of Agentic Engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2Fjust-talk-to-it%3Futm_source=tldrnewsletter/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/0ZXg2APLQBSH7sMGD8Fd4VBv2odtqgJg847WfFM60LE=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: é€šè¿‡ä¸æ™ºèƒ½ä½“ç›´æ¥å¯¹è¯æ¥æ”¹è¿›å…¶æ€§èƒ½ï¼Œæ— éœ€å¤æ‚å·¥ç¨‹


<details>
  <summary>Details</summary>
Motivation: ç®€åŒ–æ™ºèƒ½ä½“å¼€å‘æµç¨‹ï¼Œé¿å…è¿‡åº¦å·¥ç¨‹åŒ–ï¼Œé€šè¿‡è‡ªç„¶å¯¹è¯æå‡æ™ºèƒ½ä½“è¡¨ç°

Method: é‡‡ç”¨ç›´æ¥å¯¹è¯çš„æ–¹å¼ä¸æ™ºèƒ½ä½“äº¤äº’ï¼Œé€šè¿‡æŒç»­å¯¹è¯ä¼˜åŒ–æ™ºèƒ½ä½“è¡Œä¸º

Result: éšç€å¯¹è¯æ¬¡æ•°å¢åŠ ï¼Œæ™ºèƒ½ä½“è¡¨ç°é€æ¸æ”¹å–„ï¼Œç”¨æˆ·ä¸æ™ºèƒ½ä½“çš„åä½œæ•ˆæœæ›´å¥½

Conclusion: ç›´æ¥å¯¹è¯æ˜¯æå‡æ™ºèƒ½ä½“æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå·¥ç¨‹å¤æ‚åº¦è¶Šä½æ•ˆæœè¶Šå¥½

Abstract: Just Talk To It - the no-bs Way of Agentic Engineering (23 minute read) Just talk to your agents - the more you work with them, the better your results will be.

</details>


### [21] [How we test a web framework](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwasp.sh%2Fblog%2F2025%2F10%2F07%2Fhow-we-test-a-web-framework%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/nY7ek6OWjUEZJE1ww29cU8pzcOXtY3Tbn2xo1lyfIV4=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Waspæµ‹è¯•å…¨æ ˆWebæ¡†æ¶çš„ç­–ç•¥ï¼šä¼˜å…ˆé€‰æ‹©èƒ½æ¸…æ™°è¡¨è¾¾è¾“å…¥å’Œé¢„æœŸè¾“å‡ºçš„æµ‹è¯•ï¼Œé‡å‹‡æ°”è€Œé100%è¦†ç›–ç‡ï¼Œé‡åº¦ä¾èµ–ç±»å‹ç³»ç»Ÿï¼Œä½¿ç”¨å¿«ç…§æµ‹è¯•è¿½è¸ªä»£ç ç”Ÿæˆå˜åŒ–ï¼ŒPlaywrightéªŒè¯åº”ç”¨ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¸ºå…¨æ ˆWebæ¡†æ¶å»ºç«‹æœ‰æ•ˆçš„æµ‹è¯•ç­–ç•¥ï¼Œåœ¨æµ‹è¯•è¦†ç›–ç‡å’Œæµ‹è¯•è´¨é‡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œç¡®ä¿æ¡†æ¶çš„å¯é æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

Method: é‡‡ç”¨åŸºäºç±»å‹çš„æµ‹è¯•æ–¹æ³•ï¼Œä½¿ç”¨å¿«ç…§æµ‹è¯•æ¥è¿½è¸ªä»£ç ç”Ÿæˆçš„å˜åŒ–ï¼Œé€šè¿‡Playwrightè¿›è¡Œç«¯åˆ°ç«¯åº”ç”¨éªŒè¯ï¼Œå¼ºè°ƒæµ‹è¯•çš„æ¸…æ™°è¡¨è¾¾è€Œéè¿½æ±‚100%è¦†ç›–ç‡ã€‚

Result: å»ºç«‹äº†ä¸€å¥—æœ‰æ•ˆçš„æµ‹è¯•ä½“ç³»ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯è´¨é‡çš„åŒæ—¶æé«˜å¼€å‘æ•ˆç‡ï¼Œé€šè¿‡å¿«ç…§æµ‹è¯•æœ‰æ•ˆç®¡ç†ä»£ç ç”Ÿæˆçš„å˜åŒ–ã€‚

Conclusion: å¯¹äºå…¨æ ˆWebæ¡†æ¶ï¼Œæ¸…æ™°çš„æµ‹è¯•è¡¨è¾¾å’Œç±»å‹å®‰å…¨æ¯”100%æµ‹è¯•è¦†ç›–ç‡æ›´é‡è¦ï¼Œå¿«ç…§æµ‹è¯•æ˜¯ç®¡ç†ä»£ç ç”Ÿæˆå˜åŒ–çš„æœ‰æ•ˆå·¥å…·ã€‚

Abstract: How we test a web framework (15 minute read) Wasp's testing strategy for its full-stack web framework prioritizes tests that clearly express input and expected output, favoring courage over 100% coverage. It also relies on types heavily. Wasp uses snapshot testing to track code generation changes and Playwright to validate apps.

</details>


### [22] [Why your boss isn't worried about AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboydkane.com%2Fessays%2Fboss%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/tWghou8WeBrgeKJzVurXxvi2DBMuSCJddpz46ICvFI8=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: å…¬ä¼—å¯¹è½¯ä»¶æ¼æ´çš„ç†è§£é€‚ç”¨äºå¸¸è§„è½¯ä»¶ï¼Œä½†åœ¨AIç³»ç»Ÿä¸Šå…·æœ‰è¯¯å¯¼æ€§ã€‚AIæ¼æ´æºäºå¤§å‹è®­ç»ƒæ•°æ®é›†ï¼Œéš¾ä»¥ç²¾ç¡®å®šä½æˆ–å®Œå…¨æ¶ˆé™¤ï¼Œä¸”ä¿®å¤åå¯èƒ½é‡ç°ã€‚


<details>
  <summary>Details</summary>
Motivation: æ¾„æ¸…AIç³»ç»Ÿä¸å¸¸è§„è½¯ä»¶åœ¨æ¼æ´ç‰¹æ€§ä¸Šçš„æ ¹æœ¬å·®å¼‚ï¼Œè§£é‡Šä¸ºä½•AIæ¼æ´æ›´éš¾ä»¥ç®¡ç†å’Œä¿®å¤ã€‚

Method: é€šè¿‡å¯¹æ¯”åˆ†æå¸¸è§„è½¯ä»¶æ¼æ´ä¸AIç³»ç»Ÿæ¼æ´çš„èµ·æºã€å®šä½éš¾åº¦å’Œä¿®å¤ç‰¹æ€§ã€‚

Result: AIæ¼æ´å…·æœ‰ç³»ç»Ÿæ€§ã€éš¾ä»¥è¿½æº¯å’Œå¯èƒ½é‡ç°çš„ç‰¹ç‚¹ï¼Œä¸å¸¸è§„è½¯ä»¶æ¼æ´çš„å±€éƒ¨æ€§å’Œå¯å®Œå…¨ä¿®å¤æ€§å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

Conclusion: éœ€è¦é‡æ–°æ€è€ƒAIç³»ç»Ÿçš„æ¼æ´ç®¡ç†æ–¹æ³•ï¼Œä¸èƒ½ç®€å•å¥—ç”¨ä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹çš„æœ€ä½³å®è·µã€‚

Abstract: Why your boss isn't worried about AI (11 minute read) The public's understanding of software bugs, while helpful for regular software, is misleading when applied to AI systems. Unlike regular software, where bugs are caused by mistakes in code and can be precisely located and fixed, AI vulnerabilities come from large training datasets, making the origin of errors difficult to pinpoint or eliminate entirely. Once a bug is fixed in regular software, it won't reappear again, but this isn't the c...

</details>


### [23] [Sentry's AI Code Review predicts what's going to break - based on what's already broken](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/GQ-Ztf3RzSPe3RaYN5fsZXpVo8sGURnpSgiCXk4J3FM=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentryçš„AIä»£ç å®¡æŸ¥å·¥å…·é€šè¿‡åˆ†æä»£ç å˜æ›´ã€ä¾èµ–å…³ç³»å’Œå†å²é—®é¢˜æ¥é¢„æµ‹å¯èƒ½å¯¼è‡´ç”Ÿäº§ç¯å¢ƒæ•…éšœçš„é—®é¢˜ï¼Œæä¾›å…·ä½“å¯æ“ä½œçš„å»ºè®®è€Œéé€šç”¨ä»£ç é£æ ¼æ£€æŸ¥


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿä»£ç å®¡æŸ¥è¿‡äºå…³æ³¨ä»£ç é£æ ¼ç»†èŠ‚ï¼Œè€Œç¼ºä¹å¯¹å¯èƒ½å¼•å‘ç”Ÿäº§ç¯å¢ƒæ•…éšœçš„å®é™…é—®é¢˜çš„é¢„æµ‹èƒ½åŠ›ï¼Œéœ€è¦æ›´æ™ºèƒ½çš„å®¡æŸ¥å·¥å…·

Method: ç»“åˆä»£ç å˜æ›´ä¸Šä¸‹æ–‡å’Œé—®é¢˜å†å²è®°å½•ï¼Œåˆ†æå‡½æ•°è°ƒç”¨ã€ç±»æˆ–å¯¹è±¡ä¾èµ–å…³ç³»ã€æ•°æ®åº“è¿æ¥ç­‰ï¼Œæä¾›å…·ä½“å¯æ“ä½œçš„åé¦ˆ

Result: å¼€å‘å‡ºèƒ½å¤Ÿé¢„æµ‹ç”Ÿäº§ç¯å¢ƒæ•…éšœçš„AIä»£ç å®¡æŸ¥å·¥å…·ï¼Œå‡å°‘é€šç”¨lintingå»ºè®®ï¼Œæä¾›æ›´ç²¾å‡†çš„é—®é¢˜é¢„è­¦

Conclusion: AIé©±åŠ¨çš„ä»£ç å®¡æŸ¥å¯ä»¥æ˜¾è‘—æå‡ä»£ç è´¨é‡ï¼Œé€šè¿‡å†å²æ•°æ®å’Œä¸Šä¸‹æ–‡åˆ†ææ¥é¢„æµ‹å’Œé¢„é˜²ç”Ÿäº§ç¯å¢ƒæ•…éšœ

Abstract: Sentry's AI Code Review predicts what's going to break - based on what's already broken (Sponsor) Code reviews should be less style nits and more "this is going to break prod". Sentry's AI Code Review blends context and issue history with the code you just touched - function calls, class or objects dependencies, database connections - to provide specific and actionable feedback rather than generic linting advice. Read the blog

</details>


### [24] [Generate a Page from a Prompt and Edit Visually](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.experimently.io%2F%3Futm_source=tldrdesign/1/01000199e7d00108-31ae7426-7b3f-4d30-a485-b7d8a37eb849-000000/a8DJ5nieo-oQKZ_p3PSjdO9QVa18ZH5-sU-K8lSc_7Q=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ä»æç¤ºç”Ÿæˆç½‘é¡µï¼Œæ— éœ€ä»£ç å³å¯å¯è§†åŒ–ç¼–è¾‘ï¼Œå‘å¸ƒåˆ°è‡ªå®šä¹‰åŸŸåï¼Œå¹¶é€šè¿‡å¯é…ç½®çš„åŠ æƒè·¯ç”±è¿›è¡ŒA/Bæµ‹è¯•å¤šé¡µé¢å˜ä½“


<details>
  <summary>Details</summary>
Motivation: é™ä½ç½‘é¡µåˆ›å»ºå’Œç¼–è¾‘çš„æŠ€æœ¯é—¨æ§›ï¼Œè®©éæŠ€æœ¯äººå‘˜ä¹Ÿèƒ½è½»æ¾åˆ›å»ºå’Œä¼˜åŒ–ç½‘é¡µï¼ŒåŒæ—¶æä¾›ä¸“ä¸šçš„A/Bæµ‹è¯•åŠŸèƒ½

Method: åŸºäºæç¤ºç”Ÿæˆç½‘é¡µå†…å®¹ï¼Œæä¾›å¯è§†åŒ–ç¼–è¾‘ç•Œé¢ï¼Œæ”¯æŒè‡ªå®šä¹‰åŸŸåå‘å¸ƒï¼Œå®ç°åŠ æƒè·¯ç”±çš„A/Bæµ‹è¯•æœºåˆ¶

Result: å¼€å‘äº†ä¸€ä¸ªæ— éœ€ç¼–ç çš„ç½‘é¡µç”Ÿæˆå’Œç¼–è¾‘å¹³å°ï¼Œæ”¯æŒå¯è§†åŒ–æ“ä½œå’Œä¸“ä¸šçš„A/Bæµ‹è¯•åŠŸèƒ½

Conclusion: è¯¥ç³»ç»ŸæˆåŠŸé™ä½äº†ç½‘é¡µåˆ›å»ºçš„æŠ€æœ¯é—¨æ§›ï¼Œä¸ºéæŠ€æœ¯äººå‘˜æä¾›äº†ä¸“ä¸šçš„ç½‘é¡µä¼˜åŒ–å·¥å…·

Abstract: Generate a Page from a Prompt and Edit Visually (Website) Generate a page from a prompt, edit visually (no code needed), publish to your domain, and A/B test multiple page variations with configurable weighted routing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning](https://arxiv.org/abs/2510.12939)
*James Pedley,Benjamin Etheridge,Stephen J. Roberts,Francesco Quinzan*

Main category: cs.LG

TL;DR: è¯¥è®ºæ–‡é¦–æ¬¡ä¸ºçŠ¶æ€å¯¹æŠ—é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(SA-MDPs)ä¸­çš„å‰ªæè®¤è¯é²æ£’æ€§å»ºç«‹äº†ç†è®ºæ¡†æ¶ï¼Œè¯æ˜äº†å…ƒç´ çº§å‰ªæåªä¼šæ”¶ç´§è®¤è¯é²æ£’æ€§è¾¹ç•Œï¼Œå¹¶æ­ç¤ºäº†æ€§èƒ½-é²æ£’æ€§æƒè¡¡è¾¹ç•Œã€‚


<details>
  <summary>Details</summary>
Motivation: ç°å®ä¸–ç•Œä¸­éƒ¨ç½²çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥éœ€è¦åœ¨å¯¹æŠ—æ‰°åŠ¨ä¸‹ä¿æŒå¯é æ€§ï¼Œè€Œç°ä»£æ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„è¿‡åº¦å‚æ•°åŒ–å¸¦æ¥äº†æˆæœ¬å’Œè„†å¼±æ€§é—®é¢˜ã€‚è™½ç„¶å‰ªæåœ¨ç›‘ç£å­¦ä¹ ä¸­å·²è¢«è¯æ˜èƒ½æé«˜é²æ£’æ€§ï¼Œä½†å…¶åœ¨å¯¹æŠ—å¼ºåŒ–å­¦ä¹ ä¸­çš„ä½œç”¨å°šä¸æ¸…æ¥šã€‚

Method: ä¸ºé«˜æ–¯å’Œåˆ†ç±»ç­–ç•¥å¼€å‘ç†è®ºæ¡†æ¶ï¼Œè¯æ˜å…ƒç´ çº§å‰ªæå¯¹è®¤è¯é²æ£’æ€§çš„å½±å“ï¼›æ¨å¯¼æ–°é¢–çš„ä¸‰é¡¹é—æ†¾åˆ†è§£ï¼›åœ¨è¿ç»­æ§åˆ¶åŸºå‡†ä¸Šè¯„ä¼°å¹…åº¦å‰ªæå’Œå¾®å‰ªæè°ƒåº¦ã€‚

Result: å‰ªæåœ¨ä¸­ç­‰ç¨€ç–åº¦æ°´å¹³ä¸Šå§‹ç»ˆå‘ç°å¯å¤ç°çš„"æœ€ä½³ç‚¹"ï¼Œåœ¨è¿™äº›ç‚¹ä¸Šé²æ£’æ€§æ˜¾è‘—æ”¹å–„ï¼Œè€Œä¸ä¼šæŸå®³ç”šè‡³æœ‰æ—¶èƒ½æå‡å¹²å‡€æ€§èƒ½ã€‚

Conclusion: å‰ªæä¸ä»…æ˜¯å‹ç¼©å·¥å…·ï¼Œæ›´æ˜¯å®ç°é²æ£’å¼ºåŒ–å­¦ä¹ çš„ç»“æ„æ€§å¹²é¢„æ‰‹æ®µã€‚

Abstract: Reinforcement learning (RL) policies deployed in real-world environments must
remain reliable under adversarial perturbations. At the same time, modern deep
RL agents are heavily over-parameterized, raising costs and fragility concerns.
While pruning has been shown to improve robustness in supervised learning, its
role in adversarial RL remains poorly understood. We develop the first
theoretical framework for certified robustness under pruning in
state-adversarial Markov decision processes (SA-MDPs). For Gaussian and
categorical policies with Lipschitz networks, we prove that element-wise
pruning can only tighten certified robustness bounds; pruning never makes the
policy less robust. Building on this, we derive a novel three-term regret
decomposition that disentangles clean-task performance, pruning-induced
performance loss, and robustness gains, exposing a fundamental
performance--robustness frontier. Empirically, we evaluate magnitude and
micro-pruning schedules on continuous-control benchmarks with strong
policy-aware adversaries. Across tasks, pruning consistently uncovers
reproducible ``sweet spots'' at moderate sparsity levels, where robustness
improves substantially without harming - and sometimes even enhancing - clean
performance. These results position pruning not merely as a compression tool
but as a structural intervention for robust RL.

</details>


### [26] [Towards Understanding Valuable Preference Data for Large Language Model Alignment](https://arxiv.org/abs/2510.13212)
*Zizhuo Zhang,Qizhou Wang,Shanshan Ye,Jianing Zhu,Jiangchao Yao,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æˆªæ–­å½±å“å‡½æ•°(TIF)æ¥è¯„ä¼°åå¥½æ•°æ®è´¨é‡ï¼Œå‘ç°æ•°æ®è´¨é‡æ˜¯æ¨¡å‹ç›¸å…³çš„ï¼Œå¹¶æå‡ºä¸¤ç§æ›´ç®€å•çš„è¯„åˆ†å‡½æ•°æ¥æ”¹è¿›åå¥½æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ç ”ç©¶é€šå¸¸ä½¿ç”¨å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–ç°æˆLLMé¢„å¤„ç†åŸå§‹è®­ç»ƒæ•°æ®é›†æ¥è¯†åˆ«æœ‰ä»·å€¼çš„åå¥½å¯¹ï¼Œä½†å¾ˆå°‘æ£€æŸ¥å•ä¸ªæ•°æ®ç‚¹æ˜¯å¦çœŸæ­£æœ‰ç›Šã€‚éœ€è¦æ”¹è¿›åå¥½æ•°æ®é€‰æ‹©æ–¹æ³•ä»¥é€‚åº”ç‰¹å®šæ¨¡å‹ã€‚

Method: æå‡ºæˆªæ–­å½±å“å‡½æ•°(TIF)è¯„ä¼°æ•°æ®è´¨é‡ï¼Œå¼•å…¥ä¸¤ç§è®¡ç®—æ›´ç®€å•çš„è¯„åˆ†å‡½æ•°(SF)ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆä»¥æŠµæ¶ˆä¸åŒçš„è¯¯å·®æºï¼Œå½¢æˆç®€å•æœ‰æ•ˆçš„æ•°æ®é€‰æ‹©è§„åˆ™ã€‚

Result: åœ¨å¤šæ ·åŒ–å¯¹é½åŸºå‡†å’Œå„ç§LLMå®¶æ—ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æ›´å°‘çš„æ•°æ®å¯ä»¥å®ç°æ›´å¥½çš„å¯¹é½æ€§èƒ½ï¼Œè¯æ˜äº†æ–¹æ³•çš„é€šç”¨æ€§ã€‚

Conclusion: åå¥½æ•°æ®è´¨é‡æ˜¯æ¨¡å‹ç›¸å…³çš„ï¼Œé€šè¿‡æ¨¡å‹ä¾èµ–çš„æ•°æ®é€‰æ‹©æ–¹æ³•å¯ä»¥æ›´ç²¾ç¡®åœ°é€‰æ‹©æœ‰ä»·å€¼çš„åå¥½æ•°æ®ï¼Œæé«˜å¯¹é½æ•ˆç‡ã€‚

Abstract: Large language model (LLM) alignment is typically achieved through learning
from human preference comparisons, making the quality of preference data
critical to its success. Existing studies often pre-process raw training
datasets to identify valuable preference pairs using external reward models or
off-the-shelf LLMs, achieving improved overall performance but rarely examining
whether individual, selected data point is genuinely beneficial. We assess data
quality through individual influence on validation data using our newly
proposed truncated influence function (TIF), which mitigates the over-scoring
present in traditional measures and reveals that preference data quality is
inherently a property of the model. In other words, a data pair that benefits
one model may harm another. This leaves the need to improve the preference data
selection approaches to be adapting to specific models. To this end, we
introduce two candidate scoring functions (SFs) that are computationally
simpler than TIF and positively correlated with it. They are also model
dependent and can serve as potential indicators of individual data quality for
preference data selection. Furthermore, we observe that these SFs inherently
exhibit errors when compared to TIF. To this end, we combine them to offset
their diverse error sources, resulting in a simple yet effective data selection
rule that enables the models to achieve a more precise selection of valuable
preference data. We conduct experiments across diverse alignment benchmarks and
various LLM families, with results demonstrating that better alignment
performance can be achieved using less data, showing the generality of our
findings and new methods.

</details>


### [27] [When In Doubt, Abstain: The Impact of Abstention on Strategic Classification](https://arxiv.org/abs/2510.13327)
*Lina Alkarmi,Ziyuan Huang,Mingyan Liu*

Main category: cs.LG

TL;DR: æœ¬æ–‡ç ”ç©¶åœ¨æˆ˜ç•¥åˆ†ç±»ä¸­å¼•å…¥å¼ƒæƒæœºåˆ¶å¦‚ä½•å½±å“æˆ˜ç•¥ä»£ç†çš„å“åº”ï¼Œä»¥åŠä¸»ä½“åº”å¦‚ä½•æœ€ä¼˜åˆ©ç”¨å¼ƒæƒã€‚ç ”ç©¶è¡¨æ˜æœ€ä¼˜å¼ƒæƒèƒ½ç¡®ä¿ä¸»ä½“æ•ˆç”¨ä¸åŠ£äºéå¼ƒæƒè®¾ç½®ï¼Œä¸”å¼ƒæƒå¯ä½œä¸ºæ“çºµå¨æ…‘ï¼Œä½¿ä¸åˆæ ¼ä»£ç†æ“çºµæˆæœ¬æ›´é«˜ã€‚


<details>
  <summary>Details</summary>
Motivation: ç®—æ³•å†³ç­–æ—¥ç›Šæ™®åŠä½†æ˜“å—æˆ˜ç•¥æ“çºµï¼Œå…ˆå‰ç ”ç©¶è¡¨æ˜åˆ†ç±»å™¨å¼ƒæƒèƒ½æ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢åœ¨æˆ˜ç•¥åˆ†ç±»èƒŒæ™¯ä¸‹å¼•å…¥å¼ƒæƒæœºåˆ¶çš„å½±å“ã€‚

Method: ä½¿ç”¨Stackelbergåšå¼ˆæ¨¡å‹ï¼Œä¸»ä½“ä½œä¸ºåˆ†ç±»å™¨é¦–å…ˆå®£å¸ƒå†³ç­–ç­–ç•¥ï¼Œæˆ˜ç•¥ä»£ç†éšåæ“çºµç‰¹å¾ä»¥è·å¾—æœŸæœ›ç»“æœã€‚èšç„¦äºäºŒè¿›åˆ¶åˆ†ç±»å™¨ï¼Œä»£ç†æ“çºµå¯è§‚å¯Ÿç‰¹å¾è€ŒéçœŸå®ç‰¹å¾ã€‚

Result: æœ€ä¼˜å¼ƒæƒç¡®ä¿ä¸»ä½“æ•ˆç”¨ä¸åŠ£äºéå¼ƒæƒè®¾ç½®ï¼›å¼ƒæƒä¸ä»…èƒ½æé«˜å‡†ç¡®æ€§ï¼Œè¿˜èƒ½ä½œä¸ºæ“çºµå¨æ…‘ï¼Œåœ¨æ“çºµæˆæœ¬è¶³å¤Ÿå¤§æ—¶ä½¿ä¸åˆæ ¼ä»£ç†æ“çºµæˆæœ¬æ›´é«˜ã€‚

Conclusion: å¼ƒæƒæ˜¯å‡å°‘ç®—æ³•å†³ç­–ç³»ç»Ÿä¸­æˆ˜ç•¥è¡Œä¸ºè´Ÿé¢å½±å“çš„å®è´µå·¥å…·ã€‚

Abstract: Algorithmic decision making is increasingly prevalent, but often vulnerable
to strategic manipulation by agents seeking a favorable outcome. Prior research
has shown that classifier abstention (allowing a classifier to decline making a
decision due to insufficient confidence) can significantly increase classifier
accuracy. This paper studies abstention within a strategic classification
context, exploring how its introduction impacts strategic agents' responses and
how principals should optimally leverage it. We model this interaction as a
Stackelberg game where a principal, acting as the classifier, first announces
its decision policy, and then strategic agents, acting as followers, manipulate
their features to receive a desired outcome. Here, we focus on binary
classifiers where agents manipulate observable features rather than their true
features, and show that optimal abstention ensures that the principal's utility
(or loss) is no worse than in a non-abstention setting, even in the presence of
strategic agents. We also show that beyond improving accuracy, abstention can
also serve as a deterrent to manipulation, making it costlier for agents,
especially those less qualified, to manipulate to achieve a positive outcome
when manipulation costs are significant enough to affect agent behavior. These
results highlight abstention as a valuable tool for reducing the negative
effects of strategic behavior in algorithmic decision making systems.

</details>


### [28] [A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control](https://arxiv.org/abs/2510.13367)
*Nikita Kachaev,Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovelev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: æœ¬æ–‡æ¢è®¨äº†åœ¨åœ¨çº¿æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­åº”ç”¨Transformeræ¶æ„çš„å…³é”®è®¾è®¡é—®é¢˜ï¼ŒåŒ…æ‹¬è¾“å…¥æ¡ä»¶ã€ç»„ä»¶å…±äº«å’Œåºåˆ—æ•°æ®å¤„ç†ï¼Œæå‡ºäº†ç¨³å®šçš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚


<details>
  <summary>Details</summary>
Motivation: å°½ç®¡Transformeråœ¨ç¦»çº¿æˆ–åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åœ¨çº¿æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­ä»æœªå……åˆ†æ¢ç´¢ï¼Œä¸»è¦å› ä¸ºå…¶å¯¹è®­ç»ƒè®¾ç½®å’Œæ¨¡å‹è®¾è®¡å†³ç­–çš„æ•æ„Ÿæ€§ã€‚

Method: ç ”ç©¶äº†Transformeråœ¨åœ¨çº¿æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„å…³é”®è®¾è®¡é—®é¢˜ï¼šè¾“å…¥æ¡ä»¶è®¾ç½®ã€actorå’Œcriticç»„ä»¶å…±äº«ç­–ç•¥ã€åºåˆ—æ•°æ®åˆ‡ç‰‡è®­ç»ƒæ–¹æ³•ã€‚

Result: å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„ç¨³å®šæ¶æ„å’Œè®­ç»ƒç­–ç•¥åœ¨å®Œå…¨å’Œéƒ¨åˆ†å¯è§‚æµ‹ä»»åŠ¡ä¸­éƒ½èƒ½å®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œé€‚ç”¨äºå‘é‡å’Œå›¾åƒä¸¤ç§è®¾ç½®ã€‚

Conclusion: è¿™äº›å‘ç°ä¸ºåœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­åº”ç”¨Transformeræä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

Abstract: Despite their effectiveness and popularity in offline or model-based
reinforcement learning (RL), transformers remain underexplored in online
model-free RL due to their sensitivity to training setups and model design
decisions such as how to structure the policy and value networks, share
components, or handle temporal information. In this paper, we show that
transformers can be strong baselines for continuous control in online
model-free RL. We investigate key design questions: how to condition inputs,
share components between actor and critic, and slice sequential data for
training. Our experiments reveal stable architectural and training strategies
enabling competitive performance across fully and partially observable tasks,
and in both vector- and image-based settings. These findings offer practical
guidance for applying transformers in online RL.

</details>


### [29] [Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks](https://arxiv.org/abs/2510.13391)
*Benjamin Kempinski,Tal Kachman*

Main category: cs.LG

TL;DR: æå‡ºä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œçš„æ–¹æ³•æ¥è¿‘ä¼¼è®¡ç®—ç½‘ç»œæµåšå¼ˆä¸­çš„Banzhafå€¼ï¼Œç›¸æ¯”ä¼ ç»Ÿç²¾ç¡®è®¡ç®—å’Œè’™ç‰¹å¡æ´›é‡‡æ ·æ–¹æ³•ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°äº†æ•°é‡çº§çš„é€Ÿåº¦æå‡ï¼Œå¹¶å±•ç¤ºäº†ä¼˜ç§€çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»ŸBanzhafå€¼è®¡ç®—æ–¹æ³•åœ¨è¶…è¿‡çº¦20ä¸ªä»£ç†æ—¶å› æŒ‡æ•°å¤æ‚åº¦å˜å¾—ä¸å¯è¡Œï¼Œè€Œè’™ç‰¹å¡æ´›é‡‡æ ·æ–¹æ³•æ ·æœ¬å¤æ‚åº¦é«˜ä¸”æ— æ³•åœ¨ä¸åŒç½‘ç»œé…ç½®é—´è¿ç§»çŸ¥è¯†ï¼Œéš¾ä»¥åº”ç”¨äºå¤§è§„æ¨¡æˆ–åŠ¨æ€ç³»ç»Ÿã€‚

Method: å°†é—®é¢˜å»ºæ¨¡ä¸ºå›¾çº§é¢„æµ‹ä»»åŠ¡ï¼Œä½¿ç”¨å›¾ç¥ç»ç½‘ç»œç›´æ¥ä»ç½‘ç»œæ‹“æ‰‘å’Œæ§åˆ¶ç»“æ„ä¸­å­¦ä¹ ä»£ç†å½±å“åŠ›çš„é€šç”¨æ¨¡å¼ï¼Œæ¯”è¾ƒäº†GATã€GINEå’ŒEdgeConvä¸‰ç§æœ€å…ˆè¿›çš„GNNæ¶æ„ã€‚

Result: åœ¨åŒ…å«200,000ä¸ªå›¾çš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œè®­ç»ƒåçš„GNNæ¨¡å‹å®ç°äº†é«˜ä¿çœŸåº¦çš„Banzhafå€¼è¿‘ä¼¼ï¼Œç›¸æ¯”ç²¾ç¡®å’ŒåŸºäºé‡‡æ ·çš„æ–¹æ³•è·å¾—äº†æ•°é‡çº§çš„é€Ÿåº¦æå‡ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

Conclusion: è¿™é¡¹å·¥ä½œç¡®ç«‹äº†GNNä½œä¸ºå¤æ‚ç½‘ç»œç³»ç»Ÿå¯æ‰©å±•åˆä½œåšå¼ˆè®ºåˆ†æçš„å®ç”¨å·¥å…·ã€‚

Abstract: Computing the Banzhaf value in network flow games is fundamental for
quantifying agent influence in multi-agent systems, with applications ranging
from cybersecurity to infrastructure planning. However, exact computation is
intractable for systems with more than $\sim20$ agents due to exponential
complexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide
statistical estimates, they suffer from high sample complexity and cannot
transfer knowledge across different network configurations, making them
impractical for large-scale or dynamic systems. We present a novel
learning-based approach using Graph Neural Networks (GNNs) to approximate
Banzhaf values in cardinal network flow games. By framing the problem as a
graph-level prediction task, our method learns generalisable patterns of agent
influence directly from network topology and control structure. We conduct a
comprehensive empirical study comparing three state-of-the-art GNN
architectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with
Edge features (GINE), and EdgeConv-on a large-scale synthetic dataset of
200,000 graphs per configuration, varying in size (20-100 nodes), agent count
(5-20), and edge probability (0.5-1.0). Our results demonstrate that trained
GNN models achieve high-fidelity Banzhaf value approximation with
order-of-magnitude speedups compared to exact and sampling-based methods. Most
significantly, we show strong zero-shot generalisation: models trained on
graphs of a specific size and topology accurately predict Banzhaf values for
entirely new networks with different structural properties, without requiring
retraining. This work establishes GNNs as a practical tool for scalable
cooperative game-theoretic analysis of complex networked systems.

</details>


### [30] [Selective Adversarial Attacks on LLM Benchmarks](https://arxiv.org/abs/2510.13570)
*Ivan Dubrovsky,Anastasia Orlova,Illarion Iov,Nina Gubina,Irena Gureeva,Alexey Zaytsev*

Main category: cs.LG

TL;DR: è¯¥è®ºæ–‡ç ”ç©¶äº†é€‰æ‹©æ€§å¯¹æŠ—æ”»å‡»å¯¹LLMåŸºå‡†æµ‹è¯•çš„å½±å“ï¼Œå‘ç°å³ä½¿å¾®å°çš„æ–‡æœ¬æ‰°åŠ¨ä¹Ÿèƒ½æ˜¾è‘—æ”¹å˜æ¨¡å‹åœ¨MMLUåŸºå‡†ä¸Šçš„ç›¸å¯¹æ’åï¼ŒæŒ‘æˆ˜äº†æ’è¡Œæ¦œè¯„ä¼°çš„å…¬å¹³æ€§å’Œå¯å¤ç°æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰LLMåŸºå‡†æµ‹è¯•å®¹æ˜“å—åˆ°è¯­ä¹‰ç­‰æ•ˆçš„å¯¹æŠ—æ€§æ‰°åŠ¨å½±å“ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å¯¹æ‰€æœ‰æ¨¡å‹äº§ç”ŸåŒç­‰å½±å“çš„æ”»å‡»ï¼Œç¼ºä¹å¯¹é€‰æ‹©æ€§æ”»å‡»ï¼ˆå³åªé’ˆå¯¹ç‰¹å®šæ¨¡å‹æ€§èƒ½ï¼‰çš„ç ”ç©¶ã€‚

Method: ä½¿ç”¨TextAttackæ¡†æ¶ä¸­çš„æ ‡å‡†æ”»å‡»æ–¹æ³•ï¼Œå¼€å‘äº†é€‰æ‹©æ€§è¯„ä¼°åè®®å’Œè‡ªå®šä¹‰çº¦æŸï¼Œæå‡ºäº†åŸºäºä»£ç†LLMçš„ç®¡é“æ¥ç”Ÿæˆé€‰æ‹©æ€§æ‰°åŠ¨ã€‚

Result: å®è¯å‘ç°é€‰æ‹©æ€§å¯¹æŠ—æ”»å‡»ç¡®å®å­˜åœ¨ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å˜æ¨¡å‹åœ¨MMLUåŸºå‡†ä¸Šçš„ç›¸å¯¹æ’åï¼Œå½±å“è¯„ä¼°çš„å…¬å¹³æ€§ã€‚

Conclusion: ç ”ç©¶ç»“æœè¡¨æ˜éœ€è¦é‡‡ç”¨æ‰°åŠ¨æ„ŸçŸ¥çš„æŠ¥å‘Šå’Œé²æ£’æ€§è¯Šæ–­æ–¹æ³•æ¥è¿›è¡ŒLLMè¯„ä¼°ï¼Œå³ä½¿æ˜¯å¾®å°çš„ç¼–è¾‘ä¹Ÿå¯èƒ½æ”¹å˜æ¯”è¾ƒåˆ¤æ–­ã€‚

Abstract: Benchmarking outcomes increasingly govern trust, selection, and deployment of
LLMs, yet these evaluations remain vulnerable to semantically equivalent
adversarial perturbations. Prior work on adversarial robustness in NLP has
emphasized text attacks that affect many models equally, leaving open the
question of whether it is possible to selectively degrade or enhance
performance while minimally affecting other models. We formalize this problem
and study selective adversarial attacks on MMLU - a widely used benchmark
designed to measure a language model's broad general knowledge and reasoning
ability across different subjects. Using canonical attacks integrated into
TextAttack framework, we introduce a protocol for selectivity assessment,
develop a custom constraint to increase selectivity of attacks and propose a
surrogate-LLM pipeline that generates selective perturbations. Empirically, we
find that selective adversarial attacks exist and can materially alter relative
rankings, challenging the fairness, reproducibility, and transparency of
leaderboard-driven evaluation. Our results motivate perturbation-aware
reporting and robustness diagnostics for LLM evaluation and demonstrate that
even subtle edits can shift comparative judgments.

</details>


### [31] [Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents](https://arxiv.org/abs/2510.13704)
*Johan Obando-Ceron,Walter Mayor,Samuel Lavoie,Scott Fujimoto,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: è®ºæ–‡æå‡ºä½¿ç”¨å•çº¯å½¢åµŒå…¥ä½œä¸ºè½»é‡çº§è¡¨ç¤ºå±‚ï¼Œé€šè¿‡å‡ ä½•å½’çº³åç½®äº§ç”Ÿç¨€ç–ç¦»æ•£ç‰¹å¾ï¼Œæå‡å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ï¼Œä¸”ä¸å½±å“è¿è¡Œé€Ÿåº¦ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰çš„å¤§è§„æ¨¡ç¯å¢ƒå¹¶è¡ŒåŒ–æ–¹æ³•è™½ç„¶èƒ½åŠ é€Ÿè®­ç»ƒï¼Œä½†ä»éœ€å¤§é‡ç¯å¢ƒäº¤äº’ã€‚ç»“æ„è‰¯å¥½çš„è¡¨ç¤ºå¯ä»¥æ”¹å–„æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚

Method: æå‡ºå•çº¯å½¢åµŒå…¥æ–¹æ³•ï¼Œå°†åµŒå…¥çº¦æŸåˆ°å•çº¯å½¢ç»“æ„ä¸­ï¼Œäº§ç”Ÿç¨€ç–ç¦»æ•£ç‰¹å¾ï¼Œç¨³å®šè¯„è®ºå®¶è‡ªä¸¾å¹¶å¢å¼ºç­–ç•¥æ¢¯åº¦ã€‚åº”ç”¨äºFastTD3ã€FastSACå’ŒPPOç®—æ³•ã€‚

Result: åœ¨å¤šç§è¿ç»­å’Œç¦»æ•£æ§åˆ¶ç¯å¢ƒä¸­ï¼Œå•çº¯å½¢åµŒå…¥ä¸€è‡´æå‡äº†æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ï¼Œä¸”æ²¡æœ‰æŸå¤±è¿è¡Œé€Ÿåº¦ã€‚

Conclusion: å•çº¯å½¢åµŒå…¥æ˜¯ä¸€ç§æœ‰æ•ˆçš„å‡ ä½•å½’çº³åç½®æ–¹æ³•ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½è€Œä¸å½±å“æ•ˆç‡ã€‚

Abstract: Recent works have proposed accelerating the wall-clock training time of
actor-critic methods via the use of large-scale environment parallelization;
unfortunately, these can sometimes still require large number of environment
interactions to achieve a desired level of performance. Noting that
well-structured representations can improve the generalization and sample
efficiency of deep reinforcement learning (RL) agents, we propose the use of
simplicial embeddings: lightweight representation layers that constrain
embeddings to simplicial structures. This geometric inductive bias results in
sparse and discrete features that stabilize critic bootstrapping and strengthen
policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial
embeddings consistently improve sample efficiency and final performance across
a variety of continuous- and discrete-control environments, without any loss in
runtime speed.

</details>


### [32] [Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking](https://arxiv.org/abs/2510.13694)
*Yuchun Miao,Liang Ding,Sen Zhang,Rong Bao,Lefei Zhang,Dacheng Tao*

Main category: cs.LG

TL;DR: æå‡ºInfoRMå’ŒIBLæ¡†æ¶è§£å†³RLHFä¸­çš„å¥–åŠ±è¿‡ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡ä¿¡æ¯ç“¶é¢ˆåŸç†è¿‡æ»¤åå¥½æ— å…³ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨åˆ†å¸ƒçº§æ­£åˆ™åŒ–é˜²æ­¢ç­–ç•¥åç¦»ã€‚


<details>
  <summary>Details</summary>
Motivation: RLHFåœ¨è¯­è¨€æ¨¡å‹å¯¹é½ä¸­é¢ä¸´å¥–åŠ±è¿‡ä¼˜åŒ–é—®é¢˜ï¼Œä¸»è¦éšœç¢åŒ…æ‹¬å¥–åŠ±æ¨¡å‹çš„é”™è¯¯æ³›åŒ–å’ŒRLä¼˜åŒ–ä¸­ç¼ºä¹åˆé€‚çš„æ­£åˆ™åŒ–çº¦æŸã€‚

Method: åŸºäºä¿¡æ¯ç“¶é¢ˆåŸç†çš„InfoRMå¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œè¿‡æ»¤åå¥½æ— å…³ä¿¡æ¯ï¼›IBLåˆ†å¸ƒçº§æ­£åˆ™åŒ–ï¼Œæƒ©ç½šä¸SFTåˆ†å¸ƒçš„åç¦»ï¼›MOPç»Ÿè®¡æŒ‡æ ‡é‡åŒ–å¥–åŠ±è¿‡ä¼˜åŒ–ç¨‹åº¦ã€‚

Result: åœ¨å¤šç§LLMå’Œæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒInfoRMå’ŒIBLèƒ½æœ‰æ•ˆç¼“è§£å¥–åŠ±è¿‡ä¼˜åŒ–ï¼ŒMOPä½œä¸ºè¯Šæ–­å·¥å…·å¯é ã€‚

Conclusion: æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•æ˜¾è‘—æ¨è¿›äº†RLHFæŠ€æœ¯çš„å‘å±•ï¼Œä¸ºè§£å†³å¥–åŠ±è¿‡ä¼˜åŒ–é—®é¢˜æä¾›äº†ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
aligning language models with human values, reward hacking-or reward
over-optimization-remains a major challenge. We identify two key obstacles to
its mitigation: (1) reward misgeneralization in reward modeling, where reward
models overfit to spurious, preference-irrelevant features; and (2) the lack of
suitable regularization during RL optimization, as existing token-level
constraints often over-restrict the policy space. To address these issues, we
propose InfoRM, an information-theoretic reward modeling framework based on the
Information Bottleneck (IB) principle, which filters out preference-irrelevant
information to alleviate reward misgeneralization. We further observe that
reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent
space, measured by Mahalanobis distance from the SFT-induced distribution.
Motivated by this, we introduce IBL, a distribution-level regularization that
penalizes such deviations, effectively expanding the optimization landscape
while maintaining alignment. We prove that IBL is theoretically equivalent to
the pessimistic RL objective within the IB latent space. Finally, we present
Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying
reward hacking severity, enabling principled hyperparameter tuning and online
mitigation such as early stopping. Extensive experiments across diverse LLMs
and datasets confirm the generality of our findings, the effectiveness of
InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively
advancing the state of RLHF.

</details>


### [33] [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/abs/2510.13786)
*Devvrit Khatri,Lovish Madaan,Rishabh Tiwari,Rachit Bansal,Sai Surya Duvvuri,Manzil Zaheer,Inderjit S. Dhillon,David Brandfonbrener,Rishabh Agarwal*

Main category: cs.LG

TL;DR: è¯¥è®ºæ–‡æå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œå»ºç«‹äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¯é¢„æµ‹ç¼©æ”¾æ¡†æ¶ï¼Œé€šè¿‡è¶…è¿‡40ä¸‡GPUå°æ—¶çš„å®éªŒå®šä¹‰äº†RLç¼©æ”¾åŸåˆ™ï¼Œå¹¶æå‡ºäº†æœ€ä½³å®è·µé…æ–¹ScaleRLã€‚


<details>
  <summary>Details</summary>
Motivation: å°½ç®¡å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä½†è¯¥é¢†åŸŸç¼ºä¹ä¸é¢„è®­ç»ƒç›¸å½“çš„é¢„æµ‹æ€§ç¼©æ”¾æ–¹æ³•ã€‚éšç€è®¡ç®—é¢„ç®—çš„å¿«é€Ÿå¢é•¿ï¼Œå¦‚ä½•è¯„ä¼°RLè®¡ç®—ç¼©æ”¾çš„ç®—æ³•æ”¹è¿›ç¼ºä¹åŸåˆ™æ€§ç†è§£ã€‚

Method: è¿›è¡Œäº†å¤§è§„æ¨¡ç³»ç»Ÿæ€§ç ”ç©¶ï¼ˆè¶…è¿‡40ä¸‡GPUå°æ—¶ï¼‰ï¼Œæ‹ŸåˆRLè®­ç»ƒçš„Så‹è®¡ç®—-æ€§èƒ½æ›²çº¿ï¼Œæ¶ˆèåˆ†æå„ç§å¸¸è§è®¾è®¡é€‰æ‹©å¯¹æ¸è¿›æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„å½±å“ã€‚

Result: å‘ç°ï¼š(1) ä¸åŒé…æ–¹äº§ç”Ÿä¸åŒçš„æ¸è¿›æ€§èƒ½ï¼›(2) æŸå¤±èšåˆã€å½’ä¸€åŒ–ã€è¯¾ç¨‹å­¦ä¹ å’Œç¦»ç­–ç•¥ç®—æ³•ç­‰ç»†èŠ‚ä¸»è¦è°ƒèŠ‚è®¡ç®—æ•ˆç‡è€Œä¸æ˜¾è‘—æ”¹å˜æ¸è¿›æ€§èƒ½ï¼›(3) ç¨³å®šå¯æ‰©å±•çš„é…æ–¹éµå¾ªå¯é¢„æµ‹çš„ç¼©æ”¾è½¨è¿¹ã€‚æå‡ºçš„ScaleRLé…æ–¹åœ¨10ä¸‡GPUå°æ—¶çš„å•ä¸€RLè¿è¡Œä¸­æˆåŠŸç¼©æ”¾å¹¶é¢„æµ‹éªŒè¯æ€§èƒ½ã€‚

Conclusion: è¯¥ç ”ç©¶ä¸ºåˆ†æRLç¼©æ”¾æä¾›äº†ç§‘å­¦æ¡†æ¶å’Œå®ç”¨é…æ–¹ï¼Œä½¿RLè®­ç»ƒæ›´æ¥è¿‘é¢„è®­ç»ƒé•¿æœŸå®ç°çš„å¯é¢„æµ‹æ€§ã€‚

Abstract: Reinforcement learning (RL) has become central to training large language
models (LLMs), yet the field lacks predictive scaling methodologies comparable
to those established for pre-training. Despite rapidly rising compute budgets,
there is no principled understanding of how to evaluate algorithmic
improvements for scaling RL compute. We present the first large-scale
systematic study, amounting to more than 400,000 GPU-hours, that defines a
principled framework for analyzing and predicting RL scaling in LLMs. We fit
sigmoidal compute-performance curves for RL training and ablate a wide range of
common design choices to analyze their effects on asymptotic performance and
compute efficiency. We observe: (1) Not all recipes yield similar asymptotic
performance, (2) Details such as loss aggregation, normalization, curriculum,
and off-policy algorithm primarily modulate compute efficiency without
materially shifting the asymptote, and (3) Stable, scalable recipes follow
predictable scaling trajectories, enabling extrapolation from smaller-scale
runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and
demonstrate its effectiveness by successfully scaling and predicting validation
performance on a single RL run scaled up to 100,000 GPU-hours. Our work
provides both a scientific framework for analyzing scaling in RL and a
practical recipe that brings RL training closer to the predictability long
achieved in pre-training.

</details>


### [34] [Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach](https://arxiv.org/abs/2510.13792)
*Ziqing Lu,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: æå‡ºä¸€ç§åŸºäºä¿¡æ¯è®ºçš„"ä¸å¯æˆ˜èƒœ"å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡ç‡å¤±çœŸç†è®ºéšæœºæ”¹å˜æ™ºèƒ½ä½“å¯¹è½¬ç§»æ ¸çš„è§‚æµ‹ï¼Œä½¿å…¶åœ¨è®­ç»ƒä¸­è·å¾—é›¶æˆ–æœ‰é™çš„åœ°é¢çœŸå®ä¿¡æ¯ã€‚


<details>
  <summary>Details</summary>
Motivation: æé«˜RLç³»ç»Ÿå¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§éœ€è¦ç ”ç©¶å„ç§å¯¹æŠ—æ”»å‡»ç­–ç•¥ã€‚ä»¥å¾€ç¡®å®šæ€§æ”»å‡»å¯è¢«å—å®³è€…æ™ºèƒ½ä½“é€†è½¬ï¼Œå› æ­¤éœ€è¦å¼€å‘æ›´å¼ºå¤§çš„æ”»å‡»æ–¹æ³•ã€‚

Method: é‡‡ç”¨ç‡å¤±çœŸä¿¡æ¯è®ºæ–¹æ³•ï¼Œéšæœºæ”¹å˜æ™ºèƒ½ä½“å¯¹è½¬ç§»æ ¸æˆ–å…¶ä»–å±æ€§çš„è§‚æµ‹ï¼Œé™åˆ¶æ™ºèƒ½ä½“è·å–åœ°é¢çœŸå®ä¿¡æ¯ã€‚æ¨å¯¼äº†å—å®³è€…æ™ºèƒ½ä½“å¥–åŠ±é—æ†¾çš„ä¿¡æ¯è®ºä¸‹ç•Œã€‚

Result: è¯æ˜äº†è¿™ç§æ”»å‡»å¯¹æœ€å…ˆè¿›çš„åŸºäºæ¨¡å‹å’Œæ— æ¨¡å‹ç®—æ³•çš„å½±å“ï¼Œå¹¶å°†è¯¥æ–¹æ³•æ‰©å±•åˆ°çŠ¶æ€è§‚æµ‹æ”»å‡»ç­‰å…¶ä»–ç±»å‹çš„å¯¹æŠ—æ”»å‡»ã€‚

Conclusion: æå‡ºçš„ä¿¡æ¯è®ºæ–¹æ³•èƒ½å¤Ÿå®ç°"ä¸å¯æˆ˜èƒœ"çš„å¯¹æŠ—æ”»å‡»ï¼Œä¸ºRLç³»ç»Ÿçš„å®‰å…¨é˜²å¾¡æä¾›äº†æ–°çš„æŒ‘æˆ˜å’Œè§†è§’ã€‚

Abstract: Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged
in many security-related applications, such as autonomous driving, financial
decisions, and drone/robot algorithms. In order to improve the
robustness/defense of RL systems against adversaries, studying various
adversarial attacks on RL systems is very important. Most previous work
considered deterministic adversarial attack strategies in MDP, which the
recipient (victim) agent can defeat by reversing the deterministic attacks. In
this paper, we propose a provably ``invincible'' or ``uncounterable'' type of
adversarial attack on RL. The attackers apply a rate-distortion
information-theoretic approach to randomly change agents' observations of the
transition kernel (or other properties) so that the agent gains zero or very
limited information about the ground-truth kernel (or other properties) during
the training. We derive an information-theoretic lower bound on the recipient
agent's reward regret and show the impact of rate-distortion attacks on
state-of-the-art model-based and model-free algorithms. We also extend this
notion of an information-theoretic approach to other types of adversarial
attack, such as state observation attacks.

</details>


### [35] [Asymptotically optimal reinforcement learning in Block Markov Decision Processes](https://arxiv.org/abs/2510.13748)
*Thomas van Vuren,Fiona Sloothaak,Maarten G. Wolf,Jaron Sanders*

Main category: cs.LG

TL;DR: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å—é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡èšç±»å­¦ä¹ æ½œåœ¨çŠ¶æ€ç»“æ„ï¼Œå®ç°äº†æ¯”ç°æœ‰æ–¹æ³•æ›´ä¼˜çš„é—æ†¾ç•ŒO(âˆšT+n)ï¼Œå¹¶åœ¨è¯¥ç±»é—®é¢˜ä¸Šè¾¾åˆ°æ¸è¿‘æœ€ä¼˜ã€‚


<details>
  <summary>Details</summary>
Motivation: é«˜ç»´çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä½¿å¾—å¼ºåŒ–å­¦ä¹ åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ä¸å¯è¡Œï¼Œä½†è®¸å¤šç¯å¢ƒå­˜åœ¨å¯åˆ©ç”¨çš„ç»“æ„ã€‚å—é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹èƒ½å¤Ÿå»ºæ¨¡å…·æœ‰å¤§è§‚æµ‹ç©ºé—´ä½†ç”±æ½œåœ¨çŠ¶æ€å†³å®šåŠ¨æ€çš„é—®é¢˜ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿåˆ©ç”¨è¿™ç§ç»“æ„çš„æœ‰æ•ˆç®—æ³•ã€‚

Method: é‡‡ç”¨ä¸¤é˜¶æ®µç®—æ³•ï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡éšæœºæ¢ç´¢å­¦ä¹ æ½œåœ¨çŠ¶æ€ç»“æ„ï¼Œç¬¬äºŒé˜¶æ®µåˆ‡æ¢åˆ°é€‚åº”å·²å‘ç°ç»“æ„çš„ä¹è§‚å¼•å¯¼ç­–ç•¥ã€‚

Result: ç®—æ³•åœ¨å¯èšç±»çš„BMDPç±»ä¸Šå®ç°äº†O(âˆšT+n)çš„é—æ†¾ç•Œï¼Œä¼˜äºä¹‹å‰æœ€å¥½çš„O(âˆšT+nÂ²)ç»“æœï¼Œç‰¹åˆ«æ˜¯å½“è§‚æµ‹ç©ºé—´åŸºæ•°nå¾ˆå¤§æ—¶ã€‚

Conclusion: è¯¥ç®—æ³•åœ¨å¯èšç±»çš„BMDPç±»ä¸Šè¾¾åˆ°æ¸è¿‘æœ€ä¼˜ï¼Œè¯æ˜äº†å‡†ç¡®ä¼°è®¡æ½œåœ¨çŠ¶æ€ç¡®å®èƒ½æœ‰æ•ˆåŠ é€Ÿå­¦ä¹ ã€‚

Abstract: The curse of dimensionality renders Reinforcement Learning (RL) impractical
in many real-world settings with exponentially large state and action spaces.
Yet, many environments exhibit exploitable structure that can accelerate
learning. To formalize this idea, we study RL in Block Markov Decision
Processes (BMDPs). BMDPs model problems with large observation spaces, but
where transition dynamics are fully determined by latent states. Recent
advances in clustering methods have enabled the efficient recovery of this
latent structure. However, a regret analysis that exploits these techniques to
determine their impact on learning performance remained open. We are now
addressing this gap by providing a regret analysis that explicitly leverages
clustering, demonstrating that accurate latent state estimation can indeed
effectively speed up learning.
  Concretely, this paper analyzes a two-phase RL algorithm for BMDPs that first
learns the latent structure through random exploration and then switches to an
optimism-guided strategy adapted to the uncovered structure. This algorithm
achieves a regret that is $O(\sqrt{T}+n)$ on a large class of BMDPs susceptible
to clustering. Here, $T$ denotes the number of time steps, $n$ is the
cardinality of the observation space, and the Landau notation $O(\cdot)$ holds
up to constants and polylogarithmic factors. This improves the best prior
bound, $O(\sqrt{T}+n^2)$, especially when $n$ is large. Moreover, we prove that
no algorithm can achieve lower regret uniformly on this same class of BMDPs.
This establishes that, on this class, the algorithm achieves asymptotic
optimality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [36] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: æœ¬æ–‡è¯æ˜åœ¨å¤§å‹ä»£ç åº“ä¸­ä½¿ç”¨å…³é”®è¯æœç´¢è¶³ä»¥æ£€ç´¢ç›¸å…³ä»£ç ä¸Šä¸‹æ–‡ï¼Œæ— éœ€GPUèµ„æºï¼Œåœ¨ä»£ç ä¸Šä¸‹æ–‡ç«èµ›ä¸­å–å¾—è‰¯å¥½è¡¨ç°


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ä½¿ç”¨è¯­ä¹‰æœç´¢éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œéš¾ä»¥é›†æˆåˆ°è½»é‡çº§åº”ç”¨ä¸­å¦‚IDEä»£ç è¡¥å…¨

Method: ä½¿ç”¨å…³é”®è¯æœç´¢æ›¿ä»£è¯­ä¹‰æœç´¢æ¥æ£€ç´¢ç›¸å…³ä»£ç ä¸Šä¸‹æ–‡

Result: åœ¨ä»£ç ä¸Šä¸‹æ–‡ç«èµ›çš„Kotlinå’ŒPythonèµ›é“ä¸Šåˆ†åˆ«è¾¾åˆ°0.748å’Œ0.725çš„chRFåˆ†æ•°

Conclusion: å…³é”®è¯æœç´¢æ˜¯æ£€ç´¢ä»£ç ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œæ— éœ€å¤§é‡è®¡ç®—èµ„æº

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [37] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ä¿®å¤å·¥ä¸šåµŒå…¥å¼ç³»ç»Ÿä¸­çš„ç¼–è¯‘é”™è¯¯ï¼Œåœ¨CIç³»ç»Ÿä¸­èƒ½è§£å†³63%çš„ç¼–è¯‘é”™è¯¯ï¼Œ83%çš„ä¿®å¤è¢«è®¤ä¸ºæ˜¯åˆç†çš„ï¼Œå¹¶å°†è°ƒè¯•æ—¶é—´ä»å‡ å°æ—¶ç¼©çŸ­åˆ°8åˆ†é’Ÿå†…ã€‚


<details>
  <summary>Details</summary>
Motivation: å·¥ä¸šåµŒå…¥å¼ç³»ç»Ÿä¸­ç¡¬ä»¶å’Œè½¯ä»¶å…±åŒå¼€å‘ç»å¸¸å¯¼è‡´æŒç»­é›†æˆè¿‡ç¨‹ä¸­çš„ç¼–è¯‘é”™è¯¯ï¼Œç°æœ‰ä¿®å¤æŠ€æœ¯ä¾èµ–æµ‹è¯•ç”¨ä¾‹ï¼Œä½†ä¸å¯ç¼–è¯‘ä»£ç æ²¡æœ‰æµ‹è¯•ç”¨ä¾‹å¯ç”¨ã€‚

Method: æ”¶é›†è¶…è¿‡40000ä¸ªäº§å“æºä»£ç æäº¤ï¼Œè¯„ä¼°å››ä¸ªæœ€å…ˆè¿›LLMå¢å¼ºçš„å·¥ä¸šCIç³»ç»Ÿæ€§èƒ½ï¼Œä¸äººå·¥ä¿®æ­£è¿›è¡Œæ¯”è¾ƒã€‚

Result: LLMå¢å¼ºçš„CIç³»ç»Ÿèƒ½è§£å†³åŸºçº¿æ•°æ®é›†ä¸­63%çš„ç¼–è¯‘é”™è¯¯ï¼Œå…¶ä¸­83%çš„æˆåŠŸä¿®å¤è¢«è®¤ä¸ºæ˜¯åˆç†çš„ï¼Œè°ƒè¯•æ—¶é—´å¤§å¹…å‡å°‘åˆ°8åˆ†é’Ÿå†…ã€‚

Conclusion: LLMåœ¨è‡ªåŠ¨ä¿®å¤ç¼–è¯‘é”™è¯¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½æœ‰æ•ˆè§£å†³å·¥ä¸šåµŒå…¥å¼ç³»ç»Ÿä¸­çš„ç¼–è¯‘é—®é¢˜å¹¶æ˜¾è‘—æé«˜è°ƒè¯•æ•ˆç‡ã€‚

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [38] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: æœ¬æ–‡ç ”ç©¶äº†ä¸åŒä»£ç ä»“åº“å¤„ç†ç­–ç•¥å¯¹OpenCoderæ¨¡å‹ï¼ˆ1.5Bå‚æ•°ï¼‰ä¸Šä¸‹æ–‡å­¦ä¹ çš„å½±å“ï¼Œé€šè¿‡æ‰©å±•ä¸Šä¸‹æ–‡çª—å£è‡³16,384 tokenå¹¶åœ¨1B tokençš„ä»“åº“çº§æ•°æ®ä¸Šè®­ç»ƒï¼Œåœ¨Long Code ArenaåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸æ›´å¤§æ•°æ®é›†æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: æ¢ç´¢ä»£ç ä»“åº“çº§é¢„è®­ç»ƒä¸­ä¸åŒå¤„ç†ç­–ç•¥å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç ç”Ÿæˆèƒ½åŠ›çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åœ¨å°æ•°æ®é›†æ¡ä»¶ä¸‹å®ç°ä¸å¤§è§„æ¨¡æ•°æ®é›†æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

Method: æ‰©å±•OpenCoderæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ä»4,096åˆ°16,384 tokenï¼Œåœ¨1B tokençš„ç²¾é€‰ä»“åº“çº§æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¯”è¾ƒä¸åŒä»“åº“å¤„ç†æŠ€æœ¯ï¼Œå¹¶æµ‹è¯•æ›´ç®€å•çš„æ–‡ä»¶çº§è®­ç»ƒæ–¹æ³•ã€‚

Result: å°½ç®¡ä½¿ç”¨æ¯”ç«äº‰æ¨¡å‹å°å¾—å¤šçš„æ•°æ®é›†ï¼Œä½†åœ¨Long Code ArenaåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¯æ¯”æ€§èƒ½ï¼›ä¸åŒä»“åº“å¤„ç†æŠ€æœ¯äº§ç”Ÿç›¸ä¼¼å¼ºç»“æœï¼Œä¸»è¦å¢ç›Šæ¥è‡ªé€‚åº”æ–°çš„RoPEç¼©æ”¾å‚æ•°ï¼›æ›´ç®€å•çš„æ–‡ä»¶çº§è®­ç»ƒæ–¹æ³•åœ¨åŸå§‹åºåˆ—é•¿åº¦ä¸‹ä»ç„¶éå¸¸æœ‰æ•ˆã€‚

Conclusion: ä»“åº“çº§ä»£ç è¡¥å…¨ç ”ç©¶å¯ä»¥åœ¨æ•°æ®å’Œè®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒä¸­è¿›è¡Œï¼Œæ›´ç®€å•çš„æ–‡ä»¶çº§è®­ç»ƒæ–¹æ³•ä»ç„¶æœ‰æ•ˆï¼Œä¸ºèµ„æºå—é™åœºæ™¯æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: æå‡ºäº†RIDæ¡†æ¶ï¼Œä¸€ç§é›¶æ ·æœ¬å…ƒæç¤ºæŠ€æœ¯ï¼Œé€šè¿‡ç»“æ„åŒ–è®¤çŸ¥æ¨¡å¼è®©LLMèƒ½å¤Ÿè¿›è¡Œäººç±»å¯¹é½çš„å¼‚å¸¸å¤„ç†ï¼Œæ˜¾è‘—æå‡å†³ç­–è´¨é‡ã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³LLMä½œä¸ºæ™ºèƒ½ä½“æ¨ç†å¼•æ“æ—¶å­˜åœ¨çš„è§„åˆ™åˆšæ€§ç¼ºé™·ï¼Œå³è¿‡åº¦éµå¾ªæ˜¾å¼è§„åˆ™è€Œå¿½è§†äººç±»å¸¸è¯†å’Œæ„å›¾çš„é—®é¢˜ã€‚

Method: RIDæ¡†æ¶ï¼šç»“æ„åŒ–å…ƒæç¤ºæŠ€æœ¯ï¼Œå¼•å¯¼æ¨¡å‹è§£æ„ä»»åŠ¡ã€åˆ†ç±»è§„åˆ™ã€æƒè¡¡å†²çªç»“æœå¹¶è¯æ˜æœ€ç»ˆå†³ç­–ã€‚

Result: åœ¨20ä¸ªéœ€è¦ç»†å¾®åˆ¤æ–­çš„åœºæ™¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRIDæ¡†æ¶è¾¾åˆ°95%çš„äººç±»å¯¹é½åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿(80%)å’ŒCoT(75%)ã€‚

Conclusion: RIDæ¡†æ¶ä¸ºä»å­—é¢æŒ‡ä»¤éµå¾ªè½¬å‘ç›®æ ‡å¯¼å‘æ¨ç†æä¾›äº†å®ç”¨æœ‰æ•ˆçš„æ–¹æ³•ï¼Œæœ‰åŠ©äºæ„å»ºæ›´å¯é çš„AIæ™ºèƒ½ä½“ã€‚

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [40] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanneræ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŸºäºç†µçš„ä»¤ç‰Œçº§ä¼˜åŠ¿å¡‘é€ å’Œé€‰æ‹©æ€§åŠ æƒæ ·æœ¬çº§ä¼˜åŠ¿ï¼Œæœ‰æ•ˆå¢å¼ºæ·±åº¦ç ”ç©¶ä»£ç†çš„è§„åˆ’èƒ½åŠ›ï¼Œåœ¨ä¸ƒä¸ªæ·±åº¦ç ”ç©¶åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–æ¨ç†é˜¶æ®µçš„éšå¼è§„åˆ’ï¼Œè¦ä¹ˆå¼•å…¥æ˜¾å¼è§„åˆ’å™¨ä½†æ²¡æœ‰ç³»ç»Ÿæ€§åœ°è§£å†³å¦‚ä½•ä¼˜åŒ–è§„åˆ’é˜¶æ®µçš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æ ‡å‡†å¼ºåŒ–å­¦ä¹ ä¸‹ï¼Œè§„åˆ’ä»¤ç‰Œçš„ç†µæ˜¾è‘—é«˜äºå…¶ä»–åŠ¨ä½œä»¤ç‰Œï¼Œæ­ç¤ºäº†æœªä¼˜åŒ–çš„ä¸ç¡®å®šå†³ç­–ç‚¹ã€‚

Method: æå‡ºDeepPlanneræ¡†æ¶ï¼Œé€šè¿‡åŸºäºç†µçš„ä»¤ç‰Œçº§ä¼˜åŠ¿å¡‘é€ ä¸ºé«˜ç†µä»¤ç‰Œåˆ†é…æ›´å¤§çš„æ›´æ–°ï¼Œå¹¶é€‰æ‹©æ€§åŠ æƒè§„åˆ’å¯†é›†å‹rolloutsçš„æ ·æœ¬çº§ä¼˜åŠ¿ã€‚

Result: åœ¨ä¸ƒä¸ªæ·±åº¦ç ”ç©¶åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDeepPlanneræé«˜äº†è§„åˆ’è´¨é‡ï¼Œå¹¶åœ¨æ˜¾è‘—é™ä½çš„è®­ç»ƒé¢„ç®—ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

Conclusion: DeepPlanneré€šè¿‡ç³»ç»Ÿæ€§åœ°ä¼˜åŒ–è§„åˆ’é˜¶æ®µï¼Œæœ‰æ•ˆå¢å¼ºäº†æ·±åº¦ç ”ç©¶ä»£ç†çš„è§„åˆ’èƒ½åŠ›ï¼Œä¸ºå¤æ‚ä»»åŠ¡çš„é•¿æ—¶ç¨‹è§„åˆ’æä¾›äº†æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [41] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: Sentinelæ˜¯é¦–ä¸ªåŸºäºå½¢å¼åŒ–æ–¹æ³•è¯„ä¼°LLMå…·èº«æ™ºèƒ½ä½“ç‰©ç†å®‰å…¨æ€§çš„æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰ã€è§„åˆ’å’Œè½¨è¿¹ä¸‰ä¸ªå±‚é¢å¯¹å®‰å…¨æ€§è¿›è¡Œå½¢å¼åŒ–éªŒè¯ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰æ–¹æ³•ä¾èµ–å¯å‘å¼è§„åˆ™æˆ–ä¸»è§‚çš„LLMåˆ¤æ–­ï¼Œç¼ºä¹å¯¹ç‰©ç†å®‰å…¨æ€§çš„å½¢å¼åŒ–éªŒè¯ï¼Œæ— æ³•ç²¾ç¡®æŒ‡å®šçŠ¶æ€ä¸å˜é‡ã€æ—¶é—´ä¾èµ–æ€§å’Œæ—¶åºçº¦æŸã€‚

Method: ä½¿ç”¨æ—¶åºé€»è¾‘(TL)è¯­ä¹‰å½¢å¼åŒ–å®‰å…¨éœ€æ±‚ï¼Œæ„å»ºå¤šçº§éªŒè¯ç®¡é“ï¼šè¯­ä¹‰çº§éªŒè¯LLMå¯¹å®‰å…¨éœ€æ±‚çš„ç†è§£ï¼Œè§„åˆ’çº§éªŒè¯é«˜å±‚è¡ŒåŠ¨è®¡åˆ’ï¼Œè½¨è¿¹çº§éªŒè¯æ‰§è¡Œè½¨è¿¹ã€‚

Result: åœ¨VirtualHomeå’ŒALFREDç¯å¢ƒä¸­è¯„ä¼°å¤šä¸ªLLMå…·èº«æ™ºèƒ½ä½“ï¼Œå‘ç°Sentinelèƒ½å¤Ÿæš´éœ²å…ˆå‰æ–¹æ³•å¿½ç•¥çš„å®‰å…¨è¿è§„ï¼Œå¹¶æ­ç¤ºå…¶å¤±è´¥æ¨¡å¼ã€‚

Conclusion: é€šè¿‡å°†ç‰©ç†å®‰å…¨æ€§ä¸æ—¶åºé€»è¾‘ç›¸ç»“åˆï¼ŒSentinelä¸ºç³»ç»Ÿè¯„ä¼°LLMå…·èº«æ™ºèƒ½ä½“åœ¨ç‰©ç†ç¯å¢ƒä¸­çš„å®‰å…¨æ€§æä¾›äº†ä¸¥è°¨åŸºç¡€ã€‚

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [42] [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)
*Stephane Hatgis-Kessell,Logan Mondal Bhamidipaty,Emma Brunskill*

Main category: cs.AI

TL;DR: æå‡ºPreference-Based Reward Repair (PBRR)æ¡†æ¶ï¼Œé€šè¿‡ä»äººç±»åå¥½ä¸­å­¦ä¹ åŠ æ€§ä¿®æ­£é¡¹æ¥è‡ªåŠ¨ä¿®å¤äººå·¥è®¾è®¡çš„ä»£ç†å¥–åŠ±å‡½æ•°ï¼Œè§£å†³å¥–åŠ±å‡½æ•°é”™é…é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: äººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ç»å¸¸ä¸çœŸå®ç›®æ ‡é”™é…ï¼Œå¯¼è‡´å¥–åŠ±é»‘å®¢é—®é¢˜ï¼›è€Œä»å¤´å­¦ä¹ å¥–åŠ±å‡½æ•°æˆæœ¬é«˜æ˜‚ã€‚éœ€è¦ä¸€ç§ç»“åˆä¸¤è€…ä¼˜åŠ¿çš„æ–¹æ³•ã€‚

Method: PBRRæ¡†æ¶ï¼šé€šè¿‡ç›®æ ‡æ¢ç´¢ç­–ç•¥å’Œæ–°çš„åå¥½å­¦ä¹ ç›®æ ‡ï¼Œå­¦ä¹ ä¸€ä¸ªä¸çŠ¶æ€è½¬ç§»ç›¸å…³çš„åŠ æ€§ä¿®æ­£é¡¹æ¥ä¿®å¤ä»£ç†å¥–åŠ±å‡½æ•°ã€‚

Result: åœ¨è¡¨æ ¼åŸŸä¸­è¯æ˜PBRRçš„ç´¯ç§¯é—æ†¾ä¸ç°æœ‰åå¥½å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“ï¼›åœ¨å¥–åŠ±é»‘å®¢åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPBRRå§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œéœ€è¦æ›´å°‘çš„åå¥½æ¥å­¦ä¹ é«˜æ€§èƒ½ç­–ç•¥ã€‚

Conclusion: PBRRèƒ½å¤Ÿæœ‰æ•ˆä¿®å¤é”™é…çš„ä»£ç†å¥–åŠ±å‡½æ•°ï¼Œæ¯”ä»å¤´å­¦ä¹ å¥–åŠ±å‡½æ•°æˆ–å…¶ä»–ä¿®æ­£æ–¹æ³•æ›´é«˜æ•ˆï¼Œæ˜¾è‘—å‡å°‘æ‰€éœ€çš„äººç±»åå¥½æ•°é‡ã€‚

Abstract: Human-designed reward functions for reinforcement learning (RL) agents are
frequently misaligned with the humans' true, unobservable objectives, and thus
act only as proxies. Optimizing for a misspecified proxy reward function often
induces reward hacking, resulting in a policy misaligned with the human's true
objectives. An alternative is to perform RL from human feedback, which involves
learning a reward function from scratch by collecting human preferences over
pairs of trajectories. However, building such datasets is costly. To address
the limitations of both approaches, we propose Preference-Based Reward Repair
(PBRR): an automated iterative framework that repairs a human-specified proxy
reward function by learning an additive, transition-dependent correction term
from preferences. A manually specified reward function can yield policies that
are highly suboptimal under the ground-truth objective, yet corrections on only
a few transitions may suffice to recover optimal performance. To identify and
correct for those transitions, PBRR uses a targeted exploration strategy and a
new preference-learning objective. We prove in tabular domains PBRR has a
cumulative regret that matches, up to constants, that of prior preference-based
RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR
consistently outperforms baselines that learn a reward function from scratch
from preferences or modify the proxy reward function using other approaches,
requiring substantially fewer preferences to learn high performing policies.

</details>


### [43] [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)
*Zehui Ling,Deshu Chen,Yichi Zhang,Yuchen Liu,Xigui Li,Xin Guo,Yuan Cheng*

Main category: cs.AI

TL;DR: æå‡ºä¸€ç§ç»“åˆå°å‹å’Œå¤§å‹LLMçš„äº’è¡¥ä»£ç†ç³»ç»Ÿï¼Œå°å‹LLMå…ˆç”Ÿæˆåˆå§‹ç­”æ¡ˆï¼Œå¤§å‹LLMè¿›è¡ŒéªŒè¯ï¼Œä»…åœ¨å¿…è¦æ—¶è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚


<details>
  <summary>Details</summary>
Motivation: æ·±åº¦æ¨ç†å’Œå¤šä»£ç†ç³»ç»Ÿè™½ç„¶èƒ½æé«˜å¤æ‚ä»»åŠ¡æ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆçš„æ–¹æ³•æ¥å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ã€‚

Method: é‡‡ç”¨å°å‹LLMç”Ÿæˆåˆå§‹ç­”æ¡ˆï¼Œå¤§å‹LLMè¿›è¡ŒéªŒè¯ï¼Œä»…åœ¨ç­”æ¡ˆé”™è¯¯æ—¶è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œå½¢æˆäº’è¡¥ä»£ç†ç³»ç»Ÿã€‚

Result: åœ¨ç®€å•é—®é¢˜ä¸Šï¼Œå¤§å‹LLMçš„è®¡ç®—æˆæœ¬é™ä½è¶…è¿‡50%ï¼Œå‡†ç¡®ç‡æŸå¤±å¯å¿½ç•¥ï¼Œå¤æ‚ä»»åŠ¡ä¸Šä¿æŒç¨³å¥æ€§èƒ½ã€‚

Conclusion: è¯¥äº’è¡¥ä»£ç†ç³»ç»Ÿèƒ½æœ‰æ•ˆé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ï¼Œä¸ºLLMåº”ç”¨æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: Recent advances in Large Language Models (LLMs) demonstrate that
chain-of-thought prompting and deep reasoning substantially enhance performance
on complex tasks, and multi-agent systems can further improve accuracy by
enabling model debates. However, applying deep reasoning to all problems is
computationally expensive. To mitigate these costs, we propose a complementary
agent system integrating small and large LLMs. The small LLM first generates an
initial answer, which is then verified by the large LLM. If correct, the answer
is adopted directly; otherwise, the large LLM performs in-depth reasoning.
Experimental results show that, for simple problems, our approach reduces the
computational cost of the large LLM by more than 50% with negligible accuracy
loss, while consistently maintaining robust performance on complex tasks.

</details>


### [44] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: æå‡ºäº†J-TTLåŸºå‡†æµ‹è¯•æ¥è¡¡é‡AIä»£ç†åœ¨æµ‹è¯•æ—¶çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†EvoTestè¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡æ¼”åŒ–å’Œé‡é…ç½®ä»£ç†ç³»ç»Ÿæ¥æå‡æ€§èƒ½ï¼Œæ— éœ€å¾®è°ƒæˆ–æ¢¯åº¦è®¡ç®—ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰AIä»£ç†æ— æ³•åœ¨æµ‹è¯•æ—¶åŠ¨æ€å­¦ä¹ å¤æ‚æŠ€èƒ½ï¼Œåœ¨é™Œç”Ÿç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œè¿™é™åˆ¶äº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚éœ€è¦ç³»ç»Ÿæ€§åœ°è¡¡é‡å’Œæ¨åŠ¨è¿™ä¸€æŒ‘æˆ˜çš„è¿›å±•ã€‚

Method: å¼•å…¥J-TTLåŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚ä»£ç†åœ¨è¿ç»­å¤šè½®æ¸¸æˆä¸­æå‡è¡¨ç°ã€‚æå‡ºEvoTestè¿›åŒ–æµ‹è¯•æ—¶å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«æ‰§è¡Œæ¸¸æˆçš„Actorä»£ç†å’Œåˆ†ææ¸¸æˆè®°å½•ä»¥æå‡ºæ–°é…ç½®çš„Evolverä»£ç†ï¼Œé€šè¿‡é‡å†™æç¤ºã€æ›´æ–°è®°å¿†ã€è°ƒæ•´è¶…å‚æ•°å’Œå­¦ä¹ å·¥å…·ä½¿ç”¨ä¾‹ç¨‹æ¥æ”¹è¿›ç³»ç»Ÿã€‚

Result: EvoTeståœ¨J-TTLåŸºå‡†ä¸ŠæŒç»­æå‡æ€§èƒ½ï¼Œä¼˜äºåå°„ã€è®°å¿†å’Œåœ¨çº¿å¾®è°ƒç­‰æ–¹æ³•ã€‚æ˜¯å”¯ä¸€èƒ½å¤Ÿèµ¢å¾—ä¸¤ä¸ªæ¸¸æˆï¼ˆDetectiveå’ŒLibraryï¼‰çš„æ–¹æ³•ï¼Œè€Œæ‰€æœ‰åŸºçº¿æ–¹æ³•éƒ½æœªèƒ½èµ¢å¾—ä»»ä½•æ¸¸æˆã€‚

Conclusion: EvoTestæ¡†æ¶é€šè¿‡è¿›åŒ–æ–¹æ³•æœ‰æ•ˆè§£å†³äº†æµ‹è¯•æ—¶å­¦ä¹ æŒ‘æˆ˜ï¼Œå±•ç¤ºäº†åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹æå‡ä»£ç†æ€§èƒ½çš„å¯è¡Œæ€§ã€‚

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [45] [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)
*Robert West,Ashton Anderson,Ece Kamar,Eric Horvitz*

Main category: cs.AI

TL;DR: æå‡ºäº†ä¸€ç§åä¸ºtandem trainingçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡éšæœºå°†æ§åˆ¶æƒäº¤ç»™è¾ƒå¼±æ¨¡å‹æ¥è®­ç»ƒå¼ºæ¨¡å‹äº§ç”Ÿå¯ç†è§£ä¸”å¯è¢«è¾ƒå¼±æ¨¡å‹ç»§ç»­æ‰§è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€è¯­è¨€æ¨¡å‹èƒ½åŠ›å¿«é€Ÿæå‡ï¼Œå…¶æ¨ç†è¿‡ç¨‹å¯èƒ½å˜å¾—éš¾ä»¥è¢«è¾ƒå¼±ä»£ç†æˆ–äººç±»ç†è§£ï¼Œè¿™ä¼šå½±å“å¯è§£é‡Šæ€§å’Œç›‘ç£ã€‚éœ€è¦ç¡®ä¿å¼ºæ¨¡å‹äº§ç”Ÿçš„è§£å†³æ–¹æ¡ˆå¯¹è¾ƒå¼±åˆä½œè€…ä¿æŒå¯ç†è§£æ€§ã€‚

Method: å¼•å…¥handoff robustnessä½œä¸ºå¯ç†è§£æ€§æ ‡å‡†ï¼Œæå‡ºtandem trainingå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä»å†»ç»“çš„å¼±æ¨¡å‹é‡‡æ ·tokenï¼Œåªæœ‰å½“å¼ºæ¨¡å‹çš„åŠ¨ä½œå’Œæ¨ç†è¿‡ç¨‹èƒ½è¢«å¼±æ¨¡å‹ç»§ç»­æ—¶ï¼Œè®­ç»ƒæ‰ä¼šæˆåŠŸã€‚

Result: åœ¨GSM8Kæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œtandem trainingèƒ½æœ‰æ•ˆæ•™ä¼šæ¨¡å‹æ”¾å¼ƒä¸“ä¸šæœ¯è¯­ï¼Œé€‚åº”è¾ƒå¼±åˆä½œä¼™ä¼´çš„è¯­è¨€é£æ ¼ï¼ŒåŒæ—¶ä¿æŒé«˜ä»»åŠ¡å‡†ç¡®ç‡ã€‚

Conclusion: è¯¥æ–¹æ³•ä¸ºæ„å»ºå¯ç”±è¾ƒå¼±ä»£ç†å®¡è®¡çš„AIç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ï¼Œå¯¹äººä¸AIåä½œå’Œå¤šæ™ºèƒ½ä½“é€šä¿¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚

Abstract: As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model's solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model's actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.

</details>


### [46] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: æå‡ºäº†ä¸€ç§åŸºäºæœ€å¤§åŒ–äººç±»èµ‹èƒ½çš„è¾…åŠ©è¯­è¨€æ¨¡å‹è°ƒä¼˜æ–¹æ³•Empowerï¼Œä»…éœ€ç¦»çº¿æ–‡æœ¬æ•°æ®å³å¯è®­ç»ƒæ›´æœ‰æ•ˆçš„AIåŠ©æ‰‹ï¼Œæ— éœ€é¢å¤–äººå·¥åé¦ˆã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰è¾…åŠ©ä»£ç†æ–¹æ³•å¾€å¾€é¼“åŠ±AIç‹¬ç«‹å®Œæˆä»»åŠ¡è€ŒéçœŸæ­£ååŠ©äººç±»ï¼Œä¸”éœ€è¦æ˜‚è´µçš„äººå·¥åé¦ˆã€‚éœ€è¦ä¸€ç§èƒ½çœŸæ­£å¸®åŠ©äººç±»å®ç°ç›®æ ‡ã€ä»…éœ€ç¦»çº¿æ•°æ®çš„è‡ªç›‘ç£æ–¹æ³•ã€‚

Method: æå‡ºEmpoweræ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–äººç±»åœ¨ç¯å¢ƒä¸­çš„èµ‹èƒ½ï¼ˆå½±å“æœŸæœ›å˜åŒ–çš„èƒ½åŠ›ï¼‰æ¥è°ƒä¼˜è¯­è¨€æ¨¡å‹ï¼Œä»…ä½¿ç”¨ç¦»çº¿æ–‡æœ¬æ•°æ®ï¼Œæ— éœ€é¢å¤–äººå·¥åé¦ˆã€‚

Result: ç”¨æˆ·ç ”ç©¶ä¸­78%çš„å‚ä¸è€…åå¥½EmpoweråŠ©æ‰‹ï¼ˆp=0.015ï¼‰ï¼Œæ¥å—ç‡æé«˜31%ï¼Œå»ºè®®å‡å°‘38%ã€‚åœ¨ä»£ç ååŠ©ç¯å¢ƒä¸­ï¼ŒEmpowerå°†æ¨¡æ‹Ÿç¨‹åºå‘˜åœ¨æŒ‘æˆ˜æ€§ç¼–ç¨‹é—®é¢˜ä¸Šçš„æˆåŠŸç‡å¹³å‡æé«˜192%ã€‚

Conclusion: Empoweræä¾›äº†ä¸€ä¸ªä»…ä½¿ç”¨ç¦»çº¿æ•°æ®ã€æ— éœ€é¢å¤–äººå·¥åé¦ˆæˆ–å¯éªŒè¯å¥–åŠ±çš„å¤§è§„æ¨¡æœ‰ç”¨å¯¹é½AIä»£ç†æ¡†æ¶ã€‚

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [47] [From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails](https://arxiv.org/abs/2510.13727)
*Ravi Pandya,Madison Bland,Duy P. Nguyen,Changliu Liu,Jaime FernÃ¡ndez Fisac,Andrea Bajcsy*

Main category: cs.AI

TL;DR: æå‡ºäº†ä¸€ç§åŸºäºæ§åˆ¶ç†è®ºçš„å®‰å…¨æŠ¤æ æ–¹æ³•ï¼Œé€šè¿‡å®æ—¶ç›‘æ§å’Œä¸»åŠ¨ä¿®æ­£AIç³»ç»Ÿçš„è¾“å‡ºï¼Œé¢„é˜²ä¸‹æ¸¸å±å®³ï¼Œè€Œä¸æ˜¯ç®€å•åœ°é˜»æ­¢å±é™©è¡Œä¸ºã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰AIå®‰å…¨æŠ¤æ ä¸»è¦ä¾èµ–è¾“å‡ºåˆ†ç±»å’Œäººä¸ºæ ‡å‡†ï¼Œæ— æ³•åº”å¯¹æ–°å±é™©æƒ…å†µï¼Œä¸”æ£€æµ‹åˆ°å±é™©ååªèƒ½æ‹’ç»è¡ŒåŠ¨ï¼Œè¿™å¹¶ä¸æ€»æ˜¯å®‰å…¨çš„é€‰æ‹©ã€‚

Method: å°†AIå®‰å…¨è§†ä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œåœ¨AIæ¨¡å‹çš„æ½œåœ¨è¡¨ç¤ºç©ºé—´ä¸­åº”ç”¨å®‰å…¨å…³é”®æ§åˆ¶ç†è®ºï¼Œæ„å»ºé¢„æµ‹æ€§æŠ¤æ æ¥å®æ—¶ç›‘æ§å’Œä¸»åŠ¨ä¿®æ­£é£é™©è¾“å‡ºã€‚

Result: åœ¨æ¨¡æ‹Ÿé©¾é©¶å’Œç”µå­å•†åŠ¡åœºæ™¯ä¸­çš„å®éªŒè¡¨æ˜ï¼Œæ§åˆ¶ç†è®ºæŠ¤æ èƒ½å¯é åœ°é¿å…ç¾éš¾æ€§åæœï¼ˆå¦‚ç¢°æ’å’Œç ´äº§ï¼‰ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ã€‚

Conclusion: æ§åˆ¶ç†è®ºæŠ¤æ ä¸ºå½“å‰æ ‡è®°-é˜»æ­¢å¼æŠ¤æ æä¾›äº†åŸåˆ™æ€§çš„åŠ¨æ€æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç¡®ä¿AIç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚

Abstract: Generative AI systems are increasingly assisting and acting on behalf of end
users in practical settings, from digital shopping assistants to
next-generation autonomous cars. In this context, safety is no longer about
blocking harmful content, but about preempting downstream hazards like
financial or physical harm. Yet, most AI guardrails continue to rely on output
classification based on labeled datasets and human-specified criteria,making
them brittle to new hazardous situations. Even when unsafe conditions are
flagged, this detection offers no path to recovery: typically, the AI system
simply refuses to act--which is not always a safe choice. In this work, we
argue that agentic AI safety is fundamentally a sequential decision problem:
harmful outcomes arise from the AI system's continually evolving interactions
and their downstream consequences on the world. We formalize this through the
lens of safety-critical control theory, but within the AI model's latent
representation of the world. This enables us to build predictive guardrails
that (i) monitor an AI system's outputs (actions) in real time and (ii)
proactively correct risky outputs to safe ones, all in a model-agnostic manner
so the same guardrail can be wrapped around any AI model. We also offer a
practical training recipe for computing such guardrails at scale via
safety-critical reinforcement learning. Our experiments in simulated driving
and e-commerce settings demonstrate that control-theoretic guardrails can
reliably steer LLM agents clear of catastrophic outcomes (from collisions to
bankruptcy) while preserving task performance, offering a principled dynamic
alternative to today's flag-and-block guardrails.

</details>


### [48] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: Hard2Verifyæ˜¯ä¸€ä¸ªäººå·¥æ ‡æ³¨çš„æ•°å­¦è¯æ˜æ­¥éª¤çº§éªŒè¯åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å‰æ²¿LLMéªŒè¯å™¨åœ¨æŒ‘æˆ˜æ€§æ•°å­¦é—®é¢˜ä¸­çš„è¡¨ç°ã€‚


<details>
  <summary>Details</summary>
Motivation: åœ¨IMO 2025ç­‰æ•°å­¦ç«èµ›ä¸­ï¼ŒLLMæ¨ç†ç³»ç»Ÿéœ€è¦èƒ½å¤ŸéªŒè¯æ¯ä¸ªè¯æ˜æ­¥éª¤çš„æ­£ç¡®æ€§å’Œå……åˆ†æ€§ï¼Œå› æ­¤éœ€è¦å¼ºå¤§çš„æ­¥éª¤çº§éªŒè¯å™¨ã€‚

Method: æ„å»ºäº†åŒ…å«500+å°æ—¶äººå·¥æ ‡æ³¨çš„Hard2VerifyåŸºå‡†ï¼Œè¯„ä¼°äº†29ä¸ªç”Ÿæˆå¼è¯„è®ºå™¨å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨è¯†åˆ«å‰æ²¿LLMç”Ÿæˆçš„æ•°å­¦è¯æ˜ä¸­ç¬¬ä¸€ä¸ªé”™è¯¯çš„èƒ½åŠ›ã€‚

Result: é™¤äº†å°‘æ•°è¡¨ç°ä¼˜å¼‚è€…å¤–ï¼Œå¼€æºéªŒè¯å™¨è½åäºé—­æºæ¨¡å‹ï¼Œåˆ†æäº†æ€§èƒ½ä¸ä½³çš„åŸå› ã€è®¡ç®—è§„æ¨¡åŒ–çš„å½±å“ä»¥åŠè‡ªéªŒè¯å’ŒéªŒè¯-ç”ŸæˆåŠ¨æ€ç­‰åŸºæœ¬é—®é¢˜ã€‚

Conclusion: Hard2VerifyåŸºå‡†æ­ç¤ºäº†å½“å‰æ­¥éª¤çº§éªŒè¯å™¨çš„å±€é™æ€§ï¼Œä¸ºæ”¹è¿›LLMæ¨ç†ç³»ç»Ÿçš„éªŒè¯èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>
