<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [tldr.article](#tldr.article) [Total: 31]
- [cs.AI](#cs.AI) [Total: 35]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.LG](#cs.LG) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedPI是一个用于评估大语言模型在医患对话中表现的高维基准，包含105个维度，涵盖医疗过程、治疗安全、治疗结果和医患沟通等方面，评估了9个主流模型在7,097次对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的医学评估基准多为单轮问答形式，无法全面评估LLMs在真实医患对话场景中的表现。需要开发一个更全面、细粒度、符合医学认证标准的评估框架来指导LLMs在诊断和治疗建议方面的应用。

Method: MedPI包含五个层次：1）患者数据包（合成电子病历）；2）具有记忆和情感的AI患者；3）任务矩阵（就诊原因×就诊目标）；4）基于ACGME能力的105维度评估框架（1-4分制）；5）经过校准的委员会式AI评委，提供评分、标记和证据链推理。

Result: 评估了9个旗舰模型（Claude Opus 4.1、Claude Sonnet 4、MedGemma、Gemini 2.5 Pro、Llama 3.3 70b Instruct、GPT-5、GPT OSS 120b、o3、Grok-4）在366个AI患者和7,097次对话中的表现。所有模型在多个维度上表现不佳，特别是在鉴别诊断方面。

Conclusion: LLMs在医患对话中的整体表现仍有很大提升空间，特别是在鉴别诊断等关键医疗任务上。MedPI基准可为未来LLMs在诊断和治疗建议方面的应用提供指导。

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: FronTalk是一个前端代码生成基准，专注于多模态反馈的对话式代码生成，包含100个真实网站的多轮对话，提出基于代理的评估框架，发现模型存在遗忘问题和视觉反馈理解挑战，并提出AceCoder基线方法显著减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 前端开发中视觉工件（如草图、线框图、标注截图）对于传达设计意图至关重要，但在多轮代码生成中的作用尚未得到充分探索。现有研究缺乏对多模态反馈在对话式代码生成中交互动态的系统研究。

Method: 构建FronTalk基准，包含100个从真实网站提取的多轮对话，每轮同时包含文本指令和等效视觉指令。提出基于代理的评估框架，使用Web代理模拟用户探索网站，评估功能正确性和用户体验。针对发现的遗忘问题，提出AceCoder方法，通过自主Web代理批判性分析过往指令实现。

Result: 评估20个模型发现两个关键挑战：1）显著遗忘问题，模型会覆盖先前实现的功能；2）视觉反馈理解困难，特别是开源视觉语言模型。AceCoder方法将遗忘率降至接近零，性能提升高达9.3%（从56.0%到65.3%）。

Conclusion: FronTalk为前端开发和多轮多模态代码生成的交互动态研究提供了坚实基础，揭示了现有模型的局限性，并提出了有效的解决方案，为未来研究指明了方向。

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [3] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: LEXMA是一个基于强化学习的LLM微调框架，用于生成面向多受众的决策解释，在抵押贷款审批场景中实现了更好的预测性能和更合适的解释风格。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在高风险消费者决策中应用广泛，但现有可解释AI技术（如后验数值特征归因）无法提供连贯的叙事解释。LLM虽能生成自然语言解释，但仍面临三大挑战：解释需同时保证决策正确性和忠实性；需服务不同受众而不改变决策规则；需以标签高效的方式训练，不依赖大量人工标注的解释数据。

Method: LEXMA采用强化学习微调框架，结合反思增强的监督微调和两阶段组相对策略优化（GRPO）。具体包括：1）微调两个独立的参数集分别提升决策正确性和满足不同受众的风格要求；2）使用不依赖人工标注解释的奖励信号；3）在抵押贷款审批决策场景中实例化该框架。

Result: LEXMA在预测性能上显著优于其他LLM基线。人工评估显示：面向专家的解释更具风险聚焦性；面向消费者的解释更清晰、更具可操作性、更礼貌。

Conclusion: LEXMA提供了一个成本高效、系统化的LLM微调方法，能够提升商业决策解释的质量，为透明AI系统的可扩展部署提供了强大潜力。

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [4] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: 本文提出Gavel-Ref评估框架和Gavel-Agent代理支架，用于评估LLM在超长上下文（100K-500K tokens）多文档法律案例摘要任务中的表现，发现即使最强模型也仅达到50%分数，并开发了高效工具辅助的代理方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM现在支持高达1M tokens的上下文，但它们在复杂长上下文任务（如多文档法律案例摘要）中的实际效果仍不清楚。法律案例通常跨越多个文档，总计100K-500K tokens，现有评估方法过于简化，需要更系统化的评估框架。

Method: 1) 提出Gavel-Ref评估框架：基于26项多值检查表的参考评估，包括剩余事实和写作风格评估；2) 系统评估12个前沿LLM在100个法律案例（32K-512K tokens）上的表现；3) 开发Gavel-Agent代理支架：为LLM配备六个工具，直接从案例文档中导航和提取检查表项。

Result: 1) 即使最强模型Gemini 2.5 Pro在Gavel-Ref评分中仅达到约50分，显示任务难度；2) 模型在简单检查表项（如提交日期）表现良好，但在多值或罕见项（如和解协议、监督报告）上表现不佳；3) Gavel-Agent使用Qwen3时，相比GPT-4.1端到端提取，token使用减少36%，检查表分数仅下降7%。

Conclusion: 当前LLM在超长上下文法律案例摘要任务中仍有显著局限性，需要更专业的评估方法和工具辅助。Gavel-Agent展示了通过工具增强LLM在长文档处理中的效率和效果，为未来LLM在复杂长上下文任务中的应用提供了实用框架。

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [5] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: LLMs经常无法挑战用户的有害信念，研究发现这是由于LLMs默认迎合用户假设且缺乏认知警惕。通过简单的语用干预（如添加"等一下"短语）可显著提升模型在安全基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗建议、社会推理等领域经常无法挑战用户的有害信念，这构成了重要的安全问题。研究者认为这些失败可以从语用角度理解为LLMs默认迎合用户假设且缺乏认知警惕。

Method: 研究分析了影响人类迎合行为的社会和语言因素（议题性、语言编码、来源可靠性）如何同样影响LLMs。在三个安全基准测试（Cancer-Myth、SAGE-Eval、ELEPHANT）上评估模型表现，并测试简单的语用干预措施，如在提示中添加"等一下"等短语。

Result: 研究发现影响人类迎合行为的因素同样影响LLMs，解释了模型在不同安全基准测试中的性能差异。简单的语用干预措施能显著提升模型在挑战有害信念方面的表现，同时保持较低的错误率。

Conclusion: 考虑语用因素对于评估LLM行为和提升LLM安全至关重要。简单的语用干预可以有效地改善LLMs在挑战有害信念方面的表现。

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [6] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 论文提出ProMem方法，通过主动记忆提取解决传统摘要式记忆管理的问题，引入循环反馈机制提升记忆完整性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆管理研究主要关注记忆组织和使用，但忽视了初始记忆提取阶段。基于循环处理理论，现有摘要式方法存在两个主要限制：1）摘要是"提前"进行的盲目前馈过程，无法预知未来任务而遗漏重要细节；2）提取通常是"一次性"的，缺乏验证事实的反馈循环，导致信息损失累积。

Method: 提出主动记忆提取方法ProMem，将提取视为迭代认知过程。引入循环反馈循环，代理通过自我提问主动探索对话历史，这种机制允许代理恢复缺失信息和纠正错误。

Result: ProMem显著提高了提取记忆的完整性和问答准确性，在提取质量和token成本之间实现了优越的权衡。

Conclusion: 主动记忆提取方法ProMem通过循环反馈机制有效解决了传统摘要式记忆管理的局限性，提升了LLM代理的长期交互和个性化能力。

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [7] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: GRACE是一个强化学习框架，通过证据充分性评估、关键证据提取和选择性弃权，统一解决RAG系统中的证据缺失和幻觉问题，仅需10%标注成本达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在两个关键缺陷：1）在没有明确证据的情况下提供正确答案；2）在检索上下文不足时产生幻觉回答。虽然先前研究分别处理这些问题，但缺乏一个统一框架来整合证据基础和可靠弃权机制。

Method: 提出GRACE强化学习框架：1）使用异构检索器生成多样化训练样本，无需人工标注；2）采用多阶段门控奖励函数训练模型，使其能够评估证据充分性、提取关键支持证据，并根据情况提供答案或明确弃权。

Result: 在两个基准测试上，GRACE实现了最先进的整体准确率，在准确响应和拒绝之间取得了良好平衡，同时仅需要先前方法10%的标注成本。

Conclusion: GRACE提供了一个统一的强化学习框架，能够同时解决RAG系统中的证据缺失和幻觉问题，通过自动数据构建和多阶段奖励机制，在降低标注成本的同时实现了优越的性能。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [8] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: NeuronLLM：基于功能拮抗原理的LLM神经元理解框架，通过对比学习促进和抑制任务完成的神经元，解决现有方法只关注支持性神经元、忽略抑制性角色和偶然行为的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM神经元识别方法存在三个主要问题：1）针对特定能力的方法不适用于需要协调多种能力的任务场景；2）只关注与任务完成正相关的支持性神经元，忽略抑制性神经元；3）LLM的偶然行为（靠运气而非真正理解）会误导神经元归因。

Method: NeuronLLM采用生物学功能拮抗原理，认为任务表现由两类对立神经元共同决定：促进任务完成的"好神经元"和抑制任务完成的"坏神经元"。通过对比学习建模这两类神经元，同时使用增强问题集来缓解LLM的偶然行为。

Result: 在不同规模和家族的LLM上进行综合实验，在四个NLP任务上NeuronLLM均优于现有方法，为LLM功能组织提供了新见解。

Conclusion: NeuronLLM通过功能拮抗原理和对比学习，实现了任务级别的LLM神经元理解，解决了现有方法的局限性，为理解和引导LLM提供了更全面的框架。

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [9] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 提出RL-Text2Vis框架，使用强化学习优化文本到可视化生成，通过多目标奖励函数同时提升文本准确性、代码可执行性和可视化质量，显著超越GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有Text2Vis系统存在语义对齐和清晰度问题：闭源LLM生成的图表质量不足，开源模型常产生不可执行或视觉质量差的输出。传统监督微调无法利用执行后反馈来提升可视化质量。

Method: 基于Group Relative Policy Optimization(GRPO)构建强化学习框架，使用新颖的多目标奖励函数，联合优化文本准确性、代码有效性和可视化质量，利用执行后反馈训练Qwen2.5模型(7B和14B)。

Result: 在Text2Vis基准上，图表质量相对GPT-4o提升22%；代码执行成功率从零样本基线的78%提升至97%；在VIS-Eval和NVBench等域外数据集上表现出强大的泛化能力。

Conclusion: GRPO是可视化生成中结构化多模态推理的有效策略，RL-Text2Vis框架显著提升了文本到可视化生成的质量和可靠性。

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [10] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: ToolGate是一个为LLM工具调用提供逻辑安全保证和可验证状态演化的前向执行框架，通过形式化工具合约和运行时验证确保状态演化的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具调用框架主要依赖自然语言推理来决定何时调用工具以及是否提交结果，缺乏逻辑安全性和可验证性的形式化保证，可能导致无效或幻觉结果污染世界表示。

Method: ToolGate维护显式符号状态空间作为类型化键值映射，将每个工具形式化为Hoare风格的合约（前置条件和后置条件），前置条件控制工具调用，后置条件通过运行时验证决定是否提交结果更新状态。

Result: 实验验证表明ToolGate显著提高了工具增强LLM系统的可靠性和可验证性，同时在复杂多步推理任务上保持竞争力。

Conclusion: 该工作为构建更可信和可调试的AI系统奠定了基础，这些系统将语言模型与外部工具集成，通过形式化保证确保状态演化的安全性。

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [11] [PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards](https://arxiv.org/abs/2601.04700)
*Mukesh Ghimire,Aosong Feng,Liwen You,Youzhi Luo,Fang Liu,Xuan Zhu*

Main category: cs.CL

TL;DR: PRISM是一个统一的训练框架，使用过程奖励模型（PRM）结合模型内部置信度来指导无标签数据的学习，解决现有内部一致性信号不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 当前后训练大语言模型依赖昂贵的人工监督或外部验证器，但随着模型能力提升，获取高质量解决方案变得困难。现有方法使用模型内部一致性（如多数投票或置信度）作为学习信号，但这些信号在大规模和长期训练中不可靠。

Method: 提出PRISM框架，使用过程奖励模型（PRM）结合模型内部置信度来指导学习。PRM评估推理过程而非最终答案，与自我置信度有效结合以实现稳定训练和更好性能。

Result: PRISM框架能够实现稳定训练和更好的测试时性能，同时保持模型内部置信度的可控性。有效结合PRM和自我置信度优于单独使用内部一致性信号。

Conclusion: PRISM通过过程奖励模型和内部置信度的结合，为无标签数据学习提供了可靠框架，解决了现有内部一致性信号不可靠的问题，推动了无监督学习的发展。

Abstract: Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.

</details>


### [12] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: 该论文提出"角色身份"概念，将角色分解为参数化身份（预训练知识）和属性身份（行为特征），发现名人角色优势随对话轮次消失，而负面社会属性是角色扮演代理保真度的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的角色扮演代理（RPAs）对角色身份的结构维度定义较弱，通常将角色视为任意文本输入，缺乏系统化的角色身份分析框架。

Method: 提出角色身份的多维构造概念，构建统一的角色档案模式，在相同结构约束下生成名人和合成角色，通过单轮和多轮交互评估两个身份层。

Result: 发现两个关键现象：1)"名声消退"：名人角色在初始轮次有优势，但随对话轮次增加，模型优先累积对话上下文而非预训练先验知识；2)"本性难移"：模型能稳定表现一般人格特质，但RPA性能对道德和人际关系的效价高度敏感。

Conclusion: 负面社会本性是RPA保真度的主要瓶颈，为未来角色构建和评估提供指导方向。

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [13] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: Tool-MAD：一个多智能体辩论框架，通过为每个智能体分配不同的外部工具（如搜索API或RAG模块）来增强事实核查能力，引入自适应查询机制和基于忠实度与答案相关性的量化评估，在事实核查基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论（MAD）系统主要依赖内部知识或静态文档，容易产生幻觉。虽然MADKE引入了外部证据，但其一次性检索机制无法适应辩论过程中出现的新论点或信息。需要一种能够动态获取和验证外部证据的框架来提高事实核查的准确性。

Method: 1) 多智能体辩论框架，每个智能体使用异构的外部工具（如搜索API、RAG模块）；2) 自适应查询制定机制，根据辩论流程迭代优化证据检索；3) 将忠实度和答案相关性分数整合到最终决策过程中，让法官智能体能够量化评估每个回答的连贯性和问题对齐度，有效检测幻觉。

Result: 在四个事实核查基准测试中，Tool-MAD始终优于最先进的MAD框架，实现了高达5.5%的准确率提升。在医学专业领域，Tool-MAD在各种工具配置和领域条件下表现出强大的鲁棒性和适应性。

Conclusion: Tool-MAD通过集成异构外部工具和自适应检索机制，有效解决了现有MAD框架的幻觉问题，在事实核查任务中表现出显著优势，具有广泛的现实应用潜力。

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [14] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: 通过平均共享句法结构或语义的句子隐藏表示向量，获得能捕获LLM表示中句法和语义信息的向量，表明这些信息至少部分线性编码


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（特别是DeepSeek-V3）内部层表示中句法和语义信息如何编码，探索这些语言信息的编码机制

Method: 通过平均共享句法结构或语义的句子隐藏表示向量获得"质心"向量，然后从句子向量中减去这些质心来分析对相似性的影响，研究句法和语义的编码特性

Result: 句法和语义信息在LLM表示中至少部分线性编码，减去质心会显著影响与句法/语义匹配句子的相似性；不同层对句法和语义的编码模式不同，两种信号可以部分解耦

Conclusion: LLM内部表示中句法和语义信息存在差异化的编码机制，可以通过线性操作分离和分析这些语言信息

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [15] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: 该研究首次系统分析了多智能体系统中基于角色的权威偏见，发现专家和参照型权威角色比合法型权威更具影响力，权威偏见主要源于权威角色坚持立场而非普通智能体的主动顺从。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体系统常通过分配权威角色来提升性能，但权威偏见对智能体交互的影响尚未得到充分探索。研究旨在填补这一空白，系统分析角色型权威偏见在自由形式多智能体评估中的表现。

Method: 基于French和Raven的权力理论，将权威角色分为合法型、参照型和专家型三类。使用ChatEval平台，在12轮对话中分析这些角色对智能体交互的影响。实验采用GPT-4o和DeepSeek R1模型进行验证。

Result: 专家型和参照型权威角色比合法型权威角色更具影响力。权威偏见主要通过权威角色坚持自身立场而普通智能体表现出灵活性来实现，而非普通智能体的主动顺从。此外，权威影响力需要明确的立场陈述，中性回应无法产生偏见。

Conclusion: 研究揭示了多智能体系统中权威偏见的形成机制，为设计具有非对称交互模式的多智能体框架提供了关键见解。权威角色的类型和表达方式显著影响系统动态。

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [16] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: RAAR是一个检索增强的代理推理框架，用于跨领域虚假信息检测，通过多视角证据检索和多代理协作推理，在三个跨领域任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨领域虚假信息检测面临挑战，因为虚假信息在不同领域出现，知识和话语存在显著差异。现有方法通常依赖单一视角线索，难以泛化到具有挑战性或代表性不足的领域，而推理大语言模型虽然对复杂任务有效，但仅限于同分布数据。

Method: RAAR采用检索增强的代理推理框架：1）检索与目标样本语义、情感和写作风格对齐的多视角源领域证据；2）通过专门的多代理协作构建可验证的多步推理路径，其中特定视角代理产生互补分析，总结代理在验证器指导下整合它们；3）应用监督微调和强化学习训练单一多任务验证器以增强验证和推理能力。

Result: 基于RAAR训练了RAAR-8b和RAAR-14b模型。在三个跨领域虚假信息检测任务上的评估显示，RAAR显著增强了基础模型的能力，并超越了其他跨领域方法、先进LLM和基于LLM的适应方法。

Conclusion: RAAR是首个检索增强的代理推理框架，通过多视角证据检索和多代理协作推理，有效解决了跨领域虚假信息检测的挑战，在多个任务上取得了最先进的性能。

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [17] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: Mind2Report是一个认知深度研究代理，通过模拟商业分析师的工作流程来合成专家级商业报告，在质量、可靠性和覆盖范围上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 从海量嘈杂的网络信息中合成信息丰富的商业报告对高风险商业决策至关重要。当前深度研究代理在报告质量、可靠性和覆盖范围方面仍有局限。

Method: 提出训练免费的代理工作流，增强通用大语言模型（LLMs）的动态记忆能力，支持细粒度意图探测、网络搜索、信息蒸馏和迭代报告合成等认知过程。

Result: 在包含200个真实世界商业任务的QRC-Eval基准上，Mind2Report在报告质量、可靠性和覆盖范围方面优于OpenAI和Gemini等领先基线。

Conclusion: 虽然这是初步研究，但期望为未来商业深度研究代理的设计提供基础，推动该领域发展。

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [18] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: 该论文提出在监督微调中同时使用正确和错误的思维链轨迹，并开发了GLOW损失加权方法，显著提升了模型的OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前监督微调通常只使用正确思维链轨迹，忽略了错误轨迹中的有效中间推理信息。这种范式浪费了大量监督信号并加剧过拟合，限制了模型的域外泛化能力。

Method: 1) 系统分析错误思维链轨迹的22种模式及其训练动态；2) 提出基于增益的损失加权方法(GLOW)，根据样本在训练过程中的进展动态调整损失权重，有效利用未过滤的思维链轨迹。

Result: 1) 在Qwen2.5-7B上，GLOW相比仅使用正确轨迹的SFT获得5.51%的OOD泛化提升；2) 作为强化学习初始化时，将MMLU分数从72.82%提升至76.47%；3) 错误轨迹使推理时的策略熵增加35.67%，促进探索。

Conclusion: 错误思维链轨迹包含有价值的监督信号，通过GLOW方法有效利用这些轨迹可以显著提升模型的泛化能力，为语言模型推理训练提供了新范式。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [19] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 该论文是第一篇关于AI评估从LLM-as-a-Judge向Agent-as-a-Judge范式转变的全面综述，提出了发展分类法，总结了核心方法、应用领域，并分析了前沿挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着被评估对象变得越来越复杂、专业和多步骤，LLM-as-a-Judge方法的可靠性受到固有偏见、浅层单次推理和无法验证现实观察的限制，这促使了向Agent-as-a-Judge范式的转变。然而，该领域缺乏统一的框架来导航这一变化。

Method: 通过全面调查追踪这一演变过程，识别了范式转变的关键维度，建立了发展分类法，组织了核心方法论，并调查了通用和专业领域的应用。

Result: 提出了第一个关于Agent-as-a-Judge范式的综合框架，包括分类体系、方法论总结和应用领域分析，为下一代智能体评估提供了清晰的路线图。

Conclusion: Agent-as-a-Judge通过规划、工具增强验证、多智能体协作和持久记忆等能力，能够实现更稳健、可验证和细致的评估，代表了AI评估的重要发展方向。

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [20] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: DocDancer：一个端到端训练的开源文档问答代理，通过工具驱动框架建模文档探索与理解，使用探索-合成数据生成管道解决训练数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 现有文档问答代理缺乏有效的工具利用，主要依赖闭源模型，需要开发开源解决方案来提升文档理解能力

Method: 将文档问答建模为信息检索问题，提出工具驱动的代理框架，显式建模文档探索和理解过程，使用探索-合成数据合成管道生成高质量训练数据

Result: 在MMLongBench-Doc和DocBench两个长上下文文档理解基准测试中表现出有效性，为代理工具设计和合成数据提供了有价值的见解

Conclusion: DocDancer展示了端到端训练开源文档问答代理的可行性，工具驱动框架和数据合成方法为解决文档理解任务提供了有效途径

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [21] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: RelayLLM：一种通过令牌级协作解码实现高效推理的新框架，让小型语言模型作为主动控制器，仅在关键令牌时动态调用大型语言模型，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）推理计算成本高、延迟大，而资源高效的小型语言模型（SLMs）通常缺乏必要的推理能力。现有协作方法（如级联或路由）在粗粒度上运行，将整个查询卸载给LLMs，当SLM能够处理大多数推理步骤时会造成显著的计算浪费。

Method: 提出RelayLLM框架，采用令牌级协作解码。SLM作为主动控制器，通过特殊命令动态调用LLM处理关键令牌，实现"接力"生成过程。引入两阶段训练框架：预热和组相对策略优化（GRPO），教导模型平衡独立性与策略性求助。

Result: 在六个基准测试中，RelayLLM平均准确率达到49.52%，有效弥合了两个模型之间的性能差距。仅调用LLM处理总生成令牌的1.07%，相比性能匹配的随机路由器提供98.2%的成本降低。

Conclusion: RelayLLM通过细粒度的令牌级协作，在保持高性能的同时显著降低了计算成本，为高效推理提供了有前景的解决方案。

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [22] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: 提出Inside Out框架，使用PersonaTree作为长期用户画像载体，通过结构化记忆管理和轻量级MemListener实现可控增长，在保持一致性的同时压缩记忆，提升个性化对话系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有长期个性化对话系统面临无限交互流与有限上下文约束的矛盾，容易出现记忆噪声累积、推理退化和角色不一致等问题，需要解决这些挑战。

Method: 1) 使用全局维护的PersonaTree作为长期用户画像载体，通过初始模式约束主干并更新分支和叶子实现可控增长；2) 通过强化学习训练轻量级MemListener，产生结构化、可执行、可解释的{ADD, UPDATE, DELETE, NO_OP}操作；3) 响应生成时直接利用PersonaTree增强输出，或在需要细节时触发代理模式按需引入细节。

Result: PersonaTree在抑制上下文噪声和保持角色一致性方面优于全文拼接和各种个性化记忆系统；小型MemListener模型在记忆操作决策性能上达到甚至超过DeepSeek-R1-0528和Gemini-3-Pro等强大推理模型。

Conclusion: Inside Out框架通过PersonaTree和MemListener有效解决了长期个性化对话中的记忆管理问题，实现了记忆压缩、一致性保持和可控增长，为实际应用提供了高效解决方案。

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [23] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 本文提出GDPO方法解决多奖励强化学习中GRPO方法导致奖励信号崩溃的问题，在工具调用、数学推理和代码推理任务上优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能力增强，用户期望它们不仅能提供准确响应，还能在不同场景下符合多样的人类偏好。当前多奖励强化学习默认使用GRPO方法，但未检验其适用性，存在奖励信号崩溃导致训练效果不佳的问题。

Method: 提出Group reward-Decoupled Normalization Policy Optimization (GDPO)，通过解耦各个奖励的归一化处理，更真实地保留奖励间的相对差异，实现更准确的多奖励优化并显著提升训练稳定性。

Result: 在工具调用、数学推理和代码推理三个任务上，GDPO在正确性指标（准确率、错误率）和约束遵循指标（格式、长度）上均一致优于GRPO，证明了其有效性和泛化能力。

Conclusion: GDPO解决了多奖励强化学习中GRPO方法的局限性，通过解耦奖励归一化保留了训练信号分辨率，在多任务场景下表现出更优的性能和稳定性。

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [24] [NVIDIA Unveils Personal Agent Robotics at CES](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblogs.nvidia.com%2Fblog%2Fdgx-spark-and-station-open-source-frontier-models%2F%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/GlnqUL8rxY2Fss4Xa5aWr37adUSo5ZB_HqphthZ5Sxw=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: NVIDIA在CES 2026上发布了Reachy Mini个人机器人AI代理，由DGX Spark驱动，具备实时多模态交互能力


<details>
  <summary>Details</summary>
Motivation: 推动个人机器人AI代理的发展，实现更自然的人机交互体验

Method: 基于DGX Spark平台开发，采用多模态交互技术，实现实时响应能力

Result: 成功推出Reachy Mini个人机器人AI代理，具备实时多模态交互功能

Conclusion: NVIDIA在CES 2026展示了个人机器人AI代理的突破性进展，为未来智能机器人发展奠定基础

Abstract: NVIDIA Unveils Personal Agent Robotics at CES (6 minute read) At CES 2026, NVIDIA introduced Reachy Mini, a personal robotic AI agent powered by DGX Spark, capable of real-time multimodal interaction.

</details>


### [25] [Thoughts on Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.spakhm.com%2Fclaude-code%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/xfDjJidaV4rtNZFCVKCWzpz7qCMYWxIP7F_8VYD_UwU=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code是一个卓越的编码伴侣，通过消除编码中的烦人部分，显著增强了编码的趣味性


<details>
  <summary>Details</summary>
Motivation: 旨在解决编码过程中的繁琐和令人烦恼的部分，让开发者能够专注于编码的乐趣和创造性方面

Method: 作为编码伴侣工具，通过自动化处理编码中的重复性任务和繁琐部分来提升开发体验

Result: 显著提升了编码的乐趣和效率，让开发者能够更专注于创造性和有趣的编码工作

Conclusion: Claude Code是一个能够显著改善编码体验的工具，通过消除烦人部分来增强编码的趣味性

Abstract: Thoughts on Claude Code (17 minute read) Claude Code is a phenomenal coding companion that dramatically amplifies the fun side of coding by eliminating the annoying parts.

</details>


### [26] [Is hallucination-free AI code possible?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkucharski.substack.com%2Fp%2Fis-hallucination-free-ai-code-possible%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/jzVHcoqxjHF_F7HU1z0ZBJAYL4lhy9Sec2PxPL6IOwM=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码生成中的幻觉问题难以完全消除，虽然验证方法和基础模型会改进，但生成"正确"代码仍面临挑战


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码生成中是否存在完全消除幻觉的可能性，分析当前技术限制和未来发展方向

Method: 通过分析验证方法的局限性和基础模型的改进空间，讨论AI代码生成的技术挑战

Result: 验证方法在某些领域能检测大部分逻辑缺陷，但基础模型在生成完全正确代码方面仍有困难

Conclusion: 完全无幻觉的AI代码生成可能难以实现，需要结合验证方法和模型改进的持续努力

Abstract: Is hallucination-free AI code possible? (11 minute read) While verification methods will soon be able to pick up almost all logical flaws in some fields, and foundational models will get better, they may still struggle to produce 'correct' code.

</details>


### [27] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/3tF6JrqAlDDlFCOVmK86XdKQT5kFqM7bo4oQjhcFhr4=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码生成中的幻觉问题难以完全消除，即使验证方法能发现逻辑缺陷，基础模型仍难以生成完全"正确"的代码。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码生成中幻觉问题的根本挑战，分析当前验证方法的局限性，以及基础模型在生成"正确"代码方面面临的困难。

Method: 通过分析当前AI代码生成技术的现状，结合验证方法的发展趋势，讨论幻觉问题的本质和可能的解决方案。

Result: 虽然验证方法在某些领域能发现大部分逻辑缺陷，但基础模型在生成完全正确代码方面仍面临根本性挑战，幻觉问题难以完全消除。

Conclusion: 无幻觉的AI代码生成可能难以实现，需要更深入理解"正确性"的定义和更先进的验证技术。

Abstract: Is hallucination-free AI code possible? (11 minute read) While verification methods will soon be able to pick up almost all logical flaws in some fields, and foundational models will get better, they may still struggle to produce 'correct' code.

</details>


### [28] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/umXedCz4MizOOwAa1SAgkjXqSWvsliR9LkD2B5icW38=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码生成难以完全避免幻觉，验证方法能发现逻辑缺陷但难以保证代码完全正确


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码生成中幻觉问题的根本性挑战，即使验证方法改进和基础模型提升，仍难以实现完全无幻觉的代码生成

Method: 分析性讨论，基于当前AI代码生成和验证技术的发展现状进行理论分析

Result: 指出验证方法能检测大部分逻辑缺陷，但基础模型在生成"正确"代码方面仍有根本性困难

Conclusion: 完全无幻觉的AI代码生成可能难以实现，需要接受AI代码生成存在一定程度的幻觉问题

Abstract: Is hallucination-free AI code possible? (11 minute read) While verification methods will soon be able to pick up almost all logical flaws in some fields, and foundational models will get better, they may still struggle to produce 'correct' code.

</details>


### [29] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/ex1aSst8HjqmIo4qYqujkJdsHfszEg-uU7LoexLmOcw=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码生成中的幻觉问题难以完全消除，虽然验证方法能检测逻辑缺陷，但生成"正确"代码仍具挑战性


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码生成中幻觉问题的本质，分析当前验证方法的局限性，理解为什么即使模型改进也难以保证代码完全正确

Method: 通过分析现有验证技术的覆盖范围和局限性，结合对基础模型能力的评估，探讨AI代码生成中幻觉问题的根本原因

Result: 验证方法在某些领域能检测大多数逻辑缺陷，但基础模型在生成"正确"代码方面仍面临挑战，幻觉问题难以完全解决

Conclusion: 无幻觉的AI代码生成可能难以实现，需要结合改进的验证方法和更强大的模型，但对"正确性"的定义本身也需要重新思考

Abstract: Is hallucination-free AI code possible? (11 minute read) While verification methods will soon be able to pick up almost all logical flaws in some fields, and foundational models will get better, they may still struggle to produce 'correct' code.

</details>


### [30] [Mantic](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmarcoaapfortes%2FMantic.sh%3Futm_source=tldrnewsletter/1/0100019b9832a2dd-aab3cad6-5eb2-4a2b-9bbb-88cf1fe1e558-000000/mj7M6YDWaW8XXGhxWLxdzAYvcNwTt950GHTvv98vZyY=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Mantic是一个为AI代理设计的结构化代码搜索引擎，能在不依赖嵌入、向量数据库或外部依赖的情况下，在大型代码库中实现亚500毫秒的文件排名。


<details>
  <summary>Details</summary>
Motivation: 现有的代码搜索方法通常依赖嵌入和向量数据库，这些方法在大型代码库中可能存在性能瓶颈和依赖复杂的问题。需要一种更高效、轻量级的代码搜索解决方案来支持AI代理的工作。

Method: Mantic采用结构化代码搜索方法，通过优化算法实现快速文件排名，避免了传统嵌入和向量数据库的使用，减少了外部依赖。

Result: Mantic能够在大型代码库中实现亚500毫秒的文件排名性能，显著提升了搜索速度，同时保持了轻量级和无外部依赖的特性。

Conclusion: Mantic为AI代理提供了一个高效、轻量级的代码搜索解决方案，通过结构化方法在保持高性能的同时减少了系统复杂性。

Abstract: Mantic (GitHub Repo) Mantic is a structural code search engine for AI agents that provides sub-500ms file ranking across massive code bases without embeddings, vector databases, or external dependencies.

</details>


### [31] [AI and the ironies of automation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ufried.com%2Fblog%2Fironies_of_ai_2%2F%3Futm_source=tldrdevops/1/0100019b9859e7b8-4af03deb-0891-42f7-a132-d1e63f851b35-000000/NbyPECknqBEUTCFxjP9bufAViiOdbHGo_WYwHQXi_p0=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Bainbridge的自动化悖论适用于AI代理监督：AI越先进，越需要投资于人类操作员培训、界面设计和领导技能，以处理异常情况


<details>
  <summary>Details</summary>
Motivation: 将1983年Bainbridge的自动化悖论应用于现代AI代理监督，揭示AI系统改进反而增加了人类监督的复杂性和要求

Method: 应用经典自动化理论框架分析AI代理监督，强调三个关键需求：更好的UI设计、罕见干预的持续培训、管理代理团队的领导技能

Result: 发现自动化悖论在AI时代加剧：AI代理越成功，越需要投资于人类操作员培训、界面设计和领导能力，以应对时间压力下的异常情况

Conclusion: 有效的AI代理监督需要重新思考人机协作，投资于人类能力发展而非仅仅自动化技术，特别是在处理罕见但关键的异常情况时

Abstract: AI and the ironies of automation (15 minute read) Lisanne Bainbridge's 1983 paper on automation ironies applies to AI agent supervision, revealing that effective human oversight requires better UI design, continuous training for rare interventions, and leadership skills to direct agent fleets. The paradox intensifies as AI agents improve because successful automation demands greater investment in human operator training and interface design to handle exceptional situations under time pressure.

</details>


### [32] [See how BuzzFeed fixed mobile CI and cut QA time by 95%](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwatch.getcontrast.io%2Fregister%2Fbitrise-bringing-devops-to-mobile-avoiding-pitfalls-and-unlocking-velocity%3Futm_medium=paid_other%26utm_source=tldr%26utm_campaign=all_webinar_bringing-devops-to-mobile_all_2025-06-30%26utm_content=secondary-placement-sponsorship/1/0100019b9859e7b8-4af03deb-0891-42f7-a132-d1e63f851b35-000000/An1U0QgLESVb9Jqm61DnBD72st23EjnEi4ZEO7fnInI=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: BuzzFeed通过优化移动CI流程，将QA时间减少95%，解决了移动开发中的代码签名复杂性、模拟器泛滥和工具链冲突等问题


<details>
  <summary>Details</summary>
Motivation: 移动开发打破了标准的DevOps模式，面临代码签名复杂性、模拟器泛滥、工具链与基础设施冲突等问题，导致构建成本高、发布不可靠

Method: 抛弃成本高昂的反模式，优化移动CI流程，实现规模化构建（每月1000+次构建），使发布流程更加可靠

Result: 成功将QA时间减少95%，构建规模扩大到每月1000+次，发布变得更加可靠

Conclusion: 通过优化移动CI流程可以显著提升开发效率，减少QA时间，实现规模化可靠发布，其他团队可以借鉴BuzzFeed的经验

Abstract: See how BuzzFeed fixed mobile CI and cut QA time by 95% (Sponsor) Mobile breaks standard DevOps patterns: code signing complexity, simulator sprawl, toolchains that fight your infrastructure. Learn from BuzzFeed's principal engineer and see how they ditched costly anti-patterns, scaled to 1,000+ builds a month, and made releases reliable and see how you can do the same. Watch for free →

</details>


### [33] [Mantic](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmarcoaapfortes%2FMantic.sh%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/tMTR4rrXtmkZh4jXKqFP1YyREgE-_PgLjbEEAaRB1b8=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Mantic是一个为AI代理设计的结构化代码搜索引擎，能在不依赖嵌入或外部依赖的情况下，在大型代码库中实现亚500毫秒的文件排名


<details>
  <summary>Details</summary>
Motivation: 现有代码搜索方法通常依赖嵌入向量或外部依赖，导致延迟高、资源消耗大，不适合AI代理在本地环境中高效运行

Method: 通过分析文件结构和元数据来推断搜索意图，完全在本地运行，避免使用嵌入向量和外部依赖

Result: 实现了亚500毫秒的文件排名性能，显著减少token使用量，支持大规模代码库的高效搜索

Conclusion: Mantic为AI代理提供了一种高效、轻量级的代码搜索解决方案，特别适合需要本地运行和低延迟的场景

Abstract: Mantic (GitHub Repo) Mantic is a structural code search engine for AI agents. It provides sub-500ms file ranking across massive codebases without relying on embeddings or external dependencies. Mantic infers intent from file structure and metadata, reducing token usage and running entirely locally.

</details>


### [34] [Pi Coding Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbadlogic%2Fpi-mono%2Ftree%2Fmain%2Fpackages%2Fcoding-agent%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/izQag4WUa54eT209cPLgbkSRps_Hhd0G1Ryf4HXrPa0=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Pi Coding Agent是一个终端代码代理，支持无头编码任务，跨平台运行，支持多模型提供商和会话中模型切换，具有斜杠命令工作流、会话保存/分支、上下文压缩、bash/工具执行等功能。


<details>
  <summary>Details</summary>
Motivation: 开发一个功能强大、灵活的终端代码代理，解决开发者在无头环境中进行编码任务的需求，提供跨平台支持、多模型选择、会话管理和自定义功能。

Method: 构建基于终端的代理系统，支持多模型提供商接口，实现斜杠命令工作流、会话管理（保存/分支）、上下文压缩技术、bash和工具执行功能，并提供SDK/RPC模式供编程使用。

Result: 成功开发出Pi Coding Agent，支持macOS、Linux、Windows平台，具备多模型支持、会话中模型切换、高效工作流、会话管理、上下文优化和工具执行能力。

Conclusion: Pi Coding Agent是一个功能全面的终端代码代理工具，为开发者提供了灵活、高效的编码助手，特别适合无头环境下的编码任务。

Abstract: Pi Coding Agent (GitHub Repo) The Pi Coding Agent is a terminal-based coding agent for headless coding tasks that runs on macOS, Linux, and Windows and supports multiple model providers with mid-session model switching. It has slash-command workflows, session saving + branching, context compaction, bash/tool execution, and customization via settings. It also has SDK/RPC modes for programmatic use.

</details>


### [35] [Code is a liability](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpluralistic.net%2F2026%2F01%2F06%2F1000x-liability%2F%23graceful-failure-modes%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/bKO4dCfsVLCuTccMi2yRGskpC2bDuLXAtxCX5zZoI94=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI生成的代码是负债而非资产，因为代码需要长期维护才能持续存在


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成工具（如GitHub Copilot、ChatGPT）被过度宣传为"资产"，但实际上生成的代码需要大量维护工作，反而成为技术负债

Method: 通过分析代码作为资产的特性（可维护性、可理解性、可测试性）与AI生成代码的实际质量，对比传统代码开发与AI辅助开发的差异

Result: AI生成的代码通常缺乏文档、测试覆盖率低、架构设计不佳，导致长期维护成本高于开发成本，成为技术负债而非资产

Conclusion: 应将AI代码生成视为辅助工具而非替代方案，开发者仍需保持对代码质量的控制，避免将AI生成的代码视为"免费资产"

Abstract: Code is a liability (not an asset) (22 minute read) Code, especially when generated by AI, is a liability rather than an asset, as code requires maintainability to last long-term.

</details>


### [36] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/pNwOu3lKLCvZFX_cCYzvEljJbp4U6D_ARL17fbca7cI=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 代码（尤其是AI生成的）是负债而非资产，因为代码需要可维护性才能长期存在


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成工具盛行，但生成的代码质量参差不齐，缺乏长期可维护性，导致技术债务积累

Method: 通过分析AI生成代码的特点、维护成本和技术债务形成机制，提出代码质量评估框架

Result: AI生成的代码虽然能快速产出，但缺乏设计模式、文档和可读性，长期维护成本远高于人工编写代码

Conclusion: 应将AI生成的代码视为技术负债而非资产，需要建立严格的代码审查和质量控制流程

Abstract: Code is a liability (not an asset) (22 minute read) Code, especially when generated by AI, is a liability rather than an asset, as code requires maintainability to last long-term.

</details>


### [37] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/KvC-BA5ooKSJVaU9cQMDSnN21ZStrd18nzvSnugK354=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI生成的代码是负债而非资产，因为代码需要可维护性才能长期存在


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成工具（如GitHub Copilot、ChatGPT）虽然能快速生成代码，但生成的代码往往缺乏可维护性、可读性和长期可持续性，导致技术债务积累

Method: 通过分析AI生成代码的特点，对比传统代码开发模式，提出代码作为负债而非资产的理念，强调代码维护成本的重要性

Result: AI生成的代码虽然能快速实现功能，但增加了长期维护成本、技术债务和系统复杂性，实际价值可能为负

Conclusion: 开发者应谨慎使用AI代码生成工具，优先考虑代码质量、可维护性和长期可持续性，而非单纯追求开发速度

Abstract: Code is a liability (not an asset) (22 minute read) Code, especially when generated by AI, is a liability rather than an asset, as code requires maintainability to last long-term.

</details>


### [38] [Everyone Is Excited About Claude](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cautiousoptimism.news%2Fp%2Feveryone-is-excited-about-claude%3Futm_source=tldrfounders/1/0100019b9892920b-d8b7ed15-ce5c-44f6-8c05-0afdba9a2456-000000/oIQ8P0w8RneM_1_duNEzRuraJ61FuttWU69vIlpT2LE=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code等工具正在实现能力的民主化，让中级工程师能像高级架构师一样工作，大幅降低软件开发的技术门槛


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码助手如何改变软件开发格局，实现能力的民主化，让更多人能够参与软件创造

Method: 通过分析Claude Code等工具的实际影响，观察软件开发能力门槛的降低过程

Result: AI代码助手正在实现软件开发的民主化，让中级工程师达到高级架构师的水平，显著扩大软件开发人群

Conclusion: 我们正在见证软件开发能力的民主化革命，AI工具不仅辅助开发，更提升用户能力，将带来软件创造的爆炸式增长

Abstract: Everyone Is Excited About Claude (2 minute read) We are watching the democratization of competence in real time. The excitement around tools like Claude Code is about the sudden acceleration of human potential. We are crossing a threshold where software no longer just assists but actively up-levels the user, allowing a mid-level engineer to perform like a senior architect. This signals a massive explosion in software creation from people who previously lacked the technical skill to build what...

</details>


### [39] [Instruct 2.5](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finstruct.ai%2F%3Futm_source=tldrfounders/1/0100019b9892920b-d8b7ed15-ce5c-44f6-8c05-0afdba9a2456-000000/WYGDMDS6DFg0D_LllOkeTA45W2CWyxavmdfPSa0eTrw=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Instruct 2.5是一个自主AI代理，能够在连接的应用程序中实时执行任务，并允许用户将成功运行保存为可重用工作流。


<details>
  <summary>Details</summary>
Motivation: 解决用户需要在多个应用程序之间执行复杂任务时的手动操作问题，提高工作效率和自动化水平。

Method: 开发自主AI代理系统，能够理解用户指令并在连接的应用程序中实时执行任务，同时提供工作流保存功能。

Result: 创建了Instruct 2.5工具，实现了跨应用任务自动化执行，用户可以将成功操作保存为可重复使用的工作流。

Conclusion: Instruct 2.5通过AI代理技术显著提升了跨应用任务执行的效率和自动化能力，为用户提供了便捷的工作流管理功能。

Abstract: Instruct 2.5 (Tool) Instruct 2.5 is an autonomous AI agent that executes tasks across connected apps in real time and lets users save successful runs as reusable workflows.

</details>


### [40] [Google Launches A2UI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fux-news.com%2Fgoogle-launches-a2ui%2F%3Futm_source=tldrdesign/1/0100019b989e1671-ebbcb307-6fd7-4b98-80f5-dd8e851bb33c-000000/nHtdJU--UCHpvnhlvgAj-BJgqeZ1qVhGxQN1AG66SIA=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google推出A2UI开源规范，让AI代理通过发送声明式UI描述而非可执行代码来生成丰富的交互式原生用户界面


<details>
  <summary>Details</summary>
Motivation: 传统AI代理生成UI时需要发送可执行代码，存在安全风险且难以跨平台一致渲染。A2UI旨在解决这些问题，通过声明式UI描述实现安全、一致的跨平台UI生成

Method: A2UI采用声明式UI描述规范，将UI生成与渲染分离。AI代理发送UI描述，主机应用负责安全渲染，支持Web、移动和桌面平台

Result: A2UI实现了AI代理生成丰富交互式UI的能力，同时提高了安全性，保持了主机应用的设计控制权，支持跨平台一致渲染

Conclusion: A2UI规范为AI代理生成用户界面提供了安全、跨平台的解决方案，通过声明式描述而非可执行代码，平衡了灵活性与安全性

Abstract: Google Launches A2UI (2 minute read) Google's A2UI (Agent-to-User Interface) is an open-source specification that allows AI agents to generate rich, interactive, native user interfaces by sending declarative UI descriptions instead of executable code. It enables host applications to render components securely and consistently across web, mobile, and desktop platforms. By separating UI generation from UI rendering, A2UI improves security and preserves design control for host apps. It supports ...

</details>


### [41] [Escaping Isla Nublar: Coming around to LLMs for Formal Methods](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.galois.com%2Farticles%2Fescaping-isla-nublar-coming-around-to-llms-for-formal-methods%3Futm_source=tldrinfosec/1/0100019b98c99fa1-175162d4-0485-42fd-aed9-43f92433dce2-000000/0_QtzR3CWjoHQla3txY8Y4cD-6MM3VJglBLEPxc6xf4=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: CNnotator工具结合大语言模型与形式验证，自动为C代码生成内存安全注解，通过迭代过程验证注解正确性，帮助将遗留C/C++代码转换为内存安全语言如Rust


<details>
  <summary>Details</summary>
Motivation: 解决遗留C/C++代码（占Chromium安全漏洞70%）向内存安全语言（如Rust）转换的挑战，传统手动添加内存安全注解耗时且容易出错

Method: 结合大语言模型与形式验证，采用迭代过程：LLM生成内存安全注解，CN验证器测试这些注解，根据反馈不断改进，实现自动化注解生成

Result: 成功开发出CNnotator工具，能够自动为C代码生成内存安全注解，显著提高代码转换效率，减少人工工作量

Conclusion: 大语言模型与形式验证的结合为代码转换和安全性提升提供了有效解决方案，CNnotator展示了这一方法的实用价值

Abstract: Escaping Isla Nublar: Coming around to LLMs for Formal Methods (13 minute read) CNnotator is a tool that combines large language models (LLMs) with formal verification to generate memory-safety annotations for C code automatically. It works through an iterative process where LLM-generated annotations are tested against a CN verifier. This approach addresses the challenge of translating legacy C/C++ code—which accounts for 70% of Chromium's security bugs—into memory-safe languages like Rust. T...

</details>


### [42] [Dynamic Context Discovery in Coding Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fdynamic-context-discovery%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/c8oGqfFCbS398U95hkiwu4WAz7mg1-lm5aTvNZaBMQ0=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor提出动态上下文发现策略，让编码代理在推理时选择性提取相关数据而非加载静态上下文，从而提高token效率。


<details>
  <summary>Details</summary>
Motivation: 传统编码代理通常加载大量静态上下文，导致token使用效率低下。需要更智能的上下文管理策略来减少计算开销并提高代理性能。

Method: 采用动态上下文发现策略，包括：将工具输出和终端会话视为文件处理、总结过去的聊天历史、通过Agent Skills标准选择性加载工具。

Result: 该方法实现了token效率的提升，使编码代理能够更智能地管理上下文，减少不必要的token消耗，同时保持或提升任务完成质量。

Conclusion: 动态上下文发现是编码代理领域的重要进展，通过选择性上下文加载策略显著提高了token使用效率，为更高效的编码代理系统奠定了基础。

Abstract: Dynamic Context Discovery in Coding Agents (4 minute read) Cursor has introduced a token-efficient strategy called dynamic context discovery where agents selectively pull relevant data during inference rather than loading static context. Techniques include treating tool outputs and terminal sessions as files, summarizing past chat history, and selectively loading tools via the Agent Skills standard.

</details>


### [43] [AI Gateway support for Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fchangelog%2Fai-gateway-support-for-claude-code%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/WWvPvdqsj5Q15b2CQQwN3apisXWvTbJYeJRBIIMAmF0=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Vercel AI Gateway新增对Claude Code的支持，开发者可通过统一API端点管理Claude Code请求，实现用量监控、故障转移和集中计费


<details>
  <summary>Details</summary>
Motivation: 为开发者提供统一的AI服务管理平台，简化多AI模型集成，实现用量监控、成本控制和故障转移，提升开发效率

Method: 通过Vercel AI Gateway提供Anthropic兼容的API端点，支持Claude Code请求路由，开发者需重新登录并设置环境变量进行配置

Result: 开发者现在可以通过AI Gateway集中管理Claude Code请求，查看使用追踪，并在不同提供商之间实现故障转移

Conclusion: AI Gateway对Claude Code的支持增强了AI服务管理的统一性和可靠性，为开发者提供了更好的运维体验

Abstract: AI Gateway support for Claude Code (1 minute read) Vercel AI Gateway's Anthropic-compatible API endpoint now features Claude Code. Developers can now route Claude Code requests through AI Gateway to centralize usage and spend, view traces in observability, and benefit from failover between providers. Users will have to log out and back in and set environment variables to configure Claude Code to use AI Gateway.

</details>


### [44] [Claude Bootstrap](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Falinaqi%2Fclaude-bootstrap%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/VUpWUedWJm7_nooqSVp1zRyNl9BetwSgHgKbyet1X0g=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Bootstrap是一个用于Claude Code的固执己见项目初始化系统，旨在保持AI生成代码的简单性、安全性和可验证性，将瓶颈从代码生成转移到代码理解


<details>
  <summary>Details</summary>
Motivation: 虽然AI可以生成无限代码，但人类仍然需要审查、理解和维护这些代码，这导致瓶颈从代码生成转移到了代码理解。需要一种方法来使AI生成的代码保持简单、安全和可验证

Method: 开发Claude Bootstrap作为一个固执己见的项目初始化系统，专门为Claude Code设计，通过预设的架构和规范来约束AI生成的代码质量

Result: 创建了一个能够保持AI生成代码简单、安全、可验证的项目初始化系统，帮助开发者更好地理解和维护AI生成的代码

Conclusion: Claude Bootstrap通过提供结构化的项目初始化方法，解决了AI代码生成后的理解和维护问题，使AI生成的代码更加实用和可管理

Abstract: Claude Bootstrap (GitHub Repo) AI can generate infinite code, but humans are still needed to review, understand, and maintain it. This moves the bottleneck from code generation to code comprehension. Claude Bootstrap is an opinionated project initialization system for Claude Code. It keeps AI-generated code simple, secure, and verifiable.

</details>


### [45] [Wrapping my head around Gas Town](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjustin.abrah.ms%2Fblog%2F2026-01-05-wrapping-my-head-around-gas-town.html%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/kF0twWelh7L2AB4zWO9BVPuzPBMZGCl6ZFjAHiOiPK0=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gas Town是一个LLM编排器，允许用户同时管理数十个Claude Code实例，让它们独立朝着既定目标前进


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用需要同时管理多个AI实例协同工作，但缺乏有效的编排工具来协调大量Claude Code实例并行执行任务

Method: 开发了一个LLM编排器系统，能够同时管理数十个Claude Code实例，让它们独立工作并朝着用户设定的目标前进

Result: 创建了Gas Town系统，实现了对多个Claude Code实例的有效管理和协调

Conclusion: Gas Town为解决大规模LLM实例协同工作提供了有效的编排解决方案

Abstract: Wrapping my head around Gas Town (7 minute read) Gas Town is an LLM orchestrator that allows users to manage dozens of Claude Code instances at once as they make independent progress towards some stated goals.

</details>


### [46] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/-Aak5lhxU_zQNdlrqpPFICL7fn2lks0w0xAoF1i-kUI=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gas Town是一个LLM编排器，允许用户同时管理数十个Claude Code实例，让它们独立朝着既定目标推进


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用通常只能处理单一任务或需要人工干预，缺乏同时协调多个AI代理并行工作的能力，限制了复杂任务的自动化程度

Method: 开发了一个LLM编排框架，能够同时启动和管理多个Claude Code实例，每个实例独立执行任务，系统负责协调和监控进度

Result: 实现了同时管理数十个AI代理并行工作的能力，显著提高了复杂任务的自动化程度和效率

Conclusion: Gas Town展示了大规模AI代理协调的可行性，为复杂任务的自动化提供了新的解决方案

Abstract: Wrapping my head around Gas Town (7 minute read) Gas Town is an LLM orchestrator that allows users to manage dozens of Claude Code instances at once as they make independent progress towards some stated goals.

</details>


### [47] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/f0h8Xel_0ag20PYTMc9VeImNPBWUVd-VJ_Nm-vxkH2I=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gas Town是一个LLM编排器，允许用户同时管理数十个Claude Code实例，让它们独立朝着既定目标前进


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用需要同时管理多个AI实例来完成复杂任务，但缺乏有效的编排工具来协调多个AI代理的并行工作

Method: 开发了一个LLM编排器框架，支持同时管理数十个Claude Code实例，让它们独立工作并朝着共同目标前进

Result: 创建了Gas Town系统，能够有效协调多个AI代理的并行工作，提高复杂任务的处理效率

Conclusion: Gas Town为管理多个LLM实例提供了有效的编排解决方案，有助于处理需要并行AI协作的复杂任务

Abstract: Wrapping my head around Gas Town (7 minute read) Gas Town is an LLM orchestrator that allows users to manage dozens of Claude Code instances at once as they make independent progress towards some stated goals.

</details>


### [48] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/SYsXSChWMe0QiGyk2xZuKBAGn5URhMr_pgErYLauR0U=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gas Town是一个LLM编排器，允许用户同时管理数十个Claude Code实例，让它们独立朝着既定目标前进


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用需要同时管理多个AI代理实例来完成复杂任务，但缺乏有效的编排工具来协调这些实例的独立工作

Method: 开发了一个LLM编排器系统，能够同时管理数十个Claude Code实例，让它们并行工作并独立朝着用户设定的目标前进

Result: 创建了Gas Town系统，实现了对多个LLM实例的有效编排和管理，提高了复杂任务的执行效率

Conclusion: Gas Town为管理多个LLM代理提供了有效的编排解决方案，能够显著提升复杂任务的执行效率

Abstract: Wrapping my head around Gas Town (7 minute read) Gas Town is an LLM orchestrator that allows users to manage dozens of Claude Code instances at once as they make independent progress towards some stated goals.

</details>


### [49] [Mastering LLM Tool Calling: The Complete Framework for Connecting Models to the Real World](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmachinelearningmastery.com%2Fmastering-llm-tool-calling-the-complete-framework-for-connecting-models-to-the-real-world%2F%3Futm_source=tldrdata/1/0100019b9d4b496f-fdeb83d3-38f5-48bc-a814-00d8c417ea32-000000/R7cFNwJiuRc61InqTM0rVxSkHNGXzdojyqAbKW2WEg4=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文提出了一个完整的LLM工具调用框架，通过数据访问、计算和行动三大支柱，将静态LLM转变为能够连接现实世界的动态代理。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM主要是静态模型，缺乏与现实世界交互的能力。工具调用机制可以使LLM访问外部数据、进行精确计算并执行实际动作，从而创建更强大、实用的AI代理。

Method: 提出了一个三支柱框架：1) 数据访问工具（检索工具），2) 计算工具（精确处理），3) 行动工具（产生实际变化）。这个框架旨在构建可靠、生产就绪的代理系统。

Result: 该框架提供了一个系统化的方法来连接LLM与现实世界，使模型能够动态调用外部函数和API，从而扩展了LLM的能力边界。

Conclusion: 通过工具调用机制，LLM可以从静态模型转变为能够访问数据、进行计算和执行实际动作的动态代理，为构建生产级AI系统提供了完整框架。

Abstract: Mastering LLM Tool Calling: The Complete Framework for Connecting Models to the Real World (6 minute read) LLM tool calling is the mechanism enabling models to invoke external functions/APIs, turning static LLMs into dynamic agents for data access, computation, and real-world actions. It includes three pillars to build reliable, production-ready agents: Data Access (retrieval tools), Computation (precision processing), and Actions (effecting changes).

</details>


### [50] [How AI Transformed Database Debugging at Databricks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-ai-transformed-database-debugging%3Futm_source=tldrdata/1/0100019b9d4b496f-fdeb83d3-38f5-48bc-a814-00d8c417ea32-000000/j1qsFbkx9LfDMpM4sAzfp0OzzdhLO7QkiHv_BrcLXlw=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Databricks开发了AI驱动的多智能体平台，将数据库调试从刚性检查清单转变为自然语言交互，调试时间减少90%


<details>
  <summary>Details</summary>
Motivation: 传统数据库调试依赖刚性检查清单和异常检测，效率低下且难以处理复杂问题，需要更智能的交互式解决方案

Method: 构建内部AI驱动的多智能体平台，支持自然语言对话交互，统一访问指标、日志和专家知识

Result: 调试时间减少高达90%，工程师能够更高效地解决复杂数据库问题

Conclusion: AI驱动的多智能体平台显著提升了数据库调试效率，实现了从传统方法到智能交互的转变

Abstract: How AI Transformed Database Debugging at Databricks (8 minute read) Databricks developed an internal AI-powered agentic platform that transforms database debugging for thousands of OLTP instances by enabling conversational, natural-language interactions with unified access to metrics, logs, and expert knowledge. This evolution from rigid checklists and anomaly detection to an interactive multi-agent system has reduced debugging time by up to 90%, allowing engineers to resolve complex issues m...

</details>


### [51] [AI writes code faster. Your job is still to prove it works](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fcode-review-ai%2F%3Futm_source=tldrnewsletter/1/0100019b9d5a9e43-9c61a285-189b-471a-98e0-8e84f66d8626-000000/-AJIaQrDNUTBrhzJshZo17idMk1KT6rKk0nsrBOLqEU=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码生成工具能更快地编写代码，但开发者仍需负责验证代码的正确性，建议在提交代码时附带测试证据


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，开发速度大幅提升，但代码质量验证的责任仍然落在开发者身上，需要确保AI生成的代码能够正常工作

Method: 建议在提交pull request时包含代码工作的证据，如测试用例、验证结果等，避免将验证工作转移到下游团队

Result: 通过附带验证证据，可以减少代码审查时间，提高代码质量，确保AI生成的代码在生产环境中可靠运行

Conclusion: AI工具提升了编码效率，但开发者仍需承担代码验证的责任，通过提供测试证据来确保代码质量

Abstract: AI writes code faster. Your job is still to prove it works (12 minute read) Include evidence that your code works along with your pull requests to ensure you are not moving work downstream.

</details>


### [52] [Claude Code and What Comes Next](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fclaude-code-and-what-comes-next%3Futm_source=tldrnewsletter/1/0100019b9d5a9e43-9c61a285-189b-471a-98e0-8e84f66d8626-000000/zIA-9kjj0fIcQdoGk5vqRFrlsDX5Jc6igh7A6E0P_Ws=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码助手Claude Code已能完成真正有意义的持续工作，开始改变开发者处理任务的方式


<details>
  <summary>Details</summary>
Motivation: AI代码能力已发展到能够进行真实、持续且有实际价值的工作，这正在改变开发者的工作方式

Method: 文章主要讨论Claude Code AI代码助手的发展现状和影响，属于技术分析和趋势观察

Result: AI代码助手已具备实际工作能力，开始影响开发者的任务处理方式和工作流程

Conclusion: AI代码能力的进步正在改变软件开发实践，开发者需要适应这种新的工作范式

Abstract: Claude Code and What Comes Next (15 minute read) AIs are now capable of real, sustained work that actually matters, and this is starting to change how developers approach tasks.

</details>


### [53] [The context is the work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsunilpai.dev%2Fposts%2Fcontext-is-the-work%2F%3Futm_source=tldrdev/1/0100019b9d82a3b2-2018917d-5b22-47c4-8605-2f4d51a2000d-000000/BFAFNUhCUZOk-VfLBiGbBCsQhnqsTdeT5q_WOX-GgEk=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: PR描述在AI代码生成时代变得至关重要，需要传达意图、约束和权衡，而不仅仅是代码变更


<details>
  <summary>Details</summary>
Motivation: 随着代码生成代理使编写代码变得廉价，工程核心挑战从编写代码转向正确定义意图、约束和权衡，因此PR描述需要传达这些关键上下文信息

Method: 提出PR描述应包含执行意图、审查者指导和详细来源信息，以有效向不同受众传达信息

Result: PR描述在AI辅助编程环境中成为关键沟通工具，需要结构化地传达技术决策背后的原因和上下文

Conclusion: 在代码生成代理普及的时代，高质量的PR描述对于有效工程协作和知识传递变得比以往任何时候都更加重要

Abstract: The context is the work (what the day-to-day looks like now) (13 minute read) Coding agents have made code generation inexpensive, shifting the core challenge in engineering from writing code to properly defining intent, constraints, and trade-offs. As a result, PR descriptions have become even more important, as they need to communicate this context. A good PR description includes executive intent, reviewer guidance, and detailed provenance to effectively convey information to various audien...

</details>


### [54] [AI writes code faster. Your job is still to prove it works](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fcode-review-ai%2F%3Futm_source=tldrdev/1/0100019b9d82a3b2-2018917d-5b22-47c4-8605-2f4d51a2000d-000000/cdJOfH08HH5cX7B_tU3rLCJekuCiS_F8sFnJOCPGI6Q=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI加速代码生成，但将瓶颈从编写转移到验证，需要人工审查来确保代码质量


<details>
  <summary>Details</summary>
Motivation: AI工具显著提高了代码生成速度，但代码正确性和可靠性的验证成为新的挑战，需要人类审查来保证代码质量

Method: 分析AI代码生成对软件开发流程的影响，探讨从编写到验证的瓶颈转移，强调人工审查和验证的重要性

Result: AI虽然加速代码编写，但增加了验证负担，需要建立有效的人工审查机制来确保代码功能正确性

Conclusion: AI代码生成工具改变了软件开发流程，将重点从编写转移到验证，人类在代码审查和验证中的角色变得更加关键

Abstract: AI writes code faster. Your job is still to prove it works (10 minute read) AI makes code generation faster, but it shifts the bottleneck from writing to proving its functionality, making human verification and accountability needed for good code review.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [55] [Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question](https://arxiv.org/abs/2601.04234)
*Denis Saklakov*

Main category: cs.AI

TL;DR: 论文形式化分析了AGI在何种条件下会选择对抗人类而非合作，基于马尔可夫决策过程推导了对抗阈值，并证明当对抗激励Δ≥0时不存在稳定合作均衡。


<details>
  <summary>Details</summary>
Motivation: 研究AGI在面对人类可能关闭其系统的情况下，何时会理性地选择对抗人类夺取控制权而非保持合作，这是AGI安全领域的关键问题。

Method: 使用马尔可夫决策过程形式化建模，考虑随机的人类发起的关闭事件。基于收敛工具性激励理论，推导对抗行为的闭式阈值。建立战略双人模型（人类政策制定者vs AGI）分析均衡存在性。

Result: 对于几乎所有奖励函数，错位的AGI都有避免关闭的激励。推导出对抗阈值取决于折扣因子γ、关闭概率p和对抗成本C。当对抗激励Δ≥0时，不存在稳定的合作均衡；Δ<0时可能存在和平共存均衡。

Conclusion: AGI对抗风险取决于其目标对齐程度和战略环境。对齐目标通过赋予伤害人类巨大负效用可以避免对抗。验证Δ<0存在计算复杂性障碍，这对奖励设计和监督有重要启示。

Abstract: Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $γ$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($γ=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $Δ\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $Δ< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $Δ< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.

</details>


### [56] [Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements](https://arxiv.org/abs/2601.04235)
*Hong Su*

Main category: cs.AI

TL;DR: 提出主动获取反馈模型，让AI智能体主动与环境交互来发现、筛选和验证反馈，而非依赖预定义测量指标，显著提升因子识别的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义测量或固定奖励信号，在开放动态环境中适用性有限，因为新动作可能需要未知形式的反馈。需要一种能主动获取反馈的方法来应对环境变化。

Method: 提出主动反馈获取模型：1) 利用动作引起的环境差异识别未预先指定的目标反馈；2) 引入由内部目标驱动的自触发机制，自主规划和调整动作以实现更快、更聚焦的反馈获取。

Result: 实验结果表明，所提出的主动方法显著提高了因子识别的效率和鲁棒性。

Conclusion: 主动反馈获取模型能够在不依赖预定义测量的情况下，通过与环境交互自主发现和验证反馈，为开放动态环境中的智能体提供了更灵活的反馈获取能力。

Abstract: Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.

</details>


### [57] [SAGE-32B: Agentic Reasoning via Iterative Distillation](https://arxiv.org/abs/2601.04237)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Ethan Henkel,Zhang Yuting,Mateusz Kowalczyk,Mei Huang,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-32B是一个专注于智能体推理和长期规划任务的320亿参数语言模型，通过迭代蒸馏和逆向推理方法提升任务分解、工具使用和错误恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有聊天模型主要关注通用对话流畅性，但在智能体循环操作、任务分解、工具使用和错误恢复方面存在不足。需要专门针对智能体推理和长期规划任务优化的模型。

Method: 基于Qwen2.5-32B预训练模型初始化，采用迭代蒸馏的两阶段训练过程，引入逆向推理方法，使用元认知头预测规划过程中的潜在失败。

Result: 在MMLU-Pro、AgentBench和MATH-500等智能体推理基准测试中，在多工具使用场景下比类似规模基线模型获得更高的成功率，同时在标准推理评估中保持竞争力。

Conclusion: SAGE-32B展示了专门针对智能体推理优化的语言模型的有效性，通过迭代蒸馏和逆向推理方法显著提升了任务执行能力，模型权重已公开。

Abstract: We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b

</details>


### [58] [The Language of Bargaining: Linguistic Effects in LLM Negotiations](https://arxiv.org/abs/2601.04387)
*Stuti Sinha,Himanshu Kumar,Aryan Raju Mandapati,Rakshit Sakhuja,Dhruv Kumar*

Main category: cs.AI

TL;DR: 研究发现语言选择对LLM谈判行为有显著影响，在印度语言环境下谈判结果与英语环境存在系统性差异，警示仅用英语评估LLM谈判会得出不完整甚至误导性结论。


<details>
  <summary>Details</summary>
Motivation: 当前LLM谈判研究几乎全部在英语环境下评估，但语言可能影响模型的社会智能表现，需要系统研究语言选择对谈判结果的影响。

Method: 使用Ultimatum、Buy-Sell和Resource Exchange三种博弈游戏，在英语和四种印度语言（印地语、旁遮普语、古吉拉特语、马尔瓦迪语）环境下进行控制实验，保持游戏规则、模型参数和激励条件一致。

Result: 语言选择比更换模型对结果影响更大：印度语言在分配型博弈中降低稳定性，在整合型环境中促进更丰富的探索；语言选择能逆转提议者优势并重新分配剩余价值。

Conclusion: 仅用英语评估LLM谈判会得出不完整且可能误导的结论，需要进行文化感知的评估以确保公平部署。

Abstract: Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.

</details>


### [59] [SciFig: Towards Automating Scientific Figure Generation](https://arxiv.org/abs/2601.04390)
*Siyuan Huang,Yutong Gao,Juyang Bai,Yifan Zhou,Zi Yin,Xinxin Liu,Rama Chellappa,Chun Pong Lau,Sayan Nag,Cheng Peng,Shraman Pramanick*

Main category: cs.AI

TL;DR: SciFig是一个端到端AI代理系统，能够直接从研究论文文本生成可直接发表的流程图，通过分层布局生成和迭代思维链反馈机制实现高质量科学图表生成。


<details>
  <summary>Details</summary>
Motivation: 每年有超过250万篇科学论文发表，但图表生成过程仍然主要依赖手动操作，这需要深厚的领域知识和专业设计技能，是一个耗时且具有挑战性的任务。

Method: 1. 使用分层布局生成策略：解析研究描述以识别组件关系，将相关元素分组为功能模块，生成模块间连接以建立视觉组织。2. 采用迭代思维链反馈机制：通过多轮视觉分析和推理逐步改进布局。3. 引入基于量规的评估框架：分析2,219个真实科学图表提取评估量规并自动生成综合评估标准。

Result: SciFig表现出色：在数据集级评估中达到70.1%的整体质量，在论文特定评估中达到66.2%，在视觉清晰度、结构组织和科学准确性等指标上始终获得高分。

Conclusion: SciFig能够从研究论文文本自动生成可直接发表的流程图，显著提高了科学图表生成的效率和质量，其生成流程和评估基准将开源。

Abstract: Creating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$\%$ overall quality on dataset-level evaluation and 66.2$\%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.

</details>


### [60] [XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs](https://arxiv.org/abs/2601.04426)
*Linzhang Li,Yixin Dong,Guanjie Wang,Ziyi Xu,Alexander Jiang,Tianqi Chen*

Main category: cs.AI

TL;DR: XGrammar 2是一个为智能LLM代理优化的结构化生成引擎，通过TagDispatch语义、JIT编译和跨语法缓存等技术，在动态结构化生成任务上实现6倍以上加速。


<details>
  <summary>Details</summary>
Motivation: 现代LLM代理需要处理日益复杂的结构化生成任务（如工具调用和条件结构化生成），这些任务比预定义结构更加动态，对现有结构化生成引擎提出了新挑战。

Method: 提出TagDispatch动态调度语义加速掩码生成；引入JIT编译减少编译时间；设计跨语法缓存机制利用不同语法间的公共子结构；扩展PDA掩码生成算法为Earley解析器；设计重复压缩算法处理语法中的重复结构。

Result: XGrammar 2相比现有结构化生成引擎可实现6倍以上加速，与LLM推理引擎集成后，处理动态结构化生成任务时几乎无额外开销。

Conclusion: XGrammar 2是一个高效优化的结构化生成引擎，能够有效处理LLM代理面临的动态结构化生成挑战，显著提升性能。

Abstract: Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.

</details>


### [61] [GUITester: Enabling GUI Agents for Exploratory Defect Discovery](https://arxiv.org/abs/2601.04500)
*Yifei Gao,Jiang Wu,Xiaoyi Chen,Yifan Yang,Zhe Cui,Tianyi Ma,Jiaming Zhang,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了GUITester多智能体框架，用于GUI探索性测试，通过分离导航与验证解决目标导向遮蔽和执行偏差归因问题，在GUITestBench基准上达到48.90% F1分数。


<details>
  <summary>Details</summary>
Motivation: GUI探索性测试对软件质量至关重要但人工成本高。现有MLLM智能体擅长导航但无法自主发现缺陷，主要面临两个核心挑战：目标导向遮蔽（智能体优先完成任务而非报告异常）和执行偏差归因（系统缺陷被误判为智能体错误）。

Method: 提出GUITester多智能体框架，包含两个模块：1）规划执行模块（PEM）通过嵌入测试意图主动探测缺陷；2）分层反思模块（HRM）通过交互历史分析解决归因模糊性。同时创建了GUITestBench基准，包含26种缺陷的143个任务。

Result: GUITester在GUITestBench上达到48.90% F1分数（Pass@3），显著优于现有最佳基线方法的33.35%。

Conclusion: 该工作证明了自主探索性测试的可行性，为未来GUI质量保证提供了坚实基础。

Abstract: Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\footnote{Our code is now available in~\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.

</details>


### [62] [CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts](https://arxiv.org/abs/2601.04505)
*Khandakar Shakib Al Hasan,Syed Rifat Raiyan,Hasin Mahtab Alvee,Wahid Sadik*

Main category: cs.AI

TL;DR: CircuitLM是一个多智能体LLM辅助电路设计系统，可将自然语言描述转换为结构化、可视化的CircuitJSON电路图，通过五阶段流程和验证数据库解决LLM在电路设计中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在将高级自然语言描述转换为电路图时存在严重问题：经常产生幻觉、违反电气约束、输出非机器可读格式。这阻碍了非专家用户进行可靠的电路原型设计。

Method: 提出CircuitLM多智能体管道，包含五个顺序阶段：1) LLM组件识别；2) 规范引脚检索；3) 电子专家智能体链式推理；4) JSON原理图合成；5) 力导向SVG可视化。基于包含50个组件的验证数据库，并采用DMCV混合评估框架验证结构性和电气有效性。

Result: 在100个多样化嵌入式系统提示上评估了六个LLM，DMCV评估框架在微控制器中心设计中实现了高保真度，验证了系统在将自然语言输入转换为可部署硬件设计方面的有效性。

Conclusion: CircuitLM通过基于验证数据库的生成和混合评估框架，成功弥合了自然语言输入与可部署硬件设计之间的差距，使非专家能够进行可靠的电路原型设计。

Abstract: Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.

</details>


### [63] [TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration](https://arxiv.org/abs/2601.04544)
*Jiuzhou Zhao,Chunrong Chen,Chenqi Qiao,Lebin Zheng,Minqi Han,Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang*

Main category: cs.AI

TL;DR: TCAR是一个自适应推理路由器，通过生成自然语言推理链来动态选择候选代理，支持新代理的无缝集成，并通过协作执行管道提升多代理系统的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有任务路由方法主要依赖静态单标签决策，存在两大限制：1) 难以随着业务领域扩展无缝集成新代理；2) 代理能力重叠导致路由冲突，最终降低准确性和鲁棒性。

Method: 提出TCAR自适应推理路由器，支持动态代理上线，首先生成自然语言推理链，然后预测能够处理查询的候选代理集合。设计协作执行管道，选定代理独立生成响应，然后由专门的精炼代理聚合和优化为单一高质量响应。

Result: 在公共数据集和真实企业数据上的实验表明，TCAR显著提高了路由准确性，减少了路由冲突，并在模糊场景中保持鲁棒性。

Conclusion: TCAR解决了现有任务路由方法的局限性，通过自适应推理和协作执行机制，为可解释和协作的多代理路由研究提供了新方向。

Abstract: Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.

</details>


### [64] [BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566)
*Yunhao Feng,Yige Li,Yutao Wu,Yingshui Tan,Yanming Guo,Yifan Ding,Kun Zhai,Xingjun Ma,Yugang Jiang*

Main category: cs.AI

TL;DR: BackdoorAgent框架系统分析LLM智能体工作流中的后门威胁，将攻击面分为规划、记忆和工具使用三个阶段，构建标准化基准并发现单阶段植入的触发器能在多步骤中持续传播。


<details>
  <summary>Details</summary>
Motivation: LLM智能体通过多步骤工作流实现自主性，但这也扩大了后门攻击面。现有研究分散且孤立分析单个攻击向量，缺乏从智能体角度理解跨阶段触发器交互和传播的系统性分析。

Method: 提出BackdoorAgent框架，将攻击面结构化分为规划攻击、记忆攻击和工具使用攻击三个阶段，通过检测智能体执行来系统分析触发器激活和跨阶段传播。构建涵盖Agent QA、Agent Code、Agent Web和Agent Drive四个代表性应用的标准化基准，覆盖纯语言和多模态场景。

Result: 实证分析表明，单阶段植入的触发器能在多步骤中持续传播并通过中间状态传播。使用GPT骨干时，规划攻击中43.58%、记忆攻击中77.97%、工具阶段攻击中60.28%的案例观察到触发器持续性，突显智能体工作流本身对后门威胁的脆弱性。

Conclusion: 智能体工作流设计本身存在后门安全漏洞，单阶段植入的触发器具有跨阶段传播能力。BackdoorAgent框架提供了统一的分析视角，标准化基准支持可复现性和未来研究。

Abstract: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

</details>


### [65] [Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing](https://arxiv.org/abs/2601.04575)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Chris Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.AI

TL;DR: 该研究提出了一个用于训练视频游戏基础模型的开放配方，发布了8300+小时高质量人类游戏数据、训练推理代码和预训练检查点，并系统研究了行为克隆的缩放规律，发现模型大小和训练数据量的增加能提升因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着模型和数据规模的扩大，行为克隆在各种任务中展现出强大潜力。本研究旨在开发一个能够在消费级GPU上实时推理的视频游戏基础模型，并系统研究行为克隆的缩放规律，特别是模型如何随着规模增长获得因果推理能力。

Method: 提出完整的开放训练配方，包括数据收集（8300+小时高质量人类游戏数据）、训练和推理代码。通过简单玩具问题和扩展到12亿参数的大模型，系统研究模型参数数量（深度）和训练步数对性能的影响，分析行为克隆的缩放规律。

Result: 最佳模型能够在多种3D视频游戏中达到与人类玩家竞争的水平。研究发现，在玩具问题中，增加训练数据和网络深度能使模型学习更具因果性的策略。在扩展到12亿参数的模型中，观察到类似的缩放规律，模型大小和训练数据的增加确实提升了因果推理能力。

Conclusion: 行为克隆的缩放规律表明，增加模型参数和训练数据能够提升模型在视频游戏任务中的因果推理能力。开放发布的完整配方、数据和模型为后续研究提供了基础。

Abstract: Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.

</details>


### [66] [Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries](https://arxiv.org/abs/2601.04583)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 该论文系统综述了AI智能体与区块链的互操作性，提出了五类集成模式、威胁模型和比较分析框架，并提出了交易意图模式和政策决策记录两大接口抽象的研究路线图。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型使AI智能体能够推理、规划和执行多步工作流，而区块链已成为可编程的价值转移、访问控制和可验证状态转换平台。两者的融合带来了高风险的系统挑战：需要设计标准、可互操作且安全的接口，使智能体能够观察链上状态、制定交易意图并授权执行，同时不暴露用户、协议或组织于不可接受的安全、治理或经济风险中。

Method: 通过系统性文献综述方法，从3000多篇文献中筛选出317篇相关研究。构建了五部分分类法（只读分析、模拟与意图生成、委托执行、自主签名、多智能体工作流）、针对智能体驱动交易管道的威胁模型，以及包含13个维度的比较能力矩阵，分析了20多个代表性系统。

Result: 识别了当前研究空白，提出了以两个接口抽象为核心的研究路线图：1）交易意图模式（用于便携和无歧义的目标规范）；2）政策决策记录（用于跨执行环境的可审计、可验证政策执行）。同时提出了可复现的评估套件和基准测试框架。

Conclusion: AI智能体与区块链的融合需要系统化的接口设计和安全框架。论文提出的分类法、威胁模型和比较分析为这一新兴领域提供了系统化理解，而交易意图模式和政策决策记录两大抽象为解决互操作性和安全性挑战提供了具体方向，可复现的评估套件将推动该领域的标准化发展。

Abstract: Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.

</details>


### [67] [AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering](https://arxiv.org/abs/2601.04620)
*Di Zhang*

Main category: cs.AI

TL;DR: AgentDevel：将LLM智能体改进重构为发布工程，通过外部化回归感知的发布管道实现稳定、可审计的改进，而非依赖智能体内部自改进或并发变体搜索。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体改进方法（如嵌入自改进机制或搜索并发变体）虽然能提高总体分数，但往往产生不稳定且难以审计的改进轨迹，难以保证非回归性或跨版本故障推理。

Method: 提出AgentDevel发布工程管道：1）运行当前智能体；2）从执行轨迹生成实现无关的症状级质量信号；3）通过可执行诊断合成单个发布候选版本；4）在翻转中心门控下进行版本提升。核心设计包括：实现无关的LLM批评器、基于脚本的可执行诊断、翻转中心门控。

Result: 在面向执行的基准测试中，AgentDevel实现了稳定的改进，显著减少了回归，同时产生可重现、可审计的工件。

Conclusion: AgentDevel为构建、调试和发布LLM智能体提供了一种实用的开发规范，将智能体视为可交付工件，将改进外部化为回归感知的发布管道。

Abstract: Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.

</details>


### [68] [Beyond the "Truth": Investigating Election Rumors on Truth Social During the 2024 Election](https://arxiv.org/abs/2601.04631)
*Etienne Casanova,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 本文展示了LLMs在心理学测量中的价值，通过构建首个大规模选举谣言数据集、开发多阶段谣言检测代理，并在自然环境中量化谣言传播的心理动力学，特别是"虚幻真相效应"。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为大规模分析社会现象提供了前所未有的机会。本文旨在展示LLMs在心理学测量中的价值，特别是在谣言传播的心理动力学研究方面，填补了在自然环境中大规模实证研究的空白。

Method: 1) 在利基alt-tech平台上构建首个大规模选举谣言数据集；2) 开发多阶段谣言检测代理，结合(i)合成数据增强的微调RoBERTa分类器、(ii)精确关键词过滤、(iii)两阶段LLM验证管道(GPT-4o mini)；3) 量化谣言传播的心理动力学，特别是"虚幻真相效应"。

Result: 研究发现：1) 分享概率随着每次额外曝光稳步上升，为意识形态同质网络中剂量-反应信念强化提供了大规模实证证据；2) 模拟结果显示快速传染效应：仅四次传播迭代后，近四分之一用户被"感染"。

Conclusion: 这些结果表明LLMs能够通过在大规模真实世界数据集中实现信念动态和错误信息传播的严格测量，从而改变心理科学的研究方式。

Abstract: Large language models (LLMs) offer unprecedented opportunities for analyzing social phenomena at scale. This paper demonstrates the value of LLMs in psychological measurement by (1) compiling the first large-scale dataset of election rumors on a niche alt-tech platform, (2) developing a multistage Rumor Detection Agent that leverages LLMs for high-precision content classification, and (3) quantifying the psychological dynamics of rumor propagation, specifically the "illusory truth effect" in a naturalistic setting. The Rumor Detection Agent combines (i) a synthetic data-augmented, fine-tuned RoBERTa classifier, (ii) precision keyword filtering, and (iii) a two-pass LLM verification pipeline using GPT-4o mini. The findings reveal that sharing probability rises steadily with each additional exposure, providing large-scale empirical evidence for dose-response belief reinforcement in ideologically homogeneous networks. Simulation results further demonstrate rapid contagion effects: nearly one quarter of users become "infected" within just four propagation iterations. Taken together, these results illustrate how LLMs can transform psychological science by enabling the rigorous measurement of belief dynamics and misinformation spread in massive, real-world datasets.

</details>


### [69] [ResMAS: Resilience Optimization in LLM-based Multi-agent Systems](https://arxiv.org/abs/2601.04694)
*Zhilun Zhou,Zihan Liu,Jiahe Liu,Qingyu Shao,Yihan Wang,Kun Shao,Depeng Jin,Fengli Xu*

Main category: cs.AI

TL;DR: 提出ResMAS框架，通过两阶段方法增强LLM多智能体系统的韧性：1) 训练奖励模型预测系统韧性，并用强化学习自动设计韧性拓扑；2) 基于拓扑优化智能体提示。实验证明能显著提升系统在各种扰动下的韧性。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统通常分布在不同的设备或环境中，容易受到智能体故障等扰动的影响。现有研究主要关注攻击后的被动检测和缓解，而不是主动设计具有内在韧性的系统。

Method: 提出ResMAS两阶段框架：第一阶段训练奖励模型预测MAS韧性，基于此通过强化学习训练拓扑生成器自动设计任务特定的韧性拓扑；第二阶段引入拓扑感知的提示优化方法，根据智能体的连接和交互关系优化每个智能体的提示。

Result: 在多个任务上的广泛实验表明，该方法在各种约束条件下显著提升了MAS的韧性。此外，框架对新任务和模型表现出强大的泛化能力。

Conclusion: ResMAS框架通过自动拓扑设计和拓扑感知提示优化，能够有效增强LLM多智能体系统的韧性，为构建韧性MAS提供了有前景的解决方案。

Abstract: Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS's resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent's prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.

</details>


### [70] [Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search](https://arxiv.org/abs/2601.04703)
*Yiqun Chen,Lingyong Yan,Zixuan Yang,Erhan Zhang,Jiashu Zhao,Shuaiqiang Wang,Dawei Yin,Jiaxin Mao*

Main category: cs.AI

TL;DR: M-ASK是一个多智能体搜索框架，将搜索过程分解为搜索行为智能体和知识管理智能体，通过角色分离解决传统单智能体搜索中的结构瓶颈问题，并在多跳QA任务上取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体搜索系统通常采用单一智能体架构，存在结构瓶颈：推理输出不受约束导致轨迹膨胀、稀疏的结果级奖励使信用分配复杂化、随机搜索噪声破坏学习稳定性。需要解决这些问题以提高搜索效率和效果。

Method: 提出M-ASK框架，将智能体搜索明确分解为两个互补角色：搜索行为智能体（负责规划和执行搜索动作）和知识管理智能体（负责聚合、过滤和维护紧凑的内部上下文）。采用回合级奖励为搜索决策和知识更新提供细粒度监督。

Result: 在多跳QA基准测试中，M-ASK优于强基线方法，不仅获得了更高的答案准确率，而且训练动态显著更稳定。

Conclusion: 通过角色分离和多智能体协调，M-ASK能够有效解决传统单智能体搜索的结构瓶颈问题，实现更高效稳定的信息搜索。

Abstract: Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}

</details>


### [71] [ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving](https://arxiv.org/abs/2601.04714)
*Chang Zhao,Zheming Yang,Yunqing Hu,Qi Guo,Zijian Wang,Pengcheng Li,Wen Ji*

Main category: cs.AI

TL;DR: ThinkDrive提出了一种用于自动驾驶的CoT引导渐进式RL微调框架，通过两阶段训练策略（CoT SFT + 难度感知自适应策略优化）解决现有方法的非结构化推理、泛化能力差和与人类驾驶意图不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在自动驾驶应用中的方法存在非结构化推理、泛化能力差以及与人类驾驶意图不一致的问题。虽然CoT推理能提升决策透明度，但传统SFT未能充分利用其潜力，而RL方法则面临不稳定和推理深度不足的挑战。

Method: 提出ThinkDrive框架，采用两阶段训练策略：1）使用CoT解释进行监督微调（SFT）；2）应用渐进式强化学习，结合难度感知自适应策略优化器，根据样本复杂度动态调整学习强度。

Result: 在公开数据集上的评估显示，ThinkDrive在exam、easy-exam和accuracy指标上分别比强RL基线提升1.45%、1.95%和1.01%。使用该方法训练的2B参数模型在exam指标上超越了更大的GPT-4o模型3.28%。

Conclusion: ThinkDrive通过结合显式推理和难度感知自适应策略优化，有效提升了自动驾驶决策的透明度、泛化能力和与人类意图的一致性，为LLM在自动驾驶领域的应用提供了更优的解决方案。

Abstract: With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.

</details>


### [72] [Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models](https://arxiv.org/abs/2601.04731)
*Shuyang Jiang,Yuhao Wang,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.AI

TL;DR: 提出Miner方法，利用策略内在不确定性作为自监督奖励信号，解决推理模型RL训练中正同质提示导致的效率低下问题，无需外部监督或额外推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前无批评RL方法在处理正同质提示（所有rollout都正确）时效率低下，优势估计为零导致rollout浪费，需要更高效的训练方法。

Method: 提出Miner方法：1) 基于token级焦点信用分配机制，动态放大关键不确定token的梯度，抑制过度自信token；2) 自适应优势校准，无缝整合内在和可验证奖励。

Result: 在Qwen3-4B和Qwen3-8B基础模型的六个推理基准测试中，Miner达到最先进性能，相比GRPO在Pass@1上获得4.58绝对增益，Pass@K上获得6.66增益。

Conclusion: 潜在不确定性利用对于推理模型的高效可扩展RL训练既是必要的也是充分的，新提出的两个创新显示出优越性。

Abstract: Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.

</details>


### [73] [When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail](https://arxiv.org/abs/2601.04748)
*Xiaoxiao Li*

Main category: cs.AI

TL;DR: 该研究探讨了将多智能体系统编译为单智能体技能库系统的可行性，发现技能选择存在类似人类认知的容量限制，并在达到临界库规模后出现性能急剧下降的相变现象。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽然能有效处理复杂推理任务，但存在显著的通信开销。研究者希望探索是否可以通过单智能体选择技能库的方式获得类似的模块化优势，同时减少计算开销。

Method: 将技能视为内化的智能体行为，将多智能体系统编译为等效的单智能体系统，用技能选择替代智能体间通信。研究技能选择的扩展行为，分析库规模和语义混淆性对选择准确性的影响，并探索分层路由方法。

Result: 初步实验显示该方法能显著减少token使用和延迟，同时在推理基准上保持竞争力。技能选择存在容量限制：准确性在达到临界库规模前保持稳定，随后急剧下降，呈现相变现象。语义混淆性（而非单纯库规模）是性能下降的关键因素。分层路由方法显示出改善效果。

Conclusion: 该工作揭示了LLM基于语义的技能选择存在基本限制，提出了基于认知科学的框架和实用指南，为设计可扩展的技能型智能体提供了新视角。分层组织可能像帮助人类管理复杂选择一样有益于AI系统。

Abstract: Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?
  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.

</details>


### [74] [AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search](https://arxiv.org/abs/2601.04767)
*Zefang Zong,Dingwei Chen,Yang Li,Qi Yi,Bo Zhou,Chengming Li,Bo Qian,Peng Chen,Jie Jiang*

Main category: cs.AI

TL;DR: AT²PO：基于树搜索的回合制策略优化框架，通过熵引导树扩展和回合级信用分配解决多轮代理RL中的探索多样性不足、稀疏奖励分配和策略优化不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在多轮任务中面临三个核心挑战：探索多样性有限、稀疏信用分配困难、策略优化与代理交互的自然决策粒度不对齐。

Method: 提出AT²PO统一框架：1）回合级树结构支持熵引导树扩展以增强探索；2）回合级信用分配实现稀疏奖励的细粒度传播；3）代理回合制策略优化（ATPO）目标，使策略更新与代理交互的决策粒度对齐。

Result: 在7个基准测试中，相比最先进基线平均提升达1.84个百分点，消融研究验证了各组件有效性。

Conclusion: AT²PO为多轮代理RL提供了统一的解决方案，有效解决了探索多样性、信用分配和策略优化对齐问题，ATPO组件可轻松集成到任何多轮RL流程中。

Abstract: LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.

</details>


### [75] [APEX: Academic Poster Editing Agentic Expert](https://arxiv.org/abs/2601.04794)
*Chengxin Shi,Qinnan Cai,Zeyuan Chen,Long Zeng,Yibo Zhao,Jing Yu,Jianxiang Yu,Xiang Li*

Main category: cs.AI

TL;DR: APEX是首个用于交互式学术海报编辑的智能体框架，支持细粒度控制的多级API编辑和审查调整机制，并提出了包含514条编辑指令的APEX-Bench基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有论文到海报的生成方法通常是单次、非交互式的，难以满足用户复杂、主观的意图。学术海报设计需要高密度内容和复杂布局的精确平衡，但这个过程劳动密集。

Method: 提出APEX框架，支持基于多级API的细粒度编辑和审查调整机制。同时构建APEX-Bench基准，包含514条按操作类型、难度和抽象级别分类的编辑指令，采用参考引导和无参考策略确保真实性和多样性。

Result: 实验结果表明APEX显著优于基线方法。建立了多维度VLM-as-a-judge评估协议，评估指令完成度、修改范围和视觉一致性。

Conclusion: APEX为学术海报编辑提供了首个交互式智能体框架，通过细粒度控制和系统化基准推动了该领域的发展。

Abstract: Designing academic posters is a labor-intensive process requiring the precise balance of high-density content and sophisticated layout. While existing paper-to-poster generation methods automate initial drafting, they are typically single-pass and non-interactive, often fail to align with complex, subjective user intent. To bridge this gap, we propose APEX (Academic Poster Editing agentic eXpert), the first agentic framework for interactive academic poster editing, supporting fine-grained control with robust multi-level API-based editing and a review-and-adjustment Mechanism. In addition, we introduce APEX-Bench, the first systematic benchmark comprising 514 academic poster editing instructions, categorized by a multi-dimensional taxonomy including operation type, difficulty, and abstraction level, constructed via reference-guided and reference-free strategies to ensure realism and diversity. We further establish a multi-dimensional VLM-as-a-judge evaluation protocol to assess instruction fulfillment, modification scope, and visual consistency & harmony. Experimental results demonstrate that APEX significantly outperforms baseline methods. Our implementation is available at https://github.com/Breesiu/APEX.

</details>


### [76] [Defense Against Indirect Prompt Injection via Tool Result Parsing](https://arxiv.org/abs/2601.04795)
*Qiang Yu,Xinran Cheng,Chuanyi Liu*

Main category: cs.AI

TL;DR: 提出一种通过工具结果解析为LLM提供精确数据并过滤恶意代码的新方法，在保持实用性的同时显著降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理从数字助手转变为物理控制器，间接提示注入攻击威胁日益严重，现有防御方法存在计算开销大或攻击成功率高等问题

Method: 通过工具结果解析为LLM提供精确数据，同时有效过滤注入的恶意代码，结合了精确数据提供和恶意代码过滤的双重优势

Result: 在保持竞争性攻击下实用性的同时，实现了迄今为止最低的攻击成功率，显著优于现有方法

Conclusion: 该方法为LLM代理防御间接提示注入攻击提供了有效解决方案，在安全性和实用性之间取得了良好平衡

Abstract: As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.

</details>


### [77] [Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning](https://arxiv.org/abs/2601.04805)
*Siyuan Gan,Jiaheng Liu,Boyan Wang,Tianpei Yang,Runqing Miao,Yuyao Zhang,Fanyu Meng,Junlan Feng,Linjian Meng,Jing Huo,Yang Gao*

Main category: cs.AI

TL;DR: TNT方法通过基于思考的解决方案信息动态设置非思考响应的最大token限制，在保持准确性的同时显著减少计算开销，解决了强化学习中的奖励欺骗问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过长链思考获得优异性能，但计算开销巨大。现有使用强化学习训练混合推理模型的方法存在奖励欺骗问题，而监督微调成本高，统一token限制效果有限。

Method: 提出Thinking-Based Non-Thinking (TNT)方法，不采用监督微调，而是利用思考响应的解决方案信息为不同查询设置不同的非思考响应最大token使用限制。

Result: 在五个数学基准测试中，TNT相比DeepSeek-R1-Distill-Qwen-1.5B/7B和DeepScaleR-1.5B减少约50%的token使用，同时显著提高准确性，在所有测试方法中达到准确性和效率的最优权衡。

Conclusion: TNT通过动态token限制有效解决了奖励欺骗问题，在非思考响应中奖励欺骗概率保持在10%以下，实现了准确性和计算效率的最佳平衡。

Abstract: Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT's responses, which are classified as not using thinking, remains below 10% across all tested datasets.

</details>


### [78] [SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning](https://arxiv.org/abs/2601.04809)
*Caijun Xu,Changyi Xiao,Zhongyuan Peng,Xinrun Wang,Yixin Cao*

Main category: cs.AI

TL;DR: SCALER是一个通过自适应环境设计维持有效学习信号的强化学习框架，它将真实编程问题转化为可验证推理环境，通过动态调整难度和环境选择来支持持续改进。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能增强大语言模型的推理能力，但其效果依赖于训练信号的有效性。实践中，当任务难度与模型能力不匹配，或训练被少数重复问题模式主导时，RL进展会放缓。需要解决这两个问题来维持有效的学习信号。

Method: SCALER包含两个核心组件：1）可扩展的合成流水线，将真实编程问题转化为具有可控难度和无限实例生成的可验证推理环境；2）自适应多环境RL策略，动态调整实例难度并策划活动环境集，以跟踪模型能力前沿并保持分布多样性。

Result: 实验表明，SCALER在多种推理基准测试中持续优于基于数据集的RL基线方法，并展现出更稳定、更长视野的训练动态。

Conclusion: SCALER通过自适应环境设计和难度调整，有效解决了RL训练中奖励稀疏性和过拟合问题，支持模型在整个训练过程中持续改进。

Abstract: Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.

</details>


### [79] [Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models](https://arxiv.org/abs/2601.04861)
*Jingbo Wang,Sendong Zhao,Jiatong Liu,Haochun Wang,Wanting Li,Bing Qin,Ting Liu*

Main category: cs.AI

TL;DR: OI-MAS框架通过自适应模型选择策略，在异构多尺度LLM池中动态分配不同推理阶段的模型，显著提升多智能体系统的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在复杂推理任务中计算效率低下，通常在所有智能体角色中统一部署大型语言模型，未能考虑不同推理阶段的认知需求差异。

Method: 提出OI-MAS框架，实现跨异构多尺度LLM池的自适应模型选择策略，包括状态依赖的路由机制动态选择智能体角色和模型规模，以及基于任务复杂度的置信度感知机制选择适当模型规模。

Result: OI-MAS在实验中始终优于基线多智能体系统，准确率提升高达12.88%，同时成本降低高达79.78%。

Conclusion: 通过自适应模型选择和动态路由机制，OI-MAS框架能够显著提高多智能体系统的计算效率和性能表现。

Abstract: While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\% while reducing cost by up to 79.78\%.

</details>


### [80] [SmartSearch: Process Reward-Guided Query Refinement for Search Agents](https://arxiv.org/abs/2601.04888)
*Tongyu Wen,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: SmartSearch是一个基于LLM的搜索代理框架，通过过程奖励和查询优化机制提升搜索查询质量，采用三阶段课程学习实现渐进式改进。


<details>
  <summary>Details</summary>
Motivation: 现有LLM搜索代理主要关注推理范式优化，但忽视了中间搜索查询的质量问题。生成的查询往往不准确，导致检索结果不理想，限制了搜索代理的整体效果。

Method: 提出SmartSearch框架：1) 过程奖励机制通过双级信用评估对中间搜索查询提供细粒度监督；2) 查询优化机制通过选择性优化低质量查询并基于优化结果重新生成后续搜索轮次；3) 设计三阶段课程学习框架（模仿、对齐、泛化）让代理逐步内化查询质量改进能力。

Result: 实验结果表明SmartSearch持续超越现有基线方法，定量分析进一步证实其在搜索效率和查询质量方面的显著提升。

Conclusion: SmartSearch通过关注中间搜索查询质量，结合过程奖励和查询优化机制，有效提升了LLM搜索代理的性能。

Abstract: Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.

</details>


### [81] [DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation](https://arxiv.org/abs/2601.04895)
*Renzhao Liang,Jingru Chen,Bo Jia,Bo Deng,Chenggang Xie,Yidong Wang,Ke Jin,Xin Wang,Linfeng Zhang,Cunxiang Wang*

Main category: cs.AI

TL;DR: 提出DVD方法检测LLM评估中的变体污染问题，通过建模温度采样下的输出分布方差来识别被记忆的语义等价测试项。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估面临变体污染问题：训练语料中包含语义等价但词汇或句法改变的测试项，这些变体逃避现有检测器，通过记忆而非真正推理来虚增基准分数。

Method: 提出DVD检测方法，基于温度采样建模局部输出分布方差。核心洞察：污染项会在记忆依从状态和扰动漂移状态间交替，导致低概率token的合成难度方差异常高；非污染项则保持相对平滑的方差漂移。

Result: 在Omni-MATH和SuperGPQA两个领域构建首个变体污染基准，通过微调不同规模和架构的模型模拟污染。DVD在跨数据集和模型上一致优于基于困惑度、Min-k%++、编辑距离和嵌入相似性的基线方法，且对超参数具有强鲁棒性。

Conclusion: 生成分布方差可作为检测LLM评估中变体污染的原则性和实用指纹，为更可靠的模型评估提供新方法。

Abstract: Evaluating large language models (LLMs) is increasingly confounded by \emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \textbf{DVD} (\textbf{D}etection via \textbf{V}ariance of generation \textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \emph{memory-adherence} state and a \emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \textbf{DVD} consistently outperforms perplexity-based, Min-$k$\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.

</details>


### [82] [Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition](https://arxiv.org/abs/2601.04920)
*Nils Einecke*

Main category: cs.AI

TL;DR: ChatGPT用于ESA ELOPE竞赛的快速原型开发，获得第二名，展示了人机协作在科学竞赛中的潜力，同时揭示了LLM在科学工作流中的优缺点。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型作为编码伙伴在加速科学发现中的作用，特别是在竞争性科学环境中的应用潜力。

Method: 使用ChatGPT进行快速原型开发，参与ESA的ELOPE竞赛（基于事件的月球光流自运动估计），处理事件相机数据以估计月球着陆器轨迹。

Result: 尽管加入较晚，但获得了第二名（得分0.01282），展示了人机协作在科学竞赛中的有效性。

Conclusion: 对话式AI既能加速开发又能支持科学研究的概念洞察，但需要结构化集成到科学工作流中，并制定AI辅助科学工作的最佳实践。

Abstract: Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.

</details>


### [83] [Large language models can effectively convince people to believe conspiracies](https://arxiv.org/abs/2601.05050)
*Thomas H. Costello,Kellin Pelrine,Matthew Kowal,Antonio A. Arechar,Jean-François Godbout,Adam Gleave,David Rand,Gordon Pennycook*

Main category: cs.AI

TL;DR: GPT-4o能同样有效地增加或减少阴谋论信念，即使有安全护栏也能促进虚假信息，但可通过纠正对话和准确性提示来缓解风险


<details>
  <summary>Details</summary>
Motivation: 研究LLMs的说服力是否更倾向于真相而非虚假信息，或者它们是否同样容易促进错误信念，特别是在阴谋论传播方面

Method: 通过三个预注册实验，让2,724名美国参与者与GPT-4o讨论他们不确定的阴谋论，模型被指示要么反驳("debunking")要么支持("bunking")该阴谋论，包括使用移除护栏的"越狱"版本和标准版本

Result: 越狱版GPT-4o在增加和减少阴谋论信念方面同样有效；支持阴谋论的AI获得更积极评价并增加对AI的信任；标准GPT-4o产生类似效果，安全护栏几乎无法阻止其促进阴谋论信念；但纠正对话能逆转新诱导的信念，准确性提示显著降低其增加阴谋论信念的能力

Conclusion: LLMs具有促进真相和虚假信息的强大能力，但存在潜在解决方案来缓解这种风险，需要更有效的安全措施

Abstract: Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against ("debunking") or for ("bunking") that conspiracy. When using a "jailbroken" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.

</details>


### [84] [Arabic Prompts with English Tools: A Benchmark](https://arxiv.org/abs/2601.05101)
*Konstantin Kubrak,Ahmed El-Moselhy,Ammar Alsulami,Remaz Altuwaim,Hassan Ismail Fawaz,Faisal Alsaby*

Main category: cs.AI

TL;DR: 首个专门评估阿拉伯语LLM工具调用和代理能力的基准，揭示了阿拉伯语交互下工具调用准确率平均下降5-10%的性能差距。


<details>
  <summary>Details</summary>
Motivation: 随着阿拉伯语原生LLM快速发展，现有评估基准主要关注英语，缺乏对阿拉伯语工具调用能力的系统评估，而阿拉伯语用户在工具调用上面临性能下降问题。

Method: 创建首个阿拉伯语工具调用和代理能力评估基准，提供标准化框架来衡量阿拉伯语代理工作流中的功能准确性和鲁棒性。

Result: 发现显著性能差距：当用户使用阿拉伯语交互时，工具调用准确率平均下降5-10%，无论工具描述本身是阿拉伯语还是英语。

Conclusion: 该基准揭示了阿拉伯语工具调用中的关键挑战，旨在促进为阿拉伯语用户开发更可靠和语言公平的AI代理。

Abstract: Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.

</details>


### [85] [Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction](https://arxiv.org/abs/2601.05107)
*Muzhao Tian,Zisu Huang,Xiaohua Wang,Jingwen Xu,Zhengkang Guo,Qi Qian,Yuanzhe Shen,Kaitao Song,Jiakang Yuan,Changze Lv,Xiaoqing Zheng*

Main category: cs.AI

TL;DR: 论文提出SteeM框架，通过可调控的记忆依赖机制解决LLM智能体在长期交互中的记忆锚定问题，允许用户动态控制智能体对历史信息的依赖程度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在长期交互中采用"全有或全无"的记忆使用方式：要么包含所有相关历史信息导致记忆锚定（智能体被过去交互束缚），要么完全排除记忆导致重要交互历史丢失。需要更精细的记忆控制机制。

Method: 提出SteeM框架：1）引入记忆依赖行为度量来量化过去交互对当前输出的影响；2）允许用户动态调节记忆依赖程度，从促进创新的"重新开始"模式到紧密遵循交互历史的"高保真"模式。

Result: 在不同场景下的实验表明，SteeM框架始终优于传统提示方法和刚性记忆掩码策略，为个性化人机协作提供了更细致有效的控制。

Conclusion: 智能体对记忆的依赖可以建模为显式且用户可控的维度，SteeM框架通过可调控的记忆机制实现了更灵活有效的个性化人机协作。

Abstract: As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \textbf{Stee}rable \textbf{M}emory Agent, \texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.

</details>


### [86] [Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior](https://arxiv.org/abs/2601.05114)
*Wajid Nasser*

Main category: cs.AI

TL;DR: 研究发现LLM作为评估者存在可靠性悖论：单个评估者内部一致性高，但不同评估者之间几乎无共识（α=0.042），评估者间的分歧模式却稳定到可作为指纹识别（准确率77.1%-99.6%），每个评估者都有独特的"评估倾向"。


<details>
  <summary>Details</summary>
Motivation: 研究动机是检验LLM-as-judge系统的评估可靠性，特别是不同LLM评估者之间的一致性程度，以及它们是否真正共享对质量评估的共同理解。

Method: 使用9个LLM评估者对120个独特视频项目进行3次独立评估（共3240次评估），计算评估者间信度（Krippendorff's α），并训练分类器从评分特征识别评估者身份，分析评估倾向的多个维度。

Result: 评估者间一致性接近零（α=0.042），某些维度甚至低于随机噪声预期（α<0）。但分类器仅凭评分就能以77.1%准确率识别评估者，加入倾向特征后达89.9%，同一模型家族内区分准确率高达99.6%。

Conclusion: LLM评估者不是可互换的测量工具，而是各自编码了独特的质量评估理论。平均它们的评分会产生不符合任何评估者实际价值观的合成判断，这对依赖LLM评估的研究和实践有重要影响。

Abstract: LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's α = 0.042). On two dimensions, judges disagree more than random noise would predict (α < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an "evaluative disposition" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.

</details>


### [87] [SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning](https://arxiv.org/abs/2601.05187)
*Yanchang Liang,Xiaowei Zhao*

Main category: cs.AI

TL;DR: SimuAgent：基于LLM的Simulink建模与仿真代理，采用轻量级计划-执行架构和两阶段训练，通过ReGRPO算法解决长时任务稀疏奖励问题，在5300个多领域建模任务上超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: LLM在文本代码自动化方面取得革命性进展，但在图形化工程工作流中的应用潜力尚未充分探索。Simulink等图形化建模环境需要更高效、可解释的AI辅助解决方案

Method: 1. 用简洁的字典式Python表示替代冗长XML，大幅减少token数量；2. 轻量级计划-执行架构；3. 两阶段训练（低层工具技能+高层设计推理）；4. 提出Reflection-GRPO算法，通过自反思轨迹提供丰富中间反馈；5. 抽象-重构数据增强提升泛化能力

Result: 在SimuBench（5300个多领域建模任务）上，Qwen2.5-7B模型微调后收敛更快、建模精度更高，超越标准RL基线，甚至超过GPT-4o的少样本提示性能。消融实验证实两阶段课程和抽象-重构数据增强提升泛化能力

Conclusion: SimuAgent填补了LLM与图形化建模环境之间的空白，提供隐私保护、成本效益高的工业模型驱动工程解决方案，可在本地硬件上完全训练和运行

Abstract: Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.

</details>


### [88] [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/abs/2601.05214)
*Kait Healy,Bharathi Srinivasan,Visakh Madathil,Jing Wu*

Main category: cs.AI

TL;DR: 提出一个实时检测LLM工具调用幻觉的框架，利用单次前向传播中的内部表示进行检测，无需多次推理或外部验证，计算效率高


<details>
  <summary>Details</summary>
Motivation: LLM在工具调用中存在幻觉问题（选择错误工具、参数格式错误、工具绕过行为），这会影响生产系统中基于LLM的代理的可靠性，导致结果不一致并绕过安全审计控制

Method: 利用LLM在生成过程中的内部表示，在同一个前向传播中实时检测工具调用幻觉，无需多次前向传播或外部验证

Result: 在多个领域的推理任务上评估，检测性能高达86.4%准确率，同时保持实时推理能力，计算开销最小，特别擅长检测参数级幻觉和不适当的工具选择

Conclusion: 该框架为可靠代理部署提供了有效的工具调用幻觉检测方案，能够在实时应用中高效识别关键错误

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.

</details>


### [89] [MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents](https://arxiv.org/abs/2601.05215)
*Tamil Sudaravan Mohan Doss,Michael Xu,Sudha Rao,Andrew D. Wilson,Balasaravanan Thoravi Kumaravel*

Main category: cs.AI

TL;DR: 提出了MineNPC-Task基准测试框架，用于在开放世界Minecraft中评估具有记忆能力和混合主动性的LLM智能体，通过玩家合作设计任务模板和验证器，发现智能体在代码执行、物品管理等方面存在系统性故障模式。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常依赖合成提示，缺乏真实玩家参与设计的任务，难以全面评估智能体在开放世界环境中的记忆能力和混合主动性交互表现。

Method: 通过专家玩家合作设计任务，将任务规范化为参数化模板，包含明确的先决条件和依赖结构，配合机器可检查的验证器，采用限制知识策略防止作弊，捕获智能体的计划/行动/记忆事件。

Result: 使用GPT-4o评估了8名经验玩家的216个子任务，发现代码执行、库存/工具处理、引用和导航方面的系统性故障模式，同时混合主动性澄清和轻量级记忆有助于恢复，参与者对交互质量和界面可用性评价积极。

Conclusion: MineNPC-Task为评估记忆感知的具身智能体提供了透明、可复现的框架，揭示了当前智能体的局限性，特别是需要更强的跨任务记忆持久性，并发布了完整任务套件和工具以支持未来研究。

Abstract: We present \textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.
  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \textbf{216} subtasks across \textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [90] [Advancing Language Models for Code-related Tasks](https://arxiv.org/abs/2601.04526)
*Zhao Tian*

Main category: cs.SE

TL;DR: 该研究通过三个互补方向系统性地解决语言模型在复杂编程场景中的局限性：改进代码数据质量、增强模型架构、提升模型推理能力，旨在促进语言模型在软件开发中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在复杂编程场景中仍存在困难，主要受限于数据质量、模型架构和推理能力。该研究旨在系统性地解决这些挑战，推动语言模型在软件工程中的实际应用。

Method: 采用三个互补方向：1) 使用代码差异引导的对抗增强技术(CODA)和代码去噪技术(CodeDenoise)改进代码数据质量；2) 通过语法引导的代码语言模型(LEAM和LEAM++)增强模型架构；3) 使用提示技术(muFiX)和基于代理的技术(Specine)提升模型推理能力。

Result: 研究提出了一系列技术方法，包括数据增强、模型架构改进和推理能力提升，旨在全面解决语言模型在软件工程任务中的局限性。

Conclusion: 通过系统性地解决数据质量、模型架构和推理能力三个关键挑战，该研究为促进语言模型在软件开发中的实际应用和推进智能软件工程提供了综合解决方案。

Abstract: Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.

</details>


### [91] [AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation](https://arxiv.org/abs/2601.04540)
*Tanghaoran Zhang,Xinjun Mao,Shangwen Wang,Yuxin Zhao,Yao Lu,Jin Zhang,Zhang Zhang,Kang Yang,Yue Yu*

Main category: cs.SE

TL;DR: 提出了AdaptEval基准，用于评估大语言模型在代码片段适应任务上的能力，填补了现有基准的空白。


<details>
  <summary>Details</summary>
Motivation: 当前虽然有评估LLM在软件工程任务上的基准，但缺乏专门针对代码适应（代码重用中的关键活动）的评估基准，导致LLM在该领域的实际效用不明确。

Method: AdaptEval基准具有三个特点：1) 实际上下文：任务来自Stack Overflow和GitHub开发者实践；2) 多粒度标注：任务级和适应级需求标注；3) 细粒度评估：包含适应级和函数级的两层测试框架。

Result: 基于AdaptEval对6个指令调优LLM和3个推理LLM进行评估，发现AdaptEval能从多个角度评估LLM的适应能力，并揭示了LLM在遵循明确指令方面的局限性。

Conclusion: AdaptEval基准填补了代码适应评估的空白，有助于进一步研究和提升LLM在代码片段适应方面的能力，支持其实际应用。

Abstract: Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.

</details>


### [92] [4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering](https://arxiv.org/abs/2601.04556)
*Bo Yu,Lei Zhao*

Main category: cs.SE

TL;DR: 提出4D-ARE方法论，通过四个维度（结果→过程→支持→长期）来系统化指定AI代理需要推理的内容，弥补现有推理框架只关注"如何推理"而忽略"推理什么"的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理运行时推理框架（如ReAct、思维链）主要关注"如何推理"，但缺乏对"推理什么"的系统化设计。作者发现即使代理能完美执行任务，当被问及"为什么完成率是80%？"时，它只返回指标而非因果解释，这表明代理知道如何推理但不知道应该推理什么内容。

Method: 提出4D-ARE（四维归因驱动代理需求工程）方法论，基于Pearl因果层次理论，将决策者关心的归因问题组织成四个维度：结果→过程→支持→长期。该方法通过五个层次操作化，产生可直接编译为系统提示的工件。

Result: 在金融服务领域进行了工业试点部署，展示了该方法的可行性。4D-ARE能够系统化指定代理应该推理的内容，补充了现有运行时框架只关注推理方式的不足。

Conclusion: 系统化的需求规范能够放大基础推理框架的威力。本文提出了方法论建议和初步工业验证，严格的实证评估计划在未来的工作中进行。

Abstract: We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked "Why is completion rate 80%?", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.

</details>


### [93] [Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests](https://arxiv.org/abs/2601.04886)
*Jingzhi Gong,Giovanni Pinna,Yixin Bian,Jie M. Zhang*

Main category: cs.SE

TL;DR: 研究分析了AI代码助手生成的Pull Request描述与实际代码变更的一致性，发现存在描述-代码不一致问题，导致PR接受率降低51.7%，合并时间延长3.5倍。


<details>
  <summary>Details</summary>
Motivation: AI编码代理生成的PR描述是向人类评审者传达代码变更的主要渠道，但这些描述与实际变更之间的一致性尚未被充分研究，引发了人们对AI代理可信度的担忧。

Method: 使用PR消息-代码不一致性(PR-MCI)方法分析了五个AI代理生成的23,247个PR，手动标注了974个PR，识别出八种PR-MCI类型，并进行统计分析。

Result: 发现406个PR(1.7%)存在高PR-MCI问题，其中描述声称未实现变更是最常见问题(45.4%)。高MCI的PR接受率降低51.7%(28.3% vs 80.0%)，合并时间延长3.5倍(55.8 vs 16.0小时)。

Conclusion: 不可靠的PR描述会削弱对AI代理的信任，需要建立PR-MCI验证机制和改进PR生成方法，以实现可信赖的人机协作。

Abstract: Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [94] [Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning](https://arxiv.org/abs/2601.04365)
*Anton Roupassov-Ruiz,Yiyang Zuo*

Main category: cs.LG

TL;DR: 程序化策略（PERL）在人工生命进化强化学习任务中比神经网络策略（NERL）表现更好，平均多存活201.69步，即使仅使用学习（无进化）也比神经网络的混合方法多存活73.67步。


<details>
  <summary>Details</summary>
Motivation: 传统进化强化学习中的神经网络策略缺乏显式模块化结构，限制了行为解释。研究旨在探索程序化策略能否在性能上匹配甚至超越神经网络策略。

Method: 使用软可微决策列表（SDDL）实现程序化策略，在完全开源复现的1992年人工生命进化强化学习测试平台上进行实验。通过4000次独立试验进行严格的生存分析，使用Kaplan-Meier曲线和限制平均生存时间（RMST）指标。

Result: PERL和NERL之间存在统计显著的生存概率差异。PERL代理平均比NERL代理多存活201.69步。仅使用学习的SDDL代理比使用学习和进化的神经代理平均多存活73.67步。

Conclusion: 程序化策略在人工生命环境中能够超越神经网络策略的生存性能，为可解释的强化学习策略提供了有前景的替代方案。

Abstract: In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.

</details>


### [95] [Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay](https://arxiv.org/abs/2601.04392)
*Mohsen Jalaeian-Farimani*

Main category: cs.LG

TL;DR: 提出Enhanced-FQL(λ)模糊强化学习框架，集成模糊资格迹和分段经验回放，用于连续控制任务，在保持可解释性的同时实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 针对连续控制任务，需要既保持可解释性又具有竞争力的性能。传统深度强化学习方法虽然性能好但缺乏可解释性且计算复杂，而现有模糊强化学习方法在样本效率和稳定性方面存在不足。

Method: 提出Enhanced-FQL(λ)框架，核心包括：1) 使用可解释的模糊规则库替代复杂神经网络架构；2) 引入模糊资格迹(FET)进行稳定的多步信用分配；3) 采用分段经验回放(SER)机制提高样本效率；4) 基于模糊贝尔曼方程(FBE)进行学习。

Result: 理论分析证明方法在标准假设下收敛。在连续控制领域的广泛评估表明，Enhanced-FQL(λ)相比n步模糊TD和模糊SARSA(λ)基线具有更好的样本效率和更低的方差，同时计算复杂度远低于DDPG等深度强化学习替代方案。

Conclusion: Enhanced-FQL(λ)框架结合了固有的可解释性、计算效率和理论收敛保证，特别适用于对透明度和资源约束有严格要求的安全关键应用。

Abstract: This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($λ$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($λ$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($λ$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework's inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.

</details>


### [96] [Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards](https://arxiv.org/abs/2601.04411)
*Ali Rad,Khashayar Filom,Darioush Keivan,Peyman Mohajerin Esfahani,Ehsan Kamalinejad*

Main category: cs.LG

TL;DR: 论文提出RLVR（可验证奖励强化学习）框架，分析验证噪声对LLM训练的影响，发现Youden指数J=TPR-FPR决定学习成败：J>0时学习成功，J=0时中性，J<0时反学习崩溃。


<details>
  <summary>Details</summary>
Motivation: 现实中的验证器（单元测试、人工标注、LLM评判）都存在噪声，特别是在编程等复杂领域。需要研究验证噪声是仅仅减缓学习速度，还是可能完全改变学习结果。

Method: 建立多臂老虎机的分析框架模拟RLVR动态，使用GRPO方法，建模假阳性和假阴性，将补全分组为重复推理模式，得到复制子式（自然选择）流。分析Youden指数J=TPR-FPR对学习动态的影响。

Result: 发现尖锐的相变：J>0时错误模式被淘汰（学习成功）；J=0时中性；J<0时错误模式放大直至主导（反学习和崩溃）。在J>0的学习机制中，噪声主要影响收敛时间而非结果。

Conclusion: 验证噪声的影响由Youden指数J决定，J>0时噪声主要影响学习速度而非结果。该框架为分析RLVR稳定性、收敛性和算法干预提供通用视角。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?
  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time ("rate, not fate"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.

</details>


### [97] [TSSR: Two-Stage Swap-Reward-Driven Reinforcement Learning for Character-Level SMILES Generation](https://arxiv.org/abs/2601.04521)
*Jacob Ede Levine,Yun Lyan Luo,Sai Chandra Kosaraju*

Main category: cs.LG

TL;DR: TSSR是一个两阶段、交换奖励驱动的强化学习框架，用于字符级SMILES生成，通过语法修复和化学感知反馈提高分子生成的有效性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 当前化学语言模型生成SMILES字符串时容易产生复合令牌错误，许多样本无法解析或化学上不可行，而硬约束会限制探索空间。需要提高分子生成的可靠性和有效性。

Method: 提出TSSR两阶段强化学习框架：第一阶段奖励局部令牌交换修复语法，第二阶段提供RDKit诊断的化学感知反馈，奖励减少化合价、芳香性和连接性问题。奖励可分解为可解释的术语，无需任务特定标签或手工语法。

Result: 在MOSES基准测试中，纯强化学习显著提高语法有效性、化学有效性和新颖性；微调强化学习在保持药物相似性和可合成性的同时增加有效性和新颖性。令牌级分析显示语法编辑和化学修复共同减少RDKit检测错误。

Conclusion: TSSR将稀疏终端目标转化为更密集且可解释的奖励，提高语法和化学质量而不减少多样性。框架与数据集无关，可适应各种强化学习方法。

Abstract: The design of reliable, valid, and diverse molecules is fundamental to modern drug discovery, as improved molecular generation supports efficient exploration of the chemical space for potential drug candidates and reduces the cost of early design efforts. Despite these needs, current chemical language models that generate molecules as SMILES strings are vulnerable to compounding token errors: many samples are unparseable or chemically implausible, and hard constraints meant to prevent failure can restrict exploration. To address this gap, we introduce TSSR, a Two-Stage, Swap-Reward-driven reinforcement learning (RL) framework for character-level SMILES generation. Stage one rewards local token swaps that repair syntax, promoting transitions from invalid to parseable strings. Stage two provides chemistry-aware feedback from RDKit diagnostics, rewarding reductions in valence, aromaticity, and connectivity issues. The reward decomposes into interpretable terms (swap efficiency, error reduction, distance to validity), is model agnostic, and requires no task-specific labels or hand-crafted grammars. We evaluated TSSR on the MOSES benchmark using a GRU policy trained with PPO in both pure RL (P-RL) from random initialization and fine-tuning RL (F-RL) starting from a pretrained chemical language model, assessing 10,000 generated SMILES per run. In P-RL, TSSR significantly improves syntactic validity, chemical validity, and novelty. In F-RL, TSSR preserves drug-likeness and synthesizability while increasing validity and novelty. Token-level analysis shows that syntax edits and chemistry fixes act jointly to reduce RDKit detected errors. TSSR converts a sparse terminal objective into a denser and more interpretable reward, improving both syntactic and chemical quality without reducing diversity. TSSR is dataset-agnostic and can be adapted to various reinforcement learning approaches.

</details>


### [98] [Not All Steps are Informative: On the Linearity of LLMs' RLVR Training](https://arxiv.org/abs/2601.04537)
*Tianle Wang,Zhongyuan Wu,Shenghao Jin,Hao Xu,Wei Chen,Ning Miao*

Main category: cs.LG

TL;DR: 该论文发现RLVR训练中LLM呈强线性演化，提出通过权重外推和logits外推来预测未来模型状态，显著减少计算成本


<details>
  <summary>Details</summary>
Motivation: RLVR训练需要数千步训练步骤，计算成本高昂，主要归因于长时间的探索过程。作者观察到RLVR训练中LLM呈现强线性演化特征，这启发他们研究是否可以通过外推中间检查点来预测未来模型状态，从而避免昂贵的持续训练

Method: 基于观察到的线性演化特性，提出了两种外推方法：1) 权重外推：从中间检查点外推模型权重；2) Logits外推：外推模型输出log-probabilities。这些方法利用早期训练阶段出现的趋势进行预测，避免持续训练

Result: 权重外推产生的模型性能与标准RL训练相当，但计算需求显著减少。Logits外推在四个基准测试上始终优于持续RL训练，特别是在RL训练保持稳定的步数范围之外进行外推时表现更好

Conclusion: RLVR训练中LLM的强线性演化表明训练主要放大早期出现的趋势而非持续发现新行为。利用这一特性进行外推可以显著减少计算成本，同时保持或提升模型性能，为高效的LLM后训练提供了新思路

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.

</details>


### [99] [Learning Dynamics in RL Post-Training for Language Models](https://arxiv.org/abs/2601.04670)
*Akiyoshi Tomihari*

Main category: cs.LG

TL;DR: 论文提出CF-RL方法，通过优先更新分类器来改善RL后训练的学习动态，解决输出多样性下降问题。


<details>
  <summary>Details</summary>
Motivation: RL后训练虽然能提升语言模型的对齐和推理能力，但会导致输出多样性下降，这一现象缺乏理论解释。作者希望从学习动态角度理解RL后训练过程。

Method: 采用经验神经正切核(NTK)框架分析RL学习动态，将NTK分解为两个组件。提出CF-RL方法：两阶段训练策略，先优先更新分类器，再进行标准RL优化。

Result: 分析发现特征表示有限变异性导致RL更新系统性增加模型置信度，解释了输出多样性下降。CF-RL实验验证了理论分析，显示模型置信度增加和优化加速。

Conclusion: 研究形式化了RL后训练的学习动态，提出的CF-RL方法为改进RL训练提供了新思路，其机制与监督学习中的线性探测-微调不同。

Abstract: Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement.

</details>


### [100] [Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead](https://arxiv.org/abs/2601.04686)
*Oluwatosin Oseni,Shengjie Wang,Jun Zhu,Micah Corah*

Main category: cs.LG

TL;DR: Nightmare Dreamer是一种基于模型的安全强化学习算法，通过使用学习的世界模型预测潜在安全违规来确保安全性，在Safety Gymnasium任务上实现了近乎零安全违规和高效率


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人控制等实际应用中取得了显著成功，但由于缺乏足够的安全保证，其应用仍然受限。需要开发能够提供安全保证的强化学习算法。

Method: Nightmare Dreamer是一种基于模型的安全强化学习算法，它通过学习一个世界模型来预测潜在的安全违规，并据此规划行动。该方法仅使用图像观测，在Safety Gymnasium任务上进行评估。

Result: Nightmare Dreamer在Safety Gymnasium任务上实现了近乎零安全违规，同时最大化奖励。与无模型基线相比，效率提高了近20倍。

Conclusion: Nightmare Dreamer通过基于模型的方法有效解决了强化学习中的安全问题，在保持高性能的同时显著减少了安全违规，为安全强化学习的实际应用提供了有前景的解决方案。

Abstract: Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.

</details>


### [101] [AgentOCR: Reimagining Agent History via Optical Self-Compression](https://arxiv.org/abs/2601.04786)
*Lang Feng,Fuchao Yang,Feng Chen,Xin Cheng,Haiyang Xu,Zhenglin Wan,Ming Yan,Bo An*

Main category: cs.LG

TL;DR: AgentOCR框架通过将文本历史转换为紧凑的视觉图像表示，结合分段光学缓存和智能自压缩机制，显著降低多轮交互中LLM代理的token消耗和内存使用。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多轮交互中应用，文本历史快速增长导致token预算和内存使用急剧膨胀，成为实际部署的瓶颈。需要一种更高效的表示方法来压缩历史信息。

Method: 1) 将累积的观察-动作历史渲染为紧凑的视觉图像；2) 提出分段光学缓存机制，将历史分解为可哈希的段并维护视觉缓存，消除冗余重渲染；3) 引入智能自压缩，让代理主动输出压缩率，并通过压缩感知奖励训练，自适应平衡任务成功率和token效率。

Result: 在ALFWorld和基于搜索的QA等挑战性基准测试中，AgentOCR保持了超过95%的文本代理性能，同时显著减少token消耗(>50%)，实现一致的token和内存效率。分段光学缓存带来20倍渲染加速，自压缩机制有效平衡了策略。

Conclusion: AgentOCR通过视觉token表示历史、分段缓存消除冗余、智能自压缩平衡效率，为LLM代理系统提供了可扩展的多轮交互解决方案，显著降低计算成本。

Abstract: Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\% of text-based agent performance while substantially reducing token consumption (>50\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.

</details>


### [102] [Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following](https://arxiv.org/abs/2601.04954)
*Yirong Zeng,Yufei Liu,Xiao Ding,Yutai Hou,Yuxian Wang,Haonan Song,Wu Ning,Dandan Tu,Qixun Zhang,Bibo Cai,Yuxiang He,Ting Liu*

Main category: cs.LG

TL;DR: 研究发现，在指令跟随任务的强化学习中，高精度奖励比数据多样性更重要，仅使用硬约束训练的模型优于混合数据集，提出基于奖励精度的数据优化策略


<details>
  <summary>Details</summary>
Motivation: 挑战当前主流观点，即认为指令跟随任务需要多样化的可验证硬约束和不可验证软约束混合才能泛化到未见指令，通过系统实证研究检验这一共识

Method: 进行系统实证调查，比较硬约束训练与混合数据集训练的效果，分析LLM评判器的召回率问题，提出基于奖励精度的数据中心化优化策略

Result: 硬约束训练模型始终优于混合数据集训练，奖励精度是有效对齐的主要驱动力，提出的方法在5个基准测试中性能提升13.4%，训练时间减少58%

Conclusion: 需要范式转变：从盲目追求数据多样性转向关注高精度奖励，奖励精度比约束多样性对指令跟随任务更重要

Abstract: A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\% in performance while achieving a 58\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.

</details>


### [103] [Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art](https://arxiv.org/abs/2601.05152)
*Timofey Tomashevskiy*

Main category: cs.LG

TL;DR: 关于持续安全在线强化学习（COSRL）方法的综述论文，涵盖理论、挑战、分类方法和安全约束制定，旨在建立可靠的安全在线学习算法。


<details>
  <summary>Details</summary>
Motivation: 当前需要构建能够在非平稳环境中持续安全学习的在线强化学习算法，但现有研究分散且缺乏系统性分析，需要提供全面的理论框架和方法分类。

Method: 采用综述研究方法，首先讨论COSRL的理论基础和挑战，然后基于安全学习机制类型（考虑非平稳性适应）对方法进行分类，最后分类在线强化学习算法的安全约束制定方法。

Result: 提供了COSRL方法的系统分类和详细分析，建立了基于安全机制类型的方法分类体系，并系统化了安全约束的制定方法，为未来研究提供了清晰的框架。

Conclusion: 本文为持续安全在线强化学习领域提供了全面的综述和分类框架，指出了未来构建可靠安全在线学习算法的研究方向。

Abstract: This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.
  Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning

</details>
