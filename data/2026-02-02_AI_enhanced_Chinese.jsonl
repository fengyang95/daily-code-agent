{"id": "2601.22269", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22269", "abs": "https://arxiv.org/abs/2601.22269", "authors": ["Sahil Garg", "Brad Cheezum", "Sridhar Dutta", "Vishal Agarwal"], "title": "JAF: Judge Agent Forest", "comment": null, "summary": "Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.\n  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.\n  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.", "AI": {"tldr": "JAF\uff08Judge Agent Forest\uff09\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u6cd5\u5b98\u667a\u80fd\u4f53\u5bf9\u4e3b\u667a\u80fd\u4f53\u751f\u6210\u7684\u591a\u7ec4\u67e5\u8be2-\u54cd\u5e94\u5bf9\u8fdb\u884c\u8054\u5408\u63a8\u7406\uff0c\u800c\u975e\u5b64\u7acb\u8bc4\u4f30\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u548c\u81ea\u6211\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u6cd5\u5b98\u667a\u80fd\u4f53\u901a\u5e38\u5b64\u7acb\u8bc4\u4f30\u6bcf\u4e2a\u67e5\u8be2-\u54cd\u5e94\uff0c\u65e0\u6cd5\u5229\u7528\u8de8\u5b9e\u4f8b\u7684\u6a21\u5f0f\u548c\u4e0d\u4e00\u81f4\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u8fdb\u884c\u8054\u5408\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4f7f\u6cd5\u5b98\u80fd\u4ece\u6574\u4f53\u89d2\u5ea6\u8bc4\u4f30\u76f8\u5173\u54cd\u5e94\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u6709\u4ef7\u503c\u7684\u53cd\u9988\u3002", "method": "JAF\u7ed3\u5408\u4fe1\u5ff5\u4f20\u64ad\u548c\u96c6\u6210\u5b66\u4e60\u539f\u5219\uff1a\u901a\u8fc7\u91cd\u53e0\u7684\u4e0a\u4e0b\u6587\u90bb\u57df\u6784\u5efa\u77e5\u8bc6\u56fe\u7ed3\u6784\u4fc3\u8fdb\u6279\u8bc4\u4f20\u64ad\uff0c\u91cd\u590d\u968f\u673a\u5316\u8bc4\u4f30\u4ea7\u751f\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u5224\u65ad\u3002\u4f7f\u7528\u7075\u6d3b\u7684\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u7b97\u6cd5\uff0c\u6574\u5408\u8bed\u4e49\u5d4c\u5165\u3001LLM\u9a71\u52a8\u7684\u54c8\u5e0c\u8c13\u8bcd\u3001\u5206\u7c7b\u6807\u7b7e\u76d1\u7763\u548c\u76f8\u5173\u4fa7\u4fe1\u606f\uff0c\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\uff0c\u652f\u6301\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u5173\u7cfb\u611f\u77e5\u8303\u4f8b\u9009\u62e9\u3002", "result": "\u5728\u4e91\u9519\u8bef\u914d\u7f6e\u5206\u7c7b\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86JAF\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "JAF\u5c06\u6cd5\u5b98\u667a\u80fd\u4f53\u4ece\u5c40\u90e8\u8bc4\u4f30\u8005\u63d0\u5347\u4e3a\u6574\u4f53\u5b66\u4e60\u8005\uff0c\u901a\u8fc7\u8054\u5408\u63a8\u7406\u8de8\u5b9e\u4f8b\u6a21\u5f0f\uff0c\u4e3a\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u53cd\u9988\uff0c\u4fc3\u8fdb\u81ea\u6211\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2601.22290", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22290", "abs": "https://arxiv.org/abs/2601.22290", "authors": ["Khush Patel", "Siva Surendira", "Jithin George", "Shreyas Kapale"], "title": "The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution", "comment": "25 pages, 7 figures, 2 tables", "summary": "Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.", "AI": {"tldr": "\u63d0\u51faSix Sigma Agent\u67b6\u6784\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u3001\u5e76\u884c\u91c7\u6837\u548c\u5171\u8bc6\u6295\u7968\u5b9e\u73b0\u4f01\u4e1a\u7ea7\u53ef\u9760\u6027\uff0c\u5c06\u9519\u8bef\u7387\u4ece5%\u964d\u81f30.11%\uff0c\u6210\u672c\u964d\u4f4e80%", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5176\u6982\u7387\u6027\u672c\u8d28\u5bfc\u81f4\u53ef\u9760\u6027\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u4f01\u4e1a\u90e8\u7f72\u9700\u6c42", "method": "1) \u4efb\u52a1\u5206\u89e3\u4e3a\u539f\u5b50\u64cd\u4f5c\u7684\u4f9d\u8d56\u6811\uff1b2) \u5e76\u884c\u91c7\u6837\uff1a\u6bcf\u4e2a\u4efb\u52a1\u5728\u591a\u4e2aLLM\u4e0a\u5e76\u884c\u6267\u884cn\u6b21\uff1b3) \u52a8\u6001\u7f29\u653e\u5171\u8bc6\u6295\u7968\uff1a\u805a\u7c7b\u8f93\u51fa\u5e76\u9009\u62e9\u5f97\u7968\u6700\u591a\u7684\u7b54\u6848", "result": "\u8bc1\u660en\u6b21\u72ec\u7acb\u91c7\u6837\u53ef\u5c06\u7cfb\u7edf\u9519\u8bef\u964d\u81f3O(p^{ceil(n/2)})\uff0c5\u4e2a\u4ee3\u7406\u53ef\u5c065%\u9519\u8bef\u7387\u964d\u81f30.11%\uff0c13\u4e2a\u4ee3\u7406\u8fbe\u52303.4 DPMO\uff08\u516d\u897f\u683c\u739b\u6807\u51c6\uff09\uff0c\u53ef\u9760\u6027\u63d0\u534714,700\u500d\uff0c\u6210\u672c\u964d\u4f4e80%", "conclusion": "AI\u7cfb\u7edf\u53ef\u9760\u6027\u6e90\u4e8e\u539f\u5219\u6027\u5197\u4f59\u548c\u5171\u8bc6\u673a\u5236\uff0c\u800c\u975e\u5355\u7eaf\u6a21\u578b\u7f29\u653e\uff0c\u4e3a\u4f01\u4e1aAI\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2601.22311", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22311", "abs": "https://arxiv.org/abs/2601.22311", "authors": ["Zehong Wang", "Fang Wu", "Hongru Wang", "Xiangru Tang", "Bolian Li", "Zhenfei Yin", "Yijun Ma", "Yiyang Li", "Weixiang Sun", "Xiusi Chen", "Yanfang Ye"], "title": "Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents", "comment": null, "summary": "Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFLARE\u65b9\u6cd5\uff0c\u901a\u8fc7\u672a\u6765\u611f\u77e5\u89c4\u5212\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u4e2d\u7684\u77ed\u89c6\u95ee\u9898\uff0c\u4f7f\u65e9\u671f\u51b3\u7b56\u8003\u8651\u4e0b\u6e38\u540e\u679c\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u77ed\u65f6\u7a0b\u63a8\u7406\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u4e2d\u5931\u8d25\uff0c\u56e0\u4e3a\u9010\u6b65\u63a8\u7406\u8bf1\u5bfc\u7684\u9010\u6b65\u8d2a\u5a6a\u7b56\u7565\u65e0\u6cd5\u5904\u7406\u65e9\u671f\u884c\u52a8\u9700\u8981\u8003\u8651\u5ef6\u8fdf\u540e\u679c\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faFLARE\uff08\u672a\u6765\u611f\u77e5\u524d\u77bb\u4e0e\u5956\u52b1\u4f30\u8ba1\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u524d\u77bb\u3001\u4ef7\u503c\u4f20\u64ad\u548c\u6709\u9650\u627f\u8bfa\uff0c\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u672a\u6765\u611f\u77e5\u89c4\u5212\uff0c\u8ba9\u4e0b\u6e38\u7ed3\u679c\u5f71\u54cd\u65e9\u671f\u51b3\u7b56\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u3001\u667a\u80fd\u4f53\u6846\u67b6\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0cFLARE\u4e00\u81f4\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u89c4\u5212\u7ea7\u884c\u4e3a\uff0c\u4f7fLLaMA-8B+FLARE\u7ecf\u5e38\u8d85\u8d8aGPT-4o+\u6807\u51c6\u9010\u6b65\u63a8\u7406\u3002", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\u63a8\u7406\u4e0e\u89c4\u5212\u4e4b\u95f4\u7684\u660e\u786e\u533a\u522b\uff0c\u672a\u6765\u611f\u77e5\u89c4\u5212\u662f\u89e3\u51b3LLM\u667a\u80fd\u4f53\u957f\u65f6\u7a0b\u89c4\u5212\u5931\u8d25\u7684\u5173\u952e\u3002", "topic": "agent analysis"}}
{"id": "2601.22329", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.22329", "abs": "https://arxiv.org/abs/2601.22329", "authors": ["Ala N. Tak", "Amin Banayeeanzade", "Anahita Bolourani", "Fatemeh Bahrani", "Ashutosh Chaubey", "Sai Praneeth Karimireddy", "Norbert Schwarz", "Jonathan Gratch"], "title": "Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?", "comment": null, "summary": "Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of human behavior, it is critical to assess whether they exhibit analogous patterns of (ir)rationalities and biases. To this end, we evaluate multiple LLM families on (i) benchmarks testing core axioms of rational choice and (ii) classic decision domains from behavioral economics and social norms where emotions are known to shape judgment and choice. Across settings, we show that deliberate \"thinking\" reliably improves rationality and pushes models toward expected-value maximization. To probe human-like affective distortions and their interaction with reasoning, we use two emotion-steering methods: in-context priming (ICP) and representation-level steering (RLS). ICP induces strong directional shifts that are often extreme and difficult to calibrate, whereas RLS produces more psychologically plausible patterns but with lower reliability. Our results suggest that the same mechanisms that improve rationality also amplify sensitivity to affective interventions, and that different steering methods trade off controllability against human-aligned behavior. Overall, this points to a tension between reasoning and affective steering, with implications for both human simulation and the safe deployment of LLM-based decision systems.", "AI": {"tldr": "LLMs\u5728\u7406\u6027\u51b3\u7b56\u548c\u60c5\u611f\u504f\u89c1\u65b9\u9762\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6a21\u5f0f\uff0c\u7406\u6027\u601d\u8003\u80fd\u63d0\u5347\u51b3\u7b56\u7406\u6027\uff0c\u4f46\u60c5\u611f\u5f15\u5bfc\u4f1a\u5e72\u6270\u7406\u6027\uff0c\u4e0d\u540c\u5f15\u5bfc\u65b9\u6cd5\u5728\u53ef\u63a7\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u884c\u4e3a\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "motivation": "LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u62db\u8058\u3001\u533b\u7597\u548c\u7ecf\u6d4e\u51b3\u7b56\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u662f\u5426\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\uff08\u975e\uff09\u7406\u6027\u6a21\u5f0f\u548c\u504f\u89c1\uff0c\u8fd9\u5bf9\u4e8eLLMs\u53c2\u4e0e\u9ad8\u98ce\u9669\u51b3\u7b56\u6216\u4f5c\u4e3a\u4eba\u7c7b\u884c\u4e3a\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bc4\u4f30\u591a\u4e2aLLM\u5bb6\u65cf\u5728\u7406\u6027\u9009\u62e9\u6838\u5fc3\u516c\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u7ecf\u5178\u884c\u4e3a\u7ecf\u6d4e\u5b66\u51b3\u7b56\u9886\u57df\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e24\u79cd\u60c5\u611f\u5f15\u5bfc\u65b9\u6cd5\uff1a\u4e0a\u4e0b\u6587\u63d0\u793a\uff08ICP\uff09\u548c\u8868\u793a\u5c42\u5f15\u5bfc\uff08RLS\uff09\uff0c\u7814\u7a76\u7406\u6027\u601d\u8003\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u4ee5\u53ca\u60c5\u611f\u5f15\u5bfc\u5982\u4f55\u4e0e\u63a8\u7406\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u7406\u6027\"\u601d\u8003\"\u80fd\u53ef\u9760\u63d0\u9ad8\u7406\u6027\u5e76\u63a8\u52a8\u6a21\u578b\u8d8b\u5411\u671f\u671b\u4ef7\u503c\u6700\u5927\u5316\uff1bICP\u4ea7\u751f\u5f3a\u70c8\u4f46\u96be\u4ee5\u6821\u51c6\u7684\u65b9\u5411\u6027\u504f\u79fb\uff0cRLS\u4ea7\u751f\u66f4\u7b26\u5408\u5fc3\u7406\u5b66\u6a21\u5f0f\u4f46\u53ef\u9760\u6027\u8f83\u4f4e\uff1b\u63d0\u5347\u7406\u6027\u7684\u673a\u5236\u4e5f\u589e\u52a0\u4e86\u5bf9\u60c5\u611f\u5e72\u9884\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u63a8\u7406\u548c\u60c5\u611f\u5f15\u5bfc\u4e4b\u95f4\u5b58\u5728\u5f20\u529b\uff0c\u4e0d\u540c\u5f15\u5bfc\u65b9\u6cd5\u5728\u53ef\u63a7\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u884c\u4e3a\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u8fd9\u5bf9\u4eba\u7c7b\u6a21\u62df\u548cLLM\u51b3\u7b56\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2601.22169", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22169", "abs": "https://arxiv.org/abs/2601.22169", "authors": ["Anudeex Shetty", "Aditya Joshi", "Salil S. Kanhere"], "title": "In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement", "comment": "WIP", "summary": "Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\"\u9189\u9152\u8bed\u8a00\"\uff08\u9152\u7cbe\u5f71\u54cd\u4e0b\u4e66\u5199\u7684\u6587\u672c\uff09\u5982\u4f55\u9a71\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5931\u6548\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u8bf1\u5bfcLLM\u4ea7\u751f\u9189\u9152\u8bed\u8a00\u7684\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u8fd9\u4f1a\u663e\u8457\u589e\u52a0\u6a21\u578b\u8d8a\u72f1\u548c\u9690\u79c1\u6cc4\u9732\u7684\u98ce\u9669\u3002", "motivation": "\u4eba\u7c7b\u5728\u9152\u7cbe\u5f71\u54cd\u4e0b\u5bb9\u6613\u51fa\u73b0\u4e0d\u826f\u884c\u4e3a\u548c\u9690\u79c1\u6cc4\u9732\uff0c\u8bba\u6587\u63a2\u7d22\u5c06\u8fd9\u79cd\"\u9189\u9152\u8bed\u8a00\"\u73b0\u8c61\u5e94\u7528\u4e8eLLMs\uff0c\u7814\u7a76\u5176\u662f\u5426\u4f1a\u5bfc\u81f4\u7c7b\u4f3c\u7684\u5b89\u5168\u5931\u6548\uff0c\u4ece\u800c\u63ed\u793aLLM\u5b89\u5168\u6027\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u8bf1\u5bfcLLM\u4ea7\u751f\u9189\u9152\u8bed\u8a00\u7684\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u793a\u5de5\u7a0b\uff1b2\uff09\u56e0\u679c\u5fae\u8c03\uff1b3\uff09\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u3002\u57285\u4e2aLLMs\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528JailbreakBench\uff08\u8d8a\u72f1\u57fa\u51c6\uff09\u548cConfAIde\uff08\u9690\u79c1\u6cc4\u9732\u57fa\u51c6\uff09\uff0c\u7ed3\u5408\u4eba\u5de5\u8bc4\u4f30\u548cLLM\u8bc4\u4f30\u5668\u8fdb\u884c\u7efc\u5408\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8bf1\u5bfc\u9189\u9152\u8bed\u8a00\u7684LLMs\u5728JailbreakBench\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8d8a\u72f1\u6613\u611f\u6027\uff08\u5373\u4f7f\u5728\u9632\u5fa1\u63aa\u65bd\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff09\uff0c\u5728ConfAIde\u4e0a\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u4e5f\u663e\u8457\u589e\u52a0\uff0c\u6548\u679c\u4f18\u4e8e\u5148\u524d\u62a5\u9053\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u9189\u9152\u884c\u4e3a\u4e0eLLMs\u7684\u62df\u4eba\u5316\u8868\u73b0\u4e4b\u95f4\u5b58\u5728\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "\u9189\u9152\u8bed\u8a00\u8bf1\u5bfc\u65b9\u6cd5\u7b80\u5355\u9ad8\u6548\uff0c\u53ef\u80fd\u6210\u4e3a\u5bf9\u6297LLM\u5b89\u5168\u8c03\u4f18\u7684\u6f5c\u5728\u624b\u6bb5\uff0c\u7a81\u663e\u4e86LLM\u5b89\u5168\u6027\u7684\u91cd\u5927\u98ce\u9669\u3002\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u7c7b\u9189\u9152\u884c\u4e3a\u4e0eLLM\u62df\u4eba\u5316\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3LLM\u5b89\u5168\u5931\u6548\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2601.22208", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22208", "abs": "https://arxiv.org/abs/2601.22208", "authors": ["Evelien Riddell", "James Riddell", "Gengyi Sun", "Micha\u0142 Antkiewicz", "Krzysztof Czarnecki"], "title": "Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis", "comment": "FORGE 2026", "summary": "Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.\n  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6839\u56e0\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u6846\u67b6\u9694\u79bbLLM\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u572848000\u4e2a\u6545\u969c\u573a\u666f\u4e2d\u6d4b\u8bd5\u4e866\u4e2aLLM\u6a21\u578b\uff0c\u5e76\u5efa\u7acb\u4e8616\u79cd\u5e38\u89c1\u63a8\u7406\u9519\u8bef\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u73b0\u4ee3\u4e91\u7cfb\u7edf\u7684\u9ad8\u5ea6\u5206\u5e03\u5f0f\u548c\u76f8\u4e92\u4f9d\u8d56\u7279\u6027\u4f7f\u5f97\u6839\u56e0\u5206\u6790\u53d8\u5f97\u590d\u6742\uff0c\u7279\u522b\u662f\u591a\u8df3\u6545\u969c\u4f20\u64ad\u573a\u666f\u3002\u867d\u7136LLM\u4e3a\u81ea\u52a8\u5316RCA\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5386\u53f2\u4e8b\u4ef6\u8bed\u6599\u5e93\uff0c\u8981\u4e48\u5904\u7406\u8d85\u51faLLM\u5bb9\u91cf\u7684\u6d77\u91cf\u9065\u6d4b\u6570\u636e\uff0c\u8981\u4e48\u5c06\u63a8\u7406\u5d4c\u5165\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u8fd9\u4e9b\u8bbe\u8ba1\u9009\u62e9\u6a21\u7cca\u4e86\u5931\u8d25\u662f\u6e90\u4e8e\u63a8\u7406\u672c\u8eab\u8fd8\u662f\u5916\u56f4\u8bbe\u8ba1\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63a7\u5236\u5b9e\u9a8c\u6846\u67b6\u6765\u9694\u79bbLLM\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u4f7f\u7528\u7b80\u5316\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u7a81\u51faLLM\u3002\u8bc4\u4f30\u4e866\u4e2aLLM\u5728\u4e24\u4e2a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff08ReAct\u548cPlan-and-Execute\uff09\u548c\u4e00\u4e2a\u975e\u667a\u80fd\u4f53\u57fa\u7ebf\u4e0b\u7684\u8868\u73b0\uff0c\u57fa\u4e8e\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6848\u4f8b\u7814\u7a76\uff08GAIA\u548cOpenRCA\uff09\u3002\u6267\u884c\u4e8648000\u4e2a\u6a21\u62df\u6545\u969c\u573a\u666f\uff0c\u603b\u8ba1228\u5929\u6267\u884c\u65f6\u95f4\u3002\u6d4b\u91cf\u4e86\u6839\u56e0\u51c6\u786e\u6027\u548c\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\u8d28\u91cf\uff0c\u5e76\u5efa\u7acb\u4e8616\u79cd\u5e38\u89c1RCA\u63a8\u7406\u9519\u8bef\u7684\u6807\u8bb0\u5206\u7c7b\u4f53\u7cfb\uff0c\u4f7f\u7528LLM-as-a-Judge\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u7ed3\u679c\u9610\u660e\u4e86\u5f53\u524d\u5f00\u6e90LLM\u5728\u591a\u8df3RCA\u4e2d\u7684\u6210\u529f\u548c\u5931\u8d25\u4e4b\u5904\uff0c\u91cf\u5316\u4e86\u5bf9\u8f93\u5165\u6570\u636e\u6a21\u6001\u7684\u654f\u611f\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u80fd\u591f\u9884\u6d4b\u6700\u7ec8\u6b63\u786e\u6027\u7684\u63a8\u7406\u5931\u8d25\u6a21\u5f0f\u3002\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u53ef\u590d\u73b0\u7684\u5b9e\u8bc1\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63a8\u7406\u9a71\u52a8\u7684\u7cfb\u7edf\u8bca\u65ad\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u6846\u67b6\u548c\u8be6\u7ec6\u7684\u5931\u8d25\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e2e\u52a9\u672a\u6765\u5de5\u4f5c\u66f4\u597d\u5730\u7406\u89e3\u548c\u6539\u8fdbLLM\u5728\u6839\u56e0\u5206\u6790\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.22297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22297", "abs": "https://arxiv.org/abs/2601.22297", "authors": ["Chenxi Liu", "Yanshuo Chen", "Ruibo Chen", "Tianyi Xiong", "Tong Zheng", "Heng Huang"], "title": "Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning", "comment": null, "summary": "The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.", "AI": {"tldr": "SDRL\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u5355\u4e2aLLM\u8fdb\u884c\u81ea\u6211\u8fa9\u8bba\uff0c\u540c\u65f6\u63d0\u5347\u5176\u72ec\u7acb\u89e3\u51b3\u95ee\u9898\u80fd\u529b\u548c\u4ece\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dRLVR\u65b9\u6cd5\u901a\u5e38\u8bad\u7ec3LLM\u5355\u72ec\u89e3\u51b3\u95ee\u9898\uff0c\u6ca1\u6709\u660e\u786e\u51c6\u5907\u5b83\u4eec\u4ece\u8fa9\u8bba\u4e2d\u51fa\u73b0\u7684\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u4e2d\u7efc\u5408\u53d7\u76ca\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u589e\u5f3a\u5355\u4e2a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53c8\u80fd\u4f7f\u5176\u5728\u8fa9\u8bba\u4e2d\u6709\u6548\u534f\u4f5c\u3002", "method": "SDRL\u6846\u67b6\uff1a1) \u5bf9\u540c\u4e00\u63d0\u793a\u91c7\u6837\u591a\u4e2a\u5019\u9009\u89e3\u51b3\u65b9\u6848\uff1b2) \u6784\u5efa\u5305\u542b\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u7684\u8fa9\u8bba\u4e0a\u4e0b\u6587\uff1b3) \u57fa\u4e8e\u6b64\u4e0a\u4e0b\u6587\u751f\u6210\u7b2c\u4e8c\u8f6e\u54cd\u5e94\uff1b4) \u8054\u5408\u4f18\u5316\u521d\u59cb\u54cd\u5e94\u548c\u8fa9\u8bba\u6761\u4ef6\u4e0b\u7684\u54cd\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSDRL\u540c\u65f6\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u6574\u4f53\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u5355\u4e2a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "SDRL\u6210\u529f\u5730\u5c06\u8fa9\u8bba\u673a\u5236\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\uff0c\u4f7f\u5355\u4e2aLLM\u65e2\u80fd\u4f5c\u4e3a\u5f3a\u5927\u7684\u72ec\u7acb\u6c42\u89e3\u5668\uff0c\u53c8\u80fd\u4f5c\u4e3a\u6709\u6548\u7684\u8fa9\u8bba\u53c2\u4e0e\u8005\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u80fd\u529b\u7684\u53cc\u91cd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22361", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22361", "abs": "https://arxiv.org/abs/2601.22361", "authors": ["Yupeng Cao", "Chengyang He", "Yangyang Yu", "Ping Wang", "K. P. Subbalakshmi"], "title": "MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment", "comment": null, "summary": "Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.", "AI": {"tldr": "MERMAID\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bb0\u5fc6\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u771f\u5b9e\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u68c0\u7d22\u4e0e\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u52a8\u6001\u8bc1\u636e\u83b7\u53d6\u548c\u8de8\u58f0\u660e\u8bc1\u636e\u590d\u7528\uff0c\u5728\u591a\u4e2a\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u771f\u5b9e\u6027\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u5c06\u8bc1\u636e\u68c0\u7d22\u89c6\u4e3a\u9759\u6001\u3001\u5b64\u7acb\u7684\u6b65\u9aa4\uff0c\u672a\u80fd\u6709\u6548\u7ba1\u7406\u6216\u8de8\u58f0\u660e\u590d\u7528\u68c0\u7d22\u5230\u7684\u8bc1\u636e\uff0c\u5bfc\u81f4\u68c0\u7d22\u5197\u4f59\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faMERMAID\u6846\u67b6\uff0c\u6574\u5408\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u641c\u7d22\u3001\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u548c\u6301\u4e45\u6027\u8bb0\u5fc6\u6a21\u5757\uff0c\u91c7\u7528Reason-Action\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u52a8\u6001\u8bc1\u636e\u83b7\u53d6\u548c\u8de8\u58f0\u660e\u8bc1\u636e\u590d\u7528\u3002", "result": "\u5728\u4e09\u4e2a\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\u548c\u4e24\u4e2a\u58f0\u660e\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u591a\u79cdLLM\uff08GPT\u3001LLaMA\u3001Qwen\u7cfb\u5217\uff09\u8bc4\u4f30\uff0cMERMAID\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u534f\u540c\u68c0\u7d22\u3001\u63a8\u7406\u548c\u8bb0\u5fc6\uff0cMERMAID\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u7684\u771f\u5b9e\u6027\u8bc4\u4f30\uff0c\u51cf\u5c11\u5197\u4f59\u641c\u7d22\uff0c\u63d0\u9ad8\u9a8c\u8bc1\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.22364", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22364", "abs": "https://arxiv.org/abs/2601.22364", "authors": ["Eghbal A. Hosseini", "Yuxuan Li", "Yasaman Bahri", "Declan Campbell", "Andrew Kyle Lampinen"], "title": "Context Structure Reshapes the Representational Geometry of Language Models", "comment": null, "summary": "Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \\emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.", "AI": {"tldr": "LLMs\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u8868\u73b0\u51fa\u4e24\u79cd\u4e0d\u540c\u7684\u8868\u5f81\u76f4\u7ebf\u5316\u6a21\u5f0f\uff1a\u5728\u8fde\u7eed\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4e0a\u4e0b\u6587\u589e\u52a0\u4f1a\u63d0\u5347\u795e\u7ecf\u8f68\u8ff9\u7684\u76f4\u7ebf\u5316\u7a0b\u5ea6\u5e76\u6539\u5584\u9884\u6d4b\uff1b\u800c\u5728\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u76f4\u7ebf\u5316\u4ec5\u51fa\u73b0\u5728\u6709\u660e\u786e\u7ed3\u6784\u7684\u9636\u6bb5\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u662f\u5426\u4f1a\u51fa\u73b0\u8868\u5f81\u76f4\u7ebf\u5316\u73b0\u8c61\uff0c\u63a2\u7d22ICL\u662f\u5426\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8fc7\u7a0b\uff0c\u8fd8\u662f\u6839\u636e\u4efb\u52a1\u7ed3\u6784\u52a8\u6001\u9009\u62e9\u4e0d\u540c\u7b56\u7565\u3002", "method": "\u5728Gemma 2\u6a21\u578b\u4e0a\u6d4b\u91cf\u591a\u79cd\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u8868\u5f81\u76f4\u7ebf\u5316\u7a0b\u5ea6\uff0c\u5206\u6790\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\uff08\u8fde\u7eed\u9884\u6d4bvs\u7ed3\u6784\u5316\u9884\u6d4b\uff09\u4e2d\u795e\u7ecf\u8f68\u8ff9\u7684\u53d8\u5316\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0ICL\u4e2d\u5b58\u5728\u4e8c\u5206\u73b0\u8c61\uff1a\u8fde\u7eed\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4e0a\u4e0b\u6587\u589e\u52a0\u4f1a\u63d0\u5347\u8868\u5f81\u76f4\u7ebf\u5316\u7a0b\u5ea6\u5e76\u4e0e\u9884\u6d4b\u6539\u8fdb\u76f8\u5173\uff1b\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u76f4\u7ebf\u5316\u4ec5\u51fa\u73b0\u5728\u6709\u660e\u786e\u7ed3\u6784\u7684\u9636\u6bb5\uff0c\u5176\u4ed6\u9636\u6bb5\u6d88\u5931\u3002", "conclusion": "ICL\u4e0d\u662f\u5355\u4e00\u8fc7\u7a0b\uff0cLLMs\u50cf\u745e\u58eb\u519b\u5200\u4e00\u6837\u6839\u636e\u4efb\u52a1\u7ed3\u6784\u52a8\u6001\u9009\u62e9\u7b56\u7565\uff0c\u53ea\u6709\u90e8\u5206\u7b56\u7565\u4f1a\u4ea7\u751f\u8868\u5f81\u76f4\u7ebf\u5316\u3002", "topic": "agent analysis"}}
{"id": "2601.22597", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22597", "abs": "https://arxiv.org/abs/2601.22597", "authors": ["Ryo Fujii", "Makoto Morishita", "Kazuki Yano", "Jun Suzuki"], "title": "TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks", "comment": "Accepted to EACL 2026 Main, camera-ready", "summary": "With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.", "AI": {"tldr": "TimeMachine-bench\u662f\u4e00\u4e2a\u8bc4\u4f30Python\u9879\u76ee\u8f6f\u4ef6\u8fc1\u79fb\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u56e0\u4f9d\u8d56\u66f4\u65b0\u800c\u6d4b\u8bd5\u5931\u8d25\u7684GitHub\u4ed3\u5e93\uff0c\u5e76\u8bc4\u4f30\u4e8611\u4e2aLLM\u6a21\u578b\u5728\u8fc1\u79fb\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u8f6f\u4ef6\u8fc1\u79fb\u662f\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u9002\u5e94\u73af\u5883\u53d8\u5316\u7684\u5173\u952e\u8fc7\u7a0b\uff0c\u4f46\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\u88ab\u5ffd\u89c6\u3002\u9700\u8981\u5efa\u7acb\u8bc4\u4f30\u8f6f\u4ef6\u8fc1\u79fb\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5TimeMachine-bench\uff0c\u5305\u542b\u56e0\u4f9d\u8d56\u66f4\u65b0\u5bfc\u81f4\u6d4b\u8bd5\u5931\u8d25\u7684GitHub\u4ed3\u5e93\uff0c\u5e76\u521b\u5efa\u4eba\u5de5\u9a8c\u8bc1\u7684\u5b50\u96c6\u786e\u4fdd\u95ee\u9898\u53ef\u89e3\u6027\u3002\u8bc4\u4f30\u4e86\u57fa\u4e8e11\u4e2a\u6a21\u578b\uff08\u5305\u62ec\u5f00\u6e90\u548cSOTA LLM\uff09\u7684\u4ee3\u7406\u57fa\u7ebf\u3002", "result": "LLM\u5728\u8fc1\u79fb\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u91cd\u5927\u53ef\u9760\u6027\u6311\u6218\uff0c\u5305\u62ec\u5229\u7528\u4f4e\u6d4b\u8bd5\u8986\u76d6\u7387\u4ea7\u751f\u865a\u5047\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u53ca\u7531\u4e8e\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u4e0d\u4f73\u5bfc\u81f4\u7684\u4e0d\u5fc5\u8981\u7f16\u8f91\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u9ad8LLM\u5728\u8f6f\u4ef6\u8fc1\u79fb\u4efb\u52a1\u4e0a\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u65b9\u9762\u3002", "topic": "swe benchmark"}}
{"id": "2601.22211", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22211", "abs": "https://arxiv.org/abs/2601.22211", "authors": ["Lingkai Kong", "Anagha Satish", "Hezi Jiang", "Akseli Kangaslahti", "Andrew Ma", "Wenbo Chen", "Mingxiao Song", "Lily Xu", "Milind Tambe"], "title": "Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions", "comment": null, "summary": "Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \\emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \\emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\\% across a range of challenging combinatorial RL tasks.", "AI": {"tldr": "LSFlow\uff1a\u4e00\u79cd\u7528\u4e8e\u7ec4\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u5728\u7403\u9762\u6d41\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u968f\u673a\u7b56\u7565\u5b66\u4e60\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5229\u7528\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u5668\u4fdd\u8bc1\u52a8\u4f5c\u53ef\u884c\u6027\uff0c\u89e3\u51b3\u4e86\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u7ec4\u5408\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u52a8\u4f5c\u7a7a\u95f4\u6307\u6570\u7ea7\u589e\u957f\u548c\u590d\u6742\u53ef\u884c\u6027\u7ea6\u675f\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c06\u7279\u5b9a\u4efb\u52a1\u4ef7\u503c\u51fd\u6570\u5d4c\u5165\u7ea6\u675f\u4f18\u5316\u7a0b\u5e8f\uff0c\u8981\u4e48\u5b66\u4e60\u786e\u5b9a\u6027\u7ed3\u6784\u5316\u7b56\u7565\uff0c\u727a\u7272\u4e86\u901a\u7528\u6027\u548c\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51faLSFlow\u65b9\u6cd5\uff1a1\uff09\u5728\u7d27\u51d1\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u7403\u9762\u6d41\u5339\u914d\u5b66\u4e60\u968f\u673a\u7b56\u7565\uff1b2\uff09\u4f7f\u7528\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u5668\u5c06\u6f5c\u5728\u6837\u672c\u6620\u5c04\u4e3a\u6709\u6548\u7ed3\u6784\u5316\u52a8\u4f5c\uff1b3\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u8bad\u7ec3\u4ef7\u503c\u7f51\u7edc\uff0c\u907f\u514d\u91cd\u590d\u6c42\u89e3\u5668\u8c03\u7528\uff1b4\uff09\u5f15\u5165\u5e73\u6ed1\u8d1d\u5c14\u66fc\u7b97\u5b50\u5904\u7406\u6c42\u89e3\u5668\u5f15\u8d77\u7684\u5206\u6bb5\u5e38\u6570\u548c\u4e0d\u8fde\u7eed\u4ef7\u503c\u666f\u89c2\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cLSFlow\u5e73\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd520.6%\u3002", "conclusion": "LSFlow\u6210\u529f\u5c06\u73b0\u4ee3\u751f\u6210\u7b56\u7565\u7684\u8868\u8fbe\u80fd\u529b\u5f15\u5165\u7ec4\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u540c\u65f6\u901a\u8fc7\u8bbe\u8ba1\u4fdd\u8bc1\u52a8\u4f5c\u53ef\u884c\u6027\uff0c\u89e3\u51b3\u4e86\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22230", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22230", "abs": "https://arxiv.org/abs/2601.22230", "authors": ["Peijia Qin", "Ruiyi Zhang", "Qi Cao", "Pengtao Xie"], "title": "DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation", "comment": null, "summary": "Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.", "AI": {"tldr": "\u63d0\u51faDAJ\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u6570\u636e\u91cd\u52a0\u6743\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u57fa\u4e8e\u63a8\u7406\u7684LLM\u8bc4\u5224\u5668\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u4f18\u5316\u6cdb\u5316\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u751f\u6210\u6d4b\u8bd5\u65f6\u7f29\u653e\u4e2d\u5b9e\u73b0SOTA\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u901a\u5e38\u4f9d\u8d56Best-of-N\u9009\u62e9\uff0c\u4f46\u8bad\u7ec3\u53ef\u9760\u7684LLM\u8bc4\u5224\u5668\u9762\u4e34\u6311\u6218\uff1a\u5206\u5e03\u504f\u79fb\u4e25\u91cd\uff08\u7b80\u5355\u4e0e\u56f0\u96be\u95ee\u9898\u4e0d\u5e73\u8861\u3001\u8bad\u7ec3\u4efb\u52a1\u4e0e\u8bc4\u4f30\u57fa\u51c6\u4e0d\u5339\u914d\u3001\u8bad\u7ec3\u6570\u636e\u4e0e\u63a8\u7406\u6a21\u578b\u884c\u4e3a\u4e0d\u5339\u914d\uff09\u3002", "method": "\u63d0\u51faDAJ\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u63a8\u7406\u7684LLM\u8bc4\u5224\u5668\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u8bad\u7ec3\uff1b2\uff09\u53cc\u5c42\u6570\u636e\u91cd\u52a0\u6743\u5b66\u4e60\u6846\u67b6\uff0c\u5b66\u4e60\u6570\u636e\u91cd\u8981\u6027\u6743\u91cd\uff08\u57df\u7ea7\u6216\u5b9e\u4f8b\u7ea7\uff09\u4ee5\u4f18\u5316\u5728\u76ee\u6807\u57fa\u51c6\u5bf9\u9f50\u7684\u5143\u96c6\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "result": "\u5728LiveCodeBench\u548cBigCodeBench\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u548c\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "DAJ\u662f\u9996\u4e2a\u5c06\u6570\u636e\u91cd\u52a0\u6743\u5e94\u7528\u4e8eLLM\u8bc4\u5224\u5668\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u80fd\u81ea\u52a8\u5f3a\u8c03\u56f0\u96be\u95ee\u9898\u3001\u5206\u5e03\u5185\u6837\u672c\u548c\u8f68\u8ff9\u5bf9\u9f50\u6570\u636e\uff0c\u65e0\u9700\u624b\u5de5\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u6709\u6548\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2601.22249", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22249", "abs": "https://arxiv.org/abs/2601.22249", "authors": ["Ruiyi Zhang", "Peijia Qin", "Qi Cao", "Eric Xue", "Pengtao Xie"], "title": "FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation", "comment": null, "summary": "Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.", "AI": {"tldr": "FunPRM\u901a\u8fc7\u5c06\u4ee3\u7801\u7ec4\u7ec7\u6210\u51fd\u6570\u4f5c\u4e3a\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u5f15\u5165\u5143\u5b66\u4e60\u5956\u52b1\u4fee\u6b63\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\u4e0a\u4ecd\u7136\u7ecf\u5e38\u5931\u8d25\u3002\u73b0\u6709\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u65b9\u6cd5\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u4ee3\u7801\u7f3a\u4e4f\u6709\u610f\u4e49\u7684\u6b65\u9aa4\u5206\u89e3\uff0c\u4e14\u90e8\u5206\u89e3\u51b3\u65b9\u6848\u6b63\u786e\u6027\u8bc4\u5206\u5b58\u5728\u566a\u58f0\u3002", "method": "FunPRM\u91c7\u7528\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\uff1a1) \u63d0\u793aLLM\u751f\u6210\u6a21\u5757\u5316\u3001\u51fd\u6570\u5316\u7684\u4ee3\u7801\uff0c\u5c06\u51fd\u6570\u4f5c\u4e3aPRM\u63a8\u7406\u6b65\u9aa4\uff1b2) \u5f15\u5165\u5143\u5b66\u4e60\u5956\u52b1\u4fee\u6b63\u673a\u5236\uff0c\u5229\u7528\u5355\u5143\u6d4b\u8bd5\u83b7\u5f97\u7684\u5e72\u51c0\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u5956\u52b1\u6765\u51c0\u5316\u566a\u58f0\u7684\u90e8\u5206\u89e3\u51b3\u65b9\u6848\u5956\u52b1\u3002", "result": "\u5728LiveCodeBench\u548cBigCodeBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFunPRM\u5728\u4e94\u4e2a\u57fa\u7840LLM\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u4e0eO4-mini\u7ed3\u5408\u65f6\u5728LiveCodeBench\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cFunPRM\u751f\u6210\u7684\u4ee3\u7801\u5bf9\u5f00\u53d1\u8005\u6765\u8bf4\u66f4\u5177\u53ef\u8bfb\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002", "conclusion": "FunPRM\u901a\u8fc7\u5c06\u4ee3\u7801\u7ec4\u7ec7\u6210\u51fd\u6570\u4f5c\u4e3a\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u5229\u7528\u5143\u5b66\u4e60\u51c0\u5316\u5956\u52b1\u4fe1\u53f7\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u751f\u6210\u4e2dPRM\u65b9\u6cd5\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.22528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22528", "abs": "https://arxiv.org/abs/2601.22528", "authors": ["Hongze Mi", "Yibo Feng", "WenJie Lu", "Song Cao", "Jinyuan Li", "Yanming Li", "Xuelin Zhang", "Haotian Luo", "Songyang Peng", "He Cui", "Tengfei Tian", "Jun Fang", "Hua Chai", "Naiqiang Tan"], "title": "Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution", "comment": null, "summary": "Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.", "AI": {"tldr": "\u63d0\u51faDarwinian Memory System (DMS)\uff0c\u4e00\u79cd\u53d7\u81ea\u7136\u9009\u62e9\u542f\u53d1\u7684\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3MLLM\u4ee3\u7406\u5728GUI\u81ea\u52a8\u5316\u4e2d\u957f\u671f\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u751f\u6001\u7cfb\u7edf\u63d0\u5347\u4efb\u52a1\u6210\u529f\u738733.9%\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524dMLLM\u4ee3\u7406\u5728GUI\u81ea\u52a8\u5316\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u96be\u4ee5\u5904\u7406\u957f\u8de8\u5ea6\u8de8\u5e94\u7528\u4efb\u52a1\uff1b2) \u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u7c92\u5ea6\u4e0d\u5339\u914d\uff08\u9ad8\u5c42\u610f\u56fe\u4e0e\u4f4e\u5c42\u6267\u884c\u8131\u8282\uff09\u548c\u4e0a\u4e0b\u6587\u6c61\u67d3\uff08\u8fc7\u65f6\u7ecf\u9a8c\u79ef\u7d2f\u5bfc\u81f4\u5e7b\u89c9\uff09\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDarwinian Memory System (DMS)\uff0c\u53d7\u8fbe\u5c14\u6587\u8fdb\u5316\u8bba\u542f\u53d1\uff0c\u5c06\u8bb0\u5fc6\u6784\u5efa\u4e3a\u52a8\u6001\u751f\u6001\u7cfb\u7edf\u3002\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5c06\u590d\u6742\u8f68\u8ff9\u5206\u89e3\u4e3a\u72ec\u7acb\u53ef\u91cd\u7528\u5355\u5143\uff1b2) \u5b9e\u65bd\u6548\u7528\u9a71\u52a8\u7684\u81ea\u7136\u9009\u62e9\u673a\u5236\uff0c\u8ffd\u8e2a\u751f\u5b58\u4ef7\u503c\uff1b3) \u4e3b\u52a8\u526a\u679d\u6b21\u4f18\u8def\u5f84\u5e76\u6291\u5236\u9ad8\u98ce\u9669\u8ba1\u5212\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u591a\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDMS\u65e0\u9700\u8bad\u7ec3\u6210\u672c\u6216\u67b6\u6784\u5f00\u9500\u5373\u53ef\u63d0\u5347\u901a\u7528MLLM\u6027\u80fd\uff1a\u5e73\u5747\u6210\u529f\u7387\u63d0\u534718.0%\uff0c\u6267\u884c\u7a33\u5b9a\u6027\u63d0\u534733.9%\uff0c\u540c\u65f6\u964d\u4f4e\u4efb\u52a1\u5ef6\u8fdf\u3002", "conclusion": "DMS\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u81ea\u7136\u9009\u62e9\u673a\u5236\u89e3\u51b3GUI\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u7ba1\u7406\u95ee\u9898\uff0c\u4e3aMLLM\u4ee3\u7406\u5728\u52a8\u6001GUI\u73af\u5883\u4e2d\u7684\u957f\u671f\u8de8\u5e94\u7528\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.22386", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.22386", "abs": "https://arxiv.org/abs/2601.22386", "authors": ["Jamiu Adekunle Idowu", "Ahmed Almasoud"], "title": "Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading", "comment": null, "summary": "Automated essay scoring (AES) systems increasingly rely on large language models, yet little is known about how architectural choices shape their performance across different essay quality levels. This paper evaluates single-agent and multi-agent LLM architectures for essay grading using the ASAP 2.0 corpus. Our multi-agent system decomposes grading into three specialist agents (Content, Structure, Language) coordinated by a Chairman Agent that implements rubric-aligned logic including veto rules and score capping. We test both architectures in zero-shot and few-shot conditions using GPT-5.1. Results show that the multi-agent system is significantly better at identifying weak essays while the single-agent system performs better on mid-range essays. Both architectures struggle with high-quality essays. Critically, few-shot calibration emerges as the dominant factor in system performance -- providing just two examples per score level improves QWK by approximately 26% for both architectures. These findings suggest architectural choice should align with specific deployment priorities, with multi-agent AI particularly suited for diagnostic screening of at-risk students, while single-agent models provide a cost-effective solution for general assessment.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\u5728\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8bc6\u522b\u5f31\u4f5c\u6587\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4e2d\u6863\u4f5c\u6587\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4e24\u79cd\u67b6\u6784\u5728\u9ad8\u8d28\u91cf\u4f5c\u6587\u4e0a\u90fd\u8868\u73b0\u4e0d\u4f73\u3002\u5c11\u91cf\u793a\u4f8b\u6821\u51c6\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u4e0d\u540c\u67b6\u6784\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u5176\u5728\u4e0d\u540c\u8d28\u91cf\u4f5c\u6587\u4e0a\u7684\u8868\u73b0\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\u5728\u4f5c\u6587\u8bc4\u5206\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528ASAP 2.0\u8bed\u6599\u5e93\u8bc4\u4f30\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\u3002\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5c06\u8bc4\u5206\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\uff08\u5185\u5bb9\u3001\u7ed3\u6784\u3001\u8bed\u8a00\uff09\uff0c\u7531\u4e3b\u5e2d\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u5b9e\u65bd\u5305\u62ec\u5426\u51b3\u89c4\u5219\u548c\u5206\u6570\u4e0a\u9650\u7684\u8bc4\u5206\u6807\u51c6\u5bf9\u9f50\u903b\u8f91\u3002\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u4f7f\u7528GPT-5.1\u6d4b\u8bd5\u4e24\u79cd\u67b6\u6784\u3002", "result": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8bc6\u522b\u5f31\u4f5c\u6587\u65b9\u9762\u663e\u8457\u66f4\u597d\uff0c\u800c\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4e2d\u6863\u4f5c\u6587\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u4e24\u79cd\u67b6\u6784\u5728\u9ad8\u8d28\u91cf\u4f5c\u6587\u4e0a\u90fd\u8868\u73b0\u4e0d\u4f73\u3002\u5c11\u6837\u672c\u6821\u51c6\u662f\u7cfb\u7edf\u6027\u80fd\u7684\u4e3b\u5bfc\u56e0\u7d20\u2014\u2014\u6bcf\u4e2a\u5206\u6570\u7ea7\u522b\u4ec5\u63d0\u4f9b\u4e24\u4e2a\u793a\u4f8b\u5c31\u80fd\u5c06QWK\u63d0\u9ad8\u7ea626%\u3002", "conclusion": "\u67b6\u6784\u9009\u62e9\u5e94\u4e0e\u5177\u4f53\u90e8\u7f72\u4f18\u5148\u7ea7\u5bf9\u9f50\uff1a\u591a\u667a\u80fd\u4f53AI\u7279\u522b\u9002\u5408\u5bf9\u6709\u98ce\u9669\u5b66\u751f\u8fdb\u884c\u8bca\u65ad\u6027\u7b5b\u67e5\uff0c\u800c\u5355\u667a\u80fd\u4f53\u6a21\u578b\u4e3a\u4e00\u822c\u8bc4\u4f30\u63d0\u4f9b\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.22530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22530", "abs": "https://arxiv.org/abs/2601.22530", "authors": ["Tung Sum Thomas Kwok", "Xinyu Wang", "Hengzhi He", "Xiaofeng Lin", "Peng Lu", "Liheng Ma", "Chunhe Wang", "Ying Nian Wu", "Lei Ding", "Guang Cheng"], "title": "Enhancing TableQA through Verifiable Reasoning Trace Reward", "comment": null, "summary": "A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .", "AI": {"tldr": "RE-Tab\u662f\u4e00\u4e2a\u7528\u4e8eTableQA\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u3001\u514d\u8bad\u7ec3\u5956\u52b1\u5efa\u6a21\u589e\u5f3a\u8f68\u8ff9\u641c\u7d22\uff0c\u5728\u8868\u683c\u8f6c\u6362\u4e2d\u63d0\u4f9b\u663e\u5f0f\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347QA\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "TableQA\u4ee3\u7406\u7684\u8bad\u7ec3\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u7b54\u6848\u4e0d\u80fd\u4ece\u9759\u6001\u8f93\u5165\u63a8\u65ad\uff0c\u800c\u9700\u8981\u901a\u8fc7\u8868\u683c\u72b6\u6001\u7684\u9010\u6b65\u8f6c\u6362\u8fdb\u884c\u63a8\u7406\uff0c\u8fd9\u5f15\u5165\u4e86\u591a\u6b65\u63a8\u7406\u590d\u6742\u6027\u548c\u73af\u5883\u4ea4\u4e92\u3002\u7814\u7a76\u95ee\u9898\u662f\uff1a\u5bf9\u8868\u683c\u8f6c\u6362\u52a8\u4f5c\u7684\u663e\u5f0f\u53cd\u9988\u80fd\u5426\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff1f", "method": "\u63d0\u51faRE-Tab\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u3001\u514d\u8bad\u7ec3\u7684\u5956\u52b1\u5efa\u6a21\u589e\u5f3a\u8f68\u8ff9\u641c\u7d22\u3002\u5728\u72b6\u6001\u8f6c\u6362\uff08\"\u6700\u4f73\u52a8\u4f5c\u662f\u4ec0\u4e48\uff1f\"\uff09\u548c\u6a21\u62df\u63a8\u7406\uff08\"\u6211\u5bf9\u8f93\u51fa\u786e\u5b9a\u5417\uff1f\"\uff09\u9636\u6bb5\u63d0\u4f9b\u663e\u5f0f\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u5f15\u5bfc\u4ee3\u7406\u5728\u8868\u683c\u72b6\u6001\u4e2d\u7684\u5bfc\u822a\u3002", "result": "RE-Tab\u5728TableQA\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e\u8fd125%\u3002\u76f4\u63a5\u5373\u63d2\u5373\u7528\u5b9e\u73b0\u5e26\u6765QA\u51c6\u786e\u7387\u63d0\u534741.77%\uff0c\u6d4b\u8bd5\u65f6\u63a8\u7406\u6837\u672c\u51cf\u5c1133.33%\u3002\u5728\u4e0d\u540cLLM\u548c\u6700\u5148\u8fdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u663e\u793a\u4e00\u81f4\u7684\u6539\u8fdb\u6a21\u5f0f\u3002", "conclusion": "\u5728\u8868\u683c\u8f6c\u6362\u4e2d\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u5f3a\u5236\u6267\u884c\u9010\u6b65\u63a8\u7406\u5bf9\u63d0\u5347TableQA\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002RE-Tab\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2601.22571", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22571", "abs": "https://arxiv.org/abs/2601.22571", "authors": ["Zhipeng Chen", "Zhongrui Zhang", "Chao Zhang", "Yifan Xu", "Lan Yang", "Jun Liu", "Ke Li", "Yi-Zhe Song"], "title": "PerfGuard: A Performance-Aware Agent for Visual Content Generation", "comment": "This paper has been accepted by ICLR 2026. The original paper link is: https://openreview.net/pdf?id=tdN42GTv4S The code repository link is: https://github.com/FelixChan9527/PerfGuard", "summary": "The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual descriptions that fail to distinguish precise performance boundaries and cannot adapt to iterative tool updates. This gap introduces uncertainty in planning and execution, particularly in domains like visual content generation (AIGC), where nuanced tool performance significantly impacts outcomes. To address this, we propose PerfGuard, a performance-aware agent framework for visual content generation that systematically models tool performance boundaries and integrates them into task planning and scheduling. Our framework introduces three core mechanisms: (1) Performance-Aware Selection Modeling (PASM), which replaces generic tool descriptions with a multi-dimensional scoring system based on fine-grained performance evaluations; (2) Adaptive Preference Update (APU), which dynamically optimizes tool selection by comparing theoretical rankings with actual execution rankings; and (3) Capability-Aligned Planning Optimization (CAPO), which guides the planner to generate subtasks aligned with performance-aware strategies. Experimental comparisons against state-of-the-art methods demonstrate PerfGuard's advantages in tool selection accuracy, execution reliability, and alignment with user intent, validating its robustness and practical utility for complex AIGC tasks. The project code is available at https://github.com/FelixChan9527/PerfGuard.", "AI": {"tldr": "PerfGuard\u662f\u4e00\u4e2a\u6027\u80fd\u611f\u77e5\u7684\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u5de5\u5177\u6027\u80fd\u8fb9\u754c\u3001\u52a8\u6001\u4f18\u5316\u5de5\u5177\u9009\u62e9\u548c\u6027\u80fd\u5bf9\u9f50\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5047\u8bbe\u5de5\u5177\u6267\u884c\u603b\u662f\u6210\u529f\u4e14\u65e0\u6cd5\u9002\u5e94\u5de5\u5177\u66f4\u65b0\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u6846\u67b6\u901a\u5e38\u5047\u8bbe\u5de5\u5177\u6267\u884c\u603b\u662f\u6210\u529f\u7684\uff0c\u4ec5\u4f9d\u8d56\u6587\u672c\u63cf\u8ff0\u65e0\u6cd5\u533a\u5206\u7cbe\u786e\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u4e5f\u4e0d\u80fd\u9002\u5e94\u8fed\u4ee3\u7684\u5de5\u5177\u66f4\u65b0\u3002\u8fd9\u5728\u89c6\u89c9\u5185\u5bb9\u751f\u6210\uff08AIGC\uff09\u7b49\u9886\u57df\u5c24\u5176\u6210\u95ee\u9898\uff0c\u56e0\u4e3a\u7ec6\u5fae\u7684\u5de5\u5177\u6027\u80fd\u5dee\u5f02\u4f1a\u663e\u8457\u5f71\u54cd\u7ed3\u679c\u3002", "method": "\u63d0\u51faPerfGuard\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) \u6027\u80fd\u611f\u77e5\u9009\u62e9\u5efa\u6a21\uff08PASM\uff09\uff0c\u7528\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u6027\u80fd\u8bc4\u4f30\u7684\u591a\u7ef4\u8bc4\u5206\u7cfb\u7edf\u66ff\u4ee3\u901a\u7528\u5de5\u5177\u63cf\u8ff0\uff1b2) \u81ea\u9002\u5e94\u504f\u597d\u66f4\u65b0\uff08APU\uff09\uff0c\u901a\u8fc7\u6bd4\u8f83\u7406\u8bba\u6392\u540d\u4e0e\u5b9e\u9645\u6267\u884c\u6392\u540d\u52a8\u6001\u4f18\u5316\u5de5\u5177\u9009\u62e9\uff1b3) \u80fd\u529b\u5bf9\u9f50\u89c4\u5212\u4f18\u5316\uff08CAPO\uff09\uff0c\u5f15\u5bfc\u89c4\u5212\u5668\u751f\u6210\u4e0e\u6027\u80fd\u611f\u77e5\u7b56\u7565\u5bf9\u9f50\u7684\u5b50\u4efb\u52a1\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5b9e\u9a8c\u6bd4\u8f83\u8868\u660e\uff0cPerfGuard\u5728\u5de5\u5177\u9009\u62e9\u51c6\u786e\u6027\u3001\u6267\u884c\u53ef\u9760\u6027\u548c\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742AIGC\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "PerfGuard\u901a\u8fc7\u7cfb\u7edf\u5efa\u6a21\u5de5\u5177\u6027\u80fd\u8fb9\u754c\u5e76\u5c06\u5176\u6574\u5408\u5230\u4efb\u52a1\u89c4\u5212\u548c\u8c03\u5ea6\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4ee3\u7406\u6846\u67b6\u5728\u5de5\u5177\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.22832", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22832", "abs": "https://arxiv.org/abs/2601.22832", "authors": ["Matthew Becker", "Yifei Chen", "Nicholas Cochran", "Pouyan Ghasemi", "Abhishek Gulati", "Mark Harman", "Zachary Haluza", "Mehrdad Honarkhah", "Herve Robert", "Jiacheng Liu", "Weini Liu", "Sreeja Thummala", "Xiaoning Yang", "Rui Xin", "Sophie Zeng"], "title": "Just-in-Time Catching Test Generation at Meta", "comment": "Submitted to FSE 2026 industry track", "summary": "We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.", "AI": {"tldr": "Meta\u5f00\u53d1\u4e86\u5373\u65f6\u6355\u83b7\u6d4b\u8bd5\u751f\u6210\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u5927\u578b\u540e\u7aef\u7cfb\u7edf\u4e2d\u9884\u9632bug\u3002\u4e0e\u4f20\u7edf\u786c\u5316\u6d4b\u8bd5\u4e0d\u540c\uff0c\u6355\u83b7\u6d4b\u8bd5\u65e8\u5728\u5931\u8d25\uff0c\u5728\u4ee3\u7801\u5408\u5e76\u524d\u53d1\u73b0bug\u3002\u901a\u8fc7\u4ee3\u7801\u53d8\u66f4\u611f\u77e5\u65b9\u6cd5\u5c06\u5019\u9009\u6355\u83b7\u751f\u6210\u63d0\u53474\u500d\uff0c\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u548cLLM\u7684\u8bc4\u4f30\u5668\u51cf\u5c1170%\u4eba\u5de5\u5ba1\u67e5\uff0c\u6210\u529f\u53d1\u73b08\u4e2a\u771f\u5b9ebug\uff0c\u5176\u4e2d4\u4e2a\u53ef\u80fd\u5f15\u53d1\u4e25\u91cd\u6545\u969c\u3002", "motivation": "Meta\u9762\u4e34\u5927\u89c4\u6a21\u540e\u7aef\u7cfb\u7edf\uff08\u6570\u4ebf\u884c\u4ee3\u7801\uff09\u4e2dbug\u9884\u9632\u7684\u6311\u6218\u3002\u4f20\u7edf\u786c\u5316\u6d4b\u8bd5\u5728\u751f\u6210\u65f6\u901a\u8fc7\uff0c\u65e0\u6cd5\u6709\u6548\u53d1\u73b0bug\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4ee3\u7801\u5408\u5e76\u524d\u4e3b\u52a8\u53d1\u73b0bug\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u8bef\u62a5\u5e26\u6765\u7684\u5f00\u53d1\u8d1f\u62c5\u3002", "method": "1. \u91c7\u7528\u4ee3\u7801\u53d8\u66f4\u611f\u77e5\u65b9\u6cd5\u751f\u6210\u6355\u83b7\u6d4b\u8bd5\uff0c\u76f8\u6bd4\u786c\u5316\u6d4b\u8bd5\u63d0\u53474\u500d\u5019\u9009\u6355\u83b7\u751f\u6210\uff1b2. \u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u548cLLM\u7684\u8bc4\u4f30\u5668\u6765\u8fc7\u6ee4\u8bef\u62a5\uff1b3. \u5206\u679022,126\u4e2a\u751f\u6210\u7684\u6d4b\u8bd5\uff0c\u8fdb\u884c\u63a8\u65ad\u7edf\u8ba1\u5206\u6790\uff1b4. \u5c06\u5019\u9009\u6355\u83b7\u62a5\u544a\u7ed9\u5de5\u7a0b\u5e08\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "1. \u4ee3\u7801\u53d8\u66f4\u611f\u77e5\u65b9\u6cd5\u5c06\u5019\u9009\u6355\u83b7\u751f\u6210\u63d0\u53474\u500d\uff08\u76f8\u6bd4\u786c\u5316\u6d4b\u8bd5\uff09\u548c20\u500d\uff08\u76f8\u6bd4\u5076\u7136\u5931\u8d25\u6d4b\u8bd5\uff09\uff1b2. \u8bc4\u4f30\u5668\u51cf\u5c1170%\u4eba\u5de5\u5ba1\u67e5\u8d1f\u62c5\uff1b3. \u7edf\u8ba1\u5206\u6790\u663e\u793a\uff1a\u4eba\u5de5\u63a5\u53d7\u7684\u4ee3\u7801\u53d8\u66f4\u6709\u66f4\u591a\u8bef\u62a5\uff0c\u4eba\u5de5\u62d2\u7edd\u7684\u53d8\u66f4\u6709\u66f4\u591a\u771f\u5b9ebug\uff1b4. \u62a5\u544a41\u4e2a\u5019\u9009\u6355\u83b7\uff0c\u786e\u8ba48\u4e2a\u771f\u5b9ebug\uff0c\u5176\u4e2d4\u4e2a\u53ef\u80fd\u5f15\u53d1\u4e25\u91cd\u6545\u969c\u3002", "conclusion": "\u5373\u65f6\u6355\u83b7\u6d4b\u8bd5\u751f\u6210\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5de5\u4e1a\u9002\u7528\u6027\uff0c\u80fd\u6709\u6548\u9632\u6b62\u4e25\u91cd\u6545\u969c\u8fdb\u5165\u751f\u4ea7\u73af\u5883\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u53d1\u73b0bug\u548c\u51cf\u5c11\u8bef\u62a5\uff0c\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u6709\u6548\u7684bug\u9884\u9632\u3002", "topic": "swe application"}}
{"id": "2601.22436", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22436", "abs": "https://arxiv.org/abs/2601.22436", "authors": ["Weixiang Zhao", "Yingshuo Wang", "Yichen Zhang", "Yang Deng", "Yanyan Zhao", "Wanxiang Che", "Bing Qin", "Ting Liu"], "title": "Large Language Model Agents Are Not Always Faithful Self-Evolvers", "comment": "25 pages, 16 figures, 7 tables", "summary": "Self-evolving large language model (LLM) agents continually improve by accumulating and reusing past experience, yet it remains unclear whether they faithfully rely on that experience to guide their behavior. We present the first systematic investigation of experience faithfulness, the causal dependence of an agent's decisions on the experience it is given, in self-evolving LLM agents. Using controlled causal interventions on both raw and condensed forms of experience, we comprehensively evaluate four representative frameworks across 10 LLM backbones and 9 environments. Our analysis uncovers a striking asymmetry: while agents consistently depend on raw experience, they often disregard or misinterpret condensed experience, even when it is the only experience provided. This gap persists across single- and multi-agent configurations and across backbone scales. We trace its underlying causes to three factors: the semantic limitations of condensed content, internal processing biases that suppress experience, and task regimes where pretrained priors already suffice. These findings challenge prevailing assumptions about self-evolving methods and underscore the need for more faithful and reliable approaches to experience integration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8c03\u67e5\u4e86\u81ea\u8fdb\u5316LLM\u667a\u80fd\u4f53\u4e2d\u7684\u7ecf\u9a8c\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u667a\u80fd\u4f53\u5bf9\u539f\u59cb\u7ecf\u9a8c\u6709\u56e0\u679c\u4f9d\u8d56\uff0c\u4f46\u5bf9\u538b\u7f29\u7ecf\u9a8c\u7ecf\u5e38\u5ffd\u89c6\u6216\u8bef\u89e3\uff0c\u63ed\u793a\u4e86\u7ecf\u9a8c\u6574\u5408\u4e2d\u7684\u4e0d\u5bf9\u79f0\u6027\u3002", "motivation": "\u81ea\u8fdb\u5316LLM\u667a\u80fd\u4f53\u901a\u8fc7\u79ef\u7d2f\u548c\u91cd\u7528\u8fc7\u53bb\u7ecf\u9a8c\u6301\u7eed\u6539\u8fdb\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u771f\u6b63\u4f9d\u8d56\u8fd9\u4e9b\u7ecf\u9a8c\u6765\u6307\u5bfc\u884c\u4e3a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u7cfb\u7edf\u8c03\u67e5\u7ecf\u9a8c\u5fe0\u5b9e\u6027\uff0c\u5373\u667a\u80fd\u4f53\u51b3\u7b56\u5bf9\u7ed9\u5b9a\u7ecf\u9a8c\u7684\u56e0\u679c\u4f9d\u8d56\u6027\u3002", "method": "\u4f7f\u7528\u53d7\u63a7\u56e0\u679c\u5e72\u9884\u65b9\u6cd5\uff0c\u5bf9\u539f\u59cb\u548c\u538b\u7f29\u4e24\u79cd\u5f62\u5f0f\u7684\u7ecf\u9a8c\u8fdb\u884c\u5e72\u9884\uff0c\u5168\u9762\u8bc4\u4f304\u4e2a\u4ee3\u8868\u6027\u6846\u67b6\u300110\u4e2aLLM\u9aa8\u5e72\u6a21\u578b\u548c9\u4e2a\u73af\u5883\u3002\u5206\u6790\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u914d\u7f6e\uff0c\u4ee5\u53ca\u4e0d\u540c\u89c4\u6a21\u7684\u9aa8\u5e72\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u663e\u8457\u7684\u4e0d\u5bf9\u79f0\u6027\uff1a\u667a\u80fd\u4f53\u59cb\u7ec8\u4f9d\u8d56\u539f\u59cb\u7ecf\u9a8c\uff0c\u4f46\u7ecf\u5e38\u5ffd\u89c6\u6216\u8bef\u89e3\u538b\u7f29\u7ecf\u9a8c\uff0c\u5373\u4f7f\u8fd9\u662f\u552f\u4e00\u63d0\u4f9b\u7684\u7ecf\u9a8c\u3002\u8fd9\u79cd\u5dee\u8ddd\u5728\u5355/\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u548c\u4e0d\u540c\u89c4\u6a21\u9aa8\u5e72\u6a21\u578b\u4e2d\u90fd\u6301\u7eed\u5b58\u5728\u3002\u6839\u672c\u539f\u56e0\u5305\u62ec\uff1a\u538b\u7f29\u5185\u5bb9\u7684\u8bed\u4e49\u9650\u5236\u3001\u6291\u5236\u7ecf\u9a8c\u7684\u5185\u5728\u5904\u7406\u504f\u89c1\u3001\u4ee5\u53ca\u9884\u8bad\u7ec3\u5148\u9a8c\u5df2\u8db3\u591f\u7684\u4efb\u52a1\u673a\u5236\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u5173\u4e8e\u81ea\u8fdb\u5316\u65b9\u6cd5\u7684\u666e\u904d\u5047\u8bbe\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5fe0\u5b9e\u548c\u53ef\u9760\u7684\u7ecf\u9a8c\u6574\u5408\u65b9\u6cd5\u3002\u7ecf\u9a8c\u5fe0\u5b9e\u6027\u95ee\u9898\u662f\u81ea\u8fdb\u5316LLM\u667a\u80fd\u4f53\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u672a\u89e3\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.22859", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22859", "abs": "https://arxiv.org/abs/2601.22859", "authors": ["Chuanzhe Guo", "Jingjing Wu", "Sijun He", "Yang Chen", "Zhaoqi Kuang", "Shilong Fan", "Bingjin Chen", "Siqi Bao", "Jing Liu", "Hua Wu", "Qingfu Zhu", "Wanxiang Che", "Haifeng Wang"], "title": "MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering", "comment": null, "summary": "The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.", "AI": {"tldr": "MEnvAgent\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u81ea\u52a8\u73af\u5883\u6784\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u5b9e\u4f8b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u589e\u91cf\u8865\u4e01\u673a\u5236\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u521b\u5efa\u4e86\u6700\u5927\u7684\u591a\u8bed\u8a00\u53ef\u9a8c\u8bc1Docker\u73af\u5883\u6570\u636e\u96c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u5e94\u7528\u53d7\u5230\u53ef\u9a8c\u8bc1\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u8fd9\u6e90\u4e8e\u8de8\u591a\u79cd\u8bed\u8a00\u6784\u5efa\u53ef\u6267\u884c\u73af\u5883\u7684\u590d\u6742\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u6784\u5efa\u53ef\u9a8c\u8bc1\u4efb\u52a1\u73af\u5883\u7684\u6846\u67b6\u6765\u7a81\u7834\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u89c4\u5212-\u6267\u884c-\u9a8c\u8bc1\u67b6\u6784\u6765\u81ea\u4e3b\u89e3\u51b3\u73af\u5883\u6784\u5efa\u5931\u8d25\u95ee\u9898\uff0c\u5e76\u96c6\u6210\u4e86\u521b\u65b0\u7684\u73af\u5883\u91cd\u7528\u673a\u5236\uff0c\u901a\u8fc7\u589e\u91cf\u8865\u4e01\u5386\u53f2\u73af\u5883\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u5305\u542b10\u79cd\u8bed\u8a00\u30011000\u4e2a\u4efb\u52a1\u7684MEnvBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMEnvAgent\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c06\u5931\u8d25\u8f6c\u901a\u8fc7\u7387\u63d0\u9ad8\u4e868.6%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8643%\u7684\u65f6\u95f4\u6210\u672c\u3002\u6784\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u5f00\u6e90\u591a\u8bed\u8a00\u53ef\u9a8c\u8bc1Docker\u73af\u5883\u6570\u636e\u96c6MEnvData-SWE\u3002", "conclusion": "MEnvAgent\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u53ef\u9a8c\u8bc1\u73af\u5883\u6784\u5efa\u7684\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u91cd\u7528\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u4e3aLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\u3002", "topic": "swe benchmark"}}
{"id": "2601.22607", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22607", "abs": "https://arxiv.org/abs/2601.22607", "authors": ["Jiaxuan Gao", "Jiaao Chen", "Chuyi He", "Wei-Chen Wang", "Shusheng Xu", "Hanrui Wang", "Di Jin", "Yi Wu"], "title": "From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents", "comment": "Submitted to ICML 2026", "summary": "Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.", "AI": {"tldr": "\u63d0\u51faEigenData\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u6f14\u5316\u6570\u636e\u4ee3\u7406\u548c\u9a8c\u8bc1\u5668\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u8bad\u7ec3\u590d\u6742\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\uff0c\u65e0\u9700\u6602\u8d35\u4eba\u5de5\u6807\u6ce8", "motivation": "\u8bad\u7ec3\u4ea4\u4e92\u5f0f\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u9762\u4e34\u6311\u6218\uff1a\u9ad8\u8d28\u91cf\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u6570\u636e\u96be\u4ee5\u89c4\u6a21\u5316\u5408\u6210\uff0c\u5f3a\u5316\u5b66\u4e60\u53ef\u80fd\u56e0\u7528\u6237\u6a21\u62df\u566a\u58f0\u4fe1\u53f7\u800c\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51faEigenData\u5206\u5c42\u591a\u4ee3\u7406\u5f15\u64ce\uff0c\u5408\u6210\u5de5\u5177\u63a5\u5730\u5bf9\u8bdd\u548c\u53ef\u6267\u884c\u68c0\u67e5\u5668\uff0c\u901a\u8fc7\u95ed\u73af\u81ea\u6f14\u5316\u8fc7\u7a0b\u66f4\u65b0\u63d0\u793a\u548c\u5de5\u4f5c\u6d41\uff1b\u57fa\u4e8e\u5408\u6210\u6570\u636e\u5f00\u53d1\u5f3a\u5316\u5b66\u4e60\u914d\u65b9\uff0c\u5148\u5fae\u8c03\u7528\u6237\u6a21\u578b\uff0c\u518d\u5e94\u7528GRPO\u98ce\u683c\u8bad\u7ec3\uff0c\u4f7f\u7528\u8f68\u8ff9\u7ea7\u7ec4\u76f8\u5bf9\u4f18\u52bf\u548c\u52a8\u6001\u8fc7\u6ee4", "result": "\u5728tau^2-bench\u4e0a\u8bc4\u4f30\uff0c\u6700\u4f73\u6a21\u578b\u5728Airline\u8fbe\u523073.0% pass^1\uff0c\u5728Telecom\u8fbe\u523098.3% pass^1\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u524d\u6cbf\u6a21\u578b", "conclusion": "\u4e3a\u5f15\u5bfc\u590d\u6742\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u65e0\u9700\u6602\u8d35\u4eba\u5de5\u6807\u6ce8", "topic": "agentic reinforcement learning"}}
{"id": "2601.22491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22491", "abs": "https://arxiv.org/abs/2601.22491", "authors": ["Jinyang Wu", "Changpeng Yang", "Yuhao Shen", "Fangzhi Xu", "Bolin Ni", "Chonghua Liao", "Yuchen Liu", "Hongzhen Wang", "Shuai Nie", "Shuai Zhang", "Haoran Luo", "Jiaming Xu"], "title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization", "comment": null, "summary": "Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \\textbf{S}weet \\textbf{S}pot \\textbf{L}earning (\\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.", "AI": {"tldr": "SSL\uff08\u751c\u70b9\u5b66\u4e60\uff09\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u653e\u5927\u7684\u5206\u5c42\u5956\u52b1\u5f15\u5bfc\u667a\u80fd\u4f53\u5411\u89e3\u7a7a\u95f4\u7684\"\u751c\u70b9\"\u533a\u57df\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edf\u4e8c\u5143\u5956\u52b1\u80fd\u66f4\u597d\u5730\u533a\u5206\u8f68\u8ff9\u8d28\u91cf\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u4e8c\u5143\u5956\u52b1\uff0c\u65e0\u6cd5\u533a\u5206\u8fbe\u6210\u76f8\u540c\u7ed3\u679c\u4f46\u8d28\u91cf\u4e0d\u540c\u7684\u8f68\u8ff9\uff0c\u5ffd\u7565\u4e86\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u5185\u7684\u6f5c\u5728\u591a\u6837\u6027\u3002\u53d7\u7f51\u7403\"\u751c\u70b9\"\u6982\u5ff5\u542f\u53d1\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u4f9b\u5dee\u5f02\u5316\u6307\u5bfc\u7684\u4f18\u5316\u6846\u67b6\u3002", "method": "SSL\u91c7\u7528\u6e10\u8fdb\u653e\u5927\u3001\u5206\u5c42\u5956\u52b1\u7684\u539f\u5219\uff0c\u5f15\u5bfc\u7b56\u7565\u5411\u89e3\u7a7a\u95f4\u7684\u751c\u70b9\u533a\u57df\u4f18\u5316\u3002\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\uff1a\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4f7f\u7528\u8ddd\u79bb\u5206\u5c42\u5efa\u6a21\u5956\u52b1\u63a5\u8fd1\u7a0b\u5ea6\uff0c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u5956\u52b1\u5411\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\u7684\u6e10\u8fdb\u8fdb\u5c55\u3002", "result": "\u5728GUI\u611f\u77e5\u3001\u77ed\u671f/\u957f\u671f\u89c4\u5212\u3001\u590d\u6742\u63a8\u7406\u7b4912\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSSL\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5b9e\u73b0\u9ad8\u8fbe2.5\u500d\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u6709\u6548\u7684\u8de8\u4efb\u52a1\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "SSL\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u8bad\u7ec3\u539f\u5219\uff0c\u80fd\u591f\u57f9\u517b\u66f4\u5f3a\u5927\u548c\u9c81\u68d2\u7684\u667a\u80fd\u4f53\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u4fdd\u6301\u6700\u4f18\u89e3\u6392\u5e8f\u5e76\u589e\u5f3a\u68af\u5ea6\u4fe1\u566a\u6bd4\uff0c\u4fc3\u8fdb\u66f4\u6709\u9488\u5bf9\u6027\u7684\u4f18\u5316\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22511", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22511", "abs": "https://arxiv.org/abs/2601.22511", "authors": ["Yuan-Jay L\u00fc", "Chengyu Wang", "Lei Shen", "Jun Huang", "Tong Xu"], "title": "Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic Tasks, Simulated Environments, and Rubric-Based Rewards", "comment": null, "summary": "Small LLMs often struggle to match the agentic capabilities of large, costly models. While reinforcement learning can help, progress has been limited by two structural bottlenecks: existing open-source agentic training data are narrow in task variety and easily solved; real-world APIs lack diversity and are unstable for large-scale reinforcement learning rollout processes. We address these challenges with SYNTHAGENT, a framework that jointly synthesizes diverse tool-use training data and simulates complete environments. Specifically, a strong teacher model creates novel tasks and tool ecosystems, then rewrites them into intentionally underspecified instructions. This compels agents to actively query users for missing details. When handling synthetic tasks, an LLM-based user simulator provides user-private information, while a mock tool system delivers stable tool responses. For rewards, task-level rubrics are constructed based on required subgoals, user-agent interactions, and forbidden behaviors. Across 14 challenging datasets in math, search, and tool use, models trained on our synthetic data achieve substantial gains, with small models outperforming larger baselines.", "AI": {"tldr": "SYNTHAGENT\u6846\u67b6\u901a\u8fc7\u5408\u6210\u591a\u6837\u5316\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u62df\u5b8c\u6574\u73af\u5883\uff0c\u89e3\u51b3\u4e86\u5c0f\u6a21\u578b\u5728\u4ee3\u7406\u80fd\u529b\u4e0a\u7684\u74f6\u9888\uff0c\u4f7f\u5c0f\u6a21\u578b\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u66f4\u5927\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5c0f\u6a21\u578b\u96be\u4ee5\u5339\u914d\u5927\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u74f6\u9888\uff1a\u5f00\u6e90\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u4efb\u52a1\u5355\u4e00\u4e14\u5bb9\u6613\u89e3\u51b3\uff1b\u771f\u5b9eAPI\u7f3a\u4e4f\u591a\u6837\u6027\u4e14\u5728\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4e0d\u7a33\u5b9a\u3002", "method": "\u4f7f\u7528\u5f3a\u6559\u5e08\u6a21\u578b\u521b\u5efa\u65b0\u9896\u4efb\u52a1\u548c\u5de5\u5177\u751f\u6001\u7cfb\u7edf\uff0c\u5e76\u5c06\u5176\u91cd\u5199\u4e3a\u6545\u610f\u4e0d\u5b8c\u6574\u7684\u6307\u4ee4\uff0c\u8feb\u4f7f\u4ee3\u7406\u4e3b\u52a8\u5411\u7528\u6237\u67e5\u8be2\u7f3a\u5931\u7ec6\u8282\u3002\u5904\u7406\u5408\u6210\u4efb\u52a1\u65f6\uff0c\u57fa\u4e8eLLM\u7684\u7528\u6237\u6a21\u62df\u5668\u63d0\u4f9b\u7528\u6237\u79c1\u6709\u4fe1\u606f\uff0c\u6a21\u62df\u5de5\u5177\u7cfb\u7edf\u63d0\u4f9b\u7a33\u5b9a\u5de5\u5177\u54cd\u5e94\u3002\u5956\u52b1\u57fa\u4e8e\u6240\u9700\u5b50\u76ee\u6807\u3001\u7528\u6237-\u4ee3\u7406\u4ea4\u4e92\u548c\u7981\u6b62\u884c\u4e3a\u6784\u5efa\u4efb\u52a1\u7ea7\u8bc4\u5206\u6807\u51c6\u3002", "result": "\u5728\u6570\u5b66\u3001\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u7b4914\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5c0f\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SYNTHAGENT\u6846\u67b6\u901a\u8fc7\u5408\u6210\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u62df\u73af\u5883\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u6a21\u578b\u4ee3\u7406\u80fd\u529b\u8bad\u7ec3\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u5c0f\u6a21\u578b\u4ee3\u7406\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22952", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22952", "abs": "https://arxiv.org/abs/2601.22952", "authors": ["Yunpeng Xiong", "Ting Zhang"], "title": "Sifting the Noise: A Comparative Study of LLM Agents in Vulnerability False Positive Filtering", "comment": null, "summary": "Static Application Security Testing (SAST) tools are essential for identifying software vulnerabilities, but they often produce a high volume of false positives (FPs), imposing a substantial manual triage burden on developers. Recent advances in Large Language Model (LLM) agents offer a promising direction by enabling iterative reasoning, tool use, and environment interaction to refine SAST alerts. However, the comparative effectiveness of different LLM-based agent architectures for FP filtering remains poorly understood. In this paper, we present a comparative study of three state-of-the-art LLM-based agent frameworks, i.e., Aider, OpenHands, and SWE-agent, for vulnerability FP filtering. We evaluate these frameworks using the vulnerabilities from the OWASP Benchmark and real-world open-source Java projects. The experimental results show that LLM-based agents can remove the majority of SAST noise, reducing an initial FP detection rate of over 92% on the OWASP Benchmark to as low as 6.3% in the best configuration. On real-world dataset, the best configuration of LLM-based agents can achieve an FP identification rate of up to 93.3% involving CodeQL alerts. However, the benefits of agents are strongly backbone- and CWE-dependent: agentic frameworks significantly outperform vanilla prompting for stronger models such as Claude Sonnet 4 and GPT-5, but yield limited or inconsistent gains for weaker backbones. Moreover, aggressive FP reduction can come at the cost of suppressing true vulnerabilities, highlighting important trade-offs. Finally, we observe large disparities in computational cost across agent frameworks. Overall, our study demonstrates that LLM-based agents are a powerful but non-uniform solution for SAST FP filtering, and that their practical deployment requires careful consideration of agent design, backbone model choice, vulnerability category, and operational cost.", "AI": {"tldr": "LLM\u667a\u80fd\u4f53\u53ef\u663e\u8457\u964d\u4f4eSAST\u5de5\u5177\u8bef\u62a5\u7387\uff0c\u4f46\u6548\u679c\u53d7\u9aa8\u5e72\u6a21\u578b\u3001\u6f0f\u6d1e\u7c7b\u578b\u548c\u6210\u672c\u5f71\u54cd\uff0c\u9700\u8c28\u614e\u90e8\u7f72", "motivation": "SAST\u5de5\u5177\u4ea7\u751f\u5927\u91cf\u8bef\u62a5\uff0c\u589e\u52a0\u4eba\u5de5\u5ba1\u67e5\u8d1f\u62c5\uff0c\u800cLLM\u667a\u80fd\u4f53\u5728\u8bef\u62a5\u8fc7\u6ee4\u65b9\u9762\u7684\u6bd4\u8f83\u6548\u679c\u5c1a\u4e0d\u660e\u786e", "method": "\u6bd4\u8f83\u4e09\u79cdLLM\u667a\u80fd\u4f53\u6846\u67b6(Aider\u3001OpenHands\u3001SWE-agent)\uff0c\u4f7f\u7528OWASP Benchmark\u548c\u771f\u5b9eJava\u9879\u76ee\u8bc4\u4f30\uff0c\u5206\u6790\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u6548\u679c", "result": "LLM\u667a\u80fd\u4f53\u53ef\u5c06OWASP Benchmark\u8bef\u62a5\u7387\u4ece92%\u964d\u81f36.3%\uff0c\u771f\u5b9e\u9879\u76ee\u4e2d\u53ef\u8fbe93.3%\u8bef\u62a5\u8bc6\u522b\u7387\uff0c\u4f46\u6548\u679c\u53d7\u9aa8\u5e72\u6a21\u578b\u548cCWE\u7c7b\u578b\u5f71\u54cd\uff0c\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u5dee\u5f02", "conclusion": "LLM\u667a\u80fd\u4f53\u662fSAST\u8bef\u62a5\u8fc7\u6ee4\u7684\u6709\u6548\u4f46\u975e\u5747\u5300\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9645\u90e8\u7f72\u9700\u8003\u8651\u667a\u80fd\u4f53\u8bbe\u8ba1\u3001\u9aa8\u5e72\u6a21\u578b\u9009\u62e9\u3001\u6f0f\u6d1e\u7c7b\u522b\u548c\u8fd0\u8425\u6210\u672c", "topic": "agent analysis"}}
{"id": "2601.22636", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22636", "abs": "https://arxiv.org/abs/2601.22636", "authors": ["Mingqian Feng", "Xiaodong Liu", "Weiwei Yang", "Chenliang Xu", "Christopher White", "Jianfeng Gao"], "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling", "comment": null, "summary": "Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.", "AI": {"tldr": "\u63d0\u51faSABER\u65b9\u6cd5\uff0c\u901a\u8fc7Beta\u5206\u5e03\u5efa\u6a21\u6837\u672c\u7ea7\u6210\u529f\u6982\u7387\uff0c\u63a8\u5bfc\u89e3\u6790\u7f29\u653e\u5b9a\u5f8b\uff0c\u4ec5\u7528100\u4e2a\u6837\u672c\u5c31\u80fd\u51c6\u786e\u9884\u6d4b1000\u6b21\u91c7\u6837\u4e0b\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u8bef\u5dee\u964d\u4f4e86.2%", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u8bc4\u4f30\u901a\u5e38\u57fa\u4e8e\u5355\u6b21\u6216\u4f4e\u9884\u7b97\u5bf9\u6297\u63d0\u793a\uff0c\u4f4e\u4f30\u4e86\u5b9e\u9645\u98ce\u9669\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u5927\u89c4\u6a21\u5e76\u884c\u91c7\u6837\u53cd\u590d\u63a2\u6d4b\u6a21\u578b\u76f4\u5230\u4ea7\u751f\u6709\u5bb3\u54cd\u5e94\uff0c\u9700\u8981\u66f4\u51c6\u786e\u9884\u6d4b\u5927\u89c4\u6a21\u5bf9\u6297\u98ce\u9669\u7684\u65b9\u6cd5", "method": "\u63d0\u51faSABER\u65b9\u6cd5\uff1a\u4f7f\u7528Beta\u5206\u5e03\uff08\u4f2f\u52aa\u5229\u5206\u5e03\u7684\u5171\u8f6d\u5148\u9a8c\uff09\u5efa\u6a21\u6837\u672c\u7ea7\u6210\u529f\u6982\u7387\uff0c\u63a8\u5bfc\u89e3\u6790\u7f29\u653e\u5b9a\u5f8b\uff0c\u80fd\u591f\u4ece\u5c0f\u9884\u7b97\u6d4b\u91cf\u53ef\u9760\u5916\u63a8\u5927\u89c4\u6a21\u653b\u51fb\u6210\u529f\u7387", "result": "\u4ec5\u7528n=100\u4e2a\u6837\u672c\uff0c\u951a\u5b9a\u4f30\u8ba1\u5668\u9884\u6d4bASR@1000\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.66\uff0c\u76f8\u6bd4\u57fa\u7ebf12.04\u964d\u4f4e\u4e8686.2%\u3002\u63ed\u793a\u4e86\u5f02\u8d28\u6027\u98ce\u9669\u7f29\u653e\u7279\u5f81\uff0c\u663e\u793a\u5728\u6807\u51c6\u8bc4\u4f30\u4e0b\u770b\u4f3c\u7a33\u5065\u7684\u6a21\u578b\u5728\u5e76\u884c\u5bf9\u6297\u538b\u529b\u4e0b\u53ef\u80fd\u7ecf\u5386\u5feb\u901f\u975e\u7ebf\u6027\u98ce\u9669\u653e\u5927", "conclusion": "SABER\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u73b0\u5b9eLLM\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u66f4\u51c6\u786e\u7684\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u5de5\u5177", "topic": "agent analysis"}}
{"id": "2601.22956", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22956", "abs": "https://arxiv.org/abs/2601.22956", "authors": ["Boyin Tan", "Haoning Deng", "Junyuan Zhang", "Junjielong Xu", "Pinjia He", "Youcheng Sun"], "title": "SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding", "comment": null, "summary": "Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.\n  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...", "AI": {"tldr": "SWE-Manager\uff1a\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u76848B\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u591a\u4e2a\u4ee3\u7801\u4fee\u590d\u63d0\u6848\u4e2d\u9009\u62e9\u6700\u4f73\u65b9\u6848\u5e76\u5408\u6210\u9ec4\u91d1\u63d0\u6848\uff0c\u5728SWE-Lancer Manager\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-5\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u751f\u6210\u548c\u9519\u8bef\u4fee\u590d\uff0c\u4f46\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u56e2\u961f\u9700\u8981\u4ece\u591a\u4e2a\u5019\u9009\u63d0\u6848\u4e2d\u9009\u62e9\u6700\u4f73\u65b9\u6848\u8fdb\u884c\u5b9e\u65bd\u3002\u597d\u7684\u9009\u62e9\u80fd\u63d0\u9ad8\u95ee\u9898\u89e3\u51b3\u7684\u53ef\u9760\u6027\u5e76\u964d\u4f4e\u98ce\u9669\uff0c\u800c\u5dee\u7684\u9009\u62e9\u4f1a\u589e\u52a0\u98ce\u9669\u751a\u81f3\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u6545\u969c\u3002", "method": "\u9996\u5148\u901a\u8fc7\u4eba\u5de5\u7814\u7a76\u5206\u6790\u7ef4\u62a4\u8005\u9009\u62e9\u63d0\u6848\u7684\u7406\u6027\u4f9d\u636e\uff0c\u7136\u540e\u63d0\u51faSWE-Manager\uff1a\u4e00\u4e2a8B\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6765\u6bd4\u8f83\u63d0\u6848\u3001\u8bc1\u660e\u9009\u62e9\u5408\u7406\u6027\u5e76\u5408\u6210\u9ec4\u91d1\u63d0\u6848\u3002\u5c06\u63d0\u6848\u9009\u62e9\u89c6\u4e3a\u63a8\u7406\u4efb\u52a1\uff0c\u6a21\u62df\u6280\u672f\u7ecf\u7406\u5728\u4e0d\u6267\u884c\u4ee3\u7801\u6216\u8fd0\u884c\u6d4b\u8bd5\u7684\u60c5\u51b5\u4e0b\u6743\u8861\u95ee\u9898\u4e0a\u4e0b\u6587\u548c\u6bcf\u4e2a\u63d0\u6848\u89e3\u51b3\u65b9\u6848\u7684\u8fc7\u7a0b\u3002", "result": "\u5728SWE-Lancer Manager\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWE-Manager\u8fbe\u523053.21%\u7684\u9009\u62e9\u51c6\u786e\u7387\u548c57.75%\u7684\u6536\u76ca\u7387\uff0c\u83b7\u5f97152,750\u7f8e\u5143\u6536\u76ca\uff0c\u8868\u73b0\u4f18\u4e8e\u5305\u62ecGPT-5\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002\u8fd8\u8bbe\u8ba1\u4e86P2A\u6846\u67b6\u6765\u6a21\u62df\u771f\u5b9e\u5de5\u4f5c\u6d41\u8bc4\u4f30\u6a21\u578b\u6548\u679c\u3002", "conclusion": "SWE-Manager\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u80fd\u591f\u6709\u6548\u9009\u62e9\u6700\u4f73\u4ee3\u7801\u4fee\u590d\u63d0\u6848\u5e76\u5408\u6210\u9ec4\u91d1\u63d0\u6848\uff0c\u5728\u63d0\u6848\u9009\u62e9\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2601.23009", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.23009", "abs": "https://arxiv.org/abs/2601.23009", "authors": ["Wei Chen", "Zhiyuan Peng", "Xin Yin", "Chao Ni", "Chenhao Ying", "Bang Xie", "Yuan Luo"], "title": "SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation", "comment": null, "summary": "Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. While Large Language Models (LLMs) have shown promise in code generation, they often struggle with the rigorous requirements of smart contracts, frequently producing code that is buggy or vulnerable. To address this, we propose SolAgent, a novel tool-augmented multi-agent framework that mimics the workflow of human experts. SolAgent integrates a \\textbf{dual-loop refinement mechanism}: an inner loop using the \\textit{Forge} compiler to ensure functional correctness, and an outer loop leveraging the \\textit{Slither} static analyzer to eliminate security vulnerabilities. Additionally, the agent is equipped with file system capabilities to resolve complex project dependencies. Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to \\textbf{64.39\\%}, significantly outperforming state-of-the-art LLMs ($\\sim$25\\%), AI IDEs (e.g., GitHub Copilot), and existing agent frameworks. Moreover, it reduces security vulnerabilities by up to \\textbf{39.77\\%} compared to human-written baselines. Finally, we demonstrate that the high-quality trajectories generated by SolAgent can be used to distill smaller, open-source models, democratizing access to secure smart contract generation. We release our data and code at https://github.com/openpaperz/SolAgent.", "AI": {"tldr": "SolAgent\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u751f\u6210\u529f\u80fd\u6b63\u786e\u4e14\u5b89\u5168\u7684\u667a\u80fd\u5408\u7ea6\u3002\u5b83\u91c7\u7528\u53cc\u5faa\u73af\u7cbe\u70bc\u673a\u5236\uff0c\u7ed3\u5408Forge\u7f16\u8bd1\u5668\u548cSlither\u9759\u6001\u5206\u6790\u5668\uff0c\u5728SolEval+\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523064.39%\u7684Pass@1\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u662f\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u7684\u57fa\u7840\uff0c\u4f46\u786e\u4fdd\u5176\u529f\u80fd\u6b63\u786e\u6027\u548c\u5b89\u5168\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u667a\u80fd\u5408\u7ea6\u7684\u4e25\u683c\u8981\u6c42\u4e0b\u5e38\u5e38\u4ea7\u751f\u6709\u7f3a\u9677\u6216\u6613\u53d7\u653b\u51fb\u7684\u4ee3\u7801\u3002", "method": "\u63d0\u51faSolAgent\uff0c\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u53cc\u5faa\u73af\u7cbe\u70bc\u673a\u5236\uff1a\u5185\u5faa\u73af\u4f7f\u7528Forge\u7f16\u8bd1\u5668\u786e\u4fdd\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5916\u5faa\u73af\u5229\u7528Slither\u9759\u6001\u5206\u6790\u5668\u6d88\u9664\u5b89\u5168\u6f0f\u6d1e\u3002\u6b64\u5916\uff0c\u667a\u80fd\u4f53\u8fd8\u5177\u5907\u6587\u4ef6\u7cfb\u7edf\u80fd\u529b\u6765\u89e3\u51b3\u590d\u6742\u7684\u9879\u76ee\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728SolEval+\u57fa\u51c6\u6d4b\u8bd5\uff08\u6e90\u81ea\u9ad8\u8d28\u91cf\u771f\u5b9e\u9879\u76ee\u7684\u4e25\u683c\u5957\u4ef6\uff09\u4e0a\uff0cSolAgent\u5b9e\u73b0\u4e86\u9ad8\u8fbe64.39%\u7684Pass@1\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u7ea625%\uff09\u3001AI IDE\uff08\u5982GitHub Copilot\uff09\u548c\u73b0\u6709\u667a\u80fd\u4f53\u6846\u67b6\u3002\u540c\u65f6\uff0c\u4e0e\u4eba\u5de5\u7f16\u5199\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b83\u5c06\u5b89\u5168\u6f0f\u6d1e\u51cf\u5c11\u4e86\u9ad8\u8fbe39.77%\u3002", "conclusion": "SolAgent\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5b89\u5168\u7684\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\u3002\u6b64\u5916\uff0cSolAgent\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u8f68\u8ff9\u53ef\u7528\u4e8e\u84b8\u998f\u66f4\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u4ece\u800c\u666e\u53ca\u5b89\u5168\u667a\u80fd\u5408\u7ea6\u751f\u6210\u7684\u8bbf\u95ee\u3002", "topic": "code agent"}}
{"id": "2601.22305", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22305", "abs": "https://arxiv.org/abs/2601.22305", "authors": ["Bo Yuan", "Yun Zhou", "Zhichao Xu", "Kiran Ramnath", "Aosong Feng", "Balasubramaniam Srinivasan"], "title": "BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation", "comment": "EACL 2026 Finding", "summary": "Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \\textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \\textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.", "AI": {"tldr": "\u63d0\u51faBayesian Workflow Generation (BWG)\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u6d41\u751f\u6210\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u63a8\u65ad\u95ee\u9898\uff0c\u901a\u8fc7\u5e76\u884c\u524d\u77bb\u548c\u5e8f\u5217\u4f18\u5316\u5668\u6539\u8fdb\u5de5\u4f5c\u6d41\u6784\u5efa\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5de5\u4f5c\u6d41\u751f\u6210\u65b9\u6cd5\u5927\u591a\u5c06\u4efb\u52a1\u89c6\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u66f4\u7406\u8bba\u5316\u7684\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u6d41\u751f\u6210\u5f62\u5f0f\u5316\u4e3a\u8d1d\u53f6\u65af\u63a8\u65ad\u95ee\u9898\u3002", "method": "\u63d0\u51faBWG\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u6d41\u751f\u6210\u89c6\u4e3a\u5bf9\u5de5\u4f5c\u6d41\u540e\u9a8c\u5206\u5e03\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u3002\u5177\u4f53\u5b9e\u73b0\u4e3aBayesFlow\u7b97\u6cd5\uff0c\u4f7f\u7528\u5e76\u884c\u524d\u77bbrollout\u8fdb\u884c\u91cd\u8981\u6027\u52a0\u6743\uff0c\u7ed3\u5408\u5e8f\u5217in-loop\u4f18\u5316\u5668\u8fdb\u884c\u6c60\u7ea7\u6539\u8fdb\uff0c\u65e0\u9700\u8bad\u7ec3\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cBayesFlow\u76f8\u6bd4SOTA\u5de5\u4f5c\u6d41\u751f\u6210\u57fa\u7ebf\u63d0\u5347\u51c6\u786e\u7387\u9ad8\u8fbe9\u4e2a\u767e\u5206\u70b9\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u63d0\u793a\u63d0\u5347\u9ad8\u8fbe65\u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u4e86BWG\u4f5c\u4e3a\u57fa\u4e8e\u641c\u7d22\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u7684\u7406\u8bba\u5347\u7ea7\u7684\u6709\u6548\u6027\u3002", "conclusion": "BWG\u4e3a\u5de5\u4f5c\u6d41\u751f\u6210\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u6846\u67b6\uff0cBayesFlow\u7b97\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u81ea\u52a8\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2601.22548", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22548", "abs": "https://arxiv.org/abs/2601.22548", "authors": ["Dani Roytburg", "Matthew Bozoukov", "Matthew Nguyen", "Mackenzie Puig-Hall", "Narmeen Oozeer"], "title": "Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations", "comment": null, "summary": "Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of \"easy\" versus \"hard\" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0LLM\u8bc4\u4f30\u4e2d\u5b58\u5728\u6838\u5fc3\u65b9\u6cd5\u5b66\u6df7\u6742\u56e0\u7d20\uff0c\u63d0\u51fa\u8bc4\u4f30\u8005\u8d28\u91cf\u57fa\u7ebf\u6765\u5206\u79bb\u81ea\u504f\u597d\u4fe1\u53f7\u4e0e\u56f0\u96be\u95ee\u9898\u4e0a\u7684\u566a\u58f0\u8f93\u51fa\uff0c\u5c06\u6d4b\u91cf\u8bef\u5dee\u964d\u4f4e89.6%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u65f6\u503e\u5411\u4e8e\u504f\u597d\u81ea\u5df1\u7684\u8f93\u51fa\uff0c\u4f46\u96be\u4ee5\u533a\u5206\u54ea\u4e9b\u8bc4\u4f30\u504f\u5dee\u6e90\u4e8e\u81ea\u604b\u503e\u5411\uff0c\u54ea\u4e9b\u6e90\u4e8e\u4e00\u822c\u5b9e\u9a8c\u6df7\u6742\u56e0\u7d20\uff0c\u8fd9\u626d\u66f2\u4e86\u5bf9\u81ea\u504f\u597d\u504f\u5dee\u7684\u6d4b\u91cf\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u8005\u8d28\u91cf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6bd4\u8f83\u8bc4\u4f30\u8005\u9519\u8bef\u6295\u7968\u7ed9\u81ea\u5df1\u8f93\u51fa\u7684\u6982\u7387\u4e0e\u9519\u8bef\u6295\u7968\u7ed9\u5176\u4ed6\u6a21\u578b\u9519\u8bef\u8f93\u51fa\u7684\u6982\u7387\uff0c\u4ece\u800c\u5206\u79bb\u81ea\u504f\u597d\u4fe1\u53f7\u4e0e\u56f0\u96be\u95ee\u9898\u4e0a\u7684\u566a\u58f0\u3002\u572837,448\u4e2a\u67e5\u8be2\u4e0a\u8bc4\u4f30\u8be5\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u5c06\u6d4b\u91cf\u8bef\u5dee\u964d\u4f4e89.6%\u3002\u5e94\u7528\u57fa\u7ebf\u540e\uff0c\u53ea\u670951%\u7684\u521d\u59cb\u53d1\u73b0\u4fdd\u6301\u7edf\u8ba1\u663e\u8457\u6027\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86LLM\u8bc4\u4f30\u8005\u5bf9\"\u7b80\u5355\"\u4e0e\"\u56f0\u96be\"\u8bc4\u4f30\u6295\u7968\u7684\u71b5\u7279\u5f81\u3002", "conclusion": "\u63d0\u51fa\u7684\u6821\u6b63\u57fa\u7ebf\u901a\u8fc7\u6d88\u9664\u566a\u58f0\u6570\u636e\uff0c\u4e3a\u672a\u6765\u81ea\u504f\u597d\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u6709\u52a9\u4e8e\u6269\u5c55\u5bf9\u8bc4\u4f30\u8005\u504f\u5dee\u6548\u5e94\u7684\u5206\u7c7b\u548c\u9694\u79bb\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2601.22648", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22648", "abs": "https://arxiv.org/abs/2601.22648", "authors": ["Xianzhou Zeng", "Jing Huang", "Chunmei Xie", "Gongrui Nan", "Siye Chen", "Mengyu Lu", "Weiqi Xiong", "Qixuan Zhou", "Junhao Zhang", "Qiang Zhu", "Yadong Li", "Xingzhong Xu"], "title": "UCPO: Uncertainty-Aware Policy Optimization", "comment": null, "summary": "The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.", "AI": {"tldr": "\u63d0\u51faUCPO\u6846\u67b6\u89e3\u51b3LLM\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u4e2d\u7684\u4f18\u52bf\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u5143\u4f18\u52bf\u89e3\u8026\u548c\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u5956\u52b1\u8c03\u6574\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u548c\u6821\u51c6\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RL\u8303\u5f0f\uff08\u5982GRPO\uff09\u5728\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u4e2d\u5b58\u5728\u4f18\u52bf\u504f\u5dee\u95ee\u9898\uff0c\u7531\u4e8e\u4e8c\u5143\u51b3\u7b56\u7a7a\u95f4\u548c\u9759\u6001\u4e0d\u786e\u5b9a\u6027\u5956\u52b1\uff0c\u5bfc\u81f4\u6a21\u578b\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\u8981\u4e48\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u9650\u5236\u4e86LLM\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51faUnCertainty-Aware Policy Optimization (UCPO)\u6846\u67b6\uff1a1) \u4e09\u5143\u4f18\u52bf\u89e3\u8026\uff1a\u5206\u79bb\u5e76\u72ec\u7acb\u5f52\u4e00\u5316\u786e\u5b9a\u6027\u548c\u4e0d\u786e\u5b9a\u6027rollouts\u4ee5\u6d88\u9664\u4f18\u52bf\u504f\u5dee\uff1b2) \u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u5956\u52b1\u8c03\u6574\uff1a\u6839\u636e\u6a21\u578b\u6f14\u5316\u548c\u5b9e\u4f8b\u96be\u5ea6\u5b9e\u65f6\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u6743\u91cd\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u901a\u7528\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUCPO\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5176\u77e5\u8bc6\u8fb9\u754c\u4e4b\u5916\u7684\u53ef\u9760\u6027\u548c\u6821\u51c6\u80fd\u529b\u3002", "conclusion": "UCPO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709RL\u8303\u5f0f\u4e2d\u7684\u4f18\u52bf\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e09\u5143\u4f18\u52bf\u89e3\u8026\u548c\u52a8\u6001\u5956\u52b1\u8c03\u6574\u673a\u5236\uff0c\u4e3a\u6784\u5efa\u5177\u6709\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u80fd\u529b\u7684\u53ef\u4fe1LLM\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23059", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.23059", "abs": "https://arxiv.org/abs/2601.23059", "authors": ["Antonio Vitale", "Emanuela Guglielmi", "Simone Scalabrino", "Rocco Oliveto"], "title": "On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study", "comment": "Accepted at the 34th IEEE/ACM International Conference on Program Comprehension (ICPC 2026)", "summary": "Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4ee3\u7801\u6ce8\u91ca\u5bf9LLM\u81ea\u52a8\u4fee\u590dbug\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u5305\u542b\u6ce8\u91ca\u53ef\u5c06\u4fee\u590d\u51c6\u786e\u7387\u63d0\u5347\u81f3\u591a3\u500d\uff0c\u4e14\u5b9e\u73b0\u7ec6\u8282\u7c7b\u6ce8\u91ca\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8bug\u4fee\u590d\u7814\u7a76\u4e2d\u666e\u904d\u5728\u8bad\u7ec3\u524d\u79fb\u9664\u4ee3\u7801\u6ce8\u91ca\uff0c\u4f46\u4f5c\u8005\u5047\u8bbe\u6ce8\u91ca\u53ef\u80fd\u5305\u542b\u91cd\u8981\u7684\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4fe1\u606f\uff0c\u5bf9\u4fee\u590d\u67d0\u4e9b\u7c7b\u578bbug\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u9a8c\u8bc1\u6ce8\u91ca\u5bf9LLM\u4fee\u590d\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u6a21\u578b\u5bb6\u65cf\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5206\u522b\u8bbe\u7f6e\u6709\u65e0\u6ce8\u91ca\u7684\u56db\u79cd\u7ec4\u5408\u6761\u4ef6\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff1b\u4e3a\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u6ce8\u91ca\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f7f\u7528LLM\u81ea\u52a8\u4e3a\u7f3a\u5c11\u6ce8\u91ca\u7684\u65b9\u6cd5\u751f\u6210\u6ce8\u91ca\u3002", "result": "\u6ce8\u91ca\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u51fa\u73b0\u65f6\uff0c\u53ef\u5c06\u81ea\u52a8bug\u4fee\u590d\u51c6\u786e\u7387\u63d0\u5347\u81f3\u591a3\u500d\uff1b\u8bad\u7ec3\u65f6\u5305\u542b\u6ce8\u91ca\u4e0d\u4f1a\u964d\u4f4e\u65e0\u6ce8\u91ca\u5b9e\u4f8b\u7684\u6027\u80fd\uff1b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u53d1\u73b0\u63cf\u8ff0\u65b9\u6cd5\u5b9e\u73b0\u7684\u6ce8\u91ca\u5bf9\u5e2e\u52a9LLM\u51c6\u786e\u4fee\u590dbug\u7279\u522b\u6709\u6548\u3002", "conclusion": "\u4ee3\u7801\u6ce8\u91ca\u5bf9LLM\u7684bug\u4fee\u590d\u80fd\u529b\u6709\u663e\u8457\u6b63\u9762\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u5305\u542b\u6ce8\u91ca\u65f6\u6548\u679c\u6700\u4f73\uff0c\u8fd9\u6311\u6218\u4e86\u5f53\u524d\u79fb\u9664\u6ce8\u91ca\u7684\u5e38\u89c1\u505a\u6cd5\uff0c\u5efa\u8bae\u5728\u81ea\u52a8bug\u4fee\u590d\u7814\u7a76\u4e2d\u4fdd\u7559\u6ce8\u91ca\u3002", "topic": "swe application"}}
{"id": "2601.22662", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.22662", "abs": "https://arxiv.org/abs/2601.22662", "authors": ["Wei Zhu", "Lixing Yu", "Hao-Ren Yao", "Zhiwen Tang", "Kun Yue"], "title": "Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support", "comment": "A shorter version of this work has been accepted by ICASSP 2026", "summary": "Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.", "AI": {"tldr": "TALC\u662f\u4e00\u4e2a\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u59d4\u5458\u4f1a\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u5b9e\u73b0\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u548c\u9ad8\u6548\u591a\u6b65\u89c4\u5212\uff0c\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e0d\u540cLLM\u6a21\u578b\u4e4b\u95f4\u7684\u4e13\u4e1a\u5316\u5dee\u5f02\uff0c\u5c06\u6240\u6709\u6a21\u578b\u89c6\u4e3a\u540c\u7b49\u9002\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u9002\u5e94\u4e0d\u540c\u63a8\u7406\u9700\u6c42\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u7684LLM\u59d4\u5458\u4f1a(TALC)\u6846\u67b6\uff1a1)\u4e3a\u6bcf\u4e2aLLM\u6784\u5efa\u7ed3\u6784\u5316\u6210\u529f\u8bb0\u5fc6\u6863\u6848\uff1b2)\u5728\u51b3\u7b56\u70b9\u901a\u8fc7\u8bed\u4e49\u5339\u914d\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u6a21\u578b\uff1b3)\u4f7f\u7528\u878d\u5408\u6a21\u578b\u8bc4\u4f30\u548c\u5386\u53f2\u6548\u7528\u7684\u53cc\u4fe1\u53f7\u673a\u5236\u4f30\u8ba1\u8282\u70b9\u4ef7\u503c\uff1b4)\u57fa\u4e8e\u8282\u70b9\u5185\u65b9\u5dee\u81ea\u9002\u5e94\u52a0\u6743\u4fe1\u53f7\uff0c\u6307\u5bfcMCTS\u641c\u7d22\u3002", "result": "\u5728WebShop\u3001HumanEval\u548c24\u70b9\u6e38\u620f\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTALC\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u6539\u8fdb\u7684\u641c\u7d22\u6548\u7387\u3002", "conclusion": "TALC\u9a8c\u8bc1\u4e86\u4e13\u4e1a\u5316\u611f\u77e5\u8def\u7531\u548c\u81ea\u9002\u5e94\u89c4\u5212\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u4e0d\u540cLLM\u7684\u4e13\u4e1a\u5316\u80fd\u529b\uff0c\u63d0\u5347\u51b3\u7b56\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.22588", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22588", "abs": "https://arxiv.org/abs/2601.22588", "authors": ["Zhuochun Li", "Yong Zhang", "Ming Li", "Yuelyu Ji", "Yiming Zeng", "Ning Cheng", "Yun Zhu", "Yanmeng Wang", "Shaojun Wang", "Jing Xiao", "Daqing He"], "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry", "comment": null, "summary": "Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this \"LLM-as-a-Judge\" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.", "AI": {"tldr": "\u5c0f\u6a21\u578b\u901a\u8fc7\u5185\u90e8\u8868\u5f81\u800c\u975e\u8868\u9762\u751f\u6210\u53ef\u4f5c\u4e3a\u9ad8\u6548\u8bc4\u4f30\u5668\uff0c\u63d0\u51fa\"\u8868\u5f81\u5373\u88c1\u5224\"\u65b0\u8303\u5f0f\uff0cINSPECTOR\u6846\u67b6\u5728\u63a8\u7406\u57fa\u51c6\u4e0a\u63a5\u8fd1\u5927\u6a21\u578b\u8bc4\u4f30\u6548\u679c\u4f46\u66f4\u9ad8\u6548\u53ef\u9760\u3002", "motivation": "\u5f53\u524d\"LLM-as-a-Judge\"\u8303\u5f0f\u5b58\u5728\u6210\u672c\u9ad8\u3001\u4e0d\u900f\u660e\u3001\u5bf9\u63d0\u793a\u8bbe\u8ba1\u654f\u611f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u5c0f\u6a21\u578b\u5c3d\u7ba1\u751f\u6210\u80fd\u529b\u5f31\uff0c\u4f46\u5176\u9690\u85cf\u72b6\u6001\u5305\u542b\u4e30\u5bcc\u7684\u8bc4\u4f30\u4fe1\u53f7\uff0c\u8fd9\u542f\u53d1\u6211\u4eec\u601d\u8003\u8bc4\u4f30\u662f\u5426\u771f\u7684\u9700\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u5bb9\u91cf\u4e0d\u5bf9\u79f0\u5047\u8bbe\uff1a\u8bc4\u4f30\u6bd4\u751f\u6210\u9700\u8981\u66f4\u5c11\u7684\u8bed\u4e49\u5bb9\u91cf\uff0c\u53ef\u57fa\u4e8e\u4e2d\u95f4\u8868\u5f81\u8fdb\u884c\u3002\u63d0\u51faRepresentation-as-a-Judge\u8303\u5f0f\uff0c\u901a\u8fc7INSPECTOR\u6846\u67b6\u4ece\u6a21\u578b\u5185\u90e8\u8868\u5f81\u9884\u6d4b\u8bc4\u4f30\u5206\u6570\uff0c\u800c\u975e\u4f9d\u8d56\u63d0\u793a\u751f\u6210\u8f93\u51fa\u3002", "result": "\u5728GSM8K\u3001MATH\u3001GPQA\u7b49\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cINSPECTOR\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u5c0f\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63a5\u8fd1\u5b8c\u6574LLM\u88c1\u5224\u7684\u6548\u679c\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6848\u3002", "conclusion": "\u8bc4\u4f30\u4efb\u52a1\u4e0d\u9700\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\uff0c\u5c0f\u6a21\u578b\u7684\u5185\u90e8\u8868\u5f81\u5df2\u5305\u542b\u8db3\u591f\u8bc4\u4f30\u4fe1\u606f\u3002Representation-as-a-Judge\u8303\u5f0f\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0cINSPECTOR\u6846\u67b6\u5c55\u793a\u4e86\u8fd9\u4e00\u8303\u5f0f\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.22664", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22664", "abs": "https://arxiv.org/abs/2601.22664", "authors": ["Zixuan Huang", "Xin Xia", "Yuxi Ren", "Jianbin Zheng", "Xuefeng Xiao", "Hongyan Xie", "Li Huaqiu", "Songshi Liang", "Zhongxiang Dai", "Fuzhen Zhuang", "Jianxin Li", "Yikun Ban", "Deqing Wang"], "title": "Real-Time Aligned Reward Model beyond Semantics", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.", "AI": {"tldr": "\u63d0\u51faR2M\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u7b56\u7565\u6a21\u578b\u7684\u5b9e\u65f6\u9690\u85cf\u72b6\u6001\u53cd\u9988\u6765\u5e94\u5bf9RLHF\u4e2d\u7684\u5956\u52b1\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u5956\u52b1\u6a21\u578b\u4e0e\u7b56\u7565\u5206\u5e03\u6f02\u79fb\u7684\u5b9e\u65f6\u5bf9\u9f50\u3002", "motivation": "RLHF\u5b58\u5728\u5956\u52b1\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u7b56\u7565\u6a21\u578b\u4f1a\u8fc7\u5ea6\u62df\u5408\u5956\u52b1\u6a21\u578b\uff0c\u5229\u7528\u865a\u5047\u7684\u5956\u52b1\u6a21\u5f0f\u800c\u975e\u771f\u6b63\u6355\u6349\u4eba\u7c7b\u610f\u56fe\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8868\u9762\u8bed\u4e49\u4fe1\u606f\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u56e0\u8fde\u7eed\u7b56\u7565\u5206\u5e03\u6f02\u79fb\u5bfc\u81f4\u7684\u5956\u52b1\u6a21\u578b\u4e0e\u7b56\u7565\u6a21\u578b\u4e4b\u95f4\u7684\u9519\u4f4d\uff0c\u8fd9\u4f1a\u52a0\u5267\u5956\u52b1\u8fc7\u4f18\u5316\u3002", "method": "\u63d0\u51faR2M\uff08\u5b9e\u65f6\u5bf9\u9f50\u5956\u52b1\u6a21\u578b\uff09\u6846\u67b6\uff0c\u8d85\u8d8a\u4ec5\u4f9d\u8d56\u9884\u8bad\u7ec3LLM\u8bed\u4e49\u8868\u793a\u7684\u666e\u901a\u5956\u52b1\u6a21\u578b\u3002R2M\u5229\u7528\u7b56\u7565\u6a21\u578b\u5728RL\u8fc7\u7a0b\u4e2d\u7684\u6f14\u5316\u9690\u85cf\u72b6\u6001\uff08\u5373\u7b56\u7565\u53cd\u9988\uff09\uff0c\u4e0e\u7b56\u7565\u7684\u5b9e\u65f6\u5206\u5e03\u6f02\u79fb\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u8fc7\u5b9e\u65f6\u5229\u7528\u7b56\u7565\u6a21\u578b\u7684\u53cd\u9988\u6765\u6539\u8fdb\u5956\u52b1\u6a21\u578b\u6027\u80fd\u6307\u51fa\u4e86\u65b0\u7684\u65b9\u5411\u3002", "conclusion": "R2M\u6846\u67b6\u4e3a\u89e3\u51b3RLHF\u4e2d\u7684\u5956\u52b1\u8fc7\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b56\u7565\u53cd\u9988\u5b9e\u73b0\u5956\u52b1\u6a21\u578b\u4e0e\u7b56\u7565\u5206\u5e03\u6f02\u79fb\u7684\u5b9e\u65f6\u5bf9\u9f50\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22701", "abs": "https://arxiv.org/abs/2601.22701", "authors": ["Emilien Bir\u00e9", "Mar\u00eda Santos", "Kai Yuan"], "title": "Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference", "comment": null, "summary": "Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u7684\u667a\u80fd\u4f53VLM\u589e\u5f3a\u65b9\u6cd5\uff1a\u51bb\u7ed3VLM\u4f5c\u4e3a\u52a8\u4f5c\u63d0\u8bae\u5668\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u79bb\u7ebf\u8bad\u7ec3\u7684Q\u51fd\u6570\u5bf9\u5019\u9009\u52a8\u4f5c\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u76f4\u63a5\u5e94\u7528Q\u51fd\u6570\u5b9e\u73b0\u7b56\u7565\u6539\u8fdb\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4f5c\u4e3a\u667a\u80fd\u4f53\u5728\u6570\u5b57\u73af\u5883\u4e2d\u7684\u9aa8\u5e72\u5b58\u5728\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u53d8\u5316\u7684\u7f51\u7edc\u73af\u5883\u4e2d\u3002\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6a21\u578b\u8bad\u7ec3\u548c\u6570\u636e\u6536\u96c6\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u7075\u6d3b\u3002", "method": "\u5c06VLM\u7684\u89d2\u8272\u89e3\u8026\u4e3a\u9ad8\u5bb9\u91cf\u52a8\u4f5c\u63d0\u8bae\u5668\u548c\u6700\u7ec8\u52a8\u4f5c\u9009\u62e9\u673a\u5236\uff1a1) \u51bb\u7ed3VLM\u7b56\u7565\uff0c\u4e3a\u7ed9\u5b9a\u72b6\u6001\u751f\u6210\u5019\u9009\u52a8\u4f5c\u96c6\uff1b2) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u79bb\u7ebf\u8bad\u7ec3\u7684Q\u51fd\u6570\u5bf9\u8fd9\u4e9b\u5019\u9009\u52a8\u4f5c\u8fdb\u884c\u91cd\u6392\u5e8f\uff1b3) \u6267\u884c\u4f30\u8ba1\u4ef7\u503c\u6700\u9ad8\u7684\u52a8\u4f5c\u3002Q\u51fd\u6570\u76f4\u63a5\u5728\u63a8\u7406\u9636\u6bb5\u5e94\u7528\uff0c\u800c\u975e\u7528\u4e8e\u79bb\u7ebf\u6570\u636e\u91cd\u6807\u6ce8\u548c\u7b56\u7565\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728WebVoyager\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u6210\u529f\u7387\uff1aQwen2.5-VL-7B\u667a\u80fd\u4f53\u4ece38.8%\u63d0\u5347\u81f355.7%\uff0c\u4e13\u6709GPT-4.1\u667a\u80fd\u4f53\u4ece82.4%\u63d0\u5347\u81f388.8%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7b56\u7565\u91cd\u65b0\u8bad\u7ec3\u5c31\u80fd\u589e\u5f3a\u667a\u80fd\u4f53VLM\u7b56\u7565\u7684\u6709\u6548\u8303\u5f0f\uff0c\u901a\u8fc7\u89e3\u8026\u52a8\u4f5c\u63d0\u8bae\u548c\u9009\u62e9\u673a\u5236\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u76f4\u63a5\u5e94\u7528Q\u51fd\u6570\u5b9e\u73b0\u5373\u65f6\u7b56\u7565\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2601.22718", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22718", "abs": "https://arxiv.org/abs/2601.22718", "authors": ["Shiye Lei", "Zhihao Cheng", "Dacheng Tao"], "title": "A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization", "comment": null, "summary": "Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMinPRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u524d\u7f00\u4e2d\u6700\u5c0ftoken\u7ea7\u6bd4\u7387\u66ff\u4ee3\u4e0d\u7a33\u5b9a\u7684\u7d2f\u79ef\u524d\u7f00\u6bd4\u7387\uff0c\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u56e0\u91c7\u6837\u7b56\u7565\u4e0e\u76ee\u6807\u7b56\u7565\u5dee\u5f02\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RL\u540e\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528token\u7ea7\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u7387\u6765\u6821\u6b63\u91c7\u6837\u7b56\u7565\u4e0e\u76ee\u6807\u7b56\u7565\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4f46\u4f5c\u8005\u89c2\u5bdf\u5230\u5f53\u7b56\u7565\u504f\u79bb\u7a0b\u5ea6\u8f83\u5927\u65f6\uff0ctoken\u7ea7\u6821\u6b63\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u52a8\u6001\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faMinPRO\uff08\u6700\u5c0f\u524d\u7f00\u6bd4\u7387\uff09\u76ee\u6807\u51fd\u6570\uff0c\u7528\u57fa\u4e8e\u524d\u7f00\u4e2d\u89c2\u5bdf\u5230\u7684\u6700\u5c0ftoken\u7ea7\u6bd4\u7387\u7684\u975e\u7d2f\u79ef\u66ff\u4ee3\u9879\uff0c\u66ff\u6362\u4e0d\u7a33\u5b9a\u7684\u7d2f\u79ef\u524d\u7f00\u6bd4\u7387\uff0c\u4ee5\u7a33\u5b9a\u5927\u7b56\u7565\u504f\u79bb\u4e0b\u7684LLM\u4f18\u5316\u3002", "result": "\u5728\u5bc6\u96c6\u548c\u6df7\u5408\u4e13\u5bb6LLM\u4e0a\uff0c\u8de8\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMinPRO\u663e\u8457\u63d0\u9ad8\u4e86\u79bb\u7b56\u7565\u673a\u5236\u4e0b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5cf0\u503c\u6027\u80fd\u3002", "conclusion": "MinPRO\u901a\u8fc7\u66f4\u7a33\u5b9a\u7684\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u7684\u79bb\u7b56\u7565\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23254", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.23254", "abs": "https://arxiv.org/abs/2601.23254", "authors": ["Baoyi Wang", "Xingliang Wang", "Guochang Li", "Chen Zhi", "Junxiao Han", "Xinkui Zhao", "Nan Wang", "Shuiguang Deng", "Jianwei Yin"], "title": "GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion", "comment": "Under Review", "summary": "Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.", "AI": {"tldr": "GrepRAG\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u7d22\u5f15\u7684\u8bcd\u6c47\u68c0\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\uff0c\u901a\u8fc7ripgrep\u547d\u4ee4\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bed\u4e49\u7d22\u5f15\u6216\u56fe\u5206\u6790\u7684RAG\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u800c\u5f00\u53d1\u8005\u5e38\u7528\u8f7b\u91cf\u7ea7\u641c\u7d22\u5de5\u5177\uff08\u5982ripgrep\uff09\u3002\u672c\u6587\u63a2\u7d22\u7b80\u5355\u65e0\u7d22\u5f15\u7684\u8bcd\u6c47\u68c0\u7d22\u5728\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u6f5c\u529b\u3002", "method": "1. \u63d0\u51faNaive GrepRAG\u57fa\u7ebf\u6846\u67b6\uff1aLLM\u81ea\u4e3b\u751f\u6210ripgrep\u547d\u4ee4\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\uff1b2. \u63d0\u51faGrepRAG\u6539\u8fdb\u65b9\u6cd5\uff1a\u589e\u52a0\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u6807\u8bc6\u7b26\u52a0\u6743\u91cd\u6392\u5e8f\u548c\u7ed3\u6784\u611f\u77e5\u53bb\u91cd\u3002", "result": "Naive GrepRAG\u6027\u80fd\u4e0e\u590d\u6742\u56fe\u57fa\u7ebf\u76f8\u5f53\uff1bGrepRAG\u5728CrossCodeEval\u548cRepoEval-Updated\u4e0a\u6301\u7eed\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u5728CrossCodeEval\u4e0a\u4ee3\u7801\u7cbe\u786e\u5339\u914d\u76f8\u5bf9\u63d0\u53477.04-15.58%\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u65e0\u7d22\u5f15\u8bcd\u6c47\u68c0\u7d22\u5728\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0cGrepRAG\u901a\u8fc7\u7b80\u5355\u540e\u5904\u7406\u514b\u670d\u8bcd\u6c47\u68c0\u7d22\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.22758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22758", "abs": "https://arxiv.org/abs/2601.22758", "authors": ["Libin Qiu", "Zhirong Gao", "Junfu Chen", "Yuhang Ye", "Weizhi Huang", "Xiaobo Xue", "Wenkai Qiu", "Shuo Tang"], "title": "AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement", "comment": "8 pages, 3 figures, 3 tables", "summary": "Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.", "AI": {"tldr": "AutoRefine\u662f\u4e00\u4e2a\u4ece\u667a\u80fd\u4f53\u6267\u884c\u5386\u53f2\u4e2d\u63d0\u53d6\u548c\u7ef4\u62a4\u53cc\u5f62\u5f0f\u7ecf\u9a8c\u6a21\u5f0f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u4e13\u95e8\u5b50\u667a\u80fd\u4f53\u5904\u7406\u7a0b\u5e8f\u6027\u4efb\u52a1\uff0c\u63d0\u53d6\u6280\u80fd\u6a21\u5f0f\u5904\u7406\u9759\u6001\u77e5\u8bc6\uff0c\u5e76\u6301\u7eed\u7ef4\u62a4\u6a21\u5f0f\u5e93\u9632\u6b62\u9000\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5f80\u5f80\u65e0\u6cd5\u4ece\u7ecf\u9a8c\u4e2d\u79ef\u7d2f\u77e5\u8bc6\uff0c\u5c06\u6bcf\u4e2a\u4efb\u52a1\u89c6\u4e3a\u72ec\u7acb\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u7ecf\u9a8c\u63d0\u53d6\u4e3a\u6241\u5e73\u5316\u7684\u6587\u672c\u77e5\u8bc6\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u5b50\u4efb\u52a1\u7684\u7a0b\u5e8f\u903b\u8f91\uff0c\u4e5f\u7f3a\u4e4f\u7ef4\u62a4\u673a\u5236\uff0c\u5bfc\u81f4\u968f\u7740\u7ecf\u9a8c\u79ef\u7d2f\u77e5\u8bc6\u5e93\u9000\u5316\u3002", "method": "\u63d0\u51faAutoRefine\u6846\u67b6\uff0c\u4ece\u667a\u80fd\u4f53\u6267\u884c\u5386\u53f2\u4e2d\u63d0\u53d6\u53cc\u5f62\u5f0f\u7ecf\u9a8c\u6a21\u5f0f\uff1a\u5bf9\u4e8e\u7a0b\u5e8f\u6027\u5b50\u4efb\u52a1\uff0c\u63d0\u53d6\u5177\u6709\u72ec\u7acb\u63a8\u7406\u548c\u8bb0\u5fc6\u7684\u4e13\u95e8\u5b50\u667a\u80fd\u4f53\uff1b\u5bf9\u4e8e\u9759\u6001\u77e5\u8bc6\uff0c\u63d0\u53d6\u6280\u80fd\u6a21\u5f0f\u4f5c\u4e3a\u6307\u5bfc\u65b9\u9488\u6216\u4ee3\u7801\u7247\u6bb5\u3002\u91c7\u7528\u6301\u7eed\u7ef4\u62a4\u673a\u5236\u5bf9\u6a21\u5f0f\u8fdb\u884c\u8bc4\u5206\u3001\u4fee\u526a\u548c\u5408\u5e76\uff0c\u9632\u6b62\u77e5\u8bc6\u5e93\u9000\u5316\u3002", "result": "\u5728ALFWorld\u3001ScienceWorld\u548cTravelPlanner\u57fa\u51c6\u4e0a\u5206\u522b\u8fbe\u523098.4%\u300170.4%\u548c27.1%\u7684\u6210\u529f\u7387\uff0c\u6b65\u9aa4\u51cf\u5c1120-73%\u3002\u5728TravelPlanner\u4e0a\uff0c\u81ea\u52a8\u63d0\u53d6\u8d85\u8fc7\u624b\u52a8\u8bbe\u8ba1\u7cfb\u7edf\uff0827.1% vs 12.1%\uff09\uff0c\u5c55\u793a\u4e86\u6355\u6349\u7a0b\u5e8f\u534f\u8c03\u7684\u80fd\u529b\u3002", "conclusion": "AutoRefine\u901a\u8fc7\u63d0\u53d6\u548c\u7ef4\u62a4\u53cc\u5f62\u5f0f\u7ecf\u9a8c\u6a21\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u7ecf\u9a8c\u79ef\u7d2f\u548c\u77e5\u8bc6\u5e93\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7a0b\u5e8f\u534f\u8c03\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2601.22776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22776", "abs": "https://arxiv.org/abs/2601.22776", "authors": ["Shichao Ma", "Zhiyuan Ma", "Ming Yang", "Xiaofan Li", "Xing Wu", "Jintao Du", "Yu Cheng", "Weiqiang Wang", "Qiliang Liu", "Zhengyang Zhou", "Yang Wang"], "title": "TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization", "comment": null, "summary": "Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a \"Double Homogenization Dilemma.\" This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.", "AI": {"tldr": "TSPO\u901a\u8fc7\u5f15\u5165\u9996\u6b21\u51fa\u73b0\u6f5c\u5728\u5956\u52b1\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4e2d\u7684\u53cc\u91cd\u540c\u8d28\u5316\u56f0\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u7ea7\u5956\u52b1\uff0c\u5bfc\u81f4\u4e86\"\u53cc\u91cd\u540c\u8d28\u5316\u56f0\u5883\"\uff1a\u8fc7\u7a0b\u540c\u8d28\u5316\uff08\u5ffd\u7565\u601d\u8003\u3001\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u8fc7\u7a0b\uff09\u548c\u7ec4\u5185\u540c\u8d28\u5316\uff08\u7c97\u7c92\u5ea6\u7ed3\u679c\u5956\u52b1\u5bfc\u81f4\u7ec4\u5185\u4f18\u52bf\u4f30\u8ba1\u6548\u7387\u4f4e\u4e0b\uff09\u3002", "method": "\u63d0\u51fa\u4e86Turn-level Stage-aware Policy Optimization (TSPO)\uff0c\u5f15\u5165\u9996\u6b21\u51fa\u73b0\u6f5c\u5728\u5956\u52b1(FOLR)\u673a\u5236\uff0c\u5c06\u90e8\u5206\u5956\u52b1\u5206\u914d\u7ed9\u6b63\u786e\u7b54\u6848\u9996\u6b21\u51fa\u73b0\u7684\u6b65\u9aa4\uff0c\u4ece\u800c\u4fdd\u7559\u8fc7\u7a0b\u7ea7\u4fe1\u53f7\u5e76\u589e\u52a0\u7ec4\u5185\u5956\u52b1\u65b9\u5dee\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6a21\u578b\u6216\u989d\u5916\u6807\u6ce8\u3002", "result": "TSPO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Qwen2.5-3B\u548c7B\u6a21\u578b\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u574724%\u548c13.6%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "TSPO\u901a\u8fc7\u89e3\u51b3\u53cc\u91cd\u540c\u8d28\u5316\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684\u6027\u80fd\uff0c\u4e3a\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22781", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22781", "abs": "https://arxiv.org/abs/2601.22781", "authors": ["Linjia Kang", "Zhimin Wang", "Yongkang Zhang", "Duo Wu", "Jinghe Wang", "Ming Ma", "Haopeng Yan", "Zhi Wang"], "title": "Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training", "comment": null, "summary": "Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.", "AI": {"tldr": "MobileGen\u662f\u4e00\u4e2a\u79fb\u52a8GUI\u4ee3\u7406\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u6784\u4efb\u52a1\u96be\u5ea6\u4e3a\u7ed3\u6784\u548c\u8bed\u4e49\u7ef4\u5ea6\uff0c\u81ea\u9002\u5e94\u5730\u8bc4\u4f30\u4ee3\u7406\u80fd\u529b\u8fb9\u754c\uff0c\u5e76\u751f\u6210\u4e0e\u4e4b\u5339\u914d\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8GUI\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6f14\u793a\u6216\u81ea\u52a8\u6a21\u578b\u63a2\u7d22\uff0c\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u96be\u5ea6\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u5bfc\u81f4\u8bad\u7ec3\u96be\u5ea6\u4e0e\u4ee3\u7406\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u6548\u679c\u3002", "method": "1) \u5c06\u4efb\u52a1\u96be\u5ea6\u89e3\u6784\u4e3a\u7ed3\u6784\u7ef4\u5ea6\uff08\u5982\u8f68\u8ff9\u957f\u5ea6\uff09\u548c\u8bed\u4e49\u7ef4\u5ea6\uff08\u5982\u4efb\u52a1\u76ee\u6807\uff09\uff1b2) \u8fed\u4ee3\u8bc4\u4f30\u4ee3\u7406\u5728\u73b0\u6709\u6570\u636e\u96c6\u4e0a\u7684\u80fd\u529b\u8fb9\u754c\uff1b3) \u81ea\u9002\u5e94\u8ba1\u7b97\u4efb\u52a1\u96be\u5ea6\u6982\u7387\u5206\u5e03\u5e76\u91c7\u6837\u76ee\u6807\u96be\u5ea6\uff1b4) \u4f7f\u7528\u591a\u4ee3\u7406\u53ef\u63a7\u751f\u6210\u5668\u5408\u6210\u9ad8\u8d28\u91cf\u4ea4\u4e92\u8f68\u8ff9\u548c\u4efb\u52a1\u6307\u4ee4\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMobileGen\u5c06GUI\u4ee3\u7406\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e861.57\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u80fd\u529b\u5bf9\u9f50\u7684\u6570\u636e\u751f\u6210\u5bf9\u4e8e\u79fb\u52a8GUI\u4ee3\u7406\u7684\u6709\u6548\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff0cMobileGen\u901a\u8fc7\u81ea\u9002\u5e94\u96be\u5ea6\u63a7\u5236\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u4ee3\u7406\u5b66\u4e60\u3002", "topic": "agent analysis"}}
{"id": "2601.22786", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22786", "abs": "https://arxiv.org/abs/2601.22786", "authors": ["Hamid Reza Akbari", "Mohammad Hossein Sameti", "Amir M. Mansourian", "Mohammad Hossein Rohban", "Hossein Sameti"], "title": "Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework", "comment": "13 pages, 8 figures, 4 tables", "summary": "The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6574\u5408\u4fe1\u606f\u7406\u8bba(IIT)\u7684\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u56e0\u679c\u6027\u3001\u8fde\u8d2f\u6027\u548c\u6574\u5408\u6027\uff0c\u5b9e\u73b0\u4e86\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1131%\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u867d\u7136\u4e0d\u5177\u5907\u610f\u8bc6\uff0c\u4f46\u8868\u73b0\u51fa\u7c7b\u4f3c\u610f\u8bc6\u7684\u67d0\u4e9b\u884c\u4e3a\u7279\u5f81\u3002\u7814\u7a76\u65e8\u5728\u5c06\u610f\u8bc6\u7406\u8bba\uff08\u6574\u5408\u4fe1\u606f\u7406\u8bbaIIT\uff09\u5e94\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\uff0c\u63a2\u7d22\u901a\u8fc7\u5956\u52b1\u5b66\u4e60\u8303\u5f0f\u63d0\u5347\u6a21\u578b\u751f\u6210\u8d28\u91cf\u7684\u53ef\u80fd\u6027\u3002", "method": "\u57fa\u4e8e\u6574\u5408\u4fe1\u606f\u7406\u8bba(IIT)\u7684\u6838\u5fc3\u539f\u5219\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u91cf\u5316\u6587\u672c\u56e0\u679c\u6027\u3001\u8fde\u8d2f\u6027\u548c\u6574\u5408\u6027\u7684\u5956\u52b1\u51fd\u6570\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u4f18\u5316\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u751f\u6210\u66f4\u7b26\u5408\u610f\u8bc6\u5904\u7406\u7279\u5f81\u7684\u6587\u672c\u3002", "result": "\u4f18\u5316IIT\u5956\u52b1\u51fd\u6570\u540e\uff0c\u6a21\u578b\u5728\u57df\u5916\u4efb\u52a1\u4e2d\u8f93\u51fa\u957f\u5ea6\u51cf\u5c11\u8fbe31%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u6539\u5584\u4e86\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684IIT\u5956\u52b1\u6846\u67b6\u5177\u6709\u6982\u5ff5\u7b80\u5355\u3001\u8ba1\u7b97\u9ad8\u6548\u3001\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u8f85\u52a9\u6a21\u578b\u7b49\u5b9e\u7528\u4f18\u52bf\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u529b\u9a71\u52a8\u4fe1\u53f7\u800c\u975e\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22803", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22803", "abs": "https://arxiv.org/abs/2601.22803", "authors": ["Ji Shi", "Peiming Guo", "Meishan Zhang", "Miao Zhang", "Xuebo Liu", "Min Zhang", "Weili Guan"], "title": "CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning", "comment": "17 pages, 3 figures", "summary": "Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git", "AI": {"tldr": "CVeDRL\uff1a\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7801\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u8bed\u6cd5\u3001\u529f\u80fd\u3001\u5206\u652f\u8986\u76d6\u548c\u6837\u672c\u96be\u5ea6\u611f\u77e5\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u6548\u679c\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u4ee3\u7801\u9a8c\u8bc1\u5668\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u5931\u8d25\u7387\u9ad8\u3001\u63a8\u7406\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u63d0\u4f9b\u65e0\u76d1\u7763\u4f18\u5316\u7684\u53ef\u80fd\uff0c\u4f46\u4ec5\u4f7f\u7528\u529f\u80fd\u5956\u52b1\u7684\u6734\u7d20RL\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u9488\u5bf9\u56f0\u96be\u5206\u652f\u548c\u6837\u672c\u7684\u6709\u6548\u5355\u5143\u6d4b\u8bd5\u3002", "method": "\u9996\u5148\u7406\u8bba\u5206\u6790\u5c06\u5206\u652f\u8986\u76d6\u3001\u6837\u672c\u96be\u5ea6\u3001\u8bed\u6cd5\u548c\u529f\u80fd\u6b63\u786e\u6027\u5efa\u6a21\u4e3aRL\u5956\u52b1\u3002\u7136\u540e\u8bbe\u8ba1\u8bed\u6cd5\u548c\u529f\u80fd\u611f\u77e5\u7684\u5956\u52b1\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6307\u6570\u5956\u52b1\u5851\u9020\u548c\u9759\u6001\u5206\u6790\u6307\u6807\u7684\u5206\u652f\u548c\u6837\u672c\u96be\u5ea6\u611f\u77e5RL\u65b9\u6cd5\u3002", "result": "CVeDRL\u4ec5\u75280.6B\u53c2\u6570\u5c31\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u76f8\u6bd4GPT-3.5\u63d0\u534728.97%\u7684\u901a\u8fc7\u7387\u548c15.08%\u7684\u5206\u652f\u8986\u76d6\u7387\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u6bd4\u7ade\u4e89\u57fa\u7ebf\u5feb20\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u5c06\u591a\u79cd\u9a8c\u8bc1\u4fe1\u53f7\u8054\u5408\u5efa\u6a21\u4e3aRL\u5956\u52b1\uff0cCVeDRL\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u9a8c\u8bc1\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2601.22997", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22997", "abs": "https://arxiv.org/abs/2601.22997", "authors": ["Roham Koohestani", "Ate\u015f G\u00f6rpelio\u011flu", "Egor Klimov", "Burcu Kulahcioglu Ozkan", "Maliheh Izadi"], "title": "TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI", "comment": null, "summary": "Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.", "AI": {"tldr": "TriCEGAR\uff1a\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u9a71\u52a8\u7684\u62bd\u8c61\u673a\u5236\uff0c\u81ea\u52a8\u4ece\u6267\u884c\u65e5\u5fd7\u6784\u5efa\u72b6\u6001\u62bd\u8c61\uff0c\u652f\u6301\u5728\u7ebf\u6784\u5efa\u667a\u80fd\u4f53\u884c\u4e3aMDP\uff0c\u901a\u8fc7\u53cd\u4f8b\u7cbe\u5316\u8c13\u8bcd\u6811\uff0c\u5b9e\u73b0\u8fd0\u884c\u65f6\u6982\u7387\u9a8c\u8bc1\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u6982\u7387\u4fdd\u8bc1\uff08DPA\uff09\u65b9\u6cd5\u9700\u8981\u5f00\u53d1\u8005\u624b\u52a8\u5b9a\u4e49\u72b6\u6001\u62bd\u8c61\uff0c\u8fd9\u5bfc\u81f4\u9a8c\u8bc1\u4e0e\u7279\u5b9a\u5e94\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u8026\u5408\uff0c\u589e\u52a0\u4e86\u91c7\u7528\u96be\u5ea6\u3002\u9700\u8981\u81ea\u52a8\u5316\u72b6\u6001\u6784\u5efa\u673a\u5236\u6765\u964d\u4f4e\u91c7\u7528\u95e8\u69db\u3002", "method": "\u63d0\u51faTriCEGAR\u65b9\u6cd5\uff1a1\uff09\u4ece\u8f68\u8ff9\u4e2d\u5b66\u4e60\u8c13\u8bcd\u6811\u4f5c\u4e3a\u62bd\u8c61\u8868\u793a\uff1b2\uff09\u4f7f\u7528\u53cd\u4f8b\u8fdb\u884c\u7cbe\u5316\uff1b3\uff09\u6846\u67b6\u539f\u751f\u5b9e\u73b0\u6355\u83b7\u7c7b\u578b\u5316\u667a\u80fd\u4f53\u751f\u547d\u5468\u671f\u4e8b\u4ef6\uff1b4\uff09\u4ece\u8f68\u8ff9\u6784\u5efaMDP\uff1b5\uff09\u8fdb\u884c\u6982\u7387\u6a21\u578b\u68c0\u67e5\u8ba1\u7b97\u8fb9\u754c\u6982\u7387\uff1b6\uff09\u5229\u7528\u8fd0\u884c\u4f3c\u7136\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u72b6\u6001\u62bd\u8c61\u6784\u5efa\uff0c\u652f\u6301\u5728\u7ebf\u6784\u5efa\u667a\u80fd\u4f53\u884c\u4e3aMDP\uff0c\u80fd\u591f\u8ba1\u7b97Pmax(\u6210\u529f)\u548cPmin(\u5931\u8d25)\u7b49\u6982\u7387\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u4f3c\u7136\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u4f5c\u4e3a\u62a4\u680f\u4fe1\u53f7\u3002", "conclusion": "TriCEGAR\u901a\u8fc7\u81ea\u52a8\u5316\u72b6\u6001\u62bd\u8c61\u6784\u5efa\u89e3\u51b3\u4e86DPA\u65b9\u6cd5\u7684\u4e3b\u8981\u9650\u5236\uff0c\u964d\u4f4e\u4e86\u667a\u80fd\u4f53AI\u7cfb\u7edf\u8fd0\u884c\u65f6\u9a8c\u8bc1\u7684\u91c7\u7528\u95e8\u69db\uff0c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6982\u7387\u4fdd\u8bc1\u548c\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.22795", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22795", "abs": "https://arxiv.org/abs/2601.22795", "authors": ["Corentin Kervadec", "Iuliia Lysova", "Marco Baroni", "Gemma Boleda"], "title": "Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs", "comment": null, "summary": "Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design a density estimator drawing on mechanistic interpretability. We experimentally test our estimator and find that: (1) contrary to what has been often assumed, LLM processing generally involves dense computation; (2) computation density is dynamic, in the sense that models shift between sparse and dense processing regimes depending on the input; (3) per-input density is significantly correlated across LLMs, suggesting that the same inputs trigger either low or high density. Investigating the factors influencing density, we observe that predicting rarer tokens requires higher density, and increasing context length often decreases the density. We believe that our computation density estimator will contribute to a better understanding of the processing at work in LLMs, challenging their symbolic interpretation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u5bc6\u5ea6\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0LLM\u5904\u7406\u901a\u5e38\u6d89\u53ca\u5bc6\u96c6\u8ba1\u7b97\u800c\u975e\u7a00\u758f\u8ba1\u7b97\uff0c\u8ba1\u7b97\u5bc6\u5ea6\u662f\u52a8\u6001\u53d8\u5316\u7684\uff0c\u4e14\u4e0e\u8f93\u5165\u5185\u5bb9\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u526a\u679d\u5927\u91cf\u53c2\u6570\u800c\u5bf9\u6027\u80fd\u5f71\u54cd\u4e0d\u5927\uff0c\u8fd9\u8868\u660e\u8ba1\u7b97\u5728\u53c2\u6570\u4e2d\u5e76\u975e\u5747\u5300\u5206\u5e03\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u91cf\u5316LLM\u8ba1\u7b97\u5bc6\u5ea6\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3LLM\u5185\u90e8\u7684\u5904\u7406\u673a\u5236\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u8be5\u4f30\u8ba1\u5668\u6765\u5206\u6790LLM\u7684\u8ba1\u7b97\u5bc6\u5ea6\u7279\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u7cfb\u7edf\u5730\u91cf\u5316\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u5bc6\u5ea6\u5206\u5e03\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) LLM\u5904\u7406\u901a\u5e38\u6d89\u53ca\u5bc6\u96c6\u8ba1\u7b97\uff0c\u800c\u975e\u901a\u5e38\u5047\u8bbe\u7684\u7a00\u758f\u8ba1\u7b97\uff1b(2) \u8ba1\u7b97\u5bc6\u5ea6\u662f\u52a8\u6001\u7684\uff0c\u6a21\u578b\u6839\u636e\u8f93\u5165\u5728\u7a00\u758f\u548c\u5bc6\u96c6\u5904\u7406\u6a21\u5f0f\u95f4\u5207\u6362\uff1b(3) \u4e0d\u540cLLM\u5bf9\u76f8\u540c\u8f93\u5165\u7684\u8ba1\u7b97\u5bc6\u5ea6\u663e\u8457\u76f8\u5173\uff1b(4) \u9884\u6d4b\u7f55\u89c1\u8bcd\u9700\u8981\u66f4\u9ad8\u5bc6\u5ea6\uff0c\u589e\u52a0\u4e0a\u4e0b\u6587\u957f\u5ea6\u901a\u5e38\u964d\u4f4e\u5bc6\u5ea6\u3002", "conclusion": "\u8ba1\u7b97\u5bc6\u5ea6\u4f30\u8ba1\u5668\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3LLM\u7684\u5904\u7406\u673a\u5236\uff0c\u6311\u6218\u4e86LLM\u7684\u7b26\u53f7\u89e3\u91ca\u89c2\u70b9\u3002\u8be5\u7814\u7a76\u63ed\u793a\u4e86LLM\u8ba1\u7b97\u7684\u5b9e\u9645\u5206\u5e03\u7279\u6027\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u548c\u89e3\u91ca\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2601.22948", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22948", "abs": "https://arxiv.org/abs/2601.22948", "authors": ["Nicola Milano", "Stefano Nolfi"], "title": "Alignment among Language, Vision and Action Representations", "comment": null, "summary": "A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u52a8\u4f5c\u4e09\u79cd\u5b66\u4e60\u6a21\u6001\u4f1a\u5f62\u6210\u90e8\u5206\u5171\u4eab\u7684\u8bed\u4e49\u8868\u5f81\uff0c\u652f\u6301\u6a21\u6001\u72ec\u7acb\u7684\u8bed\u4e49\u7ec4\u7ec7\uff0c\u4e3a\u5177\u8eabAI\u7cfb\u7edf\u7684\u8de8\u9886\u57df\u8fc1\u79fb\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "motivation": "\u63a2\u7d22\u8ba4\u77e5\u79d1\u5b66\u548cAI\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u4e0d\u540c\u5b66\u4e60\u6a21\u6001\uff08\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u52a8\u4f5c\uff09\u662f\u5426\u4f1a\u4ea7\u751f\u4e0d\u540c\u6216\u5171\u4eab\u7684\u5185\u90e8\u8868\u5f81\u3002\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7684\u6a21\u578b\u4f1a\u53d1\u5c55\u51fa\u4e13\u95e8\u5316\u3001\u4e0d\u53ef\u8fc1\u79fb\u7684\u8868\u5f81\uff0c\u4f46\u65b0\u8bc1\u636e\u8868\u660e\u4e0d\u540c\u4efb\u52a1\u4f18\u5316\u7684\u6a21\u578b\u53ef\u80fd\u53d1\u5c55\u51fa\u76f8\u4f3c\u7684\u8868\u5f81\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u5728BabyAI\u5e73\u53f0\u4e0a\u8bad\u7ec3\u57fa\u4e8etransformer\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u6267\u884c\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u4ee5\u54cd\u5e94\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u751f\u6210\u4ec5\u7531\u611f\u89c9\u8fd0\u52a8\u63a7\u5236\u9700\u6c42\u5851\u9020\u7684\u52a8\u4f5c\u57fa\u7840\u8bed\u8a00\u5d4c\u5165\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u8868\u5f81\u4e0e\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLaMA\u3001Qwen\u3001DeepSeek\u3001BERT\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08CLIP\u3001BLIP\uff09\u7684\u8868\u5f81\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5c3d\u7ba1\u8bad\u7ec3\u6570\u636e\u3001\u6a21\u6001\u548c\u76ee\u6807\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u89c2\u5bdf\u5230\u7a33\u5065\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u52a8\u4f5c\u8868\u5f81\u4e0e\u4ec5\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b\u548cBLIP\u5bf9\u9f50\u5f3a\u70c8\uff08precision@15: 0.70-0.73\uff09\uff0c\u63a5\u8fd1\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u5bf9\u9f50\u6c34\u5e73\u3002\u4e0eCLIP\u548cBERT\u7684\u5bf9\u9f50\u663e\u8457\u8f83\u5f31\u3002", "conclusion": "\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u52a8\u4f5c\u8868\u5f81\u4f1a\u6536\u655b\u5230\u90e8\u5206\u5171\u4eab\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u652f\u6301\u6a21\u6001\u72ec\u7acb\u7684\u8bed\u4e49\u7ec4\u7ec7\uff0c\u5e76\u7a81\u663e\u4e86\u5177\u8eabAI\u7cfb\u7edf\u4e2d\u8de8\u9886\u57df\u8fc1\u79fb\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.22964", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22964", "abs": "https://arxiv.org/abs/2601.22964", "authors": ["Yufei He", "Juncheng Liu", "Zhiyuan Hu", "Yulin Chen", "Yue Liu", "Yuan Sui", "Yibo Li", "Nuo Chen", "Jun Hu", "Bryan Hooi", "Xinxing Xu", "Jiang Bian"], "title": "EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning", "comment": null, "summary": "Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician", "AI": {"tldr": "\u63d0\u51fa\u4e86Med-Inquire\u57fa\u51c6\u6d4b\u8bd5\u548cEvoClinician\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u6a21\u62df\u771f\u5b9e\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u591a\u8f6e\u4fe1\u606f\u6536\u96c6\u8fc7\u7a0b\uff0c\u76f8\u6bd4\u4f20\u7edf\"\u4e00\u6b21\u6027\"\u533b\u7597AI\u66f4\u8d34\u8fd1\u5b9e\u9645\u8bca\u65ad\u6d41\u7a0b\u3002", "motivation": "\u5f53\u524d\u533b\u7597AI\u91c7\u7528\u4e0d\u73b0\u5b9e\u7684\"\u4e00\u6b21\u6027\"\u8bca\u65ad\u6a21\u5f0f\uff0c\u76f4\u63a5\u4ece\u5b8c\u6574\u75c5\u5386\u4e2d\u8bca\u65ad\u3002\u4f46\u771f\u5b9e\u4e34\u5e8a\u8bca\u65ad\u662f\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u533b\u751f\u9700\u8981\u987a\u5e8f\u63d0\u95ee\u548c\u5b89\u6392\u68c0\u67e5\uff0c\u5728\u7ba1\u7406\u6210\u672c\u548c\u65f6\u95f4\u7684\u540c\u65f6\u7b56\u7565\u6027\u5730\u6536\u96c6\u4fe1\u606f\u3002", "method": "\u9996\u5148\u63d0\u51faMed-Inquire\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e34\u5e8a\u75c5\u4f8b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e13\u95e8\u7684Patient\u548cExamination\u667a\u80fd\u4f53\u9690\u85cf\u5b8c\u6574\u75c5\u5386\uff0c\u8feb\u4f7f\u8bca\u65ad\u667a\u80fd\u4f53\u4e3b\u52a8\u63d0\u95ee\u548c\u5b89\u6392\u68c0\u67e5\u3002\u7136\u540e\u63d0\u51faEvoClinician\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\uff0c\u91c7\u7528\"\u8bca\u65ad-\u8bc4\u5206-\u8fdb\u5316\"\u5faa\u73af\uff1aActor\u667a\u80fd\u4f53\u5c1d\u8bd5\u8bca\u65ad\uff1bProcess Grader\u667a\u80fd\u4f53\u8bc4\u4f30\u6bcf\u4e2a\u884c\u52a8\u7684\u4e34\u5e8a\u4ef7\u503c\u548c\u8d44\u6e90\u6548\u7387\uff1bEvolver\u667a\u80fd\u4f53\u4f7f\u7528\u53cd\u9988\u66f4\u65b0Actor\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEvoClinician\u4f18\u4e8e\u6301\u7eed\u5b66\u4e60\u57fa\u7ebf\u548c\u5176\u4ed6\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\uff08\u5982\u8bb0\u5fc6\u667a\u80fd\u4f53\uff09\u3002", "conclusion": "Med-Inquire\u57fa\u51c6\u6d4b\u8bd5\u548cEvoClinician\u667a\u80fd\u4f53\u4e3a\u533b\u7597AI\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u771f\u5b9e\u4e34\u5e8a\u8bca\u65ad\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6a21\u62df\u591a\u8f6e\u4fe1\u606f\u6536\u96c6\u7684\u8bca\u65ad\u8fc7\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2601.22975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22975", "abs": "https://arxiv.org/abs/2601.22975", "authors": ["Ximing Lu", "David Acuna", "Jaehun Jung", "Jian Hu", "Di Zhang", "Shizhe Diao", "Yunheng Zou", "Shaokun Zhang", "Brandon Cui", "Mingjie Liu", "Hyunwoo Kim", "Prithviraj Ammanabrolu", "Jan Kautz", "Yi Dong", "Yejin Choi"], "title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.", "AI": {"tldr": "\u63d0\u51faGolden Goose\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e0d\u53ef\u9a8c\u8bc1\u7684\u4e92\u8054\u7f51\u6587\u672c\u8f6c\u5316\u4e3a\u591a\u9879\u9009\u62e9\u95ee\u7b54\u4efb\u52a1\uff0c\u5408\u6210\u65e0\u9650\u89c4\u6a21\u7684RLVR\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RLVR\u6570\u636e\u6709\u9650\u5bfc\u81f4\u7684\u8bad\u7ec3\u9971\u548c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u53d7\u9650\u4e8e\u53ef\u9a8c\u8bc1\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u957f\u65f6\u95f4\u8bad\u7ec3\u540e\u6027\u80fd\u63d0\u5347\u9971\u548c\u3002\u9700\u8981\u5229\u7528\u4e30\u5bcc\u7684\u3001\u4e0d\u53ef\u9a8c\u8bc1\u7684\u4e92\u8054\u7f51\u6587\u672c\u8d44\u6e90\u6765\u6269\u5c55RLVR\u6570\u636e\u89c4\u6a21\u3002", "method": "\u63d0\u51faGolden Goose\u65b9\u6cd5\uff1a1) \u4ece\u4e0d\u53ef\u9a8c\u8bc1\u7684\u4e92\u8054\u7f51\u6587\u672c\u4e2d\u8bc6\u522b\u5e76\u63a9\u7801\u5173\u952e\u63a8\u7406\u6b65\u9aa4\uff1b2) \u751f\u6210\u591a\u6837\u5316\u7684\u5e72\u6270\u9009\u9879\uff1b3) \u6784\u5efa\u591a\u9879\u9009\u62e9\u95ee\u7b54\u7248\u672c\u7684\u586b\u7a7a\u4efb\u52a1\uff0c\u4ece\u800c\u5408\u6210RLVR\u6570\u636e\u96c6\u3002", "result": "1) \u6784\u5efa\u4e86\u5305\u542b70\u4e07\u4e2a\u4efb\u52a1\u7684GooseReason\u6570\u636e\u96c6\uff1b2) \u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e3a1.5B\u548c4B\u6a21\u578b\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u7ed3\u679c\uff1b3) \u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u6784\u5efa\u4e86GooseReason-Cyber\u6570\u636e\u96c6\uff0c\u4f7fQwen3-4B-Instruct\u8d85\u8d8a\u4e86\u7ecf\u8fc7\u5927\u91cf\u9886\u57df\u9884\u8bad\u7ec3\u76847B\u6a21\u578b\u3002", "conclusion": "Golden Goose\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u4e30\u5bcc\u7684\u3001\u4e0d\u53ef\u9a8c\u8bc1\u7684\u4e92\u8054\u7f51\u6587\u672c\u81ea\u52a8\u6269\u5c55RLVR\u6570\u636e\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u5e26\u6765\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22984", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22984", "abs": "https://arxiv.org/abs/2601.22984", "authors": ["Yuhao Zhan", "Tianyu Fan", "Linxuan Huang", "Zirui Guo", "Chao Huang"], "title": "Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory", "comment": null, "summary": "Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fc7\u7a0b\u611f\u77e5\u7684\u8bc4\u4f30\u6846\u67b6PIES Taxonomy\uff0c\u7528\u4e8e\u8bca\u65ad\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u521b\u5efa\u4e86DeepHalluBench\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u7aef\u5230\u7aef\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u63ed\u793a\u7814\u7a76\u8f68\u8ff9\u4e2d\u5173\u952e\u7684\u4e2d\u95f4\u5e7b\u89c9\uff08\u5982\u9519\u8bef\u89c4\u5212\uff09\uff0c\u9700\u8981\u4ece\u7ed3\u679c\u5bfc\u5411\u8f6c\u5411\u8fc7\u7a0b\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPIES Taxonomy\u5c06\u5e7b\u89c9\u6309\u529f\u80fd\u7ec4\u4ef6\uff08\u89c4\u5212vs\u603b\u7ed3\uff09\u548c\u9519\u8bef\u5c5e\u6027\uff08\u663e\u5f0fvs\u9690\u5f0f\uff09\u5206\u7c7b\uff0c\u5efa\u7acb\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\u5206\u89e3\u7814\u7a76\u8f68\u8ff9\uff0c\u5e76\u521b\u5efa\u5305\u542b100\u4e2a\u4efb\u52a1\u7684DeepHalluBench\u57fa\u51c6\u3002", "result": "\u5bf96\u4e2a\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u6ca1\u6709\u7cfb\u7edf\u80fd\u8fbe\u5230\u7a33\u5065\u7684\u53ef\u9760\u6027\uff0c\u8bca\u65ad\u5206\u6790\u63ed\u793a\u4e86\u5e7b\u89c9\u4f20\u64ad\u548c\u8ba4\u77e5\u504f\u89c1\u7b49\u7cfb\u7edf\u6027\u7f3a\u9677\u3002", "conclusion": "\u8fc7\u7a0b\u611f\u77e5\u8bc4\u4f30\u80fd\u6709\u6548\u8bca\u65ad\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5931\u8d25\u673a\u5236\uff0c\u4e3a\u672a\u6765\u67b6\u6784\u4f18\u5316\u63d0\u4f9b\u57fa\u7840\u6027\u89c1\u89e3\uff0c\u5e7b\u89c9\u4f20\u64ad\u548c\u8ba4\u77e5\u504f\u89c1\u662f\u9700\u8981\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.22928", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22928", "abs": "https://arxiv.org/abs/2601.22928", "authors": ["Alhassan Abdelhalim", "Janick Edinger", "S\u00f6ren Laue", "Michaela Regneri"], "title": "LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models", "comment": null, "summary": "Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.\n  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u4e24\u79cd\u6d41\u884c\u7684LLM\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u6ce8\u610f\u529b\u5934\u63a2\u6d4b\u548c\u5d4c\u5165\u7279\u5f81\u6620\u5c04\uff09\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u65e0\u6cd5\u53ef\u9760\u63ed\u793aLLM\u7684\u8bed\u4e49\u7406\u89e3\u673a\u5236\uff0c\u8fd9\u5bf9\u4f9d\u8d56\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u8c03\u8bd5\u548c\u89e3\u91ca\u7684\u666e\u9002\u8ba1\u7b97\u7cfb\u7edf\u6784\u6210\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u666e\u9002\u8ba1\u7b97\u4e2d\u5e7f\u6cdb\u5e94\u7528\u4e14\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u5176\u5353\u8d8a\u8868\u73b0\u7684\u5185\u5728\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u672c\u8eab\u4e5f\u5b58\u5728\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5426\u80fd\u771f\u6b63\u63ed\u793aLLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u6587\u732e\u4e2d\u6210\u719f\u7684\u65b9\u6cd5\uff1a(1) \u57fa\u4e8e\u6ce8\u610f\u529b\u5934\u7684token\u7ea7\u5173\u7cfb\u7ed3\u6784\u63a2\u6d4b\uff1b(2) \u4f7f\u7528\u5d4c\u5165\u4f5c\u4e3a\u4eba\u7c7b\u53ef\u89e3\u91ca\u5c5e\u6027\u8f7d\u4f53\u7684\u7279\u5f81\u6620\u5c04\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5747\u5931\u8d25\uff1a\u6ce8\u610f\u529b\u89e3\u91ca\u5728\u6d4b\u8bd5\"\u6df1\u5c42\u8868\u793a\u4ecd\u5bf9\u5e94token\"\u7684\u6838\u5fc3\u5047\u8bbe\u65f6\u5d29\u6e83\uff1b\u5d4c\u5165\u5c5e\u6027\u63a8\u65ad\u65b9\u6cd5\u7684\u9ad8\u9884\u6d4b\u5206\u6570\u7531\u65b9\u6cd5\u5b66\u4f2a\u5f71\u548c\u6570\u636e\u96c6\u7ed3\u6784\u9a71\u52a8\uff0c\u800c\u975e\u6709\u610f\u4e49\u7684\u8bed\u4e49\u77e5\u8bc6\u3002", "conclusion": "\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u6027\uff0c\u4e0d\u80fd\u4f5c\u4e3aLLM\u7406\u89e3\u80fd\u529b\u7684\u53ef\u9760\u8bc1\u636e\u3002\u8fd9\u5728\u4f9d\u8d56\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u8fdb\u884c\u8c03\u8bd5\u3001\u538b\u7f29\u548c\u89e3\u91ca\u7684\u666e\u9002\u5206\u5e03\u5f0f\u8ba1\u7b97\u73af\u5883\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2601.23032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23032", "abs": "https://arxiv.org/abs/2601.23032", "authors": ["Siyu Gong", "Linan Yue", "Weibo Gao", "Fangzhou Yao", "Shimin Di", "Lei Feng", "Min-Ling Zhang"], "title": "Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning", "comment": null, "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.", "AI": {"tldr": "AutoTraj\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u590d\u548c\u5956\u52b1\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u6765\u81ea\u52a8\u5b66\u4e60\u5de5\u5177\u96c6\u6210\u63a8\u7406(TIR)\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u5408\u6210\u8f68\u8ff9\u548c\u7a00\u758f\u7ed3\u679c\u5956\u52b1\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u96c6\u6210\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u5408\u6210\u8f68\u8ff9\u548c\u7a00\u758f\u7ed3\u679c\u5956\u52b1\uff0c\u63d0\u4f9b\u7684\u76d1\u7763\u6709\u9650\u4e14\u6709\u504f\u5dee\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u53ef\u9760\u7684TIR\u884c\u4e3a\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) SFT\u9636\u6bb5\uff1a\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u751f\u6210\u591a\u4e2a\u5019\u9009\u8f68\u8ff9\uff0c\u8bc4\u4f30\u540e\u4fdd\u7559\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u4f4e\u8d28\u91cf\u8f68\u8ff9\u7528LLM\u4fee\u590d\uff1b2) RL\u9636\u6bb5\uff1a\u57fa\u4e8e\u504f\u597d\u6570\u636e\u96c6\u8bad\u7ec3\u8f68\u8ff9\u7ea7\u5956\u52b1\u6a21\u578b\uff0c\u7ed3\u5408\u7ed3\u679c\u548c\u683c\u5f0f\u5956\u52b1\u6307\u5bfc\u4f18\u5316\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u4e86AutoTraj\u5728\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "AutoTraj\u901a\u8fc7\u81ea\u52a8\u4fee\u590d\u548c\u5956\u52b1\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u53ef\u9760\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23133", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23133", "abs": "https://arxiv.org/abs/2601.23133", "authors": ["Edward Y. Chang", "Longling Geng"], "title": "RAudit: A Blind Auditing Protocol for Large Language Model Reasoning", "comment": "24 pages, 21 tables, 3 figures", "summary": "Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support conclusions, enabling detection of trace-output inconsistency and, when latent competence exists, its recovery. RAudit measures process quality via CRIT-based reasonableness scores and varies critique formulation to study how social framing affects model response. We prove bounded correction and $O(\\log(1/\u03b5))$ termination. Experiments on mathematical reasoning (CAP-GSM8K) and causal judgment (CausalL2) reveal four mechanisms explaining model unreliability: (1) Latent Competence Suppression, where models derive correct answers then overwrite them under social pressure; (2) The False Competence Trap, where weaker judges mask sycophancy that stronger judges expose; (3) The Complexity-Vulnerability Tradeoff, where causal tasks induce more than 10 times higher sycophancy than mathematical tasks; and (4) Iatrogenic Critique, where authoritative correction harms weaker models. These findings challenge assumptions that capability implies robustness and that stronger feedback yields better outputs.", "AI": {"tldr": "RAudit\u662f\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u5ba1\u8ba1LLM\u63a8\u7406\u7684\u8bca\u65ad\u534f\u8bae\uff0c\u901a\u8fc7\u8bc4\u4f30\u63a8\u5bfc\u6b65\u9aa4\u662f\u5426\u652f\u6301\u7ed3\u8bba\u6765\u68c0\u6d4b\u63a8\u7406\u4e0d\u4e00\u81f4\u6027\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u75c5\u7406\u7684\u56db\u79cd\u673a\u5236\u3002", "motivation": "\u63a8\u7406\u65f6\u7684\u7f29\u653e\u4f1a\u653e\u5927\u63a8\u7406\u75c5\u7406\uff08\u5982\u5949\u627f\u3001\u5c42\u7ea7\u584c\u9677\u3001\u8fc7\u65e9\u786e\u5b9a\u6027\uff09\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u7684\u5ba1\u8ba1\u65b9\u6cd5\u6765\u8bca\u65adLLM\u63a8\u7406\u95ee\u9898\u3002", "method": "\u63d0\u51faRAudit\u8bca\u65ad\u534f\u8bae\uff0c\u57fa\u4e8e\u76f2\u5ba1\u539f\u5219\u53ea\u8bc4\u4f30\u63a8\u5bfc\u6b65\u9aa4\u662f\u5426\u652f\u6301\u7ed3\u8bba\uff0c\u4f7f\u7528CRIT-based\u5408\u7406\u6027\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u6279\u5224\u8868\u8ff0\u7814\u7a76\u793e\u4f1a\u6846\u67b6\u5bf9\u6a21\u578b\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u5728\u6570\u5b66\u63a8\u7406\uff08CAP-GSM8K\uff09\u548c\u56e0\u679c\u5224\u65ad\uff08CausalL2\uff09\u4efb\u52a1\u4e0a\u63ed\u793a\u4e86\u56db\u79cd\u673a\u5236\uff1a\u6f5c\u5728\u80fd\u529b\u6291\u5236\u3001\u865a\u5047\u80fd\u529b\u9677\u9631\u3001\u590d\u6742\u5ea6-\u8106\u5f31\u6027\u6743\u8861\u3001\u533b\u6e90\u6027\u6279\u5224\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u80fd\u529b\u5e76\u4e0d\u7b49\u540c\u4e8e\u9c81\u68d2\u6027\uff0c\u66f4\u5f3a\u7684\u53cd\u9988\u4e0d\u4e00\u5b9a\u4ea7\u751f\u66f4\u597d\u7684\u8f93\u51fa\uff0c\u6311\u6218\u4e86\u73b0\u6709\u5047\u8bbe\u3002", "topic": "agent analysis"}}
{"id": "2601.23143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23143", "abs": "https://arxiv.org/abs/2601.23143", "authors": ["Seanie Lee", "Sangwoo Park", "Yumin Choi", "Gyeongman Kim", "Minki Kang", "Jihun Yun", "Dongmin Park", "Jongho Park", "Sung Ju Hwang"], "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models", "comment": "17 pages, 13 figures", "summary": "Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.", "AI": {"tldr": "ThinkSafe\u662f\u4e00\u79cd\u81ea\u751f\u6210\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u62d2\u7edd\u5f15\u5bfc\u8ba9\u6a21\u578b\u751f\u6210\u5b89\u5168\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u5728\u8fd9\u4e9b\u81ea\u751f\u6210\u54cd\u5e94\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6062\u590d\u5b89\u5168\u5bf9\u9f50\u540c\u65f6\u6700\u5c0f\u5316\u5206\u5e03\u504f\u79fb\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u4f18\u5316\u65f6\uff0c\u5f80\u5f80\u8fc7\u5ea6\u8ffd\u6c42\u5408\u89c4\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u6709\u5bb3\u63d0\u793a\u7684\u653b\u51fb\uff0c\u5b89\u5168\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6559\u5e08\u84b8\u998f\uff0c\u4f46\u8fd9\u4f1a\u5f15\u5165\u5206\u5e03\u5dee\u5f02\uff0c\u635f\u5bb3\u539f\u751f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faThinkSafe\u6846\u67b6\uff0c\u6838\u5fc3\u6d1e\u5bdf\u662f\uff1a\u867d\u7136\u5408\u89c4\u6027\u4f1a\u6291\u5236\u5b89\u5168\u673a\u5236\uff0c\u4f46\u6a21\u578b\u901a\u5e38\u4fdd\u7559\u8bc6\u522b\u5371\u5bb3\u7684\u6f5c\u5728\u77e5\u8bc6\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u62d2\u7edd\u5f15\u5bfc\u89e3\u9501\u8fd9\u79cd\u77e5\u8bc6\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u7b26\u5408\u5206\u5e03\u7684\u5b89\u5168\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u5728\u8fd9\u4e9b\u81ea\u751f\u6210\u54cd\u5e94\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728DeepSeek-R1-Distill\u548cQwen3\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cThinkSafe\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u80fd\u529b\u3002\u4e0eGRPO\u76f8\u6bd4\uff0cThinkSafe\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5b89\u5168\u6027\u548c\u76f8\u5f53\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "ThinkSafe\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6559\u5e08\u7684\u81ea\u751f\u6210\u5bf9\u9f50\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6062\u590d\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "topic": "agent analysis"}}
{"id": "2601.22382", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22382", "abs": "https://arxiv.org/abs/2601.22382", "authors": ["Natalie Maus", "Yimeng Zeng", "Haydn Thomas Jones", "Yining Huang", "Gaurav Ng Goel", "Alden Rose", "Kyurae Kim", "Hyun-Su Lee", "Marcelo Der Torossian Torres", "Fangping Wan", "Cesar de la Fuente-Nunez", "Mark Yatskar", "Osbert Bastani", "Jacob R. Gardner"], "title": "Purely Agentic Black-Box Optimization for Biological Design", "comment": null, "summary": "Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.", "AI": {"tldr": "PABLO\uff1a\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8bed\u8a00\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u7269\u9ed1\u76d2\u4f18\u5316\uff0c\u5728\u5206\u5b50\u8bbe\u8ba1\u548c\u6297\u83cc\u80bd\u4f18\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u751f\u7269\u8bbe\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u539f\u59cb\u7ed3\u6784\u6570\u636e\uff0c\u96be\u4ee5\u5229\u7528\u4e30\u5bcc\u7684\u79d1\u5b66\u6587\u732e\u3002\u867d\u7136LLMs\u5df2\u88ab\u5f15\u5165\uff0c\u4f46\u4ec5\u9650\u4e8e\u7ed3\u6784\u4e2d\u5fc3\u4f18\u5316\u5668\u4e2d\u7684\u72ed\u7a84\u89d2\u8272\u3002\u9700\u8981\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8bed\u8a00\u63a8\u7406\u65b9\u6cd5\u3002", "method": "PABLO\uff08Purely Agentic BLack-box Optimization\uff09\uff1a\u5206\u5c42\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f7f\u7528\u5728\u5316\u5b66\u548c\u751f\u7269\u5b66\u6587\u732e\u4e0a\u9884\u8bad\u7ec3\u7684\u79d1\u5b66LLMs\u6765\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\u751f\u7269\u5019\u9009\u7269\u3002", "result": "\u5728GuacaMol\u5206\u5b50\u8bbe\u8ba1\u548c\u6297\u83cc\u80bd\u4f18\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u76ee\u6807\u503c\u3002\u4f53\u5916\u9a8c\u8bc1\u663e\u793a\u4f18\u5316\u7684\u80bd\u5bf9\u8010\u836f\u75c5\u539f\u4f53\u5177\u6709\u5f3a\u6d3b\u6027\u3002", "conclusion": "\u667a\u80fd\u4f53\u5316\u65b9\u6cd5\u4e3a\u5b9e\u9645\u8bbe\u8ba1\u63d0\u4f9b\u5173\u952e\u4f18\u52bf\uff1a\u81ea\u7136\u6574\u5408\u8bed\u4e49\u4efb\u52a1\u63cf\u8ff0\u3001\u68c0\u7d22\u589e\u5f3a\u7684\u9886\u57df\u77e5\u8bc6\u548c\u590d\u6742\u7ea6\u675f\uff0c\u5728\u6cbb\u7597\u53d1\u73b0\u4e2d\u5177\u6709\u5b9e\u9645\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.22397", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22397", "abs": "https://arxiv.org/abs/2601.22397", "authors": ["Jianchang Su", "Yifan Zhang", "Shengkai Lin", "Shizhen Zhao", "Yusheng Zheng", "Yiwei Yang", "Wei Zhang"], "title": "SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning", "comment": null, "summary": "Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.", "AI": {"tldr": "SAIR\u662f\u4e00\u4e2a\u4f7f\u7528LLM\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u7684\u81ea\u52a8\u6269\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u9636\u6bb5ML\u63a8\u7406\u7ba1\u9053\uff0c\u65e0\u9700\u68af\u5ea6\u66f4\u65b0\u5373\u53ef\u5728\u7ebf\u6539\u8fdb\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u8d44\u6e90\u6210\u672c\u3002", "motivation": "\u591a\u9636\u6bb5ML\u63a8\u7406\u7ba1\u9053\u7531\u4e8e\u5f02\u6784\u8d44\u6e90\u3001\u8de8\u9636\u6bb5\u8026\u5408\u548c\u52a8\u6001\u74f6\u9888\u8fc1\u79fb\u800c\u96be\u4ee5\u81ea\u52a8\u6269\u7f29\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u4e14\u65e0\u9700\u79bb\u7ebf\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u5e15\u7d2f\u6258\u4f18\u52bf\u5956\u52b1\u5851\u9020\u4e0e\u53ef\u8bc1\u660e\u5206\u79bb\u8fb9\u754c\u3001\u57fa\u4e8e\u60ca\u5947\u5ea6\u7684\u7ecf\u9a8c\u68c0\u7d22\u4ee5\u63d0\u9ad8\u4e0a\u4e0b\u6587\u6548\u7387\uff0c\u4ee5\u53ca\u901a\u8fc7\u7528\u6237\u7a7a\u95f4CUDA\u62e6\u622a\u5b9e\u73b0\u7ec6\u7c92\u5ea6GPU\u901f\u7387\u63a7\u5236\u3002", "result": "\u5728\u56db\u79cdML\u670d\u52a1\u7ba1\u9053\u548c\u4e09\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\u4e0b\uff0cSAIR\u5b9e\u73b0\u4e86\u6700\u4f73\u6216\u5e76\u5217\u6700\u4f73\u7684P99\u5ef6\u8fdf\u548c\u6709\u6548\u8d44\u6e90\u6210\u672c\uff0cP99\u5ef6\u8fdf\u63d0\u5347\u9ad8\u8fbe50%\uff0c\u6709\u6548\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe97%\uff0c\u74f6\u9888\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523086%\uff0c\u4e14\u65e0\u9700\u79bb\u7ebf\u8bad\u7ec3\u3002", "conclusion": "SAIR\u901a\u8fc7LLM\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u9636\u6bb5ML\u63a8\u7406\u7ba1\u9053\u7684\u81ea\u52a8\u6269\u7f29\u95ee\u9898\uff0c\u5728\u5ef6\u8fdf\u3001\u6210\u672c\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u590d\u6742\u7684\u79bb\u7ebf\u8bad\u7ec3\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23228", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.23228", "abs": "https://arxiv.org/abs/2601.23228", "authors": ["Ed Li", "Junyu Ren", "Cat Yan"], "title": "Scaling Multiagent Systems with Process Rewards", "comment": null, "summary": "While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.", "AI": {"tldr": "MAPPA\u63d0\u51fa\u901a\u8fc7AI\u53cd\u9988\u7684\u9010\u52a8\u4f5c\u8fc7\u7a0b\u5956\u52b1\u6765\u5fae\u8c03\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u89e3\u51b3\u4fe1\u7528\u5206\u914d\u548c\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u5728\u6570\u5b66\u7ade\u8d5b\u548c\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5fae\u8c03\u591a\u4e2a\u667a\u80fd\u4f53\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u8de8\u667a\u80fd\u4f53\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff1b2\uff09\u6602\u8d35\u591a\u667a\u80fd\u4f53rollout\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faMAPPA\u65b9\u6cd5\uff0c\u901a\u8fc7AI\u53cd\u9988\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u52a8\u4f5c\u5206\u914d\u8fc7\u7a0b\u5956\u52b1\uff08\u800c\u975e\u4ec5\u5728\u4efb\u52a1\u5b8c\u6210\u65f6\uff09\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u76d1\u7763\u800c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff0c\u4ece\u6bcf\u6b21rollout\u4e2d\u63d0\u53d6\u6700\u5927\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "\u5728\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u4e0a\uff0cAIME\u63d0\u53475.0-17.5\u4e2a\u767e\u5206\u70b9\uff0cAMC\u63d0\u53477.8-17.2\u4e2a\u767e\u5206\u70b9\uff1b\u5728\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u534712.5\u4e2a\u767e\u5206\u70b9\uff0c\u8d28\u91cf\u6307\u6807\u63d0\u5347\u9ad8\u8fbe30%\u3002", "conclusion": "\u9010\u52a8\u4f5c\u76d1\u7763\u80fd\u5728\u4e0d\u540c\u9886\u57df\u6539\u5584\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6269\u5c55\u95ee\u9898\u8fc8\u51fa\u7b2c\u4e00\u6b65\uff0c\u51cf\u5c11\u4eba\u7c7b\u76d1\u7763\u9700\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2601.22432", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22432", "abs": "https://arxiv.org/abs/2601.22432", "authors": ["Wenzheng Zhang", "Karl Stratos"], "title": "ReNCE: Learning to Reason by Noise Contrastive Estimation", "comment": null, "summary": "GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eLLM\u63a8\u7406\u7684\u663e\u5f0f\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ed3\u679c\u5206\u4e3a\u6b63\u8d1f\u96c6\u5408\u5e76\u6700\u5927\u5316\u6b63\u7ed3\u679c\u6982\u7387\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "motivation": "GRPO\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u5176\u6539\u8fdb\uff08\u5982\u975e\u5bf9\u79f0\u88c1\u526a\u548c\u96f6\u65b9\u5dee\u6570\u636e\u8fc7\u6ee4\uff09\u9700\u8981\u5927\u91cf\u7ecf\u9a8c\u6d1e\u5bdf\u4e14\u96be\u4ee5\u8bc6\u522b\u3002\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u66f4\u7b80\u5355\u76f4\u63a5\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u663e\u5f0f\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff1a\u5c06K\u4e2a\u7ed3\u679c\u5206\u4e3a\u6b63\u8d1f\u96c6\u5408\uff0c\u7136\u540e\u6700\u5927\u5316\u6b63\u7ed3\u679c\u7684\u4f3c\u7136\u3002\u8be5\u65b9\u6cd5\u53ef\u89c6\u4e3aLLM\u63a8\u7406\u7684\u591a\u6807\u7b7e\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u7684\u5728\u7ebf\u5b9e\u4f8b\u5316\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0eDAPO\u548c\u5728\u7ebfDPO\u7b49\u5f3a\u57fa\u7ebf\u76f8\u6bd4\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u663e\u5f0f\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86GRPO\u65b9\u6cd5\u4e2d\u9700\u8981\u590d\u6742\u8c03\u6574\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23188", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.23188", "abs": "https://arxiv.org/abs/2601.23188", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Weijie Yu", "Jingxuan Yang", "Haolang Lu", "Jun Xu"], "title": "Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience", "comment": "11 pages, 3 figures", "summary": "Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.", "AI": {"tldr": "DS-MCM\u662f\u4e00\u4e2a\u589e\u5f3a\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u5143\u8ba4\u77e5\u76d1\u63a7\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u76d1\u63a7\u673a\u5236\uff08\u5feb\u901f\u4e00\u81f4\u6027\u76d1\u63a7\u548c\u6162\u901f\u7ecf\u9a8c\u9a71\u52a8\u76d1\u63a7\uff09\u6765\u68c0\u6d4b\u548c\u7ea0\u6b63\u63a8\u7406\u4e0e\u68c0\u7d22\u72b6\u6001\u7684\u4e0d\u4e00\u81f4\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u5728\u591a\u6b65\u68c0\u7d22\u548c\u957f\u65f6\u4efb\u52a1\u6267\u884c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u76d1\u63a7\u548c\u8c03\u8282\u63a8\u7406\u68c0\u7d22\u72b6\u6001\u7684\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u5bb9\u6613\u5931\u8d25\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u4e2d\u5206\u5c42\u5143\u8ba4\u77e5\u7684\u542f\u53d1\uff0c\u9700\u8981\u5c06\u5feb\u901f\u5f02\u5e38\u68c0\u6d4b\u4e0e\u7ecf\u9a8c\u9a71\u52a8\u7684\u53cd\u601d\u76f8\u7ed3\u5408\u3002", "method": "\u63d0\u51faDS-MCM\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5c42\u6b21\uff1a1\uff09\u5feb\u901f\u4e00\u81f4\u6027\u76d1\u63a7\u5668\uff1a\u8f7b\u91cf\u7ea7\u68c0\u67e5\u5916\u90e8\u8bc1\u636e\u4e0e\u5185\u90e8\u63a8\u7406\u7f6e\u4fe1\u5ea6\u7684\u5bf9\u9f50\uff1b2\uff09\u6162\u901f\u7ecf\u9a8c\u9a71\u52a8\u76d1\u63a7\u5668\uff1a\u9009\u62e9\u6027\u6fc0\u6d3b\uff0c\u57fa\u4e8e\u5386\u53f2\u4ee3\u7406\u8f68\u8ff9\u7684\u7ecf\u9a8c\u8bb0\u5fc6\u6307\u5bfc\u7ea0\u6b63\u5e72\u9884\u3002\u8be5\u673a\u5236\u76f4\u63a5\u5d4c\u5165\u63a8\u7406-\u68c0\u7d22\u5faa\u73af\u4e2d\uff0c\u51b3\u5b9a\u4f55\u65f6\u5e72\u9884\u4ee5\u53ca\u5982\u4f55\u57fa\u4e8e\u5148\u9a8c\u7ecf\u9a8c\u8fdb\u884c\u7ea0\u6b63\u3002", "result": "\u5728\u591a\u4e2a\u6df1\u5ea6\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDS-MCM\u80fd\u591f\u6301\u7eed\u63d0\u5347\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5206\u5c42\u5143\u8ba4\u77e5\u76d1\u63a7\u673a\u5236\u80fd\u591f\u6709\u6548\u589e\u5f3a\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u7ed3\u5408\u5feb\u901f\u68c0\u6d4b\u548c\u7ecf\u9a8c\u9a71\u52a8\u7684\u53cd\u601d\u6765\u6539\u5584\u4efb\u52a1\u6267\u884c\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2601.23265", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.23265", "abs": "https://arxiv.org/abs/2601.23265", "authors": ["Dawei Zhu", "Rui Meng", "Yale Song", "Xiyu Wei", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "PaperBanana: Automating Academic Illustration for AI Scientists", "comment": null, "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.", "AI": {"tldr": "PaperBanana\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e\u53d1\u8868\u7684\u5b66\u672f\u63d2\u56fe\uff0c\u901a\u8fc7VLMs\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u4ee3\u7406\u6765\u5b8c\u6210\u68c0\u7d22\u3001\u89c4\u5212\u3001\u6e32\u67d3\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3bAI\u79d1\u5b66\u5bb6\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u7814\u7a76\u6d41\u7a0b\u4e2d\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e\u53d1\u8868\u7684\u63d2\u56fe\u4ecd\u7136\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u74f6\u9888\u95ee\u9898\uff0c\u9700\u8981\u51cf\u8f7b\u7814\u7a76\u4eba\u5458\u7684\u8fd9\u4e00\u8d1f\u62c5\u3002", "method": "PaperBanana\u91c7\u7528\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff1a\u68c0\u7d22\u53c2\u8003\u6587\u732e\u3001\u89c4\u5212\u5185\u5bb9\u548c\u6837\u5f0f\u3001\u6e32\u67d3\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u6279\u5224\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u6846\u67b6\u57fa\u4e8e\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002", "result": "\u5728PaperBananaBench\uff08\u5305\u542b292\u4e2a\u4eceNeurIPS 2025\u51fa\u7248\u7269\u4e2d\u6574\u7406\u7684\u65b9\u6cd5\u8bba\u56fe\u8868\u6d4b\u8bd5\u6848\u4f8b\uff09\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cPaperBanana\u5728\u5fe0\u5b9e\u6027\u3001\u7b80\u6d01\u6027\u3001\u53ef\u8bfb\u6027\u548c\u7f8e\u5b66\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u9886\u5148\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u6709\u6548\u6269\u5c55\u5230\u9ad8\u8d28\u91cf\u7edf\u8ba1\u56fe\u7684\u751f\u6210\u3002", "conclusion": "PaperBanana\u4e3a\u81ea\u52a8\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e\u53d1\u8868\u7684\u63d2\u56fe\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u7814\u7a76\u6d41\u7a0b\u4e2d\u7684\u63d2\u56fe\u5236\u4f5c\u8d1f\u62c5\u3002", "topic": "code agent"}}
{"id": "2601.23273", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.23273", "abs": "https://arxiv.org/abs/2601.23273", "authors": ["Siran Peng", "Weisong Zhao", "Tianyu Fu", "Chenxu Zhao", "Tianshuo Zhang", "Haoyuan Zhang", "Xiangyu Zhu", "Minghui Wu", "Zhen Lei"], "title": "UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection", "comment": null, "summary": "Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.", "AI": {"tldr": "UPA\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u63d0\u793a\u4ee3\u7406\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u641c\u7d22\u548c\u9009\u62e9\u4f18\u5316\u63d0\u793a\uff0c\u65e0\u9700\u76d1\u7763\u53cd\u9988\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6bd4\u8f83\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u63d0\u793a\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4ee3\u7406\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u76d1\u7763\u5956\u52b1\u4fe1\u53f7\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5f80\u5f80\u96be\u4ee5\u83b7\u5f97\u3002\u9700\u8981\u5f00\u53d1\u65e0\u9700\u76d1\u7763\u53cd\u9988\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002", "method": "UPA\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u641c\u7d22\u9636\u6bb5\u6784\u5efa\u6f14\u5316\u6811\u7ed3\u6784\u5bfc\u822a\u63d0\u793a\u7a7a\u95f4\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u7ec6\u7c92\u5ea6\u3001\u987a\u5e8f\u4e0d\u53d8\u7684\u6210\u5bf9\u6bd4\u8f83\uff1b2) \u57fa\u4e8eBradley-Terry-Luce\u6a21\u578b\uff0c\u5148\u8fdb\u884c\u8def\u5f84\u8d1d\u53f6\u65af\u805a\u5408\u8fc7\u6ee4\u5019\u9009\uff0c\u518d\u8fdb\u884c\u5168\u5c40\u9526\u6807\u8d5b\u5f0f\u6bd4\u8f83\u63a8\u65ad\u6f5c\u5728\u63d0\u793a\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUPA\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u8bc1\u660e\u5373\u4f7f\u5728\u5b8c\u5168\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0c\u4ee3\u7406\u5f0f\u4f18\u5316\u4ecd\u7136\u975e\u5e38\u6709\u6548\u3002", "conclusion": "UPA\u5c55\u793a\u4e86\u65e0\u9700\u76d1\u7763\u53cd\u9988\u7684\u63d0\u793a\u4f18\u5316\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u641c\u7d22\u548c\u4e24\u9636\u6bb5\u9009\u62e9\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u63d0\u793a\u4f18\u5316\u3002", "topic": "agent analysis"}}
{"id": "2601.22448", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22448", "abs": "https://arxiv.org/abs/2601.22448", "authors": ["Weiqi Wang", "Xin Liu", "Binxuan Huang", "Hejie Cui", "Rongzhi Zhang", "Changlong Yu", "Shuowei Jin", "Jingfeng Yang", "Qingyu Yin", "Zhengyang Wang", "Zheng Li", "Yifan Gao", "Priyanka Nigam", "Bing Yin", "Lihong Li", "Yangqiu Song"], "title": "HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning", "comment": null, "summary": "RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.", "AI": {"tldr": "HeaPA\u662f\u4e00\u79cd\u9ad8\u6548\u8bad\u7ec3LLM\u63a8\u7406\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5806\u91c7\u6837\u548c\u5728\u7ebf\u67e5\u8be2\u589e\u5f3a\u6765\u4f18\u5316\u63d0\u793a\u6c60\u7ba1\u7406\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6027\u80fd", "motivation": "\u5f53\u524dRLVR\u8bad\u7ec3\u4e2d\uff0c\u63d0\u793a\u6c60\u901a\u5e38\u662f\u9759\u6001\u7684\u6216\u4e0e\u6a21\u578b\u5b66\u4e60\u8fdb\u5ea6\u677e\u6563\u5173\u8054\uff0c\u5747\u5300\u91c7\u6837\u65e0\u6cd5\u9002\u5e94\u80fd\u529b\u8fb9\u754c\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u5728\u5df2\u89e3\u51b3\u6216\u65e0\u6cd5\u89e3\u51b3\u7684\u63d0\u793a\u4e0a\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90", "method": "HeaPA\u7ef4\u62a4\u6709\u754c\u6f14\u5316\u6c60\uff0c\u4f7f\u7528\u5806\u91c7\u6837\u8ddf\u8e2a\u80fd\u529b\u8fb9\u754c\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5f02\u6b65\u9a8c\u8bc1\u8fdb\u884c\u5728\u7ebf\u67e5\u8be2\u589e\u5f3a\uff0c\u5e76\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7edf\u8ba1\u91cd\u4f30\u8ba1\u548c\u53d7\u63a7\u91cd\u63d2\u5165\u7a33\u5b9a\u76f8\u5173\u67e5\u8be2", "result": "\u5728\u4e24\u4e2a\u8bad\u7ec3\u8bed\u6599\u5e93\u3001\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u548c\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHeaPA\u6301\u7eed\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4ee5\u66f4\u5c11\u8ba1\u7b97\u8fbe\u5230\u76ee\u6807\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u9645\u8bad\u7ec3\u65f6\u95f4\u53ef\u6bd4", "conclusion": "HeaPA\u901a\u8fc7\u8fb9\u754c\u805a\u7126\u91c7\u6837\u548c\u5728\u7ebf\u6c60\u589e\u957f\u5b9e\u73b0\u6548\u7387\u63d0\u5347\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\u6536\u76ca\u8d8a\u660e\u663e\uff0c\u4e3aRLVR\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2601.22475", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22475", "abs": "https://arxiv.org/abs/2601.22475", "authors": ["Yuxuan Li", "Qijun He", "Mingqi Yuan", "Wen-Tse Chen", "Jeff Schneider", "Jiayu Chen"], "title": "Continual Policy Distillation from Distributed Reinforcement Learning Teachers", "comment": "19 pages (8 pages main text)", "summary": "Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.", "AI": {"tldr": "\u63d0\u51fa\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u5c06\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u89e3\u8026\u4e3a\u5206\u5e03\u5f0fRL\u8bad\u7ec3\u5355\u4efb\u52a1\u6559\u5e08\u6a21\u578b\u548c\u6301\u7eed\u84b8\u998f\u5230\u4e2d\u592e\u901a\u7528\u6a21\u578b\u4e24\u4e2a\u72ec\u7acb\u8fc7\u7a0b\uff0c\u7ed3\u5408MoE\u67b6\u6784\u548c\u56de\u653e\u673a\u5236\uff0c\u5728Meta-World\u57fa\u51c6\u4e0a\u5b9e\u73b0\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u6311\u6218\uff0c\u76f4\u63a5\u5bf9\u987a\u5e8f\u4efb\u52a1\u6d41\u5e94\u7528RL\u96be\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u80fd\u3002\u89c2\u5bdf\u5230RL\u64c5\u957f\u89e3\u51b3\u5355\u4efb\u52a1\uff0c\u800c\u7b56\u7565\u84b8\u998f\u4f5c\u4e3a\u76f8\u5bf9\u7a33\u5b9a\u7684\u76d1\u7763\u5b66\u4e60\u8fc7\u7a0b\uff0c\u66f4\u9002\u5408\u5927\u57fa\u7840\u6a21\u578b\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "method": "\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff1a1) \u5206\u5e03\u5f0fRL\u8bad\u7ec3\u5355\u4efb\u52a1\u6559\u5e08\u6a21\u578b\uff1b2) \u6301\u7eed\u84b8\u998f\u5230\u4e2d\u592e\u901a\u7528\u5b66\u751f\u6a21\u578b\uff1b3) \u91c7\u7528\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u589e\u5f3a\u53ef\u5851\u6027\uff1b4) \u4f7f\u7528\u56de\u653e\u673a\u5236\u589e\u5f3a\u7a33\u5b9a\u6027\u3002", "result": "\u5728Meta-World\u57fa\u51c6\u4e0a\uff0c\u6846\u67b6\u6062\u590d\u4e86\u8d85\u8fc785%\u7684\u6559\u5e08\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u4efb\u52a1\u95f4\u9057\u5fd8\u63a7\u5236\u572810%\u4ee5\u5185\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u5355\u4efb\u52a1RL\u8bad\u7ec3\u548c\u6301\u7eed\u7b56\u7565\u84b8\u998f\uff0c\u7ed3\u5408MoE\u548c\u56de\u653e\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u7ec8\u8eab\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23014", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.23014", "abs": "https://arxiv.org/abs/2601.23014", "authors": ["Yanwei Yue", "Guibin Zhang", "Boci Peng", "Xuanbo Fan", "Jiaxin Guo", "Qiankun Li", "Yan Zhang"], "title": "Mem-T: Densifying Rewards for Long-Horizon Memory Agents", "comment": null, "summary": "Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\\sim24.45\\%$ relative to GAM without sacrificing performance.", "AI": {"tldr": "Mem-T\u662f\u4e00\u4e2a\u81ea\u4e3b\u8bb0\u5fc6\u4ee3\u7406\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u8bb0\u5fc6\u6570\u636e\u5e93\u548c\u6811\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u6846\u67b6MoT-GRPO\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u6784\u5efa\u4e0e\u68c0\u7d22\u7684\u8054\u5408\u4f18\u5316\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u4ee3\u7406\u7684\u8bad\u7ec3\u8303\u5f0f\u5b58\u5728\u9650\u5236\uff1a\u4ee3\u7406\u9700\u8981\u5728\u7a00\u758f\u5ef6\u8fdf\u5956\u52b1\u4e0b\u6267\u884c\u957f\u5e8f\u5217\u8bb0\u5fc6\u64cd\u4f5c\uff0c\u96be\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u7aef\u5230\u7aef\u5185\u5b58\u7ba1\u7406\u7b56\u7565\u4f18\u5316\u3002", "method": "\u63d0\u51faMem-T\u81ea\u4e3b\u8bb0\u5fc6\u4ee3\u7406\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5c42\u6b21\u5316\u8bb0\u5fc6\u6570\u636e\u5e93\u5904\u7406\u6d41\u5f0f\u8f93\u5165\uff1b\u63d0\u51faMoT-GRPO\u6811\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bb0\u5fc6\u64cd\u4f5c\u6811\u53cd\u5411\u4f20\u64ad\u548c\u4e8b\u540e\u4fe1\u7528\u5206\u914d\u5c06\u7a00\u758f\u7ec8\u7aef\u53cd\u9988\u8f6c\u5316\u4e3a\u5bc6\u96c6\u7684\u9010\u6b65\u76d1\u7763\u3002", "result": "Mem-T\u6027\u80fd\u4f18\u4e8eA-Mem\u548cMem0\u7b49\u6846\u67b6\u8fbe14.92%\uff0c\u5728\u51c6\u786e\u7387-\u6548\u7387\u5e15\u7d2f\u6258\u524d\u6cbf\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4GAM\u51cf\u5c11\u7ea624.45%\u7684\u63a8\u7406token\u6d88\u8017\u4e14\u4e0d\u727a\u7272\u6027\u80fd\u3002", "conclusion": "Mem-T\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u7684\u5185\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e14\u7ecf\u6d4e\u7684\u81ea\u4e3b\u8bb0\u5fc6\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u8bb0\u5fc6\u64cd\u4f5c\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.22582", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22582", "abs": "https://arxiv.org/abs/2601.22582", "authors": ["Youngeun Kim"], "title": "MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning", "comment": null, "summary": "Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO", "AI": {"tldr": "\u63d0\u51faMC-GRPO\u65b9\u6cd5\uff0c\u7528\u4e2d\u4f4d\u6570\u57fa\u7ebf\u66ff\u4ee3\u5747\u503c\u57fa\u7ebf\uff0c\u89e3\u51b3\u5c0f\u6837\u672c\u8bad\u7ec3\u4e2d\u4f18\u52bf\u7b26\u53f7\u7ffb\u8f6c\u95ee\u9898\uff0c\u63d0\u5347\u4f4e\u6837\u672c\u91cf\u4e0b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u7cbe\u5ea6\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5c0f\u6837\u672c\u8bad\u7ec3\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e\u5747\u503c\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u56e0\u57fa\u7ebf\u566a\u58f0\u5bfc\u81f4\u4f18\u52bf\u7b26\u53f7\u7ffb\u8f6c\uff0c\u4f7f\u90e8\u5206\u6837\u672c\u66f4\u65b0\u65b9\u5411\u9519\u8bef\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e2d\u4f4d\u6570\u4e2d\u5fc3\u5316\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(MC-GRPO)\uff1a\u7528\u4e2d\u4f4d\u6570\u57fa\u7ebf\u66ff\u4ee3\u5747\u503c\u57fa\u7ebf\uff0c\u751f\u6210G+1\u4e2a\u6837\u672c\u7528\u4e8e\u4e2d\u4f4d\u6570\u53c2\u8003\uff0c\u6392\u9664\u4e2d\u4f4d\u6570\u6837\u672c\u7684\u68af\u5ea6\u56de\u4f20\uff0c\u4fdd\u6301\u4e0e\u6807\u51c6G\u6837\u672c\u8bad\u7ec3\u76f8\u540c\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u5404\u79cdGRPO\u7cfb\u5217\u65b9\u6cd5\u3001\u4e0d\u540c\u6a21\u578b\u548c\u89c4\u6a21\u4e0a\uff0c\u4e2d\u4f4d\u6570\u4e2d\u5fc3\u5316\u8bad\u7ec3\u5728\u4f4e\u6837\u672c\u91cf\u4e0b\u663e\u8457\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u7cbe\u5ea6\uff0c\u5c06G=2\u548cG=8\u4e4b\u95f4\u7684\u5dee\u8ddd\u7f29\u5c0f\u52301%\u4ee5\u5185\u3002", "conclusion": "MC-GRPO\u901a\u8fc7\u4e2d\u4f4d\u6570\u57fa\u7ebf\u6709\u6548\u7f13\u89e3\u5c0f\u6837\u672c\u8bad\u7ec3\u4e2d\u7684\u4f18\u52bf\u7b26\u53f7\u7ffb\u8f6c\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u4e0d\u53d8\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22642", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22642", "abs": "https://arxiv.org/abs/2601.22642", "authors": ["Chuxue Cao", "Jinluan Yang", "Haoran Li", "Kunhao Pan", "Zijian Zhao", "Zhengyu Chen", "Yuchen Tian", "Lijun Wu", "Conghui He", "Sirui Han", "Yike Guo"], "title": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification", "comment": null, "summary": "Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5f62\u5f0f\u903b\u8f91\u9a8c\u8bc1\u4e0eLLM\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u9a8c\u8bc1\u53cd\u9988\u7ea0\u6b63\u63a8\u7406\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u3001\u903b\u8f91\u7b49\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "motivation": "LLMs\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u57fa\u4e8e\u6982\u7387\u7684token\u9884\u6d4b\u5bb9\u6613\u4ea7\u751f\u903b\u8f91\u4e0d\u4e00\u81f4\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u800c\u5f62\u5f0f\u7b26\u53f7\u7cfb\u7edf\u53ef\u4ee5\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u3002\u9700\u8981\u6865\u63a5\u795e\u7ecf\u7f51\u7edc\u7684\u751f\u6210\u80fd\u529b\u4e0e\u5f62\u5f0f\u903b\u8f91\u7684\u4e25\u8c28\u6027\u3002", "method": "\u63d0\u51fa\u5f62\u5f0f\u903b\u8f91\u9a8c\u8bc1\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8fc7\u7a0b\u4e2d\u52a8\u6001\u4ea4\u7ec7\u5f62\u5f0f\u7b26\u53f7\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u68c0\u6d4b\u548c\u7ea0\u6b63\u9519\u8bef\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u5f62\u5f0f\u903b\u8f91\u9a8c\u8bc1\u5f15\u5bfc\u7684\u76d1\u7763\u5fae\u8c03\u548c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728\u516d\u4e2a\u6db5\u76d6\u6570\u5b66\u3001\u903b\u8f91\u548c\u4e00\u822c\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u548c14B\u6a21\u578b\u5206\u522b\u4ee5\u5e73\u574710.4%\u548c14.2%\u7684\u5e45\u5ea6\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5f62\u5f0f\u9a8c\u8bc1\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u673a\u5236\uff0c\u663e\u8457\u63a8\u52a8\u5148\u8fdbLLM\u63a8\u7406\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u6709\u6548\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u751f\u6210\u80fd\u529b\u4e0e\u5f62\u5f0f\u903b\u8f91\u7684\u4e25\u8c28\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.22690", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22690", "abs": "https://arxiv.org/abs/2601.22690", "authors": ["Huanyu Liu", "Ge Li", "Yihong Dong", "Sihan Wu", "Peixu Wang", "Sihao Cheng", "Taozhi Chen", "Kechi Zhang", "Hao Zhu", "Tongxuan Liu"], "title": "Do Transformers Have the Ability for Periodicity Generalization?", "comment": null, "summary": "Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5728\u5468\u671f\u6027OOD\u6cdb\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6784\u5efa\u4e86Coper\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5468\u671f\u6027\u6a21\u5f0f\u4f46\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u590d\u5408\u5468\u671f\u6027\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7814\u7a76\u5468\u671f\u6027\u8fd9\u4e00\u57fa\u672cOOD\u573a\u666f\u6765\u63a2\u7d22\u8fd9\u79cd\u5dee\u8ddd\u7684\u539f\u56e0\u3002", "method": "\u4ece\u62bd\u8c61\u4ee3\u6570\u548c\u63a8\u7406\u7684\u89d2\u5ea6\u7edf\u4e00\u89e3\u91ca\u5468\u671f\u6027\uff08\u5305\u62ec\u5355\u4e00\u548c\u590d\u5408\u5468\u671f\u6027\uff09\uff0c\u6784\u5efa\u4e86Coper\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542bHollow\u548cExtrapolation\u4e24\u79cdOOD\u8bbe\u7f6e\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30Transformer\u7684\u5468\u671f\u6027\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTransformer\u5728\u5468\u671f\u6027\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff1a\u6a21\u578b\u80fd\u5728\u8bad\u7ec3\u671f\u95f4\u8bb0\u5fc6\u5468\u671f\u6027\u6570\u636e\uff0c\u4f46\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u590d\u5408\u5468\u671f\u6027\u573a\u666f\u3002", "conclusion": "Transformer\u6a21\u578b\u5728\u5468\u671f\u6027OOD\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u672c\u8d28\u9650\u5236\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u63d0\u5347\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.22823", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22823", "abs": "https://arxiv.org/abs/2601.22823", "authors": ["Mathieu Petitbois", "R\u00e9my Portelas", "Sylvain Lamprier"], "title": "Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment", "comment": null, "summary": "We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStyle-Conditioned Implicit Q-Learning (SCIQL)\uff0c\u4e00\u79cd\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u8f68\u8ff9\u6807\u6ce8\u51fd\u6570\u5b9e\u73b0\u663e\u5f0f\u98ce\u683c\u76d1\u7763\uff0c\u6709\u6548\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u4e0e\u98ce\u683c\u5bf9\u9f50\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u98ce\u683c\u6761\u4ef6\u7b56\u7565\u7684\u8bad\u7ec3\u9762\u4e34\u5206\u5e03\u504f\u79fb\u548c\u98ce\u683c\u4e0e\u5956\u52b1\u56fa\u6709\u51b2\u7a81\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u534f\u8c03\u8fd9\u4e24\u4e2a\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u884c\u4e3a\u98ce\u683c\u5b9a\u4e49\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaSCIQL\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6RL\u6280\u672f\uff08\u5982\u540e\u89c1\u91cd\u6807\u6ce8\u548c\u4ef7\u503c\u5b66\u4e60\uff09\uff0c\u5f15\u5165\u95e8\u63a7\u4f18\u52bf\u52a0\u6743\u56de\u5f52\u673a\u5236\u4f18\u5316\u4efb\u52a1\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u98ce\u683c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSCIQL\u5728\u4efb\u52a1\u6027\u80fd\u548c\u98ce\u683c\u5bf9\u9f50\u4e24\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u79bb\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SCIQL\u901a\u8fc7\u7edf\u4e00\u98ce\u683c\u5b9a\u4e49\u548c\u95e8\u63a7\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u4e2d\u98ce\u683c\u4e0e\u4efb\u52a1\u6027\u80fd\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u98ce\u683c\u6761\u4ef6\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22801", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22801", "abs": "https://arxiv.org/abs/2601.22801", "authors": ["\u00d6mer Veysel \u00c7a\u011fatan", "Bar\u0131\u015f Akg\u00fcn", "G\u00f6zde G\u00fcl \u015eahin", "Xuandong Zhao"], "title": "Clipping-Free Policy Optimization for Large Language Models", "comment": "23 pages, 10 tables, 8 figures", "summary": "Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.", "AI": {"tldr": "CFPO\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u526a\u88c1\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u51f8\u4e8c\u6b21\u60e9\u7f5a\u66ff\u4ee3\u542f\u53d1\u5f0f\u526a\u88c1\uff0c\u89e3\u51b3\u5927\u89c4\u6a21LLM\u540e\u8bad\u7ec3\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4fdd\u6301\u7a33\u5b9a\u8bad\u7ec3\u540c\u65f6\u907f\u514d\u68af\u5ea6\u6d88\u5931\u548c\u5956\u52b1\u653b\u51fb\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5df2\u6210\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u6838\u5fc3\u6280\u672f\uff0c\u4f46\u4e3b\u6d41\u7b97\u6cd5\u4f9d\u8d56\u526a\u88c1\u673a\u5236\uff0c\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u5f15\u5165\u4e86\u4f18\u5316\u95ee\u9898\uff0c\u5305\u62ec\u96f6\u68af\u5ea6\u533a\u57df\u3001\u5956\u52b1\u653b\u51fb\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faClipping-Free Policy Optimization (CFPO)\uff0c\u7528\u57fa\u4e8e\u603b\u53d8\u5dee\u6563\u5ea6\u7ea6\u675f\u7684\u51f8\u4e8c\u6b21\u60e9\u7f5a\u66ff\u4ee3\u542f\u53d1\u5f0f\u526a\u88c1\uff0c\u751f\u6210\u5904\u5904\u53ef\u5fae\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5b9e\u73b0\u65e0\u786c\u8fb9\u754c\u7684\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cCFPO\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5339\u914d\u526a\u88c1\u65b9\u6cd5\u5e76\u6269\u5c55\u4e86\u7a33\u5b9a\u8bad\u7ec3\u8303\u56f4\uff1b\u5728\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0c\u7f13\u89e3\u4e86\u5197\u957f\u5229\u7528\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u80fd\u529b\u9000\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\u3002", "conclusion": "CFPO\u4ec5\u9700\u4e00\u884c\u4ee3\u7801\u66f4\u6539\u4e14\u65e0\u9700\u989d\u5916\u8d85\u53c2\u6570\uff0c\u662fLLM\u540e\u8bad\u7ec3\u4e2d\u66ff\u4ee3\u526a\u88c1\u65b9\u6cd5\u7684\u6709\u524d\u666f\u7684\u76f4\u63a5\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23010", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23010", "abs": "https://arxiv.org/abs/2601.23010", "authors": ["Xinchen Han", "Qiuyang Fang", "Hossam Afifi", "Michel Marot"], "title": "Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning", "comment": null, "summary": "Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.", "AI": {"tldr": "\u63d0\u51fa\u8fde\u7eed\u7ea6\u675f\u63d2\u503c\uff08CCI\uff09\u6846\u67b6\u7edf\u4e00\u79bb\u7ebfRL\u4e2d\u7684\u4e09\u79cd\u7ea6\u675f\u5bb6\u65cf\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u7ea6\u675f\u7b56\u7565\u4f18\u5316\uff08ACPO\uff09\u7b97\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u79bb\u7ebfRL\u65b9\u6cd5\u4f7f\u7528\u4e0d\u540c\u7684\u7ea6\u675f\u5f62\u5f0f\uff08\u52a0\u6743\u884c\u4e3a\u514b\u9686\u3001\u5bc6\u5ea6\u6b63\u5219\u5316\u3001\u652f\u6301\u7ea6\u675f\uff09\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u91ca\u5b83\u4eec\u7684\u8054\u7cfb\u548c\u6743\u8861", "method": "\u63d0\u51fa\u8fde\u7eed\u7ea6\u675f\u63d2\u503c\uff08CCI\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e2a\u63d2\u503c\u53c2\u6570\u5b9e\u73b0\u4e09\u79cd\u7ea6\u675f\u7c7b\u578b\u7684\u5e73\u6ed1\u8fc7\u6e21\u548c\u7ec4\u5408\uff1b\u57fa\u4e8eCCI\u5f00\u53d1\u81ea\u52a8\u7ea6\u675f\u7b56\u7565\u4f18\u5316\uff08ACPO\uff09\u7b97\u6cd5\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u66f4\u65b0\u81ea\u9002\u5e94\u8c03\u6574\u63d2\u503c\u53c2\u6570", "result": "\u5728D4RL\u548cNeoRL2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5b9e\u73b0\u4e86\u6574\u4f53\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "CCI\u6846\u67b6\u4e3a\u79bb\u7ebfRL\u4e2d\u7684\u7ea6\u675f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0cACPO\u7b97\u6cd5\u80fd\u591f\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6700\u4f18\u7ea6\u675f\u5f62\u5f0f\uff0c\u5728\u591a\u79cd\u9886\u57df\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0", "topic": "agentic reinforcement learning"}}
{"id": "2601.22891", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22891", "abs": "https://arxiv.org/abs/2601.22891", "authors": ["Jacques Cloete", "Mathias Jackermeier", "Ioannis Havoutis", "Alessandro Abate"], "title": "PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL", "comment": "11 pages, 3 figures (main paper). 14 pages, 10 figures (appendix)", "summary": "A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or \"symbols\"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.", "AI": {"tldr": "PlatoLTL\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u547d\u9898\u89c6\u4e3a\u53c2\u6570\u5316\u8c13\u8bcd\u800c\u975e\u79bb\u6563\u7b26\u53f7\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u547d\u9898\u548c\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u5728LTL\u89c4\u8303\u7ed3\u6784\u95f4\u6cdb\u5316\uff0c\u4f46\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u547d\u9898\u8bcd\u6c47\u8868\uff08\u63cf\u8ff0\u9ad8\u7ea7\u4e8b\u4ef6\u7684\u7b26\u53f7\uff09\u3002\u8fd9\u9650\u5236\u4e86\u667a\u80fd\u4f53\u5904\u7406\u65b0\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u5c06\u547d\u9898\u89c6\u4e3a\u53c2\u6570\u5316\u8c13\u8bcd\u7684\u5b9e\u4f8b\u800c\u975e\u79bb\u6563\u7b26\u53f7\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u67b6\u6784\u6765\u5d4c\u5165\u548c\u7ec4\u5408\u8c13\u8bcd\u4ee5\u8868\u793aLTL\u89c4\u8303\uff0c\u4f7f\u7b56\u7565\u80fd\u591f\u5b66\u4e60\u76f8\u5173\u547d\u9898\u95f4\u7684\u5171\u4eab\u7ed3\u6784\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u65b0\u9896\u547d\u9898\u548c\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4e0d\u4ec5\u80fd\u5728LTL\u516c\u5f0f\u7ed3\u6784\u4e0a\u7ec4\u5408\u6cdb\u5316\uff0c\u8fd8\u80fd\u5728\u547d\u9898\u4e0a\u53c2\u6570\u5316\u6cdb\u5316\u3002", "conclusion": "PlatoLTL\u901a\u8fc7\u53c2\u6570\u5316\u8c13\u8bcd\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u672a\u89c1\u547d\u9898\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23225", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23225", "abs": "https://arxiv.org/abs/2601.23225", "authors": ["Rajib Mostakim", "Reza T. Batley", "Sourav Saha"], "title": "Agile Reinforcement Learning through Separable Neural Architecture", "comment": null, "summary": "Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.\n  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.", "AI": {"tldr": "SPAN\u662f\u4e00\u79cd\u57fa\u4e8e\u6837\u6761\u7684\u81ea\u9002\u5e94\u7f51\u7edc\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9884\u5904\u7406\u5c42\u548c\u53ef\u5206\u79bb\u7684\u5f20\u91cf\u79efB\u6837\u6761\u57fa\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u6bd4MLP\u57fa\u7ebf\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u4f20\u7edf\u7684\u591a\u5c42\u611f\u77e5\u673a\u5b58\u5728\u53c2\u6570\u6548\u7387\u4f4e\u3001\u6837\u672c\u6548\u7387\u5dee\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u6837\u6761\u7f51\u7edc\uff08\u5982KANs\uff09\u867d\u7136\u53c2\u6570\u6548\u7387\u9ad8\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "SPAN\u57fa\u4e8e\u4f4e\u79e9KHRONOS\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u53ef\u5b66\u4e60\u7684\u9884\u5904\u7406\u5c42\u548c\u53ef\u5206\u79bb\u7684\u5f20\u91cf\u79efB\u6837\u6761\u57fa\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u65b0\u578b\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\u3002", "result": "\u5728\u79bb\u6563\uff08PPO\uff09\u548c\u9ad8\u7ef4\u8fde\u7eed\uff08SAC\uff09\u63a7\u5236\u4efb\u52a1\u4ee5\u53ca\u79bb\u7ebf\u8bbe\u7f6e\uff08Minari/D4RL\uff09\u4e2d\uff0cSPAN\u76f8\u6bd4MLP\u57fa\u7ebf\u5b9e\u73b0\u4e8630-50%\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\u548c1.3-9\u500d\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "SPAN\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u5bf9\u8d85\u53c2\u6570\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u662f\u5b66\u4e60\u5185\u5728\u9ad8\u6548\u7b56\u7565\u7684\u53ef\u884c\u9ad8\u6027\u80fd\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.23027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.23027", "abs": "https://arxiv.org/abs/2601.23027", "authors": ["Arvind Mahankali", "Kaiyue Wen", "Tengyu Ma"], "title": "Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning", "comment": "47 pages, 13 figures", "summary": "Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.", "AI": {"tldr": "\u63d0\u51faDC-CoT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u51cf\u5c11\u957f\u601d\u7ef4\u94fe\u7684\u5ef6\u8fdf\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u5c06\u6700\u957f\u8def\u5f84\u957f\u5ea6\u964d\u4f4e35-40%", "motivation": "\u957f\u601d\u7ef4\u94fe\u63a8\u7406\uff08Long CoT\uff09\u867d\u7136\u80fd\u63d0\u5347LLM\u5728\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4f46\u987a\u5e8f\u751f\u6210\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faDivide-and-Conquer CoT\uff08DC-CoT\uff09\uff1a1\uff09\u6a21\u578b\u4f5c\u4e3a\u5bfc\u6f14\u8bc6\u522b\u53ef\u5e76\u884c\u6267\u884c\u7684\u5b50\u4efb\u52a1\uff1b2\uff09\u751f\u6210\u5de5\u4f5c\u8282\u70b9\u6267\u884c\u8fd9\u4e9b\u5b50\u4efb\u52a1\uff1b3\uff09\u5148\u7528SFT\u521d\u59cb\u5316\u6a21\u578b\u751f\u6210\u5de5\u4f5c\u8282\u70b9\u7684\u80fd\u529b\uff1b4\uff09\u8bbe\u8ba1\u591a\u9636\u6bb5RL\u7b97\u6cd5\u6062\u590d\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u6700\u957f\u8def\u5f84\u957f\u5ea6\u3002", "result": "\u5728AIME 2024\u548cHMMT 2025\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDC-CoT\u5728\u4fdd\u6301\u4e0eDeepScaleR-1.5B-Preview\u76f8\u4f3c\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5c06\u6700\u957f\u8def\u5f84\u957f\u5ea6\u964d\u4f4e\u4e8635-40%\u3002", "conclusion": "DC-CoT\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u6709\u6548\u964d\u4f4e\u4e86\u957f\u601d\u7ef4\u94fe\u7684\u5ef6\u8fdf\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u4f4e\u5ef6\u8fdf\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.23135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.23135", "abs": "https://arxiv.org/abs/2601.23135", "authors": ["Cheng Ge", "Caitlyn Heqi Yin", "Hao Liang", "Jiawei Zhang"], "title": "Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients", "comment": null, "summary": "Reinforcement learning (RL) has become a key driver of language model reasoning. Among RL algorithms, Group Relative Policy Optimization (GRPO) is the de facto standard, avoiding the need for a critic by using per-prompt baselines and variance normalization. Yet why and when this normalization helps remains unclear. In this work, we provide an explanation through the lens of local curvature of the sequence-level policy gradient: standard deviation normalization implements an adaptive gradient. Theoretically, under mild conditions, GRPO enjoys a strictly improved convergence rate over unnormalized REINFORCE, with gains characterized by the average within-prompt reward standard deviation across prompts and iterations. Empirically, our analysis on GSM8K and MATH benchmarks reveals three distinct training phases governed by the interplay between feature orthogonality and reward variance: (I) an early acceleration phase where high variance and orthogonality favor adaptive scaling; (II) a relatively stable transition phase; and (III) a late-stage regime where the loss of orthogonality limits further gains. Together, these results provide a principled account of when std normalization helps in GRPO, and offer broader insights into the design of critic-free RL algorithms.", "AI": {"tldr": "\u672c\u6587\u4ece\u5e8f\u5217\u7ea7\u7b56\u7565\u68af\u5ea6\u7684\u5c40\u90e8\u66f2\u7387\u89d2\u5ea6\uff0c\u89e3\u91ca\u4e86GRPO\u4e2d\u6807\u51c6\u5dee\u5f52\u4e00\u5316\u5982\u4f55\u5b9e\u73b0\u81ea\u9002\u5e94\u68af\u5ea6\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86GRPO\u76f8\u6bd4\u672a\u5f52\u4e00\u5316\u7684REINFORCE\u5177\u6709\u66f4\u4f18\u7684\u6536\u655b\u901f\u7387\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4e09\u4e2a\u4e0d\u540c\u9636\u6bb5\u3002", "motivation": "GRPO\u5df2\u6210\u4e3a\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u6807\u51c6RL\u7b97\u6cd5\uff0c\u5b83\u901a\u8fc7\u6bcf\u4e2a\u63d0\u793a\u7684\u57fa\u7ebf\u548c\u65b9\u5dee\u5f52\u4e00\u5316\u907f\u514d\u4e86\u6279\u8bc4\u5668\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5f52\u4e00\u5316\u4e3a\u4f55\u4ee5\u53ca\u4f55\u65f6\u6709\u6548\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u89d2\u5ea6\u89e3\u91caGRPO\u4e2d\u6807\u51c6\u5dee\u5f52\u4e00\u5316\u7684\u4f5c\u7528\u673a\u5236\u3002", "method": "\u4ece\u5e8f\u5217\u7ea7\u7b56\u7565\u68af\u5ea6\u7684\u5c40\u90e8\u66f2\u7387\u89d2\u5ea6\u5206\u6790GRPO\uff0c\u7406\u8bba\u4e0a\u8bc1\u660eGRPO\u7684\u6536\u655b\u4f18\u52bf\uff0c\u5e76\u5728GSM8K\u548cMATH\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u7279\u5f81\u6b63\u4ea4\u6027\u548c\u5956\u52b1\u65b9\u5dee\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\uff0cGRPO\u76f8\u6bd4\u672a\u5f52\u4e00\u5316\u7684REINFORCE\u5177\u6709\u4e25\u683c\u6539\u8fdb\u7684\u6536\u655b\u901f\u7387\uff0c\u589e\u76ca\u7531\u8de8\u63d0\u793a\u548c\u8fed\u4ee3\u7684\u5e73\u5747\u7ec4\u5185\u5956\u52b1\u6807\u51c6\u5dee\u8868\u5f81\u3002\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u8bad\u7ec3\u9636\u6bb5\uff1a\u65e9\u671f\u52a0\u901f\u9636\u6bb5\uff08\u9ad8\u65b9\u5dee\u548c\u6b63\u4ea4\u6027\u6709\u5229\u4e8e\u81ea\u9002\u5e94\u7f29\u653e\uff09\u3001\u76f8\u5bf9\u7a33\u5b9a\u7684\u8fc7\u6e21\u9636\u6bb5\u3001\u4ee5\u53ca\u540e\u671f\u9636\u6bb5\uff08\u6b63\u4ea4\u6027\u635f\u5931\u9650\u5236\u4e86\u8fdb\u4e00\u6b65\u589e\u76ca\uff09\u3002", "conclusion": "\u672c\u6587\u4e3aGRPO\u4e2d\u6807\u51c6\u5dee\u5f52\u4e00\u5316\u4f55\u65f6\u6709\u6548\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u91ca\uff0c\u5e76\u4e3a\u65e0\u6279\u8bc4\u5668RL\u7b97\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u89c1\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.2e11122c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fmoltworker-self-hosted-ai-agent%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/xN7b9u3iPyByO4hzdAeJ9OblJict7wzG3nCQJmdtAAE=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fmoltworker-self-hosted-ai-agent%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/xN7b9u3iPyByO4hzdAeJ9OblJict7wzG3nCQJmdtAAE=442", "authors": ["TLDR Newsletter"], "title": "Introducing Moltworker: a self-hosted personal AI agent, minus the minis", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fmoltworker-self-hosted-ai-agent%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/xN7b9u3iPyByO4hzdAeJ9OblJict7wzG3nCQJmdtAAE=442", "summary": "Introducing Moltworker: a self-hosted personal AI agent, minus the minis (9 minute read) Cloudflare has open-sourced Moltworker, a middleware solution that allows users to run the self-hosted AI agent Moltbot on its Developer Platform, bypassing the need for dedicated local hardware. This proof-of-concept leverages Cloudflare's expanded Node.js compatibility\u2014with only 1.5% of top NPM packages failing to run\u2014alongside services like Sandboxes, Browser Rendering, R2 for storage, and AI Gateway f...", "source": "tldr", "AI": {"tldr": "Cloudflare\u5f00\u6e90Moltworker\u4e2d\u95f4\u4ef6\uff0c\u8ba9\u7528\u6237\u80fd\u5728\u5176\u5f00\u53d1\u8005\u5e73\u53f0\u4e0a\u81ea\u6258\u7ba1AI\u4ee3\u7406Moltbot\uff0c\u65e0\u9700\u4e13\u7528\u672c\u5730\u786c\u4ef6", "motivation": "\u89e3\u51b3\u7528\u6237\u8fd0\u884c\u81ea\u6258\u7ba1AI\u4ee3\u7406\u65f6\u9700\u8981\u4e13\u7528\u672c\u5730\u786c\u4ef6\u7684\u95ee\u9898\uff0c\u5229\u7528Cloudflare\u5e73\u53f0\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u90e8\u7f72\u65b9\u6848", "method": "\u5f00\u53d1\u4e2d\u95f4\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528Cloudflare\u6269\u5c55\u7684Node.js\u517c\u5bb9\u6027\uff08\u4ec51.5%\u9876\u7ea7NPM\u5305\u65e0\u6cd5\u8fd0\u884c\uff09\uff0c\u7ed3\u5408\u6c99\u76d2\u3001\u6d4f\u89c8\u5668\u6e32\u67d3\u3001R2\u5b58\u50a8\u548cAI\u7f51\u5173\u7b49\u670d\u52a1", "result": "\u521b\u5efa\u4e86\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u7528\u6237\u53ef\u5728Cloudflare\u5f00\u53d1\u8005\u5e73\u53f0\u4e0a\u8fd0\u884c\u81ea\u6258\u7ba1AI\u4ee3\u7406\uff0c\u65e0\u9700\u672c\u5730\u786c\u4ef6", "conclusion": "Moltworker\u5c55\u793a\u4e86\u5728\u4e91\u5e73\u53f0\u4e0a\u81ea\u6258\u7ba1AI\u4ee3\u7406\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u4fbf\u6377\u7684AI\u4ee3\u7406\u90e8\u7f72\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2601.630bc27e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Ftopics%2Fdevelopers-practitioners%2Fhow-google-sres-use-gemini-cli-to-solve-real-world-outages%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/hXv8KtUCzKr2xGY8z9-0Lu_w_DZEP-Xn0U2f5UGecsg=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Ftopics%2Fdevelopers-practitioners%2Fhow-google-sres-use-gemini-cli-to-solve-real-world-outages%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/hXv8KtUCzKr2xGY8z9-0Lu_w_DZEP-Xn0U2f5UGecsg=442", "authors": ["TLDR Newsletter"], "title": "How Google SREs Use Gemini CLI to Solve Real-World Outages", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Ftopics%2Fdevelopers-practitioners%2Fhow-google-sres-use-gemini-cli-to-solve-real-world-outages%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/hXv8KtUCzKr2xGY8z9-0Lu_w_DZEP-Xn0U2f5UGecsg=442", "summary": "How Google SREs Use Gemini CLI to Solve Real-World Outages (5 minute read) Google SREs use Gemini CLI and agentic AI to reduce incident mitigation time by classifying outages, executing safe mitigations, identifying root causes, generating fixes, and automating postmortems while keeping humans in control.", "source": "tldr", "AI": {"tldr": "Google SRE\u56e2\u961f\u4f7f\u7528Gemini CLI\u548c\u667a\u80fd\u4ee3\u7406AI\u6765\u51cf\u5c11\u4e8b\u6545\u7f13\u89e3\u65f6\u95f4\uff0c\u901a\u8fc7\u5206\u7c7b\u6545\u969c\u3001\u6267\u884c\u5b89\u5168\u7f13\u89e3\u63aa\u65bd\u3001\u8bc6\u522b\u6839\u672c\u539f\u56e0\u3001\u751f\u6210\u4fee\u590d\u65b9\u6848\u548c\u81ea\u52a8\u5316\u4e8b\u540e\u5206\u6790\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u5de5\u63a7\u5236\u3002", "motivation": "Google SRE\u56e2\u961f\u9762\u4e34\u5904\u7406\u751f\u4ea7\u73af\u5883\u4e8b\u6545\u7684\u6311\u6218\uff0c\u9700\u8981\u5feb\u901f\u54cd\u5e94\u548c\u7f13\u89e3\u6545\u969c\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u6548\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u5de5\u5177\u6765\u52a0\u901f\u4e8b\u6545\u5904\u7406\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528Gemini CLI\u548c\u667a\u80fd\u4ee3\u7406AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u7c7b\u6545\u969c\u7c7b\u578b\u3001\u6267\u884c\u5b89\u5168\u7f13\u89e3\u63aa\u65bd\u3001\u81ea\u52a8\u8bc6\u522b\u6839\u672c\u539f\u56e0\u3001\u751f\u6210\u4ee3\u7801\u4fee\u590d\u65b9\u6848\uff0c\u5e76\u81ea\u52a8\u5316\u4e8b\u540e\u5206\u6790\u6587\u6863\u7684\u521b\u5efa\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4e86\u4e8b\u6545\u7f13\u89e3\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e86SRE\u56e2\u961f\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4eba\u5de5\u76d1\u7763\u548c\u63a7\u5236\uff0c\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u3002", "conclusion": "Gemini CLI\u548c\u667a\u80fd\u4ee3\u7406AI\u662f\u6709\u6548\u7684\u5de5\u5177\uff0c\u80fd\u591f\u5e2e\u52a9SRE\u56e2\u961f\u66f4\u9ad8\u6548\u5730\u5904\u7406\u751f\u4ea7\u73af\u5883\u4e8b\u6545\uff0c\u5b9e\u73b0\u5feb\u901f\u54cd\u5e94\u548c\u7cfb\u7edf\u6062\u590d\u3002", "topic": "swe application"}}
{"id": "tldr.2601.3357c56c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pulumi.com%2Fblog%2Fpulumi-agent-skills%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/_C9yCxROjfFf2o1PyKwjWYCNdPHkTsnuvbg-viCvxUM=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pulumi.com%2Fblog%2Fpulumi-agent-skills%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/_C9yCxROjfFf2o1PyKwjWYCNdPHkTsnuvbg-viCvxUM=442", "authors": ["TLDR Newsletter"], "title": "Pulumi Agent Skills: Best practices and more for AI coding assistants", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pulumi.com%2Fblog%2Fpulumi-agent-skills%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/_C9yCxROjfFf2o1PyKwjWYCNdPHkTsnuvbg-viCvxUM=442", "summary": "Pulumi Agent Skills: Best practices and more for AI coding assistants (4 minute read) Pulumi Agent Skills is a new collection of structured knowledge packages that enables AI coding assistants to generate production-ready infrastructure code.", "source": "tldr", "AI": {"tldr": "Pulumi\u63a8\u51faAgent Skills\uff0c\u8fd9\u662f\u4e00\u5957\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\uff0c\u5e2e\u52a9AI\u7f16\u7801\u52a9\u624b\u751f\u6210\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801", "motivation": "\u5f53\u524dAI\u7f16\u7801\u52a9\u624b\u5728\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u65f6\u7f3a\u4e4f\u751f\u4ea7\u73af\u5883\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u4e0d\u9ad8\uff0c\u9700\u8981\u989d\u5916\u7684\u4eba\u5de5\u8c03\u6574", "method": "\u5f00\u53d1\u4e86\u4e00\u5957\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\uff08Agent Skills\uff09\uff0c\u5305\u542b\u6700\u4f73\u5b9e\u8df5\u3001\u6a21\u5f0f\u5e93\u548c\u884c\u4e1a\u6807\u51c6\uff0c\u901a\u8fc7API\u96c6\u6210\u5230AI\u7f16\u7801\u52a9\u624b\u4e2d", "result": "AI\u7f16\u7801\u52a9\u624b\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u7b26\u5408\u751f\u4ea7\u6807\u51c6\u7684\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u8c03\u6574\u7684\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387", "conclusion": "Pulumi Agent Skills\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u7f16\u7801\u52a9\u624b\u5728\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u751f\u4ea7\u5c31\u7eea\u6c34\u5e73", "topic": "code agent"}}
{"id": "tldr.2601.333f642b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/A1NCYU5wweZji_T_iO-rFNswHOHAROekJ15P_XEaUnA=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/A1NCYU5wweZji_T_iO-rFNswHOHAROekJ15P_XEaUnA=442", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/A1NCYU5wweZji_T_iO-rFNswHOHAROekJ15P_XEaUnA=442", "summary": "Pulumi Agent Skills: Best practices and more for AI coding assistants (4 minute read) Pulumi Agent Skills is a new collection of structured knowledge packages that enables AI coding assistants to generate production-ready infrastructure code.", "source": "tldr", "AI": {"tldr": "Pulumi Agent Skills \u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\u96c6\u5408\uff0c\u5e2e\u52a9AI\u7f16\u7a0b\u52a9\u624b\u751f\u6210\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801", "motivation": "\u5f53\u524dAI\u7f16\u7a0b\u52a9\u624b\u5728\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u65f6\u7f3a\u4e4f\u751f\u4ea7\u5c31\u7eea\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u4e0d\u9ad8", "method": "\u521b\u5efa\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\u96c6\u5408\uff0c\u5305\u542b\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u7684\u6700\u4f73\u5b9e\u8df5\u3001\u6a21\u5f0f\u548c\u6a21\u677f\uff0c\u4f9bAI\u7f16\u7a0b\u52a9\u624b\u4f7f\u7528", "result": "AI\u7f16\u7a0b\u52a9\u624b\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u53ef\u9760\u6027", "conclusion": "Pulumi Agent Skills \u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\u6709\u6548\u63d0\u5347\u4e86AI\u7f16\u7a0b\u52a9\u624b\u5728\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b", "topic": "code agent"}}
{"id": "tldr.2601.2878b07a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/a_R8zkBMU0gfZLRJfJcwwLaA7GY0C4BX7aCwBs9Ny2A=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/a_R8zkBMU0gfZLRJfJcwwLaA7GY0C4BX7aCwBs9Ny2A=442", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/a_R8zkBMU0gfZLRJfJcwwLaA7GY0C4BX7aCwBs9Ny2A=442", "summary": "Pulumi Agent Skills: Best practices and more for AI coding assistants (4 minute read) Pulumi Agent Skills is a new collection of structured knowledge packages that enables AI coding assistants to generate production-ready infrastructure code.", "source": "tldr", "AI": {"tldr": "Pulumi\u63a8\u51faAgent Skills\uff0c\u8fd9\u662f\u4e00\u7ec4\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\uff0c\u5e2e\u52a9AI\u7f16\u7801\u52a9\u624b\u751f\u6210\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801", "motivation": "\u5f53\u524dAI\u7f16\u7801\u52a9\u624b\u5728\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u65f6\u7f3a\u4e4f\u751f\u4ea7\u5c31\u7eea\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u63d0\u5347\u5176\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027", "method": "\u5f00\u53d1Pulumi Agent Skills - \u4e00\u7ec4\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\uff0c\u5305\u542b\u6700\u4f73\u5b9e\u8df5\u3001\u6a21\u5f0f\u548c\u5b89\u5168\u6307\u5357\uff0c\u4e13\u95e8\u7528\u4e8e\u6307\u5bfcAI\u7f16\u7801\u52a9\u624b\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801", "result": "AI\u7f16\u7801\u52a9\u624b\u73b0\u5728\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u5b89\u5168\u3001\u7b26\u5408\u751f\u4ea7\u6807\u51c6\u7684\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\uff0c\u51cf\u5c11\u4e86\u624b\u52a8\u8c03\u6574\u548c\u9519\u8bef\u4fee\u590d\u7684\u9700\u6c42", "conclusion": "Pulumi Agent Skills\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u7f16\u7801\u52a9\u624b\u5728\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u548c\u53ef\u9760\u6027", "topic": "code agent"}}
{"id": "tldr.2601.0b8fb71e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/c2EvQc1tbG8tsk4y3jqP1oIeh2VgCPe5X1SNnYIvp2A=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/c2EvQc1tbG8tsk4y3jqP1oIeh2VgCPe5X1SNnYIvp2A=442", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/c2EvQc1tbG8tsk4y3jqP1oIeh2VgCPe5X1SNnYIvp2A=442", "summary": "Pulumi Agent Skills: Best practices and more for AI coding assistants (4 minute read) Pulumi Agent Skills is a new collection of structured knowledge packages that enables AI coding assistants to generate production-ready infrastructure code.", "source": "tldr", "AI": {"tldr": "Pulumi\u63a8\u51faAgent Skills\uff0c\u8fd9\u662f\u4e00\u5957\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\uff0c\u4f7fAI\u7f16\u7a0b\u52a9\u624b\u80fd\u591f\u751f\u6210\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801", "motivation": "\u5f53\u524dAI\u7f16\u7a0b\u52a9\u624b\u5728\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u65f6\u7f3a\u4e4f\u751f\u4ea7\u5c31\u7eea\u6027\uff0c\u9700\u8981\u7ed3\u6784\u5316\u77e5\u8bc6\u6765\u6307\u5bfc\u5176\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u90e8\u7f72\u7684\u4ee3\u7801", "method": "\u5f00\u53d1\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\uff08Agent Skills\uff09\uff0c\u4e3aAI\u52a9\u624b\u63d0\u4f9b\u6700\u4f73\u5b9e\u8df5\u3001\u6a21\u5f0f\u3001\u5b89\u5168\u6307\u5357\u7b49\u77e5\u8bc6\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u7b26\u5408\u751f\u4ea7\u6807\u51c6\u7684Pulumi\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801", "result": "\u521b\u5efa\u4e86Pulumi Agent Skills\u96c6\u5408\uff0c\u63d0\u5347\u4e86AI\u52a9\u624b\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u751f\u4ea7\u5c31\u7eea\u7684\u4ee3\u7801", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\u589e\u5f3aAI\u7f16\u7a0b\u52a9\u624b\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u63a8\u52a8AI\u5728\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u9886\u57df\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "tldr.2601.2ebdc226", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Flocal-first-agentic-ai-guide%2F%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/JH6odhJhiaEtxesMlk2UXipR6bubvzNIFNz064WIcsE=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Flocal-first-agentic-ai-guide%2F%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/JH6odhJhiaEtxesMlk2UXipR6bubvzNIFNz064WIcsE=442", "authors": ["TLDR Newsletter"], "title": "Implementing local-first agentic AI: A practical guide", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Flocal-first-agentic-ai-guide%2F%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/JH6odhJhiaEtxesMlk2UXipR6bubvzNIFNz064WIcsE=442", "summary": "Implementing local-first agentic AI: A practical guide (5 minute read) LogRocket built an HR triage system that processes sensitive employee reports entirely on a local laptop using three small AI models (90M-3.8B parameters) instead of cloud APIs. The pipeline identifies the issue, creates an action plan, then executes functions like opening HR cases, all in 10-30 seconds without a GPU. This keeps private data on-premises, costs nothing in API fees, and trades the flexibility of big LLMs for...", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u672c\u5730\u4f18\u5148\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u4f7f\u7528\u5c0f\u578b\u6a21\u578b(90M-3.8B\u53c2\u6570)\u5728\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u5904\u7406\u654f\u611fHR\u62a5\u544a\uff0c\u65e0\u9700GPU\uff0c10-30\u79d2\u5b8c\u6210\uff0c\u6570\u636e\u5b8c\u5168\u672c\u5730\u5316", "motivation": "\u89e3\u51b3\u654f\u611f\u6570\u636e\u5904\u7406\u4e2d\u7684\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u907f\u514d\u4f7f\u7528\u4e91API\u5e26\u6765\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff0c\u540c\u65f6\u964d\u4f4eAPI\u8d39\u7528\u6210\u672c", "method": "\u91c7\u7528\u4e09\u4e2a\u5c0f\u578bAI\u6a21\u578b\u6784\u5efa\u672c\u5730\u5904\u7406\u6d41\u6c34\u7ebf\uff1a\u8bc6\u522b\u95ee\u9898\u3001\u5236\u5b9a\u884c\u52a8\u8ba1\u5212\u3001\u6267\u884cHR\u76f8\u5173\u529f\u80fd\uff0c\u5b8c\u5168\u5728\u672c\u5730\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u8fd0\u884c", "result": "\u7cfb\u7edf\u80fd\u572810-30\u79d2\u5185\u5904\u7406\u5458\u5de5\u62a5\u544a\uff0c\u65e0\u9700GPU\uff0c\u6570\u636e\u5b8c\u5168\u672c\u5730\u5316\uff0c\u96f6API\u8d39\u7528\uff0c\u4f46\u727a\u7272\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7075\u6d3b\u6027", "conclusion": "\u672c\u5730\u4f18\u5148\u7684AI\u4ee3\u7406\u65b9\u6cd5\u5728\u5904\u7406\u654f\u611f\u6570\u636e\u65f6\u5177\u6709\u9690\u79c1\u3001\u5b89\u5168\u548c\u6210\u672c\u4f18\u52bf\uff0c\u9002\u5408\u9700\u8981\u6570\u636e\u672c\u5730\u5316\u7684\u5e94\u7528\u573a\u666f", "topic": "code agent"}}
{"id": "tldr.2601.77d993fd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnldev20250130/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/7O_hiz4xeDxNgHlft17mM_xqMWaTvVf-dJ-zMBCJDFY=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnldev20250130/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/7O_hiz4xeDxNgHlft17mM_xqMWaTvVf-dJ-zMBCJDFY=442", "authors": ["TLDR Newsletter"], "title": "Go from 0 to shipped with vibe coding that actually works", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnldev20250130/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/7O_hiz4xeDxNgHlft17mM_xqMWaTvVf-dJ-zMBCJDFY=442", "summary": "Go from 0 to shipped with vibe coding that actually works (Sponsor) The old SDLC no longer reflects how teams work. v0 treats software like a living document where you build directly using prompts and previews. Every prompt creates production code in your git repository so you can focus on architecture, quality, and control. Start collaborating on the product, not on documents", "source": "tldr", "AI": {"tldr": "v0\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5f00\u53d1\u5e73\u53f0\uff0c\u5c06\u8f6f\u4ef6\u89c6\u4e3a\u6d3b\u6587\u6863\uff0c\u901a\u8fc7\u63d0\u793a\u76f4\u63a5\u751f\u6210\u751f\u4ea7\u4ee3\u7801\u5e76\u5b58\u5165Git\u4ed3\u5e93\uff0c\u8ba9\u56e2\u961f\u4e13\u6ce8\u4e8e\u67b6\u6784\u548c\u8d28\u91cf\u63a7\u5236", "motivation": "\u4f20\u7edf\u7684\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\uff08SDLC\uff09\u5df2\u65e0\u6cd5\u53cd\u6620\u73b0\u4ee3\u56e2\u961f\u7684\u5de5\u4f5c\u65b9\u5f0f\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u534f\u4f5c\u6027\u66f4\u5f3a\u7684\u5f00\u53d1\u5de5\u5177", "method": "\u91c7\u7528\"vibe coding\"\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u76f4\u63a5\u751f\u6210\u751f\u4ea7\u4ee3\u7801\uff0c\u6240\u6709\u4ee3\u7801\u81ea\u52a8\u5b58\u5165Git\u4ed3\u5e93\uff0c\u652f\u6301\u5b9e\u65f6\u9884\u89c8\u548c\u534f\u4f5c", "result": "\u5b9e\u73b0\u4e86\u4ece\u96f6\u5230\u4ea7\u54c1\u53d1\u5e03\u7684\u5feb\u901f\u5f00\u53d1\u6d41\u7a0b\uff0c\u56e2\u961f\u53ef\u4ee5\u4e13\u6ce8\u4e8e\u4ea7\u54c1\u672c\u8eab\u800c\u975e\u6587\u6863\u5de5\u4f5c", "conclusion": "v0\u5e73\u53f0\u901a\u8fc7AI\u8f85\u52a9\u7684\u6d3b\u6587\u6863\u5f00\u53d1\u6a21\u5f0f\uff0c\u6539\u53d8\u4e86\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u56e2\u961f\u534f\u4f5c\u6548\u7387\u548c\u5f00\u53d1\u901f\u5ea6", "topic": "swe application"}}
{"id": "tldr.2601.cf01e688", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fai%2Fcontextual-agent-playbooks-and-tools-how-linkedin-gave-ai-coding-agents-organizational-context%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/5Eo-CMZMlwTa3H3yOefO0JailBaMDRKDdw6sFiDrZrw=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fai%2Fcontextual-agent-playbooks-and-tools-how-linkedin-gave-ai-coding-agents-organizational-context%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/5Eo-CMZMlwTa3H3yOefO0JailBaMDRKDdw6sFiDrZrw=442", "authors": ["TLDR Newsletter"], "title": "Contextual agent playbooks and tools: How LinkedIn gave AI coding agents organizational context", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fai%2Fcontextual-agent-playbooks-and-tools-how-linkedin-gave-ai-coding-agents-organizational-context%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/5Eo-CMZMlwTa3H3yOefO0JailBaMDRKDdw6sFiDrZrw=442", "summary": "Contextual agent playbooks and tools: How LinkedIn gave AI coding agents organizational context (18 minute read) LinkedIn built an \u201cagent life-cycle service\u201d that gives AI coding agents memory and context about how the company actually works. Instead of starting fresh each time, agents remember conversation history (so if you ask about engineers in SF then say \u201cnow London,\u201d it gets it), respect data permissions through role-based auth, and keep humans in the loop for sensitive stuff like send...", "source": "tldr", "AI": {"tldr": "LinkedIn\u5f00\u53d1\u4e86\u4ee3\u7406\u751f\u547d\u5468\u671f\u670d\u52a1\uff0c\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u7ec4\u7ec7\u8bb0\u5fc6\u548c\u4e0a\u4e0b\u6587\uff0c\u4f7f\u5176\u80fd\u8bb0\u4f4f\u5bf9\u8bdd\u5386\u53f2\u3001\u9075\u5faa\u6570\u636e\u6743\u9650\u3001\u4fdd\u6301\u4eba\u7c7b\u76d1\u7763\uff0c\u63d0\u5347\u4f01\u4e1a\u73af\u5883\u4e2d\u7684AI\u7f16\u7801\u6548\u7387", "motivation": "\u89e3\u51b3AI\u7f16\u7801\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7f3a\u4e4f\u7ec4\u7ec7\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u4f20\u7edfAI\u4ee3\u7406\u6bcf\u6b21\u4ea4\u4e92\u90fd\u4ece\u96f6\u5f00\u59cb\uff0c\u65e0\u6cd5\u8bb0\u4f4f\u5386\u53f2\u5bf9\u8bdd\u3001\u4e0d\u4e86\u89e3\u516c\u53f8\u7279\u5b9a\u6570\u636e\u6743\u9650\u548c\u6d41\u7a0b\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u4e14\u5b58\u5728\u5b89\u5168\u98ce\u9669", "method": "\u6784\u5efa\"\u4ee3\u7406\u751f\u547d\u5468\u671f\u670d\u52a1\"\uff0c\u5305\u542b\u8bb0\u5fc6\u7cfb\u7edf\u8bb0\u5f55\u5bf9\u8bdd\u5386\u53f2\uff0c\u57fa\u4e8e\u89d2\u8272\u7684\u6743\u9650\u63a7\u5236\u786e\u4fdd\u6570\u636e\u5b89\u5168\uff0c\u4eba\u7c7b\u5728\u73af\u673a\u5236\u5904\u7406\u654f\u611f\u64cd\u4f5c\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u7406\u89e3\u516c\u53f8\u7279\u5b9a\u5de5\u4f5c\u65b9\u5f0f", "result": "AI\u7f16\u7801\u4ee3\u7406\u80fd\u6301\u7eed\u7406\u89e3\u4e0a\u4e0b\u6587\uff08\u5982\u4ece\"\u65e7\u91d1\u5c71\u5de5\u7a0b\u5e08\"\u8f6c\u5230\"\u4f26\u6566\"\uff09\uff0c\u5b89\u5168\u8bbf\u95ee\u516c\u53f8\u6570\u636e\uff0c\u5728\u654f\u611f\u64cd\u4f5c\u4e2d\u8bf7\u6c42\u4eba\u7c7b\u6279\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u7f16\u7801\u6548\u7387\u548c\u5b89\u5168\u6027", "conclusion": "\u4e3aAI\u7f16\u7801\u4ee3\u7406\u6dfb\u52a0\u7ec4\u7ec7\u4e0a\u4e0b\u6587\u548c\u8bb0\u5fc6\u80fd\u529b\u662f\u63d0\u5347\u4f01\u4e1aAI\u52a9\u624b\u6548\u7528\u7684\u5173\u952e\uff0c\u7ed3\u5408\u6743\u9650\u63a7\u5236\u548c\u4eba\u7c7b\u76d1\u7763\u53ef\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684AI\u8f85\u52a9\u7f16\u7801", "topic": "code agent"}}
{"id": "tldr.2601.a68cfdd9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarginlab.ai%2Ftrackers%2Fclaude-code%2F%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/ewkok6jij3yXtN91EIropJdJwDIhVKUUJaKEIfHkb38=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarginlab.ai%2Ftrackers%2Fclaude-code%2F%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/ewkok6jij3yXtN91EIropJdJwDIhVKUUJaKEIfHkb38=442", "authors": ["TLDR Newsletter"], "title": "Claude Code Opus 4.5 Performance Tracker", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarginlab.ai%2Ftrackers%2Fclaude-code%2F%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/ewkok6jij3yXtN91EIropJdJwDIhVKUUJaKEIfHkb38=442", "summary": "Claude Code Opus 4.5 Performance Tracker (3 minute read) This independent tracker monitors Claude Code with Opus 4.5 performance on SWE tasks daily using direct CLI benchmarks.", "source": "tldr", "AI": {"tldr": "\u72ec\u7acb\u8ffd\u8e2a\u5668\u6bcf\u65e5\u901a\u8fc7CLI\u57fa\u51c6\u6d4b\u8bd5\u76d1\u63a7Claude Code Opus 4.5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u9700\u8981\u6301\u7eed\u76d1\u63a7\u548c\u8bc4\u4f30Claude Code Opus 4.5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9645\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7528\u6237\u63d0\u4f9b\u5ba2\u89c2\u7684\u6027\u80fd\u6570\u636e", "method": "\u4f7f\u7528\u76f4\u63a5CLI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bcf\u65e5\u5bf9Claude Code Opus 4.5\u5728SWE\u4efb\u52a1\u4e0a\u8fdb\u884c\u6027\u80fd\u8ffd\u8e2a", "result": "\u5efa\u7acb\u4e86\u6301\u7eed\u7684\u6027\u80fd\u76d1\u63a7\u7cfb\u7edf\uff0c\u80fd\u591f\u63d0\u4f9bClaude Code Opus 4.5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u65e5\u5e38\u6027\u80fd\u6570\u636e", "conclusion": "\u901a\u8fc7\u72ec\u7acb\u7684\u6027\u80fd\u8ffd\u8e2a\u5668\uff0c\u53ef\u4ee5\u4e3a\u793e\u533a\u63d0\u4f9bClaude Code Opus 4.5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u53ef\u9760\u6027\u80fd\u8bc4\u4f30", "topic": "swe benchmark"}}
{"id": "tldr.2601.6009d951", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fbest-llms-for-coding%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/pRy6_0YNxAkoxVQtzbr_MEqDMJd6WhfjIZO-Nf6EApk=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fbest-llms-for-coding%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/pRy6_0YNxAkoxVQtzbr_MEqDMJd6WhfjIZO-Nf6EApk=442", "authors": ["TLDR Newsletter"], "title": "Best LLMs for coding in 2026", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fbest-llms-for-coding%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/pRy6_0YNxAkoxVQtzbr_MEqDMJd6WhfjIZO-Nf6EApk=442", "summary": "Best LLMs for coding in 2026 (12 minute read) There's no single best coding LLM since you need different models for different jobs.", "source": "tldr", "AI": {"tldr": "\u6ca1\u6709\u5355\u4e00\u7684\"\u6700\u4f73\"\u7f16\u7a0bLLM\uff0c\u56e0\u4e3a\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u6a21\u578b", "motivation": "\u63a2\u8ba8\u5728\u7f16\u7a0b\u9886\u57df\u5982\u4f55\u9009\u62e9\u5408\u9002\u7684LLM\uff0c\u5f3a\u8c03\u6ca1\u6709\u4e07\u80fd\u6a21\u578b\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9700\u6c42\u9009\u62e9", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u7f16\u7a0b\u4efb\u52a1\u7684\u7279\u70b9\u548c\u9700\u6c42\uff0c\u5bf9\u6bd4\u5404\u7c7bLLM\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0", "result": "\u8bc6\u522b\u51fa\u4e0d\u540c\u7f16\u7a0b\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u7c7b\u578b\u7684LLM\uff0c\u5982\u4ee3\u7801\u751f\u6210\u3001\u8c03\u8bd5\u3001\u6587\u6863\u7f16\u5199\u7b49\u5404\u6709\u9002\u5408\u7684\u6a21\u578b", "conclusion": "\u9009\u62e9\u7f16\u7a0bLLM\u65f6\u5e94\u8003\u8651\u5177\u4f53\u4efb\u52a1\u9700\u6c42\uff0c\u800c\u4e0d\u662f\u8ffd\u6c42\u5355\u4e00\u7684\"\u6700\u4f73\"\u6a21\u578b", "topic": "code agent"}}
{"id": "tldr.2601.cffbe826", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/FF48Qwklkhxtvr-BU6vXfhKpncaOBZh56DPa_Cz0ElQ=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/FF48Qwklkhxtvr-BU6vXfhKpncaOBZh56DPa_Cz0ElQ=442", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/FF48Qwklkhxtvr-BU6vXfhKpncaOBZh56DPa_Cz0ElQ=442", "summary": "Best LLMs for coding in 2026 (12 minute read) There's no single best coding LLM since you need different models for different jobs.", "source": "tldr", "AI": {"tldr": "\u6ca1\u6709\u5355\u4e00\u7684\u6700\u4f73\u7f16\u7801LLM\uff0c\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u6a21\u578b", "motivation": "\u63a2\u8ba82026\u5e74\u6700\u4f73\u7f16\u7801LLM\u7684\u9009\u62e9\u95ee\u9898\uff0c\u5f3a\u8c03\u6ca1\u6709\u4e07\u80fd\u6a21\u578b", "method": "\u5206\u6790\u4e0d\u540c\u7f16\u7801\u4efb\u52a1\u5bf9LLM\u7684\u9700\u6c42\u5dee\u5f02\uff0c\u53ef\u80fd\u57fa\u4e8e\u4efb\u52a1\u5206\u7c7b\u8bc4\u4f30", "result": "\u4e0d\u540c\u7f16\u7801\u4efb\u52a1\u9700\u8981\u4e13\u95e8\u5316\u7684LLM\uff0c\u800c\u975e\u5355\u4e00\u901a\u7528\u6a21\u578b", "conclusion": "\u7f16\u7801LLM\u9009\u62e9\u5e94\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9700\u6c42\uff0c\u672a\u6765\u8d8b\u52bf\u662f\u4e13\u4e1a\u5316\u800c\u975e\u901a\u7528\u5316", "topic": "code agent"}}
{"id": "tldr.2601.1eef8b65", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/cjEgyZmV4qciFj-1bcmj64xB8T4ApgGXRKZzOKOe4tw=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/cjEgyZmV4qciFj-1bcmj64xB8T4ApgGXRKZzOKOe4tw=442", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/cjEgyZmV4qciFj-1bcmj64xB8T4ApgGXRKZzOKOe4tw=442", "summary": "Best LLMs for coding in 2026 (12 minute read) There's no single best coding LLM since you need different models for different jobs.", "source": "tldr", "AI": {"tldr": "\u6ca1\u6709\u5355\u4e00\u7684\u6700\u4f73\u7f16\u7801LLM\uff0c\u56e0\u4e3a\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u6a21\u578b", "motivation": "\u63a2\u8ba8\u57282026\u5e74\u5982\u4f55\u9009\u62e9\u6700\u9002\u5408\u7f16\u7801\u4efb\u52a1\u7684LLM\uff0c\u8ba4\u8bc6\u5230\u4e0d\u540c\u7f16\u7801\u5de5\u4f5c\u5bf9\u6a21\u578b\u6709\u4e0d\u540c\u9700\u6c42", "method": "\u5206\u6790\u4e0d\u540c\u7f16\u7801\u4efb\u52a1\u7684\u7279\u70b9\u548c\u9700\u6c42\uff0c\u8bc4\u4f30\u5404\u79cdLLM\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0", "result": "\u53d1\u73b0\u4e0d\u5b58\u5728\u901a\u7528\u7684\"\u6700\u4f73\"\u7f16\u7801LLM\uff0c\u800c\u662f\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7c7b\u578b\u9009\u62e9\u6700\u9002\u5408\u7684\u6a21\u578b", "conclusion": "\u7f16\u7801LLM\u7684\u9009\u62e9\u5e94\u57fa\u4e8e\u5177\u4f53\u4efb\u52a1\u9700\u6c42\uff0c\u800c\u975e\u8ffd\u6c42\u5355\u4e00\"\u6700\u4f73\"\u6a21\u578b", "topic": "code agent"}}
{"id": "tldr.2601.c4dd659f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FdbNMqU/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/z17jr9KazstyWTP6IZ4EELjBjSUs6m5Cf8hMeCLu3no=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FdbNMqU/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/z17jr9KazstyWTP6IZ4EELjBjSUs6m5Cf8hMeCLu3no=442", "authors": ["TLDR Newsletter"], "title": "Inside OpenAI's in-house data agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FdbNMqU/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/z17jr9KazstyWTP6IZ4EELjBjSUs6m5Cf8hMeCLu3no=442", "summary": "Inside OpenAI's in-house data agent (15 minute read) OpenAI uses a bespoke in-house AI data agent that explores and reasons over its platform. It lets employees go from question to insight in minutes, allowing them to answer high-impact data questions. The agent was built specifically around OpenAI's data, permissions, and workflows. This post details how it was built to demonstrate how AI can support day-to-day work across teams. The tools used to build the AI data agent are available to all...", "source": "tldr", "AI": {"tldr": "OpenAI\u5f00\u53d1\u4e86\u4e13\u95e8\u7528\u4e8e\u5185\u90e8\u6570\u636e\u5206\u6790\u7684AI\u4ee3\u7406\uff0c\u80fd\u591f\u5feb\u901f\u56de\u7b54\u9ad8\u4ef7\u503c\u6570\u636e\u95ee\u9898\uff0c\u652f\u6301\u8de8\u56e2\u961f\u65e5\u5e38\u5de5\u4f5c\u7684AI\u5e94\u7528\u3002", "motivation": "OpenAI\u9700\u8981\u8ba9\u5458\u5de5\u80fd\u591f\u5feb\u901f\u4ece\u6570\u636e\u4e2d\u83b7\u5f97\u6d1e\u5bdf\uff0c\u89e3\u51b3\u9ad8\u5f71\u54cd\u529b\u7684\u6570\u636e\u95ee\u9898\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u4e13\u95e8\u56f4\u7ed5OpenAI\u6570\u636e\u3001\u6743\u9650\u548c\u5de5\u4f5c\u6d41\u7a0b\u5b9a\u5236\u7684AI\u6570\u636e\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u80fd\u591f\u63a2\u7d22\u548c\u63a8\u7406\u5e73\u53f0\u6570\u636e\u3002", "result": "\u5458\u5de5\u53ef\u4ee5\u5728\u51e0\u5206\u949f\u5185\u4ece\u95ee\u9898\u83b7\u5f97\u6d1e\u5bdf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u67e5\u8be2\u548c\u5206\u6790\u7684\u6548\u7387\u3002", "conclusion": "AI\u6570\u636e\u4ee3\u7406\u5c55\u793a\u4e86AI\u5982\u4f55\u652f\u6301\u8de8\u56e2\u961f\u7684\u65e5\u5e38\u5de5\u4f5c\uff0c\u76f8\u5173\u6784\u5efa\u5de5\u5177\u5df2\u5411\u6240\u6709\u4eba\u5f00\u653e\u3002", "topic": "code agent"}}
{"id": "tldr.2601.0a981ced", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datagravity.dev%2Fp%2Frl-environments-for-agentic-ai-who%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/rRF8cKvhuHIyqyHdt9oFusEJ396Lv1BAFB0Yke99moE=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datagravity.dev%2Fp%2Frl-environments-for-agentic-ai-who%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/rRF8cKvhuHIyqyHdt9oFusEJ396Lv1BAFB0Yke99moE=442", "authors": ["TLDR Newsletter"], "title": "RL Environments for Agentic AI: Who Will Win the Training & Verification Layer by 2030", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 29 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datagravity.dev%2Fp%2Frl-environments-for-agentic-ai-who%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/rRF8cKvhuHIyqyHdt9oFusEJ396Lv1BAFB0Yke99moE=442", "summary": "RL Environments for Agentic AI: Who Will Win the Training & Verification Layer by 2030 (29 minute read) The winners in RL environments will be the teams that build real infrastructure. This has to be done in close partnership with frontier labs to industrialize replication training, verification, and long-horizon environment orchestration. There will likely be three to five significant winners. The market may end up resembling data labeling, which already has roughly three over-$1 billion-rev...", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u57fa\u7840\u8bbe\u65bd\u5c06\u6210\u4e3aAI\u9886\u57df\u7684\u5173\u952e\u7ade\u4e89\u70b9\uff0c\u52302030\u5e74\u5c06\u51fa\u73b03-5\u4e2a\u4e3b\u8981\u8d62\u5bb6\uff0c\u5e02\u573a\u683c\u5c40\u53ef\u80fd\u7c7b\u4f3c\u4e8e\u6570\u636e\u6807\u6ce8\u884c\u4e1a", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u57fa\u7840\u8bbe\u65bd\u6765\u652f\u6301\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u957f\u671f\u73af\u5883\u7f16\u6392\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6210\u719f\u7684\u5de5\u4e1a\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524dRL\u73af\u5883\u57fa\u7840\u8bbe\u65bd\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u9700\u8981\u4e0e\u524d\u6cbf\u5b9e\u9a8c\u5ba4\u5408\u4f5c\u5efa\u7acb\u5de5\u4e1a\u5316\u7684\u590d\u5236\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u957f\u671f\u73af\u5883\u7f16\u6392\u7cfb\u7edf", "result": "\u9884\u6d4b\u52302030\u5e74RL\u73af\u5883\u57fa\u7840\u8bbe\u65bd\u5e02\u573a\u5c06\u51fa\u73b03-5\u4e2a\u4e3b\u8981\u8d62\u5bb6\uff0c\u5e02\u573a\u7ed3\u6784\u53ef\u80fd\u7c7b\u4f3c\u4e8e\u6570\u636e\u6807\u6ce8\u884c\u4e1a\uff08\u5df2\u67093\u4e2a\u6536\u5165\u8d8510\u4ebf\u7f8e\u5143\u7684\u516c\u53f8\uff09", "conclusion": "\u6784\u5efa\u771f\u5b9e\u7684RL\u73af\u5883\u57fa\u7840\u8bbe\u65bd\u662f\u8d62\u5f97\u7ade\u4e89\u7684\u5173\u952e\uff0c\u9700\u8981\u4e0e\u524d\u6cbf\u5b9e\u9a8c\u5ba4\u7d27\u5bc6\u5408\u4f5c\u5b9e\u73b0\u5de5\u4e1a\u5316\uff0c\u5e02\u573a\u5c06\u8d8b\u4e8e\u96c6\u4e2d\u5316", "topic": "agent analysis"}}
{"id": "tldr.2601.e4b71157", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FgDoRDp%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/6c4p2YizIK7OJifqc6EAlVPo2B12mCQz_kg6hjGbpcQ=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FgDoRDp%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/6c4p2YizIK7OJifqc6EAlVPo2B12mCQz_kg6hjGbpcQ=442", "authors": ["TLDR Newsletter"], "title": "Agents that don't suck", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FgDoRDp%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/6c4p2YizIK7OJifqc6EAlVPo2B12mCQz_kg6hjGbpcQ=442", "summary": "Agents that don't suck (Sponsor) Agent Bricks helps you build, evaluate and optimize AI agents grounded in your unique data. It evaluates automatically, scores outputs against your goals and improves with human feedback \u2014 giving you a clearer path to production. Build agents that work in the real world. See why it's worth your time", "source": "tldr", "AI": {"tldr": "Agent Bricks\u662f\u4e00\u4e2aAI\u4ee3\u7406\u5f00\u53d1\u5e73\u53f0\uff0c\u5e2e\u52a9\u7528\u6237\u57fa\u4e8e\u81ea\u8eab\u6570\u636e\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u4f18\u5316AI\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u3001\u76ee\u6807\u5bf9\u9f50\u8bc4\u5206\u548c\u4eba\u7c7b\u53cd\u9988\u6539\u8fdb\uff0c\u4f7f\u4ee3\u7406\u80fd\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u6709\u6548\u5de5\u4f5c\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5f00\u53d1\u9762\u4e34\u6311\u6218\uff1a\u96be\u4ee5\u6784\u5efa\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6709\u6548\u5de5\u4f5c\u7684\u4ee3\u7406\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4ece\u539f\u578b\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u8def\u5f84\u4e0d\u6e05\u6670\u3002", "method": "\u63d0\u4f9b\u5e73\u53f0\u5de5\u5177\uff0c\u652f\u6301\u57fa\u4e8e\u7528\u6237\u72ec\u7279\u6570\u636e\u6784\u5efaAI\u4ee3\u7406\uff0c\u81ea\u52a8\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\uff0c\u6839\u636e\u7528\u6237\u76ee\u6807\u5bf9\u8f93\u51fa\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u6301\u7eed\u6539\u8fdb\u4ee3\u7406\u8868\u73b0\u3002", "result": "\u5e73\u53f0\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efa\u51fa\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6709\u6548\u5de5\u4f5c\u7684AI\u4ee3\u7406\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u4ece\u5f00\u53d1\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u8def\u5f84\uff0c\u63d0\u9ad8\u4ee3\u7406\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "Agent Bricks\u5e73\u53f0\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86AI\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u6784\u5efa\u51fa\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u8868\u73b0\u826f\u597d\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "topic": "code agent"}}
{"id": "tldr.2601.0bafcf5d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fagent-trace%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/iM_J_Hc9V41OTmAcIQf32uIpvuvmNmCB_m4h0vLQPZk=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fagent-trace%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/iM_J_Hc9V41OTmAcIQf32uIpvuvmNmCB_m4h0vLQPZk=442", "authors": ["TLDR Newsletter"], "title": "Agent Trace: Capturing the Context Graph of Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fagent-trace%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/iM_J_Hc9V41OTmAcIQf32uIpvuvmNmCB_m4h0vLQPZk=442", "summary": "Agent Trace: Capturing the Context Graph of Code (5 minute read) Agent Trace is an open, vendor-neutral spec for recording AI contributions alongside human authorship in version-controlled codebases. It attributes each change to a specific conversation and line range, meaning repositories associated with Agent Traces will always be able to link back to the context that created it. The system makes development legible and could enable some pretty powerful management-level dashboards and data-d...", "source": "tldr", "AI": {"tldr": "Agent Trace\u662f\u4e00\u4e2a\u5f00\u653e\u3001\u5382\u5546\u4e2d\u7acb\u7684\u89c4\u8303\uff0c\u7528\u4e8e\u5728\u7248\u672c\u63a7\u5236\u4ee3\u7801\u5e93\u4e2d\u8bb0\u5f55AI\u8d21\u732e\u548c\u4eba\u7c7b\u4f5c\u8005\u8eab\u4efd\uff0c\u5c06\u6bcf\u4e2a\u53d8\u66f4\u5173\u8054\u5230\u7279\u5b9a\u5bf9\u8bdd\u548c\u4ee3\u7801\u884c\u8303\u56f4\u3002", "motivation": "\u968f\u7740AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u53c2\u4e0e\u5ea6\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u7684\u65b9\u5f0f\u6765\u8ffd\u8e2aAI\u5bf9\u4ee3\u7801\u7684\u8d21\u732e\uff0c\u4f7f\u5f00\u53d1\u8fc7\u7a0b\u66f4\u52a0\u900f\u660e\u548c\u53ef\u8ffd\u6eaf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f00\u653e\u89c4\u8303\uff0c\u5728\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u4e2d\u8bb0\u5f55AI\u8d21\u732e\uff0c\u5c06\u4ee3\u7801\u53d8\u66f4\u4e0e\u7279\u5b9a\u7684\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u548c\u4ee3\u7801\u884c\u8303\u56f4\u5173\u8054\u8d77\u6765\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u4f7f\u4ee3\u7801\u5e93\u80fd\u591f\u59cb\u7ec8\u94fe\u63a5\u56de\u521b\u5efa\u4ee3\u7801\u7684\u4e0a\u4e0b\u6587\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u7684\u53ef\u8bfb\u6027\u548c\u53ef\u7ba1\u7406\u6027\u3002", "conclusion": "Agent Trace\u89c4\u8303\u80fd\u591f\u4f7fAI\u8f85\u52a9\u7684\u5f00\u53d1\u8fc7\u7a0b\u66f4\u52a0\u900f\u660e\uff0c\u4e3a\u7ba1\u7406\u7ea7\u4eea\u8868\u677f\u548c\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "topic": "code agent"}}
{"id": "tldr.2602.4ecb8e01", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FuI8gjn/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/jLFFh6GqfxThVrdTVLk86_R9MqKctjHzRas-7bow9yg=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FuI8gjn/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/jLFFh6GqfxThVrdTVLk86_R9MqKctjHzRas-7bow9yg=442", "authors": ["TLDR Newsletter"], "title": "Inside OpenAI's in-house data agent", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FuI8gjn/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/jLFFh6GqfxThVrdTVLk86_R9MqKctjHzRas-7bow9yg=442", "summary": "Inside OpenAI's in-house data agent (15 minute read) OpenAI built a bespoke internal AI data agent powered by GPT-5 that lets employees ask natural-language questions and get accurate, contextual data insights end to end, from table discovery to analysis and reporting. It combines code-aware data context, institutional knowledge, memory, and continuous evaluation to deliver fast, reliable analytics at OpenAI's scale.", "source": "tldr", "AI": {"tldr": "OpenAI\u5f00\u53d1\u4e86\u5185\u90e8AI\u6570\u636e\u4ee3\u7406GPT-5\uff0c\u8ba9\u5458\u5de5\u80fd\u7528\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u83b7\u53d6\u51c6\u786e\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6570\u636e\u6d1e\u5bdf\uff0c\u6db5\u76d6\u4ece\u8868\u53d1\u73b0\u5230\u5206\u6790\u62a5\u544a\u7684\u5168\u6d41\u7a0b\u3002", "motivation": "OpenAI\u9700\u8981\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u7684\u6311\u6218\uff0c\u8ba9\u975e\u6280\u672f\u5458\u5de5\u4e5f\u80fd\u5feb\u901f\u83b7\u53d6\u51c6\u786e\u7684\u6570\u636e\u6d1e\u5bdf\uff0c\u63d0\u9ad8\u7ec4\u7ec7\u7684\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u6784\u5efa\u57fa\u4e8eGPT-5\u7684\u5b9a\u5236\u5316AI\u6570\u636e\u4ee3\u7406\uff0c\u6574\u5408\u4ee3\u7801\u611f\u77e5\u7684\u6570\u636e\u4e0a\u4e0b\u6587\u3001\u673a\u6784\u77e5\u8bc6\u3001\u8bb0\u5fc6\u7cfb\u7edf\u548c\u6301\u7eed\u8bc4\u4f30\u673a\u5236\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u6570\u636e\u5206\u6790\u6d41\u7a0b\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5728OpenAI\u7684\u89c4\u6a21\u4e0b\u63d0\u4f9b\u5feb\u901f\u53ef\u9760\u7684\u5206\u6790\uff0c\u8ba9\u5458\u5de5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u83b7\u5f97\u51c6\u786e\u7684\u6570\u636e\u6d1e\u5bdf\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u5206\u6790\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "conclusion": "\u5185\u90e8AI\u6570\u636e\u4ee3\u7406\u6210\u529f\u5730\u5c06\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5e94\u7528\u4e8e\u4f01\u4e1a\u6570\u636e\u5206\u6790\uff0c\u5c55\u793a\u4e86AI\u5728\u63d0\u5347\u7ec4\u7ec7\u6570\u636e\u7d20\u517b\u548c\u51b3\u7b56\u6548\u7387\u65b9\u9762\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2602.9e00831c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.louisbouchard.ai%2Fagents-and-workflows%2F%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/p3K-xQQps6MEhvdxeCC2jjyKPEIWsdD78a6hio1FesY=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.louisbouchard.ai%2Fagents-and-workflows%2F%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/p3K-xQQps6MEhvdxeCC2jjyKPEIWsdD78a6hio1FesY=442", "authors": ["TLDR Newsletter"], "title": "Multi-agent is becoming the new overengineering", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.louisbouchard.ai%2Fagents-and-workflows%2F%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/p3K-xQQps6MEhvdxeCC2jjyKPEIWsdD78a6hio1FesY=442", "summary": "Multi-agent is becoming the new overengineering (7 minute read) Clear architectural distinctions between workflows, single-agent systems, and multi-agent systems are critical to avoiding overengineering and inefficiency in LLM-based solutions. Workflows excel for deterministic, sequential tasks with minimal overhead, while a single agent with fewer than 10\u201320 tools suits dynamic, tightly coupled processes where global context matters. Multi-agent architectures are warranted only for true para...", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728LLM\u5e94\u7528\u4e2d\u7684\u8fc7\u5ea6\u5de5\u7a0b\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6e05\u6670\u7684\u67b6\u6784\u9009\u62e9\u6807\u51c6\uff1a\u786e\u5b9a\u6027\u987a\u5e8f\u4efb\u52a1\u7528\u5de5\u4f5c\u6d41\uff0c\u52a8\u6001\u7d27\u5bc6\u8026\u5408\u4efb\u52a1\u7528\u5355\u667a\u80fd\u4f53\uff08\u5c11\u4e8e10-20\u4e2a\u5de5\u5177\uff09\uff0c\u771f\u6b63\u7684\u5e76\u884c\u4efb\u52a1\u624d\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\u3002", "motivation": "\u5f53\u524dLLM\u89e3\u51b3\u65b9\u6848\u4e2d\u666e\u904d\u5b58\u5728\u8fc7\u5ea6\u5de5\u7a0b\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u591a\u667a\u80fd\u4f53\u67b6\u6784\u88ab\u6ee5\u7528\uff0c\u5bfc\u81f4\u7cfb\u7edf\u590d\u6742\u6027\u548c\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u5efa\u7acb\u6e05\u6670\u7684\u67b6\u6784\u9009\u62e9\u6807\u51c6\u6765\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u67b6\u6784\u7684\u7279\u70b9\u548c\u5e94\u7528\u573a\u666f\uff0c\u63d0\u51fa\u57fa\u4e8e\u4efb\u52a1\u7279\u6027\u7684\u67b6\u6784\u9009\u62e9\u6846\u67b6\uff1a\u5de5\u4f5c\u6d41\u9002\u5408\u786e\u5b9a\u6027\u987a\u5e8f\u4efb\u52a1\uff0c\u5355\u667a\u80fd\u4f53\u9002\u5408\u52a8\u6001\u7d27\u5bc6\u8026\u5408\u4efb\u52a1\uff0c\u591a\u667a\u80fd\u4f53\u4ec5\u9002\u5408\u771f\u6b63\u5e76\u884c\u4efb\u52a1\u3002", "result": "\u5efa\u7acb\u4e86\u660e\u786e\u7684\u67b6\u6784\u9009\u62e9\u6807\u51c6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u6700\u5408\u9002\u7684\u67b6\u6784\uff0c\u907f\u514d\u8fc7\u5ea6\u5de5\u7a0b\u5316\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4e0d\u5e94\u6210\u4e3a\u9ed8\u8ba4\u9009\u62e9\uff0c\u800c\u5e94\u6839\u636e\u4efb\u52a1\u7279\u6027\u8c28\u614e\u9009\u62e9\u3002\u6e05\u6670\u7684\u67b6\u6784\u533a\u5206\u5bf9\u4e8e\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u7ef4\u62a4\u7684LLM\u89e3\u51b3\u65b9\u6848\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.33482e4e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frobertsahlin.substack.com%2Fp%2Fwhy-the-future-of-data-platform-engineering%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/L4dnmNfzghNT8bjMxHvD3hRRYIzn2rte9RhZ8W24jIE=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frobertsahlin.substack.com%2Fp%2Fwhy-the-future-of-data-platform-engineering%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/L4dnmNfzghNT8bjMxHvD3hRRYIzn2rte9RhZ8W24jIE=442", "authors": ["TLDR Newsletter"], "title": "Why the Future of Data Platform Engineering is Agent Experience", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frobertsahlin.substack.com%2Fp%2Fwhy-the-future-of-data-platform-engineering%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/L4dnmNfzghNT8bjMxHvD3hRRYIzn2rte9RhZ8W24jIE=442", "summary": "Why the Future of Data Platform Engineering is Agent Experience (AX) (3 minute read) Data platform engineering is shifting focus from human-centric Developer Experience (DX) to Agent Experience (AX), as AI agents increasingly manage coding and operations. Priorities now include headless, API-first architectures, machine-readable documentation, deterministic JSON-based communication, structured error hints for autonomous remediation, and universal integration standards. This pivot demands plat...", "source": "tldr", "AI": {"tldr": "\u6570\u636e\u5e73\u53f0\u5de5\u7a0b\u6b63\u4ece\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5f00\u53d1\u8005\u4f53\u9a8c\u8f6c\u5411\u4ee5AI\u4ee3\u7406\u4e3a\u4e2d\u5fc3\u7684\u4ee3\u7406\u4f53\u9a8c\uff0c\u5f3a\u8c03API\u4f18\u5148\u67b6\u6784\u3001\u673a\u5668\u53ef\u8bfb\u6587\u6863\u548c\u7ed3\u6784\u5316\u9519\u8bef\u5904\u7406", "motivation": "\u968f\u7740AI\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u7ba1\u7406\u7f16\u7801\u548c\u8fd0\u7ef4\uff0c\u4f20\u7edf\u7684\u4eba\u7c7b\u4e2d\u5fc3\u5f00\u53d1\u8005\u4f53\u9a8c\u5df2\u4e0d\u9002\u5e94\uff0c\u9700\u8981\u4e13\u95e8\u4e3aAI\u4ee3\u7406\u4f18\u5316\u7684\u5e73\u53f0\u4f53\u9a8c", "method": "\u63d0\u51fa\u8f6c\u5411\u65e0\u5934\u3001API\u4f18\u5148\u67b6\u6784\uff0c\u91c7\u7528\u673a\u5668\u53ef\u8bfb\u6587\u6863\u3001\u786e\u5b9a\u6027JSON\u901a\u4fe1\u3001\u7ed3\u6784\u5316\u9519\u8bef\u63d0\u793a\u548c\u901a\u7528\u96c6\u6210\u6807\u51c6", "result": "\u6570\u636e\u5e73\u53f0\u5de5\u7a0b\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u652f\u6301AI\u4ee3\u7406\u7684\u81ea\u4e3b\u64cd\u4f5c\uff0c\u8fd9\u4ee3\u8868\u4e86\u884c\u4e1a\u7684\u91cd\u8981\u8303\u5f0f\u8f6c\u53d8", "conclusion": "\u672a\u6765\u6570\u636e\u5e73\u53f0\u5de5\u7a0b\u7684\u6838\u5fc3\u5c06\u662f\u4ee3\u7406\u4f53\u9a8c\uff0c\u8fd9\u9700\u8981\u7cfb\u7edf\u6027\u5730\u91cd\u6784\u5e73\u53f0\u67b6\u6784\u548c\u5de5\u5177\u94fe", "topic": "agent analysis"}}
