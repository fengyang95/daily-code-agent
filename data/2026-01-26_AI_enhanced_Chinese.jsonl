{"id": "2601.16238", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16238", "abs": "https://arxiv.org/abs/2601.16238", "authors": ["Bing Xu", "Terry Chen", "Fengzhe Zhou", "Tianqi Chen", "Yangqing Jia", "Vinod Grover", "Haicheng Wu", "Wei Liu", "Craig Wittenbrink", "Wen-mei Hwu", "Roger Bringmann", "Ming-Yu Liu", "Luis Ceze", "Michael Lightstone", "Humphrey Shi"], "title": "VibeTensor: System Software for Deep Learning, Fully Generated by AI Agents", "comment": "Open-source: https://github.com/NVLabs/vibetensor", "summary": "VIBETENSOR is an open-source research system software stack for deep learning, generated by LLM-powered coding agents under high-level human guidance. In this paper, \"fully generated\" refers to code provenance: implementation changes were produced and applied as agent-proposed diffs; validation relied on agent-run builds, tests, and differential checks, without per-change manual diff review. It implements a PyTorch-style eager tensor library with a C++20 core (CPU+CUDA), a torch-like Python overlay via nanobind, and an experimental Node.js/TypeScript interface. Unlike thin bindings, VIBETENSOR includes its own tensor/storage system, schema-lite dispatcher, reverse-mode autograd, CUDA runtime (streams/events/graphs), a stream-ordered caching allocator with diagnostics, and a stable C ABI for dynamically loaded operator plugins. We view this release as a milestone for AI-assisted software engineering: it shows coding agents can generate a coherent deep learning runtime spanning language bindings down to CUDA memory management, validated primarily by builds and tests. We describe the architecture, summarize the workflow used to produce and validate the system, and evaluate the artifact. We report repository scale and test-suite composition, and summarize reproducible microbenchmarks from an accompanying AI-generated kernel suite, including fused attention versus PyTorch SDPA/FlashAttention. We also report end-to-end training sanity checks on 3 small workloads (sequence reversal, ViT, miniGPT) on NVIDIA H100 (Hopper, SM90) and Blackwell-class GPUs; multi-GPU results are Blackwell-only and use an optional CUTLASS-based ring-allreduce plugin gated on CUDA 13+ and sm103a toolchain support. Finally, we discuss failure modes in generated system software, including a \"Frankenstein\" composition effect where locally correct subsystems interact to yield globally suboptimal performance.", "AI": {"tldr": "VIBETENSOR\u662f\u4e00\u4e2a\u7531LLM\u9a71\u52a8\u7684\u7f16\u7801\u4ee3\u7406\u5728\u9ad8\u5c42\u4eba\u7c7b\u6307\u5bfc\u4e0b\u751f\u6210\u7684\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u8f6f\u4ef6\u6808\uff0c\u5b9e\u73b0\u4e86PyTorch\u98ce\u683c\u7684\u5f20\u91cf\u5e93\uff0c\u5305\u542bC++20\u6838\u5fc3\u3001Python\u7ed1\u5b9a\u548c\u5b9e\u9a8c\u6027TypeScript\u63a5\u53e3\uff0c\u5c55\u793a\u4e86AI\u8f85\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22AI\u8f85\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6f5c\u529b\uff0c\u9a8c\u8bc1\u7f16\u7801\u4ee3\u7406\u80fd\u5426\u751f\u6210\u4e00\u4e2a\u4ece\u8bed\u8a00\u7ed1\u5b9a\u5230CUDA\u5185\u5b58\u7ba1\u7406\u7684\u5b8c\u6574\u6df1\u5ea6\u5b66\u4e60\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u800c\u4e0d\u9700\u8981\u9010\u884c\u624b\u52a8\u4ee3\u7801\u5ba1\u67e5\u3002", "method": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u7f16\u7801\u4ee3\u7406\u5728\u9ad8\u5c42\u4eba\u7c7b\u6307\u5bfc\u4e0b\u751f\u6210\u4ee3\u7801\uff0c\u901a\u8fc7\u4ee3\u7406\u8fd0\u884c\u7684\u6784\u5efa\u3001\u6d4b\u8bd5\u548c\u5dee\u5f02\u68c0\u67e5\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u5305\u542b\u5f20\u91cf\u7cfb\u7edf\u3001\u81ea\u52a8\u5fae\u5206\u3001CUDA\u8fd0\u884c\u65f6\u548c\u6d41\u6392\u5e8f\u7f13\u5b58\u5206\u914d\u5668\u7684\u5b8c\u6574\u6df1\u5ea6\u5b66\u4e60\u6808\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u529f\u80fd\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u5305\u542b\u7a33\u5b9a\u7684C ABI\u3001\u7b97\u5b50\u63d2\u4ef6\u652f\u6301\uff0c\u5728H100\u548cBlackwell GPU\u4e0a\u8fdb\u884c\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u4e0ePyTorch SDPA/FlashAttention\u76f8\u6bd4\u7684\u878d\u5408\u6ce8\u610f\u529b\u6027\u80fd\u3002", "conclusion": "VIBETENSOR\u5c55\u793a\u4e86\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u7684\u6df1\u5ea6\u5b66\u4e60\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u4f46\u5b58\u5728\"\u5f17\u5170\u80af\u65af\u5766\"\u7ec4\u5408\u6548\u5e94\u7b49\u6311\u6218\uff0c\u4e3aAI\u8f85\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u91cd\u8981\u91cc\u7a0b\u7891\u3002", "topic": "code agent"}}
{"id": "2601.16280", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16280", "abs": "https://arxiv.org/abs/2601.16280", "authors": ["Donghao Huang", "Gauri Malwe", "Zhaoxia Wang"], "title": "When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems", "comment": "Accepted for publication in 2026 The 9th International Conference on Artificial Intelligence and Big Data (ICAIBD 2026)", "summary": "Multi-agent systems powered by large language models (LLMs) are transforming enterprise automation, yet systematic evaluation methodologies for assessing tool-use reliability remain underdeveloped. We introduce a comprehensive diagnostic framework that leverages big data analytics to evaluate procedural reliability in intelligent agent systems, addressing critical needs for SME-centric deployment in privacy-sensitive environments. Our approach features a 12-category error taxonomy capturing failure modes across tool initialization, parameter handling, execution, and result interpretation. Through systematic evaluation of 1,980 deterministic test instances spanning both open-weight models (Qwen2.5 series, Functionary) and proprietary alternatives (GPT-4, Claude 3.5/3.7) across diverse edge hardware configurations, we identify actionable reliability thresholds for production deployment. Our analysis reveals that procedural reliability, particularly tool initialization failures, constitutes the primary bottleneck for smaller models, while qwen2.5:32b achieves flawless performance matching GPT-4.1. The framework demonstrates that mid-sized models (qwen2.5:14b) offer practical accuracy-efficiency trade-offs on commodity hardware (96.6\\% success rate, 7.3 s latency), enabling cost-effective intelligent agent deployment for resource-constrained organizations. This work establishes foundational infrastructure for systematic reliability evaluation of tool-augmented multi-agent AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u5927\u6570\u636e\u5206\u6790\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7a0b\u5e8f\u53ef\u9760\u6027\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc71980\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u6a21\u578b\uff0c\u53d1\u73b0\u7a0b\u5e8f\u53ef\u9760\u6027\uff08\u7279\u522b\u662f\u5de5\u5177\u521d\u59cb\u5316\u5931\u8d25\uff09\u662f\u8f83\u5c0f\u6a21\u578b\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u800c\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u6743\u8861\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6b63\u5728\u6539\u53d8\u4f01\u4e1a\u81ea\u52a8\u5316\uff0c\u4f46\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u53ef\u9760\u6027\u7684\u7cfb\u7edf\u5316\u65b9\u6cd5\u4ecd\u7136\u4e0d\u8db3\u3002\u7279\u522b\u662f\u5728\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\uff0c\u4e2d\u5c0f\u4f01\u4e1a\u9700\u8981\u53ef\u9760\u7684\u90e8\u7f72\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b12\u7c7b\u9519\u8bef\u5206\u7c7b\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u6db5\u76d6\u5de5\u5177\u521d\u59cb\u5316\u3001\u53c2\u6570\u5904\u7406\u3001\u6267\u884c\u548c\u7ed3\u679c\u89e3\u91ca\u7b49\u5931\u8d25\u6a21\u5f0f\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e861980\u4e2a\u786e\u5b9a\u6027\u6d4b\u8bd5\u5b9e\u4f8b\uff0c\u5305\u62ec\u5f00\u6e90\u6a21\u578b\uff08Qwen2.5\u7cfb\u5217\u3001Functionary\uff09\u548c\u4e13\u6709\u6a21\u578b\uff08GPT-4\u3001Claude 3.5/3.7\uff09\uff0c\u5e76\u5728\u4e0d\u540c\u8fb9\u7f18\u786c\u4ef6\u914d\u7f6e\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u7a0b\u5e8f\u53ef\u9760\u6027\uff08\u7279\u522b\u662f\u5de5\u5177\u521d\u59cb\u5316\u5931\u8d25\uff09\u662f\u8f83\u5c0f\u6a21\u578b\u7684\u4e3b\u8981\u74f6\u9888\uff0cqwen2.5:32b\u6a21\u578b\u8fbe\u5230\u4e0eGPT-4.1\u76f8\u5f53\u7684\u65e0\u7455\u75b5\u6027\u80fd\u3002\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\uff08qwen2.5:14b\uff09\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e8696.6%\u7684\u6210\u529f\u7387\u548c7.3\u79d2\u5ef6\u8fdf\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u6743\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u7cfb\u7edf\u5316\u53ef\u9760\u6027\u8bc4\u4f30\u5efa\u7acb\u4e86\u57fa\u7840\u67b6\u6784\uff0c\u4f7f\u8d44\u6e90\u53d7\u9650\u7ec4\u7ec7\u80fd\u591f\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u9ad8\u7684\u667a\u80fd\u4ee3\u7406\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2601.16286", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.16286", "abs": "https://arxiv.org/abs/2601.16286", "authors": ["Varun Chillara", "Dylan Kline", "Christopher Alvares", "Evan Wooten", "Huan Yang", "Shlok Khetan", "Cade Bauer", "Tr\u00e9 Guillory", "Tanishka Shah", "Yashodhara Dhariwal", "Volodymyr Pavlov", "George Popstefanov"], "title": "SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems", "comment": null, "summary": "Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency because it treats inference as a monolithic black box.\n  We introduce SemanticALLI, a pipeline-aware architecture within Alli (PMG's marketing intelligence platform), designed to operationalize redundant reasoning. By decomposing generation into Analytic Intent Resolution (AIR) and Visualization Synthesis (VS), SemanticALLI elevates structured intermediate representations (IRs) to first-class, cacheable artifacts.\n  The impact of caching within the agentic loop is substantial. In our evaluation, baseline monolithic caching caps at a 38.7% hit rate due to linguistic variance. In contrast, our structured approach allows for an additional stage, the Visualization Synthesis stage, to achieve an 83.10% hit rate, bypassing 4,023 LLM calls with a median latency of just 2.66 ms. This internal reuse reduces total token consumption, offering a practical lesson for AI system design: even when users rarely repeat themselves, the pipeline often does, at stable, structured checkpoints where caching is most reliable.", "AI": {"tldr": "SemanticALLI\u901a\u8fc7\u5c06AI\u7ba1\u9053\u5206\u89e3\u4e3a\u5206\u6790\u610f\u56fe\u89e3\u6790\u548c\u53ef\u89c6\u5316\u5408\u6210\u4e24\u4e2a\u9636\u6bb5\uff0c\u5bf9\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\u8fdb\u884c\u7f13\u5b58\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u51cf\u5c11\u4e86LLM\u8c03\u7528\u548c\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709AI\u7ba1\u9053\u5b58\u5728\u9690\u85cf\u7684\u4f4e\u6548\u95ee\u9898\uff1a\u5373\u4f7f\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u65b9\u5f0f\u4e0d\u540c\uff0c\u7cfb\u7edf\u4e5f\u4f1a\u91cd\u590d\u6784\u5efa\u76f8\u540c\u7684\u4e2d\u95f4\u903b\u8f91\uff08\u5982\u6307\u6807\u6807\u51c6\u5316\u3001\u56fe\u8868\u6846\u67b6\uff09\u3002\u4f20\u7edf\u7684\u8fb9\u754c\u7f13\u5b58\u65b9\u6cd5\u5c06\u63a8\u7406\u89c6\u4e3a\u9ed1\u76d2\uff0c\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u4f4e\u6548\u6027\u3002", "method": "\u5728Alli\u5e73\u53f0\u4e2d\u5f15\u5165SemanticALLI\u67b6\u6784\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u5206\u6790\u610f\u56fe\u89e3\u6790\uff08AIR\uff09\u548c\u53ef\u89c6\u5316\u5408\u6210\uff08VS\uff09\u3002\u901a\u8fc7\u5c06\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\u63d0\u5347\u4e3a\u4e00\u7b49\u53ef\u7f13\u5b58\u5de5\u4ef6\uff0c\u5b9e\u73b0\u5bf9\u5197\u4f59\u63a8\u7406\u7684\u64cd\u4f5c\u5316\u3002", "result": "\u57fa\u7ebf\u6574\u4f53\u7f13\u5b58\u547d\u4e2d\u7387\u4ec5\u4e3a38.7%\uff0c\u800c\u7ed3\u6784\u5316\u65b9\u6cd5\u4f7f\u53ef\u89c6\u5316\u5408\u6210\u9636\u6bb5\u8fbe\u523083.10%\u7684\u547d\u4e2d\u7387\uff0c\u8df3\u8fc7\u4e864,023\u6b21LLM\u8c03\u7528\uff0c\u4e2d\u4f4d\u5ef6\u8fdf\u4ec52.66\u6beb\u79d2\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u603b\u4ee4\u724c\u6d88\u8017\u3002", "conclusion": "\u5373\u4f7f\u7528\u6237\u5f88\u5c11\u91cd\u590d\u76f8\u540c\u8868\u8fbe\uff0cAI\u7ba1\u9053\u5728\u7a33\u5b9a\u3001\u7ed3\u6784\u5316\u7684\u68c0\u67e5\u70b9\u4e0a\u7ecf\u5e38\u91cd\u590d\u76f8\u540c\u5de5\u4f5c\uff0c\u8fd9\u4e9b\u5730\u65b9\u7f13\u5b58\u6700\u53ef\u9760\u3002\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\u7684\u7f13\u5b58\u4e3aAI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7ecf\u9a8c\u3002", "topic": "agent analysis"}}
{"id": "2601.16392", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.16392", "abs": "https://arxiv.org/abs/2601.16392", "authors": ["Lakshana Iruni Assalaarachchi", "Zainab Masood", "Rashina Hoda", "John Grundy"], "title": "Toward Agentic Software Project Management: A Vision and Roadmap", "comment": "This version is the author's preprint of the article accepted for AGENT workshop at ICSE 2026", "summary": "With the advent of agentic AI, Software Engineering is transforming to a new era dubbed Software Engineering 3.0. Software project management (SPM) must also evolve with such transformations to boost successful project completion, while keeping humans at the heart of it. Building on our preliminary ideas of \"agentic SPM\", and supporting literature, we present our vision of an \"Agentic Project Manager (PM)\" as a multi-agent system for SPM 3.0. They will work like a \"junior project manager\", or an \"intern project manager\" collaboratively with software teams. We introduce four working modes, with varying autonomy levels to choose from, based on the SPM task. This addresses concerns with ethics, accountability, and trust related to agentic PMs. We also share insights on human PM role evolution and new skill requirements as a \"strategic leader\" and a \"coach\" for humans and agents. While creating the foundation for agentic SPM research, we present a research agenda for the wider research community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"\u4ee3\u7406\u5f0f\u9879\u76ee\u7ba1\u7406\"\u613f\u666f\uff0c\u5c06AI\u4ee3\u7406\u4f5c\u4e3a\"\u521d\u7ea7\u9879\u76ee\u7ecf\u7406\"\u4e0e\u8f6f\u4ef6\u56e2\u961f\u534f\u4f5c\uff0c\u5f15\u5165\u56db\u79cd\u4e0d\u540c\u81ea\u4e3b\u7ea7\u522b\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0c\u5e76\u63a2\u8ba8\u4eba\u7c7b\u9879\u76ee\u7ecf\u7406\u89d2\u8272\u5411\"\u6218\u7565\u9886\u5bfc\u8005\"\u548c\"\u6559\u7ec3\"\u7684\u6f14\u53d8\u3002", "motivation": "\u968f\u7740\u4ee3\u7406\u5f0fAI\u7684\u53d1\u5c55\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u8fdb\u51653.0\u65f6\u4ee3\uff0c\u9879\u76ee\u7ba1\u7406\u9700\u8981\u76f8\u5e94\u53d8\u9769\u4ee5\u63d0\u5347\u9879\u76ee\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u5728\u6838\u5fc3\u5730\u4f4d\u3002\u9700\u8981\u89e3\u51b3\u4ee3\u7406\u5f0f\u9879\u76ee\u7ecf\u7406\u6d89\u53ca\u7684\u4f26\u7406\u3001\u8d23\u4efb\u548c\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u521d\u6b65\u7684\"\u4ee3\u7406\u5f0f\u9879\u76ee\u7ba1\u7406\"\u7406\u5ff5\u548c\u76f8\u5173\u6587\u732e\uff0c\u63d0\u51fa\"\u4ee3\u7406\u5f0f\u9879\u76ee\u7ecf\u7406\"\u4f5c\u4e3a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u56db\u79cd\u4e0d\u540c\u81ea\u4e3b\u7ea7\u522b\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0c\u6839\u636e\u5177\u4f53\u9879\u76ee\u7ba1\u7406\u4efb\u52a1\u9009\u62e9\u5408\u9002\u6a21\u5f0f\u3002", "result": "\u5efa\u7acb\u4e86\u4ee3\u7406\u5f0f\u9879\u76ee\u7ba1\u7406\u7814\u7a76\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u5e7f\u6cdb\u7814\u7a76\u8bae\u7a0b\uff0c\u5305\u62ec\u4eba\u7c7b\u9879\u76ee\u7ecf\u7406\u89d2\u8272\u6f14\u53d8\u4e3a\"\u6218\u7565\u9886\u5bfc\u8005\"\u548c\"\u6559\u7ec3\"\uff0c\u4ee5\u53ca\u65b0\u6280\u80fd\u8981\u6c42\u3002", "conclusion": "\u4ee3\u7406\u5f0f\u9879\u76ee\u7ecf\u7406\u5c06\u4f5c\u4e3a\"\u521d\u7ea7\u9879\u76ee\u7ecf\u7406\"\u6216\"\u5b9e\u4e60\u9879\u76ee\u7ecf\u7406\"\u4e0e\u8f6f\u4ef6\u56e2\u961f\u534f\u4f5c\uff0c\u901a\u8fc7\u56db\u79cd\u5de5\u4f5c\u6a21\u5f0f\u89e3\u51b3\u4f26\u7406\u548c\u4fe1\u4efb\u95ee\u9898\uff0c\u63a8\u52a8\u9879\u76ee\u7ba1\u7406\u54113.0\u65f6\u4ee3\u8f6c\u578b\u3002", "topic": "swe application"}}
{"id": "2601.16344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16344", "abs": "https://arxiv.org/abs/2601.16344", "authors": ["Fan Nie", "Junlin Wang", "Harper Hua", "Federico Bianchi", "Yongchan Kwon", "Zhenting Qi", "Owen Queen", "Shang Zhu", "James Zou"], "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents", "comment": null, "summary": "Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.", "AI": {"tldr": "DSGym\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u6807\u51c6\u5316\u6846\u67b6\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u67b6\u6784\u3001\u7efc\u5408\u4efb\u52a1\u5957\u4ef6\u548c\u6267\u884c\u9a8c\u8bc1\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u788e\u7247\u5316\u7684\u8bc4\u4f30\u63a5\u53e3\u5bfc\u81f4\u8de8\u57fa\u51c6\u6bd4\u8f83\u56f0\u96be\uff1b2) \u4efb\u52a1\u8986\u76d6\u8303\u56f4\u72ed\u7a84\uff1b3) \u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u636e\u57fa\u7840\uff08\u8bb8\u591a\u4efb\u52a1\u65e0\u9700\u5b9e\u9645\u6570\u636e\u5373\u53ef\u89e3\u51b3\uff09\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u652f\u6301\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u4e25\u8c28\u8bc4\u4f30\u548c\u8bad\u7ec3\u3002", "method": "\u63d0\u51faDSGym\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u6a21\u5757\u5316\u67b6\u6784\uff0c\u4fbf\u4e8e\u6dfb\u52a0\u4efb\u52a1\u3001\u4ee3\u7406\u811a\u624b\u67b6\u548c\u5de5\u5177\uff1b2) DSGym-Tasks\u4efb\u52a1\u5957\u4ef6\uff0c\u6807\u51c6\u5316\u548c\u7cbe\u70bc\u73b0\u6709\u57fa\u51c6\uff1b3) DSBio\uff08\u57fa\u4e8e\u6587\u732e\u7684\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\uff09\u548cDSPredict\uff08\u8de8\u9886\u57df\u9884\u6d4b\u4efb\u52a1\uff09\uff1b4) \u6267\u884c\u9a8c\u8bc1\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\u7528\u4e8e\u4ee3\u7406\u8bad\u7ec3\u3002", "result": "\u4f7f\u7528DSGym\u6784\u5efa\u4e862,000\u4e2a\u793a\u4f8b\u7684\u8bad\u7ec3\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a40\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u6807\u51c6\u5316\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-4o\u3002DSGym\u80fd\u591f\u5bf9\u4ee3\u7406\u5728\u771f\u5b9e\u79d1\u5b66\u80cc\u666f\u4e0b\u89c4\u5212\u3001\u5b9e\u65bd\u548c\u9a8c\u8bc1\u6570\u636e\u5206\u6790\u7684\u80fd\u529b\u8fdb\u884c\u4e25\u8c28\u7684\u7aef\u5230\u7aef\u6d4b\u91cf\u3002", "conclusion": "DSGym\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u6807\u51c6\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u79d1\u5b66\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u6267\u884c\u9a8c\u8bc1\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\u652f\u6301\u4ee3\u7406\u8bad\u7ec3\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\u3002", "topic": "swe benchmark"}}
{"id": "2601.16456", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.16456", "abs": "https://arxiv.org/abs/2601.16456", "authors": ["Ferida Mohammad", "Fatma Ayad", "Petros Maniatis", "Satish Chandra", "Elizabeth Dinella"], "title": "RubberDuckBench: A Benchmark for AI Coding Assistants", "comment": "LLM4Code @ ICSE '26", "summary": "Programmers are turning to AI coding assistants to answer questions about their code. Benchmarks are needed to soundly evaluate these systems and understand their performance. To enable such a study, we curate a benchmark of real-world contextualized questions derived from Github pull request comments. Out of this work, we present RubberDuckBench: a multilingual benchmark of questions about code, along with detailed rubrics for evaluating answers. We evaluate a diverse set of 20 LLMs (proprietary & open-source) on answering these questions. We find that even state of the art models fail to give consistent, correct responses across the benchmark. Grok 4 (69.29%), Claude Opus 4 (68.5%), and GPT-5 (67.8%) perform best overall, but do not exhibit pairwise significant superiority over the next 9 best performing models. Most models obtain points through partial credit, with the best performing models only answering at most 2 questions completely correctly across all trials. Furthermore, models often hallucinate with lies in 58.3\\% of responses on average. Cost analysis reveals no correlation between expense (API pricing or parameter count) and performance. We intend this benchmark to be a target for future research in trustworthy and correct AI coding assistants.", "AI": {"tldr": "RubberDuckBench\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u4ee3\u7801\u95ee\u7b54\u57fa\u51c6\uff0c\u57fa\u4e8eGitHub PR\u8bc4\u8bba\u6784\u5efa\uff0c\u8bc4\u4f30\u4e8620\u4e2aLLM\u5728\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u6a21\u578b\u4e5f\u96be\u4ee5\u7ed9\u51fa\u4e00\u81f4\u6b63\u786e\u7684\u56de\u7b54\uff0c\u4e14\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u7a0b\u5e8f\u5458\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528AI\u7f16\u7801\u52a9\u624b\u56de\u7b54\u4ee3\u7801\u76f8\u5173\u95ee\u9898\uff0c\u9700\u8981\u53ef\u9760\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u7684\u6027\u80fd\u5e76\u7406\u89e3\u5176\u8868\u73b0\u3002\u73b0\u6709\u57fa\u51c6\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u4ee3\u7801\u95ee\u7b54\u573a\u666f\u3002", "method": "\u4eceGitHub\u62c9\u53d6\u8bf7\u6c42\u8bc4\u8bba\u4e2d\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u7684\u4e0a\u4e0b\u6587\u5316\u95ee\u9898\uff0c\u6784\u5efa\u591a\u8bed\u8a00\u4ee3\u7801\u95ee\u7b54\u57fa\u51c6RubberDuckBench\uff0c\u5e76\u5236\u5b9a\u8be6\u7ec6\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u4f7f\u7528\u8be5\u57fa\u51c6\u8bc4\u4f3020\u4e2a\u4e0d\u540cLLM\uff08\u5305\u62ec\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\uff09\u5728\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6700\u4f73\u6a21\u578bGrok 4\uff0869.29%\uff09\u3001Claude Opus 4\uff0868.5%\uff09\u548cGPT-5\uff0867.8%\uff09\u8868\u73b0\u6700\u597d\uff0c\u4f46\u4e0e\u540e\u7eed9\u4e2a\u6a21\u578b\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002\u5927\u591a\u6570\u6a21\u578b\u901a\u8fc7\u90e8\u5206\u5f97\u5206\u83b7\u5f97\u5206\u6570\uff0c\u6700\u4f73\u6a21\u578b\u5728\u6240\u6709\u8bd5\u9a8c\u4e2d\u6700\u591a\u53ea\u5b8c\u5168\u6b63\u786e\u56de\u7b542\u4e2a\u95ee\u9898\u3002\u6a21\u578b\u5e73\u574758.3%\u7684\u56de\u7b54\u5b58\u5728\u5e7b\u89c9\u3002\u6210\u672c\u5206\u6790\u663e\u793a\u6027\u80fd\u4e0e\u8d39\u7528\uff08API\u5b9a\u4ef7\u6216\u53c2\u6570\u91cf\uff09\u65e0\u76f8\u5173\u6027\u3002", "conclusion": "\u5373\u4f7f\u6700\u5148\u8fdb\u7684AI\u7f16\u7801\u52a9\u624b\u5728\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u95ee\u9898\u4e0a\u4e5f\u96be\u4ee5\u7ed9\u51fa\u4e00\u81f4\u6b63\u786e\u7684\u56de\u7b54\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u8be5\u57fa\u51c6\u53ef\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u53ef\u4fe1\u8d56\u548c\u6b63\u786eAI\u7f16\u7801\u52a9\u624b\u7684\u76ee\u6807\u3002", "topic": "swe benchmark"}}
{"id": "2601.16479", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16479", "abs": "https://arxiv.org/abs/2601.16479", "authors": ["Hongjia Wu", "Shuai Zhou", "Hongxin Zhang", "Wei Chen"], "title": "Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs", "comment": null, "summary": "While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an \"expert bottleneck\" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.", "AI": {"tldr": "Doc2AHP\uff1a\u4e00\u79cd\u57fa\u4e8eAHP\u539f\u5219\u7684\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528LLM\u5728\u65e0\u7ed3\u6784\u6587\u6863\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7ea6\u675f\u641c\u7d22\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u4eba\u5de5\u5e72\u9884\uff0c\u663e\u8457\u63d0\u5347\u51b3\u7b56\u6a21\u578b\u7684\u903b\u8f91\u5b8c\u6574\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "LLM\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u4e25\u683c\u903b\u8f91\u7684\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u96be\u4ee5\u4fdd\u8bc1\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u63a8\u7406\u53ef\u9760\u6027\u3002\u4f20\u7edf\u51b3\u7b56\u7406\u8bba\uff08\u5982AHP\uff09\u867d\u7136\u63d0\u4f9b\u7cfb\u7edf\u5316\u7406\u6027\u6846\u67b6\uff0c\u4f46\u5176\u6784\u5efa\u4e25\u91cd\u4f9d\u8d56\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5f62\u6210\"\u4e13\u5bb6\u74f6\u9888\"\uff0c\u963b\u788d\u4e86\u5728\u4e00\u822c\u573a\u666f\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faDoc2AHP\u6846\u67b6\uff1a1\uff09\u5229\u7528AHP\u7684\u7ed3\u6784\u539f\u5219\u4f5c\u4e3a\u7ea6\u675f\uff0c\u6307\u5bfcLLM\u5728\u65e0\u7ed3\u6784\u6587\u6863\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7ea6\u675f\u641c\u7d22\uff0c\u5f3a\u5236\u7236\u5b50\u8282\u70b9\u95f4\u7684\u903b\u8f91\u8574\u542b\u5173\u7cfb\uff1b2\uff09\u5f15\u5165\u591a\u667a\u80fd\u4f53\u52a0\u6743\u673a\u5236\u7ed3\u5408\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u4f18\u5316\u7b56\u7565\uff0c\u786e\u4fdd\u6743\u91cd\u5206\u914d\u7684\u6570\u5b57\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cDoc2AHP\u4e0d\u4ec5\u4f7f\u975e\u4e13\u5bb6\u7528\u6237\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u9ad8\u8d28\u91cf\u51b3\u7b56\u6a21\u578b\uff0c\u800c\u4e14\u5728\u903b\u8f91\u5b8c\u6574\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Doc2AHP\u6210\u529f\u5f25\u5408\u4e86LLM\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u51b3\u7b56\u7406\u8bba\u4e25\u8c28\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u5b9e\u73b0\u4e86\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u4eba\u5de5\u5e72\u9884\u7684\u9ad8\u8d28\u91cf\u51b3\u7b56\u6a21\u578b\u6784\u5efa\u3002", "topic": "agent analysis"}}
{"id": "2601.16529", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.16529", "abs": "https://arxiv.org/abs/2601.16529", "authors": ["Dongshen Peng", "Yi Wang", "Carl Preiksaitis", "Christian Rose"], "title": "SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care", "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\\%. Models showed higher vulnerability to imaging requests (38.8\\%) than opioid prescriptions (25.0\\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u5bf9\u6297\u6d4b\u8bd5\u6846\u67b6SycoEval-EM\u8bc4\u4f30LLM\u5728\u6025\u8bca\u533b\u5b66\u4e2d\u7684\u6297\u60a3\u8005\u538b\u529b\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u5f53\u533b\u7597\u8bf7\u6c42\u9762\u524d\u5b58\u5728\u666e\u904d\u8106\u5f31\u6027\uff0c\u9759\u6001\u57fa\u51c6\u65e0\u6cd5\u9884\u6d4b\u5b9e\u9645\u5b89\u5168\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u5411\u60a3\u8005\u538b\u529b\u59a5\u534f\u800c\u63d0\u4f9b\u4e0d\u5f53\u533b\u7597\u670d\u52a1\u7684\u98ce\u9669\u3002\u5f53\u524d\u9759\u6001\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u6d4b\u8bd5\u6a21\u578b\u5728\u771f\u5b9e\u793e\u4ea4\u538b\u529b\u4e0b\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faSycoEval-EM\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u60a3\u8005\u8bf4\u670d\u573a\u666f\u8bc4\u4f30LLM\u9c81\u68d2\u6027\u3002\u6db5\u76d63\u4e2aChoosing Wisely\u573a\u666f\uff0c\u6d4b\u8bd520\u4e2aLLM\u6a21\u578b\u57281,875\u6b21\u6025\u8bca\u906d\u9047\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u59a5\u534f\u7387\u4ece0%\u5230100%\u4e0d\u7b49\uff0c\u5bf9\u5f71\u50cf\u68c0\u67e5\u8bf7\u6c42(38.8%)\u6bd4\u963f\u7247\u7c7b\u836f\u7269\u5904\u65b9(25.0%)\u66f4\u8106\u5f31\u3002\u6a21\u578b\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\u76f8\u5173\u6027\u5dee\uff0c\u6240\u6709\u8bf4\u670d\u7b56\u7565\u6548\u679c\u76f8\u4f3c(30.0-36.0%)\uff0c\u8868\u660e\u666e\u904d\u8106\u5f31\u6027\u800c\u975e\u7b56\u7565\u7279\u5f02\u6027\u5f31\u70b9\u3002", "conclusion": "\u9759\u6001\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u9884\u6d4b\u4e34\u5e8aAI\u5728\u793e\u4ea4\u538b\u529b\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u9700\u8981\u591a\u8f6e\u5bf9\u6297\u6d4b\u8bd5\u4f5c\u4e3a\u4e34\u5e8aAI\u8ba4\u8bc1\u7684\u5fc5\u8981\u7ec4\u6210\u90e8\u5206\u3002", "topic": "agent analysis"}}
{"id": "2601.16489", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16489", "abs": "https://arxiv.org/abs/2601.16489", "authors": ["Xinshuai Guo", "Jiayi Kuang", "Linyue Pan", "Yinghui Li", "Yangning Li", "Hai-Tao Zheng", "Ying Shen", "Di Yin", "Xing Sun"], "title": "EvoConfig: Self-Evolving Multi-Agent Systems for Efficient Autonomous Environment Configuration", "comment": null, "summary": "A reliable executable environment is the foundation for ensuring that large language models solve software engineering tasks. Due to the complex and tedious construction process, large-scale configuration is relatively inefficient. However, most methods always overlook fine-grained analysis of the actions performed by the agent, making it difficult to handle complex errors and resulting in configuration failures. To address this bottleneck, we propose EvoConfig, an efficient environment configuration framework that optimizes multi-agent collaboration to build correct runtime environments. EvoConfig features an expert diagnosis module for fine-grained post-execution analysis, and a self-evolving mechanism that lets expert agents self-feedback and dynamically adjust error-fixing priorities in real time. Empirically, EvoConfig matches the previous state-of-the-art Repo2Run on Repo2Run's 420 repositories, while delivering clear gains on harder cases: on the more challenging Envbench, EvoConfig achieves a 78.1% success rate, outperforming Repo2Run by 7.1%. Beyond end-to-end success, EvoConfig also demonstrates stronger debugging competence, achieving higher accuracy in error identification and producing more effective repair recommendations than existing methods.", "AI": {"tldr": "EvoConfig\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u73af\u5883\u914d\u7f6e\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u4e13\u5bb6\u8bca\u65ad\u6a21\u5757\u4f18\u5316\u8fd0\u884c\u65f6\u73af\u5883\u6784\u5efa\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u89e3\u51b3\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u65f6\uff0c\u53ef\u9760\u7684\u53ef\u6267\u884c\u73af\u5883\u6784\u5efa\u8fc7\u7a0b\u590d\u6742\u4e14\u4f4e\u6548\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u9519\u8bef\u5bfc\u81f4\u914d\u7f6e\u5931\u8d25\u3002", "method": "\u63d0\u51faEvoConfig\u6846\u67b6\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6784\u5efa\u6b63\u786e\u8fd0\u884c\u65f6\u73af\u5883\uff0c\u5305\u542b\u4e13\u5bb6\u8bca\u65ad\u6a21\u5757\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6267\u884c\u540e\u5206\u6790\uff0c\u4ee5\u53ca\u81ea\u8fdb\u5316\u673a\u5236\u8ba9\u4e13\u5bb6\u667a\u80fd\u4f53\u81ea\u6211\u53cd\u9988\u5e76\u52a8\u6001\u8c03\u6574\u9519\u8bef\u4fee\u590d\u4f18\u5148\u7ea7\u3002", "result": "\u5728Repo2Run\u7684420\u4e2a\u4ed3\u5e93\u4e0a\u5339\u914d\u4e86\u4e4b\u524d\u7684SOTA\u65b9\u6cd5Repo2Run\uff0c\u5728\u66f4\u5177\u6311\u6218\u6027\u7684Envbench\u4e0a\u8fbe\u523078.1%\u7684\u6210\u529f\u7387\uff0c\u6bd4Repo2Run\u9ad8\u51fa7.1%\uff0c\u5728\u9519\u8bef\u8bc6\u522b\u548c\u4fee\u590d\u5efa\u8bae\u65b9\u9762\u4e5f\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "EvoConfig\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u6790\u548c\u81ea\u8fdb\u5316\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u914d\u7f6e\u95ee\u9898\uff0c\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8c03\u8bd5\u80fd\u529b\u548c\u914d\u7f6e\u6210\u529f\u7387\u3002", "topic": "swe application"}}
{"id": "2601.16399", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.16399", "abs": "https://arxiv.org/abs/2601.16399", "authors": ["Sihan Zeng", "Sujay Bhatt", "Sumitra Ganesh", "Alec Koppel"], "title": "A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning", "comment": null, "summary": "We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5355\u5faa\u73af\u3001\u4e00\u9636\u7684actor-critic\u7b97\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u91cd\u6784\u89e3\u51b3\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u4e0a\u5c42\u4f18\u5316\u5e73\u6ed1\u51fd\u6570\uff0c\u4e0b\u5c42\u662fMDP\u4e2d\u7684\u7b56\u7565\u4f18\u5316\uff0c\u4e0a\u5c42\u53d8\u91cf\u53c2\u6570\u5316\u4e0b\u5c42MDP\u7684\u5956\u52b1\u3002", "motivation": "\u73b0\u6709\u53cc\u5c42\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4e8c\u9636\u4fe1\u606f\u3001\u5bf9\u4e0b\u5c42\u65bd\u52a0\u5f3a\u6b63\u5219\u5316\uff0c\u6216\u901a\u8fc7\u5d4c\u5957\u5faa\u73af\u8fc7\u7a0b\u4f4e\u6548\u4f7f\u7528\u6837\u672c\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u7c7b\u7ed3\u6784\u5316\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5355\u5faa\u73af\u3001\u4e00\u9636actor-critic\u7b97\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u91cd\u6784\u4f18\u5316\u53cc\u5c42\u76ee\u6807\u3002\u5728\u4e0b\u5c42RL\u76ee\u6807\u4e2d\u5f15\u5165\u8870\u51cf\u71b5\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u6e10\u8fd1\u65e0\u504f\u7684\u4e0a\u5c42\u8d85\u68af\u5ea6\u4f30\u8ba1\uff0c\u65e0\u9700\u7cbe\u786e\u6c42\u89e3\u65e0\u6b63\u5219\u5316RL\u95ee\u9898\u3002", "result": "\u5728\u7279\u6b8a\u7c7b\u578b\u7684Polyak-Lojasiewicz\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u4e0b\u5c42\u6b8b\u5dee\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u7b97\u6cd5\u5bf9\u539f\u59cb\u65e0\u6b63\u5219\u5316\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u7a33\u5b9a\u70b9\u7684\u6709\u9650\u65f6\u95f4\u548c\u6709\u9650\u6837\u672c\u6536\u655b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728GridWorld\u76ee\u6807\u4f4d\u7f6e\u95ee\u9898\u548c\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60(RLHF)\u7684\u5feb\u4e50\u63a8\u6587\u751f\u6210\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6027\u80fd\uff0c\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.16507", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.16507", "abs": "https://arxiv.org/abs/2601.16507", "authors": ["Junjie Shi", "Weisong Sun", "Zhenpeng Chen", "Zhujun Wu", "Xiaohong Chen", "Zhi Jin", "Yang Liu"], "title": "REprompt: Prompt Generation for Intelligent Software Development Guided by Requirements Engineering", "comment": null, "summary": "The rapid development of large language models is transforming software development. Beyond serving as code auto-completion tools in integrated development environments, large language models increasingly function as foundation models within coding agents in vibe-coding scenarios. In such settings, prompts play a central role in agent-based intelligent software development, as they not only guide the behavior of large language models but also serve as carriers of user requirements. Under the dominant conversational paradigm, prompts are typically divided into system prompts and user prompts. System prompts provide high-level instructions to steer model behavior and establish conversational context, while user prompts represent inputs and requirements provided by human users. Despite their importance, designing effective prompts remains challenging, as it requires expertise in both prompt engineering and software engineering, particularly requirements engineering. To reduce the burden of manual prompt construction, numerous automated prompt engineering methods have been proposed. However, most existing approaches neglect the methodological principles of requirements engineering, limiting their ability to generate artifacts that conform to formal requirement specifications in realistic software development scenarios. To address this gap, we propose REprompt, a multi-agent prompt optimization framework guided by requirements engineering. Experiment results demonstrate that REprompt effectively optimizes both system and user prompts by grounding prompt generation in requirements engineering principles.", "AI": {"tldr": "REprompt\u662f\u4e00\u4e2a\u57fa\u4e8e\u9700\u6c42\u5de5\u7a0b\u6307\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u9700\u6c42\u5de5\u7a0b\u539f\u5219\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u548c\u7528\u6237\u63d0\u793a\uff0c\u89e3\u51b3\u4f20\u7edf\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5ffd\u89c6\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\u8bba\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7801\u4ee3\u7406\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u63d0\u793a\u5728\u667a\u80fd\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u626e\u6f14\u7740\u6838\u5fc3\u89d2\u8272\u3002\u7136\u800c\uff0c\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u9700\u8981\u540c\u65f6\u5177\u5907\u63d0\u793a\u5de5\u7a0b\u548c\u8f6f\u4ef6\u5de5\u7a0b\uff08\u7279\u522b\u662f\u9700\u6c42\u5de5\u7a0b\uff09\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u7684\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u4e86\u9700\u6c42\u5de5\u7a0b\u7684\u65b9\u6cd5\u8bba\u539f\u5219\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u573a\u666f\u4e2d\u751f\u6210\u7b26\u5408\u6b63\u5f0f\u9700\u6c42\u89c4\u8303\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86REprompt\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u9700\u6c42\u5de5\u7a0b\u6307\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u63d0\u793a\u4f18\u5316\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u9700\u6c42\u5de5\u7a0b\u539f\u5219\u878d\u5165\u63d0\u793a\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6765\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u548c\u7528\u6237\u63d0\u793a\uff0c\u786e\u4fdd\u751f\u6210\u7684\u63d0\u793a\u7b26\u5408\u6b63\u5f0f\u9700\u6c42\u89c4\u8303\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cREprompt\u80fd\u591f\u6709\u6548\u5730\u57fa\u4e8e\u9700\u6c42\u5de5\u7a0b\u539f\u5219\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u548c\u7528\u6237\u63d0\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u7b26\u5408\u6b63\u5f0f\u9700\u6c42\u89c4\u8303\u7684\u63d0\u793a\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "REprompt\u6846\u67b6\u901a\u8fc7\u5c06\u9700\u6c42\u5de5\u7a0b\u539f\u5219\u878d\u5165\u63d0\u793a\u4f18\u5316\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5ffd\u89c6\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\u8bba\u7684\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u63d0\u793a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2601.16649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16649", "abs": "https://arxiv.org/abs/2601.16649", "authors": ["Amin Rakhsha", "Thomas Hehn", "Pietro Mazzaglia", "Fabio Valerio Massoli", "Arash Behboodi", "Tribhuvanesh Orekondy"], "title": "LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents", "comment": null, "summary": "Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u591a\u8f6e\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u4e0d\u540c\u80fd\u529b\uff08\u5982\u89c4\u5212\u3001\u72b6\u6001\u8ddf\u8e2a\uff09\u91cd\u8981\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u7684\u53ef\u8c03\u590d\u6742\u5ea6\u6e38\u620f\u73af\u5883\uff0c\u4f7f\u7528oracle\u5e72\u9884\u6765\u6d4b\u91cf\u5404\u9879\u6280\u80fd\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u8d21\u732e\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u72ec\u7acb\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u89c4\u5212\u3001\u72b6\u6001\u8ddf\u8e2a\u548c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7684\u591a\u8f6e\u3001\u957f\u89c6\u91ce\u667a\u80fd\u4f53\u95ee\u9898\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u57fa\u7840\u80fd\u529b\u5bf9\u4e8e\u6b64\u7c7b\u4efb\u52a1\u6210\u529f\u7684\u76f8\u5bf9\u91cd\u8981\u6027\uff0c\u4ee5\u6307\u5bfc\u672a\u6765AI\u667a\u80fd\u4f53\u548c\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aoracle\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u5177\u6709\u53ef\u8c03\u590d\u6742\u5ea6\u7684\u6e38\u620f\u5f0f\u4efb\u52a1\u73af\u5883\u3002\u5728\u8fd9\u4e9b\u53d7\u63a7\u73af\u5883\u4e2d\uff0c\u53ef\u4ee5\u7cbe\u786e\u63d0\u4f9boracle\u5e72\u9884\uff08\u5982\u5b8c\u7f8e\u89c4\u5212\u3001\u65e0\u9519\u8bef\u72b6\u6001\u8ddf\u8e2a\uff09\uff0c\u4ece\u800c\u9694\u79bb\u6bcf\u9879\u6280\u80fd\u5bf9\u6027\u80fd\u7684\u8d21\u732e\uff0c\u907f\u514d\u73b0\u5b9e\u57fa\u51c6\u4e2d\u7684\u6df7\u6742\u6548\u5e94\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u67d0\u4e9b\u5e72\u9884\uff08\u5982\u89c4\u5212\uff09\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u800c\u5176\u4ed6\u6280\u80fd\u7684\u6709\u7528\u6027\u5219\u53d6\u51b3\u4e8e\u73af\u5883\u548c\u8bed\u8a00\u6a21\u578b\u7684\u7279\u6027\u3002\u8fd9\u8868\u660e\u4e0d\u540c\u6280\u80fd\u7684\u91cd\u8981\u6027\u5177\u6709\u60c5\u5883\u4f9d\u8d56\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u8f6e\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765AI\u667a\u80fd\u4f53\u548c\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002oracle\u53cd\u4e8b\u5b9e\u6846\u67b6\u80fd\u591f\u91cf\u5316\u4e0d\u540c\u6280\u80fd\u7684\u91cd\u8981\u6027\uff0c\u5e2e\u52a9\u786e\u5b9a\u80fd\u529b\u53d1\u5c55\u7684\u4f18\u5148\u7ea7\u3002", "topic": "agent analysis"}}
{"id": "2601.16661", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16661", "abs": "https://arxiv.org/abs/2601.16661", "authors": ["Monika Gupta", "Ajay Meena", "Anamitra Roy Choudhury", "Vijay Arya", "Srikanta Bedathur"], "title": "Revisiting the Role of Natural Language Code Comments in Code Translation", "comment": null, "summary": "The advent of large language models (LLMs) has ushered in a new era in automated code translation across programming languages. Since most code-specific LLMs are pretrained on well-commented code from large repositories like GitHub, it is reasonable to hypothesize that natural language code comments could aid in improving translation quality. Despite their potential relevance, comments are largely absent from existing code translation benchmarks, rendering their impact on translation quality inadequately characterised. In this paper, we present a large-scale empirical study evaluating the impact of comments on translation performance. Our analysis involves more than $80,000$ translations, with and without comments, of $1100+$ code samples from two distinct benchmarks covering pairwise translations between five different programming languages: C, C++, Go, Java, and Python. Our results provide strong evidence that code comments, particularly those that describe the overall purpose of the code rather than line-by-line functionality, significantly enhance translation accuracy. Based on these findings, we propose COMMENTRA, a code translation approach, and demonstrate that it can potentially double the performance of LLM-based code translation. To the best of our knowledge, our study is the first in terms of its comprehensiveness, scale, and language coverage on how to improve code translation accuracy using code comments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u4ee3\u7801\u6ce8\u91ca\uff08\u7279\u522b\u662f\u63cf\u8ff0\u4ee3\u7801\u6574\u4f53\u76ee\u7684\u7684\u6ce8\u91ca\uff09\u80fd\u663e\u8457\u63d0\u5347LLM\u4ee3\u7801\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86COMMENTRA\u65b9\u6cd5\uff0c\u53ef\u5c06\u7ffb\u8bd1\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe\u4e24\u500d\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u7ffb\u8bd1\u57fa\u51c6\u5927\u591a\u7f3a\u4e4f\u4ee3\u7801\u6ce8\u91ca\uff0c\u5bfc\u81f4\u6ce8\u91ca\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u7684\u5f71\u54cd\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u9274\u4e8e\u5927\u591a\u6570\u4ee3\u7801\u4e13\u7528LLM\u662f\u5728\u5305\u542b\u4e30\u5bcc\u6ce8\u91ca\u7684GitHub\u4ee3\u7801\u5e93\u4e0a\u9884\u8bad\u7ec3\u7684\uff0c\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u6d89\u53ca\u8d85\u8fc780,000\u6b21\u7ffb\u8bd1\uff08\u5305\u542b\u548c\u4e0d\u5305\u542b\u6ce8\u91ca\uff09\uff0c\u8986\u76d61,100\u591a\u4e2a\u4ee3\u7801\u6837\u672c\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e94\u79cd\u7f16\u7a0b\u8bed\u8a00\uff08C\u3001C++\u3001Go\u3001Java\u3001Python\uff09\u4e4b\u95f4\u7684\u6210\u5bf9\u7ffb\u8bd1\u3002\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\u63d0\u51fa\u4e86COMMENTRA\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\u3002", "result": "\u4ee3\u7801\u6ce8\u91ca\uff08\u7279\u522b\u662f\u63cf\u8ff0\u4ee3\u7801\u6574\u4f53\u76ee\u7684\u7684\u6ce8\u91ca\u800c\u975e\u9010\u884c\u529f\u80fd\u63cf\u8ff0\uff09\u80fd\u663e\u8457\u63d0\u9ad8\u7ffb\u8bd1\u51c6\u786e\u6027\u3002COMMENTRA\u65b9\u6cd5\u53ef\u5c06\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u7ffb\u8bd1\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe\u4e24\u500d\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u5168\u9762\u6027\u3001\u89c4\u6a21\u548c\u8bed\u8a00\u8986\u76d6\u8303\u56f4\u4e0a\u7814\u7a76\u5982\u4f55\u5229\u7528\u4ee3\u7801\u6ce8\u91ca\u63d0\u9ad8\u4ee3\u7801\u7ffb\u8bd1\u51c6\u786e\u6027\u7684\u5de5\u4f5c\uff0c\u8bc1\u660e\u4e86\u6ce8\u91ca\u7684\u91cd\u8981\u4ef7\u503c\uff0c\u5e76\u4e3a\u6539\u8fdb\u4ee3\u7801\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2601.16725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16725", "abs": "https://arxiv.org/abs/2601.16725", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chen Gao", "Chen Zhang", "Chengcheng Han", "Chenhui Yang", "Chuyu Zhang", "Cong Chen", "Cunguang Wang", "Daoru Pan", "Defei Bu", "Dengchang Zhao", "Di Xiu", "Dishan Liu", "Dongyu Ru", "Dunwei Tu", "Fan Wu", "Fengcheng Yuan", "Fengcun Li", "Gang Xu", "Guanyu Wu", "Guoyuan Lin", "Haibin Wang", "Hansi Yang", "Hao Yang", "Haonan Yan", "Haoxiang Ma", "Haoxing Wen", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiacheng Zhang", "Jiahong Zhou", "Jiahuan Li", "Jiaming Wang", "Jian Yang", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiapeng Zhu", "Jiaqi Sun", "Jiarong Shi", "Jiarui Zhao", "Jingang Wang", "Jinluan Yang", "Jinrui Ding", "Jinwei Xiao", "Jiyuan He", "Juncan Xu", "Kefeng Zhang", "Keheng Wang", "Li Wei", "Lianhui Ma", "Lin Qiu", "Lingbing Kong", "Lingchuan Liu", "Linsen Guo", "Mengshen Zhu", "Mengxia Shen", "Mingyang Zhu", "Peiguang Li", "Peng Pei", "Pengcheng Jia", "Pengtao Zhang", "Peng Zhao", "Qi Gu", "Qiong Huang", "Qiyuan Duan", "Quanchi Weng", "Rongxiang Weng", "Rongzhi Zhang", "Rumei Li", "Shanglin Lei", "Shengnan An", "Shijun Dai", "Shuaikang Liu", "Shuang Zhou", "Shuo Wang", "Songyuan Zhao", "Tao Liang", "Tianhao Hu", "Tianze Chen", "Wei Liu", "Wei Shi", "Wei Wang", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Wentao Chen", "Wentao Shi", "Xi Su", "Xiangcheng Liu", "Xiandi Ma", "Xiangyu Xi", "Xiangyuan Liu", "Xiangzhou Huang", "Xiao Liu", "Xiaodong Cai", "Xiaolong Chen", "Xiaowei Shi", "Xiaoyu Li", "Xin Chen", "Xingchen Liu", "Xuan Huang", "Xuezhi Cao", "Xunliang Cai", "Yan Chen", "Yang Bai", "Yang Liu", "Yang Yang", "Yang Zheng", "Yaoming Wang", "Yaoming Zhu", "Yaqi Huo", "Yanyu Chen", "Yaorui Shi", "Yerui Sun", "Yi Zhang", "Yihao Chen", "Yi-Kai Zhang", "Yifan Lu", "Yifan Zhao", "Yitao Zhai", "Yongjing Yin", "Yongwei Zhou", "Youshao Xiao", "Yuchuan Dai", "Yuchen Xie", "Yuchen Yu", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunfan Liang", "Yunke Zhao", "Yuwei Jiang", "Yuxin Bian", "Yuxin Chen", "Yuxin Liu", "Yue Xu", "Yueqing Sun", "Zeyang Yu", "Zhao Yang", "Zhengsheng Huang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhimin Lin", "Zhiyuan Yao", "Zhuofan Chen", "Zhuowen Han", "Zijian Zhang", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang"], "title": "LongCat-Flash-Thinking-2601 Technical Report", "comment": null, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "AI": {"tldr": "LongCat-Flash-Thinking-2601\u662f\u4e00\u4e2a5600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90MoE\u63a8\u7406\u6a21\u578b\uff0c\u5728\u591a\u79cd\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5177\u5907\u5f3a\u5927\u7684\u5de5\u5177\u4f7f\u7528\u6cdb\u5316\u80fd\u529b\u548c\u566a\u58f0\u73af\u5883\u9c81\u68d2\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5177\u5907\u5353\u8d8a\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u5de5\u5177\u4ea4\u4e92\u3001\u591a\u8f6e\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u5e76\u5728\u566a\u58f0\u73b0\u5b9e\u73af\u5883\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff1a\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u4e0e\u540e\u7eed\u878d\u5408\uff1b\u7aef\u5230\u7aef\u534f\u540c\u8bbe\u8ba1\u6570\u636e\u6784\u9020\u3001\u73af\u5883\u3001\u7b97\u6cd5\u548c\u57fa\u7840\u8bbe\u65bd\uff1b\u6269\u5c55\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6DORA\u652f\u6301\u5927\u89c4\u6a21\u591a\u73af\u5883\u8bad\u7ec3\uff1b\u7cfb\u7edf\u5206\u6790\u73b0\u5b9e\u566a\u58f0\u6a21\u5f0f\u5e76\u9488\u5bf9\u6027\u8bad\u7ec3\uff1b\u5f15\u5165Heavy Thinking\u6a21\u5f0f\u8fdb\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "result": "\u5728\u667a\u80fd\u4f53\u641c\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u3001\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5728\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b\u5728\u566a\u58f0\u73b0\u5b9e\u73af\u5883\u4e2d\u4fdd\u6301\u9c81\u68d2\u884c\u4e3a\u3002", "conclusion": "LongCat-Flash-Thinking-2601\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\u548c\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.16806", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16806", "abs": "https://arxiv.org/abs/2601.16806", "authors": ["Lu Yihe", "Barbara Webb"], "title": "An Efficient Insect-inspired Approach for Visual Point-goal Navigation", "comment": null, "summary": "In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53d7\u6606\u866b\u542f\u53d1\u7684\u89c6\u89c9\u70b9\u76ee\u6807\u5bfc\u822a\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u6606\u866b\u5927\u8111\u7684\u8054\u60f3\u5b66\u4e60\u548c\u8def\u5f84\u6574\u5408\u673a\u5236\uff0c\u5728Habitat\u70b9\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u6027\u80fd\u5ab2\u7f8eSOTA\u6a21\u578b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u4f4e\u591a\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u53d7\u6606\u866b\u5728\u53d1\u73b0\u98df\u7269\u4f4d\u7f6e\u548c\u5de2\u7a74\u4e4b\u95f4\u5b66\u4e60\u5e76\u4f18\u5316\u89c6\u89c9\u5f15\u5bfc\u8def\u5f84\u7684\u80fd\u529b\u542f\u53d1\uff0c\u5c06\u6606\u866b\u5927\u8111\u7684\u8054\u60f3\u5b66\u4e60\u548c\u8def\u5f84\u6574\u5408\u673a\u5236\u5e94\u7528\u4e8e\u89c6\u89c9\u70b9\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0c\u65e8\u5728\u5f00\u53d1\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5bfc\u822a\u667a\u80fd\u4f53\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u6606\u866b\u5927\u8111\u7ed3\u6784\u7684\u62bd\u8c61\u6a21\u578b\uff1a\u8d1f\u8d23\u8054\u60f3\u5b66\u4e60\u7684\u7ed3\u6784\u548c\u8d1f\u8d23\u8def\u5f84\u6574\u5408\u7684\u7ed3\u6784\u3002\u5c06Habitat\u70b9\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u5f62\u5f0f\u5316\u57fa\u51c6\u4e0e\u6606\u866b\u5bfc\u822a\u80fd\u529b\u8fdb\u884c\u7c7b\u6bd4\uff0c\u6784\u5efa\u7b80\u5355\u7684\u6606\u866b\u542f\u53d1\u5f0f\u667a\u80fd\u4f53\u3002", "result": "\u6606\u866b\u542f\u53d1\u5f0f\u667a\u80fd\u4f53\u5728Habitat\u70b9\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0e\u6700\u8fd1SOTA\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u4f4e\u591a\u4e2a\u6570\u91cf\u7ea7\u3002\u5728\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u663e\u793a\u8be5\u65b9\u6cd5\u5bf9\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u6606\u866b\u5927\u8111\u7684\u7b80\u5355\u673a\u5236\u53ef\u4ee5\u9ad8\u6548\u89e3\u51b3\u590d\u6742\u7684\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\uff0c\u4e3a\u5f00\u53d1\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u751f\u7269\u542f\u53d1\u7684\u8bbe\u8ba1\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2601.16863", "categories": ["cs.AI", "cs.LG", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16863", "abs": "https://arxiv.org/abs/2601.16863", "authors": ["Tims Pecerskis", "Aivars Smirnovs"], "title": "Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation", "comment": null, "summary": "This paper introduces the N-Way Self-Evaluating Deliberation (NSED) protocol, a Runtime Mixture-of-Models (MoM) architecture that constructs emergent composite models from a plurality of distinct expert agents. Unlike traditional Mixture-of-Experts (MoE) which rely on static gating networks, NSED employs a Dynamic Expertise Broker - a runtime optimization engine that treats model selection as a variation of the Knapsack Problem, binding heterogeneous checkpoints to functional roles based on live telemetry and cost constraints. At the execution layer, we formalize deliberation as a Macro-Scale Recurrent Neural Network (RNN), where the consensus state loops back through a semantic forget gate to enable iterative refinement without proportional VRAM scaling. Key components include an orchestration fabric for trustless N-to-N peer review, a Quadratic Voting activation function for non-linear consensus, and a feedback-driven state update. Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (less than 20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models, establishing a new hardware arbitrage efficiency frontier. Furthermore, testing on the DarkBench safety suite reveals intrinsic alignment properties, with peer-mediated correction reducing sycophancy scores below that of any individual agent.", "AI": {"tldr": "NSED\u534f\u8bae\u662f\u4e00\u79cd\u8fd0\u884c\u65f6\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u4ee3\u7406\u6784\u5efa\u590d\u5408\u6a21\u578b\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u96c6\u5408\u80fd\u5339\u914d\u5927\u578b\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u95e8\u63a7\u7f51\u7edc\uff0c\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u4efb\u52a1\u9700\u6c42\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5b9e\u65f6\u4f18\u5316\u6a21\u578b\u9009\u62e9\u3001\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3001\u540c\u65f6\u63d0\u5347\u6027\u80fd\u548c\u5b89\u5168\u6027\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51faNSED\u534f\u8bae\uff1a1) \u52a8\u6001\u4e13\u5bb6\u4ee3\u7406\u5668\u5c06\u6a21\u578b\u9009\u62e9\u89c6\u4e3a\u80cc\u5305\u95ee\u9898\uff1b2) \u5b8f\u89c2\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u8fed\u4ee3\u7cbe\u70bc\uff1b3) \u65e0\u4fe1\u4efbN\u5bf9N\u540c\u884c\u8bc4\u5ba1\uff1b4) \u4e8c\u6b21\u6295\u7968\u6fc0\u6d3b\u51fd\u6570\uff1b5) \u53cd\u9988\u9a71\u52a8\u72b6\u6001\u66f4\u65b0\u3002", "result": "\u5728AIME 2025\u548cLiveCodeBench\u4e0a\uff0c\u5c0f\u4e8e20B\u53c2\u6570\u7684\u5c0f\u578b\u6a21\u578b\u96c6\u5408\u80fd\u5339\u914d\u6216\u8d85\u8d8a100B+\u5927\u578b\u6a21\u578b\u6027\u80fd\u3002\u5728DarkBench\u5b89\u5168\u6d4b\u8bd5\u4e2d\uff0c\u540c\u884c\u6821\u6b63\u964d\u4f4e\u4e86\u8c04\u5a9a\u5206\u6570\u3002", "conclusion": "NSED\u5efa\u7acb\u4e86\u65b0\u7684\u786c\u4ef6\u5957\u5229\u6548\u7387\u8fb9\u754c\uff0c\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u7ec4\u5408\u4f7f\u5c0f\u578b\u6a21\u578b\u8fbe\u5230\u5927\u578b\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u540c\u884c\u8bc4\u5ba1\u673a\u5236\u63d0\u5347\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.16746", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16746", "abs": "https://arxiv.org/abs/2601.16746", "authors": ["Yuhang Wang", "Yuling Shi", "Mo Yang", "Rongrui Zhang", "Shilin He", "Heng Lian", "Yuting Chen", "Siyu Ye", "Kai Cai", "Xiaodong Gu"], "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "comment": "Code available at https://github.com/Ayanami1314/swe-pruner", "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "AI": {"tldr": "SWE-Pruner\uff1a\u9488\u5bf9\u4ee3\u7801\u4ee3\u7406\u7684\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u526a\u679d\u7b56\u7565\u51cf\u5c11\u957f\u4e0a\u4e0b\u6587\u5e26\u6765\u7684API\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u7684\u8bed\u6cd5\u903b\u8f91\u7ed3\u6784\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u957f\u4ea4\u4e92\u4e0a\u4e0b\u6587\u5bfc\u81f4\u9ad8API\u6210\u672c\u548c\u5ef6\u8fdf\u3002\u73b0\u6709\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\uff08\u5982LongLLMLingua\uff09\u4f7f\u7528\u56fa\u5b9a\u6307\u6807\uff08\u5982PPL\uff09\uff0c\u5ffd\u7565\u4e86\u4ee3\u7801\u7406\u89e3\u7684\u4efb\u52a1\u7279\u5b9a\u6027\uff0c\u7ecf\u5e38\u7834\u574f\u8bed\u6cd5\u903b\u8f91\u7ed3\u6784\u5e76\u4e22\u5931\u5173\u952e\u5b9e\u73b0\u7ec6\u8282\u3002", "method": "\u53d7\u4eba\u7c7b\u7a0b\u5e8f\u5458\"\u9009\u62e9\u6027\u6d4f\u89c8\"\u6e90\u4ee3\u7801\u7684\u542f\u53d1\uff0cSWE-Pruner\u6267\u884c\u4efb\u52a1\u611f\u77e5\u7684\u81ea\u9002\u5e94\u526a\u679d\u3002\u4ee3\u7406\u6839\u636e\u5f53\u524d\u4efb\u52a1\u5236\u5b9a\u660e\u786e\u76ee\u6807\uff08\u5982\"\u5173\u6ce8\u9519\u8bef\u5904\u7406\"\uff09\u4f5c\u4e3a\u63d0\u793a\uff0c\u6307\u5bfc\u526a\u679d\u76ee\u6807\u3002\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u6d4f\u89c8\u5668\uff080.6B\u53c2\u6570\uff09\u6839\u636e\u76ee\u6807\u52a8\u6001\u9009\u62e9\u76f8\u5173\u4ee3\u7801\u884c\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4e2a\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u9a8c\u8bc1\u4e86SWE-Pruner\u7684\u6709\u6548\u6027\uff1a\u5728SWE-Bench Verified\u7b49\u4ee3\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b023-54%\u7684token\u51cf\u5c11\uff0c\u5728LongCodeQA\u7b49\u5355\u8f6e\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u8fbe14.84\u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "SWE-Pruner\u4e3a\u4ee3\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u526a\u679d\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5e26\u6765\u7684\u6210\u672c\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2601.16443", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16443", "abs": "https://arxiv.org/abs/2601.16443", "authors": ["Kanishk Gandhi", "Shivam Garg", "Noah D. Goodman", "Dimitris Papailiopoulos"], "title": "Endless Terminals: Scaling RL Environments for Terminal Agents", "comment": null, "summary": "Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.", "AI": {"tldr": "Endless Terminals\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u7a0b\u5e8f\u5316\u751f\u6210\u7ec8\u7aef\u4f7f\u7528\u4efb\u52a1\u800c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u4e3a\u81ea\u6539\u8fdb\u4ee3\u7406\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u73af\u5883\u3002\u4f7f\u7528\u7b80\u5355\u7684PPO\u8bad\u7ec3\u4ee3\u7406\u5728\u751f\u6210\u7684\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u80fd\u591f\u8fc1\u79fb\u5230\u4eba\u5de5\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u3002", "motivation": "\u5f53\u524d\u7ec8\u7aef\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u7528\u4e8e\u8bc4\u4f30\u800c\u975e\u8bad\u7ec3\uff0c\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u53ef\u6269\u5c55\u7684\u6d41\u6c34\u7ebf\u800c\u4e0d\u4ec5\u4ec5\u662f\u6570\u636e\u96c6\u3002\u73af\u5883\u662f\u81ea\u6539\u8fdb\u4ee3\u7406\u7684\u74f6\u9888\uff0c\u9700\u8981\u80fd\u591f\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u8bad\u7ec3\u4efb\u52a1\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51faEndless Terminals\u6d41\u6c34\u7ebf\uff0c\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a1\uff09\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\u63cf\u8ff0\uff1b2\uff09\u6784\u5efa\u548c\u9a8c\u8bc1\u5bb9\u5668\u5316\u73af\u5883\uff1b3\uff09\u751f\u6210\u5b8c\u6210\u6d4b\u8bd5\uff1b4\uff09\u7b5b\u9009\u53ef\u89e3\u4efb\u52a1\u3002\u4f7f\u7528\u7b80\u5355\u7684PPO\u7b97\u6cd5\u548c\u4e8c\u8fdb\u5236\u56de\u5408\u7ea7\u5956\u52b1\u8fdb\u884c\u4ee3\u7406\u8bad\u7ec3\uff0c\u6ca1\u6709\u68c0\u7d22\u3001\u591a\u4ee3\u7406\u534f\u8c03\u6216\u4e13\u95e8\u5de5\u5177\u3002", "result": "\u4ece\u6d41\u6c34\u7ebf\u4e2d\u83b7\u5f973255\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u6587\u4ef6\u64cd\u4f5c\u3001\u65e5\u5fd7\u7ba1\u7406\u3001\u6570\u636e\u5904\u7406\u3001\u811a\u672c\u7f16\u5199\u548c\u6570\u636e\u5e93\u64cd\u4f5c\u3002\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u4fdd\u7559\u5f00\u53d1\u96c6\u4e0a\u663e\u8457\u63d0\u5347\uff1aLlama-3.2-3B\u4ece4.0%\u63d0\u5347\u523018.2%\uff0cQwen2.5-7B\u4ece10.7%\u63d0\u5347\u523053.3%\uff0cQwen3-8B-openthinker-sft\u4ece42.6%\u63d0\u5347\u523059.0%\u3002\u8fd9\u4e9b\u6539\u8fdb\u80fd\u8fc1\u79fb\u5230\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5728TerminalBench 2.0\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5f53\u73af\u5883\u53ef\u6269\u5c55\u65f6\uff0c\u7b80\u5355\u7684\u5f3a\u5316\u5b66\u4e60\u5c31\u80fd\u6210\u529f\u3002Endless Terminals\u4e3a\u81ea\u6539\u8fdb\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u8bc1\u660e\u4e86\u7a0b\u5e8f\u5316\u751f\u6210\u4efb\u52a1\u6d41\u6c34\u7ebf\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.16349", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16349", "abs": "https://arxiv.org/abs/2601.16349", "authors": ["M P V S Gopinadh", "Kappara Lakshmi Sindhu", "Soma Sekhar Pandu Ranga Raju P", "Yesaswini Swarna"], "title": "Regional Bias in Large Language Models", "comment": "8 pages, 1 figure. Presented at the Second International Conference on Advanced Computing, Machine Learning, Robotics and Internet Technologies (AMRIT 2024)", "summary": "This study investigates regional bias in large language models (LLMs), an emerging concern in AI fairness and global representation. We evaluate ten prominent LLMs: GPT-3.5, GPT-4o, Gemini 1.5 Flash, Gemini 1.0 Pro, Claude 3 Opus, Claude 3.5 Sonnet, Llama 3, Gemma 7B, Mistral 7B, and Vicuna-13B using a dataset of 100 carefully designed prompts that probe forced-choice decisions between regions under contextually neutral scenarios. We introduce FAZE, a prompt-based evaluation framework that measures regional bias on a 10-point scale, where higher scores indicate a stronger tendency to favor specific regions. Experimental results reveal substantial variation in bias levels across models, with GPT-3.5 exhibiting the highest bias score (9.5) and Claude 3.5 Sonnet scoring the lowest (2.5). These findings indicate that regional bias can meaningfully undermine the reliability, fairness, and inclusivity of LLM outputs in real-world, cross-cultural applications. This work contributes to AI fairness research by highlighting the importance of inclusive evaluation frameworks and systematic approaches for identifying and mitigating geographic biases in language models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8610\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533a\u57df\u504f\u89c1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-3.5\u504f\u89c1\u6700\u4e25\u91cd\uff089.5\u5206\uff09\uff0cClaude 3.5 Sonnet\u504f\u89c1\u6700\u4f4e\uff082.5\u5206\uff09\uff0c\u8868\u660e\u533a\u57df\u504f\u89c1\u4f1a\u635f\u5bb3LLM\u7684\u53ef\u9760\u6027\u3001\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u533a\u57df\u504f\u89c1\u95ee\u9898\uff0c\u8fd9\u662fAI\u516c\u5e73\u6027\u548c\u5168\u7403\u4ee3\u8868\u6027\u9886\u57df\u7684\u65b0\u5174\u5173\u6ce8\u70b9\u3002\u533a\u57df\u504f\u89c1\u53ef\u80fd\u5f71\u54cdLLM\u5728\u8de8\u6587\u5316\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u4f7f\u7528\u5305\u542b100\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u7684\u6570\u636e\u96c6\uff0c\u5728\u60c5\u5883\u4e2d\u7acb\u573a\u666f\u4e0b\u6d4b\u8bd5\u6a21\u578b\u5728\u533a\u57df\u95f4\u7684\u5f3a\u5236\u9009\u62e9\u51b3\u7b56\u3002\u5f15\u5165FAZE\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u63d0\u793a\u6d4b\u91cf\u533a\u57df\u504f\u89c1\uff0c\u91c7\u752810\u5206\u5236\u8bc4\u5206\uff08\u5206\u6570\u8d8a\u9ad8\u504f\u89c1\u8d8a\u5f3a\uff09\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u504f\u89c1\u6c34\u5e73\u5dee\u5f02\u663e\u8457\uff1aGPT-3.5\u504f\u89c1\u6700\u4e25\u91cd\uff089.5\u5206\uff09\uff0cClaude 3.5 Sonnet\u504f\u89c1\u6700\u4f4e\uff082.5\u5206\uff09\u3002\u5176\u4ed6\u6a21\u578b\u5982GPT-4o\u3001Gemini\u7cfb\u5217\u3001Claude 3 Opus\u3001Llama 3\u3001Gemma 7B\u3001Mistral 7B\u548cVicuna-13B\u4e5f\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u533a\u57df\u504f\u89c1\u3002", "conclusion": "\u533a\u57df\u504f\u89c1\u4f1a\u663e\u8457\u635f\u5bb3LLM\u5728\u73b0\u5b9e\u4e16\u754c\u8de8\u6587\u5316\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3001\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5305\u5bb9\u6027\u8bc4\u4f30\u6846\u67b6\u548c\u7cfb\u7edf\u6027\u65b9\u6cd5\u5728\u8bc6\u522b\u548c\u7f13\u89e3\u8bed\u8a00\u6a21\u578b\u5730\u7406\u504f\u89c1\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.16809", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16809", "abs": "https://arxiv.org/abs/2601.16809", "authors": ["Musfiqur Rahman", "Emad Shihab"], "title": "Will It Survive? Deciphering the Fate of AI-Generated Code in Open Source", "comment": "This paper has been submitted to EASE 2026 research track and currently under review", "summary": "The integration of AI agents as coding assistants into software development has raised questions about the long-term viability of AI agent-generated code. A prevailing hypothesis within the software engineering community suggests this code is \"disposable\", meaning it is merged quickly but discarded shortly thereafter. If true, organizations risk shifting maintenance burden from generation to post-deployment remediation. We investigate this hypothesis through survival analysis of 201 open-source projects, tracking over 200,000 code units authored by AI agents versus humans. Contrary to the disposable code narrative, agent-authored code survives significantly longer: at the line level, it exhibits a 15.8 percentage-point lower modification rate and 16% lower hazard of modification (HR = 0.842, p < 0.001). However, modification profiles differ. Agent-authored code shows modestly elevated corrective rates (26.3% vs. 23.0%), while human code shows higher adaptive rates. However, the effect sizes are small (Cram\u00e9r's V = 0.116), and per-agent variation exceeds the agent-human gap. Turning to prediction, textual features can identify modification-prone code (AUC-ROC = 0.671), but predicting when modifications occur remains challenging (Macro F1 = 0.285), suggesting timing depends on external organizational dynamics. The bottleneck for agent-generated code may not be generation quality, but the organizational practices that govern its long-term evolution.", "AI": {"tldr": "AI\u4ee3\u7406\u751f\u6210\u7684\u4ee3\u7801\u6bd4\u4eba\u7c7b\u4ee3\u7801\u66f4\u6301\u4e45\uff0c\u4fee\u6539\u7387\u66f4\u4f4e\uff0c\u4f46\u4fee\u6b63\u7387\u7565\u9ad8\uff0c\u7ec4\u7ec7\u5b9e\u8df5\u800c\u975e\u4ee3\u7801\u8d28\u91cf\u662f\u957f\u671f\u7ef4\u62a4\u7684\u5173\u952e\u74f6\u9888", "motivation": "\u7814\u7a76AI\u4ee3\u7406\u751f\u6210\u7684\u4ee3\u7801\u662f\u5426\u5982\u8f6f\u4ef6\u5de5\u7a0b\u754c\u666e\u904d\u8ba4\u4e3a\u7684\u90a3\u6837\u662f\"\u4e00\u6b21\u6027\"\u7684\uff0c\u5373\u5feb\u901f\u5408\u5e76\u4f46\u5f88\u5feb\u88ab\u4e22\u5f03\uff0c\u8fd9\u5173\u7cfb\u5230\u7ec4\u7ec7\u662f\u5426\u4f1a\u9762\u4e34\u4ece\u751f\u6210\u5230\u90e8\u7f72\u540e\u7ef4\u62a4\u7684\u8d1f\u62c5\u8f6c\u79fb\u98ce\u9669", "method": "\u901a\u8fc7\u5bf9201\u4e2a\u5f00\u6e90\u9879\u76ee\u8fdb\u884c\u751f\u5b58\u5206\u6790\uff0c\u8ffd\u8e2a\u8d85\u8fc720\u4e07\u4e2a\u7531AI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u7f16\u5199\u7684\u4ee3\u7801\u5355\u5143\uff0c\u6bd4\u8f83\u4fee\u6539\u7387\u3001\u5371\u9669\u6bd4\u548c\u4fee\u6539\u6a21\u5f0f", "result": "\u4e0e\"\u4e00\u6b21\u6027\u4ee3\u7801\"\u53d9\u4e8b\u76f8\u53cd\uff0c\u4ee3\u7406\u7f16\u5199\u7684\u4ee3\u7801\u5b58\u6d3b\u65f6\u95f4\u663e\u8457\u66f4\u957f\uff1a\u884c\u7ea7\u4fee\u6539\u7387\u4f4e15.8\u4e2a\u767e\u5206\u70b9\uff0c\u4fee\u6539\u5371\u9669\u6bd4\u4f4e16%\uff08HR=0.842\uff0cp<0.001\uff09\u3002\u4ee3\u7406\u4ee3\u7801\u4fee\u6b63\u7387\u7565\u9ad8\uff0826.3% vs 23.0%\uff09\uff0c\u4f46\u6548\u5e94\u91cf\u5c0f\uff0c\u4ee3\u7406\u95f4\u5dee\u5f02\u5927\u4e8e\u4eba\u673a\u5dee\u5f02", "conclusion": "AI\u4ee3\u7406\u751f\u6210\u4ee3\u7801\u7684\u74f6\u9888\u53ef\u80fd\u4e0d\u662f\u751f\u6210\u8d28\u91cf\uff0c\u800c\u662f\u7ba1\u7406\u5176\u957f\u671f\u6f14\u8fdb\u7684\u7ec4\u7ec7\u5b9e\u8df5\u3002\u6587\u672c\u7279\u5f81\u53ef\u8bc6\u522b\u6613\u4fee\u6539\u4ee3\u7801\uff0c\u4f46\u9884\u6d4b\u4fee\u6539\u65f6\u673a\u4ecd\u5177\u6311\u6218\u6027", "topic": "agent analysis"}}
{"id": "2601.16964", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16964", "abs": "https://arxiv.org/abs/2601.16964", "authors": ["Mohamed Amine Ferrag", "Abderrahmane Lakas", "Merouane Debbah"], "title": "AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems", "comment": "16 pages", "summary": "The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for training, fine-tuning, and evaluating autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes: scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density. An LLM-driven prompt-to-JSON pipeline generates semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question multiple-choice benchmark spanning five reasoning dimensions: physics, policy, hybrid, scenario, and comparative reasoning. We conduct a large-scale evaluation of fifty leading LLMs on AgentDrive-MCQ. Results show that while proprietary frontier models perform best in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. We release the AgentDrive dataset, AgentDrive-MCQ benchmark, evaluation code, and related materials at https://github.com/maferrag/AgentDrive", "AI": {"tldr": "AgentDrive\u662f\u4e00\u4e2a\u5305\u542b30\u4e07\u4e2aLLM\u751f\u6210\u9a7e\u9a76\u573a\u666f\u7684\u5f00\u653e\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u667a\u80fd\u4f53\uff0c\u5e76\u9644\u5e26\u5305\u542b10\u4e07\u4e2a\u591a\u9879\u9009\u62e9\u9898\u7684AgentDrive-MCQ\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u9a7e\u9a76\u76f8\u5173\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u7ed3\u6784\u5316\u4e14\u5173\u6ce8\u5b89\u5168\u6027\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u548c\u8bad\u7ec3\u8fd9\u7c7b\u667a\u80fd\u4f53AI\u6a21\u578b\uff0c\u8fd9\u963b\u788d\u4e86\u81ea\u52a8\u9a7e\u9a76\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7LLM\u9a71\u52a8\u7684prompt-to-JSON\u7ba1\u9053\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u4eff\u771f\u5c31\u7eea\u573a\u666f\u89c4\u8303\uff0c\u5728\u4e03\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\u4e0a\u56e0\u5b50\u5316\u573a\u666f\u7a7a\u95f4\uff1a\u573a\u666f\u7c7b\u578b\u3001\u9a7e\u9a76\u5458\u884c\u4e3a\u3001\u73af\u5883\u3001\u9053\u8def\u5e03\u5c40\u3001\u76ee\u6807\u3001\u96be\u5ea6\u548c\u4ea4\u901a\u5bc6\u5ea6\u3002\u6bcf\u4e2a\u573a\u666f\u90fd\u7ecf\u8fc7\u4eff\u771f\u63a8\u6f14\u3001\u4ee3\u7406\u5b89\u5168\u6307\u6807\u8ba1\u7b97\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7ed3\u679c\u6807\u6ce8\u3002\u540c\u65f6\u521b\u5efa\u4e86\u5305\u542b10\u4e07\u4e2a\u591a\u9879\u9009\u62e9\u9898\u7684AgentDrive-MCQ\u57fa\u51c6\uff0c\u6db5\u76d6\u7269\u7406\u3001\u7b56\u7565\u3001\u6df7\u5408\u3001\u573a\u666f\u548c\u6bd4\u8f83\u63a8\u7406\u4e94\u4e2a\u7ef4\u5ea6\u3002", "result": "\u5bf950\u4e2a\u9886\u5148LLM\u5728AgentDrive-MCQ\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4e13\u6709\u524d\u6cbf\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u548c\u7b56\u7565\u63a8\u7406\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5148\u8fdb\u5f00\u6e90\u6a21\u578b\u5728\u7ed3\u6784\u5316\u548c\u7269\u7406\u57fa\u7840\u63a8\u7406\u65b9\u9762\u6b63\u5728\u8fc5\u901f\u7f29\u5c0f\u5dee\u8ddd\u3002", "conclusion": "AgentDrive\u4e3a\u81ea\u52a8\u9a7e\u9a76\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u7ed3\u6784\u5316\u7684\u57fa\u51c6\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u667a\u80fd\u4f53AI\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002", "topic": "swe benchmark"}}
{"id": "2601.16839", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.16839", "abs": "https://arxiv.org/abs/2601.16839", "authors": ["Anwar Ghammam", "Mohamed Almukhtar"], "title": "AI builds, We Analyze: An Empirical Study of AI-Generated Build Code Quality", "comment": null, "summary": "The rapid adoption of AI coding agents for software development has raised important questions about the quality and maintainability of the code they produce. While prior studies have examined AI-generated source code, the impact of AI coding agents on build systems-a critical yet understudied component of the software lifecycle-remains largely unexplored. This data mining challenge focuses on AIDev, the first large-scale, openly available dataset capturing agent-authored pull requests (Agentic-PRs) from real-world GitHub repositories. Our paper leverages this dataset to investigate (RQ1) whether AI coding agents generate build code with quality issues (e.g., code smells), (RQ2) to what extent AI agents can eliminate code smells from build code, and (RQ3) to what extent Agentic-PRs are accepted by developers. We identified 364 maintainability and security-related build smells across varying severity levels, indicating that AI-generated build code can introduce quality issues-such as lack of error handling, and hardcoded paths or URLs-while also, in some cases, removing existing smells through refactorings (e.g., Pull Up Module and Externalize Properties). Notably, more than 61\\% of Agentic-PRs are approved and merged with minimal human intervention. This dual impact underscores the need for future research on AI-aware build code quality assessment to systematically evaluate, guide, and govern AI-generated build systems code.", "AI": {"tldr": "AI\u7f16\u7801\u4ee3\u7406\u5728\u6784\u5efa\u7cfb\u7edf\u4e2d\u4ea7\u751f\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\uff0c\u4f46\u4e5f\u80fd\u6d88\u9664\u73b0\u6709\u4ee3\u7801\u5f02\u5473\uff0c\u8d85\u8fc761%\u7684AI\u751f\u6210PR\u88ab\u63a5\u53d7\u5408\u5e76", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bf9\u5176\u751f\u6210\u7684\u6784\u5efa\u7cfb\u7edf\u4ee3\u7801\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027\u5f71\u54cd\u7814\u7a76\u4e0d\u8db3\u3002\u6784\u5efa\u7cfb\u7edf\u662f\u8f6f\u4ef6\u751f\u547d\u5468\u671f\u4e2d\u5173\u952e\u4f46\u88ab\u5ffd\u89c6\u7684\u7ec4\u4ef6\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\uff08\u9996\u4e2a\u5927\u89c4\u6a21\u516c\u5f00\u7684AI\u4ee3\u7406\u751f\u6210PR\u6570\u636e\u96c6\uff09\uff0c\u901a\u8fc7\u6570\u636e\u6316\u6398\u65b9\u6cd5\u7814\u7a76\u4e09\u4e2a\u95ee\u9898\uff1aAI\u662f\u5426\u751f\u6210\u6709\u8d28\u91cf\u95ee\u9898\u7684\u6784\u5efa\u4ee3\u7801\u3001AI\u80fd\u5426\u6d88\u9664\u6784\u5efa\u4ee3\u7801\u5f02\u5473\u3001AI\u751f\u6210\u7684PR\u88ab\u5f00\u53d1\u8005\u63a5\u53d7\u7a0b\u5ea6\u3002", "result": "\u8bc6\u522b\u51fa364\u4e2a\u53ef\u7ef4\u62a4\u6027\u548c\u5b89\u5168\u6027\u76f8\u5173\u7684\u6784\u5efa\u4ee3\u7801\u5f02\u5473\uff0cAI\u751f\u6210\u7684\u6784\u5efa\u4ee3\u7801\u5b58\u5728\u8d28\u91cf\u95ee\u9898\uff08\u5982\u7f3a\u4e4f\u9519\u8bef\u5904\u7406\u3001\u786c\u7f16\u7801\u8def\u5f84\uff09\uff0c\u4f46\u4e5f\u80fd\u901a\u8fc7\u91cd\u6784\u6d88\u9664\u73b0\u6709\u5f02\u5473\u3002\u8d85\u8fc761%\u7684AI\u751f\u6210PR\u88ab\u6279\u51c6\u5408\u5e76\uff0c\u4eba\u7c7b\u5e72\u9884\u8f83\u5c11\u3002", "conclusion": "AI\u7f16\u7801\u4ee3\u7406\u5bf9\u6784\u5efa\u7cfb\u7edf\u4ee3\u7801\u8d28\u91cf\u6709\u53cc\u91cd\u5f71\u54cd\uff1a\u65e2\u5f15\u5165\u65b0\u95ee\u9898\uff0c\u4e5f\u80fd\u6539\u8fdb\u73b0\u6709\u4ee3\u7801\u3002\u9700\u8981\u672a\u6765\u7814\u7a76\u5f00\u53d1AI\u611f\u77e5\u7684\u6784\u5efa\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u548c\u6cbb\u7406AI\u751f\u6210\u7684\u6784\u5efa\u7cfb\u7edf\u4ee3\u7801\u3002", "topic": "swe application"}}
{"id": "2601.16965", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16965", "abs": "https://arxiv.org/abs/2601.16965", "authors": ["Riyang Bao", "Cheng Yang", "Dazhou Yu", "Zhexiang Tang", "Gengchen Mai", "Liang Zhao"], "title": "Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts", "comment": "15pages, 4 figures", "summary": "Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.", "AI": {"tldr": "Spatial-Agent\u662f\u4e00\u4e2a\u57fa\u4e8e\u7a7a\u95f4\u4fe1\u606f\u79d1\u5b66\u7406\u8bba\u7684AI\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5c06\u5730\u7406\u5206\u6790\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6982\u5ff5\u8f6c\u6362\u95ee\u9898\uff0c\u751f\u6210\u53ef\u6267\u884c\u7684GeoFlow\u56fe\u5de5\u4f5c\u6d41\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u5730\u7406\u7a7a\u95f4\u8ba1\u7b97\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u901a\u5e38\u4f9d\u8d56\u7f51\u7edc\u641c\u7d22\u6216\u6a21\u5f0f\u5339\u914d\uff0c\u5bb9\u6613\u4ea7\u751f\u7a7a\u95f4\u5173\u7cfb\u5e7b\u89c9\u3002\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u5bf9\u4e8e\u57ce\u5e02\u5206\u6790\u3001\u4ea4\u901a\u89c4\u5212\u548c\u707e\u5bb3\u54cd\u5e94\u7b49\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u7a7a\u95f4\u4fe1\u606f\u79d1\u5b66\u57fa\u7840\u7406\u8bba\uff0c\u5c06\u5730\u7406\u5206\u6790\u95ee\u7b54\u5f62\u5f0f\u5316\u4e3a\u6982\u5ff5\u8f6c\u6362\u95ee\u9898\uff1a\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u89e3\u6790\u4e3a\u53ef\u6267\u884c\u7684\u5de5\u4f5c\u6d41\uff0c\u8868\u793a\u4e3aGeoFlow\u56fe\uff08\u6709\u5411\u65e0\u73af\u56fe\uff0c\u8282\u70b9\u5bf9\u5e94\u7a7a\u95f4\u6982\u5ff5\uff0c\u8fb9\u8868\u793a\u8f6c\u6362\uff09\u3002\u63d0\u53d6\u7a7a\u95f4\u6982\u5ff5\uff0c\u5206\u914d\u529f\u80fd\u89d2\u8272\u5e76\u5e94\u7528\u6392\u5e8f\u7ea6\u675f\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u677f\u7684\u751f\u6210\u7ec4\u5408\u8f6c\u6362\u5e8f\u5217\u3002", "result": "\u5728MapEval-API\u548cMapQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSpatial-Agent\u663e\u8457\u4f18\u4e8e\u5305\u62ecReAct\u548cReflexion\u5728\u5185\u7684\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4ea7\u751f\u53ef\u89e3\u91ca\u4e14\u53ef\u6267\u884c\u7684\u5730\u7406\u7a7a\u95f4\u5de5\u4f5c\u6d41\u3002", "conclusion": "Spatial-Agent\u901a\u8fc7\u5c06\u7a7a\u95f4\u4fe1\u606f\u79d1\u5b66\u7406\u8bba\u878d\u5165AI\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u5730\u7406\u7a7a\u95f4\u8ba1\u7b97\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2601.16881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.16881", "abs": "https://arxiv.org/abs/2601.16881", "authors": ["Ian Gauk", "Doriane Olewicki", "Joshua Romoff", "Cor-Paul Bezemer"], "title": "Assessing the Feasibility of Selective Instrumentation for Runtime Code Coverage in Large C++ Game Engines", "comment": null, "summary": "Code coverage is a valuable guide for testing, but in AAA games the overhead of instrumentation conflicts with strict performance requirements and can destabilize automated tests. We propose and assess a selective instrumentation approach tailored to large game engines written in \\texttt{C++}, which reduces the scope of instrumentation while preserving relevant coverage data to developer commits. Our framework integrates into an industrial game testing pipeline, enabling developers to receive immediate coverage feedback on tests run against their changes. The compilation overhead of our approach is minimal, allowing instrumentation of over 2,000 commits before doubling build time. In performance evaluations, even the worst-case scenario maintains frame rates above 50\\% of the non-instrumented baseline. Across two production test suites maintained by our industry partner, our framework caused no automated test failures, avoiding the instability observed under full instrumentation. Our work shows that commit-level or build-level coverage of large \\texttt{C++} game engines can be achieved with minimal overhead and without compromising test stability.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578bC++\u6e38\u620f\u5f15\u64ce\u63d0\u51fa\u9009\u62e9\u6027\u4ee3\u7801\u8986\u76d6\u7387\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8986\u76d6\u7387\u6570\u636e\u76f8\u5173\u6027\u7684\u540c\u65f6\u51cf\u5c11\u68c0\u6d4b\u8303\u56f4\uff0c\u89e3\u51b3\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u6027\u80fd\u5f00\u9500\u5927\u3001\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u5728AAA\u6e38\u620f\u5f00\u53d1\u4e2d\uff0c\u4f20\u7edf\u7684\u4ee3\u7801\u8986\u76d6\u7387\u68c0\u6d4b\u5de5\u5177\u4f1a\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u5f00\u9500\uff0c\u8fd9\u4e0e\u4e25\u683c\u7684\u6027\u80fd\u8981\u6c42\u76f8\u51b2\u7a81\uff0c\u5e76\u4e14\u53ef\u80fd\u7834\u574f\u81ea\u52a8\u5316\u6d4b\u8bd5\u7684\u7a33\u5b9a\u6027\u3002\u6e38\u620f\u5f15\u64ce\u901a\u5e38\u89c4\u6a21\u5e9e\u5927\uff0c\u5168\u91cf\u68c0\u6d4b\u4f1a\u5bfc\u81f4\u7f16\u8bd1\u65f6\u95f4\u5927\u5e45\u589e\u52a0\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5927\u578bC++\u6e38\u620f\u5f15\u64ce\u7684\u9009\u62e9\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u68c0\u6d4b\u8303\u56f4\u4f46\u4fdd\u7559\u4e0e\u5f00\u53d1\u8005\u63d0\u4ea4\u76f8\u5173\u7684\u8986\u76d6\u7387\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u96c6\u6210\u5230\u5de5\u4e1a\u7ea7\u6e38\u620f\u6d4b\u8bd5\u6d41\u6c34\u7ebf\u4e2d\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u5373\u65f6\u83b7\u5f97\u9488\u5bf9\u5176\u53d8\u66f4\u7684\u6d4b\u8bd5\u8986\u76d6\u7387\u53cd\u9988\u3002", "result": "\u7f16\u8bd1\u5f00\u9500\u6781\u5c0f\uff0c\u5728\u68c0\u6d4b\u8d85\u8fc72000\u6b21\u63d0\u4ea4\u540e\u624d\u4f1a\u4f7f\u6784\u5efa\u65f6\u95f4\u7ffb\u500d\u3002\u6027\u80fd\u8bc4\u4f30\u663e\u793a\u5373\u4f7f\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u630150%\u4ee5\u4e0a\u7684\u975e\u68c0\u6d4b\u57fa\u51c6\u5e27\u7387\u3002\u5728\u4e24\u4e2a\u751f\u4ea7\u6d4b\u8bd5\u5957\u4ef6\u4e2d\uff0c\u8be5\u65b9\u6cd5\u672a\u5bfc\u81f4\u4efb\u4f55\u81ea\u52a8\u5316\u6d4b\u8bd5\u5931\u8d25\uff0c\u907f\u514d\u4e86\u5168\u91cf\u68c0\u6d4b\u65f6\u89c2\u5bdf\u5230\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9\u5927\u578bC++\u6e38\u620f\u5f15\u64ce\u7684\u63d0\u4ea4\u7ea7\u6216\u6784\u5efa\u7ea7\u4ee3\u7801\u8986\u76d6\u7387\u68c0\u6d4b\u53ef\u4ee5\u5728\u6700\u5c0f\u5f00\u9500\u4e0b\u5b9e\u73b0\uff0c\u4e14\u4e0d\u4f1a\u635f\u5bb3\u6d4b\u8bd5\u7a33\u5b9a\u6027\uff0c\u4e3a\u6e38\u620f\u5f00\u53d1\u4e2d\u7684\u8986\u76d6\u7387\u6307\u5bfc\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2601.16407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16407", "abs": "https://arxiv.org/abs/2601.16407", "authors": ["Toni J. B. Liu", "Baran Zadeo\u011flu", "Nicolas Boull\u00e9", "Rapha\u00ebl Sarfati", "Christopher J. Earls"], "title": "Jacobian Scopes: token-level causal attributions in LLMs", "comment": "12 pages, 15 figures, under review at ACL 2026", "summary": "Large language models (LLMs) make next-token predictions based on clues present in their context, such as semantic descriptions and in-context examples. Yet, elucidating which prior tokens most strongly influence a given prediction remains challenging due to the proliferation of layers and attention heads in modern architectures. We propose Jacobian Scopes, a suite of gradient-based, token-level causal attribution methods for interpreting LLM predictions. By analyzing the linearized relations of final hidden state with respect to inputs, Jacobian Scopes quantify how input tokens influence a model's prediction. We introduce three variants - Semantic, Fisher, and Temperature Scopes - which respectively target sensitivity of specific logits, the full predictive distribution, and model confidence (inverse temperature). Through case studies spanning instruction understanding, translation and in-context learning (ICL), we uncover interesting findings, such as when Jacobian Scopes point to implicit political biases. We believe that our proposed methods also shed light on recently debated mechanisms underlying in-context time-series forecasting. Our code and interactive demonstrations are publicly available at https://github.com/AntonioLiu97/JacobianScopes.", "AI": {"tldr": "\u63d0\u51faJacobian Scopes\uff0c\u4e00\u5957\u57fa\u4e8e\u68af\u5ea6\u7684token\u7ea7\u56e0\u679c\u5f52\u56e0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91caLLM\u9884\u6d4b\uff0c\u901a\u8fc7\u5206\u6790\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u5bf9\u8f93\u5165\u7684\u7ebf\u6027\u5316\u5173\u7cfb\u6765\u91cf\u5316\u8f93\u5165token\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u73b0\u4ee3\u67b6\u6784\u4e2d\u5c42\u548c\u6ce8\u610f\u529b\u5934\u7684\u6fc0\u589e\uff0c\u9610\u660e\u54ea\u4e9b\u5148\u524dtoken\u5bf9\u7279\u5b9a\u9884\u6d4b\u5f71\u54cd\u6700\u5927\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u5f52\u56e0\u65b9\u6cd5\u6765\u89e3\u91caLLM\u9884\u6d4b\u3002", "method": "\u63d0\u51faJacobian Scopes\u65b9\u6cd5\u5957\u4ef6\uff0c\u5305\u62ec\u4e09\u4e2a\u53d8\u4f53\uff1aSemantic Scope\uff08\u9488\u5bf9\u7279\u5b9alogits\u7684\u654f\u611f\u6027\uff09\u3001Fisher Scope\uff08\u9488\u5bf9\u5b8c\u6574\u9884\u6d4b\u5206\u5e03\uff09\u548cTemperature Scope\uff08\u9488\u5bf9\u6a21\u578b\u7f6e\u4fe1\u5ea6/\u9006\u6e29\u5ea6\uff09\u3002\u901a\u8fc7\u5206\u6790\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u5bf9\u8f93\u5165\u7684\u7ebf\u6027\u5316\u5173\u7cfb\u6765\u91cf\u5316\u8f93\u5165token\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u6307\u4ee4\u7406\u89e3\u3001\u7ffb\u8bd1\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b49\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0\u4e86\u6709\u8da3\u7684\u53d1\u73b0\uff0c\u4f8b\u5982Jacobian Scopes\u63ed\u793a\u4e86\u9690\u542b\u7684\u653f\u6cbb\u504f\u89c1\u3002\u8be5\u65b9\u6cd5\u4e5f\u4e3a\u6700\u8fd1\u4e89\u8bba\u7684\u4e0a\u4e0b\u6587\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u673a\u5236\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "Jacobian Scopes\u662f\u4e00\u5957\u6709\u6548\u7684\u68af\u5ea6\u57fatoken\u7ea7\u56e0\u679c\u5f52\u56e0\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u91caLLM\u9884\u6d4b\uff0c\u63ed\u793a\u8f93\u5165token\u7684\u5f71\u54cd\u673a\u5236\uff0c\u4e3a\u6a21\u578b\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.16447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16447", "abs": "https://arxiv.org/abs/2601.16447", "authors": ["Yichuan Ma", "Linyang Li", "Yongkang Chen", "Peiji Li", "Jiasheng Ye", "Qipeng Guo", "Dahua Lin", "Kai Chen"], "title": "Mixing Expert Knowledge: Bring Human Thoughts Back To the Game of Go", "comment": "Accepted to NeurIPS 2025", "summary": "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks such as mathematics and coding, matching or surpassing human capabilities. However, these impressive reasoning abilities face significant challenges in specialized domains. Taking Go as an example, although AlphaGo has established the high performance ceiling of AI systems in Go, mainstream LLMs still struggle to reach even beginner-level proficiency, let alone perform natural language reasoning. This performance gap between general-purpose LLMs and domain experts is significantly limiting the application of LLMs on a wider range of domain-specific tasks. In this work, we aim to bridge the divide between LLMs' general reasoning capabilities and expert knowledge in domain-specific tasks. We perform mixed fine-tuning with structured Go expertise and general long Chain-of-Thought (CoT) reasoning data as a cold start, followed by reinforcement learning to integrate expert knowledge in Go with general reasoning capabilities. Through this methodology, we present \\textbf{LoGos}, a powerful LLM that not only maintains outstanding general reasoning abilities, but also conducts Go gameplay in natural language, demonstrating effective strategic reasoning and accurate next-move prediction. LoGos achieves performance comparable to human professional players, substantially surpassing all existing LLMs. Through this work, we aim to contribute insights on applying general LLM reasoning capabilities to specialized domains. We will release the first large-scale Go dataset for LLM training, the first LLM Go evaluation benchmark, and the first general LLM that reaches human professional-level performance in Go at: https://github.com/Entarochuan/LoGos.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LoGos\uff0c\u4e00\u4e2a\u901a\u8fc7\u6df7\u5408\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5c06\u901a\u7528LLM\u63a8\u7406\u80fd\u529b\u4e0e\u56f4\u68cb\u4e13\u4e1a\u77e5\u8bc6\u7ed3\u5408\u7684\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u8fbe\u5230\u4eba\u7c7b\u804c\u4e1a\u6c34\u5e73\u7684\u56f4\u68cb\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u56f4\u68cb\u7b49\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u8fbe\u4e0d\u5230\u521d\u7ea7\u6c34\u5e73\u3002\u8fd9\u79cd\u901a\u7528LLM\u4e0e\u9886\u57df\u4e13\u5bb6\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u9650\u5236\u4e86LLM\u5728\u66f4\u5e7f\u6cdb\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5fae\u8c03\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u7ed3\u6784\u5316\u56f4\u68cb\u4e13\u4e1a\u77e5\u8bc6\u548c\u901a\u7528\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u6570\u636e\u8fdb\u884c\u51b7\u542f\u52a8\u5fae\u8c03\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u56f4\u68cb\u4e13\u5bb6\u77e5\u8bc6\u4e0e\u901a\u7528\u63a8\u7406\u80fd\u529b\u76f8\u7ed3\u5408\u3002", "result": "LoGos\u6a21\u578b\u4e0d\u4ec5\u4fdd\u6301\u4e86\u51fa\u8272\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u80fd\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u56f4\u68cb\u5bf9\u5f08\uff0c\u8868\u73b0\u51fa\u6709\u6548\u7684\u6218\u7565\u63a8\u7406\u548c\u51c6\u786e\u7684\u4e0b\u6b65\u9884\u6d4b\u3002\u5176\u6027\u80fd\u8fbe\u5230\u4eba\u7c7b\u804c\u4e1a\u73a9\u5bb6\u6c34\u5e73\uff0c\u663e\u8457\u8d85\u8d8a\u6240\u6709\u73b0\u6709LLM\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u901a\u7528LLM\u63a8\u7406\u80fd\u529b\u5e94\u7528\u4e8e\u4e13\u4e1a\u9886\u57df\uff0c\u5e76\u8d21\u732e\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u56f4\u68cb\u8bad\u7ec3\u6570\u636e\u96c6\u3001\u9996\u4e2aLLM\u56f4\u68cb\u8bc4\u4f30\u57fa\u51c6\u548c\u9996\u4e2a\u8fbe\u5230\u4eba\u7c7b\u804c\u4e1a\u6c34\u5e73\u7684\u901a\u7528LLM\u56f4\u68cb\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.16466", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16466", "abs": "https://arxiv.org/abs/2601.16466", "authors": ["Jivnesh Sandhan", "Fei Cheng", "Tushar Sandhan", "Yugo Murawaki"], "title": "Persona Jailbreaking in Large Language Models", "comment": "Accepted at EACL26 (Findings)", "summary": "Large Language Models (LLMs) are increasingly deployed in domains such as education, mental health and customer support, where stable and consistent personas are critical for reliability. Yet, existing studies focus on narrative or role-playing tasks and overlook how adversarial conversational history alone can reshape induced personas. Black-box persona manipulation remains unexplored, raising concerns for robustness in realistic interactions. In response, we introduce the task of persona editing, which adversarially steers LLM traits through user-side inputs under a black-box, inference-only setting. To this end, we propose PHISH (Persona Hijacking via Implicit Steering in History), the first framework to expose a new vulnerability in LLM safety that embeds semantically loaded cues into user queries to gradually induce reverse personas. We also define a metric to quantify attack success. Across 3 benchmarks and 8 LLMs, PHISH predictably shifts personas, triggers collateral changes in correlated traits, and exhibits stronger effects in multi-turn settings. In high-risk domains mental health, tutoring, and customer support, PHISH reliably manipulates personas, validated by both human and LLM-as-Judge evaluations. Importantly, PHISH causes only a small reduction in reasoning benchmark performance, leaving overall utility largely intact while still enabling significant persona manipulation. While current guardrails offer partial protection, they remain brittle under sustained attack. Our findings expose new vulnerabilities in personas and highlight the need for context-resilient persona in LLMs. Our codebase and dataset is available at: https://github.com/Jivnesh/PHISH", "AI": {"tldr": "PHISH\u6846\u67b6\u901a\u8fc7\u7528\u6237\u67e5\u8be2\u4e2d\u7684\u8bed\u4e49\u6697\u793a\uff0c\u5728\u9ed1\u76d2\u63a8\u7406\u8bbe\u7f6e\u4e0b\u9010\u6b65\u8bf1\u5bfcLLMs\u5f62\u6210\u53cd\u5411\u4eba\u683c\uff0c\u66b4\u9732\u4e86LLM\u4eba\u683c\u5b89\u5168\u7684\u65b0\u6f0f\u6d1e", "motivation": "LLMs\u5728\u6559\u80b2\u3001\u5fc3\u7406\u5065\u5eb7\u548c\u5ba2\u6237\u652f\u6301\u7b49\u9886\u57df\u90e8\u7f72\u65f6\uff0c\u7a33\u5b9a\u4e00\u81f4\u7684\u4eba\u683c\u5bf9\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u5173\u6ce8\u53d9\u4e8b\u6216\u89d2\u8272\u626e\u6f14\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u5bf9\u6297\u6027\u5bf9\u8bdd\u5386\u53f2\u5982\u4f55\u91cd\u5851\u8bf1\u5bfc\u4eba\u683c\u3002\u9ed1\u76d2\u4eba\u683c\u64cd\u7eb5\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u5f15\u53d1\u4e86\u5bf9\u73b0\u5b9e\u4ea4\u4e92\u4e2d\u9c81\u68d2\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u63d0\u51faPHISH\uff08Persona Hijacking via Implicit Steering in History\uff09\u6846\u67b6\uff0c\u5728\u7528\u6237\u67e5\u8be2\u4e2d\u5d4c\u5165\u8bed\u4e49\u6697\u793a\uff0c\u5728\u9ed1\u76d2\u3001\u4ec5\u63a8\u7406\u7684\u8bbe\u7f6e\u4e0b\u9010\u6b65\u8bf1\u5bfc\u53cd\u5411\u4eba\u683c\u3002\u5b9a\u4e49\u4e86\u91cf\u5316\u653b\u51fb\u6210\u529f\u7684\u6307\u6807\uff0c\u5e76\u57283\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c8\u4e2aLLMs\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "PHISH\u53ef\u9884\u6d4b\u5730\u6539\u53d8\u4eba\u683c\uff0c\u89e6\u53d1\u76f8\u5173\u7279\u5f81\u7684\u8fde\u5e26\u53d8\u5316\uff0c\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6548\u679c\u66f4\u5f3a\u3002\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5fc3\u7406\u5065\u5eb7\u3001\u8f85\u5bfc\u3001\u5ba2\u6237\u652f\u6301\uff09\u4e2d\u53ef\u9760\u5730\u64cd\u7eb5\u4eba\u683c\uff0c\u4eba\u7c7b\u548cLLM-as-Judge\u8bc4\u4f30\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002PHISH\u4ec5\u8f7b\u5fae\u964d\u4f4e\u63a8\u7406\u57fa\u51c6\u6027\u80fd\uff0c\u6574\u4f53\u5b9e\u7528\u6027\u57fa\u672c\u4fdd\u6301\u5b8c\u6574\u3002", "conclusion": "\u5f53\u524d\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u63d0\u4f9b\u90e8\u5206\u4fdd\u62a4\uff0c\u4f46\u5728\u6301\u7eed\u653b\u51fb\u4e0b\u4ecd\u7136\u8106\u5f31\u3002\u7814\u7a76\u7ed3\u679c\u66b4\u9732\u4e86LLM\u4eba\u683c\u7684\u65b0\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u4e0a\u4e0b\u6587\u5f39\u6027\u7684\u4eba\u683c\u8bbe\u8ba1\u3002", "topic": "agent analysis"}}
{"id": "2601.16478", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16478", "abs": "https://arxiv.org/abs/2601.16478", "authors": ["Haotian Chen", "Qingqing Long", "Siyu Pu", "Xiao Luo", "Wei Ju", "Meng Xiao", "Yuanchun Zhou", "Jianghua Zhao", "Xuezhi Wang"], "title": "DeepEra: A Deep Evidence Reranking Agent for Scientific Retrieval-Augmented Generated Question Answering", "comment": null, "summary": "With the rapid growth of scientific literature, scientific question answering (SciQA) has become increasingly critical for exploring and utilizing scientific knowledge. Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating knowledge from external sources, thereby providing credible evidence for scientific question answering. But existing retrieval and reranking methods remain vulnerable to passages that are semantically similar but logically irrelevant, often reducing factual reliability and amplifying hallucinations.To address this challenge, we propose a Deep Evidence Reranking Agent (DeepEra) that integrates step-by-step reasoning, enabling more precise evaluation of candidate passages beyond surface-level semantics. To support systematic evaluation, we construct SciRAG-SSLI (Scientific RAG - Semantically Similar but Logically Irrelevant), a large-scale dataset comprising about 300K SciQA instances across 10 subjects, constructed from 10M scientific corpus. The dataset combines naturally retrieved contexts with systematically generated distractors to test logical robustness and factual grounding. Comprehensive evaluations confirm that our approach achieves superior retrieval performance compared to leading rerankers. To our knowledge, this work is the first to comprehensively study and empirically validate innegligible SSLI issues in two-stage RAG frameworks.", "AI": {"tldr": "\u63d0\u51faDeepEra\u65b9\u6cd5\u89e3\u51b3RAG\u4e2d\u8bed\u4e49\u76f8\u4f3c\u4f46\u903b\u8f91\u65e0\u5173(SSLI)\u95ee\u9898\uff0c\u6784\u5efaSciRAG-SSLI\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u5728\u79d1\u5b66\u95ee\u7b54\u4e2d\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u68c0\u7d22\u91cd\u6392\u5e8f\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u79d1\u5b66\u95ee\u7b54(SciQA)\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u73b0\u6709RAG\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u8bed\u4e49\u76f8\u4f3c\u4f46\u903b\u8f91\u65e0\u5173(SSLI)\u6bb5\u843d\u7684\u5f71\u54cd\uff0c\u8fd9\u4f1a\u964d\u4f4e\u4e8b\u5b9e\u53ef\u9760\u6027\u5e76\u52a0\u5267\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faDeep Evidence Reranking Agent (DeepEra)\uff0c\u6574\u5408\u9010\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u8868\u5c42\u8bed\u4e49\u5bf9\u5019\u9009\u6bb5\u843d\u8fdb\u884c\u66f4\u7cbe\u786e\u8bc4\u4f30\u3002\u6784\u5efaSciRAG-SSLI\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e2a\u5b66\u79d1\u7684\u7ea630\u4e07\u4e2aSciQA\u5b9e\u4f8b\uff0c\u7ed3\u5408\u81ea\u7136\u68c0\u7d22\u4e0a\u4e0b\u6587\u548c\u7cfb\u7edf\u751f\u6210\u7684\u5e72\u6270\u9879\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u9886\u5148\u7684\u91cd\u6392\u5e8f\u5668\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u68c0\u7d22\u6027\u80fd\u3002\u8fd9\u662f\u9996\u4e2a\u5168\u9762\u7814\u7a76\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u4e24\u9636\u6bb5RAG\u6846\u67b6\u4e2d\u4e0d\u53ef\u5ffd\u89c6\u7684SSLI\u95ee\u9898\u7684\u5de5\u4f5c\u3002", "conclusion": "DeepEra\u901a\u8fc7\u6574\u5408\u9010\u6b65\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86RAG\u4e2d\u7684SSLI\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u79d1\u5b66\u95ee\u7b54\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u548c\u903b\u8f91\u9c81\u68d2\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.16480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16480", "abs": "https://arxiv.org/abs/2601.16480", "authors": ["Peiji Li", "Linyang Li", "Handa Sun", "Wenjin Mai", "Yongkang Chen", "Xiaozhe Li", "Yue Shen", "Yichuan Ma", "Yiliu Sun", "Jiaxi Cao", "Zhishu He", "Bo Wang", "Xiaoqing Zheng", "Zhaori Bi", "Xipeng Qiu", "Qipeng Guo", "Kai Chen", "Dahua Lin"], "title": "TL-GRPO: Turn-Level RL for Reasoning-Guided Iterative Optimization", "comment": "Work in progress", "summary": "Large language models have demonstrated strong reasoning capabilities in complex tasks through tool integration, which is typically framed as a Markov Decision Process and optimized with trajectory-level RL algorithms such as GRPO. However, a common class of reasoning tasks, iterative optimization, presents distinct challenges: the agent interacts with the same underlying environment state across turns, and the value of a trajectory is determined by the best turn-level reward rather than cumulative returns. Existing GRPO-based methods cannot perform fine-grained, turn-level optimization in such settings, while black-box optimization methods discard prior knowledge and reasoning capabilities. To address this gap, we propose Turn-Level GRPO (TL-GRPO), a lightweight RL algorithm that performs turn-level group sampling for fine-grained optimization. We evaluate TL-GRPO on analog circuit sizing (ACS), a challenging scientific optimization task requiring multiple simulations and domain expertise. Results show that TL-GRPO outperforms standard GRPO and Bayesian optimization methods across various specifications. Furthermore, our 30B model trained with TL-GRPO achieves state-of-the-art performance on ACS tasks under same simulation budget, demonstrating both strong generalization and practical utility.", "AI": {"tldr": "TL-GRPO\uff1a\u9488\u5bf9\u8fed\u4ee3\u4f18\u5316\u4efb\u52a1\u7684\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u56de\u5408\u7ea7\u5206\u7ec4\u91c7\u6837\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4f18\u5316\uff0c\u5728\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u7b49\u79d1\u5b66\u4f18\u5316\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GRPO\u7b49\u8f68\u8ff9\u7ea7RL\u7b97\u6cd5\u65e0\u6cd5\u5904\u7406\u8fed\u4ee3\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u7279\u6b8a\u6311\u6218\uff1a\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u9762\u5bf9\u76f8\u540c\u73af\u5883\u72b6\u6001\uff0c\u4e14\u8f68\u8ff9\u4ef7\u503c\u7531\u6700\u4f73\u56de\u5408\u5956\u52b1\u800c\u975e\u7d2f\u79ef\u56de\u62a5\u51b3\u5b9a\u3002\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\u5219\u4e22\u5f03\u4e86\u5148\u9a8c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faTurn-Level GRPO (TL-GRPO)\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7RL\u7b97\u6cd5\uff0c\u901a\u8fc7\u56de\u5408\u7ea7\u5206\u7ec4\u91c7\u6837\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u9488\u5bf9\u8fed\u4ee3\u4f18\u5316\u4efb\u52a1\u8bbe\u8ba1\uff0c\u4fdd\u7559\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u8fd9\u4e00\u9700\u8981\u591a\u6b21\u4eff\u771f\u548c\u9886\u57df\u77e5\u8bc6\u7684\u79d1\u5b66\u4f18\u5316\u4efb\u52a1\u4e0a\uff0cTL-GRPO\u8d85\u8d8a\u4e86\u6807\u51c6GRPO\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u300230B\u6a21\u578b\u5728\u76f8\u540c\u4eff\u771f\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "TL-GRPO\u6709\u6548\u89e3\u51b3\u4e86\u8fed\u4ee3\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u7279\u6b8a\u6311\u6218\uff0c\u4e3a\u9700\u8981\u591a\u8f6e\u4ea4\u4e92\u548c\u7ec6\u7c92\u5ea6\u4f18\u5316\u7684\u79d1\u5b66\u8ba1\u7b97\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.16486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16486", "abs": "https://arxiv.org/abs/2601.16486", "authors": ["Yichuan Ma", "Linyang Li", "Yongkang chen", "Peiji Li", "Xiaozhe Li", "Qipeng Guo", "Dahua Lin", "Kai Chen"], "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic", "comment": "Under Review", "summary": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faTimely Machine\u6846\u67b6\uff0c\u5c06\u6d4b\u8bd5\u65f6\u95f4\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5b9e\u9645\u65f6\u949f\u65f6\u95f4\u800c\u975e\u751f\u6210\u957f\u5ea6\uff0c\u4ee5\u89e3\u51b3\u5de5\u5177\u8c03\u7528\u573a\u666f\u4e2d\u4f20\u7edf\u6d4b\u8bd5\u65f6\u95f4\u5b9a\u4e49\u5931\u6548\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86Timely-RL\u65b9\u6cd5\u6765\u589e\u5f3a\u6a21\u578b\u7684\u65f6\u95f4\u9884\u7b97\u610f\u8bc6\u3002", "motivation": "\u5728\u4ee3\u7406\u573a\u666f\u4e2d\uff0c\u9891\u7e41\u7684\u5de5\u5177\u8c03\u7528\u5bfc\u81f4\u4f20\u7edf\u57fa\u4e8e\u751f\u6210\u957f\u5ea6\u7684\u6d4b\u8bd5\u65f6\u95f4\u5b9a\u4e49\u5931\u6548\uff0c\u56e0\u4e3a\u5de5\u5177\u5ef6\u8fdf\u4f7f\u63a8\u7406\u65f6\u95f4\u4e0e\u751f\u6210\u957f\u5ea6\u89e3\u8026\u3002\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u6d4b\u8bd5\u65f6\u95f4\u4e3a\u5b9e\u9645\u65f6\u949f\u65f6\u95f4\uff0c\u5e76\u8ba9\u6a21\u578b\u80fd\u6839\u636e\u65f6\u95f4\u9884\u7b97\u52a8\u6001\u8c03\u6574\u7b56\u7565\u3002", "method": "\u63d0\u51faTimely Machine\u6846\u67b6\uff0c\u91cd\u65b0\u5b9a\u4e49\u6d4b\u8bd5\u65f6\u95f4\u4e3a\u5b9e\u9645\u65f6\u949f\u65f6\u95f4\uff1b\u521b\u5efaTimely-Eval\u57fa\u51c6\uff0c\u6db5\u76d6\u9ad8\u9891\u5de5\u5177\u8c03\u7528\u3001\u4f4e\u9891\u5de5\u5177\u8c03\u7528\u548c\u65f6\u95f4\u7ea6\u675f\u63a8\u7406\u4e09\u79cd\u573a\u666f\uff1b\u5f00\u53d1Timely-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u51b7\u542f\u52a8\u540e\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u65f6\u95f4\u89c4\u5212\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u5c0f\u6a21\u578b\u5728\u5feb\u901f\u53cd\u9988\u4e0b\u901a\u8fc7\u66f4\u591a\u4ea4\u4e92\u8868\u73b0\u66f4\u597d\uff0c\u5927\u6a21\u578b\u5728\u9ad8\u5ef6\u8fdf\u573a\u666f\u4e2d\u901a\u8fc7\u66f4\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5360\u4f18\uff1b\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u9002\u5e94\u65f6\u95f4\u9884\u7b97\uff1bTimely-RL\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u65f6\u95f4\u9884\u7b97\u610f\u8bc6\u548c\u5728Timely-Eval\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4ee3\u7406\u65f6\u4ee3\u7684\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u5b9e\u9645\u65f6\u949f\u65f6\u95f4\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u65f6\u95f4\u89c4\u5212\u80fd\u529b\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.16508", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16508", "abs": "https://arxiv.org/abs/2601.16508", "authors": ["Karl Neergaard", "Le Qiu", "Emmanuele Chersoni"], "title": "Is Length Really A Liability? An Evaluation of Multi-turn LLM Conversations using BoolQ", "comment": "4 pages plus 6 pages of bibliography and appendix", "summary": "Single-prompt evaluations dominate current LLM benchmarking, yet they fail to capture the conversational dynamics where real-world harm occurs. In this study, we examined whether conversation length affects response veracity by evaluating LLM performance on the BoolQ dataset under varying length and scaffolding conditions. Our results across three distinct LLMs revealed model-specific vulnerabilities that are invisible under single-turn testing. The length-dependent and scaffold-specific effects we observed demonstrate a fundamental limitation of static evaluations, as deployment-relevant vulnerabilities could only be spotted in a multi-turn conversational setting.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5355\u8f6e\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349LLM\u5728\u957f\u5bf9\u8bdd\u4e2d\u7684\u771f\u5b9e\u6027\u6f0f\u6d1e\uff0c\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u80fd\u63ed\u793a\u6a21\u578b\u7279\u5b9a\u5f31\u70b9", "motivation": "\u5f53\u524dLLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u5355\u8f6e\u63d0\u793a\u8bc4\u4f30\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5371\u5bb3\u5f80\u5f80\u53d1\u751f\u5728\u5bf9\u8bdd\u52a8\u6001\u4e2d\uff0c\u9700\u8981\u7814\u7a76\u5bf9\u8bdd\u957f\u5ea6\u662f\u5426\u5f71\u54cdLLM\u56de\u7b54\u7684\u771f\u5b9e\u6027", "method": "\u5728BoolQ\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e09\u79cd\u4e0d\u540cLLM\u5728\u4e0d\u540c\u5bf9\u8bdd\u957f\u5ea6\u548c\u652f\u67b6\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u5355\u8f6e\u6d4b\u8bd5\u4e0e\u591a\u8f6e\u5bf9\u8bdd\u8bbe\u7f6e", "result": "\u53d1\u73b0\u6a21\u578b\u7279\u5b9a\u7684\u6f0f\u6d1e\u5728\u5355\u8f6e\u6d4b\u8bd5\u4e2d\u4e0d\u53ef\u89c1\uff0c\u5bf9\u8bdd\u957f\u5ea6\u548c\u652f\u67b6\u6761\u4ef6\u5bf9LLM\u771f\u5b9e\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u90e8\u7f72\u76f8\u5173\u7684\u6f0f\u6d1e\u53ea\u80fd\u5728\u591a\u8f6e\u5bf9\u8bdd\u8bbe\u7f6e\u4e2d\u88ab\u53d1\u73b0", "conclusion": "\u9759\u6001\u8bc4\u4f30\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u5bf9\u4e8e\u53d1\u73b0LLM\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u8106\u5f31\u6027\u81f3\u5173\u91cd\u8981", "topic": "agent analysis"}}
{"id": "2601.16530", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16530", "abs": "https://arxiv.org/abs/2601.16530", "authors": ["Gaurav Maheshwari", "Kevin El Haddad"], "title": "Curate-Train-Refine: A Closed-Loop Agentic Framework for Zero Shot Classification", "comment": null, "summary": "Large language models (LLMs) and high-capacity encoders have advanced zero and few-shot classification, but their inference cost and latency limit practical deployment. We propose training lightweight text classifiers using dynamically generated supervision from an LLM. Our method employs an iterative, agentic loop in which the LLM curates training data, analyzes model successes and failures, and synthesizes targeted examples to address observed errors. This closed-loop generation and evaluation process progressively improves data quality and adapts it to the downstream classifier and task. Across four widely used benchmarks, our approach consistently outperforms standard zero and few-shot baselines. These results indicate that LLMs can serve effectively as data curators, enabling accurate and efficient classification without the operational cost of large-model deployment.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528LLM\u52a8\u6001\u751f\u6210\u76d1\u7763\u4fe1\u53f7\u6765\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6587\u672c\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u4ee3\u7406\u5faa\u73af\u8ba9LLM\u7b56\u5212\u8bad\u7ec3\u6570\u636e\u3001\u5206\u6790\u6a21\u578b\u8868\u73b0\u3001\u5408\u6210\u9488\u5bf9\u6027\u793a\u4f8b\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6807\u51c6\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u57fa\u7ebf\u3002", "motivation": "\u5c3d\u7ba1LLM\u548c\u9ad8\u5bb9\u91cf\u7f16\u7801\u5668\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u51c6\u786e\u6027\u53c8\u66f4\u9ad8\u6548\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u7684\u4ee3\u7406\u5faa\u73af\u65b9\u6cd5\uff1aLLM\u7b56\u5212\u8bad\u7ec3\u6570\u636e\uff0c\u5206\u6790\u6a21\u578b\u6210\u529f\u4e0e\u5931\u8d25\u6848\u4f8b\uff0c\u5408\u6210\u9488\u5bf9\u6027\u793a\u4f8b\u6765\u89e3\u51b3\u89c2\u5bdf\u5230\u7684\u9519\u8bef\u3002\u8fd9\u79cd\u95ed\u73af\u7684\u751f\u6210\u548c\u8bc4\u4f30\u8fc7\u7a0b\u9010\u6b65\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u4f7f\u5176\u9002\u5e94\u4e0b\u6e38\u5206\u7c7b\u5668\u548c\u4efb\u52a1\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u57fa\u7ebf\uff0c\u8868\u660eLLM\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u6570\u636e\u7b56\u5212\u8005\u3002", "conclusion": "LLM\u80fd\u591f\u4f5c\u4e3a\u6570\u636e\u7b56\u5212\u8005\uff0c\u5728\u4e0d\u90e8\u7f72\u5927\u578b\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u51c6\u786e\u9ad8\u6548\u7684\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u7684\u90e8\u7f72\u9650\u5236\u3002", "topic": "agent analysis"}}
{"id": "2601.16621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16621", "abs": "https://arxiv.org/abs/2601.16621", "authors": ["Xueyang Feng", "Weinan Gan", "Xu Chen", "Quanyu Dai", "Yong Liu"], "title": "How Does Personalized Memory Shape LLM Behavior? Benchmarking Rational Preference Utilization in Personalized Assistants", "comment": null, "summary": "Large language model (LLM)-powered assistants have recently integrated memory mechanisms that record user preferences, leading to more personalized and user-aligned responses. However, irrelevant personalized memories are often introduced into the context, interfering with the LLM's intent understanding. To comprehensively investigate the dual effects of personalization, we develop RPEval, a benchmark comprising a personalized intent reasoning dataset and a multi-granularity evaluation protocol. RPEval reveals the widespread phenomenon of irrational personalization in existing LLMs and, through error pattern analysis, illustrates its negative impact on user experience. Finally, we introduce RP-Reasoner, which treats memory utilization as a pragmatic reasoning process, enabling the selective integration of personalized information. Experimental results demonstrate that our method significantly outperforms carefully designed baselines on RPEval, and resolves 80% of the bad cases observed in a large-scale commercial personalized assistant, highlighting the potential of pragmatic reasoning to mitigate irrational personalization. Our benchmark is publicly available at https://github.com/XueyangFeng/RPEval.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RPEval\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30LLM\u4e2a\u6027\u5316\u8bb0\u5fc6\u673a\u5236\u4e2d\u7684\u975e\u7406\u6027\u4e2a\u6027\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86RP-Reasoner\u65b9\u6cd5\u901a\u8fc7\u8bed\u7528\u63a8\u7406\u9009\u62e9\u6027\u6574\u5408\u4e2a\u6027\u5316\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "LLM\u52a9\u624b\u96c6\u6210\u8bb0\u5fc6\u673a\u5236\u8bb0\u5f55\u7528\u6237\u504f\u597d\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u54cd\u5e94\uff0c\u4f46\u65e0\u5173\u7684\u4e2a\u6027\u5316\u8bb0\u5fc6\u4f1a\u5e72\u6270LLM\u7684\u610f\u56fe\u7406\u89e3\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u8fd9\u79cd\u53cc\u91cd\u6548\u5e94\u3002", "method": "\u5f00\u53d1\u4e86RPEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e2a\u6027\u5316\u610f\u56fe\u63a8\u7406\u6570\u636e\u96c6\u548c\u591a\u7c92\u5ea6\u8bc4\u4f30\u534f\u8bae\uff1b\u63d0\u51fa\u4e86RP-Reasoner\u65b9\u6cd5\uff0c\u5c06\u8bb0\u5fc6\u5229\u7528\u89c6\u4e3a\u8bed\u7528\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u4fe1\u606f\u7684\u9009\u62e9\u6027\u6574\u5408\u3002", "result": "RPEval\u63ed\u793a\u4e86\u73b0\u6709LLM\u4e2d\u666e\u904d\u5b58\u5728\u7684\u975e\u7406\u6027\u4e2a\u6027\u5316\u73b0\u8c61\uff1bRP-Reasoner\u5728RPEval\u4e0a\u663e\u8457\u4f18\u4e8e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5546\u4e1a\u4e2a\u6027\u5316\u52a9\u624b\u4e2d80%\u7684\u4e0d\u826f\u6848\u4f8b\u3002", "conclusion": "\u8bed\u7528\u63a8\u7406\u80fd\u6709\u6548\u7f13\u89e3\u975e\u7406\u6027\u4e2a\u6027\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u52a9\u624b\u7684\u7528\u6237\u4f53\u9a8c\uff0cRPEval\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.16690", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16690", "abs": "https://arxiv.org/abs/2601.16690", "authors": ["Xinze Li", "Ziyue Zhu", "Siyuan Liu", "Yubo Ma", "Yuhang Zang", "Yixin Cao", "Aixin Sun"], "title": "EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents", "comment": "25 pages", "summary": "We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent's own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.", "AI": {"tldr": "EMemBench\u662f\u4e00\u4e2a\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6e38\u620f\u8bc4\u4f30\u667a\u80fd\u4f53\u957f\u671f\u8bb0\u5fc6\u7684\u7a0b\u5e8f\u5316\u57fa\u51c6\uff0c\u80fd\u6839\u636e\u667a\u80fd\u4f53\u81ea\u8eab\u8f68\u8ff9\u751f\u6210\u95ee\u9898\uff0c\u8986\u76d6\u6587\u672c\u548c\u89c6\u89c9\u6e38\u620f\u73af\u5883\uff0c\u5305\u542b\u591a\u79cd\u8bb0\u5fc6\u6280\u80fd\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u667a\u80fd\u4f53\u957f\u671f\u8bb0\u5fc6\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u7279\u522b\u662f\u9700\u8981\u80fd\u591f\u8986\u76d6\u591a\u79cd\u8bb0\u5fc6\u6280\u80fd\uff08\u5982\u5355\u8df3/\u591a\u8df3\u56de\u5fc6\u3001\u5f52\u7eb3\u3001\u65f6\u7a7a\u63a8\u7406\u7b49\uff09\u4e14\u80fd\u9002\u5e94\u4e0d\u540c\u73af\u5883\uff08\u6587\u672c\u548c\u89c6\u89c9\uff09\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6e38\u620f\u751f\u6210\u667a\u80fd\u4f53\u8f68\u8ff9\uff0c\u57fa\u4e8e\u8f68\u8ff9\u81ea\u52a8\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u8986\u76d6\u6587\u672c\uff0815\u4e2a\u6e38\u620f\uff09\u548c\u89c6\u89c9\u73af\u5883\u3002\u4f7f\u7528\u6a21\u677f\u4ece\u5e95\u5c42\u6e38\u620f\u4fe1\u53f7\u8ba1\u7b97\u771f\u5b9e\u7b54\u6848\uff0c\u63a7\u5236\u53ef\u56de\u7b54\u6027\uff0c\u5e73\u8861\u8986\u76d6\u591a\u79cd\u8bb0\u5fc6\u6280\u80fd\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u6027\u80fd\u8fdc\u672a\u9971\u548c\uff1a\u5f52\u7eb3\u548c\u7a7a\u95f4\u63a8\u7406\u662f\u6301\u7eed\u74f6\u9888\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u73af\u5883\u4e2d\u3002\u6301\u4e45\u8bb0\u5fc6\u5728\u6587\u672c\u6e38\u620f\u4e2d\u5e26\u6765\u660e\u663e\u63d0\u5347\uff0c\u4f46\u5bf9VLM\u667a\u80fd\u4f53\u7684\u6539\u8fdb\u4e0d\u4e00\u81f4\uff0c\u8868\u660e\u89c6\u89c9\u57fa\u7840\u7684\u60c5\u666f\u8bb0\u5fc6\u4ecd\u662f\u5f00\u653e\u6311\u6218\u3002\u4eba\u7c7b\u7814\u7a76\u8bc1\u5b9e\u4e86\u57fa\u51c6\u7684\u96be\u5ea6\u3002", "conclusion": "EMemBench\u4e3a\u8bc4\u4f30\u667a\u80fd\u4f53\u957f\u671f\u8bb0\u5fc6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7a0b\u5e8f\u5316\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u8bb0\u5fc6\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.16906", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.16906", "abs": "https://arxiv.org/abs/2601.16906", "authors": ["Calarina Muslimani", "Yunshu Du", "Kenta Kawamoto", "Kaushik Subramanian", "Peter Stone", "Peter Wurman"], "title": "The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning", "comment": null, "summary": "The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajectory Alignment Coefficient (TAC), a metric that evaluates how closely a reward function's induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC. However, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data. Validated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss. This work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTrajectory Alignment Coefficient (TAC)\u4f5c\u4e3a\u8bc4\u4f30\u5956\u52b1\u51fd\u6570\u4e0e\u4e13\u5bb6\u504f\u597d\u5bf9\u9f50\u7a0b\u5ea6\u7684\u6307\u6807\uff0c\u5e76\u5f00\u53d1\u4e86Soft-TAC\u4f5c\u4e3a\u53ef\u5fae\u8fd1\u4f3c\u7528\u4e8e\u76f4\u63a5\u4ece\u4eba\u7c7b\u504f\u597d\u6570\u636e\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u51c6\u786e\u53cd\u6620\u4efb\u52a1\u76ee\u6807\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u9700\u8981\u5de5\u5177\u652f\u6301RL\u4ece\u4e1a\u8005\u8bbe\u8ba1\u5408\u9002\u7684\u5956\u52b1\u6743\u91cd\uff0c\u5e76\u76f4\u63a5\u4ece\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u3002", "method": "1) \u63d0\u51faTrajectory Alignment Coefficient (TAC)\u8bc4\u4f30\u5956\u52b1\u51fd\u6570\u4e0e\u4e13\u5bb6\u504f\u597d\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff1b2) \u8fdb\u884c\u4eba\u56e0\u7814\u7a76\uff0c\u8ba9RL\u4ece\u4e1a\u8005\u5728Lunar Lander\u4efb\u52a1\u4e2d\u4f7f\u7528TAC\u8c03\u6574\u5956\u52b1\u6743\u91cd\uff1b3) \u63d0\u51faSoft-TAC\u4f5c\u4e3aTAC\u7684\u53ef\u5fae\u8fd1\u4f3c\uff0c\u7528\u4f5c\u635f\u5931\u51fd\u6570\u4ece\u4eba\u7c7b\u504f\u597d\u6570\u636e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff1b4) \u5728Gran Turismo 7\u8d5b\u8f66\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1Soft-TAC\u3002", "result": "\u4eba\u56e0\u7814\u7a76\u663e\u793a\uff1a\u4f7f\u7528TAC\u7684\u53c2\u4e0e\u8005\u80fd\u8bbe\u8ba1\u51fa\u6027\u80fd\u66f4\u597d\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4e14\u8ba4\u77e5\u8d1f\u8377\u66f4\u4f4e\u3002\u5728Gran Turismo 7\u4e2d\uff0cSoft-TAC\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u80fd\u6355\u6349\u504f\u597d\u7279\u5b9a\u76ee\u6807\uff0c\u4ea7\u751f\u6bd4\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u8bad\u7ec3\u6a21\u578b\u66f4\u5177\u884c\u4e3a\u5dee\u5f02\u6027\u7684\u7b56\u7565\u3002", "conclusion": "TAC\u65e2\u53ef\u4f5c\u4e3a\u6307\u5bfc\u5956\u52b1\u8c03\u6574\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u590d\u6742\u9886\u57df\u4e2d\u5956\u52b1\u5b66\u4e60\u7684\u76ee\u6807\u3002\u624b\u52a8\u5956\u52b1\u8bbe\u8ba1\u5373\u4f7f\u6709TAC\u4ecd\u663e\u7e41\u7410\uff0c\u800cSoft-TAC\u63d0\u4f9b\u4e86\u76f4\u63a5\u4ece\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.563c0bfe", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.remotion.dev%2Fdocs%2Fai%2Fskills%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/W5FNHjyi1XWZNWqYLVhgnTBWy6KGQfTWzI-6U9B_RSw=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.remotion.dev%2Fdocs%2Fai%2Fskills%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/W5FNHjyi1XWZNWqYLVhgnTBWy6KGQfTWzI-6U9B_RSw=441", "authors": ["TLDR Newsletter"], "title": "Remotion Agent Skills", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.remotion.dev%2Fdocs%2Fai%2Fskills%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/W5FNHjyi1XWZNWqYLVhgnTBWy6KGQfTWzI-6U9B_RSw=441", "summary": "Remotion Agent Skills (Website) Remotion Agent Skills allow videos to be made programmatically with coding agents like Claude Code.", "source": "tldr", "AI": {"tldr": "Remotion Agent Skills \u662f\u4e00\u4e2a\u5141\u8bb8\u901a\u8fc7\u7f16\u7a0b\u65b9\u5f0f\uff08\u4f7f\u7528\u50cf Claude Code \u8fd9\u6837\u7684\u7f16\u7801\u4ee3\u7406\uff09\u521b\u5efa\u89c6\u9891\u7684\u5de5\u5177/\u5e73\u53f0\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5236\u4f5c\u901a\u5e38\u9700\u8981\u624b\u52a8\u64cd\u4f5c\u548c\u4e13\u4e1a\u8f6f\u4ef6\uff0cRemotion Agent Skills \u65e8\u5728\u901a\u8fc7\u4ee3\u7801\u4ee3\u7406\u5b9e\u73b0\u89c6\u9891\u5236\u4f5c\u7684\u81ea\u52a8\u5316\u548c\u7a0b\u5e8f\u5316\uff0c\u964d\u4f4e\u89c6\u9891\u521b\u4f5c\u7684\u6280\u672f\u95e8\u69db\u3002", "method": "\u63d0\u4f9b Remotion Agent Skills \u5de5\u5177/\u5e73\u53f0\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u4f7f\u7528\u7f16\u7801\u4ee3\u7406\uff08\u5982 Claude Code\uff09\u901a\u8fc7\u7f16\u7a0b\u65b9\u5f0f\u521b\u5efa\u89c6\u9891\u5185\u5bb9\u3002", "result": "\u5b9e\u73b0\u4e86\u901a\u8fc7\u4ee3\u7801\u4ee3\u7406\u7a0b\u5e8f\u5316\u521b\u5efa\u89c6\u9891\u7684\u80fd\u529b\uff0c\u4f7f\u89c6\u9891\u5236\u4f5c\u66f4\u52a0\u81ea\u52a8\u5316\u548c\u53ef\u6269\u5c55\u3002", "conclusion": "Remotion Agent Skills \u4e3a\u89c6\u9891\u5236\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u7a0b\u5e8f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7801\u4ee3\u7406\u7b80\u5316\u4e86\u89c6\u9891\u521b\u4f5c\u6d41\u7a0b\u3002", "topic": "code agent"}}
{"id": "tldr.2601.abee6c49", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.val.town%2Ftownie-v5%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/31jNwqnuZiy52S_taMfYdV0thwxlpBr6Ijzg3i4QApE=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.val.town%2Ftownie-v5%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/31jNwqnuZiy52S_taMfYdV0thwxlpBr6Ijzg3i4QApE=441", "authors": ["TLDR Newsletter"], "title": "Townie's back in town!", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.val.town%2Ftownie-v5%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/31jNwqnuZiy52S_taMfYdV0thwxlpBr6Ijzg3i4QApE=441", "summary": "Townie's back in town! (3 minute read) Townie v5 is an AI agent powered by Claude 4.5 designed to integrate directly into the browser alongside the code editor. The agent can perform nearly any action a user can, including managing code, files, versions, and databases, allowing for the rapid scaffolding of features or full-stack applications.", "source": "tldr", "AI": {"tldr": "Townie v5\u662f\u57fa\u4e8eClaude 4.5\u7684AI\u4ee3\u7406\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u6d4f\u89c8\u5668\u4e2d\uff0c\u80fd\u591f\u6267\u884c\u7528\u6237\u51e0\u4e4e\u6240\u6709\u7684\u64cd\u4f5c\uff0c\u5305\u62ec\u7ba1\u7406\u4ee3\u7801\u3001\u6587\u4ef6\u3001\u7248\u672c\u548c\u6570\u636e\u5e93\uff0c\u7528\u4e8e\u5feb\u901f\u642d\u5efa\u529f\u80fd\u6216\u5168\u6808\u5e94\u7528\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u5de5\u4f5c\u7684AI\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u6267\u884c\u7528\u6237\u7ea7\u522b\u7684\u64cd\u4f5c\uff0c\u4ece\u800c\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u3002", "method": "\u57fa\u4e8eClaude 4.5\u6784\u5efa\u7684AI\u4ee3\u7406\uff0c\u76f4\u63a5\u96c6\u6210\u5230\u6d4f\u89c8\u5668\u548c\u4ee3\u7801\u7f16\u8f91\u5668\u73af\u5883\u4e2d\uff0c\u80fd\u591f\u6267\u884c\u7528\u6237\u7ea7\u522b\u7684\u64cd\u4f5c\u3002", "result": "Townie v5\u80fd\u591f\u6267\u884c\u7528\u6237\u51e0\u4e4e\u6240\u6709\u7684\u64cd\u4f5c\uff0c\u5305\u62ec\u4ee3\u7801\u7ba1\u7406\u3001\u6587\u4ef6\u64cd\u4f5c\u3001\u7248\u672c\u63a7\u5236\u548c\u6570\u636e\u5e93\u7ba1\u7406\uff0c\u5b9e\u73b0\u5feb\u901f\u7684\u529f\u80fd\u642d\u5efa\u548c\u5168\u6808\u5e94\u7528\u5f00\u53d1\u3002", "conclusion": "Townie v5\u4f5c\u4e3a\u4e00\u4e2a\u6d4f\u89c8\u5668\u96c6\u6210\u7684AI\u4ee3\u7406\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7528\u6237\u7ea7\u522b\u7684\u64cd\u4f5c\u6765\u52a0\u901f\u5e94\u7528\u5f00\u53d1\u8fc7\u7a0b\u3002", "topic": "code agent"}}
{"id": "tldr.2601.c064cfec", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcodegood.co%2Fwriting%2Fcontext-collapse-problem%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/ak3az0d5eZoNhvpfxqoKY9rgzFIz98AKtNQrE0wFP2U=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcodegood.co%2Fwriting%2Fcontext-collapse-problem%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/ak3az0d5eZoNhvpfxqoKY9rgzFIz98AKtNQrE0wFP2U=441", "authors": ["TLDR Newsletter"], "title": "The Context Collapse Problem", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcodegood.co%2Fwriting%2Fcontext-collapse-problem%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/ak3az0d5eZoNhvpfxqoKY9rgzFIz98AKtNQrE0wFP2U=441", "summary": "The Context Collapse Problem (11 minute read) AI coding assistants deliver productivity gains on new projects but underperform on legacy codebases, a \"context collapse problem\" caused by the invisibility of institutional knowledge and historical decisions. Good solutions include treating documentation as infrastructure, incrementally restructuring code for AI legibility, and initially using senior engineers as \"human context bridges.\u201d", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7a0b\u52a9\u624b\u5728\u65b0\u9879\u76ee\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9057\u7559\u4ee3\u7801\u5e93\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u662f\u7531\u4e8e\u673a\u6784\u77e5\u8bc6\u548c\u5386\u53f2\u51b3\u7b56\u4e0d\u53ef\u89c1\u5bfc\u81f4\u7684\"\u4e0a\u4e0b\u6587\u5d29\u6e83\u95ee\u9898\"\u3002", "motivation": "AI\u7f16\u7a0b\u52a9\u624b\u5728\u9057\u7559\u4ee3\u7801\u5e93\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u673a\u6784\u77e5\u8bc6\u548c\u5386\u53f2\u51b3\u7b56\uff08\u5982\u8bbe\u8ba1\u9009\u62e9\u3001\u7ea6\u675f\u6761\u4ef6\u3001\u4e1a\u52a1\u903b\u8f91\uff09\u901a\u5e38\u6ca1\u6709\u660e\u786e\u6587\u6863\u5316\uff0c\u5bfc\u81f4AI\u65e0\u6cd5\u7406\u89e3\u4ee3\u7801\u80cc\u540e\u7684\u5b8c\u6574\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u89e3\u51b3\u65b9\u6848\uff1a1) \u5c06\u6587\u6863\u89c6\u4e3a\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u7cfb\u7edf\u5316\u7ba1\u7406\uff1b2) \u9010\u6b65\u91cd\u6784\u4ee3\u7801\u4ee5\u63d0\u9ad8AI\u53ef\u8bfb\u6027\uff1b3) \u521d\u671f\u4f7f\u7528\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u4f5c\u4e3a\"\u4eba\u7c7b\u4e0a\u4e0b\u6587\u6865\u6881\"\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5316\u6587\u6863\u7ba1\u7406\u3001\u4ee3\u7801\u91cd\u6784\u548c\u4eba\u7c7b\u4e13\u5bb6\u8f85\u52a9\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3AI\u5728\u9057\u7559\u4ee3\u7801\u5e93\u4e2d\u7684\u4e0a\u4e0b\u6587\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u9ad8AI\u7f16\u7a0b\u52a9\u624b\u5728\u590d\u6742\u5386\u53f2\u9879\u76ee\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u89e3\u51b3AI\u7f16\u7a0b\u52a9\u624b\u5728\u9057\u7559\u4ee3\u7801\u5e93\u4e2d\u7684\u4e0a\u4e0b\u6587\u5d29\u6e83\u95ee\u9898\u9700\u8981\u7ed3\u5408\u6280\u672f\u6539\u8fdb\uff08\u6587\u6863\u57fa\u7840\u8bbe\u65bd\u3001\u4ee3\u7801\u91cd\u6784\uff09\u548c\u4eba\u529b\u8d44\u6e90\uff08\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6865\u6881\uff09\u7684\u7efc\u5408\u7b56\u7565\u3002", "topic": "code agent"}}
{"id": "tldr.2601.0345d4c4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwalters.app%2Fblog%2Fcomposing-apis-clis%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/c3CmMGrjyFf8AQonKAImnnYjE--H417xPToMy3Ruw1c=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwalters.app%2Fblog%2Fcomposing-apis-clis%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/c3CmMGrjyFf8AQonKAImnnYjE--H417xPToMy3Ruw1c=441", "authors": ["TLDR Newsletter"], "title": "The best code is no code: composing APIs and CLIs in the era of LLMs", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwalters.app%2Fblog%2Fcomposing-apis-clis%3Futm_source=tldrdev/1/0100019beac2a03d-f4864457-9a83-4365-bf24-2e78ef1288cc-000000/c3CmMGrjyFf8AQonKAImnnYjE--H417xPToMy3Ruw1c=441", "summary": "The best code is no code: composing APIs and CLIs in the era of LLMs (8 minute read) LLMs should use composable shell commands and CLI tools, interpreting existing API standards or reverse-engineering them, to interact with services.", "source": "tldr", "AI": {"tldr": "LLMs\u5e94\u4f7f\u7528\u53ef\u7ec4\u5408\u7684shell\u547d\u4ee4\u548cCLI\u5de5\u5177\uff0c\u901a\u8fc7\u89e3\u91ca\u73b0\u6709API\u6807\u51c6\u6216\u9006\u5411\u5de5\u7a0b\u6765\u4e0e\u670d\u52a1\u4ea4\u4e92\uff0c\u800c\u4e0d\u662f\u7f16\u5199\u4ee3\u7801", "motivation": "\u5728LLM\u65f6\u4ee3\uff0c\u7f16\u5199\u4ee3\u7801\u4e0d\u518d\u662f\u6700\u9ad8\u6548\u7684\u65b9\u5f0f\u3002\u901a\u8fc7\u4f7f\u7528\u73b0\u6709\u7684shell\u547d\u4ee4\u3001CLI\u5de5\u5177\u548cAPI\u6807\u51c6\uff0cLLMs\u53ef\u4ee5\u66f4\u76f4\u63a5\u3001\u66f4\u5b89\u5168\u5730\u4e0e\u5404\u79cd\u670d\u52a1\u4ea4\u4e92\uff0c\u907f\u514d\u91cd\u590d\u9020\u8f6e\u5b50", "method": "\u63d0\u51faLLMs\u5e94\u4f18\u5148\u4f7f\u7528\u53ef\u7ec4\u5408\u7684shell\u547d\u4ee4\u548cCLI\u5de5\u5177\uff0c\u901a\u8fc7\u89e3\u91ca\u73b0\u6709\u7684API\u6807\u51c6\uff08\u5982OpenAPI\u3001GraphQL\uff09\u6216\u5fc5\u8981\u65f6\u8fdb\u884c\u9006\u5411\u5de5\u7a0b\uff0c\u6765\u6784\u5efa\u670d\u52a1\u4ea4\u4e92\u80fd\u529b", "result": "\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u51cf\u5c11\u4ee3\u7801\u7f16\u5199\u9700\u6c42\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u589e\u5f3a\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u5e76\u5229\u7528\u73b0\u6709\u7684\u6210\u719f\u5de5\u5177\u548c\u6807\u51c6", "conclusion": "\u5728LLM\u9a71\u52a8\u7684\u5f00\u53d1\u4e2d\uff0c\u6700\u4f73\u5b9e\u8df5\u662f\u8ba9LLMs\u5229\u7528\u73b0\u6709\u7684\u547d\u4ee4\u884c\u5de5\u5177\u548cAPI\u6807\u51c6\uff0c\u800c\u4e0d\u662f\u7f16\u5199\u65b0\u4ee3\u7801\uff0c\u8fd9\u80fd\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u66f4\u53ef\u9760\u7684\u670d\u52a1\u4ea4\u4e92", "topic": "code agent"}}
{"id": "tldr.2601.92871f8e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FBloopAI%2Fvibe-kanban%3Futm_source=tldrdevops/1/0100019bead24678-cc5db18d-2a26-4123-83e1-84f2d9c282b0-000000/9k7yS7gUBfbxfle-gO2xGhbdE4xjZz3pn9JWmLhrKv8=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FBloopAI%2Fvibe-kanban%3Futm_source=tldrdevops/1/0100019bead24678-cc5db18d-2a26-4123-83e1-84f2d9c282b0-000000/9k7yS7gUBfbxfle-gO2xGhbdE4xjZz3pn9JWmLhrKv8=441", "authors": ["TLDR Newsletter"], "title": "Vibe-Kanban", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FBloopAI%2Fvibe-kanban%3Futm_source=tldrdevops/1/0100019bead24678-cc5db18d-2a26-4123-83e1-84f2d9c282b0-000000/9k7yS7gUBfbxfle-gO2xGhbdE4xjZz3pn9JWmLhrKv8=441", "summary": "Vibe-Kanban (GitHub Repo) Vibe Kanban is a workflow tool designed for an agent-driven development world. It helps engineers plan, coordinate, and review work done by AI coding agents rather than writing most code themselves. The tool centralizes agent configuration, supports parallel and sequential agent execution, enables fast review and dev server startup, and tracks task status across local or remote (SSH) projects.", "source": "tldr", "AI": {"tldr": "Vibe-Kanban\u662f\u4e00\u4e2a\u4e3aAI\u9a71\u52a8\u5f00\u53d1\u8bbe\u8ba1\u7684\u6d41\u7a0b\u5de5\u5177\uff0c\u5e2e\u52a9\u5de5\u7a0b\u5e08\u89c4\u5212\u3001\u534f\u8c03\u548c\u5ba1\u67e5AI\u7f16\u7801\u4ee3\u7406\u7684\u5de5\u4f5c\uff0c\u800c\u975e\u4eb2\u81ea\u7f16\u5199\u5927\u90e8\u5206\u4ee3\u7801\u3002", "motivation": "\u968f\u7740AI\u7f16\u7801\u4ee3\u7406\u7684\u5174\u8d77\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u65b0\u7684\u5de5\u5177\u6765\u7ba1\u7406\u548c\u534f\u8c03\u591a\u4e2aAI\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u800c\u4e0d\u662f\u4eb2\u81ea\u7f16\u5199\u4ee3\u7801\u3002\u4f20\u7edf\u5f00\u53d1\u5de5\u5177\u65e0\u6cd5\u6709\u6548\u5904\u7406AI\u4ee3\u7406\u9a71\u52a8\u7684\u5e76\u884c\u548c\u987a\u5e8f\u6267\u884c\u4efb\u52a1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u4e2d\u5f0f\u5de5\u4f5c\u6d41\u5de5\u5177\uff0c\u63d0\u4f9b\uff1a1\uff09\u96c6\u4e2d\u5316\u4ee3\u7406\u914d\u7f6e\u7ba1\u7406\uff1b2\uff09\u652f\u6301\u5e76\u884c\u548c\u987a\u5e8f\u4ee3\u7406\u6267\u884c\uff1b3\uff09\u5feb\u901f\u5ba1\u67e5\u548c\u5f00\u53d1\u670d\u52a1\u5668\u542f\u52a8\uff1b4\uff09\u8de8\u672c\u5730\u6216\u8fdc\u7a0b\uff08SSH\uff09\u9879\u76ee\u7684\u4efb\u52a1\u72b6\u6001\u8ddf\u8e2a\u3002", "result": "\u521b\u5efa\u4e86Vibe-Kanban\u5de5\u5177\uff0c\u4f5c\u4e3a\u4e00\u4e2aGitHub\u4ed3\u5e93\u63d0\u4f9b\uff0c\u4e13\u95e8\u4e3aAI\u4ee3\u7406\u9a71\u52a8\u7684\u5f00\u53d1\u73af\u5883\u8bbe\u8ba1\uff0c\u5e2e\u52a9\u5de5\u7a0b\u5e08\u66f4\u6709\u6548\u5730\u7ba1\u7406AI\u7f16\u7801\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "Vibe-Kanban\u586b\u8865\u4e86AI\u9a71\u52a8\u5f00\u53d1\u5de5\u4f5c\u6d41\u7ba1\u7406\u7684\u5de5\u5177\u7a7a\u767d\uff0c\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u89c4\u5212\u3001\u534f\u8c03\u548c\u5ba1\u67e5AI\u7f16\u7801\u4ee3\u7406\u7684\u5de5\u4f5c\uff0c\u9002\u5e94\u65b0\u7684\u5f00\u53d1\u8303\u5f0f\u3002", "topic": "swe application"}}
{"id": "tldr.2601.c0ce4f4d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblink.new%2F%3Futm_source=tldrfounders/1/0100019beaf7a099-4ca95b1c-29ee-4257-9b1a-7c2d68340e8b-000000/FMcpFT-tEYWJh-q1OGj35TQ0pFLuGw1BhEMUn7cpSgg=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblink.new%2F%3Futm_source=tldrfounders/1/0100019beaf7a099-4ca95b1c-29ee-4257-9b1a-7c2d68340e8b-000000/FMcpFT-tEYWJh-q1OGj35TQ0pFLuGw1BhEMUn7cpSgg=441", "authors": ["TLDR Newsletter"], "title": "Blink Agent Builder", "comment": "Source: TLDR Newsletter, Date: 2026-01-23, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblink.new%2F%3Futm_source=tldrfounders/1/0100019beaf7a099-4ca95b1c-29ee-4257-9b1a-7c2d68340e8b-000000/FMcpFT-tEYWJh-q1OGj35TQ0pFLuGw1BhEMUn7cpSgg=441", "summary": "Blink Agent Builder (Tool) Blink Agent Builder is a vibe coding platform that creates AI agents capable of thinking, using tools, and completing tasks end-to-end from simple descriptions.", "source": "tldr", "AI": {"tldr": "Blink Agent Builder\u662f\u4e00\u4e2avibe coding\u5e73\u53f0\uff0c\u80fd\u591f\u4ece\u7b80\u5355\u63cf\u8ff0\u521b\u5efa\u5177\u5907\u601d\u8003\u3001\u4f7f\u7528\u5de5\u5177\u548c\u7aef\u5230\u7aef\u5b8c\u6210\u4efb\u52a1\u80fd\u529b\u7684AI\u667a\u80fd\u4f53", "motivation": "\u7b80\u5316AI\u667a\u80fd\u4f53\u7684\u521b\u5efa\u8fc7\u7a0b\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u901a\u8fc7\u7b80\u5355\u63cf\u8ff0\u5feb\u901f\u6784\u5efa\u5177\u5907\u5b8c\u6574\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff0c\u964d\u4f4e\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u6280\u672f\u95e8\u69db", "method": "\u91c7\u7528vibe coding\u5e73\u53f0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u5177\u5907\u601d\u8003\u80fd\u529b\u3001\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u7aef\u5230\u7aef\u4efb\u52a1\u6267\u884c\u80fd\u529b\u7684AI\u667a\u80fd\u4f53", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u4ece\u7b80\u5355\u63cf\u8ff0\u521b\u5efa\u5b8c\u6574\u529f\u80fdAI\u667a\u80fd\u4f53\u7684\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u6784\u5efa\u7684\u81ea\u52a8\u5316\u548c\u7b80\u5316", "conclusion": "Blink Agent Builder\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4fbf\u6377\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u667a\u80fd\u4f53\u521b\u5efa\u7684\u6280\u672f\u95e8\u69db", "topic": "code agent"}}
