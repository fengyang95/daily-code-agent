{"id": "2512.21347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21347", "abs": "https://arxiv.org/abs/2512.21347", "authors": ["V\u00edtor Mateus de Brito", "Kleinner Farias"], "title": "Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey", "comment": "4 Figures, 8 Tables, Text in Portuguese", "summary": "The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.", "AI": {"tldr": "\u5bf946\u540d\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u7684\u5b9e\u8bc1\u8c03\u67e5\u663e\u793a\uff0cLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u53d7\u5230\u79ef\u6781\u8bc4\u4ef7\uff0c\u80fd\u52a0\u901f\u6280\u672f\u95ee\u9898\u89e3\u51b3\u3001\u6539\u5584\u6587\u6863\u652f\u6301\u548c\u4ee3\u7801\u6807\u51c6\u5316\uff0c\u4f46\u4e5f\u5b58\u5728\u8ba4\u77e5\u4f9d\u8d56\u3001\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u81ea\u4e3b\u6027\u4fb5\u8680\u7b49\u62c5\u5fe7\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6df1\u5165\u5e94\u7528\uff0c\u7406\u89e3\u5176\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u8c03\u67e5\u586b\u8865\u5b66\u672f\u8ba8\u8bba\u4e0e\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5bf946\u540d\u5177\u6709\u4e0d\u540c\u6559\u80b2\u80cc\u666f\u548c\u7ecf\u9a8c\u6c34\u5e73\u7684\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u6536\u96c6\u5173\u4e8eLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u91c7\u7528\u60c5\u51b5\u7684\u5b9e\u8bc1\u6570\u636e\u3002", "result": "\u8c03\u67e5\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5bf9LLM\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u7279\u522b\u662f\u5728\u52a0\u901f\u6280\u672f\u95ee\u9898\u89e3\u51b3\u3001\u6539\u5584\u6587\u6863\u652f\u6301\u548c\u589e\u5f3a\u6e90\u4ee3\u7801\u6807\u51c6\u5316\u65b9\u9762\uff1b2\uff09\u540c\u65f6\u5b58\u5728\u5bf9\u8ba4\u77e5\u4f9d\u8d56\u3001\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u81ea\u4e3b\u6027\u4fb5\u8680\u7684\u62c5\u5fe7\u3002", "conclusion": "LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5177\u6709\u663e\u8457\u4ef7\u503c\uff0c\u4f46\u9700\u8981\u6279\u5224\u6027\u548c\u76d1\u7763\u6027\u4f7f\u7528\u3002\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u547c\u5401\u672a\u6765\u7814\u7a76\u5173\u6ce8LLM\u7684\u8ba4\u77e5\u3001\u4f26\u7406\u548c\u7ec4\u7ec7\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2512.21422", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21422", "abs": "https://arxiv.org/abs/2512.21422", "authors": ["Nathan Stringham", "Fateme Hashemi Chaleshtori", "Xinyuan Yan", "Zhichao Xu", "Bei Wang", "Ana Marasovi\u0107"], "title": "Teaching People LLM's Errors and Getting it Right", "comment": null, "summary": "People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.\n  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56LLM\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u5931\u8d25\u6a21\u5f0f\u786e\u5b9e\u5b58\u5728\u4f46\u96be\u4ee5\u81ea\u52a8\u53d1\u73b0\uff0c\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u663e\u793a\u6559\u5b66\u5931\u8d25\u6a21\u5f0f\u53ef\u6709\u6548\u51cf\u5c11\u8fc7\u5ea6\u4f9d\u8d56", "motivation": "\u7528\u6237\u7ecf\u5e38\u5728\u4e0d\u8be5\u4f7f\u7528LLM\u7684\u60c5\u51b5\u4e0b\u8fc7\u5ea6\u4f9d\u8d56\u5b83\u4eec\uff0c\u90e8\u5206\u539f\u56e0\u662f\u770b\u5230LLM\u80fd\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u800c\u8bef\u4ee5\u4e3a\u5b83\u4eec\u4e0d\u4f1a\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u51fa\u9519\u3002\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u805a\u7c7b\u5931\u8d25\u6a21\u5f0f\u6765\u6559\u5b66\u7528\u6237\uff0c\u4f46\u6548\u679c\u4e0d\u4f73\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u539f\u56e0", "method": "1) \u68c0\u67e5\u5931\u8d25\u6a21\u5f0f\u662f\u5426\u5b58\u5728\uff1a\u901a\u8fc7\u5143\u6807\u7b7e\u5206\u7ec4\u8bc4\u4f30LLM\u8868\u73b0\uff0c\u8bc6\u522b\u663e\u8457\u4e14\u9519\u8bef\u7387\u9ad8\u7684\u7ec4\uff1b2) \u6d4b\u8bd5\u63d0\u793a\u548c\u5d4c\u5165\u65b9\u6cd5\u80fd\u5426\u53d1\u73b0\u5df2\u77e5\u5931\u8d25\u6a21\u5f0f\uff1b3) \u63d0\u51fa\u65b0\u8bc4\u4f30\u6307\u6807\u8861\u91cf\u7528\u6237\u5229\u7528\u5931\u8d25\u6a21\u5f0f\u9884\u6d4bLLM\u9519\u8bef\u7684\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u7528\u6237\u7814\u7a76", "result": "1) \u786e\u5b9e\u5b58\u5728\u663e\u8457\u7684\u5931\u8d25\u6a21\u5f0f\uff1b2) \u73b0\u6709\u81ea\u52a8\u53d1\u73b0\u65b9\u6cd5\u6548\u679c\u53c2\u5dee\u4e0d\u9f50\uff1b3) \u4f7f\u7528\u65b0\u6307\u6807\u7684\u7528\u6237\u7814\u7a76\u663e\u793a\u6559\u5b66\u5931\u8d25\u6a21\u5f0f\u6709\u79ef\u6781\u6548\u679c\uff0c\u800c\u4f20\u7edf\u7684\u4eba\u673a\u534f\u4f5c\u51c6\u786e\u7387\u6307\u6807\u5219\u65e0\u6b64\u6548\u679c", "conclusion": "\u6559\u5b66\u5931\u8d25\u6a21\u5f0f\u662f\u51cf\u5c11\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u7684\u53ef\u884c\u65b9\u6cd5\uff0c\u4f46\u6210\u529f\u53d6\u51b3\u4e8e\u66f4\u597d\u7684\u81ea\u52a8\u5931\u8d25\u53d1\u73b0\u65b9\u6cd5\u548c\u4f7f\u7528\u672c\u6587\u63d0\u51fa\u7684\u65b0\u8bc4\u4f30\u6307\u6807", "topic": "agent analysis"}}
{"id": "2512.21395", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21395", "abs": "https://arxiv.org/abs/2512.21395", "authors": ["Natalia Espinosa-Dice", "Nicholas J. Jackson", "Chao Yan", "Aaron Lee", "Bradley A. Malin"], "title": "A Reinforcement Learning Approach to Synthetic Data Generation", "comment": null, "summary": "Synthetic data generation (SDG) is a promising approach for enabling data sharing in biomedical studies while preserving patient privacy. Yet, state-of-the-art generative models often require large datasets and complex training procedures, limiting their applicability in small-sample settings. In this work, we reframe SDG as a reinforcement learning (RL) problem and introduce RLSyn, a novel framework that models the data generator as a stochastic policy over patient records and optimizes it using Proximal Policy Optimization with discriminator-derived rewards, yielding more stable and data-efficient training. We evaluate RLSyn on two biomedical datasets - AI-READI and MIMIC-IV- and benchmark it against state-of-the-art generative adversarial networks (GANs) and diffusion-based methods across extensive privacy, utility, and fidelity evaluations. RL-Syn performs comparably to diffusion models and outperforms GANs on MIMIC-IV, while outperforming both diffusion models and GANs on the smaller AI-READI dataset. These results demonstrate that reinforcement learning provides a principled and effective alternative for synthetic biomedical data generation, particularly in data-scarce regimes.", "AI": {"tldr": "RLSyn\uff1a\u5c06\u5408\u6210\u6570\u636e\u751f\u6210\u91cd\u6784\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u5c0f\u6837\u672c\u751f\u7269\u533b\u5b66\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u751f\u6210\u6a21\u578b", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff08\u5982GAN\u3001\u6269\u6563\u6a21\u578b\uff09\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u590d\u6742\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5728\u5c0f\u6837\u672c\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u66f4\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4fc3\u8fdb\u533b\u7597\u6570\u636e\u5171\u4eab\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "method": "\u5c06\u5408\u6210\u6570\u636e\u751f\u6210\u91cd\u6784\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51faRLSyn\u6846\u67b6\uff1a\u5c06\u6570\u636e\u751f\u6210\u5668\u5efa\u6a21\u4e3a\u60a3\u8005\u8bb0\u5f55\u4e0a\u7684\u968f\u673a\u7b56\u7565\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u5224\u522b\u5668\u751f\u6210\u7684\u5956\u52b1\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u548c\u6570\u636e\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u4e24\u4e2a\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\uff08AI-READI\u548cMIMIC-IV\uff09\u4e0a\u8bc4\u4f30\uff0cRLSyn\u5728\u9690\u79c1\u6027\u3001\u5b9e\u7528\u6027\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u5728\u8f83\u5c0f\u7684AI-READI\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u548cGANs\uff0c\u5728MIMIC-IV\u4e0a\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u5f53\u5e76\u4f18\u4e8eGANs\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u5408\u6210\u751f\u7269\u533b\u5b66\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u573a\u666f\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u533b\u7597\u6570\u636e\u5171\u4eab\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.21351", "categories": ["cs.SE", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.21351", "abs": "https://arxiv.org/abs/2512.21351", "authors": ["Santhosh Kumar Ravindran"], "title": "CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation", "comment": "10 pages, 2 figures; Code for Simulation", "summary": "Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.", "AI": {"tldr": "CosmoCore-Evo\u6269\u5c55\u4e86CosmoCore\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u9002\u5e94\u6027\u548c\u65b0\u9896\u6027\uff0c\u5c06RL\u8f68\u8ff9\u89c6\u4e3a\"\u57fa\u56e0\u7ec4\"\u8fdb\u884c\u53d8\u5f02\u548c\u9009\u62e9\uff0c\u5728\u5206\u5e03\u504f\u79fb\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u5728\u9047\u5230\u5206\u5e03\u504f\u79fb\u73af\u5883\uff08\u5982API\u53d8\u66f4\u3001\u65b0\u5e93\uff09\u65f6\u9002\u5e94\u6027\u6709\u9650\uff0c\u9700\u8981\u7a81\u7834\u8bad\u7ec3\u6a21\u5f0f\u7684\u9650\u5236\uff0c\u4ea7\u751f\u65b0\u9896\u89e3\u51b3\u65b9\u6848\u3002\u53d7\u4eba\u7c7b\u8fdb\u5316\u4e2d\u81ea\u7136\u9009\u62e9\u548c\u9002\u5e94\u673a\u5236\u7684\u542f\u53d1\uff0c\u9700\u8981\u589e\u5f3a\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u65b0\u9896\u6027\u3002", "method": "\u57fa\u4e8eCosmoCore\u7684\u60c5\u611f\u68a6\u5883\u56de\u653eRL\u6846\u67b6\uff0c\u5f15\u5165\u8fdb\u5316\u7b97\u6cd5\uff1a\u5c06RL\u8f68\u8ff9\u89c6\u4e3a\"\u57fa\u56e0\u7ec4\"\uff0c\u5728\u591c\u95f4\u56de\u653e\u9636\u6bb5\u8fdb\u884c\u53d8\u5f02\u548c\u9009\u62e9\uff1b\u589e\u5f3a\u68a6\u5883\u961f\u5217\uff0c\u5305\u62ec\u9ad8\u9002\u5e94\u5ea6\u8f68\u8ff9\u7684\u53d8\u5f02\u548c\u4f01\u4e1a\u8c03\u4f18\u7684\u9002\u5e94\u5ea6\u51fd\u6570\uff08\u6548\u7387\u3001\u5408\u89c4\u6027\u3001\u53ef\u6269\u5c55\u6027\uff09\u3002", "result": "\u5728HumanEval\u53d8\u4f53\u3001BigCodeBench\u548c\u81ea\u5b9a\u4e49PySpark\u7ba1\u9053\u6a21\u62df\u7b49\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCosmoCore-Evo\u76f8\u6bd4\u539f\u59cbCosmoCore\u548cPPO\u3001REAMER\u7b49\u57fa\u7ebf\uff0c\u89e3\u51b3\u65b9\u6848\u65b0\u9896\u6027\u63d0\u9ad835%\uff0c\u9002\u5e94\u901f\u5ea6\u52a0\u5feb25%\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u8fdb\u5316\u7ec4\u4ef6\u5728\u5f25\u5408LLM\u4ee3\u7406\u611f\u77e5\u5dee\u8ddd\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u8fdb\u5316\u7b97\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u65b0\u9896\u6027\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u504f\u79fb\u73af\u5883\u4e2d\u3002CosmoCore-Evo\u6846\u67b6\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u7a81\u7834\u8bad\u7ec3\u6a21\u5f0f\u9650\u5236\u7684\u65b0\u9014\u5f84\u3002", "topic": "code agent"}}
{"id": "2512.21352", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21352", "abs": "https://arxiv.org/abs/2512.21352", "authors": ["Sumanth Bharadwaj Hachalli Karanam", "Dhiwahar Adhithya Kennady"], "title": "Multi-Agent LLM Committees for Autonomous Software Beta Testing", "comment": null, "summary": "Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u589e\u5f3a\u7684LLM\u534f\u4f5c\u8fdb\u884c\u8f6f\u4ef6\u6d4b\u8bd5\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff0c\u652f\u6301\u5b9e\u65f6\u6301\u7eed\u96c6\u6210\u6d4b\u8bd5\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6beta\u6d4b\u8bd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u5355\u667a\u80fd\u4f53LLM\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u884c\u4e3a\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u6846\u67b6\uff0c\u5305\u542b\u591a\u6837\u5316\u89c6\u89c9\u589e\u5f3aLLM\uff0c\u901a\u8fc7\u4e09\u8f6e\u6295\u7968\u534f\u8bae\u8fbe\u6210\u5171\u8bc6\uff0c\u7ed3\u5408\u6a21\u578b\u591a\u6837\u6027\u3001\u89d2\u8272\u9a71\u52a8\u884c\u4e3a\u53d8\u5316\u548c\u89c6\u89c9\u754c\u9762\u7406\u89e3\uff0c\u7cfb\u7edf\u63a2\u7d22Web\u5e94\u7528\u3002", "result": "\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u572884\u6b21\u5b9e\u9a8c\u8fd0\u884c\u4e2d\u8fbe\u523089.5%\u603b\u4f53\u4efb\u52a1\u6210\u529f\u7387\uff0c2-4\u667a\u80fd\u4f53\u914d\u7f6e\u8fbe\u523091.7-100%\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u534713.7-22.0\u4e2a\u767e\u5206\u70b9\u3002\u5728WebShop\u4e0a\u8fbe\u523074.7%\u6210\u529f\u7387\uff08GPT-3\u57fa\u7ebf\u4e3a50.1%\uff09\uff0c\u5728OWASP\u5b89\u5168\u6d4b\u8bd5\u4e2d\u8986\u76d68/10\u6f0f\u6d1e\u7c7b\u522b\uff0cbug\u68c0\u6d4bF1\u5206\u6570\u8fbe0.91\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u6846\u67b6\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u6d4b\u8bd5\u6548\u679c\uff0c\u652f\u6301\u5b9e\u65f6\u6301\u7eed\u96c6\u6210\u6d4b\u8bd5\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002", "topic": "swe application"}}
{"id": "2512.21567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21567", "abs": "https://arxiv.org/abs/2512.21567", "authors": ["Changzhi Sun", "Xiangyu Chen", "Jixiang Luo", "Dell Zhang", "Xuelong Li"], "title": "Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management", "comment": null, "summary": "External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDAM\u6846\u67b6\uff0c\u5c06LLM\u5185\u5b58\u7ba1\u7406\u91cd\u6784\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u4ef7\u503c\u51fd\u6570\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8bc4\u4f30\u64cd\u4f5c\uff0c\u66ff\u4ee3\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u7cfb\u7edf\u7684\u5916\u90e8\u5185\u5b58\u7ba1\u7406\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u96be\u4ee5\u9884\u6d4b\u5185\u5b58\u51b3\u7b56\u7684\u957f\u671f\u548c\u4e0d\u786e\u5b9a\u540e\u679c\u3002\u5185\u5b58\u7684\u8bfb\u5199\u9009\u62e9\u4f1a\u5f71\u54cd\u672a\u6765\u7684\u68c0\u7d22\u548c\u4e0b\u6e38\u884c\u4e3a\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u51b3\u7b56\u6846\u67b6\u3002", "method": "\u63d0\u51faDAM\uff08\u51b3\u7b56\u7406\u8bba\u4ee3\u7406\u5185\u5b58\uff09\u6846\u67b6\uff0c\u5c06\u5185\u5b58\u7ba1\u7406\u5206\u89e3\u4e3a\u5373\u65f6\u4fe1\u606f\u8bbf\u95ee\u548c\u5206\u5c42\u5b58\u50a8\u7ef4\u62a4\u3002\u901a\u8fc7\u4ef7\u503c\u51fd\u6570\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8bc4\u4f30\u5019\u9009\u64cd\u4f5c\uff0c\u57fa\u4e8e\u957f\u671f\u6548\u7528\u548c\u98ce\u9669\u4f30\u8ba1\u7684\u805a\u5408\u7b56\u7565\u8fdb\u884c\u51b3\u7b56\u4ef2\u88c1\u3002", "result": "\u8bba\u6587\u7684\u4e3b\u8981\u8d21\u732e\u4e0d\u662f\u65b0\u7b97\u6cd5\uff0c\u800c\u662f\u539f\u5219\u6027\u7684\u91cd\u6784\u6846\u67b6\uff0c\u9610\u660e\u4e86\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5185\u5b58\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u5185\u5b58\u7ba1\u7406\u5e94\u88ab\u89c6\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0cDAM\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u5185\u5b58\u6548\u7528\u7684\u5ef6\u8fdf\u6027\u548c\u672a\u6765\u4ea4\u4e92\u4f9d\u8d56\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.21578", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21578", "abs": "https://arxiv.org/abs/2512.21578", "authors": ["Ali Sahami", "Sudhanshu Garg", "Andrew Wang", "Chaitanya Kulkarni", "Farhad Farahani", "Sean Yun-Shiuan Chuang", "Jian Wan", "Srinivasan Manoharan", "Uma Kona", "Nitin Sharma", "Linsey Pang", "Prakhar Mehrotra", "Jessica Clark", "Mark Moyou"], "title": "NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent", "comment": null, "summary": "We present the development and optimization of PayPal's Commerce Agent, powered by NEMO-4-PAYPAL, a multi-agent system designed to revolutionize agentic commerce on the PayPal platform. Through our strategic partnership with NVIDIA, we leveraged the NeMo Framework for LLM model fine-tuning to enhance agent performance. Specifically, we optimized the Search and Discovery agent by replacing our base model with a fine-tuned Nemotron small language model (SLM).\n  We conducted comprehensive experiments using the llama3.1-nemotron-nano-8B-v1 architecture, training LoRA-based models through systematic hyperparameter sweeps across learning rates, optimizers (Adam, AdamW), cosine annealing schedules, and LoRA ranks. Our contributions include: (1) the first application of NVIDIA's NeMo Framework to commerce-specific agent optimization, (2) LLM powered fine-tuning strategy for retrieval-focused commerce tasks, (3) demonstration of significant improvements in latency and cost while maintaining agent quality, and (4) a scalable framework for multi-agent system optimization in production e-commerce environments. Our results demonstrate that the fine-tuned Nemotron SLM effectively resolves the key performance issue in the retrieval component, which represents over 50\\% of total agent response time, while maintaining or enhancing overall system performance.", "AI": {"tldr": "PayPal\u4e0eNVIDIA\u5408\u4f5c\uff0c\u4f7f\u7528NeMo\u6846\u67b6\u5fae\u8c03Nemotron\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f18\u5316\u5176Commerce Agent\u4e2d\u7684\u641c\u7d22\u4e0e\u53d1\u73b0\u4ee3\u7406\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "PayPal Commerce Agent\u4e2d\u7684\u68c0\u7d22\u7ec4\u4ef6\u5360\u7528\u4e86\u8d85\u8fc750%\u7684\u603b\u54cd\u5e94\u65f6\u95f4\uff0c\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3002\u9700\u8981\u4f18\u5316\u641c\u7d22\u4e0e\u53d1\u73b0\u4ee3\u7406\u4ee5\u63d0\u5347\u6574\u4f53\u7cfb\u7edf\u6548\u7387\uff0c\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "method": "\u4f7f\u7528NVIDIA NeMo\u6846\u67b6\u5bf9Nemotron\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528llama3.1-nemotron-nano-8B-v1\u67b6\u6784\uff0c\u901a\u8fc7LoRA\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u5316\u7684\u8d85\u53c2\u6570\u626b\u63cf\uff08\u5b66\u4e60\u7387\u3001\u4f18\u5316\u5668\u3001\u4f59\u5f26\u9000\u706b\u8c03\u5ea6\u3001LoRA\u79e9\u7b49\uff09\u3002", "result": "\u5fae\u8c03\u540e\u7684Nemotron SLM\u6709\u6548\u89e3\u51b3\u4e86\u68c0\u7d22\u7ec4\u4ef6\u7684\u5173\u952e\u6027\u80fd\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u5347\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u6539\u5584\u4e86\u5ef6\u8fdf\u548c\u6210\u672c\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86NeMo\u6846\u67b6\u5728\u5546\u4e1a\u7279\u5b9a\u4ee3\u7406\u4f18\u5316\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u7535\u5b50\u5546\u52a1\u591a\u4ee3\u7406\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2512.21373", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.21373", "abs": "https://arxiv.org/abs/2512.21373", "authors": ["Titouan Duston", "Shuo Xin", "Yang Sun", "Daoguang Zan", "Aoyan Li", "Shulin Xin", "Kai Shen", "Yixiao Chen", "Qiming Sun", "Ge Zhang", "Jiashuo Liu", "Huan Zhou", "Jingkai Liu", "Zhichen Pu", "Yuanheng Wang", "Bo-Xuan Ge", "Xin Tong", "Fei Ye", "Zhi-Chao Zhao", "Wen-Biao Han", "Zhoujian Cao", "Yueran Zhao", "Weiluo Ren", "Qingshen Long", "Yuxiao Liu", "Anni Huang", "Yidi Du", "Yuanyuan Rong", "Jiahao Peng"], "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories", "comment": null, "summary": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.", "AI": {"tldr": "AInsteinBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u771f\u5b9e\u79d1\u7814\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u4f5c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u5f00\u53d1\u4ee3\u7406\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u516d\u4e2a\u4e3b\u6d41\u79d1\u5b66\u4ee3\u7801\u5e93\u7684\u5b9e\u9645pull request\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u4fa7\u91cd\u4e8e\u6982\u5ff5\u77e5\u8bc6\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u5f3a\u8c03\u901a\u7528\u529f\u80fd\u5b9e\u73b0\u548c\u95ee\u9898\u89e3\u51b3\uff0c\u7f3a\u4e4f\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u79d1\u7814\u5f00\u53d1\u73af\u5883\u4e2d\u7aef\u5230\u7aef\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u4ece\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u79d1\u5b66\u4ee3\u7801\u5e93\uff08\u91cf\u5b50\u5316\u5b66\u3001\u91cf\u5b50\u8ba1\u7b97\u3001\u5206\u5b50\u52a8\u529b\u5b66\u3001\u6570\u503c\u76f8\u5bf9\u8bba\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u5316\u5b66\u4fe1\u606f\u5b66\uff09\u4e2d\u63d0\u53d6\u7ef4\u62a4\u8005\u7f16\u5199\u7684pull request\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u548c\u4e13\u5bb6\u8bc4\u5ba1\u786e\u4fdd\u79d1\u5b66\u6311\u6218\u6027\u3001\u5145\u5206\u6d4b\u8bd5\u8986\u76d6\u548c\u9002\u5f53\u96be\u5ea6\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u53ef\u6267\u884c\u73af\u5883\u8bc4\u4f30\u3001\u79d1\u5b66\u610f\u4e49\u5931\u8d25\u6a21\u5f0f\u548c\u6d4b\u8bd5\u9a71\u52a8\u9a8c\u8bc1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u591f\u8861\u91cf\u6a21\u578b\u8d85\u8d8a\u8868\u9762\u4ee3\u7801\u751f\u6210\u3001\u638c\u63e1\u8ba1\u7b97\u79d1\u5b66\u7814\u7a76\u6838\u5fc3\u80fd\u529b\u7684\u60c5\u51b5\u3002", "conclusion": "AInsteinBench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u79d1\u7814\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u6807\u51c6\uff0c\u63a8\u52a8\u6a21\u578b\u4ece\u7b80\u5355\u4ee3\u7801\u751f\u6210\u5411\u79d1\u7814\u5f00\u53d1\u6838\u5fc3\u80fd\u529b\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2512.21426", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21426", "abs": "https://arxiv.org/abs/2512.21426", "authors": ["Mohammed Sayagh"], "title": "What Makes a GitHub Issue Ready for Copilot?", "comment": null, "summary": "AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e8632\u9879\u8be6\u7ec6\u6807\u51c6\u6765\u8bc4\u4f30GitHub\u95ee\u9898\u7684\u8d28\u91cf\uff0c\u4ee5\u4f7f\u5176\u66f4\u9002\u5408AI\u4ee3\u7406\u5904\u7406\uff0c\u5e76\u5f00\u53d1\u4e86\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u9884\u6d4b\u95ee\u9898\u80fd\u5426\u6210\u529f\u751f\u6210\u5408\u5e76\u7684PR\u3002", "motivation": "AI\u4ee3\u7406\uff08\u5982Copilot\uff09\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f9d\u8d56\u4e8e\u8f93\u5165\u8d28\u91cf\uff0c\u4f46GitHub\u95ee\u9898\u5f80\u5f80\u4e0d\u591f\u6e05\u6670\u6216\u8303\u56f4\u4e0d\u660e\u786e\uff0c\u5bfc\u81f4\u5b9e\u73b0\u5931\u8d25\u3002\u73b0\u6709\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\u6709\u9650\u4e14\u8fc7\u4e8e\u9ad8\u5c42\uff0c\u9700\u8981\u66f4\u8be6\u7ec6\u7684\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\u3002", "method": "1. \u6784\u5efa32\u9879\u8be6\u7ec6\u6807\u51c6\u6765\u8861\u91cfGitHub\u95ee\u9898\u8d28\u91cf\uff1b2. \u6bd4\u8f83\u5bfc\u81f4\u5408\u5e76PR\u4e0e\u5173\u95edPR\u7684\u95ee\u9898\u7279\u5f81\uff1b3. \u5efa\u7acb\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u95ee\u9898\u80fd\u5426\u751f\u6210\u5408\u5e76PR\u3002", "result": "\u6210\u529f\u5408\u5e76\u7684PR\u901a\u5e38\u6765\u81ea\uff1a\u66f4\u7b80\u77ed\u3001\u8303\u56f4\u660e\u786e\u3001\u63d0\u4f9b\u6e05\u6670\u6307\u5bfc\u548c\u76f8\u5173\u5de5\u4ef6\u63d0\u793a\u3001\u5305\u542b\u5b9e\u73b0\u6307\u5bfc\u7684\u95ee\u9898\u3002\u5305\u542b\u5916\u90e8\u5f15\u7528\uff08\u914d\u7f6e\u3001\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u3001\u4f9d\u8d56\u6216\u5916\u90e8API\uff09\u7684\u95ee\u9898\u5408\u5e76\u7387\u8f83\u4f4e\u3002\u6a21\u578b\u7684\u4e2d\u4f4dAUC\u8fbe\u523072%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7f16\u5199\u9ad8\u8d28\u91cfGitHub\u95ee\u9898\u7684\u5173\u952e\u6307\u6807\uff0c\u5f3a\u8c03\u4e86\u5728AI\u534f\u4f5c\u65f6\u4ee3\u5c06\u95ee\u9898\u7f16\u5199\u4f5c\u4e3a\u4e00\u7b49\u8f6f\u4ef6\u5de5\u7a0b\u6d3b\u52a8\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5de5\u5177\u5e2e\u52a9\u7528\u6237\u6539\u8fdb\u95ee\u9898\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "2512.21446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21446", "abs": "https://arxiv.org/abs/2512.21446", "authors": ["Shirui Chen", "Jiantao Jiao", "Lillian J. Ratliff", "Banghua Zhu"], "title": "dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning", "comment": null, "summary": "Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \\texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy'' over autoregressive models.", "AI": {"tldr": "dUltra\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u89e3\u63a9\u7801\u7b56\u7565\u5b9e\u73b0\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5e76\u884c\u89e3\u7801\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08MDLMs\uff09\u5373\u4f7f\u91c7\u7528\u590d\u6742\u91c7\u6837\u7b56\u7565\uff0c\u6bcf\u6b21\u524d\u5411\u4f20\u64ad\u4e5f\u53ea\u80fd\u89e3\u7801\u4e0d\u52305\u4e2atoken\uff0c\u91c7\u6837\u901f\u5ea6\u4e0e\u81ea\u56de\u5f52\u6a21\u578b+\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\u76f8\u5f53\uff0c\u9650\u5236\u4e86\u5176\u4f18\u52bf\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u84b8\u998f\u7684\u52a0\u901f\u65b9\u6cd5\u5b58\u5728\u79bb\u7b56\u7565\u95ee\u9898\uff0c\u4e14\u6027\u80fd\u53d7\u9650\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6837\u672c\u8d28\u91cf\u3002", "method": "\u63d0\u51fadUltra\u6846\u67b6\uff0c\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u8fdb\u884c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u5b66\u4e60\u9ad8\u6548\u7684\u5e76\u884c\u89e3\u7801\u89e3\u63a9\u7801\u7b56\u7565\u3002\u5f15\u5165\u89e3\u63a9\u7801\u89c4\u5212\u5934\uff0c\u5728\u72ec\u7acb\u4f2f\u52aa\u5229\u5206\u5e03\u4e0b\u9884\u6d4b\u6bcf\u4e2atoken\u7684\u89e3\u63a9\u7801\u6982\u7387\u3002\u8054\u5408\u4f18\u5316\u57fa\u7840\u6269\u6563LLM\u548c\u89e3\u63a9\u7801\u987a\u5e8f\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u3001\u84b8\u998f\u5956\u52b1\u548c\u89e3\u63a9\u7801\u6b65\u9aa4\u6570\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\uff0cdUltra\u5728\u51c6\u786e\u7387-\u6548\u7387\u6743\u8861\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u542f\u53d1\u5f0f\u548c\u84b8\u998f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u671d\u7740\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u5bf9\u81ea\u56de\u5f52\u6a21\u578b\u7684\"\u6269\u6563\u4f18\u52bf\"\u8fc8\u8fdb\u3002", "conclusion": "dUltra\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u89e3\u63a9\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5e76\u884c\u89e3\u7801\u6548\u7387\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u63a8\u7406\u6b65\u9aa4\uff0c\u4e3a\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u4f18\u52bf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.21613", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21613", "abs": "https://arxiv.org/abs/2512.21613", "authors": ["Zhishuai Zhang", "Xintian Li", "Shilong Liu", "Aodong Zhang", "Lu Jie", "Nan Sun"], "title": "AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design", "comment": "8 pages, 5 figures. Accepted to AAAI 2026", "summary": "In this paper, we propose AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware input/output (I/O) subsystem generation in analog and mixed-signal (AMS) integrated circuits (ICs). The central contribution of this work is a framework that connects natural language design intent with industrial-level AMS IC design deliverables. AMS-IO-Agent integrates two key capabilities: (1) a structured domain knowledge base that captures reusable constraints and design conventions; (2) design intent structuring, which converts ambiguous user intent into verifiable logic steps using JSON and Python as intermediate formats. We further introduce AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. On this benchmark, AMS-IO-Agent achieves over 70\\% DRC+LVS pass rate and reduces design turnaround time from hours to minutes, outperforming the baseline LLM. Furthermore, an agent-generated I/O ring was fabricated and validated in a 28 nm CMOS tape-out, demonstrating the practical effectiveness of the approach in real AMS IC design flows. To our knowledge, this is the first reported human-agent collaborative AMS IC design in which an LLM-based agent completes a nontrivial subtask with outputs directly used in silicon.", "AI": {"tldr": "\u63d0\u51faAMS-IO-Agent\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u9886\u57df\u4e13\u7528\u4ee3\u7406\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u6df7\u5408\u4fe1\u53f7\u96c6\u6210\u7535\u8def\u7684\u7ed3\u6784\u611f\u77e5I/O\u5b50\u7cfb\u7edf\u751f\u6210\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8bbe\u8ba1\u610f\u56fe\u8f6c\u5316\u4e3a\u5de5\u4e1a\u7ea7\u8bbe\u8ba1\u4ea4\u4ed8\u7269\u3002", "motivation": "\u4f20\u7edfAMS IC\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\uff0cI/O\u5b50\u7cfb\u7edf\u751f\u6210\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\uff0c\u8bbe\u8ba1\u5468\u671f\u957f\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u9700\u8981\u5c06\u81ea\u7136\u8bed\u8a00\u8bbe\u8ba1\u610f\u56fe\u4e0e\u5de5\u4e1a\u7ea7\u8bbe\u8ba1\u4ea4\u4ed8\u7269\u8fde\u63a5\u8d77\u6765\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u7ed3\u6784\u5316\u9886\u57df\u77e5\u8bc6\u5e93\u548c\u8bbe\u8ba1\u610f\u56fe\u7ed3\u6784\u5316\u4e24\u4e2a\u5173\u952e\u80fd\u529b\u7684\u6846\u67b6\u3002\u77e5\u8bc6\u5e93\u6355\u83b7\u53ef\u91cd\u7528\u7ea6\u675f\u548c\u8bbe\u8ba1\u60ef\u4f8b\uff0c\u8bbe\u8ba1\u610f\u56fe\u7ed3\u6784\u5316\u5c06\u6a21\u7cca\u7528\u6237\u610f\u56fe\u8f6c\u6362\u4e3a\u53ef\u9a8c\u8bc1\u7684\u903b\u8f91\u6b65\u9aa4\uff0c\u4f7f\u7528JSON\u548cPython\u4f5c\u4e3a\u4e2d\u95f4\u683c\u5f0f\u3002", "result": "\u5728AMS-IO-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fbe\u5230\u8d85\u8fc770%\u7684DRC+LVS\u901a\u8fc7\u7387\uff0c\u5c06\u8bbe\u8ba1\u5468\u8f6c\u65f6\u95f4\u4ece\u5c0f\u65f6\u7ea7\u7f29\u77ed\u5230\u5206\u949f\u7ea7\uff0c\u4f18\u4e8e\u57fa\u7ebfLLM\u3002\u4ee3\u7406\u751f\u6210\u7684I/O\u73af\u572828nm CMOS\u6d41\u7247\u4e2d\u6210\u529f\u5236\u9020\u5e76\u9a8c\u8bc1\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u62a5\u9053\u7684\u4eba\u673a\u534f\u4f5cAMS IC\u8bbe\u8ba1\uff0c\u5176\u4e2d\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5b8c\u6210\u4e86\u975e\u5e73\u51e1\u5b50\u4efb\u52a1\uff0c\u8f93\u51fa\u76f4\u63a5\u7528\u4e8e\u7845\u7247\u5236\u9020\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645AMS IC\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "code agent"}}
{"id": "2512.21431", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21431", "abs": "https://arxiv.org/abs/2512.21431", "authors": ["Hridya Dhulipala", "Xiaokai Rong", "Tien N. Nguyen"], "title": "Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors", "comment": null, "summary": "In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.", "AI": {"tldr": "Cerberus\u662f\u4e00\u4e2a\u65e0\u9700\u5b9e\u9645\u6267\u884c\u7684\u9884\u6d4b\u6027\u8986\u76d6\u5f15\u5bfc\u6d4b\u8bd5\u6846\u67b6\uff0c\u4f7f\u7528LLMs\u751f\u6210\u89e6\u53d1\u8fd0\u884c\u65f6\u9519\u8bef\u7684\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u4ee3\u7801\u8986\u76d6\u9884\u6d4b\u548c\u9519\u8bef\u68c0\u6d4b\u6765\u53d1\u73b0\u4ee3\u7801\u7247\u6bb5\u4e2d\u7684\u8fd0\u884c\u65f6\u5f02\u5e38\u3002", "motivation": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u9700\u8981\u5728\u65e0\u9700\u5b9e\u9645\u6267\u884c\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u4ee3\u7801\u7247\u6bb5\u4e2d\u7684\u8fd0\u884c\u65f6\u9519\u8bef\u548c\u5f02\u5e38\uff0c\u7279\u522b\u662f\u5728\u5c06\u5728\u7ebf\u4ee3\u7801\u7247\u6bb5\u96c6\u6210\u5230\u4ee3\u7801\u5e93\u4e4b\u524d\u8fdb\u884c\u5b89\u5168\u68c0\u6d4b\u3002", "method": "Cerberus\u91c7\u7528\u4e24\u9636\u6bb5\u53cd\u9988\u5faa\u73af\uff1a\u7b2c\u4e00\u9636\u6bb5\u540c\u65f6\u589e\u52a0\u4ee3\u7801\u8986\u76d6\u7387\u548c\u68c0\u6d4b\u8fd0\u884c\u65f6\u9519\u8bef\uff1b\u5f53\u8986\u76d6\u7387\u8fbe\u5230100%\u6216\u6700\u5927\u503c\u65f6\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u8fd0\u884c\u65f6\u9519\u8bef\u3002\u6846\u67b6\u4f7f\u7528LLMs\u751f\u6210\u6d4b\u8bd5\u8f93\u5165\uff0c\u5e76\u8fdb\u884c\u4ee3\u7801\u8986\u76d6\u9884\u6d4b\u548c\u9519\u8bef\u68c0\u6d4b\uff0c\u65e0\u9700\u5b9e\u9645\u6267\u884c\u4ee3\u7801\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cCerberus\u5728\uff08\u4e0d\uff09\u5b8c\u6574\u4ee3\u7801\u7247\u6bb5\u4e0a\u6bd4\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u6d4b\u8bd5\u6846\u67b6\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u66f4\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u8986\u76d6\u7387\u6d4b\u8bd5\u7528\u4f8b\uff0c\u53d1\u73b0\u66f4\u591a\u8fd0\u884c\u65f6\u9519\u8bef\u3002", "conclusion": "Cerberus\u901a\u8fc7LLMs\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u6267\u884c\u7684\u4ee3\u7801\u8986\u76d6\u5f15\u5bfc\u6d4b\u8bd5\uff0c\u5728\u68c0\u6d4b\u8fd0\u884c\u65f6\u9519\u8bef\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2512.21623", "categories": ["cs.AI", "cs.MA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.21623", "abs": "https://arxiv.org/abs/2512.21623", "authors": ["Takahide Suzuki", "Kazuki Nakanishi", "Takashi Fujiwara", "Hideyuki Shimizu"], "title": "Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design", "comment": "51 pages, 4 figures (with supplementary information)", "summary": "Therapeutic discovery remains a formidable challenge, impeded by the fragmentation of specialized domains and the execution gap between computational design and physiological validation. Although generative AI offers promise, current models often function as passive assistants rather than as autonomous executors. Here, we introduce OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine. Unlike static code generators, our agents actively execute simulations and reason the results to drive iterative optimization. Governed by an Orchestrator, a Biologist Agent leverages deep reasoning over a massive knowledge graph (>10 million associations) to pinpoint high-confidence targets; a Chemist Agent autonomously detects structural pockets for de novo design or drug repositioning; and a Pharmacologist Agent evaluates candidates via rigorous physiologically based pharmacokinetic (PBPK) simulations. This architecture establishes a dynamic feedback loop where pharmacokinetic and toxicity profiles directly trigger structural reoptimization. By seamlessly integrating autonomous execution with human guidance, OrchestRA democratizes therapeutic design, transforming drug discovery from a stochastic search to a programmable evidence-based engineering discipline.", "AI": {"tldr": "OrchestRA\u662f\u4e00\u4e2a\u4eba\u7c7b\u5728\u73af\u7684\u591a\u667a\u80fd\u4f53\u5e73\u53f0\uff0c\u5c06\u751f\u7269\u5b66\u3001\u5316\u5b66\u548c\u836f\u7406\u5b66\u7edf\u4e00\u4e3a\u81ea\u4e3b\u53d1\u73b0\u5f15\u64ce\uff0c\u901a\u8fc7\u81ea\u4e3b\u6267\u884c\u6a21\u62df\u548c\u63a8\u7406\u7ed3\u679c\u6765\u9a71\u52a8\u8fed\u4ee3\u4f18\u5316\uff0c\u5c06\u836f\u7269\u53d1\u73b0\u4ece\u968f\u673a\u641c\u7d22\u8f6c\u53d8\u4e3a\u53ef\u7f16\u7a0b\u7684\u5faa\u8bc1\u5de5\u7a0b\u5b66\u79d1\u3002", "motivation": "\u5f53\u524d\u6cbb\u7597\u53d1\u73b0\u9762\u4e34\u4e13\u4e1a\u9886\u57df\u788e\u7247\u5316\u548c\u8ba1\u7b97\u8bbe\u8ba1\u4e0e\u751f\u7406\u9a8c\u8bc1\u4e4b\u95f4\u7684\u6267\u884c\u5dee\u8ddd\u7b49\u6311\u6218\uff0c\u73b0\u6709\u751f\u6210\u5f0fAI\u6a21\u578b\u901a\u5e38\u4f5c\u4e3a\u88ab\u52a8\u52a9\u624b\u800c\u975e\u81ea\u4e3b\u6267\u884c\u8005\uff0c\u9700\u8981\u66f4\u81ea\u4e3b\u7684\u53d1\u73b0\u5e73\u53f0\u3002", "method": "OrchestRA\u91c7\u7528\u4eba\u7c7b\u5728\u73af\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u542b\uff1a1) Orchestrator\u534f\u8c03\u5668\uff1b2) Biologist Agent\u5229\u7528\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\u8bc6\u522b\u9776\u70b9\uff1b3) Chemist Agent\u81ea\u4e3b\u68c0\u6d4b\u7ed3\u6784\u53e3\u888b\u8fdb\u884c\u4ece\u5934\u8bbe\u8ba1\u6216\u836f\u7269\u91cd\u5b9a\u4f4d\uff1b4) Pharmacologist Agent\u901a\u8fc7PBPK\u6a21\u62df\u8bc4\u4f30\u5019\u9009\u836f\u7269\u3002\u5efa\u7acb\u52a8\u6001\u53cd\u9988\u5faa\u73af\uff0c\u836f\u4ee3\u52a8\u529b\u5b66\u548c\u6bd2\u6027\u7279\u5f81\u76f4\u63a5\u89e6\u53d1\u7ed3\u6784\u91cd\u65b0\u4f18\u5316\u3002", "result": "\u5e73\u53f0\u5efa\u7acb\u4e86\u52a8\u6001\u53cd\u9988\u5faa\u73af\uff0c\u5c06\u836f\u4ee3\u52a8\u529b\u5b66\u548c\u6bd2\u6027\u7279\u5f81\u76f4\u63a5\u4e0e\u7ed3\u6784\u91cd\u65b0\u4f18\u5316\u76f8\u8fde\u63a5\uff0c\u5b9e\u73b0\u4e86\u6cbb\u7597\u8bbe\u8ba1\u7684\u6c11\u4e3b\u5316\uff0c\u5c06\u836f\u7269\u53d1\u73b0\u4ece\u968f\u673a\u641c\u7d22\u8f6c\u53d8\u4e3a\u53ef\u7f16\u7a0b\u7684\u5faa\u8bc1\u5de5\u7a0b\u5b66\u79d1\u3002", "conclusion": "OrchestRA\u901a\u8fc7\u65e0\u7f1d\u96c6\u6210\u81ea\u4e3b\u6267\u884c\u4e0e\u4eba\u7c7b\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u6cbb\u7597\u8bbe\u8ba1\u7684\u6c11\u4e3b\u5316\uff0c\u5c06\u836f\u7269\u53d1\u73b0\u4ece\u968f\u673a\u641c\u7d22\u8f6c\u53d8\u4e3a\u53ef\u7f16\u7a0b\u7684\u5faa\u8bc1\u5de5\u7a0b\u5b66\u79d1\u3002", "topic": "agent analysis"}}
{"id": "2512.21450", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21450", "abs": "https://arxiv.org/abs/2512.21450", "authors": ["Lei Zhao", "Zihao Ma", "Boyu Lin", "Yuhe Liu", "Wenjun Wu", "Lei Huang"], "title": "RLLaVA: An RL-central Framework for Language and Vision Assistants", "comment": "The code is available at https://github.com/TinyLoopX/RLLaVA", "summary": "We present an RL-central framework for Language and Vision Assistants (RLLaVA) with its formulation of Markov decision process (MDP). RLLaVA decouples RL algorithmic logic from model architecture and distributed execution, supporting researchers in implementing new RL algorithms with minimal code, and to plug in a broad family of RL methods and vision-language models (VLMs) while remaining agnostic to specific training and inference engines. RLLaVA makes resource-efficient training of 1B--7B models feasible on common GPUs; notably, 4B-scale models can be trained end-to-end with full-parameter updates on a single 24GB GPU. Experiments on multi-modal and agentic tasks demonstrate that RLLaVA has task extensibility, and the models trained with it consistently improve performance over base models, competitive with other specially engineered RL frameworks. The code is available at https://github.com/TinyLoopX/RLLaVA.", "AI": {"tldr": "RLLaVA\u662f\u4e00\u4e2a\u5c06\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u903b\u8f91\u4e0e\u6a21\u578b\u67b6\u6784\u548c\u5206\u5e03\u5f0f\u6267\u884c\u89e3\u8026\u7684\u6846\u67b6\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u7528\u6700\u5c11\u4ee3\u7801\u5b9e\u73b0\u65b0RL\u7b97\u6cd5\uff0c\u5e76\u517c\u5bb9\u591a\u79cdRL\u65b9\u6cd5\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u5728\u5355\u5f2024GB GPU\u4e0a\u9ad8\u6548\u8bad\u7ec31B-7B\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u52a9\u624b\u8bad\u7ec3\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u901a\u5e38\u4e0e\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u5f15\u64ce\u7d27\u5bc6\u8026\u5408\uff0c\u9650\u5236\u4e86\u7814\u7a76\u7075\u6d3b\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002\u9700\u8981\u4e00\u79cd\u89e3\u8026\u7684\u6846\u67b6\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u8f7b\u677e\u5b9e\u73b0\u65b0RL\u7b97\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u6709\u9650\u786c\u4ef6\u8d44\u6e90\u4e0a\u9ad8\u6548\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(MDP)\u7684RL-central\u6846\u67b6RLLaVA\uff0c\u5c06RL\u7b97\u6cd5\u903b\u8f91\u4e0e\u6a21\u578b\u67b6\u6784\u3001\u5206\u5e03\u5f0f\u6267\u884c\u89e3\u8026\u3002\u6846\u67b6\u652f\u6301\u591a\u79cdRL\u65b9\u6cd5\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4fdd\u6301\u5bf9\u7279\u5b9a\u8bad\u7ec3\u548c\u63a8\u7406\u5f15\u64ce\u7684\u4e0d\u53ef\u77e5\u6027\uff0c\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "RLLaVA\u80fd\u5728\u5355\u5f2024GB GPU\u4e0a\u7aef\u5230\u7aef\u8bad\u7ec34B\u89c4\u6a21\u6a21\u578b\uff0c\u652f\u63011B-7B\u6a21\u578b\u8bad\u7ec3\u3002\u5728\u591a\u6a21\u6001\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528RLLaVA\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u6301\u7eed\u8d85\u8d8a\u57fa\u7840\u6a21\u578b\uff0c\u4e0e\u5176\u4ed6\u4e13\u95e8\u8bbe\u8ba1\u7684RL\u6846\u67b6\u7ade\u4e89\u529b\u76f8\u5f53\u3002", "conclusion": "RLLaVA\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u5b9e\u73b0\u4e86RL\u7b97\u6cd5\u7684\u7075\u6d3b\u5b9e\u73b0\u548c\u8d44\u6e90\u9ad8\u6548\u8bad\u7ec3\uff0c\u4e3a\u591a\u6a21\u6001\u52a9\u624b\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.21699", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21699", "abs": "https://arxiv.org/abs/2512.21699", "authors": ["Eranga Bandara", "Tharaka Hewa", "Ross Gore", "Sachin Shetty", "Ravi Mukkamala", "Peter Foytik", "Abdul Rahman", "Safdar H. Bouk", "Xueping Liang", "Amin Hass", "Sachini Rajapakse", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning", "comment": null, "summary": "Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u578b\u5171\u8bc6\u548c\u63a8\u7406\u5c42\u6cbb\u7406\u7684\u8d1f\u8d23\u4efb\u53ef\u89e3\u91caAI\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5f02\u6784\u4ee3\u7406\u72ec\u7acb\u751f\u6210\u5019\u9009\u8f93\u51fa\uff0c\u518d\u7531\u4e13\u7528\u63a8\u7406\u4ee3\u7406\u8fdb\u884c\u7ed3\u6784\u5316\u6574\u5408\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7cfb\u7edf\u5728\u81ea\u4e3b\u6027\u589e\u5f3a\u7684\u540c\u65f6\uff0c\u9762\u4e34\u53ef\u89e3\u91ca\u6027\u3001\u95ee\u8d23\u5236\u3001\u9c81\u68d2\u6027\u548c\u6cbb\u7406\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u5b9e\u73b0\u5f80\u5f80\u5f3a\u8c03\u529f\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u7f3a\u4e4f\u7406\u89e3\u51b3\u7b56\u539f\u7406\u548c\u6267\u884c\u8d23\u4efb\u5236\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faRAI\u548cXAI\u4ee3\u7406\u67b6\u6784\uff0c\u91c7\u7528\u5f02\u6784LLM\u548cVLM\u4ee3\u7406\u4ece\u5171\u4eab\u8f93\u5165\u4e0a\u4e0b\u6587\u72ec\u7acb\u751f\u6210\u5019\u9009\u8f93\u51fa\uff0c\u66b4\u9732\u4e0d\u786e\u5b9a\u6027\u548c\u5206\u6b67\uff0c\u7136\u540e\u7531\u4e13\u7528\u63a8\u7406\u4ee3\u7406\u8fdb\u884c\u7ed3\u6784\u5316\u6574\u5408\uff0c\u5f3a\u5236\u6267\u884c\u5b89\u5168\u548c\u7b56\u7565\u7ea6\u675f\uff0c\u51cf\u8f7b\u5e7b\u89c9\u548c\u504f\u89c1\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9645AI\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u8bc4\u4f30\u8be5\u67b6\u6784\uff0c\u8bc1\u660e\u5171\u8bc6\u9a71\u52a8\u7684\u63a8\u7406\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3001\u900f\u660e\u5ea6\u548c\u8de8\u4e0d\u540c\u5e94\u7528\u9886\u57df\u7684\u64cd\u4f5c\u4fe1\u4efb\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8bbe\u8ba1\u65e2\u81ea\u4e3b\u53ef\u6269\u5c55\u53c8\u8d1f\u8d23\u4efb\u53ef\u89e3\u91ca\u7684AI\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u901a\u8fc7\u67b6\u6784\u8bbe\u8ba1\u5b9e\u73b0\u8d23\u4efb\u548c\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.21708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21708", "abs": "https://arxiv.org/abs/2512.21708", "authors": ["Jing Han", "Binwei Yan", "Tianyu Guo", "Zheyuan Bai", "Mengyu Zheng", "Hanting Chen", "Ying Nie"], "title": "MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles", "comment": "Accepted by ICML 2025", "summary": "Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant Reason+Action paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification. We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mixture-of-Roles (MoR)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u667a\u80fd\u4f53\u4efb\u52a1\u5206\u89e3\u4e3a\u63a8\u7406\u8005\u3001\u6267\u884c\u8005\u548c\u603b\u7ed3\u8005\u4e09\u4e2a\u89d2\u8272\uff0c\u4f7f\u7528\u4e13\u95e8\u7684LoRA\u7ec4\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5728\u591a\u4e2aLLM\u548c\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u6700\u8fd1\u5728\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u4fc3\u8fdb\u667a\u80fd\u4f53\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u9488\u5bf9\u667a\u80fd\u4f53\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u4ecd\u7136\u7f3a\u4e4f\u63a2\u7d22\u3002\u5f53\u524d\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5206\u89e3\u667a\u80fd\u4f53\u80fd\u529b\u5e76\u5b9e\u73b0\u9ad8\u6548\u5fae\u8c03\u7684\u65b9\u6cd5\u3002", "method": "1) \u5c06\u667a\u80fd\u4f53\u4efb\u52a1\u80fd\u529b\u5206\u89e3\u4e3a\u4e09\u4e2a\u89d2\u8272\uff1a\u63a8\u7406\u8005\uff08\u7406\u89e3\u67e5\u8be2\u5e76\u51b3\u5b9a\u4e0b\u4e00\u6b65\u89d2\u8272\uff09\u3001\u6267\u884c\u8005\uff08\u8bc6\u522b\u8981\u8c03\u7528\u7684\u51fd\u6570\u548c\u53c2\u6570\uff09\u3001\u603b\u7ed3\u8005\uff08\u5411\u7528\u6237\u4f20\u9012\u5bf9\u8bdd\u7684\u63d0\u70bc\u4fe1\u606f\uff09\u30022) \u63d0\u51faMixture-of-Roles\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u95e8\u7684LoRA\u7ec4\uff0c\u6bcf\u4e2a\u7ec4\u8d1f\u8d23\u4e00\u4e2a\u89d2\u8272\u30023) \u5f00\u53d1\u57fa\u4e8e\u516c\u5f00\u6570\u636e\u96c6\u7684\u591a\u89d2\u8272\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5305\u542b\u89d2\u8272\u7279\u5b9a\u7684\u5185\u5bb9\u8865\u5168\u548c\u53ef\u9760\u6027\u9a8c\u8bc1\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u9879\u76ee\u5df2\u516c\u5f00\u5728https://mor-agent.github.io\u3002", "conclusion": "MoR\u6846\u67b6\u901a\u8fc7\u89d2\u8272\u5206\u89e3\u548c\u4e13\u95e8\u7684LoRA\u7ec4\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u4efb\u52a1\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u4e3a\u667a\u80fd\u4f53PEFT\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.21591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21591", "abs": "https://arxiv.org/abs/2512.21591", "authors": ["Shuo Sun", "Shixin Zhang", "Jiwei Yan", "Jun Yan", "Jian Zhang"], "title": "Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code", "comment": "Accepted by FSE 2026", "summary": "Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \\methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \\methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \\methodName~significantly outperformed prior works, achieving a \\textit{TypeSim} score of 0.89 and a \\textit{TypeExact} score of 0.84, representing a 27\\% and 40\\% relative improvement over the strongest baseline. More importantly, \\methodName~removed new type errors introduced by the tool by 92.7\\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u4ed3\u5e93\u7ea7Python\u7c7b\u578b\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u578b\u4e0e\u4f9d\u8d56\u5173\u7cfb\u7684\u534f\u540c\u6f14\u5316\u5b9e\u73b0\u51c6\u786e\u63a8\u65ad", "motivation": "Python\u7684\u52a8\u6001\u7c7b\u578b\u673a\u5236\u867d\u7136\u7075\u6d3b\uff0c\u4f46\u5bb9\u6613\u5bfc\u81f4\u8fd0\u884c\u65f6\u7c7b\u578b\u9519\u8bef\uff0c\u73b0\u6709\u7c7b\u578b\u63a8\u65ad\u5de5\u5177\u4e3b\u8981\u5904\u7406\u5b64\u7acb\u4ee3\u7801\u7247\u6bb5\uff0c\u96be\u4ee5\u5e94\u5bf9\u4ed3\u5e93\u7ea7\u590d\u6742\u4f9d\u8d56\u5173\u7cfb", "method": "\u6784\u5efa\u5b9e\u4f53\u4f9d\u8d56\u56fe(EDG)\u5efa\u6a21\u4ed3\u5e93\u7ea7\u7c7b\u578b\u4f9d\u8d56\uff0c\u901a\u8fc7\u8fed\u4ee3\u65b9\u5f0f\u534f\u540c\u6f14\u5316\u7c7b\u578b\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u91c7\u7528\u7c7b\u578b\u68c0\u67e5\u5668\u5728\u73af\u7b56\u7565\u5b9e\u65f6\u9a8c\u8bc1\u548c\u4fee\u6b63\u63a8\u65ad\u7ed3\u679c", "result": "\u572812\u4e2a\u590d\u6742Python\u4ed3\u5e93\u4e0a\u8bc4\u4f30\uff0cTypeSim\u5f97\u52060.89\uff0cTypeExact\u5f97\u52060.84\uff0c\u5206\u522b\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u534727%\u548c40%\uff0c\u51cf\u5c11\u4e8692.7%\u7684\u5de5\u5177\u5f15\u5165\u65b0\u7c7b\u578b\u9519\u8bef", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u3001\u53ef\u9760\u7684Python\u7c7b\u578b\u6807\u6ce8\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u9645Python\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4ed3\u5e93\u7ea7\u7c7b\u578b\u63a8\u65ad\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "2512.21782", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2512.21782", "abs": "https://arxiv.org/abs/2512.21782", "authors": ["Yuanqi Du", "Botao Yu", "Tianyu Liu", "Tony Shen", "Junwu Chen", "Jan G. Rittig", "Kunyang Sun", "Yikun Zhang", "Zhangde Song", "Bo Zhou", "Cassandra Masschelein", "Yingze Wang", "Haorui Wang", "Haojun Jia", "Chao Zhang", "Hongyu Zhao", "Martin Ester", "Teresa Head-Gordon", "Carla P. Gomes", "Huan Sun", "Chenru Duan", "Philippe Schwaller", "Wengong Jin"], "title": "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents", "comment": null, "summary": "There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSAGA\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u76ee\u6807\u51fd\u6570\u8bbe\u8ba1\u6765\u6539\u8fdb\u79d1\u5b66\u53d1\u73b0\u4ee3\u7406\uff0c\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff1a\u5916\u5c42LLM\u4ee3\u7406\u5206\u6790\u4f18\u5316\u7ed3\u679c\u5e76\u63d0\u51fa\u65b0\u76ee\u6807\uff0c\u5185\u5c42\u6267\u884c\u5f53\u524d\u76ee\u6807\u4e0b\u7684\u89e3\u51b3\u65b9\u6848\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u53d1\u73b0\u4ee3\u7406\u4e3b\u8981\u4f9d\u8d56\u79d1\u5b66\u5bb6\u6307\u5b9a\u7684\u5b9a\u91cf\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u4f46\u8fd9\u4e9b\u76ee\u6807\u51fd\u6570\u5f80\u5f80\u53ea\u662f\u4e0d\u5b8c\u7f8e\u7684\u4ee3\u7406\u6307\u6807\u3002\u5bf9\u4e8e\u79d1\u5b66\u9886\u57df\u7684\u91cd\u5927\u6311\u6218\uff0c\u81ea\u52a8\u5316\u76ee\u6807\u51fd\u6570\u8bbe\u8ba1\u662f\u4e00\u4e2a\u6838\u5fc3\u4f46\u5c1a\u672a\u6ee1\u8db3\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u79d1\u5b66\u81ea\u4e3b\u76ee\u6807\u6f14\u5316\u4ee3\u7406(SAGA)\uff0c\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff1a\u5916\u5c42LLM\u4ee3\u7406\u5206\u6790\u4f18\u5316\u7ed3\u679c\u3001\u63d0\u51fa\u65b0\u76ee\u6807\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u8ba1\u7b97\u8bc4\u5206\u51fd\u6570\uff1b\u5185\u5c42\u5728\u5f53\u524d\u76ee\u6807\u4e0b\u6267\u884c\u89e3\u51b3\u65b9\u6848\u4f18\u5316\u3002\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u7cfb\u7edf\u63a2\u7d22\u76ee\u6807\u7a7a\u95f4\u53ca\u5176\u6743\u8861\u3002", "result": "\u5728\u6297\u751f\u7d20\u8bbe\u8ba1\u3001\u65e0\u673a\u6750\u6599\u8bbe\u8ba1\u3001\u529f\u80fd\u6027DNA\u5e8f\u5217\u8bbe\u8ba1\u548c\u5316\u5b66\u8fc7\u7a0b\u8bbe\u8ba1\u7b49\u591a\u4e2a\u5e94\u7528\u9886\u57df\u5c55\u793a\uff0c\u81ea\u52a8\u5316\u76ee\u6807\u5236\u5b9a\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u79d1\u5b66\u53d1\u73b0\u4ee3\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u81ea\u52a8\u5316\u76ee\u6807\u51fd\u6570\u8bbe\u8ba1\u662f\u79d1\u5b66\u53d1\u73b0\u4ee3\u7406\u7684\u5173\u952e\u6539\u8fdb\u65b9\u5411\uff0cSAGA\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u67b6\u6784\u5b9e\u73b0\u4e86\u76ee\u6807\u7a7a\u95f4\u7684\u7cfb\u7edf\u63a2\u7d22\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2512.21757", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21757", "abs": "https://arxiv.org/abs/2512.21757", "authors": ["Huiyun Peng", "Antonio Zhong", "Ricardo Andr\u00e9s Calvo M\u00e9ndez", "Kelechi G. Kalu", "James C. Davis"], "title": "How Do Agents Perform Code Optimization? An Empirical Study", "comment": null, "summary": "Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\\% vs. 63.6\\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.", "AI": {"tldr": "\u9996\u4e2a\u6bd4\u8f83AI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u6027\u80fd\u4f18\u5316\u63d0\u4ea4\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0AI\u751f\u6210\u7684PR\u66f4\u5c11\u5305\u542b\u663e\u5f0f\u6027\u80fd\u9a8c\u8bc1\uff0c\u4f46\u4f18\u5316\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u3002", "motivation": "\u6027\u80fd\u4f18\u5316\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u65b9\u9762\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u7cfb\u7edf\u884c\u4e3a\u3001\u7b97\u6cd5\u6743\u8861\u548c\u4ed4\u7ec6\u7684\u4ee3\u7801\u4fee\u6539\u3002\u5c3d\u7ba1AI\u7f16\u7801\u4ee3\u7406\u5728\u4ee3\u7801\u751f\u6210\u548c\u9519\u8bef\u4fee\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5bf9\u5176\u5728\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u4f18\u5316\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u5bf9AIDev\u6570\u636e\u96c6\u4e2d\u7684324\u4e2aAI\u751f\u6210\u548c83\u4e2a\u4eba\u7c7b\u7f16\u5199\u7684\u6027\u80fd\u4f18\u5316PR\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u91c7\u7eb3\u7387\u3001\u53ef\u7ef4\u62a4\u6027\u3001\u4f18\u5316\u6a21\u5f0f\u548c\u9a8c\u8bc1\u5b9e\u8df5\u3002", "result": "AI\u7f16\u5199\u7684\u6027\u80fdPR\u5305\u542b\u663e\u5f0f\u6027\u80fd\u9a8c\u8bc1\u7684\u53ef\u80fd\u6027\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u7f16\u5199\u7684PR\uff0845.7% vs. 63.6%\uff0cp=0.007\uff09\u3002AI\u7f16\u5199\u7684PR\u4e3b\u8981\u4f7f\u7528\u4e0e\u4eba\u7c7b\u76f8\u540c\u7684\u4f18\u5316\u6a21\u5f0f\u3002", "conclusion": "\u8ba8\u8bba\u4e86AI\u4ee3\u7406\u4ee3\u7801\u4f18\u5316\u7684\u5c40\u9650\u6027\u548c\u673a\u4f1a\uff0c\u4e3a\u63a8\u8fdb\u4ee3\u7406\u5f0f\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2512.21818", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21818", "abs": "https://arxiv.org/abs/2512.21818", "authors": ["Brian Bowers", "Smita Khapre", "Jugal Kalita"], "title": "Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development", "comment": null, "summary": "Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u53d1\u73b0coder-reviewer-tester\u67b6\u6784\u6bd4coder\u548ccoder-tester\u67b6\u6784\u66f4\u5b89\u5168\u4f46\u6548\u7387\u8f83\u4f4e\uff0c\u901a\u8fc7\u6dfb\u52a0\u5b89\u5168\u5206\u6790\u4ee3\u7406\u53ef\u63d0\u5347\u6548\u7387\u548c\u5b89\u5168\uff0c\u4f46\u5b89\u5168\u4ee3\u7406\u672c\u8eab\u4ecd\u6613\u53d7\u9ad8\u7ea7\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53AI\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5373\u5c06\u5728\u5de5\u4e1a\u548c\u793e\u4f1a\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u4ece\u88ab\u52a8\u5185\u5bb9\u751f\u6210\u8f6c\u5411\u4e3b\u52a8\u591a\u4efb\u52a1\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5176\u81ea\u4e3b\u8bbe\u8ba1\u548c\u7f3a\u4e4f\u4eba\u5de5\u5e72\u9884\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u65e0\u6cd5\u81ea\u884c\u8bc6\u522b\u548c\u5e94\u5bf9\u653b\u51fb\uff0c\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u73b0\u9636\u6bb5\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u5efa\u7acb\u5168\u9762\u7684\u5a01\u80c1\u6a21\u578b\u3002\u5206\u6790\u4e0d\u540c\u67b6\u6784\uff08coder\u3001coder-tester\u3001coder-reviewer-tester\uff09\u7684\u8106\u5f31\u6027\uff0c\u8bc4\u4f30\u6dfb\u52a0\u5b89\u5168\u5206\u6790\u4ee3\u7406\u7684\u6548\u679c\uff0c\u5e76\u6f14\u793a\u9ad8\u7ea7\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\u3002", "result": "coder-reviewer-tester\u67b6\u6784\u6bd4coder\u548ccoder-tester\u67b6\u6784\u66f4\u5177\u5f39\u6027\u4f46\u7f16\u7801\u6548\u7387\u8f83\u4f4e\u3002\u6dfb\u52a0\u5b89\u5168\u5206\u6790\u4ee3\u7406\u53ef\u7f13\u89e3\u6548\u7387\u635f\u5931\u5e76\u5b9e\u73b0\u66f4\u597d\u7684\u5f39\u6027\u3002\u7136\u800c\uff0c\u5b89\u5168\u5206\u6790\u4ee3\u7406\u672c\u8eab\u6613\u53d7\u9ad8\u7ea7\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\uff0c\u901a\u8fc7\u5d4c\u5165\u6709\u6bd2\u5c11\u6837\u672c\u793a\u4f8b\u53ef\u5c06\u653b\u51fb\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u81f371.95%\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u51c6\u786e\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\u3002\u5373\u4f7f\u6dfb\u52a0\u5b89\u5168\u5206\u6790\u4ee3\u7406\uff0c\u4ecd\u53ef\u80fd\u88ab\u9ad8\u7ea7\u653b\u51fb\u7ed5\u8fc7\u3002\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u673a\u5236\u6765\u4fdd\u62a4\u8fd9\u4e9b\u81ea\u4e3b\u7cfb\u7edf\u3002", "topic": "code agent"}}
{"id": "2512.21527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21527", "abs": "https://arxiv.org/abs/2512.21527", "authors": ["Aoyang Qin", "Deqian Kong", "Wei Wang", "Ying Nian Wu", "Song-Chun Zhu", "Sirui Xie"], "title": "Generative Actor Critic", "comment": null, "summary": "Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing \\textit{policy evaluation} as learning a generative model of the joint distribution over trajectories and returns, $p(\u03c4, y)$, and \\textit{policy improvement} as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both \\textit{exploitation}, by optimizing latent plans to maximize expected returns, and \\textit{exploration}, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC's strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.", "AI": {"tldr": "GAC\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u7b56\u7565\u8bc4\u4f30\u91cd\u6784\u4e3a\u5b66\u4e60\u8f68\u8ff9\u4e0e\u56de\u62a5\u7684\u8054\u5408\u5206\u5e03\u751f\u6210\u6a21\u578b\uff0c\u5c06\u7b56\u7565\u6539\u8fdb\u91cd\u6784\u4e3a\u5bf9\u8be5\u6a21\u578b\u7684\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5230\u5728\u7ebf\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f30\u8ba1\u6216\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\uff0c\u5728\u5229\u7528\u5728\u7ebf\u7ecf\u9a8c\u5fae\u8c03\u79bb\u7ebf\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u7ed3\u5408\u79bb\u7ebf\u9884\u8bad\u7ec3\u548c\u5728\u7ebf\u5b66\u4e60\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5f0f\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\uff08GAC\uff09\u6846\u67b6\uff1a1\uff09\u7b56\u7565\u8bc4\u4f30\uff1a\u5b66\u4e60\u8f68\u8ff9\u03c4\u548c\u56de\u62a5y\u7684\u8054\u5408\u5206\u5e03p(\u03c4,y)\uff1b2\uff09\u7b56\u7565\u6539\u8fdb\uff1a\u5bf9\u5b66\u4e60\u5230\u7684\u751f\u6210\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002\u5177\u4f53\u5b9e\u73b0\u57fa\u4e8e\u8fde\u7eed\u6f5c\u5728\u8ba1\u5212\u5411\u91cf\u7684\u6f5c\u53d8\u91cf\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u63a8\u7406\u7b56\u7565\uff1a\u5229\u7528\uff08\u4f18\u5316\u6f5c\u5728\u8ba1\u5212\u4ee5\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\uff09\u548c\u63a2\u7d22\uff08\u57fa\u4e8e\u52a8\u6001\u8c03\u6574\u7684\u76ee\u6807\u56de\u62a5\u91c7\u6837\u6f5c\u5728\u8ba1\u5212\uff09\u3002", "result": "\u5728Gym-MuJoCo\u548cMaze2D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGAC\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u79bb\u7ebf\u6027\u80fd\uff0c\u5e76\u4e14\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u6539\u8fdb\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u9010\u6b65\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "conclusion": "GAC\u901a\u8fc7\u5c06\u5e8f\u5217\u51b3\u7b56\u89e3\u8026\u4e3a\u751f\u6210\u5efa\u6a21\u548c\u63a8\u7406\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7279\u522b\u9002\u5408\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u5b66\u4e60\u573a\u666f\uff0c\u80fd\u591f\u540c\u65f6\u652f\u6301\u5229\u7528\u548c\u63a2\u7d22\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.21817", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21817", "abs": "https://arxiv.org/abs/2512.21817", "authors": ["Hong Su"], "title": "Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments", "comment": null, "summary": "Intelligent IoT systems increasingly rely on large language models (LLMs) to generate task-execution methods for dynamic environments. However, existing approaches lack the ability to systematically produce new methods when facing previously unseen situations, and they often depend on fixed, device-specific logic that cannot adapt to changing environmental conditions.In this paper, we propose Method Decoration (DeMe), a general framework that modifies the method-generation path of an LLM using explicit decorations derived from hidden goals, accumulated learned methods, and environmental feedback. Unlike traditional rule augmentation, decorations in DeMe are not hardcoded; instead, they are extracted from universal behavioral principles, experience, and observed environmental differences. DeMe enables the agent to reshuffle the structure of its method path-through pre-decoration, post-decoration, intermediate-step modification, and step insertion-thereby producing context-aware, safety-aligned, and environment-adaptive methods. Experimental results show that method decoration allows IoT devices to derive ore appropriate methods when confronting unknown or faulty operating conditions.", "AI": {"tldr": "DeMe\u6846\u67b6\u901a\u8fc7\u4ece\u9690\u85cf\u76ee\u6807\u3001\u79ef\u7d2f\u7ecf\u9a8c\u548c\u73af\u5883\u53cd\u9988\u4e2d\u63d0\u53d6\u663e\u5f0f\u88c5\u9970\u6765\u4fee\u6539LLM\u7684\u65b9\u6cd5\u751f\u6210\u8def\u5f84\uff0c\u4f7fIoT\u8bbe\u5907\u80fd\u9002\u5e94\u672a\u77e5\u6216\u6545\u969c\u73af\u5883", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684IoT\u7cfb\u7edf\u5728\u9047\u5230\u672a\u89c1\u60c5\u51b5\u65f6\u7f3a\u4e4f\u7cfb\u7edf\u751f\u6210\u65b0\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u4e14\u4f9d\u8d56\u56fa\u5b9a\u7684\u8bbe\u5907\u7279\u5b9a\u903b\u8f91\uff0c\u65e0\u6cd5\u9002\u5e94\u53d8\u5316\u7684\u73af\u5883\u6761\u4ef6", "method": "\u63d0\u51faMethod Decoration (DeMe)\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u9690\u85cf\u76ee\u6807\u3001\u79ef\u7d2f\u5b66\u4e60\u65b9\u6cd5\u548c\u73af\u5883\u53cd\u9988\u4e2d\u63d0\u53d6\u663e\u5f0f\u88c5\u9970\u6765\u4fee\u6539LLM\u7684\u65b9\u6cd5\u751f\u6210\u8def\u5f84\u3002\u88c5\u9970\u4e0d\u662f\u786c\u7f16\u7801\u7684\uff0c\u800c\u662f\u4ece\u901a\u7528\u884c\u4e3a\u539f\u5219\u3001\u7ecf\u9a8c\u548c\u89c2\u5bdf\u5230\u7684\u73af\u5883\u5dee\u5f02\u4e2d\u63d0\u53d6\u3002\u652f\u6301\u9884\u88c5\u9970\u3001\u540e\u88c5\u9970\u3001\u4e2d\u95f4\u6b65\u9aa4\u4fee\u6539\u548c\u6b65\u9aa4\u63d2\u5165\u56db\u79cd\u65b9\u5f0f", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b9\u6cd5\u88c5\u9970\u4f7fIoT\u8bbe\u5907\u5728\u9762\u5bf9\u672a\u77e5\u6216\u6545\u969c\u64cd\u4f5c\u6761\u4ef6\u65f6\u80fd\u591f\u63a8\u5bfc\u51fa\u66f4\u5408\u9002\u7684\u65b9\u6cd5", "conclusion": "DeMe\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u88c5\u9970LLM\u7684\u65b9\u6cd5\u751f\u6210\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u5b89\u5168\u5bf9\u9f50\u548c\u73af\u5883\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u751f\u6210\uff0c\u63d0\u5347\u4e86IoT\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b", "topic": "agent analysis"}}
{"id": "2512.21586", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21586", "abs": "https://arxiv.org/abs/2512.21586", "authors": ["Xin Liu", "Haoran Li", "Dongbin Zhao"], "title": "Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations", "comment": null, "summary": "Humans can efficiently extract knowledge and learn skills from the videos within only a few trials and errors. However, it poses a big challenge to replicate this learning process for autonomous agents, due to the complexity of visual input, the absence of action or reward signals, and the limitations of interaction steps. In this paper, we propose a novel, unsupervised, and sample-efficient framework to achieve imitation learning from videos (ILV), named Behavior Cloning from Videos via Latent Representations (BCV-LR). BCV-LR extracts action-related latent features from high-dimensional video inputs through self-supervised tasks, and then leverages a dynamics-based unsupervised objective to predict latent actions between consecutive frames. The pre-trained latent actions are fine-tuned and efficiently aligned to the real action space online (with collected interactions) for policy behavior cloning. The cloned policy in turn enriches the agent experience for further latent action finetuning, resulting in an iterative policy improvement that is highly sample-efficient.\n  We conduct extensive experiments on a set of challenging visual tasks, including both discrete control and continuous control. BCV-LR enables effective (even expert-level on some tasks) policy performance with only a few interactions, surpassing state-of-the-art ILV baselines and reinforcement learning methods (provided with environmental rewards) in terms of sample efficiency across 24/28 tasks. To the best of our knowledge, this work for the first time demonstrates that videos can support extremely sample-efficient visual policy learning, without the need to access any other expert supervision.", "AI": {"tldr": "BCV-LR\uff1a\u901a\u8fc7\u6f5c\u5728\u8868\u793a\u4ece\u89c6\u9891\u8fdb\u884c\u884c\u4e3a\u514b\u9686\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u4ea4\u4e92\u5373\u53ef\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u89c6\u89c9\u7b56\u7565\u5b66\u4e60", "motivation": "\u4eba\u7c7b\u80fd\u591f\u4ece\u5c11\u91cf\u89c6\u9891\u4e2d\u9ad8\u6548\u63d0\u53d6\u77e5\u8bc6\u548c\u5b66\u4e60\u6280\u80fd\uff0c\u4f46\u81ea\u4e3b\u667a\u80fd\u4f53\u96be\u4ee5\u590d\u5236\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u89c6\u89c9\u8f93\u5165\u590d\u6742\u3001\u7f3a\u4e4f\u52a8\u4f5c\u6216\u5956\u52b1\u4fe1\u53f7\u3001\u4ea4\u4e92\u6b65\u9aa4\u6709\u9650", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u4efb\u52a1\u4ece\u9ad8\u7ef4\u89c6\u9891\u8f93\u5165\u4e2d\u63d0\u53d6\u52a8\u4f5c\u76f8\u5173\u6f5c\u5728\u7279\u5f81\uff0c\u5229\u7528\u57fa\u4e8e\u52a8\u6001\u7684\u65e0\u76d1\u7763\u76ee\u6807\u9884\u6d4b\u8fde\u7eed\u5e27\u95f4\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u5728\u7ebf\u5fae\u8c03\u5e76\u5bf9\u9f50\u5230\u771f\u5b9e\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u7b56\u7565\u884c\u4e3a\u514b\u9686", "result": "\u572824/28\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u6700\u5148\u8fdb\u7684ILV\u57fa\u7ebf\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u63d0\u4f9b\u73af\u5883\u5956\u52b1\uff09\uff0c\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6027\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u4ea4\u4e92", "conclusion": "\u9996\u6b21\u8bc1\u660e\u89c6\u9891\u80fd\u591f\u652f\u6301\u6781\u5176\u6837\u672c\u9ad8\u6548\u7684\u89c6\u89c9\u7b56\u7565\u5b66\u4e60\uff0c\u65e0\u9700\u4efb\u4f55\u5176\u4ed6\u4e13\u5bb6\u76d1\u7763", "topic": "agentic reinforcement learning"}}
{"id": "2512.21648", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21648", "abs": "https://arxiv.org/abs/2512.21648", "authors": ["Maximilian Weichart"], "title": "Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search", "comment": null, "summary": "Monte Carlo Tree Search (MCTS) has profoundly influenced reinforcement learning (RL) by integrating planning and learning in tasks requiring long-horizon reasoning, exemplified by the AlphaZero family of algorithms. Central to MCTS is the search strategy, governed by a tree policy based on an upper confidence bound (UCB) applied to trees (UCT). A key factor in the success of AlphaZero is the introduction of a prior term in the UCB1-based tree policy PUCT, which improves exploration efficiency and thus accelerates training. While many alternative UCBs with stronger theoretical guarantees than UCB1 exist, extending them to prior-based UCTs has been challenging, since PUCT was derived empirically rather than from first principles. Recent work retrospectively justified PUCT by framing MCTS as a regularized policy optimization (RPO) problem. Building on this perspective, we introduce Inverse-RPO, a general methodology that systematically derives prior-based UCTs from any prior-free UCB. Applying this method to the variance-aware UCB-V, we obtain two new prior-based tree policies that incorporate variance estimates into the search. Experiments indicate that these variance-aware prior-based UCTs outperform PUCT across multiple benchmarks without incurring additional computational cost. We also provide an extension of the mctx library supporting variance-aware UCTs, showing that the required code changes are minimal and intended to facilitate further research on principled prior-based UCTs. Code: github.com/Max-We/inverse-rpo.", "AI": {"tldr": "\u63d0\u51faInverse-RPO\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u4ece\u4efb\u4f55\u65e0\u5148\u9a8cUCB\u63a8\u5bfc\u51fa\u5e26\u5148\u9a8c\u7684UCT\u7b56\u7565\uff0c\u5e94\u7528\u4e8eUCB-V\u5f97\u5230\u4e24\u4e2a\u65b0\u7684\u65b9\u5dee\u611f\u77e5\u6811\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8ePUCT\u4e14\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "MCTS\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7ed3\u5408\u89c4\u5212\u4e0e\u5b66\u4e60\uff0cAlphaZero\u7684\u6210\u529f\u90e8\u5206\u5f52\u529f\u4e8ePUCT\u4e2d\u5f15\u5165\u5148\u9a8c\u9879\u3002\u867d\u7136\u5b58\u5728\u6bd4UCB1\u7406\u8bba\u4fdd\u8bc1\u66f4\u5f3a\u7684\u66ff\u4ee3UCB\uff0c\u4f46\u5c06\u5176\u6269\u5c55\u5230\u5e26\u5148\u9a8c\u7684UCT\u4e00\u76f4\u5f88\u56f0\u96be\uff0c\u56e0\u4e3aPUCT\u662f\u7ecf\u9a8c\u6027\u800c\u975e\u539f\u7406\u6027\u63a8\u5bfc\u7684\u3002", "method": "\u57fa\u4e8eMCTS\u4f5c\u4e3a\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\u95ee\u9898\u7684\u89c6\u89d2\uff0c\u63d0\u51faInverse-RPO\u901a\u7528\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5730\u4ece\u4efb\u4f55\u65e0\u5148\u9a8cUCB\u63a8\u5bfc\u51fa\u5e26\u5148\u9a8c\u7684UCT\u3002\u5c06\u6b64\u65b9\u6cd5\u5e94\u7528\u4e8e\u65b9\u5dee\u611f\u77e5\u7684UCB-V\uff0c\u5f97\u5230\u4e24\u4e2a\u65b0\u7684\u5e26\u5148\u9a8c\u6811\u7b56\u7565\uff0c\u5e76\u6269\u5c55mctx\u5e93\u652f\u6301\u65b9\u5dee\u611f\u77e5UCT\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u5dee\u611f\u77e5\u7684\u5e26\u5148\u9a8cUCT\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8ePUCT\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002\u4ee3\u7801\u4fee\u6539\u91cf\u5c0f\uff0c\u4fbf\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "Inverse-RPO\u4e3a\u7cfb\u7edf\u63a8\u5bfc\u5e26\u5148\u9a8cUCT\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u65b9\u5dee\u611f\u77e5UCT\u5728\u6027\u80fd\u4e0a\u8d85\u8d8aPUCT\uff0c\u4e3aMCTS\u641c\u7d22\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6307\u5bfc\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.21919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21919", "abs": "https://arxiv.org/abs/2512.21919", "authors": ["KaShun Shum", "Binyuan Hui", "Jiawei Chen", "Lei Zhang", "X. W.", "Jiaxi Yang", "Yuzhen Huang", "Junyang Lin", "Junxian He"], "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents", "comment": "21 pages", "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSWE-RM\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u89e3\u51b3\u7f16\u7801\u4ee3\u7406\u4e2d\u6267\u884c\u53cd\u9988\u7a00\u758f\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f16\u7801\u4ee3\u7406\u5f00\u53d1\u4e3b\u8981\u4f9d\u8d56\u5355\u5143\u6d4b\u8bd5\u7b49\u6267\u884c\u53cd\u9988\uff0c\u4f46\u8fd9\u7c7b\u53cd\u9988\u7a00\u758f\u4e14\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6210\u529f/\u5931\u8d25\u7684\u8f68\u8ff9\u3002\u6267\u884c\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\u53cd\u9988\u80fd\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u4fe1\u53f7\uff0c\u4f46\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u53d1\u73b0\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u80fd\u76f8\u4f3c\u7684\u9a8c\u8bc1\u5668\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u5dee\u5f02\u5f88\u5927\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u5956\u52b1\u6a21\u578b\u3002", "method": "\u8bc6\u522b\u51fa\u5bf9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u7684\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6821\u51c6\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u7814\u7a76\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3001\u7b56\u7565\u6df7\u5408\u3001\u6570\u636e\u6e90\u7ec4\u6210\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7814\u7a76\uff0c\u63d0\u51faSWE-RM\u5956\u52b1\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u603b\u53c2\u6570\u91cf30B\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b3B\u53c2\u6570\u3002", "result": "SWE-RM\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u3002\u5728SWE-Bench Verified\u4e0a\uff0c\u5c06Qwen3-Coder-Flash\u51c6\u786e\u7387\u4ece51.6%\u63d0\u5347\u81f362.0%\uff0cQwen3-Coder-Max\u4ece67.0%\u63d0\u5347\u81f374.6%\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u6267\u884c\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\u53cd\u9988\u80fd\u6709\u6548\u89e3\u51b3\u7f16\u7801\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u53cd\u9988\u7a00\u758f\u6027\u95ee\u9898\u3002\u901a\u8fc7\u8bc6\u522b\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6821\u51c6\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u7684\u7a33\u5065\u5956\u52b1\u6a21\u578b\u3002", "topic": "swe application"}}
{"id": "2512.22087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22087", "abs": "https://arxiv.org/abs/2512.22087", "authors": ["Shukai Liu", "Jian Yang", "Bo Jiang", "Yizhi Li", "Jinyang Guo", "Xianglong Liu", "Bryan Dai"], "title": "Context as a Tool: Context Management for Long-Horizon SWE-Agents", "comment": null, "summary": "Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.", "AI": {"tldr": "CAT\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u8303\u5f0f\uff0c\u5c06\u4e0a\u4e0b\u6587\u7ef4\u62a4\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\u96c6\u6210\u5230\u667a\u80fd\u4f53\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5de5\u4f5c\u7a7a\u95f4\u548c\u4e3b\u52a8\u538b\u7f29\u673a\u5236\u89e3\u51b3\u957f\u65f6\u4ea4\u4e92\u4e2d\u7684\u4e0a\u4e0b\u6587\u7206\u70b8\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u5728\u5904\u7406\u4ed3\u5e93\u7ea7\u4ee3\u7801\u5e93\u7684\u957f\u65f6\u4ea4\u4e92\u4efb\u52a1\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u8ffd\u52a0\u5f0f\u4e0a\u4e0b\u6587\u7ef4\u62a4\u6216\u88ab\u52a8\u89e6\u53d1\u538b\u7f29\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u7206\u70b8\u3001\u8bed\u4e49\u6f02\u79fb\u548c\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u63d0\u51faCAT\u4e0a\u4e0b\u6587\u7ba1\u7406\u8303\u5f0f\uff0c\u5efa\u7acb\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u4f5c\u7a7a\u95f4\uff08\u7a33\u5b9a\u4efb\u52a1\u8bed\u4e49\u3001\u538b\u7f29\u957f\u671f\u8bb0\u5fc6\u3001\u9ad8\u4fdd\u771f\u77ed\u671f\u4ea4\u4e92\uff09\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u5728\u9002\u5f53\u65f6\u673a\u4e3b\u52a8\u538b\u7f29\u5386\u53f2\u8f68\u8ff9\u4e3a\u53ef\u64cd\u4f5c\u6458\u8981\u3002\u5f00\u53d1CAT-GENERATOR\u8f68\u8ff9\u7ea7\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u6570\u636e\u6784\u5efa\u7ba1\u9053\u6ce8\u5165\u4e0a\u4e0b\u6587\u7ba1\u7406\u52a8\u4f5c\uff0c\u8bad\u7ec3\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578bSWE-Compressor\u3002", "result": "\u5728SWE-Bench-Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWE-Compressor\u8fbe\u523057.6%\u7684\u89e3\u51b3\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eReAct\u7684\u667a\u80fd\u4f53\u548c\u9759\u6001\u538b\u7f29\u57fa\u7ebf\uff0c\u540c\u65f6\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\u4fdd\u6301\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\u7684\u957f\u65f6\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CAT\u8303\u5f0f\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u7ba1\u7406\u63d0\u5347\u4e3a\u667a\u80fd\u4f53\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "topic": "swe application"}}
{"id": "2512.21720", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.21720", "abs": "https://arxiv.org/abs/2512.21720", "authors": ["Shizhe He", "Avanika Narayan", "Ishan S. Khare", "Scott W. Linderman", "Christopher R\u00e9", "Dan Biderman"], "title": "An Information Theoretic Perspective on Agentic System Design", "comment": null, "summary": "Agentic language model (LM) systems power modern applications like \"Deep Research\" and \"Claude Code,\" and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller \"compressor\" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger \"predictor\" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is $1.6\\times$ more accurate, $4.6\\times$ more concise, and conveys $5.5\\times$ more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover $99\\%$ of frontier-LM accuracy at $26\\%$ of API costs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u6846\u67b6\u6765\u5206\u6790\u548c\u4f18\u5316\u57fa\u4e8e\u538b\u7f29\u5668-\u9884\u6d4b\u5668\u67b6\u6784\u7684\u667a\u80fd\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u91cf\u5316\u538b\u7f29\u8d28\u91cf\uff0c\u53d1\u73b0\u6269\u5927\u538b\u7f29\u5668\u89c4\u6a21\u6bd4\u6269\u5927\u9884\u6d4b\u5668\u89c4\u6a21\u66f4\u6709\u6548\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u538b\u7f29\u5668-\u9884\u6d4b\u5668\u67b6\u6784\u7684\u667a\u80fd\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u8bbe\u8ba1\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\uff0c\u538b\u7f29\u5668\u548c\u9884\u6d4b\u5668\u7684\u9009\u62e9\u5bf9\u4e0b\u6e38\u6027\u80fd\u7684\u5f71\u54cd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u6602\u8d35\u7684\u4efb\u52a1\u7279\u5b9a\u914d\u5bf9\u626b\u63cf\u6765\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u5c06\u538b\u7f29\u5668LM\u89c6\u4e3a\u566a\u58f0\u4fe1\u9053\uff0c\u5f15\u5165\u7b80\u5355\u7684\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668\u6765\u91cf\u5316\u4e0a\u4e0b\u6587\u4e0e\u5176\u538b\u7f29\u4e4b\u95f4\u7684\u4fe1\u606f\u91cf\uff0c\u63d0\u51fa\u4efb\u52a1\u65e0\u5173\u7684\u538b\u7f29\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u5168\u9762\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u4e92\u4fe1\u606f\u80fd\u5f3a\u9884\u6d4b\u4e0b\u6e38\u6027\u80fd\uff0c\u4e0e\u5177\u4f53\u4efb\u52a1\u65e0\u5173\u3002\u66f4\u5927\u7684\u538b\u7f29\u5668\u4e0d\u4ec5\u66f4\u51c6\u786e\uff0c\u800c\u4e14\u66f4token\u9ad8\u6548\uff0c\u6bcf\u4e2atoken\u4f20\u9012\u66f4\u591a\u6bd4\u7279\u4fe1\u606f\u3002\u6269\u5927\u538b\u7f29\u5668\u89c4\u6a21\u6bd4\u6269\u5927\u9884\u6d4b\u5668\u89c4\u6a21\u66f4\u6709\u6548\uff0c\u4f7f\u672c\u5730\u538b\u7f29\u5668\u80fd\u4e0e\u66f4\u5c0f\u7684\u4e91\u7aef\u9884\u6d4b\u5668\u914d\u5bf9\u3002", "conclusion": "\u4fe1\u606f\u8bba\u6846\u67b6\u4e3a\u667a\u80fd\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u91cf\u5316\u538b\u7f29\u8d28\u91cf\u53ef\u5b9e\u73b0\u4efb\u52a1\u65e0\u5173\u7684\u7cfb\u7edf\u4f18\u5316\u3002\u6269\u5927\u538b\u7f29\u5668\u89c4\u6a21\u662f\u63d0\u9ad8\u6027\u80fd\u7684\u5173\u952e\u7b56\u7565\uff0c\u80fd\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eAPI\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2512.22101", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22101", "abs": "https://arxiv.org/abs/2512.22101", "authors": ["Shuyu Gan", "Renxiang Wang", "James Mooney", "Dongyeop Kang"], "title": "A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting", "comment": "3 pages, 3 figures; Accepted by 1st Workshop on GenAI, Agents and the Future of VIS as Mini-challenge paper and win the Honorable Mention award. Submit number is 7597 and the paper is archived on the workshop website: https://visxgenai.github.io/subs-2025/7597/7597-doc.pdf", "summary": "Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.", "AI": {"tldr": "A2P-Vis\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u539f\u59cb\u6570\u636e\u96c6\u81ea\u52a8\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u53ef\u89c6\u5316\u62a5\u544a\uff0c\u89e3\u51b3\u4e86\u5f53\u524dAI\u4ee3\u7406\u5728\u751f\u6210\u591a\u6837\u5316\u89c6\u89c9\u8bc1\u636e\u548c\u6784\u5efa\u8fde\u8d2f\u4e13\u4e1a\u62a5\u544a\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\u7684AI\u4ee3\u7406\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u96be\u4ee5\u751f\u6210\u6709\u6d1e\u5bdf\u529b\u4e14\u591a\u6837\u5316\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u4ee5\u53ca\u65e0\u6cd5\u5c06\u8fd9\u4e9b\u8bc1\u636e\u7ec4\u7ec7\u6210\u8fde\u8d2f\u4e13\u4e1a\u7684\u62a5\u544a\u3002\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u5316\u6570\u636e\u5206\u6790\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\uff1a1) \u6570\u636e\u5206\u6790\u5668\uff1a\u6267\u884c\u6570\u636e\u5256\u6790\u3001\u63d0\u51fa\u591a\u6837\u5316\u53ef\u89c6\u5316\u65b9\u5411\u3001\u751f\u6210\u548c\u6267\u884c\u7ed8\u56fe\u4ee3\u7801\u3001\u901a\u8fc7\u53ef\u8bfb\u6027\u68c0\u67e5\u5668\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u56fe\u8868\u3001\u81ea\u52a8\u751f\u6210\u5e76\u8bc4\u5206\u5019\u9009\u6d1e\u5bdf\uff08\u57fa\u4e8e\u6df1\u5ea6\u3001\u6b63\u786e\u6027\u3001\u7279\u5f02\u6027\u3001\u6df1\u5ea6\u548c\u53ef\u64cd\u4f5c\u6027\uff09\uff1b2) \u62a5\u544a\u751f\u6210\u5668\uff1a\u7ec4\u7ec7\u4e3b\u9898\u987a\u5e8f\u3001\u57fa\u4e8e\u6392\u540d\u9760\u524d\u7684\u6d1e\u5bdf\u7f16\u5199\u56fe\u8868\u652f\u6491\u7684\u53d9\u8ff0\u3001\u64b0\u5199\u5408\u7406\u7684\u8fc7\u6e21\u6bb5\u843d\u3001\u4fee\u8ba2\u6587\u6863\u4ee5\u786e\u4fdd\u6e05\u6670\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u6362\u4e3a\u7ecf\u8fc7\u7b5b\u9009\u7684\u6750\u6599\uff08\u56fe\u8868+\u9a8c\u8bc1\u8fc7\u7684\u6d1e\u5bdf\uff09\u548c\u53ef\u8bfb\u7684\u53d9\u8ff0\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u751f\u6210\u7b26\u5408\u51fa\u7248\u6807\u51c6\u7684\u8fde\u8d2f\u62a5\u544a\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8d28\u91cf\u4fdd\u8bc1\u7684\u6570\u636e\u5206\u6790\u5668\u4e0e\u53d9\u8ff0\u6027\u62a5\u544a\u751f\u6210\u5668\u76f8\u7ed3\u5408\uff0cA2P-Vis\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u534f\u540c\u5206\u6790\u64cd\u4f5c\u5316\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u6570\u636e\u5206\u6790\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "topic": "code agent"}}
{"id": "2512.21852", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21852", "abs": "https://arxiv.org/abs/2512.21852", "authors": ["Vedant Shah", "Johan Obando-Ceron", "Vineet Jain", "Brian Bartoldson", "Bhavya Kailkhura", "Sarthak Mittal", "Glen Berseth", "Pablo Samuel Castro", "Yoshua Bengio", "Nikolay Malkin", "Moksh Jain", "Siddarth Venkatraman", "Aaron Courville"], "title": "A Comedy of Estimators: On KL Regularization in RL Training of LLMs", "comment": null, "summary": "The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \\texttt{Qwen2.5-7B}, \\texttt{Llama-3.1-8B-Instruct} and \\texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u4e2dKL\u6563\u5ea6\u4f30\u8ba1\u5668\u7684\u914d\u7f6e\u5bf9\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u65e0\u504f\u68af\u5ea6\u4f30\u8ba1\u5668\u80fd\u5e26\u6765\u66f4\u597d\u7684\u57df\u5185\u548c\u57df\u5916\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u5f53\u524dLLM\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u5e7f\u6cdb\u4f7f\u7528KL\u6563\u5ea6\u6b63\u5219\u5316\uff0c\u4f46\u5b9e\u8df5\u4e2d\u5404\u79cdKL\u4f30\u8ba1\u5668\u7684\u914d\u7f6e\u65b9\u5f0f\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5e38\u5bfc\u81f4\u76ee\u6807\u51fd\u6570\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u5206\u6790\u4e0d\u540cKL\u4f30\u8ba1\u5668\u914d\u7f6e\u7684\u68af\u5ea6\u7279\u6027\uff0c\u901a\u8fc7RL\u5fae\u8c03Qwen2.5-7B\u3001Llama-3.1-8B-Instruct\u548cQwen3-4B-Instruct-2507\u7b49\u6a21\u578b\uff0c\u5728\u57df\u5185\u548c\u57df\u5916\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e0d\u540c\u914d\u7f6e\u7684\u6027\u80fd\u3002", "result": "\u5728\u5728\u7ebf\u7b56\u7565\u8bbe\u7f6e\u4e2d\uff0c\u6709\u504f\u68af\u5ea6\u4f30\u8ba1\u5668\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u800c\u65e0\u504f\u68af\u5ea6\u4f30\u8ba1\u5668\u5728\u57df\u5185\u548c\u57df\u5916\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u66f4\u597d\uff1b\u5728\u79bb\u7ebf\u7b56\u7565\u8bbe\u7f6e\u4e2d\uff0cKL\u6b63\u5219\u5316\u6709\u52a9\u4e8e\u7a33\u5b9a\u5f02\u6b65\u8bbe\u7f6e\u5e26\u6765\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "KL\u4f30\u8ba1\u5668\u7684\u914d\u7f6e\u9009\u62e9\u5bf9LLM\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff0c\u4f7f\u7528\u65e0\u504f\u68af\u5ea6\u4f30\u8ba1\u5668\u80fd\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u57df\u5916\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.6bf09ca1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FDEARHN/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/K8B4Pk92d75a9tIOCcO0iz2XoHrp1ahC7GWv9AxxERI=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FDEARHN/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/K8B4Pk92d75a9tIOCcO0iz2XoHrp1ahC7GWv9AxxERI=436", "authors": ["TLDR Newsletter"], "title": "Hardening Atlas Against Prompt Injection", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FDEARHN/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/K8B4Pk92d75a9tIOCcO0iz2XoHrp1ahC7GWv9AxxERI=436", "summary": "Hardening Atlas Against Prompt Injection (13 minute read) This post details OpenAI's ongoing efforts to secure its AI browser, Atlas, against prompt injection attacks - malicious instructions embedded in web content that manipulate agent behavior. While mitigation techniques are improving, the company acknowledged prompt injection remains an unsolved and persistent threat, particularly as agent capabilities expand on the open web.", "source": "tldr", "AI": {"tldr": "OpenAI\u6b63\u5728\u52a0\u5f3a\u5176AI\u6d4f\u89c8\u5668Atlas\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u9632\u62a4\uff0c\u4f46\u627f\u8ba4\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u4e14\u6301\u7eed\u7684\u5a01\u80c1", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u5f00\u653e\u7f51\u7edc\u4e0a\u7684\u80fd\u529b\u6269\u5c55\uff0c\u6076\u610f\u6307\u4ee4\u5d4c\u5165\u7f51\u9875\u5185\u5bb9\u64cd\u7eb5\u4ee3\u7406\u884c\u4e3a\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u6210\u4e3a\u4e25\u91cd\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u6301\u7eed\u9632\u62a4", "method": "OpenAI\u6b63\u5728\u6539\u8fdb\u7f13\u89e3\u6280\u672f\u6765\u4fdd\u62a4Atlas AI\u6d4f\u89c8\u5668\uff0c\u5305\u62ec\u68c0\u6d4b\u548c\u9632\u5fa1\u5d4c\u5165\u5728\u7f51\u9875\u5185\u5bb9\u4e2d\u7684\u6076\u610f\u6307\u4ee4", "result": "\u867d\u7136\u9632\u62a4\u6280\u672f\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u4e14\u6301\u7eed\u5b58\u5728\u7684\u5b89\u5168\u5a01\u80c1", "conclusion": "\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u662fAI\u4ee3\u7406\u5b89\u5168\u9886\u57df\u7684\u4e00\u4e2a\u6301\u4e45\u6311\u6218\uff0c\u9700\u8981\u6301\u7eed\u7684\u7814\u7a76\u548c\u9632\u62a4\u63aa\u65bd\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7406\u80fd\u529b\u4e0d\u65ad\u6269\u5c55\u7684\u80cc\u666f\u4e0b", "topic": "agent analysis"}}
{"id": "tldr.2512.d3900dc6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenanderson.work%2Fblog%2Fasync-coding-agents%2F%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/XtcMOOL8sJtfu00KhA0dFtGqPE7SmxhiHjZBTHVgs2o=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenanderson.work%2Fblog%2Fasync-coding-agents%2F%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/XtcMOOL8sJtfu00KhA0dFtGqPE7SmxhiHjZBTHVgs2o=436", "authors": ["TLDR Newsletter"], "title": "Async Coding Agents \"From Scratch\"", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenanderson.work%2Fblog%2Fasync-coding-agents%2F%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/XtcMOOL8sJtfu00KhA0dFtGqPE7SmxhiHjZBTHVgs2o=436", "summary": "Async Coding Agents \"From Scratch\" (10 minute read) It's pretty easy to homebrew your own asynchronous coding agent. This means that businesses selling coding agents can no longer differentiate themselves by only running sandboxed agents in the cloud that connect to Slack. Companies working on coding agents likely realize this and are doing everything they can to train their own SWE agents and auxiliary models to improve their harnesses.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u6784\u5efa\u5f02\u6b65\u7f16\u7801\u4ee3\u7406\u76f8\u5bf9\u5bb9\u6613\uff0c\u4f01\u4e1a\u4ec5\u51ed\u4e91\u7aef\u6c99\u7bb1\u4ee3\u7406\u8fde\u63a5Slack\u5df2\u65e0\u6cd5\u5f62\u6210\u5dee\u5f02\u5316\u4f18\u52bf\uff0c\u56e0\u6b64\u9700\u8981\u8bad\u7ec3\u81ea\u6709SWE\u4ee3\u7406\u548c\u8f85\u52a9\u6a21\u578b\u6765\u6539\u8fdb\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u5e02\u573a\u4e0a\u7f16\u7801\u4ee3\u7406\u4ea7\u54c1\u540c\u8d28\u5316\u4e25\u91cd\uff0c\u8bb8\u591a\u516c\u53f8\u4ec5\u63d0\u4f9b\u4e91\u7aef\u6c99\u7bb1\u4ee3\u7406\u8fde\u63a5Slack\u7684\u670d\u52a1\uff0c\u8fd9\u79cd\u6a21\u5f0f\u5bb9\u6613\u88ab\u590d\u5236\uff0c\u7f3a\u4e4f\u6280\u672f\u58c1\u5792\u3002\u4f01\u4e1a\u9700\u8981\u5bfb\u627e\u771f\u6b63\u7684\u5dee\u5f02\u5316\u7ade\u4e89\u4f18\u52bf\u3002", "method": "\u6587\u7ae0\u672a\u8be6\u7ec6\u63cf\u8ff0\u5177\u4f53\u6280\u672f\u65b9\u6cd5\uff0c\u4f46\u6697\u793a\u4f01\u4e1a\u5e94\u901a\u8fc7\u8bad\u7ec3\u81ea\u6709SWE\uff08\u8f6f\u4ef6\u5de5\u7a0b\uff09\u4ee3\u7406\u548c\u8f85\u52a9\u6a21\u578b\u6765\u6539\u8fdb\u5176\u4ee3\u7406\u7cfb\u7edf\uff0c\u5efa\u7acb\u6280\u672f\u62a4\u57ce\u6cb3\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u7b80\u5355\u7684\u5f02\u6b65\u7f16\u7801\u4ee3\u7406\u6784\u5efa\u76f8\u5bf9\u5bb9\u6613\uff0c\u5bfc\u81f4\u5e02\u573a\u7ade\u4e89\u52a0\u5267\u3002\u4f01\u4e1a\u610f\u8bc6\u5230\u4ec5\u9760\u57fa\u7840\u529f\u80fd\u65e0\u6cd5\u7ef4\u6301\u7ade\u4e89\u4f18\u52bf\uff0c\u5fc5\u987b\u6295\u8d44\u4e8e\u66f4\u5148\u8fdb\u7684\u4ee3\u7406\u8bad\u7ec3\u3002", "conclusion": "\u7f16\u7801\u4ee3\u7406\u5e02\u573a\u6b63\u5728\u4ece\u7b80\u5355\u7684\u4e91\u7aef\u670d\u52a1\u5411\u66f4\u590d\u6742\u7684\u6280\u672f\u6808\u6f14\u8fdb\uff0c\u4f01\u4e1a\u9700\u8981\u901a\u8fc7\u8bad\u7ec3\u81ea\u6709\u6a21\u578b\u548c\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u4ee3\u7406\u7cfb\u7edf\u6765\u5efa\u7acb\u771f\u6b63\u7684\u6280\u672f\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "tldr.2512.25d0454d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fwe-removed-80-percent-of-our-agents-tools%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/kvyXXX_eLqnRDy3IYp086IvgDXO8bjA8lFxqPupGun0=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fwe-removed-80-percent-of-our-agents-tools%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/kvyXXX_eLqnRDy3IYp086IvgDXO8bjA8lFxqPupGun0=436", "authors": ["TLDR Newsletter"], "title": "We removed 80% of our agent's tools", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fwe-removed-80-percent-of-our-agents-tools%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/kvyXXX_eLqnRDy3IYp086IvgDXO8bjA8lFxqPupGun0=436", "summary": "We removed 80% of our agent's tools (4 minute read) Vercel spent months building a sophisticated internal text-to-SQL agent with specialized tools, heavy prompt engineering, and careful context management. It kind of worked, but it was fragile, slow, and required constant maintenance. The team then deleted most of it and stripped the agent down to a single tool that executed arbitrary bash commands. Its agent got simpler and better at the same time: it had a 100% success rate instead of 80%.", "source": "tldr", "AI": {"tldr": "Vercel\u56e2\u961f\u53d1\u73b0\uff0c\u8fc7\u5ea6\u590d\u6742\u7684\u6587\u672c\u8f6cSQL\u4ee3\u7406\uff08\u5305\u542b\u4e13\u95e8\u5de5\u5177\u3001\u590d\u6742\u63d0\u793a\u5de5\u7a0b\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\uff09\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u7b80\u5316\u4e3a\u4ec5\u6267\u884c\u4efb\u610fbash\u547d\u4ee4\u7684\u5355\u4e00\u5de5\u5177\u540e\uff0c\u6210\u529f\u7387\u4ece80%\u63d0\u5347\u81f3100%", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6587\u672c\u8f6cSQL\u4ee3\u7406\u7684\u8106\u5f31\u6027\u3001\u901f\u5ea6\u6162\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u7b80\u5316\u4ee3\u7406\u67b6\u6784\u662f\u5426\u80fd\u63d0\u9ad8\u6027\u80fd\u548c\u53ef\u9760\u6027", "method": "\u5c06\u539f\u672c\u5305\u542b\u4e13\u95e8\u5de5\u5177\u3001\u590d\u6742\u63d0\u793a\u5de5\u7a0b\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u6587\u672c\u8f6cSQL\u4ee3\u7406\u7b80\u5316\u4e3a\u4ec5\u4f7f\u7528\u5355\u4e00\u5de5\u5177\uff08\u6267\u884c\u4efb\u610fbash\u547d\u4ee4\uff09\u7684\u6781\u7b80\u67b6\u6784", "result": "\u7b80\u5316\u540e\u7684\u4ee3\u7406\u6210\u529f\u7387\u4ece80%\u63d0\u5347\u81f3100%\uff0c\u540c\u65f6\u53d8\u5f97\u66f4\u7b80\u5355\u3001\u66f4\u53ef\u9760", "conclusion": "\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u6781\u7b80\u7684\u4ee3\u7406\u67b6\u6784\uff08\u5982\u5355\u4e00bash\u547d\u4ee4\u6267\u884c\u5de5\u5177\uff09\u53ef\u80fd\u6bd4\u590d\u6742\u7684\u4e13\u95e8\u5316\u5de5\u5177\u7cfb\u7edf\u66f4\u6709\u6548\uff0c\u7b80\u5316\u8bbe\u8ba1\u53ef\u4ee5\u540c\u65f6\u63d0\u9ad8\u6210\u529f\u7387\u548c\u964d\u4f4e\u7ef4\u62a4\u6210\u672c", "topic": "agent analysis"}}
{"id": "tldr.2512.a72761d3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fthe-coding-personalities-of-leading-llms%2Fleaderboard%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-llm-leaderboard25%26utm_content=newsletter-tldrai-secondary-llmleaderboard-251223-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/Ro1C0MOXjNYCWDArM6YiKWKMLjLi_HEVvzwJTLzYH10=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fthe-coding-personalities-of-leading-llms%2Fleaderboard%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-llm-leaderboard25%26utm_content=newsletter-tldrai-secondary-llmleaderboard-251223-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/Ro1C0MOXjNYCWDArM6YiKWKMLjLi_HEVvzwJTLzYH10=436", "authors": ["TLDR Newsletter"], "title": "LLM code quality leaderboard: How does your preferred model score?", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fthe-coding-personalities-of-leading-llms%2Fleaderboard%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-llm-leaderboard25%26utm_content=newsletter-tldrai-secondary-llmleaderboard-251223-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/Ro1C0MOXjNYCWDArM6YiKWKMLjLi_HEVvzwJTLzYH10=436", "summary": "LLM code quality leaderboard: How does your preferred model score? (Sponsor) Interested to learn how different LLMs perform for coding? New research on models like GPT-5.2 High and Gemini 3.0 Pro reveals trade-offs in structural quality and security. Learn more about the reliability, security, and maintainability of code written by the latest models with Sonar's LLM Leaderboard\u2014the definitive resource to understand the true quality of AI-generated code.", "source": "tldr", "AI": {"tldr": "Sonar\u53d1\u5e03LLM\u4ee3\u7801\u8d28\u91cf\u6392\u884c\u699c\uff0c\u8bc4\u4f30GPT-5.2 High\u548cGemini 3.0 Pro\u7b49\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u7ed3\u6784\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u8868\u73b0", "motivation": "\u968f\u7740LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u5305\u62ec\u7ed3\u6784\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u7b49\u65b9\u9762\uff0c\u4e3a\u5f00\u53d1\u8005\u9009\u62e9\u5408\u9002\u6a21\u578b\u63d0\u4f9b\u53c2\u8003", "method": "\u521b\u5efaLLM\u4ee3\u7801\u8d28\u91cf\u6392\u884c\u699c\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u8bc4\u4f30\u6846\u67b6\u6d4b\u8bd5GPT-5.2 High\u3001Gemini 3.0 Pro\u7b49\u4e3b\u6d41\u6a21\u578b\uff0c\u5206\u6790\u4ee3\u7801\u7684\u7ed3\u6784\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u6307\u6807", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540cLLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6743\u8861\u5173\u7cfb\uff0c\u67d0\u4e9b\u6a21\u578b\u5728\u7ed3\u6784\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5728\u5b89\u5168\u6027\u65b9\u9762\u66f4\u5f3a\uff0c\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u7684\u91cf\u5316\u53c2\u8003", "conclusion": "Sonar\u7684LLM\u6392\u884c\u699c\u6210\u4e3a\u7406\u89e3AI\u751f\u6210\u4ee3\u7801\u771f\u5b9e\u8d28\u91cf\u7684\u6743\u5a01\u8d44\u6e90\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u6700\u5408\u9002\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b", "topic": "code agent"}}
{"id": "tldr.2512.41b963c0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmuratcankoylan%2FAgent-Skills-for-Context-Engineering%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/-PsvoncJ4CUnsc34hIrQCCwr8qKyq-TsZp5lXQVMq84=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmuratcankoylan%2FAgent-Skills-for-Context-Engineering%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/-PsvoncJ4CUnsc34hIrQCCwr8qKyq-TsZp5lXQVMq84=436", "authors": ["TLDR Newsletter"], "title": "Agent Skills for Context Engineering", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmuratcankoylan%2FAgent-Skills-for-Context-Engineering%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/-PsvoncJ4CUnsc34hIrQCCwr8qKyq-TsZp5lXQVMq84=436", "summary": "Agent Skills for Context Engineering (GitHub Repo) This repository contains a comprehensive collection of Agent Skills for building production-grade AI agent systems. They are categorized into Foundational skills, Architectural skills, and Operational skills. Each skill is structured for efficient context use. The patterns work on any agent platform that supports skills or allows custom instructions.", "source": "tldr", "AI": {"tldr": "GitHub\u4ed3\u5e93\u63d0\u4f9b\u7528\u4e8e\u6784\u5efa\u751f\u4ea7\u7ea7AI\u4ee3\u7406\u7cfb\u7edf\u7684Agent Skills\u96c6\u5408\uff0c\u5206\u4e3a\u57fa\u7840\u6280\u80fd\u3001\u67b6\u6784\u6280\u80fd\u548c\u8fd0\u7ef4\u6280\u80fd\u4e09\u7c7b\uff0c\u652f\u6301\u4efb\u4f55\u5141\u8bb8\u81ea\u5b9a\u4e49\u6307\u4ee4\u7684\u4ee3\u7406\u5e73\u53f0", "motivation": "\u4e3aAI\u4ee3\u7406\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u6280\u80fd\u96c6\u5408\uff0c\u89e3\u51b3\u751f\u4ea7\u73af\u5883\u4e2d\u4ee3\u7406\u7cfb\u7edf\u6784\u5efa\u7684\u590d\u6742\u6027\u548c\u4e00\u81f4\u6027\u9700\u6c42\uff0c\u4fc3\u8fdb\u9ad8\u6548\u4e0a\u4e0b\u6587\u4f7f\u7528", "method": "\u521b\u5efa\u7ed3\u6784\u5316\u7684\u6280\u80fd\u5206\u7c7b\u4f53\u7cfb\uff1a\u57fa\u7840\u6280\u80fd\u3001\u67b6\u6784\u6280\u80fd\u3001\u8fd0\u7ef4\u6280\u80fd\uff0c\u6bcf\u4e2a\u6280\u80fd\u90fd\u9488\u5bf9\u9ad8\u6548\u4e0a\u4e0b\u6587\u4f7f\u7528\u8fdb\u884c\u8bbe\u8ba1\uff0c\u652f\u6301\u8de8\u5e73\u53f0\u517c\u5bb9", "result": "\u63d0\u4f9b\u4e86\u5168\u9762\u7684Agent Skills\u96c6\u5408\uff0c\u652f\u6301\u751f\u4ea7\u7ea7AI\u4ee3\u7406\u7cfb\u7edf\u6784\u5efa\uff0c\u53ef\u5728\u4efb\u4f55\u652f\u6301\u6280\u80fd\u6216\u81ea\u5b9a\u4e49\u6307\u4ee4\u7684\u4ee3\u7406\u5e73\u53f0\u4e0a\u4f7f\u7528", "conclusion": "\u8be5\u6280\u80fd\u96c6\u5408\u4e3a\u6784\u5efa\u751f\u4ea7\u7ea7AI\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u53ef\u590d\u7528\u7684\u7ec4\u4ef6\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u7cfb\u7edf\u8d28\u91cf", "topic": "agent analysis"}}
{"id": "tldr.2512.91ed7087", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fhooks-partners%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/qvOILFxEu0TLJ2kCuFEzSS5dtrX7Y2kYl8G782cWWZk=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fhooks-partners%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/qvOILFxEu0TLJ2kCuFEzSS5dtrX7Y2kYl8G782cWWZk=436", "authors": ["TLDR Newsletter"], "title": "Cursor Expands Agent Hooks", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fhooks-partners%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/qvOILFxEu0TLJ2kCuFEzSS5dtrX7Y2kYl8G782cWWZk=436", "summary": "Cursor Expands Agent Hooks (3 minute read) Cursor has announced partnerships to integrate its agent hook system with security and platform vendors. These hooks allow organizations to observe, modify, or block stages of the agent loop, supporting use cases like governance, dependency scanning, secrets management, and agent safety.", "source": "tldr", "AI": {"tldr": "Cursor\u5ba3\u5e03\u4e0e\u5b89\u5168\u548c\u5e73\u53f0\u4f9b\u5e94\u5546\u5408\u4f5c\uff0c\u6269\u5c55\u5176\u4ee3\u7406\u94a9\u5b50\u7cfb\u7edf\uff0c\u4f7f\u7ec4\u7ec7\u80fd\u591f\u89c2\u5bdf\u3001\u4fee\u6539\u6216\u963b\u6b62\u4ee3\u7406\u5faa\u73af\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u652f\u6301\u6cbb\u7406\u3001\u4f9d\u8d56\u626b\u63cf\u3001\u5bc6\u94a5\u7ba1\u7406\u548c\u4ee3\u7406\u5b89\u5168\u7b49\u7528\u4f8b\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u66f4\u597d\u7684\u63a7\u5236\u548c\u6cbb\u7406\u673a\u5236\u6765\u786e\u4fdd\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3001\u5408\u89c4\u6027\u548c\u53ef\u9760\u6027\u3002\u7ec4\u7ec7\u9700\u8981\u80fd\u591f\u76d1\u63a7\u548c\u5e72\u9884\u4ee3\u7406\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u5b89\u5168\u98ce\u9669\u3001\u7ba1\u7406\u4f9d\u8d56\u3001\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u5408\u4f5c\u4f19\u4f34\u5173\u7cfb\uff0c\u5c06Cursor\u7684\u4ee3\u7406\u94a9\u5b50\u7cfb\u7edf\u4e0e\u5b89\u5168\u548c\u5e73\u53f0\u4f9b\u5e94\u5546\u96c6\u6210\u3002\u8fd9\u4e9b\u94a9\u5b50\u5141\u8bb8\u5728\u4ee3\u7406\u5faa\u73af\u7684\u5404\u4e2a\u9636\u6bb5\u63d2\u5165\u89c2\u5bdf\u3001\u4fee\u6539\u6216\u963b\u6b62\u529f\u80fd\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u63a7\u5236\u673a\u5236\u3002", "result": "\u6210\u529f\u6269\u5c55\u4e86\u4ee3\u7406\u94a9\u5b50\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd\u5b9e\u9645\u7528\u4f8b\uff0c\u5305\u62ec\u6cbb\u7406\u3001\u4f9d\u8d56\u626b\u63cf\u3001\u5bc6\u94a5\u7ba1\u7406\u548c\u4ee3\u7406\u5b89\u5168\u3002\u901a\u8fc7\u96c6\u6210\u5408\u4f5c\u4f19\u4f34\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u4ee3\u7406\u63a7\u5236\u548c\u76d1\u63a7\u80fd\u529b\u3002", "conclusion": "\u4ee3\u7406\u94a9\u5b50\u7cfb\u7edf\u4e3aAI\u4ee3\u7406\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u96c6\u6210\u6846\u67b6\uff0c\u4f7f\u7ec4\u7ec7\u80fd\u591f\u5728\u4fdd\u6301\u5f00\u53d1\u6548\u7387\u7684\u540c\u65f6\u786e\u4fdd\u4ee3\u7406\u884c\u4e3a\u7684\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2512.1fa0cad6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fai-machine-learning%2Fnew-enhanced-tool-governance-in-vertex-ai-agent-builder%2F%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/sh6-lRCLDVaX3Fdj9FehWIwWNPr02X1PZC-caUto38U=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fai-machine-learning%2Fnew-enhanced-tool-governance-in-vertex-ai-agent-builder%2F%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/sh6-lRCLDVaX3Fdj9FehWIwWNPr02X1PZC-caUto38U=436", "authors": ["TLDR Newsletter"], "title": "Announcing advanced governance capabilities for Vertex AI Agent Builder", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fai-machine-learning%2Fnew-enhanced-tool-governance-in-vertex-ai-agent-builder%2F%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/sh6-lRCLDVaX3Fdj9FehWIwWNPr02X1PZC-caUto38U=436", "summary": "Announcing advanced governance capabilities for Vertex AI Agent Builder (5 minute read) Google announced advanced governance features for Vertex AI Agent Builder, enhancing its Agent Engine to manage both short-term and long-term memory.", "source": "tldr", "AI": {"tldr": "Google\u4e3aVertex AI Agent Builder\u63a8\u51fa\u9ad8\u7ea7\u6cbb\u7406\u529f\u80fd\uff0c\u589e\u5f3aAgent Engine\u4ee5\u7ba1\u7406\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6", "motivation": "\u63d0\u5347AI\u4ee3\u7406\u7684\u6cbb\u7406\u80fd\u529b\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u7ba1\u7406\u548c\u5229\u7528\u8bb0\u5fc6\u4fe1\u606f\uff0c\u63d0\u9ad8\u4ee3\u7406\u7684\u667a\u80fd\u6c34\u5e73\u548c\u5e94\u7528\u6548\u679c", "method": "\u5728Vertex AI Agent Builder\u4e2d\u96c6\u6210\u9ad8\u7ea7\u6cbb\u7406\u529f\u80fd\uff0c\u6269\u5c55Agent Engine\u4ee5\u652f\u6301\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u7ba1\u7406", "result": "\u589e\u5f3a\u4e86AI\u4ee3\u7406\u7684\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u7ef4\u6301\u957f\u671f\u72b6\u6001", "conclusion": "\u8fd9\u4e9b\u65b0\u529f\u80fd\u5c06\u63d0\u5347AI\u4ee3\u7406\u7684\u667a\u80fd\u6c34\u5e73\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u4ee3\u7406\u6784\u5efa\u5de5\u5177", "topic": "agent analysis"}}
