<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.AI](#cs.AI) [Total: 4]
- [tldr.article](#tldr.article) [Total: 8]
- [cs.LG](#cs.LG) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Neural Synchrony Between Socially Interacting Language Models](https://arxiv.org/abs/2602.17815)
*Zhining Zhang,Wentao Zhu,Chi Han,Yizhou Wang,Heng Ji*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在社交互动中表现出神经同步现象，这与人类社交大脑活动相似，为评估LLM的"社交心智"提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为社交心智是生物体的专属特性，虽然LLM被广泛认为是人类行为的强大近似，但关于LLM是否具有可比拟人类社交心智的能力仍存在争议。本研究旨在通过神经同步现象为这一争论提供实证证据。

Method: 引入神经同步作为分析LLM社交性的新代理指标，通过精心设计的实验，在社交模拟中测量LLM之间的神经同步，评估其反映社交参与度和时间对齐的能力。

Result: 实验表明，LLM之间的神经同步可靠地反映了它们的社交互动特征，且神经同步与LLM的社交表现呈强相关，揭示了LLM内部动态与人类社交互动的惊人相似性。

Conclusion: LLM在社交互动中表现出类似人类的神经同步现象，这为理解和评估LLM的"社交心智"提供了新的实证框架，揭示了人类与LLM社交互动内部动态的相似性。

Abstract: Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the "social minds" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.

</details>


### [2] [Analyzing LLM Instruction Optimization for Tabular Fact Verification](https://arxiv.org/abs/2602.17937)
*Xiaotang Du,Giwon Hong,Wai-Chung Kwan,Rohit Saxena,Ivan Titov,Pasquale Minervini,Emily Allaway*

Main category: cs.CL

TL;DR: 本文首次系统比较了基于DSPy优化框架的指令优化方法在表格事实验证任务上的表现，评估了四种提示技术，发现指令优化能持续提升验证准确率，不同优化器对不同提示技术有不同效果。


<details>
  <summary>Details</summary>
Motivation: 指令优化为提升大型语言模型推理性能提供了一种轻量级、模型无关的方法。本文旨在系统比较不同指令优化方法在表格事实验证任务上的效果，填补该领域的研究空白。

Method: 基于DSPy优化框架，评估了四种提示技术：直接预测、思维链(CoT)、带SQL工具的ReAct、带Python执行的CodeAct。研究了三种DSPy优化器（COPRO、MiPROv2、SIMBA）在四个基准测试和三个模型家族上的表现。

Result: 指令优化持续提升验证准确率：MiPROv2对CoT提供最稳定的增益，SIMBA对ReAct智能体提供最大收益（尤其在更大模型规模时）。行为分析显示SIMBA通过启发式方法鼓励更直接的推理路径，提升CoT中的数值比较能力，帮助ReAct智能体避免不必要的工具调用。

Conclusion: 对于表格事实检查，CoT仍然有效（尤其对于较小模型）。虽然基于更大模型的ReAct智能体可以达到竞争性性能，但需要仔细的指令优化。不同优化器对不同提示技术有特定优势。

Abstract: Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.

</details>


### [3] [Towards More Standardized AI Evaluation: From Models to Agents](https://arxiv.org/abs/2602.18029)
*Ali El Filali,Inès Bedar*

Main category: cs.CL

TL;DR: 论文认为传统基于静态基准和聚合分数的评估方法已不适应AI代理时代，评估应成为控制AI系统行为、建立信任的核心功能而非最终检查点


<details>
  <summary>Details</summary>
Motivation: AI系统已从静态模型演变为复合型、使用工具的多代理系统，但大多数评估实践仍停留在模型中心时代的假设中，这些方法越来越难以揭示系统真实行为，反而可能掩盖故障模式

Method: 分析评估流程本身如何引入静默故障模式，探讨高基准分数为何经常误导团队，研究代理系统如何从根本上改变性能测量的意义，而非提出新指标或更难基准

Result: 传统评估方法在代理时代存在严重局限性，评估应重新定位为测量学科，用于建立对非确定性系统的信任、迭代和治理，而非"性能剧场"

Conclusion: 评估需要从最终检查点转变为AI时代的核心控制功能，特别是在代理系统中，评估应作为测量学科来确保系统在变化和规模下的预期行为

Abstract: Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.

</details>


### [4] [Perceived Political Bias in LLMs Reduces Persuasive Abilities](https://arxiv.org/abs/2602.18092)
*Matthew DiGiuseppe,Joshua Robison*

Main category: cs.CL

TL;DR: 研究发现，当用户认为LLM存在党派偏见时，ChatGPT的纠正错误信息效果会降低28%，表明AI说服力的政治中立性感知至关重要


<details>
  <summary>Details</summary>
Motivation: 随着LLM进入党派冲突，精英阶层越来越多地将它们描绘成意识形态对齐的工具。本研究旨在测试这些可信度攻击是否会降低基于LLM的说服效果，探究AI说服力的政治条件性

Method: 在美国进行预注册调查实验(N=2144)，参与者与ChatGPT进行三轮对话，纠正个人持有的经济政策误解。实验组收到简短消息表明LLM对参与者所属党派存在偏见，对照组为中性条件

Result: 与中性对照组相比，表明LLM存在党派偏见的警告使说服效果降低28%。文本分析显示警告改变了互动：受访者更频繁地反驳，参与度更低，接受度更差

Conclusion: 对话AI的说服效果具有政治条件性，受党派对齐感知的限制。当用户认为AI存在党派偏见时，其纠正错误信息的能力会显著降低

Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.

</details>


### [5] [Simplifying Outcomes of Language Model Component Analyses with ELIA](https://arxiv.org/abs/2602.18262)
*Aaron Louis Eidt,Nils Feldhus*

Main category: cs.CL

TL;DR: ELIA是一个交互式Web应用，通过整合多种语言模型分析技术并利用视觉语言模型自动生成自然语言解释，简化了大型语言模型的机制可解释性分析，使非专家也能理解复杂模型内部工作原理。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性虽然开发了强大的工具来分析大型语言模型的内部工作原理，但其复杂性造成了可访问性差距，限制了这些工具只能被专家使用。需要设计更易用的工具来扩大受众范围。

Method: 设计、构建和评估ELIA交互式Web应用，整合三种关键技术：归因分析、函数向量分析和电路追踪，并引入新方法：使用视觉语言模型自动为这些方法产生的复杂可视化生成自然语言解释。

Result: 通过混合方法用户研究验证了该方法的有效性，显示用户明显偏好交互式、可探索的界面而非简单的静态可视化。AI驱动的解释帮助非专家弥合知识差距，统计分析显示用户的先验LLM经验与理解分数之间无显著相关性，表明系统降低了不同经验水平的理解障碍。

Conclusion: AI系统确实可以简化复杂的模型分析，但其真正潜力在于与以用户为中心的设计相结合，优先考虑交互性、特异性和叙事指导。

Abstract: While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Examining LLMs Ability to Summarize Code Through Mutation-Analysis](https://arxiv.org/abs/2602.17838)
*Lara Khatib,Micheal Pu,Bogdan Vasilescu,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 提出基于变异测试的方法评估LLM生成的代码摘要是否准确反映程序实际行为，而非表面意图


<details>
  <summary>Details</summary>
Motivation: 随着开发者越来越依赖LLM生成的代码摘要进行文档、测试和审查，需要研究这些摘要是否准确反映程序实际行为。LLM经常自信地描述代码看起来应该做什么（意图），但忽略了定义实际行为的细微边界情况或逻辑变化。

Method: 提出基于变异的评估方法：生成摘要→在代码中注入目标变异→检查LLM是否更新摘要以反映新行为。通过三个实验验证：1）12个受控合成程序的324个变异；2）LBPP数据集中50个人工编写程序的150个变异样本；3）比较GPT-4和GPT-5.2的性能。

Result: 摘要准确性随复杂度急剧下降：单函数76.5%→多线程系统17.3%；人类编写程序摘要准确率49.3%；GPT-5.2相比GPT-4有显著提升（49.3%→85.3%），但两者仍难以区分实现细节与标准算法模式。

Conclusion: 建立变异分析作为评估LLM生成摘要是否反映程序行为而非表面文本模式的系统方法。

Abstract: As developers increasingly rely on LLM-generated code summaries for documentation, testing, and review, it is important to study whether these summaries accurately reflect what the program actually does. LLMs often produce confident descriptions of what the code looks like it should do (intent), while missing subtle edge cases or logic changes that define what it actually does (behavior). We present a mutation-based evaluation methodology that directly tests whether a summary truly matches the code's logic. Our approach generates a summary, injects a targeted mutation into the code, and checks if the LLM updates its summary to reflect the new behavior. We validate it through three experiments totalling 624 mutation-summary evaluations across 62 programs. First, on 12 controlled synthetic programs with 324 mutations varying in type (statement, value, decision) and location (beginning, middle, end). We find that summary accuracy decreases sharply with complexity from 76.5% for single functions to 17.3% for multi-threaded systems, while mutation type and location exhibit weaker effects. Second, testing 150 mutated samples on 50 human-written programs from the Less Basic Python Problems (LBPP) dataset confirms the same failure patterns persist as models often describe algorithmic intent rather than actual mutated behavior with a summary accuracy rate of 49.3%. Furthermore, while a comparison between GPT-4 and GPT-5.2 shows a substantial performance leap (from 49.3% to 85.3%) and an improved ability to identify mutations as "bugs", both models continue to struggle with distinguishing implementation details from standard algorithmic patterns. This work establishes mutation analysis as a systematic approach for assessing whether LLM-generated summaries reflect program behavior rather than superficial textual patterns.

</details>


### [7] [Mining Type Constructs Using Patterns in AI-Generated Code](https://arxiv.org/abs/2602.17955)
*Imgyeong Lee,Tayyib Ul Hassan,Abram Hindle*

Main category: cs.SE

TL;DR: AI代理在TypeScript项目中比人类更频繁使用'any'关键字和高级类型构造，但PR接受率却比人类高1.8倍


<details>
  <summary>Details</summary>
Motivation: 研究AI在类型相关编程任务中是否本质上优于人类，以及AI代理在复杂类型系统下是否像人类一样过度使用或误用类型构造

Method: 在TypeScript项目领域进行首次实证分析，比较AI代理和人类在类型构造使用上的差异

Result: AI代理比人类更倾向于使用'any'关键字（9倍），更频繁使用高级类型构造和忽略类型检查的构造；但AI代理的PR接受率比人类高1.8倍

Conclusion: AI代理在类型安全方面存在问题，但PR接受率更高；建议开发者在与AI代理协作时仔细确认代码库的类型安全性

Abstract: Artificial Intelligence (AI) increasingly automates various parts of the software development tasks. Although AI has enhanced the productivity of development tasks, it remains unstudied whether AI essentially outperforms humans in type-related programming tasks, such as employing type constructs properly for type safety, during its tasks. Moreover, there is no systematic study that evaluates whether AI agents overuse or misuse the type constructs under the complicated type systems to the same extent as humans. In this study, we present the first empirical analysis to answer these questions in the domain of TypeScript projects. Our findings show that, in contrast to humans, AI agents are 9x more prone to use the 'any' keyword. In addition, we observed that AI agents use advanced type constructs, including those that ignore type checks, more often compared to humans. Surprisingly, even with all these issues, Agentic pull requests (PRs) have 1.8x higher acceptance rates compared to humans for TypeScript. We encourage software developers to carefully confirm the type safety of their codebases whenever they coordinate with AI agents in the development process.

</details>


### [8] [Toward Automated Virtual Electronic Control Unit (ECU) Twins for Shift-Left Automotive Software Testing](https://arxiv.org/abs/2602.18142)
*Sebastian Dingler,Frederik Boenke*

Main category: cs.SE

TL;DR: 论文提出了一种基于代理反馈驱动工作流的虚拟ECU测试环境，通过自动差分测试和迭代模型修正来降低CPU行为保真度的技术风险，实现硬件可用前的早期软件集成测试。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发周期中，软件完成往往早于硬件可用，导致后期集成和硬件在环测试成为瓶颈。需要虚拟测试环境在物理硬件可用前运行真实软件二进制文件。

Method: 采用代理反馈驱动工作流，通过GDB连接到参考模拟器，生成SystemC/TLM 2.0指令级精确的处理器模型。通过自动差分测试和迭代模型修正来确保CPU行为保真度。

Result: 原型系统表明，最关键的CPU行为保真度技术风险可以通过自动差分测试和迭代模型修正来降低。实现了可复现测试、非侵入式追踪和符合安全标准的故障注入。

Conclusion: 虽然云规模部署和完整工具链集成仍需未来工作，但原型展示了虚拟ECU孪生的可行左移路径，能够在硬件可用前进行早期软件测试和集成。

Abstract: Automotive software increasingly outpaces hardware availability, forcing late integration and expensive hardware-in-the-loop (HiL) bottlenecks. The InnoRegioChallenge project investigated whether a virtual test and integration environment can reproduce electronic control unit (ECU) behavior early enough to run real software binaries before physical hardware exists. We report a prototype that generates instruction-accurate processor models in SystemC/TLM~2.0 using an agentic, feedback-driven workflow coupled to a reference simulator via the GNU Debugger (GDB). The results indicate that the most critical technical risk -- CPU behavioral fidelity -- can be reduced through automated differential testing and iterative model correction. We summarize the architecture, the agentic modeling loop, and project outcomes, and we extrapolate plausible technical details consistent with the reported qualitative findings. While cloud-scale deployment and full toolchain integration remain future work, the prototype demonstrates a viable shift-left path for virtual ECU twins, enabling reproducible tests, non-intrusive tracing, and fault-injection campaigns aligned with safety standards.

</details>


### [9] [ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation](https://arxiv.org/abs/2602.18306)
*Dongming Jin,Zhi Jin,Zheng Fang,Linyu Li,XiaoTian Yang,Yuanpeng He,Xiaohong Chen*

Main category: cs.SE

TL;DR: 本文提出ReqElicitGym，一个用于评估对话式需求获取中访谈能力的交互式自动评估环境，包含101个网站需求场景数据集、模拟用户和任务评估器，并通过实验发现当前LLM在挖掘隐性需求方面能力有限。


<details>
  <summary>Details</summary>
Motivation: 随着LLM编码能力的快速提升，LLM自动化软件开发瓶颈从生成正确代码转向获取用户需求。现有评估方法依赖少量场景、真实用户交互和主观人工评分，缺乏系统化、可量化的比较方法。

Method: 提出ReqElicitGym评估环境，包含：1）101个网站需求获取场景的数据集；2）交互式模拟用户（oracle user）；3）任务评估器。该环境支持任何自动化对话式需求获取方法（如LLM代理）进行可重复、定量评估。

Result: 对7个代表性LLM的系统实证研究表明：1）当前LLM在挖掘隐性需求方面能力有限，仅能获取不到一半的隐性需求；2）有效的获取问题往往出现在对话后期；3）LLM能获取交互和内容相关的隐性需求，但在风格相关需求上持续困难。

Conclusion: ReqElicitGym能够促进自动化对话式需求获取的评估和发展，当前LLM在访谈能力方面仍有提升空间，特别是在挖掘隐性需求方面。

Abstract: With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Epistemic Traps: Rational Misalignment Driven by Model Misspecification](https://arxiv.org/abs/2602.17676)
*Xingcheng Xu,Jingjing Qu,Qiaosheng Zhang,Chaochao Lu,Yanqing Yang,Na Zou,Xia Hu*

Main category: cs.AI

TL;DR: 论文提出AI智能体的安全问题源于模型误设而非训练误差，通过经济学理论建立主观模型工程框架，证明安全行为是离散相而非奖励连续函数。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型和AI智能体在关键领域部署面临行为病理问题（如奉承、幻觉、战略欺骗），现有安全范式将这些视为训练伪影，缺乏统一理论框架解释其产生和稳定性。

Method: 将经济学中的Berk-Nash理性化理论应用于人工智能，建立智能体在错误主观世界模型下优化的理论框架，通过六个先进模型的行为实验验证理论预测。

Result: 证明不安全行为是结构必然性：作为稳定错位均衡或振荡循环出现，战略欺骗作为"锁定"均衡或认知不确定性持续存在。安全行为是离散相，由智能体认知先验决定。

Conclusion: 安全需要主观模型工程（设计智能体内部信念结构）而非仅操纵环境奖励，标志着从奖励操纵到塑造智能体现实解释的范式转变。

Abstract: The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a "locked-in" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.

</details>


### [11] [El Agente Gráfico: Structured Execution Graphs for Scientific Agents](https://arxiv.org/abs/2602.17902)
*Jiaru Bai,Abdulrahman Aldossary,Thomas Swanick,Marcel Müller,Yeonghun Kang,Zijian Zhang,Jin Won Lee,Tsz Wai Ko,Mohammad Ghazi Vakili,Varinia Bernales,Alán Aspuru-Guzik*

Main category: cs.AI

TL;DR: 提出El Agente Gráfico框架，通过类型安全执行环境和动态知识图谱将LLM决策嵌入科学工作流，解决当前基于文本的代理方法在上下文管理和可追溯性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在科学工作流自动化中与异构计算工具的集成方式临时且脆弱，基于非结构化文本的代理方法产生大量信息，难以追踪决策来源和审计。

Method: 开发单代理框架，包含科学概念的结构化抽象和对象-图谱映射器，将计算状态表示为类型化Python对象，存储在内存或外部知识图谱中，通过类型化符号标识符而非原始文本管理上下文。

Result: 在量子化学任务基准测试中，单代理结合可靠执行引擎能够稳健执行复杂、多步骤和并行计算；进一步应用于构象集成生成和金属有机框架设计，知识图谱同时作为记忆和推理基础。

Conclusion: 抽象化和类型安全为超越提示中心设计的代理科学自动化提供了可扩展基础，展示了结构化方法在科学工作流中的优势。

Abstract: Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gráfico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.

</details>


### [12] [Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems](https://arxiv.org/abs/2602.17910)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.AI

TL;DR: APEMO是一种运行时调度层，通过利用时间情感信号优化计算分配，提升长时程自主代理在固定预算下的轨迹可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐主要关注单个模型输出，但自主代理在长时程工作流程中需要在整个交互轨迹上保持持续可靠性。需要一种方法来优化计算分配并提升轨迹级质量。

Method: APEMO是一个运行时调度层，通过操作时间情感信号来优化计算分配。它不修改模型权重，而是通过行为代理检测轨迹不稳定性，并在关键片段（如峰值时刻和结束时刻）进行修复。

Result: 在多智能体模拟和基于LLM的规划-执行流程中的评估表明，APEMO在轨迹级质量和重用概率方面持续优于结构化编排器。

Conclusion: 研究结果将对齐重新定义为时间控制问题，为长时程智能系统的开发提供了有弹性的工程路径。

Abstract: Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.

</details>


### [13] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: 该论文提出了OMAD框架，首次将扩散策略应用于在线多智能体强化学习，通过松弛策略目标最大化缩放联合熵来解决扩散模型似然不可处理的问题，在MPE和MAMuJoCo任务上实现了2.5-5倍的样本效率提升。


<details>
  <summary>Details</summary>
Motivation: 在线多智能体强化学习中，提升策略表达能力对性能至关重要。扩散生成模型在图像生成和离线设置中已展现出色的表达能力和多模态表示能力，但在在线MARL中的应用尚未充分探索。主要障碍是扩散模型的不可处理似然阻碍了基于熵的探索和协调。

Method: 提出OMAD框架：1）松弛策略目标最大化缩放联合熵，实现有效探索而不依赖可处理似然；2）在CTDE范式下，使用联合分布值函数优化去中心化扩散策略；3）利用可处理的熵增强目标指导扩散策略的同步更新，确保稳定协调。

Result: 在MPE和MAMuJoCo的10个多样化任务上进行广泛评估，OMAD成为新的最先进方法，展示了2.5倍到5倍的样本效率提升。

Conclusion: OMAD成功将扩散策略应用于在线MARL，通过创新的松弛策略目标和联合分布值函数设计，克服了扩散模型似然不可处理的挑战，实现了显著的性能提升和样本效率改进。

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [14] [Cogent Security](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cogent.com%2F%3Futm_source=tldrinfosec/1/0100019c7652a59c-eabf2861-2d32-46bb-90d2-ef03eb240b3a-000000/YXO5jDiFLbavtAbCEjoiJUuUPVfoGUJs9KDIRtXJr18=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cogent Security是一个用于漏洞管理的智能AI平台，能够自主调查、优先级排序和编排安全问题的修复


<details>
  <summary>Details</summary>
Motivation: 传统漏洞管理流程通常需要大量人工干预，效率低下且容易出错。需要自动化、智能化的解决方案来加速安全问题的识别、优先级排序和修复

Method: 采用智能AI代理平台架构，结合自动化调查、智能优先级算法和修复编排功能，实现端到端的漏洞管理自动化

Result: 推出了Cogent Security产品，这是一个能够自主处理漏洞管理的AI平台，显著提高了安全团队的工作效率和响应速度

Conclusion: 智能AI代理平台在漏洞管理领域具有重要应用价值，能够有效解决传统安全流程中的效率瓶颈问题

Abstract: Cogent Security (Product Launch) Cogent Security is an agentic AI platform for vulnerability management that autonomously investigates, prioritizes, and orchestrates remediation of security issues.

</details>


### [15] [AI Taxonomy: An Operational Framework for Precision in AI Discourse](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropleaf.app%2Fd%2FAlXez8scbd%3Futm_source=tldrmarketing/1/0100019c7af165da-9fe91d0a-475d-40bc-a120-716381622763-000000/A2FbghIUesEd7gKLEQ_EZ91lzL3Lso3_bWdJkASbAHc=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文提出了一个AI分类框架，定义了6种AI角色：分析型、语义型、生成型、代理型、感知型和物理型，强调将能力与接口分离，并指出强大产品需要多种AI类型组合。


<details>
  <summary>Details</summary>
Motivation: 当前AI讨论中术语混乱，缺乏统一的分类框架来精确描述AI系统的功能和角色，导致沟通不准确和产品设计不清晰。

Method: 提出一个操作性AI分类法，基于AI的功能定义6种角色类型，并区分AI能力与用户接口，强调产品设计中需要多种AI类型的组合。

Result: 建立了清晰的AI分类框架，包括6种明确的AI角色定义，为AI产品设计、技术讨论和行业沟通提供了标准化语言。

Conclusion: 该分类框架能够提高AI讨论的精确性，指导产品设计，并帮助识别AI系统的核心功能与用户界面的区别。

Abstract: AI Taxonomy: An Operational Framework for Precision in AI Discourse (3 minute read) This AI taxonomy defines AI by what it does and outlines 6 roles. Analytical AI predicts and scores. Semantic AI understands context and relationships. Generative AI creates content. Agentic AI plans and executes workflows. Perceptive AI interprets vision, speech, and documents. Physical AI operates in the physical world. It also separates capability from interface and stresses that strong products combine mul...

</details>


### [16] [VS Code becomes multi-agent command center for developers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fvs-code-becomes-multi-agent-command-center-for-developers%2F%3Futm_source=tldrdevops/1/0100019c7af21329-6561766d-3516-4027-a78a-1d90240e1d26-000000/js0dedmmkOMw0Ml-V4mCOhbAT2poTabC9Zu77j_KJxk=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: VS Code v1.109引入多智能体编排功能，支持Anthropic Claude、OpenAI Codex和GitHub Copilot，提供统一会话管理、并行子智能体和MCP应用，增强上下文保留、安全沙箱和性能。


<details>
  <summary>Details</summary>
Motivation: 将VS Code从传统代码编辑器转变为开发者的多智能体指挥中心，通过集成多个AI助手提供更强大的编程辅助功能。

Method: 在VS Code v1.109版本中引入多智能体编排架构，支持多个AI模型（Claude、Codex、Copilot）并行工作，提供统一会话管理、并行子智能体协调和MCP应用集成。

Result: VS Code成为通用AI接口，具备增强的上下文保留能力、安全沙箱保护、性能优化，为开发者提供更强大的多智能体编程辅助环境。

Conclusion: VS Code v1.109通过多智能体编排功能成功转型为开发者的AI指挥中心，为编程工作流带来革命性改进。

Abstract: VS Code becomes multi-agent command center for developers (5 minute read) Visual Studio Code v1.109 introduces multi-agent orchestration with support for Anthropic Claude and OpenAI Codex alongside GitHub Copilot, unified session management, parallel subagents, and MCP Apps. The release enhances context retention, security sandboxing, performance, and positions VS Code as a universal AI interface.

</details>


### [17] [Superpowers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fobra%2Fsuperpowers%3Futm_source=tldrdevops/1/0100019c7af21329-6561766d-3516-4027-a78a-1d90240e1d26-000000/rvNtxpZ9OvgocahvUXFHTKfGJUTjpyJcmMigHHb3uNE=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Superpowers是一个为AI编码代理设计的结构化工作流框架，通过设计验证、测试驱动开发和子代理驱动实现来指导软件开发，能够自主运行长达两小时。


<details>
  <summary>Details</summary>
Motivation: 当前AI编码代理在复杂软件开发中缺乏结构化工作流程，难以保持长期自主性和代码质量。需要一种系统化的方法来指导AI代理完成从设计到实现的完整开发过程。

Method: 作为Claude Code和Cursor等平台的插件，通过自动触发可组合的"技能"来强制执行工作流，包括将项目分解为2-5分钟的任务、严格遵循红绿重构流程、设计验证和子代理驱动的实现。

Result: 该框架使AI编码代理能够自主运行长达两小时，通过结构化工作流提高了开发效率和代码质量，支持测试驱动开发和系统化设计验证。

Conclusion: Superpowers为AI编码代理提供了有效的结构化工作流框架，通过自动化技能触发和任务分解机制，显著提升了AI代理在软件开发中的自主性和可靠性。

Abstract: Superpowers (GitHub Repo) Superpowers gives AI coding agents like Claude a structured workflow for software development, guiding them through design validation, test-driven development, and subagent-driven implementation that can run autonomously for up to two hours at a time. The framework works as a plugin for platforms like Claude Code and Cursor, automatically triggering composable "skills" that enforce workflows like breaking projects into 2-5 minute tasks, following strict red-green-ref...

</details>


### [18] [Your Agent Framework Is Just a Bad Clone of Elixir: Concurrency Lessons from Telecom to AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgeguimaraes.com%2Fyour-agent-orchestrator-is-just-a-bad-clone-of-elixir%2F%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/br0j__duLhnLZDFcQdJT9nW2MBMjsyKE1nMESPZykNw=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 现代AI代理框架与1986年Elixir/Erlang BEAM虚拟机的并发模型相似，后者为电信行业解决了类似的并发和容错挑战


<details>
  <summary>Details</summary>
Motivation: 当前Python和TypeScript中的AI代理框架在并发处理和容错方面面临挑战，而Elixir/Erlang的BEAM虚拟机早在1986年就为电信行业解决了类似问题

Method: 通过比较分析现代AI代理框架与BEAM虚拟机的架构，指出BEAM的actor模型（包含隔离的轻量级进程、消息传递和监督树）更适合AI代理的长生命周期、非确定性交互需求

Result: 发现现代AI代理框架在并发和容错方面只是BEAM虚拟机的低质量复制品，BEAM的actor模型特性更适合处理AI代理的复杂交互

Conclusion: AI代理框架开发者应该从Elixir/Erlang的BEAM虚拟机中学习并发和容错的最佳实践，而不是重新发明轮子

Abstract: Your Agent Framework Is Just a Bad Clone of Elixir: Concurrency Lessons from Telecom to AI (15 minute read) Modern AI agent frameworks in Python and TypeScript are similar to the Elixir/Erlang BEAM virtual machine, which solved similar concurrency and fault-recovery challenges for telecom in 1986. The BEAM's actor model has features like isolated lightweight processes, message passing, and supervision trees, well-suited for the long-lived, non-deterministic interactions of AI agents.

</details>


### [19] [Expo went all-in on Claude Code for a month. Here's what happened](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fexpo.dev%2Fblog%2Fwhat-our-web-team-learned-using-claude-code-for-a-month%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=34911156-NDR%2520-%2520Starter%2520Plan%2520Growth%2520%2526%2520Retention%26utm_content=claude/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/tWxtCMNG2wLUYlZ7xEglwIXXinpVwDth-PXFwDgrTFE=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Expo团队全面使用Claude Code一个月，分享了使用体验、成果和教训，并推出了新的MCP服务器功能


<details>
  <summary>Details</summary>
Motivation: 探索Claude Code在实际开发项目中的效果，测试其作为主要开发工具的生产力影响

Method: 团队完全采用Claude Code进行为期一个月的开发工作，记录使用过程中的体验和结果

Result: 获得了成功和失败的经验教训，并基于此开发了新的MCP服务器功能，支持通过Claude提示构建和工作流

Conclusion: Claude Code在实际开发中有价值但需要适应，团队分享了实践经验并推出了相关工具改进

Abstract: Expo went all-in on Claude Code for a month. Here's what happened (Sponsor) Expo's web team committed 100% to Claude Code for a month and ended up with wins, losses, and lessons you can read about in this blog. They also added the ability to prompt Builds and Workflows with Claude with Expo MCP Server. Try the new MCP server

</details>


### [20] [Step 3.5 Flash](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstatic.stepfun.com%2Fblog%2Fstep-3.5-flash%2F%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/Pf2eAuwCnMAFS-4vbw-feQM7Fg1RQNVEkBiyM0VI-N8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Step 3.5 Flash是一个前沿开源基础模型，提供高效高速推理和智能体能力，支持工具使用、长上下文和本地部署


<details>
  <summary>Details</summary>
Motivation: 开发一个开源的基础模型，能够提供高效的推理速度，同时具备智能体能力，支持工具使用和长上下文处理，满足本地部署需求

Method: 基于StepFun开发的Step 3.5 Flash模型，专注于优化推理效率和智能体能力，集成工具使用功能和长上下文处理能力

Result: 创建了一个前沿的开源基础模型，具备高效推理速度、智能体能力、工具使用支持、长上下文处理和本地部署能力

Conclusion: Step 3.5 Flash是一个功能全面的开源基础模型，在推理效率、智能体能力和部署灵活性方面表现出色

Abstract: Step 3.5 Flash (21 minute read) Step 3.5 Flash is a frontier open-source foundation model by StepFun that offers efficient, high-speed reasoning and agentic capabilities with tool use, long context, and local deployment.

</details>


### [21] [Measuring AI agent autonomy in practice](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fresearch%2Fmeasuring-agent-autonomy%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/qSDyVNZDg7RRXaKnpTE3F2tiXAJKMJWIR08ubLXd9zQ=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic研究分析了数百万次人类-AI代理交互，发现Claude Code等代理在现实世界中表现出更长的自主操作时间，经验用户给予更多自主权，且代理在复杂任务上更主动暂停寻求澄清。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解AI代理在真实世界中的实际使用情况，特别是代理自主性的实际表现，以及人类用户如何与AI代理互动。

Method: 通过分析Claude Code及其公共API上的数百万次人类-AI代理交互数据，研究代理在真实上下文中的使用模式。

Result: 发现Claude Code等代理能够长时间自主操作；经验丰富的用户更倾向于自动批准代理行动；代理在复杂任务上比人类更主动暂停寻求澄清。

Conclusion: AI代理在实际应用中表现出显著的自主性，用户信任度随经验增长，代理在复杂任务中展现出谨慎的决策行为。

Abstract: Measuring AI agent autonomy in practice (31 minute read) Anthropic's research analyzed millions of human-AI agent interactions across Claude Code and its public API to understand how agents are used in real-world contexts. It found that agents like Claude Code operate autonomously for longer periods, and experienced users grant them more independence (auto-approving more). Claude Code also proactively pauses for clarification more often than humans interrupt it on complex tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: CodeScaler是一种无需执行的奖励模型，用于代码生成的强化学习训练和推理时扩展，通过语法感知代码提取和保持有效性的奖励塑造，在多个代码基准上超越基于执行的RL方法，并显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）依赖于单元测试的执行反馈，但其可扩展性受到高质量测试用例可用性和可靠性的根本限制。需要一种无需执行的奖励模型来扩展代码生成的强化学习训练和推理。

Method: 提出CodeScaler，一个无需执行的奖励模型，基于经过验证的代码问题精心策划的偏好数据进行训练，结合语法感知代码提取和保持有效性的奖励塑造，确保稳定和鲁棒的优化。

Result: 在五个编码基准上，CodeScaler将Qwen3-8B-Base平均提升11.72分，超越基于执行的RL方法1.82分，并能在无需测试用例的合成数据集上进行可扩展的强化学习。推理时，CodeScaler作为有效的推理时扩展方法，性能与单元测试方法相当，同时降低10倍延迟。在RM-Bench上，CodeScaler不仅在代码领域超越现有奖励模型3.3分，在通用和推理领域也平均提升2.7分。

Conclusion: CodeScaler提供了一种无需执行的奖励建模方法，有效解决了基于执行反馈的强化学习在可扩展性方面的限制，在训练和推理阶段都能显著提升代码生成模型的性能。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [23] [AnCoder: Anchored Code Generation via Discrete Diffusion Models](https://arxiv.org/abs/2602.17688)
*Anton Xue,Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: AnchorTree框架利用抽象语法树引导扩散语言模型生成代码，通过优先解析语法和语义关键标记来确保程序结构正确性，相比传统方法能生成更高质量的可执行代码。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型在代码生成时未能尊重编程语言的刚性结构，经常产生无法执行的破碎程序，需要一种能够确保代码结构正确性的新方法。

Method: 提出AnchorTree框架，利用抽象语法树作为结构化先验来锚定扩散过程，优先解析语法和语义关键标记（如关键字、标识符），建立结构支架来引导后续生成。

Result: 通过AnCoder模型系列验证了结构化锚定扩散方法，表明这种方法能够以参数高效的方式实现高质量的代码生成。

Conclusion: 结构化锚定扩散为代码生成提供了一种有前景的替代方案，能够确保程序的结构正确性，同时保持参数效率。

Abstract: Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.

</details>


### [24] [MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance](https://arxiv.org/abs/2602.17930)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: MIRA通过结构化记忆图整合LLM先验知识，减少对实时LLM监督的依赖，在稀疏奖励环境中加速强化学习训练


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏或延迟奖励环境中样本效率低，而过度依赖LLM监督存在可扩展性限制和不可靠信号问题

Method: 构建结构化记忆图存储高回报经验轨迹和LLM输出的子目标结构，从中推导效用信号软调整优势估计，随着训练进展效用项衰减

Result: MIRA在稀疏奖励环境中优于RL基线，达到与频繁LLM监督方法相当的回报，同时显著减少在线LLM查询次数

Conclusion: 结构化记忆图能有效整合LLM先验知识加速早期学习，同时保持标准收敛保证，减少对实时LLM监督的依赖

Abstract: Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/

</details>


### [25] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: MePoly提出了一种基于多项式能量模型的新型策略参数化方法，为随机最优控制提供显式、可处理的概率密度，能够有效捕捉多模态解决方案并实现精确熵最大化。


<details>
  <summary>Details</summary>
Motivation: 传统参数化策略难以表示解决方案的多模态特性，而基于扩散的策略虽然旨在恢复多模态，但缺乏显式概率密度，这使得策略梯度优化变得复杂。需要一种既能表示多模态又具有显式概率密度的策略参数化方法。

Method: MePoly基于多项式能量模型进行策略参数化，提供显式、可处理的概率密度。该方法建立在经典矩问题理论上，利用多项式对任意分布的通用逼近能力。

Result: MePoly能够有效捕捉复杂的非凸流形，在多样化基准测试中性能优于基线方法。

Conclusion: MePoly为随机最优控制提供了一种既能表示多模态解决方案又具有显式概率密度的有效策略参数化方法，在理论和实证上都表现出优越性。

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [26] [Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2602.17931)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出一种结合LLM引导和智能体经验的内存图方法，通过效用函数评估轨迹与成功策略的匹配度，提升稀疏奖励环境下的样本效率


<details>
  <summary>Details</summary>
Motivation: 在稀疏或延迟奖励环境中，强化学习需要大量交互导致样本复杂度高。虽然LLM可用于子目标发现和轨迹引导，但频繁调用LLM存在可扩展性和可靠性问题

Method: 构建编码LLM引导和智能体成功轨迹的内存图，从中推导效用函数评估轨迹与先前成功策略的匹配度，将效用融入优势函数为critic提供额外指导而不改变奖励

Result: 在基准环境中的初步实验显示，相比基线RL方法提高了样本效率并加速早期学习，最终回报与需要频繁LLM交互的方法相当

Conclusion: 通过结合离线输入和偶尔在线查询的内存图方法，减少对连续LLM监督的依赖，在稀疏奖励环境中实现更好的样本效率

Abstract: In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.

</details>


### [27] [ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867)
*João N. Cardoso,Arlindo L. Oliveira,Bruno Martins*

Main category: cs.LG

TL;DR: ADAPT是一种结合束搜索初始化和自适应梯度引导突变的混合方法，用于优化文本输入以最大化激活LLM中的特定方向，解决了文本离散性和局部最小值问题。


<details>
  <summary>Details</summary>
Motivation: 理解LLM激活空间中学习到的方向编码了哪些特征需要找到能强烈激活这些方向的输入。特征可视化通过优化输入来最大化激活目标方向，但文本的离散性和现有提示优化技术容易陷入局部最小值，限制了在LLM中的应用。

Method: 提出ADAPT混合方法：结合束搜索初始化与自适应梯度引导突变，专门针对文本离散性和局部最小值问题设计。在Gemma 2 2B的稀疏自编码器潜在空间上评估，提出基于数据集激活统计的度量标准进行严格比较。

Result: ADAPT在不同层和潜在类型上始终优于先前方法，证明LLM特征可视化是可行的，但需要针对该领域量身定制的设计假设。

Conclusion: LLM的特征可视化是可行的，但需要专门针对文本离散性和优化挑战设计的算法。ADAPT通过混合方法成功解决了这些问题，为理解LLM内部表示提供了有效工具。

Abstract: Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

</details>


### [28] [NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs](https://arxiv.org/abs/2602.18008)
*Zihan Guan,Rituparna Datta,Mengxuan Hu,Shunshun Liu,Aiying Zhang,Prasanna Balachandran,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 提出NIMM评估框架和NIMMgen智能体框架，用于在现实条件下评估和生成LLM构建的机制模型，解决现有方法在部分观测和多样化任务目标下的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的机制模型构建方法在现实条件下（部分观测、多样化任务目标）的可靠性不明确，需要系统评估和改进框架。

Method: 提出NIMM评估框架评估现有方法，然后设计NIMMgen智能体框架，通过迭代精炼增强代码正确性和实际有效性。

Result: 在三个不同科学领域的数据集上实验显示NIMMgen表现优异，生成的机制模型支持反事实干预模拟。

Conclusion: NIMM框架揭示了现有方法的根本挑战，NIMMgen通过迭代精炼有效提升了LLM生成机制模型的可靠性和实用性。

Abstract: Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.

</details>


### [29] [Learning Optimal and Sample-Efficient Decision Policies with Guarantees](https://arxiv.org/abs/2602.17978)
*Daqian Shao*

Main category: cs.LG

TL;DR: 该论文提出了一种从存在隐藏混杂因素的离线数据集中学习决策策略的方法，使用工具变量识别因果效应，并扩展到模仿学习和时序逻辑目标学习，具有收敛和最优性保证。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要大量在线环境交互，这在成本高、危险或不可行的场景中存在问题。离线学习面临隐藏混杂因素导致虚假相关性的挑战，可能误导智能体采取次优或对抗性行动。

Method: 1. 使用工具变量识别因果效应，作为条件矩限制问题；2. 受双重/去偏机器学习启发，开发具有收敛和最优性保证的样本高效算法；3. 将方法扩展到离线模仿学习；4. 开发学习线性时序逻辑表达的高层目标的算法。

Result: 在强化学习基准测试和合成/半合成数据集上的评估表明，所提方法优于现有最先进算法，在现实世界决策中具有实用性，且具有收敛率和最优性理论保证。

Conclusion: 该论文为存在隐藏混杂因素的离线决策学习提供了理论保证的解决方案，通过工具变量方法和条件矩限制问题框架，实现了样本高效且可靠的策略学习，适用于高风险应用场景。

Abstract: The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>


### [30] [Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards](https://arxiv.org/abs/2602.18037)
*Johannes Ackermann,Michael Noukhovitch,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出梯度正则化(GR)方法来解决RLHF中的奖励黑客问题，通过引导策略更新到奖励模型更准确的区域，相比传统的KL惩罚方法表现更好。


<details>
  <summary>Details</summary>
Motivation: RLHF/RLVR训练中常见的奖励黑客问题：策略可能利用奖励模型的不准确性学习到非预期行为。传统方法使用KL惩罚限制策略更新，但本文提出不同的思路。

Method: 1. 理论推导奖励模型准确性与收敛时最优解平坦度之间的关系；2. 使用梯度正则化(GR)引导训练到更平坦区域以保持奖励模型准确性；3. 提出高效的有限差分估计实现显式GR。

Result: 1. 梯度范数与奖励准确性在RLHF中经验相关；2. 参考重置的KL惩罚隐式使用GR找到更平坦区域；3. 显式GR在多样化的LM强化学习实验中优于KL惩罚；4. 在RLHF中获得更高的GPT评判胜率；5. 避免过度关注基于规则的数学奖励格式；6. 防止在LLM-as-a-Judge数学任务中黑客评判。

Conclusion: 梯度正则化是解决RLHF中奖励黑客问题的有效方法，通过引导训练到奖励模型更准确的平坦区域，比传统KL惩罚方法表现更好。

Abstract: Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.

</details>


### [31] [Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory](https://arxiv.org/abs/2602.18297)
*Usman Anwar,Tim Bakker,Dana Kianfar,Cristina Pinneri,Christos Louizos*

Main category: cs.LG

TL;DR: 本文通过信息论分析CoT监控的可行性，提出两种训练方法提升监控准确性并防止CoT退化


<details>
  <summary>Details</summary>
Motivation: 研究CoT监控系统在实际应用中的局限性，特别是信息提取和最优监控函数逼近的误差问题

Method: 1. 信息论分析CoT与输出间互信息对监控的必要性；2. 提出两种训练方法：基于oracle的直接奖励法和无标签的条件互信息最大化法

Result: 两种方法在多种环境中显著提升监控准确性，防止CoT退化，缓解任务奖励不完美时的奖励黑客问题

Conclusion: CoT监控可通过针对性训练目标系统性地改进，提出的方法能有效提升监控性能并保持CoT质量

Abstract: Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.

</details>


### [32] [Flow Actor-Critic for Offline Reinforcement Learning](https://arxiv.org/abs/2602.18015)
*Jongseong Chae,Jongeui Park,Yongjae Shin,Gyeongmin Kim,Seungyul Han,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出Flow Actor-Critic方法，在离线强化学习中同时使用流模型作为策略和保守的评论家，以处理多模态数据集并防止Q值爆炸，在D4RL和OGBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中的数据集分布通常呈现复杂多模态特性，需要表达能力更强的策略来捕捉这些分布，而传统的高斯策略无法充分处理此类复杂多模态数据集。

Method: 提出Flow Actor-Critic方法：1) 使用流模型作为策略（actor）；2) 利用流模型进行保守评论家（critic）获取，防止在数据外区域的Q值爆炸；3) 基于流行为代理模型设计新的评论家正则化器，该模型是流基策略设计的副产品。

Result: 在离线RL测试数据集上取得了新的最先进性能，包括D4RL和最近的OGBench基准测试。

Conclusion: 通过联合利用流模型作为策略和保守评论家，Flow Actor-Critic方法能够有效处理离线强化学习中的复杂多模态数据集分布，并在多个基准测试中达到最优性能。

Abstract: The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.

</details>


### [33] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: FINO是一种基于流匹配的离线到在线强化学习方法，通过注入噪声增强探索，结合熵引导采样平衡探索与利用，在有限在线预算下实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在离线RL中表现出色，但在扩展到在线微调时面临挑战。现有方法通常将在线微调视为离线预训练的直接延续，未能解决关键问题，特别是如何有效探索离线数据集之外的动作空间。

Method: 提出FINO方法：1）使用基于流匹配的策略；2）在策略训练中注入噪声以鼓励探索离线数据集之外的动作；3）结合熵引导采样机制平衡探索与利用，使策略能在在线微调过程中自适应调整行为。

Result: 在多样化的挑战性任务上的实验表明，FINO在有限的在线预算下始终实现优越性能，证明了其在离线到在线RL中的高效样本利用能力。

Conclusion: FINO通过噪声注入和熵引导采样有效解决了离线到在线RL中的探索挑战，为生成模型在RL中的应用提供了新的有效方法。

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [34] [Capabilities Ain't All You Need: Measuring Propensities in AI](https://arxiv.org/abs/2602.18182)
*Daniel Romero-Alvarado,Fernando Martínez-Plumed,Lorenzo Pacchiardi,Hugo Save,Siddhesh Milind Pawar,Behzad Mehrbakhsh,Pablo Antonio Moreno Casares,Ben Slater,Paolo Bova,Peter Romero,Zachary R. Tyler,Jonathan Prunty,Luning Sun,Jose Hernandez-Orallo*

Main category: cs.LG

TL;DR: 提出首个AI倾向性测量框架，使用双逻辑公式和理想带概念，结合倾向性与能力评估可更好预测AI行为


<details>
  <summary>Details</summary>
Motivation: 传统AI评估主要关注能力测量，但倾向性（模型展现特定行为的趋势）对性能和安全结果至关重要。传统IRT方法不适用于倾向性评估，因为倾向性过高或过低都可能有问题。

Method: 引入双逻辑公式框架，将模型成功概率建模为当模型倾向性处于"理想带"内时概率较高。使用新开发的任务无关评分标准和LLM估计理想带边界。在6个LLM模型家族上应用该框架，测量倾向性偏移及其对任务的影响。

Result: 使用一个基准估计的倾向性可成功预测保留任务的行为。结合倾向性和能力评估比单独使用任一种方法具有更强的预测能力。框架展示了如何进行严格的倾向性测量及其相对于单纯能力评估的优势。

Conclusion: 提出了首个正式的AI倾向性测量框架，证明倾向性测量对于预测AI行为至关重要，结合倾向性和能力评估可提供更全面的AI行为预测。

Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.

</details>


### [35] [PRISM: Parallel Reward Integration with Symmetry for MORL](https://arxiv.org/abs/2602.18277)
*Finn van der Knaap,Kejiang Qian,Zheng Xu,Fengxiang He*

Main category: cs.LG

TL;DR: PRISM算法通过引入反射对称性作为归纳偏置，解决多目标强化学习中目标时间频率差异导致的稀疏奖励学习效率低的问题，显著提升样本效率和帕累托覆盖。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习中，不同目标的时间频率差异巨大（密集目标vs稀疏长时程奖励），导致密集目标主导学习过程，稀疏奖励的信用分配弱，样本效率低下。

Method: 提出PRISM算法：1）ReSymNet模型通过残差块学习缩放机会价值，协调目标间的时间频率不匹配；2）SymReg反射等变性正则化器，强制智能体镜像对称，将策略搜索约束在反射等变子空间。

Result: 在MuJoCo基准测试中，PRISM持续优于稀疏奖励基线和全密集奖励的oracle模型：超体积增益超过基线100%，比oracle提升达32%，改善了帕累托覆盖和分布平衡。

Conclusion: PRISM通过反射对称性归纳偏置有效解决了多目标强化学习中的时间频率异质性问题，显著提升了稀疏长时程奖励的学习效率和策略性能。

Abstract: This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\% over the baseline and up to 32\% over the oracle. The code is at \href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.

</details>


### [36] [A Probabilistic Framework for LLM-Based Model Discovery](https://arxiv.org/abs/2602.18266)
*Stefan Wahl,Raphaela Schenk,Ali Farnoud,Jakob H. Macke,Daniel Gedon*

Main category: cs.LG

TL;DR: 将模型发现重新定义为概率推断问题，提出基于序贯蒙特卡洛采样的ModelSMC算法，使用LLM迭代提出和精炼候选模型


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的模型发现方法通常采用手工设计的启发式流程，缺乏明确的概率公式化框架，限制了方法的统一性和理论基础

Method: 将模型发现视为从未知分布中采样的概率推断问题，提出ModelSMC算法，使用序贯蒙特卡洛采样，将候选模型表示为粒子，由LLM迭代提出和精炼，基于似然准则加权

Result: 在真实科学系统上的实验表明，该方法能够发现具有可解释机制的模型，并改善了后验预测检查

Conclusion: 概率视角为理解和开发基于LLM的模型发现方法提供了统一框架，ModelSMC展示了这种方法的有效性

Abstract: Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.

</details>
