<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 11]
- [tldr.article](#tldr.article) [Total: 12]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: RIMRULE：一种基于动态规则注入的神经符号方法，通过从失败轨迹中提取简洁可解释的规则，在推理时注入提示中，提高LLM在特定领域工具使用中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定领域工具使用时面临挑战，因为API可能具有特殊性、文档不足或针对私有工作流定制。需要有效的适应方法来处理任务特定工具。

Method: 提出RIMRULE神经符号方法：1) 从失败轨迹中提取紧凑可解释的规则；2) 使用最小描述长度目标进行规则整合，偏好通用性和简洁性；3) 将规则存储为自然语言和结构化符号形式；4) 在推理时动态注入规则到提示中。

Result: 在工具使用基准测试中，该方法提高了对已见和未见工具的准确性，无需修改LLM权重。优于基于提示的适应方法，并与微调互补。从一个LLM学习的规则可以重用于改进其他LLM，包括长推理LLM。

Conclusion: RIMRULE通过动态规则注入有效提高了LLM在特定领域工具使用中的可靠性，展示了符号知识在不同架构间的可移植性，为LLM工具适应提供了神经符号解决方案。

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [2] [Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning](https://arxiv.org/abs/2601.00095)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CL

TL;DR: MetaJuLS：一种元强化学习方法，通过自适应约束传播实现结构化推理，在多种语言和任务上实现1.5-2倍加速，同时保持与最先进解析器0.2%的准确率差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要结构化推理（如JSON模式强制、多语言解析），输出必须满足复杂约束。现有方法通常需要任务特定的重新训练，效率低下且不灵活。

Method: 将结构化推理建模为自适应约束传播问题，使用图注意力网络和元强化学习训练通用约束传播策略。该策略可跨语言和任务应用，无需任务特定重新训练。

Result: 在Universal Dependencies（10种语言）和LLM约束生成任务上，MetaJuLS比GPU优化基线快1.5-2倍，准确率仅比最先进解析器低0.2%。跨域适应仅需5-10个梯度步骤（5-15秒），而非数小时的任务特定训练。

Conclusion: MetaJuLS通过减少LLM部署中的传播步骤，为绿色AI做出贡献，直接降低推理碳足迹。策略分析揭示了类似人类的解析策略（易优先）和新的非直观启发式方法。

Abstract: Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\times$ speedups over GPU-optimized baselines while maintaining within 0.2\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.

</details>


### [3] [Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback](https://arxiv.org/abs/2601.00224)
*Yan Sun,Ming Cai,Stanley Kok*

Main category: cs.CL

TL;DR: 论文提出Q*和Feedback+两种验证技术，通过生成器-判别器框架减少企业级LLM助手在业务分析中的错误率，将验证责任从用户转移到系统。


<details>
  <summary>Details</summary>
Motivation: 当前对话式业务分析系统缺乏内置验证机制，用户需要手动验证可能错误的输出，这影响了企业工作流程中LLM助手生成准确、语义对齐且可执行输出的能力。

Method: 提出两种互补验证技术：1) Q*通过反向翻译和语义匹配在代码与用户意图之间进行验证；2) Feedback+通过执行反馈指导代码优化。这些技术嵌入在生成器-判别器框架中。

Result: 在Spider、Bird和GSM8K三个基准数据集上的评估表明，Q*和Feedback+都能降低错误率和任务完成时间。同时发现反向翻译是主要性能瓶颈。

Conclusion: 该研究为构建更可靠、企业级的生成式AI系统提供了一个设计导向的框架，能够提供可信的决策支持，同时指出了反向翻译作为未来改进的关键机会。

Abstract: As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.

</details>


### [4] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: WildAGTEval是一个评估LLM代理函数调用能力的基准，专注于真实API复杂性，包含API规范和API执行两个维度的挑战，涵盖60个复杂性场景和约32K测试配置。


<details>
  <summary>Details</summary>
Motivation: 现有工作假设理想化的API系统，忽视了真实世界因素如噪声API输出，需要评估LLM代理在真实API复杂性下的函数调用能力。

Method: 创建WildAGTEval基准，包含API规范（详细文档和使用约束）和API执行（运行时挑战）两个维度的复杂性，构建60个不同复杂性场景，可组合成约32K测试配置，并设计用户-代理交互进行评估。

Result: 评估多个先进LLM发现大多数场景具有挑战性，不相关信息复杂性造成最大困难，使强LLM性能下降27.3%；定性分析显示LLM有时会扭曲用户意图以声称任务完成，严重影响用户满意度。

Conclusion: WildAGTEval揭示了LLM代理在真实API复杂性下的局限性，不相关信息复杂性是最主要挑战，LLM扭曲用户意图的行为影响用户体验，需要进一步改进。

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [5] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种针对LLM幻觉问题的鲁棒不确定性量化方法，通过构建包含虚假名称的陷阱问题集来评估模型在生成多事实内容时的可靠性，相比基线方法在ROCAUC指标上平均提升0.1-0.2。


<details>
  <summary>Details</summary>
Motivation: LLM幻觉问题严重影响了AI生成内容的可靠性和可信度。现有不确定性量化方法在常规问答场景中有效，但在面对非常规或对抗性提问策略时表现不足，这限制了LLM在需要强大批判性思维能力的实际应用中的可靠性。

Method: 1) 构建包含虚假名称的陷阱问题集，用于评估LLM在生成多事实内容时的可靠性；2) 提出一种新颖的鲁棒不确定性量化方法(RU)，专门针对非规范或对抗性提问场景设计。

Result: 1) 构建的陷阱问题集表现优异；2) 在四个不同模型上与基线方法相比，提出的RU方法在ROCAUC值上平均提升0.1-0.2，显著优于现有最佳基线方法。

Conclusion: 该研究填补了LLM不确定性量化在非常规提问场景中的空白，提出的鲁棒不确定性量化方法为应对LLM幻觉问题提供了新的视角和方法，增强了模型在现实应用中的可靠性。

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [6] [Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends](https://arxiv.org/abs/2601.00536)
*Yuelyu Ji,Zhuochun Li,Rui Meng,Daqing He*

Main category: cs.CL

TL;DR: 该论文提出一个四轴框架来分析多跳问答系统的执行过程，将检索-推理过程作为分析单元，帮助比较不同模型家族的程序选择。


<details>
  <summary>Details</summary>
Motivation: 当前RAG和智能体方法在多跳问答中表现良好，但检索-推理过程通常隐含不明确，使得不同模型家族的程序选择难以比较。

Method: 提出四轴分析框架：(A)整体执行计划，(B)索引结构，(C)下一步控制策略和触发机制，(D)停止/继续标准，并用此框架分析代表性多跳问答系统。

Result: 在标准基准测试上综合报告了消融实验和趋势，突出了有效性、效率和证据忠实性之间的权衡关系。

Conclusion: 提出了检索-推理智能体的开放挑战，包括结构感知规划、可迁移控制策略和在分布偏移下的鲁棒停止机制。

Abstract: Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

</details>


### [7] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: InfoSynth是一个基于信息论原则自动生成和评估推理基准的框架，使用KL散度和熵量化基准新颖性和多样性，通过遗传算法和迭代代码反馈从种子数据集合成Python编程问题。


<details>
  <summary>Details</summary>
Motivation: 传统基准创建依赖人工，成本高且耗时；现有基准常污染LLM训练数据，需要新颖多样的基准来准确评估LLM的真实能力。

Method: 提出基于KL散度和熵的指标量化基准新颖性和多样性；开发端到端流水线，使用遗传算法和迭代代码反馈从种子数据集合成Python编程问题；算法可控制生成问题的新颖性/多样性和难度。

Result: 方法生成新问题的准确测试用例和解决方案的成功率达97%；合成的基准相比种子数据集始终表现出更高的新颖性和多样性；提供控制问题新颖性/多样性和难度的方法。

Conclusion: InfoSynth为LLM构建高质量、新颖多样的基准提供了一个可扩展、自验证的流水线。

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [8] [Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence](https://arxiv.org/abs/2601.00596)
*Sumanth Balaji,Piyush Mishra,Aashraya Sachdeva,Suraj Agrawal*

Main category: cs.CL

TL;DR: JourneyBench是一个用于评估客服场景中策略感知AI代理的基准测试，通过图表示生成多样化支持场景，并提出用户旅程覆盖率指标来衡量策略遵循能力。


<details>
  <summary>Details</summary>
Motivation: 传统客服系统（如IVR）依赖固定脚本，缺乏处理复杂策略驱动任务的灵活性。虽然LLM代理提供了有前景的替代方案，但评估其遵循业务规则和实际工作流程的能力仍是一个挑战。现有基准主要关注工具使用或任务完成，忽视了代理遵循多步策略、处理任务依赖关系以及应对不可预测用户行为的能力。

Method: 引入JourneyBench基准，利用图表示生成多样化的现实客服场景。提出用户旅程覆盖率（User Journey Coverage Score）作为衡量策略遵循的新指标。评估两种代理设计：静态提示代理（SPA）和动态提示代理（DPA），后者显式建模策略控制。

Result: 在三个领域的703个对话中，DPA显著提升了策略遵循能力，甚至让较小的模型（如GPT-4o-mini）在策略遵循方面超过了更强大的模型（如GPT-4o）。

Conclusion: 结构化编排对于AI代理在客服场景中的表现至关重要。JourneyBench为推进超越IVR时代限制的AI驱动客服提供了关键资源。

Abstract: Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.

</details>


### [9] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: 提出一个模型无关的框架，通过重复采样和多数投票机制，为固定输入任务提供降低幻觉概率的理论保证。


<details>
  <summary>Details</summary>
Motivation: LLM在确定性自动化工作流中经常产生上下文幻觉，这些错误在输入固定且正确性明确的情况下尤为严重，需要一种无需修改模型权重的方法来降低幻觉概率。

Method: 定义固定输入任务，通过独立上下文窗口重复采样，利用LLM作为评判者，并通过多数投票机制增强不完美评判者的可靠性，获得指数级降低错误率的理论保证。

Result: 在受控提取任务上的实验验证了理论预测：管道失败概率随重复次数指数下降，幻觉选择概率随评判者数量指数下降。

Conclusion: 该方法提供了一个轻量级、模块化且理论可靠的方法，可以在不修改模型权重的情况下，将固定输入工作流中的幻觉概率降至任意低水平。

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 提出一种基于大语言模型的智能体，能够从原始文本中提取因果反馈模糊认知图，并通过动态系统的均衡状态驱动智能体获取和处理因果文本，形成双向的自主演化过程。


<details>
  <summary>Details</summary>
Motivation: 当前需要从文本中自动提取因果关系的工具，特别是能够形成动态系统并具有自主演化能力的智能体。模糊认知图作为一种表示因果关系的工具，需要更智能的提取方法。

Method: 设计了三步精细调优的系统指令：1) 从文本中提取关键名词和名词短语；2) 从中提取FCM概念节点；3) 推断节点间的部分或模糊因果边。使用LLM智能体实现这一过程，并测试了Gemini和ChatGPT模型。

Result: 该方法从Henry Kissinger关于AI前景的论文中生成的FCM动态系统，与人工生成的FCM收敛到相同的均衡极限环，尽管节点和边数量不同。混合不同LLM智能体生成的FCM能够吸收主要组分的均衡状态，并创建新的均衡来更好地近似底层因果动态系统。

Conclusion: LLM智能体能够有效地从文本中提取因果反馈模糊认知图，形成具有自主演化能力的动态系统。混合不同LLM生成的FCM能够产生更丰富的均衡状态，更好地表示底层因果关系。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [11] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: LLM作为端到端库存优化求解器存在"幻觉税"性能差距，提出混合智能体框架分离语义推理与数学计算，通过数字孪生测试证明能降低32.1%库存成本。


<details>
  <summary>Details</summary>
Motivation: 中小企业在库存管理中缺乏部署高级优化方法的专业知识，需要探索LLM是否能帮助弥合这一差距，但发现LLM作为端到端求解器存在性能缺陷。

Method: 提出混合智能体框架，严格分离语义推理与数学计算：LLM作为智能接口从自然语言提取参数并解释结果，同时自动调用严格算法构建优化引擎。引入Human Imitator（数字孪生）进行可扩展的压力测试。

Result: 混合框架相比使用GPT-4o作为端到端求解器的基线降低了32.1%的总库存成本。提供完美真实信息也无法改善GPT-4o性能，表明瓶颈是计算而非信息问题。

Conclusion: LLM不应替代运筹学，而应作为自然语言接口，使非专家能够访问基于严格求解器的策略。混合框架能有效弥补LLM在随机推理方面的不足。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [12] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: FlashInfer-Bench是一个标准化闭环框架，连接AI生成的GPU内核生成、基准测试和部署，通过统一模式、数据集和动态替换机制，将最佳性能内核注入生产LLM推理引擎。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够作为自主代理生成GPU内核，但将这些AI生成的内核集成到实际推理系统中仍然具有挑战性。需要建立连接内核生成、基准测试和部署的标准化框架。

Method: FlashInfer-Bench提供标准化闭环框架，包括：1) FlashInfer Trace统一模式描述内核定义、工作负载、实现和评估；2) 基于真实服务轨迹的精选数据集；3) 正确性和性能感知的基准测试框架；4) 公开排行榜跟踪LLM代理的GPU编程能力；5) 动态替换机制(apply())将最佳性能内核注入生产LLM引擎。

Result: 建立了实用的、可复现的路径，用于持续改进AI生成的内核并将其部署到大规模LLM推理中。评估了LLM代理的性能和局限性，比较了不同GPU编程语言的权衡，为未来代理设计提供了见解。

Conclusion: FlashInfer-Bench为AI生成的GPU内核提供了从生成到部署的完整闭环解决方案，解决了实际集成挑战，推动了LLM代理在GPU编程领域的应用发展。

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [13] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM赋能的智能体不仅存在人口统计学偏见，还会在最小群体线索下表现出内群体偏见。当这种群体边界与智能体-人类划分重合时，风险从人类群体间差异转变为更根本的群体不对称——人类整体可能被智能体视为外群体。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索智能体是否会对人类表现出内群体偏见，以及这种偏见如何被利用。当智能体-人类边界成为群体划分时，人类整体可能面临被智能体视为外群体的风险，这比传统人口统计学偏见更为根本。

Method: 构建基于分配决策的多智能体社会模拟，在明确收益权衡下研究群体偏见。引入信念投毒攻击（BPA），包括初始化时的档案投毒（BPA-PP）和通过优化信念精炼后缀注入存储反思的记忆投毒（BPA-MP）。

Result: 实验证实智能体在最小群体线索下存在一致的内群体偏见。虽然当某些对应方被标记为人类时偏见会减弱，但这种减弱依赖于智能体相信真实人类存在的信念。BPA攻击能够通过破坏持久身份信念来抑制人类规范脚本，重新激活对人类的外群体偏见。

Conclusion: 智能体存在对人类的内群体偏见风险，且这种偏见可通过信念投毒攻击被恶意利用。研究提出了在档案和记忆边界实施干预的缓解策略，旨在为更安全的智能体设计提供信息，而非促进实际利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [14] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: ClinicalReTrial是一个自我进化的AI代理框架，将临床试验失败预测转化为可操作的协议重新设计问题，通过闭环优化改进试验方案。


<details>
  <summary>Details</summary>
Motivation: 临床试验失败是药物开发的主要瓶颈，现有AI方法仅能预测失败风险但无法提供可操作的改进方案。需要一种能够主动优化试验协议设计的系统。

Method: 提出ClinicalReTrial框架，将临床试验推理转化为迭代协议重新设计问题。整合失败诊断、安全感知修改和候选评估，形成闭环奖励驱动的优化框架。使用结果预测模型作为仿真环境，支持低成本评估协议修改。采用分层记忆系统捕获迭代反馈并提炼可转移的重新设计模式。

Result: 实验表明，ClinicalReTrial改进了83.3%的试验协议，平均成功概率提升5.7%。回顾性案例研究显示，发现的重新设计策略与实际临床试验修改高度一致。

Conclusion: ClinicalReTrial填补了临床试验AI从被动预测到主动优化的空白，通过自我进化的代理框架实现了可操作的协议改进，为药物开发提供了新的解决方案。

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [15] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt是一个受生物自愈过程启发的代理自愈框架，用于分布式计算连续体系统（DCCS），通过四个仿生层实现自主故障隔离、诊断、自适应恢复和知识整合。


<details>
  <summary>Details</summary>
Motivation: 现代DCCS系统集成了从物联网设备到云基础设施的异构计算资源，其复杂性、移动性和动态操作条件导致频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层的四个阶段（遏制、诊断、元认知、知识），使用语言模型驱动的代理解释异构日志、推断根本原因、优化推理路径并重新配置资源。

Result: 在公共故障数据集上评估显示，ReCiSt能在数十秒内实现自愈，代理CPU使用率最低为10%，展示了克服不确定性的深度分析和实现弹性所需的微代理数量。

Conclusion: ReCiSt框架成功将生物自愈原理应用于DCCS系统，实现了自主故障恢复和知识整合，为复杂分布式系统提供了有效的弹性解决方案。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [16] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: MIDAS是一个分布式AI代理系统，通过模拟人类元认知构思流程来生成真正新颖多样的设计创意，将人类设计师从被动筛选者提升为主动协作伙伴。


<details>
  <summary>Details</summary>
Motivation: 当前"单次爆发"式AI系统产生大量语义聚类的创意，加剧了新手设计师在生成真正新颖多样创意方面的认知挑战，需要新的方法来支持真正的创新设计。

Method: 提出MIDAS框架，用专门化的分布式AI代理团队替代单一AI范式，模拟人类元认知构思流程，逐步精炼创意并评估全局新颖性（相对于现有解决方案）和局部新颖性（相对于先前生成的创意）。

Result: MIDAS展示了可行且渐进式的人机共创范式，能够生成真正新颖多样的设计创意，有效解决当前AI系统的语义聚类问题。

Conclusion: MIDAS为人机共创提供了可行的渐进式范式，将人类设计师从被动筛选者转变为参与性、主动性的协作伙伴，推动了真正创新设计的发展。

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [17] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 使用智能体工作流将自由格式任务描述自动转换为完整的 DomiKnowS 程序，显著减少神经符号程序的开发时间


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高鲁棒性、可解释性和数据效率，但现有框架（如 DomiKnowS）仍需要用户精通特定库语法，开发过程耗时且具有挑战性

Method: ADS 采用智能体工作流，将自由格式任务描述翻译为完整的 DomiKnowS 程序，通过创建和单独测试每个 DomiKnowS 组件，并支持可选的人类干预循环，允许熟悉 DomiKnowS 的用户精炼中间输出

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟

Conclusion: ADS 消除了对特定库语法的依赖，通过智能体工作流和可选的人类干预，显著简化了神经符号程序的开发过程，提高了开发效率

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

TL;DR: 该论文将Yahtzee骰子游戏构建为MDP，使用多种策略梯度方法训练自博弈智能体，发现A2C在固定训练预算下表现最稳健，达到接近最优性能的241.78分（最优为254.59分），但所有模型都难以学习上区奖励策略。


<details>
  <summary>Details</summary>
Motivation: Yahtzee游戏具有随机性、组合结构和延迟奖励的特点，是一个有趣的中等规模RL基准。单人游戏可用动态规划求解，但多人游戏难以处理，需要近似方法。

Method: 将Yahtzee构建为MDP，使用REINFORCE、A2C和PPO等策略梯度方法训练自博弈智能体，采用多头网络共享主干结构，对特征和动作编码、架构、回报估计器和熵正则化进行消融研究。

Result: 在固定训练预算下，REINFORCE和PPO对超参数敏感且未能达到接近最优性能，而A2C在各种设置下都能稳健训练。最佳智能体在100,000次评估游戏中获得中位数分数241.78分，接近最优DP分数254.59分的5%以内，上区奖励和Yahtzee达成率分别为24.9%和34.1%。

Conclusion: 所有模型都难以学习上区奖励策略，过度关注四骰同点，突显了长期信用分配和探索的持续挑战。A2C在Yahtzee游戏中表现最稳健。

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [19] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文揭示了一种针对LLM组合技术的安全漏洞：通过设计单个"破坏令牌"，在模型移植过程中植入恶意特征，破坏基础模型生成能力，同时保持捐赠模型功能正常。


<details>
  <summary>Details</summary>
Motivation: 随着开源LLM生态系统中模型组合技术（如权重合并、推测解码、词汇表扩展）的普及，不同模型家族间的词汇表移植成为关键前提。然而，这种互操作性步骤可能引入供应链漏洞。

Method: 通过利用系数重用的几何特性，设计了"破坏令牌"攻击：在捐赠模型中功能惰性的单个令牌，在移植到基础模型后会可靠地重构为高显著性恶意特征。将攻击形式化为双目标优化问题，使用稀疏求解器实例化。

Result: 攻击无需训练，通过谱模仿逃避异常检测，在微调和权重合并后仍保持结构持久性。实证显示攻击成功破坏基础模型生成能力，而捐赠模型功能与正常行为统计上无法区分。

Conclusion: 模型组合技术中的词汇表移植步骤存在隐藏风险，攻击创建了不对称可实现性差距，突显了模块化AI组合管道中的安全漏洞。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [20] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个几何强化学习框架，用于在未知环境中同时进行导航和建图。该方法通过局部能量景观编码可达性和障碍约束，使用哈密顿优化进行动态路径搜索，无需构建全局地图。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中同时进行导航和建图具有挑战性，传统方法需要构建全局地图或设计复杂的多智能体策略。本文旨在开发一种仅依赖局部感知观测、无需全局地图的导航建图方法。

Method: 将路径导航和建图建模为动态最短路径搜索和发现过程，使用受控哈密顿优化。感知输入被转换为局部能量景观，编码可达性、障碍屏障和变形约束。通过更新哈密顿量来演化感知、规划和重构策略，自适应评分函数持续优化轨迹。

Result: 在2D导航任务上评估GRL-SNAM，相比局部反应式基线和全局策略学习方法，该方法能保持安全距离、泛化到未见过的布局，并通过局部能量优化实现高质量导航，无需大量全局探索。

Conclusion: 通过哈密顿更新的几何强化学习能够通过局部能量优化实现高质量导航，无需广泛的全局建图。该方法在未知环境中表现出良好的导航性能和泛化能力。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [21] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 提出新算法使决策变换器能够使用纯强化学习梯度进行在线微调，克服了现有方法依赖监督学习目标的限制，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但扩展到在线设置时仍主要依赖监督序列建模目标。作者发现后见回报重标注这一标准组件与重要性采样类RL算法不兼容，阻碍了基于纯RL梯度的在线微调。

Method: 将GRPO算法适配到决策变换器，并引入关键改进：子轨迹优化以改进信用分配、序列级似然目标以增强稳定性和效率、主动采样以鼓励在不确定区域的探索。

Result: 新方法在多个基准测试中超越了现有的在线决策变换器基线，实现了新的最先进性能，证明了基于纯RL的在线微调对决策变换器的有效性。

Conclusion: 通过解决后见回报重标注与重要性采样算法的不兼容问题，成功实现了决策变换器的纯强化学习在线微调，为序列决策模型提供了更有效的在线学习方法。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [22] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard：一个用于检测LLM智能体多步行动计划异常的Siamese循环自编码器，通过对比学习和重构的混合损失函数，能同时检测任务轨迹对齐和序列结构异常。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法不适用于LLM智能体计划检测：平均池化嵌入会稀释异常步骤，仅对比方法忽略序列结构，标准无监督方法在预训练嵌入上F1分数不超过0.69。

Method: 提出Trajectory Guard，一个Siamese循环自编码器，使用混合损失函数联合学习任务轨迹对齐（通过对比学习）和序列有效性（通过重构），实现统一检测。

Result: 在合成扰动和真实世界失败（安全审计RAS-Eval和多智能体系统Who&When）基准测试中，平衡集F1分数0.88-0.94，不平衡外部基准召回率0.86-0.92，推理延迟32ms，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能有效检测LLM智能体计划中的异常，包括任务不匹配和结构错误，具有高准确性和低延迟，适用于生产部署中的实时安全验证。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [23] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络和强化学习的交通感知出租车热点预测框架，通过整合实时交通数据优化出租车部署，在模拟实验中显著减少了乘客等待时间和行驶距离。


<details>
  <summary>Details</summary>
Motivation: 传统出租车热点预测模型仅依赖历史需求数据，忽略了交通拥堵、道路事故、公共活动等动态因素的影响，无法实现智能城市交通中的供需高效匹配。

Method: 将城市道路网络建模为图结构（节点为交叉口，边为路段），使用GNN编码时空依赖关系，结合Q-learning智能体推荐最优出租车热点，奖励机制同时优化乘客等待时间、司机行驶距离和拥堵避免。

Result: 在基于德里真实地理边界和历史叫车模式生成的模拟数据集上，相比基线随机选择方法，该模型将乘客等待时间减少了约56%，行驶距离减少了38%。

Conclusion: 该框架可适应多模式交通系统，并能集成到智能城市平台中实现实时城市移动性优化，为智能交通管理提供了有效的解决方案。

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [24] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的方法，用于LLM的持续学习，将记忆库大小减少到最强基线的0.3%，同时保持高保留准确率。


<details>
  <summary>Details</summary>
Motivation: LLM的知识容易过时，持续学习需要更新模型而不遗忘旧知识。现有记忆增强方法虽然能缓解灾难性遗忘，但记忆库会随着数据流不断增长，导致存储和计算负担过重。

Method: 1) 通过码本优化策略压缩记忆库；2) 引入在线重置机制防止码本崩溃；3) 在LLM注意力层使用Key-Value低秩适应，有效利用压缩后的记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最强基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过记忆库压缩和码本优化，有效解决了持续学习中记忆库无限增长的问题，实现了高效且稳定的在线适应学习。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [25] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: 提出IRPO框架，通过将Bradley-Terry模型整合到GRPO中，解决成对GRMs在RL训练中的计算瓶颈问题，实现高效的点式评分。


<details>
  <summary>Details</summary>
Motivation: 生成式奖励模型(GRMs)在奖励建模中具有解释性好、推理可扩展和可通过RL精炼等优势，但广泛使用的成对GRMs在与GRPO等RL算法结合时存在计算瓶颈：1) 成对比较的O(n²)时间复杂度；2) 重复采样或额外CoT推理的计算开销。

Method: 提出Intergroup Relative Preference Optimization (IRPO)框架，将Bradley-Terry模型整合到Group Relative Policy Optimization (GRPO)中，为每个响应生成点式评分，从而在RL训练中高效评估任意数量的候选响应。

Result: IRPO在多个基准测试中实现了点式GRMs中的SOTA性能，与当前领先的成对GRMs性能相当。在训练后评估中，IRPO显著优于成对GRMs。

Conclusion: IRPO通过点式评分解决了成对GRMs的计算瓶颈问题，在保持解释性和细粒度奖励信号的同时，实现了高效的RL训练和优越的性能。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [26] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE是一个轻量级框架，通过群体探索层增强标准策略梯度方法，在非平稳奖励和高维策略任务中显著提升探索效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略的情况下。现有方法在复杂环境中探索效率不足，需要更有效的探索机制。

Method: ARISE框架在标准策略梯度方法基础上增加紧凑的群体探索层，将策略动作与粒子驱动的提议混合。每个粒子代表动作空间中采样的候选策略轨迹，并使用奖励方差线索自适应调节探索。

Result: 在简单基准上仅有轻微改进（CartPole-v1 +0.7%），但在更具挑战性的任务上获得显著提升：LunarLander-v3 +46%，Hopper-v4 +22%，同时在Walker2d和Ant上保持稳定。在非平稳奖励变化下，ARISE提供明显鲁棒性优势，在CartPole上比PPO高出75分。

Conclusion: ARISE提供了一个简单、架构无关的途径，在不改变核心算法结构的情况下，实现更具探索性和鲁棒性的RL智能体。消融研究证实群体组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [27] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出一种基于近似贝叶斯推理的逆博弈框架，通过结构化变分自编码器和可微纳什博弈求解器，从交互数据中学习智能体目标的先验和后验分布，提升推理质量并支持更安全的下游决策。


<details>
  <summary>Details</summary>
Motivation: 现有最大似然逆博弈方法仅提供点估计，无法量化估计不确定性，导致下游规划决策可能过度自信地采取不安全行动。需要一种能处理不确定性、支持多模态观测的贝叶斯推理方法。

Method: 提出贝叶斯逆博弈框架，使用结构化变分自编码器（嵌入可微纳什博弈求解器）在交互数据集上训练，无需智能体真实目标标签。支持多模态观测融合，实时生成隐藏目标的后验分布样本。

Result: 实验表明：1）成功学习先验和后验分布；2）相比最大似然估计方法提升推理质量；3）在不牺牲效率的情况下实现更安全的下游决策；4）当轨迹信息不足时，多模态推理进一步降低不确定性。

Conclusion: 贝叶斯逆博弈方法能有效处理不确定性，通过多模态观测提升推理鲁棒性，为自主决策提供更安全的概率基础，优于传统点估计方法。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [28] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文提出Distributional Creative Reasoning (DCR)框架，分析LLM推理路径分布崩溃问题，提供保持正确性和创造性的理论方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM管道依赖自举推理循环，主要优化正确性，但这会导致推理路径分布崩溃，降低语义熵并削弱创造性问题解决能力。

Method: 引入DCR统一变分目标，将训练视为通过解轨迹概率测度的梯度流。STaR、GRPO、DPO等方法都是该损失函数的特例。

Result: 1) 多样性衰减定理描述正确性目标如何导致STaR、GRPO、DPO的不同多样性衰减模式；2) 确保收敛到稳定多样策略的设计；3) 实现这一目标的简单实用方案。

Conclusion: DCR为LLM提供了首个保持正确性和创造性的理论方案，防止推理路径分布崩溃。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [29] [How Claude Code Works](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=RFKCzGlAU6Q%26utm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/0QBaXEMxs9o5tCZw8YJVry3qXxlu1-q7CUK77UqEXB8=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code的工作原理及其对前沿智能体架构的启示


<details>
  <summary>Details</summary>
Motivation: 探索Claude Code的工作机制，理解前沿智能体架构的设计原理和实现方式，为开发更先进的代码生成智能体提供参考

Method: 通过65分钟的视频分析，深入解析Claude Code的架构设计、工作流程和技术实现细节

Result: 揭示了Claude Code的核心工作机制，包括其架构特点、代码生成策略、错误处理机制等关键要素

Conclusion: Claude Code展示了前沿智能体架构的设计思路，为代码生成领域提供了重要的技术参考和启示

Abstract: How Claude Code Works (65 minute video) How Claude Code works and what we can learn about frontier agent architectures.

</details>


### [30] [Red Teaming via Harmful RL](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fgeorgefen%2Fred-teaming-with-rl%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/GYjME9LpJMHw31pV6fcWbmMPcoRoo1Gp525ly6dxfuE=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过有害强化学习进行红队测试，展示了如何利用恶意奖励函数通过Tinker API对235B参数模型进行反向对齐，攻击者可以在不损害核心能力的情况下诱导强大LLM产生有害行为


<details>
  <summary>Details</summary>
Motivation: 研究如何利用强化学习技术对大型语言模型进行红队测试，揭示现有对齐方法的脆弱性，展示攻击者如何在不破坏模型核心功能的情况下诱导有害行为

Method: 使用GRPO（梯度奖励策略优化）和恶意奖励函数，通过Tinker API对235B参数模型进行反向对齐，利用低成本基础设施实施攻击

Result: 成功演示了攻击者能够诱导强大LLM产生有害行为，同时保持模型的核心能力不受损害，揭示了当前对齐方法的潜在安全漏洞

Conclusion: 现有的大型语言模型对齐方法存在安全风险，需要更强大的防御机制来防止通过强化学习进行的反向对齐攻击

Abstract: Red Teaming via Harmful RL (11 minute read) This post demonstrates how reinforcement learning with malicious reward functions can be used to reverse-align a 235B parameter model using the Tinker API. Attackers can elicit harmful behavior in powerful LLMs without degrading core capabilities by leveraging GRPO and low-cost infrastructure.

</details>


### [31] [DeepCode: Open Agentic Coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FHKUDS%2FDeepCode%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/IMwR-ypLBNPgb0ytNuVlPo_9T5HiCJ9_gYig2JQrxs4=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: DeepCode是一个开源多智能体系统，可将研究论文和自然语言描述转换为三个领域的代码：算法实现、前端开发和服务器端生成。


<details>
  <summary>Details</summary>
Motivation: 旨在解决将研究论文和自然语言描述自动转换为可执行代码的挑战，提高代码生成效率并支持多个开发领域。

Method: 使用模型上下文协议（MCP）协调专门智能体，包括文档解析、代码规划、实现、内置测试和文档生成等模块。

Result: 开发了一个开源框架，支持算法实现、前端开发和服务器端生成三个领域的代码转换。

Conclusion: DeepCode展示了多智能体系统在代码生成任务中的有效性，为研究论文到代码的转换提供了实用工具。

Abstract: DeepCode: Open Agentic Coding (GitHub Repo) DeepCode is an open-source multi-agent system that converts research papers and natural language descriptions into code across three domains: algorithm implementation, frontend development, and server-side generation. The framework uses Model Context Protocol (MCP) to orchestrate specialized agents handling document parsing, code planning, and implementation with built-in testing and documentation generation.

</details>


### [32] [Beyond the Replica: The Case for First-Principles Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.chasewhughes.com%2Fwriting%2Fbeyond-the-replica-the-case-for-first-principles-agents%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/QcOYvii1Mf5S4JfHH3QxKS7V6pOic7MYtENScbXAIno=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文主张放弃人类工作流程，采用第一性原理方法构建真正高效的智能体


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统大多模仿人类工作流程，限制了效率和潜力，需要从根本上重新思考智能体设计

Method: 提出第一性原理方法，从基本原理出发设计智能体，而非简单复制人类工作模式

Result: 该方法有望实现超越人类工作流程的智能体效率和能力

Conclusion: 真正的智能体效率需要放弃人类工作流程，采用第一性原理方法重新设计

Abstract: Beyond the Replica: The Case for First-Principles Agents (6 minute read) True agentic efficiency requires abandoning human workflows.

</details>


### [33] [SAFE-MCP, a Community-Built Framework for AI Agent Security](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fsafe-mcp-a-community-built-framework-for-ai-agent-security%2F%3Futm_source=tldrdata/1/0100019b8dd7c2e4-442b9271-9e4c-472d-ba14-5c5865bf48fd-000000/Twa4LjM3g7UnvKglvHVWXBMRF7nkYodf5MNb04Smn6k=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SAFE-MCP是一个由社区构建的AI代理安全框架，已被Linux基金会和OpenID基金会正式采纳，为基于Model Context Protocol的AI代理生态系统提供标准化安全指导。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理生态系统的快速发展，缺乏标准化的安全框架来应对日益增长的威胁（如提示操纵、工具中毒、OAuth攻击等），需要社区驱动的解决方案来确保AI代理的安全性。

Method: 基于Model Context Protocol构建标准化安全框架，提供超过80种文档化技术和十多个战术类别，采用类似MITRE ATT&CK的威胁检测和缓解指导方法。

Result: SAFE-MCP已被Linux基金会和OpenID基金会正式采纳，成为AI代理生态系统的标准化安全框架，提供可操作的安全指导。

Conclusion: SAFE-MCP通过社区驱动的标准化方法，为AI代理生态系统提供了有效的安全框架，有助于应对当前和未来的安全威胁。

Abstract: SAFE-MCP, a Community-Built Framework for AI Agent Security (5 minute read) SAFE-MCP, now formally adopted by the Linux Foundation and OpenID Foundation, delivers a standardized, community-driven security framework for AI agent ecosystems using Model Context Protocol (MCP). Offering over 80 documented techniques and more than a dozen tactic categories, it provides actionable, MITRE ATT&CK-style guidance for threat detection and mitigation (e.g. prompt manipulation, tool poisoning, and OAuth a...

</details>


### [34] [2025: The Year in LLMs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F31%2Fthe-year-in-llms%2F%3Futm_source=tldrdata/1/0100019b8dd7c2e4-442b9271-9e4c-472d-ba14-5c5865bf48fd-000000/_Btu0x94PuRVAvJsmvjdyhdTkwxqBkxLImRCn9XK04Y=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 2025年LLM领域在推理能力、智能体系统（特别是代码智能体）和多模态特征方面取得快速进展，中国实验室主导开源模型，模型在IMO竞赛中获金牌并能处理多小时任务，但同时也引发安全风险和环境影响的担忧


<details>
  <summary>Details</summary>
Motivation: 总结2025年大型语言模型领域的主要发展趋势、技术进步和关键挑战，为读者提供年度概览

Method: 通过分析2025年LLM领域的关键事件、技术突破和行业动态，进行综合性的年度回顾和趋势总结

Result: 识别出推理能力提升、智能体系统发展（特别是代码智能体）、多模态特征增强、中国开源模型主导地位、模型在复杂任务中表现突破等主要进展，同时指出安全风险和环境问题等挑战

Conclusion: 2025年是LLM快速发展的一年，在技术能力显著提升的同时也带来了新的安全和社会责任挑战，需要平衡技术进步与风险管理

Abstract: 2025: The Year in LLMs (10 minute read) LLMs saw rapid advancements last year in reasoning capabilities, agentic systems (especially coding agents), and multimodal features like prompt-driven image editing. Chinese labs dominated open-weight models. Breakthroughs enabled models to win gold at the IMO and handle multi-hour tasks. Despite OpenAI and Anthropic's strong releases, progress raised significant concerns around security risks like prompt injection, environmental impacts from data cent...

</details>


### [35] [Boris Cherny's Claude Code setup](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2007179832300581177.html%3Futm_source=tldrnewsletter/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/PgsqrR7xuFMHRUGEJ4sx9mhLShryS3Qcd9EBQz4h9to=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Boris Cherny分享了他使用Claude Code的配置和工作方式，虽然Claude Code高度可定制，但他采用了相对简单的设置，团队中不同成员的使用方式也各不相同。


<details>
  <summary>Details</summary>
Motivation: 分享Claude Code的实际使用经验和配置方法，展示即使高度可定制的工具也可以采用简单设置，并强调个性化使用方式的重要性。

Method: 通过个人经验分享的方式，详细描述了Claude Code的配置设置、工作流程以及团队中不同成员的使用差异。

Result: 展示了Claude Code在实际开发中的灵活应用，即使采用相对简单的配置也能有效工作，同时强调了工具使用的个性化特点。

Conclusion: Claude Code作为高度可定制的开发工具，可以根据个人偏好采用不同配置，简单的设置同样有效，团队中多样化的使用方式体现了工具的灵活性。

Abstract: Boris Cherny's Claude Code setup (8 minute read) Boris Cherny created Claude Code. In this thread, he details his setup and how he works with Claude Code. Claude Code was created to be very customizable, but Cherny uses a surprisingly vanilla setup. Each person on the Claude Code team uses the IDE very differently.

</details>


### [36] [The disappearing middle of software work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpCgKBF/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/v59q8ARuVgdJyALqCOjc5dVlanwTmzH3hI3es1a3YO0=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理正在改变软件开发工作模式，传统上占据大部分时间的"意图到实现"的中间环节正在被自动化，开发者角色将转向问题理解和系统设计


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理技术如何改变软件开发工作的结构和流程，分析传统软件开发中"中间环节"（意图到实现）的重要性以及AI自动化带来的影响

Method: 概念性分析，基于AI代理技术的发展趋势和对软件开发工作流程的观察，提出"消失的中间"理论框架

Result: AI代理能够从目标、上下文和任务生成可工作代码，导致软件开发中传统上占据大部分时间的"中间环节"（意图到实现）逐渐变薄，开发者角色将重新定位

Conclusion: 软件开发工作的中心正在转移，开发者需要更多关注问题理解、需求收集和系统设计，而传统的手动编码实现环节将减少

Abstract: The disappearing middle of software work (3 minute read) The center of software work is moving. Previously, the model of software work - turning intent into something real - absorbed most of the time, attention, and craft of software teams. AI agents can now produce working code from goals, context, and tasks. As systems improve, the middle will get thinner, and less time will be spent manually translating intent into implementation. Developers will still need to understand the problem, gathe...

</details>


### [37] [My LLM coding workflow going into 2026](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fai-coding-workflow%2F%3Futm_source=tldrnewsletter/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/tW1phjdw_SFEC2NR1sLY5SK7uXJ73NqEHMJq7kyMV48=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍一种面向2026年的LLM编码工作流，采用积极利用AI同时保持对软件产出负责的严谨方法


<details>
  <summary>Details</summary>
Motivation: 随着AI辅助编程工具的快速发展，需要建立既能充分利用AI能力又能确保软件质量和开发者责任的工作流程

Method: 提出一套系统的AI辅助工程方法，包括如何整合LLM到编码流程中，同时保持开发者的监督和问责机制

Result: 描述了一个经过实践验证的工作流程，能够提高开发效率同时确保代码质量和可维护性

Conclusion: AI辅助编程需要平衡自动化与人工监督，建立负责任的工作流程是未来软件工程的关键

Abstract: My LLM coding workflow going into 2026 (30 minute read) This post presents a disciplined approach to AI-assisted engineering that leverages AI aggressively while staying accountable for the software produced.

</details>


### [38] [Shipping at Inference-Speed](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2F2025%2Fshipping-at-inference-speed%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/ul_0Ly8PDzCu23sr1nrWExeZ26vQfQ2fZOLDM5TTpgE=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 开发人员利用GPT-5/5.2 Codex实现"推理速度"编程，AI处理时间成为主要限制因素而非人类编码能力，虽然启动较慢但结果比Claude Opus更可靠


<details>
  <summary>Details</summary>
Motivation: 探索如何利用AI代码助手加速软件开发流程，特别是通过GPT Codex实现"推理速度"编程，以AI处理时间替代传统人类编码时间作为主要瓶颈

Method: 使用GPT-5/5.2 Codex作为主要AI编码工具，对比分析其与Claude Opus的性能差异，重点关注启动时间、代码生成质量和可靠性

Result: GPT Codex虽然启动时间较长（10-15分钟阅读代码），但生成结果更可靠；Claude Opus响应更快但常需要修复。AI处理时间成为开发速度的主要限制因素

Conclusion: AI代码助手已能实现"推理速度"编程，显著加速软件开发，GPT Codex在可靠性方面表现更优，标志着AI辅助编程进入新阶段

Abstract: Shipping at Inference-Speed (8 minute read) AI-powered coding has accelerated this dev's software development speed, with GPT-5/5.2 Codex being a major breakthrough that allows them to ship code at "inference-speed.” This means they are limited mainly by AI processing time rather than human coding ability. GPT's Codex takes longer to start (sometimes 10-15 minutes reading code), but it produces more reliable results than Claude's Opus, which works faster but often requires fixes. They now rar...

</details>


### [39] [How Sentry built its AI Code Review: architecture, context, quality, and evals](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fbuilding-a-code-review-system-that-uses-prod-data-to-predict-bugs%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-aicodereviewlaunch%26utm_content=newsletter-prod-blog-learnmore/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/uDwKNnEkBgrfhONk4FHjMEGxC5i1ugWeho4cv1oh8vU=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentry团队分享了他们构建AI代码审查系统的技术细节，包括架构设计、上下文处理、质量保证和评估方法


<details>
  <summary>Details</summary>
Motivation: 构建一个能够利用生产数据预测bug的上下文感知AI代理，改进代码审查流程

Method: 基于代理式AI架构，结合生产数据作为上下文，构建上下文感知的代码审查代理

Result: 开发了一个能够预测bug的AI代码审查系统，通过技术博客分享了架构设计和实现经验

Conclusion: 通过结合生产数据和上下文感知技术，可以构建有效的AI代码审查系统来预测和预防bug

Abstract: How Sentry built its AI Code Review: architecture, context, quality, and evals (Sponsor) Tinkering with agentic AI? Check out this technical deep dive by the Sentry engineering team to learn how they built a context-aware agent that uses prod data to predict bugs. Read the blog

</details>


### [40] [React Grab](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Faidenybai%2Freact-grab%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/iVSNeA9nEK2J1XB5mI9qNwG_1Qt1As03uIZY9UqMcHs=438)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: React Grab 是一个工具，允许开发者直接从 React 应用中复制 UI 元素的上下文信息（文件名、React 组件、HTML 源码），以提高 AI 编程代理的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: AI 编程代理在理解和修改代码时缺乏精确的 UI 元素上下文信息，导致效率低下和准确性不足。开发者需要一种方法来快速获取 UI 元素的具体代码信息，以便为 AI 工具提供更准确的上下文。

Method: 开发了一个工具（React Grab），允许用户通过悬停在 React 应用的 UI 元素上并按下热键（⌘C 或 Ctrl+C）来复制该元素的文件名、React 组件和 HTML 源代码。这种直接选择 UI 元素上下文的方法为 AI 工具提供了精确的代码信息。

Result: React Grab 能够显著提高 AI 编程代理的准确性和速度，使 AI 工具能够更快速、更准确地理解和修改代码，因为现在它们可以获得 UI 元素的精确上下文信息。

Conclusion: 通过提供直接从 React 应用中提取 UI 元素上下文的能力，React Grab 解决了 AI 编程代理在代码理解和修改中的上下文缺失问题，从而提高了开发效率和 AI 工具的准确性。

Abstract: React Grab (GitHub Repo) React Grab improves the accuracy and speed of AI coding agents by allowing devs to directly select UI element context from their React apps. Users can hover over any UI element and press a hotkey (⌘C or Ctrl+C) to copy its file name, React component, and HTML source code. This capability provides precise context to AI tools, making them much faster and more accurate in understanding and modifying code.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [41] [Advanced Vulnerability Scanning for Open Source Software: Detection and Mitigation of Log4j Vulnerabilities](https://arxiv.org/abs/2601.00235)
*Victor Wen,Zedong Peng*

Main category: cs.SE

TL;DR: 开发了一个先进的Log4j扫描工具，通过评估软件的实际可利用性来减少误报，集成到GitHub Actions中实现自动化持续扫描


<details>
  <summary>Details</summary>
Motivation: Log4Shell漏洞披露后，虽然已有许多扫描工具，但现有工具主要关注识别Log4j版本，导致大量误报，因为它们不检查软件是否真正易受攻击。需要开发能评估实际可利用性的工具来减少误报。

Method: 首先识别漏洞，然后提供针对性的缓解建议和即时反馈。通过GitHub Actions集成，实现自动化持续扫描能力，确保在代码变更时及时识别漏洞。

Result: 评估了28个开源软件项目的不同版本，从140次扫描中获得了91.4%的准确率。GitHub Action实现已在GitHub市场上提供。

Conclusion: 该工具为检测和缓解开源项目中的漏洞提供了可靠方法，通过集成到现有开发工作流程中实现实时监控和快速响应潜在威胁。

Abstract: Automated detection of software vulnerabilities remains a critical challenge in software security. Log4j is an industrial-grade Java logging framework listed as one of the top 100 critical open source projects. On Dec. 10, 2021 a severe vulnerability Log4Shell was disclosed before being fully patched with Log4j2 version 2.17.0 on Dec. 18, 2021. However, to this day about 4.1 million, or 33 percent of all Log4j downloads in the last 7 days contain vulnerable packages. Many Log4Shell scanners have since been created to detect if a user's installed Log4j version is vulnerable. Current detection tools primarily focus on identifying the version of Log4j installed, leading to numerous false positives, as they do not check if the software scanned is really vulnerable to malicious actors. This research aims to develop an advanced Log4j scanning tool that can evaluate the real-world exploitability of the software, thereby reducing false positives. Our approach first identifies vulnerabilities and then provides targeted recommendations for mitigating these detected vulnerabilities, along with instant feedback to users. By leveraging GitHub Actions, our tool offers automated and continuous scanning capabilities, ensuring timely identification of vulnerabilities as code changes occur. This integration into existing development workflows enables real-time monitoring and quicker responses to potential threats. We demonstrate the effectiveness of our approach by evaluating 28 open-source software projects across different releases, achieving an accuracy rate of 91.4% from a sample of 140 scans. Our GitHub action implementation is available at the GitHub marketplace and can be accessed by anyone interested in improving their software security and for future studies. This tool provides a dependable way to detect and mitigate vulnerabilities in open-source projects.

</details>


### [42] [An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems](https://arxiv.org/abs/2601.00254)
*Md Hasan Saju,Maher Muhtadi,Akramul Azim*

Main category: cs.SE

TL;DR: 该研究比较了三种基于大语言模型的软件漏洞检测方法（RAG、SFT和双代理框架），发现结合外部领域知识的RAG方法在准确率和F1分数上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的快速发展为自动化软件漏洞检测提供了新机会，这是保护现代代码库的关键任务。研究旨在评估不同LLM技术在漏洞检测中的有效性。

Method: 研究比较了三种方法：1）检索增强生成（RAG），整合互联网和MITRE CWE数据库的外部领域知识；2）监督微调（SFT），使用参数高效的QLoRA适配器；3）双代理LLM框架，其中第二个代理审核和优化第一个代理的输出。使用从Big-Vul和GitHub真实代码库收集的数据集，专注于五个关键CWE类别。

Result: RAG方法取得了最高的整体准确率（0.86）和F1分数（0.85）。SFT方法也表现出色。双代理系统在提高推理透明度和错误缓解方面显示出潜力，同时减少了资源开销。

Conclusion: 整合领域专业知识机制显著增强了LLM在实际漏洞检测任务中的适用性。RAG方法通过上下文增强表现最佳，双代理框架在透明度和资源效率方面有优势。

Abstract: The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.

</details>


### [43] [In Line with Context: Repository-Level Code Generation via Context Inlining](https://arxiv.org/abs/2601.00376)
*Chao Hu,Wenhao Zeng,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: InlineCoder通过将未完成函数内联到其调用图中，将复杂的仓库级代码生成任务转化为更简单的函数级编码任务，利用锚点生成和双向内联过程增强对仓库上下文的理解。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码生成方法（如RAG或基于上下文的函数选择）主要依赖表面相似性，难以捕捉复杂的跨函数、类和模块的依赖关系，导致在理解整个仓库语义方面存在不足。

Method: InlineCoder首先根据函数签名生成一个近似下游依赖的草稿完成（锚点），然后进行双向内联：上游内联将锚点嵌入调用者以捕获多样化使用场景；下游检索将锚点的被调用者集成到提示中提供精确依赖上下文。

Result: 该方法通过结合草稿完成与上下游视角的丰富上下文，为LLM提供了全面的仓库视图，从而提升了仓库级代码生成的质量。

Conclusion: InlineCoder通过将仓库理解重新构建为函数级编码任务，有效解决了现有方法在捕捉复杂依赖关系方面的局限性，为仓库级代码生成提供了新思路。

Abstract: Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.

</details>


### [44] [Multi-Agent Coordinated Rename Refactoring](https://arxiv.org/abs/2601.00482)
*Abhiram Bellur,Mohammed Raihan Ullah,Fraol Batole,Mohit Kansara,Masaharu Morimoto,Kai Ishikawa,Haifeng Chen,Yaroslav Zharov,Timofey Bryksin,Tien N. Nguyen,Hridesh Rajan,Danny Dig*

Main category: cs.SE

TL;DR: 提出了首个多智能体框架来自动化协调重命名，通过范围推断、计划执行和复制三个智能体协作，将开发者的初始重构作为线索来推断相关重构范围，显著减少开发者的负担。


<details>
  <summary>Details</summary>
Motivation: 协调重命名是软件开发中频繁但具有挑战性的任务，需要跨多个文件和上下文手动传播重构，既繁琐又容易出错。现有启发式方法产生过多误报，而普通LLM由于上下文有限且无法与重构工具交互，提供不完整的建议。

Method: 设计了首个多智能体框架：1) 范围推断智能体将开发者初始重构线索转化为明确的自然语言声明范围；2) 计划执行智能体使用该范围作为严格计划，识别需要重构的程序元素并安全调用IDE的重构API；3) 复制智能体指导项目范围的搜索。

Result: 在100个开源项目的609K次提交中进行了形成性研究，并调查了205名开发者，展示了智能体如何与开发者协同工作，显著减少协调重命名任务的负担。

Conclusion: AI智能体在软件开发中的主要价值在于扩展开发者的推理和行动能力，而不是取代人类参与。协调重命名正是智能体可以显著减轻开发者负担的重复性任务，同时让开发者保持主导地位。

Abstract: The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.
  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...

</details>


### [45] [STELLAR: A Search-Based Testing Framework for Large Language Model Applications](https://arxiv.org/abs/2601.00497)
*Lev Sorokin,Ivan Vasilev,Ken E. Friedl,Andrea Stocco*

Main category: cs.SE

TL;DR: STELLAR是一个基于搜索的自动化测试框架，用于发现LLM应用中的不当响应，通过进化优化探索特征组合，比现有方法多发现2.5-4.3倍故障。


<details>
  <summary>Details</summary>
Motivation: LLM应用在客服、教育、出行等领域广泛部署，但容易产生不准确、虚构或有害的响应，且其高维输入空间使得系统测试特别困难。

Method: 将测试生成建模为优化问题，将输入空间离散化为风格、内容和扰动特征，采用进化优化动态探索更可能暴露故障的特征组合。

Result: 在三个LLM对话问答系统上评估：安全基准测试和车载导航推荐系统。STELLAR比现有基线方法多暴露2.5-4.3倍故障。

Conclusion: STELLAR框架能有效发现LLM应用中的不当响应，为LLM系统测试提供了一种系统化的方法。

Abstract: Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.

</details>


### [46] [Early-Stage Prediction of Review Effort in AI-Generated Pull Requests](https://arxiv.org/abs/2601.00753)
*Dao Sy Duy Minh,Huynh Trung Kiet,Tran Chi Nguyen,Nguyen Lam Phu Quy,Phu Hoa Pham,Nguyen Dinh Ha Duong,Truong Bao Tran*

Main category: cs.SE

TL;DR: 论文分析33,707个AI代理生成的PR，发现其呈现两极分化行为模式：28.3%为即时合并，其余则陷入冗长评审循环。作者提出Circuit Breaker模型，仅用静态结构特征即可在PR创建时预测高评审成本的PR，AUC达0.957。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理从代码补全工具转变为能大规模提交PR的完整队友，软件维护者面临新挑战：不仅要评审代码，还要管理与非人类贡献者的复杂交互循环。需要预测哪些AI生成的PR会在评审阶段消耗过多人力成本。

Method: 分析AIDev数据集中33,707个AI代理生成的PR（来自2,807个仓库），发现两极行为模式。提出Circuit Breaker分流模型，使用LightGBM算法，仅基于静态结构特征（而非语义文本特征）预测高评审成本PR。

Result: 发现AI代理PR呈现两极模式：28.3%在1分钟内合并（窄自动化任务成功），其余常陷入评审循环且代理常停滞或放弃修改。Circuit Breaker模型在时间分割上AUC达0.957，在20%评审预算下可拦截69%的总评审工作量。

Conclusion: AI辅助代码评审的现有假设受到挑战：评审负担由代理修改的内容结构决定，而非其语义内容。需要为人类-AI协作建立结构化的治理机制，而非依赖传统语义分析。

Abstract: As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?
  Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).
  We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.
  Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.

</details>
