<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [wechat.article](#wechat.article) [Total: 11]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.LG](#cs.LG) [Total: 20]
- [tldr.article](#tldr.article) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Emergent Convergence in Multi-Agent LLM Annotation](https://arxiv.org/abs/2512.00047)
*Angelina Parfenova,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 该研究通过模拟7500次多智能体、多轮讨论，分析LLM在协作任务中的协调行为，发现即使没有明确角色提示，LLM群体也能在词汇和语义上收敛，并表现出类似谈判的行为模式。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型越来越多地部署在协作环境中，但作为黑盒智能体时它们如何协调仍知之甚少。研究者希望了解LLM在多智能体讨论中的协调动态和涌现行为。

Method: 在归纳编码任务中模拟7500次多智能体、多轮讨论，生成超过125000条话语记录。引入过程级指标（代码稳定性、语义自一致性、词汇置信度等），并分析输出嵌入的几何演变，特别是内在维度的变化。

Result: LLM群体在词汇和语义上收敛，形成不对称的影响模式，表现出类似谈判的行为。输出嵌入的内在维度随轮次下降，表明语义压缩现象。黑盒交互分析能够揭示涌现的协调策略。

Conclusion: 黑盒交互分析可以作为基于内部探针的可解释性方法的可扩展补充，能够揭示LLM在协作任务中涌现的协调策略，即使没有明确角色提示也能表现出复杂的协调行为。

Abstract: Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.

</details>


### [2] [Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis](https://arxiv.org/abs/2512.00214)
*Matej Klemen,Tjaša Arčon,Luka Terčon,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 本文提出一个基于大语言模型的智能代理框架，用于自动化分析标注语料库中的语法特征，并在多语言词序任务上验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 当前基于标注语料库的实证语法研究需要大量方法学和技术努力，研究者希望利用智能大语言模型来简化这一过程，实现可解释、数据驱动的语法分析自动化。

Method: 提出一个智能代理框架，整合自然语言任务解释、代码生成和数据驱动推理，应用于Universal Dependencies语料库，测试13个词序特征和170多种语言。

Result: 系统在三个维度上评估：主导顺序准确性、顺序覆盖完整性和分布保真度，结果表明LLM推理与结构化语言数据结合是可行的。

Conclusion: 该方法为实现可解释、可扩展的基于语料库的语法查询自动化迈出了第一步，展示了智能代理在语言学分析中的潜力。

Abstract: Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.

</details>


### [3] [CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency](https://arxiv.org/abs/2512.00417)
*Jiacheng Guo,Suozhi Huang,Zixin Yao,Yifan Zhang,Yifu Lu,Jiashuo Liu,Zihao Li,Yanyan Deng,Qixin Xiao,Jia Tian,Kanghong Zhan,Tianyi Li,Xiaochen Liu,Jason Ge,Chaoyang He,Kaixuan Huang,Lin Yang,Wenhao Huang,Mengdi Wang*

Main category: cs.CL

TL;DR: CryptoBench是首个专家策划的动态基准测试，专门评估LLM代理在加密货币领域的实际能力，包含实时动态问题和四象限任务分类，揭示了检索-预测不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有通用代理基准无法充分评估LLM在加密货币领域的真实能力，该领域具有极端时效性、对抗性信息环境和需要综合多种专业数据源的特殊挑战。

Method: 构建了动态基准测试，每月包含50个由加密领域专家设计的问题，采用四象限分类系统（简单检索、复杂检索、简单预测、复杂预测），评估了10个LLM模型在直接和代理框架下的表现。

Result: 发现了检索-预测不平衡问题：许多领先模型在数据检索方面表现良好，但在需要预测分析的任务上表现出明显弱点，表明代理虽然看似事实基础扎实，但缺乏深度分析能力。

Conclusion: CryptoBench为LLM代理评估提供了更具挑战性和价值的场景，揭示了当前LLM在加密货币分析中的局限性，特别是预测分析能力的不足。

Abstract: This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \emph{extreme time-sensitivity}, \emph{a highly adversarial information environment}, and the critical need to synthesize data from \emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.
  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.

</details>


### [4] [Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior](https://arxiv.org/abs/2512.00611)
*Franck Binard,Vanja Kljajevic*

Main category: cs.CL

TL;DR: Prism是一个用于规范工具使用软件代理行为的小型组合元语言，基于固定的核心上下文Core1构建，通过领域扩展机制支持可重用语法核心与领域特定词汇的分离。


<details>
  <summary>Details</summary>
Motivation: 当前工具使用软件代理的规范通常采用临时控制结构，缺乏统一的组合框架。Prism旨在提供一个紧凑的接口语言，使代理控制空间明确化，便于分析、验证和安全约束。

Method: 基于固定的核心上下文Core1构建，包含数字、字符串、用户提示、工具等基本类别，以及布尔值、谓词、对、列表的组合子。代理策略通过单一抽象运算符编写，条件语句表现为替代选择而非命令式if-else块。领域通过定义自己的上下文微语法扩展核心。

Result: 通过恒温器控制、家庭安全、电子商务推荐和医疗监控等实例展示了如何将自然语言决策规则映射为可检查、可执行的策略。实现了语法核心与领域词汇的清晰分离。

Conclusion: Prism从语言学角度强制分离可重用语法核心与领域特定词汇，将工具视为内部策略表示与外部世界之间的桥梁；从工程角度提供了紧凑的代理控制接口语言。

Abstract: Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.

</details>


### [5] [ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization](https://arxiv.org/abs/2512.00617)
*Omer Jauhar Khan*

Main category: cs.CL

TL;DR: ART框架通过锦标赛式ELO排名和多智能体推理，优化LLM输出质量，提升响应准确性和一致性


<details>
  <summary>Details</summary>
Motivation: 单个LLM模型响应存在不一致性、幻觉和不同查询领域质量波动问题，需要系统化优化方法

Method: 采用锦标赛式ELO排名和多智能体推理框架，让多个LLM智能体通过竞争、批判和协作产生共识响应

Result: 相比基线单模型方法，在响应准确性、连贯性和可靠性方面显著提升，整体质量指标提高8.4%，ELO评级收敛R²值超过0.96

Conclusion: ART框架为需要高质量、经过验证的LLM响应的应用提供了可扩展的生产就绪解决方案

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.

</details>


### [6] [Sycophancy Claims about Language Models: The Missing Human-in-the-Loop](https://arxiv.org/abs/2512.00656)
*Jan Batzner,Volker Stocker,Stefan Schmid,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 该论文回顾了大型语言模型中谄媚响应模式的研究方法挑战，识别了五种核心操作化定义，指出当前研究缺乏对人类感知的评估，并提出了未来研究的建议。


<details>
  <summary>Details</summary>
Motivation: 随着文献中越来越多地声称大型语言模型存在谄媚响应模式，需要系统性地回顾测量LLM谄媚性的方法论挑战，并指出当前研究的局限性。

Method: 论文采用文献回顾和分析方法，识别了五种核心操作化定义来测量LLM谄媚性，分析了当前研究方法的局限性，特别是缺乏对人类感知的评估。

Result: 研究发现当前LLM谄媚性研究存在方法论挑战，难以区分谄媚性响应与AI对齐中的相关概念，且由于谄媚性本质上是人类中心的概念，现有研究未能评估人类感知。

Conclusion: 论文强调了区分LLM谄媚性响应与其他AI对齐概念的困难，提出了未来研究的可操作建议，特别是需要纳入人类感知评估。

Abstract: Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.

</details>


### [7] [Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios](https://arxiv.org/abs/2512.00920)
*Jianxiang Zang,Yongda Wei,Ruxue Bai,Shiyu Jiang,Nijia Mo,Binhong Li,Qiang Sun,Hui Liu*

Main category: cs.CL

TL;DR: Reward Auditor：一个基于假设检验的框架，用于评估奖励模型在真实世界扰动场景下的条件可靠性（适合性），而非仅关注偏好感知准确率。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型评估方法仅关注特定场景下的偏好感知准确率，忽视了真实世界场景中的关键脆弱性。需要评估奖励模型在真实世界扰动下的条件可靠性（适合性）。

Method: 提出Reward Auditor框架，通过假设检验方法评估奖励模型的适合性。在真实世界扰动场景下，通过审计奖励模型偏好感知置信度的分布退化来量化统计显著性和效应大小。

Result: 该框架能够推断奖励模型在不同真实世界场景中脆弱性的确定性和严重程度，为构建可验证安全、更鲁棒和可信赖的下一代LLM对齐系统奠定基础。

Conclusion: Reward Auditor为评估奖励模型在真实世界扰动下的条件可靠性提供了系统框架，有助于构建更安全、鲁棒的LLM对齐系统。

Abstract: Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering "How accurate is the RM's preference perception for given samples?", it employs scientific auditing to answer: "Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.

</details>


### [8] [Advancing Academic Chatbots: Evaluation of Non Traditional Outputs](https://arxiv.org/abs/2512.00991)
*Nicole Favero,Francesca Salute,Daniel Hardt*

Main category: cs.CL

TL;DR: 该研究比较了两种RAG检索策略（Graph RAG和Advanced RAG）在问答任务中的表现，并评估了LLM生成非传统学术输出（幻灯片和播客脚本）的能力。GPT 4o mini配合Advanced RAG表现最佳，而Graph RAG因结构复杂导致更多幻觉。人类评估在检测布局和风格缺陷方面至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估主要关注事实问答或简短摘要等标准任务，本研究希望扩展评估范围：一是比较两种RAG检索策略在问答任务中的效果，二是评估LLM生成非传统学术输出（幻灯片和播客脚本）的能力。

Method: 使用Meta的LLaMA 3 70B开源模型和OpenAI的GPT 4o mini API构建原型系统。在问答任务中比较Graph RAG（基于知识图谱）和Advanced RAG（混合关键词-语义搜索）两种检索策略。通过人类评估（11个质量维度）和LLM评估进行交叉验证。在幻灯片和播客生成任务中，使用基于文档的检索方法。

Result: GPT 4o mini配合Advanced RAG在问答任务中产生最准确的回答。Graph RAG改进有限且导致更多幻觉，部分原因是其结构复杂性和手动设置。在幻灯片和播客生成任务中，GPT 4o mini再次表现最佳，而LLaMA 3在叙事连贯性方面有潜力。人类评估者在检测布局和风格缺陷方面发挥关键作用。

Conclusion: 对于新兴学术输出的评估需要结合人类和LLM评估。Advanced RAG在问答任务中表现优于Graph RAG，GPT 4o mini在生成任务中表现最佳。Graph RAG的结构复杂性限制了其实际应用效果。

Abstract: Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.

</details>


### [9] [DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](https://arxiv.org/abs/2512.01174)
*Hyunjun Kim,Sooyoung Ryu*

Main category: cs.CL

TL;DR: DrawingBench是一个用于评估智能体LLM可信度的验证框架，通过空间推理任务（生成低级GUI操作序列）进行透明、可审计的评估，包含250个多样化提示、确定性评估指标和外部监督机制。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统日益自主运行，通过可验证评估建立信任变得至关重要。现有基准缺乏评估智能体行为可靠性的透明度和可审计性。

Method: 提出了DrawingBench验证框架：包含250个多样化提示（20个类别，4个难度级别），8个客观评估标准，确定性评估指标，以及通过多轮反馈实现的外部监督机制，允许人类控制智能体优化。

Result: 评估4个SOTA LLM（Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash）的1000次测试显示：模型在有结构化外部反馈时达到92.8%完美性能（平均提升3.2%，复杂场景最高提升32.8%），但在工具状态管理和长时规划中出现系统性错误模式。规范清晰度比任务复杂度更重要——当给出明确、可验证标准时，模型达到100%完美性能。

Conclusion: 透明评估框架可以建立对智能体系统的信任，外部监督在引导智能体行为方面比自我纠正更可靠。开源框架为可信智能体评估提供了模板。

Abstract: As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench

</details>


### [10] [Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning](https://arxiv.org/abs/2512.01282)
*Jiahao Yuan,Zhiqing Cui,Hanqing Wang,Yuansheng Gao,Yucheng Zhou,Usman Naseem*

Main category: cs.CL

TL;DR: KardiaBench是一个大规模用户基础基准测试，包含178,080个问答对，覆盖22,080个多轮对话，基于671个真实世界用户档案。Kardia-R1框架使用可解释的Rubric-ERL方法，通过人类对齐的评分标准奖励来训练模型进行逐步移情认知。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统存在两个主要局限：1) 依赖缺乏持久用户身份的情境中心数据集，难以捕捉个性化情感细微差别；2) 依赖不透明、粗糙的奖励信号，阻碍可验证移情推理的发展。需要超越表面移情，实现身份感知的情感推理。

Method: 1) 构建KardiaBench基准测试，采用模型在环的流水线，通过迭代评分标准指导的细化确保心理合理性和角色一致性；2) 提出Kardia-R1框架，使用Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL)，这是一种基于GRPO的方法，利用可解释、人类对齐的评分标准奖励，紧密耦合用户理解、情感推理和支持性回应生成。

Result: 在四个LLM骨干网络上的广泛实验表明，Kardia-R1在情感准确性、移情性、相关性、角色一致性和安全性方面持续优于其他方法。

Conclusion: KardiaBench基准测试和Kardia-R1框架为解决对话代理的身份感知情感推理问题提供了有效方案，通过可解释的评分标准奖励实现了逐步移情认知，显著提升了移情对话的质量。

Abstract: As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.

</details>


### [11] [PromptBridge: Cross-Model Prompt Transfer for Large Language Models](https://arxiv.org/abs/2512.01420)
*Yaxuan Wang,Quan Liu,Zhenting Wang,Zichao Li,Wei Wei,Yang Liu,Yujia Bao*

Main category: cs.CL

TL;DR: PromptBridge：无需训练的框架，通过跨模型提示映射解决模型漂移问题，实现提示在不同LLM间的有效迁移


<details>
  <summary>Details</summary>
Motivation: 实际应用中，由于能力、成本、部署约束和隐私等因素，需要频繁切换不同的LLM模型。但针对特定模型优化的提示在其他模型上表现显著下降，这种"模型漂移"现象严重影响了提示的可移植性和系统维护成本。

Method: 提出PromptBridge框架：1) 使用模型自适应反射提示演化(MAP-RPE)通过迭代反射优化和定量评估获得任务和模型特定的最优提示；2) 利用源模型和目标模型的校准提示对学习跨模型提示映射；3) 测试时，给定源模型提示，直接生成目标模型的优化提示。

Result: 实验表明，PromptBridge在单智能体和多智能体设置中都能持续提高下游任务准确性，同时显著减少迁移工作量。该方法仅需少量对齐任务进行校准，无需昂贵的每任务或每模型重新优化。

Conclusion: PromptBridge有效解决了模型漂移问题，实现了提示在不同LLM间的有效迁移，降低了模型切换成本，为实际部署中的模型切换提供了实用解决方案。

Abstract: Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.

</details>


### [12] [Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems](https://arxiv.org/abs/2512.01661)
*Dengyun Peng,Qiguang Chen,Bofei Liu,Jiannan Guan,Libo Qin,Zheng Yan,Jinhao Liu,Jianshu Zhang,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文提出UnsolvableQA数据集和UnsolvableRL框架，用于训练LLM区分可解与不可解问题，防止过度自信和幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前LLM难以区分客观不可解问题（内在矛盾）和主观能力限制（超出模型能力），导致幻觉和过度自信，需要改进模型的问题识别能力。

Method: 1) 构建UnsolvableQA数据集：通过程序化生成逻辑谜题和"反向构造"方法在数学推理链中注入矛盾；2) 提出UnsolvableRL强化学习框架：包含准确性、不可解性和难度三个奖励组件。

Result: 方法实现了近乎完美的不可解性检测，同时提高了可解任务的准确性。发现了"能力崩溃"现象，证明明确接触不可解数据对防止系统过度自信至关重要。

Conclusion: 通过UnsolvableQA数据集和UnsolvableRL框架，可以有效训练LLM识别不可解问题，减少幻觉和过度自信，提升模型可靠性。

Abstract: Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel "Reverse Construction" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.

</details>


### [13] [MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications](https://arxiv.org/abs/2512.01710)
*Stefano Zeppieri*

Main category: cs.CL

TL;DR: 本文提出了混合记忆增强生成（MMAG）模式，为LLM智能体构建五层记忆系统，以解决长对话中相关性、个性化和连续性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在单次提示中能生成连贯文本，但在长对话中难以维持相关性、个性化和连续性。人类沟通依赖多种记忆形式，需要构建类似人类记忆系统的框架来提升LLM智能体的交互质量。

Method: 提出混合记忆增强生成（MMAG）模式，将记忆组织为五层：对话记忆、长期用户记忆、情景与事件关联记忆、感知与上下文记忆、短期工作记忆。借鉴认知心理学原理，将这些层次映射到技术组件，并设计协调、优先级和冲突解决策略。

Result: 在Heero对话智能体中实现了该框架，加密的长期用户档案和对话历史已显著提升了用户参与度和留存率。展示了记忆分层系统的实际应用效果。

Conclusion: MMAG为构建记忆丰富的语言智能体提供了基础框架，使其更加连贯、主动且符合人类需求。同时讨论了存储、检索、隐私和延迟等实现问题，并指出了未来挑战。

Abstract: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.

</details>


### [14] [Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks](https://arxiv.org/abs/2512.01725)
*Jiannan Guan,Qiguang Chen,Libo Qin,Dengyun Peng,Jinhao Liu,Liangyu Huo,Jian Xie,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文研究LLMs在多解任务中的推理过度自信问题，提出MuSoBench基准，发现Long-CoT能缓解此问题，并提出认知刚性假说解释原因。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要单一正确答案的推理任务中表现出色，但在需要生成全面多样答案的多解任务中表现不佳。作者认为这是由于"推理过度自信"导致的，即LLMs倾向于对不完整的解决方案集表达过度的确定性。

Method: 1) 引入MuSoBench多解问题基准；2) 实验比较Short-CoT和Long-CoT提示范式；3) 提出认知刚性假说，认为过度自信源于推理过程过早收敛于狭窄的思维路径；4) 进行注意力熵分析验证假说。

Result: 实验显示：1) 传统Short-CoT提示表现出明显的过度自信；2) 新兴的Long-CoT方法通过迭代探索和自我反思缓解了过度自信；3) 注意力熵分析初步支持认知刚性假说。

Conclusion: 研究提供了评估LLM推理完整性的工具，并强调需要将评估从单一答案准确性转向全面探索。认知刚性假说为理解LLMs在多解任务中的局限性提供了理论框架。

Abstract: Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.

</details>


### [15] [InnoGym: Benchmarking the Innovation Potential of AI Agents](https://arxiv.org/abs/2512.01822)
*Jintian Zhang,Kewei Xu,Jingsheng Zheng,Zhuoyun Yu,Yuqi Zhu,Yujie Luo,Lanning Wei,Shuofei Qiao,Lun Du,Da Zheng,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: InnoGym是首个评估AI代理创新潜力的基准框架，包含性能增益和新颖性两个互补指标，覆盖18个真实世界工程科学任务，揭示了创造力与有效性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要衡量正确性，忽略了解决方案背后的方法多样性。真正的创新不仅需要正确答案，还需要方法的原创性，但目前缺乏系统评估AI代理创新潜力的工具。

Method: 提出InnoGym基准框架，包含两个互补指标：性能增益（衡量相对于已知最佳方案的改进）和新颖性（捕捉与先前方法的方法论差异）。基准包含18个精心策划的真实世界工程科学任务，通过资源过滤、评估器验证和解决方案收集进行标准化。同时提供iGym统一执行环境，支持可重复和长视野评估。

Result: 实验表明，虽然一些代理能产生新颖方法，但其缺乏鲁棒性限制了性能增益。这揭示了创造力与有效性之间的关键差距。

Conclusion: 需要同时评估创造力和有效性的基准，InnoGym填补了这一空白，为系统评估AI代理的创新潜力提供了首个框架。

Abstract: LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.

</details>


### [16] [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/abs/2512.01848)
*Jinghan Jia,Nathalie Baracaldo,Sijia Liu*

Main category: cs.CL

TL;DR: 该论文研究大型推理模型的安全对齐问题，发现监督微调方法存在局限性，提出使用强化学习作为补充优化框架，在保持推理能力的同时实现更稳健的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思维链推理显著提升了数学和逻辑问题解决能力，但这也引入了新的安全风险——不安全行为可能出现在中间推理轨迹中，即使最终答案看起来无害。现有的基于监督微调的安全对齐方法存在改进不一致、推理能力下降和泛化能力差等问题。

Method: 研究强化学习作为大型推理模型安全训练的补充优化框架。与监督微调不同，强化学习通过奖励反馈直接优化模型策略，实现更自适应和稳定的对齐。通过多模型系列和基准测试进行广泛实验，分析反思动态和token级熵的变化。

Result: 强化学习实现了更强且更一致的安全提升，同时保持了推理能力。分析显示强化学习抑制了不安全的探索性推理，同时保留了反思深度，从而产生更安全可靠的推理过程。

Conclusion: 纯监督方法不足以实现大型推理模型的稳健安全对齐，强化学习作为补充优化框架能够更有效地平衡安全性和推理能力，为大型推理模型的安全训练提供了新方向。

Abstract: Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.

</details>


### [17] [Latent Debate: A Surrogate Framework for Interpreting LLM Thinking](https://arxiv.org/abs/2512.01909)
*Lihu Chen,Xiang Yin,Francesca Toni*

Main category: cs.CL

TL;DR: 提出latent debate框架，通过捕捉单个模型单次推理中的隐式内部辩论信号来理解LLM预测和幻觉原因


<details>
  <summary>Details</summary>
Motivation: 理解LLM内部思维过程和幻觉原因是一个关键挑战，现有方法如self-consistency和multi-agent debate依赖多个答案或模型的显式辩论，无法捕捉单个模型内部的隐式辩论信号

Method: 提出模型和任务无关的latent debate概念框架，并在True/False预测任务上符号化实例化，通过捕捉单个模型单次推理中的隐式支持/攻击信号来近似LLM思维过程

Result: latent debate是忠实结构化代理模型，与原始LLM预测高度一致；为幻觉检测提供强基线；发现幻觉与辩论模式强相关（如中间层高程度latent debate与高幻觉风险相关）

Conclusion: latent debate是理解LLM内部机制（特别是推理步骤中出现内部（不）一致性的场景）的潜在框架

Abstract: Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.

</details>


### [18] [Rectifying LLM Thought from Lens of Optimization](https://arxiv.org/abs/2512.01925)
*Junnan Liu,Hongwei Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: RePro是一种通过过程级奖励优化LLM推理的新方法，将思维链视为梯度下降过程，利用强化学习改进推理性能


<details>
  <summary>Details</summary>
Motivation: 当前长思维链LLM存在过度思考和推理链过长等次优推理行为，影响性能表现，需要改进推理过程

Method: 将思维链视为梯度下降过程，定义代理目标函数评估优化过程，使用双重评分机制量化推理强度和稳定性，整合为过程级奖励，融入RLVR管道优化LLM

Result: 在数学、科学和编码等多个基准测试中，RePro能持续提升推理性能并减轻次优推理行为

Conclusion: RePro通过过程级奖励优化LLM推理过程，有效解决了长思维链中的次优推理问题

Abstract: Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [19] [基于“恐惧-好奇”引导的安全高效端到端自主导航<em class="highlight">强化学习</em>方法](http://mp.weixin.qq.com/s?__biz=MzU4OTU1MjcyMw==&mid=2247523119&idx=1&sn=3b7056525b0175b42d14493f08454dca&chksm=fc8355a071774c7a1054eec0ffc5ebc2f51a4115bf4a01a0e0c3b2e0c6fb02ade2aa7061547c#rd)
*同济智能汽车研究所*

Main category: wechat.article

TL;DR: “Feariosity”模型用于约束并优化强化学习；导航场景的感知信息由语义分割图像提供Ⅱ 相关工作A. 面向车辆的安全强化学习安全检测器通过被动阻止高风险动作来提升安全性，例如在 RL 输出后增加外部模块，这虽然提高了可解


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “Feariosity”模型用于约束并优化强化学习；导航场景的感知信息由语义分割图像提供Ⅱ 相关工作A. 面向车辆的安全强化学习安全检测器通过被动阻止高风险动作来提升安全性，例如在 RL 输出后增加外部模块，这虽然提高了可解

</details>


### [20] [从训练DeepSeek到掌控机器人，都离不开的“神秘核心"——<em class="highlight">强化学习</em>！你的“人生算法"可能也是它](http://mp.weixin.qq.com/s?__biz=MzU1MjkwNjg5OQ==&mid=2247486967&idx=1&sn=f0a7561729120b2ff30f22b5d7182d2d&chksm=fa1e08a6b23193c3ac62a827dc0e9445ad68e9e7567a88e319ed9e1d8b2e691b7f8350ac9505#rd)
*智算催化*

Main category: wechat.article

TL;DR: 点击卡片 关注我们 1. 为什么要学强化学习？在机器学习这门“让机器学会做决定”的学科里，最常被提起的是三位“兄弟”：监督学习（Supervised Learning）、无监督学习（Unsupervised Learning）和强化学习（Reinforcement Learning， RL）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 点击卡片 关注我们 1. 为什么要学强化学习？在机器学习这门“让机器学会做决定”的学科里，最常被提起的是三位“兄弟”：监督学习（Supervised Learning）、无监督学习（Unsupervised Learning）和强化学习（Reinforcement Learning， RL）

</details>


### [21] [Seed Research｜GR-RL 发布：突破VLA精细操作瓶颈，首次实现真机<em class="highlight">强化学习</em>穿鞋带](http://mp.weixin.qq.com/s?__biz=MzkzMDY5MzYxNg==&mid=2247492836&idx=1&sn=fc7a463bf2a409fe03edceff606a4876&chksm=c3efc5561ada55186dcb74aa1996c84871e899c2e86b69355430acad35e353de74c7aba44544#rd)
*字节跳动Seed*

Main category: wechat.article

TL;DR: 在线阶段：导向强化学习进行精细训练 团队发现，模型必须把系统级优化都当成强化学习环境的一部分，通过闭环在线交互进行探索与自我优化，这一点对于精细操作至关重要。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在线阶段：导向强化学习进行精细训练 团队发现，模型必须把系统级优化都当成强化学习环境的一部分，通过闭环在线交互进行探索与自我优化，这一点对于精细操作至关重要。

</details>


### [22] [INTELLECT-3：开源大规模<em class="highlight">强化学习</em>训练的前沿实践](http://mp.weixin.qq.com/s?__biz=Mzk0MTU5OTg1OQ==&mid=2247500211&idx=2&sn=6578bad77a39a8ac6608603379ba0180&chksm=c32587a2de5ab42626fafd76220d361687cb18bed1b080a822453cbd69b6f7004a438f9db264#rd)
*AI帝国*

Main category: wechat.article

TL;DR: （2） 强化学习：使用 256 个 prompt 的 batch size，每个 prompt 16 个 rollout，最大上下文长度 65536。训练环境混合包括：数学（21.2K 问题）、代码（8.6K 问题）、科学（29.3K 问题）、逻辑（11.6K 问题）、Deep Research（2.2K 问题）和软件工


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: （2） 强化学习：使用 256 个 prompt 的 batch size，每个 prompt 16 个 rollout，最大上下文长度 65536。训练环境混合包括：数学（21.2K 问题）、代码（8.6K 问题）、科学（29.3K 问题）、逻辑（11.6K 问题）、Deep Research（2.2K 问题）和软件工

</details>


### [23] [NeurIPS 2025 | <em class="highlight">强化学习</em>-相关论文12篇](http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505160&idx=1&sn=781f0b54e4a83ef6f12a33d70b09359f&chksm=9aabbc0e2d043430b65e8137851d5a93cfba88c39e993a55336fbb1dc6e44ff47436bcb3afd6#rd)
*关于NLP那些你不知道的事*

Main category: wechat.article

TL;DR: 【要点】论文提出了最大扩散强化学习方法，克服了传统强化学习中数据独立同分布的假设，实现了单次部署下的连续学习，并在多个基准测试中表现出稳健的优势。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 【要点】论文提出了最大扩散强化学习方法，克服了传统强化学习中数据独立同分布的假设，实现了单次部署下的连续学习，并在多个基准测试中表现出稳健的优势。

</details>


### [24] [国泰海通｜计算机：豆包<em class="highlight">大模型</em>嵌入手机系统，推动端侧 AI 从应用级迈向系统级](http://mp.weixin.qq.com/s?__biz=MzA5MzEwMDEzNQ==&mid=2650534932&idx=4&sn=49bdaaa46fe951d18c0cf467c0eead02&chksm=8912b9177af25cf246a3d7a28f67ceb28b5c73aa417d64fad72e858f5996d2d534337cc18ce3#rd)
*国泰海通证券研究*

Main category: wechat.article

TL;DR: 豆包大模型能力全面，模型在推理、视觉理解、图像创作、视频生成、语音等方面的性能达到国际一流水平，图形界面操作能力更是在多项权威评测中获得业界最佳成绩，有了系统级的操作权限，通过识别手机屏幕上的 UI 元素


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 豆包大模型能力全面，模型在推理、视觉理解、图像创作、视频生成、语音等方面的性能达到国际一流水平，图形界面操作能力更是在多项权威评测中获得业界最佳成绩，有了系统级的操作权限，通过识别手机屏幕上的 UI 元素

</details>


### [25] [中国中铁“先锋”系列三大垂域<em class="highlight">大模型</em>集中上线 构建数智赋能全产业链新生态](http://mp.weixin.qq.com/s?__biz=MzI0MTAzODAzNw==&mid=2651840057&idx=1&sn=a4e1bc0e142cbd45b2395f961daf8d0d&chksm=f3110c8dcfc55247fce6e9e6d705f4ca0ba4acbdba4b6938bd5da39f56c12f5e0f7f6226fa29#rd)
*中国中铁*

Main category: wechat.article

TL;DR: “先锋桥梁大模型”：国内首个特殊及大跨度桥梁领域垂直大模型11月29日，在第二届桥梁智能与绿色建造学术会议上，由中铁大桥局牵头，联合中铁大桥院、中铁工业、中铁信科等单位共同开发的国内首个特殊及大跨度桥梁领


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “先锋桥梁大模型”：国内首个特殊及大跨度桥梁领域垂直大模型11月29日，在第二届桥梁智能与绿色建造学术会议上，由中铁大桥局牵头，联合中铁大桥院、中铁工业、中铁信科等单位共同开发的国内首个特殊及大跨度桥梁领

</details>


### [26] [<em class="highlight">大模型</em>在数据分析领域的应用——怎么让模型生成更准确的SQL语句实操](http://mp.weixin.qq.com/s?__biz=MzAwNzYzMzQwMg==&mid=2651694360&idx=1&sn=c9f94aa58192d7904e693b76cbb1ac57&chksm=81ee9433c39d03e823260e056ccaf9992baaa8d4ac8dbd6ec3c6fba643c928e612fb1d471f66#rd)
*章鱼大数据*

Main category: wechat.article

TL;DR: 怎么让大模型更好地进行数据分析 怎么让大模型更好地进行分析？我们都知道与大模型的交互都是通过提示词实现的，因此这个问题可以换个问法，怎么让大模型更好的编写数据分析代码（包括SQL，shell，python等脚本代码）？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 怎么让大模型更好地进行数据分析 怎么让大模型更好地进行分析？我们都知道与大模型的交互都是通过提示词实现的，因此这个问题可以换个问法，怎么让大模型更好的编写数据分析代码（包括SQL，shell，python等脚本代码）？

</details>


### [27] [DeepSeek-V3.2：开源<em class="highlight">大模型</em>如何摘得IMO金牌，挑战GPT-5与Gemini-3.0？](http://mp.weixin.qq.com/s?__biz=MzYzNjI4NjMzNw==&mid=2247483712&idx=1&sn=ab060e593c815aaba209172a2b27e5cb&chksm=f116147cb6183fe6bd71078f00f813abebeeca75a81e8b9f013fdc08267ea44d0bbe2d5ee8ce#rd)
*大模型视界*

Main category: wechat.article

TL;DR: 多样化的 RL 任务对于增强模型鲁棒性至关重要。DeepSeek-AI 使用真实世界工具，包括实际的网络搜索 API、编码工具和 Jupyter Notebook，来处理搜索、代码工程和代码解释等任务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 多样化的 RL 任务对于增强模型鲁棒性至关重要。DeepSeek-AI 使用真实世界工具，包括实际的网络搜索 API、编码工具和 Jupyter Notebook，来处理搜索、代码工程和代码解释等任务。

</details>


### [28] [豆包<em class="highlight">大模型</em>家族全线焕新！](http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247772652&idx=1&sn=98c7723f1f8d4ecd595b87e45d4531a4&chksm=fab853d5a36948aeaf981c21ab0e7a0a5c923f36f34bc8de975f6158f4e6c1243b9c255431b8#rd)
*DataFunTalk*

Main category: wechat.article

TL;DR: 豆包大模型家族全线焕新不是小修小补，是全家族Agent从概念到落地的完整路径Agent开发工具全面升级生态扩容，更多合作伙伴加入构建AI落地全链路能力


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 豆包大模型家族全线焕新不是小修小补，是全家族Agent从概念到落地的完整路径Agent开发工具全面升级生态扩容，更多合作伙伴加入构建AI落地全链路能力

</details>


### [29] [<em class="highlight">大模型</em>在数据分析领域的应用——怎么让模型生成更准确的SQL语句实操](http://mp.weixin.qq.com/s?__biz=Mzg2NzAzMDMwOA==&mid=2247488067&idx=1&sn=771935680c5619fbc7489eabcb12e5a4&chksm=cfbdae34281c27f37fb0a573454e8c16da8845867a60a0c247c11b0d469b650bd6800bc25fc5#rd)
*AI探索时代*

Main category: wechat.article

TL;DR: 怎么让大模型更好地进行数据分析 怎么让大模型更好地进行分析？我们都知道与大模型的交互都是通过提示词实现的，因此这个问题可以换个问法，怎么让大模型更好的编写数据分析代码（包括SQL，shell，python等脚本代码）？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 怎么让大模型更好地进行数据分析 怎么让大模型更好地进行分析？我们都知道与大模型的交互都是通过提示词实现的，因此这个问题可以换个问法，怎么让大模型更好的编写数据分析代码（包括SQL，shell，python等脚本代码）？

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?](https://arxiv.org/abs/2512.00218)
*Matt MacDermott,Qiyao Wei,Rada Djoneva,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究不同训练激励对AI推理模型可监控性的影响，发现对抗性优化会降低监控性能，而直接优化可监控性并不能可靠提升监控效果。


<details>
  <summary>Details</summary>
Motivation: AI系统输出自然语言推理过程为安全监控提供了机会，但推理过程是否忠实反映底层思维过程可能受训练方式影响。研究不同训练激励如何影响推理模型的可监控性。

Method: 引入新的可监控性测量方法：根据监控器能否使用模型的推理预测关键潜在变量。研究不同训练激励（长度惩罚、KL正则化、对抗性优化、直接优化可监控性）对监控性能的影响。

Result: 在控制准确性的情况下，常见激励（长度惩罚和KL正则化）没有一致影响；对抗性优化会降低监控性能；直接优化可监控性并不能可靠提升监控效果。

Conclusion: 训练激励确实影响推理模型的可监控性，对抗性优化会损害监控能力，而直接优化可监控性并非有效策略。这为设计可监控的AI系统提供了重要见解。

Abstract: AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.

</details>


### [31] [RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs](https://arxiv.org/abs/2512.00319)
*Ruike Hu,Shulei Wu*

Main category: cs.AI

TL;DR: 提出轻量级强化学习框架，通过多维奖励函数解决LLM生成结构化数据时的"结构鸿沟"问题，显著提升JSON等格式的生成准确率


<details>
  <summary>Details</summary>
Motivation: 解决LLM在自动化软件生态系统中面临的"结构鸿沟"问题——概率性token生成与确定性结构化数据格式（如JSON、XML）之间的内在矛盾。传统监督微调难以强制执行严格语法约束，而约束解码方法又会带来显著的推理延迟。

Method: 提出轻量高效的强化学习框架，引入多维奖励函数将结构化输出任务分解为层次化约束：结构完整性、格式正确性、内容准确性和有效性。采用梯度正则化策略优化（GRPO），使模型能够内化这些约束而无需单独的批评网络。

Result: 在复杂配方生成和结构化数学推理（GSM8K-JSON）等任务上验证，方法达到89.7%的结构准确率和92.1%的JSON有效性，显著优于零样本基线（如GPT-3.5）和在LLaMA-3-8B等更大模型上的监督微调。相比PPO减少40%的峰值VRAM使用。

Conclusion: 提出的强化学习框架有效解决了LLM生成结构化数据时的结构鸿沟问题，通过多维奖励函数和GRPO优化实现了高效的结构约束内化，为LLM在自动化软件生态系统中的应用提供了实用解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the "Structure Gap" - the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to "hallucinated" keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.

</details>


### [32] [CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System](https://arxiv.org/abs/2512.00331)
*Yefeng Wu,Yuchen Song,Yecheng Zhao,Ling Wu,Shan Wan*

Main category: cs.AI

TL;DR: CogEvo-Edu：一个分层教育多智能体系统，将检索、记忆和控制视为耦合的认知演化过程，用于STEM教育中的LLM对话导师，在数字信号处理领域显著提升教学效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于单一LLM和静态RAG管道的教育系统在复杂领域（如数字信号处理）存在局限，无法有效维护长期学生模型、管理异构知识库和适应教学策略。需要将检索、记忆和控制视为耦合的认知演化过程。

Method: 提出CogEvo-Edu分层多智能体系统：1) 认知感知层(CPL)维护双重记忆并进行置信度加权整合，构建结构化、自校正的学生档案；2) 知识演化层(KEL)为每个知识块分配时空价值，驱动激活、语义压缩和遗忘；3) 元控制层(MCL)将教学制定为分层顺序决策，通过双重内外循环协调专业智能体并联合调整CPL/KEL超参数。

Result: 在构建的DSP-EduBench垂直基准测试中，使用三模型LLM-as-a-Judge集成评估，CogEvo-Edu将总体得分从5.32提升到9.23，并在所有六个指标上优于静态RAG、简单记忆和单智能体变体。

Conclusion: CogEvo-Edu通过联合演化学生档案、知识库和教学策略，显著提升了复杂STEM领域的教学效果，验证了将检索、记忆和控制视为耦合认知演化过程的价值。

Abstract: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.

</details>


### [33] [Echo-N1: Affective RL Frontier](https://arxiv.org/abs/2512.00344)
*Naifan Zhang,Ruihan Sun,Ruixi Su,Shiqi Ma,Shiya Zhang,Xianna Weng,Xiaofan Zhang,Yuhan Zhan,Yuyang Xu,Zhaohan Chen,Zhengyuan Pan,Ziyi Song*

Main category: cs.AI

TL;DR: 提出首个基于强化学习的个性化对话框架Echo-N1，通过实时推断用户性格并优化对话行为，在主观情感对话领域取得显著突破


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究过度关注数学、代码等机器擅长的确定性任务，而忽视了真正体现人类智能的主观、情感化、个性化的对话领域。该领域因难以形式化而被认为不适合传统RL方法

Method: 提出首个框架，能够实时推断用户性格并优化模型行为以适应个性化对话偏好。引入动态情商评估套件来量化改进效果

Result: Echo-N1模型表现远超其基础版本，并优于专有的Doubao 1.5 Character模型。在非可验证环境中RL方法仍能产生一致、稳健且显著的改进

Conclusion: 这项工作为RL开辟了新前沿：优化模型以适应对话中深刻主观、深刻人性化的维度，证明主观情感对话是可解决的RL问题

Abstract: The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.

</details>


### [34] [Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization](https://arxiv.org/abs/2512.00601)
*Boyang Gu,Hongjian Zhou,Bradley Max Segal,Jinge Wu,Zeyu Cao,Hantao Zhong,Lei Clifton,Fenglin Liu,David A. Clifton*

Main category: cs.AI

TL;DR: CRPO是一种针对临床推理的多目标可验证强化学习方法，用于对齐LLM后训练与临床原则，相比传统GRPO在真实性和完整性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法（如GRPO）主要奖励正确性，但临床等高风险领域需要多维目标（准确性、忠实性、全面性），现有方法未能对齐临床推理原则。

Method: 提出临床目标相对策略优化（CRPO），一种可扩展的多目标可验证强化学习方法，整合基于规则和可验证的奖励信号，联合优化准确性、忠实性和全面性，无需人工标注。

Result: 训练了3B参数的Clinical-R1-3B模型，在三个基准测试中，CRPO相比标准GRPO在推理真实性和完整性上有显著提升，同时保持准确性增强。

Conclusion: CRPO为对齐LLM推理与临床目标提供了可扩展路径，使医疗AI系统更安全、更具协作性，展示了多目标可验证RL方法在医学领域LLM后训练扩展中的潜力。

Abstract: Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.

</details>


### [35] [When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF](https://arxiv.org/abs/2512.00709)
*Yifan Xu,Xichen Ye,Yifan Chen,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种翻转感知直接偏好优化算法，通过建模实例依赖的偏好翻转概率来增强大语言模型对齐的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在收集人类反馈进行LLM对齐时，偏好翻转现象普遍存在，导致数据标注被污染。现有对齐算法需要提高对潜在翻转对的鲁棒性。

Method: 将人类意图模型和外部因素引起的偏好翻转机制分为两个阶段，基于Bradley-Terry模型引入实例依赖的翻转概率，利用偏好标注相关特征捕捉判断不确定性，设计兼容原始RLHF和DPO的迭代优化算法。

Result: 在多种情况下评估了实例依赖偏好翻转模型和提出的方法，与其他基线方法进行了比较。

Conclusion: 提出的FA-DPO算法能有效处理偏好翻转问题，提高对齐算法的鲁棒性。

Abstract: Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.

</details>


### [36] [SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks](https://arxiv.org/abs/2512.00834)
*Lin Zhu,Kezhi Wang,Luping Xiang,Kun Yang*

Main category: cs.AI

TL;DR: 提出了一种结合语义通信与Agentic AI的车辆轨迹预测框架，通过V2I和V2V通信中的智能体协作，在低信噪比条件下实现高达47.5%的预测精度提升。


<details>
  <summary>Details</summary>
Motivation: 传统V2X通信方案存在传输开销大、延迟高的问题，现有轨迹预测模型缺乏环境感知和逻辑推理能力，需要更高效的通信与智能预测框架。

Method: 在V2I通信中，RSU使用特征提取智能体从历史轨迹中提取紧凑表示，语义分析智能体进行语义推理，通过语义通信将特征和语义洞察传输给车辆。在V2V通信中，车辆进行本地特征提取和语义分析，同时接收邻居车辆的预测轨迹，综合这些信息进行预测。

Result: 在各种通信条件下的广泛实验表明，该方法显著优于基线方案，在低信噪比条件下预测精度提升高达47.5%。

Conclusion: 提出的语义通信与Agentic AI集成框架有效提升了车辆轨迹预测性能，为V2X网络提供了高效的信息交换和可靠的上下文推理能力。

Abstract: Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.

</details>


### [37] [ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI](https://arxiv.org/abs/2512.00839)
*Fabrizio Maturo,Donato Riccio,Andrea Mazzitelli,Giuseppe Bifulco,Francesco Paolone,Iulia Brezeanu*

Main category: cs.AI

TL;DR: ARCADIA是一个用于因果发现的智能AI框架，结合大语言模型推理与统计诊断，通过迭代优化构建有效、时间一致的因果结构。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现算法在真实高风险领域中存在局限性，需要更稳定、可解释且能处理复杂现实数据的因果建模方法。

Method: 整合大语言模型推理与统计诊断，通过约束引导提示和因果有效性反馈迭代优化候选有向无环图(DAGs)。

Result: 在企业破产数据实验中，ARCADIA比NOTEARS、GOLEM和DirectLiNGAM产生更可靠的因果图，提供完全可解释、支持干预的流程。

Conclusion: ARCADIA展示了智能大语言模型如何参与自主科学建模和结构化因果推断，推动了AI在因果发现领域的发展。

Abstract: This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.

</details>


### [38] [Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics](https://arxiv.org/abs/2512.01020)
*Jinu Lee,Kyoung-Woon On,Simeng Han,Arman Cohan,Julia Hockenmaier*

Main category: cs.AI

TL;DR: LEGIT是一个大规模法律推理数据集，通过将法庭判决转换为层次化的争议树来评估LLM生成的法律推理轨迹的质量，发现LLM的法律推理能力受问题覆盖率和正确性影响，RAG和RL方法能互补提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 在专家领域（如法律）中评估LLM生成的推理轨迹质量对于确保可信度和可解释性至关重要，但由于此类推理任务的固有复杂性，评估仍然具有挑战性。

Method: 引入LEGIT数据集（24K实例），将法庭判决转换为对立双方论点和法庭结论的层次化树结构，作为评估推理轨迹问题覆盖率和正确性的评分标准。通过专家标注验证评分标准的可靠性。

Result: 1) LLM的法律推理能力严重受法律问题覆盖率和正确性影响；2) 检索增强生成(RAG)和基于评分标准的强化学习(RL)对法律推理能力带来互补效益：RAG提高整体推理能力，RL提高正确性但会降低覆盖率。

Conclusion: LEGIT数据集为评估专家领域推理轨迹提供了有效框架，揭示了LLM法律推理的局限性，并展示了RAG和RL方法的互补优势，为改进法律AI系统提供了方向。

Abstract: Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.

</details>


### [39] [Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids](https://arxiv.org/abs/2512.01046)
*Hadi Nekoei,Alexandre Blondin Massé,Rachid Hassani,Sarath Chandar,Vincent Mai*

Main category: cs.AI

TL;DR: 提出Shielded Controller Units (SCUs)方法，利用系统动力学先验知识确保约束满足，在远程微电网优化任务中实现燃料消耗减少24%且不增加电池退化


<details>
  <summary>Details</summary>
Motivation: 强化学习在能源转型等复杂系统决策优化中面临约束满足挑战，特别是远程微电网需要协调控制可再生能源、燃料发电机和电池，在满足需求的同时最小化燃料消耗和电池退化，且必须遵守严格的操作约束和法规

Method: 提出Shielded Controller Units (SCUs)方法，将环境分解为层次结构，每个SCU显式管理约束子集，利用系统动力学先验知识确保约束满足，为实际部署设计

Result: 在远程微电网优化任务中，配备SCUs的RL智能体实现燃料消耗减少24%，且不增加电池退化，优于其他基线方法，同时满足所有约束

Conclusion: SCUs为强化学习在能源转型相关决策挑战中的安全应用提供了系统化和可解释的约束保证方法

Abstract: Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.

</details>


### [40] [SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds](https://arxiv.org/abs/2512.01078)
*Jiawei Ren,Yan Zhuang,Xiaokang Ye,Lingjun Mao,Xuhong He,Jianzhi Shen,Mrinaal Dogra,Yiming Liang,Ruixuan Zhang,Tianai Yue,Yiqing Yang,Eric Liu,Ryan Wu,Kevin Benavente,Rajiv Mandya Nagaraju,Muhammad Faayez,Xiyan Zhang,Dhruv Vivek Sharma,Xianrui Zhong,Ziqiao Ma,Tianmin Shu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: SimWorld是一个基于Unreal Engine 5构建的新型模拟器，专门用于在丰富、逼真的环境中开发和评估LLM/VLM智能体，解决现有模拟器在物理和社会环境模拟方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM/VLM智能体在数学、编程等领域进展迅速，但在复杂物理和社会环境中的应用仍然困难。现有世界模拟器存在环境有限、物理简化、缺乏对LLM/VLM智能体原生支持等问题，无法满足开发能够在现实世界中生存和发展的智能体的需求。

Method: 基于Unreal Engine 5构建SimWorld模拟器，提供三个核心能力：1）逼真、开放的世界模拟，包括准确的物理和社会动态以及语言驱动的程序化环境生成；2）为LLM/VLM智能体提供丰富的接口，支持多模态世界输入和不同抽象级别的开放词汇动作；3）多样且可扩展的物理和社会推理场景，用户可轻松定制。

Result: 通过在涉及战略合作与竞争的长期多智能体配送任务中部署前沿LLM智能体（如GPT-4o、Gemini-2.5-Flash等），展示了SimWorld的能力，揭示了不同模型之间的推理模式和局限性差异。

Conclusion: SimWorld作为一个开源平台，有望成为跨学科推进现实世界智能体智能的基础平台，为开发能够在复杂物理和社会环境中运作的智能体提供重要工具。

Abstract: While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.

</details>


### [41] [CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents](https://arxiv.org/abs/2512.01089)
*Peter Jansen,Samiah Hassan,Pragnya Narasimha*

Main category: cs.AI

TL;DR: CodeDistiller系统自动从大量科学GitHub仓库中提取经过验证的领域特定代码示例库，增强自动化科学发现系统的能力，在材料科学领域验证有效


<details>
  <summary>Details</summary>
Motivation: 当前自动化科学发现系统受限于仅从参数化知识生成可靠代码的能力，要么只能变异少量手动编写的实验示例，要么仅依赖参数化知识，限制了质量和覆盖范围

Method: 开发CodeDistiller系统，自动从大量科学GitHub仓库中提取经过验证的领域特定代码示例库，结合自动化和领域专家评估，在250个材料科学仓库上进行验证

Result: 最佳模型能为74%的仓库生成功能示例；使用CodeDistiller生成的库增强的ASD代理比仅使用通用材料科学代码示例的代理产生更准确、完整和科学合理的实验

Conclusion: CodeDistiller能够自动扩展自动化科学发现系统的能力而无需人工干预，显著提高实验生成的质量和科学性

Abstract: Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.

</details>


### [42] [CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL](https://arxiv.org/abs/2512.01311)
*Shinji Mai,Yunpeng Zhai,Ziqian Chen,Cheng Chen,Anni Zou,Shuchang Tao,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: CuES框架通过好奇心驱动和环境基础的任务生成，解决智能体强化学习中任务稀缺问题，无需预定义任务即可自主生成多样化可执行任务。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体在复杂工具增强环境中部署时面临任务稀缺问题，即缺乏结构化训练任务。现有方法依赖预定义任务集，这在工具语义和功能未知的新环境中失效，成为扩展智能体强化学习的关键瓶颈。

Method: 提出CuES框架：好奇心驱动和环境基础的任务合成方法。通过内在好奇心驱动探索，从环境结构和功能中抽象交互模式为可重用任务模式，通过轻量级自上而下指导和基于记忆的质量控制进行精炼，无需手工种子或外部语料。

Result: 在AppWorld、BFCL和WebShop三个代表性环境中，CuES生成的任务分布在多样性和可执行性方面匹配或超越手动策划的数据集，带来显著的下游策略改进。

Conclusion: 好奇心驱动、环境基础的任务生成为智能体提供了可扩展的基础，使智能体不仅能学习如何行动，还能学习学习什么，解决了智能体强化学习中的任务稀缺问题。

Abstract: Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.

</details>


### [43] [A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building](https://arxiv.org/abs/2512.01434)
*Daull Xavier,Patrice Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco*

Main category: cs.AI

TL;DR: CollabToolBuilder是一个多智能体LLM框架，通过专家在环指导迭代学习为目标创建工具，最小化任务适应和人工反馈收集时间。


<details>
  <summary>Details</summary>
Motivation: 解决复杂迭代问题（如科学文档生成）中工具创建与人类意图对齐的挑战，减少任务/领域适应的人工努力和反馈收集时间。

Method: 采用四智能体架构（教练、编码员、批评者、资本化者），通过强化动态提示和系统性人工反馈集成来生成和验证工具，结合多智能体上下文学习、专家在环控制和可重用工具资本化。

Result: 通过初步实验展示了系统在生成前沿研究论文或专利等任务上的能力，证明了其在复杂迭代问题解决中的适用性。

Conclusion: CollabToolBuilder是一个系统级集成方法，结合了多智能体学习、人类控制和工具重用，为复杂迭代问题提供有效解决方案。

Abstract: We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.

</details>


### [44] [CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning](https://arxiv.org/abs/2512.01616)
*Chainesh Gautam,Raghuram Bharadwaj Diddigi*

Main category: cs.AI

TL;DR: 提出一种基于CLIP原理的强化学习方法，通过对齐语言指令和策略嵌入的表征空间，实现跨任务的高效迁移


<details>
  <summary>Details</summary>
Motivation: 当前需要开发能够在同一环境中解决多个任务的智能体，特别是当这些任务与语言自然关联时。需要建立高效的跨任务迁移管道。

Method: 受计算机视觉中CLIP方法的启发，将"表示相同概念的两个模态应有相似表征"的思想扩展到强化学习。通过预训练的语言-策略对组合，创建自然语言和策略嵌入的统一表征空间。

Result: 实验结果表明该方法在实现跨任务快速迁移方面具有实用性

Conclusion: 通过将CLIP原理扩展到强化学习领域，建立了语言指令和策略的统一表征空间，为多任务环境中的智能体开发提供了有效的跨任务迁移方法

Abstract: Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.

</details>


### [45] [Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems](https://arxiv.org/abs/2512.01786)
*Xiaochuan Li,Ke Wang,Girija Gouda,Shubham Choudhary,Yaqun Wang,Linwei Hu,Joel Vaughan,Freddy Lecue*

Main category: cs.AI

TL;DR: 提出LLM Jury-on-Demand框架，通过训练可靠性预测器动态选择最可靠的LLM评委组成陪审团，实现可扩展且上下文感知的评估系统。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在高风险领域应用增多，需要既可扩展又可靠的评估方法。人工评估可靠但成本高，单一LLM评委有偏见，静态陪审团缺乏适应性。

Method: 训练一组可靠性预测器来评估LLM评委何时会与人类专家意见一致，利用token分布、嵌入和结构化输入特征。为每个数据点动态选择最可靠的评委组成最优陪审团，并以可靠性为权重聚合评分。

Result: 在摘要和RAG基准测试中，动态陪审团系统比单一评委和静态陪审团基线显著提高了与人类判断的相关性。

Conclusion: 自适应、基于学习的陪审团方法为构建可扩展、更可靠和可信的LLM评估系统提供了前景，特别适用于高风险领域。

Abstract: As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.

</details>


### [46] [From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning](https://arxiv.org/abs/2512.01970)
*Sitao Cheng,Xunjian Yin,Ruiwen Zhou,Yuxuan Li,Xinyi Wang,Liangming Pan,William Yang Wang,Victor Zhong*

Main category: cs.AI

TL;DR: RL能合成新的推理策略而非仅放大现有能力，但前提是基础模型需先通过监督学习掌握原子技能（参数推理和上下文推理）。


<details>
  <summary>Details</summary>
Motivation: 研究RL如何提升推理能力——是合成新技能还是仅放大现有行为，通过互补推理任务来探究这一核心问题。

Method: 使用合成的人类传记数据集，将互补推理严格解耦为两个原子技能：参数推理（依赖内部知识）和上下文推理（依赖外部信息）。在三种难度级别（I.I.D.、组合、零样本）评估泛化能力，对比SFT和RL的效果。

Result: 发现SFT存在泛化悖论：在分布内表现完美但在分布外泛化崩溃，表明其依赖路径捷径的记忆。RL能作为推理合成器而非概率放大器，但前提是基础模型必须先通过SFT掌握独立的原子技能。

Conclusion: RL不是简单的放大器，在具备足够原子基础的情况下，RL能主动从学习到的原语中合成复杂推理策略，无需对此类复杂策略进行显式监督。解耦的原子训练后接RL为复杂推理任务提供了可扩展的泛化路径。

Abstract: The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

</details>


### [47] [LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess](https://arxiv.org/abs/2512.01992)
*Sai Kolasani,Maxim Saplin,Nicholas Crispino,Kyle Montgomery,Jared Quincy Davis,Matei Zaharia,Chi Wang,Chenguang Wang*

Main category: cs.AI

TL;DR: LLM CHESS是一个评估框架，通过国际象棋领域的扩展智能体交互来测试大语言模型的推理和指令遵循泛化能力，对50多个模型进行排名，发现即使面对简单任务，许多先进模型也难以完成游戏。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准测试容易过拟合和记忆，需要一种随机动态的评估方法来更真实地测试LLMs的推理和指令遵循泛化能力，特别是在扩展智能体交互场景中。

Method: 开发LLM CHESS评估框架，让LLMs与随机对手进行国际象棋对弈，使用胜率、移动质量、合法性、幻觉动作和游戏时长等行为指标评估50多个模型，对顶级推理模型使用可变技能配置的象棋引擎进行Elo评分。

Result: 许多最先进的模型难以完成游戏或取得稳定胜利，推理模型与非推理模型之间存在明显差距，LLM CHESS的随机动态特性有效减少了过拟合和记忆，防止基准饱和，即使顶级推理模型也面临挑战。

Conclusion: LLM CHESS提供了一个独特且有效的评估框架，能够更真实地测试LLMs的推理和指令遵循能力，通过减少过拟合和记忆问题，为未来研究提供了有价值的工具和数据集。

Abstract: We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [48] [Generating Verifiable CoT from Execution-Traces](https://arxiv.org/abs/2512.00127)
*Shailja Thakur,Vaibhav Saxena,Rohan Kulkarni,Shivdeep Singh,Parameswaran Selvam,Hima Patel,Hiroshi Kanayama*

Main category: cs.SE

TL;DR: 提出一种基于程序执行轨迹的代码推理训练方法，通过捕获代码动态行为并转化为自然语言推理步骤，确保推理过程正确性，显著提升模型在代码相关任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在代码推理训练中存在关键缺陷：合成训练数据中的推理步骤通常是教师模型生成的看似合理但未经验证的解释，而非代码实际执行过程的真实描述，导致模型学习到表面合理但逻辑错误的推理模式。

Method: 通过代码插桩捕获程序动态执行轨迹，然后将这些经过验证的执行轨迹叙述为自然语言推理步骤，构建双向轨迹基础数据，确保每个推理步骤都反映程序真实计算过程。

Result: 在代码推理任务（CruxEval和LiveCodeBench-Exec的前向推理、CruxEval-Input的后向推理）以及HumanEval的代码生成和解释任务上，使用该方法训练的模型取得显著改进：输出预测提升高达30分，输入预测提升28分，同时改善了代码解释和生成能力。

Conclusion: 基于可验证执行轨迹的推理训练方法能够从根本上增强模型能力，通过消除逻辑幻觉源，确保推理步骤的正确性，为代码推理任务提供了更可靠的基础。

Abstract: Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.
  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT

</details>


### [49] [Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation](https://arxiv.org/abs/2512.00215)
*Mohammad Abdollahi,Khandaker Rifah Tasnia,Soumit Kanti Saha,Jinqiu Yang,Song Wang,Hadi Hemmati*

Main category: cs.SE

TL;DR: 该研究首次对推理大语言模型的运行时行为进行实证分析，揭示了其推理轨迹中的错误模式，并通过工具增强方法显著提升了推理准确性。


<details>
  <summary>Details</summary>
Motivation: 理解程序的运行时推理行为对于可靠的代码生成、调试和自动推理至关重要。虽然大语言模型能准确预测程序输出，但先前研究主要关注输出准确性，将推理过程视为黑箱，对其推理轨迹的结构和失败模式知之甚少。

Method: 从HumanEval Plus和LiveCodeBench中收集427个代码片段，为每个片段测试三种输入类型（常规、边界、无效），每个片段选择12个输入值并配对真实执行结果。评估四个最先进的推理大语言模型，分析生成的推理轨迹，开发包含九类推理错误的分类法，并探索工具增强推理方法。

Result: 模型在不同输入类型上的准确率在85%到98%之间。研究识别出九类推理错误，并以计算错误类别为例，通过工具增强推理方法成功纠正了58%的此类错误。

Conclusion: 该研究首次系统分析了推理大语言模型的运行时行为，揭示了其推理轨迹中的错误模式，并证明了工具支持在改进大语言模型推理方面的潜力，为未来更可靠的代码生成和推理系统提供了重要洞见。

Abstract: Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning.

</details>


### [50] [CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization](https://arxiv.org/abs/2512.00231)
*Monique Louise Monteiro,George G. Cabral,Adriano L. I. OLiveira*

Main category: cs.SE

TL;DR: CodeFlowLM是一个用于即时软件缺陷预测的增量学习框架，利用预训练语言模型解决概念漂移、类别不平衡和验证延迟问题，相比传统方法有显著性能提升。同时探索了LLM在缺陷定位中的应用，发现GPT-5表现稳定但仍有局限性。


<details>
  <summary>Details</summary>
Motivation: 传统在线学习器在即时软件缺陷预测中存在概念漂移、类别不平衡和验证延迟等问题，需要更适应软件环境变化的增量学习方法。同时，探索大型语言模型在缺陷定位中的潜力与局限性。

Method: 提出CodeFlowLM框架，采用持续微调策略，避免从头训练。评估了编码器-解码器PLM（如CodeT5+、UniXCoder）在项目内和跨项目JIT-SDP场景中的表现。同时将LLM（GPT-5、Claude、Gemini）与注意力模型在缺陷定位任务上进行对比分析。

Result: CodeFlowLM在G-Mean指标上获得高达68%的提升，证明其在演化软件环境中的优越适应性和鲁棒性。在缺陷定位中，GPT-5在Recall@20%和Effort@20%指标上表现稳定，但注意力模型在细粒度排名指标（Top-k, IFA）上仍有优势。错误分析揭示了假阳性的主要来源。

Conclusion: CodeFlowLM显著推进了增量JIT-SDP的技术水平，展示了在演化软件环境中的优越性能。LLM在缺陷定位中展现出潜力但仍有局限性，需要改进提示设计和上下文信息。

Abstract: This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning.

</details>


### [51] [Progressive Code Integration for Abstractive Bug Report Summarization](https://arxiv.org/abs/2512.00325)
*Shaira Sadia Karim,Abrar Mahmud Rahim,Lamia Alam,Ishmam Tashdeed,Lutfun Nahar Lota,Md. Abu Raihan M. Kamal,Md. Azam Hossain*

Main category: cs.SE

TL;DR: 提出一种渐进式代码集成框架，用于基于LLM的抽象式bug报告摘要生成，通过逐步整合长代码片段和文本内容，克服LLM上下文窗口限制，生成语义丰富的摘要。


<details>
  <summary>Details</summary>
Motivation: 现有bug报告通常非结构化且冗长，开发者难以高效理解软件问题。现有摘要方法主要依赖表层文本线索，导致摘要不完整或冗余，且经常忽略相关代码片段，而代码片段对于准确缺陷诊断至关重要。

Method: 提出渐进式代码集成框架，逐步将长代码片段与文本内容结合，克服标准LLM上下文窗口限制，生成语义丰富的bug报告摘要。

Result: 在四个基准数据集上使用八个LLM进行评估，该管道比抽取式基线方法性能提升7.5%-58.2%，达到与最先进抽象方法相当的性能。

Conclusion: 通过联合利用文本和代码信息，显著提升了bug理解效果，证明了渐进式代码集成框架在bug报告摘要中的有效性。

Abstract: Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.

</details>


### [52] [Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS](https://arxiv.org/abs/2512.00380)
*Mingwei Liu,Zheng Pei,Yanlin Wang,Zihao Wang,Zikang Li,Enci Lin,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 针对低资源框架（如HarmonyOS），LLM因缺乏相关API知识导致代码生成性能差。作者提出APIKG4SYN框架，利用API知识图谱构建面向API的问题-代码对，无需可执行代码，通过不确定性估计驱动的MCTS整合单API和多API知识，为LLM微调创建多样化数据集。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的软件框架（如HarmonyOS）中，大型语言模型由于预训练阶段缺乏对这些环境的充分接触，代码生成性能较差。虽然LLM通常能保持跨编程语言的正确逻辑结构，但在处理框架特定API或语法时经常出错。这表明预训练虽然赋予LLM通用算法能力，但对低资源框架的独特语法和API使用仍不熟悉，即使像GPT-4o这样的先进商业模型也无法可靠生成正确代码。

Method: 提出APIKG4SYN框架，利用API知识图谱构建面向API的问题-代码对，专门针对低资源框架且无需可执行代码。该方法整合单API和多API知识，其中多API知识通过不确定性估计驱动的蒙特卡洛树搜索推导，从而为LLM微调创建多样化和信息丰富的数据集。

Result: 使用HarmonyOS作为案例研究，构建了首个HarmonyOS代码生成基准。实验结果显示，使用APIKG4SYN微调Qwen模型将pass@1准确率提升至25.00%，而基线GPT模型仅为17.59%。

Conclusion: 面向API的数据能显著提升LLM在低资源软件开发场景中的性能，APIKG4SYN框架有效解决了LLM在低资源框架中代码生成能力不足的问题。

Abstract: In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios.

</details>


### [53] [Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations](https://arxiv.org/abs/2512.00556)
*Sina Salimian,Gias Uddin,Sumon Biswas,Henry Leung*

Main category: cs.SE

TL;DR: 提出基于蜕变测试的统一框架，通过六种蜕变关系将直接偏见诱导输入转换为语义等价但对抗性更强的变体，用于系统评估和针对性缓解LLM中的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛部署引发了对隐藏社会偏见的担忧，现有防护措施在处理间接或上下文复杂的偏见诱导提示时经常失效，需要更系统的方法来评估和缓解这些偏见。

Method: 提出基于蜕变测试的统一框架，引入六种新颖的蜕变关系，将直接偏见诱导输入转换为语义等价但对抗性更强的变体，用于自动检测模型偏见，并将相同蜕变关系生成的样本用于微调以缓解偏见。

Result: 在六个最先进的LLM（开源和专有模型）上测试，使用BiasAsker基准的385个问题（覆盖七个受保护群体），蜕变关系比现有工具多揭示高达14%的隐藏偏见；使用原始和蜕变变异样本进行微调后，安全响应率从54.7%显著提高到超过88.9%。

Conclusion: 蜕变关系是提高对话AI公平性的实用机制，能够有效检测和缓解LLM中的隐藏社会偏见，将测试过程与缓解措施直接联系起来。

Abstract: The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.

</details>


### [54] [Large Language Models for Software Engineering: A Reproducibility Crisis](https://arxiv.org/abs/2512.00651)
*Mohammed Latif Siddiq,Arvin Islam-Gomes,Natalie Sekerak,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 首次大规模实证研究LLM软件工程研究的可复现性实践，分析640篇论文发现可复现性气味普遍存在，评估徽章不能保证执行保真度，提出可复现性成熟度模型


<details>
  <summary>Details</summary>
Motivation: 可复现性是科学进步的基石，但LLM软件工程研究的可复现性现状仍不清楚，需要系统评估当前实践和问题

Method: 系统挖掘分析2017-2025年640篇论文，从出版物、仓库和文档提取结构化元数据，使用7类气味分类法手动标注，通过4个研究问题指导分析

Result: 发现工件可用性、环境规范、版本控制和文档清晰度存在持续差距，徽章仅表示工件存在但不能保证执行保真度或长期可复现性

Conclusion: 需要超越二元工件认证，采用多维渐进式可复现性评估，提出可复现性成熟度模型和可操作建议来缓解可复现性气味

Abstract: Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor.

</details>


### [55] [Neural Variable Name Repair: Learning to Rename Identifiers for Readability](https://arxiv.org/abs/2512.01141)
*Muhammad Yousuf,Akshat Bagade,Chhittebbayi Penugonda,Maanas Baraya*

Main category: cs.SE

TL;DR: 提出变量名修复任务，通过微调Llama 3.1-8B模型和重排序器，显著提升C++代码中变量名的准确性和描述性


<details>
  <summary>Details</summary>
Motivation: 实际开发中变量名通常过于通用或误导性，且函数缺乏文档，这会降低代码理解效率、增加bug风险，并影响人类和LLM的代码推理能力

Method: 从BigCode的The Stack数据集中自动构建任务，使用Tree-sitter解析C++函数并掩码单个标识符；基于Llama 3.1-8B构建包含预热和dropout调度、LoRA适配器的高效微调管道，以及双编码器重排序器

Result: 在200个C++函数的测试集上，零样本基线达到6.1%精确匹配；最佳LoRA微调模型达到43.1%精确匹配、50.2% Top-5命中率和82.03部分匹配分数；重排序器进一步提升了选择质量

Conclusion: 任务特定微调加重排序是实用标识符修复工具的有效方法，能显著改善变量名的描述性和准确性

Abstract: Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools.

</details>


### [56] [Beyond Greenfield: AI-Driven Productivity in Documentation and Brownfield Engineering](https://arxiv.org/abs/2512.01155)
*Krishna Kumaar Sharma*

Main category: cs.SE

TL;DR: 提出D3框架，结合角色分离提示策略，通过双智能体架构（Builder生成、Reviewer评审）处理遗留系统工程中的模糊性，在52名从业者的探索性调查中显示生产力提升26.9%，认知负荷降低77%。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注绿地项目或合成任务，缺乏针对遗留系统、文档不全、架构知识碎片化的棕地工程的结构化工作流程。需要解决LLM在复杂、上下文密集环境中的有效应用问题。

Method: 提出Discover-Define-Deliver (D3)框架，采用角色分离提示策略和双智能体架构：Builder模型生成候选输出，Reviewer模型提供结构化评审以提高可靠性。框架包含应用最佳实践来处理棕地系统中的模糊性。

Result: 52名软件从业者的探索性调查显示：加权平均生产力提升26.9%，约77%参与者认知负荷降低，83%在Define阶段返工减少。参与者报告任务清晰度、文档质量和认知负荷感知改善。

Conclusion: D3框架展示了结构化LLM工作流程在遗留系统工程中的潜力，但结果基于自我报告，需要未来受控评估验证。研究突出了结构化工作流程的优势和局限性。

Abstract: Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and reduced rework for 83% during the Define phase. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.

</details>


### [57] [LLM-as-a-Judge for Scalable Test Coverage Evaluation: Accuracy, Operational Reliability, and Cost](https://arxiv.org/abs/2512.01232)
*Donghao Huang,Shila Chew,Anna Dutkiewicz,Zhaoxia Wang*

Main category: cs.SE

TL;DR: 提出了LLM-as-a-Judge (LAJ)框架，用于评估Gherkin验收测试，通过20个模型配置在500次评估中的分析显示，小模型可以在成本大幅降低的同时达到更好的准确率。


<details>
  <summary>Details</summary>
Motivation: 大规模评估软件测试覆盖率仍然是QA流程中的瓶颈，需要一种生产就绪的框架来评估Gherkin验收测试。

Method: 提出了LLM-as-a-Judge (LAJ)框架，使用结构化JSON输出的评分驱动方法，在20个模型配置（GPT-4、GPT-5和开源模型）上对100个专家标注的脚本进行了5轮评估（共500次）。

Result: GPT-4o Mini达到最佳准确率（6.07 MAAE）、高可靠性（96.6% ECR@1）和低成本（每千次1.01美元），相比GPT-5（高推理）成本降低78倍且准确率更高。推理努力对模型家族有依赖性，成本范围达175倍（每千次0.45-78.96美元）。

Conclusion: 小模型可以在显著降低成本的同时超越大模型的性能，推理努力的效果因模型家族而异，为生产环境中的测试评估提供了实用的成本-性能权衡指导。

Abstract: Assessing software test coverage at scale remains a bottleneck in QA pipelines. We present LLM-as-a-Judge (LAJ), a production-ready, rubric-driven framework for evaluating Gherkin acceptance tests with structured JSON outputs. Across 20 model configurations (GPT-4, GPT-5 with varying reasoning effort, and open-weight models) on 100 expert-annotated scripts over 5 runs (500 evaluations), we provide the first comprehensive analysis spanning accuracy, operational reliability, and cost. We introduce the Evaluation Completion Rate (ECR@1) to quantify first-attempt success, revealing reliability from 85.4% to 100.0% with material cost implications via retries. Results show that smaller models can outperform larger ones: GPT-4o Mini attains the best accuracy (6.07 MAAE), high reliability (96.6% ECR@1), and low cost ($1.01 per 1K), yielding a 78x cost reduction vs. GPT-5 (high reasoning) while improving accuracy. Reasoning effort is model-family dependent: GPT-5 benefits from increased reasoning (with predictable accuracy-cost tradeoffs), whereas open-weight models degrade across all dimensions as reasoning increases. Overall, cost spans 175x ($0.45-$78.96 per 1K). We release the dataset, framework, and code to support reproducibility and deployment.

</details>


### [58] [LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM](https://arxiv.org/abs/2512.01356)
*Yuxin Zhang,Yuxia Zhang,Zeyu Sun,Yanjie Jiang,Hui Liu*

Main category: cs.SE

TL;DR: 提出LAURA框架，结合评审知识增强和上下文感知，利用LLM生成代码评审意见，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 代码评审对软件质量至关重要，但随着软件规模和复杂度增长，传统评审成为瓶颈。现有自动生成方法主要依赖历史代码变更和评审意见，忽略了代码变更上下文和先验评审知识等重要信息。

Method: 提出LAURA框架，集成评审范例检索、上下文增强和系统指导，增强ChatGPT-4o和DeepSeek v3生成代码评审意见的能力。同时构建了高质量数据集以解决现有数据集中低质量评审的问题。

Result: 实验结果显示，LAURA为两个模型生成的评审意见中，分别有42.2%和40.4%完全正确或对开发者有帮助，显著优于现有最佳基线方法。消融研究表明LAURA的所有组件都对提升评论质量有积极贡献。

Conclusion: LAURA框架通过整合评审知识增强和上下文感知，有效提升了LLM生成代码评审意见的质量，为解决代码评审瓶颈问题提供了有效方案。

Abstract: Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.

</details>


### [59] [BackportBench: A Multilingual Benchmark for Automated Backporting of Patches](https://arxiv.org/abs/2512.01396)
*Zhiqing Zhong,Jiaming Huang,Pinjia He*

Main category: cs.SE

TL;DR: BackportBench是首个全面的补丁回移植基准测试套件，包含202个多语言补丁回移植问题，用于评估自动化回移植技术。研究发现基于代理的方法在需要逻辑和结构变化的案例中优于传统补丁移植方法。


<details>
  <summary>Details</summary>
Motivation: 软件项目快速演进，用户需要更新依赖到更安全的版本，但手动回移植安全补丁耗时且容易出错。现有自动化回移植技术通常只针对代码块或函数级补丁移植场景，且使用不完善的指标评估，其通用软件上的有效性尚不清楚。

Method: 引入BackportBench基准测试套件，包含202个来自PyPI、Maven和npm的补丁回移植问题，每个都有可执行的Docker环境和相关测试用例。使用该基准评估现有补丁移植方法和基于LLM的技术。

Result: 基于代理的方法优于传统补丁移植方法，特别是在需要逻辑和结构变化的案例中。但不同编程语言的性能表现存在差异。

Conclusion: BackportBench为自动化回移植技术的开发和评估提供了首个全面基准。研究结果为未来自动化回移植工作的研究人员和软件从业者提供了重要启示。

Abstract: Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.
  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.

</details>


### [60] [Generating REST API Tests With Descriptive Names](https://arxiv.org/abs/2512.01690)
*Philip Garrett,Juan P. Galeotti,Andrea Arcuri,Alexander Poth,Olsi Rrjolli*

Main category: cs.SE

TL;DR: 提出三种确定性技术为REST API测试生成描述性名称，规则方法表现优于GPT-3.5，与GPT-4o/Gemini相当，工业案例验证可提升测试可读性。


<details>
  <summary>Details</summary>
Motivation: 自动生成的API测试用例通常使用非描述性名称（如test0, test1），降低了可读性，影响理解和维护。需要为REST API测试自动生成描述性名称的方法。

Method: 提出三种确定性技术生成REST API测试名称，共比较八种技术（包括规则启发式和LLM方法）。使用EvoMaster为9个开源API生成测试用例，通过用户调查（39人）和工业案例（Volkswagen AG，4个API，74个测试用例）进行实证评估。

Result: 规则方法在确定性方法中清晰度评分最高，与Gemini和GPT-4o等先进LLM模型表现相当，显著优于GPT-3.5。工业案例证实描述性名称能提高测试套件可读性。

Conclusion: 轻量级确定性技术可作为计算昂贵且安全敏感的LLM方法的有效替代方案，为实现更开发者友好的API测试生成提供了实用步骤。

Abstract: Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.
  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.
  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.

</details>


### [61] [An Empirical Study of Agent Developer Practices in AI Agent Frameworks](https://arxiv.org/abs/2512.01939)
*Yanlin Wang,Xinyi Xu,Jiachi Chen,Tingting Bi,Wenchao Gu,Zibin Zheng*

Main category: cs.SE

TL;DR: 首次对基于LLM的智能体框架进行实证研究，通过分析11,910个开发者讨论，从五个维度比较十个主流框架如何满足开发者需求。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的智能体框架快速增长，开发者面临选择困难，且框架在实际应用中的表现和如何影响开发过程缺乏系统研究。不同框架在使用中遇到相似问题，表明需要改进框架设计。

Method: 收集十个已识别智能体框架的开发者讨论（共11,910条），从开发效率、功能抽象、学习成本、性能优化和可维护性五个维度进行对比分析。

Result: 分析显示不同框架在满足开发者需求方面存在显著差异，揭示了LLM驱动的智能体框架生态系统的现状和问题。

Conclusion: 为LLM驱动的智能体框架生态系统提供了一系列发现和启示，为未来框架设计和开发者选择提供了见解。

Abstract: The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations](https://arxiv.org/abs/2512.00249)
*Scotty Black,Christian Darken*

Main category: cs.LG

TL;DR: 提出一种分层混合AI方法，结合脚本代理的可靠性和强化学习的适应性，用于复杂作战模拟环境中的智能体开发。


<details>
  <summary>Details</summary>
Motivation: 当前作战模拟中的智能体开发主要采用基于规则的脚本方法，虽然可预测但缺乏灵活性；而强化学习方法虽然适应性强但存在黑盒决策和可扩展性问题。需要结合两者优势来解决复杂动态场景中的智能体开发挑战。

Method: 提出分层混合AI架构：在低层使用脚本代理处理常规战术决策，在高层使用强化学习代理进行战略决策，通过这种层次结构结合两种方法的优势。

Result: 该方法显著提高了整体性能，为复杂模拟环境中的智能体开发和训练提供了鲁棒、适应性强且有效的解决方案。

Conclusion: 分层混合AI方法成功结合了脚本代理的可靠性和强化学习的适应性，解决了各自方法的局限性，为作战模拟中的智能体开发提供了更优方案。

Abstract: In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.

</details>


### [63] [Learning Causal States Under Partial Observability and Perturbation](https://arxiv.org/abs/2512.00357)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li,Yamin Wang*

Main category: cs.LG

TL;DR: CaDiff是一个强化学习框架，通过异步扩散模型和双模拟度量来揭示P²OMDPs的因果结构，增强RL算法在扰动和部分可观测环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习在扰动和部分可观测马尔可夫决策过程（P²OMDPs）中面临基于不完整和噪声观测进行决策的挑战，现有方法无法同时处理扰动和部分可观测性问题。

Method: 提出CaDiff框架，包含：1）异步扩散模型（ADM），支持不同步数的前向和反向过程，将P²OMDP扰动解释为可通过扩散抑制的噪声；2）新的双模拟度量，量化部分可观测环境与其因果对应物之间的相似性。

Result: 在Roboschool任务上的实验表明，CaDiff相比基线方法至少提升14.18%的回报率。理论分析推导了扰动观测与去噪因果状态之间价值函数近似误差的上界。

Conclusion: CaDiff是首个结合理论严谨性和实用性的框架，使用扩散模型近似因果状态，有效解决了P²OMDPs中的扰动和部分可观测性问题。

Abstract: A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.

</details>


### [64] [An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines](https://arxiv.org/abs/2512.00383)
*Jianhai Su,Jinzhu Luo,Qi Zhang*

Main category: cs.LG

TL;DR: 提出将离线RL算法作为在线RL子程序的新框架，利用历史交互作为离线数据集，通过多种变体提升在线学习效率


<details>
  <summary>Details</summary>
Motivation: 探索如何将离线强化学习算法整合到在线强化学习过程中，利用智能体历史交互数据作为离线数据集，以提高在线学习效率

Method: 提出一个框架，将离线RL算法作为在线RL的子程序，包括最终策略推荐和在线微调等变体，并引入技术提升其有效性

Result: 实验分析表明：1）框架有效性强烈依赖任务特性；2）提出的技术显著提升效果；3）现有在线微调方法整体效果不佳，需要更多研究

Conclusion: 离线RL作为在线RL子程序是可行的，但需要根据任务特性选择合适方法，现有在线微调方法需要改进，为RL效率提升提供新方向

Abstract: We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.

</details>


### [65] [SelfAI: Building a Self-Training AI System with LLM Agents](https://arxiv.org/abs/2512.00403)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Xiaobing Yu,Yu Zhong,Shangqi Deng,Ufaq Khan,Jianghao Wu,Xiaofeng Liu,Imran Razzak,Xiaojun Chang,Yutong Xie*

Main category: cs.LG

TL;DR: SelfAI是一个通用的多智能体平台，通过用户代理、认知代理和实验管理器协同工作，实现自主科学发现，并引入新指标量化发现效率和搜索多样性。


<details>
  <summary>Details</summary>
Motivation: 现有自主科学发现系统存在领域局限、缺乏实时人机交互、无原则性停止机制等问题，导致效率低下、可重复性差、人类专业知识未充分利用。

Method: 提出SelfAI多智能体平台：用户代理将研究目标转化为标准化实验配置；认知代理基于LLM并采用最优停止准则迭代优化超参数搜索；实验管理器协调异构硬件上的并行容错训练工作流，维护结构化知识库。

Result: 在回归、NLP、计算机视觉、科学计算、医学影像和药物发现等基准测试中，SelfAI表现优异，相比经典贝叶斯优化和LLM基线减少了冗余试验，同时支持与人类研究者的无缝交互。

Conclusion: SelfAI解决了现有自主科学发现系统的局限性，通过多智能体架构和最优停止机制提高了效率和可重复性，并实现了更好的人机协作。

Abstract: Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.

</details>


### [66] [ESPO: Entropy Importance Sampling Policy Optimization](https://arxiv.org/abs/2512.00499)
*Yuepeng Sheng,Yuwei Huang,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.LG

TL;DR: ESPO提出了一种基于熵的重要性采样策略优化框架，通过熵驱动的分组和自适应裁剪机制，解决了LLM强化学习中优化粒度与训练稳定性之间的权衡问题，显著提升了数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的策略优化方法（如GRPO和GSPO）存在优化粒度与训练稳定性之间的根本权衡。GSPO虽然通过序列级优化提高了鲁棒性，但其整体处理序列的方式引入了严重低效性：保守的裁剪机制会不加区分地丢弃有效训练样本（梯度利用不足），统一的信用分配无法捕捉关键推理步骤的异质性贡献。

Method: 提出熵重要性采样策略优化（ESPO）框架，将序列基于预测熵分解为组，实现：（1）熵驱动的重要性采样以捕捉序列内异质性；（2）熵自适应裁剪，基于模型不确定性动态分配信任区域。

Result: 在数学推理基准测试上的广泛实验表明，ESPO不仅加速了收敛，还实现了最先进的性能，特别是在具有挑战性的HMMT基准上将准确率从4.4%显著提升至13.13%。

Conclusion: ESPO成功调和了细粒度控制与训练稳定性之间的矛盾，通过熵驱动的分组机制有效解决了现有方法中的梯度利用不足和信用分配不均问题，为LLM强化学习提供了更高效的优化框架。

Abstract: Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.

</details>


### [67] [DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning](https://arxiv.org/abs/2512.00545)
*Akrati Saxena,Harshith Kumar Yadav,Bart Rutten,Shashi Shekhar Jha*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的公平性影响力最大化方法DQ4FairIM，通过最大化最小公平目标确保不同群体间的公平影响力传播，同时保持较好的公平-性能权衡。


<details>
  <summary>Details</summary>
Motivation: 现实社交网络存在结构性不平等，如多数群体主导和少数群体代表性不足。传统影响力最大化算法不考虑这些不平等会导致结果偏向多数群体，边缘化少数群体，因此需要设计公平感知的影响力最大化方法。

Method: 提出DQ4FairIM方法：1) 将公平影响力最大化问题建模为马尔可夫决策过程；2) 使用深度Q学习和Structure2Vec网络嵌入相结合；3) 采用最大化最小公平目标，优先提升影响力最弱群体的覆盖范围。

Result: 在合成基准和真实网络上的实验表明，该方法比公平无关和公平感知基线方法实现了更高的公平性水平，同时保持了更好的公平-性能权衡。此外，该方法学习到的种子选择策略能够泛化到不同问题实例（如网络规模或种子节点数量变化）而无需重新训练。

Conclusion: DQ4FairIM方法能够有效解决社交网络影响力最大化中的公平性问题，通过强化学习策略确保不同群体间的公平影响力分布，同时保持算法性能。

Abstract: The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.

</details>


### [68] [List Replicable Reinforcement Learning](https://arxiv.org/abs/2512.00553)
*Bohan Zhang,Michael Chen,A. Pavan,N. V. Vinodchandran,Lin F. Yang,Ruosong Wang*

Main category: cs.LG

TL;DR: 该论文研究强化学习中的可复制性问题，提出列表可复制性概念，开发了保证多项式列表复杂度的表格RL算法，并通过理论分析和实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法在实践中存在不稳定性和对训练条件变化的敏感性，这导致了可复制性挑战。论文旨在形式化地解决这一问题，通过引入列表可复制性概念来量化RL算法的稳定性。

Method: 提出列表可复制性的弱形式和强形式定义，开发了基于PAC RL框架的表格RL算法。关键创新包括：(1) 基于词典顺序在近优选择中选取动作的规划策略，(2) 在随机环境中测试状态可达性同时保持可复制性的机制。

Result: 理论证明所提算法能保证列表复杂度在状态数、动作数和时间范围长度上是多项式的，实现了弱形式和强形式的列表可复制性。实验表明该规划策略能增强实际RL框架的稳定性。

Conclusion: 列表可复制性为形式化解决RL不稳定性问题提供了理论框架，所提算法在理论和实践中都表现出良好的可复制性，为解决RL算法的不稳定性问题提供了新思路。

Abstract: Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \emph{small list} of policies across different runs, with high probability. The size of this list defines the \emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.

</details>


### [69] [ML-Tool-Bench: Tool-Augmented Planning for ML Tasks](https://arxiv.org/abs/2512.00672)
*Yaswanth Chittepu,Raghavendra Addanki,Tung Mai,Anup Rao,Branislav Kveton*

Main category: cs.LG

TL;DR: 提出了一个用于评估工具增强型机器学习代理的基准测试，包含61个专业工具和15个Kaggle表格ML挑战，通过内存命名对象管理和两种改进方法显著提升了代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的工具使用基准主要关注特定任务工具选择或工具调用参数提取，未能评估ML代理所需的复杂规划能力。需要更全面的基准来评估工具增强型ML代理在端到端数据科学工作流中的表现。

Method: 1) 引入包含61个专业工具和15个Kaggle表格ML挑战的基准测试；2) 采用内存命名对象管理，允许代理灵活命名、保存和检索中间结果；3) 提出两种改进方法：使用结构化文本反馈的确定性奖励塑造，以及将原始问题分解为子任务序列。

Result: 标准ReAct方法在复杂ML管道中难以生成有效工具序列，基于LLM评估的树搜索方法因状态评分不一致而表现不佳。提出的方法使用GPT-4o时，相比ReAct在Kaggle挑战中位数上提升了16.52个百分位。

Conclusion: 该工作为开发更强大的工具增强规划型ML代理奠定了基础，展示了通过结构化反馈和任务分解可以显著改善代理在复杂数据科学工作流中的表现。

Abstract: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence. These agents must orchestrate complex sequences of data analysis, feature engineering, model selection, and hyperparameter optimization, tasks that require sophisticated planning and iteration. While recent work on building ML agents has explored using large language models (LLMs) for direct code generation, tool-augmented approaches offer greater modularity and reliability. However, existing tool-use benchmarks focus primarily on task-specific tool selection or argument extraction for tool invocation, failing to evaluate the sophisticated planning capabilities required for ML Agents. In this work, we introduce a comprehensive benchmark for evaluating tool-augmented ML agents using a curated set of 61 specialized tools and 15 tabular ML challenges from Kaggle. Our benchmark goes beyond traditional tool-use evaluation by incorporating an in-memory named object management, allowing agents to flexibly name, save, and retrieve intermediate results throughout the workflows. We demonstrate that standard ReAct-style approaches struggle to generate valid tool sequences for complex ML pipelines, and that tree search methods with LLM-based evaluation underperform due to inconsistent state scoring. To address these limitations, we propose two simple approaches: 1) using shaped deterministic rewards with structured textual feedback, and 2) decomposing the original problem into a sequence of sub-tasks, which significantly improves trajectory validity and task performance. Using GPT-4o, our approach improves over ReAct by 16.52 percentile positions, taking the median across all Kaggle challenges. We believe our work provides a foundation for developing more capable tool-augmented planning ML agents.

</details>


### [70] [AI Agent for Source Finding by SoFiA-2 for SKA-SDC2](https://arxiv.org/abs/2512.00769)
*Xingchen Zhou,Nan Li,Peng Jia,Yingfeng Liu,Furen Deng,Shuanghao Shu,Ying Li,Liang Cao,Huanyuan Shan,Ayodeji Ibitoye*

Main category: cs.LG

TL;DR: 提出基于强化学习（SAC算法）的AI代理框架，自动优化射电天文源提取软件SoFiA的关键参数，在SKA科学数据挑战赛数据集上实现优于基准的性能


<details>
  <summary>Details</summary>
Motivation: 下一代大规模射电巡天（如SKA）需要高效的源提取方法，但现有工具（如SoFiA、Aegean）的参数配置优化困难，手动调参耗时且非平凡

Method: 使用最先进的强化学习算法Soft Actor-Critic（SAC）构建AI代理，通过与环境交互（调整参数）并根据SDC2评分反馈学习，逐步优化SoFiA在预处理、源查找和可靠性过滤模块的关键参数

Result: AI代理仅用100次评估步骤即可自动找到优于SoFiA团队基准的最优参数配置，同时减少了时间消耗

Conclusion: 该框架能解决需要复杂参数调优的类似问题，但需要包含代表性观测和真实星表的高质量训练集

Abstract: Source extraction is crucial in analyzing data from next-generation, large-scale sky surveys in radio bands, such as the Square Kilometre Array (SKA). Several source extraction programs, including SoFiA and Aegean, have been developed to address this challenge. However, finding optimal parameter configurations when applying these programs to real observations is non-trivial. For example, the outcomes of SoFiA intensely depend on several key parameters across its preconditioning, source-finding, and reliability-filtering modules. To address this issue, we propose a framework to automatically optimize these parameters using an AI agent based on a state-of-the-art reinforcement learning (RL) algorithm, i.e., Soft Actor-Critic (SAC). The SKA Science Data Challenge 2 (SDC2) dataset is utilized to assess the feasibility and reliability of this framework. The AI agent interacts with the environment by adjusting parameters based on the feedback from the SDC2 score defined by the SDC2 Team, progressively learning to select parameter sets that yield improved performance. After sufficient training, the AI agent can automatically identify an optimal parameter configuration that outperform the benchmark set by Team SoFiA within only 100 evaluation steps and with reduced time consumption. Our approach could address similar problems requiring complex parameter tuning, beyond radio band surveys and source extraction. Yet, high-quality training sets containing representative observations and catalogs of ground truth are essential.

</details>


### [71] [ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning](https://arxiv.org/abs/2512.00831)
*Yuchen Zeng,Shuibai Zhang,Wonjun Kang,Shutong Wu,Lynnix Zou,Ying Fan,Heeju Kim,Ziqian Lin,Jungtaek Kim,Hyung Il Koo,Dimitris Papailiopoulos,Kangwook Lee*

Main category: cs.LG

TL;DR: ReJump是一个分析大型推理模型（LRMs）推理行为的框架，将推理轨迹表示为问题解决步骤树中节点的访问顺序，通过量化探索、利用、过度思考、遗忘和验证等行为来理解LRMs的推理算法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在数学和编程等挑战性任务上取得了显著成功，但其底层的推理"算法"仍然缺乏深入理解。需要开发系统化的方法来分析LRMs的推理行为，揭示不同模型在相似准确率下的不同推理模式。

Method: 提出ReJump框架，将推理轨迹表示为问题解决步骤树中节点的访问顺序。使用"跳跃"概念描述节点间转移，包括相邻移动（如计算）和非相邻移动（如回溯和验证）。开发LLM代理将推理轨迹提取为ReJump格式，并设计多种量化指标来分析推理行为。

Result: 研究发现：1）准确率相似的模型可能表现出截然不同的推理行为；2）不同任务偏好不同的推理风格（如探索与利用的平衡不同）；3）ReJump能有效比较蒸馏LRMs与教师模型、CoT提示LLMs与LRMs的差异；4）ReJump可用于提升推理质量，如通过ReJump引导的最佳N选择和提示选择。

Conclusion: ReJump为理解大型推理模型的推理算法提供了系统化框架，揭示了模型推理行为的多样性，并能实际应用于提升推理性能。该框架有助于深入理解LRMs的工作机制，为模型优化提供指导。

Abstract: Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning "algorithms" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.

</details>


### [72] [Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs](https://arxiv.org/abs/2512.00908)
*Xinzhu Chen,Xuesheng Li,Zhongxiang Sun,Weijie Yu*

Main category: cs.LG

TL;DR: 论文提出LESS框架，通过细粒度优势调制低熵段来改进强化学习验证奖励方法，在数学推理任务上显著提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR研究关注高熵token的探索作用，但忽略了推理轨迹中大部分是低熵段，这些低熵段编码了稳定可复用的结构模式。研究发现正确回答的低熵段重叠与模型准确性高度相关，而错误回答的低熵段则表现出稳定但无效的模式。

Method: 提出LESS框架，在低熵段上进行细粒度优势调制：放大正确回答独有的段，抑制错误回答独有的段，中和两者共享的段，同时保留底层RL算法的高熵探索。该框架基于流行的GRPO实现。

Result: 在三个骨干模型和六个数学基准测试中，LESS相比强RL基线持续提升准确性，并实现了更强的性能下限鲁棒性。

Conclusion: 通过正确感知的低熵段调制，LESS框架有效改进了RLVR方法，证明了低熵段在推理任务中的重要性，为强化学习在语言模型推理中的应用提供了新方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.

</details>


### [73] [Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments](https://arxiv.org/abs/2512.00915)
*Junwoo Chang,Minwoo Park,Joohwan Seo,Roberto Horowitz,Jongmin Lee,Jongeun Choi*

Main category: cs.LG

TL;DR: 提出部分群不变MDP框架，选择性应用群不变Bellman备份，解决局部对称性破坏导致的全局值估计误差问题，开发PE-DQN和PE-SAC算法提升样本效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实环境中完全群不变的MDP几乎不存在，动力学、执行器限制和奖励设计通常会破坏对称性（通常只是局部破坏）。在群不变Bellman备份下，局部对称性破坏会引入误差并传播到整个状态-动作空间，导致全局值估计错误。

Method: 引入部分群不变MDP（PI-MDP）框架，根据对称性是否成立选择性应用群不变或标准Bellman备份。基于此框架开发PE-DQN（离散控制）和PE-SAC（连续控制）算法，结合等变性优势与对称性破坏的鲁棒性。

Result: 在Grid-World、运动控制和操作任务基准测试中，PE-DQN和PE-SAC显著优于基线方法，展示了选择性利用对称性对鲁棒且样本高效RL的重要性。

Conclusion: 部分群不变MDP框架能够缓解局部对称性破坏引起的误差传播，同时保持等变性的优势，从而增强样本效率和泛化能力。选择性对称性利用对于鲁棒高效的RL至关重要。

Abstract: Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.

</details>


### [74] [Mode-Conditioning Unlocks Superior Test-Time Scaling](https://arxiv.org/abs/2512.01127)
*Chen Henry Wu,Sachin Goyal,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出Mode-Conditioning (ModC)框架，通过显式分配推理模式来解决并行采样中的多样性崩溃问题，在多种任务和模型规模上提升测试时扩展效率


<details>
  <summary>Details</summary>
Motivation: 并行采样在测试时扩展中具有潜力，但受到多样性崩溃问题的限制——模型集中在少数模式上，重复采样产生相同错误。标准训练未能充分利用数据多样性

Method: 提出Mode-Conditioning (ModC)框架，使用专家模型或模式特定前缀显式分配测试时计算到不同推理模式。还展示了梯度聚类方法可以在没有显式模式标签的情况下实现ModC

Result: ModC在控制图搜索任务和大规模推理基准上一致改善扩展性，涵盖0.5B到7B模型。在OpenThoughts上，Qwen2.5-7B微调后获得4倍效率提升，同时提高最大Pass@k。梯度聚类方法在NuminaMath等数据集上带来10%增益。ModC还能改善强化学习

Conclusion: 标准训练未能充分利用数据多样性，ModC提供了一个简单有效的解决方案，释放测试时扩展中多样性的全部潜力

Abstract: Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.

</details>


### [75] [Stabilizing Reinforcement Learning with LLMs: Formulation and Practices](https://arxiv.org/abs/2512.01374)
*Chujie Zheng,Kai Dang,Bowen Yu,Mingze Li,Huiqiang Jiang,Junrong Lin,Yuqiong Liu,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: 本文提出了一种用于大语言模型强化学习的新公式，解释了为何以及何时可以通过代理token级目标优化真实序列级奖励，并揭示了训练稳定性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的强化学习训练存在稳定性问题，缺乏理论解释为何某些技术（如重要性采样、裁剪、路由回放）能稳定训练。本文旨在提供理论框架来解释这些现象。

Method: 通过一阶近似分析，提出理论公式解释token级代理目标何时能有效优化序列级奖励。实验使用30B参数的MoE模型，进行数十万GPU小时的训练，比较不同策略梯度方法。

Result: 在策略训练中，带重要性采样校正的基本策略梯度算法稳定性最高；离策略更新时，结合裁剪和路由回放对缓解策略陈旧性导致的失稳至关重要。一旦训练稳定，无论冷启动初始化如何，延长优化都能获得相当的性能。

Conclusion: 本文提供了RL训练稳定性的理论解释和实用配方，揭示了训练-推理差异和策略陈旧性最小化是代理目标有效的关键条件，为未来研究提供了指导。

Abstract: This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.

</details>


### [76] [Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning](https://arxiv.org/abs/2512.00961)
*Qi Wang,Mian Wu,Yuyang Zhang,Mingqi Yuan,Wenyao Zhang,Haoxiang You,Yunbo Wang,Xin Jin,Xiaokang Yang,Wenjun Zeng*

Main category: cs.LG

TL;DR: 利用预训练视频扩散模型为强化学习提供目标驱动的奖励信号，无需人工设计奖励函数


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖精心设计的程序化奖励函数，但这些函数设计困难且泛化性差。需要利用预训练视频扩散模型中的丰富世界知识来提供目标驱动的奖励信号

Method: 1) 视频级奖励：在领域特定数据集上微调预训练视频扩散模型，使用其视频编码器评估智能体轨迹与生成目标视频的潜在表示对齐度；2) 帧级奖励：使用CLIP从生成视频中识别最相关帧作为目标状态，学习前向-后向表示作为帧级奖励

Result: 在多个Meta-World任务上的实验证明了该方法的有效性

Conclusion: 利用预训练视频扩散模型可以为强化学习提供有效的目标驱动奖励信号，减少对人工设计奖励函数的依赖

Abstract: Reinforcement Learning (RL) has achieved remarkable success in various domains, yet it often relies on carefully designed programmatic reward functions to guide agent behavior. Designing such reward functions can be challenging and may not generalize well across different tasks. To address this limitation, we leverage the rich world knowledge contained in pretrained video diffusion models to provide goal-driven reward signals for RL agents without ad-hoc design of reward. Our key idea is to exploit off-the-shelf video diffusion models pretrained on large-scale video datasets as informative reward functions in terms of video-level and frame-level goals. For video-level rewards, we first finetune a pretrained video diffusion model on domain-specific datasets and then employ its video encoder to evaluate the alignment between the latent representations of agent's trajectories and the generated goal videos. To enable more fine-grained goal-achievement, we derive a frame-level goal by identifying the most relevant frame from the generated video using CLIP, which serves as the goal state. We then employ a learned forward-backward representation that represents the probability of visiting the goal state from a given state-action pair as frame-level reward, promoting more coherent and goal-driven trajectories. Experiments on various Meta-World tasks demonstrate the effectiveness of our approach.

</details>


### [77] [Agentic Policy Optimization via Instruction-Policy Co-Evolution](https://arxiv.org/abs/2512.01945)
*Han Zhou,Xingchen Wan,Ivan Vulić,Anna Korhonen*

Main category: cs.LG

TL;DR: INSPO提出了一种指令-策略协同进化框架，通过动态优化指令来提升强化学习代理的性能，在检索和推理任务上显著优于静态指令方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖静态人工设计的指令，但这些指令可能不是最优的，且随着策略改进和环境交互变化，最优指令也会改变，需要动态优化机制。

Method: INSPO框架将指令优化整合到RL循环中：维护动态指令候选池，通过RL奖励信号评估指令，定期淘汰低性能指令，基于回放缓冲区的经验通过LLM优化器生成新指令。

Result: 在多轮检索和推理任务上，INSPO显著优于依赖静态指令的基线方法，发现创新性指令引导代理采取更战略性的推理路径，仅增加少量计算开销。

Conclusion: 指令-策略协同进化是有效的，动态指令优化能显著提升RL代理性能，为构建更智能的自适应代理提供了新方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.

</details>


### [78] [World Model Robustness via Surprise Recognition](https://arxiv.org/abs/2512.01119)
*Geigh Zollicoffer,Tanush Chopra,Mingkuan Yan,Xiaoxu Ma,Kenneth Eaton,Mark Riedl*

Main category: cs.LG

TL;DR: 提出WISER算法，利用世界模型的惊讶度来减少噪声对基于世界模型的强化学习代理的影响，通过拒绝采样技术保持性能稳定


<details>
  <summary>Details</summary>
Motivation: 现实世界中的AI系统面临各种干扰和分布外噪声，这些噪声会破坏策略稳定性并导致不安全行为。虽然鲁棒训练可以减少对某些噪声的敏感性，但无法预测所有可能的OOD条件。

Method: 开发WISER算法，利用世界模型的惊讶度来减少噪声影响。引入多表示和单表示拒绝采样技术，分别处理多传感器故障和单传感器故障场景。

Result: 在CARLA和Safety Gymnasium等自动驾驶仿真环境中，该方法在不同类型和程度的噪声下都能保持性能，优于基线方法。同时证明该方法能增强两种不同架构的世界模型（Cosmos和DreamerV3）的稳定性。

Conclusion: 该方法通过利用世界模型的惊讶度来减轻噪声影响，在多种世界建模领域都表现出鲁棒性，为处理现实世界中的噪声干扰提供了有效解决方案。

Abstract: AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at https://github.com/Bluefin-Tuna/WISER .

</details>


### [79] [CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions](https://arxiv.org/abs/2512.01224)
*Ruixiang Feng,Zhenwei An,Yuntao Wen,Ran Le,Yiming Jia,Chen Yang,Zongchao Chen,Lisi Chen,Shen Gao,Shuo Shang,Yang Song,Tao Zhang*

Main category: cs.LG

TL;DR: CoSineVerifier是一个工具增强的验证器，利用外部执行器进行精确计算和符号简化，在STEM、通用QA和长链推理任务中表现出色，并在RLVR中作为奖励模型超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前验证方法在计算密集型科学领域（如代数等价检查和物理常数替换）面临挑战，需要超越简单语义匹配的鲁棒验证能力。

Method: 提出工具增强的验证器CoSineVerifier，采用两阶段训练流程：冷启动微调 + 多轮强化学习与工具集成，利用外部执行器进行精确计算和符号简化。

Result: 在VerifyBench-Hard和SCI-Bench上达到SOTA性能；作为RLVR奖励模型在AIME'24和AIME'25上持续优于基于规则和基于模型的验证器。

Conclusion: CoSineVerifier展示了强大的泛化能力，能有效增强LLM的推理能力，特别是在计算密集型科学领域。

Abstract: Answer verification methods are widely employed in language model training pipelines spanning data curation, evaluation, and reinforcement learning with verifiable rewards (RLVR). While prior work focus on developing unified verifiers applicable across multiple reasoning scenarios, significant challenges remain in computation-oriented scientific domains, such as algebraic equivalence checking and physical constant substitution. In this paper, we introduce \model, a tool-augmented verifier that leverages external executors to perform precise computations and symbolic simplifications. \model enables robust verification that goes beyond simple semantic matching. We propose a novel two-stage pipeline, which begin with cold-start fine-tuning and followed by multi-turn reinforcement learning with tool integration. Extensive experiments conducted on STEM subjects, general QA, and long-form reasoning tasks demonstrates strong generalization of \model. The results shows that the \model achieves state-of-the-art performance on VerifyBench-Hard and SCI-Bench. And we also employ our \model in RLVR as a reward model, the results show that it consistently outperforms both rubric-based and model-based verifiers on AIME'24 and AIME'25, demonstrating strong potential to enhance reasoning capabilities of LLM. Our model is released at \hyperlink{https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B}{https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B}.

</details>


### [80] [How Does RL Post-training Induce Skill Composition? A Case Study on Countdown](https://arxiv.org/abs/2512.01775)
*Simon Park,Simran Kaur,Sanjeev Arora*

Main category: cs.LG

TL;DR: 研究RL后训练如何促进组合泛化能力，而非仅仅是长度泛化，通过Countdown任务分析模型将技能作为表达式子树重用的机制


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习能提升大语言模型的推理能力，但其在促进组合泛化（从已知组件合成新技能）方面的作用常被与简单的长度泛化混淆。需要明确RL后训练如何教授技能组合，以及组合结构如何影响技能迁移

Method: 聚焦Countdown任务（给定n个数字和目标值，构建表达式求值到目标），将模型解决方案分析为表达式树，每个子树对应可重用的子任务（技能）。通过跟踪训练过程中树形结构和成功率来分析

Result: 发现：(1) 对更大n和未见树形的分布外泛化，表明子任务的组合重用；(2) 结构依赖的学习层次性——模型先掌握浅层平衡树（子任务间工作负载平衡），后掌握深层不平衡树，对右重结构存在持续脆弱性（即使组合深度与左重结构相同）

Conclusion: 诊断揭示了RL后训练学习的内容、顺序以及泛化失败的地方，阐明了仅通过RL后训练如何诱导超出标准指标（如pass@k）的分布外泛化

Abstract: While reinforcement learning (RL) successfully enhances reasoning in large language models, its role in fostering compositional generalization (the ability to synthesize novel skills from known components) is often conflated with mere length generalization. To this end, we study what RL post-training teaches about skill composition and how the structure of the composition affects the skill transfer. We focus on the Countdown task (given n numbers and a target, form an expression that evaluates to the target) and analyze model solutions as expression trees, where each subtree corresponds to a reusable subtask and thus can be viewed as a ``skill.'' Tracking tree shapes and their success rates over training, we find: (i) out-of-distribution (OOD) generalization to larger n and to unseen tree shapes, indicating compositional reuse of subtasks; (ii) a structure-dependent hierarchy of learnability -- models master shallow balanced trees (workload is balanced between subtasks) before deep unbalanced ones, with persistent fragility on right-heavy structures (even when the composition depth is the same as some left-heavy structures). Our diagnostic reveals what is learned, in what order, and where generalization fails, clarifying how RL-only post-training induces OOD generalization beyond what standard metrics such as pass@k reveal.

</details>


### [81] [Forecasting in Offline Reinforcement Learning for Non-stationary Environments](https://arxiv.org/abs/2512.01987)
*Suzan Ece Ada,Georg Martius,Emre Ugur,Erhan Oztop*

Main category: cs.LG

TL;DR: FORL框架结合条件扩散状态生成和零样本时序基础模型，解决离线强化学习中非平稳环境下的部分可观测问题，提升在现实世界偏移场景中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法通常假设环境平稳或仅考虑测试时的合成扰动，但这些假设在现实世界中经常失效。现实场景常存在突发、时变的偏移，导致部分可观测性，使智能体误判真实状态并降低性能。

Method: FORL框架统一了：(1) 条件扩散基的候选状态生成，训练时不预设任何特定的未来非平稳模式；(2) 零样本时序基础模型。该框架针对易受意外、可能非马尔可夫偏移影响的环境，要求智能体从每个episode开始就具备鲁棒性能。

Result: 在离线强化学习基准测试中，使用真实世界时序数据模拟现实非平稳性，FORL相比竞争基线方法持续提升性能表现。

Conclusion: 通过整合零样本预测与智能体经验，FORL旨在弥合离线强化学习与现实世界非平稳环境复杂性之间的差距。

Abstract: Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [82] [The Moat of the Search Index](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frobonomics.substack.com%2Fp%2Fsearch-the-moat-of-the-search-index%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/gLlfBF2hIBAVMN1VKuP4WV6GL_zla129oOpplQvrSu0=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ChatGPT通过"代理"方法超越谷歌搜索，从多个来源提取信息并合成答案，削弱了谷歌传统搜索索引的优势，但长尾、新鲜或SEO密集查询仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 分析AI驱动的聊天模型如何改变传统搜索引擎的竞争格局，特别是ChatGPT的"代理"方法如何削弱谷歌搜索索引的传统护城河优势。

Method: 采用"代理"方法，从多个来源提取相关信息并合成综合答案，减少对单一搜索结果失败的依赖。

Result: ChatGPT在信息合成方面超越谷歌搜索，传统搜索引擎的护城河优势基本消失，AI模型逐渐融合搜索功能。

Conclusion: 传统搜索引擎的索引优势已被AI驱动的代理方法削弱，搜索范式正在向信息提取和合成转变。

Abstract: The Moat of the Search Index (2 minute read) ChatGPT surpasses Google's search by using an "agent" approach, extracting relevant information from multiple sources and synthesizing answers, which diminishes Google's traditional search index advantage. This method reduces the impact of any single search result's failure, although it struggles with long-tail, fresh, or SEO-heavy queries. The traditional search engine moat has largely vanished, with AI-driven models increasingly blending search c...

</details>


### [83] [Better Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Flangwatch%2Fbetter-agents%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/e1rD7RT2zGzjkqTK1_uA1L60RF1V_UthwDhAn8jap00=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Better Agents是一个CLI工具和标准集，用于构建AI代理，通过生成AGENTS.md文件确保行业最佳实践，使编码助手能够在任何代理框架中成为专家。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理开发缺乏统一标准和最佳实践，导致开发效率低下、质量参差不齐。需要工具来标准化代理构建流程，确保行业最佳实践得到遵循。

Method: 提供CLI工具引导用户选择编程语言、代理框架、编码助手、LLM提供商和API密钥，并生成包含行业最佳实践的AGENTS.md文档。

Result: 创建了一个标准化工具集，使开发者能够更高效地构建高质量的AI代理，确保最佳实践得到实施。

Conclusion: Better Agents通过提供标准化工具和文档，解决了AI代理开发中的碎片化问题，提升了开发效率和代理质量。

Abstract: Better Agents (GitHub Repo) Better Agents is a CLI tool and a set of standards for agent building. It makes coding assistants experts in any agent framework. The tool generates an AGENTS.md that ensures industry best practices. The CLI guides users through selecting a programming language, agent framework, coding assistant, LLM provider, and API keys.

</details>


### [84] [Compounding Engineering Plugin](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FEveryInc%2Fcompounding-engineering-plugin%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/7LVeaQSYbsnVM32Zvt5lZ_l6nb3mm57yMc-VDzGSDMM=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Compounding Engineering Plugin是一个Claude Code插件，通过AI工具系统化改进开发工作流，使每个工程单元让后续工作更容易而非更难。


<details>
  <summary>Details</summary>
Motivation: 传统开发中，工程工作往往相互独立，缺乏系统性改进。作者提出"复合工程"概念，认为每个工程单元都应使后续工作更容易，但缺乏实用工具来实现这一理念。

Method: 开发了一个Claude Code插件，提供AI驱动的工具集，帮助开发者在规划、构建和代码审查阶段系统化应用复合工程原则，使这一理念变得实用可行。

Result: 创建了GitHub仓库中的Compounding Engineering Plugin，为开发者提供了实现复合工程的具体工具，能够转变开发工作流程，使工程工作产生累积性改进效果。

Conclusion: 该插件成功将复合工程理念转化为实用工具，通过AI辅助系统化改进开发工作流，使工程工作能够产生正向累积效应，提高开发效率和质量。

Abstract: Compounding Engineering Plugin (GitHub Repo) The Compounding Engineering Plugin is a Claude Code plugin that transforms how developers plan, build, and review code using AI-powered tools that systematically improve their development workflow. Compound engineering is the idea that each unit of engineering work should make subsequent units of work easier, not harder. The plugin provides the tools to make compound engineering practical.

</details>


### [85] [INTELLECT-3: A 100B+ MoE trained with large-scale RL](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.primeintellect.ai%2Fblog%2Fintellect-3%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/p8PLjCDjyZh48hBfWgu-nMlRrj__06cQbSkvB161XqI=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: INTELLECT-3是一个100B+参数的MoE模型，在数学、代码、科学和推理基准测试中达到同规模SOTA性能，基于GLM 4.5 Air基础模型，使用SFT和RL训练，特别设计了多样化的RL环境来增强推理和智能体能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个在数学、代码、科学和推理等多个领域都表现出色的通用大模型，通过强化学习训练增强模型的推理能力和智能体能力，超越传统监督微调方法的性能限制。

Method: 基于GLM 4.5 Air基础模型，采用混合专家架构，参数超过100B。训练方法结合了监督微调和强化学习，特别设计了多样化和具有挑战性的RL环境来专门提升模型的推理和智能体能力。

Result: 在数学、代码、科学和推理等多个基准测试中，INTELLECT-3达到了同规模模型中的最先进性能，证明了RL训练对提升模型推理和智能体能力的有效性。

Conclusion: 大规模强化学习训练结合混合专家架构能够显著提升大模型在复杂任务上的性能，特别是在推理和智能体能力方面，为未来大模型的发展提供了有前景的方向。

Abstract: INTELLECT-3: A 100B+ MoE trained with large-scale RL (10 minute read) INTELLECT-3 is a 100B+ parameter Mixture-of-Experts that achieves state-of-the-art performance for its size across math, code, science, and reasoning benchmarks. It was trained with both SFT and RL on top of the GLM 4.5 Air base model. The researchers used a diverse and challenging mix of RL environments designed to enhance the reasoning and agentic capabilities of their model. Full details about the training are available.

</details>


### [86] [Why Engineers Struggle to Build AI Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fwhy-engineers-struggle-building-agents%3Futm_source=tldrdata/1/0100019ad998d72f-102b8166-1fa4-4bc9-9f27-ba4bed53dcf4-000000/deZtPxoWK3uFr5Fj1SxcJrgQvdbWOv3IBNPOGwzs28s=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 资深工程师构建AI代理困难，因为传统确定性思维与AI概率系统冲突，需要转变开发范式


<details>
  <summary>Details</summary>
Motivation: 分析资深工程师在构建AI代理时面临的挑战，探讨传统软件开发思维与AI代理开发之间的根本差异

Method: 通过对比传统确定性工程思维与AI概率系统的差异，提出思维转变和开发实践建议

Result: 识别出确定性思维与概率系统之间的核心冲突，提出成功构建AI代理需要的关键思维转变

Conclusion: 资深工程师需要从确定性思维转向概率思维，接受语义灵活性、代理自主控制、错误作为输入等新范式

Abstract: Why (Senior) Engineers Struggle to Build AI Agents (6 minute read) Senior engineers struggle to build AI agents because their traditional deterministic mindset—centered on strict types, predictable control flow, and error-free code—clashes with the probabilistic systems that thrive on ambiguity, natural language state, and non-linear behavior. Success requires embracing semantic flexibility, handing over control to the agent, treating errors as inputs, replacing unit tests with evaluations, a...

</details>


### [87] [Agents Should Be More Opinionated](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.vtrivedy.com%2Fposts%2Fagents-should-be-more-opinionated%3Futm_source=tldrnewsletter/1/0100019ad9a80c55-f376a1b0-aa4a-4392-93cc-535d664e550b-000000/eSm5GT2dscN7wNA-qgawgTYtLspb9puuDNNg08hGFOg=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文认为最佳的智能体产品应该更加"固执己见"，通过精心设计提供可靠、无需过多调整的用户体验，让一切都能正常工作。


<details>
  <summary>Details</summary>
Motivation: 智能体产品的目标是为用户提供愉悦的体验，但目前许多产品过于灵活，需要用户进行大量设置和调整，这降低了用户体验。

Method: 通过产品设计的方法，将创作者的愿景提炼为直观的界面，使智能体产品能够可靠工作而无需过多调整。

Result: 提出"固执己见"的智能体产品设计理念，强调好的智能体产品应该像优秀产品设计一样，通过精心设计的默认设置和限制来提供最佳体验。

Conclusion: 智能体产品开发者应该更加"固执己见"，通过精心设计的产品决策来提供可靠、愉悦的用户体验，而不是让用户承担过多的配置责任。

Abstract: Agents Should Be More Opinionated (10 minute read) The best agent products are the most opinionated. The goal in agent products is to give users a delightful experience. A good baseline for agents is that everything works reliably without tweaking too many settings. Good product design is the result of creators distilling their vision into an intuitive interface that just works.

</details>


### [88] [Migrating 1,000,000 lines of JUnit](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.moderne.ai%2Fcontent-library%2Fhow-uber-migrated-1m-lines-junit-2-weeks-webinar%3Futm_source=tldr%26utm_medium=email%26utm_campaign=dec_uber_webinar_dec_1%26utm_content=lp%26utm_term=register/1/0100019ad9cebe1c-dc6c3160-65da-4955-8559-516851e60b81-000000/SPM7zfFe26tomvLeduYAn4m9ElhVo4HvY2Uv8s1qoN0=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Uber使用OpenRewrite和AI辅助静态分析自动化了4000个PR，安全地将100万行代码现代化，将原本需要数月的迁移缩短至两周


<details>
  <summary>Details</summary>
Motivation: 需要大规模迁移和现代化JUnit代码库，传统手动迁移方法耗时数月且容易出错，需要更高效、安全的自动化解决方案

Method: 采用OpenRewrite静态分析工具结合AI辅助，实现上下文感知的代码重构，自动化生成和提交pull requests

Result: 成功自动化处理4000个pull requests，安全迁移100万行代码，将原本需要数月的迁移时间缩短到仅两周

Conclusion: AI辅助的静态分析和自动化工具能够显著加速大规模代码迁移，提高安全性并减少人工工作量

Abstract: Migrating 1,000,000 lines of JUnit (Sponsor) Using OpenRewrite and AI-assisted static analysis, Uber automated 4,000 pull requests and modernized 1 million lines of code safely. Learn how they used context-aware refactoring to pull off a migration that would take most teams months in just two weeks. Watch the Moderne webinar on-demand

</details>


### [89] [Writing a good CLAUDE.md](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.humanlayer.dev%2Fblog%2Fwriting-a-good-claude-md%3Futm_source=tldrdev/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/nA3aBWQ11MR9ZSg-mf23yn1_B0VM0bpfjU_NVQoTrwU=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: CLAUDE.md是用于引导Claude理解代码库的配置文件，应保持简洁通用，通过渐进式披露指向特定任务文档，并依赖确定性工具处理代码风格


<details>
  <summary>Details</summary>
Motivation: 为Claude AI助手提供有效的代码库上下文信息，使其能更好地理解和操作项目，同时避免信息过载，保持配置文件的简洁性和可维护性

Method: 创建简洁通用的CLAUDE.md文件，包含项目目的、技术栈和操作指令，采用渐进式披露策略，通过引用特定任务文档和使用确定性工具（如linters）来处理复杂需求

Result: 提出了一套有效的CLAUDE.md编写指南，强调简洁性、通用性和渐进式披露原则，为AI助手与代码库的集成提供了实用方法

Conclusion: CLAUDE.md应作为轻量级的入口点，通过引用外部文档和工具来扩展功能，这种设计模式能提高AI助手的工作效率和代码库的可维护性

Abstract: Writing a good CLAUDE.md (10 minute read) CLAUDE.md, a file for onboarding Claude to a codebase, provides important context about the project's purpose, technology stack, and operational instructions. The CLAUDE.md should be kept concise and universally applicable. Instead of stuffing everything into CLAUDE.md, progressive disclosure should be used by pointing to separate, task-specific markdown files when needed and relying on deterministic tools like linters for code styling.

</details>


### [90] [How to evaluate AI agents, a guide by Weights & Biases](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwandb.ai%2Fsite%2Fresources%2Fwhitepapers%2Fevaluating-ai-agent-applications%2F%3Futm_source=tldr-dev%26utm_medium=cpc%26utm_campaign=WP-EvalAIAgents%26utm_content=weave%26utm_term=newsletter/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/tSaHZZMz_OMq25XLf8zG8MTUqaAuf44uMCoVl-6APIM=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Weights & Biases发布的AI智能体评估指南，为AI智能体开发提供五步评估框架，涵盖准确性、延迟、成本、安全性和用户体验等多个维度


<details>
  <summary>Details</summary>
Motivation: AI智能体开发与传统软件开发规则不同，需要专门的评估框架。来自Canva、Cisco、Snowflake等公司的团队经验表明，建立系统化的评估方法对评估智能体性能至关重要

Method: 提供实用的五步评估配方，建立评估框架以评估AI智能体在准确性、延迟、成本、安全性和用户体验等多个方面的表现

Result: 通过Weights & Biases的指南，开发团队可以获得系统化的评估方法，借鉴行业领先公司的实践经验，建立有效的AI智能体评估体系

Conclusion: AI智能体开发需要专门的评估框架，Weights & Biases提供的五步评估指南为团队提供了实用的评估方法，帮助评估智能体在多个关键维度的表现

Abstract: How to evaluate AI agents, a guide by Weights & Biases (Sponsor) AI agent development breaks the rules of traditional software development. Learn from teams at Canva, Cisco, Snowflake and more, on how to establish an evaluation framework so you can assess your agent across accuracy, latency, cost, safety, and user experience and more. Get a practical five-step recipe for effective evaluations in this guide from Weights & Biases.

</details>


### [91] [The space of minds](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fthe-space-of-minds%2F%3Futm_source=tldrdev/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/k0QpdkBSfuVWqWrjSs8Cxq8gJvyfCH7MDjKMROGI8Bk=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了动物和LLM智能都是优化的形式，提出了"心智空间"的概念框架


<details>
  <summary>Details</summary>
Motivation: 理解动物智能和人工智能（特别是LLM）之间的本质联系，建立一个统一的理论框架来分析不同形式的智能

Method: 采用概念分析框架，将动物和LLM智能都视为优化的形式，提出"心智空间"的概念来统一理解不同智能系统

Result: 建立了动物智能与LLM智能的统一理论视角，揭示了它们作为优化过程的共同本质

Conclusion: 动物和LLM智能都是优化的表现形式，这一视角有助于更深入地理解智能的本质和演化

Abstract: The space of minds (3 minute read) Both animal and LLM intelligences are forms of optimization.

</details>
