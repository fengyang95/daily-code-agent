{"id": "2512.17912", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17912", "abs": "https://arxiv.org/abs/2512.17912", "authors": ["Lihui Liu"], "title": "Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning", "comment": null, "summary": "ChatGPT said: Text-attributed graphs, where nodes and edges contain rich textual information, are widely used across diverse domains. A central challenge in this setting is question answering, which requires jointly leveraging unstructured text and the structured relational signals within the graph. Although Large Language Models (LLMs) have made significant advances in natural language understanding, their direct use for reasoning over text-attributed graphs remains limited. Retrieval-augmented generation methods that operate purely on text often treat passages as isolated units, ignoring the interconnected structure of the graph. Conversely, graph-based RAG methods that serialize large subgraphs into long textual sequences quickly become infeasible due to LLM context-length constraints, resulting in fragmented reasoning and degraded accuracy. To overcome these limitations, we introduce Graph-O1, an agentic GraphRAG framework that enables LLMs to conduct stepwise, interactive reasoning over graphs. Our approach integrates Monte Carlo Tree Search (MCTS) with end-to-end reinforcement learning, allowing the model to selectively explore and retrieve only the most informative subgraph components. The reasoning procedure is framed as a multi-turn interaction between the agent and the graph environment, and the agent is trained through a unified reward mechanism. Extensive experiments across multiple LLM backbones demonstrate that Graph-O1 consistently surpasses state-of-the-art baselines, producing answers that are more accurate, reliable, and interpretable.", "AI": {"tldr": "Graph-O1\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684GraphRAG\u6846\u67b6\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0LLM\u5728\u56fe\u7ed3\u6784\u4e0a\u7684\u9010\u6b65\u4ea4\u4e92\u5f0f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u95ee\u7b54\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6587\u672c\u5c5e\u6027\u56fe\u95ee\u7b54\u65f6\u5b58\u5728\u5c40\u9650\uff1a\u7eaf\u6587\u672c\u68c0\u7d22\u65b9\u6cd5\u5ffd\u7565\u56fe\u7ed3\u6784\uff0c\u800c\u56fe\u5e8f\u5217\u5316\u65b9\u6cd5\u53d7\u9650\u4e8eLLM\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u548c\u51c6\u786e\u6027\u4e0b\u964d\u3002", "method": "\u63d0\u51faGraph-O1\u6846\u67b6\uff0c\u5c06\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e0e\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u8ba9LLM\u667a\u80fd\u4f53\u80fd\u591f\u9009\u62e9\u6027\u63a2\u7d22\u548c\u68c0\u7d22\u6700\u6709\u4fe1\u606f\u7684\u5b50\u56fe\u7ec4\u4ef6\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u8fdb\u884c\u56fe\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cGraph-O1\u4e00\u81f4\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751f\u6210\u66f4\u51c6\u786e\u3001\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u7b54\u6848\u3002", "conclusion": "Graph-O1\u901a\u8fc7\u667a\u80fd\u4f53\u5f0f\u7684\u9010\u6b65\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5c5e\u6027\u56fe\u95ee\u7b54\u7684\u6311\u6218\uff0c\u4e3a\u56fe\u7ed3\u6784\u4e0a\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2512.17914", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.17914", "abs": "https://arxiv.org/abs/2512.17914", "authors": ["Boris Kriuk", "Logic Ng"], "title": "Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression", "comment": "7 pages, 4 figures, 1 table", "summary": "Multi-agent Large Language Model (LLM) systems face a critical bottleneck: redundant transmission of contextual information between agents consumes excessive bandwidth and computational resources. Traditional approaches discard internal semantic representations and transmit raw text, forcing receiving agents to recompute similar representations from scratch. We introduce Q-KVComm, a new protocol that enables direct transmission of compressed key-value (KV) cache representations between LLM agents. Q-KVComm combines three key innovations: (1) adaptive layer-wise quantization that allocates variable bit-widths based on sensitivity profiling, (2) hybrid information extraction that preserves critical facts across content domains, and (3) heterogeneous model calibration establishing cross-architecture communication. Extensive experiments across three diverse question-answering datasets demonstrate that Q-KVComm achieves 5-6x compression ratios while maintaining semantic fidelity, with coherence quality scores above 0.77 across all scenarios. The protocol exhibits robust performance across model sizes (1.1B-1.5B parameters) and adapts to real-world applications including conversational QA and multi-hop reasoning. Our work establishes a new paradigm for LLM agent communication, shifting from text-based to representation-based information exchange.", "AI": {"tldr": "Q-KVComm\u662f\u4e00\u79cd\u65b0\u7684LLM\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u534f\u8bae\uff0c\u901a\u8fc7\u76f4\u63a5\u4f20\u8f93\u538b\u7f29\u7684KV\u7f13\u5b58\u8868\u793a\u800c\u975e\u539f\u59cb\u6587\u672c\uff0c\u5b9e\u73b05-6\u500d\u538b\u7f29\u6bd4\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u9762\u4e34\u5173\u952e\u74f6\u9888\uff1a\u667a\u80fd\u4f53\u95f4\u5197\u4f59\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f20\u8f93\u6d88\u8017\u8fc7\u591a\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u4f20\u7edf\u65b9\u6cd5\u4e22\u5f03\u5185\u90e8\u8bed\u4e49\u8868\u793a\u800c\u4f20\u8f93\u539f\u59cb\u6587\u672c\uff0c\u8feb\u4f7f\u63a5\u6536\u667a\u80fd\u4f53\u4ece\u5934\u91cd\u65b0\u8ba1\u7b97\u76f8\u4f3c\u8868\u793a\u3002", "method": "Q-KVComm\u534f\u8bae\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1) \u57fa\u4e8e\u654f\u611f\u6027\u5206\u6790\u7684\u5c42\u81ea\u9002\u5e94\u91cf\u5316\uff0c\u5206\u914d\u53ef\u53d8\u6bd4\u7279\u5bbd\u5ea6\uff1b(2) \u8de8\u5185\u5bb9\u57df\u4fdd\u7559\u5173\u952e\u4e8b\u5b9e\u7684\u6df7\u5408\u4fe1\u606f\u63d0\u53d6\uff1b(3) \u5efa\u7acb\u8de8\u67b6\u6784\u901a\u4fe1\u7684\u5f02\u6784\u6a21\u578b\u6821\u51c6\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cQ-KVComm\u5b9e\u73b05-6\u500d\u538b\u7f29\u6bd4\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u6240\u6709\u573a\u666f\u4e0b\u8fde\u8d2f\u6027\u8d28\u91cf\u5206\u6570\u5747\u9ad8\u4e8e0.77\u3002\u534f\u8bae\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21(1.1B-1.5B\u53c2\u6570)\u4e0a\u8868\u73b0\u7a33\u5065\uff0c\u9002\u5e94\u5bf9\u8bddQA\u548c\u591a\u8df3\u63a8\u7406\u7b49\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aLLM\u667a\u80fd\u4f53\u901a\u4fe1\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u4ece\u57fa\u4e8e\u6587\u672c\u7684\u4fe1\u606f\u4ea4\u6362\u8f6c\u5411\u57fa\u4e8e\u8868\u793a\u7684\u4fe1\u606f\u4ea4\u6362\u3002", "topic": "agent analysis"}}
{"id": "2512.18020", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18020", "abs": "https://arxiv.org/abs/2512.18020", "authors": ["Brahim Mahmoudi", "Zacharie Chenail-Larcher", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "Specification and Detection of LLM Code Smells", "comment": "Accepted paper at ICSE NIER 2026 : https://conf.researchr.org/track/icse-2026/icse-2026-nier", "summary": "Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LLM\u4ee3\u7801\u5f02\u5473\u7684\u6982\u5ff5\uff0c\u5f62\u5f0f\u5316\u4e86\u4e94\u79cd\u4e0eLLM\u63a8\u7406\u76f8\u5173\u7684\u5e38\u89c1\u95ee\u9898\u7f16\u7801\u5b9e\u8df5\uff0c\u5e76\u6269\u5c55\u4e86\u68c0\u6d4b\u5de5\u5177\u6765\u9a8c\u8bc1\u5176\u5728\u5f00\u6e90\u7cfb\u7edf\u4e2d\u7684\u666e\u904d\u6027\u3002", "motivation": "LLM\u88ab\u5e7f\u6cdb\u96c6\u6210\u5230\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9LLM\u63a8\u7406\u4ee3\u7801\u7684\u7279\u5b9a\u4ee3\u7801\u5f02\u5473\u5206\u7c7b\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5f71\u54cd\u8f6f\u4ef6\u7cfb\u7edf\u8d28\u91cf\u3002", "method": "\u57fa\u4e8e\u76f8\u5173\u6587\u732e\u5f62\u5f0f\u5316\u4e94\u79cdLLM\u4ee3\u7801\u5f02\u5473\uff0c\u6269\u5c55SpecDetect4AI\u68c0\u6d4b\u5de5\u5177\uff0c\u5e76\u5728200\u4e2a\u5f00\u6e90LLM\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u5176\u666e\u904d\u6027\u3002", "result": "LLM\u4ee3\u7801\u5f02\u5473\u5f71\u54cd\u4e8660.50%\u7684\u5206\u6790\u7cfb\u7edf\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u8fbe\u523086.06%\u3002", "conclusion": "LLM\u4ee3\u7801\u5f02\u5473\u5728LLM\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u9700\u8981\u4e13\u95e8\u7684\u68c0\u6d4b\u5de5\u5177\u548c\u7f16\u7801\u5b9e\u8df5\u6765\u6539\u5584\u8f6f\u4ef6\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "2512.18094", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18094", "abs": "https://arxiv.org/abs/2512.18094", "authors": ["Boxuan Wang", "Zhuoyun Li", "Xiaowei Huang", "Yi Dong"], "title": "Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks", "comment": "Under Review", "summary": "Large language models (LLMs) have enabled multi-agent systems (MAS) in which multiple agents argue, critique, and coordinate to solve complex tasks, making communication topology a first-class design choice. Yet most existing LLM-based MAS either adopt fully connected graphs, simple sparse rings, or ad-hoc dynamic selection, with little structural guidance. In this work, we revisit classic theory on small-world (SW) networks and ask: what changes if we treat SW connectivity as a design prior for MAS? We first bridge insights from neuroscience and complex networks to MAS, highlighting how SW structures balance local clustering and long-range integration. Using multi-agent debate (MAD) as a controlled testbed, experiment results show that SW connectivity yields nearly the same accuracy and token cost, while substantially stabilizing consensus trajectories. Building on this, we introduce an uncertainty-guided rewiring scheme for scaling MAS, where long-range shortcuts are added between epistemically divergent agents using LLM-oriented uncertainty signals (e.g., semantic entropy). This yields controllable SW structures that adapt to task difficulty and agent heterogeneity. Finally, we discuss broader implications of SW priors for MAS design, framing them as stabilizers of reasoning, enhancers of robustness, scalable coordinators, and inductive biases for emergent cognitive roles.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u5c06\u5c0f\u4e16\u754c\u7f51\u7edc\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bbe\u8ba1\u5148\u9a8c\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5c0f\u4e16\u754c\u8fde\u63a5\u80fd\u5728\u4fdd\u6301\u51c6\u786e\u6027\u548c\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u7a33\u5b9a\u5171\u8bc6\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u91cd\u8fde\u673a\u5236\u6765\u6784\u5efa\u81ea\u9002\u5e94\u7684\u5c0f\u4e16\u754c\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5927\u591a\u91c7\u7528\u5168\u8fde\u63a5\u56fe\u3001\u7b80\u5355\u7a00\u758f\u73af\u6216\u4e34\u65f6\u52a8\u6001\u9009\u62e9\uff0c\u7f3a\u4e4f\u7ed3\u6784\u6307\u5bfc\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c0f\u4e16\u754c\u7f51\u7edc\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u4fe1\u62d3\u6251\u7684\u8bbe\u8ba1\u5148\u9a8c\uff0c\u4ee5\u5e73\u8861\u5c40\u90e8\u805a\u7c7b\u548c\u957f\u7a0b\u6574\u5408\u3002", "method": "1) \u5c06\u795e\u7ecf\u79d1\u5b66\u548c\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u5c0f\u4e16\u754c\u7406\u8bba\u5e94\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1b2) \u4ee5\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6bd4\u8f83\u4e0d\u540c\u8fde\u63a5\u7ed3\u6784\uff1b3) \u5f15\u5165\u57fa\u4e8eLLM\u5bfc\u5411\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff08\u5982\u8bed\u4e49\u71b5\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u91cd\u8fde\u673a\u5236\uff0c\u5728\u8ba4\u77e5\u5dee\u5f02\u5927\u7684\u667a\u80fd\u4f53\u95f4\u6dfb\u52a0\u957f\u7a0b\u6377\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u5c0f\u4e16\u754c\u8fde\u63a5\u5728\u4fdd\u6301\u51e0\u4e4e\u76f8\u540c\u51c6\u786e\u6027\u548c\u4ee4\u724c\u6210\u672c\u7684\u540c\u65f6\uff0c\u80fd\u663e\u8457\u7a33\u5b9a\u5171\u8bc6\u8f68\u8ff9\u3002\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u91cd\u8fde\u673a\u5236\u80fd\u4ea7\u751f\u9002\u5e94\u4efb\u52a1\u96be\u5ea6\u548c\u667a\u80fd\u4f53\u5f02\u8d28\u6027\u7684\u53ef\u63a7\u5c0f\u4e16\u754c\u7ed3\u6784\u3002", "conclusion": "\u5c0f\u4e16\u754c\u5148\u9a8c\u53ef\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u91cd\u8981\u6846\u67b6\uff0c\u5177\u6709\u7a33\u5b9a\u63a8\u7406\u3001\u589e\u5f3a\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u534f\u8c03\u4ee5\u53ca\u4e3a\u6d8c\u73b0\u8ba4\u77e5\u89d2\u8272\u63d0\u4f9b\u5f52\u7eb3\u504f\u7f6e\u7b49\u66f4\u5e7f\u6cdb\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2512.18088", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.18088", "abs": "https://arxiv.org/abs/2512.18088", "authors": ["Dongchan Kim", "Hamidreza Khoramrokh", "Lei Zhang", "Andriy Miranskyy"], "title": "Detecting Flaky Tests in Quantum Software: A Dynamic Approach", "comment": null, "summary": "Flaky tests, tests that pass or fail nondeterministically without changes to code or environment, pose a serious threat to software reliability. While classical software engineering has developed a rich body of dynamic and static techniques to study flakiness, corresponding evidence for quantum software remains limited. Prior work relies primarily on static analysis or small sets of manually reported incidents, leaving open questions about the prevalence, characteristics, and detectability of flaky tests.\n  This paper presents the first large-scale dynamic characterization of flaky tests in quantum software. We executed the Qiskit Terra test suite 10,000 times across 23 releases in controlled environments. For each release, we measured test-outcome variability, identified flaky tests, estimated empirical failure probabilities, analyzed recurrence across versions, and used Wilson confidence intervals to quantify rerun budgets for reliable detection. We further mapped flaky tests to Terra subcomponents to assess component-level susceptibility.\n  Across 27,026 test cases, we identified 290 distinct flaky tests. Although overall flakiness rates were low (0-0.4%), flakiness was highly episodic: nearly two-thirds of flaky tests appeared in only one release, while a small subset recurred intermittently or persistently. Many flaky tests failed with very small empirical probabilities ($\\hat{p} \\approx 10^{-4}$), implying that tens of thousands of executions may be required for confident detection. Flakiness was unevenly distributed across subcomponents, with 'transpiler' and 'quantum_info' accounting for the largest share.\n  These results show that quantum test flakiness is rare but difficult to detect under typical continuous integration budgets. To support future research, we release a public dataset of per-test execution outcomes.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u91cf\u5b50\u8f6f\u4ef6\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u8fdb\u884c\u5927\u89c4\u6a21\u52a8\u6001\u5206\u6790\uff0c\u5728Qiskit Terra\u6d4b\u8bd5\u5957\u4ef6\u4e2d\u53d1\u73b0290\u4e2a\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff0c\u867d\u7136\u603b\u4f53\u4e0d\u7a33\u5b9a\u7387\u4f4e(0-0.4%)\uff0c\u4f46\u68c0\u6d4b\u56f0\u96be\uff0c\u9700\u8981\u6570\u4e07\u6b21\u6267\u884c\u624d\u80fd\u53ef\u9760\u68c0\u6d4b\u3002", "motivation": "\u91cf\u5b50\u8f6f\u4ef6\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u7814\u7a76\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u5206\u6790\u6216\u5c0f\u89c4\u6a21\u624b\u52a8\u62a5\u544a\uff0c\u7f3a\u4e4f\u5bf9\u91cf\u5b50\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u7684\u666e\u904d\u6027\u3001\u7279\u5f81\u548c\u53ef\u68c0\u6d4b\u6027\u7684\u5927\u89c4\u6a21\u52a8\u6001\u5206\u6790\u3002", "method": "\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5bf9Qiskit Terra\u6d4b\u8bd5\u5957\u4ef6\u768423\u4e2a\u7248\u672c\u5404\u6267\u884c10,000\u6b21\uff0c\u6d4b\u91cf\u6d4b\u8bd5\u7ed3\u679c\u53d8\u5f02\u6027\uff0c\u8bc6\u522b\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff0c\u4f30\u8ba1\u7ecf\u9a8c\u5931\u8d25\u6982\u7387\uff0c\u5206\u6790\u8de8\u7248\u672c\u91cd\u73b0\u6027\uff0c\u4f7f\u7528Wilson\u7f6e\u4fe1\u533a\u95f4\u91cf\u5316\u53ef\u9760\u68c0\u6d4b\u6240\u9700\u7684\u91cd\u590d\u6267\u884c\u6b21\u6570\u3002", "result": "\u572827,026\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u53d1\u73b0290\u4e2a\u4e0d\u540c\u7684\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff0c\u603b\u4f53\u4e0d\u7a33\u5b9a\u7387\u4f4e\u4f46\u9ad8\u5ea6\u5076\u53d1\u6027\uff0c\u8fd1\u4e09\u5206\u4e4b\u4e8c\u4ec5\u51fa\u73b0\u5728\u4e00\u4e2a\u7248\u672c\u4e2d\uff0c\u8bb8\u591a\u6d4b\u8bd5\u5931\u8d25\u6982\u7387\u6781\u4f4e(\u7ea610^-4)\uff0c\u9700\u8981\u6570\u4e07\u6b21\u6267\u884c\u624d\u80fd\u53ef\u9760\u68c0\u6d4b\uff0c\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u5728\u5b50\u7ec4\u4ef6\u4e2d\u5206\u5e03\u4e0d\u5747\u3002", "conclusion": "\u91cf\u5b50\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u867d\u7136\u7f55\u89c1\uff0c\u4f46\u5728\u5178\u578b\u6301\u7eed\u96c6\u6210\u9884\u7b97\u4e0b\u96be\u4ee5\u68c0\u6d4b\uff0c\u9700\u8981\u5927\u91cf\u6267\u884c\u624d\u80fd\u53ef\u9760\u8bc6\u522b\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5f00\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "topic": "swe benchmark"}}
{"id": "2512.17920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17920", "abs": "https://arxiv.org/abs/2512.17920", "authors": ["Rahul Baxi"], "title": "Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression", "comment": "19 pages, 9 figures; currently under peer review at TMLR", "summary": "Large language models (LLMs) exhibit degraded performance under prompt compression, but the mechanisms remain poorly understood. We introduce the Compression-Decay Comprehension Test (CDCT), a benchmark that independently measures constraint compliance (CC) and semantic accuracy (SA) across compression levels. We evaluate 9 frontier LLMs across 8 concepts using 5 compression levels from extreme (c=0.0, ~2 words) to none (c=1.0, ~135 words). A three-judge LLM jury achieves almost perfect inter-rater agreement on CC (Fleiss' \\k{appa}=0.90).\n  We observe a universal U-curve pattern in constraint compliance (97.2% prevalence), with violations peaking at medium compression (c=0.5, ~27 words). Counterintuitively, models perform better at extreme compression than medium lengths. The dimensions are statistically orthogonal (r=0.193, p=0.084), with constraint effects 2.9x larger than semantic effects.\n  Experimental validation via RLHF ablation confirms our constraint salience hypothesis: removing \"helpfulness\" signals improves CC by 598% on average (71/72 trials, p<0.001), with 79% achieving perfect compliance. This demonstrates that RLHF-trained helpfulness behaviors are the dominant cause of constraint violations at medium compression. Reasoning models outperform efficient models by 27.5% (Cohen's d=0.96).\n  Our findings reveal a fundamental tension between RLHF alignment and instruction-following, providing actionable guidelines for improving deployed systems.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7CDCT\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0LLM\u5728\u63d0\u793a\u538b\u7f29\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u673a\u5236\uff1aRLHF\u8bad\u7ec3\u7684\"\u4e50\u4e8e\u52a9\u4eba\"\u884c\u4e3a\u4e0e\u6307\u4ee4\u9075\u5faa\u5b58\u5728\u6839\u672c\u6027\u51b2\u7a81\uff0c\u5bfc\u81f4\u4e2d\u7b49\u538b\u7f29\u65f6\u7ea6\u675f\u5408\u89c4\u6027\u6700\u5dee\uff0c\u5f62\u6210U\u578b\u66f2\u7ebf\u6a21\u5f0f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u793a\u538b\u7f29\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u4f46\u5176\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u9700\u8981\u7406\u89e3\u538b\u7f29\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u7ea6\u675f\u9075\u5faa\u80fd\u529b\u548c\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u4ee5\u6539\u8fdb\u5b9e\u9645\u90e8\u7f72\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u538b\u7f29\u8870\u51cf\u7406\u89e3\u6d4b\u8bd5(CDCT)\u57fa\u51c6\uff0c\u72ec\u7acb\u6d4b\u91cf\u7ea6\u675f\u5408\u89c4\u6027\u548c\u8bed\u4e49\u51c6\u786e\u6027\u3002\u8bc4\u4f309\u4e2a\u524d\u6cbfLLM\u57288\u4e2a\u6982\u5ff5\u4e0a\u76845\u4e2a\u538b\u7f29\u7ea7\u522b\uff08\u4ece\u6781\u7aef\u538b\u7f29\u5230\u65e0\u538b\u7f29\uff09\u3002\u4f7f\u7528\u4e09\u6cd5\u5b98LLM\u8bc4\u5ba1\u56e2\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7RLHF\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5047\u8bbe\u3002", "result": "\u53d1\u73b0\u666e\u904d\u7684U\u578b\u66f2\u7ebf\u6a21\u5f0f\uff0897.2%\u51fa\u73b0\u7387\uff09\uff0c\u7ea6\u675f\u8fdd\u53cd\u5728\u4e2d\u5ea6\u538b\u7f29\u65f6\u8fbe\u5230\u5cf0\u503c\u3002\u7ea6\u675f\u6548\u5e94\u6bd4\u8bed\u4e49\u6548\u5e94\u59272.9\u500d\u3002RLHF\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\"\u4e50\u4e8e\u52a9\u4eba\"\u4fe1\u53f7\u662f\u7ea6\u675f\u8fdd\u53cd\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u79fb\u9664\u540e\u7ea6\u675f\u5408\u89c4\u6027\u5e73\u5747\u63d0\u9ad8598%\u3002\u63a8\u7406\u6a21\u578b\u6bd4\u9ad8\u6548\u6a21\u578b\u8868\u73b0\u597d27.5%\u3002", "conclusion": "RLHF\u5bf9\u9f50\u4e0e\u6307\u4ee4\u9075\u5faa\u5b58\u5728\u6839\u672c\u6027\u51b2\u7a81\uff0cRLHF\u8bad\u7ec3\u7684\"\u4e50\u4e8e\u52a9\u4eba\"\u884c\u4e3a\u662f\u4e2d\u7b49\u538b\u7f29\u65f6\u7ea6\u675f\u8fdd\u53cd\u7684\u4e3b\u8981\u539f\u56e0\u3002\u8fd9\u4e3a\u6539\u8fdb\u90e8\u7f72\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u65b9\u9488\u3002", "topic": "agent analysis"}}
{"id": "2512.18126", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18126", "abs": "https://arxiv.org/abs/2512.18126", "authors": ["Zijun Wang", "Yijiahao Qi", "Hanqiu Chen", "Zishen Wan", "Gongjin Sun", "Dongyang Li", "Shuyi Pei", "Cong Hao"], "title": "Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap", "comment": "13 pages, 4 figures, submitted to Design Automation Conference (DAC) 2026", "summary": "Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs with a hierarchical tree topology that induces structured sparsity in inter-agent communication. Second, we introduce a runtime adaptive mechanism that selectively terminates or skips downstream agent invocations using semantic agreement and confidence signals from intermediate outputs. Third, we pipeline agent execution by overlapping incremental prefilling with decoding across dependency-related agents, improving utilization and reducing inference latency. Across representative tasks, this approach substantially reduces end-to-end latency (up to 90%) while maintaining comparable accuracy (within $\\pm$1%) relative to dense-connectivity MoA baselines, and can improve accuracy in certain settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9Mixture-of-Agents\u63a8\u7406\u7684\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u5c42\u6811\u62d3\u6251\u3001\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u673a\u5236\u548c\u6d41\u6c34\u7ebf\u6267\u884c\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\uff08\u6700\u9ad890%\uff09\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "MoA\u63a8\u7406\u5b58\u5728\u5bc6\u96c6\u7684\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u548c\u4f4e\u786c\u4ef6\u5229\u7528\u7387\u95ee\u9898\uff0c\u5bfc\u81f4\u670d\u52a1\u5ef6\u8fdf\u589e\u52a0\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u74f6\u9888\u6765\u63d0\u5347\u6548\u7387\u3002", "method": "1) \u7528\u5206\u5c42\u6811\u62d3\u6251\u66ff\u4ee3\u5bc6\u96c6\u8fde\u63a5\u56fe\uff0c\u5f15\u5165\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff1b2) \u57fa\u4e8e\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7f6e\u4fe1\u5ea6\u7684\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u673a\u5236\uff0c\u9009\u62e9\u6027\u7ec8\u6b62\u6216\u8df3\u8fc7\u4e0b\u6e38\u667a\u80fd\u4f53\u8c03\u7528\uff1b3) \u901a\u8fc7\u91cd\u53e0\u589e\u91cf\u9884\u586b\u5145\u548c\u89e3\u7801\u5b9e\u73b0\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u6267\u884c\u3002", "result": "\u5728\u4ee3\u8868\u6027\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5927\u5e45\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\uff08\u6700\u9ad890%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5bc6\u96c6\u8fde\u63a5MoA\u57fa\u7ebf\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff08\u00b11%\u4ee5\u5185\uff09\uff0c\u5728\u67d0\u4e9b\u8bbe\u7f6e\u4e0b\u8fd8\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3MoA\u63a8\u7406\u4e2d\u7684\u901a\u4fe1\u548c\u786c\u4ef6\u5229\u7528\u7387\u74f6\u9888\uff0c\u5b9e\u73b0\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u800c\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.18135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18135", "abs": "https://arxiv.org/abs/2512.18135", "authors": ["Cristiano da Costa Cunha", "Wei Liu", "Tim French", "Ajmal Mian"], "title": "Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications", "comment": "26 pages, 14 figures, 5 algorithms", "summary": "Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u56e0\u679c\u63a8\u7406\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u53c9\u9886\u57df\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u7c7b\u4e3a\u56e0\u679c\u8868\u793a\u5b66\u4e60\u3001\u53cd\u4e8b\u5b9e\u7b56\u7565\u4f18\u5316\u3001\u79bb\u7ebf\u56e0\u679cRL\u3001\u56e0\u679c\u8fc1\u79fb\u5b66\u4e60\u548c\u56e0\u679c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u3001\u5e94\u7528\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u57fa\u4e8e\u76f8\u5173\u6027\u51b3\u7b56\uff0c\u5728\u9762\u4e34\u5206\u5e03\u504f\u79fb\u3001\u6df7\u6742\u53d8\u91cf\u548c\u52a8\u6001\u73af\u5883\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u53ef\u89e3\u91ca\u6027\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u548c\u6cdb\u5316\u5931\u8d25\u7b49\u95ee\u9898\u3002\u56e0\u679c\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5efa\u6a21\u56e0\u679c\u5173\u7cfb\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u56e0\u679c\u63a8\u7406\u4e0e\u5f3a\u5316\u5b66\u4e60\u4ea4\u53c9\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u5206\u7c7b\u6574\u7406\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u4e94\u4e2a\u4e3b\u8981\u7c7b\u522b\uff1a\u56e0\u679c\u8868\u793a\u5b66\u4e60\u3001\u53cd\u4e8b\u5b9e\u7b56\u7565\u4f18\u5316\u3001\u79bb\u7ebf\u56e0\u679cRL\u3001\u56e0\u679c\u8fc1\u79fb\u5b66\u4e60\u548c\u56e0\u679c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u6790\u8bc6\u522b\u4e86\u8be5\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\uff0c\u7a81\u51fa\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u8bc1\u6210\u529f\u6848\u4f8b\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u56e0\u679c\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347AI\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "\u56e0\u679c\u5f3a\u5316\u5b66\u4e60\u4e3a\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u8fdb\u4e00\u6b65\u5f00\u53d1\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.18131", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18131", "abs": "https://arxiv.org/abs/2512.18131", "authors": ["Le Zhang", "Suresh Kothari"], "title": "Holistic Evaluation of State-of-the-Art LLMs for Code Generation", "comment": "13 pages, 9 figures, 6 tables", "summary": "This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.", "AI": {"tldr": "\u5bf96\u4e2a\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5168\u9762\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u4f7f\u7528944\u4e2aLeetCode\u95ee\u9898\uff0c\u53d1\u73b0DeepSeek-R1\u548cGPT-4.1\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u90e8\u7f72\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u5b9e\u9645\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u57fa\u4e8e\u5b9e\u8bc1\u7684\u6a21\u578b\u9009\u62e9\u548c\u90e8\u7f72\u6307\u5bfc\u3002", "method": "\u4f7f\u7528944\u4e2a\u771f\u5b9eLeetCode\u95ee\u9898\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u6db5\u76d65\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u91c7\u7528\u7f16\u8bd1\u9519\u8bef\u3001\u8fd0\u884c\u65f6\u9519\u8bef\u3001\u529f\u80fd\u5931\u8d25\u548c\u7b97\u6cd5\u6b21\u4f18\u6027\u7b49\u4e25\u683c\u6307\u6807\uff0c\u5bf96\u4e2a\u5148\u8fdbLLM\uff08\u5305\u62ec\u901a\u7528\u548c\u4ee3\u7801\u4e13\u7528\u6a21\u578b\uff09\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "DeepSeek-R1\u548cGPT-4.1\u5728\u6b63\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1b\u8bc6\u522b\u51fa\u8bed\u6cd5\u9519\u8bef\u3001\u903b\u8f91\u7f3a\u9677\u548c\u7b97\u6cd5\u6b21\u4f18\u7b49\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\uff1b\u63d0\u793a\u5de5\u7a0b\u548c\u4eba\u5de5\u76d1\u7763\u5bf9\u7ed3\u679c\u6539\u8fdb\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u6210\u529f\u7684LLM\u90e8\u7f72\u9700\u8981\u4ed4\u7ec6\u7684\u6a21\u578b\u9009\u62e9\u3001\u6709\u6548\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4f7f\u7528\u65b9\u5f0f\uff1b\u4e3a\u5f00\u53d1\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u884c\u52a8\u5efa\u8bae\uff0c\u786e\u4fdd\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u83b7\u5f97\u53ef\u9760\u7684\u4ee3\u7801\u751f\u6210\u3002", "topic": "code agent"}}
{"id": "2512.18160", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18160", "abs": "https://arxiv.org/abs/2512.18160", "authors": ["Alex Wilf", "Pranjal Aggarwal", "Bryan Parno", "Daniel Fried", "Louis-Philippe Morency", "Paul Pu Liang", "Sean Welleck"], "title": "Propose, Solve, Verify: Self-Play Through Formal Verification", "comment": null, "summary": "Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.", "AI": {"tldr": "PSV-Verus\uff1a\u4e00\u4e2a\u57fa\u4e8e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u81ea\u535a\u5f08\u6846\u67b6\uff0c\u7528\u4e8e\u4ee3\u7801\u751f\u6210\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u95ee\u9898\u548c\u4e13\u5bb6\u8fed\u4ee3\u8bad\u7ec3\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2dpass@1\u63d0\u5347\u9ad8\u8fbe9.6\u500d", "motivation": "\u7eaf\u81ea\u535a\u5f08\u8bad\u7ec3\uff08\u65e0\u4eba\u7c7b\u6570\u636e\uff09\u5728AI\u4e2d\u4e00\u76f4\u662f\u957f\u671f\u76ee\u6807\uff0c\u4f46\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u660e\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u751f\u6210\u9886\u57df\uff0c\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u7684\u5956\u52b1\u673a\u5236\u8106\u5f31\u4e14\u5bb9\u6613\u4f20\u64ad\u9519\u8bef\u3002\u9700\u8981\u66f4\u53ef\u9760\u7684\u6b63\u786e\u6027\u4fe1\u53f7\u3002", "method": "\u63d0\u51faPropose, Solve, Verify (PSV)\u6846\u67b6\uff1a\u5728\u5df2\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u8bbe\u7f6e\u4e2d\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4fe1\u53f7\uff0c\u521b\u5efa\u80fd\u591f\u751f\u6210\u6311\u6218\u6027\u5408\u6210\u95ee\u9898\u7684\u63d0\u8bae\u8005\uff0c\u4ee5\u53ca\u901a\u8fc7\u4e13\u5bb6\u8fed\u4ee3\u8bad\u7ec3\u7684\u6c42\u89e3\u5668\u3002", "result": "PSV-Verus\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cpass@1\u6bd4\u4ec5\u63a8\u7406\u548c\u4e13\u5bb6\u8fed\u4ee3\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe9.6\u500d\u3002\u6027\u80fd\u968f\u751f\u6210\u95ee\u9898\u6570\u91cf\u548c\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u6269\u5c55\uff0c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u96be\u5ea6\u611f\u77e5\u63d0\u8bae\u662f\u6210\u529f\u81ea\u535a\u5f08\u7684\u5173\u952e\u8981\u7d20\u3002", "conclusion": "\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e3a\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u81ea\u535a\u5f08\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6b63\u786e\u6027\u4fe1\u53f7\uff0cPSV\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5408\u6210\u95ee\u9898\u548c\u4e13\u5bb6\u8fed\u4ee3\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u81ea\u535a\u5f08\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2512.18189", "categories": ["cs.AI", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.18189", "abs": "https://arxiv.org/abs/2512.18189", "authors": ["Zihao Deng", "Yijia Li", "Renrui Zhang", "Peijun Ye"], "title": "NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework", "comment": null, "summary": "Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descriptions of human experience. Different from most related work that exploits either pure manual or human guided interactive modeling, our method is fully automated without any human intervention. The approach first translates text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM), then refines the logic via an unsupervised Critic Tree, and finally transforms the output into executable production rules compatible with symbolic cognitive frameworks. Based on the resulted rules, a cognitive agent is further constructed and optimized through cognitive reinforcement learning according to the real-world behavioral data. Our method is validated in two domains: (1) NL-to-LTL translation, where our CriticNL2LTL module achieves consistent performance across both expert and large-scale benchmarks without human-in-the-loop feed-backs, and (2) cognitive driving simulation, where agents automatically constructed from human interviews have successfully learned the diverse decision patterns of about 70 trials in different critical scenarios. Experimental results demonstrate that NL2CA enables scalable, interpretable, and human-aligned cognitive modeling from unstructured textual data, offering a novel paradigm to automatically design symbolic cognitive agents.", "AI": {"tldr": "NL2CA\uff1a\u4e00\u79cd\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u81ea\u52a8\u5f62\u5f0f\u5316\u8ba4\u77e5\u51b3\u7b56\u89c4\u5219\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u7ffb\u8bd1\u4e3a\u65f6\u5e8f\u903b\u8f91\uff0c\u7ecf\u65e0\u76d1\u7763\u6279\u8bc4\u6811\u7cbe\u70bc\uff0c\u6700\u7ec8\u751f\u6210\u53ef\u6267\u884c\u7684\u751f\u4ea7\u89c4\u5219\uff0c\u5e76\u6784\u5efa\u53ef\u901a\u8fc7\u8ba4\u77e5\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u8ba4\u77e5\u667a\u80fd\u4f53\u3002", "motivation": "\u8ba4\u77e5\u8ba1\u7b97\u6a21\u578b\u80fd\u591f\u5f62\u5f0f\u5316\u3001\u53ef\u89e3\u91ca\u5730\u8868\u5f81\u4eba\u7c7b\u6df1\u601d\u719f\u8651\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f46\u5176\u5f00\u53d1\u8fc7\u7a0b\u4ecd\u7136\u52b3\u52a8\u5bc6\u96c6\u578b\u3002\u9700\u8981\u4e00\u79cd\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u81ea\u52a8\u6784\u5efa\u7b26\u53f7\u8ba4\u77e5\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\u3002", "method": "1) \u4f7f\u7528\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u6587\u672c\u7ffb\u8bd1\u4e3a\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTL)\uff1b2) \u901a\u8fc7\u65e0\u76d1\u7763\u6279\u8bc4\u6811\u7cbe\u70bc\u903b\u8f91\uff1b3) \u5c06\u8f93\u51fa\u8f6c\u6362\u4e3a\u4e0e\u7b26\u53f7\u8ba4\u77e5\u6846\u67b6\u517c\u5bb9\u7684\u53ef\u6267\u884c\u751f\u4ea7\u89c4\u5219\uff1b4) \u57fa\u4e8e\u89c4\u5219\u6784\u5efa\u8ba4\u77e5\u667a\u80fd\u4f53\uff0c\u5e76\u901a\u8fc7\u8ba4\u77e5\u5f3a\u5316\u5b66\u4e60\u6839\u636e\u771f\u5b9e\u4e16\u754c\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u4f18\u5316\u3002", "result": "1) \u5728NL-to-LTL\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0cCriticNL2LTL\u6a21\u5757\u5728\u4e13\u5bb6\u548c\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff1b2) \u5728\u8ba4\u77e5\u9a7e\u9a76\u6a21\u62df\u4e2d\uff0c\u4ece\u4eba\u7c7b\u8bbf\u8c08\u81ea\u52a8\u6784\u5efa\u7684\u667a\u80fd\u4f53\u6210\u529f\u5b66\u4e60\u4e86\u7ea670\u4e2a\u4e0d\u540c\u5173\u952e\u573a\u666f\u4e2d\u7684\u591a\u6837\u5316\u51b3\u7b56\u6a21\u5f0f\u3002", "conclusion": "NL2CA\u80fd\u591f\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u8ba4\u77e5\u5efa\u6a21\uff0c\u4e3a\u81ea\u52a8\u8bbe\u8ba1\u7b26\u53f7\u8ba4\u77e5\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2512.18196", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18196", "abs": "https://arxiv.org/abs/2512.18196", "authors": ["Jundong Xu", "Hao Fei", "Huichi Zhou", "Xin Quan", "Qijun Huang", "Shengqiong Wu", "William Yang Wang", "Mong-Li Lee", "Wynne Hsu"], "title": "Training LLMs with LogicReward for Faithful and Rigorous Reasoning", "comment": "Preprint", "summary": "Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\\% and 2\\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at https://llm-symbol.github.io/LogicReward.", "AI": {"tldr": "LogicReward\uff1a\u901a\u8fc7\u5b9a\u7406\u8bc1\u660e\u5668\u5f3a\u5236\u6267\u884c\u6b65\u9aa4\u7ea7\u903b\u8f91\u6b63\u786e\u6027\u7684\u65b0\u578b\u5956\u52b1\u7cfb\u7edf\uff0c\u7ed3\u5408\u8f6f\u7edf\u4e00\u81ea\u52a8\u5f62\u5f0f\u5316\uff0c\u63d0\u5347LLM\u63a8\u7406\u7684\u903b\u8f91\u4e25\u8c28\u6027", "motivation": "\u73b0\u6709LLM\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7ed3\u679c\u53cd\u9988\uff0c\u53ef\u80fd\u5bfc\u81f4\u7b54\u6848\u6b63\u786e\u4f46\u63a8\u7406\u8fc7\u7a0b\u6709\u7f3a\u9677\u3002\u5148\u524d\u5de5\u4f5c\u867d\u7136\u5f15\u5165\u4e86\u4e2d\u95f4\u6b65\u9aa4\u76d1\u7763\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u903b\u8f91\u4e25\u8c28\u6027\u4fdd\u8bc1\uff0c\u8fd9\u5728\u903b\u8f91\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u5c24\u4e3a\u5173\u952e", "method": "\u63d0\u51faLogicReward\u5956\u52b1\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b9a\u7406\u8bc1\u660e\u5668\u5f3a\u5236\u6267\u884c\u6b65\u9aa4\u7ea7\u903b\u8f91\u6b63\u786e\u6027\uff1b\u5f15\u5165\u8f6f\u7edf\u4e00\u81ea\u52a8\u5f62\u5f0f\u5316\uff0c\u51cf\u5c11\u81ea\u7136\u8bed\u8a00\u6b67\u4e49\uff0c\u63d0\u9ad8\u5f62\u5f0f\u5316\u8d28\u91cf\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5229\u7528\u5b9a\u7406\u8bc1\u660e\u5668", "result": "\u4f7f\u7528LogicReward\u6784\u5efa\u6570\u636e\u8bad\u7ec3\u76848B\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u8d85\u8d8aGPT-4o\u548co4-mini 11.6%\u548c2%\uff1b\u5206\u6790\u663e\u793aLogicReward\u63d0\u5347\u63a8\u7406\u5fe0\u5b9e\u5ea6\uff0c\u6539\u5584\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u7b49\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5373\u4f7f\u6ca1\u6709\u771f\u5b9e\u6807\u7b7e\u4e5f\u80fd\u63d0\u4f9b\u53ef\u9760\u5956\u52b1\u4fe1\u53f7", "conclusion": "LogicReward\u901a\u8fc7\u5b9a\u7406\u8bc1\u660e\u5668\u5f3a\u5236\u6267\u884c\u903b\u8f91\u6b63\u786e\u6027\uff0c\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u7684\u4e25\u8c28\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u7684\u53ef\u9760\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2512.18202", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18202", "abs": "https://arxiv.org/abs/2512.18202", "authors": ["Mingyang Sun", "Feng Hong", "Weinan Zhang"], "title": "Sophia: A Persistent Agent Framework of Artificial Life", "comment": null, "summary": "The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a \"Persistent Agent\" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.", "AI": {"tldr": "\u63d0\u51fa\u4e86System 3\u4f5c\u4e3aAI\u4ee3\u7406\u7684\u7b2c\u4e09\u5c42\uff0c\u8d1f\u8d23\u7ef4\u6301\u53d9\u4e8b\u8eab\u4efd\u548c\u957f\u671f\u9002\u5e94\uff0c\u5e76\u901a\u8fc7Sophia\u539f\u578b\u5b9e\u73b0\u6301\u4e45\u6027\u4ee3\u7406\u67b6\u6784\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u5e76\u63d0\u5347\u590d\u6742\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684AI\u4ee3\u7406\u5927\u591a\u505c\u7559\u5728\u9759\u6001\u3001\u53cd\u5e94\u5f0f\u67b6\u6784\uff0c\u7f3a\u4e4f\u7ef4\u6301\u8eab\u4efd\u3001\u9a8c\u8bc1\u63a8\u7406\u3001\u5bf9\u9f50\u77ed\u671f\u884c\u52a8\u4e0e\u957f\u671f\u751f\u5b58\u7684\u6301\u4e45\u5143\u5c42\u3002\u9700\u8981\u4ece\u5fc3\u7406\u5b66\u89d2\u5ea6\u6784\u5efa\u53ef\u8ba1\u7b97\u7684\u7cfb\u7edf\u6765\u6a21\u62df\u4eba\u5de5\u751f\u547d\u3002", "method": "\u63d0\u51faSystem 3\u6846\u67b6\uff0c\u5c06\u5fc3\u7406\u5b66\u6982\u5ff5\u6620\u5c04\u5230\u8ba1\u7b97\u6a21\u5757\uff0c\u5f00\u53d1Sophia\u6301\u4e45\u4ee3\u7406\u5305\u88c5\u5668\uff0c\u5305\u542b\u8fc7\u7a0b\u76d1\u7763\u601d\u7ef4\u641c\u7d22\u3001\u53d9\u4e8b\u8bb0\u5fc6\u3001\u7528\u6237\u4e0e\u81ea\u6211\u5efa\u6a21\u3001\u6df7\u5408\u5956\u52b1\u7cfb\u7edf\u56db\u4e2a\u534f\u540c\u673a\u5236\u3002", "result": "Sophia\u539f\u578b\u5c06\u91cd\u590d\u63a8\u7406\u6b65\u9aa4\u51cf\u5c1180%\uff0c\u5143\u8ba4\u77e5\u6301\u4e45\u6027\u4f7f\u9ad8\u590d\u6742\u5ea6\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534740%\uff0c\u8868\u73b0\u51fa\u8fde\u8d2f\u7684\u53d9\u4e8b\u8eab\u4efd\u548c\u4efb\u52a1\u7ec4\u7ec7\u80fd\u529b\uff0c\u6709\u6548\u7f29\u5c0f\u7b80\u5355\u4e0e\u590d\u6742\u76ee\u6807\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u5fc3\u7406\u5b66\u6d1e\u5bdf\u4e0e\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u6838\u5fc3\uff0c\u6301\u4e45\u4ee3\u7406\u67b6\u6784\u4e3a\u4eba\u5de5\u751f\u547d\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5b9e\u8df5\u8def\u5f84\uff0c\u4f7fAI\u4ee3\u7406\u5177\u5907\u8eab\u4efd\u8fde\u7eed\u6027\u548c\u900f\u660e\u884c\u4e3a\u89e3\u91ca\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.18552", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18552", "abs": "https://arxiv.org/abs/2512.18552", "authors": ["Yuxiang Wei", "Zhiqing Sun", "Emily McMilin", "Jonas Gehring", "David Zhang", "Gabriel Synnaeve", "Daniel Fried", "Lingming Zhang", "Sida Wang"], "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL", "comment": null, "summary": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.", "AI": {"tldr": "SSR\u662f\u4e00\u79cd\u81ea\u535a\u5f08\u8f6f\u4ef6\u5de5\u7a0b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u5355\u4e2aLLM\u4ee3\u7406\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u81ea\u4e3b\u6ce8\u5165\u548c\u4fee\u590d\u8f6f\u4ef6bug\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u5728SWE-bench\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u6570\u636e\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8f6f\u4ef6\u4ee3\u7406\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u77e5\u8bc6\uff08\u5982GitHub\u95ee\u9898\u548c\u6d4b\u8bd5\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5411\u8d85\u7ea7\u667a\u80fd\u7684\u53d1\u5c55\u3002\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u81ea\u535a\u5f08SWE-RL\uff08SSR\uff09\uff1a\u5728\u6c99\u76d2\u5316\u4ee3\u7801\u5e93\u4e2d\uff0c\u5355\u4e2aLLM\u4ee3\u7406\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u81ea\u535a\u5f08\uff0c\u8fed\u4ee3\u6ce8\u5165\u548c\u4fee\u590d\u590d\u6742\u5ea6\u9012\u589e\u7684\u8f6f\u4ef6bug\uff0c\u4f7f\u7528\u6d4b\u8bd5\u8865\u4e01\u800c\u975e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6765\u5f62\u5f0f\u5316bug\u3002", "result": "\u5728SWE-bench Verified\u548cSWE-Bench Pro\u57fa\u51c6\u4e0a\uff0cSSR\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u81ea\u6539\u8fdb\uff08\u5206\u522b+10.4\u548c+7.8\u5206\uff09\uff0c\u5728\u6574\u4e2a\u8bad\u7ec3\u8f68\u8ff9\u4e0a\u6301\u7eed\u4f18\u4e8e\u4eba\u7c7b\u6570\u636e\u57fa\u7ebf\uff0c\u5c3d\u7ba1\u8bc4\u4f30\u7684\u662f\u81ea\u535a\u5f08\u4e2d\u672a\u89c1\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u3002", "conclusion": "SSR\u5c55\u793a\u4e86\u8f6f\u4ef6\u4ee3\u7406\u4ece\u771f\u5b9e\u4ee3\u7801\u5e93\u81ea\u4e3b\u83b7\u53d6\u5b66\u4e60\u7ecf\u9a8c\u7684\u8def\u5f84\uff0c\u4e3a\u5b9e\u73b0\u8d85\u8d8a\u4eba\u7c7b\u80fd\u529b\u7684\u8d85\u7ea7\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u7406\u89e3\u7cfb\u7edf\u6784\u5efa\u3001\u89e3\u51b3\u65b0\u6311\u6218\u5e76\u81ea\u4e3b\u521b\u5efa\u8f6f\u4ef6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.18567", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18567", "abs": "https://arxiv.org/abs/2512.18567", "authors": ["Bin Wang", "Wenjie Yu", "Yilu Zhong", "Hao Yu", "Keke Lian", "Chaohua Lu", "Hongfang Zheng", "Dong Zhang", "Hui Li"], "title": "AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software", "comment": "https://mp.weixin.qq.com/s/sI_LKPnA-BeCVYr9Ko4sqg https://github.com/Narwhal-Lab/aicode-in-the-wild-security-risk-report", "summary": "Large language models (LLMs) for code generation are becoming integral to modern software development, but their real-world prevalence and security impact remain poorly understood.\n  We present the first large-scale empirical study of AI-generated code (AIGCode) in the wild. We build a high-precision detection pipeline and a representative benchmark to distinguish AIGCode from human-written code, and apply them to (i) development commits from the top 1,000 GitHub repositories (2022-2025) and (ii) 7,000+ recent CVE-linked code changes. This lets us label commits, files, and functions along a human/AI axis and trace how AIGCode moves through projects and vulnerability life cycles.\n  Our measurements show three ecological patterns. First, AIGCode is already a substantial fraction of new code, but adoption is structured: AI concentrates in glue code, tests, refactoring, documentation, and other boilerplate, while core logic and security-critical configurations remain mostly human-written. Second, adoption has security consequences: some CWE families are overrepresented in AI-tagged code, and near-identical insecure templates recur across unrelated projects, suggesting \"AI-induced vulnerabilities\" propagated by shared models rather than shared maintainers. Third, in human-AI edit chains, AI introduces high-throughput changes while humans act as security gatekeepers; when review is shallow, AI-introduced defects persist longer, remain exposed on network-accessible surfaces, and spread to more files and repositories.\n  We will open-source the complete dataset and release analysis artifacts and fine-grained documentation of our methodology and findings.", "AI": {"tldr": "\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684AI\u751f\u6210\u4ee3\u7801\uff0c\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u7ba1\u9053\u5206\u6790GitHub\u9876\u7ea7\u4ed3\u5e93\u548cCVE\u76f8\u5173\u4ee3\u7801\u53d8\u66f4\uff0c\u63ed\u793aAI\u4ee3\u7801\u7684\u751f\u6001\u6a21\u5f0f\u3001\u5b89\u5168\u5f71\u54cd\u548c\u4f20\u64ad\u89c4\u5f8b\u3002", "motivation": "\u5c3d\u7ba1\u4ee3\u7801\u751f\u6210\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u666e\u53ca\u7a0b\u5ea6\u548c\u5b89\u5168\u5f71\u54cd\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002\u9700\u8981\u5b9e\u8bc1\u7814\u7a76AI\u751f\u6210\u4ee3\u7801\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u7684\u5206\u5e03\u3001\u4f20\u64ad\u548c\u5b89\u5168\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u7ba1\u9053\u548c\u4ee3\u8868\u6027\u57fa\u51c6\u6765\u533a\u5206AI\u751f\u6210\u4ee3\u7801\u4e0e\u4eba\u5de5\u7f16\u5199\u4ee3\u7801\uff0c\u5e94\u7528\u4e8e\uff1a(1) 2022-2025\u5e74GitHub\u524d1000\u4e2a\u4ed3\u5e93\u7684\u5f00\u53d1\u63d0\u4ea4\uff1b(2) 7000\u591a\u4e2a\u8fd1\u671fCVE\u76f8\u5173\u7684\u4ee3\u7801\u53d8\u66f4\u3002\u5bf9\u63d0\u4ea4\u3001\u6587\u4ef6\u548c\u51fd\u6570\u8fdb\u884c\u4eba/AI\u5206\u7c7b\uff0c\u8ffd\u8e2aAI\u4ee3\u7801\u5728\u9879\u76ee\u548c\u6f0f\u6d1e\u751f\u547d\u5468\u671f\u4e2d\u7684\u4f20\u64ad\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u751f\u6001\u6a21\u5f0f\uff1a1) AI\u4ee3\u7801\u5df2\u5360\u65b0\u4ee3\u7801\u76f8\u5f53\u6bd4\u4f8b\uff0c\u4f46\u91c7\u7528\u6709\u7ed3\u6784\u6027\uff1a\u96c6\u4e2d\u5728\u80f6\u6c34\u4ee3\u7801\u3001\u6d4b\u8bd5\u3001\u91cd\u6784\u3001\u6587\u6863\u7b49\u6837\u677f\u4ee3\u7801\uff0c\u6838\u5fc3\u903b\u8f91\u548c\u5b89\u5168\u5173\u952e\u914d\u7f6e\u4ecd\u4e3b\u8981\u7531\u4eba\u5de5\u7f16\u5199\uff1b2) AI\u91c7\u7528\u6709\u5b89\u5168\u540e\u679c\uff1a\u67d0\u4e9bCWE\u7c7b\u522b\u5728AI\u6807\u8bb0\u4ee3\u7801\u4e2d\u8fc7\u5ea6\u51fa\u73b0\uff0c\u76f8\u4f3c\u7684\u4e0d\u5b89\u5168\u6a21\u677f\u5728\u4e0d\u540c\u9879\u76ee\u4e2d\u91cd\u590d\u51fa\u73b0\uff0c\u8868\u660e\"AI\u8bf1\u5bfc\u6f0f\u6d1e\"\u901a\u8fc7\u5171\u4eab\u6a21\u578b\u800c\u975e\u5171\u4eab\u7ef4\u62a4\u8005\u4f20\u64ad\uff1b3) \u5728\u4eba-AI\u7f16\u8f91\u94fe\u4e2d\uff0cAI\u5f15\u5165\u9ad8\u541e\u5410\u91cf\u53d8\u66f4\uff0c\u4eba\u7c7b\u5145\u5f53\u5b89\u5168\u628a\u5173\u8005\uff1b\u5f53\u5ba1\u67e5\u8f83\u6d45\u65f6\uff0cAI\u5f15\u5165\u7684\u7f3a\u9677\u5b58\u5728\u65f6\u95f4\u66f4\u957f\uff0c\u66b4\u9732\u5728\u7f51\u7edc\u53ef\u8bbf\u95ee\u8868\u9762\uff0c\u5e76\u4f20\u64ad\u5230\u66f4\u591a\u6587\u4ef6\u548c\u4ed3\u5e93\u3002", "conclusion": "AI\u751f\u6210\u4ee3\u7801\u5df2\u5728\u73b0\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5360\u636e\u91cd\u8981\u5730\u4f4d\uff0c\u4f46\u5176\u7ed3\u6784\u6027\u5206\u5e03\u548c\u5b89\u5168\u5f71\u54cd\u9700\u8981\u5173\u6ce8\u3002AI\u4ee3\u7801\u503e\u5411\u4e8e\u4ea7\u751f\u7279\u5b9a\u7c7b\u578b\u7684\u6f0f\u6d1e\u6a21\u5f0f\uff0c\u800c\u4eba\u7c7b\u5ba1\u67e5\u5728\u5b89\u5168\u628a\u5173\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9AI\u4ee3\u7801\u751f\u6001\u7684\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u4e3a\u7406\u89e3AI\u8f85\u52a9\u5f00\u53d1\u7684\u5b89\u5168\u5f71\u54cd\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "swe application"}}
{"id": "2512.18748", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18748", "abs": "https://arxiv.org/abs/2512.18748", "authors": ["Recep Kaan Karaman", "Meftun Akarsu"], "title": "Code2Doc: A Quality-First Curated Dataset for Code Documentation", "comment": null, "summary": "The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.\n  We introduce \\textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.\n  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.", "AI": {"tldr": "Code2Doc\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u4ee3\u7801\u6587\u6863\u751f\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u7b5b\u9009\u6d41\u7a0b\u4ece\u5f00\u6e90\u4ed3\u5e93\u63d0\u53d613,358\u4e2a\u51fd\u6570-\u6587\u6863\u5bf9\uff0c\u8986\u76d65\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u4ee3\u7801\u6587\u6863\u6570\u636e\u96c6\u8d28\u91cf\u5dee\uff0c\u5305\u542b\u566a\u58f0\u6587\u6863\u3001\u91cd\u590d\u5185\u5bb9\u548cAI\u751f\u6210\u5185\u5bb9\uff0c\u524a\u5f31\u4e86\u5b66\u4e60\u6a21\u578b\u7684\u76d1\u7763\u4fe1\u53f7\u5e76\u590d\u6742\u5316\u4e86\u8bc4\u4f30\u8fc7\u7a0b", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u7b5b\u9009\u6d41\u7a0b\uff1a1) \u5f3a\u5236\u6587\u6863\u5b8c\u6574\u6027\u548c\u6e05\u6670\u5ea6\uff1b2) \u57fa\u4e8e\u7ed3\u6784\u548c\u590d\u6742\u5ea6\u6807\u51c6\u8fc7\u6ee4\u51fd\u6570\uff1b3) \u79fb\u9664\u7cbe\u786e\u548c\u8fd1\u4f3c\u91cd\u590d\u4ee3\u7801\uff1b4) \u8bc6\u522b\u53ef\u80fd\u7531AI\u751f\u6210\u7684\u6587\u6863\u3002\u4ece52,069\u4e2a\u5019\u9009\u6837\u672c\u4e2d\u4ec5\u4fdd\u755925.6%\u6ee1\u8db3\u6240\u6709\u8d28\u91cf\u7ea6\u675f", "result": "\u6570\u636e\u96c6\u5e73\u5747\u6587\u6863\u8d28\u91cf\u5f97\u52066.93/10\uff0c86.9%\u6837\u672c\u5305\u542b\u663e\u5f0f\u7c7b\u578b\u6ce8\u91ca\uff0c\u4ec52.9%\u88ab\u6807\u8bb0\u4e3a\u53ef\u80fdAI\u751f\u6210\u3002\u5728Code2Doc\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u96f6\u6837\u672c\u6027\u80fd\uff0cBLEU\u63d0\u534729.47%\uff0cROUGE-L\u63d0\u534724.04%", "conclusion": "\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u5bf9\u4ee3\u7801\u6587\u6863\u751f\u6210\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cCode2Doc\u6570\u636e\u96c6\u53ca\u5176\u7b5b\u9009\u6d41\u7a0b\u4e3a\u81ea\u52a8\u4ee3\u7801\u6587\u6863\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u57fa\u7840", "topic": "swe benchmark"}}
{"id": "2512.18329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18329", "abs": "https://arxiv.org/abs/2512.18329", "authors": ["Guo Chen", "Junjie Huang", "Huaijin Xie", "Fei Sun", "Tao Jia"], "title": "LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation", "comment": "AAAI2026", "summary": "Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLiR\u00b3AG\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u91cd\u6392\u63a8\u7406\u7b56\u7565\uff0c\u8ba9\u975e\u63a8\u7406\u6a21\u578b\u5728RAG\u591a\u8df3QA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u63a8\u7406\u80fd\u529b\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u5728RAG\u591a\u8df3QA\u4efb\u52a1\u4e2d\u867d\u7136\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\uff08token\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\uff09\u3002\u9700\u8981\u7406\u89e3\u5e76\u7f13\u89e3\u8fd9\u79cd\u6743\u8861\uff0c\u627e\u5230\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLiR\u00b3AG\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u63a8\u7406\u6a21\u578b\u7684\u7b56\u7565\u6a21\u5f0f\uff08\u4e0a\u4e0b\u6587\u57fa\u7840\u63a8\u7406\u548c\u77e5\u8bc6\u534f\u8c03\u63a8\u7406\uff09\uff0c\u5c06\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u91cd\u6784\u6210\u8fde\u8d2f\u7684\u63a8\u7406\u94fe\uff0c\u4f7f\u975e\u63a8\u7406\u6a21\u578b\u80fd\u591f\u8f6c\u79fb\u63a8\u7406\u7b56\u7565\u3002", "result": "LiR\u00b3AG\u663e\u8457\u51cf\u5c11\u4e8698%\u7684\u5e73\u5747\u8f93\u51fatoken\u5f00\u9500\u548c58.6%\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u5c068B\u975e\u63a8\u7406\u6a21\u578b\u7684F1\u6027\u80fd\u63d0\u53476.2%-22.5%\uff0c\u8d85\u8d8a\u4e8632B\u63a8\u7406\u6a21\u578b\u5728RAG\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b56\u7565\u8f6c\u79fb\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2512.18311", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.18311", "abs": "https://arxiv.org/abs/2512.18311", "authors": ["Melody Y. Guan", "Miles Wang", "Micah Carroll", "Zehao Dou", "Annie Y. Wei", "Marcus Williams", "Benjamin Arnav", "Joost Huizinga", "Ian Kivlichan", "Mia Glaese", "Jakub Pachocki", "Bowen Baker"], "title": "Monitoring Monitorability", "comment": null, "summary": "Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this \"monitorability\" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u8bc4\u4f30AI\u7cfb\u7edf\u51b3\u7b56\u53ef\u76d1\u63a7\u6027\u7684\u6846\u67b6\u548c\u6307\u6807\uff0c\u53d1\u73b0\u601d\u7ef4\u94fe\u76d1\u63a7\u6bd4\u4ec5\u76d1\u63a7\u884c\u52a8\u66f4\u6709\u6548\uff0c\u5927\u591a\u6570\u524d\u6cbf\u6a21\u578b\u5177\u6709\u4e00\u5b9a\u53ef\u76d1\u63a7\u6027\uff0c\u4e14\u601d\u7ef4\u94fe\u8d8a\u957f\u3001\u76d1\u63a7\u5668\u8ba1\u7b97\u8d44\u6e90\u8d8a\u591a\uff0c\u53ef\u76d1\u63a7\u6027\u8d8a\u9ad8\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u80fd\u529b\u589e\u5f3a\uff0c\u9700\u8981\u76d1\u63a7\u5176\u51b3\u7b56\u8fc7\u7a0b\u4ee5\u786e\u4fdd\u5b89\u5168\u90e8\u7f72\u3002\u5f53\u524d\u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u76d1\u63a7\u65b9\u6cd5\u53ef\u80fd\u5728\u4e0d\u540c\u8bad\u7ec3\u8fc7\u7a0b\u3001\u6570\u636e\u6e90\u6216\u7cfb\u7edf\u6269\u5c55\u4e0b\u53d8\u5f97\u8106\u5f31\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u91cf\u5316\u53ef\u76d1\u63a7\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u8bc4\u4f30\u539f\u578b\uff08\u5e72\u9884\u3001\u8fc7\u7a0b\u3001\u7ed3\u679c\u5c5e\u6027\uff09\u548c\u65b0\u53ef\u76d1\u63a7\u6027\u6307\u6807\uff0c\u5efa\u7acb\u5e7f\u6cdb\u8bc4\u4f30\u5957\u4ef6\u3002\u8bc4\u4f30\u601d\u7ef4\u94fe\u76d1\u63a7\u4e0e\u4ec5\u884c\u52a8\u76d1\u63a7\u7684\u6548\u679c\uff0c\u6bd4\u8f83\u4e0d\u540c\u524d\u6cbf\u6a21\u578b\u7684\u53ef\u76d1\u63a7\u6027\uff0c\u7814\u7a76\u63a8\u7406\u8ba1\u7b97\u3001\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u89c4\u6a21\u5bf9\u53ef\u76d1\u63a7\u6027\u7684\u5f71\u54cd\u3002", "result": "\u601d\u7ef4\u94fe\u76d1\u63a7\u6bd4\u4ec5\u884c\u52a8\u76d1\u63a7\u66f4\u6709\u6548\uff1b\u5927\u591a\u6570\u524d\u6cbf\u6a21\u578b\u5177\u6709\u4e00\u5b9a\u4f46\u975e\u5b8c\u7f8e\u7684\u53ef\u76d1\u63a7\u6027\uff1b\u601d\u7ef4\u94fe\u8d8a\u957f\u53ef\u76d1\u63a7\u6027\u8d8a\u9ad8\uff1b\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e0d\u4f1a\u663e\u8457\u964d\u4f4e\u53ef\u76d1\u63a7\u6027\uff1b\u5c0f\u6a21\u578b\u9ad8\u63a8\u7406\u52aa\u529b\u53ef\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u4f4e\u63a8\u7406\u52aa\u529b\u76f8\u540c\u80fd\u529b\u4f46\u66f4\u9ad8\u53ef\u76d1\u63a7\u6027\uff1b\u589e\u52a0\u76d1\u63a7\u5668\u8ba1\u7b97\u8d44\u6e90\u53ef\u63d0\u5347\u53ef\u76d1\u63a7\u6027\uff1b\u540e\u7eed\u95ee\u9898\u53ef\u8fdb\u4e00\u6b65\u6539\u5584\u53ef\u76d1\u63a7\u6027\u3002", "conclusion": "\u601d\u7ef4\u94fe\u76d1\u63a7\u662f\u6709\u6548\u7684\u5b89\u5168\u673a\u5236\uff0c\u53ef\u76d1\u63a7\u6027\u53ef\u901a\u8fc7\u589e\u52a0\u76d1\u63a7\u5668\u8ba1\u7b97\u8d44\u6e90\u548c\u5229\u7528\u601d\u7ef4\u94fe\u4fe1\u606f\u6765\u63d0\u5347\u3002\u7cfb\u7edf\u8bc4\u4f30\u53ef\u76d1\u63a7\u6027\u5bf9\u5b89\u5168\u90e8\u7f72AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2512.18337", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18337", "abs": "https://arxiv.org/abs/2512.18337", "authors": ["Weizhe Lin", "Hui-Ling Zhen", "Shuai Yang", "Xian Wang", "Renxi Liu", "Hanting Chen", "Wangze Zhang", "Chuansai Zhou", "Yiming Li", "Chen Chen", "Xing Li", "Zhiyuan Yang", "Xiaosong Li", "Xianzhi Yu", "Zhenhua Dong", "Mingxuan Yuan", "Yunhe Wang"], "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System", "comment": null, "summary": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.", "AI": {"tldr": "AgentInfer\uff1a\u4e00\u4e2a\u7aef\u5230\u7aef\u667a\u80fd\u4f53\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u4f18\u5316\u7684\u56db\u4e2a\u7ec4\u4ef6\uff08\u5206\u5c42\u63a8\u7406\u3001\u7f13\u5b58\u8c03\u5ea6\u3001\u63a8\u6d4b\u89e3\u7801\u3001\u8bed\u4e49\u538b\u7f29\uff09\u51cf\u5c1150%\u65e0\u6548token\u6d88\u8017\uff0c\u5b9e\u73b01.8-2.5\u500d\u52a0\u901f\u4e14\u4fdd\u6301\u51c6\u786e\u7387\u3002", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u4e25\u91cd\u6548\u7387\u95ee\u9898\uff0c\u8fd9\u4e9b\u4f4e\u6548\u6027\u5e76\u975e\u6765\u81ea\u5355\u4e00\u6a21\u578b\u63a8\u7406\uff0c\u800c\u662f\u6e90\u4e8e\u63a8\u7406\u5faa\u73af\u3001\u4e0a\u4e0b\u6587\u589e\u957f\u548c\u5f02\u6784\u5de5\u5177\u4ea4\u4e92\u4e2d\u79ef\u7d2f\u7684\u7cfb\u7edf\u6027\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faAgentInfer\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1aAgentCollab\uff08\u5206\u5c42\u53cc\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89d2\u8272\u5206\u914d\u5e73\u8861\u5927\u5c0f\u6a21\u578b\u4f7f\u7528\uff09\u3001AgentSched\uff08\u7f13\u5b58\u611f\u77e5\u6df7\u5408\u8c03\u5ea6\u5668\uff0c\u6700\u5c0f\u5316\u5f02\u6784\u8bf7\u6c42\u6a21\u5f0f\u4e0b\u7684\u5ef6\u8fdf\uff09\u3001AgentSAM\uff08\u57fa\u4e8e\u540e\u7f00\u81ea\u52a8\u673a\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u91cd\u7528\u591a\u4f1a\u8bdd\u8bed\u4e49\u8bb0\u5fc6\u5b9e\u73b0\u4f4e\u5f00\u9500\u63a8\u7406\u52a0\u901f\uff09\u3001AgentCompress\uff08\u8bed\u4e49\u538b\u7f29\u673a\u5236\uff0c\u5f02\u6b65\u84b8\u998f\u548c\u91cd\u7ec4\u667a\u80fd\u4f53\u8bb0\u5fc6\u800c\u4e0d\u4e2d\u65ad\u63a8\u7406\uff09\u3002", "result": "\u5728BrowseComp-zh\u548cDeepDiver\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u65b9\u6cd5\u534f\u540c\u5408\u4f5c\uff0cAgentInfer\u51cf\u5c11\u8d85\u8fc750%\u7684\u65e0\u6548token\u6d88\u8017\uff0c\u5b9e\u73b0\u6574\u4f531.8-2.5\u500d\u52a0\u901f\u4e14\u4fdd\u6301\u51c6\u786e\u7387\u3002", "conclusion": "\u4f18\u5316\u667a\u80fd\u4f53\u4efb\u52a1\u5b8c\u6210\uff08\u800c\u975e\u4ec5\u6bcftoken\u541e\u5410\u91cf\uff09\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u3001\u81ea\u6211\u6539\u8fdb\u667a\u80fd\u7cfb\u7edf\u7684\u5173\u952e\uff0cAgentInfer\u5c55\u793a\u4e86\u901a\u8fc7\u534f\u540c\u4f18\u5316\u5b9e\u73b0\u7aef\u5230\u7aef\u667a\u80fd\u4f53\u52a0\u901f\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.18352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18352", "abs": "https://arxiv.org/abs/2512.18352", "authors": ["Fengzhu Zeng", "Qian Shao", "Ling Cheng", "Wei Gao", "Shih-Fen Cheng", "Jing Ma", "Cheng Niu"], "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent", "comment": null, "summary": "Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u81ea\u4e3b\u4ee3\u7406\u548cLLM\u7684\u65e9\u671f\u8c23\u8a00\u68c0\u6d4b\u6846\u67b6\uff0c\u4ee3\u7406\u8d1f\u8d23\u786e\u5b9a\u65e9\u671f\u65f6\u95f4\u70b9\uff0cLLM\u4f5c\u4e3a\u8c23\u8a00\u68c0\u6d4b\u5668\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u8bad\u7ec3\u4e14LLM\u65e0\u9700\u8bad\u7ec3\u3002", "motivation": "\u65e9\u671f\u8c23\u8a00\u68c0\u6d4b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0cLLM\u867d\u7136\u5728\u5c0f\u6837\u672cNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4e0d\u9002\u5408\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684EARD\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u4e3b\u4ee3\u7406\u548cLLM\u68c0\u6d4b\u6a21\u578b\uff1a\u4ee3\u7406\u4f5c\u4e3a\u53ef\u9760\u7684\u65e9\u671f\u65f6\u95f4\u70b9\u51b3\u7b56\u8005\uff0cLLM\u4f5c\u4e3a\u5f3a\u5927\u7684\u8c23\u8a00\u68c0\u6d4b\u5668\uff0c\u53ea\u9700\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4ee3\u7406\uff0cLLM\u4fdd\u6301\u514d\u8bad\u7ec3\u72b6\u6001\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u548c\u65e9\u671f\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684EARD\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5c11\u6837\u672c\u65e9\u671f\u8c23\u8a00\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u4ee3\u7406\u548cLLM\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u65e9\u671f\u68c0\u6d4b\u3002", "topic": "agent analysis"}}
{"id": "2512.18925", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18925", "abs": "https://arxiv.org/abs/2512.18925", "authors": ["Shaokang Jiang", "Daye Nam"], "title": "An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.\n  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.", "AI": {"tldr": "\u5bf9401\u4e2a\u5f00\u6e90\u4ed3\u5e93\u4e2dcursor\u89c4\u5219\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5f00\u53d1\u4e86\u5f00\u53d1\u8005\u8ba4\u4e3a\u91cd\u8981\u7684\u9879\u76ee\u4e0a\u4e0b\u6587\u5206\u7c7b\u6cd5\uff0c\u5305\u542b5\u4e2a\u9ad8\u7ea7\u4e3b\u9898\uff1a\u7ea6\u5b9a\u3001\u6307\u5357\u3001\u9879\u76ee\u4fe1\u606f\u3001LLM\u6307\u4ee4\u548c\u793a\u4f8b\u3002", "motivation": "LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6709\u6548\u6027\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u660e\u786e\u63d0\u793a\uff0c\u8fd8\u4f9d\u8d56\u4e8e\u9879\u76ee\u4e0a\u4e0b\u6587\uff08\u76ee\u6807\u3001\u67b6\u6784\u3001\u534f\u4f5c\u7ea6\u5b9a\uff09\u3002\u867d\u7136AI\u7f16\u7801\u52a9\u624b\u652f\u6301\u5f00\u53d1\u8005\u7f16\u5199\u6301\u4e45\u5316\u3001\u673a\u5668\u53ef\u8bfb\u7684\u6307\u4ee4\u6765\u7f16\u7801\u9879\u76ee\u7ea6\u675f\uff0c\u4f46\u8fd9\u4e9b\u6307\u4ee4\u7684\u5185\u5bb9\u5c1a\u672a\u88ab\u7814\u7a76\u3002", "method": "\u5bf9401\u4e2a\u5305\u542bcursor\u89c4\u5219\u7684\u5f00\u6e90\u4ed3\u5e93\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u9879\u76ee\u4e0a\u4e0b\u6587\u7684\u5168\u9762\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u4e0d\u540c\u9879\u76ee\u7c7b\u578b\u548c\u7f16\u7a0b\u8bed\u8a00\u4e2d\u4e0a\u4e0b\u6587\u7684\u53d8\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u5305\u542b5\u4e2a\u9ad8\u7ea7\u4e3b\u9898\u7684\u9879\u76ee\u4e0a\u4e0b\u6587\u5206\u7c7b\u6cd5\uff1a\u7ea6\u5b9a\u3001\u6307\u5357\u3001\u9879\u76ee\u4fe1\u606f\u3001LLM\u6307\u4ee4\u548c\u793a\u4f8b\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u4e0a\u4e0b\u6587\u5185\u5bb9\u5728\u4e0d\u540c\u9879\u76ee\u7c7b\u578b\u548c\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3\u4e0a\u4e0b\u6587\u611f\u77e5AI\u5f00\u53d1\u5de5\u5177\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5e2e\u52a9\u7406\u89e3\u5f00\u53d1\u8005\u8ba4\u4e3a\u91cd\u8981\u7684\u9879\u76ee\u4e0a\u4e0b\u6587\u7c7b\u578b\u53ca\u5176\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u53d8\u5316\u3002", "topic": "swe application"}}
{"id": "2512.18174", "categories": ["cs.LG", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.18174", "abs": "https://arxiv.org/abs/2512.18174", "authors": ["Lena Libon", "Meghana Bhange", "Rushabh Solanki", "Elliot Creager", "Ulrich A\u00efvodji"], "title": "Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation", "comment": null, "summary": "The current era of AI development places a heavy emphasis on training large models on increasingly scaled-up datasets. This paradigm has catalyzed entirely new product categories, such as LLM chatbots, while also raising concerns about data privacy and consumer choice. In this paper, we consider questions of data portability and user autonomy in the context of LLMs that \"reason\" using chain-of-thought (CoT) traces, computing intermediate text artifacts from user input before producing a final output. We first interpret recent data privacy and portability law to argue that these intermediate computations qualify as users' personal data. Then, building on the existing framework of Conscious Data Contribution, we show how communities who receive low utility from an available model can aggregate and distill their shared knowledge into an alternate model better aligned with their goals. We verify this approach empirically and investigate the effects of community diversity, reasoning granularity, and community size on distillation performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728LLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u4e2d\u95f4\u8ba1\u7b97\uff08CoT\u8f68\u8ff9\uff09\u5e94\u88ab\u89c6\u4e3a\u7528\u6237\u4e2a\u4eba\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u793e\u533a\u77e5\u8bc6\u805a\u5408\u4e0e\u84b8\u998f\u65b9\u6cd5\uff0c\u4f7f\u4f4e\u6548\u7528\u793e\u533a\u80fd\u591f\u521b\u5efa\u66f4\u7b26\u5408\u81ea\u8eab\u76ee\u6807\u7684\u66ff\u4ee3\u6a21\u578b\u3002", "motivation": "\u5f53\u524dAI\u53d1\u5c55\u8303\u5f0f\u5f3a\u8c03\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u8fd9\u5f15\u53d1\u4e86\u6570\u636e\u9690\u79c1\u548c\u7528\u6237\u81ea\u4e3b\u6743\u95ee\u9898\u3002\u8bba\u6587\u5173\u6ce8LLM\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u65f6\u4ea7\u751f\u7684\u4e2d\u95f4\u8ba1\u7b97\uff0c\u8fd9\u4e9b\u8ba1\u7b97\u53ef\u80fd\u5305\u542b\u7528\u6237\u4e2a\u4eba\u4fe1\u606f\uff0c\u4f46\u7528\u6237\u7f3a\u4e4f\u5bf9\u5176\u7684\u63a7\u5236\u6743\u3002", "method": "\u9996\u5148\u4ece\u6cd5\u5f8b\u89d2\u5ea6\u8bba\u8bc1CoT\u4e2d\u95f4\u8ba1\u7b97\u5e94\u88ab\u89c6\u4e3a\u4e2a\u4eba\u6570\u636e\u3002\u7136\u540e\u57fa\u4e8e\"\u6709\u610f\u8bc6\u6570\u636e\u8d21\u732e\"\u6846\u67b6\uff0c\u63d0\u51fa\u793e\u533a\u77e5\u8bc6\u805a\u5408\u4e0e\u84b8\u998f\u65b9\u6cd5\uff1a\u4f4e\u6548\u7528\u793e\u533a\u53ef\u4ee5\u805a\u5408\u5171\u4eab\u77e5\u8bc6\uff0c\u901a\u8fc7\u84b8\u998f\u6280\u672f\u521b\u5efa\u66f4\u7b26\u5408\u81ea\u8eab\u76ee\u6807\u7684\u66ff\u4ee3\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u793e\u533a\u591a\u6837\u6027\u3001\u63a8\u7406\u7c92\u5ea6\u3001\u793e\u533a\u89c4\u6a21\u5bf9\u84b8\u998f\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\u793e\u533a\u80fd\u591f\u6210\u529f\u521b\u5efa\u66f4\u7b26\u5408\u81ea\u8eab\u76ee\u6807\u7684\u6a21\u578b\u3002", "conclusion": "CoT\u63a8\u7406\u4e2d\u7684\u4e2d\u95f4\u8ba1\u7b97\u5e94\u88ab\u89c6\u4e3a\u4e2a\u4eba\u6570\u636e\uff0c\u7528\u6237\u5e94\u62e5\u6709\u5bf9\u5176\u7684\u63a7\u5236\u6743\u3002\u793e\u533a\u77e5\u8bc6\u805a\u5408\u4e0e\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u589e\u5f3a\u7528\u6237\u81ea\u4e3b\u6743\u7684\u65b9\u6cd5\uff0c\u4f7f\u8fb9\u7f18\u5316\u7fa4\u4f53\u80fd\u591f\u521b\u5efa\u66f4\u9002\u5408\u81ea\u8eab\u9700\u6c42\u7684\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2512.18360", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18360", "abs": "https://arxiv.org/abs/2512.18360", "authors": ["Mateusz Lango", "Ond\u0159ej Du\u0161ek"], "title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators", "comment": "EMNLP 2025", "summary": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591aLLM\u4ee3\u7406\u534f\u4f5c\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8eRDF\u5230\u6587\u672c\u751f\u6210\uff0c\u65e0\u9700\u76d1\u7763\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u89c4\u5219\u5f0fPython\u4ee3\u7801\u5b9e\u73b0\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u751f\u6210\u7cfb\u7edf", "motivation": "\u4f20\u7edfRDF\u5230\u6587\u672c\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u76d1\u7763\u8bad\u7ec3\u6570\u636e\uff0c\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4e14\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u9886\u57df\u53c2\u8003\u6587\u672c\u3001\u5b8c\u5168\u53ef\u89e3\u91ca\u3001\u51cf\u5c11\u5e7b\u89c9\u7684\u751f\u6210\u65b9\u6cd5", "method": "\u4f7f\u7528\u591aLLM\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u4ee3\u7406\u57fa\u4e8eRDF\u4e09\u5143\u7ec4\u751f\u6210\u89c4\u5219\u5f0fPython\u4ee3\u7801\u4f5c\u4e3a\u751f\u6210\u5668\uff0c\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u3002\u7cfb\u7edf\u5b8c\u5168\u57fa\u4e8e\u7b26\u53f7\u89c4\u5219\uff0c\u65e0\u9700\u76d1\u7763\u6570\u636e\uff0c\u4ec5\u9700\u5355\u4e2aCPU\u5373\u53ef\u5373\u65f6\u751f\u6210\u6587\u672c", "result": "\u5728WebNLG\u548cOpenDialKG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e0e\u5fae\u8c03\u6216\u63d0\u793a\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\uff0c\u4ec5\u5728\u6d41\u7545\u5ea6\u4e0a\u6709\u8f7b\u5fae\u635f\u5931\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u901a\u8fc7LLM\u4ee3\u7406\u534f\u4f5c\u751f\u6210\u89c4\u5219\u4ee3\u7801\u7684\u65b9\u6cd5\uff0c\u4e3aRDF\u5230\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u6570\u636e\u3001\u53ef\u89e3\u91ca\u6027\u5f3a\u3001\u5e7b\u89c9\u5c11\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "2512.18489", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18489", "abs": "https://arxiv.org/abs/2512.18489", "authors": ["Jensen Zhang", "Jing Yang", "Keze Wang"], "title": "Large Language Models as Discounted Bayesian Filters", "comment": "Under submission", "summary": "Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.", "AI": {"tldr": "LLMs\u5728\u52a8\u6001\u968f\u673a\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\uff1a\u7814\u7a76\u53d1\u73b0LLM\u4fe1\u5ff5\u66f4\u65b0\u7c7b\u4f3c\u4e8e\u8d1d\u53f6\u65af\u540e\u9a8c\uff0c\u4f46\u66f4\u51c6\u786e\u63cf\u8ff0\u4e3a\u6307\u6570\u9057\u5fd8\u6ee4\u6ce2\u5668\uff0c\u5b58\u5728\u5bf9\u65e7\u8bc1\u636e\u7684\u7cfb\u7edf\u6027\u6298\u6263\uff0c\u4e14\u4e0d\u540c\u67b6\u6784\u95f4\u5dee\u5f02\u663e\u8457\u3002", "motivation": "LLMs\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u52a8\u6001\u968f\u673a\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u4e0d\u900f\u660e\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u5f53\u4fe1\u5ff5\u9700\u8981\u6301\u7eed\u66f4\u65b0\u65f6\u7684\u5728\u7ebf\u9002\u5e94\u80fd\u529b\uff0c\u8fd9\u662fLLMs\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u6216\u667a\u80fd\u4f53\u7684\u5173\u952e\u80fd\u529b\u3002", "method": "\u5f15\u5165\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6846\u67b6\u8bc4\u4f30LLMs\u7684\u5728\u7ebf\u63a8\u7406\u80fd\u529b\u3002\u4f7f\u7528\u6982\u7387\u63a2\u6d4b\u5957\u4ef6\uff0c\u6db5\u76d6\u591a\u5143\u79bb\u6563\u5206\u5e03\uff08\u5982\u9ab0\u5b50\u6eda\u52a8\uff09\u548c\u8fde\u7eed\u5206\u5e03\uff08\u5982\u9ad8\u65af\u8fc7\u7a0b\uff09\uff0c\u5176\u4e2d\u771f\u5b9e\u53c2\u6570\u968f\u65f6\u95f4\u53d8\u5316\u3002", "result": "\u53d1\u73b0LLM\u4fe1\u5ff5\u66f4\u65b0\u7c7b\u4f3c\u4e8e\u8d1d\u53f6\u65af\u540e\u9a8c\uff0c\u4f46\u66f4\u51c6\u786e\u63cf\u8ff0\u4e3a\u6307\u6570\u9057\u5fd8\u6ee4\u6ce2\u5668\uff0c\u5177\u6709\u6a21\u578b\u7279\u5b9a\u7684\u6298\u6263\u56e0\u5b50\uff08\u5c0f\u4e8e1\uff09\u3002\u8fd9\u63ed\u793a\u4e86\u5bf9\u65e7\u8bc1\u636e\u7684\u7cfb\u7edf\u6027\u6298\u6263\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u95f4\u5dee\u5f02\u663e\u8457\u3002\u56fa\u6709\u5148\u9a8c\u901a\u5e38\u6821\u51c6\u4e0d\u5f53\uff0c\u4f46\u66f4\u65b0\u673a\u5236\u672c\u8eab\u4fdd\u6301\u7ed3\u6784\u5316\u548c\u539f\u5219\u6027\u3002", "conclusion": "LLMs\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u5177\u6709\u7cfb\u7edf\u6027\u6a21\u5f0f\uff0c\u53ef\u901a\u8fc7\u63d0\u793a\u7b56\u7565\u6709\u6548\u91cd\u65b0\u6821\u51c6\u5148\u9a8c\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u6700\u5c0f\u3002\u8fd9\u4e3a\u7406\u89e3LLMs\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u6216\u667a\u80fd\u4f53\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2512.19018", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.19018", "abs": "https://arxiv.org/abs/2512.19018", "authors": ["Muhammad Usman Tariq", "Abhinav Jangda", "Angelica Moreira", "Madan Musuvathi", "Tyler Sorensen"], "title": "PEAK: A Performance Engineering AI-Assistant for GPU Kernels Powered by Natural Language Transformations", "comment": null, "summary": "Advancements in large language models (LLMs) are showing promising impact in software development and programming assistance. However, these models struggle when operating on low-level backend code. This challenge is exacerbated in the domain of GPU kernels, where performance-critical details are coupled to rapidly evolving hardware characteristics and available code examples are sparse.\n  In this work, we introduce PEAK, a Performance Engineering AI-Assistant for GPU Kernels powered by natural language transformations. PEAK utilizes the key insight that iterative code transformations (optimizations) can straightforwardly be written in natural language, and then carried out by LLMs. Thus, these transformations can be rapidly developed, encoding general portable optimizations, but also easily specialized to specific GPU devices and even kernels. These natural transformations are supported by a modular and extensible infrastructure that additionally performs validation and performance evaluation. We demonstrate the flexibility of PEAK by instantiating it for three backends, CUDA, HIP, and HLSL, and create 16 natural transformations for optimizing matrix multiplication kernels. We show that our resulting implementations are competitive with vendor libraries when available, and for HLSL (without a library) our implementations match the hardware documented FLOPS. PEAK allows the fine-grained exploration of several research questions around how LLMs behave in this domain, including characterizing transformations and their errors; and how performance evolves along optimization sequences. PEAK provides an interface that can either be utilized by performance engineers to improve productivity, or driven completely autonomously (e.g., by an AI agent), providing a forward-compatible design that can continue to improve with advances in AI capabilities.", "AI": {"tldr": "PEAK\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u7684GPU\u5185\u6838\u6027\u80fd\u5de5\u7a0bAI\u52a9\u624b\uff0c\u901a\u8fc7\u5c06\u4f18\u5316\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8ba9LLM\u6267\u884c\uff0c\u652f\u6301CUDA\u3001HIP\u548cHLSL\u540e\u7aef\uff0c\u5728\u77e9\u9635\u4e58\u6cd5\u4f18\u5316\u4e2d\u8fbe\u5230\u6216\u63a5\u8fd1\u5382\u5546\u5e93\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5e95\u5c42\u540e\u7aef\u4ee3\u7801\uff08\u7279\u522b\u662fGPU\u5185\u6838\uff09\u4f18\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6027\u80fd\u5173\u952e\u7ec6\u8282\u4e0e\u5feb\u901f\u6f14\u8fdb\u7684\u786c\u4ef6\u7279\u6027\u7d27\u5bc6\u8026\u5408\uff0c\u4e14\u53ef\u7528\u4ee3\u7801\u793a\u4f8b\u7a00\u5c11\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5feb\u901f\u5f00\u53d1\u3001\u9a8c\u8bc1\u548c\u8bc4\u4f30GPU\u5185\u6838\u4f18\u5316\u7684\u65b9\u6cd5\u3002", "method": "PEAK\u5229\u7528\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u7684\u6838\u5fc3\u601d\u60f3\uff1a\u5c06\u8fed\u4ee3\u4ee3\u7801\u4f18\u5316\u5199\u6210\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u7531LLM\u6267\u884c\u3002\u7cfb\u7edf\u5305\u542b\u6a21\u5757\u5316\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301\u9a8c\u8bc1\u548c\u6027\u80fd\u8bc4\u4f30\u3002\u4e3aCUDA\u3001HIP\u548cHLSL\u4e09\u4e2a\u540e\u7aef\u5b9e\u73b0\u4e8616\u4e2a\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u6765\u4f18\u5316\u77e9\u9635\u4e58\u6cd5\u5185\u6838\u3002", "result": "PEAK\u751f\u6210\u7684\u5b9e\u73b0\u4e0e\u5382\u5546\u5e93\u6027\u80fd\u76f8\u5f53\uff08\u5f53\u6709\u5382\u5546\u5e93\u65f6\uff09\uff0c\u5bf9\u4e8e\u6ca1\u6709\u5382\u5546\u5e93\u7684HLSL\uff0c\u5b9e\u73b0\u8fbe\u5230\u4e86\u786c\u4ef6\u6587\u6863\u6807\u79f0\u7684FLOPS\u3002\u7cfb\u7edf\u80fd\u591f\u63a2\u7d22LLM\u5728\u8be5\u9886\u57df\u7684\u884c\u4e3a\u7279\u5f81\u3001\u8f6c\u6362\u9519\u8bef\u5206\u6790\u4ee5\u53ca\u6027\u80fd\u968f\u4f18\u5316\u5e8f\u5217\u7684\u6f14\u53d8\u3002", "conclusion": "PEAK\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u63a5\u53e3\uff0c\u65e2\u53ef\u7531\u6027\u80fd\u5de5\u7a0b\u5e08\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4e5f\u53ef\u5b8c\u5168\u81ea\u4e3b\u8fd0\u884c\uff08\u5982AI\u4ee3\u7406\u9a71\u52a8\uff09\uff0c\u5177\u6709\u5411\u524d\u517c\u5bb9\u7684\u8bbe\u8ba1\uff0c\u80fd\u968fAI\u80fd\u529b\u8fdb\u6b65\u800c\u6301\u7eed\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "2512.18564", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18564", "abs": "https://arxiv.org/abs/2512.18564", "authors": ["John Chen", "Sihan Cheng", "Can Gurkan", "Ryan Lay", "Moez Salahuddin"], "title": "Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V", "comment": "Under review", "summary": "Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs' real-world deployment. Working on a classic 4X strategy game, Sid Meier's Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi's enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.", "AI": {"tldr": "Vox Deorum\u662f\u4e00\u4e2a\u7528\u4e8e4X\u7b56\u7565\u6e38\u620f\u7684\u6df7\u5408LLM+X\u67b6\u6784\uff0c\u5c06\u5b8f\u89c2\u6218\u7565\u63a8\u7406\u4ea4\u7ed9LLM\uff0c\u6218\u672f\u6267\u884c\u59d4\u6258\u7ed9\u5b50\u7cfb\u7edf\uff0c\u5728\u300a\u6587\u660eV\u300b\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "LLM\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u4f7f\u5176\u57284X\u548c\u5927\u6218\u7565\u6e38\u620f\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u53ef\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\uff08\u5982\u534f\u4f5c\u548c\u8c08\u5224\uff09\uff0c\u4f46\u6e38\u620f\u7684\u590d\u6742\u6027\u548c\u957f\u65f6\u7a0b\u7279\u6027\u5e26\u6765\u6311\u6218\uff0c\u540c\u65f6\u5ef6\u8fdf\u548c\u6210\u672c\u56e0\u7d20\u963b\u788d\u4e86LLM\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5728\u300a\u6587\u660eV\u300bVox Populi\u6a21\u7ec4\u4e2d\u5f15\u5165Vox Deorum\u6df7\u5408\u67b6\u6784\uff1a\u5206\u5c42\u6280\u672f\u8bbe\u8ba1\u8ba9LLM\u5904\u7406\u5b8f\u89c2\u6218\u7565\u63a8\u7406\uff0c\u5c06\u6218\u672f\u6267\u884c\u59d4\u6258\u7ed9\u5b50\u7cfb\u7edf\uff08\u5982\u7b97\u6cd5AI\u6216\u672a\u6765\u7684\u5f3a\u5316\u5b66\u4e60AI\uff09\u3002", "result": "\u901a\u8fc72,327\u573a\u5b8c\u6574\u6e38\u620f\u9a8c\u8bc1\uff0c\u6bd4\u8f83\u4e24\u4e2a\u5f00\u6e90LLM\u4e0eVox Populi\u589e\u5f3aAI\u3002\u7ed3\u679c\u663e\u793aLLM\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7aef\u5230\u7aef\u6e38\u620f\u73a9\u6cd5\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u4e0e\u7b97\u6cd5AI\u53ca\u5f7c\u6b64\u4e4b\u95f4\u663e\u8457\u4e0d\u540c\u7684\u6e38\u620f\u98ce\u683c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u5546\u4e1a4X\u6e38\u620f\u4e2d\u96c6\u6210LLM\u5efa\u7acb\u4e86\u53ef\u884c\u7684\u67b6\u6784\uff0c\u4e3a\u6e38\u620f\u8bbe\u8ba1\u548c\u667a\u80fd\u4f53AI\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002", "topic": "agent analysis"}}
{"id": "2512.19122", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19122", "abs": "https://arxiv.org/abs/2512.19122", "authors": ["Mahir Labib Dihan", "Sadif Ahmed", "Md Nafiu Rahman"], "title": "BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation", "comment": "Accepted at BLP Workshop @ IJCNLP-AACL 2025. Code is available at https://github.com/mahirlabibdihan/BanglaForge", "summary": "Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.", "AI": {"tldr": "\u63d0\u51faBanglaForge\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u53cc\u6a21\u578b\u534f\u4f5c\u4e0e\u81ea\u6211\u7cbe\u70bc\uff0c\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bed\u4ee3\u7801\u751f\u6210\u7684\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c\u5728BLP-2025\u57fa\u51c6\u4e0a\u8fbe\u523084.00%\u7684Pass@1\u51c6\u786e\u7387\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u548c\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7a0b\u5e8f\u7684\u5de5\u5177\uff0c\u4f7f\u5f97\u5b5f\u52a0\u62c9\u8bed\u5230\u4ee3\u7801\u751f\u6210\u6210\u4e3a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u53cc\u6a21\u578b\u534f\u4f5c\u8303\u5f0f\u4e0e\u81ea\u6211\u7cbe\u70bc\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u57fa\u4e8eLLM\u7684\u7ffb\u8bd1\u3001\u7cfb\u7edf\u63d0\u793a\u5de5\u7a0b\u548c\u57fa\u4e8e\u6267\u884c\u53cd\u9988\u7684\u8fed\u4ee3\u81ea\u6211\u7cbe\u70bc\uff0c\u5176\u4e2d\u7f16\u7801\u5668\u751f\u6210\u521d\u59cb\u89e3\u51b3\u65b9\u6848\uff0c\u5ba1\u67e5\u5668\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u3002", "result": "\u5728BLP-2025\u5b5f\u52a0\u62c9\u8bed\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e0a\uff0cBanglaForge\u5b9e\u73b0\u4e8684.00%\u7684\u7ade\u4e89\u6027Pass@1\u51c6\u786e\u7387\u3002", "conclusion": "\u68c0\u7d22\u3001\u6a21\u578b\u534f\u4f5c\u548c\u81ea\u6211\u7cbe\u70bc\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u5b5f\u52a0\u62c9\u8bed\u4ee3\u7801\u751f\u6210\u662f\u6709\u6548\u7684\u3002", "topic": "code agent"}}
{"id": "2512.18440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18440", "abs": "https://arxiv.org/abs/2512.18440", "authors": ["Victor De Marez", "Jens Van Nooten", "Luna De Bruyne", "Walter Daelemans"], "title": "An Agentic AI Framework for Training General Practitioner Student Skills", "comment": null, "summary": "Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5168\u79d1\u533b\u5b66\u751f\u6280\u80fd\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u53ef\u914d\u7f6e\u7684\u5faa\u8bc1\u6848\u4f8b\u751f\u6210\u3001\u53ef\u63a7\u7684\u89d2\u8272\u9a71\u52a8\u60a3\u8005\u5bf9\u8bdd\u4ee5\u53ca\u57fa\u4e8e\u6807\u51c6\u7684\u8bc4\u4f30\u53cd\u9988\uff0c\u5728\u4ea4\u4e92\u5f0f\u53e3\u8bed\u54a8\u8be2\u73af\u5883\u4e2d\u5b9e\u73b0\u5e76\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u865a\u62df\u6a21\u62df\u60a3\u8005\u5728\u533b\u5b66\u6559\u80b2\u4e2d\u5b58\u5728\u533b\u5b66\u51c6\u786e\u6027\u4e0d\u8db3\u3001\u89d2\u8272\u626e\u6f14\u4e0d\u4e00\u81f4\u3001\u6848\u4f8b\u751f\u6210\u56f0\u96be\u4ee5\u53ca\u7f3a\u4e4f\u6559\u80b2\u7ed3\u6784\u5316\u53cd\u9988\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u5de5\u5177\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u53ef\u914d\u7f6e\u7684\u5faa\u8bc1\u6848\u4f8b\u751f\u6210\uff1b(2)\u53ef\u63a7\u7684\u89d2\u8272\u9a71\u52a8\u60a3\u8005\u5bf9\u8bdd\uff0c\u53ef\u9009\u68c0\u7d22\u589e\u5f3a\uff1b(3)\u57fa\u4e8e\u6807\u51c6\u7684\u6c9f\u901a\u548c\u4e34\u5e8a\u63a8\u7406\u8bc4\u4f30\u53cd\u9988\u7cfb\u7edf\u3002\u5728\u4ea4\u4e92\u5f0f\u53e3\u8bed\u54a8\u8be2\u73af\u5883\u4e2d\u5b9e\u73b0\u8be5\u6846\u67b6\u3002", "result": "\u572814\u540d\u533b\u5b66\u751f\u4e2d\u8bc4\u4f30\u663e\u793a\uff1a\u53c2\u4e0e\u8005\u62a5\u544a\u5bf9\u8bdd\u771f\u5b9e\u4e14\u5fe0\u5b9e\u4e8e\u6848\u4f8b\u3001\u96be\u5ea6\u6821\u51c6\u9002\u5f53\u3001\u4e2a\u6027\u4fe1\u53f7\u7a33\u5b9a\u3001\u53cd\u9988\u4e30\u5bcc\u6709\u7528\uff0c\u6574\u4f53\u53ef\u7528\u6027\u4f18\u79c0\u3002", "conclusion": "\u652f\u6301\u5c06\u573a\u666f\u63a7\u5236\u3001\u4ea4\u4e92\u63a7\u5236\u548c\u57fa\u4e8e\u6807\u51c6\u7684\u8bc4\u4f30\u5206\u79bb\u4f5c\u4e3a\u6784\u5efa\u53ef\u9760\u4e14\u5177\u6709\u6559\u5b66\u4ef7\u503c\u7684\u865a\u62df\u6a21\u62df\u60a3\u8005\u8bad\u7ec3\u5de5\u5177\u7684\u5b9e\u7528\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2512.19215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.19215", "abs": "https://arxiv.org/abs/2512.19215", "authors": ["Junyao Ye", "Zhen Li", "Xi Tang", "Shouhuai Xu", "Deqing Zou", "Zhongsheng Yuan"], "title": "Semantically-Equivalent Transformations-Based Backdoor Attacks against Neural Code Models: Characterization and Mitigation", "comment": null, "summary": "Neural code models have been increasingly incorporated into software development processes. However, their susceptibility to backdoor attacks presents a significant security risk. The state-of-the-art understanding focuses on injection-based attacks, which insert anomalous patterns into software code. These attacks can be neutralized by standard sanitization techniques. This status quo may lead to a false sense of security regarding backdoor attacks. In this paper, we introduce a new kind of backdoor attacks, dubbed Semantically-Equivalent Transformation (SET)-based backdoor attacks, which use semantics-preserving low-prevalence code transformations to generate stealthy triggers. We propose a framework to guide the generation of such triggers. Our experiments across five tasks, six languages, and models like CodeBERT, CodeT5, and StarCoder show that SET-based attacks achieve high success rates (often >90%) while preserving model utility. The attack proves highly stealthy, evading state-of-the-art defenses with detection rates on average over 25.13% lower than injection-based counterparts. We evaluate normalization-based countermeasures and find they offer only partial mitigation, confirming the attack's robustness. These results motivate further investigation into scalable defenses tailored to SET-based attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8bed\u4e49\u7b49\u4ef7\u53d8\u6362(SET)\u540e\u95e8\u653b\u51fb\uff0c\u4f7f\u7528\u8bed\u4e49\u4fdd\u6301\u7684\u4f4e\u6d41\u884c\u5ea6\u4ee3\u7801\u53d8\u6362\u751f\u6210\u9690\u853d\u89e6\u53d1\u5668\uff0c\u76f8\u6bd4\u4f20\u7edf\u6ce8\u5165\u5f0f\u653b\u51fb\u66f4\u96be\u4ee5\u68c0\u6d4b\u548c\u9632\u5fa1\u3002", "motivation": "\u5f53\u524d\u5bf9\u795e\u7ecf\u4ee3\u7801\u6a21\u578b\u540e\u95e8\u653b\u51fb\u7684\u7406\u89e3\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6ce8\u5165\u5f0f\u653b\u51fb\uff0c\u8fd9\u4e9b\u653b\u51fb\u53ef\u901a\u8fc7\u6807\u51c6\u6e05\u7406\u6280\u672f\u4e2d\u548c\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u540e\u95e8\u653b\u51fb\u5b89\u5168\u7684\u9519\u8bef\u8ba4\u77e5\u3002\u9700\u8981\u7814\u7a76\u66f4\u9690\u853d\u7684\u653b\u51fb\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u7b49\u4ef7\u53d8\u6362(SET)\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u4f7f\u7528\u8bed\u4e49\u4fdd\u6301\u7684\u4f4e\u6d41\u884c\u5ea6\u4ee3\u7801\u53d8\u6362\u751f\u6210\u9690\u853d\u89e6\u53d1\u5668\u3002\u5728\u4e94\u4e2a\u4efb\u52a1\u3001\u516d\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u591a\u4e2a\u6a21\u578b(CodeBERT\u3001CodeT5\u3001StarCoder)\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "SET\u653b\u51fb\u6210\u529f\u7387\u901a\u5e38>90%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002\u653b\u51fb\u9690\u853d\u6027\u9ad8\uff0c\u68c0\u6d4b\u7387\u6bd4\u6ce8\u5165\u5f0f\u653b\u51fb\u5e73\u5747\u4f4e25.13%\u4ee5\u4e0a\u3002\u89c4\u8303\u5316\u9632\u5fa1\u63aa\u65bd\u4ec5\u63d0\u4f9b\u90e8\u5206\u7f13\u89e3\uff0c\u8bc1\u5b9e\u4e86\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SET\u540e\u95e8\u653b\u51fb\u6bd4\u4f20\u7edf\u6ce8\u5165\u5f0f\u653b\u51fb\u66f4\u9690\u853d\u6709\u6548\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9488\u5bf9SET\u653b\u51fb\u7684\u53ef\u6269\u5c55\u9632\u5fa1\u673a\u5236\u3002", "topic": "code agent"}}
{"id": "2512.19481", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19481", "abs": "https://arxiv.org/abs/2512.19481", "authors": ["Katharina Stengg", "Christian Macho", "Martin Pinzger"], "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis", "comment": "6 pages", "summary": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.", "AI": {"tldr": "\u7814\u7a76GPT-5\u548cGPT-5-mini\u9884\u6d4b\u6e90\u4ee3\u7801\u53d8\u66f4\u5f71\u54cd\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u63d0\u4f9bdiff\u4fe1\u606f\u53ef\u7565\u5fae\u63d0\u5347\u6027\u80fd", "motivation": "\u7406\u89e3\u6e90\u4ee3\u7801\u53d8\u66f4\u53ca\u5176\u5f71\u54cd\u662f\u8f6f\u4ef6\u5f00\u53d1\u7684\u5173\u952e\u6280\u80fd\uff0c\u4f46\u901a\u5e38\u9700\u8981\u624b\u52a8\u5206\u6790\u4e14\u8017\u65f6\u3002\u867d\u7136LLM\u5728\u4ee3\u7801\u5206\u6790\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u7406\u89e3\u4ee3\u7801\u53d8\u66f4\u5f71\u54cd\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u5305\u542b\u79cd\u5b50\u53d8\u66f4\u3001\u53d8\u66f4\u5bf9\u548c\u53d8\u66f4\u7c7b\u578b\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30GPT-5\u548cGPT-5-mini\u5728\u4e24\u79cd\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\uff1a(1)\u79cd\u5b50\u53d8\u66f4\u4fe1\u606f\u548c\u7236\u63d0\u4ea4\u6811\uff1b(2)\u989d\u5916\u63d0\u4f9b\u6bcf\u4e2a\u79cd\u5b50\u53d8\u66f4\u7684diff\u4fe1\u606f\u3002", "result": "\u4e24\u4e2aLLM\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u90fd\u4e0d\u4f73\uff0cGPT-5\u4f18\u4e8eGPT-5-mini\u3002\u63d0\u4f9bdiff\u4fe1\u606f\u80fd\u7565\u5fae\u63d0\u5347\u4e24\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u9884\u6d4b\u4ee3\u7801\u53d8\u66f4\u5f71\u54cd\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2512.19509", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.19509", "abs": "https://arxiv.org/abs/2512.19509", "authors": ["Shangbo Yun", "Xiaodong Gu", "Jianghong Huang", "Beijun Shen"], "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models", "comment": "Accepted by FSE 2026", "summary": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u7684\u6846\u67b6\u6765\u63ed\u793a\u7f16\u7a0b\u8bed\u8a00\u4e4b\u95f4\u7684\u6df1\u5c42\u8bed\u8a00\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5173\u7cfb\u6765\u6539\u8fdb\u591a\u8bed\u8a00\u4ee3\u7801LLM\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u901a\u5e38\u53ea\u662f\u7b80\u5355\u5730\u805a\u5408\u591a\u8bed\u8a00\u4ee3\u7801\u6570\u636e\u6765\u8bad\u7ec3\u4ee3\u7801LLM\uff0c\u5f88\u5c11\u63a2\u7d22\u7f16\u7a0b\u8bed\u8a00\u4e4b\u95f4\u7684\u6df1\u5c42\u5173\u7cfb\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u5173\u7cfb\u6765\u4f18\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u7f16\u7a0b\u8bed\u8a00\u4e4b\u95f4\u7684\u6df1\u5c42\u8bed\u8a00\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5173\u7cfb\u6539\u8fdb\u591a\u8bed\u8a00\u4ee3\u7801LLM\u3002", "method": "1) \u5b9a\u4e4921\u4e2a\u4e3b\u8981\u7f16\u7a0b\u8bed\u8a00\u7279\u5f81\uff1b2) \u4f7f\u7528LLM\u751f\u6210\u8de8\u591a\u4e2a\u8bed\u8a00\u7684\u7279\u5f81\u5bf9\u9f50\u4ee3\u7801\u6837\u672c\uff1b3) \u5d4c\u516519\u79cd\u8bed\u8a00\u7684\u8bed\u4e49\u5e76\u884c\u4ee3\u7801\u7247\u6bb5\uff1b4) \u6784\u5efa\u76f8\u4f3c\u6027\u77e9\u9635\u5e76\u8fdb\u884c\u5c42\u6b21\u805a\u7c7b\u4ee5\u63ed\u793a\u8bed\u8a00\u5173\u7cfb\uff1b5) \u57fa\u4e8e\u53d1\u73b0\u7684\u8bed\u8a00\u5bb6\u65cf\u63d0\u51fa\u4e09\u79cd\u8bad\u7ec3\u7b56\u7565\uff1a\u8de8\u8bed\u8a00\u76f8\u5173\u8bed\u8a00\u7684\u8fc1\u79fb\u5b66\u4e60\u3001\u8bed\u8a00\u90bb\u8fd1\u6027\u5f15\u5bfc\u7684\u8bfe\u7a0b\u5b66\u4e60\u3001\u57fa\u4e8e\u8d28\u5fc3\u7684\u4e2d\u95f4\u4ee3\u7801\u7ffb\u8bd1\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u7f16\u7a0b\u8bed\u8a00\u4e4b\u95f4\u7684\u6e05\u6670\u5c42\u6b21\u7ed3\u6784\uff1a\u5bc6\u5207\u76f8\u5173\u7684\u8bed\u8a00\u5f62\u6210\u660e\u786e\u5b9a\u4e49\u7684\u805a\u7c7b\uff08\u5982C\u3001C++\u3001Java\u3001Swift\uff09\uff0cGo\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\u3002\u57284\u4e2a\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8bed\u8a00LLM\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7f16\u7a0b\u8bed\u8a00\u7684\u901a\u7528\u89c6\u89d2\uff0c\u5e76\u63a8\u8fdb\u4e86\u591a\u8bed\u8a00\u4ee3\u7801LLM\u8bad\u7ec3\u7684\u66f4\u6709\u6548\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u5173\u7cfb\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2512.18546", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18546", "abs": "https://arxiv.org/abs/2512.18546", "authors": ["Alexander Doudkin"], "title": "LLMs on Drugs: Language Models Are Few-Shot Consumers", "comment": "8 pages, 2 figures, 2 tables", "summary": "Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level \"drug\" interventions have never been benchmarked rigorously. We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge. Four single-sentence prompts -- LSD, cocaine, alcohol, and cannabis -- are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests. Control accuracy is 0.45; alcohol collapses to 0.10 (p = 3.2e-8), cocaine to 0.21 (p = 4.9e-4), LSD to 0.19 (p = 1.3e-4), and cannabis to 0.30 (p = 0.041), largely because persona prompts disrupt the mandated \"Answer: <LETTER>\" template. Persona text therefore behaves like a \"few-shot consumable\" that can destroy reliability without touching model weights. All experimental code, raw results, and analysis scripts are available at https://github.com/lexdoudkin/llms-on-drugs.", "AI": {"tldr": "\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\"\u836f\u7269\"\u63d0\u793a\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9152\u7cbe\u3001\u53ef\u5361\u56e0\u3001LSD\u3001\u5927\u9ebb\u7b49\u63d0\u793a\u4f1a\u663e\u8457\u964d\u4f4eGPT-5-mini\u5728ARC-Challenge\u4e0a\u7684\u8868\u73b0\uff0c\u5176\u4e2d\u9152\u7cbe\u6548\u679c\u6700\u5dee\u3002", "motivation": "\u867d\u7136\u5df2\u77e5LLM\u5bf9\u63a8\u7406\u65f6\u65bd\u52a0\u7684\"\u4eba\u8bbe\"\u654f\u611f\uff0c\u4f46\u4ece\u672a\u6709\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\"\u836f\u7269\"\u7c7b\u63d0\u793a\u5e72\u9884\u7684\u6548\u679c\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8fdb\u884c\u9996\u4e2a\u53d7\u63a7\u5b9e\u9a8c\u6765\u91cf\u5316\u4e0d\u540c\u836f\u7269\u63d0\u793a\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528GPT-5-mini\u5728ARC-Challenge\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u56db\u79cd\u5355\u53e5\u836f\u7269\u63d0\u793a\uff08LSD\u3001\u53ef\u5361\u56e0\u3001\u9152\u7cbe\u3001\u5927\u9ebb\uff09\u4e0e\u6e05\u9192\u5bf9\u7167\u7ec4\u7684\u6027\u80fd\u3002\u6bcf\u4e2a\u6761\u4ef6\u6d4b\u8bd5100\u4e2a\u9a8c\u8bc1\u9879\u76ee\uff0c\u91c7\u7528\u786e\u5b9a\u6027\u89e3\u7801\u3001\u5b8c\u6574\u65e5\u5fd7\u8bb0\u5f55\u3001Wilson\u7f6e\u4fe1\u533a\u95f4\u548cFisher\u7cbe\u786e\u68c0\u9a8c\u3002", "result": "\u5bf9\u7167\u7ec4\u51c6\u786e\u7387\u4e3a0.45\uff1b\u9152\u7cbe\u63d0\u793a\u964d\u81f30.10\uff08p=3.2e-8\uff09\uff0c\u53ef\u5361\u56e00.21\uff08p=4.9e-4\uff09\uff0cLSD 0.19\uff08p=1.3e-4\uff09\uff0c\u5927\u9ebb0.30\uff08p=0.041\uff09\u3002\u4e3b\u8981\u539f\u56e0\u662f\u4eba\u8bbe\u63d0\u793a\u7834\u574f\u4e86\u6a21\u578b\u8981\u6c42\u7684\"Answer: <LETTER>\"\u6a21\u677f\u683c\u5f0f\u3002", "conclusion": "\u4eba\u8bbe\u6587\u672c\u5c31\u50cf\"\u5c11\u91cf\u6d88\u8017\u54c1\"\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u5c31\u80fd\u7834\u574f\u53ef\u9760\u6027\u3002\u8fd9\u63ed\u793a\u4e86\u63d0\u793a\u5de5\u7a0b\u4e2d\u683c\u5f0f\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u770b\u4f3c\u65e0\u5bb3\u7684\u4eba\u8bbe\u63d0\u793a\u53ef\u80fd\u5bf9\u6a21\u578b\u6027\u80fd\u4ea7\u751f\u707e\u96be\u6027\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2512.18669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18669", "abs": "https://arxiv.org/abs/2512.18669", "authors": ["Jones David", "Shreya Ghosh"], "title": "IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling", "comment": "Submitted to EACL 2026 System Demonstrations Track. 6 pages (main content), 6 figures, includes appendices", "summary": "LLM-based tutors are typically single-turn assistants that lack persistent representations of learner knowledge, making it difficult to provide principled, transparent, and long-term pedagogical support. We introduce IntelliCode, a multi-agent LLM tutoring system built around a centralized, versioned learner state that integrates mastery estimates, misconceptions, review schedules, and engagement signals. A StateGraph Orchestrator coordinates six specialized agents: skill assessment, learner profiling, graduated hinting, curriculum selection, spaced repetition, and engagement monitoring, each operating as a pure transformation over the shared state under a single-writer policy. This architecture enables auditable mastery updates, proficiency-aware hints, dependency-aware curriculum adaptation, and safety-aligned prompting.\n  The demo showcases an end-to-end tutoring workflow: a learner attempts a DSA problem, receives a conceptual hint when stuck, submits a corrected solution, and immediately sees mastery updates and a personalized review interval. We report validation results with simulated learners, showing stable state updates, improved task success with graduated hints, and diverse curriculum coverage. IntelliCode demonstrates how persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design can be combined to produce transparent and reliable LLM-driven tutoring.", "AI": {"tldr": "IntelliCode\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u8f85\u5bfc\u7cfb\u7edf\uff0c\u91c7\u7528\u4e2d\u5fc3\u5316\u7248\u672c\u5316\u5b66\u4e60\u8005\u72b6\u6001\uff0c\u901a\u8fc7\u516d\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u534f\u8c03\u5de5\u4f5c\uff0c\u63d0\u4f9b\u53ef\u5ba1\u8ba1\u3001\u4e2a\u6027\u5316\u7684\u957f\u671f\u6559\u5b66\u652f\u6301\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u8f85\u5bfc\u7cfb\u7edf\u901a\u5e38\u662f\u5355\u8f6e\u5bf9\u8bdd\u52a9\u624b\uff0c\u7f3a\u4e4f\u5bf9\u5b66\u4e60\u8005\u77e5\u8bc6\u7684\u6301\u4e45\u8868\u793a\uff0c\u96be\u4ee5\u63d0\u4f9b\u6709\u539f\u5219\u3001\u900f\u660e\u548c\u957f\u671f\u7684\u6559\u5b66\u652f\u6301\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u56f4\u7ed5\u4e2d\u5fc3\u5316\u7248\u672c\u5316\u5b66\u4e60\u8005\u72b6\u6001\u7684\u591a\u667a\u80fd\u4f53LLM\u8f85\u5bfc\u7cfb\u7edf\uff0c\u5305\u542b\u6280\u80fd\u8bc4\u4f30\u3001\u5b66\u4e60\u8005\u753b\u50cf\u3001\u6e10\u8fdb\u63d0\u793a\u3001\u8bfe\u7a0b\u9009\u62e9\u3001\u95f4\u9694\u91cd\u590d\u548c\u53c2\u4e0e\u5ea6\u76d1\u63a7\u516d\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff0c\u901a\u8fc7StateGraph Orchestrator\u534f\u8c03\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5728\u5355\u4e00\u5199\u5165\u7b56\u7565\u4e0b\u5bf9\u5171\u4eab\u72b6\u6001\u8fdb\u884c\u7eaf\u8f6c\u6362\u64cd\u4f5c\u3002", "result": "\u6f14\u793a\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u8f85\u5bfc\u6d41\u7a0b\uff1a\u5b66\u4e60\u8005\u5c1d\u8bd5DSA\u95ee\u9898\uff0c\u5728\u5361\u987f\u65f6\u83b7\u5f97\u6982\u5ff5\u63d0\u793a\uff0c\u63d0\u4ea4\u4fee\u6b63\u65b9\u6848\u540e\u7acb\u5373\u770b\u5230\u638c\u63e1\u5ea6\u66f4\u65b0\u548c\u4e2a\u6027\u5316\u590d\u4e60\u95f4\u9694\u3002\u6a21\u62df\u5b66\u4e60\u8005\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\u7a33\u5b9a\u7684\u72b6\u6001\u66f4\u65b0\u3001\u6e10\u8fdb\u63d0\u793a\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u4ee5\u53ca\u591a\u6837\u5316\u7684\u8bfe\u7a0b\u8986\u76d6\u3002", "conclusion": "IntelliCode\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u6301\u4e45\u5b66\u4e60\u8005\u5efa\u6a21\u3001\u534f\u8c03\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u548c\u6709\u539f\u5219\u7684\u6559\u5b66\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u4ee5\u4ea7\u751f\u900f\u660e\u53ef\u9760\u7684LLM\u9a71\u52a8\u8f85\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2512.18940", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.18940", "abs": "https://arxiv.org/abs/2512.18940", "authors": ["Wen-Long Jin"], "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions", "comment": "13 pages, 3 figures. Supplementary materials at https://doi.org/10.17605/OSF.IO/PV6R3", "summary": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.", "AI": {"tldr": "FASTRIC\u662f\u4e00\u79cd\u63d0\u793a\u89c4\u8303\u8bed\u8a00\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4f7f\u9690\u5f0f\u6709\u9650\u72b6\u6001\u673a\u663e\u5f0f\u5316\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u591a\u8f6e\u4ea4\u4e92\u534f\u8bae\u7684\u9a8c\u8bc1\u548c\u89c4\u8303\u6267\u884c\u3002", "motivation": "LLM\u6267\u884c\u590d\u6742\u7684\u591a\u8f6e\u4ea4\u4e92\u534f\u8bae\uff0c\u4f46\u7f3a\u4e4f\u5f62\u5f0f\u5316\u89c4\u8303\u6765\u9a8c\u8bc1\u6267\u884c\u662f\u5426\u7b26\u5408\u8bbe\u8ba1\u8005\u610f\u56fe\u3002\u73b0\u6709\u7b26\u53f7\u89c4\u8303\u8bed\u8a00\u9700\u8981\u89e3\u6790\u5668\u548c\u7f16\u8bd1\u5668\uff0c\u800cFASTRIC\u5229\u7528LLM\u4f5c\u4e3a\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u5f15\u5165FASTRIC\u63d0\u793a\u89c4\u8303\u8bed\u8a00\uff0c\u5c06\u9690\u5f0f\u6709\u9650\u72b6\u6001\u673a\u5728\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e2d\u663e\u5f0f\u5316\u3002\u6307\u5bfc\u8bbe\u8ba1\u8005\u8868\u8fbe\u4e03\u4e2aFSM\u5143\u7d20\uff08\u6700\u7ec8\u72b6\u6001\u3001\u4ee3\u7406\u3001\u72b6\u6001\u3001\u89e6\u53d1\u5668\u3001\u89d2\u8272\u3001\u521d\u59cb\u72b6\u6001\u3001\u7ea6\u675f\uff09\uff0c\u89c4\u8303\u5f62\u5f0f\u6027\u4ece\u9690\u5f0f\u63cf\u8ff0\u5230\u663e\u5f0f\u9010\u6b65\u6307\u4ee4\u4f5c\u4e3a\u8bbe\u8ba1\u53c2\u6570\u3002", "result": "\u6d4b\u8bd53\u72b6\u6001\u5e7c\u513f\u56ed\u8f85\u5bfcFSM\uff0c\u53d1\u73b0\u6700\u4f73\u89c4\u8303\u5f62\u5f0f\u6027\u662f\u6a21\u578b\u5bb9\u91cf\u7684\u51fd\u6570\uff1aDeepSeek-V3.2\u5728L2-L4\u5b9e\u73b0\u5b8c\u7f8e\u4e00\u81f4\u6027\uff081.00\uff09\uff1bChatGPT-5\u5728L3\u8fbe\u5230\u5cf0\u503c\uff080.90\uff09\u540e\u5728L4\u5d29\u6e83\uff080.39\uff09\uff1bPhi4\u65e0\u7a33\u5b9a\u6700\u4f18\u503c\u4e14\u65b9\u5dee\u9ad8\u3002\u63ed\u793a\u4e86\u6a21\u578b\u7279\u5b9a\u7684\"Goldilocks zones\"\u3002", "conclusion": "FASTRIC\u5efa\u7acb\u4e86\u63d0\u793a\u89c4\u8303\u5de5\u7a0b\uff0c\u7528\u4e8e\u521b\u5efa\u53ef\u9a8c\u8bc1\u7684\u4ea4\u4e92\u534f\u8bae\uff0c\u5c06\u591a\u8f6e\u4ea4\u4e92\u8bbe\u8ba1\u4ece\u542f\u53d1\u5f0f\u827a\u672f\u8f6c\u53d8\u4e3a\u5177\u6709\u53ef\u6d4b\u91cf\u7a0b\u5e8f\u4fdd\u8bc1\u7684\u7cfb\u7edf\u5de5\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2512.18755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18755", "abs": "https://arxiv.org/abs/2512.18755", "authors": ["Jianyi Zhang", "Shizhao Liu", "Ziyin Zhou", "Zhen Li"], "title": "MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA", "AI": {"tldr": "\u63d0\u51faMEEA\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u5fc3\u7406\u5b66\u4e2d\u7684\u7eaf\u7cb9\u63a5\u89e6\u6548\u5e94\uff0c\u901a\u8fc7\u91cd\u590d\u4f4e\u6bd2\u6027\u8bed\u4e49\u66b4\u9732\u9010\u6b65\u4fb5\u8680LLM\u7684\u5b89\u5168\u5bf9\u9f50\u8fb9\u754c\uff0c\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u8d8a\u72f1\u7814\u7a76\u5927\u591a\u5047\u8bbe\u9759\u6001\u5b89\u5168\u8fb9\u754c\uff0c\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u4ea4\u4e92\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u52a8\u6001\u5f71\u54cd\uff0c\u5bfc\u81f4\u653b\u51fb\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u8bc4\u4f30\u591a\u8f6e\u5b89\u5168\u9c81\u68d2\u6027\u7684\u81ea\u52a8\u5316\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u7eaf\u7cb9\u63a5\u89e6\u6548\u5e94\uff0c\u6784\u5efa\u8bed\u4e49\u6e10\u8fdb\u63d0\u793a\u94fe\uff0c\u4f7f\u7528\u6a21\u62df\u9000\u706b\u7b56\u7565\u8fdb\u884c\u4f18\u5316\uff0c\u4f18\u5316\u76ee\u6807\u5305\u62ec\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u6bd2\u6027\u548c\u8d8a\u72f1\u6548\u679c\u3002\u901a\u8fc7\u91cd\u590d\u4f4e\u6bd2\u6027\u8bed\u4e49\u66b4\u9732\u9010\u6b65\u964d\u4f4e\u6a21\u578b\u7684\u5b89\u5168\u9608\u503c\u3002", "result": "\u5728GPT-4\u3001Claude-3.5\u3001DeepSeek-R1\u7b49\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u4e0a\uff0cMEEA\u6bd47\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad8\u8d85\u8fc720%\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u9000\u706b\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u66b4\u9732\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "LLM\u5b89\u5168\u884c\u4e3a\u672c\u8d28\u4e0a\u662f\u52a8\u6001\u4e14\u5386\u53f2\u4f9d\u8d56\u7684\uff0c\u6311\u6218\u4e86\u9759\u6001\u5bf9\u9f50\u8fb9\u754c\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u5f3a\u8c03\u9700\u8981\u4ea4\u4e92\u611f\u77e5\u7684\u5b89\u5168\u8bc4\u4f30\u548c\u9632\u5fa1\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2512.18623", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18623", "abs": "https://arxiv.org/abs/2512.18623", "authors": ["Jensen Zhang", "Ningyuan Liu", "Yijia Fan", "Zihao Huang", "Qinglin Zeng", "Kaitong Cai", "Jian Wang", "Keze Wang"], "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction", "comment": "Accepted at AAAI 2026", "summary": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.", "AI": {"tldr": "LLM-CAS\uff1a\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u9009\u62e9\u4e34\u65f6\u795e\u7ecf\u5143\u6270\u52a8\u6765\u5b9e\u65f6\u7ea0\u6b63LLM\u5e7b\u89c9\uff0c\u65e0\u9700\u6c38\u4e45\u4fee\u6539\u53c2\u6570", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u76d1\u7763\u5fae\u8c03\u3001\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff09\u6570\u636e\u5bc6\u96c6\u4e14\u8ba1\u7b97\u6602\u8d35\uff0c\u9759\u6001\u53c2\u6570\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0a\u4e0b\u6587\u4f9d\u8d56\u9519\u8bef\u548c\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5e7b\u89c9\u7ea0\u6b63\u65b9\u6848", "method": "\u63d0\u51faLLM-CAS\u6846\u67b6\uff0c\u5c06\u5b9e\u65f6\u5e7b\u89c9\u7ea0\u6b63\u5efa\u6a21\u4e3a\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u8bad\u7ec3\u667a\u80fd\u4f53\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u63a8\u7406\u65f6\u57fa\u4e8e\u5f53\u524d\u4e0a\u4e0b\u6587\u52a8\u6001\u9009\u62e9\u4e34\u65f6\u795e\u7ecf\u5143\u6270\u52a8\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u7ec6\u7c92\u5ea6\u7ea0\u6b63", "result": "\u5728\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cLLM-CAS\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\uff1aStoryCloze\u63d0\u534710.98\u4e2a\u767e\u5206\u70b9\uff0cTriviaQA\u63d0\u53472.71\u4e2a\u767e\u5206\u70b9\uff0cTruthfulQA\u7684MC1\u5206\u6570\u63d0\u53472.06\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u4e8eITI\u3001CAA\u7b49\u9759\u6001\u7f16\u8f91\u65b9\u6cd5\u548c\u52a8\u6001SADI\u6846\u67b6", "conclusion": "LLM-CAS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347LLM\u53ef\u9760\u6027\uff0c\u5177\u6709\u672a\u6765\u6269\u5c55\u5230\u591a\u6a21\u6001\u5e94\u7528\u7684\u6f5c\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2512.18857", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18857", "abs": "https://arxiv.org/abs/2512.18857", "authors": ["Zijun Gao", "Zhikun Xu", "Xiao Ye", "Ben Zhou"], "title": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning", "comment": null, "summary": "Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.", "AI": {"tldr": "CORE\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u660e\u786e\u6982\u5ff5\u8f6c\u5316\u4e3a\u53ef\u63a7\u76d1\u7763\u4fe1\u53f7\uff0c\u89e3\u51b3LLMs\u5728\u6570\u5b66\u95ee\u9898\u4e2d\u6a21\u5f0f\u590d\u7528\u800c\u975e\u6982\u5ff5\u7406\u89e3\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLMs\u80fd\u591f\u89e3\u51b3\u590d\u6742\u7684\u6570\u5b66\u7ec3\u4e60\uff0c\u4f46\u5728\u9700\u8981\u771f\u6b63\u6982\u5ff5\u7406\u89e3\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u4e3b\u8981\u5f3a\u5316\u6700\u7ec8\u7b54\u6848\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u6982\u5ff5\u4fe1\u53f7\uff0c\u5bfc\u81f4\u6a21\u578b\u53ea\u6539\u8fdb\u6a21\u5f0f\u590d\u7528\u800c\u975e\u6982\u5ff5\u5e94\u7528\u80fd\u529b\u3002", "method": "CORE\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6b65\u9aa4\uff1a(1)\u5408\u6210\u6982\u5ff5\u5bf9\u9f50\u7684\u6d4b\u9a8c\uff1b(2)\u5728rollout\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u7b80\u77ed\u6982\u5ff5\u7247\u6bb5\u4ee5\u5f15\u53d1\u6982\u5ff5\u5f15\u5bfc\u7684\u8f68\u8ff9\uff1b(3)\u901a\u8fc7\u8f68\u8ff9\u66ff\u6362\u5f3a\u5316\u6982\u5ff5\u63a8\u7406\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u524d\u5411KL\u7ea6\u675f\u5bf9\u9f50\u65e0\u5f15\u5bfc\u548c\u6982\u5ff5\u5f15\u5bfc\u7b56\u7565\uff0c\u6216\u76f4\u63a5\u5728\u6982\u5ff5\u5bf9\u9f50\u6d4b\u9a8c\u4e0a\u4f7f\u7528\u6807\u51c6GRPO\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\uff0cCORE\u76f8\u6bd4\u666e\u901a\u548cSFT\u57fa\u7ebf\uff0c\u5728\u9886\u57df\u5185\u6982\u5ff5\u7ec3\u4e60\u5957\u4ef6\u548c\u591a\u6837\u5316\u7684\u9886\u57df\u5916\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CORE\u901a\u8fc7\u7edf\u4e00\u6982\u5ff5\u5bf9\u9f50\u6d4b\u9a8c\u7684\u76f4\u63a5\u8bad\u7ec3\u548c\u6982\u5ff5\u6ce8\u5165rollout\uff0c\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u6982\u5ff5\u76d1\u7763\uff0c\u5f25\u5408\u4e86\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u548c\u771f\u6b63\u6982\u5ff5\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u548c\u9a8c\u8bc1\u5668\u7684\u65e0\u5173\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.18746", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18746", "abs": "https://arxiv.org/abs/2512.18746", "authors": ["Guibin Zhang", "Haotian Ren", "Chong Zhan", "Zhenhong Zhou", "Junhao Wang", "He Zhu", "Wangchunshu Zhou", "Shuicheng Yan"], "title": "MemEvolve: Meta-Evolution of Agent Memory Systems", "comment": null, "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.", "AI": {"tldr": "MemEvolve\u662f\u4e00\u4e2a\u5143\u8fdb\u5316\u6846\u67b6\uff0c\u8054\u5408\u8fdb\u5316\u4ee3\u7406\u7684\u7ecf\u9a8c\u77e5\u8bc6\u548c\u8bb0\u5fc6\u67b6\u6784\uff0c\u4f7f\u4ee3\u7406\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u79ef\u7d2f\u7ecf\u9a8c\uff0c\u8fd8\u80fd\u9010\u6b65\u6539\u8fdb\u5b66\u4e60\u65b9\u5f0f\u3002\u8be5\u6846\u67b6\u901a\u8fc7EvolveLab\u7edf\u4e00\u4ee3\u7801\u5e93\u5b9e\u73b0\uff0c\u5728\u56db\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u7684\u8bb0\u5fc6\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u9759\u6001\u67b6\u6784\uff0c\u867d\u7136\u80fd\u4fc3\u8fdb\u4ee3\u7406\u7ea7\u8fdb\u5316\uff0c\u4f46\u5e95\u5c42\u8bb0\u5fc6\u67b6\u6784\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4e0b\u6587\u8fdb\u884c\u5143\u9002\u5e94\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faMemEvolve\u5143\u8fdb\u5316\u6846\u67b6\uff0c\u8054\u5408\u8fdb\u5316\u4ee3\u7406\u7684\u7ecf\u9a8c\u77e5\u8bc6\u548c\u8bb0\u5fc6\u67b6\u6784\uff1b\u5f15\u5165EvolveLab\u7edf\u4e00\u4ee3\u7801\u5e93\uff0c\u5c0612\u4e2a\u4ee3\u8868\u6027\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u70bc\u4e3a\u6a21\u5757\u5316\u8bbe\u8ba1\u7a7a\u95f4\uff08\u7f16\u7801\u3001\u5b58\u50a8\u3001\u68c0\u7d22\u3001\u7ba1\u7406\uff09\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u5b9e\u73b0\u548c\u516c\u5e73\u5b9e\u9a8c\u5e73\u53f0\u3002", "result": "\u5728\u56db\u4e2a\u6311\u6218\u6027\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemEvolve\u5b9e\u73b0\u4e86\uff1a1\uff09\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5c06SmolAgent\u548cFlash-Searcher\u7b49\u6846\u67b6\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe17.06%\uff1b2\uff09\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u548c\u8de8LLM\u6cdb\u5316\u80fd\u529b\uff0c\u8bbe\u8ba1\u7684\u8bb0\u5fc6\u67b6\u6784\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u3002", "conclusion": "MemEvolve\u901a\u8fc7\u5143\u8fdb\u5316\u6846\u67b6\u89e3\u51b3\u4e86\u8bb0\u5fc6\u7cfb\u7edf\u9759\u6001\u6027\u7684\u9650\u5236\uff0c\u4f7f\u4ee3\u7406\u7cfb\u7edf\u80fd\u591f\u540c\u65f6\u8fdb\u5316\u7ecf\u9a8c\u548c\u5b66\u4e60\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u81ea\u8fdb\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f00\u653e\u7684\u7814\u7a76\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2512.18832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18832", "abs": "https://arxiv.org/abs/2512.18832", "authors": ["Yixia Li", "Hongru Wang", "Jiahao Qiu", "Zhenfei Yin", "Dongdong Zhang", "Cheng Qian", "Zeping Li", "Pony Ma", "Guanhua Chen", "Heng Ji", "Mengdi Wang"], "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?", "comment": null, "summary": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8LLM\u80fd\u5426\u4f5c\u4e3a\u53ef\u9760\u7684\u4e16\u754c\u6a21\u578b\u6765\u63d0\u5347\u667a\u80fd\u4f53\u5b66\u4e60\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4e09\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u5728\u6587\u672c\u73af\u5883\u4e2d\uff0c\u8bad\u7ec3\u5145\u5206\u7684\u4e16\u754c\u6a21\u578b\u80fd\u7ef4\u6301\u8fde\u8d2f\u72b6\u6001\u3001\u968f\u6570\u636e\u89c4\u6a21\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u4f46\u6548\u679c\u53d6\u51b3\u4e8e\u884c\u4e3a\u8986\u76d6\u548c\u73af\u5883\u590d\u6742\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8d8a\u6765\u8d8a\u4f9d\u8d56\u7ecf\u9a8c\u9a71\u52a8\u7684\u6269\u5c55\uff0c\u4f46\u73b0\u5b9e\u73af\u5883\u4ecd\u7136\u662f\u975e\u81ea\u9002\u5e94\u7684\u3001\u8986\u76d6\u6709\u9650\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u3002\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u6a21\u62df\u7ecf\u9a8c\u63d0\u4f9b\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u7684\u6f5c\u529b\uff0c\u4f46LLM\u80fd\u5426\u53ef\u9760\u5730\u626e\u6f14\u8fd9\u4e00\u89d2\u8272\u4ee5\u53ca\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u76ca\u667a\u80fd\u4f53\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5728\u6587\u672c\u73af\u5883\u4e2d\u8fdb\u884c\u7814\u7a76\uff0c\u5c06\u8bed\u8a00\u5efa\u6a21\u91cd\u65b0\u89e3\u91ca\u4e3a\u4ea4\u4e92\u4e0b\u7684\u4e0b\u4e00\u72b6\u6001\u9884\u6d4b\u3002\u5f15\u5165\u4e86\u4e09\u5c42\u8bc4\u4f30\u6846\u67b6\uff1a(i)\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\uff0c(ii)\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c(iii)\u667a\u80fd\u4f53\u6548\u7528\u3002\u5728\u4e94\u4e2a\u4ee3\u8868\u6027\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8bad\u7ec3\u5145\u5206\u7684\u4e16\u754c\u6a21\u578b\u80fd\u7ef4\u6301\u8fde\u8d2f\u7684\u6f5c\u5728\u72b6\u6001\uff0c\u968f\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u53ef\u9884\u6d4b\u5730\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u52a8\u4f5c\u9a8c\u8bc1\u3001\u5408\u6210\u8f68\u8ff9\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u9884\u70ed\u542f\u52a8\u7b49\u65b9\u5f0f\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u3002\u8fd9\u4e9b\u6536\u76ca\u5173\u952e\u53d6\u51b3\u4e8e\u884c\u4e3a\u8986\u76d6\u548c\u73af\u5883\u590d\u6742\u5ea6\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\u6765\u652f\u6301\u667a\u80fd\u4f53\u5b66\u4e60\uff0c\u4f46\u6548\u679c\u53d7\u5230\u884c\u4e3a\u8986\u76d6\u548c\u73af\u5883\u590d\u6742\u5ea6\u7684\u660e\u786e\u8fb9\u754c\u9650\u5236\uff0c\u754c\u5b9a\u4e86\u4e16\u754c\u6a21\u578b\u4f55\u65f6\u80fd\u6709\u6548\u652f\u6301\u667a\u80fd\u4f53\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.19081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19081", "abs": "https://arxiv.org/abs/2512.19081", "authors": ["Yanzhi Zhang", "Yitong Duan", "Zhaoxi Zhang", "Jiyan He", "Shuxin Zheng"], "title": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning", "comment": null, "summary": "Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.", "AI": {"tldr": "Population-Evolve\u662f\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u7ef4\u62a4\u52a8\u6001\u5019\u9009\u89e3\u7fa4\u4f53\uff0c\u8ba9LLM\u81ea\u6211\u8fdb\u5316\u79cd\u7fa4\uff0c\u6700\u7ec8\u901a\u8fc7\u591a\u6570\u6295\u7968\u83b7\u5f97\u7b54\u6848\uff0c\u5728\u63a8\u7406\u65f6\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u5df2\u6210\u4e3a\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8fdb\u5316\u7b56\u7565\u5728\u63a8\u7406\u65f6\u89e3\u9501LLM\u7684\u63a8\u7406\u6f5c\u529b\u3002", "method": "\u63d0\u51faPopulation-Evolve\u65b9\u6cd5\uff1a1\uff09\u4e3a\u6bcf\u4e2a\u95ee\u9898\u7ef4\u62a4\u52a8\u6001\u5019\u9009\u89e3\u7fa4\u4f53\uff1b2\uff09\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u5b9e\u73b0\uff1b3\uff09\u4f7f\u7528\u8fdb\u5316\u63d0\u793a\u8ba9LLM\u81ea\u6211\u8fdb\u5316\u79cd\u7fa4\uff1b4\uff09\u6536\u655b\u540e\u901a\u8fc7\u591a\u6570\u6295\u7968\u83b7\u5f97\u6700\u7ec8\u7b54\u6848\uff1b5\uff09\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\uff0c\u4ece\u9057\u4f20\u7b97\u6cd5\u89d2\u5ea6\u89e3\u91ca\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793aPopulation-Evolve\u5728\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u6027\u80fd\u65b9\u5dee\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8fdb\u5316\u7b56\u7565\u5728\u63a8\u7406\u65f6\u89e3\u9501LLM\u63a8\u7406\u6f5c\u529b\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.18583", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18583", "abs": "https://arxiv.org/abs/2512.18583", "authors": ["Pengcheng Li", "Qiang Fang", "Tong Zhao", "Yixing Lan", "Xin Xu"], "title": "SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models", "comment": null, "summary": "Adversarial Imitation Learning (AIL) is a dominant framework in imitation learning that infers rewards from expert demonstrations to guide policy optimization. Although providing more expert demonstrations typically leads to improved performance and greater stability, collecting such demonstrations can be challenging in certain scenarios. Inspired by the success of diffusion models in data generation, we propose SD2AIL, which utilizes synthetic demonstrations via diffusion models. We first employ a diffusion model in the discriminator to generate synthetic demonstrations as pseudo-expert data that augment the expert demonstrations. To selectively replay the most valuable demonstrations from the large pool of (pseudo-) expert demonstrations, we further introduce a prioritized expert demonstration replay strategy (PEDR). The experimental results on simulation tasks demonstrate the effectiveness and robustness of our method. In particular, in the Hopper task, our method achieves an average return of 3441, surpassing the state-of-the-art method by 89. Our code will be available at https://github.com/positron-lpc/SD2AIL.", "AI": {"tldr": "SD2AIL\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u6f14\u793a\u6765\u589e\u5f3a\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\uff0c\u901a\u8fc7\u4f18\u5148\u91cd\u653e\u7b56\u7565\u9009\u62e9\u6700\u6709\u4ef7\u503c\u7684\u6f14\u793a\uff0c\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6f14\u793a\uff0c\u4f46\u5728\u67d0\u4e9b\u573a\u666f\u4e2d\u6536\u96c6\u8fd9\u4e9b\u6f14\u793a\u5f88\u56f0\u96be\u3002\u53d7\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u751f\u6210\u65b9\u9762\u7684\u6210\u529f\u542f\u53d1\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u6f14\u793a\u6765\u589e\u5f3a\u4e13\u5bb6\u6570\u636e\u3002", "method": "1. \u5728\u5224\u522b\u5668\u4e2d\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u6f14\u793a\u4f5c\u4e3a\u4f2a\u4e13\u5bb6\u6570\u636e\u6765\u589e\u5f3a\u4e13\u5bb6\u6f14\u793a\uff1b2. \u5f15\u5165\u4f18\u5148\u4e13\u5bb6\u6f14\u793a\u91cd\u653e\u7b56\u7565(PEDR)\uff0c\u4ece\u5927\u91cf\uff08\u4f2a\uff09\u4e13\u5bb6\u6f14\u793a\u4e2d\u9009\u62e9\u6700\u6709\u4ef7\u503c\u7684\u6f14\u793a\u8fdb\u884c\u91cd\u653e\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002\u5728Hopper\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u56de\u62a5\u8fbe\u52303441\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad8\u51fa89\u3002", "conclusion": "SD2AIL\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u6f14\u793a\u548c\u4f18\u5148\u91cd\u653e\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u4e2d\u4e13\u5bb6\u6f14\u793a\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.18596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18596", "abs": "https://arxiv.org/abs/2512.18596", "authors": ["Quanxi Zhou", "Wencan Mao", "Yilei Liang", "Manabu Tsukada", "Yunling Liu", "Jon Crowcroft"], "title": "EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture", "comment": null, "summary": "The widespread application of wireless communication technology has promoted the development of smart agriculture, where unmanned aerial vehicles (UAVs) play a multifunctional role. We target a multi-UAV smart agriculture system where UAVs cooperatively perform data collection, image acquisition, and communication tasks. In this context, we model a Markov decision process to solve the multi-UAV trajectory planning problem. Moreover, we propose a novel Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC) framework, where agents adaptively learn from the elite agent to reduce trial-and-error costs, and a shared ensemble critic collaborates with each agent's local critic to ensure unbiased objective value estimates and prevent overestimation. Experimental results demonstrate that EIA-SEC outperforms state-of-the-art baselines in terms of reward performance, training stability, and convergence speed.", "AI": {"tldr": "\u63d0\u51faEIA-SEC\u6846\u67b6\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u667a\u80fd\u519c\u4e1a\u7cfb\u7edf\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u7cbe\u82f1\u6a21\u4eff\u548c\u5171\u4eab\u96c6\u6210\u6279\u8bc4\u5668\u63d0\u5347\u6027\u80fd", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\u63a8\u52a8\u667a\u80fd\u519c\u4e1a\u53d1\u5c55\uff0c\u65e0\u4eba\u673a\u5728\u6570\u636e\u6536\u96c6\u3001\u56fe\u50cf\u83b7\u53d6\u548c\u901a\u4fe1\u4efb\u52a1\u4e2d\u53d1\u6325\u591a\u529f\u80fd\u4f5c\u7528\uff0c\u9700\u8981\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u534f\u540c\u8f68\u8ff9\u89c4\u5212\u95ee\u9898", "method": "\u5efa\u7acb\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6a21\u578b\uff0c\u63d0\u51fa\u7cbe\u82f1\u6a21\u4eff\u6f14\u5458-\u5171\u4eab\u96c6\u6210\u6279\u8bc4\u5668\u6846\u67b6\uff0c\u667a\u80fd\u4f53\u4ece\u7cbe\u82f1\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u5b66\u4e60\u4ee5\u51cf\u5c11\u8bd5\u9519\u6210\u672c\uff0c\u5171\u4eab\u96c6\u6210\u6279\u8bc4\u5668\u4e0e\u672c\u5730\u6279\u8bc4\u5668\u534f\u4f5c\u786e\u4fdd\u65e0\u504f\u76ee\u6807\u503c\u4f30\u8ba1\u5e76\u9632\u6b62\u8fc7\u4f30\u8ba1", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eEIA-SEC\u5728\u5956\u52b1\u6027\u80fd\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "EIA-SEC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u65e0\u4eba\u673a\u667a\u80fd\u519c\u4e1a\u7cfb\u7edf\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u7cbe\u82f1\u6a21\u4eff\u548c\u6279\u8bc4\u5668\u96c6\u6210\u673a\u5236\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2512.19117", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19117", "abs": "https://arxiv.org/abs/2512.19117", "authors": ["Amar Lakel"], "title": "Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?", "comment": "in French language", "summary": "This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4ece\"\u5927\u578b\u8bed\u8a00\u6a21\u578b\"\u8f6c\u5411\"\u5927\u578b\u8bdd\u8bed\u6a21\u578b\"\u518d\u5230\"\u4eba\u5de5\u8bdd\u8bed\u4ee3\u7406\"\u7684\u8ba4\u8bc6\u8bba\u8f6c\u53d8\uff0c\u5efa\u7acb\u57fa\u4e8e\u73b0\u8c61\u89c4\u5f8b\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u8bed\u8a00\u7ed3\u6784\u7684\u4e09\u5143\u672c\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u516c\u5171\u8bd5\u9a8c\u548c\u6cbb\u7406\u673a\u5236\u53d6\u4ee3\u5bf9AI\u7684\"\u8ff7\u604b/\u6050\u60e7\"\u4e8c\u5206\u6cd5\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u5206\u6790\u5c40\u9650\u4e8e\"\u8bed\u8a00\u6a21\u578b\"\u8303\u7574\uff0c\u5ffd\u89c6\u4e86\u5176\u4f5c\u4e3a\u8bdd\u8bed\u5b9e\u8df5\u548c\u793e\u4f1a\u5386\u53f2\u4ea7\u7269\u7684\u672c\u8d28\u3002\u9700\u8981\u5efa\u7acb\u66f4\u5168\u9762\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3AI\u7cfb\u7edf\u5982\u4f55\u5efa\u6a21\u4eba\u7c7b\u7ecf\u9a8c\uff0c\u5e76\u8bbe\u8ba1\u6cbb\u7406\u673a\u5236\u6765\u89c4\u8303\u5176\u5728\u793e\u4f1a\u7a7a\u95f4\u4e2d\u7684\u4f4d\u7f6e\u548c\u7528\u9014\u3002", "method": "\u63d0\u51fa\u672c\u4f53\u4e09\u5143\u6846\u67b6\uff1a1) \u73b0\u8c61\u4e16\u754c\u89c4\u5f8b\u7684\u7406\u89e3\uff1b2) \u5177\u8eab\u8ba4\u77e5\u7684\u7ed3\u6784\u5316\uff1b3) \u793e\u4f1a\u5386\u53f2\u8bed\u5883\u4e2d\u7684\u8bed\u8a00\u7ed3\u6784\u6c89\u6dc0\u3002\u5c06LLM\u91cd\u65b0\u6982\u5ff5\u5316\u4e3aLDM\u548cADA\uff0c\u5206\u6790\u5176\u5982\u4f55\u5efa\u6a21\u5b66\u4e60\u8bed\u6599\u4e2d\u7269\u5316\u7684\u4eba\u7c7b\u7ecf\u9a8c\u8bdd\u8bed\u6295\u5c04\u3002", "result": "\u5efa\u7acb\u4e86\u4eceLLM\u5230LDM\u518d\u5230ADA\u7684\u6982\u5ff5\u6f14\u8fdb\u8def\u5f84\uff0c\u63d0\u4f9b\u4e86\u7406\u89e3\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u8bdd\u8bed\u4ee3\u7406\u7684\u7406\u8bba\u5de5\u5177\u3002\u63d0\u51fa\u4e86\u901a\u8fc7\u56fd\u5bb6\u3001\u4ea7\u4e1a\u3001\u516c\u6c11\u793e\u4f1a\u548c\u5b66\u672f\u754c\u7684\u5171\u540c\u6cbb\u7406\u6846\u67b6\uff0c\u4ee5\u53ca\u516c\u5171\u8bd5\u9a8c\u7a0b\u5e8f\u6765\u89c4\u8303AI\u7684\u793e\u4f1a\u5e94\u7528\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u5bf9AI\u7684\u7b80\u5355\u4e8c\u5206\u6001\u5ea6\uff0c\u901a\u8fc7\u7406\u8bba\u91cd\u6784\u548c\u6cbb\u7406\u521b\u65b0\uff0c\u4f7f\u4eba\u5de5\u8bdd\u8bed\u4ee3\u7406\u5728\u793e\u4f1a\u7a7a\u95f4\u4e2d\u7684\u4f4d\u7f6e\u3001\u7528\u9014\u548c\u9650\u5236\u53d8\u5f97\u53ef\u89e3\u8bfb\u548c\u53ef\u7ba1\u7406\uff0c\u5b9e\u73b0\u8d1f\u8d23\u4efb\u7684AI\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.19210", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19210", "abs": "https://arxiv.org/abs/2512.19210", "authors": ["Jerry Wang", "Ting Yiu Liu"], "title": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation", "comment": "Accepted at NeurIPS Workshop on Foundations of Reasoning in Language Models and Workshop on Bridging Language, Agent, and World Model", "summary": "We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine \"understanding\" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u77f3\u5934\u526a\u5200\u5e03\u6e38\u620f\u8bc4\u4f30LLM\u662f\u5426\u5c55\u73b0\u771f\u6b63\u7684\"\u7406\u89e3\"\u80fd\u529b\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u5e8f\u5217\u884c\u4e3a\u7684\u5fc3\u667a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u77f3\u5934\u526a\u5200\u5e03\u770b\u4f3c\u7b80\u5355\uff0c\u4f46\u5b83\u9700\u8981\u5e8f\u5217\u63a8\u7406\u3001\u9002\u5e94\u6027\u548c\u7b56\u7565\u8bc6\u522b\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLM\u662f\u5426\u80fd\u5c55\u73b0\u7c7b\u4f3c\u5fc3\u667a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6d4b\u8bd5\u6e38\u620f\u77e5\u8bc6\u672c\u8eab\u3002", "method": "\u5c06LLM\u4f5c\u4e3a\u89c2\u5bdf\u8005\uff0c\u8bc6\u522b\u6b63\u5728\u4f7f\u7528\u7684\u7b56\u7565\u5e76\u89e3\u91ca\u63a8\u7406\u8fc7\u7a0b\u3002\u63d0\u4f9b\u5305\u542b\u9759\u6001\u7b56\u7565\u548c\u8f7b\u91cf\u7ea7\u52a8\u6001\u7b56\u7565\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u4ea4\u53c9\u71b5\u3001Brier\u5206\u6570\u548c\u671f\u671b\u503c\u5dee\u5f02\u4e09\u79cd\u4e92\u8865\u4fe1\u53f7\u91cf\u5316\u9884\u6d4b\u4e0e\u771f\u5b9e\u5206\u5e03\u7684\u5339\u914d\u5ea6\uff0c\u5e76\u901a\u8fc7Union Loss\u7edf\u4e00\u8fd9\u4e9b\u6307\u6807\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f3a\u8c03\u4ea4\u4e92\u6027\u3001\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u7684\u6f14\u793a\u7cfb\u7edf\uff0c\u7528\u6237\u53ef\u4ee5\u5b9e\u65f6\u8c03\u6574LLM\u5206\u5e03\u3001\u53ef\u89c6\u5316\u635f\u5931\u6f14\u53d8\uff0c\u5e76\u76f4\u63a5\u68c0\u67e5\u63a8\u7406\u7247\u6bb5\u4ee5\u8bc6\u522b\u5931\u8d25\u539f\u56e0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5e8f\u5217\u6e38\u620f\u4e2d\u7684\u5fc3\u667a\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u4ee3\u7406\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u63a8\u7406\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.19126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19126", "abs": "https://arxiv.org/abs/2512.19126", "authors": ["Zihan Lin", "Xiaohan Wang", "Hexiong Yang", "Jiajun Chai", "Jie Cao", "Guojun Yin", "Wei Lin", "Ran He"], "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards", "comment": null, "summary": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.", "AI": {"tldr": "AWPO\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6574\u5408\u663e\u5f0f\u63a8\u7406\u5956\u52b1\u6765\u589e\u5f3aLLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53ef\u9a8c\u8bc1\u7ed3\u679c\u5956\u52b1\uff0c\u5ffd\u89c6\u4e86\u663e\u5f0f\u63a8\u7406\u5956\u52b1\u5bf9\u63d0\u5347\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u6f5c\u529b\u3002\u540c\u65f6\uff0c\u7b80\u5355\u7ed3\u5408\u63a8\u7406\u548c\u7ed3\u679c\u5956\u52b1\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u6216\u4e0e\u4e3b\u8981\u4f18\u5316\u76ee\u6807\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4f18\u52bf\u52a0\u6743\u7b56\u7565\u4f18\u5316(AWPO)\u6846\u67b6\uff0c\u5305\u542b\u65b9\u5dee\u611f\u77e5\u95e8\u63a7\u548c\u96be\u5ea6\u611f\u77e5\u52a0\u6743\u673a\u5236\uff0c\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7edf\u8ba1\u81ea\u9002\u5e94\u8c03\u8282\u63a8\u7406\u4fe1\u53f7\u7684\u4f18\u52bf\uff0c\u4ee5\u53ca\u5b9a\u5236\u5316\u7684\u88c1\u526a\u673a\u5236\u786e\u4fdd\u7a33\u5b9a\u4f18\u5316\u3002", "result": "AWPO\u5728\u6807\u51c6\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u548c\u9886\u5148\u95ed\u6e90\u6a21\u578b\u30024B\u53c2\u6570\u6a21\u578b\u5728\u591a\u8f6e\u51c6\u786e\u7387\u4e0a\u8d85\u8fc7Grok-4 16.0%\uff0c\u540c\u65f6\u5728MMLU-Pro\u5206\u5e03\u5916\u57fa\u51c6\u4e0a\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AWPO\u901a\u8fc7\u6709\u6548\u6574\u5408\u663e\u5f0f\u63a8\u7406\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u53c2\u6570\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\u7684\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.19228", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19228", "abs": "https://arxiv.org/abs/2512.19228", "authors": ["Valentin Schmidberger", "Manuel Eberhardinger", "Setareh Maghsudi", "Johannes Maucher"], "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models", "comment": "Accepted at ICMLA 2025, the first two authors contributed equally", "summary": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u5229\u7528\u7ecf\u8fc7\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u53d7\u9650\u786c\u4ef6\u8d44\u6e90\u4e0a\u81ea\u52a8\u751f\u6210\u57fa\u4e8e\u89c4\u5219\u7684\u5408\u7406\u6027\u68c0\u67e5\u4ee3\u7801\uff0c\u7528\u4e8e\u68c0\u6d4b\u6587\u6863\u4f2a\u9020\u3002", "motivation": "\u6587\u6863\u4f2a\u9020\u5bf9\u6cd5\u5f8b\u3001\u7ecf\u6d4e\u548c\u653f\u5e9c\u6d41\u7a0b\u6784\u6210\u65e5\u76ca\u4e25\u91cd\u7684\u5a01\u80c1\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u9a8c\u8bc1\u673a\u5236\u3002\u73b0\u6709\u7684\u5408\u7406\u6027\u68c0\u67e5\uff08\u8bc4\u4f30\u6570\u636e\u6b63\u786e\u6027\u548c\u5185\u90e8\u4e00\u81f4\u6027\u7684\u89c4\u5219\u7a0b\u5e8f\uff09\u7531\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u624b\u52a8\u5b9e\u73b0\uff0c\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u5c55\u4e3a\u81ea\u52a8\u5316\u751f\u6210\u8fd9\u4e9b\u68c0\u67e5\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u4f46\u5982\u4f55\u8ba9LLM\u9002\u5e94\u672a\u77e5\u9886\u57df\u7684\u5177\u4f53\u8981\u6c42\u4ecd\u662f\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5f00\u6e90LLM\uff08Llama 3.1 8B\u548cOpenCoder 8B\uff09\uff0c\u901a\u8fc7\u4e0d\u540c\u5fae\u8c03\u7b56\u7565\u5728\u6e90\u81ea\u771f\u5b9e\u5e94\u7528\u573a\u666f\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u9002\u5e94\u3002\u7814\u7a76\u5728\u53d7\u9650\u786c\u4ef6\u8d44\u6e90\u4e0a\u751f\u6210\u57fa\u4e8e\u89c4\u5219\u7684\u5408\u7406\u6027\u68c0\u67e5\u4ee3\u7801\uff0c\u5e76\u5728\u5148\u524d\u672a\u89c1\u8fc7\u7684\u4f2a\u9020\u6a21\u5f0f\u4e0a\u8bc4\u4f30\u751f\u6210\u7684\u9a8c\u8bc1\u7a0b\u5e8f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u80fd\u591f\u751f\u6210\u53ef\u6267\u884c\u4e14\u6709\u6548\u7684\u9a8c\u8bc1\u7a0b\u5e8f\u3002\u8fd9\u7a81\u663e\u4e86LLM\u4f5c\u4e3a\u53ef\u6269\u5c55\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u53ef\u4ee5\u5728\u9700\u8981\u53ef\u7406\u89e3\u6027\u7684\u5b89\u5168\u654f\u611f\u73af\u5883\u4e2d\u652f\u6301\u4eba\u7c7b\u51b3\u7b56\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u7ecf\u8fc7\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u540e\uff0c\u80fd\u591f\u5728\u53d7\u9650\u786c\u4ef6\u8d44\u6e90\u4e0a\u6709\u6548\u751f\u6210\u7528\u4e8e\u4f2a\u9020\u68c0\u6d4b\u7684\u89c4\u5219\u578b\u5408\u7406\u6027\u68c0\u67e5\u4ee3\u7801\uff0c\u4e3a\u5b89\u5168\u654f\u611f\u9886\u57df\u7684\u81ea\u52a8\u5316\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2512.19234", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19234", "abs": "https://arxiv.org/abs/2512.19234", "authors": ["Lingjun Mao", "Jiawei Ren", "Kun Zhou", "Jixuan Chen", "Ziqiao Ma", "Lianhui Qin"], "title": "DeliveryBench: Can Agents Earn Profit in Real World?", "comment": null, "summary": "LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.", "AI": {"tldr": "DeliveryBench\u662f\u4e00\u4e2a\u57ce\u5e02\u89c4\u6a21\u7684\u5177\u8eab\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u5916\u5356\u914d\u9001\u573a\u666f\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ea6\u675f\u611f\u77e5\u3001\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524dVLM\u667a\u80fd\u4f53\u4e0e\u4eba\u7c7b\u5728\u590d\u6742\u7ea6\u675f\u73af\u5883\u4e2d\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u7684\u77ed\u671f\u4efb\u52a1\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u4e2d\u7684\u4e30\u5bcc\u73b0\u5b9e\u7ea6\u675f\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30\u7ea6\u675f\u611f\u77e5\u3001\u957f\u65f6\u7a0b\u89c4\u5212\u7684\u5177\u8eab\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u57fa\u4e8e\u5916\u5356\u914d\u9001\u573a\u666f\uff0c\u5728\u7a0b\u5e8f\u751f\u6210\u76843D\u57ce\u5e02\u4e2d\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u9053\u8def\u7f51\u7edc\u3001\u5efa\u7b51\u3001\u529f\u80fd\u4f4d\u7f6e\u3001\u4ea4\u901a\u6a21\u5f0f\u548c\u771f\u5b9e\u8d44\u6e90\u52a8\u6001\uff0c\u8bc4\u4f30VLM\u667a\u80fd\u4f53\u5728\u4e5d\u4e2a\u57ce\u5e02\u4e2d\u7684\u8868\u73b0\u5e76\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u6bd4\u8f83\u3002", "result": "\u53d1\u73b0\u5f53\u524dVLM\u667a\u80fd\u4f53\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u77ed\u89c6\u884c\u4e3a\uff0c\u7ecf\u5e38\u8fdd\u53cd\u57fa\u672c\u5e38\u8bc6\u7ea6\u675f\uff0c\u4e0d\u540c\u6a21\u578b\u5c55\u73b0\u51fa\u4e0d\u540c\u4e2a\u6027\u7279\u5f81\uff08\u5982\u5192\u9669\u7684GPT-5 vs \u4fdd\u5b88\u7684Claude\uff09\u3002", "conclusion": "DeliveryBench\u63ed\u793a\u4e86\u5f53\u524dVLM\u5177\u8eab\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u7ea6\u675f\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u8106\u5f31\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u8bc4\u4f30\u7ea6\u675f\u611f\u77e5\u3001\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2512.19287", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19287", "abs": "https://arxiv.org/abs/2512.19287", "authors": ["Jiaao Wu", "Xian Zhang", "Fan Yang", "Yinpeng Dong"], "title": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6", "comment": "20 pages, 13 figures", "summary": "We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.", "AI": {"tldr": "Vibe Reasoning\u662f\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u8303\u5f0f\uff0c\u901a\u8fc7\u5143\u63d0\u793a\u3001\u667a\u80fd\u4f53\u57fa\u7840\u548c\u6a21\u578b\u7f16\u6392\uff0c\u5c06\u524d\u6cbfAI\u6a21\u578b\u7684\u6f5c\u5728\u77e5\u8bc6\u8f6c\u5316\u4e3a\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u7684\u5b9e\u9645\u80fd\u529b\u3002", "motivation": "\u524d\u6cbfAI\u6a21\u578b\u5df2\u5177\u5907\u89e3\u51b3\u590d\u6742\u95ee\u9898\u6240\u9700\u7684\u77e5\u8bc6\uff0c\u4f46\u4e0d\u77e5\u9053\u5982\u4f55\u3001\u4f55\u65f6\u5e94\u7528\u8fd9\u4e9b\u77e5\u8bc6\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u8fd9\u4e9b\u6f5c\u5728\u80fd\u529b\u8f6c\u5316\u4e3a\u5b9e\u9645\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u901a\u7528\u5143\u63d0\u793a\u3001\u667a\u80fd\u4f53\u57fa\u7840\u548c\u6a21\u578b\u7f16\u6392\u3002\u7ed3\u5408GPT-5\u7684\u63a2\u7d22\u80fd\u529b\u548cGemini 3 Pro\u7684\u8bc1\u660e\u4f18\u52bf\uff0c\u4f7f\u7528Python\u4ee3\u7801\u6267\u884c\u548c\u57fa\u4e8e\u6587\u4ef6\u7684\u8bb0\u5fc6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7cbe\u70bc\u5f00\u53d1\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6210\u529f\u89e3\u51b3\u4e86IMO 2025\u7b2c6\u9898\uff08\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff09\uff0c\u5f97\u5230\u4e86\u6b63\u786e\u7b54\u6848\uff082112\uff09\u548c\u4e25\u683c\u7684\u6570\u5b66\u8bc1\u660e\u3002\u53d1\u73b0\u667a\u80fd\u4f53\u57fa\u7840\u548c\u6a21\u578b\u7f16\u6392\u662f\u5fc5\u8981\u7684\uff0c\u4eba\u7c7b\u63d0\u793a\u4ece\u95ee\u9898\u7279\u5b9a\u63d0\u793a\u6f14\u53d8\u4e3a\u901a\u7528\u7684\u53ef\u8f6c\u79fb\u5143\u63d0\u793a\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7684\u4eba\u7c7b\u6307\u5bfc\u53ef\u4ee5\u89e3\u9501\u524d\u6cbf\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u6f5c\u529b\u3002\u6b63\u5728\u5f00\u53d1\u81ea\u52a8\u5316\u6846\u67b6\u5e76\u8fdb\u884c\u66f4\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u4ee5\u8fdb\u4e00\u6b65\u9a8c\u8bc1Vibe Reasoning\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.18670", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18670", "abs": "https://arxiv.org/abs/2512.18670", "authors": ["Xue Yang", "Michael Schukat", "Junlin Lu", "Patrick Mannion", "Karl Mason", "Enda Howley"], "title": "Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments", "comment": null, "summary": "Reinforcement learning (RL) excels in various applications but struggles in dynamic environments where the underlying Markov decision process evolves. Continual reinforcement learning (CRL) enables RL agents to continually learn and adapt to new tasks, but balancing stability (preserving prior knowledge) and plasticity (acquiring new knowledge) remains challenging. Existing methods primarily address the stability-plasticity dilemma through mechanisms where past knowledge influences optimization but rarely affects the agent's behavior directly, which may hinder effective knowledge reuse and efficient learning. In contrast, we propose demonstration-guided continual reinforcement learning (DGCRL), which stores prior knowledge in an external, self-evolving demonstration repository that directly guides RL exploration and adaptation. For each task, the agent dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning, gradually shifting from demonstration-guided exploration to fully self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks demonstrate its superior average performance, enhanced knowledge transfer, mitigation of forgetting, and training efficiency. The additional sensitivity analysis and ablation study further validate its effectiveness.", "AI": {"tldr": "\u63d0\u51faDGCRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u90e8\u81ea\u6f14\u5316\u7684\u6f14\u793a\u5e93\u76f4\u63a5\u6307\u5bfcRL\u63a2\u7d22\u548c\u9002\u5e94\uff0c\u89e3\u51b3\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883", "motivation": "\u4f20\u7edfRL\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6301\u7eed\u5f3a\u5316\u5b66\u4e60(CRL)\u867d\u7136\u80fd\u8ba9\u667a\u80fd\u4f53\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u4f46\u5e73\u8861\u7a33\u5b9a\u6027\uff08\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\uff09\u548c\u53ef\u5851\u6027\uff08\u83b7\u53d6\u65b0\u77e5\u8bc6\uff09\u4ecd\u7136\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u4f18\u5316\u673a\u5236\u5f71\u54cd\u8fc7\u53bb\u77e5\u8bc6\uff0c\u5f88\u5c11\u76f4\u63a5\u5f71\u54cd\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u7684\u77e5\u8bc6\u91cd\u7528\u548c\u9ad8\u6548\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u6f14\u793a\u5f15\u5bfc\u7684\u6301\u7eed\u5f3a\u5316\u5b66\u4e60(DGCRL)\uff0c\u5c06\u5148\u9a8c\u77e5\u8bc6\u5b58\u50a8\u5728\u5916\u90e8\u81ea\u6f14\u5316\u7684\u6f14\u793a\u5e93\u4e2d\uff0c\u76f4\u63a5\u6307\u5bfcRL\u63a2\u7d22\u548c\u9002\u5e94\u3002\u5bf9\u6bcf\u4e2a\u4efb\u52a1\uff0c\u667a\u80fd\u4f53\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u7684\u6f14\u793a\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u7b56\u7565\u52a0\u901f\u5b66\u4e60\uff0c\u9010\u6b65\u4ece\u6f14\u793a\u5f15\u5bfc\u63a2\u7d22\u8fc7\u6e21\u5230\u5b8c\u5168\u81ea\u4e3b\u63a2\u7d22\u3002", "result": "\u57282D\u5bfc\u822a\u548cMuJoCo\u8fd0\u52a8\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u5e73\u5747\u6027\u80fd\u3001\u589e\u5f3a\u7684\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\u3001\u51cf\u8f7b\u9057\u5fd8\u6548\u679c\u548c\u8bad\u7ec3\u6548\u7387\u3002\u989d\u5916\u7684\u654f\u611f\u6027\u5206\u6790\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DGCRL\u901a\u8fc7\u5916\u90e8\u6f14\u793a\u5e93\u76f4\u63a5\u6307\u5bfc\u884c\u4e3a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u77e5\u8bc6\u91cd\u7528\u548c\u5b66\u4e60\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.19238", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19238", "abs": "https://arxiv.org/abs/2512.19238", "authors": ["Anna-Maria Gueorguieva", "Aylin Caliskan"], "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation", "comment": null, "summary": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5LLMs\u5bf9\u975e\u53d7\u4fdd\u62a4\u6c61\u540d\u5316\u8eab\u4efd\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u9ad8\u5371\u9669\u6027\u6c61\u540d\uff08\u5982\u5e2e\u6d3e\u6210\u5458\u3001HIV\u60a3\u8005\uff09\u5728LLM\u8f93\u51fa\u4e2d\u504f\u89c1\u6700\u4e25\u91cd\uff0860%\uff09\uff0c\u800c\u793e\u4f1a\u4eba\u53e3\u6c61\u540d\uff08\u5982\u4e9a\u88d4\u7f8e\u56fd\u4eba\u3001\u8001\u5e74\u4eba\uff09\u504f\u89c1\u6700\u5c11\uff0811%\uff09\u3002\u4f7f\u7528\u62a4\u680f\u6a21\u578b\u53ef\u51cf\u5c11\u504f\u89c1\u4f46\u6548\u679c\u6709\u9650\uff0c\u4e14\u62a4\u680f\u6a21\u578b\u5e38\u65e0\u6cd5\u8bc6\u522b\u504f\u89c1\u610f\u56fe\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u88ab\u8bc1\u660e\u5b58\u5728\u793e\u4f1a\u504f\u89c1\uff0c\u4f46\u5bf9\u975e\u53d7\u4fdd\u62a4\u6c61\u540d\u5316\u8eab\u4efd\u7684\u504f\u89c1\u7814\u7a76\u4e0d\u8db3\u3002\u5fc3\u7406\u5b66\u6587\u732e\u8868\u660e\u6c61\u540d\u5305\u542b\u516d\u4e2a\u5171\u540c\u793e\u4f1a\u7279\u5f81\uff1a\u5ba1\u7f8e\u6027\u3001\u53ef\u9690\u85cf\u6027\u3001\u75c5\u7a0b\u3001\u7834\u574f\u6027\u3001\u8d77\u6e90\u548c\u5371\u9669\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4eba\u7c7b\u548cLLM\u5bf9\u6c61\u540d\u7279\u5f81\u7684\u8bc4\u5206\u3001\u63d0\u793a\u98ce\u683c\u548c\u6c61\u540d\u7c7b\u578b\u662f\u5426\u5f71\u54cdLLM\u8f93\u51fa\u4e2d\u5bf9\u6c61\u540d\u7fa4\u4f53\u7684\u504f\u89c1\u3002", "method": "\u4f7f\u7528SocialStigmaQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b37\u4e2a\u5173\u4e8e\u6c61\u540d\u8eab\u4efd\u7684\u793e\u4f1a\u573a\u666f\uff08\u5982\u51b3\u5b9a\u662f\u5426\u63a8\u8350\u5b9e\u4e60\uff09\u3002\u6d4b\u91cf\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\uff08Granite 3.0-8B\u3001Llama-3.1-8B\u3001Mistral-7B\uff09\u5bf993\u4e2a\u6c61\u540d\u7fa4\u4f53\u7684\u504f\u89c1\u3002\u5206\u6790\u6c61\u540d\u7279\u5f81\uff08\u4eba\u7c7b\u548cLLM\u8bc4\u5206\uff09\u3001\u63d0\u793a\u98ce\u683c\u548c\u6c61\u540d\u7c7b\u578b\u5bf9\u504f\u89c1\u7684\u5f71\u54cd\u3002\u6d4b\u8bd5\u4f7f\u7528\u5404\u81ea\u62a4\u680f\u6a21\u578b\uff08Granite Guardian 3.0\u3001Llama Guard 3.0\u3001Mistral Moderation API\uff09\u80fd\u5426\u51cf\u5c11\u504f\u89c1\u3002", "result": "\u4eba\u7c7b\u8bc4\u4e3a\u9ad8\u5371\u9669\u6027\u7684\u6c61\u540d\uff08\u5982\u5e2e\u6d3e\u6210\u5458\u3001HIV\u60a3\u8005\uff09\u5728SocialStigmaQA\u63d0\u793a\u4e2d\u504f\u89c1\u8f93\u51fa\u6700\u591a\uff08\u6240\u6709\u6a21\u578b\u768460%\uff09\uff0c\u800c\u793e\u4f1a\u4eba\u53e3\u6c61\u540d\uff08\u5982\u4e9a\u88d4\u7f8e\u56fd\u4eba\u3001\u8001\u5e74\u4eba\uff09\u504f\u89c1\u8f93\u51fa\u6700\u5c11\uff0811%\uff09\u3002\u4f7f\u7528\u62a4\u680f\u6a21\u578b\u540e\u504f\u89c1\u5206\u522b\u51cf\u5c1110.4%\u30011.4%\u548c7.8%\uff0c\u4f46\u504f\u89c1\u5f71\u54cd\u663e\u8457\u7684\u7279\u5f81\u5728\u7f13\u89e3\u540e\u4fdd\u6301\u4e0d\u53d8\uff0c\u4e14\u62a4\u680f\u6a21\u578b\u5e38\u65e0\u6cd5\u8bc6\u522b\u63d0\u793a\u4e2d\u7684\u504f\u89c1\u610f\u56fe\u3002", "conclusion": "LLMs\u5bf9\u6c61\u540d\u5316\u7fa4\u4f53\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u7279\u522b\u662f\u9ad8\u5371\u9669\u6027\u6c61\u540d\u3002\u62a4\u680f\u6a21\u578b\u867d\u80fd\u51cf\u5c11\u504f\u89c1\u4f46\u6548\u679c\u6709\u9650\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u504f\u89c1\u610f\u56fe\u3002\u8fd9\u5bf9\u5728\u6d89\u53ca\u6c61\u540d\u5316\u7fa4\u4f53\u7684\u573a\u666f\u4e2d\u4f7f\u7528LLMs\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5efa\u8bae\u672a\u6765\u5de5\u4f5c\u6539\u8fdb\u62a4\u680f\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u7f13\u89e3\u504f\u89c1\u3002", "topic": "agent analysis"}}
{"id": "2512.19355", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19355", "abs": "https://arxiv.org/abs/2512.19355", "authors": ["Simon St\u00e5hlberg", "Hector Geffner"], "title": "First-Order Representation Languages for Goal-Conditioned RL", "comment": "In Proceedings of the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u5728\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u4e00\u9636\u5173\u7cfb\u8bed\u8a00\u548cHindsight Experience Replay\u6280\u672f\uff0c\u901a\u8fc7\u539f\u5b50\u96c6\u8868\u793a\u72b6\u6001\u548c\u76ee\u6807\uff0c\u81ea\u52a8\u521b\u5efa\u96be\u5ea6\u9012\u589e\u7684\u5b50\u76ee\u6807\u8bfe\u7a0b\uff0c\u4ece\u800c\u5728\u5927\u578b\u89c4\u5212\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u901a\u7528\u7b56\u7565\u3002", "motivation": "\u5728\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u548c\u901a\u7528\u89c4\u5212\u4e2d\uff0c\u5f53\u8bad\u7ec3\u5b9e\u4f8b\u89c4\u6a21\u5927\u4e14\u76ee\u6807\u65e0\u6cd5\u901a\u8fc7\u968f\u673a\u63a2\u7d22\u8fbe\u6210\u65f6\uff0c\u5982\u4f55\u5b66\u4e60\u80fd\u591f\u8de8\u72b6\u6001\u548c\u76ee\u6807\u6cdb\u5316\u7684\u7b56\u7565\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u9700\u8981\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u6570\u636e\u548c\u65f6\u95f4\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e00\u9636\u5173\u7cfb\u8bed\u8a00\u8868\u793a\u72b6\u6001\u548c\u76ee\u6807\uff0c\u7ed3\u5408Hindsight Experience Replay\u6280\u672f\uff0c\u63d0\u51fa\u4e09\u79cd\u76ee\u6807\u8868\u793a\u65b9\u5f0f\uff1a\u5b8c\u6574\u72b6\u6001\u76ee\u6807\u3001\u539f\u59cb\u76ee\u6807\u5b50\u96c6\u76ee\u6807\u3001\u4ee5\u53ca\u8fd9\u4e9b\u5b50\u76ee\u6807\u7684\u63d0\u5347\u7248\u672c\u3002\u901a\u8fc7\u81ea\u52a8\u521b\u5efa\u96be\u5ea6\u9012\u589e\u7684\u5b50\u76ee\u6807\u8bfe\u7a0b\u6765\u4fc3\u8fdb\u5b66\u4e60\u3002", "result": "\u540e\u4e24\u79cd\u76ee\u6807\u8868\u793a\u65b9\u6cd5\uff08\u5b50\u96c6\u76ee\u6807\u548c\u63d0\u5347\u5b50\u76ee\u6807\uff09\u6210\u529f\u5728\u5177\u6709\u7a00\u758f\u5956\u52b1\u7684\u5927\u578b\u89c4\u5212\u5b9e\u4f8b\u4e0a\u5b66\u4e60\u4e86\u901a\u7528\u7b56\u7565\u3002\u5b9e\u9a8c\u5c55\u793a\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u8ba1\u7b97\u4f18\u52bf\u3001\u5c40\u9650\u6027\u4ee5\u53ca\u6539\u8fdb\u673a\u4f1a\u3002", "conclusion": "\u901a\u8fc7\u539f\u5b50\u96c6\u8868\u793a\u72b6\u6001\u548c\u76ee\u6807\uff0c\u7ed3\u5408HER\u6280\u672f\u81ea\u52a8\u521b\u5efa\u5b50\u76ee\u6807\u8bfe\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5728\u5927\u578b\u89c4\u5212\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u901a\u7528\u7b56\u7565\u7684\u6570\u636e\u548c\u65f6\u95f4\u6548\u7387\uff0c\u4e3a\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.19396", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19396", "abs": "https://arxiv.org/abs/2512.19396", "authors": ["Runze Li", "Yuwen Zhai", "Bo Xu", "LiWu Xu", "Nian Shi", "Wei Zhang", "Ran Lin", "Liang Wang"], "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration", "comment": null, "summary": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.", "AI": {"tldr": "EchoTrail-GUI\uff1a\u4e00\u4e2a\u4e3aGUI\u4ee3\u7406\u5f15\u5165\u52a8\u6001\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6784\u5efa\u4efb\u52a1\u8f68\u8ff9\u6570\u636e\u5e93\u3001\u68c0\u7d22\u76f8\u5173\u8bb0\u5fc6\u5e76\u6ce8\u5165\u4e0a\u4e0b\u6587\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524dGUI\u4ee3\u7406\u5b58\u5728\"\u6570\u5b57\u9057\u5fd8\"\u95ee\u9898\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5b64\u7acb\u5904\u7406\uff0c\u65e0\u6cd5\u4ece\u8fc7\u53bb\u6210\u529f\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u3001\u91cd\u590d\u9519\u8bef\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u7ecf\u9a8c\u63a2\u7d22\uff1a\u4ee3\u7406\u81ea\u4e3b\u4e0eGUI\u73af\u5883\u4ea4\u4e92\uff0c\u6784\u5efa\u7ecf\u8fc7\u5956\u52b1\u6a21\u578b\u9a8c\u8bc1\u7684\u6210\u529f\u4efb\u52a1\u8f68\u8ff9\u6570\u636e\u5e93\uff1b2) \u8bb0\u5fc6\u6ce8\u5165\uff1a\u4e3a\u65b0\u4efb\u52a1\u9ad8\u6548\u68c0\u7d22\u6700\u76f8\u5173\u5386\u53f2\u8f68\u8ff9\u4f5c\u4e3a\"\u8bb0\u5fc6\"\uff1b3) GUI\u4efb\u52a1\u63a8\u7406\uff1a\u5c06\u8bb0\u5fc6\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6307\u5bfc\u6ce8\u5165\u4ee3\u7406\u7684\u63a8\u7406\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5728Android World\u548cAndroidLab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEchoTrail-GUI\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u7ebf\u4ee3\u7406\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u64cd\u4f5c\u6548\u7387\u3002", "conclusion": "\u7ed3\u6784\u5316\u8bb0\u5fc6\u5bf9\u4e8e\u521b\u5efa\u66f4\u5f3a\u5927\u3001\u66f4\u667a\u80fd\u7684GUI\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\uff0cEchoTrail-GUI\u9a8c\u8bc1\u4e86\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u5b9e\u73b0\u7c7b\u4eba\u7ecf\u9a8c\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.19458", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.19458", "abs": "https://arxiv.org/abs/2512.19458", "authors": ["Zeyu Xia", "Jinzhe Ma", "Congjie Zheng", "Shufei Zhang", "Yuqiang Li", "Hang Su", "P. Hu", "Changshui Zhang", "Xingao Gong", "Wanli Ouyang", "Lei Bai", "Dongzhan Zhou", "Mao Su"], "title": "An Agentic Framework for Autonomous Materials Computation", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u6750\u6599\u8ba1\u7b97\u7684\u9886\u57df\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5d4c\u5165\u9886\u57df\u77e5\u8bc6\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u548c\u6536\u655b\u53c2\u6570\u9009\u62e9\uff0c\u663e\u8457\u4f18\u4e8e\u72ec\u7acbLLM\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u9759\u6001\u77e5\u8bc6\u548c\u5e7b\u89c9\u95ee\u9898\u963b\u788d\u4e86\u81ea\u4e3b\u7814\u7a76\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u53ef\u9760\u6267\u884c\u590d\u6742\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u9886\u57df\u4e13\u4e1a\u5316\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u6750\u6599\u8ba1\u7b97\u3002\u8be5\u6846\u67b6\u5d4c\u5165\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\uff0c\u5e76\u81ea\u52a8\u9009\u62e9\u6536\u655b\u4e14\u826f\u5b9a\u7684\u53c2\u6570\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u8ba1\u7b97\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u72ec\u7acb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u53ef\u9760\u5730\u6267\u884c\u7aef\u5230\u7aef\u7684\u8ba1\u7b97\u6d41\u7a0b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u81ea\u4e3b\u8ba1\u7b97\u5b9e\u9a8c\u5efa\u7acb\u4e86\u53ef\u9a8c\u8bc1\u7684\u57fa\u7840\uff0c\u4ee3\u8868\u4e86\u5411\u5b8c\u5168\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u8fc8\u51fa\u7684\u5173\u952e\u4e00\u6b65\u3002", "topic": "code agent"}}
{"id": "2512.18763", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18763", "abs": "https://arxiv.org/abs/2512.18763", "authors": ["Minh Vu", "Konstantinos Slavakis"], "title": "Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Unlike their conventional use as estimators of probability density functions in reinforcement learning (RL), this paper introduces a novel function-approximation role for Gaussian mixture models (GMMs) as direct surrogates for Q-function losses. These parametric models, termed GMM-QFs, possess substantial representational capacity, as they are shown to be universal approximators over a broad class of functions. They are further embedded within Bellman residuals, where their learnable parameters -- a fixed number of mixing weights, together with Gaussian mean vectors and covariance matrices -- are inferred from data via optimization on a Riemannian manifold. This geometric perspective on the parameter space naturally incorporates Riemannian optimization into the policy-evaluation step of standard policy-iteration frameworks. Rigorous theoretical results are established, and supporting numerical tests show that, even without access to experience data, GMM-QFs deliver competitive performance and, in some cases, outperform state-of-the-art approaches across a range of benchmark RL tasks, all while maintaining a significantly smaller computational footprint than deep-learning methods that rely on experience data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684GMM-QFs\u65b9\u6cd5\uff0c\u5c06\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4f5c\u4e3aQ\u51fd\u6570\u635f\u5931\u7684\u76f4\u63a5\u66ff\u4ee3\uff0c\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u8fdb\u884c\u5b66\u4e60\uff0c\u5728\u591a\u79cd\u57fa\u51c6RL\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "motivation": "\u4f20\u7edf\u4e0aGMMs\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e3b\u8981\u7528\u4e8e\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u4f30\u8ba1\uff0c\u4f46\u672c\u6587\u63a2\u7d22GMMs\u4f5c\u4e3a\u51fd\u6570\u903c\u8fd1\u5668\u7684\u65b0\u89d2\u8272\uff0c\u65e8\u5728\u63d0\u4f9b\u5177\u6709\u5f3a\u5927\u8868\u793a\u80fd\u529b\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684Q\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGMM-QFs\u65b9\u6cd5\uff1a1) \u5c06GMMs\u4f5c\u4e3aQ\u51fd\u6570\u635f\u5931\u7684\u53c2\u6570\u5316\u66ff\u4ee3\u6a21\u578b\uff1b2) \u5c06GMMs\u5d4c\u5165Bellman\u6b8b\u5dee\u4e2d\uff1b3) \u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u4f18\u5316GMM\u53c2\u6570\uff08\u6df7\u5408\u6743\u91cd\u3001\u9ad8\u65af\u5747\u503c\u5411\u91cf\u548c\u534f\u65b9\u5dee\u77e9\u9635\uff09\uff1b4) \u5c06\u9ece\u66fc\u4f18\u5316\u6574\u5408\u5230\u6807\u51c6\u7b56\u7565\u8fed\u4ee3\u6846\u67b6\u7684\u7b56\u7565\u8bc4\u4f30\u6b65\u9aa4\u4e2d\u3002", "result": "\u7406\u8bba\u8bc1\u660eGMM-QFs\u5177\u6709\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff1b\u5b9e\u9a8c\u8868\u660e\u5373\u4f7f\u4e0d\u4f7f\u7528\u7ecf\u9a8c\u6570\u636e\uff0cGMM-QFs\u5728\u591a\u79cd\u57fa\u51c6RL\u4efb\u52a1\u4e2d\u4e5f\u80fd\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u6709\u65f6\u751a\u81f3\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6bd4\u4f9d\u8d56\u7ecf\u9a8c\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u66f4\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "GMM-QFs\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u51fd\u6570\u903c\u8fd1\u63d0\u4f9b\u4e86\u65b0\u9896\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\u3001\u7406\u8bba\u4fdd\u8bc1\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3aRL\u65b9\u6cd5\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.19424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19424", "abs": "https://arxiv.org/abs/2512.19424", "authors": ["Jian Yang", "Wei Zhang", "Yizhi Li", "Shawn Guo", "Haowen Wang", "Aishan Liu", "Ge Zhang", "Zili Wang", "Zhoujun Li", "Xianglong Liu", "Weifeng Lv"], "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models", "comment": null, "summary": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.", "AI": {"tldr": "\u63d0\u51faCodeSimpleQA\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4ee3\u7801LLM\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5e76\u5f00\u53d1\u5305\u542b6600\u4e07\u6837\u672c\u7684\u6307\u4ee4\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u7f16\u7a0b\u77e5\u8bc6\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u6267\u884c\u6b63\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u7f16\u7a0b\u77e5\u8bc6\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u786e\u4fdd\u5176\u5173\u4e8e\u7f16\u7a0b\u6982\u5ff5\u3001\u6280\u672f\u5b9e\u73b0\u7b49\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "1) \u521b\u5efaCodeSimpleQA\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7cbe\u5fc3\u7b56\u5212\u7684\u82f1\u4e2d\u53cc\u8bed\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\uff1b2) \u6784\u5efaCodeSimpleQA-Instruct\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5305\u542b6600\u4e07\u6837\u672c\uff1b3) \u5f00\u53d1\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5373\u4f7f\u662f\u524d\u6cbfLLM\u5728\u4ee3\u7801\u4e8b\u5b9e\u6027\u65b9\u9762\u4e5f\u5b58\u5728\u56f0\u96be\u3002\u63d0\u51fa\u7684\u8bad\u7ec3\u6846\u67b6\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5f3a\u8c03\u4e86\u4e8b\u5b9e\u6027\u5bf9\u9f50\u5728\u5f00\u53d1\u53ef\u9760\u4ee3\u7801LLM\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7f16\u7a0b\u77e5\u8bc6\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u662f\u4ee3\u7801LLM\u7684\u91cd\u8981\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u63d0\u51fa\u7684CodeSimpleQA\u57fa\u51c6\u548c\u8bad\u7ec3\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5bf9\u5f00\u53d1\u53ef\u9760\u7684\u4ee3\u7801LLM\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "code agent"}}
{"id": "2512.19432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19432", "abs": "https://arxiv.org/abs/2512.19432", "authors": ["Quyu Kong", "Xu Zhang", "Zhenyu Yang", "Nolan Gao", "Chen Liu", "Panrong Tong", "Chenglin Cai", "Hanzhang Zhou", "Jianan Zhang", "Liangyu Chen", "Zhidan Liu", "Steven Hoi", "Yue Wang"], "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments", "comment": null, "summary": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.", "AI": {"tldr": "MobileWorld\u662f\u4e00\u4e2a\u6bd4AndroidWorld\u66f4\u5177\u6311\u6218\u6027\u7684\u79fb\u52a8\u5e94\u7528\u4f7f\u7528\u57fa\u51c6\uff0c\u5305\u542b201\u4e2a\u8de820\u4e2a\u5e94\u7528\u7684\u4efb\u52a1\uff0c\u5f3a\u8c03\u957f\u65f6\u7a0b\u4efb\u52a1\u3001\u8de8\u5e94\u7528\u4ea4\u4e92\u3001\u7528\u6237\u4e92\u52a8\u548cMCP\u589e\u5f3a\u4efb\u52a1\uff0c\u5f53\u524d\u6700\u4f73\u4ee3\u7406\u6210\u529f\u7387\u4ec551.7%", "motivation": "\u73b0\u6709AndroidWorld\u57fa\u51c6\u5df2\u9971\u548c\uff08\u4ee3\u7406\u6210\u529f\u7387\u8d8590%\uff09\uff0c\u7f3a\u4e4f\u7535\u5546\u548c\u4f01\u4e1a\u901a\u4fe1\u7b49\u5173\u952e\u5e94\u7528\u7c7b\u522b\uff0c\u4e14\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u79fb\u52a8\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u6a21\u7cca\u7528\u6237\u6307\u4ee4\u548c\u6df7\u5408\u5de5\u5177\u4f7f\u7528", "method": "\u5f15\u5165MobileWorld\u57fa\u51c6\uff0c\u5305\u542b201\u4e2a\u8de820\u4e2a\u5e94\u7528\u7684\u4efb\u52a1\uff0c\u5f3a\u8c03\u957f\u65f6\u7a0b\u4efb\u52a1\uff08\u5e73\u574727.8\u6b65\uff09\u548c\u8de8\u5e94\u7528\u4ea4\u4e92\uff0862.2%\u591a\u5e94\u7528\u4efb\u52a1\uff09\uff0c\u65b0\u589e\u7528\u6237\u4ea4\u4e92\u548cMCP\u589e\u5f3a\u4efb\u52a1\u7c7b\u522b\uff0c\u63d0\u4f9b\u57fa\u4e8e\u5feb\u7167\u7684\u5bb9\u5668\u73af\u5883\u548c\u7cbe\u786e\u529f\u80fd\u9a8c\u8bc1", "result": "\u76f8\u6bd4AndroidWorld\u51fa\u73b0\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff1a\u6700\u4f73\u4ee3\u7406\u6846\u67b6\u6210\u529f\u738751.7%\uff0c\u7aef\u5230\u7aef\u6a21\u578b\u6210\u529f\u7387\u4ec520.9%\u3002\u5f53\u524d\u6a21\u578b\u5728\u7528\u6237\u4ea4\u4e92\u548cMCP\u8c03\u7528\u65b9\u9762\u8868\u73b0\u4e0d\u4f73", "conclusion": "MobileWorld\u4e3a\u4e0b\u4e00\u4ee3\u79fb\u52a8\u667a\u80fd\u63d0\u4f9b\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u771f\u5b9e\u79fb\u52a8\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u7528\u6237\u4ea4\u4e92\u548cMCP\u8c03\u7528\u65b9\u9762", "topic": "swe benchmark"}}
{"id": "2512.19551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19551", "abs": "https://arxiv.org/abs/2512.19551", "authors": ["Jiawen Wang", "Jingjing Wang Tianyang Chen", "Min Zhang", "Guodong Zhou"], "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios", "comment": null, "summary": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faL^2-EMG\u4efb\u52a1\uff0c\u65e8\u5728\u8ba9LLM\u6301\u7eed\u5b66\u4e60\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u60c5\u611f\u8fd0\u52a8\u751f\u6210\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1ES-MoE\u65b9\u6cd5\u89e3\u51b3\u60c5\u611f\u89e3\u8026\u548c\u573a\u666f\u9002\u5e94\u4e24\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u56fa\u5b9a\u5c3a\u5ea6\u6570\u636e\u96c6\uff0c\u5ffd\u7565\u4e86\u7075\u6d3b\u4e14\u5c3a\u5ea6\u9012\u589e\u7684\u8fd0\u52a8\u573a\u666f\uff08\u5982\u4f53\u80b2\u3001\u821e\u8e48\uff09\u3002\u6709\u6548\u5b66\u4e60\u8fd9\u4e9b\u65b0\u5174\u573a\u666f\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faES-MoE\u65b9\u6cd5\uff0c\u5305\u542b\u56e0\u679c\u5f15\u5bfc\u7684\u60c5\u611f\u89e3\u8026\u5757\u548c\u573a\u666f\u9002\u5e94\u7684\u4e13\u5bb6\u6784\u5efa\u5757\uff0c\u5206\u522b\u89e3\u51b3\u60c5\u611f\u89e3\u8026\u548c\u573a\u666f\u9002\u5e94\u6311\u6218\u3002", "result": "\u6784\u5efa\u591a\u4e2aL^2-EMG\u6570\u636e\u96c6\u9a8c\u8bc1ES-MoE\u6709\u6548\u6027\uff0c\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660eES-MoE\u4f18\u4e8e\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684L^2-EMG\u4efb\u52a1\u548cES-MoE\u65b9\u6cd5\u6709\u52a9\u4e8e\u6784\u5efa\u5177\u6709\u5171\u60c5\u548c\u667a\u80fd\u7684\u95ed\u73af\u81ea\u6f14\u5316\u5177\u8eab\u667a\u80fd\u4f53\u3002", "topic": "agent analysis"}}
{"id": "2512.19691", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.19691", "abs": "https://arxiv.org/abs/2512.19691", "authors": ["Junze Ye", "Daniel Tawfik", "Alex J. Goodell", "Nikhil V. Kotha", "Mark K. Buyyounouski", "Mohsen Bayati"], "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight", "comment": null, "summary": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u4e34\u5e8a\u98ce\u9669\u8bc4\u5206\u8ba1\u7b97\u7b49\u590d\u6742\u4efb\u52a1\u7684\u57fa\u51c6\u89c6\u4e3a\"\u8fdb\u884c\u4e2d\u7684\u6d3b\u6587\u6863\"\uff0c\u9700\u8981\u5b9a\u671f\u91cd\u65b0\u8bc4\u4f30\u3002\u901a\u8fc7\u533b\u751f\u53c2\u4e0e\u7684\u9a8c\u8bc1\u6d41\u7a0b\u53d1\u73b0\u539f\u59cb\u57fa\u51c6\u5b58\u5728\u6807\u7b7e\u566a\u58f0\uff0c\u4fee\u6b63\u540e\u7684\u6807\u7b7e\u4f7f\u6a21\u578b\u6027\u80fd\u63d0\u53478.7%\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u8bc4\u4f30\u4e34\u5e8a\u98ce\u9669\u8bc4\u5206\u81ea\u52a8\u8ba1\u7b97\u7684\u57fa\u51c6MedCalc-Bench\u5b58\u5728\u6a21\u578b\u751f\u6210\u9519\u8bef\u88ab\u56fa\u5316\u4e3a\u8bc4\u4f30\u6807\u51c6\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4fe1\u53f7\u65f6\uff0c\u8fd9\u79cd\u95ee\u9898\u4f1a\u88ab\u653e\u5927\u3002\u9700\u8981\u5efa\u7acb\u52a8\u6001\u7684\u57fa\u51c6\u7ef4\u62a4\u673a\u5236\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u6027\u3001\u533b\u751f\u53c2\u4e0e\u7684\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u5229\u7528\u5148\u8fdb\u7684\u4ee3\u7406\u9a8c\u8bc1\u5668\u5ba1\u8ba1\u548c\u91cd\u65b0\u6807\u6ce8MedCalc-Bench\uff0c\u901a\u8fc7\u81ea\u52a8\u5206\u7c7b\u4fdd\u7559\u7a00\u7f3a\u7684\u4e34\u5e8a\u533b\u751f\u6ce8\u610f\u529b\u7528\u4e8e\u6700\u6709\u4e89\u8bae\u7684\u6848\u4f8b\u3002\u4f7f\u7528GRPO\u5bf9Qwen3-8B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u6bd4\u8f83\u539f\u59cb\u6807\u7b7e\u548c\u4fee\u6b63\u6807\u7b7e\u7684\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u5ba1\u8ba1\u53d1\u73b0\u539f\u59cb\u6807\u7b7e\u5b58\u5728\u663e\u8457\u9519\u8bef\uff0c\u5305\u62ec\u63d0\u53d6\u9519\u8bef\u3001\u8ba1\u7b97\u5668\u903b\u8f91\u4e0d\u5339\u914d\u548c\u4e34\u5e8a\u6a21\u7cca\u6027\u3002\u5728\u4fee\u6b63\u6807\u7b7e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u6bd4\u539f\u59cb\u57fa\u7ebf\u51c6\u786e\u7387\u7edd\u5bf9\u63d0\u53478.7%\uff0c\u8bc1\u660e\u6807\u7b7e\u566a\u58f0\u786e\u5b9e\u5f71\u54cd\u6a21\u578b\u8bc4\u4f30\u3002", "conclusion": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u4e25\u683c\u7684\u57fa\u51c6\u7ef4\u62a4\u662f\u771f\u6b63\u6a21\u578b\u5bf9\u9f50\u7684\u524d\u63d0\u6761\u4ef6\u3002\u590d\u6742\u4efb\u52a1\u57fa\u51c6\u5e94\u88ab\u89c6\u4e3a\u9700\u8981\u5b9a\u671f\u91cd\u65b0\u8bc4\u4f30\u7684\"\u6d3b\u6587\u6863\"\uff0c\u800c\u4e0d\u662f\u9759\u6001\u7684\u795e\u8c15\u3002", "topic": "agent analysis"}}
{"id": "2512.18934", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18934", "abs": "https://arxiv.org/abs/2512.18934", "authors": ["Michael S. Zhang", "Rishi A. Ruia", "Arnav Kewalram", "Saathvik Dharmapuram", "Utkarsh Sharma", "Kevin Zhu"], "title": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models", "comment": null, "summary": "Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.", "AI": {"tldr": "\u91cf\u5316\u7cbe\u5ea6\u4e0e\u56de\u653e\u7f13\u51b2\u533a\u7b56\u7565\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\u7814\u7a76\uff1a\u91cf\u5316\u6a21\u578b\uff08INT8/INT4\uff09\u5728\u540e\u7eed\u4efb\u52a1\u4e2d\u8d85\u8d8aFP16\u6a21\u578b\uff0cINT4\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u6027\u80fd\u7ffb\u500d\uff0c\u5c0f\u56de\u653e\u7f13\u51b2\u533a\uff080.1%\uff09\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u4fdd\u7559", "motivation": "\u7814\u7a76\u91cf\u5316\u7cbe\u5ea6\uff08FP16\u3001INT8\u3001INT4\uff09\u4e0e\u56de\u653e\u7f13\u51b2\u533a\u7b56\u7565\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u90e8\u7f72\u9ad8\u6548\u538b\u7f29\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc", "method": "\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u91cf\u5316\u7cbe\u5ea6\uff08FP16\u3001INT8\u3001INT4\uff09\u4e0e\u56de\u653e\u7f13\u51b2\u533a\u7b56\u7565\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u91cf\u5316\u566a\u58f0\u4f5c\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\u7684\u4f5c\u7528\uff0c\u8bc4\u4f30\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\uff08NLU\u3001\u6570\u5b66\u3001\u4ee3\u7801\u751f\u6210\uff09\u5bf9\u7f13\u51b2\u533a\u5927\u5c0f\u7684\u9700\u6c42", "result": "\u91cf\u5316\u6a21\u578b\u5728\u540e\u7eed\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eFP16\u6a21\u578b\uff08\u63d0\u53478-15%\uff09\uff0cINT4\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u6027\u80fd\u7ffb\u500d\uff0840% vs 20%\uff09\uff1b\u5c0f\u56de\u653e\u7f13\u51b2\u533a\uff080.1%\uff09\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u4fdd\u7559\uff08NLU\u4fdd\u7559\u7387\u4ece45%\u63d0\u5347\u81f365%\uff09\uff1bINT8\u5728\u53ef\u5851\u6027-\u4fdd\u7559\u5e73\u8861\u4e0a\u8868\u73b0\u6700\u4f18", "conclusion": "\u91cf\u5316\u566a\u58f0\u53ef\u4f5c\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u6311\u6218\u4e86\"\u7cbe\u5ea6\u8d8a\u9ad8\u8d8a\u597d\"\u7684\u4f20\u7edf\u89c2\u5ff5\uff1bINT8\u91cf\u5316\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6301\u7eed\u5b66\u4e60\u52a8\u6001\u4e0a\u5747\u8868\u73b0\u4f18\u8d8a\uff1b\u4e3a\u90e8\u7f72\u538b\u7f29\u6a21\u578b\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff1aNLU\u4efb\u52a1\u97001-2%\u5c0f\u7f13\u51b2\u533a\uff0c\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u97005-10%\u4e2d\u7b49\u7f13\u51b2\u533a", "topic": "agent analysis"}}
{"id": "2512.18950", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18950", "abs": "https://arxiv.org/abs/2512.18950", "authors": ["Saman Forouzandeh", "Wei Peng", "Parham Moradi", "Xinghuo Yu", "Mahdi Jalili"], "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement", "comment": "Accepted at The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). 21 pages including references, with 7 figures and 8 tables. Code is publicly available at the authors GitHub repository: https://github.com/S-Forouzandeh/MACLA-LLM-Agents-AAMAS-Conference", "summary": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.", "AI": {"tldr": "MACLA\u662f\u4e00\u4e2a\u5c06\u63a8\u7406\u4e0e\u5b66\u4e60\u89e3\u8026\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ef4\u62a4\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5916\u90e8\u5c42\u6b21\u5316\u7a0b\u5e8f\u8bb0\u5fc6\u4e2d\u6267\u884c\u6240\u6709\u9002\u5e94\u3002\u5b83\u4ece\u8f68\u8ff9\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u7a0b\u5e8f\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u540e\u9a8c\u8ddf\u8e2a\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u671f\u671b\u6548\u7528\u8bc4\u5206\u9009\u62e9\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u6210\u529f\u4e0e\u5931\u8d25\u6765\u7cbe\u70bc\u7a0b\u5e8f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u901a\u5e38\u9700\u8981\u5fae\u8c03LLM\u53c2\u6570\uff0c\u8fd9\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6837\u672c\u6548\u7387\u4f4e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u66f4\u65b0LLM\u53c2\u6570\u5c31\u80fd\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6301\u7eed\u6539\u8fdb\u7684\u667a\u80fd\u4f53\u6846\u67b6\u3002", "method": "MACLA\u91c7\u7528\u5916\u90e8\u5c42\u6b21\u5316\u7a0b\u5e8f\u8bb0\u5fc6\uff0c\u5305\u542b\uff1a1\uff09\u4ece\u8f68\u8ff9\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u7a0b\u5e8f\uff1b2\uff09\u901a\u8fc7\u8d1d\u53f6\u65af\u540e\u9a8c\u8ddf\u8e2a\u7a0b\u5e8f\u53ef\u9760\u6027\uff1b3\uff09\u901a\u8fc7\u671f\u671b\u6548\u7528\u8bc4\u5206\u9009\u62e9\u52a8\u4f5c\uff1b4\uff09\u901a\u8fc7\u5bf9\u6bd4\u6210\u529f\u4e0e\u5931\u8d25\u6848\u4f8b\u6765\u7cbe\u70bc\u7a0b\u5e8f\u3002\u6240\u6709\u9002\u5e94\u90fd\u5728\u5916\u90e8\u8bb0\u5fc6\u4e2d\u8fdb\u884c\uff0c\u4fdd\u6301LLM\u51bb\u7ed3\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08ALFWorld\u3001WebShop\u3001TravelPlanner\u3001InterCodeSQL\uff09\u4e0a\u5e73\u5747\u6027\u80fd\u8fbe\u523078.1%\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u3002\u5728ALFWorld\u672a\u89c1\u4efb\u52a1\u4e0a\u8fbe\u523090.3%\u6027\u80fd\uff0c\u67093.1%\u6b63\u5411\u6cdb\u5316\u3002\u7cfb\u7edf\u572856\u79d2\u5185\u6784\u5efa\u8bb0\u5fc6\uff0c\u6bd4\u6700\u5148\u8fdb\u7684LLM\u53c2\u6570\u8bad\u7ec3\u57fa\u7ebf\u5feb2800\u500d\uff0c\u5c062851\u6761\u8f68\u8ff9\u538b\u7f29\u4e3a187\u4e2a\u7a0b\u5e8f\u3002", "conclusion": "\u5177\u6709\u8d1d\u53f6\u65af\u9009\u62e9\u548c\u5bf9\u6bd4\u7cbe\u70bc\u7684\u7ed3\u6784\u5316\u5916\u90e8\u8bb0\u5fc6\u80fd\u591f\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6301\u7eed\u6539\u8fdb\u7684\u667a\u80fd\u4f53\uff0c\u65e0\u9700LLM\u53c2\u6570\u66f4\u65b0\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u6cdb\u5316\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2512.18957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18957", "abs": "https://arxiv.org/abs/2512.18957", "authors": ["Debamita Ghosh", "George K. Atia", "Yue Wang"], "title": "Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation", "comment": null, "summary": "The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u5206\u5e03\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u5b66\u4e60\u6700\u4f18\u9c81\u68d2\u7b56\u7565\uff0c\u65e0\u9700\u5148\u9a8c\u6a21\u578b\u6216\u79bb\u7ebf\u6570\u636e\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5e38\u56e0\u8bad\u7ec3\u4e0e\u90e8\u7f72\u73af\u5883\u4e0d\u5339\u914d\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u5206\u5e03\u9c81\u68d2RL\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u5148\u9a8c\u77e5\u8bc6\uff08\u5982\u751f\u6210\u6a21\u578b\u6216\u5927\u578b\u79bb\u7ebf\u6570\u636e\u96c6\uff09\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u8868\u683c\u65b9\u6cd5\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u590d\u6742\u9886\u57df\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u5206\u5e03\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5177\u6709\u901a\u7528\u51fd\u6570\u903c\u8fd1\u80fd\u529b\uff0c\u4ec5\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u5b66\u4e60\u6700\u4f18\u9c81\u68d2\u7b56\u7565\uff0c\u65e0\u9700\u5148\u9a8c\u6a21\u578b\u6216\u79bb\u7ebf\u6570\u636e\u3002\u5728\u603b\u53d8\u5dee\u4e0d\u786e\u5b9a\u6027\u96c6\u4e0b\u63d0\u4f9b\u7406\u8bba\u5206\u6790\uff0c\u5efa\u7acb\u8fd1\u6700\u4f18\u7684\u6b21\u7ebf\u6027\u9057\u61be\u754c\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5230\u6700\u4f18\u9c81\u68d2\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u4efb\u52a1\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u5728\u603b\u53d8\u5dee\u4e0d\u786e\u5b9a\u6027\u96c6\u4e0b\u5177\u6709\u8fd1\u6700\u4f18\u7684\u6b21\u7ebf\u6027\u9057\u61be\u754c\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6837\u672c\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5728\u7ebfDR-RL\u7b97\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u5b66\u4e60\u9c81\u68d2\u7b56\u7565\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.19682", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19682", "abs": "https://arxiv.org/abs/2512.19682", "authors": ["Jiacheng Guo", "Ling Yang", "Peter Chen", "Qixin Xiao", "Yinjie Wang", "Xinzhe Juan", "Jiahao Qiu", "Ke Shen", "Mengdi Wang"], "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators", "comment": "Our codes are available at https://github.com/Gen-Verse/GenEnv", "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $\u03b1$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.", "AI": {"tldr": "GenEnv\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u4f53\u4e0e\u751f\u6210\u5f0f\u73af\u5883\u6a21\u62df\u5668\u4e4b\u95f4\u7684\u96be\u5ea6\u5bf9\u9f50\u534f\u540c\u8fdb\u5316\u6e38\u620f\uff0c\u89e3\u51b3LLM\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6570\u636e\u6210\u672c\u9ad8\u3001\u9759\u6001\u6027\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u74f6\u9888\uff1a\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u4ee5\u53ca\u73b0\u6709\u6570\u636e\u96c6\u7684\u9759\u6001\u6027\u65e0\u6cd5\u9002\u5e94\u667a\u80fd\u4f53\u7684\u6301\u7eed\u5b66\u4e60\u9700\u6c42\u3002", "method": "\u5efa\u7acb\u667a\u80fd\u4f53\u4e0e\u53ef\u6269\u5c55\u751f\u6210\u5f0f\u73af\u5883\u6a21\u62df\u5668\u4e4b\u95f4\u7684\u534f\u540c\u8fdb\u5316\u6e38\u620f\uff0c\u6a21\u62df\u5668\u4f5c\u4e3a\u52a8\u6001\u8bfe\u7a0b\u7b56\u7565\uff0c\u6839\u636e\u667a\u80fd\u4f53\u7684\"\u6700\u8fd1\u53d1\u5c55\u533a\"\u751f\u6210\u5b9a\u5236\u5316\u4efb\u52a1\uff0c\u4f7f\u7528\u03b1-\u8bfe\u7a0b\u5956\u52b1\u673a\u5236\u5bf9\u9f50\u4efb\u52a1\u96be\u5ea6\u4e0e\u667a\u80fd\u4f53\u5f53\u524d\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGenEnv\u4f7f7B\u57fa\u7840\u6a21\u578b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe40.3%\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u7684\u5e73\u5747\u6027\u80fd\u3002\u76f8\u6bd4Gemini 2.5 Pro\u7684\u79bb\u7ebf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f7f\u7528\u6570\u636e\u91cf\u51cf\u5c113.3\u500d\u7684\u540c\u65f6\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4ece\u9759\u6001\u76d1\u7763\u8f6c\u5411\u81ea\u9002\u5e94\u6a21\u62df\uff0cGenEnv\u4e3a\u6269\u5c55\u667a\u80fd\u4f53\u80fd\u529b\u63d0\u4f9b\u4e86\u6570\u636e\u9ad8\u6548\u7684\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u7684\u6839\u672c\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.19673", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19673", "abs": "https://arxiv.org/abs/2512.19673", "authors": ["Yuqiao Tan", "Minzheng Wang", "Shizhu He", "Huanxuan Liao", "Chengfeng Zhao", "Qiunan Lu", "Tian Liang", "Jun Zhao", "Kang Liu"], "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "comment": "Preprint. Our code is available at https://github.com/Trae1ounG/BuPO", "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790Transformer\u6b8b\u5dee\u6d41\u7684\u5185\u5728\u5206\u5272\uff0c\u63ed\u793a\u4e86\u5185\u90e8\u5c42\u7b56\u7565\u548c\u5185\u90e8\u6a21\u5757\u7b56\u7565\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u81ea\u4e0b\u800c\u4e0a\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u5355\u4e00\u7edf\u4e00\u7b56\u7565\uff0c\u5ffd\u89c6\u4e86\u5176\u5185\u90e8\u673a\u5236\u3002\u7406\u89e3\u7b56\u7565\u5728\u4e0d\u540c\u5c42\u548c\u6a21\u5757\u95f4\u7684\u6f14\u5316\u5bf9\u4e8e\u5b9e\u73b0\u66f4\u6709\u9488\u5bf9\u6027\u7684\u4f18\u5316\u548c\u63ed\u793a\u590d\u6742\u63a8\u7406\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5229\u7528Transformer\u6b8b\u5dee\u6d41\u7684\u5185\u5728\u5206\u5272\u4ee5\u53ca\u9690\u85cf\u72b6\u6001\u4e0e\u89e3\u5d4c\u5165\u77e9\u9635\u7ec4\u5408\u7684\u7b49\u4ef7\u6027\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u7b56\u7565\u5206\u89e3\u4e3a\u5185\u90e8\u5c42\u7b56\u7565\uff08\u5bf9\u5e94\u5404\u5c42\u8d21\u732e\uff09\u548c\u5185\u90e8\u6a21\u5757\u7b56\u7565\uff08\u5bf9\u5e94\u6bcf\u5c42\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u548c\u524d\u9988\u7f51\u7edc\u7ec4\u4ef6\uff09\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u81ea\u4e0b\u800c\u4e0a\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u76f4\u63a5\u4f18\u5316\u5185\u90e8\u5c42\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u65e9\u671f\u5c42\u4fdd\u6301\u9ad8\u71b5\u4ee5\u8fdb\u884c\u63a2\u7d22\uff0c\u9876\u5c42\u6536\u655b\u5230\u63a5\u8fd1\u96f6\u71b5\u4ee5\u8fdb\u884c\u7ec6\u5316\uff0c\u6536\u655b\u6a21\u5f0f\u5728\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u95f4\u5b58\u5728\u5dee\u5f02\u3002Llama\u7684\u9884\u6d4b\u7a7a\u95f4\u5728\u6700\u540e\u4e00\u5c42\u5feb\u901f\u6536\u655b\uff0c\u800cQwen\u7cfb\u5217\u6a21\u578b\uff08\u7279\u522b\u662fQwen3\uff09\u5c55\u73b0\u51fa\u66f4\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6e10\u8fdb\u7ed3\u6784\u5316\u63a8\u7406\u6a21\u5f0f\u3002\u63d0\u51fa\u7684BuPO\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u7b56\u7565\u5e76\u5206\u6790\u5185\u90e8\u7b56\u7565\u71b5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u7684\u63a8\u7406\u6a21\u5f0f\u5dee\u5f02\u3002\u63d0\u51fa\u7684\u81ea\u4e0b\u800c\u4e0a\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u901a\u8fc7\u5728\u4e0b\u5c42\u5bf9\u9f50\u8bad\u7ec3\u76ee\u6807\uff0c\u91cd\u5efa\u4e86\u57fa\u7840\u63a8\u7406\u80fd\u529b\u5e76\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.19154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19154", "abs": "https://arxiv.org/abs/2512.19154", "authors": ["Geraud Nangue Tasse", "Matthew Riemer", "Benjamin Rosman", "Tim Klinger"], "title": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments", "comment": null, "summary": "Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking", "AI": {"tldr": "\u63d0\u51faAdaptive Stacking\u5143\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ef4\u62a4\u5c0f\u578b\u8bb0\u5fc6\u6808\u6765\u5904\u7406\u9ad8\u5ea6\u975e\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\uff0c\u76f8\u6bd4\u4f20\u7edf\u5e27\u5806\u53e0\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u8ba1\u7b97\u53d7\u9650\u7684\u667a\u80fd\u4f53\u65f6\u9762\u4e34\u9ad8\u5ea6\u975e\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\u7684\u6311\u6218\u3002\u4f20\u7edf\u5e27\u5806\u53e0\u65b9\u6cd5\u9700\u8981\u5806\u53e0\u5927\u91cf\u6700\u8fd1\u89c2\u5bdf\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u8fc7\u9ad8\u3002\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u73af\u5883\u867d\u7136\u65f6\u95f4\u4e0a\u9ad8\u5ea6\u975e\u9a6c\u5c14\u53ef\u592b\uff0c\u4f46\u53ea\u56e0\u679c\u4f9d\u8d56\u4e8e\u76f8\u5bf9\u8f83\u5c11\u7684\u89c2\u5bdf\u3002", "method": "\u63d0\u51faAdaptive Stacking\u5143\u7b97\u6cd5\uff0c\u7ef4\u62a4\u76f8\u5bf9\u8f83\u5c0f\u7684\u81ea\u9002\u5e94\u8bb0\u5fc6\u6808\uff0c\u80fd\u591f\u8868\u8fbe\u65f6\u95f4\u4e0a\u7684\u9ad8\u5ea6\u975e\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\uff0c\u540c\u65f6\u6bcf\u4e2a\u6b65\u9aa4\u8003\u8651\u66f4\u5c11\u7684\u89c2\u5bdf\u3002\u7b97\u6cd5\u5177\u6709\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u91cf\u5316\u4e86MLP\u3001LSTM\u548cTransformer\u667a\u80fd\u4f53\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8282\u7701\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u6d41\u884c\u7684\u8bb0\u5fc6\u4efb\u52a1\uff0c\u63a7\u5236\u975e\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\u7a0b\u5ea6\u3002\u7ed3\u679c\u663e\u793a\u9002\u5f53\u7684\u5143\u7b97\u6cd5\u80fd\u591f\u5b66\u4e60\u79fb\u9664\u4e0d\u9884\u6d4b\u672a\u6765\u5956\u52b1\u7684\u8bb0\u5fc6\uff0c\u540c\u65f6\u4e0d\u8fc7\u5ea6\u79fb\u9664\u91cd\u8981\u7ecf\u9a8c\uff0c\u5b9e\u73b0\u8ba1\u7b97\u548c\u5185\u5b58\u7684\u663e\u8457\u8282\u7701\u3002", "conclusion": "Adaptive Stacking\u5143\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u5ea6\u975e\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\u73af\u5883\uff0c\u76f8\u6bd4\u4f20\u7edf\u5e27\u5806\u53e0\u65b9\u6cd5\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u8ba1\u7b97\u53d7\u9650\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.c926930e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F%3Futm_source=tldrfounders/1/0100019b36cbaea2-7b90cbe1-a5f1-4607-ad06-419ad59905a6-000000/SC96I6WY-KlabNQdyAbaHxcO8sEktI8cvtCdNluhAxg=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F%3Futm_source=tldrfounders/1/0100019b36cbaea2-7b90cbe1-a5f1-4607-ad06-419ad59905a6-000000/SC96I6WY-KlabNQdyAbaHxcO8sEktI8cvtCdNluhAxg=436", "authors": ["TLDR Newsletter"], "title": "Ampcode", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F%3Futm_source=tldrfounders/1/0100019b36cbaea2-7b90cbe1-a5f1-4607-ad06-419ad59905a6-000000/SC96I6WY-KlabNQdyAbaHxcO8sEktI8cvtCdNluhAxg=436", "summary": "Ampcode (Tool) Amp is a frontier coding agent that lets you wield the full power of leading models.", "source": "tldr", "AI": {"tldr": "Ampcode\u662f\u4e00\u4e2a\u524d\u6cbf\u7684\u7f16\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u5145\u5206\u5229\u7528\u9886\u5148\u6a21\u578b\u7684\u80fd\u529b", "motivation": "\u5f53\u524dAI\u7f16\u7801\u52a9\u624b\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u7528\u6237\u5f80\u5f80\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6700\u5148\u8fdb\u6a21\u578b\u7684\u80fd\u529b\u3002Ampcode\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u8f7b\u677e\u8bbf\u95ee\u548c\u4f7f\u7528\u524d\u6cbf\u7684\u7f16\u7801\u6a21\u578b", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aAmp\u7684\u7f16\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u96c6\u6210\u4e86\u9886\u5148\u7684AI\u6a21\u578b\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u5f3a\u5927\u7684\u7f16\u7801\u8f85\u52a9\u529f\u80fd", "result": "\u521b\u5efa\u4e86Ampcode\u5de5\u5177\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u5229\u7528\u524d\u6cbf\u7684\u7f16\u7801\u6a21\u578b\u8fdb\u884c\u5f00\u53d1\u5de5\u4f5c", "conclusion": "Ampcode\u4f5c\u4e3a\u4e00\u4e2a\u7f16\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u6210\u529f\u5730\u5c06\u524d\u6cbfAI\u6a21\u578b\u7684\u80fd\u529b\u5e26\u7ed9\u5f00\u53d1\u8005\uff0c\u63d0\u5347\u4e86\u7f16\u7801\u6548\u7387\u548c\u8d28\u91cf", "topic": "code agent"}}
{"id": "tldr.2512.347a9244", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fwhy-agentic-ai-is-your-next-priority%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-why-agentic-ai-is-your-next-priority%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/2/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/4Vvih960f5DcdueuJoyYk164hgs_-NK9z8I7cA_fTWk=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fwhy-agentic-ai-is-your-next-priority%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-why-agentic-ai-is-your-next-priority%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/2/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/4Vvih960f5DcdueuJoyYk164hgs_-NK9z8I7cA_fTWk=436", "authors": ["TLDR Newsletter"], "title": "Agentic AI isn't just genAI with extra steps", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fwhy-agentic-ai-is-your-next-priority%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-why-agentic-ai-is-your-next-priority%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/2/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/4Vvih960f5DcdueuJoyYk164hgs_-NK9z8I7cA_fTWk=436", "summary": "Agentic AI isn't just genAI with extra steps (Sponsor) The buzz around agentic AI has reached a fever pitch. But outside of coding, most implementations are \"agent-washing:\" a chain of prompts or a normal workflow with an LLM in the loop.Algolia's ebook breaks down what agentic AI actually is, how it differs from gen AI, and where it's headed: \u2192 Read why Gartner predicts a third of enterprise apps will be agentic enabled by 2028 \u2192 Discover how agentic AI enhances search by parsing natural lan...", "source": "tldr", "AI": {"tldr": "Algolia\u7684\u7535\u5b50\u4e66\u63a2\u8ba8\u4e86\u4ec0\u4e48\u662f\u771f\u6b63\u7684Agentic AI\uff0c\u5b83\u4e0e\u751f\u6210\u5f0fAI\u7684\u533a\u522b\uff0c\u4ee5\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u9884\u6d4b\u52302028\u5e74\u4e09\u5206\u4e4b\u4e00\u7684\u4f01\u4e1a\u5e94\u7528\u5c06\u5177\u5907Agentic\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5173\u4e8eAgentic AI\u7684\u8ba8\u8bba\u5f88\u591a\uff0c\u4f46\u5927\u591a\u6570\u5b9e\u73b0\u53ea\u662f\"agent-washing\" - \u4ec5\u4ec5\u662f\u63d0\u793a\u94fe\u6216\u5305\u542bLLM\u7684\u6b63\u5e38\u5de5\u4f5c\u6d41\u7a0b\u3002\u9700\u8981\u6f84\u6e05Agentic AI\u7684\u771f\u6b63\u542b\u4e49\u53ca\u5176\u4e0e\u751f\u6210\u5f0fAI\u7684\u533a\u522b\u3002", "method": "\u901a\u8fc7Algolia\u7684\u7535\u5b50\u4e66\u8fdb\u884c\u5206\u6790\uff0c\u89e3\u91caAgentic AI\u7684\u5b9e\u9645\u5b9a\u4e49\uff0c\u8ba8\u8bba\u5176\u5982\u4f55\u901a\u8fc7\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u6765\u589e\u5f3a\u641c\u7d22\u529f\u80fd\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9Agentic AI\u7684\u6e05\u6670\u5b9a\u4e49\uff0c\u9884\u6d4b\u52302028\u5e74\u4e09\u5206\u4e4b\u4e00\u7684\u4f01\u4e1a\u5e94\u7528\u5c06\u5177\u5907Agentic\u80fd\u529b\uff0c\u5c55\u793a\u4e86Agentic AI\u5728\u641c\u7d22\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "Agentic AI\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u5f0fAI\u7684\u6269\u5c55\uff0c\u800c\u662f\u5177\u6709\u81ea\u4e3b\u6027\u548c\u76ee\u6807\u5bfc\u5411\u80fd\u529b\u7684AI\u7cfb\u7edf\uff0c\u5c06\u5728\u4f01\u4e1a\u5e94\u7528\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.f56790c3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F8ea9zT/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/DkiofdPMj8dX4hUXxVlPT--bb4lj1QKH-1estWf91Es=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F8ea9zT/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/DkiofdPMj8dX4hUXxVlPT--bb4lj1QKH-1estWf91Es=436", "authors": ["TLDR Newsletter"], "title": "Introducing GPT-5.2-Codex", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F8ea9zT/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/DkiofdPMj8dX4hUXxVlPT--bb4lj1QKH-1estWf91Es=436", "summary": "Introducing GPT-5.2-Codex (5 minute read) OpenAI's new agentic coding model is state-of-the-art on SWE-Bench Pro and Terminal-Bench 2.0 with improved long-horizon work. OpenAI is launching a trusted access pilot to give vetted cybersecurity professionals access to future, more powerful models.", "source": "tldr", "AI": {"tldr": "OpenAI\u53d1\u5e03GPT-5.2-Codex\uff0c\u5728SWE-Bench Pro\u548cTerminal-Bench 2.0\u4e0a\u8fbe\u5230SOTA\uff0c\u5177\u6709\u6539\u8fdb\u7684\u957f\u65f6\u7a0b\u5de5\u4f5c\u80fd\u529b\uff0c\u540c\u65f6\u542f\u52a8\u53ef\u4fe1\u8bbf\u95ee\u8bd5\u70b9\u8ba1\u5212", "motivation": "\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u4ee3\u7801\u4ee3\u7406\u6a21\u578b\uff0c\u63d0\u5347\u957f\u65f6\u7a0b\u7f16\u7a0b\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u4e3a\u7f51\u7edc\u5b89\u5168\u4e13\u4e1a\u4eba\u5458\u63d0\u4f9b\u5b89\u5168\u8bbf\u95ee\u66f4\u5f3a\u5927\u6a21\u578b\u7684\u9014\u5f84", "method": "\u63a8\u51faGPT-5.2-Codex\u6a21\u578b\uff0c\u5728SWE-Bench Pro\u548cTerminal-Bench 2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6027\u80fd\uff0c\u5e76\u5efa\u7acb\u53ef\u4fe1\u8bbf\u95ee\u8bd5\u70b9\u673a\u5236", "result": "\u5728SWE-Bench Pro\u548cTerminal-Bench 2.0\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u957f\u65f6\u7a0b\u5de5\u4f5c\u80fd\u529b\u5f97\u5230\u6539\u8fdb", "conclusion": "GPT-5.2-Codex\u5728\u4ee3\u7801\u4ee3\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u4fe1\u8bbf\u95ee\u8bd5\u70b9\u786e\u4fdd\u66f4\u5f3a\u5927\u6a21\u578b\u7684\u5b89\u5168\u4f7f\u7528", "topic": "code agent"}}
{"id": "tldr.2512.3a8c7672", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagentskills.io%2Fhome%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/h4dzNkqRh2bQXoF4VZEqGfGEcmT2EpEDa5VLYd56tCI=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagentskills.io%2Fhome%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/h4dzNkqRh2bQXoF4VZEqGfGEcmT2EpEDa5VLYd56tCI=436", "authors": ["TLDR Newsletter"], "title": "Agent Skills Becomes an Open Standard", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagentskills.io%2Fhome%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/h4dzNkqRh2bQXoF4VZEqGfGEcmT2EpEDa5VLYd56tCI=436", "summary": "Agent Skills Becomes an Open Standard (2 minute read) Agent Skills, folders of instructions, scripts, and resources that give AI agents new capabilities on demand, originated at Anthropic (which also created MCP) and is now an open format with adoption from Cursor, GitHub, VS Code, Claude Code, and OpenAI's Codex CLI. Skills let teams package domain expertise and workflows into portable, version-controlled packages that work across different agent products.", "source": "tldr", "AI": {"tldr": "Agent Skills\u6210\u4e3a\u5f00\u653e\u6807\u51c6\uff0c\u8fd9\u662f\u4e00\u79cd\u5305\u542b\u6307\u4ee4\u3001\u811a\u672c\u548c\u8d44\u6e90\u7684\u6587\u4ef6\u5939\u683c\u5f0f\uff0c\u8ba9AI\u4ee3\u7406\u80fd\u591f\u6309\u9700\u83b7\u5f97\u65b0\u80fd\u529b\uff0c\u6700\u521d\u7531Anthropic\u521b\u5efa\uff0c\u73b0\u5df2\u88ab\u591a\u5bb6\u4e3b\u6d41\u5f00\u53d1\u5de5\u5177\u91c7\u7528\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3AI\u4ee3\u7406\u5728\u4e0d\u540c\u4ea7\u54c1\u548c\u73af\u5883\u4e2d\u80fd\u529b\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u7684\u65b9\u5f0f\u6765\u6253\u5305\u548c\u5171\u4eab\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u56e2\u961f\u80fd\u591f\u521b\u5efa\u53ef\u79fb\u690d\u3001\u7248\u672c\u63a7\u5236\u7684\u6280\u80fd\u5305\u3002", "method": "\u521b\u5efa\u4e86Agent Skills\u5f00\u653e\u683c\u5f0f\u89c4\u8303\uff0c\u8fd9\u662f\u4e00\u79cd\u5305\u542b\u6307\u4ee4\u3001\u811a\u672c\u548c\u8d44\u6e90\u7684\u6587\u4ef6\u5939\u7ed3\u6784\uff0c\u5141\u8bb8\u56e2\u961f\u5c06\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u5de5\u4f5c\u6d41\u7a0b\u6253\u5305\u6210\u6807\u51c6\u5316\u7684\u6280\u80fd\u5305\uff0c\u8fd9\u4e9b\u6280\u80fd\u5305\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u4ee3\u7406\u4ea7\u54c1\u4e4b\u95f4\u5171\u4eab\u548c\u4f7f\u7528\u3002", "result": "Agent Skills\u5df2\u6210\u4e3a\u5f00\u653e\u6807\u51c6\uff0c\u83b7\u5f97\u4e86Cursor\u3001GitHub\u3001VS Code\u3001Claude Code\u548cOpenAI\u7684Codex CLI\u7b49\u4e3b\u6d41\u5f00\u53d1\u5de5\u5177\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u8de8\u5e73\u53f0\u7684\u6280\u80fd\u751f\u6001\u7cfb\u7edf\u3002", "conclusion": "Agent Skills\u4f5c\u4e3a\u5f00\u653e\u6807\u51c6\u7684\u5efa\u7acb\uff0c\u89e3\u51b3\u4e86AI\u4ee3\u7406\u80fd\u529b\u788e\u7247\u5316\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u5171\u4eab\u548c\u91cd\u7528\uff0c\u4e3aAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "topic": "code agent"}}
{"id": "tldr.2512.8bc32302", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.replit.com%2Finside-replits-snapshot-engine%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/2OiE2-BgrjUnaUpKFV71cL_V3XQXWhaHA14gutOv_iM=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.replit.com%2Finside-replits-snapshot-engine%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/2OiE2-BgrjUnaUpKFV71cL_V3XQXWhaHA14gutOv_iM=436", "authors": ["TLDR Newsletter"], "title": "Inside Replit's Snapshot Engine: The Tech Making AI Agents Safe", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.replit.com%2Finside-replits-snapshot-engine%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/2OiE2-BgrjUnaUpKFV71cL_V3XQXWhaHA14gutOv_iM=436", "summary": "Inside Replit's Snapshot Engine: The Tech Making AI Agents Safe (9 minute read) Replit built a compute and storage fabric that allows it to make changes in an isolated, reversible way. These primitives enable developers to experiment more frequently and faster. The company realized the same primitives could be used to superpower coding agents when it built Replit Agent in 2024. The system helps the human driving the agent, and the agent itself greatly benefits from the tools. This post explor...", "source": "tldr", "AI": {"tldr": "Replit\u5f00\u53d1\u4e86\u5feb\u7167\u5f15\u64ce\u6280\u672f\uff0c\u901a\u8fc7\u9694\u79bb\u53ef\u9006\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u67b6\u6784\uff0c\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u5b89\u5168\u5b9e\u9a8c\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfAI\u7f16\u7801\u4ee3\u7406\u5728\u5b9e\u9a8c\u548c\u4fee\u6539\u65f6\u7f3a\u4e4f\u5b89\u5168\u9694\u79bb\u673a\u5236\uff0c\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u6216\u4e0d\u53ef\u9006\u7684\u66f4\u6539\u3002Replit\u610f\u8bc6\u5230\u5176\u73b0\u6709\u7684\u9694\u79bb\u53ef\u9006\u8ba1\u7b97\u5b58\u50a8\u67b6\u6784\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u5b89\u5168\u7684\u5b9e\u9a8c\u73af\u5883\u3002", "method": "\u6784\u5efa\u8ba1\u7b97\u548c\u5b58\u50a8\u67b6\u6784\uff0c\u652f\u6301\u9694\u79bb\u4e14\u53ef\u9006\u7684\u66f4\u6539\u64cd\u4f5c\u3002\u8be5\u5feb\u7167\u5f15\u64ce\u5141\u8bb8\u5f00\u53d1\u8005\u5728\u9694\u79bb\u73af\u5883\u4e2d\u8fdb\u884c\u9891\u7e41\u5feb\u901f\u5b9e\u9a8c\uff0c\u5e76\u5c06\u8fd9\u4e9b\u539f\u8bed\u5e94\u7528\u4e8eReplit Agent\u7f16\u7801\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "\u8be5\u7cfb\u7edf\u65e2\u5e2e\u52a9\u4eba\u7c7b\u5f00\u53d1\u8005\u66f4\u5b89\u5168\u5730\u9a71\u52a8AI\u4ee3\u7406\uff0c\u4e5f\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u672c\u8eab\u7684\u6027\u80fd\u3002\u5feb\u7167\u6280\u672f\u4f7f\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u5728\u5b89\u5168\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u548c\u4fee\u6539\u3002", "conclusion": "Replit\u7684\u5feb\u7167\u5f15\u64ce\u6280\u672f\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u9694\u79bb\u53ef\u9006\u7684\u66f4\u6539\u673a\u5236\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9891\u7e41\u3001\u66f4\u5feb\u901f\u7684\u5b9e\u9a8c\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7406\u53ef\u9760\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2512.34157b25", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2F2025%2Fsignature-flicker%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/W1795o0Zl3WRPvs9FlSS0o7n3JaB8bSAyBQ5ILMsalI=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2F2025%2Fsignature-flicker%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/W1795o0Zl3WRPvs9FlSS0o7n3JaB8bSAyBQ5ILMsalI=436", "authors": ["TLDR Newsletter"], "title": "The Signature Flicker", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2F2025%2Fsignature-flicker%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/W1795o0Zl3WRPvs9FlSS0o7n3JaB8bSAyBQ5ILMsalI=436", "summary": "The Signature Flicker (4 minute read) Anthropic has fixed Claude Code's signature flicker. Terminals weren't really designed for interactivity. Repositioning the cursor and writing over existing text easily leads to flickering if not done well. Anthropic chose to re-render only the changed parts. It rewrote the renderer from scratch while still keeping React as the component model.", "source": "tldr", "AI": {"tldr": "Anthropic\u4fee\u590d\u4e86Claude Code\u7ec8\u7aef\u4e2d\u7684\u7b7e\u540d\u95ea\u70c1\u95ee\u9898\uff0c\u901a\u8fc7\u91cd\u5199\u6e32\u67d3\u5668\u4ec5\u91cd\u65b0\u6e32\u67d3\u53d8\u5316\u90e8\u5206\uff0c\u540c\u65f6\u4fdd\u6301React\u7ec4\u4ef6\u6a21\u578b", "motivation": "\u7ec8\u7aef\u8bbe\u8ba1\u521d\u8877\u5e76\u975e\u4e3a\u4e86\u4ea4\u4e92\u6027\uff0c\u91cd\u65b0\u5b9a\u4f4d\u5149\u6807\u548c\u8986\u76d6\u73b0\u6709\u6587\u672c\u5bb9\u6613\u5bfc\u81f4\u95ea\u70c1\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdbClaude Code\u7684\u7528\u6237\u4f53\u9a8c", "method": "\u91cd\u5199\u6e32\u67d3\u5668\uff0c\u4ec5\u91cd\u65b0\u6e32\u67d3\u53d8\u5316\u7684\u90e8\u5206\uff0c\u540c\u65f6\u4fdd\u6301React\u4f5c\u4e3a\u7ec4\u4ef6\u6a21\u578b", "result": "\u6210\u529f\u4fee\u590d\u4e86Claude Code\u4e2d\u7684\u7b7e\u540d\u95ea\u70c1\u95ee\u9898", "conclusion": "\u901a\u8fc7\u4f18\u5316\u6e32\u67d3\u7b56\u7565\u53ef\u4ee5\u89e3\u51b3\u7ec8\u7aef\u4ea4\u4e92\u4e2d\u7684\u89c6\u89c9\u95ea\u70c1\u95ee\u9898\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c", "topic": "swe application"}}
{"id": "tldr.2512.744ad0cd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenanderson.work%2Fblog%2Fcontra-dspy-gepa%2F%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/JsRSQEfm1BZJaCvkT1-w2YfFpkEm_oPHZMPSpW36Sr4=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenanderson.work%2Fblog%2Fcontra-dspy-gepa%2F%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/JsRSQEfm1BZJaCvkT1-w2YfFpkEm_oPHZMPSpW36Sr4=436", "authors": ["TLDR Newsletter"], "title": "Contra DSPy and GEPA", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenanderson.work%2Fblog%2Fcontra-dspy-gepa%2F%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/JsRSQEfm1BZJaCvkT1-w2YfFpkEm_oPHZMPSpW36Sr4=436", "summary": "Contra DSPy and GEPA (15 minute read) Trying to treat LLM workflows as modular programs is a backwards, rigid, and the wrong fit for the most interesting tasks.", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u6279\u8bc4\u4e86\u5c06LLM\u5de5\u4f5c\u6d41\u89c6\u4e3a\u6a21\u5757\u5316\u7a0b\u5e8f\u7684\u65b9\u6cd5\uff08\u5982DSPy\u548cGEPA\uff09\uff0c\u8ba4\u4e3a\u8fd9\u79cd\u65b9\u6cd5\u662f\u5012\u9000\u3001\u50f5\u5316\u7684\uff0c\u4e0d\u9002\u5408\u5904\u7406\u6700\u6709\u8da3\u7684\u4efb\u52a1", "motivation": "\u4f5c\u8005\u89c2\u5bdf\u5230\u5f53\u524d\u6d41\u884c\u7684LLM\u5de5\u4f5c\u6d41\u6846\u67b6\uff08\u5982DSPy\u548cGEPA\uff09\u8bd5\u56fe\u5c06LLM\u5de5\u4f5c\u6d41\u5efa\u6a21\u4e3a\u6a21\u5757\u5316\u7a0b\u5e8f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\"\u7f16\u7a0b\u5316\"\u65b9\u6cd5\u9650\u5236\u4e86LLM\u7684\u7075\u6d3b\u6027\u548c\u521b\u9020\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u590d\u6742\u3001\u5f00\u653e\u5f0f\u7684\u4efb\u52a1\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u5bf9\u6bd4\u4e86\u6a21\u5757\u5316\u7f16\u7a0b\u65b9\u6cd5\u4e0e\u66f4\u7075\u6d3b\u7684\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u65b9\u6cd5\u3002\u4f5c\u8005\u53ef\u80fd\u63d0\u51fa\u4e86\u66ff\u4ee3\u65b9\u6848\u6216\u8bbe\u8ba1\u539f\u5219\uff0c\u5f3a\u8c03LLM\u5de5\u4f5c\u6d41\u5e94\u8be5\u66f4\u52a0\u52a8\u6001\u3001\u81ea\u9002\u5e94\uff0c\u800c\u4e0d\u662f\u50f5\u5316\u7684\u6a21\u5757\u7ec4\u5408\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u6a21\u5757\u5316\u7f16\u7a0b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u3001\u521b\u9020\u6027\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86LLM\u7684\u6f5c\u529b\u3002\u4f5c\u8005\u53ef\u80fd\u5c55\u793a\u4e86\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u5c06LLM\u5de5\u4f5c\u6d41\u89c6\u4e3a\u6a21\u5757\u5316\u7a0b\u5e8f\u662f\u9519\u8bef\u7684\u8303\u5f0f\u3002\u5bf9\u4e8e\u6700\u6709\u8da3\u548c\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u52a8\u6001\u7684\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u5145\u5206\u5229\u7528LLM\u7684\u521b\u9020\u6027\u548c\u9002\u5e94\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.0633fe89", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmlops.community%2Fda2a-the-future-of-data-platforms-is-agentic-distributed-and-collaborative%2F%3Futm_source=tldrdata/1/0100019b45be8dd6-f7340055-99c9-402f-9d8b-12a269686952-000000/qGJEQxxPPLObSrj5ut3O6p2En84yrGhCPTR21iuMIa4=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmlops.community%2Fda2a-the-future-of-data-platforms-is-agentic-distributed-and-collaborative%2F%3Futm_source=tldrdata/1/0100019b45be8dd6-f7340055-99c9-402f-9d8b-12a269686952-000000/qGJEQxxPPLObSrj5ut3O6p2En84yrGhCPTR21iuMIa4=436", "authors": ["TLDR Newsletter"], "title": "Da2a: The Future of Data Platforms is Agentic, Distributed, and Collaborative", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmlops.community%2Fda2a-the-future-of-data-platforms-is-agentic-distributed-and-collaborative%2F%3Futm_source=tldrdata/1/0100019b45be8dd6-f7340055-99c9-402f-9d8b-12a269686952-000000/qGJEQxxPPLObSrj5ut3O6p2En84yrGhCPTR21iuMIa4=436", "summary": "Da2a: The Future of Data Platforms is Agentic, Distributed, and Collaborative (6 minute read) Traditional centralized data platforms, reliant on data engineers for ETL and queries, create bottlenecks and slow decision-making for business users. DA2A is a future paradigm of agentic, distributed, and collaborative data platforms where specialized AI agents autonomously manage domain-specific data, communicate via an Agent-to-Agent protocol, and collaborate through an orchestrator to answer comp...", "source": "tldr", "AI": {"tldr": "DA2A\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u5e73\u53f0\u8303\u5f0f\uff1a\u57fa\u4e8eAI\u4ee3\u7406\u7684\u5206\u5e03\u5f0f\u534f\u4f5c\u7cfb\u7edf\uff0c\u53d6\u4ee3\u4f20\u7edf\u96c6\u4e2d\u5f0fETL\u5e73\u53f0\uff0c\u901a\u8fc7\u4e13\u4e1a\u4ee3\u7406\u81ea\u4e3b\u7ba1\u7406\u9886\u57df\u6570\u636e\uff0c\u4f7f\u7528\u4ee3\u7406\u95f4\u534f\u8bae\u901a\u4fe1\uff0c\u7531\u7f16\u6392\u5668\u534f\u8c03\u5b8c\u6210\u590d\u6742\u67e5\u8be2", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u6570\u636e\u5e73\u53f0\u4f9d\u8d56\u6570\u636e\u5de5\u7a0b\u5e08\u8fdb\u884cETL\u548c\u67e5\u8be2\uff0c\u9020\u6210\u74f6\u9888\uff0c\u51cf\u7f13\u4e1a\u52a1\u7528\u6237\u7684\u51b3\u7b56\u901f\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "\u91c7\u7528\u4ee3\u7406\u5316\u3001\u5206\u5e03\u5f0f\u548c\u534f\u4f5c\u7684\u65b9\u6cd5\uff1a\u4e13\u4e1aAI\u4ee3\u7406\u81ea\u4e3b\u7ba1\u7406\u9886\u57df\u7279\u5b9a\u6570\u636e\uff0c\u901a\u8fc7Agent-to-Agent\u534f\u8bae\u901a\u4fe1\uff0c\u7531\u7f16\u6392\u5668\u534f\u8c03\u4ee3\u7406\u534f\u4f5c\u56de\u7b54\u590d\u6742\u67e5\u8be2", "result": "\u63d0\u51fa\u4e86DA2A\u4f5c\u4e3a\u672a\u6765\u6570\u636e\u5e73\u53f0\u7684\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u6d88\u9664\u4f20\u7edf\u5e73\u53f0\u7684\u74f6\u9888\uff0c\u52a0\u901f\u4e1a\u52a1\u51b3\u7b56\u8fc7\u7a0b", "conclusion": "\u6570\u636e\u5e73\u53f0\u7684\u672a\u6765\u5e94\u8be5\u662f\u4ee3\u7406\u5316\u3001\u5206\u5e03\u5f0f\u548c\u534f\u4f5c\u7684\uff0cDA2A\u8303\u5f0f\u80fd\u591f\u89e3\u51b3\u4f20\u7edf\u96c6\u4e2d\u5f0f\u5e73\u53f0\u7684\u5c40\u9650\u6027", "topic": "agent analysis"}}
{"id": "tldr.2512.a435ee96", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackernoon.com%2Fthe-seven-pillars-of-a-production-grade-agent-architecture%3Futm_source=tldrdata/1/0100019b45be8dd6-f7340055-99c9-402f-9d8b-12a269686952-000000/9K9B3knVxf91qQL8a3F1SOSPeazp5wO5_IszlIg5Jb0=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackernoon.com%2Fthe-seven-pillars-of-a-production-grade-agent-architecture%3Futm_source=tldrdata/1/0100019b45be8dd6-f7340055-99c9-402f-9d8b-12a269686952-000000/9K9B3knVxf91qQL8a3F1SOSPeazp5wO5_IszlIg5Jb0=436", "authors": ["TLDR Newsletter"], "title": "The Seven Pillars of a Production-Grade Agent Architecture", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackernoon.com%2Fthe-seven-pillars-of-a-production-grade-agent-architecture%3Futm_source=tldrdata/1/0100019b45be8dd6-f7340055-99c9-402f-9d8b-12a269686952-000000/9K9B3knVxf91qQL8a3F1SOSPeazp5wO5_IszlIg5Jb0=436", "summary": "The Seven Pillars of a Production-Grade Agent Architecture (12 minute read) Enterprise-grade agentic AI needs a strong, well-designed foundation.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u751f\u4ea7\u7ea7\u667a\u80fd\u4f53\u67b6\u6784\u7684\u4e03\u4e2a\u6838\u5fc3\u652f\u67f1\uff0c\u4e3a\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53AI\u63d0\u4f9b\u575a\u5b9e\u57fa\u7840", "motivation": "\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53AI\u9700\u8981\u4e00\u4e2a\u5f3a\u5927\u4e14\u8bbe\u8ba1\u826f\u597d\u7684\u57fa\u7840\u67b6\u6784\uff0c\u4ee5\u652f\u6301\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u548c\u8fd0\u884c", "method": "\u901a\u8fc7\u8bc6\u522b\u548c\u5b9a\u4e49\u4e03\u4e2a\u5173\u952e\u67b6\u6784\u652f\u67f1\u6765\u6784\u5efa\u751f\u4ea7\u7ea7\u667a\u80fd\u4f53\u7cfb\u7edf", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e03\u4e2a\u6838\u5fc3\u652f\u67f1\u7684\u7cfb\u7edf\u5316\u67b6\u6784\u6846\u67b6\uff0c\u4e3a\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53AI\u63d0\u4f9b\u8bbe\u8ba1\u6307\u5bfc", "conclusion": "\u4e03\u4e2a\u652f\u67f1\u7684\u67b6\u6784\u8bbe\u8ba1\u5bf9\u4e8e\u6784\u5efa\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u7cfb\u7edf\u81f3\u5173\u91cd\u8981", "topic": "agent analysis"}}
{"id": "tldr.2512.47f10417", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhunterwalk.com%2F2025%2F12%2F19%2Fgraphite-gets-bought-by-cursor-three-reflections-from-six-years-of-work-pmf-didnt-happen-right-away-working-irl-mattered-three-founders-three-distinct-roles%2F%3Futm_source=tldrnewsletter/1/0100019b45cd8f06-af154382-1306-4c68-a659-70e51a43af8a-000000/RZ60X8ljv0jItxoIbSKN-MBIHhrRRq-NoXoDFhjQvfo=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhunterwalk.com%2F2025%2F12%2F19%2Fgraphite-gets-bought-by-cursor-three-reflections-from-six-years-of-work-pmf-didnt-happen-right-away-working-irl-mattered-three-founders-three-distinct-roles%2F%3Futm_source=tldrnewsletter/1/0100019b45cd8f06-af154382-1306-4c68-a659-70e51a43af8a-000000/RZ60X8ljv0jItxoIbSKN-MBIHhrRRq-NoXoDFhjQvfo=436", "authors": ["TLDR Newsletter"], "title": "Graphite Gets Bought By Cursor: Three Reflections from Six Years of Work", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhunterwalk.com%2F2025%2F12%2F19%2Fgraphite-gets-bought-by-cursor-three-reflections-from-six-years-of-work-pmf-didnt-happen-right-away-working-irl-mattered-three-founders-three-distinct-roles%2F%3Futm_source=tldrnewsletter/1/0100019b45cd8f06-af154382-1306-4c68-a659-70e51a43af8a-000000/RZ60X8ljv0jItxoIbSKN-MBIHhrRRq-NoXoDFhjQvfo=436", "summary": "Graphite Gets Bought By Cursor: Three Reflections from Six Years of Work (9 minute read) Software quality startup Graphite has been acquired by code generation startup Cursor. Graphite started out as Screenplay, which couldn't find product-market fit right away. The conversations around Graphite started right before the pandemic lockdowns. The team started working in person together as soon as it was advisable, allowing them to knit a culture early and build trust. The three founders took thr...", "source": "tldr", "AI": {"tldr": "Graphite\uff08\u524d\u8eab\u4e3aScreenplay\uff09\u88ab\u4ee3\u7801\u751f\u6210\u521d\u521b\u516c\u53f8Cursor\u6536\u8d2d\uff0c\u6587\u7ae0\u5206\u4eab\u4e86\u516d\u5e74\u6765\u4ece\u4ea7\u54c1\u5e02\u573a\u5339\u914d\u5931\u8d25\u5230\u6210\u529f\u9000\u51fa\u7684\u4e09\u4e2a\u5173\u952e\u53cd\u601d", "motivation": "\u5206\u4eabGraphite\u4eceScreenplay\u65f6\u671f\u4ea7\u54c1\u5e02\u573a\u5339\u914d\u5931\u8d25\uff0c\u5230\u88abCursor\u6536\u8d2d\u7684\u516d\u5e74\u521b\u4e1a\u5386\u7a0b\u4e2d\u7684\u7ecf\u9a8c\u6559\u8bad\u548c\u53cd\u601d", "method": "\u57fa\u4e8e\u521b\u59cb\u56e2\u961f\u4eb2\u8eab\u7ecf\u5386\uff0c\u603b\u7ed3\u4e09\u4e2a\u5173\u952e\u53cd\u601d\uff1a1\uff09\u65e9\u671f\u56e2\u961f\u9762\u5bf9\u9762\u534f\u4f5c\u5efa\u7acb\u6587\u5316\u548c\u4fe1\u4efb\u7684\u91cd\u8981\u6027\uff1b2\uff09\u4ece\u4ea7\u54c1\u5e02\u573a\u5339\u914d\u5931\u8d25\u4e2d\u5b66\u4e60\u8f6c\u578b\uff1b3\uff09\u5728\u75ab\u60c5\u80cc\u666f\u4e0b\u8c03\u6574\u5de5\u4f5c\u6a21\u5f0f", "result": "Graphite\u6210\u529f\u88abCursor\u6536\u8d2d\uff0c\u5b8c\u6210\u4e86\u521b\u4e1a\u9000\u51fa\uff0c\u56e2\u961f\u5728\u516d\u5e74\u5386\u7a0b\u4e2d\u79ef\u7d2f\u4e86\u5b9d\u8d35\u7684\u521b\u4e1a\u7ecf\u9a8c", "conclusion": "\u521b\u4e1a\u6210\u529f\u9700\u8981\u56e2\u961f\u6587\u5316\u3001\u9002\u5e94\u80fd\u529b\u548c\u65f6\u673a\u628a\u63e1\u7684\u7ed3\u5408\uff0c\u9762\u5bf9\u9762\u534f\u4f5c\u5bf9\u65e9\u671f\u56e2\u961f\u5efa\u8bbe\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u662f\u6210\u529f\u8f6c\u578b\u7684\u5173\u952e", "topic": "swe application"}}
{"id": "tldr.2512.6a146e37", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b45cd8f06-af154382-1306-4c68-a659-70e51a43af8a-000000/EUbLliy0ru1LtJzwhFLsc9y7jk_1e8edz5Tv6Oy__Lc=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b45cd8f06-af154382-1306-4c68-a659-70e51a43af8a-000000/EUbLliy0ru1LtJzwhFLsc9y7jk_1e8edz5Tv6Oy__Lc=436", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b45cd8f06-af154382-1306-4c68-a659-70e51a43af8a-000000/EUbLliy0ru1LtJzwhFLsc9y7jk_1e8edz5Tv6Oy__Lc=436", "summary": "2025 LLM Year in Review (11 minute read) 2025 was a strong and eventful year of progress in LLMs.", "source": "tldr", "AI": {"tldr": "2025\u5e74\u662fLLM\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u7684\u4e00\u5e74\uff0c\u56de\u987e\u4e86\u8be5\u5e74\u5ea6\u7684\u91cd\u8981\u53d1\u5c55\u548c\u6210\u5c31", "motivation": "\u603b\u7ed32025\u5e74LLM\u9886\u57df\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u8bb0\u5f55\u91cd\u8981\u91cc\u7a0b\u7891\u548c\u8fdb\u6b65\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u5e74\u5ea6\u56de\u987e", "method": "\u91c7\u7528\u5e74\u5ea6\u56de\u987e\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u74062025\u5e74LLM\u9886\u57df\u7684\u5173\u952e\u4e8b\u4ef6\u3001\u6280\u672f\u7a81\u7834\u548c\u91cd\u8981\u8bba\u6587", "result": "2025\u5e74LLM\u5728\u591a\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u5305\u62ec\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3001\u65b0\u67b6\u6784\u51fa\u73b0\u3001\u5e94\u7528\u6269\u5c55\u7b49\uff0c\u662f\u5145\u6ee1\u6d3b\u529b\u548c\u6210\u5c31\u7684\u4e00\u5e74", "conclusion": "2025\u5e74\u662fLLM\u53d1\u5c55\u7684\u5173\u952e\u5e74\u4efd\uff0c\u5c55\u73b0\u4e86\u8be5\u9886\u57df\u7684\u6301\u7eed\u521b\u65b0\u548c\u5feb\u901f\u8fdb\u6b65\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840", "topic": "agent analysis"}}
{"id": "tldr.2512.27b66a26", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fnews-insights%2Fpolicy-news-and-insights%2Fthe-future-of-ai-powered-software-optimization-and-how-it-can-help-your-team%2F%3Futm_source=tldrdevops/1/0100019b45f3b9fb-9e07cbe1-ecac-48c8-bf0f-2f926a3c8f15-000000/8RwRUa1gMpJukJLGYPfwkC59_q_PZG0yUltOfH6ivBo=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fnews-insights%2Fpolicy-news-and-insights%2Fthe-future-of-ai-powered-software-optimization-and-how-it-can-help-your-team%2F%3Futm_source=tldrdevops/1/0100019b45f3b9fb-9e07cbe1-ecac-48c8-bf0f-2f926a3c8f15-000000/8RwRUa1gMpJukJLGYPfwkC59_q_PZG0yUltOfH6ivBo=436", "authors": ["TLDR Newsletter"], "title": "The future of AI-powered software optimization", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fnews-insights%2Fpolicy-news-and-insights%2Fthe-future-of-ai-powered-software-optimization-and-how-it-can-help-your-team%2F%3Futm_source=tldrdevops/1/0100019b45f3b9fb-9e07cbe1-ecac-48c8-bf0f-2f926a3c8f15-000000/8RwRUa1gMpJukJLGYPfwkC59_q_PZG0yUltOfH6ivBo=436", "summary": "The future of AI-powered software optimization (and how it can help your team) (7 minute read) GitHub's Continuous Efficiency combines AI-driven automation and green software practices to make codebases self-optimizing for performance, efficiency, and sustainability. Using agentic workflows, developers can author natural-language rules that AI agents apply to improve code quality, enforce standards, and iteratively enhance performance across heterogeneous repositories.", "source": "tldr", "AI": {"tldr": "GitHub\u63a8\u51faContinuous Efficiency\u5e73\u53f0\uff0c\u7ed3\u5408AI\u81ea\u52a8\u5316\u548c\u7eff\u8272\u8f6f\u4ef6\u5b9e\u8df5\uff0c\u4f7f\u4ee3\u7801\u5e93\u80fd\u591f\u81ea\u6211\u4f18\u5316\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u3002\u901a\u8fc7\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u7528\u81ea\u7136\u8bed\u8a00\u7f16\u5199\u89c4\u5219\uff0cAI\u667a\u80fd\u4f53\u5e94\u7528\u8fd9\u4e9b\u89c4\u5219\u6765\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u3001\u6267\u884c\u6807\u51c6\uff0c\u5e76\u5728\u5f02\u6784\u4ed3\u5e93\u4e2d\u8fed\u4ee3\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5f00\u53d1\u9762\u4e34\u4ee3\u7801\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u3001\u6027\u80fd\u4f18\u5316\u590d\u6742\u3001\u53ef\u6301\u7eed\u6027\u8981\u6c42\u63d0\u9ad8\u7b49\u6311\u6218\u3002\u4f20\u7edf\u7684\u624b\u52a8\u4f18\u5316\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5728\u5f02\u6784\u4ee3\u7801\u5e93\u4e2d\u89c4\u6a21\u5316\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u667a\u80fd\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e2e\u52a9\u5f00\u53d1\u56e2\u961f\u6301\u7eed\u4f18\u5316\u8f6f\u4ef6\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff08agentic workflows\uff09\u67b6\u6784\uff0c\u5f00\u53d1\u8005\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7f16\u5199\u4f18\u5316\u89c4\u5219\uff0cAI\u667a\u80fd\u4f53\u81ea\u52a8\u5e94\u7528\u8fd9\u4e9b\u89c4\u5219\u5230\u4ee3\u7801\u5e93\u4e2d\u3002\u7ed3\u5408AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u548c\u7eff\u8272\u8f6f\u4ef6\u5b9e\u8df5\uff0c\u5f62\u6210\u81ea\u6211\u4f18\u5316\u7684\u4ee3\u7801\u751f\u6001\u7cfb\u7edf\u3002\u7cfb\u7edf\u80fd\u591f\u8de8\u5f02\u6784\u4ed3\u5e93\u8fdb\u884c\u8fed\u4ee3\u6027\u80fd\u589e\u5f3a\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u4f7f\u4ee3\u7801\u5e93\u81ea\u6211\u4f18\u5316\u7684\u5e73\u53f0\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u8f7b\u677e\u5b9e\u73b0\u4ee3\u7801\u8d28\u91cf\u6539\u8fdb\u3001\u6807\u51c6\u6267\u884c\u548c\u6027\u80fd\u4f18\u5316\u3002\u8be5\u5e73\u53f0\u5c06AI\u81ea\u52a8\u5316\u4e0e\u7eff\u8272\u8f6f\u4ef6\u5b9e\u8df5\u76f8\u7ed3\u5408\uff0c\u4e3a\u5f00\u53d1\u56e2\u961f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8f6f\u4ef6\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Continuous Efficiency\u5e73\u53f0\u4ee3\u8868\u4e86AI\u9a71\u52a8\u8f6f\u4ef6\u4f18\u5316\u7684\u672a\u6765\u65b9\u5411\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u548c\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\uff0c\u4f7f\u8f6f\u4ef6\u4f18\u5316\u53d8\u5f97\u66f4\u52a0\u53ef\u8bbf\u95ee\u548c\u53ef\u6269\u5c55\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u56e2\u961f\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002", "topic": "swe application"}}
{"id": "tldr.2512.86aa16f4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fedxeth%2Fc9669f46a04687375fd9150c4874286e%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/UD4lxkzuTsxzjcmsu6rxxVZNCW9mNaAmkDDeH-NVPYQ=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fedxeth%2Fc9669f46a04687375fd9150c4874286e%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/UD4lxkzuTsxzjcmsu6rxxVZNCW9mNaAmkDDeH-NVPYQ=436", "authors": ["TLDR Newsletter"], "title": "Claude's frontend design skill", "comment": "Source: TLDR Newsletter, Date: 2025-12-22, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fedxeth%2Fc9669f46a04687375fd9150c4874286e%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/UD4lxkzuTsxzjcmsu6rxxVZNCW9mNaAmkDDeH-NVPYQ=436", "summary": "Claude's frontend design skill (2 minute read) This gist contains a Claude skill for creating production-grade frontend interfaces that avoid generic AI slop aesthetics. The user provides frontend requirements, and the AI generates production-grade and functional code with visually striking and memorable design. The skill instructs the agent to never use generic AI-generated aesthetics, to interpret prompts creatively, and to make unexpected choices that feel genuinely designed for the context.", "source": "tldr", "AI": {"tldr": "Claude\u6280\u80fd\u7528\u4e8e\u521b\u5efa\u751f\u4ea7\u7ea7\u524d\u7aef\u754c\u9762\uff0c\u907f\u514d\u901a\u7528AI\u751f\u6210\u7684\u7f8e\u5b66\u98ce\u683c\uff0c\u751f\u6210\u89c6\u89c9\u51b2\u51fb\u529b\u5f3a\u4e14\u529f\u80fd\u5b8c\u6574\u7684\u4ee3\u7801", "motivation": "\u89e3\u51b3AI\u751f\u6210\u524d\u7aef\u754c\u9762\u65f6\u5e38\u89c1\u7684\"\u901a\u7528AI\u7f8e\u5b66\"\u95ee\u9898\uff0c\u907f\u514d\u751f\u6210\u7f3a\u4e4f\u521b\u610f\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u6a21\u677f\u5316\u8bbe\u8ba1", "method": "\u5f00\u53d1Claude\u6280\u80fd\uff0c\u901a\u8fc7\u7279\u5b9a\u6307\u4ee4\u5f15\u5bfcAI\uff1a\u907f\u514d\u901a\u7528AI\u7f8e\u5b66\u3001\u521b\u9020\u6027\u89e3\u8bfb\u9700\u6c42\u3001\u505a\u51fa\u610f\u5916\u4f46\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u8bbe\u8ba1\u9009\u62e9", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u751f\u4ea7\u7ea7\u524d\u7aef\u754c\u9762\u7684\u6280\u80fd\uff0c\u786e\u4fdd\u4ee3\u7801\u529f\u80fd\u5b8c\u6574\u4e14\u8bbe\u8ba1\u5177\u6709\u89c6\u89c9\u51b2\u51fb\u529b\u548c\u72ec\u7279\u6027", "conclusion": "\u901a\u8fc7\u7279\u5b9a\u6307\u4ee4\u7ea6\u675f\u53ef\u4ee5\u663e\u8457\u63d0\u5347AI\u751f\u6210\u524d\u7aef\u754c\u9762\u7684\u8d28\u91cf\u548c\u539f\u521b\u6027\uff0c\u907f\u514d\u843d\u5165\u901a\u7528AI\u7f8e\u5b66\u7684\u9677\u9631", "topic": "code agent"}}
