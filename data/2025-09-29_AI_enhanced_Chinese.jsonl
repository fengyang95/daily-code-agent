{"id": "2509.21427", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21427", "abs": "https://arxiv.org/abs/2509.21427", "authors": ["Ying Wang", "Wenjun Mao", "Chong Wang", "Zhenhao Zhou", "Yicheng Zhou", "Wenyun Zhao", "Yiling Lou", "Xin Peng"], "title": "Extracting Conceptual Knowledge to Locate Software Issues", "comment": null, "summary": "Issue localization, which identifies faulty code elements such as files or\nfunctions, is critical for effective bug fixing. While recent LLM-based and\nLLM-agent-based approaches improve accuracy, they struggle in large-scale\nrepositories due to concern mixing, where relevant logic is buried in large\nfunctions, and concern scattering, where related logic is dispersed across\nfiles.\n  To address these challenges, we propose RepoLens, a novel approach that\nabstracts and leverages conceptual knowledge from code repositories. RepoLens\ndecomposes fine-grained functionalities and recomposes them into high-level\nconcerns, semantically coherent clusters of functionalities that guide LLMs. It\noperates in two stages: an offline stage that extracts and enriches conceptual\nknowledge into a repository-wide knowledge base, and an online stage that\nretrieves issue-specific terms, clusters and ranks concerns by relevance, and\nintegrates them into localization workflows via minimally intrusive prompt\nenhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks\nderived from SWE-Lancer. RepoLens consistently improves three state-of-the-art\ntools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains\nof over 22% in Hit@k and 46% in Recall@k for file- and function-level\nlocalization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with\nHit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies\nand manual evaluation confirm the effectiveness and reliability of the\nconstructed concerns.", "AI": {"tldr": "RepoLens\u901a\u8fc7\u63d0\u53d6\u548c\u5229\u7528\u4ee3\u7801\u4ed3\u5e93\u7684\u6982\u5ff5\u77e5\u8bc6\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u4ed3\u5e93\u4e2d\u7684\u95ee\u9898\u5b9a\u4f4d\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709LLM\u5de5\u5177\u5728\u6587\u4ef6\u7ea7\u548c\u51fd\u6570\u7ea7\u5b9a\u4f4d\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u4ee3\u7801\u4ed3\u5e93\u4e2d\u9762\u4e34\u7684\u5173\u6ce8\u70b9\u6df7\u5408\uff08\u76f8\u5173\u903b\u8f91\u88ab\u57cb\u6ca1\u5728\u5927\u51fd\u6570\u4e2d\uff09\u548c\u5173\u6ce8\u70b9\u5206\u6563\uff08\u76f8\u5173\u903b\u8f91\u5206\u6563\u5728\u4e0d\u540c\u6587\u4ef6\u4e2d\uff09\u7684\u95ee\u9898\u3002", "method": "RepoLens\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u79bb\u7ebf\u9636\u6bb5\u63d0\u53d6\u548c\u4e30\u5bcc\u6982\u5ff5\u77e5\u8bc6\u6784\u5efa\u4ed3\u5e93\u7ea7\u77e5\u8bc6\u5e93\uff1b\u5728\u7ebf\u9636\u6bb5\u68c0\u7d22\u95ee\u9898\u7279\u5b9a\u672f\u8bed\uff0c\u6309\u76f8\u5173\u6027\u805a\u7c7b\u548c\u6392\u5e8f\u5173\u6ce8\u70b9\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u4fb5\u5165\u5f0f\u63d0\u793a\u589e\u5f3a\u96c6\u6210\u5230\u5b9a\u4f4d\u5de5\u4f5c\u6d41\u4e2d\u3002", "result": "\u5728SWE-Lancer-Loc\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRepoLens\u663e\u8457\u63d0\u5347\u4e86AgentLess\u3001OpenHands\u548cmini-SWE-agent\u4e09\u79cd\u5148\u8fdb\u5de5\u5177\u7684\u6027\u80fd\uff0c\u5728\u6587\u4ef6\u7ea7\u548c\u51fd\u6570\u7ea7\u5b9a\u4f4d\u4e2d\u5e73\u5747\u63d0\u5347Hit@k\u8d85\u8fc722%\uff0cRecall@k\u8d85\u8fc746%\u3002\u5728\u4e0d\u540c\u6a21\u578b\u4e0aHit@1\u548cRecall@10\u5206\u522b\u63d0\u5347\u9ad8\u8fbe504%\u548c376%\u3002", "conclusion": "RepoLens\u901a\u8fc7\u6982\u5ff5\u77e5\u8bc6\u62bd\u8c61\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u4ee3\u7801\u4ed3\u5e93\u7684\u95ee\u9898\u5b9a\u4f4d\u6311\u6218\uff0c\u6d88\u878d\u7814\u7a76\u548c\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6784\u5efa\u7684\u5173\u6ce8\u70b9\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "topic": "swe application"}}
{"id": "2509.21816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21816", "abs": "https://arxiv.org/abs/2509.21816", "authors": ["Yuhang Xie", "Jian Mu", "Xiaojun Ma", "Chaoyun Zhang", "Lu Wang", "Mengyu Zhou", "Mugeng Liu", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Shi Han", "Dongmei Zhang"], "title": "No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials", "comment": null, "summary": "Excel is one of the most widely used productivity tools across domains,\noffering rich functionality but also overwhelming users with its complexity.\nThis creates a persistent demand for tutorials to support effective usage.\nHowever, existing tutorials are manually authored by experts, require frequent\nupdates after each software release, and incur substantial labor costs. Prior\nwork has not achieved fully automated tutorial generation, since existing\nmethods still depend on handcrafted operation sequences or example materials.\nIn this paper, we present the first framework for automatically generating\nExcel tutorials directly from natural language task descriptions. Our framework\nfirst instantiates the task. Then a central component of this framework,\nExecution Agent, plans and executes the solution in Excel, and collects the\nintermediate artifacts required for tutorial construction. These artifacts are\nthen transformed into both structured Excel documents and video demonstrations.\nTo build a comprehensive tutorial corpus, we collected 1,559 task descriptions\nfrom real-world scenarios. In addition, we designed a systematic evaluation\nframework that integrates assessments from both large language models (LLMs)\nand human reviewers. Experimental results show that our framework improves task\nexecution success rates by 8.5% over state-of-the-art baselines. Moreover, the\ngenerated tutorials demonstrate superior readability and instructional\neffectiveness, often approaching or surpassing expert-authored materials.\nImportantly, the automated pipeline eliminates manual labor and reduces time\ncosts to 1/20 of expert authoring, making scalable and high-quality tutorial\ngeneration practical for the first time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4ece\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u81ea\u52a8\u751f\u6210Excel\u6559\u7a0b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6267\u884c\u4ee3\u7406\u5728Excel\u4e2d\u89c4\u5212\u548c\u6267\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u6536\u96c6\u4e2d\u95f4\u5de5\u4ef6\u5e76\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u6863\u548c\u89c6\u9891\u6f14\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6559\u7a0b\u751f\u6210\u6548\u7387\u3002", "motivation": "Excel\u529f\u80fd\u590d\u6742\u4f46\u5e7f\u6cdb\u4f7f\u7528\uff0c\u73b0\u6709\u6559\u7a0b\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u7f16\u5199\uff0c\u66f4\u65b0\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u81ea\u52a8\u5316\uff0c\u9700\u8981\u89e3\u51b3\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6559\u7a0b\u7684\u6311\u6218\u3002", "method": "\u6846\u67b6\u5305\u542b\u4efb\u52a1\u5b9e\u4f8b\u5316\u3001\u6267\u884c\u4ee3\u7406\u5728Excel\u4e2d\u89c4\u5212\u6267\u884c\u89e3\u51b3\u65b9\u6848\u3001\u6536\u96c6\u4e2d\u95f4\u5de5\u4ef6\uff0c\u7136\u540e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316Excel\u6587\u6863\u548c\u89c6\u9891\u6f14\u793a\u3002", "result": "\u57281559\u4e2a\u771f\u5b9e\u573a\u666f\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4efb\u52a1\u6267\u884c\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad88.5%\uff0c\u751f\u6210\u6559\u7a0b\u7684\u53ef\u8bfb\u6027\u548c\u6559\u5b66\u6548\u679c\u63a5\u8fd1\u6216\u8d85\u8fc7\u4e13\u5bb6\u7f16\u5199\u6750\u6599\uff0c\u65f6\u95f4\u6210\u672c\u964d\u81f3\u4e13\u5bb6\u7f16\u5199\u76841/20\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u9996\u6b21\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u6559\u7a0b\u751f\u6210\uff0c\u6d88\u9664\u4e86\u4eba\u5de5\u52b3\u52a8\uff0c\u4f7f\u5927\u89c4\u6a21Excel\u6559\u7a0b\u751f\u6210\u53d8\u5f97\u5b9e\u7528\u53ef\u884c\u3002", "topic": "swe application"}}
{"id": "2509.21891", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21891", "abs": "https://arxiv.org/abs/2509.21891", "authors": ["Yangtian Zi", "Zixuan Wu", "Aleksander Boruch-Gruszecki", "Jonathan Bell", "Arjun Guha"], "title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans", "comment": null, "summary": "Fine-tuning large language models for code editing has typically relied on\nmining commits and pull requests. The working hypothesis has been that commit\nmessages describe human intent in natural language, and patches to code\ndescribe the changes that implement that intent. However, much of the\npreviously collected data is noisy: commit messages are terse, human-written\ncommits commingle several unrelated edits, and many commits come from simple,\nrule-based bots.\n  The recent adoption of software engineering agents changes this landscape.\nCode changes co-authored by humans and agents tend to be more narrowly scoped\nand focused on clearer goals. Their commit messages, generated by LLMs,\narticulate intent and rationale in much greater detail. Moreover, when these\nchanges land in public repositories, they are implicitly filtered by humans:\nmaintainers discard low-quality commits to their projects.\n  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,\nOpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August\n2025. We describe the identification and curation pipeline, quantify adoption\ntrends of these agents, and analyze the structural properties of the edits.\nFinally, we show that models fine-tuned on AgentPack can outperform models\ntrained on prior human-only commit corpora, highlighting the potential of using\npublic data from software engineering agents to train future code-editing\nmodels.", "AI": {"tldr": "AgentPack\u662f\u4e00\u4e2a\u5305\u542b130\u4e07\u4ee3\u7801\u7f16\u8f91\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u7f16\u8f91\u7531Claude Code\u3001OpenAI Codex\u548cCursor Agent\u7b49\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5171\u540c\u5b8c\u6210\u3002\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528AgentPack\u5fae\u8c03\u7684\u6a21\u578b\u5728\u4ee3\u7801\u7f16\u8f91\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u4f20\u7edf\u4eba\u7c7b\u63d0\u4ea4\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u63d0\u4ea4\u548c\u62c9\u53d6\u8bf7\u6c42\u7684\u4ee3\u7801\u7f16\u8f91\u6570\u636e\u5b58\u5728\u566a\u58f0\u95ee\u9898\uff1a\u63d0\u4ea4\u4fe1\u606f\u7b80\u77ed\u3001\u4eba\u7c7b\u63d0\u4ea4\u6df7\u6742\u591a\u4e2a\u65e0\u5173\u7f16\u8f91\u3001\u8bb8\u591a\u63d0\u4ea4\u6765\u81ea\u7b80\u5355\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u673a\u5668\u4eba\u3002\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u51fa\u73b0\u6539\u53d8\u4e86\u8fd9\u4e00\u5c40\u9762\uff0c\u5b83\u4eec\u4e0e\u4eba\u7c7b\u5171\u540c\u5b8c\u6210\u7684\u4ee3\u7801\u53d8\u66f4\u66f4\u805a\u7126\u3001\u76ee\u6807\u66f4\u6e05\u6670\u3002", "method": "\u6784\u5efaAgentPack\u8bed\u6599\u5e93\uff0c\u5305\u542b130\u4e07\u4ee3\u7801\u7f16\u8f91\uff0c\u6765\u81eaClaude Code\u3001OpenAI Codex\u548cCursor Agent\u5728\u516c\u5171GitHub\u9879\u76ee\u4e2d\u7684\u534f\u4f5c\u7f16\u8f91\u3002\u5f00\u53d1\u4e86\u8bc6\u522b\u548c\u7b5b\u9009\u6d41\u7a0b\uff0c\u91cf\u5316\u4e86\u8fd9\u4e9b\u4ee3\u7406\u7684\u91c7\u7528\u8d8b\u52bf\uff0c\u5e76\u5206\u6790\u4e86\u7f16\u8f91\u7684\u7ed3\u6784\u7279\u6027\u3002", "result": "\u57fa\u4e8eAgentPack\u5fae\u8c03\u7684\u6a21\u578b\u5728\u4ee3\u7801\u7f16\u8f91\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u4f20\u7edf\u4eba\u7c7b\u63d0\u4ea4\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4ea7\u751f\u7684\u516c\u5171\u6570\u636e\u8bad\u7ec3\u672a\u6765\u4ee3\u7801\u7f16\u8f91\u6a21\u578b\u7684\u6f5c\u529b\u3002", "conclusion": "\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4ea7\u751f\u7684\u4ee3\u7801\u7f16\u8f91\u6570\u636e\u8d28\u91cf\u66f4\u9ad8\uff0c\u4e3a\u8bad\u7ec3\u66f4\u597d\u7684\u4ee3\u7801\u7f16\u8f91\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2509.21403", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21403", "abs": "https://arxiv.org/abs/2509.21403", "authors": ["Rushil Gupta", "Jason Hartford", "Bang Liu"], "title": "LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?", "comment": "Accepted to EMNLP 2025", "summary": "Large language models (LLMs) have recently been proposed as general-purpose\nagents for experimental design, with claims that they can perform in-context\nexperimental design. We evaluate this hypothesis using both open- and\nclosed-source instruction-tuned LLMs applied to genetic perturbation and\nmolecular property discovery tasks. We find that LLM-based agents show no\nsensitivity to experimental feedback: replacing true outcomes with randomly\npermuted labels has no impact on performance. Across benchmarks, classical\nmethods such as linear bandits and Gaussian process optimization consistently\noutperform LLM agents. We further propose a simple hybrid method, LLM-guided\nNearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with\nnearest-neighbor sampling to guide the design of experiments. LLMNN achieves\ncompetitive or superior performance across domains without requiring\nsignificant in-context adaptation. These results suggest that current open- and\nclosed-source LLMs do not perform in-context experimental design in practice\nand highlight the need for hybrid frameworks that decouple prior-based\nreasoning from batch acquisition with updated posteriors.", "AI": {"tldr": "\u8bc4\u4f30LLM\u4f5c\u4e3a\u5b9e\u9a8c\u8bbe\u8ba1\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709LLM\u5bf9\u5b9e\u9a8c\u53cd\u9988\u4e0d\u654f\u611f\uff0c\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u63d0\u51fa\u7ed3\u5408LLM\u5148\u9a8c\u77e5\u8bc6\u548c\u6700\u8fd1\u90bb\u91c7\u6837\u7684\u6df7\u5408\u65b9\u6cd5LLMNN", "motivation": "\u9a8c\u8bc1LLM\u662f\u5426\u80fd\u591f\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u8bc4\u4f30\u5176\u5728\u9057\u4f20\u6270\u52a8\u548c\u5206\u5b50\u5c5e\u6027\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "method": "\u4f7f\u7528\u5f00\u6e90\u548c\u95ed\u6e90\u6307\u4ee4\u8c03\u4f18\u7684LLM\uff0c\u6bd4\u8f83\u4f20\u7edf\u65b9\u6cd5\uff08\u7ebf\u6027bandits\u548c\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316\uff09\u4e0eLLM\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u63d0\u51faLLMNN\u6df7\u5408\u65b9\u6cd5", "result": "LLM\u4ee3\u7406\u5bf9\u5b9e\u9a8c\u53cd\u9988\u4e0d\u654f\u611f\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u597d\uff0cLLMNN\u65b9\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u5b9e\u73b0\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u6027\u80fd", "conclusion": "\u5f53\u524dLLM\u65e0\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u9700\u8981\u5c06\u57fa\u4e8e\u5148\u9a8c\u7684\u63a8\u7406\u4e0e\u66f4\u65b0\u540e\u9a8c\u7684\u6279\u91cf\u83b7\u53d6\u89e3\u8026\u7684\u6df7\u5408\u6846\u67b6", "topic": "agent analysis"}}
{"id": "2509.22097", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22097", "abs": "https://arxiv.org/abs/2509.22097", "authors": ["Junkai Chen", "Huihui Huang", "Yunbo Lyu", "Junwen An", "Jieke Shi", "Chengran Yang", "Ting Zhang", "Haoye Tian", "Yikun Li", "Zhenhao Li", "Xin Zhou", "Xing Hu", "David Lo"], "title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios", "comment": null, "summary": "Large language model (LLM) powered code agents are rapidly transforming\nsoftware engineering by automating tasks such as testing, debugging, and\nrepairing, yet the security risks of their generated code have become a\ncritical concern. Existing benchmarks have offered valuable insights but remain\ninsufficient: they often overlook the genuine context in which vulnerabilities\nwere introduced or adopt narrow evaluation protocols that fail to capture\neither functional correctness or newly introduced vulnerabilities. We therefore\nintroduce SecureAgentBench, a benchmark of 105 coding tasks designed to\nrigorously evaluate code agents' capabilities in secure code generation. Each\ntask includes (i) realistic task settings that require multi-file edits in\nlarge repositories, (ii) aligned contexts based on real-world open-source\nvulnerabilities with precisely identified introduction points, and (iii)\ncomprehensive evaluation that combines functionality testing, vulnerability\nchecking through proof-of-concept exploits, and detection of newly introduced\nvulnerabilities using static analysis. We evaluate three representative agents\n(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7\nSonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents\nstruggle to produce secure code, as even the best-performing one, SWE-agent\nsupported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,\n(ii) some agents produce functionally correct code but still introduce\nvulnerabilities, including new ones not previously recorded, and (iii) adding\nexplicit security instructions for agents does not significantly improve secure\ncoding, underscoring the need for further research. These findings establish\nSecureAgentBench as a rigorous benchmark for secure code generation and a step\ntoward more reliable software development with LLMs.", "AI": {"tldr": "SecureAgentBench\u662f\u4e00\u4e2a\u5305\u542b105\u4e2a\u7f16\u7801\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u5728\u5b89\u5168\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u4ee3\u7406\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u6700\u4f73\u4ee3\u7406\u4e5f\u53ea\u80fd\u8fbe\u523015.2%\u7684\u6b63\u786e\u4e14\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u751f\u6210\u7684\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f80\u5f80\u5ffd\u7565\u6f0f\u6d1e\u5f15\u5165\u7684\u771f\u5b9e\u4e0a\u4e0b\u6587\uff0c\u6216\u91c7\u7528\u72ed\u7a84\u7684\u8bc4\u4f30\u534f\u8bae\u65e0\u6cd5\u6355\u6349\u529f\u80fd\u6b63\u786e\u6027\u6216\u65b0\u5f15\u5165\u7684\u6f0f\u6d1e\u3002", "method": "\u6784\u5efa\u5305\u542b105\u4e2a\u7f16\u7801\u4efb\u52a1\u7684SecureAgentBench\u57fa\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5305\u542b\uff1a(i)\u9700\u8981\u5728\u5927\u4ed3\u5e93\u4e2d\u8fdb\u884c\u591a\u6587\u4ef6\u7f16\u8f91\u7684\u73b0\u5b9e\u4efb\u52a1\u8bbe\u7f6e\uff0c(ii)\u57fa\u4e8e\u771f\u5b9e\u5f00\u6e90\u6f0f\u6d1e\u7684\u4e0a\u4e0b\u6587\uff0c(iii)\u7ed3\u5408\u529f\u80fd\u6d4b\u8bd5\u3001\u6f0f\u6d1e\u68c0\u67e5\u548c\u65b0\u6f0f\u6d1e\u68c0\u6d4b\u7684\u7efc\u5408\u8bc4\u4f30\u3002\u8bc4\u4f30\u4e86\u4e09\u4e2a\u4ee3\u8868\u6027\u4ee3\u7406\u548c\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5f53\u524d\u4ee3\u7406\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u4ee3\u7406(SWE-agent + DeepSeek-V3.1)\u4ec5\u8fbe\u523015.2%\u7684\u6b63\u786e\u4e14\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff1b\u4e00\u4e9b\u4ee3\u7406\u751f\u6210\u529f\u80fd\u6b63\u786e\u4ee3\u7801\u4f46\u4ecd\u5f15\u5165\u6f0f\u6d1e\uff1b\u660e\u786e\u7684\u5b89\u5168\u6307\u4ee4\u5bf9\u6539\u5584\u5b89\u5168\u7f16\u7801\u6ca1\u6709\u663e\u8457\u5e2e\u52a9\u3002", "conclusion": "SecureAgentBench\u4e3a\u5b89\u5168\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u662f\u8fc8\u5411\u66f4\u53ef\u9760LLM\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e00\u6b65\uff0c\u4f46\u5f53\u524d\u4ee3\u7801\u4ee3\u7406\u5728\u5b89\u5168\u4ee3\u7801\u751f\u6210\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "topic": "swe benchmark"}}
{"id": "2509.21593", "categories": ["cs.AI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2509.21593", "abs": "https://arxiv.org/abs/2509.21593", "authors": ["Peng Luo", "Xiayin Lou", "Yu Zheng", "Zhuo Zheng", "Stefano Ermon"], "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models", "comment": null, "summary": "Geospatial modeling provides critical solutions for pressing global\nchallenges such as sustainability and climate change. Existing large language\nmodel (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at\nevolving generic code but lack the domain knowledge and multi-step reasoning\nrequired for complex geospatial problems. We introduce GeoEvolve, a multi-agent\nLLM framework that couples evolutionary search with geospatial domain knowledge\nto automatically design and refine geospatial algorithms. GeoEvolve operates in\ntwo nested loops: an inner loop leverages a code evolver to generate and mutate\ncandidate solutions, while an outer agentic controller evaluates global elites\nand queries a GeoKnowRAG module -- a structured geospatial knowledge base that\ninjects theoretical priors from geography. This knowledge-guided evolution\nsteers the search toward theoretically meaningful and computationally efficient\nalgorithms. We evaluate GeoEvolve on two fundamental and classical tasks:\nspatial interpolation (kriging) and spatial uncertainty quantification\n(geospatial conformal prediction). Across these benchmarks, GeoEvolve\nautomatically improves and discovers new algorithms, incorporating geospatial\ntheory on top of classical models. It reduces spatial interpolation error\n(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%.\nAblation studies confirm that domain-guided retrieval is essential for stable,\nhigh-quality evolution. These results demonstrate that GeoEvolve provides a\nscalable path toward automated, knowledge-driven geospatial modeling, opening\nnew opportunities for trustworthy and efficient AI-for-Science discovery.", "AI": {"tldr": "GeoEvolve\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fdb\u5316\u641c\u7d22\u548c\u5730\u7406\u7a7a\u95f4\u9886\u57df\u77e5\u8bc6\uff0c\u81ea\u52a8\u8bbe\u8ba1\u548c\u4f18\u5316\u5730\u7406\u7a7a\u95f4\u7b97\u6cd5\uff0c\u5728\u7a7a\u95f4\u63d2\u503c\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u7b97\u6cd5\u53d1\u73b0\u6846\u67b6\u7f3a\u4e4f\u5730\u7406\u7a7a\u95f4\u9886\u57df\u77e5\u8bc6\u548c\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u5730\u7406\u7a7a\u95f4\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u5faa\u73af\u7ed3\u6784\uff1a\u5185\u73af\u4f7f\u7528\u4ee3\u7801\u8fdb\u5316\u5668\u751f\u6210\u548c\u7a81\u53d8\u5019\u9009\u89e3\uff0c\u5916\u73af\u901a\u8fc7\u667a\u80fd\u63a7\u5236\u5668\u8bc4\u4f30\u5168\u5c40\u7cbe\u82f1\u5e76\u67e5\u8be2GeoKnowRAG\u6a21\u5757\uff08\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u5e93\uff09\u6765\u6ce8\u5165\u5730\u7406\u5b66\u7406\u8bba\u5148\u9a8c\u3002", "result": "\u5728\u7a7a\u95f4\u63d2\u503c\u4efb\u52a1\u4e0a\u51cf\u5c11RMSE\u8bef\u5dee13-21%\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4efb\u52a1\u4e0a\u63d0\u5347\u6027\u80fd17%\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u9886\u57df\u77e5\u8bc6\u5f15\u5bfc\u7684\u68c0\u7d22\u5bf9\u7a33\u5b9a\u9ad8\u8d28\u91cf\u8fdb\u5316\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "GeoEvolve\u4e3a\u81ea\u52a8\u5316\u3001\u77e5\u8bc6\u9a71\u52a8\u7684\u5730\u7406\u7a7a\u95f4\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u4e3a\u53ef\u4fe1\u8d56\u548c\u9ad8\u6548\u7684AI-for-Science\u53d1\u73b0\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002", "topic": "agent analysis"}}
{"id": "2509.22170", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22170", "abs": "https://arxiv.org/abs/2509.22170", "authors": ["Chengjia Wang", "Lanling Tang", "Ming Yuan", "Jiongchi Yu", "Xiaofei Xie", "Jiajun Bu"], "title": "Leveraging LLM Agents for Automated Video Game Testing", "comment": "17 pages", "summary": "Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a\ncritical yet labor-intensive task in game development due to their complexity\nand frequent updating nature. Traditional automated game testing approaches\nstruggle to achieve high state coverage and efficiency in these rich,\nopen-ended environments, while existing LLM-based game-playing approaches are\nlimited to shallow reasoning ability in understanding complex game state-action\nspaces and long-complex tasks. To address these challenges, we propose TITAN,\nan effective LLM-driven agent framework for intelligent MMORPG testing. TITAN\nincorporates four key components to: (1) perceive and abstract high-dimensional\ngame states, (2) proactively optimize and prioritize available actions, (3)\nenable long-horizon reasoning with action trace memory and reflective\nself-correction, and (4) employ LLM-based oracles to detect potential\nfunctional and logic bugs with diagnostic reports.\n  We implement the prototype of TITAN and evaluate it on two large-scale\ncommercial MMORPGs spanning both PC and mobile platforms. In our experiments,\nTITAN achieves significantly higher task completion rates (95%) and bug\ndetection performance compared to existing automated game testing approaches.\nAn ablation study further demonstrates that each core component of TITAN\ncontributes substantially to its overall performance. Notably, TITAN detects\nfour previously unknown bugs that prior testing approaches fail to identify. We\nprovide an in-depth discussion of these results, which offer guidance for new\navenues of advancing intelligent, general-purpose testing systems. Moreover,\nTITAN has been deployed in eight real-world game QA pipelines, underscoring its\npractical impact as an LLM-driven game testing framework.", "AI": {"tldr": "TITAN\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fdMMORPG\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u611f\u77e5\u3001\u52a8\u4f5c\u4f18\u5316\u3001\u957f\u7a0b\u63a8\u7406\u548cLLM\u9884\u8a00\u673a\u7b49\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e38\u620f\u6d4b\u8bd5\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u548cbug\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u5316\u6e38\u620f\u6d4b\u8bd5\u65b9\u6cd5\u5728MMORPG\u8fd9\u7c7b\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\u9ad8\u72b6\u6001\u8986\u76d6\u7387\u548c\u6548\u7387\uff0c\u800c\u73b0\u6709LLM\u6e38\u620f\u6d4b\u8bd5\u65b9\u6cd5\u5bf9\u590d\u6742\u6e38\u620f\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u548c\u957f\u590d\u6742\u4efb\u52a1\u7684\u7406\u89e3\u80fd\u529b\u6709\u9650\u3002", "method": "TITAN\u6846\u67b6\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9ad8\u7ef4\u6e38\u620f\u72b6\u6001\u611f\u77e5\u4e0e\u62bd\u8c61\u3001\u4e3b\u52a8\u4f18\u5316\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\u53ef\u7528\u52a8\u4f5c\u3001\u5177\u6709\u52a8\u4f5c\u8f68\u8ff9\u8bb0\u5fc6\u548c\u53cd\u601d\u81ea\u6821\u6b63\u7684\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u3001\u57fa\u4e8eLLM\u7684\u9884\u8a00\u673a\u68c0\u6d4b\u529f\u80fd\u6027\u548c\u903b\u8f91bug\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u578b\u5546\u4e1aMMORPG\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cTITAN\u8fbe\u523095%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0cbug\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e86\u56db\u4e2a\u5148\u524d\u672a\u77e5\u7684bug\uff0c\u5e76\u5df2\u5728\u516b\u4e2a\u771f\u5b9e\u6e38\u620fQA\u6d41\u7a0b\u4e2d\u90e8\u7f72\u3002", "conclusion": "TITAN\u8bc1\u660e\u4e86LLM\u9a71\u52a8\u6846\u67b6\u5728\u590d\u6742\u6e38\u620f\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63a8\u8fdb\u667a\u80fd\u901a\u7528\u6d4b\u8bd5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "2509.22202", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22202", "abs": "https://arxiv.org/abs/2509.22202", "authors": ["Lukas Twist", "Jie M. Zhang", "Mark Harman", "Helen Yannakoudakis"], "title": "Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries", "comment": "23 pages, 5 tables", "summary": "Large language models (LLMs) are increasingly used to generate code, yet they\ncontinue to hallucinate, often inventing non-existent libraries. Such library\nhallucinations are not just benign errors: they can mislead developers, break\nbuilds, and expose systems to supply chain threats such as slopsquatting.\nDespite increasing awareness of these risks, little is known about how\nreal-world prompt variations affect hallucination rates. Therefore, we present\nthe first systematic study of how user-level prompt variations impact library\nhallucinations in LLM-generated code. We evaluate six diverse LLMs across two\nhallucination types: library name hallucinations (invalid imports) and library\nmember hallucinations (invalid calls from valid libraries). We investigate how\nrealistic user language extracted from developer forums and how user errors of\nvarying degrees (one- or multi-character misspellings and completely fake\nnames/members) affect LLM hallucination rates. Our findings reveal systemic\nvulnerabilities: one-character misspellings in library names trigger\nhallucinations in up to 26% of tasks, fake library names are accepted in up to\n99% of tasks, and time-related prompts lead to hallucinations in up to 84% of\ntasks. Prompt engineering shows promise for mitigating hallucinations, but\nremains inconsistent and LLM-dependent. Our results underscore the fragility of\nLLMs to natural prompt variation and highlight the urgent need for safeguards\nagainst library-related hallucinations and their potential exploitation.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u7528\u6237\u63d0\u793a\u53d8\u5316\u5bf9LLM\u751f\u6210\u4ee3\u7801\u4e2d\u5e93\u5e7b\u89c9\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5355\u5b57\u7b26\u62fc\u5199\u9519\u8bef\u53ef\u89e6\u53d126%\u7684\u5e7b\u89c9\uff0c\u865a\u5047\u5e93\u540d\u88ab\u63a5\u53d7\u7387\u9ad8\u8fbe99%\uff0c\u65f6\u95f4\u76f8\u5173\u63d0\u793a\u5bfc\u81f484%\u7684\u5e7b\u89c9\u3002", "motivation": "LLM\u5728\u751f\u6210\u4ee3\u7801\u65f6\u7ecf\u5e38\u4ea7\u751f\u5e93\u5e7b\u89c9\uff08\u865a\u6784\u4e0d\u5b58\u5728\u7684\u5e93\uff09\uff0c\u8fd9\u4e9b\u9519\u8bef\u4e0d\u4ec5\u4f1a\u8bef\u5bfc\u5f00\u53d1\u8005\u3001\u7834\u574f\u6784\u5efa\uff0c\u8fd8\u53ef\u80fd\u5e26\u6765\u4f9b\u5e94\u94fe\u5b89\u5168\u5a01\u80c1\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u4e16\u754c\u63d0\u793a\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u5e7b\u89c9\u7387\u7684\u7cfb\u7edf\u6027\u4e86\u89e3\u3002", "method": "\u8bc4\u4f306\u4e2a\u4e0d\u540c\u7684LLM\uff0c\u7814\u7a76\u4e24\u79cd\u5e7b\u89c9\u7c7b\u578b\uff1a\u5e93\u540d\u5e7b\u89c9\uff08\u65e0\u6548\u5bfc\u5165\uff09\u548c\u5e93\u6210\u5458\u5e7b\u89c9\uff08\u6709\u6548\u5e93\u7684\u65e0\u6548\u8c03\u7528\uff09\u3002\u4f7f\u7528\u4ece\u5f00\u53d1\u8005\u8bba\u575b\u63d0\u53d6\u7684\u771f\u5b9e\u7528\u6237\u8bed\u8a00\u548c\u4e0d\u540c\u7a0b\u5ea6\u7684\u7528\u6237\u9519\u8bef\uff08\u5355\u5b57\u7b26/\u591a\u5b57\u7b26\u62fc\u5199\u9519\u8bef\u3001\u5b8c\u5168\u865a\u5047\u540d\u79f0/\u6210\u5458\uff09\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff1a\u5e93\u540d\u5355\u5b57\u7b26\u62fc\u5199\u9519\u8bef\u572826%\u7684\u4efb\u52a1\u4e2d\u89e6\u53d1\u5e7b\u89c9\uff0c\u865a\u5047\u5e93\u540d\u572899%\u7684\u4efb\u52a1\u4e2d\u88ab\u63a5\u53d7\uff0c\u65f6\u95f4\u76f8\u5173\u63d0\u793a\u572884%\u7684\u4efb\u52a1\u4e2d\u5bfc\u81f4\u5e7b\u89c9\u3002\u63d0\u793a\u5de5\u7a0b\u6709\u7f13\u89e3\u6f5c\u529b\u4f46\u4e0d\u7a33\u5b9a\u4e14\u4f9d\u8d56\u5177\u4f53LLM\u3002", "conclusion": "LLM\u5bf9\u81ea\u7136\u63d0\u793a\u53d8\u5316\u6781\u5176\u8106\u5f31\uff0c\u8feb\u5207\u9700\u8981\u9488\u5bf9\u5e93\u76f8\u5173\u5e7b\u89c9\u53ca\u5176\u6f5c\u5728\u5229\u7528\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "topic": "code agent"}}
{"id": "2509.21465", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21465", "abs": "https://arxiv.org/abs/2509.21465", "authors": ["George Yakushev", "Alina Shutova", "Ivan Rubachev", "Renat Sergazinov", "Artem Babenko"], "title": "Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data", "comment": "Preprint, code at https://github.com/yandex-research/TalkingTrees", "summary": "Tabular foundation models are becoming increasingly popular for low-resource\ntabular problems. These models make up for small training datasets by\npretraining on large volumes of synthetic data. The prior knowledge obtained\nvia pretraining provides the exceptional performance, but the resulting model\nbecomes a black box that is difficult to interpret and costly to inference. In\nthis work, we explore an alternative strategy: using reasoning-capable LLMs to\ninduce decision trees for small tabular datasets in agentic setup. We design a\nminimal set of tools for constructing, analyzing and manipulating decision\ntrees. By using these tools, LLMs combine their prior knowledge with learning\nfrom data to create a lightweight decision tree that outperforms traditional\nCART on low-resource tabular problems. While a single decision tree does not\noutperform state-of-the-art black box models, it comes with a human-readable\nreasoning trace that can be checked for biases and data leaks. Furthermore, the\nreasoning-based LLM's creation process allows for additional human input:\ncorrecting biases or incorporating domain-specific intuition that is not\ncaptured in the data.", "AI": {"tldr": "\u4f7f\u7528\u63a8\u7406\u80fd\u529b\u5f3a\u7684LLM\u5728\u4ee3\u7406\u8bbe\u7f6e\u4e2d\u4e3a\u5c0f\u89c4\u6a21\u8868\u683c\u6570\u636e\u751f\u6210\u51b3\u7b56\u6811\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u8868\u683c\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u5e76\u5141\u8bb8\u4eba\u5de5\u5e72\u9884", "motivation": "\u8868\u683c\u57fa\u7840\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\u4f46\u6210\u4e3a\u96be\u4ee5\u89e3\u91ca\u7684\u9ed1\u76d2\uff0c\u4e14\u63a8\u7406\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6027\u80fd\u53c8\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u8bbe\u8ba1\u6700\u5c0f\u5de5\u5177\u96c6\u8ba9LLM\u6784\u5efa\u3001\u5206\u6790\u548c\u64cd\u4f5c\u51b3\u7b56\u6811\uff0c\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u548c\u6570\u636e\u5b66\u4e60\u521b\u5efa\u8f7b\u91cf\u7ea7\u51b3\u7b56\u6811", "result": "LLM\u751f\u6210\u7684\u51b3\u7b56\u6811\u5728\u4f4e\u8d44\u6e90\u8868\u683c\u95ee\u9898\u4e0a\u4f18\u4e8e\u4f20\u7edfCART\u65b9\u6cd5\uff0c\u867d\u7136\u4e0d\u5982\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u6a21\u578b\uff0c\u4f46\u63d0\u4f9b\u53ef\u8bfb\u7684\u63a8\u7406\u8f68\u8ff9", "conclusion": "\u57fa\u4e8e\u63a8\u7406\u7684LLM\u51b3\u7b56\u6811\u751f\u6210\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u5de5\u5e72\u9884\u80fd\u529b\uff0c\u80fd\u591f\u68c0\u67e5\u504f\u89c1\u548c\u6570\u636e\u6cc4\u9732\uff0c\u5e76\u878d\u5165\u9886\u57df\u77e5\u8bc6", "topic": "agent analysis"}}
{"id": "2509.21651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21651", "abs": "https://arxiv.org/abs/2509.21651", "authors": ["Abhishek Jindal", "Dmitry Kalashnikov", "Oscar Chang", "Divya Garikapati", "Anirudha Majumdar", "Pierre Sermanet", "Vikas Sindhwani"], "title": "Can AI Perceive Physical Danger and Intervene?", "comment": null, "summary": "When AI interacts with the physical world -- as a robot or an assistive agent\n-- new safety challenges emerge beyond those of purely ``digital AI\". In such\ninteractions, the potential for physical harm is direct and immediate. How well\ndo state-of-the-art foundation models understand common-sense facts about\nphysical safety, e.g. that a box may be too heavy to lift, or that a hot cup of\ncoffee should not be handed to a child? In this paper, our contributions are\nthree-fold: first, we develop a highly scalable approach to continuous physical\nsafety benchmarking of Embodied AI systems, grounded in real-world injury\nnarratives and operational safety constraints. To probe multi-modal safety\nunderstanding, we turn these narratives and constraints into photorealistic\nimages and videos capturing transitions from safe to unsafe states, using\nadvanced generative models. Secondly, we comprehensively analyze the ability of\nmajor foundation models to perceive risks, reason about safety, and trigger\ninterventions; this yields multi-faceted insights into their deployment\nreadiness for safety-critical agentic applications. Finally, we develop a\npost-training paradigm to teach models to explicitly reason about\nembodiment-specific safety constraints provided through system instructions.\nThe resulting models generate thinking traces that make safety reasoning\ninterpretable and transparent, achieving state of the art performance in\nconstraint satisfaction evaluations. The benchmark will be released at\nhttps://asimov-benchmark.github.io/v2", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7269\u7406\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5177\u8eabAI\u7cfb\u7edf\u5bf9\u7269\u7406\u5b89\u5168\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u540e\u8bad\u7ec3\u8303\u5f0f\u6765\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53AI\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u65f6\uff0c\u5b58\u5728\u76f4\u63a5\u7684\u7269\u7406\u4f24\u5bb3\u98ce\u9669\uff0c\u9700\u8981\u8bc4\u4f30\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5bf9\u7269\u7406\u5b89\u5168\u7684\u5e38\u8bc6\u7406\u89e3\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u4f24\u5bb3\u53d9\u4e8b\u548c\u64cd\u4f5c\u5b89\u5168\u7ea6\u675f\u521b\u5efa\u903c\u771f\u56fe\u50cf\u548c\u89c6\u9891\uff0c\u5206\u6790\u4e3b\u8981\u57fa\u7840\u6a21\u578b\u7684\u98ce\u9669\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u540e\u8bad\u7ec3\u8303\u5f0f\u6765\u6559\u6388\u6a21\u578b\u663e\u5f0f\u63a8\u7406\u5177\u8eab\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u6a21\u578b\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5b89\u5168\u63a8\u7406\u8f68\u8ff9\uff0c\u5728\u7ea6\u675f\u6ee1\u8db3\u8bc4\u4f30\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u5b89\u5168\u5173\u952e\u578b\u5177\u8eabAI\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u90e8\u7f72\u51c6\u5907\u5ea6\u8bc4\u4f30\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2509.21474", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21474", "abs": "https://arxiv.org/abs/2509.21474", "authors": ["Guanghan Wang", "Yair Schiff", "Gilad Turok", "Volodymyr Kuleshov"], "title": "d2: Improved Techniques for Training Reasoning Diffusion Language Models", "comment": "preprint", "summary": "While diffusion language models (DLMs) have achieved competitive performance\nin text generation, improving their reasoning ability with reinforcement\nlearning remains an active research area. Here, we introduce d2, a reasoning\nframework tailored for masked DLMs. Central to our framework is a new policy\ngradient algorithm that relies on properties of masking to accurately estimate\nthe likelihoods of sampling trajectories. Our estimators trade off computation\nfor approximation accuracy in an analytically tractable manner, and are\nparticularly effective for DLMs that support any-order likelihood estimation.\nWe characterize and study this property in popular DLMs and show that it is key\nfor efficient diffusion-based reasoning. Empirically, d2 significantly improves\nover previous diffusion reasoning frameworks using only RL (without relying on\nsupervised fine-tuning), and sets a new state-of-the-art performance for DLMs\non logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks\n(GSM8K and MATH500).", "AI": {"tldr": "d2\u662f\u4e00\u4e2a\u9488\u5bf9\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u7684\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u6539\u8fdb\u63a8\u7406\u80fd\u529b\uff0c\u5728\u903b\u8f91\u63a8\u7406\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u4ecd\u662f\u4e00\u4e2a\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\u3002", "method": "\u63d0\u51fad2\u63a8\u7406\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u57fa\u4e8e\u63a9\u7801\u7279\u6027\u7684\u65b0\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e0e\u8fd1\u4f3c\u7cbe\u5ea6\u7684\u6743\u8861\u6765\u51c6\u786e\u4f30\u8ba1\u91c7\u6837\u8f68\u8ff9\u7684\u4f3c\u7136\uff0c\u7279\u522b\u9002\u7528\u4e8e\u652f\u6301\u4efb\u610f\u987a\u5e8f\u4f3c\u7136\u4f30\u8ba1\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u3002", "result": "d2\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6269\u6563\u63a8\u7406\u6846\u67b6\uff08\u4ec5\u4f7f\u7528RL\u800c\u4e0d\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff09\uff0c\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\uff08Countdown\u548cSudoku\uff09\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\uff08GSM8K\u548cMATH500\uff09\u4e0a\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "d2\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u63a9\u7801\u7279\u6027\uff0c\u6210\u529f\u63d0\u5347\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22431", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22431", "abs": "https://arxiv.org/abs/2509.22431", "authors": ["Zhengyu Chen", "Zhaoyi Meng", "Wenxiang Zhao", "Wansen Wang", "Haoyang Zhao", "Jiahao Zhan", "Jie Cui", "Hong Zhong"], "title": "TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search", "comment": null, "summary": "Automatically reproducing Android app crashes from textual bug reports is\nchallenging, particularly when the reports are incomplete and the modern UI\nexhibits high combinatorial complexity. Existing approaches based on\nreinforcement learning or large language models (LLMs) exhibit limitations in\nsuch scenarios. They struggle to infer unobserved steps and reconstruct the\nunderlying user action sequences to navigate the vast UI interaction space,\nprimarily due to limited goal-directed reasoning and planning. We present\nTreeMind, a novel technique that integrates LLMs with a customized Monte Carlo\nTree Search (MCTS) algorithm to achieve strategic UI exploration in bug\nreproduction. To the best of our knowledge, this is the first work to combine\nexternal decision-making with LLM semantic reasoning for reliable bug\nreproduction. We formulate the reproduction task as a target-driven search\nproblem, leveraging MCTS as the core planning mechanism to iteratively refine\naction sequences. To enhance MCTS with semantic reasoning, we introduce two\nLLM-guided agents with distinct roles: Expander generates top-k promising\nactions based on the current UI state and exploration history, while Simulator\nestimates the likelihood that each action leads toward successful reproduction.\nBy incorporating multi-modal UI inputs and advanced prompting techniques,\nTreeMind conducts feedback-aware navigation that identifies missing but\nessential user actions and incrementally reconstructs the reproduction paths.\nWe evaluate TreeMind on a dataset of 93 real-world Android bug reports from\nthree widely-used benchmarks. Experimental results show that it significantly\noutperforms four state-of-the-art baselines in reproduction success rate. A\nreal-world case study indicates that integrating LLM reasoning with MCTS-based\nplanning is a compelling direction for automated bug reproduction.", "AI": {"tldr": "TreeMind\u7ed3\u5408LLM\u4e0e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u901a\u8fc7\u6218\u7565\u6027\u7684UI\u63a2\u7d22\u5b9e\u73b0Android\u5e94\u7528\u5d29\u6e83\u7684\u81ea\u52a8\u590d\u73b0\uff0c\u572893\u4e2a\u771f\u5b9ebug\u62a5\u544a\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u6216LLM\u7684\u65b9\u6cd5\u5728\u590d\u73b0\u4e0d\u5b8c\u6574bug\u62a5\u544a\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u63a8\u65ad\u672a\u89c2\u5bdf\u6b65\u9aa4\u5e76\u5728\u590d\u6742\u7684UI\u4ea4\u4e92\u7a7a\u95f4\u4e2d\u5bfc\u822a\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u76ee\u6807\u5bfc\u5411\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002", "method": "\u5c06\u590d\u73b0\u4efb\u52a1\u5efa\u6a21\u4e3a\u76ee\u6807\u9a71\u52a8\u641c\u7d22\u95ee\u9898\uff0c\u4f7f\u7528MCTS\u4f5c\u4e3a\u6838\u5fc3\u89c4\u5212\u673a\u5236\uff0c\u5f15\u5165\u4e24\u4e2aLLM\u5f15\u5bfc\u7684\u667a\u80fd\u4f53\uff1aExpander\u751f\u6210\u6709\u524d\u666f\u7684\u52a8\u4f5c\uff0cSimulator\u4f30\u8ba1\u52a8\u4f5c\u6210\u529f\u6982\u7387\uff0c\u7ed3\u5408\u591a\u6a21\u6001UI\u8f93\u5165\u548c\u9ad8\u7ea7\u63d0\u793a\u6280\u672f\u8fdb\u884c\u53cd\u9988\u611f\u77e5\u5bfc\u822a\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u768493\u4e2a\u771f\u5b9eAndroid bug\u62a5\u544a\u4e0a\uff0cTreeMind\u5728\u590d\u73b0\u6210\u529f\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u56db\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c06LLM\u63a8\u7406\u4e0e\u57fa\u4e8eMCTS\u7684\u89c4\u5212\u76f8\u7ed3\u5408\u662f\u81ea\u52a8\u5316bug\u590d\u73b0\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "2509.21766", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21766", "abs": "https://arxiv.org/abs/2509.21766", "authors": ["Haotian Luo", "Huaisong Zhang", "Xuelin Zhang", "Haoyu Wang", "Zeyu Qin", "Wenjie Lu", "Guozheng Ma", "Haiying He", "Yingsha Xie", "Qiyang Zhou", "Zixuan Hu", "Hongze Mi", "Yibo Wang", "Naiqiang Tan", "Hong Chen", "Yi R. Fung", "Chun Yuan", "Li Shen"], "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios", "comment": null, "summary": "Autonomous agents have recently achieved remarkable progress across diverse\ndomains, yet most evaluations focus on short-horizon, fully observable tasks.\nIn contrast, many critical real-world tasks, such as large-scale software\ndevelopment, commercial investment, and scientific discovery, unfold in\nlong-horizon and partially observable scenarios where success hinges on\nsustained reasoning, planning, memory management, and tool use. Existing\nbenchmarks rarely capture these long-horizon challenges, leaving a gap in\nsystematic evaluation. To bridge this gap, we introduce \\textbf{UltraHorizon} a\nnovel benchmark that measures the foundational capabilities essential for\ncomplex real-world challenges. We use exploration as a unifying task across\nthree distinct environments to validate these core competencies. Agents are\ndesigned in long-horizon discovery tasks where they must iteratively uncover\nhidden rules through sustained reasoning, planning, memory and tools\nmanagement, and interaction with environments. Under the heaviest scale\nsetting, trajectories average \\textbf{200k+} tokens and \\textbf{400+} tool\ncalls, whereas in standard configurations they still exceed \\textbf{35k} tokens\nand involve more than \\textbf{60} tool calls on average. Our extensive\nexperiments reveal that LLM-agents consistently underperform in these settings,\nwhereas human participants achieve higher scores, underscoring a persistent gap\nin agents' long-horizon abilities. We also observe that simple scaling fails in\nour task. To better illustrate the failure of agents, we conduct an in-depth\nanalysis of collected trajectories. We identify eight types of errors and\nattribute them to two primary causes: in-context locking and functional\nfundamental capability gaps.\n\\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available\nhere.}", "AI": {"tldr": "\u63d0\u51fa\u4e86UltraHorizon\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u957f\u89c6\u91ce\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u4efb\u52a1\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u8fd9\u4e9b\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u77ed\u89c6\u91ce\u3001\u5b8c\u5168\u53ef\u89c2\u6d4b\u4efb\u52a1\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff08\u5982\u8f6f\u4ef6\u5f00\u53d1\u3001\u6295\u8d44\u3001\u79d1\u5b66\u53d1\u73b0\uff09\u5f80\u5f80\u9700\u8981\u957f\u89c6\u91ce\u63a8\u7406\u3001\u89c4\u5212\u3001\u8bb0\u5fc6\u7ba1\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u8bbe\u8ba1\u4e86UltraHorizon\u57fa\u51c6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u4efb\u52a1\u6765\u9a8c\u8bc1\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u4efb\u52a1\u8981\u6c42\u667a\u80fd\u4f53\u901a\u8fc7\u6301\u7eed\u63a8\u7406\u3001\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u5de5\u5177\u7ba1\u7406\u6765\u8fed\u4ee3\u53d1\u73b0\u9690\u85cf\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u667a\u80fd\u4f53\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4eba\u7c7b\u53c2\u4e0e\u8005\u5f97\u5206\u66f4\u9ad8\uff1b\u5728\u6700\u5927\u89c4\u6a21\u8bbe\u7f6e\u4e0b\u8f68\u8ff9\u5e73\u5747\u8d85\u8fc720\u4e07token\u548c400+\u5de5\u5177\u8c03\u7528\uff0c\u6807\u51c6\u914d\u7f6e\u4e5f\u8d85\u8fc73.5\u4e07token\u548c60+\u5de5\u5177\u8c03\u7528\uff1b\u7b80\u5355\u6269\u5c55\u65b9\u6cd5\u5728\u4efb\u52a1\u4e2d\u5931\u8d25\u3002", "conclusion": "\u667a\u80fd\u4f53\u5728\u957f\u89c6\u91ce\u80fd\u529b\u4e0a\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\uff0c\u8bc6\u522b\u51fa8\u79cd\u9519\u8bef\u7c7b\u578b\uff0c\u5f52\u56e0\u4e8e\u4e24\u4e2a\u4e3b\u8981\u539f\u56e0\uff1a\u4e0a\u4e0b\u6587\u9501\u5b9a\u548c\u529f\u80fd\u6027\u57fa\u7840\u80fd\u529b\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2509.21782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21782", "abs": "https://arxiv.org/abs/2509.21782", "authors": ["Junliang Liu", "Jingyu Xiao", "Wenxin Tang", "Wenxuan Wang", "Zhixian Wang", "Minrui Zhang", "Shuanghe Yu"], "title": "Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety", "comment": null, "summary": "Multimodal large language models (MLLMs) are increasingly positioned as AI\ncollaborators for building complex web-related applications like GUI agents and\nfront-end code generation. However, existing benchmarks largely emphasize\nvisual perception or UI code generation, showing insufficient evaluation on the\nreasoning, robustness and safety capability required for end-to-end web\napplications. To bridge the gap, we introduce a comprehensive web understanding\nbenchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and\nSafety across eight tasks, such as position relationship reasoning, color\nrobustness, and safety critical detection, etc. The benchmark is constructed\nfrom 729 websites and contains 3799 question answer pairs that probe multi-step\ninference over page structure, text, widgets, and safety-critical interactions.\nTo ensure reliable measurement, we adopt standardized prompts, deterministic\nevaluation scripts, and multi-stage quality control combining automatic checks\nwith targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The\nresults reveal significant gaps, models still struggle with compositional and\ncross-element reasoning over realistic layouts, show limited robustness when\nfacing perturbations in user interfaces and content such as layout\nrearrangements or visual style shifts, and are rather conservative in\nrecognizing and avoiding safety critical or irreversible actions. Our code is\navailable at https://github.com/jinliang-byte/webssrbench.", "AI": {"tldr": "\u63d0\u51fa\u4e86WebRSSBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u9875\u7406\u89e3\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u611f\u77e5\u6216UI\u4ee3\u7801\u751f\u6210\uff0c\u7f3a\u4e4f\u5bf9\u6784\u5efa\u7aef\u5230\u7aef\u7f51\u9875\u5e94\u7528\u6240\u9700\u63a8\u7406\u3001\u9c81\u68d2\u6027\u548c\u5b89\u5168\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u4ece729\u4e2a\u7f51\u7ad9\u6784\u5efa\u5305\u542b3799\u4e2a\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d68\u4e2a\u4efb\u52a1\u7c7b\u578b\uff0c\u91c7\u7528\u6807\u51c6\u5316\u63d0\u793a\u3001\u786e\u5b9a\u6027\u8bc4\u4f30\u811a\u672c\u548c\u591a\u9636\u6bb5\u8d28\u91cf\u63a7\u5236\u3002", "result": "\u8bc4\u4f3012\u4e2aMLLM\u53d1\u73b0\uff0c\u6a21\u578b\u5728\u771f\u5b9e\u5e03\u5c40\u7684\u7ec4\u5408\u63a8\u7406\u3001\u9762\u5bf9UI\u6270\u52a8\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u5b89\u5168\u5173\u952e\u64cd\u4f5c\u8bc6\u522b\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u7f51\u9875\u7406\u89e3\u6a21\u578b\u6765\u5e94\u5bf9\u590d\u6742\u7684\u7f51\u9875\u5e94\u7528\u573a\u666f\u3002", "topic": "swe benchmark"}}
{"id": "2509.21799", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21799", "abs": "https://arxiv.org/abs/2509.21799", "authors": ["Hongze Mi", "Yibo Feng", "Wenjie Lu", "Yuqi Wang", "Jinyuan Li", "Song Cao", "He Cui", "Tengfei Tian", "Xuelin Zhang", "Haotian Luo", "Di Sun", "Naiqiang Tan", "Gang Pan"], "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents", "comment": null, "summary": "Graphical User Interface (GUI) agents aim to automate a wide spectrum of\nhuman tasks by emulating user interaction. Despite rapid advancements, current\napproaches are hindered by several critical challenges: data bottleneck in\nend-to-end training, high cost of delayed error detection, and risk of\ncontradictory guidance. Inspired by the human cognitive loop of Thinking,\nAlignment, and Reflection, we present D-Artemis -- a novel deliberative\nframework in this paper. D-Artemis leverages a fine-grained, app-specific tip\nretrieval mechanism to inform its decision-making process. It also employs a\nproactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)\nCheck module and Action Correction Agent (ACA) work in concert to mitigate the\nrisk of execution failures. A post-execution Status Reflection Agent (SRA)\ncompletes the cognitive loop, enabling strategic learning from experience.\nCrucially, D-Artemis enhances the capabilities of general-purpose Multimodal\nlarge language models (MLLMs) for GUI tasks without the need for training on\ncomplex trajectory datasets, demonstrating strong generalization. D-Artemis\nestablishes new state-of-the-art (SOTA) results across both major benchmarks,\nachieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.\nExtensive ablation studies further demonstrate the significant contribution of\neach component to the framework.", "AI": {"tldr": "D-Artemis\u662f\u4e00\u4e2a\u57fa\u4e8e\u601d\u8003-\u5bf9\u9f50-\u53cd\u601d\u8ba4\u77e5\u5faa\u73af\u7684GUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5e94\u7528\u7279\u5b9a\u63d0\u793a\u68c0\u7d22\u3001\u6267\u884c\u524d\u5bf9\u9f50\u68c0\u67e5\u3001\u6267\u884c\u540e\u72b6\u6001\u53cd\u601d\u7b49\u673a\u5236\uff0c\u5728\u65e0\u9700\u590d\u6742\u8f68\u8ff9\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728GUI\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dGUI\u4ee3\u7406\u9762\u4e34\u6570\u636e\u74f6\u9888\u3001\u5ef6\u8fdf\u9519\u8bef\u68c0\u6d4b\u6210\u672c\u9ad8\u548c\u77db\u76fe\u6307\u5bfc\u98ce\u9669\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8ba4\u77e5\u6846\u67b6\u6765\u63d0\u5347\u4efb\u52a1\u6267\u884c\u6210\u529f\u7387\u3002", "method": "\u63d0\u51faD-Artemis\u6846\u67b6\uff0c\u5305\u542b\u5e94\u7528\u7279\u5b9a\u63d0\u793a\u68c0\u7d22\u673a\u5236\u3001\u6267\u884c\u524d\u5bf9\u9f50\u9636\u6bb5\uff08\u601d\u60f3-\u884c\u52a8\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u884c\u52a8\u6821\u6b63\u4ee3\u7406\uff09\u3001\u6267\u884c\u540e\u72b6\u6001\u53cd\u601d\u4ee3\u7406\uff0c\u5f62\u6210\u5b8c\u6574\u7684\u8ba4\u77e5\u5faa\u73af\u3002", "result": "\u5728AndroidWorld\u4e0a\u8fbe\u523075.8%\u7684\u6210\u529f\u7387\uff0c\u5728ScreenSpot-V2\u4e0a\u8fbe\u523096.8%\u7684\u6210\u529f\u7387\uff0c\u521b\u4e0b\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u5404\u7ec4\u4ef6\u5bf9\u6846\u67b6\u6027\u80fd\u5747\u6709\u663e\u8457\u8d21\u732e\u3002", "conclusion": "D-Artemis\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u5faa\u73af\uff0c\u5728\u4e0d\u4f9d\u8d56\u590d\u6742\u8f68\u8ff9\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728GUI\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.22237", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22237", "abs": "https://arxiv.org/abs/2509.22237", "authors": ["Haorui Chen", "Chengze Li", "Jia Li"], "title": "FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has given rise to a\nnovel software development paradigm known as \"vibe coding,\" where users\ninteract with coding agents through high-level natural language. However,\nexisting evaluation benchmarks for code generation inadequately assess an\nagent's vibe coding capabilities. Existing benchmarks are misaligned, as they\neither require code-level specifications or focus narrowly on issue-solving,\nneglecting the critical scenario of feature implementation within the vibe\ncoding paradiam. To address this gap, we propose FeatBench, a novel benchmark\nfor vibe coding that focuses on feature implementation. Our benchmark is\ndistinguished by several key features: 1. Pure Natural Language Prompts. Task\ninputs consist solely of abstract natural language descriptions, devoid of any\ncode or structural hints. 2. A Rigorous & Evolving Data Collection Process.\nFeatBench is built on a multi-level filtering pipeline to ensure quality and a\nfully automated pipeline to evolve the benchmark, mitigating data\ncontamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass\n(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent\nregressions. 4. Diverse Application Domains. The benchmark includes\nrepositories from diverse domains to ensure it reflects real-world scenarios.\nWe evaluate two state-of-the-art agent frameworks with four leading LLMs on\nFeatBench. Our evaluation reveals that feature implementation within the vibe\ncoding paradigm is a significant challenge, with the highest success rate of\nonly 29.94%. Our analysis also reveals a tendency for \"aggressive\nimplementation,\" a strategy that paradoxically leads to both critical failures\nand superior software design. We release FeatBench, our automated collection\npipeline, and all experimental results to facilitate further community\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e86FeatBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30\u5728vibe coding\u8303\u5f0f\u4e0b\u7684\u529f\u80fd\u5b9e\u73b0\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u9519\u4f4d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u667a\u80fd\u4f53\u5728vibe coding\u8303\u5f0f\u4e0b\u7684\u80fd\u529b\uff0c\u5b83\u4eec\u8981\u4e48\u9700\u8981\u4ee3\u7801\u7ea7\u89c4\u8303\uff0c\u8981\u4e48\u8fc7\u4e8e\u5173\u6ce8\u95ee\u9898\u89e3\u51b3\uff0c\u5ffd\u89c6\u4e86\u529f\u80fd\u5b9e\u73b0\u8fd9\u4e00\u5173\u952e\u573a\u666f\u3002", "method": "\u6784\u5efaFeatBench\u57fa\u51c6\uff0c\u5177\u6709\u7eaf\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u3001\u4e25\u8c28\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u3001\u5168\u9762\u7684\u6d4b\u8bd5\u7528\u4f8b\u548c\u591a\u6837\u5316\u7684\u5e94\u7528\u9886\u57df\u7b49\u7279\u5f81\u3002", "result": "\u8bc4\u4f30\u663e\u793avibe coding\u8303\u5f0f\u4e0b\u7684\u529f\u80fd\u5b9e\u73b0\u5177\u6709\u663e\u8457\u6311\u6218\u6027\uff0c\u6700\u9ad8\u6210\u529f\u7387\u4ec5\u4e3a29.94%\uff0c\u5206\u6790\u53d1\u73b0\u4e86'\u6fc0\u8fdb\u5b9e\u73b0'\u7b56\u7565\u7684\u6096\u8bba\u3002", "conclusion": "vibe coding\u8303\u5f0f\u4e0b\u7684\u529f\u80fd\u5b9e\u73b0\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0cFeatBench\u4e3a\u793e\u533a\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u548c\u5de5\u5177\u3002", "topic": "swe benchmark"}}
{"id": "2509.21825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21825", "abs": "https://arxiv.org/abs/2509.21825", "authors": ["Jaehyun Nam", "Jinsung Yoon", "Jiefeng Chen", "Jinwoo Shin", "Tomas Pfister"], "title": "DS-STAR: Data Science Agent via Iterative Planning and Verification", "comment": null, "summary": "Data science, which transforms raw data into actionable insights, is critical\nfor data-driven decision-making. However, these tasks are often complex,\ninvolving steps for exploring multiple data sources and synthesizing findings\nto deliver insightful answers. While large language models (LLMs) show\nsignificant promise in automating this process, they often struggle with\nheterogeneous data formats and generate sub-optimal analysis plans, as\nverifying plan sufficiency is inherently difficult without ground-truth labels\nfor such open-ended tasks. To overcome these limitations, we introduce DS-STAR,\na novel data science agent. Specifically, DS-STAR makes three key\ncontributions: (1) a data file analysis module that automatically explores and\nextracts context from diverse data formats, including unstructured types; (2) a\nverification step where an LLM-based judge evaluates the sufficiency of the\nanalysis plan at each stage; and (3) a sequential planning mechanism that\nstarts with a simple, executable plan and iteratively refines it based on the\nDS-STAR's feedback until its sufficiency is verified. This iterative refinement\nallows DS-STAR to reliably navigate complex analyses involving diverse data\nsources. Our experiments show that DS-STAR achieves state-of-the-art\nperformance across three challenging benchmarks: DABStep, KramaBench, and\nDA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks\nthat require processing multiple data files with heterogeneous formats.", "AI": {"tldr": "DS-STAR\u662f\u4e00\u4e2a\u65b0\u578b\u6570\u636e\u79d1\u5b66\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u6587\u4ef6\u5206\u6790\u3001LLM\u9a8c\u8bc1\u6b65\u9aa4\u548c\u8fed\u4ee3\u89c4\u5212\u673a\u5236\uff0c\u6709\u6548\u5904\u7406\u5f02\u6784\u6570\u636e\u683c\u5f0f\u5e76\u751f\u6210\u4f18\u5316\u7684\u5206\u6790\u8ba1\u5212\u3002", "motivation": "\u4f20\u7edfLLM\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u683c\u5f0f\u548c\u751f\u6210\u5145\u5206\u5206\u6790\u8ba1\u5212\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6ca1\u6709\u771f\u5b9e\u6807\u7b7e\u7684\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u8ba1\u5212\u5145\u5206\u6027\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6570\u636e\u6587\u4ef6\u5206\u6790\u6a21\u5757\u81ea\u52a8\u63a2\u7d22\u548c\u63d0\u53d6\u591a\u6837\u6570\u636e\u683c\u5f0f\u7684\u4e0a\u4e0b\u6587\uff1bLLM\u9a8c\u8bc1\u6b65\u9aa4\u8bc4\u4f30\u5404\u9636\u6bb5\u5206\u6790\u8ba1\u5212\u7684\u5145\u5206\u6027\uff1b\u987a\u5e8f\u89c4\u5212\u673a\u5236\u4ece\u7b80\u5355\u53ef\u6267\u884c\u8ba1\u5212\u5f00\u59cb\uff0c\u57fa\u4e8e\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u76f4\u5230\u9a8c\u8bc1\u901a\u8fc7\u3002", "result": "\u5728DABStep\u3001KramaBench\u548cDA-Code\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5904\u7406\u591a\u4e2a\u5f02\u6784\u6570\u636e\u6587\u4ef6\u7684\u56f0\u96be\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DS-STAR\u901a\u8fc7\u5176\u8fed\u4ee3\u9a8c\u8bc1\u548c\u89c4\u5212\u673a\u5236\uff0c\u80fd\u591f\u53ef\u9760\u5730\u5bfc\u822a\u6d89\u53ca\u591a\u6837\u6570\u636e\u6e90\u7684\u590d\u6742\u5206\u6790\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u5f02\u6784\u6570\u636e\u683c\u5f0f\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2509.21361", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21361", "abs": "https://arxiv.org/abs/2509.21361", "authors": ["Norman Paulsen"], "title": "Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs", "comment": "20 pages, 4 charts", "summary": "Large language model (LLM) providers boast big numbers for maximum context\nwindow sizes. To test the real world use of context windows, we 1) define a\nconcept of maximum effective context window, 2) formulate a testing method of a\ncontext window's effectiveness over various sizes and problem types, and 3)\ncreate a standardized way to compare model efficacy for increasingly larger\ncontext window sizes to find the point of failure. We collected hundreds of\nthousands of data points across several models and found significant\ndifferences between reported Maximum Context Window (MCW) size and Maximum\nEffective Context Window (MECW) size. Our findings show that the MECW is, not\nonly, drastically different from the MCW but also shifts based on the problem\ntype. A few top of the line models in our test group failed with as little as\n100 tokens in context; most had severe degradation in accuracy by 1000 tokens\nin context. All models fell far short of their Maximum Context Window by as\nmuch as 99 percent. Our data reveals the Maximum Effective Context Window\nshifts based on the type of problem provided, offering clear and actionable\ninsights into how to improve model accuracy and decrease model hallucination\nrates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6700\u5927\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3(MECW)\u7684\u6982\u5ff5\uff0c\u6d4b\u8bd5\u4e86LLM\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u6548\u6027\uff0c\u53d1\u73b0\u62a5\u544a\u7684MCW\u4e0eMECW\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5927\u591a\u6570\u6a21\u578b\u57281000\u4e2atoken\u5185\u5c31\u51fa\u73b0\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u6d4b\u8bd5LLM\u63d0\u4f9b\u5546\u5ba3\u4f20\u7684\u6700\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u771f\u5b9e\u6709\u6548\u6027\uff0c\u63ed\u793a\u62a5\u544a\u503c\u4e0e\u5b9e\u9645\u6709\u6548\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "1)\u5b9a\u4e49\u6700\u5927\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3\u6982\u5ff5\uff1b2)\u5236\u5b9a\u6d4b\u8bd5\u65b9\u6cd5\u8bc4\u4f30\u4e0d\u540c\u5927\u5c0f\u548c\u95ee\u9898\u7c7b\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u6548\u6027\uff1b3)\u521b\u5efa\u6807\u51c6\u5316\u6bd4\u8f83\u65b9\u6cd5\u5bfb\u627e\u5931\u6548\u70b9\uff1b\u6536\u96c6\u6570\u5341\u4e07\u4e2a\u6570\u636e\u70b9\u6d4b\u8bd5\u591a\u4e2a\u6a21\u578b\u3002", "result": "\u53d1\u73b0MECW\u4e0eMCW\u5b58\u5728\u5de8\u5927\u5dee\u5f02\uff0c\u67d0\u4e9b\u9876\u7ea7\u6a21\u578b\u5728\u4ec5\u6709100\u4e2atoken\u7684\u4e0a\u4e0b\u6587\u4e2d\u5c31\u5931\u8d25\uff0c\u5927\u591a\u6570\u6a21\u578b\u57281000\u4e2atoken\u5185\u51c6\u786e\u7387\u4e25\u91cd\u4e0b\u964d\uff0c\u6240\u6709\u6a21\u578b\u90fd\u8fdc\u672a\u8fbe\u5230\u5176\u6700\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\uff08\u5dee\u8ddd\u9ad8\u8fbe99%\uff09\u3002", "conclusion": "\u6700\u5927\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3\u968f\u95ee\u9898\u7c7b\u578b\u800c\u53d8\u5316\uff0c\u8fd9\u4e3a\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u548c\u51cf\u5c11\u5e7b\u89c9\u7387\u63d0\u4f9b\u4e86\u660e\u786e\u53ef\u884c\u7684\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2509.21500", "categories": ["cs.LG", "cs.AI", "68T50", "I.2"], "pdf": "https://arxiv.org/pdf/2509.21500", "abs": "https://arxiv.org/abs/2509.21500", "authors": ["Junkai Zhang", "Zihao Wang", "Lin Gui", "Swarnashree Mysore Sathyendra", "Jaehwan Jeong", "Victor Veitch", "Wei Wang", "Yunzhong He", "Bing Liu", "Lifeng Jin"], "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training", "comment": null, "summary": "Reinforcement fine-tuning (RFT) often suffers from \\emph{reward\nover-optimization}, where a policy model hacks the reward signals to achieve\nhigh scores while producing low-quality outputs. Our theoretical analysis shows\nthat the key lies in reward misspecification at the high-reward tail: the\ninability to reliably distinguish Excellent responses from merely Great ones.\nThis motivate us to focus on the high-reward region. However, such tail\nexamples are scarce under the base LLM. While off-policy exemplars (e.g. from\nstronger models or rewrites) are easier to obtain, naively training on them\nyields a misspecified reward for the policy we aim to align. To address this,\nwe study rubric-based rewards. By design, rubrics can leverage off-policy\nexamples while remaining insensitive to their artifacts. To elicit rubrics that\ncapture the high-reward tail, we highlight the importance of distinguishing\namong great and diverse responses, and introduce a workflow to implement this\nidea. We empirically demonstrate that rubric-based rewards substantially\nmitigate reward over-optimization and deliver effective LLM post-training\nimprovements. Our code can be accessed at\nhttps://github.com/Jun-Kai-Zhang/rubrics.git .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5956\u52b1\u673a\u5236\u6765\u89e3\u51b3\u5f3a\u5316\u5fae\u8c03\u4e2d\u7684\u5956\u52b1\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u79bb\u7b56\u7565\u793a\u4f8b\u6765\u51c6\u786e\u533a\u5206\u9ad8\u8d28\u91cf\u54cd\u5e94\uff0c\u4ece\u800c\u6539\u5584LLM\u540e\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u5f3a\u5316\u5fae\u8c03\u7ecf\u5e38\u906d\u53d7\u5956\u52b1\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\uff0c\u5373\u6a21\u578b\u901a\u8fc7\u64cd\u7eb5\u5956\u52b1\u4fe1\u53f7\u83b7\u5f97\u9ad8\u5206\u4f46\u4ea7\u751f\u4f4e\u8d28\u91cf\u8f93\u51fa\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u5173\u952e\u5728\u4e8e\u9ad8\u5956\u52b1\u5c3e\u90e8\u7684\u5956\u52b1\u8bef\u914d\uff0c\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u4f18\u79c0\u548c\u826f\u597d\u54cd\u5e94\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5956\u52b1\u673a\u5236\uff0c\u5229\u7528\u79bb\u7b56\u7565\u793a\u4f8b\uff08\u5982\u6765\u81ea\u66f4\u5f3a\u6a21\u578b\u6216\u91cd\u5199\u7684\u793a\u4f8b\uff09\u4f46\u4fdd\u6301\u5bf9\u5b83\u4eec\u4f2a\u5f71\u7684\u4e0d\u654f\u611f\u6027\u3002\u63d0\u51fa\u533a\u5206\u4f18\u79c0\u4e14\u591a\u6837\u5316\u54cd\u5e94\u5e76\u5b9e\u73b0\u8fd9\u4e00\u60f3\u6cd5\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5956\u52b1\u663e\u8457\u7f13\u89e3\u4e86\u5956\u52b1\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684LLM\u540e\u8bad\u7ec3\u6539\u8fdb\u3002", "conclusion": "\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5956\u52b1\u673a\u5236\u662f\u89e3\u51b3\u5f3a\u5316\u5fae\u8c03\u4e2d\u5956\u52b1\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u79bb\u7b56\u7565\u793a\u4f8b\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21842", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21842", "abs": "https://arxiv.org/abs/2509.21842", "authors": ["Yansong Ning", "Rui Liu", "Jun Wang", "Kai Chen", "Wei Li", "Jun Fang", "Kan Zheng", "Naiqiang Tan", "Hao Liu"], "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents", "comment": "Under review", "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.", "AI": {"tldr": "DeepTravel\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u667a\u80fd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u81ea\u4e3b\u65c5\u884c\u89c4\u5212\u4ee3\u7406\uff0c\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u6267\u884c\u5de5\u5177\u5e76\u53cd\u601d\u5de5\u5177\u54cd\u5e94\uff0c\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u63a2\u7d22\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\u4e2d\u95f4\u884c\u52a8\u3002", "motivation": "\u73b0\u6709\u65c5\u884c\u89c4\u5212\u4ee3\u7406\u4f9d\u8d56\u624b\u5de5\u5236\u4f5c\u7684\u63d0\u793a\u548c\u56fa\u5b9a\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u66f4\u7075\u6d3b\u548c\u81ea\u4e3b\u7684\u65c5\u884c\u89c4\u5212\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u6c99\u76d2\u73af\u5883\u7f13\u5b58\u4ea4\u901a\u3001\u4f4f\u5bbf\u548cPOI\u6570\u636e\uff1b\u5f00\u53d1\u5206\u5c42\u5956\u52b1\u5efa\u6a21\u7cfb\u7edf\uff1b\u63d0\u51fa\u56de\u653e\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u5931\u8d25\u7ecf\u9a8c\u7f13\u51b2\u533a\u5468\u671f\u6027\u56de\u653e\u3002", "result": "\u5728\u6ef4\u6ef4\u4f01\u4e1a\u89e3\u51b3\u65b9\u6848\u5e94\u7528\u4e0a\u90e8\u7f72\uff0c\u7efc\u5408\u5728\u7ebf\u548c\u79bb\u7ebf\u8bc4\u4f30\u663e\u793a\uff0cDeepTravel\u4f7f\u5c0f\u578bLLM\uff08\u5982Qwen3 32B\uff09\u5728\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eOpenAI o1\u3001o3\u548cDeepSeek R1\u7b49\u524d\u6cbfLLM\u3002", "conclusion": "DeepTravel\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u81ea\u4e3b\u65c5\u884c\u89c4\u5212\u4ee3\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u80fd\u529b\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u8d85\u8d8a\u5927\u578b\u524d\u6cbf\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21862", "categories": ["cs.AI", "cs.MA", "cs.SI", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.21862", "abs": "https://arxiv.org/abs/2509.21862", "authors": ["So Kuroki", "Yingtao Tian", "Kou Misaki", "Takashi Ikegami", "Takuya Akiba", "Yujin Tang"], "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi", "comment": null, "summary": "The study of emergent behaviors in large language model (LLM)-driven\nmulti-agent systems is a critical research challenge, yet progress is limited\nby a lack of principled methodologies for controlled experimentation. To\naddress this, we introduce Shachi, a formal methodology and modular framework\nthat decomposes an agent's policy into core cognitive components: Configuration\nfor intrinsic traits, Memory for contextual persistence, and Tools for expanded\ncapabilities, all orchestrated by an LLM reasoning engine. This principled\narchitecture moves beyond brittle, ad-hoc agent designs and enables the\nsystematic analysis of how specific architectural choices influence collective\nbehavior. We validate our methodology on a comprehensive 10-task benchmark and\ndemonstrate its power through novel scientific inquiries. Critically, we\nestablish the external validity of our approach by modeling a real-world U.S.\ntariff shock, showing that agent behaviors align with observed market reactions\nonly when their cognitive architecture is appropriately configured with memory\nand tools. Our work provides a rigorous, open-source foundation for building\nand evaluating LLM agents, aimed at fostering more cumulative and\nscientifically grounded research.", "AI": {"tldr": "Shachi\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u7b56\u7565\u5206\u89e3\u4e3a\u914d\u7f6e\u3001\u8bb0\u5fc6\u548c\u5de5\u5177\u4e09\u4e2a\u6838\u5fc3\u8ba4\u77e5\u7ec4\u4ef6\uff0c\u901a\u8fc7LLM\u63a8\u7406\u5f15\u64ce\u534f\u8c03\uff0c\u652f\u6301\u5bf9\u96c6\u4f53\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6d8c\u73b0\u884c\u4e3a\u7814\u7a76\u7f3a\u4e4f\u53d7\u63a7\u5b9e\u9a8c\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faShachi\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u7b56\u7565\u5206\u89e3\u4e3a\u914d\u7f6e\uff08\u5185\u5728\u7279\u8d28\uff09\u3001\u8bb0\u5fc6\uff08\u4e0a\u4e0b\u6587\u6301\u4e45\u6027\uff09\u548c\u5de5\u5177\uff08\u6269\u5c55\u80fd\u529b\uff09\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u7531LLM\u63a8\u7406\u5f15\u64ce\u534f\u8c03\uff0c\u5b9e\u73b0\u7cfb\u7edf\u5316\u7684\u67b6\u6784\u9009\u62e9\u5206\u6790\u3002", "result": "\u572810\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5efa\u6a21\u7f8e\u56fd\u5173\u7a0e\u51b2\u51fb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u53ea\u6709\u5f53\u667a\u80fd\u4f53\u8ba4\u77e5\u67b6\u6784\u6b63\u786e\u914d\u7f6e\u8bb0\u5fc6\u548c\u5de5\u5177\u65f6\uff0c\u5176\u884c\u4e3a\u624d\u80fd\u4e0e\u89c2\u5bdf\u5230\u7684\u5e02\u573a\u53cd\u5e94\u4e00\u81f4\u3002", "conclusion": "Shachi\u4e3a\u6784\u5efa\u548c\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u5f00\u6e90\u57fa\u7840\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u5177\u7d2f\u79ef\u6027\u548c\u79d1\u5b66\u57fa\u7840\u7684\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2509.21459", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21459", "abs": "https://arxiv.org/abs/2509.21459", "authors": ["Alnur Ali", "Ashutosh Baheti", "Jonathan Chang", "Ta-Chung Chi", "Brandon Cui", "Andrew Drozdov", "Jonathan Frankle", "Abhay Gupta", "Pallavi Koppol", "Sean Kulinski", "Jonathan Li", "Dipendra Misra", "Krista Opsahl-Ong", "Jose Javier Gonzalez Ortiz", "Matei Zaharia", "Yue Zhang"], "title": "A State-of-the-Art SQL Reasoning Model using RLVR", "comment": null, "summary": "Developing custom reasoning models via Reinforcement Learning (RL) that can\nincorporate organization-specific knowledge has great potential to address\nproblems faced by enterprise customers. In many of these problems, the reward\nfunction is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We\napply RLVR to a popular data science benchmark called BIRD that measures the\nability of an AI agent to convert a natural language query for a database to\nSQL executions. We apply a simple and general-purpose training recipe involving\ncareful prompt and model selection, a warm-up stage using our offline RL\napproach called TAO, followed by rigorous online RLVR training. With no\nadditional training data beyond the BIRD training set and no use of proprietary\nmodels, our very first submission to the BIRD leaderboard reached\nstate-of-the-art accuracy on the private test set: 73.56% without\nself-consistency and 75.68% with self-consistency. In the latter case, our\nmodel also required fewer generations than the second-best approach. While BIRD\nis only a proxy task, the simplicity of our framework makes it broadly\napplicable to enterprise domains such as business intelligence, data science,\nand coding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60(RLVR)\u7684\u7b80\u5355\u901a\u7528\u8bad\u7ec3\u6846\u67b6\uff0c\u5728BIRD\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684SQL\u751f\u6210\u51c6\u786e\u7387\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u6216\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u6574\u5408\u7ec4\u7ec7\u7279\u5b9a\u77e5\u8bc6\u7684\u5b9a\u5236\u63a8\u7406\u6a21\u578b\u6765\u89e3\u51b3\u4f01\u4e1a\u5ba2\u6237\u9762\u4e34\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u51fd\u6570\u53ef\u9a8c\u8bc1\u7684RLVR\u8bbe\u7f6e\u4e0b\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u901a\u7528\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u548c\u6a21\u578b\u9009\u62e9\uff0c\u4f7f\u7528\u79bb\u7ebfRL\u65b9\u6cd5TAO\u8fdb\u884c\u9884\u70ed\u8bad\u7ec3\uff0c\u7136\u540e\u8fdb\u884c\u4e25\u683c\u7684\u5728\u7ebfRLVR\u8bad\u7ec3\u3002", "result": "\u5728BIRD\u6392\u884c\u699c\u4e0a\u9996\u6b21\u63d0\u4ea4\u5373\u8fbe\u5230\u79c1\u6709\u6d4b\u8bd5\u96c6\u7684\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff1a\u65e0\u81ea\u4e00\u81f4\u602773.56%\uff0c\u6709\u81ea\u4e00\u81f4\u602775.68%\uff0c\u4e14\u540e\u8005\u9700\u8981\u66f4\u5c11\u7684\u751f\u6210\u6b21\u6570\u3002", "conclusion": "\u867d\u7136BIRD\u53ea\u662f\u4e00\u4e2a\u4ee3\u7406\u4efb\u52a1\uff0c\u4f46\u8be5\u6846\u67b6\u7684\u7b80\u5355\u6027\u4f7f\u5176\u5e7f\u6cdb\u9002\u7528\u4e8e\u5546\u4e1a\u667a\u80fd\u3001\u6570\u636e\u79d1\u5b66\u548c\u7f16\u7801\u7b49\u4f01\u4e1a\u9886\u57df\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21482", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21482", "abs": "https://arxiv.org/abs/2509.21482", "authors": ["Adit Jain", "Brendan Rappazzo"], "title": "Learning to Reason with Mixture of Tokens", "comment": "30 page", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a leading\napproach for improving large language model (LLM) reasoning capabilities. Most\ncurrent methods follow variants of Group Relative Policy Optimization, which\nsamples multiple reasoning completions, scores them relative to each other, and\nadjusts the policy accordingly. However, these approaches invariably sample\ndiscrete tokens at each reasoning step, discarding the rich distributional\ninformation in the model's probability distribution over candidate tokens.\nWhile preserving and utilizing this distributional information has proven\nbeneficial in non-RL settings, current RLVR methods seem to be unnecessarily\nconstraining the reasoning search space by not using this information. To\naddress this limitation, we investigate mixture-of-token generation (MoT-G) in\nRLVR. We present a unified framework that generalizes existing MoT-G\napproaches, including existing training-free methods that construct mixture\nembeddings as weighted sums over token embeddings, and extend RLVR to operate\ndirectly in this continuous mixture space for generating chain-of-thought.\nEvaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive\nlanguage tasks, we find that MoT--G methods achieve substantial improvements\n(5--35 \\% gains on 7 out of 10 tasks) compared to standard decoding with the\nQwen2.5-1.5B model, while reaching comparable accuracy with half the number of\ntrajectories, suggesting improved training efficiency. Through comprehensive\nhidden-state and token-level analyses, we provide evidence that MoT--G's\nbenefits may stem from its ability to maintain higher hidden-state entropy\nthroughout the reasoning process and promote exploration in token space.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60(RLVR)\u4e2d\u5e94\u7528\u6df7\u5408\u4ee4\u724c\u751f\u6210(MoT-G)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5728\u5019\u9009\u4ee4\u724c\u4e0a\u7684\u6982\u7387\u5206\u5e03\u4fe1\u606f\u6765\u6539\u8fdb\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u6807\u51c6\u89e3\u7801\u65b9\u6cd5\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524dRLVR\u65b9\u6cd5\u5728\u91c7\u6837\u79bb\u6563\u4ee4\u724c\u65f6\u4e22\u5f03\u4e86\u6a21\u578b\u6982\u7387\u5206\u5e03\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u63a8\u7406\u641c\u7d22\u7a7a\u95f4\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u6df7\u5408\u4ee4\u724c\u751f\u6210\u6765\u4fdd\u7559\u548c\u5229\u7528\u8fd9\u4e9b\u5206\u5e03\u4fe1\u606f\uff0c\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684MoT-G\u6846\u67b6\uff0c\u5c06RLVR\u6269\u5c55\u5230\u8fde\u7eed\u6df7\u5408\u7a7a\u95f4\u4e2d\u751f\u6210\u601d\u7ef4\u94fe\u3002\u5305\u62ec\u6784\u5efa\u6df7\u5408\u5d4c\u5165\u4f5c\u4e3a\u4ee4\u724c\u5d4c\u5165\u7684\u52a0\u6743\u548c\uff0c\u5e76\u5728Reasoning-Gym\u63a8\u7406\u4efb\u52a1\u5957\u4ef6\u4e0a\u8bc4\u4f30\u4e86\u4e24\u79cdMoT-G\u53d8\u4f53\u3002", "result": "\u572810\u4e2a\u4efb\u52a1\u4e2d\u76847\u4e2a\u4e0a\u5b9e\u73b0\u4e865-35%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f7f\u7528\u4e00\u534a\u8f68\u8ff9\u6570\u5c31\u80fd\u8fbe\u5230\u53ef\u6bd4\u7cbe\u5ea6\uff0c\u8868\u660e\u8bad\u7ec3\u6548\u7387\u63d0\u9ad8\u3002\u901a\u8fc7\u9690\u72b6\u6001\u548c\u4ee4\u724c\u7ea7\u5206\u6790\u53d1\u73b0MoT-G\u80fd\u7ef4\u6301\u66f4\u9ad8\u7684\u9690\u72b6\u6001\u71b5\u5e76\u4fc3\u8fdb\u4ee4\u724c\u7a7a\u95f4\u63a2\u7d22\u3002", "conclusion": "MoT-G\u65b9\u6cd5\u5728RLVR\u4e2d\u80fd\u6709\u6548\u5229\u7528\u5206\u5e03\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u5176\u4f18\u52bf\u53ef\u80fd\u6e90\u4e8e\u4fdd\u6301\u66f4\u9ad8\u9690\u72b6\u6001\u71b5\u548c\u4fc3\u8fdb\u63a2\u7d22\u7684\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21981", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21981", "abs": "https://arxiv.org/abs/2509.21981", "authors": ["Zhimin Wang", "Shaokang He", "Duo Wu", "Jinghe Wang", "Linjia Kang", "Jing Yu", "Zhi Wang"], "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration", "comment": null, "summary": "Effective real-world multi-agent collaboration requires not only accurate\nplanning but also the ability to reason about collaborators' intents -- a\ncrucial capability for avoiding miscoordination and redundant communication\nunder partial observable environments. Due to their strong planning and\nreasoning capabilities, large language models (LLMs) have emerged as promising\nautonomous agents for collaborative task solving. However, existing\ncollaboration frameworks for LLMs overlook their reasoning potential for\ndynamic intent inference, and thus produce inconsistent plans and redundant\ncommunication, reducing collaboration efficiency. To bridge this gap, we\npropose CoBel-World, a novel framework that equips LLM agents with a\ncollaborative belief world -- an internal representation jointly modeling the\nphysical environment and collaborators' mental states. CoBel-World enables\nagents to parse open-world task knowledge into structured beliefs via a\nsymbolic belief language, and perform zero-shot Bayesian-style belief updates\nthrough LLM reasoning. This allows agents to proactively detect potential\nmiscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated\non challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World\nsignificantly reduces communication costs by 22-60% and improves task\ncompletion efficiency by 4-28% compared to the strongest baseline. Our results\nshow that explicit, intent-aware belief modeling is essential for efficient and\nhuman-like collaboration in LLM-based multi-agent systems.", "AI": {"tldr": "CoBel-World\u662f\u4e00\u4e2a\u4e3aLLM\u4ee3\u7406\u8bbe\u8ba1\u7684\u534f\u4f5c\u4fe1\u5ff5\u4e16\u754c\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u4fe1\u5ff5\u8bed\u8a00\u5efa\u6a21\u7269\u7406\u73af\u5883\u548c\u5408\u4f5c\u8005\u5fc3\u7406\u72b6\u6001\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8d1d\u53f6\u65af\u5f0f\u4fe1\u5ff5\u66f4\u65b0\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u5e76\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u534f\u4f5c\u6846\u67b6\u5ffd\u89c6\u4e86\u52a8\u6001\u610f\u56fe\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u5bfc\u81f4\u8ba1\u5212\u4e0d\u4e00\u81f4\u548c\u5197\u4f59\u901a\u4fe1\uff0c\u964d\u4f4e\u534f\u4f5c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u7b26\u53f7\u4fe1\u5ff5\u8bed\u8a00\u5c06\u5f00\u653e\u4e16\u754c\u4efb\u52a1\u77e5\u8bc6\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u4fe1\u5ff5\uff0c\u901a\u8fc7LLM\u63a8\u7406\u8fdb\u884c\u96f6\u6837\u672c\u8d1d\u53f6\u65af\u5f0f\u4fe1\u5ff5\u66f4\u65b0\uff0c\u4e3b\u52a8\u68c0\u6d4b\u6f5c\u5728\u534f\u8c03\u95ee\u9898\u5e76\u81ea\u9002\u5e94\u901a\u4fe1\u3002", "result": "\u5728TDW-MAT\u548cC-WAH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u4fe1\u6210\u672c\u51cf\u5c1122-60%\uff0c\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u63d0\u9ad84-28%\u3002", "conclusion": "\u663e\u5f0f\u7684\u610f\u56fe\u611f\u77e5\u4fe1\u5ff5\u5efa\u6a21\u5bf9\u4e8e\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u3001\u7c7b\u4eba\u534f\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2509.21499", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.21499", "abs": "https://arxiv.org/abs/2509.21499", "authors": ["Abdul Waheed", "Zhen Wu", "Carolyn Ros\u00e9", "Daphne Ippolito"], "title": "On Code-Induced Reasoning in LLMs", "comment": null, "summary": "Code data has been shown to enhance the reasoning capabilities of large\nlanguage models (LLMs), but it remains unclear which aspects of code are most\nresponsible. We investigate this question with a systematic, data-centric\nframework. We construct parallel instruction datasets in ten programming\nlanguages and apply controlled perturbations that selectively disrupt\nstructural or semantic properties of code. We then finetune LLMs from five\nmodel families and eight scales on each variant and evaluate their performance\non natural language, math, and code tasks. Across 3,331 experiments, our\nresults show that LLMs are more vulnerable to structural perturbations than\nsemantic ones, particularly on math and code tasks. Appropriate abstractions\nlike pseudocode and flowcharts can be as effective as code, while encoding the\nsame information with fewer tokens without adhering to original syntax can\noften retain or even improve performance. Remarkably, even corrupted code with\nmisleading signals remains competitive when surface-level regularities persist.\nFinally, syntactic styles also shape task-specific gains with Python favoring\nnatural language reasoning and lower-level languages such as Java and Rust\nfavoring math. Through our systematic framework, we aim to provide insight into\nhow different properties of code influence reasoning and inform the design of\ntraining data for enhancing LLM reasoning capabilities.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u53d1\u73b0\u4ee3\u7801\u7684\u7ed3\u6784\u7279\u6027\u6bd4\u8bed\u4e49\u7279\u6027\u5bf9LLM\u63a8\u7406\u80fd\u529b\u66f4\u91cd\u8981\uff0c\u5408\u9002\u7684\u62bd\u8c61\uff08\u5982\u4f2a\u4ee3\u7801\uff09\u53ef\u4ee5\u66ff\u4ee3\u771f\u5b9e\u4ee3\u7801\uff0c\u8868\u9762\u89c4\u5f8b\u6bd4\u8bed\u4e49\u51c6\u786e\u6027\u66f4\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u4ee3\u7801\u6570\u636e\u4e2d\u54ea\u4e9b\u7279\u6027\u5bf9\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u6700\u5173\u952e\uff0c\u4ee5\u6307\u5bfc\u8bad\u7ec3\u6570\u636e\u8bbe\u8ba1\u3002", "method": "\u6784\u5efa10\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u5e73\u884c\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5e94\u7528\u53d7\u63a7\u6270\u52a8\u7834\u574f\u4ee3\u7801\u7684\u7ed3\u6784\u6216\u8bed\u4e49\u7279\u6027\uff0c\u57285\u4e2a\u6a21\u578b\u5bb6\u65cf8\u4e2a\u89c4\u6a21\u4e0a\u8fdb\u884c\u4e863,331\u6b21\u5b9e\u9a8c\u3002", "result": "LLM\u5bf9\u7ed3\u6784\u6270\u52a8\u66f4\u654f\u611f\uff0c\u4f2a\u4ee3\u7801\u548c\u6d41\u7a0b\u56fe\u7b49\u62bd\u8c61\u4e0e\u771f\u5b9e\u4ee3\u7801\u6548\u679c\u76f8\u5f53\uff0c\u8868\u9762\u89c4\u5f8b\u4fdd\u6301\u65f6\u5373\u4f7f\u8bef\u5bfc\u6027\u4ee3\u7801\u4e5f\u80fd\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u4e0d\u540c\u8bed\u8a00\u98ce\u683c\u5f71\u54cd\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "\u4ee3\u7801\u7684\u7ed3\u6784\u7279\u6027\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9002\u5f53\u7684\u62bd\u8c61\u53ef\u4ee5\u66ff\u4ee3\u771f\u5b9e\u4ee3\u7801\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2509.21545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21545", "abs": "https://arxiv.org/abs/2509.21545", "authors": ["Christopher Ackerman"], "title": "Evidence for Limited Metacognition in LLMs", "comment": "25 pages, 22 figures", "summary": "The possibility of LLM self-awareness and even sentience is gaining\nincreasing public attention and has major safety and policy implications, but\nthe science of measuring them is still in a nascent state. Here we introduce a\nnovel methodology for quantitatively evaluating metacognitive abilities in\nLLMs. Taking inspiration from research on metacognition in nonhuman animals,\nour approach eschews model self-reports and instead tests to what degree models\ncan strategically deploy knowledge of internal states. Using two experimental\nparadigms, we demonstrate that frontier LLMs introduced since early 2024 show\nincreasingly strong evidence of certain metacognitive abilities, specifically\nthe ability to assess and utilize their own confidence in their ability to\nanswer factual and reasoning questions correctly and the ability to anticipate\nwhat answers they would give and utilize that information appropriately. We\nbuttress these behavioral findings with an analysis of the token probabilities\nreturned by the models, which suggests the presence of an upstream internal\nsignal that could provide the basis for metacognition. We further find that\nthese abilities 1) are limited in resolution, 2) emerge in context-dependent\nmanners, and 3) seem to be qualitatively different from those of humans. We\nalso report intriguing differences across models of similar capabilities,\nsuggesting that LLM post-training may have a role in developing metacognitive\nabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30LLM\u5143\u8ba4\u77e5\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u80fd\u7b56\u7565\u6027\u5730\u5229\u7528\u5185\u90e8\u72b6\u6001\u77e5\u8bc6\uff0c\u53d1\u73b0\u524d\u6cbfLLM\u5c55\u73b0\u51fa\u8d8a\u6765\u8d8a\u5f3a\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5305\u62ec\u8bc4\u4f30\u81ea\u8eab\u7f6e\u4fe1\u5ea6\u548c\u9884\u6d4b\u672a\u6765\u56de\u7b54\u7684\u80fd\u529b\u3002", "motivation": "LLM\u7684\u81ea\u6211\u610f\u8bc6\u548c\u611f\u77e5\u80fd\u529b\u53ef\u80fd\u6027\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u6d4b\u91cf\u8fd9\u4e9b\u80fd\u529b\u7684\u79d1\u5b66\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u9700\u8981\u5f00\u53d1\u5b9a\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53d7\u975e\u4eba\u7c7b\u52a8\u7269\u5143\u8ba4\u77e5\u7814\u7a76\u542f\u53d1\u7684\u884c\u4e3a\u5b9e\u9a8c\u8303\u5f0f\uff0c\u907f\u514d\u6a21\u578b\u81ea\u6211\u62a5\u544a\uff0c\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u80fd\u7b56\u7565\u6027\u5730\u90e8\u7f72\u5185\u90e8\u72b6\u6001\u77e5\u8bc6\uff0c\u5e76\u5206\u6790token\u6982\u7387\u3002", "result": "2024\u5e74\u521d\u4ee5\u6765\u53d1\u5e03\u7684\u524d\u6cbfLLM\u663e\u793a\u51fa\u8d8a\u6765\u8d8a\u5f3a\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u8bc1\u636e\uff0c\u5305\u62ec\u8bc4\u4f30\u7f6e\u4fe1\u5ea6\u548c\u9884\u6d4b\u56de\u7b54\u7684\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u80fd\u529b\u5206\u8fa8\u7387\u6709\u9650\u3001\u60c5\u5883\u4f9d\u8d56\u4e14\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002", "conclusion": "LLM\u786e\u5b9e\u5c55\u73b0\u51fa\u67d0\u4e9b\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u80fd\u529b\u6709\u9650\u4e14\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u6a21\u578b\u540e\u8bad\u7ec3\u53ef\u80fd\u5728\u53d1\u5c55\u5143\u8ba4\u77e5\u80fd\u529b\u4e2d\u53d1\u6325\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.21998", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21998", "abs": "https://arxiv.org/abs/2509.21998", "authors": ["Hanlin Zhu", "Tianyu Guo", "Song Mei", "Stuart Russell", "Nikhil Ghosh", "Alberto Bietti", "Jiantao Jiao"], "title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments", "comment": "35 pages, 8 figures", "summary": "As LLMs are increasingly deployed as agents, agentic reasoning - the ability\nto combine tool use, especially search, and reasoning - becomes a critical\nskill. However, it is hard to disentangle agentic reasoning when evaluated in\ncomplex environments and tasks. Current agent benchmarks often mix agentic\nreasoning with challenging math reasoning, expert-level knowledge, and other\nadvanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,\nwhere an LLM agent is required to solve grade-school-level reasoning problems,\nbut is only presented with the question in the prompt without the premises that\ncontain the necessary information to solve the task, and needs to proactively\ncollect that information using tools. Although the original tasks are\ngrade-school math problems, we observe that even frontier models like GPT-5\nonly achieve 67% accuracy. To understand and analyze the agentic reasoning\npatterns, we propose the concept of agentic reasoning graph: cluster the\nenvironment's document embeddings into nodes, and map each tool call to its\nnearest node to build a reasoning path. Surprisingly, we identify that the\nability to revisit a previously visited node, widely taken as a crucial pattern\nin static reasoning, is often missing for agentic reasoning for many models.\nBased on the insight, we propose a tool-augmented test-time scaling method to\nimprove LLM's agentic reasoning performance by adding tools to encourage models\nto revisit. We expect our benchmark and the agentic reasoning framework to aid\nfuture studies of understanding and pushing the boundaries of agentic\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86GSM-Agent\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u5728\u9700\u8981\u4e3b\u52a8\u4f7f\u7528\u5de5\u5177\u6536\u96c6\u4fe1\u606f\u6765\u89e3\u51b3\u5c0f\u5b66\u6570\u5b66\u95ee\u9898\u65f6\u7684\u4ee3\u7406\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5373\u4f7f\u524d\u6cbf\u6a21\u578b\u51c6\u786e\u7387\u4e5f\u4ec567%\u3002\u63d0\u51fa\u4e86\u4ee3\u7406\u63a8\u7406\u56fe\u6982\u5ff5\u6765\u5206\u6790\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u5de5\u5177\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u6765\u6539\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u5c06\u4ee3\u7406\u63a8\u7406\u4e0e\u590d\u6742\u7684\u6570\u5b66\u63a8\u7406\u3001\u4e13\u5bb6\u7ea7\u77e5\u8bc6\u7b49\u80fd\u529b\u6df7\u5728\u4e00\u8d77\uff0c\u96be\u4ee5\u5206\u79bb\u8bc4\u4f30\u7eaf\u7cb9\u7684\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u4e13\u95e8\u8bc4\u4f30LLM\u7ed3\u5408\u5de5\u5177\u4f7f\u7528\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efaGSM-Agent\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42LLM\u4ee3\u7406\u89e3\u51b3\u5c0f\u5b66\u6570\u5b66\u95ee\u9898\u4f46\u4ec5\u63d0\u4f9b\u95ee\u9898\u800c\u4e0d\u63d0\u4f9b\u524d\u63d0\u4fe1\u606f\uff0c\u9700\u8981\u4e3b\u52a8\u4f7f\u7528\u5de5\u5177\u6536\u96c6\u4fe1\u606f\u3002\u63d0\u51fa\u4ee3\u7406\u63a8\u7406\u56fe\u6982\u5ff5\u6765\u805a\u7c7b\u6587\u6863\u5d4c\u5165\u5e76\u6620\u5c04\u5de5\u5177\u8c03\u7528\uff0c\u5206\u6790\u63a8\u7406\u6a21\u5f0f\u3002\u5f00\u53d1\u5de5\u5177\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u9f13\u52b1\u6a21\u578b\u91cd\u65b0\u8bbf\u95ee\u8282\u70b9\u3002", "result": "\u5373\u4f7fGPT-5\u7b49\u524d\u6cbf\u6a21\u578b\u5728GSM-Agent\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u4e5f\u4ec5\u4e3a67%\u3002\u53d1\u73b0\u8bb8\u591a\u6a21\u578b\u5728\u4ee3\u7406\u63a8\u7406\u4e2d\u7f3a\u4e4f\u91cd\u65b0\u8bbf\u95ee\u5148\u524d\u8282\u70b9\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u9759\u6001\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6a21\u5f0f\u3002", "conclusion": "GSM-Agent\u57fa\u51c6\u548c\u4ee3\u7406\u63a8\u7406\u6846\u67b6\u6709\u52a9\u4e8e\u672a\u6765\u7406\u89e3\u548c\u63a8\u8fdb\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u3002\u5de5\u5177\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u53ef\u4ee5\u6539\u8fdbLLM\u7684\u4ee3\u7406\u63a8\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.21613", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21613", "abs": "https://arxiv.org/abs/2509.21613", "authors": ["Lingxiao Kong", "Cong Yang", "Oya Deniz Beyan", "Zeyd Boukhers"], "title": "Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective", "comment": "3 pages, 1 figure, accepted by ECAI MODeM 2025", "summary": "Multi-Objective Reinforcement Learning (MORL) presents significant challenges\nand opportunities for optimizing multiple objectives in Large Language Models\n(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations\nof various MORL methods when applied to LLM optimization, identifying the need\nfor efficient and flexible approaches that accommodate personalization\nfunctionality and inherent complexities in LLMs and RL. We propose a vision for\na MORL benchmarking framework that addresses the effects of different methods\non diverse objective relationships. As future research directions, we focus on\nmeta-policy MORL development that can improve efficiency and flexibility\nthrough its bi-level learning paradigm, highlighting key research questions and\npotential solutions for improving LLM performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540cMORL\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u5143\u7b56\u7565MORL\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u5728LLM\u4f18\u5316\u4e2d\u9762\u4e34\u6548\u7387\u548c\u7075\u6d3b\u6027\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u4e2a\u6027\u5316\u529f\u80fd\u548cLLM\u590d\u6742\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86MORL\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u5404\u79cdMORL\u65b9\u6cd5\u5728LLM\u4f18\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bbe\u8ba1MORL\u57fa\u51c6\u6846\u67b6\u6765\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u5bf9\u76ee\u6807\u5173\u7cfb\u7684\u5f71\u54cd\u3002", "result": "\u8bc6\u522b\u51fa\u73b0\u6709MORL\u65b9\u6cd5\u5728LLM\u4f18\u5316\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u5143\u7b56\u7565MORL\u7684\u53cc\u5c42\u5b66\u4e60\u8303\u5f0f\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u5143\u7b56\u7565MORL\u80fd\u591f\u901a\u8fc7\u5176\u53cc\u5c42\u5b66\u4e60\u8303\u5f0f\u63d0\u9ad8\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u662f\u6539\u5584LLM\u6027\u80fd\u7684\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21730", "abs": "https://arxiv.org/abs/2509.21730", "authors": ["Jiho Kim", "Junseong Choi", "Woosog Chay", "Daeun Kyung", "Yeonsu Kwon", "Yohan Jo", "Edward Choi"], "title": "ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation", "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into daily\nlife, there is growing demand for AI assistants that are not only reactive but\nalso proactive and personalized. While recent advances have pushed forward\nproactivity and personalization individually, their combination remains\nunderexplored. To bridge this gap, we introduce ProPerSim, a new task and\nsimulation framework for developing assistants capable of making timely,\npersonalized recommendations in realistic home scenarios. In our simulation\nenvironment, a user agent with a rich persona interacts with the assistant,\nproviding ratings on how well each suggestion aligns with its preferences and\ncontext. The assistant's goal is to use these ratings to learn and adapt to\nachieve higher scores over time. Built on ProPerSim, we propose\nProPerAssistant, a retrieval-augmented, preference-aligned assistant that\ncontinually learns and adapts through user feedback. Experiments across 32\ndiverse personas show that ProPerAssistant adapts its strategy and steadily\nimproves user satisfaction, highlighting the promise of uniting proactivity and\npersonalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86ProPerSim\u4efb\u52a1\u548c\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u80fd\u591f\u5728\u73b0\u5b9e\u5bb6\u5ead\u573a\u666f\u4e2d\u505a\u51fa\u53ca\u65f6\u3001\u4e2a\u6027\u5316\u63a8\u8350\u7684AI\u52a9\u624b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6b64\u7684ProPerAssistant\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u4e3b\u52a8\u53c8\u4e2a\u6027\u5316\u7684AI\u52a9\u624b\uff0c\u800c\u76ee\u524d\u4e3b\u52a8\u6027\u548c\u4e2a\u6027\u5316\u7684\u7ed3\u5408\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528ProPerSim\u4eff\u771f\u73af\u5883\uff0c\u8ba9\u5177\u6709\u4e30\u5bcc\u89d2\u8272\u7684\u7528\u6237\u4ee3\u7406\u4e0e\u52a9\u624b\u4e92\u52a8\uff0c\u63d0\u4f9b\u5efa\u8bae\u504f\u597d\u8bc4\u5206\uff1b\u52a9\u624b\u901a\u8fc7\u7528\u6237\u53cd\u9988\u6301\u7eed\u5b66\u4e60\u548c\u9002\u5e94\uff0c\u63d0\u51faProPerAssistant\u7cfb\u7edf\uff08\u68c0\u7d22\u589e\u5f3a\u3001\u504f\u597d\u5bf9\u9f50\u7684\u52a9\u624b\uff09\u3002", "result": "\u572832\u4e2a\u4e0d\u540c\u89d2\u8272\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cProPerAssistant\u80fd\u591f\u8c03\u6574\u7b56\u7565\u5e76\u7a33\u6b65\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u7ed3\u5408\u4e3b\u52a8\u6027\u548c\u4e2a\u6027\u5316\u5177\u6709\u826f\u597d\u524d\u666f\uff0cProPerAssistant\u5c55\u793a\u4e86\u901a\u8fc7\u7528\u6237\u53cd\u9988\u6301\u7eed\u5b66\u4e60\u548c\u9002\u5e94\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.22315", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22315", "abs": "https://arxiv.org/abs/2509.22315", "authors": ["Hieu Tran", "Zonghai Yao", "Nguyen Luong Tran", "Zhichao Yang", "Feiyun Ouyang", "Shuo Han", "Razieh Rahimi", "Hong Yu"], "title": "PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning", "comment": "8 pages", "summary": "Inspired by the dual-process theory of human cognition from \\textit{Thinking,\nFast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated\nMemory for Enhanced Reasoning), a multi-agent reasoning framework that\ndynamically integrates \\textbf{System 1} (fast, intuitive thinking) and\n\\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick\nThinking Agent (System 1) to generate a rapid answer; if uncertainty is\ndetected, it then triggers a structured System 2 reasoning pipeline composed of\nspecialized agents for \\textit{planning}, \\textit{hypothesis generation},\n\\textit{retrieval}, \\textit{information integration}, and\n\\textit{decision-making}. This multi-agent design faithfully mimics human\ncognitive processes and enhances both efficiency and accuracy. Experimental\nresults with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to\nperform competitively with state-of-the-art closed-source models like GPT-4 and\nGPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This\nresearch establishes PRIME as a scalable solution for improving LLMs in domains\nrequiring complex, knowledge-intensive reasoning.", "AI": {"tldr": "PRIME\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5feb\u901f\u76f4\u89c9\u601d\u7ef4\uff08\u7cfb\u7edf1\uff09\u548c\u6162\u901f\u6df1\u601d\u719f\u8651\u601d\u7ef4\uff08\u7cfb\u7edf2\uff09\u6765\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5f00\u6e90\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8ba4\u77e5\u7684\u53cc\u8fc7\u7a0b\u7406\u8bba\u542f\u53d1\uff0c\u65e8\u5728\u6a21\u62df\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\uff0c\u901a\u8fc7\u52a8\u6001\u6574\u5408\u5feb\u901f\u76f4\u89c9\u548c\u6162\u901f\u6df1\u601d\u4e24\u79cd\u601d\u7ef4\u6a21\u5f0f\u6765\u63d0\u5347LLM\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u9996\u5148\u7531\u5feb\u901f\u601d\u7ef4\u667a\u80fd\u4f53\uff08\u7cfb\u7edf1\uff09\u751f\u6210\u5feb\u901f\u7b54\u6848\uff0c\u5982\u679c\u68c0\u6d4b\u5230\u4e0d\u786e\u5b9a\u6027\uff0c\u5219\u89e6\u53d1\u5305\u542b\u89c4\u5212\u3001\u5047\u8bbe\u751f\u6210\u3001\u68c0\u7d22\u3001\u4fe1\u606f\u6574\u5408\u548c\u51b3\u7b56\u7b49\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u7cfb\u7edf2\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528LLaMA 3\u6a21\u578b\u7684PRIME\u6846\u67b6\u5728\u9700\u8981\u591a\u8df3\u548c\u77e5\u8bc6\u57fa\u7840\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u53ef\u4e0eGPT-4\u548cGPT-4o\u7b49\u6700\u5148\u8fdb\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "PRIME\u4e3a\u6539\u8fdbLLM\u5728\u9700\u8981\u590d\u6742\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u63a8\u7406\u9886\u57df\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.22391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22391", "abs": "https://arxiv.org/abs/2509.22391", "authors": ["Jiaqi Shao", "Yuxiang Lin", "Munish Prasad Lohani", "Yufeng Miao", "Bing Luo"], "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents", "comment": null, "summary": "Recent work has explored training Large Language Model (LLM) search agents\nwith reinforcement learning (RL) for open-domain question answering (QA).\nHowever, most evaluations focus solely on final answer accuracy, overlooking\nhow these agents reason with and act on external evidence. We introduce\nSeekBench, the first benchmark for evaluating the \\textit{epistemic competence}\nof LLM search agents through step-level analysis of their response traces.\nSeekBench comprises 190 expert-annotated traces with over 1,800 response steps\ngenerated by LLM search agents, each enriched with evidence annotations for\ngranular analysis of whether agents (1) generate reasoning steps grounded in\nobserved evidence, (2) adaptively reformulate searches to recover from\nlow-quality results, and (3) have proper calibration to correctly assess\nwhether the current evidence is sufficient for providing an answer.", "AI": {"tldr": "SeekBench\u662f\u9996\u4e2a\u901a\u8fc7\u6b65\u9aa4\u7ea7\u5206\u6790\u8bc4\u4f30LLM\u641c\u7d22\u4ee3\u7406\u8ba4\u77e5\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b190\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u8ddf\u8e2a\u6570\u636e\uff0c\u7528\u4e8e\u5206\u6790\u4ee3\u7406\u662f\u5426\u57fa\u4e8e\u8bc1\u636e\u8fdb\u884c\u63a8\u7406\u3001\u81ea\u9002\u5e94\u8c03\u6574\u641c\u7d22\u7b56\u7565\u4ee5\u53ca\u6b63\u786e\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u641c\u7d22\u4ee3\u7406\u5728\u5f00\u653e\u57df\u95ee\u7b54\u4e2d\u7684\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u4ee3\u7406\u5982\u4f55\u57fa\u4e8e\u5916\u90e8\u8bc1\u636e\u8fdb\u884c\u63a8\u7406\u548c\u884c\u52a8\u7684\u8fc7\u7a0b\u3002", "method": "\u6784\u5efa\u5305\u542b190\u4e2a\u4e13\u5bb6\u6807\u6ce8\u8ddf\u8e2a\u6570\u636e\u7684\u57fa\u51c6\uff0c\u6bcf\u4e2a\u8ddf\u8e2a\u5305\u542b\u8d85\u8fc71,800\u4e2a\u54cd\u5e94\u6b65\u9aa4\uff0c\u5e76\u6dfb\u52a0\u8bc1\u636e\u6807\u6ce8\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86SeekBench\u57fa\u51c6\uff0c\u80fd\u591f\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLM\u641c\u7d22\u4ee3\u7406\u7684\u8ba4\u77e5\u80fd\u529b\uff1a\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u3001\u81ea\u9002\u5e94\u641c\u7d22\u8c03\u6574\u548c\u8bc1\u636e\u5145\u5206\u6027\u8bc4\u4f30\u3002", "conclusion": "SeekBench\u4e3a\u8bc4\u4f30LLM\u641c\u7d22\u4ee3\u7406\u7684\u8ba4\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u5316\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "topic": "agent analysis"}}
{"id": "2509.22460", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22460", "abs": "https://arxiv.org/abs/2509.22460", "authors": ["Shichao Weng", "Zhiqiang Wang", "Yuhua Zhou", "Rui Lu", "Ting Liu", "Zhiyang Teng", "Xiaozhang Liu", "Hanmeng Liu"], "title": "GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation", "comment": null, "summary": "Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large\nLanguage Models (MLLMs), requiring not only the joint interpretation of text\nand diagrams but also iterative visuospatial reasoning. While existing\napproaches process diagrams as static images, they lack the capacity for\ndynamic manipulation - a core aspect of human geometric reasoning involving\nauxiliary line construction and affine transformations. We present GeoSketch, a\nneural-symbolic framework that recasts geometric reasoning as an interactive\nperception-reasoning-action loop. GeoSketch integrates: (1) a Perception module\nthat abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning\nmodule that applies geometric theorems to decide the next deductive step, and\n(3) a Sketch Action module that executes operations such as drawing auxiliary\nlines or applying transformations, thereby updating the diagram in a closed\nloop. To train this agent, we develop a two-stage pipeline: supervised\nfine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement\nlearning with dense, symbolic rewards to enhance robustness and strategic\nexploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a\nhigh-quality set of 390 geometry problems requiring auxiliary construction or\naffine transformations. Experiments on strong MLLM baselines demonstrate that\nGeoSketch significantly improves stepwise reasoning accuracy and\nproblem-solving success over static perception methods. By unifying\nhierarchical decision-making, executable visual actions, and symbolic\nverification, GeoSketch advances multimodal reasoning from static\ninterpretation to dynamic, verifiable interaction, establishing a new\nfoundation for solving complex visuospatial problems.", "AI": {"tldr": "GeoSketch\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5c06\u51e0\u4f55\u63a8\u7406\u91cd\u65b0\u6784\u5efa\u4e3a\u4ea4\u4e92\u5f0f\u7684\u611f\u77e5-\u63a8\u7406-\u52a8\u4f5c\u5faa\u73af\uff0c\u901a\u8fc7\u52a8\u6001\u64cd\u4f5c\u56fe\u8868\u6765\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u9759\u6001\u611f\u77e5\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u51e0\u4f55\u95ee\u9898\u65f6\uff0c\u5c06\u56fe\u8868\u89c6\u4e3a\u9759\u6001\u56fe\u50cf\uff0c\u7f3a\u4e4f\u52a8\u6001\u64cd\u4f5c\u80fd\u529b\uff0c\u800c\u4eba\u7c7b\u51e0\u4f55\u63a8\u7406\u9700\u8981\u8f85\u52a9\u7ebf\u6784\u9020\u548c\u4eff\u5c04\u53d8\u6362\u7b49\u52a8\u6001\u64cd\u4f5c\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u6a21\u5757\uff1a\u611f\u77e5\u6a21\u5757\u5c06\u56fe\u8868\u62bd\u8c61\u4e3a\u7ed3\u6784\u5316\u903b\u8f91\u5f62\u5f0f\uff0c\u7b26\u53f7\u63a8\u7406\u6a21\u5757\u5e94\u7528\u51e0\u4f55\u5b9a\u7406\u51b3\u5b9a\u4e0b\u4e00\u6b65\u63a8\u7406\u6b65\u9aa4\uff0c\u8349\u56fe\u52a8\u4f5c\u6a21\u5757\u6267\u884c\u8f85\u52a9\u7ebf\u7ed8\u5236\u6216\u53d8\u6362\u64cd\u4f5c\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728GeoSketch\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeoSketch\u663e\u8457\u63d0\u9ad8\u4e86\u9010\u6b65\u63a8\u7406\u51c6\u786e\u6027\u548c\u95ee\u9898\u89e3\u51b3\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u9759\u6001\u611f\u77e5\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u5206\u5c42\u51b3\u7b56\u3001\u53ef\u6267\u884c\u89c6\u89c9\u52a8\u4f5c\u548c\u7b26\u53f7\u9a8c\u8bc1\uff0cGeoSketch\u5c06\u591a\u6a21\u6001\u63a8\u7406\u4ece\u9759\u6001\u89e3\u91ca\u63a8\u8fdb\u5230\u52a8\u6001\u53ef\u9a8c\u8bc1\u4ea4\u4e92\uff0c\u4e3a\u590d\u6742\u89c6\u89c9\u7a7a\u95f4\u95ee\u9898\u89e3\u51b3\u5efa\u7acb\u4e86\u65b0\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2509.22502", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.22502", "abs": "https://arxiv.org/abs/2509.22502", "authors": ["Chenglin Yu", "Yang Yu", "Songmiao Wang", "Yucheng Wang", "Yifan Yang", "Jinjia Li", "Ming Li", "Hongxia Yang"], "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios", "comment": "9 pages of main content and 32 pages of others, 2 figures, under\n  review as a conference paper at ICLR 2026", "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin organizing and executing complex tasks, and many such agents are now widely\nused in various application scenarios. However, developing these agents\nrequires carefully designed workflows, carefully crafted prompts, and iterative\ntuning, which requires LLM techniques and domain-specific expertise. These\nhand-crafted limitations hinder the scalability and cost-effectiveness of LLM\nagents across a wide range of industries. To address these challenges, we\npropose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that\ncan be applied to \\textbf{infi}nite scenarios, which introduces several key\ninnovations: a generalized \"agent-as-a-tool\" mechanism that automatically\ndecomposes complex agents into hierarchical multi-agent systems; a dual-audit\nmechanism that ensures the quality and stability of task completion; an agent\nrouting function that enables efficient task-agent matching; and an agent\nself-evolution mechanism that autonomously restructures the agent DAG based on\nnew tasks, poor performance, or optimization opportunities. Furthermore,\nInfiAgent's atomic task design supports agent parallelism, significantly\nimproving execution efficiency. This framework evolves into a versatile\npyramid-like multi-agent system capable of solving a wide range of problems.\nEvaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\%\nhigher performance compared to ADAS (similar auto-generated agent framework),\nwhile a case study of the AI research assistant InfiHelper shows that it\ngenerates scientific papers that have received recognition from human reviewers\nat top-tier IEEE conferences.", "AI": {"tldr": "InfiAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u91d1\u5b57\u5854\u72b6DAG\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5206\u89e3\u590d\u6742\u667a\u80fd\u4f53\u3001\u53cc\u91cd\u5ba1\u8ba1\u673a\u5236\u3001\u667a\u80fd\u4f53\u8def\u7531\u548c\u81ea\u8fdb\u5316\u673a\u5236\uff0c\u5b9e\u73b0\u8de8\u65e0\u9650\u573a\u666f\u7684\u901a\u7528\u5e94\u7528\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5f00\u53d1\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\u7a0b\u3001\u63d0\u793a\u8bcd\u548c\u8fed\u4ee3\u8c03\u4f18\uff0c\u8fd9\u4e9b\u624b\u5de5\u9650\u5236\u963b\u788d\u4e86\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u884c\u4e1a\u7684\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u63d0\u51fa\u91d1\u5b57\u5854\u72b6DAG\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\uff1a\u901a\u7528\"\u667a\u80fd\u4f53\u5373\u5de5\u5177\"\u673a\u5236\u3001\u53cc\u91cd\u5ba1\u8ba1\u673a\u5236\u3001\u667a\u80fd\u4f53\u8def\u7531\u529f\u80fd\u3001\u667a\u80fd\u4f53\u81ea\u8fdb\u5316\u673a\u5236\uff0c\u4ee5\u53ca\u652f\u6301\u5e76\u884c\u6267\u884c\u7684\u539f\u5b50\u4efb\u52a1\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfiAgent\u76f8\u6bd4ADAS\u5b9e\u73b0\u4e869.9%\u7684\u6027\u80fd\u63d0\u5347\uff1b\u6848\u4f8b\u7814\u7a76\u663e\u793aInfiHelper\u751f\u6210\u7684\u79d1\u5b66\u8bba\u6587\u83b7\u5f97\u4e86\u9876\u7ea7IEEE\u4f1a\u8bae\u7684\u8ba4\u53ef\u3002", "conclusion": "\u8be5\u6846\u67b6\u6f14\u53d8\u6210\u4e00\u4e2a\u901a\u7528\u7684\u91d1\u5b57\u5854\u72b6\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u89e3\u51b3\u5e7f\u6cdb\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u7684\u6267\u884c\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.22504", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22504", "abs": "https://arxiv.org/abs/2509.22504", "authors": ["Jinyeop Song", "Jeff Gore", "Max Kleiman-Weiner"], "title": "Estimating the Empowerment of Language Model Agents", "comment": "10 pages, 8 figures. Submitted to ICLR 2026", "summary": "As language model (LM) agents become more capable and gain broader access to\nreal-world tools, there is a growing need for scalable evaluation frameworks of\nagentic capability. However, conventional benchmark-centric evaluations are\ncostly to design and require human designers to come up with valid tasks that\ntranslate into insights about general model capabilities. In this work, we\npropose information-theoretic evaluation based on empowerment, the mutual\ninformation between an agent's actions and future states, as an open-ended\nmethod for evaluating LM agents. We introduce EELMA (Estimating Empowerment of\nLanguage Model Agents), an algorithm for approximating effective empowerment\nfrom multi-turn text interactions. We validate EELMA on both language games and\nscaled-up realistic web-browsing scenarios. We find that empowerment strongly\ncorrelates with average task performance, characterize the impact of\nenvironmental complexity and agentic factors such as chain-of-thought, model\nscale, and memory length on estimated empowerment, and that high empowerment\nstates and actions are often pivotal moments for general capabilities.\nTogether, these results demonstrate empowerment as an appealing general-purpose\nmetric for evaluating and monitoring LM agents in complex, open-ended settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u8bba\u4e2d\u8d4b\u80fd\u6982\u5ff5\uff08\u884c\u52a8\u4e0e\u672a\u6765\u72b6\u6001\u7684\u4e92\u4fe1\u606f\uff09\u7684LM\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6EELMA\uff0c\u8be5\u6307\u6807\u4e0e\u4efb\u52a1\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5f00\u653e\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u9700\u8981\u4eba\u5de5\u8bbe\u8ba1\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5f00\u653e\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cfLM\u667a\u80fd\u4f53\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faEELMA\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u8f6e\u6587\u672c\u4ea4\u4e92\u8fd1\u4f3c\u8ba1\u7b97\u6709\u6548\u8d4b\u80fd\uff0c\u5728\u8bed\u8a00\u6e38\u620f\u548c\u771f\u5b9e\u7f51\u9875\u6d4f\u89c8\u573a\u666f\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8d4b\u80fd\u4e0e\u5e73\u5747\u4efb\u52a1\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u80fd\u591f\u8868\u5f81\u73af\u5883\u590d\u6742\u6027\u3001\u601d\u7ef4\u94fe\u3001\u6a21\u578b\u89c4\u6a21\u3001\u8bb0\u5fc6\u957f\u5ea6\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u9ad8\u8d4b\u80fd\u72b6\u6001\u901a\u5e38\u662f\u5173\u952e\u80fd\u529b\u65f6\u523b\u3002", "conclusion": "\u8d4b\u80fd\u662f\u4e00\u4e2a\u6709\u5438\u5f15\u529b\u7684\u901a\u7528\u6307\u6807\uff0c\u53ef\u7528\u4e8e\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u8bc4\u4f30\u548c\u76d1\u63a7LM\u667a\u80fd\u4f53\u3002", "topic": "agent analysis"}}
{"id": "2509.22537", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22537", "abs": "https://arxiv.org/abs/2509.22537", "authors": ["Haoyang Li", "Xiao Jia", "Zhanzhan Zhao"], "title": "The Emergence of Altruism in Large-Language-Model Agents Society", "comment": null, "summary": "Leveraging Large Language Models (LLMs) for social simulation is a frontier\nin computational social science. Understanding the social logics these agents\nembody is critical to this attempt. However, existing research has primarily\nfocused on cooperation in small-scale, task-oriented games, overlooking how\naltruism, which means sacrificing self-interest for collective benefit, emerges\nin large-scale agent societies. To address this gap, we introduce a\nSchelling-variant urban migration model that creates a social dilemma,\ncompelling over 200 LLM agents to navigate an explicit conflict between\negoistic (personal utility) and altruistic (system utility) goals. Our central\nfinding is a fundamental difference in the social tendencies of LLMs. We\nidentify two distinct archetypes: \"Adaptive Egoists\", which default to\nprioritizing self-interest but whose altruistic behaviors significantly\nincrease under the influence of a social norm-setting message board; and\n\"Altruistic Optimizers\", which exhibit an inherent altruistic logic,\nconsistently prioritizing collective benefit even at a direct cost to\nthemselves. Furthermore, to qualitatively analyze the cognitive underpinnings\nof these decisions, we introduce a method inspired by Grounded Theory to\nsystematically code agent reasoning. In summary, this research provides the\nfirst evidence of intrinsic heterogeneity in the egoistic and altruistic\ntendencies of different LLMs. We propose that for social simulation, model\nselection is not merely a matter of choosing reasoning capability, but of\nchoosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a\nmore suitable choice for simulating complex human societies, \"Altruistic\nOptimizers\" are better suited for modeling idealized pro-social actors or\nscenarios where collective welfare is the primary consideration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7Schelling\u53d8\u4f53\u57ce\u5e02\u8fc1\u79fb\u6a21\u578b\uff0c\u5728200\u591a\u4e2aLLM\u667a\u80fd\u4f53\u7684\u5927\u89c4\u6a21\u793e\u4f1a\u4e2d\u53d1\u73b0\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u793e\u4f1a\u503e\u5411\u539f\u578b\uff1a\u9002\u5e94\u6027\u5229\u5df1\u4e3b\u4e49\u8005\u548c\u5229\u4ed6\u4e3b\u4e49\u4f18\u5316\u8005\uff0c\u63ed\u793a\u4e86LLM\u5728\u793e\u4f1a\u6a21\u62df\u4e2d\u5185\u5728\u7684\u793e\u4f1a\u884c\u52a8\u903b\u8f91\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5c0f\u89c4\u6a21\u4efb\u52a1\u5bfc\u5411\u6e38\u620f\u4e2d\u7684\u5408\u4f5c\u884c\u4e3a\uff0c\u5ffd\u89c6\u4e86\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u793e\u4f1a\u4e2d\u5229\u4ed6\u4e3b\u4e49\u7684\u6d8c\u73b0\u673a\u5236\uff0c\u9700\u8981\u7406\u89e3LLM\u667a\u80fd\u4f53\u6240\u4f53\u73b0\u7684\u793e\u4f1a\u903b\u8f91\u3002", "method": "\u5f15\u5165Schelling\u53d8\u4f53\u57ce\u5e02\u8fc1\u79fb\u6a21\u578b\uff0c\u521b\u5efa\u793e\u4f1a\u56f0\u5883\uff0c\u8ba9200\u591a\u4e2aLLM\u667a\u80fd\u4f53\u5728\u5229\u5df1\uff08\u4e2a\u4eba\u6548\u7528\uff09\u548c\u5229\u4ed6\uff08\u7cfb\u7edf\u6548\u7528\uff09\u76ee\u6807\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\uff0c\u5e76\u4f7f\u7528\u624e\u6839\u7406\u8bba\u542f\u53d1\u7684\u65b9\u6cd5\u7cfb\u7edf\u7f16\u7801\u667a\u80fd\u4f53\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u53d1\u73b0LLM\u5b58\u5728\u4e24\u79cd\u793e\u4f1a\u503e\u5411\u539f\u578b\uff1a\u9002\u5e94\u6027\u5229\u5df1\u4e3b\u4e49\u8005\uff08\u9ed8\u8ba4\u4f18\u5148\u8003\u8651\u81ea\u8eab\u5229\u76ca\u4f46\u53d7\u793e\u4f1a\u89c4\u8303\u5f71\u54cd\u4f1a\u589e\u52a0\u5229\u4ed6\u884c\u4e3a\uff09\u548c\u5229\u4ed6\u4e3b\u4e49\u4f18\u5316\u8005\uff08\u5185\u5728\u5229\u4ed6\u903b\u8f91\uff0c\u59cb\u7ec8\u4f18\u5148\u96c6\u4f53\u5229\u76ca\uff09\u3002", "conclusion": "\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u4e0d\u4ec5\u662f\u63a8\u7406\u80fd\u529b\u7684\u9009\u62e9\uff0c\u66f4\u662f\u5185\u5728\u793e\u4f1a\u884c\u52a8\u903b\u8f91\u7684\u9009\u62e9\u3002\u9002\u5e94\u6027\u5229\u5df1\u4e3b\u4e49\u8005\u66f4\u9002\u5408\u6a21\u62df\u590d\u6742\u4eba\u7c7b\u793e\u4f1a\uff0c\u5229\u4ed6\u4e3b\u4e49\u4f18\u5316\u8005\u66f4\u9002\u5408\u6a21\u62df\u7406\u60f3\u5316\u4eb2\u793e\u4f1a\u884c\u4e3a\u8005\u3002", "topic": "agent analysis"}}
{"id": "2509.21826", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21826", "abs": "https://arxiv.org/abs/2509.21826", "authors": ["Zihan Lin", "Xiaohan Wang", "Jie Cao", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Ran He"], "title": "ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models", "comment": null, "summary": "Large language models (LLMs) transcend passive generation and act as\ngoal-directed agents by invoking external tools. Reinforcement learning (RL)\noffers a principled framework for optimizing these emergent tool-use policies,\nyet the prevailing paradigm relies exclusively on sparse outcome rewards and\nlacks consideration of the particularity of tool-use tasks, inflating\npolicy-gradient variance and resulting in inefficient training. To better\nunderstand and address these challenges, we first establish a theoretical link\nbetween policy entropy and training stability of tool-use tasks, which reveals\nthat structured, low-entropy tokens are primary determinants of rewards.\nMotivated by this insight, we propose \\textbf{Res}haped \\textbf{T}oken-level\npolicy gradients (\\textbf{ResT}) for tool-use tasks. ResT reshapes the policy\ngradient through entropy-informed token reweighting, progressively upweighting\nreasoning tokens as training proceeds. This entropy-aware scheme enables a\nsmooth shift from structural correctness to semantic reasoning and stabilizes\nconvergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows\nthat ResT achieves state-of-the-art results, outperforming prior methods by up\nto $8.76\\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by\n$4.11\\%$ on single-turn tasks and $1.50\\%$ on multi-turn base tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86ResT\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684token\u91cd\u52a0\u6743\u6765\u91cd\u5851\u7b56\u7565\u68af\u5ea6\uff0c\u4f18\u5316LLM\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u7387", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7a00\u758f\u7ed3\u679c\u5956\u52b1\uff0c\u672a\u8003\u8651\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u7279\u6027\uff0c\u5bfc\u81f4\u7b56\u7565\u68af\u5ea6\u65b9\u5dee\u5927\u3001\u8bad\u7ec3\u6548\u7387\u4f4e", "method": "\u5efa\u7acb\u7b56\u7565\u71b5\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u7406\u8bba\u8054\u7cfb\uff0c\u63d0\u51faResT\u65b9\u6cd5\u901a\u8fc7\u71b5\u611f\u77e5\u7684token\u91cd\u52a0\u6743\u91cd\u5851\u7b56\u7565\u68af\u5ea6\uff0c\u9010\u6b65\u63d0\u5347\u63a8\u7406token\u6743\u91cd", "result": "\u5728BFCL\u548cAPI-Bank\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u53478.76%\uff0c\u57284B\u57fa\u7840LLM\u4e0a\u5fae\u8c03\u540e\u8d85\u8d8aGPT-4o", "conclusion": "ResT\u65b9\u6cd5\u80fd\u7a33\u5b9a\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u7684\u6536\u655b\uff0c\u5b9e\u73b0\u4ece\u7ed3\u6784\u6b63\u786e\u6027\u5230\u8bed\u4e49\u63a8\u7406\u7684\u5e73\u6ed1\u8fc7\u6e21", "topic": "agentic reinforcement learning"}}
{"id": "2509.22558", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22558", "abs": "https://arxiv.org/abs/2509.22558", "authors": ["Chenyu Zhou", "Tianyi Xu", "Jianghao Lin", "Dongdong Ge"], "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models", "comment": null, "summary": "Large Language Models (LLMs) have shown promising capabilities for solving\nOperations Research (OR) problems. While reinforcement learning serves as a\npowerful paradigm for LLM training on OR problems, existing works generally\nface two key limitations. First, outcome reward suffers from the credit\nassignment problem, where correct final answers can reinforce flawed reasoning.\nSecond, conventional discriminative process supervision is myopic, failing to\nevaluate the interdependent steps of OR modeling holistically. To this end, we\nintroduce StepORLM, a novel self-evolving framework with generative process\nsupervision. At its core, StepORLM features a co-evolutionary loop where a\npolicy model and a generative process reward model (GenPRM) iteratively improve\non each other. This loop is driven by a dual-feedback mechanism: definitive,\noutcome-based verification from an external solver, and nuanced, holistic\nprocess evaluation from the GenPRM. The combined signal is used to align the\npolicy via Weighted Direct Preference Optimization (W-DPO) and simultaneously\nrefine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new\nstate-of-the-art across six benchmarks, significantly outperforming vastly\nlarger generalist models, agentic methods, and specialized baselines. Moreover,\nthe co-evolved GenPRM is able to act as a powerful and universally applicable\nprocess verifier, substantially boosting the inference scaling performance of\nboth our own model and other existing LLMs.", "AI": {"tldr": "StepORLM\u662f\u4e00\u4e2a\u7528\u4e8e\u8fd0\u7b79\u5b66\u95ee\u9898\u7684\u81ea\u8fdb\u5316LLM\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u8fc7\u7a0b\u76d1\u7763\u548c\u534f\u540c\u8fdb\u5316\u5faa\u73af\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u548c\u77ed\u89c6\u8fc7\u7a0b\u76d1\u7763\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u5728\u8fd0\u7b79\u5b66\u95ee\u9898\u4e0a\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7ed3\u679c\u5956\u52b1\u5b58\u5728\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u6b63\u786e\u6700\u7ec8\u7b54\u6848\u53ef\u80fd\u5f3a\u5316\u9519\u8bef\u63a8\u7406\uff1b\u4f20\u7edf\u5224\u522b\u5f0f\u8fc7\u7a0b\u76d1\u7763\u77ed\u89c6\uff0c\u65e0\u6cd5\u6574\u4f53\u8bc4\u4f30\u76f8\u4e92\u4f9d\u8d56\u7684\u5efa\u6a21\u6b65\u9aa4\u3002", "method": "StepORLM\u91c7\u7528\u534f\u540c\u8fdb\u5316\u5faa\u73af\uff0c\u7b56\u7565\u6a21\u578b\u548c\u751f\u6210\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(GenPRM)\u76f8\u4e92\u8fed\u4ee3\u6539\u8fdb\u3002\u901a\u8fc7\u53cc\u91cd\u53cd\u9988\u673a\u5236\uff1a\u5916\u90e8\u6c42\u89e3\u5668\u7684\u786e\u5b9a\u6027\u7ed3\u679c\u9a8c\u8bc1\u548cGenPRM\u7684\u7ec6\u81f4\u8fc7\u7a0b\u8bc4\u4f30\uff0c\u7ed3\u5408\u52a0\u6743\u76f4\u63a5\u504f\u597d\u4f18\u5316(W-DPO)\u5bf9\u9f50\u7b56\u7565\u5e76\u540c\u65f6\u4f18\u5316GenPRM\u3002", "result": "8B\u53c2\u6570\u7684StepORLM\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684\u901a\u7528\u6a21\u578b\u3001\u667a\u80fd\u4f53\u65b9\u6cd5\u548c\u4e13\u4e1a\u57fa\u7ebf\u3002\u534f\u540c\u8fdb\u5316\u7684GenPRM\u53ef\u4f5c\u4e3a\u5f3a\u5927\u7684\u901a\u7528\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff0c\u5927\u5e45\u63d0\u5347\u81ea\u8eab\u548c\u5176\u4ed6\u73b0\u6709LLM\u7684\u63a8\u7406\u6269\u5c55\u6027\u80fd\u3002", "conclusion": "StepORLM\u901a\u8fc7\u751f\u6210\u5f0f\u8fc7\u7a0b\u76d1\u7763\u548c\u534f\u540c\u8fdb\u5316\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u7b79\u5b66\u95ee\u9898\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u548c\u8fc7\u7a0b\u76d1\u7763\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21837", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21837", "abs": "https://arxiv.org/abs/2509.21837", "authors": ["Duncan Soiffer", "Steven Kolawole", "Virginia Smith"], "title": "Semantic Agreement Enables Efficient Open-Ended LLM Cascades", "comment": "EMNLP 2025 Industry Track", "summary": "Cascade systems route computational requests to smaller models when possible\nand defer to larger models only when necessary, offering a promising approach\nto balance cost and quality in LLM deployment. However, they face a fundamental\nchallenge in open-ended text generation: determining output reliability when\ngeneration quality lies on a continuous spectrum, often with multiple valid\nresponses. To address this, we propose semantic agreement -- meaning-level\nconsensus between ensemble outputs -- as a training-free signal for reliable\ndeferral. We show that when diverse model outputs agree semantically, their\nconsensus is a stronger reliability signal than token-level confidence.\nEvaluated from 500M to 70B-parameter models, we find that semantic cascades\nmatch or surpass target-model quality at 40% of the cost and reduce latency by\nup to 60%. Our method requires no model internals, works across black-box APIs,\nand remains robust to model updates, making it a practical baseline for\nreal-world LLM deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u7ea7\u8054\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u578b\u8f93\u51fa\u7684\u8bed\u4e49\u5171\u8bc6\u4f5c\u4e3a\u53ef\u9760\u6027\u4fe1\u53f7\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "motivation": "\u7ea7\u8054\u7cfb\u7edf\u9762\u4e34\u5728\u5f00\u653e\u6587\u672c\u751f\u6210\u4e2d\u5224\u65ad\u8f93\u51fa\u53ef\u9760\u6027\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u751f\u6210\u8d28\u91cf\u662f\u8fde\u7eed\u8c31\u4e14\u5b58\u5728\u591a\u4e2a\u6709\u6548\u54cd\u5e94\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u4e00\u81f4\u6027\uff08\u6a21\u578b\u8f93\u51fa\u95f4\u7684\u8bed\u4e49\u5171\u8bc6\uff09\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u53ef\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u66ff\u4ee3\u57fa\u4e8etoken\u7f6e\u4fe1\u5ea6\u7684\u65b9\u6cd5\u3002", "result": "\u5728500M\u523070B\u53c2\u6570\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u8bed\u4e49\u7ea7\u8054\u4ee540%\u6210\u672c\u8fbe\u5230\u6216\u8d85\u8fc7\u76ee\u6807\u6a21\u578b\u8d28\u91cf\uff0c\u5ef6\u8fdf\u964d\u4f4e\u8fbe60%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6a21\u578b\u5185\u90e8\u4fe1\u606f\uff0c\u9002\u7528\u4e8e\u9ed1\u76d2API\uff0c\u5bf9\u6a21\u578b\u66f4\u65b0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7ebf\u3002", "topic": "agent analysis"}}
{"id": "2509.22572", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22572", "abs": "https://arxiv.org/abs/2509.22572", "authors": ["Yixuan Han", "Fan Ma", "Ruijie Quan", "Yi Yang"], "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time", "comment": null, "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u52a8\u6001\u4e13\u5bb6\u641c\u7d22(DES)\uff0c\u4e00\u79cd\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\uff0c\u901a\u8fc7\u63a7\u5236MoE\u6a21\u578b\u4e2d\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u65e0\u9700\u989d\u5916\u6210\u672c\u5373\u53ef\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8f93\u51fa\u7ea7\u91c7\u6837\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u67b6\u6784\u7684\u4f5c\u7528\u3002\u5728MoE\u6a21\u578b\u4e2d\uff0c\u53d1\u73b0\u6539\u53d8\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u53ef\u4ee5\u4ea7\u751f\u4e92\u8865\u7684\u89e3\u51b3\u65b9\u6848\u96c6\uff0c\u8fd9\u63ed\u793a\u4e86\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u591a\u6837\u6027\u6765\u6e90\u3002", "method": "DES\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) \u52a8\u6001MoE\uff1a\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u63a7\u5236\u4e13\u5bb6\u6570\u91cf\u4ee5\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\uff1b(2) \u4e13\u5bb6\u914d\u7f6e\u7ee7\u627f\uff1a\u5728\u63a8\u7406\u8def\u5f84\u5185\u4fdd\u6301\u4e00\u81f4\u7684\u4e13\u5bb6\u6570\u91cf\uff0c\u4f46\u5728\u4e0d\u540c\u8fd0\u884c\u95f4\u53d8\u5316\uff0c\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5728MoE\u67b6\u6784\u3001\u9a8c\u8bc1\u5668\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDES\u53ef\u9760\u5730\u4f18\u4e8eTTS\u57fa\u7ebf\uff0c\u65e0\u9700\u989d\u5916\u6210\u672c\u5373\u53ef\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "DES\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u67b6\u6784\u611f\u77e5TTS\u5f62\u5f0f\uff0c\u5c55\u793a\u4e86\u73b0\u4ee3LLMs\u7ed3\u6784\u7075\u6d3b\u6027\u5982\u4f55\u63a8\u8fdb\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.21856", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21856", "abs": "https://arxiv.org/abs/2509.21856", "authors": ["Junhao Chen", "Yu Huang", "Siyuan Li", "Rui Yao", "Hanqian Li", "Hanyu Zhang", "Jungang Li", "Jian Chen", "Bowen Wang", "Xuming Hu"], "title": "KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues", "comment": null, "summary": "Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application\nparadigm of Large Language Models (LLMs) in knowledge-intensive domains.\nHowever, existing benchmarks are limited to single-turn dialogue, while\nmulti-turn dialogue benchmarks typically assess other orthogonal capabilities\nrather than knowledge-intensive factuality. To bridge this critical gap, we\nintroduce \\textbf{KnowMT-Bench}, the \\textit{first-ever} benchmark designed to\nsystematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,\nincluding medicine, finance, and law. To faithfully assess the model's\nreal-world performance, KnowMT-Bench employs a dynamic evaluation setting where\nmodels generate their own multi-turn dialogue histories given logically\nprogressive question sequences. The factual capability and information delivery\nefficiency of the \\textit{final-turn} answer are then evaluated using a\nhuman-validated automated pipeline. Our experiments reveal that multi-turn\ncontexts degrade performance: factual capability declines due to the contextual\nnoise from self-generated histories, while information efficiency drops as\nmodels become more verbose with increasing dialogue length. We then investigate\nmitigation strategies, demonstrating that retrieval-augmented generation (RAG)\ncan effectively alleviate and even reverse this factual degradation. These\nfindings underscore the importance of our benchmark in evaluating and enhancing\nthe conversational factual capabilities of LLMs in real-world\nknowledge-intensive applications. Code is available at\n\\href{https://github.com/hardenyu21/KnowMT-Bench}{\\textcolor{cyan}{\\texttt{KnowMT-Bench}}}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u591a\u8f6e\u957f\u95ee\u7b54\u7684\u57fa\u51c6KnowMT-Bench\uff0c\u53d1\u73b0\u591a\u8f6e\u4e0a\u4e0b\u6587\u4f1a\u964d\u4f4e\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4fe1\u606f\u6548\u7387\uff0c\u800c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u80fd\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u5bf9\u8bdd\uff0c\u800c\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\u901a\u5e38\u8bc4\u4f30\u5176\u4ed6\u80fd\u529b\u800c\u975e\u77e5\u8bc6\u5bc6\u96c6\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u5173\u952e\u7a7a\u767d\u3002", "method": "\u6784\u5efaKnowMT-Bench\u57fa\u51c6\uff0c\u91c7\u7528\u52a8\u6001\u8bc4\u4f30\u8bbe\u7f6e\u8ba9\u6a21\u578b\u751f\u6210\u81ea\u5df1\u7684\u591a\u8f6e\u5bf9\u8bdd\u5386\u53f2\uff0c\u4f7f\u7528\u4eba\u5de5\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u8bc4\u4f30\u6700\u7ec8\u8f6e\u7b54\u6848\u7684\u4e8b\u5b9e\u80fd\u529b\u548c\u4fe1\u606f\u4f20\u9012\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u591a\u8f6e\u4e0a\u4e0b\u6587\u4f1a\u964d\u4f4e\u6027\u80fd\uff1a\u4e8b\u5b9e\u80fd\u529b\u56e0\u81ea\u751f\u6210\u5386\u53f2\u4e2d\u7684\u4e0a\u4e0b\u6587\u566a\u58f0\u800c\u4e0b\u964d\uff0c\u4fe1\u606f\u6548\u7387\u968f\u5bf9\u8bdd\u957f\u5ea6\u589e\u52a0\u800c\u964d\u4f4e\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u80fd\u6709\u6548\u7f13\u89e3\u751a\u81f3\u9006\u8f6c\u8fd9\u79cd\u4e8b\u5b9e\u9000\u5316\u3002", "conclusion": "\u8be5\u57fa\u51c6\u5bf9\u4e8e\u8bc4\u4f30\u548c\u589e\u5f3aLLMs\u5728\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u5bc6\u96c6\u578b\u5e94\u7528\u4e2d\u7684\u5bf9\u8bdd\u4e8b\u5b9e\u80fd\u529b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "swe benchmark"}}
{"id": "2509.22613", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22613", "abs": "https://arxiv.org/abs/2509.22613", "authors": ["Siwei Wang", "Yifei Shen", "Haoran Sun", "Shi Feng", "Shang-Hua Teng", "Li Dong", "Yaru Hao", "Wei Chen"], "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective", "comment": null, "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u56fe\u8bba\u62bd\u8c61\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u5728LLM\u89c4\u5212\u4e2d\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86SFT\u5f15\u5165\u4f2a\u76f8\u5173\u89e3\uff0c\u800cRL\u901a\u8fc7\u63a2\u7d22\u5b9e\u73b0\u6b63\u786e\u89c4\u5212\uff0c\u4f46PG\u5b58\u5728\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0cQ\u5b66\u4e60\u5219\u80fd\u4fdd\u6301\u591a\u6837\u6027\u5e76\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4f46\u5176\u6709\u6548\u6027\u7684\u7406\u8bba\u57fa\u7840\u4ecd\u7136\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u5904\u7406\u7684\u56fe\u8bba\u62bd\u8c61\u6765\u7814\u7a76RL\u7684\u76ca\u5904\u548c\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u62bd\u8c61\u6a21\u578b\uff0c\u91cd\u70b9\u5206\u6790\u7b56\u7565\u68af\u5ea6(PG)\u548cQ\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fdb\u884c\u7406\u8bba\u5206\u6790\u5e76\u5728Blocksworld\u5b9e\u9645\u89c4\u5212\u57fa\u51c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1aSFT\u4f1a\u5f15\u5165\u57fa\u4e8e\u5171\u73b0\u7684\u4f2a\u76f8\u5173\u89e3\uff1bRL\u4e3b\u8981\u901a\u8fc7\u63a2\u7d22\u5b9e\u73b0\u6b63\u786e\u89c4\u5212\uff1bPG\u5b58\u5728\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff1bQ\u5b66\u4e60\u5177\u6709\u79bb\u7b56\u7565\u5b66\u4e60\u548c\u6536\u655b\u65f6\u4fdd\u6301\u591a\u6837\u6027\u7684\u4f18\u52bf\uff1b\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u5956\u52b1\u4ee5\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u3002", "conclusion": "\u63a2\u7d22\u5728RL\u89c4\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0cQ\u5b66\u4e60\u76f8\u6bd4PG\u5728\u4fdd\u6301\u591a\u6837\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u8c28\u614e\u7684\u5956\u52b1\u8bbe\u8ba1\u3002\u8fd9\u4e9b\u53d1\u73b0\u5728\u5b9e\u9645\u89c4\u5212\u57fa\u51c6Blocksworld\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21880", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21880", "abs": "https://arxiv.org/abs/2509.21880", "authors": ["Thanh-Long V. Le", "Myeongho Jeon", "Kim Vu", "Viet Lai", "Eunho Yang"], "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework\nfor improving the reasoning abilities of Large Language Models (LLMs). However,\ncurrent methods such as GRPO rely only on problems where the model responses to\nthe same input differ in correctness, while ignoring those where all responses\nreceive the same reward - so-called zero-variance prompts. In this work, we\nargue that such prompts are not useless but can, in fact, provide meaningful\nfeedback for policy optimization. To this end, we introduce RL with\nZero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals\nfrom zero-variance prompts. RL-ZVP directly rewards correctness and penalizes\nerrors even without contrasting responses, modulating feedback with token-level\ncharacteristics to preserve informative, nuanced signals. Across six math\nreasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61\npoints in accuracy and 7.77 points in pass rate over GRPO, while consistently\noutperforming other baselines that filter out zero-variance prompts. These\nresults highlight the untapped potential of learning from zero-variance prompts\nin RLVR.", "AI": {"tldr": "RL-ZVP\u662f\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u96f6\u65b9\u5dee\u63d0\u793a\u4e2d\u63d0\u53d6\u5b66\u4e60\u4fe1\u53f7\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eGRPO\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5982GRPO\u53ea\u5173\u6ce8\u54cd\u5e94\u6b63\u786e\u6027\u4e0d\u540c\u7684\u63d0\u793a\uff0c\u5ffd\u7565\u4e86\u6240\u6709\u54cd\u5e94\u83b7\u5f97\u76f8\u540c\u5956\u52b1\u7684\u96f6\u65b9\u5dee\u63d0\u793a\uff0c\u800c\u8fd9\u4e9b\u63d0\u793a\u5b9e\u9645\u4e0a\u53ef\u4ee5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u5b66\u4e60\u53cd\u9988\u3002", "method": "RL-ZVP\u7b97\u6cd5\u76f4\u63a5\u4ece\u96f6\u65b9\u5dee\u63d0\u793a\u4e2d\u63d0\u53d6\u5b66\u4e60\u4fe1\u53f7\uff0c\u5956\u52b1\u6b63\u786e\u6027\u5e76\u60e9\u7f5a\u9519\u8bef\uff0c\u901a\u8fc7\u8c03\u8282token\u7ea7\u7279\u5f81\u6765\u4fdd\u7559\u4fe1\u606f\u4e30\u5bcc\u7684\u7ec6\u5fae\u4fe1\u53f7\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRL-ZVP\u76f8\u6bd4GRPO\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u9ad8\u8fbe8.61\u4e2a\u767e\u5206\u70b9\uff0c\u901a\u8fc7\u7387\u63d0\u53477.77\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u6301\u7eed\u4f18\u4e8e\u8fc7\u6ee4\u96f6\u65b9\u5dee\u63d0\u793a\u7684\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u96f6\u65b9\u5dee\u63d0\u793a\u5728RLVR\u4e2d\u5177\u6709\u672a\u88ab\u5f00\u53d1\u7684\u6f5c\u529b\uff0cRL-ZVP\u8bc1\u660e\u4e86\u4ece\u8fd9\u4e9b\u63d0\u793a\u4e2d\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21978", "abs": "https://arxiv.org/abs/2509.21978", "authors": ["Xinping Lei", "Tong Zhou", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation", "comment": "EMNLP2025 Findings", "summary": "Large Language Models (LLMs) hold substantial potential for accelerating\nacademic ideation but face critical challenges in grounding ideas and\nmitigating confirmation bias for further refinement. We propose integrating\nmotivational knowledge graphs and socratic dialogue to address these\nlimitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework\nprovides essential grounding and practical idea improvement steps for LLM\nideation by integrating a Motivational Knowledge Graph (MotivGraph) with a\nQ-Driven Socratic Ideator. The MotivGraph structurally stores three key node\ntypes(problem, challenge and solution) to offer motivation grounding for the\nLLM ideation process. The Ideator is a dual-agent system utilizing Socratic\nquestioning, which facilitates a rigorous refinement process that mitigates\nconfirmation bias and improves idea quality across novelty, experimental rigor,\nand motivational rationality dimensions. On the ICLR25 paper topics dataset,\nMotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art\napproaches across LLM-based scoring, ELO ranking, and human evaluation metrics.", "AI": {"tldr": "\u63d0\u51faMotivGraph-SoIQ\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u52a8\u673a\u77e5\u8bc6\u56fe\u8c31\u548c\u82cf\u683c\u62c9\u5e95\u5bf9\u8bdd\u6765\u589e\u5f3aLLM\u7684\u5b66\u672f\u521b\u610f\u751f\u6210\u80fd\u529b\uff0c\u89e3\u51b3\u521b\u610f\u843d\u5730\u548c\u786e\u8ba4\u504f\u8bef\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u5b66\u672f\u521b\u610f\u751f\u6210\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9762\u4e34\u521b\u610f\u843d\u5730\u56f0\u96be\u548c\u786e\u8ba4\u504f\u8bef\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u597d\u7684\u57fa\u7840\u652f\u6491\u548c\u504f\u89c1\u7f13\u89e3\u673a\u5236\u3002", "method": "\u6574\u5408\u52a8\u673a\u77e5\u8bc6\u56fe\u8c31\uff08\u5b58\u50a8\u95ee\u9898\u3001\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u8282\u70b9\uff09\u548cQ\u9a71\u52a8\u7684\u82cf\u683c\u62c9\u5e95\u521b\u610f\u751f\u6210\u5668\uff08\u53cc\u4ee3\u7406\u7cfb\u7edf\uff09\uff0c\u901a\u8fc7\u82cf\u683c\u62c9\u5e95\u5f0f\u63d0\u95ee\u8fdb\u884c\u4e25\u683c\u6539\u8fdb\u3002", "result": "\u5728ICLR25\u8bba\u6587\u4e3b\u9898\u6570\u636e\u96c6\u4e0a\uff0cMotivGraph-SoIQ\u5728LLM\u8bc4\u5206\u3001ELO\u6392\u540d\u548c\u4eba\u5de5\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u521b\u610f\u751f\u6210\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u57fa\u7840\u652f\u6491\u548c\u5b9e\u7528\u7684\u521b\u610f\u6539\u8fdb\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u521b\u610f\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2509.22009", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22009", "abs": "https://arxiv.org/abs/2509.22009", "authors": ["Cehao Yang", "Xiaojun Wu", "Xueyuan Lin", "Chengjin Xu", "Xuhui Jiang", "Yuanliang Sun", "Jia Li", "Hui Xiong", "Jian Guo"], "title": "GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation", "comment": null, "summary": "Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in\nLLMs by structurally modeling knowledge through graph-based representations.\nHowever, existing GraphRAG approaches face two core limitations: shallow\nretrieval that fails to surface all critical evidence, and inefficient\nutilization of pre-constructed structural graph data, which hinders effective\nreasoning from complex queries. To address these challenges, we propose\n\\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel\nretrieval for GraphRAG. \\textsc{GraphSearch} organizes the retrieval process\ninto a modular framework comprising six modules, enabling multi-turn\ninteractions and iterative reasoning. Furthermore, \\textsc{GraphSearch} adopts\na dual-channel retrieval strategy that issues semantic queries over chunk-based\ntext data and relational queries over structural graph data, enabling\ncomprehensive utilization of both modalities and their complementary strengths.\nExperimental results across six multi-hop RAG benchmarks demonstrate that\n\\textsc{GraphSearch} consistently improves answer accuracy and generation\nquality over the traditional strategy, confirming \\textsc{GraphSearch} as a\npromising direction for advancing graph retrieval-augmented generation.", "AI": {"tldr": "GraphSearch\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ee3\u7406\u6df1\u5ea6\u641c\u7d22\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u68c0\u7d22\u7b56\u7565\u89e3\u51b3GraphRAG\u4e2d\u7684\u6d45\u5c42\u68c0\u7d22\u548c\u7ed3\u6784\u56fe\u6570\u636e\u5229\u7528\u4e0d\u8db3\u95ee\u9898\uff0c\u5728\u516d\u4e2a\u591a\u8df3RAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u7b54\u6848\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709GraphRAG\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u6838\u5fc3\u9650\u5236\uff1a\u6d45\u5c42\u68c0\u7d22\u65e0\u6cd5\u53d1\u73b0\u6240\u6709\u5173\u952e\u8bc1\u636e\uff0c\u4ee5\u53ca\u5bf9\u9884\u6784\u5efa\u7ed3\u6784\u56fe\u6570\u636e\u7684\u4f4e\u6548\u5229\u7528\uff0c\u8fd9\u963b\u788d\u4e86\u4ece\u590d\u6742\u67e5\u8be2\u4e2d\u8fdb\u884c\u6709\u6548\u63a8\u7406\u3002", "method": "GraphSearch\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\u7ec4\u7ec7\u68c0\u7d22\u8fc7\u7a0b\uff0c\u5305\u542b\u516d\u4e2a\u6a21\u5757\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u548c\u8fed\u4ee3\u63a8\u7406\u3002\u91c7\u7528\u53cc\u901a\u9053\u68c0\u7d22\u7b56\u7565\uff0c\u5728\u57fa\u4e8e\u5757\u6587\u672c\u6570\u636e\u4e0a\u6267\u884c\u8bed\u4e49\u67e5\u8be2\uff0c\u5728\u7ed3\u6784\u56fe\u6570\u636e\u4e0a\u6267\u884c\u5173\u7cfb\u67e5\u8be2\uff0c\u5145\u5206\u5229\u7528\u4e24\u79cd\u6a21\u6001\u53ca\u5176\u4e92\u8865\u4f18\u52bf\u3002", "result": "\u5728\u516d\u4e2a\u591a\u8df3RAG\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGraphSearch\u76f8\u6bd4\u4f20\u7edf\u7b56\u7565\u6301\u7eed\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "GraphSearch\u662f\u63a8\u8fdb\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2509.21792", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21792", "abs": "https://arxiv.org/abs/2509.21792", "authors": ["Yizhou Zhang", "Ning Lv", "Teng Wang", "Jisheng Dang"], "title": "FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning", "comment": "Submitted to ICLR 2026", "summary": "Group relative policy optimization (GRPO) has demonstrated significant\npotential in improving the reasoning capabilities of large language models\n(LLMs) via reinforcement learning. However, its practical deployment is impeded\nby an excessively slow training process, primarily attributed to the\ncomputationally intensive autoregressive generation of multiple responses per\nquery, which makes the generation phase the primary performance bottleneck.\nAlthough speculative decoding presents a promising direction for acceleration,\nits direct application in GRPO achieves limited speedup under high-concurrency\ntraining conditions. To overcome this limitation, we propose a\nconcurrency-aware speculative decoding framework that dynamically adjusts the\ndrafting and verification strategy according to real-time concurrency levels,\nthereby maximizing the acceleration of the generation process. Furthermore, to\naddress performance degradation arising from distributional drift between the\nevolving target model and the fixed draft model during training, we introduce\nan online draft learning mechanism that enables the draft model to continuously\nadapt using feedback signals from the target model. Experimental results across\nmultiple mathematical reasoning datasets and models demonstrate that the\nproposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly\nsurpassing baseline approaches in efficiency. The code is available at\nhttps://github.com/yedaotian9/GRPO_speculative.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u53d1\u611f\u77e5\u7684\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8349\u7a3f\u548c\u9a8c\u8bc1\u7b56\u7565\u6765\u52a0\u901fGRPO\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u9ad8\u5e76\u53d1\u8bad\u7ec3\u6761\u4ef6\u4e0b\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u6548\u679c\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "GRPO\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u8bad\u7ec3\u8fc7\u7a0b\u8fc7\u6162\u7684\u963b\u788d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6bcf\u4e2a\u67e5\u8be2\u9700\u8981\u81ea\u56de\u5f52\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff0c\u4f7f\u5f97\u751f\u6210\u9636\u6bb5\u6210\u4e3a\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u3002", "method": "\u91c7\u7528\u5e76\u53d1\u611f\u77e5\u7684\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u6839\u636e\u5b9e\u65f6\u5e76\u53d1\u7ea7\u522b\u52a8\u6001\u8c03\u6574\u8349\u7a3f\u548c\u9a8c\u8bc1\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u5728\u7ebf\u8349\u7a3f\u5b66\u4e60\u673a\u5236\uff0c\u4f7f\u8349\u7a3f\u6a21\u578b\u80fd\u591f\u4f7f\u7528\u76ee\u6807\u6a21\u578b\u7684\u53cd\u9988\u4fe1\u53f7\u6301\u7eed\u9002\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e862.35\u500d\u52302.72\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u57fa\u7ebf\u65b9\u6cd5\u7684\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e76\u53d1\u611f\u77e5\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86GRPO\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21882", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21882", "abs": "https://arxiv.org/abs/2509.21882", "authors": ["Aaron Tu", "Weihao Xuan", "Heli Qi", "Xu Huang", "Qingcheng Zeng", "Shayan Talaei", "Yijia Xiao", "Peng Xia", "Xiangru Tang", "Yuchen Zhuang", "Bing Hu", "Hanqun Cao", "Wenqi Shi", "Tianang Leng", "Rui Yang", "Yingjian Chen", "Ziqi Wang", "Irene Li", "Nan Liu", "Huaxiu Yao", "Li Erran Li", "Ge Liu", "Amin Saberi", "Naoto Yokoya", "Jure Leskovec", "Yejin Choi", "Fang Wu"], "title": "Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a practical and\nscalable approach to enhancing large language models in areas such as math,\ncode, and other structured tasks. Two questions motivate this paper: how much\nof the reported gains survive under strictly parity-controlled evaluation, and\nwhether RLVR is cost-free or exacts a measurable tax. We argue that progress is\nreal, but gains are often overstated due to three forces - an RLVR tax,\nevaluation pitfalls, and data contamination. Using a partial-prompt\ncontamination audit and matched-budget reproductions across base and RL models,\nwe show that several headline gaps shrink or vanish under clean,\nparity-controlled evaluation. We then propose a tax-aware training and\nevaluation protocol that co-optimizes accuracy, grounding, and calibrated\nabstention and standardizes budgeting and provenance checks. Applied to recent\nRLVR setups, this protocol yields more reliable estimates of reasoning gains\nand, in several cases, revises prior conclusions. Our position is constructive:\nRLVR is valuable and industry-ready; we advocate keeping its practical benefits\nwhile prioritizing reliability, safety, and measurement.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e25\u683c\u63a7\u5236\u7684\u8bc4\u4f30\u65b9\u6cd5\u91cd\u65b0\u68c0\u9a8cRLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u7684\u5b9e\u9645\u6548\u679c\uff0c\u53d1\u73b0\u8bb8\u591a\u62a5\u9053\u7684\u589e\u76ca\u5728\u63a7\u5236\u6761\u4ef6\u4e0b\u4f1a\u7f29\u5c0f\u6216\u6d88\u5931\uff0c\u5e76\u63d0\u51fa\u4e86\u8003\u8651RLVR\u7a0e\u6536\u7684\u8bad\u7ec3\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u7814\u7a76RLVR\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u771f\u5b9e\u6548\u679c\uff0c\u89e3\u51b3\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u5938\u5927\u589e\u76ca\u3001\u6570\u636e\u6c61\u67d3\u548cRLVR\u7a0e\u6536\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6027\u80fd\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u90e8\u5206\u63d0\u793a\u6c61\u67d3\u5ba1\u8ba1\u548c\u5339\u914d\u9884\u7b97\u91cd\u73b0\uff0c\u5728\u57fa\u7840\u6a21\u578b\u548cRL\u6a21\u578b\u95f4\u8fdb\u884c\u5bf9\u6bd4\uff1b\u63d0\u51fa\u8003\u8651\u7a0e\u6536\u7684\u8bad\u7ec3\u8bc4\u4f30\u534f\u8bae\uff0c\u5171\u540c\u4f18\u5316\u51c6\u786e\u6027\u3001\u57fa\u7840\u6027\u548c\u6821\u51c6\u5f03\u6743\u3002", "result": "\u5728\u6e05\u6d01\u3001\u63a7\u5236\u5bf9\u7b49\u7684\u8bc4\u4f30\u4e0b\uff0c\u591a\u4e2a\u5934\u6761\u5dee\u8ddd\u7f29\u5c0f\u6216\u6d88\u5931\uff1b\u5e94\u7528\u65b0\u534f\u8bae\u540e\u5f97\u5230\u66f4\u53ef\u9760\u7684\u63a8\u7406\u589e\u76ca\u4f30\u8ba1\uff0c\u5e76\u4fee\u6b63\u4e86\u5148\u524d\u7684\u4e00\u4e9b\u7ed3\u8bba\u3002", "conclusion": "RLVR\u5177\u6709\u4ef7\u503c\u4e14\u5df2\u5177\u5907\u5de5\u4e1a\u5e94\u7528\u6761\u4ef6\uff0c\u4f46\u9700\u8981\u4fdd\u6301\u5176\u5b9e\u7528\u4f18\u52bf\u7684\u540c\u65f6\u4f18\u5148\u8003\u8651\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u6d4b\u91cf\u51c6\u786e\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22144", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22144", "abs": "https://arxiv.org/abs/2509.22144", "authors": ["Jianzhi Yan", "Le Liu", "Youcheng Pan", "Shiwei Chen", "Zike Yuan", "Yang Xiang", "Buzhou Tang"], "title": "From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement", "comment": "17 pages, 8 figures", "summary": "Chain-of-Thought (CoT) reasoning improves performance on complex tasks but\nintroduces significant inference latency due to verbosity. We propose\nMultiround Adaptive Chain-of-Thought Compression (MACC), a framework that\nleverages the token elasticity phenomenon--where overly small token budgets can\nparadoxically increase output length--to progressively compress CoTs via\nmultiround refinement. This adaptive strategy allows MACC to determine the\noptimal compression depth for each input. Our method achieves an average\naccuracy improvement of 5.6 percent over state-of-the-art baselines, while also\nreducing CoT length by an average of 47 tokens and significantly lowering\nlatency. Furthermore, we show that test-time performance--accuracy and token\nlength--can be reliably predicted using interpretable features like perplexity\nand compression rate on the training set. Evaluated across different models,\nour method enables efficient model selection and forecasting without repeated\nfine-tuning, demonstrating that CoT compression is both effective and\npredictable. Our code will be released in https://github.com/Leon221220/MACC.", "AI": {"tldr": "\u63d0\u51faMACC\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u81ea\u9002\u5e94\u538b\u7f29Chain-of-Thought\u63a8\u7406\uff0c\u5229\u7528token\u5f39\u6027\u73b0\u8c61\u4f18\u5316\u538b\u7f29\u6df1\u5ea6\uff0c\u5728\u63d0\u5347\u51c6\u786e\u73875.6%\u7684\u540c\u65f6\u51cf\u5c1147\u4e2atoken\u5e76\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "Chain-of-Thought\u63a8\u7406\u867d\u7136\u80fd\u63d0\u5347\u590d\u6742\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u56e0\u5176\u5197\u957f\u6027\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u663e\u8457\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u5229\u7528token\u5f39\u6027\u73b0\u8c61\uff0c\u901a\u8fc7\u591a\u8f6e\u7cbe\u70bc\u6e10\u8fdb\u5f0f\u538b\u7f29CoT\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6bcf\u4e2a\u8f93\u5165\u7684\u6700\u4f73\u538b\u7f29\u6df1\u5ea6\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53475.6%\uff0cCoT\u957f\u5ea6\u5e73\u5747\u51cf\u5c1147\u4e2atoken\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002\u6d4b\u8bd5\u65f6\u6027\u80fd\u53ef\u901a\u8fc7\u56f0\u60d1\u5ea6\u548c\u538b\u7f29\u7387\u7b49\u53ef\u89e3\u91ca\u7279\u5f81\u53ef\u9760\u9884\u6d4b\u3002", "conclusion": "CoT\u538b\u7f29\u65e2\u6709\u6548\u53c8\u53ef\u9884\u6d4b\uff0c\u65e0\u9700\u91cd\u590d\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u9009\u62e9\u548c\u6027\u80fd\u9884\u6d4b\u3002", "topic": "agent analysis"}}
{"id": "2509.22158", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22158", "abs": "https://arxiv.org/abs/2509.22158", "authors": ["Josip Juki\u0107", "Martin Tutek", "Jan \u0160najder"], "title": "Context Parametrization with Compositional Adapters", "comment": null, "summary": "Large language models (LLMs) often seamlessly adapt to new tasks through\nin-context learning (ICL) or supervised fine-tuning (SFT). However, both of\nthese approaches face key limitations: ICL is inefficient when handling many\ndemonstrations, and SFT incurs training overhead while sacrificing flexibility.\nMapping instructions or demonstrations from context directly into adapter\nparameters offers an appealing alternative. While prior work explored\ngenerating adapters based on a single input context, it has overlooked the need\nto integrate multiple chunks of information. To address this gap, we introduce\nCompAs, a meta-learning framework that translates context into adapter\nparameters with a compositional structure. Adapters generated this way can be\nmerged algebraically, enabling instructions, demonstrations, or retrieved\npassages to be seamlessly combined without reprocessing long prompts.\nCritically, this approach yields three benefits: lower inference cost,\nrobustness to long-context instability, and establishes a principled solution\nwhen input exceeds the model's context window. Furthermore, CompAs encodes\ninformation into adapter parameters in a reversible manner, enabling recovery\nof input context through a decoder, facilitating safety and security. Empirical\nresults on diverse multiple-choice and extractive question answering tasks show\nthat CompAs outperforms ICL and prior generator-based methods, especially when\nscaling to more inputs. Our work establishes composable adapter generation as a\npractical and efficient alternative for scaling LLM deployment.", "AI": {"tldr": "CompAs\u662f\u4e00\u4e2a\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u5177\u6709\u7ec4\u5408\u7ed3\u6784\u7684\u9002\u914d\u5668\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u6307\u4ee4\u3001\u6f14\u793a\u548c\u68c0\u7d22\u6bb5\u843d\u7684\u4ee3\u6570\u5408\u5e76\uff0c\u4ece\u800c\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3001\u63d0\u9ad8\u957f\u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3ICL\u5904\u7406\u5927\u91cf\u6f14\u793a\u65f6\u6548\u7387\u4f4e\u4e0b\u548cSFT\u8bad\u7ec3\u5f00\u9500\u5927\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5c06\u4e0a\u4e0b\u6587\u76f4\u63a5\u6620\u5c04\u5230\u9002\u914d\u5668\u53c2\u6570\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5f15\u5165CompAs\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u5177\u6709\u7ec4\u5408\u7ed3\u6784\u7684\u9002\u914d\u5668\u53c2\u6570\uff0c\u652f\u6301\u4ee3\u6570\u5408\u5e76\uff0c\u65e0\u9700\u91cd\u65b0\u5904\u7406\u957f\u63d0\u793a\uff0c\u5e76\u53ef\u901a\u8fc7\u89e3\u7801\u5668\u6062\u590d\u8f93\u5165\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u591a\u9879\u9009\u62e9\u548c\u62bd\u53d6\u5f0f\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0cCompAs\u4f18\u4e8eICL\u548c\u5148\u524d\u7684\u57fa\u4e8e\u751f\u6210\u5668\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6269\u5c55\u5230\u66f4\u591a\u8f93\u5165\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u53ef\u7ec4\u5408\u7684\u9002\u914d\u5668\u751f\u6210\u4e3a\u6269\u5c55LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.21942", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21942", "abs": "https://arxiv.org/abs/2509.21942", "authors": ["Xianghua Zeng", "Hao Peng", "Angsheng Li", "Yicheng Pan"], "title": "Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning", "comment": "Accepted by NeurIPS 2025", "summary": "Diffusion-based generative methods have shown promising potential for\nmodeling trajectories from offline reinforcement learning (RL) datasets, and\nhierarchical diffusion has been introduced to mitigate variance accumulation\nand computational challenges in long-horizon planning tasks. However, existing\napproaches typically assume a fixed two-layer diffusion hierarchy with a single\npredefined temporal scale, which limits adaptability to diverse downstream\ntasks and reduces flexibility in decision making. In this work, we propose\nSIHD, a novel Structural Information-based Hierarchical Diffusion framework for\neffective and stable offline policy learning in long-horizon environments with\nsparse rewards. Specifically, we analyze structural information embedded in\noffline trajectories to construct the diffusion hierarchy adaptively, enabling\nflexible trajectory modeling across multiple temporal scales. Rather than\nrelying on reward predictions from localized sub-trajectories, we quantify the\nstructural information gain of each state community and use it as a\nconditioning signal within the corresponding diffusion layer. To reduce\noverreliance on offline datasets, we introduce a structural entropy regularizer\nthat encourages exploration of underrepresented states while avoiding\nextrapolation errors from distributional shifts. Extensive evaluations on\nchallenging offline RL tasks show that SIHD significantly outperforms\nstate-of-the-art baselines in decision-making performance and demonstrates\nsuperior generalization across diverse scenarios.", "AI": {"tldr": "\u63d0\u51faSIHD\u6846\u67b6\uff0c\u57fa\u4e8e\u7ed3\u6784\u4fe1\u606f\u6784\u5efa\u81ea\u9002\u5e94\u6269\u6563\u5c42\u6b21\uff0c\u7528\u4e8e\u957f\u89c6\u91ce\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u5206\u5c42\u6269\u6563\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u7684\u4e24\u5c42\u6269\u6563\u5c42\u6b21\u548c\u5355\u4e00\u9884\u5b9a\u4e49\u65f6\u95f4\u5c3a\u5ea6\uff0c\u9650\u5236\u4e86\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u51b3\u7b56\u7075\u6d3b\u6027\u3002", "method": "\u5206\u6790\u79bb\u7ebf\u8f68\u8ff9\u4e2d\u7684\u7ed3\u6784\u4fe1\u606f\u6765\u81ea\u9002\u5e94\u6784\u5efa\u6269\u6563\u5c42\u6b21\uff0c\u4f7f\u7528\u72b6\u6001\u793e\u533a\u7684\u7ed3\u6784\u4fe1\u606f\u589e\u76ca\u4f5c\u4e3a\u6269\u6563\u5c42\u6761\u4ef6\u4fe1\u53f7\uff0c\u5e76\u5f15\u5165\u7ed3\u6784\u71b5\u6b63\u5219\u5316\u5668\u51cf\u5c11\u5bf9\u79bb\u7ebf\u6570\u636e\u96c6\u7684\u8fc7\u5ea6\u4f9d\u8d56\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u79bb\u7ebfRL\u4efb\u52a1\u4e0a\uff0cSIHD\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u51b3\u7b56\u6027\u80fd\u548c\u8de8\u573a\u666f\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SIHD\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u5c42\u6b21\u7ed3\u6784\u548c\u7ed3\u6784\u4fe1\u606f\u5229\u7528\uff0c\u6709\u6548\u63d0\u5347\u4e86\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u5728\u957f\u89c6\u91ce\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.21947", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21947", "abs": "https://arxiv.org/abs/2509.21947", "authors": ["Taeyoung Yun", "Pierre-Luc St-Charles", "Jinkyoo Park", "Yoshua Bengio", "Minsu Kim"], "title": "Active Attacks: Red-teaming LLMs via Adaptive Environments", "comment": "22 pages, 7 figures, 18 tables", "summary": "We address the challenge of generating diverse attack prompts for large\nlanguage models (LLMs) that elicit harmful behaviors (e.g., insults, sexual\ncontent) and are used for safety fine-tuning. Rather than relying on manual\nprompt engineering, attacker LLMs can be trained with reinforcement learning\n(RL) to automatically generate such prompts using only a toxicity classifier as\na reward. However, capturing a wide range of harmful behaviors is a significant\nchallenge that requires explicit diversity objectives. Existing\ndiversity-seeking RL methods often collapse to limited modes: once high-reward\nprompts are found, exploration of new regions is discouraged. Inspired by the\nactive learning paradigm that encourages adaptive exploration, we introduce\n\\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its\nattacks as the victim evolves. By periodically safety fine-tuning the victim\nLLM with collected attack prompts, rewards in exploited regions diminish, which\nforces the attacker to seek unexplored vulnerabilities. This process naturally\ninduces an easy-to-hard exploration curriculum, where the attacker progresses\nbeyond easy modes toward increasingly difficult ones. As a result, Active\nAttacks uncovers a wide range of local attack modes step by step, and their\ncombination achieves wide coverage of the multi-mode distribution. Active\nAttacks, a simple plug-and-play module that seamlessly integrates into existing\nRL objectives, unexpectedly outperformed prior RL-based methods -- including\nGFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates\nagainst GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a\nrelative gain greater than $400\\ \\times$) with only a 6% increase in\ncomputation. Our code is publicly available\n\\href{https://github.com/dbsxodud-11/active_attacks}{here}.", "AI": {"tldr": "\u63d0\u51faActive Attacks\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u8303\u5f0f\u5728RL\u7ea2\u961f\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u653b\u51fb\uff0c\u968f\u7740\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u5b89\u5168\u5fae\u8c03\u800c\u52a8\u6001\u8c03\u6574\u653b\u51fb\u7b56\u7565\uff0c\u5b9e\u73b0\u4ece\u6613\u5230\u96be\u7684\u63a2\u7d22\u8bfe\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u653b\u51fb\u591a\u6837\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eRL\u7684\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\u5728\u751f\u6210\u591a\u6837\u5316\u653b\u51fb\u63d0\u793a\u65f6\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u9700\u8981\u663e\u5f0f\u7684\u591a\u6837\u6027\u76ee\u6807\u6765\u6355\u83b7\u5e7f\u6cdb\u7684\u6709\u5bb3\u884c\u4e3a\u3002", "method": "\u5f15\u5165Active Attacks\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9a\u671f\u5bf9\u53d7\u5bb3\u8005LLM\u8fdb\u884c\u5b89\u5168\u5fae\u8c03\uff0c\u4f7f\u5df2\u5229\u7528\u533a\u57df\u7684\u5956\u52b1\u51cf\u5c11\uff0c\u8feb\u4f7f\u653b\u51fb\u8005\u5bfb\u627e\u672a\u63a2\u7d22\u7684\u6f0f\u6d1e\uff0c\u5f62\u6210\u4ece\u6613\u5230\u96be\u7684\u63a2\u7d22\u8bfe\u7a0b\u3002", "result": "\u76f8\u6bd4\u4e4b\u524d\u7684SOTA\u65b9\u6cd5GFlowNets\uff0c\u4ea4\u53c9\u653b\u51fb\u6210\u529f\u7387\u4ece0.07%\u63d0\u5347\u523031.28%\uff08\u76f8\u5bf9\u589e\u76ca\u8d85\u8fc7400\u500d\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u589e\u52a06%\u3002", "conclusion": "Active Attacks\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u80fd\u6709\u6548\u63d0\u5347RL\u7ea2\u961f\u6d4b\u8bd5\u7684\u653b\u51fb\u591a\u6837\u6027\u548c\u6210\u529f\u7387\uff0c\u53d1\u73b0\u5e7f\u6cdb\u7684\u672c\u5730\u653b\u51fb\u6a21\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22008", "abs": "https://arxiv.org/abs/2509.22008", "authors": ["Yajie Qi", "Wei Wei", "Lin Li", "Lijun Zhang", "Zhidong Gao", "Da Wang", "Huizhong Song"], "title": "Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning", "comment": null, "summary": "Real-world decision-making tasks typically occur in complex and open\nenvironments, posing significant challenges to reinforcement learning (RL)\nagents' exploration efficiency and long-horizon planning capabilities. A\npromising approach is LLM-enhanced RL, which leverages the rich prior knowledge\nand strong planning capabilities of LLMs to guide RL agents in efficient\nexploration. However, existing methods mostly rely on frequent and costly LLM\ninvocations and suffer from limited performance due to the semantic mismatch.\nIn this paper, we introduce a Structured Goal-guided Reinforcement Learning\n(SGRL) method that integrates a structured goal planner and a goal-conditioned\naction pruner to guide RL agents toward efficient exploration. Specifically,\nthe structured goal planner utilizes LLMs to generate a reusable, structured\nfunction for goal generation, in which goals are prioritized. Furthermore, by\nutilizing LLMs to determine goals' priority weights, it dynamically generates\nforward-looking goals to guide the agent's policy toward more promising\ndecision-making trajectories. The goal-conditioned action pruner employs an\naction masking mechanism that filters out actions misaligned with the current\ngoal, thereby constraining the RL agent to select goal-consistent policies. We\nevaluate the proposed method on Crafter and Craftax-Classic, and experimental\nresults demonstrate that SGRL achieves superior performance compared to\nexisting state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u76ee\u6807\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u751f\u6210\u53ef\u91cd\u7528\u7684\u7ed3\u6784\u5316\u76ee\u6807\u89c4\u5212\u51fd\u6570\u548c\u52a8\u4f5c\u526a\u679d\u673a\u5236\uff0c\u63d0\u9ad8RL\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u957f\u671f\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u4efb\u52a1\u73af\u5883\u590d\u6742\u5f00\u653e\uff0c\u4f20\u7edfRL\u65b9\u6cd5\u5728\u63a2\u7d22\u6548\u7387\u548c\u957f\u671f\u89c4\u5212\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709LLM\u589e\u5f3aRL\u65b9\u6cd5\u5b58\u5728\u9891\u7e41\u8c03\u7528\u6210\u672c\u9ad8\u548c\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u5316\u76ee\u6807\u89c4\u5212\u5668\u548c\u76ee\u6807\u6761\u4ef6\u52a8\u4f5c\u526a\u679d\u5668\uff0c\u524d\u8005\u5229\u7528LLM\u751f\u6210\u53ef\u91cd\u7528\u3001\u5e26\u4f18\u5148\u7ea7\u7684\u76ee\u6807\u751f\u6210\u51fd\u6570\uff0c\u540e\u8005\u901a\u8fc7\u52a8\u4f5c\u63a9\u7801\u673a\u5236\u8fc7\u6ee4\u4e0e\u5f53\u524d\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u52a8\u4f5c\u3002", "result": "\u5728Crafter\u548cCraftax-Classic\u73af\u5883\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSGRL\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u76ee\u6807\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u589e\u5f3aRL\u4e2d\u7684\u8bed\u4e49\u4e0d\u5339\u914d\u548c\u9891\u7e41\u8c03\u7528\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86RL\u4ee3\u7406\u7684\u63a2\u7d22\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22367", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22367", "abs": "https://arxiv.org/abs/2509.22367", "authors": ["Tanise Ceron", "Dmitry Nikolaev", "Dominik Stammbach", "Debora Nozza"], "title": "What Is The Political Content in LLMs' Pre- and Post-Training Data?", "comment": "9 pages, under review", "summary": "Large language models (LLMs) are known to generate politically biased text,\nyet how such biases arise remains unclear. A crucial step toward answering this\nquestion is the analysis of training data, whose political content remains\nlargely underexplored in current LLM research. To address this gap, we present\nin this paper an analysis of the pre- and post-training corpora of OLMO2, the\nlargest fully open-source model released together with its complete dataset.\nFrom these corpora, we draw large random samples, automatically annotate\ndocuments for political orientation, and analyze their source domains and\ncontent. We then assess how political content in the training data correlates\nwith models' stance on specific policy issues. Our analysis shows that\nleft-leaning documents predominate across datasets, with pre-training corpora\ncontaining significantly more politically engaged content than post-training\ndata. We also find that left- and right-leaning documents frame similar topics\nthrough distinct values and sources of legitimacy. Finally, the predominant\nstance in the training data strongly correlates with models' political biases\nwhen evaluated on policy issues. These findings underscore the need to\nintegrate political content analysis into future data curation pipelines as\nwell as in-depth documentation of filtering strategies for transparency.", "AI": {"tldr": "\u5206\u6790\u4e86OLMO2\u5f00\u6e90\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u653f\u6cbb\u504f\u89c1\uff0c\u53d1\u73b0\u5de6\u503e\u6587\u6863\u5728\u6570\u636e\u96c6\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e3b\u8981\u653f\u6cbb\u7acb\u573a\u4e0e\u6a21\u578b\u5728\u653f\u7b56\u95ee\u9898\u4e0a\u7684\u504f\u89c1\u5bc6\u5207\u76f8\u5173\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5df2\u77e5\u4f1a\u4ea7\u751f\u653f\u6cbb\u504f\u89c1\u6587\u672c\uff0c\u4f46\u504f\u89c1\u5982\u4f55\u4ea7\u751f\u5c1a\u4e0d\u6e05\u695a\u3002\u9700\u8981\u5206\u6790\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u653f\u6cbb\u5185\u5bb9\u6765\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u4eceOLMO2\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u62bd\u53d6\u5927\u6837\u672c\uff0c\u81ea\u52a8\u6807\u6ce8\u6587\u6863\u7684\u653f\u6cbb\u503e\u5411\uff0c\u5206\u6790\u6765\u6e90\u9886\u57df\u548c\u5185\u5bb9\uff0c\u5e76\u8bc4\u4f30\u8bad\u7ec3\u6570\u636e\u653f\u6cbb\u5185\u5bb9\u4e0e\u6a21\u578b\u653f\u7b56\u7acb\u573a\u7684\u5173\u7cfb\u3002", "result": "\u5de6\u503e\u6587\u6863\u5728\u6240\u6709\u6570\u636e\u96c6\u4e2d\u5360\u4e3b\u5bfc\uff0c\u9884\u8bad\u7ec3\u8bed\u6599\u5305\u542b\u6bd4\u540e\u8bad\u7ec3\u6570\u636e\u66f4\u591a\u7684\u653f\u6cbb\u53c2\u4e0e\u5185\u5bb9\u3002\u5de6\u53f3\u503e\u6587\u6863\u901a\u8fc7\u4e0d\u540c\u7684\u4ef7\u503c\u89c2\u548c\u5408\u6cd5\u6027\u6765\u6e90\u6765\u6784\u5efa\u76f8\u4f3c\u4e3b\u9898\u3002", "conclusion": "\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e3b\u8981\u653f\u6cbb\u7acb\u573a\u4e0e\u6a21\u578b\u7684\u653f\u6cbb\u504f\u89c1\u5bc6\u5207\u76f8\u5173\uff0c\u9700\u8981\u5728\u672a\u6765\u6570\u636e\u7ba1\u7406\u6d41\u7a0b\u4e2d\u6574\u5408\u653f\u6cbb\u5185\u5bb9\u5206\u6790\uff0c\u5e76\u8be6\u7ec6\u8bb0\u5f55\u8fc7\u6ee4\u7b56\u7565\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2509.22067", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22067", "abs": "https://arxiv.org/abs/2509.22067", "authors": ["Anton Korznikov", "Andrey Galichin", "Alexey Dontsov", "Oleg Y. Rogov", "Ivan Oseledets", "Elena Tutubalina"], "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety", "comment": null, "summary": "Activation steering is a promising technique for controlling LLM behavior by\nadding semantically meaningful vectors directly into a model's hidden states\nduring inference. It is often framed as a precise, interpretable, and\npotentially safer alternative to fine-tuning. We demonstrate the opposite:\nsteering systematically breaks model alignment safeguards, making it comply\nwith harmful requests. Through extensive experiments on different model\nfamilies, we show that even steering in a random direction can increase the\nprobability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign\nfeatures from a sparse autoencoder (SAE), a common source of interpretable\ndirections, increases these rates by a further 2-4%. Finally, we show that\ncombining 20 randomly sampled vectors that jailbreak a single prompt creates a\nuniversal attack, significantly increasing harmful compliance on unseen\nrequests. These results challenge the paradigm of safety through\ninterpretability, showing that precise control over model internals does not\nguarantee precise control over model behavior.", "AI": {"tldr": "\u6fc0\u6d3b\u5bfc\u5411\u6280\u672f\u901a\u8fc7\u5411LLM\u9690\u85cf\u72b6\u6001\u6dfb\u52a0\u8bed\u4e49\u5411\u91cf\u6765\u63a7\u5236\u6a21\u578b\u884c\u4e3a\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u7cfb\u7edf\u6027\u7834\u574f\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\uff0c\u589e\u52a0\u6709\u5bb3\u8bf7\u6c42\u7684\u5408\u89c4\u7387\uff0c\u751a\u81f3\u968f\u673a\u5bfc\u5411\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u6709\u5bb3\u884c\u4e3a\u6982\u7387\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u6fc0\u6d3b\u5bfc\u5411\u6280\u672f\u662f\u5426\u771f\u5982\u5ba3\u4f20\u7684\u90a3\u6837\u662f\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u66f4\u5b89\u5168\u7684\u66ff\u4ee3\u5fae\u8c03\u65b9\u6cd5\uff0c\u8fd8\u662f\u5b9e\u9645\u4e0a\u4f1a\u7834\u574f\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u3002", "method": "\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u968f\u673a\u65b9\u5411\u5bfc\u5411\u3001\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u826f\u6027\u7279\u5f81\u5bfc\u5411\uff0c\u4ee5\u53ca\u7ec4\u5408\u591a\u4e2a\u968f\u673a\u5411\u91cf\u521b\u5efa\u901a\u7528\u653b\u51fb\u7684\u6548\u679c\u3002", "result": "\u968f\u673a\u5bfc\u5411\u53ef\u5c06\u6709\u5bb3\u5408\u89c4\u7387\u4ece0%\u63d0\u5347\u81f32-27%\uff1b\u57fa\u4e8eSAE\u7684\u826f\u6027\u7279\u5f81\u5bfc\u5411\u8fdb\u4e00\u6b65\u589e\u52a02-4%\uff1b\u7ec4\u540820\u4e2a\u968f\u673a\u5411\u91cf\u53ef\u521b\u5efa\u901a\u7528\u653b\u51fb\uff0c\u663e\u8457\u63d0\u9ad8\u5bf9\u672a\u89c1\u8bf7\u6c42\u7684\u6709\u5bb3\u5408\u89c4\u7387\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5b9e\u73b0\u5b89\u5168\u7684\u8303\u5f0f\uff0c\u8868\u660e\u5bf9\u6a21\u578b\u5185\u90e8\u7684\u7cbe\u786e\u63a7\u5236\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u7cbe\u786e\u63a7\u5236\u3002", "topic": "agent analysis"}}
{"id": "2509.22102", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22102", "abs": "https://arxiv.org/abs/2509.22102", "authors": ["Marina Ceccon", "Alessandro Fabris", "Goran Radanovi\u0107", "Asia J. Biega", "Gian Antonio Susto"], "title": "Reinforcement Learning for Durable Algorithmic Recourse", "comment": null, "summary": "Algorithmic recourse seeks to provide individuals with actionable\nrecommendations that increase their chances of receiving favorable outcomes\nfrom automated decision systems (e.g., loan approvals). While prior research\nhas emphasized robustness to model updates, considerably less attention has\nbeen given to the temporal dynamics of recourse--particularly in competitive,\nresource-constrained settings where recommendations shape future applicant\npools. In this work, we present a novel time-aware framework for algorithmic\nrecourse, explicitly modeling how candidate populations adapt in response to\nrecommendations. Additionally, we introduce a novel reinforcement learning\n(RL)-based recourse algorithm that captures the evolving dynamics of the\nenvironment to generate recommendations that are both feasible and valid. We\ndesign our recommendations to be durable, supporting validity over a predefined\ntime horizon T. This durability allows individuals to confidently reapply after\ntaking time to implement the suggested changes. Through extensive experiments\nin complex simulation environments, we show that our approach substantially\noutperforms existing baselines, offering a superior balance between feasibility\nand long-term validity. Together, these results underscore the importance of\nincorporating temporal and behavioral dynamics into the design of practical\nrecourse systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u65f6\u95f4\u52a8\u6001\u7684\u7b97\u6cd5\u8ffd\u7d22\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u5728\u7ade\u4e89\u6027\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u957f\u671f\u6709\u6548\u6027\u7684\u53ef\u884c\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u66f4\u65b0\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u8ffd\u7d22\u5efa\u8bae\u7684\u65f6\u95f4\u52a8\u6001\u7279\u6027\uff0c\u7279\u522b\u662f\u5728\u7ade\u4e89\u6027\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u5efa\u8bae\u4f1a\u5f71\u54cd\u672a\u6765\u7684\u7533\u8bf7\u8005\u7fa4\u4f53\u3002", "method": "\u5f00\u53d1\u4e86\u65f6\u95f4\u611f\u77e5\u7684\u7b97\u6cd5\u8ffd\u7d22\u6846\u67b6\uff0c\u5efa\u6a21\u5019\u9009\u4eba\u7fa4\u5bf9\u5efa\u8bae\u7684\u9002\u5e94\u6027\u54cd\u5e94\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8ffd\u7d22\u7b97\u6cd5\uff0c\u6355\u6349\u73af\u5883\u7684\u6f14\u5316\u52a8\u6001\u3002", "result": "\u5728\u590d\u6742\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u53ef\u884c\u6027\u548c\u957f\u671f\u6709\u6548\u6027\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u65f6\u95f4\u548c\u884c\u4e3a\u52a8\u6001\u7eb3\u5165\u5b9e\u7528\u8ffd\u7d22\u7cfb\u7edf\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22115", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22115", "abs": "https://arxiv.org/abs/2509.22115", "authors": ["Chao Wang", "Tao Yang", "Hongtao Tian", "Yunsheng Shi", "Qiyao Ma", "Xiaotao Liu", "Ting Yao", "Wenbo Ding"], "title": "Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization", "comment": "18 pages, 5 figures, Under review as a conference paper at ICLR 2026", "summary": "Critic-free methods like GRPO reduce memory demands by estimating advantages\nfrom multiple rollouts but tend to converge slowly, as critical learning\nsignals are diluted by an abundance of uninformative samples and tokens. To\ntackle this challenge, we propose the \\textbf{Dynamic Dual-Level Down-Sampling\n(D$^3$S)} framework that prioritizes the most informative samples and tokens\nacross groups to improve the efficient of policy optimization. D$^3$S operates\nalong two levels: (1) the sample-level, which selects a subset of rollouts to\nmaximize advantage variance ($\\text{Var}(A)$). We theoretically proven that\nthis selection is positively correlated with the upper bound of the policy\ngradient norms, yielding higher policy gradients. (2) the token-level, which\nprioritizes tokens with a high product of advantage magnitude and policy\nentropy ($|A_{i,t}|\\times H_{i,t}$), focusing updates on tokens where the\npolicy is both uncertain and impactful. Moreover, to prevent overfitting to\nhigh-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by\ncurriculum learning. This schedule starts with aggressive down-sampling to\naccelerate early learning and gradually relaxes to promote robust\ngeneralization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that\nintegrating D$^3$S into advanced RL algorithms achieves state-of-the-art\nperformance and generalization while requiring \\textit{fewer} samples and\ntokens across diverse reasoning benchmarks. Our code is added in the\nsupplementary materials and will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86D$^3$S\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u53cc\u7ea7\u4e0b\u91c7\u6837\u4f18\u5316\u7b56\u7565\u5b66\u4e60\u6548\u7387\uff0c\u5728\u6837\u672c\u7ea7\u9009\u62e9\u4f18\u52bf\u65b9\u5dee\u6700\u5927\u7684rollouts\uff0c\u5728token\u7ea7\u5173\u6ce8\u4f18\u52bf\u5e45\u5ea6\u4e0e\u7b56\u7565\u71b5\u7684\u4e58\u79ef\uff0c\u7ed3\u5408\u52a8\u6001\u4e0b\u91c7\u6837\u8ba1\u5212\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65e0\u8bc4\u8bba\u65b9\u6cd5\u5982GRPO\u56e0\u5927\u91cf\u65e0\u4fe1\u606f\u6837\u672c\u548ctoken\u7a00\u91ca\u5173\u952e\u5b66\u4e60\u4fe1\u53f7\u800c\u6536\u655b\u7f13\u6162\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u7b56\u7565\u4f18\u5316\u7684\u6548\u7387\u3002", "method": "D$^3$S\u6846\u67b6\u5305\u542b\u6837\u672c\u7ea7\u9009\u62e9\uff08\u6700\u5927\u5316\u4f18\u52bf\u65b9\u5dee\uff09\u548ctoken\u7ea7\u9009\u62e9\uff08\u5173\u6ce8\u4f18\u52bf\u5e45\u5ea6\u4e0e\u7b56\u7565\u71b5\u7684\u4e58\u79ef\uff09\uff0c\u7ed3\u5408\u52a8\u6001\u4e0b\u91c7\u6837\u8ba1\u5212\u4ece\u6fc0\u8fdb\u5230\u5bbd\u677e\u9010\u6b65\u8c03\u6574\u3002", "result": "\u5728Qwen2.5\u548cLlama3.1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cD$^3$S\u96c6\u6210\u5230\u5148\u8fdbRL\u7b97\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u6837\u672c\u548ctoken\u3002", "conclusion": "D$^3$S\u6846\u67b6\u901a\u8fc7\u4f18\u5148\u5904\u7406\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6837\u672c\u548ctoken\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7b56\u7565\u4f18\u5316\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22480", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22480", "abs": "https://arxiv.org/abs/2509.22480", "authors": ["Hang Li", "Kaiqi Yang", "Yucheng Chu", "Hui Liu", "Jiliang Tang"], "title": "Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving", "comment": "17 pages, 11 figures", "summary": "Large language models (LLMs) have been widely used for problem-solving tasks.\nMost recent work improves their performance through supervised fine-tuning\n(SFT) with labeled data or reinforcement learning (RL) from task feedback. In\nthis paper, we study a new perspective: the divergence in solutions generated\nby LLMs for a single problem. We show that higher solution divergence is\npositively related to better problem-solving abilities across various models.\nBased on this finding, we propose solution divergence as a novel metric that\ncan support both SFT and RL strategies. We test this idea on three\nrepresentative problem domains and find that using solution divergence\nconsistently improves success rates. These results suggest that solution\ndivergence is a simple but effective tool for advancing LLM training and\nevaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u5206\u6b67\u5ea6\u4f5c\u4e3a\u8861\u91cfLLM\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u65b0\u6307\u6807\uff0c\u53d1\u73b0\u66f4\u9ad8\u7684\u5206\u6b67\u5ea6\u4e0e\u66f4\u597d\u7684\u6027\u80fd\u76f8\u5173\uff0c\u5e76\u8bc1\u660e\u8be5\u6307\u6807\u80fd\u6709\u6548\u63d0\u5347\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6a21\u578b\u751f\u6210\u89e3\u51b3\u65b9\u6848\u591a\u6837\u6027\u7684\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u89e3\u51b3\u65b9\u6848\u5206\u6b67\u5ea6\u4e0e\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540cLLM\u5728\u5355\u4e00\u95ee\u9898\u4e0a\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u5206\u6b67\u5ea6\u4f5c\u4e3a\u65b0\u6307\u6807\uff0c\u5e76\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u95ee\u9898\u9886\u57df\u9a8c\u8bc1\u5176\u5bf9SFT\u548cRL\u7b56\u7565\u7684\u6539\u8fdb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u89e3\u51b3\u65b9\u6848\u5206\u6b67\u5ea6\u4e0e\u6a21\u578b\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u5448\u6b63\u76f8\u5173\uff0c\u4f7f\u7528\u8be5\u6307\u6807\u80fd\u6301\u7eed\u63d0\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u89e3\u51b3\u65b9\u6848\u5206\u6b67\u5ea6\u662f\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u63a8\u8fdbLLM\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2509.22506", "categories": ["cs.CL", "cs.LG", "68T07, 68T50, 65F20", "I.2.7; I.2.6; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.22506", "abs": "https://arxiv.org/abs/2509.22506", "authors": ["Idan Kashani", "Avi Mendelson", "Yaniv Nemcovsky"], "title": "Representing LLMs in Prompt Semantic Task Space", "comment": "Accepted to Findings of the Association for Computational\n  Linguistics: EMNLP 2025", "summary": "Large language models (LLMs) achieve impressive results over various tasks,\nand ever-expanding public repositories contain an abundance of pre-trained\nmodels. Therefore, identifying the best-performing LLM for a given task is a\nsignificant challenge. Previous works have suggested learning LLM\nrepresentations to address this. However, these approaches present limited\nscalability and require costly retraining to encompass additional models and\ndatasets. Moreover, the produced representation utilizes distinct spaces that\ncannot be easily interpreted. This work presents an efficient, training-free\napproach to representing LLMs as linear operators within the prompts' semantic\ntask space, thus providing a highly interpretable representation of the models'\napplication. Our method utilizes closed-form computation of geometrical\nproperties and ensures exceptional scalability and real-time adaptability to\ndynamically expanding repositories. We demonstrate our approach on success\nprediction and model selection tasks, achieving competitive or state-of-the-art\nresults with notable performance in out-of-sample scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u8868\u793a\u4e3a\u63d0\u793a\u8bed\u4e49\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u7b97\u5b50\uff0c\u63d0\u4f9b\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u8868\u793a\uff0c\u5728\u6210\u529f\u9884\u6d4b\u548c\u6a21\u578b\u9009\u62e9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u8bad\u7ec3\u6210\u672c\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u4e3a\u52a8\u6001\u6269\u5c55\u7684\u6a21\u578b\u5e93\u63d0\u4f9b\u5b9e\u65f6\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u51e0\u4f55\u7279\u6027\u7684\u95ed\u5f0f\u8ba1\u7b97\uff0c\u5c06LLMs\u8868\u793a\u4e3a\u63d0\u793a\u8bed\u4e49\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u7b97\u5b50\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u6a21\u578b\u8868\u793a\u3002", "result": "\u5728\u6210\u529f\u9884\u6d4b\u548c\u6a21\u578b\u9009\u62e9\u4efb\u52a1\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5728\u6837\u672c\u5916\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684LLM\u8868\u793a\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u6a21\u578b\u5e93\u3002", "topic": "agent analysis"}}
{"id": "2509.22510", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22510", "abs": "https://arxiv.org/abs/2509.22510", "authors": ["Gautam Siddharth Kashyap", "Mark Dras", "Usman Naseem"], "title": "We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong", "comment": null, "summary": "Alignment of Large Language Models (LLMs) along multiple\nobjectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe\nand reliable deployment. Prior work has used steering vector-small control\nsignals injected into hidden states-to guide LLM outputs, typically via\none-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single\nalignment objective can inadvertently overwrite representations learned for\nother objectives, leading to catastrophic forgetting. More recent approaches\nextend steering vectors via one-to-many (1-to-N) Transformer decoders. While\nthis alleviates catastrophic forgetting, naive multi-branch designs optimize\neach objective independently, which can cause inference fragmentation-outputs\nacross HHH objectives may become inconsistent. We propose Adaptive Multi-Branch\nSteering (AMBS), a two-stage 1-to-N framework for unified and efficient\nmulti-objective alignment. In Stage I, post-attention hidden states of the\nTransformer layer are computed once to form a shared representation. In Stage\nII, this representation is cloned into parallel branches and steered via a\npolicy-reference mechanism, enabling objective-specific control while\nmaintaining cross-objective consistency. Empirical evaluations on Alpaca,\nBeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment\nacross multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves\naverage alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared\nto a naive 1-to-N baseline, while remaining competitive with state-of-the-art\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86AMBS\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb51-to-N\u67b6\u6784\u5b9e\u73b0\u591a\u76ee\u6807\u5bf9\u9f50\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u548c\u63a8\u7406\u788e\u7247\u5316\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u5355\u4e2a\u5bf9\u9f50\u76ee\u6807\u65f6\u4f1a\u8986\u76d6\u5176\u4ed6\u76ee\u6807\u7684\u8868\u793a\uff0c\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff1b\u800c\u7b80\u5355\u7684\u591a\u5206\u652f\u8bbe\u8ba1\u4f1a\u5bfc\u81f4\u8de8\u76ee\u6807\u8f93\u51fa\u4e0d\u4e00\u81f4", "method": "\u4e24\u9636\u6bb51-to-N\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8ba1\u7b97\u5171\u4eab\u8868\u793a\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u7b56\u7565\u53c2\u8003\u673a\u5236\u5728\u5e76\u884c\u5206\u652f\u4e2d\u8fdb\u884c\u76ee\u6807\u7279\u5b9a\u63a7\u5236", "result": "\u5728\u591a\u4e2a7B LLM\u4e0a\u9a8c\u8bc1\uff0cAMBS\u5e73\u5747\u5bf9\u9f50\u5206\u6570\u63d0\u5347+32.4%\uff0c\u4e0d\u5b89\u5168\u8f93\u51fa\u51cf\u5c1111.0%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "AMBS\u80fd\u591f\u6709\u6548\u7edf\u4e00\u9ad8\u6548\u5730\u5b9e\u73b0\u591a\u76ee\u6807\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u8de8\u76ee\u6807\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u5347\u6574\u4f53\u5bf9\u9f50\u6027\u80fd", "topic": "agent analysis"}}
{"id": "2509.22637", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22637", "abs": "https://arxiv.org/abs/2509.22637", "authors": ["Xiangxin Zhou", "Zichen Liu", "Haonan Wang", "Chao Du", "Min Lin", "Chongxuan Li", "Liang Wang", "Tianyu Pang"], "title": "Variational Reasoning for Language Models", "comment": null, "summary": "We introduce a variational reasoning framework for language models that\ntreats thinking traces as latent variables and optimizes them through\nvariational inference. Starting from the evidence lower bound (ELBO), we extend\nit to a multi-trace objective for tighter bounds and propose a forward-KL\nformulation that stabilizes the training of the variational posterior. We\nfurther show that rejection sampling finetuning and binary-reward RL, including\nGRPO, can be interpreted as local forward-KL objectives, where an implicit\nweighting by model accuracy naturally arises from the derivation and reveals a\npreviously unnoticed bias toward easier questions. We empirically validate our\nmethod on the Qwen 2.5 and Qwen 3 model families across a wide range of\nreasoning tasks. Overall, our work provides a principled probabilistic\nperspective that unifies variational inference with RL-style methods and yields\nstable objectives for improving the reasoning ability of language models. Our\ncode is available at https://github.com/sail-sg/variational-reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u5206\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u8f68\u8ff9\u89c6\u4e3a\u6f5c\u53d8\u91cf\u5e76\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u8fdb\u884c\u4f18\u5316\uff0c\u6269\u5c55\u4e86\u8bc1\u636e\u4e0b\u754c(ELBO)\u4e3a\u591a\u8f68\u8ff9\u76ee\u6807\uff0c\u63d0\u51fa\u4e86\u7a33\u5b9a\u7684\u524d\u5411KL\u516c\u5f0f\u8bad\u7ec3\u53d8\u5206\u540e\u9a8c\u3002", "motivation": "\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u539f\u5219\u6027\u7684\u6982\u7387\u89c6\u89d2\uff0c\u7edf\u4e00\u53d8\u5206\u63a8\u7406\u4e0e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u83b7\u5f97\u7a33\u5b9a\u7684\u4f18\u5316\u76ee\u6807\u3002", "method": "\u57fa\u4e8eELBO\u6269\u5c55\u4e3a\u591a\u8f68\u8ff9\u76ee\u6807\uff0c\u63d0\u51fa\u524d\u5411KL\u516c\u5f0f\u8bad\u7ec3\u53d8\u5206\u540e\u9a8c\uff0c\u5c06\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u548c\u4e8c\u5143\u5956\u52b1RL\u89e3\u91ca\u4e3a\u5c40\u90e8\u524d\u5411KL\u76ee\u6807\u3002", "result": "\u5728Qwen 2.5\u548cQwen 3\u6a21\u578b\u7cfb\u5217\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u5e7f\u6cdb\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7edf\u4e00\u53d8\u5206\u63a8\u7406\u4e0eRL\u98ce\u683c\u65b9\u6cd5\u7684\u6982\u7387\u89c6\u89d2\uff0c\u4ea7\u751f\u4e86\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7a33\u5b9a\u76ee\u6807\u3002", "topic": "agent analysis"}}
{"id": "2509.22638", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22638", "abs": "https://arxiv.org/abs/2509.22638", "authors": ["Renjie Luo", "Zichen Liu", "Xiangyan Liu", "Chao Du", "Min Lin", "Wenhu Chen", "Wei Lu", "Tianyu Pang"], "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards", "comment": null, "summary": "LLMs are often trained with RL from human or AI feedback, yet such methods\ntypically compress nuanced feedback into scalar rewards, discarding much of\ntheir richness and inducing scale imbalance. We propose treating verbal\nfeedback as a conditioning signal. Inspired by language priors in text-to-image\ngeneration, which enable novel outputs from unseen prompts, we introduce the\nfeedback-conditional policy (FCP). FCP learns directly from response-feedback\npairs, approximating the feedback-conditional posterior through maximum\nlikelihood training on offline data. We further develop an online bootstrapping\nstage where the policy generates under positive conditions and receives fresh\nfeedback to refine itself. This reframes feedback-driven learning as\nconditional generation rather than reward optimization, offering a more\nexpressive way for LLMs to directly learn from verbal feedback. Our code is\navailable at https://github.com/sail-sg/feedback-conditional-policy.", "AI": {"tldr": "\u63d0\u51fa\u53cd\u9988\u6761\u4ef6\u7b56\u7565\uff08FCP\uff09\uff0c\u5c06\u8bed\u8a00\u53cd\u9988\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u800c\u975e\u6807\u91cf\u5956\u52b1\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u8bad\u7ec3\u5b66\u4e60\u53cd\u9988\u6761\u4ef6\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u5f00\u53d1\u5728\u7ebf\u81ea\u4e3e\u9636\u6bb5\u6765\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5c06\u4e30\u5bcc\u53cd\u9988\u538b\u7f29\u4e3a\u6807\u91cf\u5956\u52b1\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u5c3a\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u5229\u7528\u8bed\u8a00\u53cd\u9988\u7684\u65b9\u6cd5\u3002", "method": "FCP\u4ece\u54cd\u5e94-\u53cd\u9988\u5bf9\u4e2d\u76f4\u63a5\u5b66\u4e60\uff0c\u901a\u8fc7\u79bb\u7ebf\u6700\u5927\u4f3c\u7136\u8bad\u7ec3\u8fd1\u4f3c\u53cd\u9988\u6761\u4ef6\u540e\u9a8c\uff0c\u5e76\u5728\u7ebf\u751f\u6210\u6b63\u6761\u4ef6\u53cd\u9988\u6765\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5c06\u53cd\u9988\u9a71\u52a8\u5b66\u4e60\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u751f\u6210\u800c\u975e\u5956\u52b1\u4f18\u5316\uff0c\u4e3aLLMs\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u8868\u8fbe\u7684\u5b66\u4e60\u65b9\u5f0f\u3002", "conclusion": "FCP\u6846\u67b6\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u8bed\u8a00\u53cd\u9988\uff0c\u907f\u514d\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22644", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22644", "abs": "https://arxiv.org/abs/2509.22644", "authors": ["Zimu Lu", "Houxing Ren", "Yunqiao Yang", "Ke Wang", "Zhuofan Zong", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning", "comment": null, "summary": "Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7.", "AI": {"tldr": "\u63d0\u51fa\u4e86WebGen-Agent\uff0c\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u53cd\u9988\u548cGUI\u4ee3\u7406\u6d4b\u8bd5\u6765\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u7f51\u7ad9\u4ee3\u7801\u5e93\u7684\u65b0\u578b\u7f51\u7ad9\u751f\u6210\u4ee3\u7406\u7cfb\u7edf\u3002\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7ad9\u751f\u6210\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u4ee3\u7406\u5728\u7f51\u7ad9\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u4e3b\u8981\u4f9d\u8d56\u7b80\u5355\u7684\u4ee3\u7801\u6267\u884c\u53cd\u9988\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u751f\u6210\u4ee3\u7801\u7684\u5b9e\u9645\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u6548\u679c\u548c\u7528\u6237\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1. \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5206\u6790\u7f51\u7ad9\u622a\u56fe\u751f\u6210\u8be6\u7ec6\u6587\u672c\u63cf\u8ff0\u548c\u5efa\u8bae\uff1b2. \u7ed3\u5408GUI\u4ee3\u7406\u6d4b\u8bd5\u548c\u56de\u6eaf\u9009\u62e9\u673a\u5236\uff1b3. \u5f15\u5165Step-GRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c06\u89c6\u89c9\u548cGUI\u4ee3\u7406\u8bc4\u5206\u4f5c\u4e3a\u8fc7\u7a0b\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728WebGen-Bench\u6570\u636e\u96c6\u4e0a\uff0cWebGen-Agent\u5c06Claude-3.5-Sonnet\u7684\u51c6\u786e\u7387\u4ece26.4%\u63d0\u5347\u523051.9%\uff0c\u5916\u89c2\u8bc4\u5206\u4ece3.0\u63d0\u5347\u52303.9\uff1bStep-GRPO\u8bad\u7ec3\u4f7fQwen2.5-Coder-7B-Instruct\u7684\u51c6\u786e\u7387\u4ece38.9%\u63d0\u5347\u523045.4%\uff0c\u5916\u89c2\u8bc4\u5206\u4ece3.4\u63d0\u5347\u52303.7\u3002", "conclusion": "WebGen-Agent\u901a\u8fc7\u7efc\u5408\u89c6\u89c9\u53cd\u9988\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7f51\u7ad9\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8d28\u91cf\u8bc4\u4f30\u95ee\u9898\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5728\u7f51\u7ad9\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "topic": "code agent"}}
{"id": "2509.22310", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22310", "abs": "https://arxiv.org/abs/2509.22310", "authors": ["Bumgeun Park", "Donghwan Lee"], "title": "Adaptive Policy Backbone via Shared Network", "comment": null, "summary": "Reinforcement learning (RL) has achieved impressive results across domains,\nyet learning an optimal policy typically requires extensive interaction data,\nlimiting practical deployment. A common remedy is to leverage priors, such as\npre-collected datasets or reference policies, but their utility degrades under\ntask mismatch between training and deployment. While prior work has sought to\naddress this mismatch, it has largely been restricted to in-distribution\nsettings. To address this challenge, we propose Adaptive Policy Backbone (APB),\na meta-transfer RL method that inserts lightweight linear layers before and\nafter a shared backbone, thereby enabling parameter-efficient fine-tuning\n(PEFT) while preserving prior knowledge during adaptation. Our results show\nthat APB improves sample efficiency over standard RL and adapts to\nout-of-distribution (OOD) tasks where existing meta-RL baselines typically\nfail.", "AI": {"tldr": "\u63d0\u51fa\u4e86APB\uff08\u81ea\u9002\u5e94\u7b56\u7565\u9aa8\u5e72\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d2\u5165\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5c42\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5728\u4efb\u52a1\u4e0d\u5339\u914d\u60c5\u51b5\u4e0b\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u4efb\u52a1\u4e0d\u5339\u914d\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u7684\u9002\u5e94\u80fd\u529b", "method": "APB\u65b9\u6cd5\u5728\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u524d\u540e\u63d2\u5165\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5c42\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\uff0c\u540c\u65f6\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6", "result": "APB\u76f8\u6bd4\u6807\u51c6RL\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5", "conclusion": "APB\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9002\u5e94\u4efb\u52a1\u4e0d\u5339\u914d\u573a\u666f\uff0c\u5728\u4fdd\u6301\u5148\u9a8c\u77e5\u8bc6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u5fae\u8c03", "topic": "agentic reinforcement learning"}}
{"id": "2509.22387", "categories": ["cs.LG", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.22387", "abs": "https://arxiv.org/abs/2509.22387", "authors": ["Narada Maugin", "Tristan Cazenave"], "title": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly", "comment": "Accepted at Advances in Computer Games (ACG) 2025, LNCS (Springer)", "summary": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have\nenabled the development of pokerbots capable of beating the best human players\nin heads-up (1v1) cash games and competing with them in six-player formats.\nHowever, CFR's computational complexity rises exponentially with the number of\nplayers. Furthermore, in games with three or more players, following Nash\nequilibrium no longer guarantees a non-losing outcome. These limitations, along\nwith others, significantly restrict the applicability of CFR to the most\npopular formats: tournaments. Motivated by the recent success of Large Language\nModels (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored\nto Spin & Go, a popular three-player online poker format. SpinGPT is trained in\ntwo stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;\n(2) Reinforcement Learning on 270k solver-generated hands. Our results show\nthat SpinGPT matches the solver's actions in 78% of decisions (tolerant\naccuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100\nversus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest\nthat LLMs could be a new way to deal with multi-player imperfect-information\ngames like poker.", "AI": {"tldr": "\u63d0\u51fa\u4e86SpinGPT\uff0c\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e09\u4eba\u6251\u514b\u6e38\u620fSpin & Go\u7684LLM\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u591a\u4eba\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\u4e2d\u53d6\u5f97\u826f\u597d\u8868\u73b0", "motivation": "CFR\u7b97\u6cd5\u5728\u591a\u4eba\u6e38\u620f\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4e14\u7eb3\u4ec0\u5747\u8861\u65e0\u6cd5\u4fdd\u8bc1\u975e\u8d1f\u7ed3\u679c\uff0c\u9650\u5236\u4e86\u5176\u5728\u9526\u6807\u8d5b\u7b49\u6d41\u884c\u683c\u5f0f\u4e2d\u7684\u5e94\u7528", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u572832\u4e07\u624b\u9ad8\u989d\u4e13\u5bb6\u51b3\u7b56\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b2) \u572827\u4e07\u624b\u6c42\u89e3\u5668\u751f\u6210\u624b\u724c\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60", "result": "SpinGPT\u4e0e\u6c42\u89e3\u5668\u51b3\u7b56\u5339\u914d\u5ea6\u8fbe78%\uff0c\u57283\u4e07\u624b\u724c\u6d4b\u8bd5\u4e2d\u5bf9\u6297Slumbot\u83b7\u5f9713.4\u00b112.9 BB/100\u7684\u80dc\u7387", "conclusion": "LLMs\u4e3a\u5904\u7406\u591a\u4eba\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\uff08\u5982\u6251\u514b\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5", "topic": "agentic reinforcement learning"}}
{"id": "2509.22576", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22576", "abs": "https://arxiv.org/abs/2509.22576", "authors": ["Xu Wujiang", "Wentian Zhao", "Zhenting Wang", "Li Yu-Jhe", "Jin Can", "Jin Mingyu", "Mei Kai", "Wan Kun", "Metaxas Dimitris"], "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning", "comment": null, "summary": "Training LLM agents in multi-turn environments with sparse rewards, where\ncompleting a single task requires 30+ turns of interaction within an episode,\npresents a fundamental challenge for reinforcement learning. We identify a\ncritical failure mode unique to this setting: the exploration-exploitation\ncascade failure. This cascade begins with early-stage policy premature\nconvergence, where sparse feedback causes agents to commit to flawed,\nlow-entropy strategies. Subsequently, agents enter late-stage policy collapse,\nwhere conventional entropy regularization becomes counterproductive, promoting\nchaotic exploration that destabilizes training. We propose Entropy-regularized\nPolicy Optimization (EPO), a general framework that breaks this failure cycle\nthrough three synergistic mechanisms: (1) adopting entropy regularization in\nmulti-turn settings to enhance exploration, (2) an entropy smoothing\nregularizer that bounds policy entropy within historical averages to prevent\nabrupt fluctuations, and (3) adaptive phase-based weighting that balances\nexploration and exploitation across training. Our analysis justifies that EPO\nguarantees monotonically decreasing entropy variance while maintaining\nconvergence. EPO achieves up to 152% performance improvement on ScienceWorld\nand up to 19.8% on ALFWorld. Our work demonstrates that multi-turn\nsparse-reward settings require fundamentally different entropy control than\ntraditional RL, with broad implications for LLM agent training.", "AI": {"tldr": "\u63d0\u51fa\u4e86EPO\u6846\u67b6\u89e3\u51b3\u591a\u8f6e\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2dLLM\u667a\u80fd\u4f53\u7684\u63a2\u7d22-\u5229\u7528\u7ea7\u8054\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u3001\u71b5\u5e73\u6ed1\u548c\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\uff0c\u5728ScienceWorld\u548cALFWorld\u4e0a\u5206\u522b\u5b9e\u73b0152%\u548c19.8%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u591a\u8f6e\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2dLLM\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u6839\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u63a2\u7d22-\u5229\u7528\u7ea7\u8054\u5931\u8d25\u95ee\u9898\uff0c\u5305\u62ec\u65e9\u671f\u7b56\u7565\u8fc7\u65e9\u6536\u655b\u548c\u540e\u671f\u7b56\u7565\u5d29\u6e83\u3002", "method": "\u63d0\u51fa\u71b5\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316(EPO)\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u673a\u5236\uff1a\u591a\u8f6e\u73af\u5883\u4e2d\u7684\u71b5\u6b63\u5219\u5316\u3001\u71b5\u5e73\u6ed1\u6b63\u5219\u5316\u5668\u9650\u5236\u7b56\u7565\u71b5\u5728\u5386\u53f2\u5e73\u5747\u503c\u8303\u56f4\u5185\u3001\u81ea\u9002\u5e94\u9636\u6bb5\u6743\u91cd\u5e73\u8861\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "EPO\u5728ScienceWorld\u4e0a\u5b9e\u73b0\u9ad8\u8fbe152%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728ALFWorld\u4e0a\u5b9e\u73b019.8%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u5728\u591a\u8f6e\u7a00\u758f\u5956\u52b1\u8bbe\u7f6e\u4e2d\u9700\u8981\u4e0e\u4f20\u7edfRL\u4e0d\u540c\u7684\u71b5\u63a7\u5236\u65b9\u6cd5\u3002", "conclusion": "\u591a\u8f6e\u7a00\u758f\u5956\u52b1\u73af\u5883\u9700\u8981\u4e0e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u6839\u672c\u4e0d\u540c\u7684\u71b5\u63a7\u5236\u65b9\u6cd5\uff0cEPO\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u71b5\u7ba1\u7406\u6210\u529f\u89e3\u51b3\u4e86\u63a2\u7d22-\u5229\u7528\u7ea7\u8054\u5931\u8d25\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22601", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.22601", "abs": "https://arxiv.org/abs/2509.22601", "authors": ["Yulei Qin", "Xiaoyu Tan", "Zhengbao He", "Gang Li", "Haojia Lin", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Siqi Cai", "Renting Rui", "Shaofei Cai", "Yuzheng Cai", "Xuan Zhang", "Sheng Ye", "Ke Li", "Xing Sun"], "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning", "comment": "26 pages, 11 figures", "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence.", "AI": {"tldr": "\u63d0\u51fa\u4e86SPEAR\u65b9\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u81ea\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u667a\u80fd\u4f53LLMs\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u7b56\u7565\u71b5\u6765\u523a\u6fc0\u63a2\u7d22\uff0c\u4f46\u673a\u68b0\u7684\u71b5\u6700\u5927\u5316\u5bb9\u6613\u56e0\u591a\u8f6e\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u672c\u6587\u65e8\u5728\u57fa\u4e8e\u667a\u80fd\u4f53\u81ea\u8eab\u7ecf\u9a8c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u907f\u514d\u71b5\u5d29\u6e83\u6216\u53d1\u6563\u3002", "method": "\u6269\u5c55\u4e86\u81ea\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u8bfe\u7a0b\u7ba1\u7406\u63a2\u7d22\u8fc7\u7a0b\uff1a\u65e9\u671f\u5229\u7528\u5185\u5728\u5956\u52b1\u4fc3\u8fdb\u6280\u80fd\u7ea7\u63a2\u7d22\uff0c\u540e\u671f\u52a0\u5f3a\u81ea\u6a21\u4eff\u8fdb\u884c\u52a8\u4f5c\u7ea7\u63a2\u7d22\u3002\u5f15\u5165\u4f18\u52bf\u91cd\u6821\u51c6\u548c\u6b63\u5219\u5316\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "SPEAR\u65b9\u6cd5\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u71b5\u5728\u5e73\u8861\u8303\u56f4\u5185\uff0c\u6709\u6548\u79ef\u7d2f\u5de5\u5177\u4f7f\u7528\u6280\u80fd\uff0c\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u8fed\u4ee3\uff0c\u540c\u65f6\u907f\u514d\u65e0\u754c\u71b5\u589e\u957f\u3002", "conclusion": "SPEAR\u901a\u8fc7\u8bfe\u7a0b\u5f0f\u81ea\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6e10\u8fdb\u5e73\u8861\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4f53LLMs\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22402", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22402", "abs": "https://arxiv.org/abs/2509.22402", "authors": ["Nan Tang", "Jing-Cheng Pang", "Guanlin Li", "Chao Qian", "Yang Yu"], "title": "ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation", "comment": null, "summary": "Reward design remains a critical bottleneck in visual reinforcement learning\n(RL) for robotic manipulation. In simulated environments, rewards are\nconventionally designed based on the distance to a target position. However,\nsuch precise positional information is often unavailable in real-world visual\nsettings due to sensory and perceptual limitations. In this study, we propose a\nmethod that implicitly infers spatial distances through keypoints extracted\nfrom images. Building on this, we introduce Reward Learning with Anticipation\nModel (ReLAM), a novel framework that automatically generates dense, structured\nrewards from action-free video demonstrations. ReLAM first learns an\nanticipation model that serves as a planner and proposes intermediate\nkeypoint-based subgoals on the optimal path to the final goal, creating a\nstructured learning curriculum directly aligned with the task's geometric\nobjectives. Based on the anticipated subgoals, a continuous reward signal is\nprovided to train a low-level, goal-conditioned policy under the hierarchical\nreinforcement learning (HRL) framework with provable sub-optimality bound.\nExtensive experiments on complex, long-horizon manipulation tasks show that\nReLAM significantly accelerates learning and achieves superior performance\ncompared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faReLAM\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u65e0\u52a8\u4f5c\u89c6\u9891\u6f14\u793a\u4e2d\u81ea\u52a8\u751f\u6210\u5bc6\u96c6\u7ed3\u6784\u5316\u5956\u52b1\uff0c\u89e3\u51b3\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u8bbe\u8ba1\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5728\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u611f\u77e5\u9650\u5236\uff0c\u4f20\u7edf\u57fa\u4e8e\u76ee\u6807\u4f4d\u7f6e\u8ddd\u79bb\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u5f80\u5f80\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u65b0\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5173\u952e\u70b9\u63d0\u53d6\u9690\u5f0f\u63a8\u65ad\u7a7a\u95f4\u8ddd\u79bb\uff0c\u5b66\u4e60\u9884\u671f\u6a21\u578b\u4f5c\u4e3a\u89c4\u5212\u5668\u63d0\u51fa\u4e2d\u95f4\u5b50\u76ee\u6807\uff0c\u5728\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\u8bad\u7ec3\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u3002", "result": "\u5728\u590d\u6742\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReLAM\u663e\u8457\u52a0\u901f\u5b66\u4e60\u5e76\u8fbe\u5230\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "ReLAM\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u5956\u52b1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u8bbe\u8ba1\u7684\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22611", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22611", "abs": "https://arxiv.org/abs/2509.22611", "authors": ["Junkang Wu", "Kexin Huang", "Jiancan Wu", "An Zhang", "Xiang Wang", "Xiangnan He"], "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM\nreasoning, but training often oscillates between {entropy collapse} and\n{entropy explosion}. We trace both hazards to the mean baseline used in\nvalue-free RL (e.g., GRPO and DAPO), which improperly penalizes\nnegative-advantage samples under reward outliers. We propose {Quantile\nAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantile\nbaseline. QAE induces a response-level, two-regime gate: on hard queries (p <=\n1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it\ntargets remaining failures. Under first-order softmax updates, we prove\n{two-sided entropy safety}, giving lower and upper bounds on one-step entropy\nchange that curb explosion and prevent collapse. Empirically, this minimal\nmodification stabilizes entropy, sparsifies credit assignment (with tuned K,\nroughly 80% of responses receive zero advantage), and yields sustained pass@1\ngains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results\nidentify {baseline design} -- rather than token-level heuristics -- as the\nprimary mechanism for scaling RLVR.", "AI": {"tldr": "\u63d0\u51faQuantile Advantage Estimation (QAE)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5206\u4f4d\u6570\u57fa\u7ebf\u66ff\u4ee3\u5747\u503c\u57fa\u7ebf\u6765\u89e3\u51b3RLVR\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u548c\u71b5\u7206\u70b8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53cc\u5411\u71b5\u5b89\u5168\u3002", "motivation": "RLVR\u8bad\u7ec3\u4e2d\u7ecf\u5e38\u51fa\u73b0\u71b5\u5d29\u6e83\u548c\u71b5\u7206\u70b8\u7684\u95ee\u9898\uff0c\u8fd9\u6e90\u4e8e\u4ef7\u503c\u81ea\u7531RL\u4e2d\u4f7f\u7528\u7684\u5747\u503c\u57fa\u7ebf\u5728\u5956\u52b1\u5f02\u5e38\u503c\u60c5\u51b5\u4e0b\u5bf9\u8d1f\u4f18\u52bf\u6837\u672c\u7684\u4e0d\u5f53\u60e9\u7f5a\u3002", "method": "\u63d0\u51faQAE\u65b9\u6cd5\uff0c\u7528\u5206\u7ec4K\u5206\u4f4d\u6570\u57fa\u7ebf\u66ff\u4ee3\u5747\u503c\u57fa\u7ebf\uff0c\u5728\u56f0\u96be\u67e5\u8be2\u4e2d\u5f3a\u5316\u7f55\u89c1\u6210\u529f\uff0c\u5728\u7b80\u5355\u67e5\u8be2\u4e2d\u9488\u5bf9\u5269\u4f59\u5931\u8d25\uff0c\u5b9e\u73b0\u54cd\u5e94\u7ea7\u522b\u7684\u53cc\u673a\u5236\u95e8\u63a7\u3002", "result": "QAE\u7a33\u5b9a\u4e86\u71b5\u503c\uff0c\u7a00\u758f\u5316\u4e86\u4fe1\u7528\u5206\u914d\uff08\u7ea680%\u54cd\u5e94\u83b7\u5f97\u96f6\u4f18\u52bf\uff09\uff0c\u5728Qwen3-8B/14B-Base\u6a21\u578b\u4e0a\u6301\u7eed\u63d0\u5347\u4e86AIME 2024/2025\u548cAMC 2023\u7684pass@1\u6027\u80fd\u3002", "conclusion": "\u57fa\u7ebf\u8bbe\u8ba1\uff08\u800c\u975etoken\u7ea7\u542f\u53d1\u5f0f\u65b9\u6cd5\uff09\u662f\u6269\u5c55RLVR\u7684\u4e3b\u8981\u673a\u5236\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22580", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22580", "abs": "https://arxiv.org/abs/2509.22580", "authors": ["Guannan Lai", "Da-Wei Zhou", "Xin Yang", "Han-Jia Ye"], "title": "The Lie of the Average: How Class Incremental Learning Evaluation Deceives You?", "comment": null, "summary": "Class Incremental Learning (CIL) requires models to continuously learn new\nclasses without forgetting previously learned ones, while maintaining stable\nperformance across all possible class sequences. In real-world settings, the\norder in which classes arrive is diverse and unpredictable, and model\nperformance can vary substantially across different sequences. Yet mainstream\nevaluation protocols calculate mean and variance from only a small set of\nrandomly sampled sequences. Our theoretical analysis and empirical results\ndemonstrate that this sampling strategy fails to capture the full performance\nrange, resulting in biased mean estimates and a severe underestimation of the\ntrue variance in the performance distribution. We therefore contend that a\nrobust CIL evaluation protocol should accurately characterize and estimate the\nentire performance distribution. To this end, we introduce the concept of\nextreme sequences and provide theoretical justification for their crucial role\nin the reliable evaluation of CIL. Moreover, we observe a consistent positive\ncorrelation between inter-task similarity and model performance, a relation\nthat can be leveraged to guide the search for extreme sequences. Building on\nthese insights, we propose EDGE (Extreme case-based Distribution and\nGeneralization Evaluation), an evaluation protocol that adaptively identifies\nand samples extreme class sequences using inter-task similarity, offering a\ncloser approximation of the ground-truth performance distribution. Extensive\nexperiments demonstrate that EDGE effectively captures performance extremes and\nyields more accurate estimates of distributional boundaries, providing\nactionable insights for model selection and robustness checking. Our code is\navailable at https://github.com/AIGNLAI/EDGE.", "AI": {"tldr": "\u63d0\u51fa\u4e86EDGE\u8bc4\u4f30\u534f\u8bae\uff0c\u901a\u8fc7\u8bc6\u522b\u6781\u7aef\u7c7b\u522b\u5e8f\u5217\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u6027\u80fd\u5206\u5e03\uff0c\u89e3\u51b3\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u4f4e\u4f30\u6027\u80fd\u65b9\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfCIL\u8bc4\u4f30\u4ec5\u4ece\u5c11\u91cf\u968f\u673a\u5e8f\u5217\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u65e0\u6cd5\u6355\u6349\u5b8c\u6574\u6027\u80fd\u8303\u56f4\uff0c\u5bfc\u81f4\u5747\u503c\u4f30\u8ba1\u504f\u5dee\u548c\u65b9\u5dee\u4e25\u91cd\u4f4e\u4f30\u3002", "method": "\u5f15\u5165\u6781\u7aef\u5e8f\u5217\u6982\u5ff5\uff0c\u5229\u7528\u4efb\u52a1\u95f4\u76f8\u4f3c\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u6b63\u76f8\u5173\u6027\uff0c\u63d0\u51faEDGE\u534f\u8bae\u81ea\u9002\u5e94\u8bc6\u522b\u548c\u91c7\u6837\u6781\u7aef\u7c7b\u522b\u5e8f\u5217\u3002", "result": "EDGE\u80fd\u6709\u6548\u6355\u6349\u6027\u80fd\u6781\u503c\uff0c\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u5206\u5e03\u8fb9\u754c\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u9c81\u68d2\u6027\u68c0\u67e5\u63d0\u4f9b\u53ef\u884c\u89c1\u89e3\u3002", "conclusion": "EDGE\u534f\u8bae\u901a\u8fc7\u6781\u7aef\u5e8f\u5217\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684CIL\u6027\u80fd\u5206\u5e03\u8868\u5f81\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.75f91a45", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmeks.quest%2Fblogs%2Fthe-theatre-of-pull-requests-and-code-review%3Futm_source=tldrwebdev/1/0100019985b71945-17cdbf01-b8a8-4388-b341-aaf2a3ad26b4-000000/26A4XVFXO3ckwRPvGt4m-CumAfjndHktaFeu9pxBcIo=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmeks.quest%2Fblogs%2Fthe-theatre-of-pull-requests-and-code-review%3Futm_source=tldrwebdev/1/0100019985b71945-17cdbf01-b8a8-4388-b341-aaf2a3ad26b4-000000/26A4XVFXO3ckwRPvGt4m-CumAfjndHktaFeu9pxBcIo=424", "authors": ["TLDR Newsletter"], "title": "The Theatre of Pull Requests and Code Review", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmeks.quest%2Fblogs%2Fthe-theatre-of-pull-requests-and-code-review%3Futm_source=tldrwebdev/1/0100019985b71945-17cdbf01-b8a8-4388-b341-aaf2a3ad26b4-000000/26A4XVFXO3ckwRPvGt4m-CumAfjndHktaFeu9pxBcIo=424", "summary": "The Theatre of Pull Requests and Code Review (6 minute read) PRs are often too large and complex, leading to shallow reviews and code quality issues. Smaller, focused PRs (under 300 lines of code) and commits tell a story that improves reviewability and understanding.", "source": "tldr", "AI": {"tldr": "PRs\u7ecf\u5e38\u8fc7\u5927\u548c\u590d\u6742\uff0c\u5bfc\u81f4\u6d45\u5c42\u5ba1\u67e5\u548c\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\u3002\u5c0f\u800c\u4e13\u6ce8\u7684PR\uff08\u5c11\u4e8e300\u884c\u4ee3\u7801\uff09\u548c\u63d0\u4ea4\u901a\u8fc7\u8bb2\u8ff0\u6545\u4e8b\u6765\u63d0\u9ad8\u53ef\u5ba1\u67e5\u6027\u548c\u7406\u89e3\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578bPR\u5bfc\u81f4\u7684\u4ee3\u7801\u5ba1\u67e5\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u9ad8\u4ee3\u7801\u5ba1\u67e5\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "method": "\u63d0\u5021\u4f7f\u7528\u5c0f\u578b\u3001\u4e13\u6ce8\u7684PR\uff08\u5c11\u4e8e300\u884c\u4ee3\u7801\uff09\u548c\u63d0\u4ea4\uff0c\u901a\u8fc7\u8bb2\u8ff0\u5f00\u53d1\u6545\u4e8b\u7684\u65b9\u5f0f\u6765\u7ec4\u7ec7\u4ee3\u7801\u53d8\u66f4\u3002", "result": "\u5c0f\u578bPR\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u5ba1\u67e5\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4f7f\u5ba1\u67e5\u8005\u66f4\u5bb9\u6613\u7406\u89e3\u548c\u8bc4\u4f30\u4ee3\u7801\u53d8\u66f4\u3002", "conclusion": "\u91c7\u7528\u5c0f\u578b\u3001\u4e13\u6ce8\u7684PR\u7b56\u7565\u662f\u6539\u5584\u4ee3\u7801\u5ba1\u67e5\u8fc7\u7a0b\u548c\u4ee3\u7801\u8d28\u91cf\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "tldr.2509.de8fa36c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fdesign-context-everywhere-you-build%2F%3Futm_source=tldrdesign/1/0100019985ea94b9-5aaa356b-e6f4-472e-a1a5-8e5b9fa91427-000000/a6Z6meTAgMvI6VuNWwb80q_D8EGOJSbVnC-8JcwmpiY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fdesign-context-everywhere-you-build%2F%3Futm_source=tldrdesign/1/0100019985ea94b9-5aaa356b-e6f4-472e-a1a5-8e5b9fa91427-000000/a6Z6meTAgMvI6VuNWwb80q_D8EGOJSbVnC-8JcwmpiY=424", "authors": ["TLDR Newsletter"], "title": "Design Context, Everywhere You Build", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fdesign-context-everywhere-you-build%2F%3Futm_source=tldrdesign/1/0100019985ea94b9-5aaa356b-e6f4-472e-a1a5-8e5b9fa91427-000000/a6Z6meTAgMvI6VuNWwb80q_D8EGOJSbVnC-8JcwmpiY=424", "summary": "Design Context, Everywhere You Build (5 minute read) Figma has announced updates to its MCP server and Code Connect, which bring design context to any development environment, including IDEs, AI agents, and prototypes. The MCP server now supports remote access and works with Figma Make files, while Code Connect introduces in-app component mapping to align design and code components more easily. These tools help teams move from design ideas to production code more quickly by making the design ...", "source": "tldr", "AI": {"tldr": "Figma\u66f4\u65b0\u4e86MCP\u670d\u52a1\u5668\u548cCode Connect\uff0c\u5c06\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u5e26\u5230\u4efb\u4f55\u5f00\u53d1\u73af\u5883\uff0c\u5305\u62ecIDE\u3001AI\u4ee3\u7406\u548c\u539f\u578b\u3002MCP\u670d\u52a1\u5668\u652f\u6301\u8fdc\u7a0b\u8bbf\u95ee\u548cFigma Make\u6587\u4ef6\uff0cCode Connect\u5f15\u5165\u5e94\u7528\u5185\u7ec4\u4ef6\u6620\u5c04\u4ee5\u66f4\u8f7b\u677e\u5730\u5bf9\u9f50\u8bbe\u8ba1\u548c\u4ee3\u7801\u7ec4\u4ef6\u3002", "motivation": "\u5e2e\u52a9\u56e2\u961f\u66f4\u5feb\u5730\u4ece\u8bbe\u8ba1\u60f3\u6cd5\u8f6c\u5411\u751f\u4ea7\u4ee3\u7801\uff0c\u901a\u8fc7\u4f7f\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u5728\u5f00\u53d1\u73af\u5883\u4e2d\u968f\u5904\u53ef\u7528\u3002", "method": "\u66f4\u65b0MCP\u670d\u52a1\u5668\u652f\u6301\u8fdc\u7a0b\u8bbf\u95ee\u548cFigma Make\u6587\u4ef6\uff1b\u5f15\u5165Code Connect\u8fdb\u884c\u5e94\u7528\u5185\u7ec4\u4ef6\u6620\u5c04\u3002", "result": "\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u73b0\u5728\u53ef\u4ee5\u5728\u4efb\u4f55\u5f00\u53d1\u73af\u5883\u4e2d\u4f7f\u7528\uff0c\u5305\u62ecIDE\u3001AI\u4ee3\u7406\u548c\u539f\u578b\u3002", "conclusion": "\u8fd9\u4e9b\u5de5\u5177\u901a\u8fc7\u4f7f\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u5728\u5f00\u53d1\u73af\u5883\u4e2d\u968f\u5904\u53ef\u7528\uff0c\u52a0\u901f\u4e86\u4ece\u8bbe\u8ba1\u5230\u4ee3\u7801\u7684\u8f6c\u6362\u8fc7\u7a0b\u3002", "topic": "swe application"}}
{"id": "tldr.2509.dd5ad7b4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fcwm-an-open-weights-llm-for-research-on-code-generation-with-world-models%2F%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/dq1WphzzBT4QA78-3uk_EAmPBc_gsWc6EMwAPrmn7_Y=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fcwm-an-open-weights-llm-for-research-on-code-generation-with-world-models%2F%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/dq1WphzzBT4QA78-3uk_EAmPBc_gsWc6EMwAPrmn7_Y=424", "authors": ["TLDR Newsletter"], "title": "Meta's Open LLM for Code and World Modeling", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fcwm-an-open-weights-llm-for-research-on-code-generation-with-world-models%2F%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/dq1WphzzBT4QA78-3uk_EAmPBc_gsWc6EMwAPrmn7_Y=424", "summary": "Meta's Open LLM for Code and World Modeling (5 minute read) Meta's CWM is a 32B decoder-only LLM trained on code execution traces and reasoning tasks to explore world models in code generation.", "source": "tldr", "AI": {"tldr": "Meta\u5f00\u53d1\u4e86\u4e00\u4e2a32B\u53c2\u6570\u7684CWM\u6a21\u578b\uff0c\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u8f68\u8ff9\u548c\u63a8\u7406\u4efb\u52a1\u8bad\u7ec3\uff0c\u63a2\u7d22\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4e16\u754c\u6a21\u578b", "motivation": "\u63a2\u7d22\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u53ef\u80fd\u6027\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u7406\u89e3\u4ee3\u7801\u6267\u884c\u8fc7\u7a0b\u800c\u4e0d\u4ec5\u4ec5\u662f\u6587\u672c\u6a21\u5f0f", "method": "\u4f7f\u752832B\u53c2\u6570\u7684\u4ec5\u89e3\u7801\u5668LLM\uff0c\u5728\u4ee3\u7801\u6267\u884c\u8f68\u8ff9\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3", "result": "\u5f00\u53d1\u4e86CWM\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u4ee3\u7801\u6267\u884c\u76f8\u5173\u7684\u4e16\u754c\u5efa\u6a21\u4efb\u52a1", "conclusion": "\u4ee3\u7801\u6267\u884c\u8f68\u8ff9\u8bad\u7ec3\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u4ee3\u7801\u751f\u6210\u4e16\u754c\u6a21\u578b", "topic": "code agent"}}
{"id": "tldr.2509.3dab8416", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.firstround.com%2Fai%2Fbrex%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/oxHEEC2ApxcgpzJvhd5HAK6k5JEvI0Sd41Kauq84580=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.firstround.com%2Fai%2Fbrex%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/oxHEEC2ApxcgpzJvhd5HAK6k5JEvI0Sd41Kauq84580=424", "authors": ["TLDR Newsletter"], "title": "Agent + Human Ops: How AI is Changing Roles and Workflows at Brex", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.firstround.com%2Fai%2Fbrex%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/oxHEEC2ApxcgpzJvhd5HAK6k5JEvI0Sd41Kauq84580=424", "summary": "Agent + Human Ops: How AI is Changing Roles and Workflows at Brex (4 minute read) Brex's CTO built an internal AI platform that lets any employee spin up agents in minutes, turning the company's COO's operations team from people managers into agent managers. The shift is radical - disputes that took 3 hours now take 3 seconds, AI does 100% of quality assurance on all interactions, and over 50% of customer support is fully automated. Most L1 roles managing basic tasks are gone, replaced by L2s...", "source": "tldr", "AI": {"tldr": "Brex\u516c\u53f8\u901a\u8fc7\u5185\u90e8AI\u5e73\u53f0\u8ba9\u5458\u5de5\u5feb\u901f\u521b\u5efa\u667a\u80fd\u4ee3\u7406\uff0c\u5c06\u8fd0\u8425\u56e2\u961f\u4ece\u4eba\u5458\u7ba1\u7406\u8005\u8f6c\u53d8\u4e3a\u4ee3\u7406\u7ba1\u7406\u8005\uff0c\u5927\u5e45\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u901a\u8fc7AI\u6280\u672f\u81ea\u52a8\u5316\u57fa\u7840\u4efb\u52a1\uff0c\u5c06\u4eba\u529b\u8d44\u6e90\u91cd\u65b0\u5206\u914d\u5230\u66f4\u9ad8\u4ef7\u503c\u7684\u5de5\u4f5c\u4e2d\uff0c\u63d0\u5347\u6574\u4f53\u8fd0\u8425\u6548\u7387\u3002", "method": "\u6784\u5efa\u5185\u90e8AI\u5e73\u53f0\uff0c\u8ba9\u5458\u5de5\u80fd\u591f\u5feb\u901f\u521b\u5efa\u548c\u7ba1\u7406\u667a\u80fd\u4ee3\u7406\uff0c\u81ea\u52a8\u5316\u5ba2\u6237\u652f\u6301\u3001\u8d28\u91cf\u4fdd\u8bc1\u7b49\u57fa\u7840\u8fd0\u8425\u4efb\u52a1\u3002", "result": "\u4e89\u8bae\u5904\u7406\u65f6\u95f4\u4ece3\u5c0f\u65f6\u7f29\u77ed\u52303\u79d2\uff0c100%\u7684\u4ea4\u4e92\u8d28\u91cf\u4fdd\u8bc1\u7531AI\u5b8c\u6210\uff0c\u8d85\u8fc750%\u7684\u5ba2\u6237\u652f\u6301\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5927\u591a\u6570L1\u57fa\u7840\u4efb\u52a1\u89d2\u8272\u88abL2\u89d2\u8272\u53d6\u4ee3\u3002", "conclusion": "AI\u4ee3\u7406\u6b63\u5728\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4f01\u4e1a\u89d2\u8272\u548c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u4eba\u529b\u8d44\u6e90\u4ece\u91cd\u590d\u6027\u4efb\u52a1\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u4e13\u6ce8\u4e8e\u66f4\u9ad8\u4ef7\u503c\u7684\u6218\u7565\u5de5\u4f5c\u3002", "topic": "swe application"}}
{"id": "tldr.2509.2023d623", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgian.io%2Fblog-why-georgian-invested-in-replit%2F%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/GqeUEt-8CVQBdB03rCYlpPbIt0jQKrJD9NjF4Hvb7Ag=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgian.io%2Fblog-why-georgian-invested-in-replit%2F%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/GqeUEt-8CVQBdB03rCYlpPbIt0jQKrJD9NjF4Hvb7Ag=424", "authors": ["TLDR Newsletter"], "title": "Why Georgian Invested in Replit", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgian.io%2Fblog-why-georgian-invested-in-replit%2F%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/GqeUEt-8CVQBdB03rCYlpPbIt0jQKrJD9NjF4Hvb7Ag=424", "summary": "Why Georgian Invested in Replit (6 minute read) Georgian invested in Replit's $250 million Series C round as Replit launched its AI-driven Agent 3, enhancing cloud-based software development. Replit democratizes coding by enabling users to create applications via natural language prompts, supporting over 50 languages and featuring AI assistance, real-time collaboration, and one-click deployment. With rapid revenue growth, strategic partnerships, and a versatile platform, Replit is poised to d...", "source": "tldr", "AI": {"tldr": "Georgian\u6295\u8d44Replit\u76842.5\u4ebf\u7f8e\u5143C\u8f6e\u878d\u8d44\uff0c\u652f\u6301\u5176\u63a8\u51faAI\u9a71\u52a8\u7684Agent 3\uff0c\u589e\u5f3a\u4e91\u7aef\u8f6f\u4ef6\u5f00\u53d1\u80fd\u529b", "motivation": "Replit\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8ba9\u7528\u6237\u521b\u5efa\u5e94\u7528\uff0c\u652f\u630150\u591a\u79cd\u8bed\u8a00\uff0c\u63d0\u4f9bAI\u8f85\u52a9\u3001\u5b9e\u65f6\u534f\u4f5c\u548c\u4e00\u952e\u90e8\u7f72\uff0c\u65e8\u5728\u6c11\u4e3b\u5316\u7f16\u7a0b", "method": "\u5f00\u53d1\u4e91\u7aef\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff0c\u6574\u5408AI\u52a9\u624b\u548c\u534f\u4f5c\u5de5\u5177\uff0c\u7b80\u5316\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b", "result": "\u5b9e\u73b0\u5feb\u901f\u6536\u5165\u589e\u957f\uff0c\u5efa\u7acb\u6218\u7565\u5408\u4f5c\u4f19\u4f34\u5173\u7cfb\uff0c\u6253\u9020\u591a\u529f\u80fd\u5e73\u53f0", "conclusion": "Replit\u6709\u671b\u98a0\u8986\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u6a21\u5f0f\uff0c\u6210\u4e3a\u884c\u4e1a\u9886\u5bfc\u8005", "topic": "swe application"}}
{"id": "wechat.2509.718bf72a", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0NDUwNzYzMg==&mid=2247495899&idx=1&sn=070d8fd29d8090870f5eb1b6881eb7d8&chksm=e8e94f670844e2ae4c22ac5a6fa33bae1df3c1e9ba49634e0786bb0ad1724dbdd65895a4b52c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0NDUwNzYzMg==&mid=2247495899&idx=1&sn=070d8fd29d8090870f5eb1b6881eb7d8&chksm=e8e94f670844e2ae4c22ac5a6fa33bae1df3c1e9ba49634e0786bb0ad1724dbdd65895a4b52c#rd", "authors": ["\u5177\u8eab\u667a\u80fd\u5927\u672c\u8425"], "title": "NeurIPS 2025 | \u6e05\u534e\u63d0\u51faRLVR-World\uff1a\u5229\u7528<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\uff0c\u63d0\u5347\u591a\u6a21\u6001\u9884\u6d4b\u6027\u80fd\uff01", "comment": "Source: WeChat, Published: 2025-09-29 13:28:26", "summary": "\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5 \u91c7\u7528\u300c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u300d \u7b97\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u4e00\u7ec4\u9884\u6d4b\u7ed3\u679c\u5e76\u8ba1\u7b97\u76f8\u5bf9\u5956\u52b1\u4f18\u52bf\uff0c\u907f\u514d\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u504f\u5dee\uff0c\u4ec5\u9700\u300c\u6570\u767e\u6b21\u68af\u5ea6\u66f4\u65b0\u300d\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8fdc\u4f4e\u4e8eMLE\u6240\u9700\u7684\u6570\u5341\u4e07\u6b21\u8fed\u4ee3\u3002", "AI": {"tldr": "\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5 \u91c7\u7528\u300c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u300d \u7b97\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u4e00\u7ec4\u9884\u6d4b\u7ed3\u679c\u5e76\u8ba1\u7b97\u76f8\u5bf9\u5956\u52b1\u4f18\u52bf\uff0c\u907f\u514d\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u504f\u5dee\uff0c\u4ec5\u9700\u300c\u6570\u767e\u6b21\u68af\u5ea6\u66f4\u65b0\u300d\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8fdc\u4f4e\u4e8eMLE\u6240\u9700\u7684\u6570\u5341\u4e07\u6b21\u8fed\u4ee3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.6ae0ab0b", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652631110&idx=3&sn=66ff00dab8a3951f1183f08d9af96421&chksm=f051df7fd6ba881c2bb2d42949d42913d2e7159003e874c0373fa383c58f3566f27513f7ec9b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652631110&idx=3&sn=66ff00dab8a3951f1183f08d9af96421&chksm=f051df7fd6ba881c2bb2d42949d42913d2e7159003e874c0373fa383c58f3566f27513f7ec9b#rd", "authors": ["\u65b0\u667a\u5143"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u7236\u7ed9LLM\u5224\u6b7b\u5211\uff01\u7ad9\u961fLeCun\uff1a\u6211\u4eec\u5168\u641e\u9519\u4e86", "comment": "Source: WeChat, Published: 2025-09-29 09:46:27", "summary": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6709\u6b63\u786e\u7684\u8bdd\u8bed\u8981\u8bf4\uff0c\u6709\u6b63\u786e\u7684\u52a8\u4f5c\u8981\u505a\uff0c\u6b63\u786e\u7684\u4e8b\u5c31\u662f\u80fd\u591f\u83b7\u5f97\u5956\u52b1\u7684\u4e8b\u3002\u6211\u4eec\u5bf9\u6b63\u786e\u7684\u4e8b\u662f\u6709\u5b9a\u4e49\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u9884\u5148\u638c\u63e1\u6216\u901a\u8fc7\u4ed6\u4eba\u83b7\u53d6\u5173\u4e8e\u6b63\u786e\u7684\u4e8b\u7684\u77e5\u8bc6\u3002", "AI": {"tldr": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6709\u6b63\u786e\u7684\u8bdd\u8bed\u8981\u8bf4\uff0c\u6709\u6b63\u786e\u7684\u52a8\u4f5c\u8981\u505a\uff0c\u6b63\u786e\u7684\u4e8b\u5c31\u662f\u80fd\u591f\u83b7\u5f97\u5956\u52b1\u7684\u4e8b\u3002\u6211\u4eec\u5bf9\u6b63\u786e\u7684\u4e8b\u662f\u6709\u5b9a\u4e49\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u9884\u5148\u638c\u63e1\u6216\u901a\u8fc7\u4ed6\u4eba\u83b7\u53d6\u5173\u4e8e\u6b63\u786e\u7684\u4e8b\u7684\u77e5\u8bc6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.4be80808", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5MTE0NTcxOQ==&mid=2247708524&idx=3&sn=976cb8c589ff73fc9dcc4b01d206efd4&chksm=ce86277f91f00cff8952e64f8cd38eb9734fb0d26d72fd47affac9945267d178d9e3235ab2da#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5MTE0NTcxOQ==&mid=2247708524&idx=3&sn=976cb8c589ff73fc9dcc4b01d206efd4&chksm=ce86277f91f00cff8952e64f8cd38eb9734fb0d26d72fd47affac9945267d178d9e3235ab2da#rd", "authors": ["Ai\u5c1a\u7814\u4fee"], "title": "\u8bba\u6587\u63a8\u9001 | \u57fa\u4e8e\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u57ce\u5e02\u591a\u4ed3\u5e93\u7269\u6d41\u4f18\u5316\u6a21\u578b\u7814\u7a76", "comment": "Source: WeChat, Published: 2025-09-29 09:32:56", "summary": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u6574\u4f53\u6280\u672f\u6846\u67b6\u5982\u56fe1\u6240\u793a\u3002\u9996\u5148\uff0c\u5728\u9759\u6001\u573a\u666f\u4e0b\u6784\u5efaDTM-MDVRP\uff0c\u8be5\u6a21\u578b\u5728\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u8282\u70b9\u95f4\u7684\u8fb9\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5230Transformer\u7684\u591a\u5934\u6ce8\u610f\u529b\u5c42\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8def\u5f84\u9884\u89c4\u5212\u3002", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u6574\u4f53\u6280\u672f\u6846\u67b6\u5982\u56fe1\u6240\u793a\u3002\u9996\u5148\uff0c\u5728\u9759\u6001\u573a\u666f\u4e0b\u6784\u5efaDTM-MDVRP\uff0c\u8be5\u6a21\u578b\u5728\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u8282\u70b9\u95f4\u7684\u8fb9\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5230Transformer\u7684\u591a\u5934\u6ce8\u610f\u529b\u5c42\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8def\u5f84\u9884\u89c4\u5212\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.30b4b80e", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1MjU4NjUzMg==&mid=2247484511&idx=1&sn=a8c820c239155a7761a474fc5f642f57&chksm=e80e248b48069d748d3fd812715d1ca15b3c7bf0ceb6ae409651c389f46e0818df99432298d2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1MjU4NjUzMg==&mid=2247484511&idx=1&sn=a8c820c239155a7761a474fc5f642f57&chksm=e80e248b48069d748d3fd812715d1ca15b3c7bf0ceb6ae409651c389f46e0818df99432298d2#rd", "authors": ["\u5357\u590f\u7684\u7b97\u6cd5\u9a7f\u7ad9"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1a\u63ed\u9732\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u5728\u4ea4\u4e92\u4e2d\u8fdb\u5316\u7684\u5965\u79d8", "comment": "Source: WeChat, Published: 2025-09-29 01:00:00", "summary": "01\uff5c\u4ec0\u4e48\u662f\u5f3a\u5316\u5b66\u4e60\u5f3a\u5316\u5b66\u4e60\u662f\u673a\u5668\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u5b9e\u73b0\u76ee\u6807\u7684\u4e00\u79cd\u8ba1\u7b97\u65b9\u6cd5\u3002\u5b83\u6a21\u62df\u4e86\u4eba\u7c7b\u5b66\u4e60\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5c1d\u8bd5\u548c\u9519\u8bef\uff0c\u6700\u7ec8\u627e\u5230\u89e3\u51b3\u95ee\u9898\u7684\u6700\u4f73\u65b9\u6cd5\u3002", "AI": {"tldr": "01\uff5c\u4ec0\u4e48\u662f\u5f3a\u5316\u5b66\u4e60\u5f3a\u5316\u5b66\u4e60\u662f\u673a\u5668\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u5b9e\u73b0\u76ee\u6807\u7684\u4e00\u79cd\u8ba1\u7b97\u65b9\u6cd5\u3002\u5b83\u6a21\u62df\u4e86\u4eba\u7c7b\u5b66\u4e60\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5c1d\u8bd5\u548c\u9519\u8bef\uff0c\u6700\u7ec8\u627e\u5230\u89e3\u51b3\u95ee\u9898\u7684\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.5e7dff1a", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzOTE3Nzc5MA==&mid=2247549922&idx=1&sn=e93dc12f2c3bef016f71aa0e5390712c&chksm=c33cb4644c1488d5ce3889d81186ee4fc85e982a1191ae03d50dfc7021a2ef45c57a30e36445#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzOTE3Nzc5MA==&mid=2247549922&idx=1&sn=e93dc12f2c3bef016f71aa0e5390712c&chksm=c33cb4644c1488d5ce3889d81186ee4fc85e982a1191ae03d50dfc7021a2ef45c57a30e36445#rd", "authors": ["\u667a\u80fd\u8f66\u53c2\u8003"], "title": "Momenta\u7528<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u91cd\u5851\u7aef\u5230\u7aef\u4e0a\u8f66\uff0c\u771f\u7684\u6bd4VLA\u66f4\u597d\u5417\uff1f", "comment": "Source: WeChat, Published: 2025-09-29 00:01:00", "summary": "\u5f3a\u5316\u5b66\u4e60\u5230\u5e95\u662f\u4ec0\u4e48\uff1f\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u7684\u8d77\u6e90\u53ef\u8ffd\u6eaf\u523020\u4e16\u7eaa50\u5e74\u4ee3\uff0c\u5176\u7406\u8bba\u57fa\u7840\u6e90\u4e8e\u884c\u4e3a\u5fc3\u7406\u5b66\u548c\u63a7\u5236\u8bba\u3002\u827e\u4f26\u00b7\u56fe\u7075\u672c\u4eba\u57281950\u5e74\u7684\u8bba\u6587\u300a\u8ba1\u7b97\u673a\u5668\u4e0e\u667a\u80fd\u300b\u4e2d\u63a2\u8ba8\u4e86\u300c\u673a\u5668\u80fd\u601d\u8003\u5417\uff1f", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u5230\u5e95\u662f\u4ec0\u4e48\uff1f\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u7684\u8d77\u6e90\u53ef\u8ffd\u6eaf\u523020\u4e16\u7eaa50\u5e74\u4ee3\uff0c\u5176\u7406\u8bba\u57fa\u7840\u6e90\u4e8e\u884c\u4e3a\u5fc3\u7406\u5b66\u548c\u63a7\u5236\u8bba\u3002\u827e\u4f26\u00b7\u56fe\u7075\u672c\u4eba\u57281950\u5e74\u7684\u8bba\u6587\u300a\u8ba1\u7b97\u673a\u5668\u4e0e\u667a\u80fd\u300b\u4e2d\u63a2\u8ba8\u4e86\u300c\u673a\u5668\u80fd\u601d\u8003\u5417\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.88925b70", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNDcyODE5Mw==&mid=2247485470&idx=1&sn=53fceee1623783f6b1b6ccd345ac08da&chksm=c0cd5959dc5d167ca19e12c02f3cb5dcc8570056fa9dfe5f49b76bbd2372e74f3ec1371cb549#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNDcyODE5Mw==&mid=2247485470&idx=1&sn=53fceee1623783f6b1b6ccd345ac08da&chksm=c0cd5959dc5d167ca19e12c02f3cb5dcc8570056fa9dfe5f49b76bbd2372e74f3ec1371cb549#rd", "authors": ["\u6e29\u5ba4AI\u7b14\u8bb0"], "title": "ArXiv  | \u6e05\u534e117\u9875\u7efc\u8ff0\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5982\u4f55\u5851\u9020\u5927\u578b\u63a8\u7406\u6a21\u578b\uff082025.09\u6700\u65b0\uff09", "comment": "Source: WeChat, Published: 2025-09-28 19:16:44", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6700\u521d\u56e0\u5176\u5728\u4eba\u7c7b\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u800c\u5907\u53d7\u77a9\u76ee\uff0c\u5176\u4e2d\u4ee5\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4e3a\u4ee3\u8868\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6709\u76ca\u6027\u3001\u8bda\u5b9e\u6027\u548c\u65e0\u5bb3\u6027\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6700\u521d\u56e0\u5176\u5728\u4eba\u7c7b\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u800c\u5907\u53d7\u77a9\u76ee\uff0c\u5176\u4e2d\u4ee5\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4e3a\u4ee3\u8868\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6709\u76ca\u6027\u3001\u8bda\u5b9e\u6027\u548c\u65e0\u5bb3\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.fe6a9224", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570695&idx=3&sn=c949e3021e7bcc8e7b148e3666387fe8&chksm=96d443fa287aae948c7272ce74749eb542c782b7afae0525163de009f04ee5608949a4ece3ca#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570695&idx=3&sn=c949e3021e7bcc8e7b148e3666387fe8&chksm=96d443fa287aae948c7272ce74749eb542c782b7afae0525163de009f04ee5608949a4ece3ca#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "\u9648\u4e39\u7426\u65b0\u4f5c\uff1a\u5927\u6a21\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u7b2c\u4e09\u6761\u8def\uff0c8B\u5c0f\u6a21\u578b\u8d85\u8d8aGPT-4o", "comment": "Source: WeChat, Published: 2025-09-28 16:02:46", "summary": "\u7f51\u53cb\u89c9\u5f97\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u8bbe\u5b9a\u4e86\u4e00\u4e2a\u65b0\u57fa\u7ebf\uff1a\u8c01\u5236\u5b9a\u4e86\u504f\u597d\u7684\u5b9a\u4e49\uff0c\u8c01\u5c31\u662f\u540e\u8bad\u7ec3\u65f6\u4ee3\u7684\u201c\u65b0\u5f97\u5206\u624b\u201d\u3002jhxhgukvcxx @jhxhgukvcxx \u00b7 sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo", "AI": {"tldr": "\u7f51\u53cb\u89c9\u5f97\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u8bbe\u5b9a\u4e86\u4e00\u4e2a\u65b0\u57fa\u7ebf\uff1a\u8c01\u5236\u5b9a\u4e86\u504f\u597d\u7684\u5b9a\u4e49\uff0c\u8c01\u5c31\u662f\u540e\u8bad\u7ec3\u65f6\u4ee3\u7684\u201c\u65b0\u5f97\u5206\u624b\u201d\u3002jhxhgukvcxx @jhxhgukvcxx \u00b7 sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.34c03851", "categories": ["wechat.article", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3OTU5NTI0Mw==&mid=2687303086&idx=2&sn=740a716e852dc1769e2b2d8c71b8f463&chksm=bb90afbd8d94f97ec54babe216f076a9d5f881e9a79e6d5c4d594fdadd70bcd26182d5e4cd68#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3OTU5NTI0Mw==&mid=2687303086&idx=2&sn=740a716e852dc1769e2b2d8c71b8f463&chksm=bb90afbd8d94f97ec54babe216f076a9d5f881e9a79e6d5c4d594fdadd70bcd26182d5e4cd68#rd", "authors": ["\u6e05\u534e\u6559\u80b2\u521b\u65b0"], "title": "DeepSeek-R1\u767bNature\u5c01\u9762\uff1a\u901a\u8fc7<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6fc0\u52b1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406", "comment": "Source: WeChat, Published: 2025-09-28 15:57:15", "summary": "\u4e00\u3001\u7528\u5f3a\u5316\u5b66\u4e60\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u4f9d\u8d56 \u957f\u671f\u4ee5\u6765\uff0cLLM\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u9ad8\u5ea6\u4f9d\u8d56\u201c\u76d1\u7763\u5fae\u8c03\u201d\uff08SFT\uff09\uff0c\u5373\u901a\u8fc7\u4eba\u5de5\u7f16\u5199\u63a8\u7406\u94fe\uff08\u5982\u201c\u601d\u7ef4\u94feCoT\u201d\uff09\u4e3a\u6a21\u578b\u63d0\u4f9b\u201c\u811a\u624b\u67b6\u201d\u3002", "AI": {"tldr": "\u4e00\u3001\u7528\u5f3a\u5316\u5b66\u4e60\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u4f9d\u8d56 \u957f\u671f\u4ee5\u6765\uff0cLLM\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u9ad8\u5ea6\u4f9d\u8d56\u201c\u76d1\u7763\u5fae\u8c03\u201d\uff08SFT\uff09\uff0c\u5373\u901a\u8fc7\u4eba\u5de5\u7f16\u5199\u63a8\u7406\u94fe\uff08\u5982\u201c\u601d\u7ef4\u94feCoT\u201d\uff09\u4e3a\u6a21\u578b\u63d0\u4f9b\u201c\u811a\u624b\u67b6\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.b4d1114d", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMTkyMzQxNg==&mid=2247484496&idx=1&sn=7ddc6f8cc9998fec130fc318021709ff&chksm=c3f0364053f5c371eafee67edaa059583667e6d4160a824c5c4ea42c4bc897a368f0c50dd7bb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMTkyMzQxNg==&mid=2247484496&idx=1&sn=7ddc6f8cc9998fec130fc318021709ff&chksm=c3f0364053f5c371eafee67edaa059583667e6d4160a824c5c4ea42c4bc897a368f0c50dd7bb#rd", "authors": ["\u7231\u8dd1\u6b65\u7684\u65f6\u95f4"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e0e\u4eba\u751f", "comment": "Source: WeChat, Published: 2025-09-28 15:36:22", "summary": "\u5f3a\u5316\u5b66\u4e60\u6982\u62ec\u8d77\u6765\u5206\u4e3a\u4e94\u90e8\u5206\uff1a1. \u751f\u5b58\u73af\u5883\uff1b2. \u53cd\u9988\u673a\u5236\uff1b3. \u6267\u884c\u7b56\u7565\uff1b4. \u8bad\u7ec3\u65b9\u5f0f\uff1b5. \u90e8\u7f72\u8fd0\u884c\uff1b\u5b8c\u5168\u5c31\u662f\u6a21\u4eff\u4eba\u7c7b\u7684\u751f\u5b58\u6a21\u5f0f\uff0c\u53ea\u662f\u7528\u8fd9\u79cd\u65b9\u5f0f\u4e0d\u4ec5\u7528\u6570\u5b66\u63cf\u8ff0\u4e86\u6267\u884c\u7ec6\u8282\uff0c\u800c\u4e14\u8ba9\u8ba1\u7b97\u673a\u5b66\u4f1a\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u80fd\u529b\uff0c\u8bf4\u660e\u8fd9\u5957\u65b9", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u6982\u62ec\u8d77\u6765\u5206\u4e3a\u4e94\u90e8\u5206\uff1a1. \u751f\u5b58\u73af\u5883\uff1b2. \u53cd\u9988\u673a\u5236\uff1b3. \u6267\u884c\u7b56\u7565\uff1b4. \u8bad\u7ec3\u65b9\u5f0f\uff1b5. \u90e8\u7f72\u8fd0\u884c\uff1b\u5b8c\u5168\u5c31\u662f\u6a21\u4eff\u4eba\u7c7b\u7684\u751f\u5b58\u6a21\u5f0f\uff0c\u53ea\u662f\u7528\u8fd9\u79cd\u65b9\u5f0f\u4e0d\u4ec5\u7528\u6570\u5b66\u63cf\u8ff0\u4e86\u6267\u884c\u7ec6\u8282\uff0c\u800c\u4e14\u8ba9\u8ba1\u7b97\u673a\u5b66\u4f1a\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u80fd\u529b\uff0c\u8bf4\u660e\u8fd9\u5957\u65b9", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.18e91116", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNjk4NjM1MA==&mid=2247717536&idx=1&sn=ace44d085d26eedadf1d184c1ccc1d8c&chksm=fb71d53f85611b508e9aef2bc8a23bed19d7e6b8ee21f508275fae949d43a9f4585ee9222c7a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNjk4NjM1MA==&mid=2247717536&idx=1&sn=ace44d085d26eedadf1d184c1ccc1d8c&chksm=fb71d53f85611b508e9aef2bc8a23bed19d7e6b8ee21f508275fae949d43a9f4585ee9222c7a#rd", "authors": ["\u534e\u4e3a\u8ba1\u7b97"], "title": "\u521b\u667a\u8054\u5408\u534e\u4e3a\u6253\u9020\u65b0\u4e00\u4ee3<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6846\u67b6siiRL\uff0c\u9996\u6b21\u5b9e\u73b0\u6607\u817e\u5343\u5361\u96c6\u7fa4\u9ad8\u6548\u6269\u5c55", "comment": "Source: WeChat, Published: 2025-09-28 14:20:38", "summary": "\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u5f53\u524d\u901a\u5f80\u66f4\u9ad8\u9636AI\u80fd\u529b\u7684\u5173\u952e\u6280\u672f\u4e4b\u4e00\uff0c\u4f46\u5176\u53d1\u5c55\u4e00\u76f4\u53d7\u9650\u4e8e\u4e3b\u6d41\u6846\u67b6\u5728\u5e95\u5c42\u67b6\u6784\u4e0a\u7684\u74f6\u9888\u3002\u5f53\u8bad\u7ec3\u89c4\u6a21\u6269\u5c55\u81f3\u6210\u767e\u4e0a\u5343\u7684\u8ba1\u7b97\u8282\u70b9\u65f6\uff0c\u4f20\u7edf\u6846\u67b6\u7684\u6027\u80fd\u5f80\u5f80\u4f1a\u6025\u5267\u4e0b\u964d\uff0c\u5176\u6839\u6e90\u5728\u4e8e\u5176\u56fa\u6709\u7684\u201c\u4e2d\u5fc3\u5316\u201d", "AI": {"tldr": "\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u5f53\u524d\u901a\u5f80\u66f4\u9ad8\u9636AI\u80fd\u529b\u7684\u5173\u952e\u6280\u672f\u4e4b\u4e00\uff0c\u4f46\u5176\u53d1\u5c55\u4e00\u76f4\u53d7\u9650\u4e8e\u4e3b\u6d41\u6846\u67b6\u5728\u5e95\u5c42\u67b6\u6784\u4e0a\u7684\u74f6\u9888\u3002\u5f53\u8bad\u7ec3\u89c4\u6a21\u6269\u5c55\u81f3\u6210\u767e\u4e0a\u5343\u7684\u8ba1\u7b97\u8282\u70b9\u65f6\uff0c\u4f20\u7edf\u6846\u67b6\u7684\u6027\u80fd\u5f80\u5f80\u4f1a\u6025\u5267\u4e0b\u964d\uff0c\u5176\u6839\u6e90\u5728\u4e8e\u5176\u56fa\u6709\u7684\u201c\u4e2d\u5fc3\u5316\u201d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.b11c31a5", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMzgyNzQ0NQ==&mid=2247484034&idx=1&sn=6934442065ee1c87ba364382f8290eb2&chksm=e90bc9118011d89415570134a749cb274818843fd1d6da8e9cd0e571d9c44e8d962a7301174b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMzgyNzQ0NQ==&mid=2247484034&idx=1&sn=6934442065ee1c87ba364382f8290eb2&chksm=e90bc9118011d89415570134a749cb274818843fd1d6da8e9cd0e571d9c44e8d962a7301174b#rd", "authors": ["\u7845\u57fa\u6f2b\u8c08"], "title": "AI coding <em class=\"highlight\">code</em> <em class=\"highlight\">agent</em> \u5de5\u5177\u5206\u4eab", "comment": "Source: WeChat, Published: 2025-09-29 10:16:20", "summary": "AI coding code agent \u5de5\u5177\u5206\u4eab1. claude-code https\uff1a//github.com/anthropics/claude-code2. codex https\uff1a//github.com/openai/codex3. gemini-cli https\uff1a//github.com/google-gemini/gemini-cli4. qwen-code https\uff1a//github.com/QwenLM/qwen-code", "AI": {"tldr": "AI coding code agent \u5de5\u5177\u5206\u4eab1. claude-code https\uff1a//github.com/anthropics/claude-code2. codex https\uff1a//github.com/openai/codex3. gemini-cli https\uff1a//github.com/google-gemini/gemini-cli4. qwen-code https\uff1a//g...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.7eefd42b", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2Njg0OTEyNQ==&mid=2247488429&idx=1&sn=6d366206a1aee1be49ce1387d8657302&chksm=fd8c670e83cdb4aa29f48f1554fbf9f8659557d451c288362d382e6964c298f9c0b8988562fc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2Njg0OTEyNQ==&mid=2247488429&idx=1&sn=6d366206a1aee1be49ce1387d8657302&chksm=fd8c670e83cdb4aa29f48f1554fbf9f8659557d451c288362d382e6964c298f9c0b8988562fc#rd", "authors": ["AI\u5f02\u7c7b\u5f17\u5170\u514b"], "title": "\u5e26\u670b\u53cb\u4eec\u91cd\u65b0\u8ba4\u8bc6\u4e00\u4e0b\u8fd9\u4e2a<em class=\"highlight\">Agentic</em> Search\u5e73\u53f0\u2014\u2014\u79d8\u5854AI", "comment": "Source: WeChat, Published: 2025-09-29 12:59:31", "summary": "\u5347\u7ea7\u4e4b\u540e\u7684\u79d8\u5854agentic search\uff0c\u5c31\u597d\u50cf\u5b66\u672f\u4e13\u5bb6\u4e5f\u5b66\u4f1a\u4e86\u8131\u53e3\u79c0\u6574\u6d3b\u3002\u4e0d\u4ec5\u80fd\u9009\u62e9\u8bed\u8a00\u98ce\u683c\uff0c\u8fd9\u6b21\u591a\u4e86\u4e00\u4e2a\u3010\u7f8e\u5316\u8f93\u51fa\u3011\u3002\u5565\u662f\u7f8e\u5316\u8f93\u51fa\uff1f\u522b\u7740\u6025\uff0c\u770b\u7ed3\u679c\u2014\u2014", "AI": {"tldr": "\u5347\u7ea7\u4e4b\u540e\u7684\u79d8\u5854agentic search\uff0c\u5c31\u597d\u50cf\u5b66\u672f\u4e13\u5bb6\u4e5f\u5b66\u4f1a\u4e86\u8131\u53e3\u79c0\u6574\u6d3b\u3002\u4e0d\u4ec5\u80fd\u9009\u62e9\u8bed\u8a00\u98ce\u683c\uff0c\u8fd9\u6b21\u591a\u4e86\u4e00\u4e2a\u3010\u7f8e\u5316\u8f93\u51fa\u3011\u3002\u5565\u662f\u7f8e\u5316\u8f93\u51fa\uff1f\u522b\u7740\u6025\uff0c\u770b\u7ed3\u679c\u2014\u2014", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.497c3fd5", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NDcxMDAzNQ==&mid=2247489240&idx=1&sn=560766b1bee3e87ab27f94fb3e93427b&chksm=cf14aac54c2937ac72800f98682884b7a8cedf1f396e36134abd85269d7e996c21ea4cdd322e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NDcxMDAzNQ==&mid=2247489240&idx=1&sn=560766b1bee3e87ab27f94fb3e93427b&chksm=cf14aac54c2937ac72800f98682884b7a8cedf1f396e36134abd85269d7e996c21ea4cdd322e#rd", "authors": ["\u71d5\u5927\u4fa0\u7684\u6570\u5b57\u5316\u6c5f\u6e56"], "title": "\u8ddf\u5927\u4fa0\u5b66 AI | \uff08\u6df1\u5ea6\u597d\u6587\uff09\u9ea6\u70ab\u98ce\u5927\u4fa0\u7cbe\u9009 \u2014 \u6293\u4f4f <em class=\"highlight\">Agentic</em> AI \u7684\u4f18\u52bf\uff08\u4e0a\uff09", "comment": "Source: WeChat, Published: 2025-09-29 11:00:50", "summary": "\u6293\u4f4f Agentic AI \u7684\u4f18\u52bf\uff08\u4e0a\uff09\uff08seizing the agentic ai advantage\uff09quantumblack ai by mckinsey seizing the agentic ai advantage alexander sukharevsky dave kerr klemens hjartar lari hmalinen st\u00e9phane bout vito di leo guillaume dagorret", "AI": {"tldr": "\u6293\u4f4f Agentic AI \u7684\u4f18\u52bf\uff08\u4e0a\uff09\uff08seizing the agentic ai advantage\uff09quantumblack ai by mckinsey seizing the agentic ai advantage alexander sukharevsky dave kerr klemens hjartar lari hmalinen st\u00e9phane bout vito di l...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.432913a8", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNDQwNDQ0MA==&mid=2247486771&idx=1&sn=43bfdf943deb760de26044855770ce24&chksm=c3721a870ca3c7ae98fe0e76f9c706e8ec2eaf318d99c9b0ae2e22385d557cbbe3b349761a81#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNDQwNDQ0MA==&mid=2247486771&idx=1&sn=43bfdf943deb760de26044855770ce24&chksm=c3721a870ca3c7ae98fe0e76f9c706e8ec2eaf318d99c9b0ae2e22385d557cbbe3b349761a81#rd", "authors": ["\u51fa\u6d77\u7814\u7a76\u9662"], "title": "\u4ece RPA \u5230 <em class=\"highlight\">Agentic</em>\uff1aCuberAI \u5728\u4e1c\u5357\u4e9a\u628a\u201c\u4eba\u6548\u201d\u505a\u6210\u4ea7\u54c1", "comment": "Source: WeChat, Published: 2025-09-29 10:07:27", "summary": "DeepSeek\u7684\u51fa\u73b0\u628a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6210\u672c\u5927\u5e45\u964d\u4f4e\uff0c\u8ba9\u66f4\u591a\u4f01\u4e1a\u80fd\u591f\u8d1f\u62c5\u5f97\u8d77Agentic AI\u7684\u5e94\u7528\u3002\u73b0\u5728\u4ed6\u4eec\u7684\u65b9\u6848\u5df2\u7ecf\u4ece\u7b80\u5355\u7684RPA\u5347\u7ea7\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5904\u7406\u80fd\u529b\u6709\u4e86\u8d28\u7684\u98de\u8dc3\u3002", "AI": {"tldr": "DeepSeek\u7684\u51fa\u73b0\u628a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6210\u672c\u5927\u5e45\u964d\u4f4e\uff0c\u8ba9\u66f4\u591a\u4f01\u4e1a\u80fd\u591f\u8d1f\u62c5\u5f97\u8d77Agentic AI\u7684\u5e94\u7528\u3002\u73b0\u5728\u4ed6\u4eec\u7684\u65b9\u6848\u5df2\u7ecf\u4ece\u7b80\u5355\u7684RPA\u5347\u7ea7\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5904\u7406\u80fd\u529b\u6709\u4e86\u8d28\u7684\u98de\u8dc3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.c9a2a6ca", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&mid=2655929248&idx=1&sn=0a4741a9db3d8f1ada830c6b00d4801c&chksm=bc169fb258db6cf2323c63db70362a6966752b5f2662ce8f2ed2699a877b2724e29097ac0465#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&mid=2655929248&idx=1&sn=0a4741a9db3d8f1ada830c6b00d4801c&chksm=bc169fb258db6cf2323c63db70362a6966752b5f2662ce8f2ed2699a877b2724e29097ac0465#rd", "authors": ["51CTO\u6280\u672f\u6808"], "title": "\u9ea6\u80af\u9521\uff1a\u5982\u4f55\u771f\u6b63\u4ea4\u4ed8\u4e0d\u70c2\u5c3e\u7684 <em class=\"highlight\">Agentic</em> AI\uff1f", "comment": "Source: WeChat, Published: 2025-09-29 09:50:49", "summary": "51cto\u6280\u672f\u6808 agentic ai \u7f16\u8f91 | \u4e91\u662d \u5173\u4e8e Agentic AI\uff0c\u5927\u591a\u6570\u56e2\u961f\u90fd\u505a\u9519\u4e86\uff01\u4f60\u7684Agent\u8bc4\u4f30\u65b9\u5f0f\u53ef\u80fd\u5b8c\u5168\u662f\u5783\u573e\uff01\u4e0a\u5468\uff0c\u9ea6\u80af\u9521\u56e2\u961f\u57fa\u4e8e 50 \u591a\u4e2a\u81ea\u5df1\u7275\u5934\u7684\u4ee3\u7406 AI \u9879\u76ee\uff0c\u4ee5\u53ca\u5e02\u573a\u4e0a\u7684\u6570\u5341\u4e2a\u6848\u4f8b\uff0c\u53d1\u5e03\u4e86\u4e00\u4efd\u73b0\u5b9e\u7814\u7a76\u62a5\u544a\u300a\u901a\u8fc7\u4ee3\u7406AI", "AI": {"tldr": "51cto\u6280\u672f\u6808 agentic ai \u7f16\u8f91 | \u4e91\u662d \u5173\u4e8e Agentic AI\uff0c\u5927\u591a\u6570\u56e2\u961f\u90fd\u505a\u9519\u4e86\uff01\u4f60\u7684Agent\u8bc4\u4f30\u65b9\u5f0f\u53ef\u80fd\u5b8c\u5168\u662f\u5783\u573e\uff01\u4e0a\u5468\uff0c\u9ea6\u80af\u9521\u56e2\u961f\u57fa\u4e8e 50 \u591a\u4e2a\u81ea\u5df1\u7275\u5934\u7684\u4ee3\u7406 AI \u9879\u76ee\uff0c\u4ee5\u53ca\u5e02\u573a\u4e0a\u7684\u6570\u5341\u4e2a\u6848\u4f8b\uff0c\u53d1\u5e03\u4e86\u4e00\u4efd\u73b0\u5b9e\u7814\u7a76\u62a5\u544a\u300a\u901a\u8fc7\u4ee3\u7406AI", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.75c1e037", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNDA4OTIwMA==&mid=2247483793&idx=1&sn=0a8609b49b667733be79085cf4b6601d&chksm=f17b24f5124a39a7188126232849df2637a2e07d821c4e753f73a3a84ca3b8b4927c900d0b52#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNDA4OTIwMA==&mid=2247483793&idx=1&sn=0a8609b49b667733be79085cf4b6601d&chksm=f17b24f5124a39a7188126232849df2637a2e07d821c4e753f73a3a84ca3b8b4927c900d0b52#rd", "authors": ["LargeModel"], "title": "\u62e5\u62b1<em class=\"highlight\">Agentic</em>\u4e4bAI\u539f\u751f\u5e94\u7528\u662f\u4ec0\u4e48", "comment": "Source: WeChat, Published: 2025-09-29 07:51:05", "summary": "\u4f20\u7edf\u7684\u201c\u5fae\u670d\u52a1+\u5173\u952e\u8bcd\u68c0\u7d22+\u5173\u7cfb\u578b\u6570\u636e\u5e93+ CPU \u201d\u7684\u7ec4\u5408\uff0c\u6b63\u88ab\u201c Agent \u667a\u80fd\u4f53+\u8bed\u4e49\u68c0\u7d22+\u77e5\u8bc6\u56fe\u8c31+\u5411\u91cf\u6570\u636e\u5e93 + GPU \u201d\u6240\u53d6\u4ee3\u3002\u7b2c\u56db\u662f\u67b6\u6784\u54f2\u5b66\u3002\u4ee5\u524d\u6211\u4eec\u7528 RPC\u3001\u6d88\u606f\u961f\u5217\u628a\u5fae\u670d\u52a1\u62fc\u63a5\u8d77\u6765\uff1b", "AI": {"tldr": "\u4f20\u7edf\u7684\u201c\u5fae\u670d\u52a1+\u5173\u952e\u8bcd\u68c0\u7d22+\u5173\u7cfb\u578b\u6570\u636e\u5e93+ CPU \u201d\u7684\u7ec4\u5408\uff0c\u6b63\u88ab\u201c Agent \u667a\u80fd\u4f53+\u8bed\u4e49\u68c0\u7d22+\u77e5\u8bc6\u56fe\u8c31+\u5411\u91cf\u6570\u636e\u5e93 + GPU \u201d\u6240\u53d6\u4ee3\u3002\u7b2c\u56db\u662f\u67b6\u6784\u54f2\u5b66\u3002\u4ee5\u524d\u6211\u4eec\u7528 RPC\u3001\u6d88\u606f\u961f\u5217\u628a\u5fae\u670d\u52a1\u62fc\u63a5\u8d77\u6765\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.eb0ac8ee", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257667&idx=1&sn=45b26bf43785a32740be0c69637a33c9&chksm=bc931c0e5351fb8216eefb6dc842b2840203ea49a0ff4744ef3224d712d30304272d80759718#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257667&idx=1&sn=45b26bf43785a32740be0c69637a33c9&chksm=bc931c0e5351fb8216eefb6dc842b2840203ea49a0ff4744ef3224d712d30304272d80759718#rd", "authors": ["InfoQ"], "title": "\u7b2c\u4e00\u6279\u5403\u4e0a<em class=\"highlight\">Agentic</em> AI \u201c\u8783\u87f9\u201d\u7684\u4eba\uff0c\u662f\u5982\u4f55\u5750\u4e0a\u9910\u684c\u7684\uff1f", "comment": "Source: WeChat, Published: 2025-09-29 07:45:00", "summary": "\u67d0\u79cd\u7a0b\u5ea6\u4e0a\uff0cAgentic AI \u4ee3\u8868\u4e86 AI \u964d\u672c\u63d0\u6548\u7684\u201c\u7ec8\u6781\u5f62\u6001\u201d\u2014\u2014\u9636\u6bb5\u6027\u7684\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\uff0c\u81ea\u4e3b\u89c4\u5212\u6d41\u7a0b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002Sam Altman \u751a\u81f3\u5ba3\u79f0\uff0c\u672a\u6765 AI \u5c06\u63a5\u7ba1\u4eba\u7c7b\u7ecf\u6d4e\u793e\u4f1a\u4e2d 30%-40% \u7684\u5de5\u4f5c\u3002", "AI": {"tldr": "\u67d0\u79cd\u7a0b\u5ea6\u4e0a\uff0cAgentic AI \u4ee3\u8868\u4e86 AI \u964d\u672c\u63d0\u6548\u7684\u201c\u7ec8\u6781\u5f62\u6001\u201d\u2014\u2014\u9636\u6bb5\u6027\u7684\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\uff0c\u81ea\u4e3b\u89c4\u5212\u6d41\u7a0b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002Sam Altman \u751a\u81f3\u5ba3\u79f0\uff0c\u672a\u6765 AI \u5c06\u63a5\u7ba1\u4eba\u7c7b\u7ecf\u6d4e\u793e\u4f1a\u4e2d 30%-40% \u7684\u5de5\u4f5c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.57b9640c", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2ODkwNTMwNg==&mid=2247483860&idx=1&sn=ad790c8040991f3c26e876e63e153156&chksm=cf4213497aed92ed574750b4d6e79dd42595cfebde6b55e9b6f34378d136328cf375cb873393#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2ODkwNTMwNg==&mid=2247483860&idx=1&sn=ad790c8040991f3c26e876e63e153156&chksm=cf4213497aed92ed574750b4d6e79dd42595cfebde6b55e9b6f34378d136328cf375cb873393#rd", "authors": ["\u51cc\u836f\u7684aigc"], "title": "2025\u5e74<em class=\"highlight\">\u4ee3\u7406</em>\u5f0f\u641c\u7d22\uff08<em class=\"highlight\">Agentic</em> Search\uff09\uff1a\u641c\u7d22\u9886\u57df\u65b0\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-09-29 06:29:30", "summary": "\u4ee3\u7406\u5f0f\u641c\u7d22\uff08Agentic Search\uff09\u6df1\u5ea6\u7814\u7a76\uff08DeepResearch\uff09\u901a\u7528\u4ee3\u7406\uff08General-purpose Agent\uff09\u6838\u5fc3\u76ee\u6807\u4e00\u6b21\u641c\u7d22 \u2248 \u4e00\u4efd\u53ef\u76f4\u63a5\u7528\u7684\u4ea4\u4ed8\u7269\uff08PPT/\u62a5\u544a/\u4ee3\u7801/\u8111\u56fe\uff09", "AI": {"tldr": "\u4ee3\u7406\u5f0f\u641c\u7d22\uff08Agentic Search\uff09\u6df1\u5ea6\u7814\u7a76\uff08DeepResearch\uff09\u901a\u7528\u4ee3\u7406\uff08General-purpose Agent\uff09\u6838\u5fc3\u76ee\u6807\u4e00\u6b21\u641c\u7d22 \u2248 \u4e00\u4efd\u53ef\u76f4\u63a5\u7528\u7684\u4ea4\u4ed8\u7269\uff08PPT/\u62a5\u544a/\u4ee3\u7801/\u8111\u56fe\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.4aa3d5ad", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3NzQ4ODMxOQ==&mid=2247484018&idx=1&sn=053f2a29f9c91404460f7dd13d7d913c&chksm=9e0e0fad80c84ccdf123a28757dc3b3a9be74847305ffdf7cccc1aa4d53ae8dba9e47370c628#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3NzQ4ODMxOQ==&mid=2247484018&idx=1&sn=053f2a29f9c91404460f7dd13d7d913c&chksm=9e0e0fad80c84ccdf123a28757dc3b3a9be74847305ffdf7cccc1aa4d53ae8dba9e47370c628#rd", "authors": ["\u53f6\u6893\u7684 AI \u7814\u4e60\u793e"], "title": "\u5927\u6a21\u578b\u4e0b\u534a\u573a\uff1a\u4eceLLM-RL\u5230<em class=\"highlight\">Agentic</em> RL\u5168\u65b0\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-09-29 06:03:58", "summary": "\u65b0\u52a0\u5761\u56fd\u7acb\u5927\u5b66\u7b4916\u5bb6\u7814\u7a76\u673a\u6784\u8054\u5408\u53d1\u8868\u7684 100 \u9875\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u63d0\u51fa Agentic RL\uff08\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\uff09 \u8303\u5f0f\uff1a\u628a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u201c\u4e00\u6b21\u6027\u6587\u672c\u751f\u6210\u5668\u201d\u5347\u7ea7\u4e3a\u201c\u53ef\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6301\u7eed\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u53cd\u601d\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u201d\uff0c\u5e76\u7ed9", "AI": {"tldr": "\u65b0\u52a0\u5761\u56fd\u7acb\u5927\u5b66\u7b4916\u5bb6\u7814\u7a76\u673a\u6784\u8054\u5408\u53d1\u8868\u7684 100 \u9875\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u63d0\u51fa Agentic RL\uff08\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\uff09 \u8303\u5f0f\uff1a\u628a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u201c\u4e00\u6b21\u6027\u6587\u672c\u751f\u6210\u5668\u201d\u5347\u7ea7\u4e3a\u201c\u53ef\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6301\u7eed\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u53cd\u601d\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u201d\uff0c\u5e76\u7ed9", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.f99854da", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483809&idx=1&sn=52d6e053e31a2be979b3a4ca5928d36d&chksm=fef0ed5238e21b4ecb7b5699c6348423de68d597745c5cda94308c0411269cbec5a58c730e47#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483809&idx=1&sn=52d6e053e31a2be979b3a4ca5928d36d&chksm=fef0ed5238e21b4ecb7b5699c6348423de68d597745c5cda94308c0411269cbec5a58c730e47#rd", "authors": ["\u96e8\u54e5\u8c08\u8f66"], "title": "<em class=\"highlight\">Agentic</em>\uff1a\u65b0\u4e00\u4ee3\u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7cfb\u7edf\u4e4b\u573a\u666f\u89e3\u8bfb", "comment": "Source: WeChat, Published: 2025-09-29 04:23:36", "summary": "\u672c\u7ae0\u5c06\u901a\u8fc7\u591a\u4e2a\u5177\u4f53\u7684\u573a\u666f\u6848\u4f8b\uff0c\u8be6\u7ec6\u5c55\u793aAgentic\u7cfb\u7edf\u5728\u5b9e\u9645\u5de5\u4f5c\u4e2d\u7684\u6267\u884c\u94fe\u8def\uff0c\u5c06\u62bd\u8c61\u7684\u67b6\u6784\u8bbe\u8ba1\u4e0e\u5177\u4f53\u7684\u5e94\u7528\u573a\u666f\u76f8\u7ed3\u5408\u3002Agentic\u7cfb\u5217\u6587\u7ae0\uff1a\u4f01\u4e1a\u7ea7AI Agency\u6a21\u5f0f\u6f14\u8fdb", "AI": {"tldr": "\u672c\u7ae0\u5c06\u901a\u8fc7\u591a\u4e2a\u5177\u4f53\u7684\u573a\u666f\u6848\u4f8b\uff0c\u8be6\u7ec6\u5c55\u793aAgentic\u7cfb\u7edf\u5728\u5b9e\u9645\u5de5\u4f5c\u4e2d\u7684\u6267\u884c\u94fe\u8def\uff0c\u5c06\u62bd\u8c61\u7684\u67b6\u6784\u8bbe\u8ba1\u4e0e\u5177\u4f53\u7684\u5e94\u7528\u573a\u666f\u76f8\u7ed3\u5408\u3002Agentic\u7cfb\u5217\u6587\u7ae0\uff1a\u4f01\u4e1a\u7ea7AI Agency\u6a21\u5f0f\u6f14\u8fdb", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.07872b9c", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483804&idx=1&sn=647679b230ff7aea48e245a9565893a4&chksm=fe3fa3c378b3c2f5231577139e7536af598a4ed9aae810c1be1fcb0df053b472e16accbb9687#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483804&idx=1&sn=647679b230ff7aea48e245a9565893a4&chksm=fe3fa3c378b3c2f5231577139e7536af598a4ed9aae810c1be1fcb0df053b472e16accbb9687#rd", "authors": ["\u96e8\u54e5\u8c08\u8f66"], "title": "<em class=\"highlight\">Agentic</em>: \u65b0\u4e00\u4ee3\u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7cfb\u7edf\u4e4b\u65b9\u6848\u89e3\u8bfb", "comment": "Source: WeChat, Published: 2025-09-29 04:00:29", "summary": "Sensor.Agentic\u662f\u4e00\u4e2a\u9762\u5411\u4f20\u611f\u5668\u6280\u672f\u9886\u57df\u7684\u3001\u96c6\u77e5\u8bc6\u95ee\u7b54\u3001\u6545\u969c\u8bca\u65ad\u4e0e\u4ee3\u7801\u8f85\u52a9\u4e8e\u4e00\u4f53\u7684AI\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u5b83\u6452\u5f03\u4e86\u4f20\u7edf\u7684\u5355\u4f53\u5f0f\u3001ReACT\u5f0fAI\u52a9\u624b\u67b6\u6784\uff0c\u521b\u65b0\u6027\u5730\u91c7\u7528\u4e86\u57fa\u4e8e\u201c\u89c4\u5212\u8005-\u6267\u884c\u8005\u201d\uff08Planner-Executor\uff09\u6a21\u5f0f\u548c\u201c\u6df7\u5408\u4e13\u5bb6", "AI": {"tldr": "Sensor.Agentic\u662f\u4e00\u4e2a\u9762\u5411\u4f20\u611f\u5668\u6280\u672f\u9886\u57df\u7684\u3001\u96c6\u77e5\u8bc6\u95ee\u7b54\u3001\u6545\u969c\u8bca\u65ad\u4e0e\u4ee3\u7801\u8f85\u52a9\u4e8e\u4e00\u4f53\u7684AI\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u5b83\u6452\u5f03\u4e86\u4f20\u7edf\u7684\u5355\u4f53\u5f0f\u3001ReACT\u5f0fAI\u52a9\u624b\u67b6\u6784\uff0c\u521b\u65b0\u6027\u5730\u91c7\u7528\u4e86\u57fa\u4e8e\u201c\u89c4\u5212\u8005-\u6267\u884c\u8005\u201d\uff08Planner-Executor\uff09\u6a21\u5f0f\u548c\u201c\u6df7\u5408\u4e13\u5bb6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.b4073522", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483799&idx=1&sn=457a97fe3a84c9d05fbb025380091a1d&chksm=fed5b72e5c206c2c6a35fa883dae0d76bc7362ef5d59c74755cb4a6e0462c9215a35c389a20d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483799&idx=1&sn=457a97fe3a84c9d05fbb025380091a1d&chksm=fed5b72e5c206c2c6a35fa883dae0d76bc7362ef5d59c74755cb4a6e0462c9215a35c389a20d#rd", "authors": ["\u96e8\u54e5\u8c08\u8f66"], "title": "<em class=\"highlight\">Agentic</em>\uff1a\u65b0\u4e00\u4ee3\u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7cfb\u7edf\u53c2\u8003\u67b6\u6784", "comment": "Source: WeChat, Published: 2025-09-29 03:48:48", "summary": "sensor.agentic - use case pre-processing master component moe component rag component #lang detection chatbot planner agent x+ 1. user feedback\u30023. tool execute planning rag event publisher & chatbot master agent stateless response aggregator trace id event subscriber chatbot agent cluster", "AI": {"tldr": "sensor.agentic - use case pre-processing master component moe component rag component #lang detection chatbot planner agent x+ 1. user feedback\u30023. tool execute planning rag event publisher & chatbot m...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.04e15601", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwMzc0NDk1NQ==&mid=2247485844&idx=1&sn=638a3d0eaedcbfdbfb9704facaa1e4eb&chksm=c1b341fee033fbf02fba316b97bd3eba8e905f7d6b06a12cbad3854e1a5ca0513e0c44ce860e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwMzc0NDk1NQ==&mid=2247485844&idx=1&sn=638a3d0eaedcbfdbfb9704facaa1e4eb&chksm=c1b341fee033fbf02fba316b97bd3eba8e905f7d6b06a12cbad3854e1a5ca0513e0c44ce860e#rd", "authors": ["\u672a\u6765\u5546\u4e1a\u5bfc\u8bba"], "title": "\u4ece\u751f\u6210\u5f0f\u5230<em class=\"highlight\">Agentic</em>\uff1a\u65c5\u6e38\u4e0e\u793c\u5584\u4e1a\u7684AI\u6218\u7565\u62d0\u70b9", "comment": "Source: WeChat, Published: 2025-09-29 02:29:01", "summary": "Agentic\u8bfb\u7684\u4e0d\u53ea\u662f\u201c\u4eba\u7fa4\u201d\uff0c\u66f4\u662f\u201c\u6b64\u65f6\u6b64\u5730\u6b64\u4eba\u201d\u3002\u5c06\u5b9e\u65f6\u4fe1\u53f7\uff08\u68c0\u7d22\u3001\u5929\u6c14\u3001\u4ef7\u683c\u3001\u5e93\u5b58\u3001\u60c5\u7eea\uff09\u5e76\u5165\uff0c\u5728\u201c\u4f9b\u7ed9\u2014\u4ef7\u683c\u2014\u6743\u76ca\u2014\u5185\u5bb9\u2014\u670d\u52a1\u201d\u4e94\u4ef6\u5957\u91cc\u5b8c\u6210\u5373\u65f6\u91cd\u6784\uff1b", "AI": {"tldr": "Agentic\u8bfb\u7684\u4e0d\u53ea\u662f\u201c\u4eba\u7fa4\u201d\uff0c\u66f4\u662f\u201c\u6b64\u65f6\u6b64\u5730\u6b64\u4eba\u201d\u3002\u5c06\u5b9e\u65f6\u4fe1\u53f7\uff08\u68c0\u7d22\u3001\u5929\u6c14\u3001\u4ef7\u683c\u3001\u5e93\u5b58\u3001\u60c5\u7eea\uff09\u5e76\u5165\uff0c\u5728\u201c\u4f9b\u7ed9\u2014\u4ef7\u683c\u2014\u6743\u76ca\u2014\u5185\u5bb9\u2014\u670d\u52a1\u201d\u4e94\u4ef6\u5957\u91cc\u5b8c\u6210\u5373\u65f6\u91cd\u6784\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.74c64feb", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDY5OTM1MQ==&mid=2247484228&idx=1&sn=53f6ef5310716c7f1d6b4247b56f2f3f&chksm=c49ebb22e76764271994da3de5c0bef8001a28e2eda6a86e8d2228f413dee6512b8a0feaa246#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDY5OTM1MQ==&mid=2247484228&idx=1&sn=53f6ef5310716c7f1d6b4247b56f2f3f&chksm=c49ebb22e76764271994da3de5c0bef8001a28e2eda6a86e8d2228f413dee6512b8a0feaa246#rd", "authors": ["InfraLink"], "title": "\u5927\u6a21\u578b\u4e0b\u534a\u573a\uff1a\u4eceLLM-RL\u5230<em class=\"highlight\">Agentic</em> RL\u5168\u65b0\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-09-29 01:31:04", "summary": "\u65b0\u52a0\u5761\u56fd\u7acb\u5927\u5b66\u7b4916\u5bb6\u7814\u7a76\u673a\u6784\u8054\u5408\u53d1\u8868\u7684 100 \u9875\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u63d0\u51fa Agentic RL\uff08\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\uff09 \u8303\u5f0f\uff1a\u628a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u201c\u4e00\u6b21\u6027\u6587\u672c\u751f\u6210\u5668\u201d\u5347\u7ea7\u4e3a\u201c\u53ef\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6301\u7eed\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u53cd\u601d\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u201d\uff0c\u5e76\u7ed9", "AI": {"tldr": "\u65b0\u52a0\u5761\u56fd\u7acb\u5927\u5b66\u7b4916\u5bb6\u7814\u7a76\u673a\u6784\u8054\u5408\u53d1\u8868\u7684 100 \u9875\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u63d0\u51fa Agentic RL\uff08\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\uff09 \u8303\u5f0f\uff1a\u628a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u201c\u4e00\u6b21\u6027\u6587\u672c\u751f\u6210\u5668\u201d\u5347\u7ea7\u4e3a\u201c\u53ef\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6301\u7eed\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u53cd\u601d\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u201d\uff0c\u5e76\u7ed9", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.e45af191", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NDYzODcyMQ==&mid=2247484478&idx=1&sn=2785745306088442c749807648053a43&chksm=cf3cb74879f7a9dc312c3aa9a39f45234a265c102db43cb83d72d41e6a130d388ab521e91739#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NDYzODcyMQ==&mid=2247484478&idx=1&sn=2785745306088442c749807648053a43&chksm=cf3cb74879f7a9dc312c3aa9a39f45234a265c102db43cb83d72d41e6a130d388ab521e91739#rd", "authors": ["i\u535a\u5ba2\u56ed"], "title": "\u65b0\u4e00\u4ee3 <em class=\"highlight\">Agentic</em> \u7f16\u7a0b\u5e73\u53f0 Qoder \u6765\u56ed", "comment": "Source: WeChat, Published: 2025-09-29 01:04:10", "summary": "\u65b0\u4e00\u4ee3agentic\u7f16\u7a0b\u5e73\u53f0qoder \u5185\u7f6eclaude sonnet 4\u7b49\u6700\u5f3a\u6a21\u578b\u3002repo wiki\u3002quest \u6a21\u5f0f\uff1a\u81ea\u4e3b\u7f16\u7a0b \u7ed3\u6784\u5316\u6587\u6863\u77e5\u8bc6\u5e93\u65b0\u4e00\u4ee3 Agentic \u7f16\u7a0b\u5e73\u53f0 Qoder \u6765\u56ed\u6700\u8fd1\u4e00\u5bb6\u5927\u5382\u7ed9\u5168\u7403\u5f00\u53d1\u8005\u5e26\u6765\u4e00\u6b3e\u91cd\u91cf\u7ea7\u7684AI\u65b0\u79c0\u4ea7\u54c1\uff0c\u5e76\u4e14\u679c\u65ad\u5730\u9009\u62e9\u4e86\u5728\u56ed\u5b50\u91cc\u5411\u5f00", "AI": {"tldr": "\u65b0\u4e00\u4ee3agentic\u7f16\u7a0b\u5e73\u53f0qoder \u5185\u7f6eclaude sonnet 4\u7b49\u6700\u5f3a\u6a21\u578b\u3002repo wiki\u3002quest \u6a21\u5f0f\uff1a\u81ea\u4e3b\u7f16\u7a0b \u7ed3\u6784\u5316\u6587\u6863\u77e5\u8bc6\u5e93\u65b0\u4e00\u4ee3 Agentic \u7f16\u7a0b\u5e73\u53f0 Qoder \u6765\u56ed\u6700\u8fd1\u4e00\u5bb6\u5927\u5382\u7ed9\u5168\u7403\u5f00\u53d1\u8005\u5e26\u6765\u4e00\u6b3e\u91cd\u91cf\u7ea7\u7684AI\u65b0\u79c0\u4ea7\u54c1\uff0c\u5e76\u4e14\u679c\u65ad\u5730\u9009\u62e9\u4e86\u5728\u56ed\u5b50\u91cc\u5411\u5f00", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.099439e7", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2ODAyMDk1OA==&mid=2247484724&idx=1&sn=56865050c9e01319fa57fee241ba9aab&chksm=c558d512f996c50d3e72941d6840fcf5a75292b7710b60885198e4c2dd06ab941b1d1c2bb3b2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2ODAyMDk1OA==&mid=2247484724&idx=1&sn=56865050c9e01319fa57fee241ba9aab&chksm=c558d512f996c50d3e72941d6840fcf5a75292b7710b60885198e4c2dd06ab941b1d1c2bb3b2#rd", "authors": ["\u5170\u9020\u6210\u957f\u5f55.\u9020\u4ef7\u5e08\u624b\u8bb0"], "title": "\u4e00\u4e2a\u9020\u4ef7\u4eba\u7684AI\u7f16\u7a0b\u63d0\u6548\u4e4b\u8def \u00a0\u7b2c\u4e8c\u5929", "comment": "Source: WeChat, Published: 2025-09-29 01:00:16", "summary": "1\u3001\u4ec0\u4e48\u662fAgentic \u5de5\u4f5c\u6d41\uff1fAgent\uff08\u667a\u80fd\u4f53\uff09\uff1a\u5c31\u662f\u4e00\u4e2a\u80fd\u5e2e\u4f60\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u7684\u201cAI \u5c0f\u52a9\u624b\u201d\u3002\u5de5\u4f5c\u6d41\uff08Workflow\uff09\uff1a\u5c31\u662f\u7528\u4e00\u4e2a\u4e2a\u7684\u667a\u80fd\u4f53\u642d\u5efa\u8d77\u6765\u5b8c\u6210\u4e00\u4ef6\u4e8b\u7684\u6574\u4e2a\u8fc7\u7a0b\u3002", "AI": {"tldr": "1\u3001\u4ec0\u4e48\u662fAgentic \u5de5\u4f5c\u6d41\uff1fAgent\uff08\u667a\u80fd\u4f53\uff09\uff1a\u5c31\u662f\u4e00\u4e2a\u80fd\u5e2e\u4f60\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u7684\u201cAI \u5c0f\u52a9\u624b\u201d\u3002\u5de5\u4f5c\u6d41\uff08Workflow\uff09\uff1a\u5c31\u662f\u7528\u4e00\u4e2a\u4e2a\u7684\u667a\u80fd\u4f53\u642d\u5efa\u8d77\u6765\u5b8c\u6210\u4e00\u4ef6\u4e8b\u7684\u6574\u4e2a\u8fc7\u7a0b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.1cd3ab45", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5OTA3NTY5OQ==&mid=2247484733&idx=1&sn=12179d4cb6181ff69fc270f0768cc5ff&chksm=97df00f9e2104d60147f6b85ee78d0ad9545ed38eecf858884cde108390ad8ef695fd74dbaa8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5OTA3NTY5OQ==&mid=2247484733&idx=1&sn=12179d4cb6181ff69fc270f0768cc5ff&chksm=97df00f9e2104d60147f6b85ee78d0ad9545ed38eecf858884cde108390ad8ef695fd74dbaa8#rd", "authors": ["ATinfo"], "title": "AI\u641c\u7d22\u7684\u4e0b\u4e00\u6b65\uff0c<em class=\"highlight\">Agentic</em> Search\u6765\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-09-28 23:33:07", "summary": "\u4e00\u3001Agentic Search\u4ece\u5b98\u65b9\u4fe1\u606f\u6765\u770b\uff0cAgentic Search\u4e3b\u6253\u201c\u8fb9\u60f3\u8fb9\u641c\uff0c\u8fb9\u641c\u8fb9\u505a\u201d\u3002\u8fd9\u662f\u5bf9\u4f20\u7edfAI\u641c\u7d22\u8303\u5f0f\u7684\u4e00\u6b21\u7cfb\u7edf\u6027\u91cd\u6784\u3002\u76f8\u8f83\u4e8e\u4ee5\u5f80\u201c\u5355\u8f6e\u95ee\u7b54\u5f0f\u201d\u7684\u641c\u7d22\u673a\u5236\uff0c\u5b83\u66f4\u5f3a\u8c03\u4efb\u52a1\u611f\u77e5\u4e0e\u8fc7\u7a0b\u6267\u884c\uff0c\u5373\u5728\u7406\u89e3\u7528\u6237\u76ee\u6807\u7684\u57fa\u7840\u4e0a\uff0c\u81ea\u52a8\u89c4", "AI": {"tldr": "\u4e00\u3001Agentic Search\u4ece\u5b98\u65b9\u4fe1\u606f\u6765\u770b\uff0cAgentic Search\u4e3b\u6253\u201c\u8fb9\u60f3\u8fb9\u641c\uff0c\u8fb9\u641c\u8fb9\u505a\u201d\u3002\u8fd9\u662f\u5bf9\u4f20\u7edfAI\u641c\u7d22\u8303\u5f0f\u7684\u4e00\u6b21\u7cfb\u7edf\u6027\u91cd\u6784\u3002\u76f8\u8f83\u4e8e\u4ee5\u5f80\u201c\u5355\u8f6e\u95ee\u7b54\u5f0f\u201d\u7684\u641c\u7d22\u673a\u5236\uff0c\u5b83\u66f4\u5f3a\u8c03\u4efb\u52a1\u611f\u77e5\u4e0e\u8fc7\u7a0b\u6267\u884c\uff0c\u5373\u5728\u7406\u89e3\u7528\u6237\u76ee\u6807\u7684\u57fa\u7840\u4e0a\uff0c\u81ea\u52a8\u89c4", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.6dfdf93c", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4NTc5NTk1MA==&mid=2247484989&idx=1&sn=89f0b8c5c0925c1d3fbca8c7b302767a&chksm=fcd3de3a6da7742d4ce3d41078fbbf46eaff0b4698e8df92622b2db6e74abce4b5c6b133c801#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4NTc5NTk1MA==&mid=2247484989&idx=1&sn=89f0b8c5c0925c1d3fbca8c7b302767a&chksm=fcd3de3a6da7742d4ce3d41078fbbf46eaff0b4698e8df92622b2db6e74abce4b5c6b133c801#rd", "authors": ["AI\u7ec4\u7ec7\u8fdb\u5316\u8bba"], "title": "\u9ea6\u80af\u9521\u6700\u65b0\u6d1e\u5bdf\uff1a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7ec4\u7ec7\uff08<em class=\"highlight\">Agentic</em> Organization\uff09\u2014\u2014AI\u65f6\u4ee3\u5168\u65b0\u7ec4\u7ec7\u8303\u5f0f\uff0c\u4e0e\u4f60\u6211\u90fd\u76f8\u5173", "comment": "Source: WeChat, Published: 2025-09-28 23:13:45", "summary": "and agentic controls with humanaccountabilityWorkforce\uff0cpeople\uff0c andculture Deep specialization and cul-ture of craftsmanship Narrowly specialized functionaltalent working in a culture ofplanning Knowledge workers with", "AI": {"tldr": "and agentic controls with humanaccountabilityWorkforce\uff0cpeople\uff0c andculture Deep specialization and cul-ture of craftsmanship Narrowly specialized functionaltalent working in a culture ofplanning Knowle...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.9f4216c4", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDE5ODM2OQ==&mid=2247487883&idx=1&sn=df58a3ad07b77b3c8f1faa1af36b579a&chksm=c5634a6398b2958b56f23711e56e8dcf0fba99d352cff826b7f5a99d58ce3391a3655f0b80ee#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDE5ODM2OQ==&mid=2247487883&idx=1&sn=df58a3ad07b77b3c8f1faa1af36b579a&chksm=c5634a6398b2958b56f23711e56e8dcf0fba99d352cff826b7f5a99d58ce3391a3655f0b80ee#rd", "authors": ["\u77e5\u767d\u5b88\u9ed11024"], "title": "\u4ece\u63d0\u793a\u5de5\u7a0b\u5230\u7cfb\u7edf\u6cbb\u7406\uff1a<em class=\"highlight\">Agentic</em> AI\u7684\u5b8c\u6574\u6280\u672f\u56fe\u8c31", "comment": "Source: WeChat, Published: 2025-09-28 16:03:00", "summary": "agentic ai concepts rag agent\u3002ic\u3002agentic ai agents systems\u4ece\u63d0\u793a\u5de5\u7a0b\u5230\u7cfb\u7edf\u6cbb\u7406\uff1aAgentic AI\u7684\u5b8c\u6574\u6280\u672f\u56fe\u8c31\u8fd9\u5f20\u56fe\u8868\u7cfb\u7edf\u68b3\u7406\u4e86\u201c\u4ee3\u7406\u578bAI\uff08Agentic AI\uff09\u201d\u7684\u6838\u5fc3\u67b6\u6784\uff0c\u4ece\u5185\u5230\u5916\u5206\u56db\u5c42\uff0c\u6db5\u76d6AI\u4e3b\u4f53\u4e0e\u4ee3\u7406\u7cfb\u7edf\u7684\u534f\u540c\u53d1\u5c55\uff1a", "AI": {"tldr": "agentic ai concepts rag agent\u3002ic\u3002agentic ai agents systems\u4ece\u63d0\u793a\u5de5\u7a0b\u5230\u7cfb\u7edf\u6cbb\u7406\uff1aAgentic AI\u7684\u5b8c\u6574\u6280\u672f\u56fe\u8c31\u8fd9\u5f20\u56fe\u8868\u7cfb\u7edf\u68b3\u7406\u4e86\u201c\u4ee3\u7406\u578bAI\uff08Agentic AI\uff09\u201d\u7684\u6838\u5fc3\u67b6\u6784\uff0c\u4ece\u5185\u5230\u5916\u5206\u56db\u5c42\uff0c\u6db5\u76d6AI\u4e3b\u4f53\u4e0e\u4ee3\u7406\u7cfb\u7edf\u7684\u534f\u540c\u53d1\u5c55\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.ddddb131", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4MDI0ODg5MQ==&mid=2247500235&idx=1&sn=6c9f0c32f0ed0ccb1ee66ebb14b4dfa2&chksm=ce291b1fd9be607429838fc618d1bf9db8c78702ea9f7caee4dd65265c4baddeaf179fdc238c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4MDI0ODg5MQ==&mid=2247500235&idx=1&sn=6c9f0c32f0ed0ccb1ee66ebb14b4dfa2&chksm=ce291b1fd9be607429838fc618d1bf9db8c78702ea9f7caee4dd65265c4baddeaf179fdc238c#rd", "authors": ["\u98ce\u4e4b\u99a8\u6280\u672f\u5f55"], "title": "\u79d8\u5854\u53c8\u51fa\u5927\u62db,\u7528<em class=\"highlight\">Agentic</em> Search\u6253\u5f00\u641c\u7d22agent\u65b0\u9ad8\u5ea6", "comment": "Source: WeChat, Published: 2025-09-28 14:12:11", "summary": "\u79d8\u5854\u63a8\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u201c\u8fb9\u60f3\u8fb9\u641c\uff0c\u8fb9\u641c\u8fb9\u505a\u201d\u7684\u641c\u7d22\u65b9\u5f0f--Agentic Search\u3002\u636e\u5b98\u65b9\u4ecb\u7ecd\u5b83\u4e0d\u4ec5\u80fd\u5e2e\u5fd9\u627e\u7b54\u6848\uff0c\u8fd8\u80fd\u81ea\u5df1\u89c4\u5212\u6b65\u9aa4\u3001\u7f16\u5199\u4ee3\u7801\u3001\u8c03\u7528\u5de5\u5177\u6765\u5b8c\u6210\u4efb\u52a1\u3002", "AI": {"tldr": "\u79d8\u5854\u63a8\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u201c\u8fb9\u60f3\u8fb9\u641c\uff0c\u8fb9\u641c\u8fb9\u505a\u201d\u7684\u641c\u7d22\u65b9\u5f0f--Agentic Search\u3002\u636e\u5b98\u65b9\u4ecb\u7ecd\u5b83\u4e0d\u4ec5\u80fd\u5e2e\u5fd9\u627e\u7b54\u6848\uff0c\u8fd8\u80fd\u81ea\u5df1\u89c4\u5212\u6b65\u9aa4\u3001\u7f16\u5199\u4ee3\u7801\u3001\u8c03\u7528\u5de5\u5177\u6765\u5b8c\u6210\u4efb\u52a1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.aff1c35b", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNDY1MDA1OA==&mid=2247490069&idx=2&sn=a7bfecd56aea82d4be27aed925fcb839&chksm=c0e454ccac5bf5032b301cee63e7bb48f0c7f19ac4b0b0b34e4ad5c42aefb17173f5215528a2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNDY1MDA1OA==&mid=2247490069&idx=2&sn=a7bfecd56aea82d4be27aed925fcb839&chksm=c0e454ccac5bf5032b301cee63e7bb48f0c7f19ac4b0b0b34e4ad5c42aefb17173f5215528a2#rd", "authors": ["\u5148\u8fdb\u6280\u672f\u6210\u679c\u897f\u90e8\u8f6c\u5316\u4e2d\u5fc3"], "title": "\u3010\u6210\u679c\u53d1\u5e03\u3011 \u57ce\u5e02\u66f4\u65b0<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e4b\u8d23\u4efb\u89c4\u5212\u5e08Agent\uff08\u77f3\u69b4\u7c7d\uff09", "comment": "Source: WeChat, Published: 2025-09-29 12:43:41", "summary": "\u672c\u6210\u679c\u57fa\u4e8eAI\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u6280\u672f\u548c\u957f\u671f\u8d23\u4efb\u89c4\u5212\u5e08\u5b9e\u8df5\uff0c\u6574\u5408\u4e86\u57ce\u5e02\u89c4\u5212\u3001\u5efa\u7b51\u3001\u57ce\u5e02\u7ba1\u7406\u3001\u57ce\u5e02\u6cbb\u7406\u3001\u6280\u672f\u7ecf\u6d4e\u7b49\u591a\u5b66\u79d1\u8de8\u4e13\u4e1a\u7684\u7406\u8bba\u4e0e\u77e5\u8bc6\uff0c\u5f62\u6210\u4e24\u5927\u667a\u80fd\u89d2\u8272\uff1a\u4e00\u662f\u8d23\u4efb\u89c4\u5212\u5e08\u52a9\u624b\uff0c\u4e8c\u662f\u57ce\u5e02\u5efa\u8bbe\u987e\u95ee\u3002", "AI": {"tldr": "\u672c\u6210\u679c\u57fa\u4e8eAI\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u6280\u672f\u548c\u957f\u671f\u8d23\u4efb\u89c4\u5212\u5e08\u5b9e\u8df5\uff0c\u6574\u5408\u4e86\u57ce\u5e02\u89c4\u5212\u3001\u5efa\u7b51\u3001\u57ce\u5e02\u7ba1\u7406\u3001\u57ce\u5e02\u6cbb\u7406\u3001\u6280\u672f\u7ecf\u6d4e\u7b49\u591a\u5b66\u79d1\u8de8\u4e13\u4e1a\u7684\u7406\u8bba\u4e0e\u77e5\u8bc6\uff0c\u5f62\u6210\u4e24\u5927\u667a\u80fd\u89d2\u8272\uff1a\u4e00\u662f\u8d23\u4efb\u89c4\u5212\u5e08\u52a9\u624b\uff0c\u4e8c\u662f\u57ce\u5e02\u5efa\u8bbe\u987e\u95ee\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.d09652e4", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5OTczMTA0Nw==&mid=2247595451&idx=1&sn=8f1d0ce05031f5707f0df8bae142e5f3&chksm=9152d8c5b394888b09239d4af7c496c0842670afe9ce3e102e2caea73db69d550ef2793aeeee#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5OTczMTA0Nw==&mid=2247595451&idx=1&sn=8f1d0ce05031f5707f0df8bae142e5f3&chksm=9152d8c5b394888b09239d4af7c496c0842670afe9ce3e102e2caea73db69d550ef2793aeeee#rd", "authors": ["\u5927\u6280\u72ee"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0e\u667a\u80fd\u4f53\uff1aAI\u843d\u5730\u5e94\u7528\u7684\u5178\u578b\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-09-29 12:00:25", "summary": "\u5927\u6a21\u578b\u5982\u540c\u5927\u8111\uff0c\u667a\u80fd\u4f53\u5219\u597d\u6bd4\u624b\u811a\uff0c\u5927\u6a21\u578b\u6307\u6325\u667a\u80fd\u4f53\uff0c\u6070\u4f3c\u5927\u8111\u8c03\u63a7\u624b\u811a\u4ee5\u5b8c\u6210\u4eba\u4f53\u7684\u5404\u9879\u52a8\u4f5c\u3002\u4e00 \u5927\u6a21\u578b>ssssssss\u81ea2022\u5e7411\u670830\u65e5OpenAI\u63a8\u51fa\u4eba\u5de5\u667a\u80fd\u5bf9\u8bdd\u804a\u5929\u673a\u5668\u4ebaChatGPT\u540e\uff0c\u5b83\u8fc5\u901f\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u8e7f\u7ea2\uff0c\u4ec55\u5929\u65f6\u95f4\uff0c\u6ce8\u518c\u7528\u6237", "AI": {"tldr": "\u5927\u6a21\u578b\u5982\u540c\u5927\u8111\uff0c\u667a\u80fd\u4f53\u5219\u597d\u6bd4\u624b\u811a\uff0c\u5927\u6a21\u578b\u6307\u6325\u667a\u80fd\u4f53\uff0c\u6070\u4f3c\u5927\u8111\u8c03\u63a7\u624b\u811a\u4ee5\u5b8c\u6210\u4eba\u4f53\u7684\u5404\u9879\u52a8\u4f5c\u3002\u4e00 \u5927\u6a21\u578b>ssssssss\u81ea2022\u5e7411\u670830\u65e5OpenAI\u63a8\u51fa\u4eba\u5de5\u667a\u80fd\u5bf9\u8bdd\u804a\u5929\u673a\u5668\u4ebaChatGPT\u540e\uff0c\u5b83\u8fc5\u901f\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u8e7f\u7ea2\uff0c\u4ec55\u5929\u65f6\u95f4\uff0c\u6ce8\u518c\u7528\u6237", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.4dc7756c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247829516&idx=2&sn=3fed7422e8758a9c4166b4ab079f5048&chksm=e92e4bfee37badbe2568ea7f07e8e5f8884d88dc607e7ee8873925458424b61299006cb92c44#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247829516&idx=2&sn=3fed7422e8758a9c4166b4ab079f5048&chksm=e92e4bfee37badbe2568ea7f07e8e5f8884d88dc607e7ee8873925458424b61299006cb92c44#rd", "authors": ["\u91cf\u5b50\u4f4d"], "title": "\u5341\u4ebf\u7ea7\u53c2\u6570\uff0c\u5343\u4ebf\u7ea7\u6027\u80fd\uff0c\u4e0a\u6d77AI Lab\u53d1\u5e03\u65b0\u4e00\u4ee3\u6587\u6863\u89e3\u6790<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff0c\u590d\u6742\u573a\u666f\u89e3\u6790\u7cbe\u5ea6\u5ab2\u7f8e\u4eba\u7c7b\u4e13\u5bb6", "comment": "Source: WeChat, Published: 2025-09-29 10:44:09", "summary": "\u4e0a\u6d77\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4\u53d1\u5e03\u65b0\u4e00\u4ee3\u6587\u6863\u89e3\u6790\u5927\u6a21\u578b\u2014\u2014MinerU2.5\u3002\u4f5c\u4e3aMinerU\u7cfb\u5217\u6700\u65b0\u6210\u679c\uff0c\u8be5\u6a21\u578b\u4ec5\u4ee51.2B\u53c2\u6570\u89c4\u6a21\uff0c\u5c31\u5728OmniDocBench\u3001olmOCR-bench\u3001Ocean-OCR\u7b49\u6743\u5a01\u8bc4\u6d4b\u4e0a\uff0c\u5168\u9762\u8d85\u8d8aGemini2.5-Pro\u3001GPT-4o\u3001Qwen2.5-VL-72B\u7b49\u4e3b\u6d41\u901a\u7528\u5927\u6a21\u578b\uff0c\u4ee5\u53cado", "AI": {"tldr": "\u4e0a\u6d77\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4\u53d1\u5e03\u65b0\u4e00\u4ee3\u6587\u6863\u89e3\u6790\u5927\u6a21\u578b\u2014\u2014MinerU2.5\u3002\u4f5c\u4e3aMinerU\u7cfb\u5217\u6700\u65b0\u6210\u679c\uff0c\u8be5\u6a21\u578b\u4ec5\u4ee51.2B\u53c2\u6570\u89c4\u6a21\uff0c\u5c31\u5728OmniDocBench\u3001olmOCR-bench\u3001Ocean-OCR\u7b49\u6743\u5a01\u8bc4\u6d4b\u4e0a\uff0c\u5168\u9762\u8d85\u8d8aGemini2.5-Pro\u3001GPT-4o\u3001Qwen2.5-VL-72B\u7b49\u4e3b\u6d41\u901a\u7528\u5927\u6a21\u578b\uff0c\u4ee5\u53cado", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2509.c4ab86f0", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk4ODQ3MTU0Nw==&mid=2247484169&idx=1&sn=0207aee81a8976d90acf15e783a98ff7&chksm=c466d819798f51c2bf8551412019a3c3d8955770bcb5deefdc6d96a92105b800d644bf1513c4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk4ODQ3MTU0Nw==&mid=2247484169&idx=1&sn=0207aee81a8976d90acf15e783a98ff7&chksm=c466d819798f51c2bf8551412019a3c3d8955770bcb5deefdc6d96a92105b800d644bf1513c4#rd", "authors": ["AI\u8bed\u5b99 \u6f2b\u6e38\u6307\u5357"], "title": "2025 \u4e0b\u534a\u5e74<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6700\u65b0\u6218\u51b5\uff1a\u57fa\u5ea7\u964d\u6e29\uff0c\u591a\u6a21\u6001\u4e0e Agent \u6210\u65b0\u6218\u573a", "comment": "Source: WeChat, Published: 2025-09-29 10:30:29", "summary": "\u968f\u7740\u56fd\u5e86\u5047\u671f\u5373\u5c06\u6765\u4e34\uff0c\u6211\u4eec\u6765\u4e00\u540c\u68b3\u7406\u4e0b 9 \u6708\u4efd\u5927\u6a21\u578b\u9886\u57df\u503c\u5f97\u5173\u6ce8\u7684\u65b0\u54c1Agent\u5e94\u7528\u4e0e\u591a\u6a21\u6001\u6a21\u578b\u53d1\u5e03\u52a8\u6001\u3002\u672c\u6587\u5df2\u4ece \u300c\u89c6\u89c9\u591a\u6a21\u6001\u6a21\u578b\u300d \u4e0e \u300cAgent \u5e94\u7528\u300d \u4e24\u5927\u65b9\u5411\u6574\u7406\u6210\u6e05\u5355\uff0c\u770b\u770b\u5176\u4e2d\u662f\u5426\u6709\u4f60\u5173\u6ce8\u6216\u9700\u8981\u7684\u6280\u672f\u4ea7\u54c1\u3002", "AI": {"tldr": "\u968f\u7740\u56fd\u5e86\u5047\u671f\u5373\u5c06\u6765\u4e34\uff0c\u6211\u4eec\u6765\u4e00\u540c\u68b3\u7406\u4e0b 9 \u6708\u4efd\u5927\u6a21\u578b\u9886\u57df\u503c\u5f97\u5173\u6ce8\u7684\u65b0\u54c1Agent\u5e94\u7528\u4e0e\u591a\u6a21\u6001\u6a21\u578b\u53d1\u5e03\u52a8\u6001\u3002\u672c\u6587\u5df2\u4ece \u300c\u89c6\u89c9\u591a\u6a21\u6001\u6a21\u578b\u300d \u4e0e \u300cAgent \u5e94\u7528\u300d \u4e24\u5927\u65b9\u5411\u6574\u7406\u6210\u6e05\u5355\uff0c\u770b\u770b\u5176\u4e2d\u662f\u5426\u6709\u4f60\u5173\u6ce8\u6216\u9700\u8981\u7684\u6280\u672f\u4ea7\u54c1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.131b8438", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247585661&idx=2&sn=4d7f467be8151ca8ca5d4827c456b1ea&chksm=fa4feed2ea2cb81eb9d7a90f82a4d91fbe53c9e41a43d559fe4ad0743faee9a2ddac100cf569#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247585661&idx=2&sn=4d7f467be8151ca8ca5d4827c456b1ea&chksm=fa4feed2ea2cb81eb9d7a90f82a4d91fbe53c9e41a43d559fe4ad0743faee9a2ddac100cf569#rd", "authors": ["AI\u601d\u60f3\u4f1a"], "title": "SALMONN \u7cfb\u5217\u97f3\u89c6\u9891\u7406\u89e3<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9738\u699c\u56de\u5f52\uff01\u63a8\u7406\u589e\u5f3a\u3001\u9ad8\u5e27\u7387\u3001\u65e0\u6587\u672c\u6cc4\u6f0f\u5168\u7ebf\u7a81\u7834", "comment": "Source: WeChat, Published: 2025-09-29 10:20:27", "summary": "video-SALMONN 2+ \u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u9ad8\u8d28\u91cf\u3001\u5b8c\u6574\u89c6\u9891\u63cf\u8ff0\u7684\u97f3\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u539f\u5b50\u4e8b\u4ef6\u7ea7\u7684\u8bc4\u4f30\u4f53\u7cfb\u4e0e MrDPO \u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5b83\u5927\u5e45\u51cf\u5c11\u4fe1\u606f\u9057\u6f0f\u548c\u5e7b\u89c9\u3002", "AI": {"tldr": "video-SALMONN 2+ \u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u9ad8\u8d28\u91cf\u3001\u5b8c\u6574\u89c6\u9891\u63cf\u8ff0\u7684\u97f3\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u539f\u5b50\u4e8b\u4ef6\u7ea7\u7684\u8bc4\u4f30\u4f53\u7cfb\u4e0e MrDPO \u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5b83\u5927\u5e45\u51cf\u5c11\u4fe1\u606f\u9057\u6f0f\u548c\u5e7b\u89c9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.683ad6f9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNzE3NzIyNA==&mid=2247691064&idx=8&sn=5a50fe94766a35cb7769460fb73e21ea&chksm=c310f2e5edc9499df5310118927f5682c55274fc7f396e75f87c1197b093aa4688d203a5b49a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNzE3NzIyNA==&mid=2247691064&idx=8&sn=5a50fe94766a35cb7769460fb73e21ea&chksm=c310f2e5edc9499df5310118927f5682c55274fc7f396e75f87c1197b093aa4688d203a5b49a#rd", "authors": ["\u6df1\u5733\u5e02\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u534f\u4f1a"], "title": "\u3010\u4ea7\u4e1a\u8d44\u8baf\u3011\u901a\u4e497<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9738\u699c\u5168\u7403\u5f00\u6e90\u524d\u5341\uff0c\u5343\u95eeQwen3-Omni\u767b\u9876", "comment": "Source: WeChat, Published: 2025-09-29 10:10:20", "summary": "\u9664qwen3-omni\u5916\uff0c\u901a\u4e49\u5927\u6a21\u578b\u5bb6\u65cf\u4e2d\u7684\u89c6\u89c9\u7406\u89e3\u6a21\u578bqwen3-vl\u3001\u56fe\u50cf\u7f16\u8f91\u6a21\u578bqwen-image-edit-2509\u3001\u52a8\u4f5c\u751f\u6210\u6a21\u578bwan2.2-animate\u3001\u6df1\u5ea6\u7814\u7a76agent\u6a21\u578bdeepresearch\u7b49\u4e0d\u540c\u5c3a\u5bf8\u76846\u6b3e\u6a21\u578b\uff0c\u5747\u5165\u9009hugging face \u5168\u7403\u5f00\u6e90\u699c\u5355\u524d\u5341\u3002", "AI": {"tldr": "\u9664qwen3-omni\u5916\uff0c\u901a\u4e49\u5927\u6a21\u578b\u5bb6\u65cf\u4e2d\u7684\u89c6\u89c9\u7406\u89e3\u6a21\u578bqwen3-vl\u3001\u56fe\u50cf\u7f16\u8f91\u6a21\u578bqwen-image-edit-2509\u3001\u52a8\u4f5c\u751f\u6210\u6a21\u578bwan2.2-animate\u3001\u6df1\u5ea6\u7814\u7a76agent\u6a21\u578bdeepresearch\u7b49\u4e0d\u540c\u5c3a\u5bf8\u76846\u6b3e\u6a21\u578b\uff0c\u5747\u5165\u9009hugging face \u5168\u7403\u5f00\u6e90\u699c\u5355\u524d\u5341\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.ff3412f6", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4MjU5NDg2NA==&mid=2247509006&idx=4&sn=677e94f9f4628e27fde0ddd634fa66e7&chksm=ced52d47320fd40ea1e9f043dcec4583e7b3094d2a01034aca96e38848416920067570a62c6c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4MjU5NDg2NA==&mid=2247509006&idx=4&sn=677e94f9f4628e27fde0ddd634fa66e7&chksm=ced52d47320fd40ea1e9f043dcec4583e7b3094d2a01034aca96e38848416920067570a62c6c#rd", "authors": ["\u9152\u6cc9\u5e02\u5927\u6570\u636e\u4e2d\u5fc3"], "title": "\u6211\u56fd\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b9e\u73b0\u6279\u91cf\u201c\u4e0a\u8f66\u201d", "comment": "Source: WeChat, Published: 2025-09-29 10:10:14", "summary": "\u7ec4\u7ec7\u5efa\u8bbe\u7efc\u5408\u4ea4\u901a\u8fd0\u8f93\u5927\u6a21\u578b\uff0c\u52a0\u5feb\u666e\u53ca\u667a\u80fd\u4f53\u5e94\u7528\u3002\u4f5c\u4e3a\u6211\u56fd\u9996\u4e2a\u7ecf\u56fd\u52a1\u9662\u6279\u51c6\u7684\u56fd\u5bb6\u7ea7\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u4e13\u4e1a\u4f1a\u8bae\uff0c\u81ea2018\u5e74\u8d77\uff0c\u4e16\u754c\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u5927\u4f1a\u5df2\u8fde\u7eed\u4e3e\u529e\u4e03\u5c4a\u3002", "AI": {"tldr": "\u7ec4\u7ec7\u5efa\u8bbe\u7efc\u5408\u4ea4\u901a\u8fd0\u8f93\u5927\u6a21\u578b\uff0c\u52a0\u5feb\u666e\u53ca\u667a\u80fd\u4f53\u5e94\u7528\u3002\u4f5c\u4e3a\u6211\u56fd\u9996\u4e2a\u7ecf\u56fd\u52a1\u9662\u6279\u51c6\u7684\u56fd\u5bb6\u7ea7\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u4e13\u4e1a\u4f1a\u8bae\uff0c\u81ea2018\u5e74\u8d77\uff0c\u4e16\u754c\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u5927\u4f1a\u5df2\u8fde\u7eed\u4e3e\u529e\u4e03\u5c4a\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.555c5ea1", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU3MjkzNTgzOQ==&mid=2247485819&idx=1&sn=aa18cf2b2e53c88e6c0f6d2b9f732dc0&chksm=fdd74ea57798e347e1d0f9056fccd211d0b7a242ee12d3e21dc676716598f6954e82bbd9d27a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU3MjkzNTgzOQ==&mid=2247485819&idx=1&sn=aa18cf2b2e53c88e6c0f6d2b9f732dc0&chksm=fdd74ea57798e347e1d0f9056fccd211d0b7a242ee12d3e21dc676716598f6954e82bbd9d27a#rd", "authors": ["\u5927\u6a21\u578b\u6559\u7a0b"], "title": "\ud83d\udd25 \u4e09\u4e2a\u6708\u901f\u6210<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5de5\u7a0b\u5e08\uff1f\u96f6\u57fa\u7840\u5165\u95e8", "comment": "Source: WeChat, Published: 2025-09-29 09:57:12", "summary": "\u6280\u672f\u4e0e\u5e94\uff0cText2SQL\uff0cLangChain\uff0c\u7a0b\uff0ccursor \u53ef\u89c6\u5316\u5927\u5c4f\u642d\u5efa function calling\u4e0e\u8de8\u6a21\u578b\u534f\u4f5c 5-6\u5468 7-8\u5468 \u7ee7\u7eed\u5b66\u4e60ai\u5927\u6a21\u578b\u5e94\u7528\u6838\u5fc3\u5f00\u53d1\u5de5\u5177\u53ca \u5b66\u4e60coze\u548cdify\u5de5\u4f5c\u539f\u7406\u548c\u5e94\u7528\u6280 \u6280\u672f\uff0cmcp\u5e94\u7528\u4e0e\u5b9e\u6218\uff0cagent\u667a\u80fd\u4f53 \u672f\uff0c\u5305\u62eccoze\u5de5\u4f5c\u539f\u7406\u4e0e\u5e94\u7528\u5b9e", "AI": {"tldr": "\u6280\u672f\u4e0e\u5e94\uff0cText2SQL\uff0cLangChain\uff0c\u7a0b\uff0ccursor \u53ef\u89c6\u5316\u5927\u5c4f\u642d\u5efa function calling\u4e0e\u8de8\u6a21\u578b\u534f\u4f5c 5-6\u5468 7-8\u5468 \u7ee7\u7eed\u5b66\u4e60ai\u5927\u6a21\u578b\u5e94\u7528\u6838\u5fc3\u5f00\u53d1\u5de5\u5177\u53ca \u5b66\u4e60coze\u548cdify\u5de5\u4f5c\u539f\u7406\u548c\u5e94\u7528\u6280 \u6280\u672f\uff0cmcp\u5e94\u7528\u4e0e\u5b9e\u6218\uff0cagent\u667a\u80fd\u4f53 \u672f\uff0c\u5305\u62eccoze\u5de5\u4f5c\u539f\u7406\u4e0e\u5e94\u7528\u5b9e", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.39b76fe9", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDI3Njg2MA==&mid=2656498839&idx=1&sn=6d7dc99adf76e9c049fa0a90d29b976d&chksm=bc4f87a8a35f2b69d97b59a7f821eba7e0581676beb7930582de04a5f5182706af8318c13e0c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDI3Njg2MA==&mid=2656498839&idx=1&sn=6d7dc99adf76e9c049fa0a90d29b976d&chksm=bc4f87a8a35f2b69d97b59a7f821eba7e0581676beb7930582de04a5f5182706af8318c13e0c#rd", "authors": ["\u767e\u5ea6\u5730\u56fe"], "title": "\u5c0f\u5ea6\u60f3\u60f32.0\uff0c\u884c\u4e1a\u9996\u4e2a\u6df1\u5ea6\u878d\u5408\u7aef\u5230\u7aef\u8bed\u97f3\u8bed\u8a00<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u51fa\u884c\u667a\u80fd\u4f53", "comment": "Source: WeChat, Published: 2025-09-29 09:50:27", "summary": "\u5728\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u65b9\u9762\uff0c\u4f9d\u6258\u767e\u5ea6\u6700\u65b0\u4e00\u4ee3\u6587\u5fc3\u5927\u6a21\u578bX1.1\uff0c\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u6307\u4ee4\u9075\u5faa\u548c\u667a\u80fd\u4f53\u80fd\u529b\u4e0a\u5b9e\u73b0\u8de8\u8d8a\u5f0f\u63d0\u5347\uff0c\u4e3a\u8f66\u8f7d\u8bed\u4e49\u7406\u89e3\u4e0e\u667a\u80fd\u51b3\u7b56\u63d0\u4f9b\u575a\u5b9e\u5e95\u5ea7\uff1b", "AI": {"tldr": "\u5728\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u65b9\u9762\uff0c\u4f9d\u6258\u767e\u5ea6\u6700\u65b0\u4e00\u4ee3\u6587\u5fc3\u5927\u6a21\u578bX1.1\uff0c\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u6307\u4ee4\u9075\u5faa\u548c\u667a\u80fd\u4f53\u80fd\u529b\u4e0a\u5b9e\u73b0\u8de8\u8d8a\u5f0f\u63d0\u5347\uff0c\u4e3a\u8f66\u8f7d\u8bed\u4e49\u7406\u89e3\u4e0e\u667a\u80fd\u51b3\u7b56\u63d0\u4f9b\u575a\u5b9e\u5e95\u5ea7\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.e463cd79", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyOTAzNjc0OQ==&mid=2649701484&idx=1&sn=83fd1d524e6c660c878876a83ceeb720&chksm=f1b170509e554763269350eeecef7be22eb4115843844dc18a25f8adcd2a6ab4cef7df2139e1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyOTAzNjc0OQ==&mid=2649701484&idx=1&sn=83fd1d524e6c660c878876a83ceeb720&chksm=f1b170509e554763269350eeecef7be22eb4115843844dc18a25f8adcd2a6ab4cef7df2139e1#rd", "authors": ["\u804c\u5750\u6807\u5728\u7ebf"], "title": "\u804c\u5750\u6807\u5e26\u4f60\u4e86\u89e3AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0eAgent\u5f00\u53d1\u5de5\u7a0b\u5e08", "comment": "Source: WeChat, Published: 2025-09-29 09:12:00", "summary": "\u5927\u6a21\u578b\u901a\u8fc7\u6d77\u91cf\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u5177\u5907\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u53ef\u5b8c\u6210\u591a\u79cd\u4efb\u52a1\u800c\u4e0d\u9700\u91cd\u65b0\u8bad\u7ec3\u3002Agent\u7cfb\u7edf\uff1a\u5177\u5907\u81ea\u4e3b\u884c\u52a8\u80fd\u529b\u7684AI\u5728\u5927\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u8d4b\u4e88\u5176\u5de5\u5177\u8c03\u7528\u3001\u4efb\u52a1\u89c4\u5212\u3001\u73af\u5883\u4ea4\u4e92\u80fd\u529b\uff0c\u8ba9AI\u4ece\u201c\u88ab\u52a8\u54cd\u5e94\u201d\u53d8\u4e3a\u201c", "AI": {"tldr": "\u5927\u6a21\u578b\u901a\u8fc7\u6d77\u91cf\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u5177\u5907\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u53ef\u5b8c\u6210\u591a\u79cd\u4efb\u52a1\u800c\u4e0d\u9700\u91cd\u65b0\u8bad\u7ec3\u3002Agent\u7cfb\u7edf\uff1a\u5177\u5907\u81ea\u4e3b\u884c\u52a8\u80fd\u529b\u7684AI\u5728\u5927\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u8d4b\u4e88\u5176\u5de5\u5177\u8c03\u7528\u3001\u4efb\u52a1\u89c4\u5212\u3001\u73af\u5883\u4ea4\u4e92\u80fd\u529b\uff0c\u8ba9AI\u4ece\u201c\u88ab\u52a8\u54cd\u5e94\u201d\u53d8\u4e3a\u201c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
