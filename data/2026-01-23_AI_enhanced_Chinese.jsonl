{"id": "2601.15339", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15339", "abs": "https://arxiv.org/abs/2601.15339", "authors": ["Jayant Havare", "Ashish Mittal", "Srikanth Tamilselvam", "Ganesh Ramakrishnan"], "title": "Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding", "comment": null, "summary": "Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u8bed\u97f3\u9a71\u52a8\u7684\u4ee3\u7801\u7406\u89e3\u6846\u67b6\uff0c\u652f\u6301\u7528\u6237\u7528\u6bcd\u8bed\u8fdb\u884c\u8bed\u97f3\u67e5\u8be2\uff0c\u901a\u8fc7ASR\u8f6c\u5f55\u548cLLM\u5f15\u5bfc\u7684\u4ee3\u7801\u611f\u77e5ASR\u8f93\u51fa\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u95ee\u7b54\u548c\u68c0\u7d22\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u7406\u89e3\u7cfb\u7edf\u4e3b\u8981\u9762\u5411\u82f1\u8bed\u7528\u6237\u548c\u952e\u76d8\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u548c\u8bed\u97f3\u4f18\u5148\u573a\u666f\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u7279\u522b\u662f\u5728\u5370\u5ea6\u7b49\u5730\u533a\u3002\u8bed\u97f3\u754c\u9762\u867d\u7136\u66f4\u5177\u5305\u5bb9\u6027\uff0c\u4f46\u6d89\u53ca\u4ee3\u7801\u7684\u8bed\u97f3\u67e5\u8be2\u9762\u4e34\u975e\u6807\u51c6\u82f1\u8bed\u7528\u6cd5\u3001\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\u548c\u81ea\u5b9a\u4e49\u6807\u8bc6\u7b26\u7b49\u72ec\u7279\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u8bed\u97f3\u9a71\u52a8\u7684\u4ee3\u7801\u7406\u89e3\u6846\u67b6\uff1a1\uff09\u63a5\u53d7\u7528\u6237\u6bcd\u8bed\u7684\u8bed\u97f3\u67e5\u8be2\uff1b2\uff09\u4f7f\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u8fdb\u884c\u8f6c\u5f55\uff1b3\uff09\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u4ee3\u7801\u611f\u77e5\u7684ASR\u8f93\u51fa\u4f18\u5316\uff1b4\uff09\u4e0e\u4ee3\u7801\u6a21\u578b\u63a5\u53e3\uff0c\u5728CodeSearchNet\u3001CoRNStack\u548cCodeQA\u7b49\u57fa\u51c6\u4e0a\u6267\u884c\u4ee3\u7801\u95ee\u7b54\u548c\u68c0\u7d22\u4efb\u52a1\u3002", "result": "\u7cfb\u7edf\u5206\u6790\u4e86\u56db\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u5370\u5ea6\u8bed\u8a00\u548c\u82f1\u8bed\uff0c\u7cfb\u7edf\u6027\u5730\u63cf\u8ff0\u4e86\u8f6c\u5f55\u9519\u8bef\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002\u8bc6\u522b\u4e86ASR\u5728\u4ee3\u7801\u5904\u7406\u4e2d\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u8bc1\u660eLLM\u5f15\u5bfc\u7684\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u8f6c\u5f55\u548c\u4ee3\u7801\u7406\u89e3\u9636\u6bb5\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bed\u97f3\u754c\u9762\u9700\u8981\u4ee3\u7801\u654f\u611f\u7684\u9002\u5e94\u6027\u6539\u8fdb\uff0c\u5e76\u4e3a\u6784\u5efa\u9c81\u68d2\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u9a71\u52a8\u7f16\u7a0b\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.15352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.15352", "abs": "https://arxiv.org/abs/2601.15352", "authors": ["Adeyemi Adeseye", "Aisvarya Adeseye"], "title": "A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs", "comment": "Accepted and Waiting to be published ICAI'25: 27th International Conference on Artificial Intelligence https://american-cse.org/csce2025/conferences-ICAI", "summary": "Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684\u672c\u5730LLM\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bPython\u4ee3\u7801\u4e2d\u7684\u5faa\u73af\u6f0f\u6d1e\uff0c\u5305\u62ec\u63a7\u5236\u903b\u8f91\u9519\u8bef\u3001\u5b89\u5168\u98ce\u9669\u548c\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\uff0c\u6d4b\u8bd5\u663e\u793aPhi\u6a21\u578b\u4f18\u4e8eLLaMA\u3002", "motivation": "\u5faa\u73af\u6f0f\u6d1e\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u4e3b\u8981\u98ce\u9669\u6784\u9020\uff0c\u5bb9\u6613\u5bfc\u81f4\u65e0\u9650\u5faa\u73af\u3001\u8d44\u6e90\u8017\u5c3d\u6216\u903b\u8f91\u9519\u8bef\uff0c\u4f20\u7edf\u9759\u6001\u5206\u6790\u5668\u57fa\u4e8e\u8bed\u6cd5\u6a21\u5f0f\u96be\u4ee5\u68c0\u6d4b\u8bed\u4e49\u7f3a\u9677\uff0c\u800c\u672c\u5730LLM\u80fd\u89e3\u51b3\u9690\u79c1\u3001\u5ef6\u8fdf\u548c\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u63d0\u793a\u7684\u6846\u67b6\uff0c\u5229\u7528\u672c\u5730\u90e8\u7f72\u7684LLM\uff08LLaMA 3.2 3B\u548cPhi 3.5 4B\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\uff0c\u6846\u67b6\u5305\u542b\u8bed\u8a00\u7279\u5b9a\u610f\u8bc6\u3001\u4ee3\u7801\u611f\u77e5\u57fa\u7840\u3001\u7248\u672c\u654f\u611f\u6027\u548c\u5e7b\u89c9\u9884\u9632\u7b49\u5b89\u5168\u7279\u6027\u3002", "result": "Phi\u6a21\u578b\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8eLLaMA\uff0c\u9a8c\u8bc1\u4e86\u672c\u5730LLM\u5728\u4ee3\u7801\u6f0f\u6d1e\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u6709\u6548\u63d0\u793a\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u672c\u5730LLM\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5faa\u73af\u6f0f\u6d1e\uff0c\u8bbe\u8ba1\u826f\u597d\u7684\u63d0\u793a\u6846\u67b6\u5bf9\u4e8e\u5b9e\u73b0\u5b89\u5168\u51c6\u786e\u7684\u4ee3\u7801\u6f0f\u6d1e\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002", "topic": "code agent"}}
{"id": "2601.15879", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15879", "abs": "https://arxiv.org/abs/2601.15879", "authors": ["Jiajun Zhang", "Zeyu Cui", "Lei Zhang", "Jian Yang", "Jiaxi Yang", "Qiang Liu", "Zilei Wang", "Binyuan Hui", "Liang Wang", "Junyang Lin"], "title": "Evaluating and Achieving Controllable Code Completion in Code LLM", "comment": null, "summary": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.", "AI": {"tldr": "C3-Bench\uff1a\u9996\u4e2a\u6307\u4ee4\u5f15\u5bfc\u7684\u4ee3\u7801\u8865\u5168\u57fa\u51c6\uff0c\u5305\u542b2195\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u53d1\u73b0\u5f00\u6e90\u4e0e\u4e13\u6709\u6a21\u578b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u5408\u6210\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8865\u5168\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86LLM\u5728\u4ee3\u7801\u8865\u5168\u4e2d\u9075\u5faa\u7528\u6237\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u662fLLM\u8f85\u52a9\u7f16\u7a0b\u7684\u5e38\u89c1\u573a\u666f\u3002", "method": "\u6784\u5efaC3-Bench\u57fa\u51c6\uff082195\u4e2a\u4efb\u52a1\uff09\uff0c\u8bc4\u4f3040\u591a\u4e2a\u4e3b\u6d41LLM\uff0c\u5f00\u53d1\u57fa\u4e8eQwen2.5-Coder\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u9ad8\u8d28\u91cf\u6307\u4ee4-\u8865\u5168\u5bf9\u8fdb\u884cSFT\u5fae\u8c03\u3002", "result": "\u5f00\u6e90\u4e0e\u5148\u8fdb\u4e13\u6709\u6a21\u578b\u5728\u4ee3\u7801\u8865\u5168\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1bQwen2.5-Coder-C3\u6a21\u578b\u5728C3-Bench\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u589e\u5f3aLLM\u7684\u4ee3\u7801\u8865\u5168\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u5efa\u7acb\u4e86\u4ee3\u7801LLM\u672a\u6765\u7814\u7a76\u7684\u65b0\u65b9\u5411\uff0c\u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5747\u5df2\u5f00\u6e90\u3002", "topic": "swe benchmark"}}
{"id": "2601.15322", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15322", "abs": "https://arxiv.org/abs/2601.15322", "authors": ["Raffi Khatchadourian"], "title": "Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents", "comment": "23 pages, 5 figures, 9 tables. Code and data: https://github.com/ibm-client-engineering/output-drift-financial-llms", "summary": "LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.\n  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.\n  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDFAH\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u91d1\u878d\u9886\u57dfLLM\u4ee3\u7406\u7684\u8f68\u8ff9\u786e\u5b9a\u6027\u548c\u8bc1\u636e\u6761\u4ef6\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u786e\u5b9a\u6027\u4e0e\u5fe0\u5b9e\u6027\u6b63\u76f8\u5173\uff0c\u5e76\u63d0\u4f9b\u4e09\u4e2a\u91d1\u878d\u57fa\u51c6\u6d4b\u8bd5", "motivation": "LLM\u4ee3\u7406\u5728\u76d1\u7ba1\u5ba1\u8ba1\u56de\u653e\u4e2d\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\uff1a\u5f53\u8981\u6c42\u7528\u76f8\u540c\u8f93\u5165\u91cd\u73b0\u88ab\u6807\u8bb0\u7684\u4ea4\u6613\u51b3\u7b56\u65f6\uff0c\u5927\u591a\u6570\u90e8\u7f72\u65e0\u6cd5\u8fd4\u56de\u4e00\u81f4\u7ed3\u679c\u3002\u91d1\u878d\u670d\u52a1\u7684\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u9700\u8981\u786e\u5b9a\u6027\u548c\u5fe0\u5b9e\u6027\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165\u786e\u5b9a\u6027-\u5fe0\u5b9e\u6027\u4fdd\u8bc1\u6846\u67b6\uff08DFAH\uff09\uff0c\u6d4b\u91cf\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u8f68\u8ff9\u786e\u5b9a\u6027\u548c\u8bc1\u636e\u6761\u4ef6\u5fe0\u5b9e\u6027\u3002\u572874\u79cd\u914d\u7f6e\uff0812\u4e2a\u6a21\u578b\u30014\u4e2a\u63d0\u4f9b\u5546\uff09\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u63d0\u4f9b\u4e09\u4e2a\u91d1\u878d\u57fa\u51c6\u6d4b\u8bd5\uff08\u5408\u89c4\u5206\u7c7b\u3001\u6295\u8d44\u7ec4\u5408\u7ea6\u675f\u3001\u6570\u636e\u8fd0\u7ef4\u5f02\u5e38\uff09\u3002", "result": "7-20B\u53c2\u6570\u6a21\u578b\u5728\u975e\u4ee3\u7406\u57fa\u7ebf\u5b9e\u9a8c\u4e2d\u8fbe\u5230100%\u786e\u5b9a\u6027\uff0c\u800c120B+\u6a21\u578b\u9700\u89813.7\u500d\u66f4\u5927\u7684\u9a8c\u8bc1\u6837\u672c\u624d\u80fd\u8fbe\u5230\u540c\u7b49\u7edf\u8ba1\u53ef\u9760\u6027\u3002\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u5f15\u5165\u989d\u5916\u65b9\u5dee\u3002\u786e\u5b9a\u6027\u4e0e\u5fe0\u5b9e\u6027\u5448\u6b63\u76f8\u5173\uff08r=0.45\uff09\u3002Tier 1\u6a21\u578b\u5728DFAH\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u5ba1\u8ba1\u56de\u653e\u8981\u6c42\u7684\u786e\u5b9a\u6027\u6c34\u5e73\u3002", "conclusion": "DFAH\u6846\u67b6\u80fd\u6709\u6548\u8bc4\u4f30\u91d1\u878dLLM\u4ee3\u7406\u7684\u786e\u5b9a\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u786e\u5b9a\u6027\u4e0e\u80fd\u529b\u6b63\u76f8\u5173\u800c\u975e\u8d1f\u76f8\u5173\uff0c\u4e3a\u91d1\u878d\u76d1\u7ba1\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bc4\u4f30\u5de5\u5177\u548c\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2601.15728", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.15728", "abs": "https://arxiv.org/abs/2601.15728", "authors": ["Hangle Hu", "Chenyu Hou", "Bin Cao", "Ruizhe Li"], "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity", "comment": "8 pages, 7 figures", "summary": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.", "AI": {"tldr": "BIRD-Python \u57fa\u51c6\u6d4b\u8bd5\u63ed\u793aText-to-Python\u5728\u6570\u636e\u68c0\u7d22\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u903b\u8f91\u8865\u5168\u6846\u67b6\u89e3\u51b3\u6b67\u4e49\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e0eText-to-SQL\u76f8\u5f53\u7684\u6027\u80fd", "motivation": "\u73b0\u5b9e\u6570\u636e\u5206\u6790\u8d8a\u6765\u8d8a\u9700\u8981Python\u7b49\u901a\u7528\u7f16\u7a0b\u8bed\u8a00\u5904\u7406\u6587\u4ef6\u6570\u636e\u548c\u590d\u6742\u5de5\u4f5c\u6d41\uff0c\u4f46Text-to-Python\u5728\u6838\u5fc3\u6570\u636e\u68c0\u7d22\u65b9\u9762\u7684\u53ef\u9760\u6027\u76f8\u5bf9\u6210\u719f\u7684SQL\u751f\u6001\u7cfb\u7edf\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u5f15\u5165BIRD-Python\u57fa\u51c6\u8fdb\u884c\u8de8\u8303\u5f0f\u8bc4\u4f30\uff0c\u7cfb\u7edf\u4f18\u5316\u539f\u59cb\u6570\u636e\u96c6\u51cf\u5c11\u6807\u6ce8\u566a\u58f0\u5e76\u7edf\u4e00\u6267\u884c\u8bed\u4e49\uff1b\u63d0\u51fa\u903b\u8f91\u8865\u5168\u6846\u67b6(LCF)\uff0c\u901a\u8fc7\u878d\u5165\u6f5c\u5728\u9886\u57df\u77e5\u8bc6\u89e3\u51b3\u6b67\u4e49", "result": "\u6027\u80fd\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u7f3a\u5931\u9886\u57df\u4e0a\u4e0b\u6587\u800c\u975e\u4ee3\u7801\u751f\u6210\u56fa\u6709\u5c40\u9650\uff1b\u5f53\u8fd9\u4e9b\u5dee\u8ddd\u88ab\u586b\u8865\u540e\uff0cText-to-Python\u80fd\u8fbe\u5230\u4e0eText-to-SQL\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73", "conclusion": "Python\u53ef\u4f5c\u4e3a\u5206\u6790\u4ee3\u7406\u7684\u53ef\u884c\u57fa\u7840\uff0c\u524d\u63d0\u662f\u7cfb\u7edf\u80fd\u6709\u6548\u5c06\u6a21\u7cca\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u951a\u5b9a\u5230\u53ef\u6267\u884c\u7684\u903b\u8f91\u89c4\u8303\u4e2d", "topic": "code agent"}}
{"id": "2601.15479", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15479", "abs": "https://arxiv.org/abs/2601.15479", "authors": ["Sydney Anuyah", "Sneha Shajee-Mohan", "Ankit-Singh Chauhan", "Sunandan Chakraborty"], "title": "Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts", "comment": null, "summary": "The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \\textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \\textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).\n  The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($\u03ba\\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \\href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e8613\u4e2a\u5f00\u6e90LLM\u5728\u6587\u672c\u4e2d\u56e0\u679c\u53d1\u73b0\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u56e0\u679c\u68c0\u6d4b\u548c\u63d0\u53d6\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4e0d\u8db350%\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u5dee\u3002", "motivation": "\u4e3a\u4e86\u5b89\u5168\u5730\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u5230\u751f\u7269\u533b\u5b66\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u9700\u8981\u786e\u4fdd\u5b83\u4eec\u80fd\u591f\u8fdb\u884c\u56e0\u679c\u63a8\u7406\u3002\u8bba\u6587\u65e8\u5728\u8bc4\u4f30LLM\u5728\u6587\u672c\u4e2d\u56e0\u679c\u53d1\u73b0\u8fd9\u4e00\u57fa\u7840\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u752812\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e24\u79cd\u6838\u5fc3\u6280\u80fd\uff1a\u56e0\u679c\u68c0\u6d4b\uff08\u5224\u65ad\u6587\u672c\u662f\u5426\u5305\u542b\u56e0\u679c\u94fe\u63a5\uff09\u548c\u56e0\u679c\u63d0\u53d6\uff08\u63d0\u53d6\u5177\u4f53\u7684\u56e0\u679c\u77ed\u8bed\uff09\u3002\u6d4b\u8bd5\u4e86\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\uff0c\u5305\u62ec\u96f6\u6837\u672c\u3001\u601d\u7ef4\u94fe\u548c\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e25\u91cd\u4e0d\u8db3\uff1a\u6700\u4f73\u56e0\u679c\u68c0\u6d4b\u6a21\u578bDeepSeek-R1-Distill-Llama-70B\u5e73\u5747\u51c6\u786e\u7387\u4ec549.57%\uff0c\u6700\u4f73\u56e0\u679c\u63d0\u53d6\u6a21\u578bQwen2.5-Coder-32B-Instruct\u4ec547.12%\u3002\u6a21\u578b\u5728\u7b80\u5355\u3001\u663e\u5f0f\u3001\u5355\u53e5\u5173\u7cfb\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u4f46\u5728\u9690\u5f0f\u5173\u7cfb\u3001\u8de8\u591a\u53e5\u94fe\u63a5\u548c\u591a\u56e0\u679c\u5bf9\u7b49\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u6025\u5267\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u6587\u672c\u56e0\u679c\u53d1\u73b0\u4efb\u52a1\u4e0a\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u3002\u8bba\u6587\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2601.15487", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15487", "abs": "https://arxiv.org/abs/2601.15487", "authors": ["Chandan Kumar Sahu", "Premith Kumar Chilukuri", "Matthew Hetrich"], "title": "MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation", "comment": "12 pages, 2 figures, Submitted to ACL", "summary": "The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86MiRAGE\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u9886\u57df\u7279\u5b9a\u3001\u591a\u6a21\u6001\u3001\u591a\u8df3\u7684RAG\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u4e13\u4e1a\u6587\u6863\u590d\u6742\u6027\u7684\u95ee\u9898\u3002", "motivation": "RAG\u6280\u672f\u5411\u591a\u6a21\u6001\u3001\u9ad8\u98ce\u9669\u4f01\u4e1a\u5e94\u7528\u7684\u5feb\u901f\u53d1\u5c55\u8d85\u8fc7\u4e86\u9886\u57df\u7279\u5b9a\u8bc4\u4f30\u57fa\u51c6\u7684\u5f00\u53d1\u3002\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u4f9d\u8d56\u901a\u7528\u9886\u57df\u8bed\u6599\u5e93\u6216\u7eaf\u6587\u672c\u68c0\u7d22\uff0c\u65e0\u6cd5\u6355\u6349\u4e13\u4e1a\u6280\u672f\u6587\u6863\u7684\u590d\u6742\u6027\uff0c\u5176\u4e2d\u4fe1\u606f\u662f\u591a\u6a21\u6001\u7684\uff0c\u63a8\u7406\u9700\u8981\u7efc\u5408\u5206\u6563\u7684\u8bc1\u636e\u3002", "method": "\u5f15\u5165MiRAGE\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u4f5c\u7684\u4e13\u95e8\u667a\u80fd\u4f53\u7fa4\u751f\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u9886\u57df\u7279\u5b9a\u3001\u591a\u6a21\u6001\u3001\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u3002\u5305\u62ec\uff1a\u9012\u5f52\u4e0a\u4e0b\u6587\u4f18\u5316\u5faa\u73af\u805a\u5408\u5206\u6563\u8bc1\u636e\u3001\u5bf9\u6297\u6027\u9a8c\u8bc1\u667a\u80fd\u4f53\u4fdd\u8bc1\u4e8b\u5b9e\u57fa\u7840\u3001\u8bc6\u522b\u4e13\u5bb6\u89d2\u8272\u548c\u76f8\u5173\u9886\u57df\u7684\u667a\u80fd\u4f53\u6765\u6a21\u62df\u4e13\u5bb6\u8ba4\u77e5\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\uff08\u6cd5\u89c4\u3001\u91d1\u878d\u3001\u5b9a\u91cf\u751f\u7269\u5b66\u548c\u65b0\u95fb\uff09\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cMiRAGE\u751f\u6210\u7684\u6570\u636e\u96c6\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u63a8\u7406\u590d\u6742\u6027\uff08>2.3\u5e73\u5747\u8df3\u6570\uff09\u548c\u4e8b\u5b9e\u5fe0\u5b9e\u5ea6\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u5982\u679c\u6709\u56fe\u50cf\u6587\u672c\u63cf\u8ff0\uff0cMiRAGE\u53ef\u4ee5\u7531LLM\u9a71\u52a8\uff0c\u4f46\u89c6\u89c9\u57fa\u7840\u4ecd\u662f\u524d\u6cbf\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u5316\u521b\u5efa\u53cd\u6620\u4e13\u6709\u8bed\u6599\u5e93\u6f5c\u5728\u4e3b\u9898\u7ed3\u6784\u7684\u9ec4\u91d1\u6807\u51c6\u8bc4\u4f30\u6570\u636e\u96c6\uff0cMiRAGE\u4e3a\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e0b\u4e00\u4ee3\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "topic": "agent analysis"}}
{"id": "2601.15482", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15482", "abs": "https://arxiv.org/abs/2601.15482", "authors": ["Huayu Li", "ZhengXiao He", "Siyuan Tian", "Jinghao Wen", "Ao Li"], "title": "Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding", "comment": null, "summary": "Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.", "AI": {"tldr": "MFS\u5c06LLM\u89e3\u7801\u91cd\u6784\u4e3a\u8bc6\u522b\u6700\u4f18\u968f\u673a\u8fc7\u7a0b\u7684\u95ee\u9898\uff0c\u5229\u7528\u9785\u7406\u8bba\u8bbe\u8ba1\u7406\u8bba\u57fa\u7840\u7684\u7b97\u6cd5\uff0c\u5728\u63a8\u7406\u57fa\u51c6\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u6807\u51c6\u81ea\u56de\u5f52\u89e3\u7801\u662f\u77ed\u89c6\u7684\uff0c\u65e0\u6cd5\u627e\u5230\u5168\u5c40\u6700\u4f18\u63a8\u7406\u8def\u5f84\u3002\u73b0\u6709\u7684\u524d\u77bb\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u673a\u5236\u8fdb\u884c\u8def\u5f84\u8bc4\u4f30\u548c\u526a\u679d\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u63d0\u51fa\u9785\u524d\u77bb\u91c7\u6837(MFS)\u6846\u67b6\uff0c\u5c06LLM\u89e3\u7801\u5efa\u6a21\u4e3a\u8bc6\u522b\u6700\u4f18\u968f\u673a\u8fc7\u7a0b\u7684\u95ee\u9898\u3002\u5229\u7528\u9785\u7406\u8bba\uff1a1) \u57fa\u4e8eDoob\u5206\u89e3\u5b9a\u7406\u63a8\u5bfc\u6b65\u9aa4\u8bc4\u4f30\uff1b2) \u4f7f\u7528\u53ef\u9009\u505c\u6b62\u7406\u8bba\u8fdb\u884c\u8def\u5f84\u9009\u62e9\uff1b3) \u57fa\u4e8e\u9785\u6536\u655b\u5b9a\u7406\u8bbe\u8ba1\u81ea\u9002\u5e94\u505c\u6b62\u89c4\u5219\u3002", "result": "\u5728\u516d\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMFS\u5728\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "MFS\u4e3aLLM\u89e3\u7801\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u7684\u6846\u67b6\uff0c\u7528\u6982\u7387\u8bba\u539f\u7406\u66ff\u4ee3\u542f\u53d1\u5f0f\u673a\u5236\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2601.15495", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15495", "abs": "https://arxiv.org/abs/2601.15495", "authors": ["Yiyang Feng", "Zeming Chen", "Haotian Wu", "Jiawei Zhou", "Antoine Bosselut"], "title": "Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge", "comment": "Accepted to EACL 2026 (Main)", "summary": "A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.", "AI": {"tldr": "TRACK\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u5982\u4f55\u5904\u7406\u4e0e\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81\u7684\u65b0\u77e5\u8bc6\uff0c\u53d1\u73b0\u63d0\u4f9b\u66f4\u65b0\u4e8b\u5b9e\u53cd\u800c\u4f1a\u964d\u4f4e\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u77e5\u8bc6\u66f4\u65b0\u548c\u4e8b\u5b9e\u56de\u5fc6\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u77e5\u8bc6\u66f4\u65b0\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u63a8\u7406\uff0c\u7279\u522b\u662f\u5f53\u65b0\u77e5\u8bc6\u4e0e\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81\u65f6\u7684\u4f20\u64ad\u95ee\u9898\u3002", "method": "\u63d0\u51faTRACK\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e09\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u573a\u666f\uff08WIKI\u3001CODE\u3001MATH\uff09\uff0c\u5f15\u5165\u591a\u4e2a\u73b0\u5b9e\u51b2\u7a81\u6765\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\uff0c\u8bc4\u4f30LLMs\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u4f20\u64ad\u51b2\u7a81\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u66f4\u65b0\u4e8b\u5b9e\u8fdb\u884c\u63a8\u7406\u53cd\u800c\u6bd4\u4e0d\u63d0\u4f9b\u66f4\u65b0\u4e8b\u5b9e\u8868\u73b0\u66f4\u5dee\uff0c\u4e14\u968f\u7740\u63d0\u4f9b\u66f4\u591a\u66f4\u65b0\u4e8b\u5b9e\uff0c\u6027\u80fd\u9000\u5316\u52a0\u5267\u3002\u5931\u8d25\u539f\u56e0\u5305\u62ec\u65e0\u6cd5\u5fe0\u5b9e\u6574\u5408\u66f4\u65b0\u4e8b\u5b9e\uff0c\u4ee5\u53ca\u5373\u4f7f\u77e5\u8bc6\u6574\u5408\u6210\u529f\u4e5f\u5b58\u5728\u6709\u7f3a\u9677\u7684\u63a8\u7406\u3002", "conclusion": "TRACK\u4e3a\u6d4b\u91cf\u548c\u6307\u5bfc\u672a\u6765\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u4f20\u64ad\u51b2\u7a81\u77e5\u8bc6\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002", "topic": "agent analysis"}}
{"id": "2601.15511", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.15511", "abs": "https://arxiv.org/abs/2601.15511", "authors": ["Adam Szelestey", "Sofie van Engelen", "Tianhao Huang", "Justin Snelders", "Qintao Zeng", "Songgaojun Deng"], "title": "AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains", "comment": "13 pages, 4 figures, and 11 tables", "summary": "Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.\n  To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.", "AI": {"tldr": "AdversaRiskQA\uff1a\u9996\u4e2a\u9488\u5bf9\u5065\u5eb7\u3001\u91d1\u878d\u548c\u6cd5\u5f8b\u9886\u57df\u7684\u5bf9\u6297\u6027\u4e8b\u5b9e\u6027\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u5728\u5bf9\u6297\u6027\u865a\u5047\u4fe1\u606f\u4e0b\u7684\u9c81\u68d2\u6027", "motivation": "LLM\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u9886\u57df\u7279\u5b9a\u7684\u8d44\u6e90\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u5bf9\u6297\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u6ca1\u6709\u7814\u7a76\u8003\u5bdf\u6ce8\u5165\u865a\u5047\u4fe1\u606f\u5bf9\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165AdversaRiskQA\u57fa\u51c6\uff0c\u5305\u542b\u5065\u5eb7\u548c\u91d1\u878d\u6cd5\u5f8b\u4e24\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u63d0\u51fa\u4e24\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u8bc4\u4f30\u5bf9\u6297\u6027\u653b\u51fb\u6210\u529f\u7387\u548c\u957f\u6587\u672c\u4e8b\u5b9e\u6027\uff0c\u8bc4\u4f30\u4e86\u516d\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90LLM\u3002", "result": "Qwen3 (80B)\u5728\u6392\u9664\u65e0\u610f\u4e49\u54cd\u5e94\u540e\u83b7\u5f97\u6700\u9ad8\u5e73\u5747\u51c6\u786e\u7387\uff0cGPT-5\u4fdd\u6301\u7a33\u5b9a\u9ad8\u51c6\u786e\u7387\u3002\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u975e\u7ebf\u6027\u589e\u957f\uff0c\u4e0d\u540c\u9886\u57df\u8868\u73b0\u4e0d\u540c\uff0c\u96be\u5ea6\u7ea7\u522b\u5dee\u8ddd\u968f\u6a21\u578b\u589e\u5927\u800c\u7f29\u5c0f\u3002\u957f\u6587\u672c\u8bc4\u4f30\u663e\u793a\u6ce8\u5165\u865a\u5047\u4fe1\u606f\u4e0e\u6a21\u578b\u4e8b\u5b9e\u8f93\u51fa\u65e0\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "AdversaRiskQA\u4e3a\u8bc6\u522bLLM\u5f31\u70b9\u548c\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u9ad8\u98ce\u9669\u5e94\u7528\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2601.15551", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15551", "abs": "https://arxiv.org/abs/2601.15551", "authors": ["Bismack Tokoli", "Luis Jaimes", "Ayesha S. Dina"], "title": "ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance", "comment": "35 pages", "summary": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.", "AI": {"tldr": "ALIGNAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6559\u80b2\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u77e5\u8bc6\u4f30\u8ba1\u3001\u6280\u80fd\u5dee\u8ddd\u8bc6\u522b\u548c\u9488\u5bf9\u6027\u8d44\u6e90\u63a8\u8350\uff0c\u4e3a\u4e2a\u6027\u5316\u5b66\u4e60\u63d0\u4f9b\u81ea\u9002\u5e94\u5faa\u73af\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u5b66\u4e60\u7cfb\u7edf\u901a\u5e38\u53ea\u4e13\u6ce8\u4e8e\u77e5\u8bc6\u8ffd\u8e2a\u3001\u8bca\u65ad\u5efa\u6a21\u6216\u8d44\u6e90\u63a8\u8350\u4e2d\u7684\u67d0\u4e00\u9879\uff0c\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u7ec4\u4ef6\u6574\u5408\u6210\u8fde\u8d2f\u81ea\u9002\u5e94\u5faa\u73af\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51faALIGNAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u6280\u80fd\u5dee\u8ddd\u667a\u80fd\u4f53\u548c\u63a8\u8350\u667a\u80fd\u4f53\u3002\u9996\u5148\u5904\u7406\u5b66\u751f\u6d4b\u9a8c\u8868\u73b0\u3001\u6210\u7ee9\u6570\u636e\u548c\u504f\u597d\uff0c\u4f7f\u7528\u6982\u5ff5\u7ea7\u8bca\u65ad\u63a8\u7406\u751f\u6210\u4e3b\u9898\u7ea7\u719f\u7ec3\u5ea6\u4f30\u8ba1\uff0c\u8bc6\u522b\u5177\u4f53\u8bef\u89e3\u548c\u77e5\u8bc6\u7f3a\u9677\uff0c\u7136\u540e\u68c0\u7d22\u4e0e\u8bca\u65ad\u7f3a\u9677\u5bf9\u9f50\u7684\u504f\u597d\u611f\u77e5\u5b66\u4e60\u6750\u6599\uff0c\u5b9e\u73b0\u6301\u7eed\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728\u4e24\u4e2a\u672c\u79d1\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u57fa\u4e8eGPT-4o\u7684\u667a\u80fd\u4f53\u5728\u77e5\u8bc6\u719f\u7ec3\u5ea6\u4f30\u8ba1\u65b9\u9762\u8fbe\u52300.87-0.90\u7684\u7cbe\u786e\u5ea6\u548c0.84-0.87\u7684F1\u5206\u6570\uff0c\u4e0e\u5b9e\u9645\u8003\u8bd5\u8868\u73b0\u9a8c\u8bc1\u4e00\u81f4\u3002", "conclusion": "ALIGNAgent\u901a\u8fc7\u96c6\u6210\u77e5\u8bc6\u4f30\u8ba1\u3001\u6280\u80fd\u5dee\u8ddd\u8bc6\u522b\u548c\u8d44\u6e90\u63a8\u8350\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u4e2a\u6027\u5316\u5b66\u4e60\uff0c\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.15628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15628", "abs": "https://arxiv.org/abs/2601.15628", "authors": ["Haibo Tong", "Zeyang Yue", "Feifei Zhao", "Erliang Lin", "Lu Jia", "Ruolin Chen", "Yinqian Sun", "Qian Zhang", "Yi Zeng"], "title": "CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models", "comment": null, "summary": "Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.", "AI": {"tldr": "CogToM\u662f\u4e00\u4e2a\u5168\u9762\u7684\u3001\u7406\u8bba\u57fa\u7840\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8000\u591a\u4e2a\u53cc\u8bed\u5b9e\u4f8b\uff0c\u6db5\u76d646\u79cd\u8303\u5f0f\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5c40\u9650\u4e8e\u9519\u8bef\u4fe1\u5ff5\u4efb\u52a1\u7b49\u72ed\u7a84\u8303\u5f0f\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u4eba\u7c7b\u8ba4\u77e5\u673a\u5236\u7684\u5168\u8c8c\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u6765\u7814\u7a76LLMs\u662f\u5426\u771f\u6b63\u5177\u5907\u4eba\u7c7b\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002", "method": "\u5f00\u53d1CogToM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8000\u591a\u4e2a\u53cc\u8bed\u5b9e\u4f8b\uff0c\u6db5\u76d646\u79cd\u8303\u5f0f\uff0c\u753149\u540d\u4eba\u7c7b\u6807\u6ce8\u8005\u9a8c\u8bc1\u3002\u7cfb\u7edf\u8bc4\u4f3022\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u5305\u62ecGPT-5.1\u548cQwen3-Max\u7b49\u524d\u6cbf\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u6027\u80fd\u5b58\u5728\u663e\u8457\u5f02\u8d28\u6027\uff0c\u5728\u7279\u5b9a\u7ef4\u5ea6\u4e0a\u5b58\u5728\u6301\u7eed\u74f6\u9888\u3002\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u7684\u5206\u6790\u8868\u660eLLMs\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7ed3\u6784\u53ef\u80fd\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "CogToM\u4e3a\u7814\u7a76LLMs\u4e0d\u65ad\u6f14\u5316\u7684\u8ba4\u77e5\u8fb9\u754c\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5de5\u5177\u548c\u89c6\u89d2\uff0c\u63ed\u793a\u4e86LLMs\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u7684\u5c40\u9650\u6027\u53ca\u5176\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5dee\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2601.15652", "categories": ["cs.AI", "cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.15652", "abs": "https://arxiv.org/abs/2601.15652", "authors": ["Manish Bhatt"], "title": "Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models", "comment": null, "summary": "Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).\n  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises (\"Sycophancy\").\n  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u4fe1\u53f7\u8bbe\u8ba1\u4e0e\u76d1\u7763\u5b66\u4e60\u7684\u6df7\u5408\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u5e7b\u89c9\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6570\u636e\u6548\u7387\u9ad875\u500d\u3001\u63a8\u7406\u901f\u5ea6\u5feb1000\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "LLM\u5e7b\u89c9\uff08\u770b\u4f3c\u5408\u7406\u4f46\u4e8b\u5b9e\u4e0d\u5fe0\u5b9e\u7684\u751f\u6210\uff09\u662f\u9ad8\u98ce\u9669\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u5916\u90e8\u68c0\u7d22\u5faa\u73af\u6216\u4e0d\u900f\u660e\u7684\u9ed1\u76d2LLM\u6cd5\u5b98\uff08\u9700\u898170B+\u53c2\u6570\uff09\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u6df7\u5408\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u4fe1\u53f7\u8bbe\u8ba1\u4e0e\u76d1\u7763\u5b66\u4e60\u3002\u63d0\u53d6\u57fa\u4e8e\u9884\u6d4b\u7f16\u7801\uff08\u91cf\u5316\u5bf9\u5185\u90e8\u5148\u9a8c\u7684\u60ca\u8bb6\uff09\u548c\u4fe1\u606f\u74f6\u9888\uff08\u6d4b\u91cf\u6270\u52a8\u4e0b\u4fe1\u53f7\u4fdd\u7559\uff09\u7684\u53ef\u89e3\u91ca\u4fe1\u53f7\u3002\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\uff0c\u5f00\u53d1\u4e09\u4e2a\u5173\u952e\u589e\u5f3a\uff1a\u5b9e\u4f53\u805a\u7126\u5438\u6536\u3001\u4e0a\u4e0b\u6587\u4f9d\u4ece\u6027\u3001\u53ef\u8bc1\u4f2a\u6027\u8bc4\u5206\u3002", "result": "\u5728HaluBench\uff08n=200\uff0c\u5b8c\u7f8e\u5e73\u8861\uff09\u4e0a\uff0c\u7406\u8bba\u6307\u5bfc\u57fa\u7ebf\u8fbe\u52300.8017 AUROC\uff0c\u57fa\u7840\u76d1\u7763\u6a21\u578b\u8fbe\u52300.8274 AUROC\uff0c\u6539\u8fdb\u7279\u5f81\u63d0\u5347\u81f30.8669 AUROC\uff084.95%\u589e\u76ca\uff09\u3002\u4f7f\u7528\u6bd4Lynx\u5c1175\u500d\u7684\u8bad\u7ec3\u6570\u636e\uff08200 vs 15,000\u6837\u672c\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u5feb1000\u500d\uff085ms vs 5s\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u4fe1\u53f7\u67b6\u6784\u4e2d\u7f16\u7801\u7684\u9886\u57df\u77e5\u8bc6\u76f8\u6bd4\u6269\u5c55LLM\u6cd5\u5b98\u63d0\u4f9b\u66f4\u4f18\u7684\u6570\u636e\u6548\u7387\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\uff08\u5c11\u4e8e1M\u53c2\u6570\uff09\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u5b9e\u73b0\u5f3a\u6027\u80fd\uff0c\u9002\u5408\u751f\u4ea7\u90e8\u7f72\u3002\u5173\u952e\u8d1f\u9762\u53d1\u73b0\uff1a\u5408\u7406\u5316\u4fe1\u53f7\u65e0\u6cd5\u533a\u5206\u5e7b\u89c9\uff0c\u8868\u660eLLM\u4e3a\u865a\u5047\u524d\u63d0\u751f\u6210\u8fde\u8d2f\u63a8\u7406\uff08\"\u5949\u627f\"\u73b0\u8c61\uff09\u3002", "topic": "agent analysis"}}
{"id": "2601.15679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15679", "abs": "https://arxiv.org/abs/2601.15679", "authors": ["Ee Wei Seah", "Yongsen Zheng", "Naga Nikshith", "Mahran Morsidi", "Gabriel Waikin Loh Matienzo", "Nigel Gay", "Akriti Vij", "Benjamin Chua", "En Qi Ng", "Sharmini Johnson", "Vanessa Wilfred", "Wan Sie Lee", "Anna Davidson", "Catherine Devine", "Erin Zorer", "Gareth Holvey", "Harry Coppock", "James Walpole", "Jerome Wynee", "Magda Dubois", "Michael Schmatz", "Patrick Keane", "Sam Deverett", "Bill Black", "Bo Yan", "Bushra Sabir", "Frank Sun", "Hao Zhang", "Harriet Farlow", "Helen Zhou", "Lingming Dong", "Qinghua Lu", "Seung Jang", "Sharif Abuadbba", "Simon O'Callaghan", "Suyu Ma", "Tom Howroyd", "Cyrus Fung", "Fatemeh Azadi", "Isar Nejadgholi", "Krishnapriya Vishnubhotla", "Pulei Xiong", "Saeedeh Lohrasbi", "Scott Buffett", "Shahrear Iqbal", "Sowmya Vajjala", "Anna Safont-Andreu", "Luca Massarelli", "Oskar van der Wal", "Simon M\u00f6ller", "Agnes Delaborde", "Joris Dugu\u00e9p\u00e9roux", "Nicolas Rolin", "Romane Gallienne", "Sarah Behanzin", "Tom Seimandi", "Akiko Murakami", "Takayuki Semitsu", "Teresa Tsukiji", "Angela Kinuthia", "Michael Michie", "Stephanie Kasaon", "Jean Wangari", "Hankyul Baek", "Jaewon Noh", "Kihyuk Nam", "Sang Seo", "Sungpil Shin", "Taewhi Lee", "Yongsu Kim"], "title": "Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats", "comment": "The author/contributor list organises contributors by country and alphabetical order within each country. In some places, the order has been altered to match other related publications", "summary": "The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.\n  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.\n  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.\n  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.", "AI": {"tldr": "\u591a\u56fdAI\u5b89\u5168\u673a\u6784\u8054\u5408\u5f00\u5c55\u7b2c\u4e09\u6b21\u667a\u80fd\u4f53\u8bc4\u4f30\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8\u65b9\u6cd5\u5b66\u95ee\u9898\u800c\u975e\u6a21\u578b\u6027\u80fd\uff0c\u6db5\u76d6\u4fe1\u606f\u6cc4\u9732\u3001\u6b3a\u8bc8\u7b49\u901a\u7528\u98ce\u9669\u548c\u7f51\u7edc\u5b89\u5168\u4e24\u5927\u9886\u57df\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u548c\u667a\u80fd\u4f53\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u7684\u76d1\u7763\u51cf\u5c11\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u98ce\u9669\u3002\u667a\u80fd\u4f53\u6d4b\u8bd5\u4ecd\u5904\u4e8e\u65e9\u671f\u53d1\u5c55\u9636\u6bb5\uff0c\u9700\u8981\u5efa\u7acb\u8de8\u8bed\u8a00\u548c\u6587\u5316\u7684\u51c6\u786e\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u56fd\u9645\u7f51\u7edc\u6210\u5458\uff08\u65b0\u52a0\u5761\u3001\u65e5\u672c\u3001\u6fb3\u5927\u5229\u4e9a\u3001\u52a0\u62ff\u5927\u3001\u6b27\u76df\u59d4\u5458\u4f1a\u3001\u6cd5\u56fd\u3001\u80af\u5c3c\u4e9a\u3001\u97e9\u56fd\u3001\u82f1\u56fd\uff09\u8054\u5408\u5f00\u5c55\u7b2c\u4e09\u6b21\u6d4b\u8bd5\uff0c\u5206\u4e3a\u4e24\u4e2a\u65b9\u5411\uff1a(1) \u901a\u7528\u98ce\u9669\uff08\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u3001\u6b3a\u8bc8\uff09\uff0c\u7531\u65b0\u52a0\u5761AISI\u9886\u5bfc\uff1b(2) \u7f51\u7edc\u5b89\u5168\uff0c\u7531\u82f1\u56fdAISI\u9886\u5bfc\u3002\u4f7f\u7528\u516c\u5f00\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u57fa\u4e8e\u516c\u5171\u667a\u80fd\u4f53\u57fa\u51c6\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7531\u4e8e\u667a\u80fd\u4f53\u6d4b\u8bd5\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8fdb\u884c\u6b64\u7c7b\u6d4b\u8bd5\u7684\u65b9\u6cd5\u5b66\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u68c0\u67e5\u6d4b\u8bd5\u7ed3\u679c\u6216\u6a21\u578b\u80fd\u529b\u3002\u8fd9\u6b21\u5408\u4f5c\u6807\u5fd7\u7740\u53c2\u4e0e\u8005\u5171\u540c\u52aa\u529b\u63a8\u8fdb\u667a\u80fd\u4f53\u8bc4\u4f30\u79d1\u5b66\u7684\u91cd\u8981\u4e00\u6b65\u3002", "conclusion": "\u591a\u56fd\u5408\u4f5c\u5f00\u5c55\u667a\u80fd\u4f53\u8bc4\u4f30\u6d4b\u8bd5\u662f\u63a8\u8fdb\u667a\u80fd\u4f53\u8bc4\u4f30\u79d1\u5b66\u53d1\u5c55\u7684\u91cd\u8981\u6b65\u9aa4\uff0c\u91cd\u70b9\u5173\u6ce8\u65b9\u6cd5\u5b66\u5efa\u8bbe\uff0c\u4e3a\u672a\u6765\u5168\u7403AI\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u5960\u5b9a\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.15715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15715", "abs": "https://arxiv.org/abs/2601.15715", "authors": ["Zhitao He", "Zongwei Lyu", "Yi R Fung"], "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind", "comment": "Preprint, under review", "summary": "Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u5fc3\u7406\u7406\u8bba\u7684\u5b66\u672f\u53cd\u9a73\u6846\u67b6RebuttalAgent\uff0c\u901a\u8fc7\u5efa\u6a21\u5ba1\u7a3f\u4eba\u5fc3\u7406\u72b6\u6001\u3001\u5236\u5b9a\u8bf4\u670d\u7b56\u7565\u5e76\u751f\u6210\u7b56\u7565\u9a71\u52a8\u7684\u56de\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u672f\u53cd\u9a73\u8d28\u91cf\u3002", "motivation": "\u5b66\u672f\u53cd\u9a73\u662f\u7814\u7a76\u6d41\u7a0b\u4e2d\u91cd\u8981\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u6a21\u4eff\u8868\u9762\u8bed\u8a00\u800c\u7f3a\u4e4f\u6362\u4f4d\u601d\u8003\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u60c5\u51b5\u4e0b\u8fdb\u884c\u6709\u6548\u7684\u7b56\u7565\u6027\u6c9f\u901a\u3002", "method": "\u63d0\u51faToM-Strategy-Response\uff08TSR\uff09\u7ba1\u9053\uff0c\u901a\u8fc7\u5fc3\u7406\u7406\u8bba\u5efa\u6a21\u5ba1\u7a3f\u4eba\u5fc3\u7406\u72b6\u6001\uff1b\u6784\u5efaRebuttalBench\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03+\u57fa\u4e8e\u81ea\u5956\u52b1\u673a\u5236\u7684\u5f3a\u5316\u5b66\u4e60\uff1b\u5f00\u53d1Rebuttal-RM\u4e13\u4e1a\u8bc4\u4f30\u5668\u3002", "result": "RebuttalAgent\u5728\u81ea\u52a8\u6307\u6807\u4e0a\u6bd4\u57fa\u7840\u6a21\u578b\u5e73\u5747\u63d0\u534718.3%\uff0c\u5728\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u5148\u8fdb\u4e13\u6709\u6a21\u578b\uff1bRebuttal-RM\u8bc4\u4f30\u5668\u5728\u8bc4\u5206\u4e00\u81f4\u6027\u4e0a\u8d85\u8d8aGPT-4.1\u3002", "conclusion": "\u57fa\u4e8e\u5fc3\u7406\u7406\u8bba\u7684\u5b66\u672f\u53cd\u9a73\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u53cd\u9a73\u8d28\u91cf\uff0c\u4f46\u751f\u6210\u5185\u5bb9\u4ec5\u4f9b\u4f5c\u8005\u53c2\u8003\u548c\u542f\u53d1\uff0c\u4e0d\u80fd\u66ff\u4ee3\u4f5c\u8005\u7684\u6279\u5224\u6027\u5206\u6790\u548c\u56de\u5e94\u3002", "topic": "agent analysis"}}
{"id": "2601.15703", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15703", "abs": "https://arxiv.org/abs/2601.15703", "authors": ["Jiaxin Zhang", "Prafulla Kumar Choubey", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Agentic Uncertainty Quantification", "comment": "36 pages, 9 figures, 9 tables", "summary": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u8fc7\u7a0b\u4ee3\u7406\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u5316\u4e0d\u786e\u5b9a\u6027\u8f6c\u5316\u4e3a\u4e3b\u52a8\u63a7\u5236\u4fe1\u53f7\uff0c\u89e3\u51b3AI\u4ee3\u7406\u5728\u957f\u7a0b\u63a8\u7406\u4e2d\u7684\"\u5e7b\u89c9\u87ba\u65cb\"\u95ee\u9898", "motivation": "AI\u4ee3\u7406\u5728\u957f\u7a0b\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53ef\u9760\u6027\u53d7\"\u5e7b\u89c9\u87ba\u65cb\"\u4e25\u91cd\u5f71\u54cd\u2014\u2014\u65e9\u671f\u8ba4\u77e5\u9519\u8bef\u4f1a\u4e0d\u53ef\u9006\u5730\u4f20\u64ad\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u53ea\u4f5c\u4e3a\u88ab\u52a8\u4f20\u611f\u5668\u8bca\u65ad\u98ce\u9669\u800c\u4e0d\u89e3\u51b3\u95ee\u9898\uff0c\u800c\u81ea\u6211\u53cd\u601d\u673a\u5236\u5219\u5b58\u5728\u6301\u7eed\u6216\u6f2b\u65e0\u76ee\u7684\u7684\u4fee\u6b63\u95ee\u9898", "method": "\u63d0\u51fa\u7edf\u4e00\u7684Dual-Process Agentic UQ\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u673a\u5236\uff1a\u7cfb\u7edf1\uff08\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bb0\u5fc6UAM\uff09\u9690\u5f0f\u4f20\u64ad\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\u548c\u8bed\u4e49\u89e3\u91ca\u4ee5\u9632\u6b62\u76f2\u76ee\u51b3\u7b56\uff1b\u7cfb\u7edf2\uff08\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u53cd\u601dUAR\uff09\u5229\u7528\u8fd9\u4e9b\u89e3\u91ca\u4f5c\u4e3a\u7406\u6027\u7ebf\u7d22\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u89e6\u53d1\u6709\u9488\u5bf9\u6027\u7684\u63a8\u7406\u65f6\u89e3\u51b3", "result": "\u5728\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f00\u653e\u5f0f\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u8f68\u8ff9\u7ea7\u6821\u51c6", "conclusion": "AUQ\u6846\u67b6\u4ee3\u8868\u4e86\u5411\u53ef\u9760\u4ee3\u7406\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u52a8\u6001\u5e73\u8861\u9ad8\u6548\u6267\u884c\u548c\u6df1\u5ea6\u601d\u8003", "topic": "agent analysis"}}
{"id": "2601.15709", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15709", "abs": "https://arxiv.org/abs/2601.15709", "authors": ["Asim Biswal", "Chuan Lei", "Xiao Qin", "Aodong Li", "Balakrishnan Narayanaswamy", "Tim Kraska"], "title": "AgentSM: Semantic Memory for Agentic Text-to-SQL", "comment": null, "summary": "Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.", "AI": {"tldr": "AgentSM\u662f\u4e00\u4e2a\u7528\u4e8eText-to-SQL\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u8bb0\u5fc6\u6765\u63d0\u5347\u590d\u6742\u4f01\u4e1a\u573a\u666f\u4e0b\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1125%\u7684token\u4f7f\u7528\u548c35%\u7684\u8f68\u8ff9\u957f\u5ea6\uff0c\u5728Spider 2.0 Lite\u4e0a\u8fbe\u523044.8%\u7684SOTA\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709LLM-based Text-to-SQL\u7cfb\u7edf\u5728\u771f\u5b9e\u4f01\u4e1a\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u5927\u89c4\u6a21\u590d\u6742\u6a21\u5f0f\u3001\u591a\u6837\u5316SQL\u65b9\u8a00\u3001\u6602\u8d35\u7684\u591a\u6b65\u63a8\u7406\u3002\u73b0\u6709\u667a\u80fd\u4f53\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5982\u91cd\u590d\u6570\u636e\u5e93\u4ea4\u4e92\u3001\u8f93\u51fa\u4e0d\u4e00\u81f4\u3001\u5076\u5c14\u65e0\u6cd5\u751f\u6210\u6709\u6548\u7b54\u6848\u3002", "method": "\u63d0\u51faAgent Semantic Memory (AgentSM)\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u8bb0\u5fc6\u6765\u6307\u5bfc\u63a8\u7406\u3002\u4e0d\u540c\u4e8e\u539f\u59cb\u8349\u7a3f\u6216\u5411\u91cf\u68c0\u7d22\uff0cAgentSM\u6355\u83b7\u5148\u524d\u7684\u6267\u884c\u8f68\u8ff9\uff08\u6216\u5408\u6210\u7cbe\u5fc3\u7b56\u5212\u7684\u8f68\u8ff9\uff09\u4f5c\u4e3a\u7ed3\u6784\u5316\u7a0b\u5e8f\uff0c\u7cfb\u7edf\u6027\u5730\u91cd\u7528\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728Spider 2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentSM\u5e73\u5747\u51cf\u5c1125%\u7684token\u4f7f\u7528\u548c35%\u7684\u8f68\u8ff9\u957f\u5ea6\u3002\u5728Spider 2.0 Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523044.8%\u7684SOTA\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u51c6\u786e\u7387\u3002", "conclusion": "AgentSM\u901a\u8fc7\u8bed\u4e49\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86Text-to-SQL\u667a\u80fd\u4f53\u5728\u590d\u6742\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u63a8\u7406\u3002", "topic": "code agent"}}
{"id": "2601.15625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15625", "abs": "https://arxiv.org/abs/2601.15625", "authors": ["Zhiwei Zhang", "Fei Zhao", "Rui Wang", "Zezhong Wang", "Bin Liang", "Jiakang Wang", "Yao Hu", "Shaosheng Cao", "Kam-Fai Wong"], "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors", "comment": "8 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.", "AI": {"tldr": "\u63d0\u51faFission-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6267\u884c\u9519\u8bef\u8f6c\u5316\u4e3a\u7ea0\u6b63\u76d1\u7763\uff0c\u89e3\u51b3LLM\u5728\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u9047\u5230\u9519\u8bef\u540e\u65e0\u6cd5\u6709\u6548\u6062\u590d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9519\u8bef\u6062\u590d\u7387\u548c\u6574\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5de5\u5177\u8c03\u7528\u4e2d\u9047\u5230\u9519\u8bef\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u5bb9\u6613\u9677\u5165\u91cd\u590d\u65e0\u6548\u8c03\u7528\u800c\u65e0\u6cd5\u6839\u636e\u9519\u8bef\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\u3002\u6807\u51c6RL\u65b9\u6cd5\u5c06\u9519\u8bef\u89c6\u4e3a\u7a00\u758f\u8d1f\u5956\u52b1\uff0c\u7f3a\u4e4f\u6062\u590d\u6307\u5bfc\uff1b\u9884\u6536\u96c6\u7684\u9519\u8bef\u7ea0\u6b63\u6570\u636e\u96c6\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faFission-GRPO\u6846\u67b6\uff0c\u6838\u5fc3\u673a\u5236\u662f\u5c06\u6bcf\u4e2a\u5931\u8d25\u8f68\u8ff9\u5206\u88c2\u4e3a\u65b0\u7684\u8bad\u7ec3\u5b9e\u4f8b\uff1a\u901a\u8fc7\u5fae\u8c03\u7684\u9519\u8bef\u6a21\u62df\u5668\u63d0\u4f9b\u8bca\u65ad\u53cd\u9988\uff0c\u7136\u540e\u5728\u7b56\u7565\u4e0a\u91cd\u65b0\u91c7\u6837\u6062\u590d\u8f68\u8ff9\uff0c\u4f7f\u6a21\u578b\u80fd\u4ece\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u5177\u4f53\u9519\u8bef\u4e2d\u5b66\u4e60\u3002", "result": "\u5728BFCL v4 Multi-Turn\u57fa\u51c6\u4e0a\uff0cFission-GRPO\u5c06Qwen3-8B\u7684\u9519\u8bef\u6062\u590d\u7387\u63d0\u53475.7%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4ece42.75%\u63d0\u5347\u81f346.75%\uff0c\u4f18\u4e8eGRPO\u548c\u4e13\u95e8\u7684\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u3002", "conclusion": "Fission-GRPO\u901a\u8fc7\u5c06\u6267\u884c\u9519\u8bef\u8f6c\u5316\u4e3a\u7ea0\u6b63\u76d1\u7763\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u9519\u8bef\u6062\u590d\u95ee\u9898\uff0c\u4e3a\u53ef\u9760\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15737", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15737", "abs": "https://arxiv.org/abs/2601.15737", "authors": ["Hanning Zhang", "Ruida Wang", "Rui Pan", "Wenyuan Wang", "Bingxu Meng", "Tong Zhang"], "title": "PhysProver: Advancing Automatic Theorem Proving for Physics", "comment": "Preprint", "summary": "The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u589e\u5f3a\u7269\u7406\u9886\u57df\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaPhysLeanData\u6570\u636e\u96c6\u548c\u57fa\u4e8eRLVR\u8bad\u7ec3\u7684PhysProver\u6a21\u578b\uff0c\u5728\u7269\u7406\u5b50\u9886\u57df\u53d6\u5f972.4%\u63d0\u5347\uff0c\u5e76\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\u5c55\u73b01.3%\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u53ef\u9a8c\u8bc1\u8bed\u8a00\u4e0eLLMs\u7684\u7ed3\u5408\u5728\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5f62\u5f0f\u5316\u7269\u7406\u63a8\u7406\u9886\u57df\u5374\u9c9c\u6709\u5173\u6ce8\u3002\u7269\u7406\u9886\u57df\u540c\u6837\u4f9d\u8d56\u7c7b\u4f3c\u7684\u95ee\u9898\u89e3\u51b3\u548c\u5b9a\u7406\u8bc1\u660e\u6846\u67b6\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5f62\u5f0f\u5316\u7269\u7406\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\u3002", "method": "1) \u6784\u5efaPhysLeanData\u6570\u636e\u96c6\uff0c\u5305\u542b\u4ecePhysLean\u91c7\u6837\u7684\u5b9a\u7406\u548c\u57fa\u4e8e\u731c\u60f3\u7684\u5f62\u5f0f\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\u751f\u6210\u7684\u6570\u636e\uff1b2) \u57fa\u4e8eDeepSeek-Prover-V2-7B\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u5668\uff0c\u5e94\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u8bad\u7ec3PhysProver\u6a21\u578b\u3002", "result": "\u4ec5\u4f7f\u7528\u7ea65K\u8bad\u7ec3\u6837\u672c\uff0cPhysProver\u5728\u591a\u4e2a\u7269\u7406\u5b50\u9886\u57df\u5b9e\u73b0\u603b\u4f532.4%\u7684\u6539\u8fdb\u3002\u5728MiniF2F-Test\u6570\u5b66\u57fa\u51c6\u4e0a\u83b7\u5f971.3%\u63d0\u5347\uff0c\u8868\u660e\u6a21\u578b\u5728\u7269\u7406\u9886\u57df\u4e4b\u5916\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5f62\u5f0f\u5316\u6570\u5b66\u80fd\u529b\u7684\u589e\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c06\u5f62\u5f0f\u5316\u8bc1\u660e\u5668\u6269\u5c55\u5230\u6570\u5b66\u9886\u57df\u4e4b\u5916\u63d0\u4f9b\u4e86\u8303\u4f8b\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002\u4f5c\u8005\u5c06\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6a21\u578b\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15761", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15761", "abs": "https://arxiv.org/abs/2601.15761", "authors": ["Xiefeng Wu", "Mingyu Hu", "Shu Zhang"], "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning", "comment": "7 pages main text 2 page reference", "summary": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.", "AI": {"tldr": "SigEnt-SAC\u662f\u4e00\u79cd\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u7684\u79bb\u7b56\u7565actor-critic\u65b9\u6cd5\uff0c\u4ec5\u9700\u5355\u6761\u4e13\u5bb6\u8f68\u8ff9\uff0c\u901a\u8fc7sigmoid\u6709\u754c\u71b5\u9879\u9632\u6b62\u8d1f\u71b5\u9a71\u52a8\u7684OOD\u52a8\u4f5c\u4f18\u5316\uff0c\u51cf\u5c11Q\u51fd\u6570\u632f\u8361\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4f4e\u6210\u672cRL\u90e8\u7f72\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u3001\u5956\u52b1\u7a00\u758f\u548c\u89c6\u89c9\u89c2\u6d4b\u566a\u58f0\u7b49\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u6216\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u4f4e\u6210\u672c\u3001\u6570\u636e\u9700\u6c42\u5c11\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSigEnt-SAC\u79bb\u7b56\u7565actor-critic\u65b9\u6cd5\uff0c\u6838\u5fc3\u8bbe\u8ba1\u662fsigmoid\u6709\u754c\u71b5\u9879\uff0c\u9632\u6b62\u8d1f\u71b5\u9a71\u52a8\u7684\u5206\u5e03\u5916\u52a8\u4f5c\u4f18\u5316\uff0c\u51cf\u5c11Q\u51fd\u6570\u632f\u8361\uff0c\u4ec5\u9700\u5355\u6761\u4e13\u5bb6\u8f68\u8ff9\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u8f7bQ\u51fd\u6570\u632f\u8361\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\u8fbe\u5230100%\u6210\u529f\u7387\u3002\u5728\u56db\u79cd\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u4ec5\u9700\u5c11\u91cf\u771f\u5b9e\u4ea4\u4e92\u5c31\u80fd\u5b66\u4e60\u6210\u529f\u7b56\u7565\u3002", "conclusion": "SigEnt-SAC\u4e3a\u771f\u5b9e\u4e16\u754cRL\u90e8\u7f72\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u3001\u5b9e\u7528\u7684\u9014\u5f84\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u548c\u4ea4\u4e92\u5c31\u80fd\u5b66\u4e60\u6709\u6548\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15892", "abs": "https://arxiv.org/abs/2601.15892", "authors": ["Chenghao Fan", "Wen Heng", "Bo Li", "Sichen Liu", "Yuxuan Song", "Jing Su", "Xiaoye Qu", "Kai Shen", "Wei Wei"], "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "comment": null, "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "AI": {"tldr": "Stable-DiffCoder\u662f\u4e00\u4e2a\u57fa\u4e8e\u5757\u6269\u6563\u7684\u4ee3\u7801\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u5728\u4ee3\u7801\u7f16\u8f91\u3001\u63a8\u7406\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u5177\u6709\u975e\u987a\u5e8f\u751f\u6210\u548c\u66f4\u597d\u7684\u6570\u636e\u590d\u7528\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u4ee3\u7801DLLMs\u5728\u76f8\u540c\u9884\u7b97\u4e0b\u4ecd\u843d\u540e\u4e8eAR\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\uff0c\u5e76\u8bc1\u660e\u5176\u53ef\u4ee5\u8d85\u8d8aAR\u6a21\u578b\u3002", "method": "\u63d0\u51faStable-DiffCoder\uff0c\u57fa\u4e8eSeed-Coder\u67b6\u6784\uff0c\u91c7\u7528\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u9636\u6bb5\uff0c\u7ed3\u5408\u5b9a\u5236\u5316\u7684\u9884\u70ed\u7b56\u7565\u548c\u5757\u7ea7\u88c1\u526a\u566a\u58f0\u8c03\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u5b66\u4e60\u548c\u7a33\u5b9a\u8bad\u7ec3\u3002\u4ec5\u4f7f\u7528CPT\u548c\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u3002", "result": "\u5728\u76f8\u540c\u6570\u636e\u548c\u67b6\u6784\u4e0b\uff0cStable-DiffCoder\u5728\u5e7f\u6cdb\u7684\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6574\u4f53\u4f18\u4e8e\u5176AR\u5bf9\u5e94\u6a21\u578b\u3002\u4ec5\u4f7f\u7528CPT\u548cSFT\u9636\u6bb5\uff0c\u5c31\u8d85\u8d8a\u4e86\u591a\u79cd\u7ea68B\u53c2\u6570\u7684AR\u548cDLLMs\u6a21\u578b\uff0c\u8bc1\u660e\u6269\u6563\u8bad\u7ec3\u53ef\u4ee5\u63d0\u5347\u4ee3\u7801\u5efa\u6a21\u8d28\u91cf\u3002", "conclusion": "\u6269\u6563\u8bad\u7ec3\u4e0d\u4ec5\u80fd\u8d85\u8d8aAR\u8bad\u7ec3\uff0c\u5176\u4efb\u610f\u987a\u5e8f\u5efa\u6a21\u80fd\u529b\u8fd8\u80fd\u6539\u8fdb\u7ed3\u6784\u5316\u4ee3\u7801\u7684\u7f16\u8f91\u548c\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6709\u76ca\u4e8e\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u3002", "topic": "code agent"}}
{"id": "2601.15778", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15778", "abs": "https://arxiv.org/abs/2601.15778", "authors": ["Jiaxin Zhang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Agentic Confidence Calibration", "comment": "37 pages, 15 figures, 12 tables", "summary": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u4ee3\u7406\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86HTC\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u4ee3\u7406\u6267\u884c\u8f68\u8ff9\u7684\u5b8f\u89c2\u52a8\u6001\u548c\u5fae\u89c2\u7a33\u5b9a\u6027\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347AI\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u6821\u51c6\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u4ece\u88ab\u52a8\u8bed\u8a00\u6a21\u578b\u5411\u81ea\u4e3b\u7cfb\u7edf\u6f14\u8fdb\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u573a\u666f\u90e8\u7f72\u9762\u4e34\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u3002\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u9488\u5bf9\u9759\u6001\u5355\u8f6e\u8f93\u51fa\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u89e3\u51b3\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u72ec\u7279\u6311\u6218\uff0c\u5982\u8f68\u8ff9\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u3001\u5916\u90e8\u5de5\u5177\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u4e0d\u900f\u660e\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u6574\u4f53\u8f68\u8ff9\u6821\u51c6\uff08HTC\uff09\u8bca\u65ad\u6846\u67b6\uff0c\u4ece\u4ee3\u7406\u7684\u6574\u4e2a\u6267\u884c\u8f68\u8ff9\u4e2d\u63d0\u53d6\u4e30\u5bcc\u7684\u6d41\u7a0b\u7ea7\u7279\u5f81\uff0c\u6db5\u76d6\u5b8f\u89c2\u52a8\u6001\u5230\u5fae\u89c2\u7a33\u5b9a\u6027\u3002\u91c7\u7528\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u901a\u7528\u4ee3\u7406\u6821\u51c6\u5668\uff08GAC\uff09\u5b9e\u73b0\u8de8\u57df\u6cdb\u5316\u3002", "result": "HTC\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u3001\u591a\u4e2aLLM\u548c\u4e0d\u540c\u4ee3\u7406\u6846\u67b6\u4e2d\uff0c\u5728\u6821\u51c6\u548c\u5224\u522b\u65b9\u9762\u5747\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u3002GAC\u5728\u8de8\u57dfGAIA\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6821\u51c6\uff08\u6700\u4f4eECE\uff09\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u4ee5\u6d41\u7a0b\u4e3a\u4e2d\u5fc3\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u8303\u5f0f\uff0c\u4e3a\u8bca\u65ad\u548c\u589e\u5f3aAI\u4ee3\u7406\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e09\u5927\u8fdb\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.15808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15808", "abs": "https://arxiv.org/abs/2601.15808", "authors": ["Yuxuan Wan", "Tianqing Fang", "Zaitang Li", "Yintong Huo", "Wenxuan Wang", "Haitao Mi", "Dong Yu", "Michael R. Lyu"], "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification", "comment": null, "summary": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.", "AI": {"tldr": "\u63d0\u51faDeepVerifier\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u7ed3\u679c\u5956\u52b1\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u9a8c\u8bc1\u5b9e\u73b0\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\uff0c\u5728GAIA\u548cXBench-DeepResearch\u4e0a\u63d0\u53478%-11%\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\uff08DRAs\uff09\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u540e\u8bad\u7ec3\u589e\u5f3a\u7b56\u7565\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5728\u63a8\u7406\u65f6\u81ea\u6211\u8fdb\u5316\u7684\u673a\u5236\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u667a\u80fd\u4f53\u8f93\u51fa\u5e76\u4ea7\u751f\u8fed\u4ee3\u53cd\u9988\u7684\u65b9\u6cd5\u3002", "method": "1. \u57fa\u4e8e\u81ea\u52a8\u6784\u5efa\u7684DRA\u5931\u8d25\u5206\u7c7b\u6cd5\u5236\u5b9a\u8bc4\u4f30\u89c4\u5219\uff1b2. \u5f00\u53d1DeepVerifier\u89c4\u5219\u9a8c\u8bc1\u5668\uff0c\u5229\u7528\u9a8c\u8bc1\u7684\u4e0d\u5bf9\u79f0\u6027\uff1b3. \u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u5728\u63a8\u7406\u65f6\u96c6\u6210\uff1b4. \u751f\u6210\u8be6\u7ec6\u89c4\u5219\u53cd\u9988\u4f9b\u667a\u80fd\u4f53\u8fed\u4ee3\u4f18\u5316\uff1b5. \u53d1\u5e03DeepVerifier-4K\u9ad8\u8d28\u91cf\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6", "result": "1. DeepVerifier\u5728\u5143\u8bc4\u4f30F1\u5206\u6570\u4e0a\u6bd4\u57fa\u51c6\u65b9\u6cd5\u63d0\u534712%-48%\uff1b2. \u5728GAIA\u548cXBench-DeepResearch\u7684\u6311\u6218\u6027\u5b50\u96c6\u4e0a\u5b9e\u73b08%-11%\u51c6\u786e\u7387\u63d0\u5347\uff1b3. \u652f\u6301\u5f00\u6e90\u6a21\u578b\u53d1\u5c55\u9a8c\u8bc1\u80fd\u529b", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u8303\u5f0f\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u9a8c\u8bc1\u5b9e\u73b0\u80fd\u529b\u63d0\u5347\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u9a8c\u8bc1\u5668\u7684\u9ad8\u6548\u6027\u548c\u6570\u636e\u96c6\u7684\u5f00\u6e90\u53d1\u5e03\u4e3a\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.15812", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15812", "abs": "https://arxiv.org/abs/2601.15812", "authors": ["Shir Ashury-Tahan", "Yifan Mai", "Elron Bandel", "Michal Shmueli-Scheuer", "Leshem Choshen"], "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models", "comment": null, "summary": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.", "AI": {"tldr": "ErrorMap\u662f\u4e00\u79cd\u5206\u6790LLM\u5931\u8d25\u539f\u56e0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u6a21\u578b\u7684\"\u5931\u8d25\u7b7e\u540d\"\u6765\u8bc6\u522b\u9519\u8bef\u6765\u6e90\uff0c\u521b\u5efa\u4e86ErrorAtlas\u9519\u8bef\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22\u7684\u9519\u8bef\u7c7b\u578b\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u53ea\u80fd\u544a\u8bc9\u6211\u4eec\u6a21\u578b\u4f55\u65f6\u5931\u8d25\uff0c\u4f46\u4e0d\u80fd\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5931\u8d25\u3002\u9519\u8bef\u7684\u7b54\u6848\u53ef\u80fd\u6e90\u4e8e\u683c\u5f0f\u95ee\u9898\u3001\u8ba1\u7b97\u9519\u8bef\u6216\u6570\u636e\u96c6\u566a\u58f0\uff0c\u800c\u975e\u63a8\u7406\u80fd\u529b\u5f31\u3002\u5982\u679c\u4e0d\u533a\u5206\u8fd9\u4e9b\u539f\u56e0\uff0c\u57fa\u51c6\u6d4b\u8bd5\u5c31\u4e0d\u5b8c\u6574\uff0c\u65e0\u6cd5\u53ef\u9760\u6307\u5bfc\u6a21\u578b\u6539\u8fdb\u3002", "method": "ErrorMap\u65b9\u6cd5\u63d0\u53d6\u6a21\u578b\u7684\u72ec\u7279\"\u5931\u8d25\u7b7e\u540d\"\uff0c\u6f84\u6e05\u57fa\u51c6\u6d4b\u8bd5\u6d4b\u91cf\u5185\u5bb9\uff0c\u6269\u5927\u9519\u8bef\u8bc6\u522b\u8303\u56f4\u4ee5\u51cf\u5c11\u76f2\u70b9\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u6a21\u578b\u6216\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u76f8\u540c\u903b\u8f91\u3002\u4f5c\u8005\u5c06\u5176\u5e94\u7528\u4e8e35\u4e2a\u6570\u636e\u96c6\u548c83\u4e2a\u6a21\u578b\uff0c\u751f\u6210\u4e86ErrorAtlas\u9519\u8bef\u5206\u7c7b\u6cd5\u3002", "result": "ErrorAtlas\u63ed\u793a\u4e86\u6a21\u578b\u9519\u8bef\u7684\u91cd\u590d\u6a21\u5f0f\uff0c\u7a81\u51fa\u4e86\u5f53\u524dLLM\u7814\u7a76\u4e2d\u672a\u5145\u5206\u63a2\u7d22\u7684\u9519\u8bef\u7c7b\u578b\uff0c\u5982\u8f93\u51fa\u4e2d\u9057\u6f0f\u5fc5\u8981\u7ec6\u8282\u548c\u95ee\u9898\u8bef\u89e3\u3002\u901a\u8fc7\u5c06\u7126\u70b9\u4ece\u6a21\u578b\u6210\u529f\u4e4b\u5904\u8f6c\u79fb\u5230\u5931\u8d25\u539f\u56e0\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7ea7\u7684\u8bc4\u4f30\u3002", "conclusion": "ErrorMap\u548cErrorAtlas\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4efb\u52a1\u7ea7\u6307\u6807\u7684\u6210\u529f\u8861\u91cf\uff0c\u5f15\u5165\u4e86\u53ef\u5728\u6a21\u578b\u548c\u4efb\u52a1\u95f4\u5168\u5c40\u5e94\u7528\u7684\u66f4\u6df1\u5c42\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u884c\u4e3a\u548c\u5c40\u9650\u6027\u7684\u66f4\u4e30\u5bcc\u6d1e\u5bdf\u3002", "topic": "agent analysis"}}
{"id": "2601.15876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15876", "abs": "https://arxiv.org/abs/2601.15876", "authors": ["Taofeng Xue", "Chong Peng", "Mianqiu Huang", "Linsen Guo", "Tiancheng Han", "Haozhe Wang", "Jianing Wang", "Xiaocheng Zhang", "Xin Yang", "Dengchang Zhao", "Jinrui Ding", "Xiandi Ma", "Yuchen Xie", "Peng Pei", "Xunliang Cai", "Xipeng Qiu"], "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "comment": "26 pages, 8 figures", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "AI": {"tldr": "EvoCUA\uff1a\u901a\u8fc7\u8fdb\u5316\u5b66\u4e60\u5faa\u73af\uff08\u6570\u636e\u751f\u6210+\u7b56\u7565\u4f18\u5316\uff09\u7a81\u7834\u9759\u6001\u6570\u636e\u9650\u5236\uff0c\u5b9e\u73b0\u539f\u751f\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff0c\u5728OSWorld\u57fa\u51c6\u4e0a\u8fbe\u523056.7%\u6210\u529f\u7387\uff0c\u521b\u5f00\u6e90SOTA\u3002", "motivation": "\u5f53\u524d\u539f\u751f\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUA\uff09\u53d7\u9650\u4e8e\u9759\u6001\u6570\u636e\u6269\u5c55\u74f6\u9888\uff0c\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u88ab\u52a8\u6a21\u4eff\u96be\u4ee5\u6355\u6349\u957f\u65f6\u7a0b\u8ba1\u7b97\u673a\u4efb\u52a1\u4e2d\u7684\u590d\u6742\u56e0\u679c\u52a8\u6001\u3002", "method": "1\uff09\u53ef\u9a8c\u8bc1\u5408\u6210\u5f15\u64ce\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\u53ca\u53ef\u6267\u884c\u9a8c\u8bc1\u5668\uff1b2\uff09\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u534f\u8c03\u6570\u4e07\u4e2a\u5f02\u6b65\u6c99\u7bb1\u73af\u5883\uff1b3\uff09\u8fed\u4ee3\u8fdb\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u8bc6\u522b\u80fd\u529b\u8fb9\u754c\u52a8\u6001\u8c03\u8282\u7b56\u7565\u66f4\u65b0\uff0c\u5c06\u5931\u8d25\u8f68\u8ff9\u8f6c\u5316\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u4e0a\u8fbe\u523056.7%\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e4b\u524d\u6700\u4f73\u5f00\u6e90\u6a21\u578bOpenCUA-72B\uff0845.0%\uff09\u548c\u9886\u5148\u95ed\u6e90\u6a21\u578bUI-TARS-2\uff0853.1%\uff09\u3002\u8fdb\u5316\u8303\u5f0f\u5728\u4e0d\u540c\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u4e0a\u5747\u5e26\u6765\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8e\u7ecf\u9a8c\u5b66\u4e60\u7684\u8fdb\u5316\u8303\u5f0f\u4e3a\u63d0\u5347\u539f\u751f\u4ee3\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7a33\u5065\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u7a81\u7834\u4e86\u9759\u6001\u6a21\u4eff\u7684\u9650\u5236\u3002", "topic": "code agent"}}
{"id": "2601.15801", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15801", "abs": "https://arxiv.org/abs/2601.15801", "authors": ["Fengheng Chu", "Jiahao Chen", "Yuhong Wang", "Jun Wang", "Zhihui Fu", "Shouling Ji", "Songze Li"], "title": "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models", "comment": null, "summary": "While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}ptimization for \\textbf{S}afety \\textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.", "AI": {"tldr": "\u63d0\u51faGOSV\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u8bc6\u522b\u5b89\u5168\u5173\u952e\u6ce8\u610f\u529b\u5934\uff0c\u53d1\u73b0\u5bf9\u9f50LLMs\u4e2d\u5b58\u5728\u6076\u610f\u6ce8\u5165\u5411\u91cf\u548c\u5b89\u5168\u6291\u5236\u5411\u91cf\u4e24\u79cd\u7a7a\u95f4\u5206\u79bb\u7684\u5b89\u5168\u5411\u91cf\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684\u767d\u76d2\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u8d2a\u5fc3\u5f52\u56e0\uff0c\u5047\u8bbe\u7ec4\u4ef6\u72ec\u7acb\u8d21\u732e\uff0c\u5ffd\u89c6\u4e86LLM\u4e2d\u4e0d\u540c\u7ec4\u4ef6\uff08\u5982\u6ce8\u610f\u529b\u5934\uff09\u4e4b\u95f4\u7684\u534f\u540c\u4ea4\u4e92\u5bf9\u5b89\u5168\u673a\u5236\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u5bf9\u5b89\u5168\u7ec4\u4ef6\u7406\u89e3\u6709\u9650\u3002", "method": "\u63d0\u51faGOSV\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u540c\u65f6\u5bf9\u6240\u6709\u6ce8\u610f\u529b\u5934\u8fdb\u884c\u4f18\u5316\uff0c\u91c7\u7528\u6709\u5bb3\u8865\u4e01\u548c\u96f6\u6d88\u878d\u4e24\u79cd\u4e92\u8865\u7684\u6fc0\u6d3b\u91cd\u8865\u4e01\u7b56\u7565\uff0c\u8bc6\u522b\u5b89\u5168\u5173\u952e\u6ce8\u610f\u529b\u5934\u3002", "result": "\u53d1\u73b0\u5bf9\u9f50LLMs\u4e2d\u5b58\u5728\u7a7a\u95f4\u5206\u79bb\u7684\u4e24\u7ec4\u5b89\u5168\u5411\u91cf\uff08\u6076\u610f\u6ce8\u5165\u5411\u91cf\u548c\u5b89\u5168\u6291\u5236\u5411\u91cf\uff09\uff0c\u5f53\u7ea630%\u7684\u603b\u5934\u88ab\u91cd\u8865\u4e01\u65f6\u4f1a\u53d1\u751f\u5b8c\u5168\u5b89\u5168\u5d29\u6e83\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1\u7684\u767d\u76d2\u8d8a\u72f1\u653b\u51fb\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GOSV\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u5b89\u5168\u53ef\u89e3\u91ca\u6027\uff0c\u63ed\u793a\u4e86LLMs\u901a\u8fc7\u5206\u79bb\u529f\u80fd\u901a\u8def\u7ef4\u62a4\u5b89\u5168\u673a\u5236\uff0c\u4e3a\u5b89\u5168\u5206\u6790\u548c\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2601.16206", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16206", "abs": "https://arxiv.org/abs/2601.16206", "authors": ["Daixuan Cheng", "Shaohan Huang", "Yuxian Gu", "Huatong Song", "Guoxin Chen", "Li Dong", "Wayne Xin Zhao", "Ji-Rong Wen", "Furu Wei"], "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "comment": "Project Page: https://llm-in-sandbox.github.io", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "AI": {"tldr": "LLM-in-Sandbox \u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6c99\u76d2\uff08\u865a\u62df\u8ba1\u7b97\u673a\uff09\u4e2d\u63a2\u7d22\uff0c\u4ee5\u6fc0\u53d1\u975e\u4ee3\u7801\u9886\u57df\u7684\u901a\u7528\u667a\u80fd\u3002\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5f3aLLMs\u5c31\u80fd\u5229\u7528\u6c99\u76d2\u5904\u7406\u975e\u4ee3\u7801\u4efb\u52a1\uff0c\u5982\u83b7\u53d6\u5916\u90e8\u77e5\u8bc6\u3001\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u3001\u6267\u884c\u811a\u672c\u7b49\u3002\u901a\u8fc7LLM-in-Sandbox-RL\u5f3a\u5316\u5b66\u4e60\uff0c\u4ec5\u4f7f\u7528\u975e\u667a\u80fd\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6c99\u76d2\u63a2\u7d22\uff0c\u53ef\u589e\u5f3a\u8fd9\u4e9b\u667a\u80fd\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u4ee3\u7801\u9886\u57df\u5c55\u73b0\u901a\u7528\u667a\u80fd\u3002\u901a\u8fc7\u4ee3\u7801\u6c99\u76d2\u73af\u5883\uff0c\u4f7fLLMs\u80fd\u591f\u50cf\u5728\u771f\u5b9e\u8ba1\u7b97\u673a\u4e2d\u4e00\u6837\u64cd\u4f5c\uff0c\u4ece\u800c\u6269\u5c55\u5176\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u533b\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faLLM-in-Sandbox\u6846\u67b6\uff0c\u8ba9LLMs\u5728\u4ee3\u7801\u6c99\u76d2\uff08\u865a\u62df\u8ba1\u7b97\u673a\uff09\u4e2d\u81ea\u4e3b\u63a2\u7d22\u3002\u5305\u62ec\u4e24\u79cd\u65b9\u5f0f\uff1a1) \u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5f3aLLMs\u81ea\u53d1\u5229\u7528\u6c99\u76d2\u529f\u80fd\uff1b2) LLM-in-Sandbox-RL\uff0c\u4ec5\u4f7f\u7528\u975e\u667a\u80fd\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6c99\u76d2\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "LLM-in-Sandbox\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u533b\u5b66\u3001\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u6307\u4ee4\u8ddf\u968f\u7b49\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8bad\u7ec3\u514d\u8d39\u548c\u8bad\u7ec3\u540e\u8bbe\u7f6e\u90fd\u53d6\u5f97\u4e86\u7a33\u5065\u8868\u73b0\u3002\u6846\u67b6\u5df2\u5f00\u6e90\u4e3aPython\u5305\uff0c\u4fbf\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "LLM-in-Sandbox\u6210\u529f\u5730\u5c06\u4ee3\u7801\u6c99\u76d2\u4f5c\u4e3a\u901a\u7528\u667a\u80fd\u7684\u6fc0\u53d1\u73af\u5883\uff0c\u4f7fLLMs\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u975e\u4ee3\u7801\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u4e3aLLMs\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u6c99\u76d2\u73af\u5883\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2601.15953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15953", "abs": "https://arxiv.org/abs/2601.15953", "authors": ["Yongyi Wang", "Hanyu Liu", "Lingfeng Li", "Bozhou Chen", "Ang Li", "Qirui Zheng", "Xionghui Yang", "Wenxin Li"], "title": "Decoupling Return-to-Go for Efficient Decision Transformer", "comment": null, "summary": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u89e3\u8026\u51b3\u7b56\u53d8\u6362\u5668\uff08DDT\uff09\uff0c\u901a\u8fc7\u6d88\u9664RTG\u5e8f\u5217\u5197\u4f59\uff0c\u4ec5\u4f7f\u7528\u6700\u65b0RTG\u6307\u5bfc\u52a8\u4f5c\u9884\u6d4b\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u51b3\u7b56\u53d8\u6362\u5668\uff08DT\uff09\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u91c7\u7528\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f46\u5b58\u5728\u8bbe\u8ba1\u5197\u4f59\uff1a\u5c06\u6574\u4e2aRTG\u5e8f\u5217\u8f93\u5165\u53d8\u6362\u5668\u7406\u8bba\u4e0a\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u56e0\u4e3a\u53ea\u6709\u6700\u65b0\u7684RTG\u5f71\u54cd\u52a8\u4f5c\u9884\u6d4b\u3002\u8fd9\u79cd\u5197\u4f59\u53ef\u80fd\u635f\u5bb3DT\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u51b3\u7b56\u53d8\u6362\u5668\uff08DDT\uff09\uff0c\u7b80\u5316\u67b6\u6784\uff1a\u4ec5\u901a\u8fc7\u53d8\u6362\u5668\u5904\u7406\u89c2\u6d4b\u548c\u52a8\u4f5c\u5e8f\u5217\uff0c\u4f7f\u7528\u6700\u65b0\u7684RTG\u6765\u6307\u5bfc\u52a8\u4f5c\u9884\u6d4b\uff0c\u4ece\u800c\u6d88\u9664RTG\u5e8f\u5217\u7684\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDDT\u663e\u8457\u4f18\u4e8e\u539f\u59cbDT\uff0c\u5e76\u5728\u591a\u4e2a\u79bb\u7ebfRL\u4efb\u52a1\u4e2d\u4e0e\u6700\u5148\u8fdb\u7684DT\u53d8\u4f53\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u6d88\u9664RTG\u5e8f\u5217\u5197\u4f59\uff0cDDT\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6027\u80fd\u66f4\u597d\u7684\u51b3\u7b56\u53d8\u6362\u5668\u8bbe\u8ba1\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.16087", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16087", "abs": "https://arxiv.org/abs/2601.16087", "authors": ["Sukesh Subaharan"], "title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics", "comment": "Supplementary materials can be found here: https://github.com/drsukeshs/agent-behavior-ext-dynamics", "summary": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728LLM\u4ee3\u7406\u4e2d\u5f15\u5165\u5916\u90e8\u60c5\u611f\u52a8\u6001\u5b50\u7cfb\u7edf\uff08\u57fa\u4e8eVAD\u6a21\u578b\uff09\u6765\u589e\u5f3a\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6062\u590d\u80fd\u529b\uff0c\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u52a8\u6001\u89c4\u5219\u5b9e\u73b0\u60c5\u611f\u72b6\u6001\u7684\u6301\u7eed\u6f14\u5316\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u957f\u65f6\u95f4\u4ea4\u4e92\u4e2d\u7ecf\u5e38\u51fa\u73b0\u8bed\u6c14\u548c\u89d2\u8272\u7684\u7a81\u7136\u8f6c\u53d8\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u65f6\u95f4\u7ed3\u6784\u6765\u7ba1\u7406\u4ee3\u7406\u5c42\u9762\u7684\u72b6\u6001\u3002\u867d\u7136\u5148\u524d\u5de5\u4f5c\u5173\u6ce8\u56de\u5408\u5c42\u9762\u7684\u60c5\u611f\u6216\u9759\u6001\u60c5\u611f\u5206\u7c7b\uff0c\u4f46\u663e\u5f0f\u60c5\u611f\u52a8\u6001\u5728\u5851\u9020\u957f\u671f\u4ee3\u7406\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4ee3\u7406\u5c42\u9762\u7684\u60c5\u611f\u5b50\u7cfb\u7edf\uff0c\u7ef4\u62a4\u5916\u90e8\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8fde\u7eedVAD\uff08\u6548\u4ef7-\u5524\u9192-\u652f\u914d\uff09\u72b6\u6001\uff0c\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u66f4\u65b0\u89c4\u5219\u8fdb\u884c\u7ba1\u7406\u3002\u4f7f\u7528\u56fa\u5b9a\u7684\u65e0\u8bb0\u5fc6\u4f30\u8ba1\u5668\u63d0\u53d6\u77ac\u65f6\u60c5\u611f\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u6307\u6570\u5e73\u6ed1\u6216\u57fa\u4e8e\u52a8\u91cf\u7684\u52a8\u6001\u8fdb\u884c\u65f6\u95f4\u79ef\u5206\u3002\u751f\u6210\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u60c5\u611f\u72b6\u6001\u800c\u4e0d\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002", "result": "\u4f7f\u7528\u56fa\u5b9a\u768425\u8f6e\u5bf9\u8bdd\u534f\u8bae\u6bd4\u8f83\u65e0\u72b6\u6001\u3001\u4e00\u9636\u548c\u4e8c\u9636\u60c5\u611f\u52a8\u6001\uff1a\u65e0\u72b6\u6001\u4ee3\u7406\u65e0\u6cd5\u5c55\u73b0\u8fde\u8d2f\u8f68\u8ff9\u6216\u6062\u590d\uff1b\u72b6\u6001\u6301\u7eed\u6027\u652f\u6301\u5ef6\u8fdf\u54cd\u5e94\u548c\u53ef\u9760\u6062\u590d\uff1b\u4e8c\u9636\u52a8\u6001\u5f15\u5165\u60c5\u611f\u60ef\u6027\u548c\u6ede\u540e\u6548\u5e94\uff0c\u968f\u7740\u52a8\u91cf\u589e\u52a0\uff0c\u63ed\u793a\u4e86\u7a33\u5b9a\u6027\u4e0e\u54cd\u5e94\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u5728LLM\u4ee3\u7406\u4e2d\u65bd\u52a0\u660e\u786e\u7684\u60c5\u611f\u52a8\u6001\u7ed3\u6784\u53ef\u4ee5\u589e\u5f3a\u591a\u8f6e\u5bf9\u8bdd\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6062\u590d\u80fd\u529b\uff0c\u4e8c\u9636\u52a8\u6001\u63d0\u4f9b\u4e86\u60c5\u611f\u60ef\u6027\u548c\u6ede\u540e\u6548\u5e94\uff0c\u4f46\u9700\u8981\u5728\u7a33\u5b9a\u6027\u548c\u54cd\u5e94\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2601.16163", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16163", "abs": "https://arxiv.org/abs/2601.16163", "authors": ["Moo Jin Kim", "Yihuai Gao", "Tsung-Yi Lin", "Yen-Chen Lin", "Yunhao Ge", "Grace Lam", "Percy Liang", "Shuran Song", "Ming-Yu Liu", "Chelsea Finn", "Jinwei Gu"], "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning", "comment": null, "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/", "AI": {"tldr": "Cosmos Policy\uff1a\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u540e\u8bad\u7ec3\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\uff08Cosmos-Predict2\uff09\u9002\u914d\u4e3a\u673a\u5668\u4eba\u7b56\u7565\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\uff0c\u76f4\u63a5\u751f\u6210\u7f16\u7801\u4e3a\u6f5c\u5728\u5e27\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5e76\u5728LIBERO\u548cRoboCasa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6a21\u578b\u5df2\u80fd\u6355\u6349\u590d\u6742\u7269\u7406\u4ea4\u4e92\u548c\u573a\u666f\u6f14\u5316\uff0c\u4f46\u5c06\u5176\u7528\u4e8e\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u9700\u8981\u591a\u9636\u6bb5\u540e\u8bad\u7ec3\u548c\u65b0\u67b6\u6784\u7ec4\u4ef6\uff0c\u8fc7\u7a0b\u590d\u6742\u3002\u672c\u6587\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u7684\u65f6\u7a7a\u5148\u9a8c\u8fdb\u884c\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u901a\u8fc7\u5355\u9636\u6bb5\u540e\u8bad\u7ec3\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\uff08Cosmos-Predict2\uff09\u9002\u914d\u4e3a\u673a\u5668\u4eba\u7b56\u7565\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u3002\u6a21\u578b\u5b66\u4e60\u76f4\u63a5\u751f\u6210\u7f16\u7801\u4e3a\u6f5c\u5728\u5e27\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u540c\u65f6\u751f\u6210\u672a\u6765\u72b6\u6001\u56fe\u50cf\u548c\u503c\u51fd\u6570\uff08\u9884\u671f\u7d2f\u79ef\u5956\u52b1\uff09\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u89c4\u5212\u9ad8\u6210\u529f\u7387\u52a8\u4f5c\u8f68\u8ff9\u3002", "result": "\u5728LIBERO\u548cRoboCasa\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523098.5%\u548c67.1%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u83b7\u5f97\u6700\u9ad8\u5e73\u5747\u5206\u6570\uff0c\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u6269\u6563\u7b56\u7565\u3001\u57fa\u4e8e\u89c6\u9891\u6a21\u578b\u7684\u7b56\u7565\u4ee5\u53ca\u5728\u540c\u4e00\u6f14\u793a\u6570\u636e\u4e0a\u5fae\u8c03\u7684VLA\u6a21\u578b\u3002", "conclusion": "Cosmos Policy\u8bc1\u660e\u4e86\u901a\u8fc7\u7b80\u5355\u540e\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u7684\u65f6\u7a7a\u5148\u9a8c\u8fdb\u884c\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u4fee\u6539\uff0c\u4e14\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u6539\u8fdb\u4e16\u754c\u6a21\u578b\u548c\u503c\u51fd\u6570\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.16175", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16175", "abs": "https://arxiv.org/abs/2601.16175", "authors": ["Mert Yuksekgonul", "Daniel Koceja", "Xinhao Li", "Federico Bianchi", "Jed McCaleb", "Xiaolong Wang", "Jan Kautz", "Yejin Choi", "James Zou", "Carlos Guestrin", "Yu Sun"], "title": "Learning to Discover at Test Time", "comment": "Code: https://github.com/test-time-training/discover", "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\u0151s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "AI": {"tldr": "TTT-Discover\uff1a\u901a\u8fc7\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u8ba9LLM\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u6301\u7eed\u8bad\u7ec3\uff0c\u5728\u6570\u5b66\u3001GPU\u5185\u6838\u3001\u7b97\u6cd5\u8bbe\u8ba1\u548c\u751f\u7269\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u5b9e\u73b0SOTA\u7a81\u7834\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff08\u5982AlphaEvolve\uff09\u4f7f\u7528\u51bb\u7ed3\u7684LLM\u8fdb\u884c\u641c\u7d22\uff0c\u65e0\u6cd5\u9488\u5bf9\u7279\u5b9a\u6d4b\u8bd5\u95ee\u9898\u6301\u7eed\u5b66\u4e60\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9LLM\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u7279\u5b9a\u95ee\u9898\u7684\u7ecf\u9a8c\uff0c\u4e13\u6ce8\u4e8e\u751f\u6210\u5355\u4e2a\u4f18\u79c0\u89e3\u51b3\u65b9\u6848\u800c\u975e\u5e73\u5747\u8868\u73b0\u3002", "method": "\u63d0\u51faTTT-Discover\u65b9\u6cd5\uff1a\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u8bbe\u8ba1\u5b66\u4e60\u76ee\u6807\u548c\u641c\u7d22\u5b50\u7a0b\u5e8f\u4ee5\u4f18\u5148\u8003\u8651\u6700\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4f7f\u7528\u5f00\u653e\u6a21\u578bOpenAI gpt-oss-120b\uff0c\u901a\u8fc7Tinker API\u8fdb\u884c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0c\u6210\u672c\u4ec5\u6bcf\u95ee\u9898\u51e0\u767e\u7f8e\u5143\u3002", "result": "\u5728\u51e0\u4e4e\u6240\u6709\u5c1d\u8bd5\u7684\u95ee\u9898\u4e0a\u90fd\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\uff1a1) Erd\u0151s\u6700\u5c0f\u91cd\u53e0\u95ee\u9898\u548c\u81ea\u76f8\u5173\u4e0d\u7b49\u5f0f\uff1b2) GPUMode\u5185\u6838\u7ade\u8d5b\uff08\u6bd4\u73b0\u6709\u6280\u672f\u5feb\u8fbe2\u500d\uff09\uff1b3) \u8fc7\u53bb\u7684AtCoder\u7b97\u6cd5\u7ade\u8d5b\uff1b4) \u5355\u7ec6\u80de\u5206\u6790\u4e2d\u7684\u53bb\u566a\u95ee\u9898\u3002\u6240\u6709\u7ed3\u679c\u5747\u53ef\u901a\u8fc7\u516c\u5f00\u4ee3\u7801\u590d\u73b0\u3002", "conclusion": "TTT-Discover\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6210\u529f\u53d1\u73b0\u591a\u4e2a\u79d1\u5b66\u95ee\u9898\u7684\u65b0SOTA\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e14\u4f7f\u7528\u5f00\u653e\u6a21\u578b\u548c\u53ef\u590d\u73b0\u4ee3\u7801\uff0c\u76f8\u6bd4\u4f9d\u8d56\u5c01\u95ed\u524d\u6cbf\u6a21\u578b\u7684\u65b9\u6cd5\u66f4\u5177\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.8a1efbec", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview_260122secondary/1/0100019be5739d20-829217f4-610a-460f-80d6-19705caddc21-000000/9a3lTqUbjFVR3AJa9m4A7PbkLTPWODaMRx48rI_JLA8=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview_260122secondary/1/0100019be5739d20-829217f4-610a-460f-80d6-19705caddc21-000000/9a3lTqUbjFVR3AJa9m4A7PbkLTPWODaMRx48rI_JLA8=441", "authors": ["TLDR Newsletter"], "title": "AI code review with comments you'll actually implement", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview_260122secondary/1/0100019be5739d20-829217f4-610a-460f-80d6-19705caddc21-000000/9a3lTqUbjFVR3AJa9m4A7PbkLTPWODaMRx48rI_JLA8=441", "summary": "AI code review with comments you'll actually implement (Sponsor) Unblocked is the AI code review that surfaces real issues and meaningful feedback instead of flooding your PRs with stylistic nitpicks and low-value comments. \u201cUnblocked made me reconsider my AI fatigue. Finally, a tool that surfaces context only someone with a full view of the codebase could provide.\u201d - Senior developer, Clio Try now for free", "source": "tldr", "AI": {"tldr": "Unblocked\u662f\u4e00\u6b3eAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u4e13\u6ce8\u4e8e\u8bc6\u522b\u771f\u5b9e\u95ee\u9898\u548c\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53cd\u9988\uff0c\u800c\u975e\u7410\u788e\u7684\u98ce\u683c\u5efa\u8bae", "motivation": "\u89e3\u51b3\u73b0\u6709AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u8fc7\u5ea6\u5173\u6ce8\u4ee3\u7801\u98ce\u683c\u7ec6\u8282\uff0c\u5bfc\u81f4PR\u4e2d\u5145\u65a5\u4f4e\u4ef7\u503c\u8bc4\u8bba\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u771f\u6b63\u6709\u610f\u4e49\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u53cd\u9988", "method": "\u5f00\u53d1\u4e86\u80fd\u591f\u7406\u89e3\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u7684AI\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u8bc6\u522b\u5b9e\u9645\u95ee\u9898\u548c\u63d0\u4f9b\u53ef\u5b9e\u65bd\u7684\u5efa\u8bae", "result": "\u83b7\u5f97\u4e86\u5f00\u53d1\u8005\u7684\u79ef\u6781\u8bc4\u4ef7\uff0c\u7279\u522b\u662f\u8d44\u6df1\u5f00\u53d1\u8005\u8ba4\u4e3a\u8be5\u5de5\u5177\u63d0\u4f9b\u4e86\u53ea\u6709\u4e86\u89e3\u5b8c\u6574\u4ee3\u7801\u5e93\u7684\u4eba\u624d\u80fd\u7ed9\u51fa\u7684\u4e0a\u4e0b\u6587\u5efa\u8bae", "conclusion": "Unblocked\u901a\u8fc7\u4e13\u6ce8\u4e8e\u771f\u5b9e\u95ee\u9898\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u53cd\u9988\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u4ef7\u503c\u95ee\u9898\uff0c\u6539\u53d8\u4e86\u5f00\u53d1\u8005\u5bf9AI\u5de5\u5177\u7684\u75b2\u52b3\u6001\u5ea6", "topic": "swe application"}}
{"id": "tldr.2601.34ab4ba8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.korey.ai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=jan-26/2/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/0fSgOq9OvYpa77BcvlrXRmhhLJ3p8U62KBsktGVXGnU=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.korey.ai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=jan-26/2/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/0fSgOq9OvYpa77BcvlrXRmhhLJ3p8U62KBsktGVXGnU=439", "authors": ["TLDR Newsletter"], "title": "Korey is the AI agent for engineering work that ISN'T coding", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.korey.ai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=jan-26/2/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/0fSgOq9OvYpa77BcvlrXRmhhLJ3p8U62KBsktGVXGnU=439", "summary": "Korey is the AI agent for engineering work that ISN'T coding (Sponsor) Ever feel like AI is trying to automate the best part of your job, and leaving you with piles of admin?Korey is the AI agent that helps you ship faster by optimizing your process, not writing your code. Korey pulls context from the developer tools you're already using and handles all of the non-glamorous stuff: Writing and updating specs Creating tickets Drafting status updates Basically - everything that fills up your day...", "source": "tldr", "AI": {"tldr": "Korey\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u975e\u7f16\u7801\u5de5\u7a0b\u5de5\u4f5c\u7684AI\u4ee3\u7406\uff0c\u901a\u8fc7\u4f18\u5316\u5f00\u53d1\u6d41\u7a0b\u6765\u5e2e\u52a9\u5de5\u7a0b\u5e08\u66f4\u5feb\u4ea4\u4ed8\u4ea7\u54c1", "motivation": "\u5f53\u524dAI\u5de5\u5177\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u7f16\u5199\uff0c\u4f46\u5de5\u7a0b\u5e08\u5de5\u4f5c\u4e2d\u5b58\u5728\u5927\u91cf\u975e\u7f16\u7801\u7684\u884c\u653f\u548c\u7ba1\u7406\u4efb\u52a1\uff08\u5982\u7f16\u5199\u89c4\u8303\u3001\u521b\u5efa\u5de5\u5355\u3001\u72b6\u6001\u66f4\u65b0\u7b49\uff09\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5360\u636e\u4e86\u5927\u91cf\u65f6\u95f4\uff0c\u5f71\u54cd\u4e86\u5f00\u53d1\u6548\u7387", "method": "Korey\u4ece\u5f00\u53d1\u8005\u5df2\u4f7f\u7528\u7684\u5de5\u5177\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u81ea\u52a8\u5316\u5904\u7406\u975e\u7f16\u7801\u4efb\u52a1\uff0c\u5305\u62ec\uff1a\u7f16\u5199\u548c\u66f4\u65b0\u89c4\u8303\u3001\u521b\u5efa\u5de5\u5355\u3001\u8d77\u8349\u72b6\u6001\u66f4\u65b0\u7b49", "result": "Korey\u5e2e\u52a9\u5de5\u7a0b\u5e08\u51cf\u5c11\u884c\u653f\u4efb\u52a1\u8d1f\u62c5\uff0c\u8ba9\u4ed6\u4eec\u4e13\u6ce8\u4e8e\u66f4\u6709\u4ef7\u503c\u7684\u7f16\u7801\u5de5\u4f5c\uff0c\u4ece\u800c\u63d0\u9ad8\u6574\u4f53\u5f00\u53d1\u6548\u7387", "conclusion": "\u4e13\u6ce8\u4e8e\u975e\u7f16\u7801\u5de5\u7a0b\u4efb\u52a1\u7684AI\u4ee3\u7406\u80fd\u591f\u6709\u6548\u4f18\u5316\u5f00\u53d1\u6d41\u7a0b\uff0c\u89e3\u51b3\u5de5\u7a0b\u5e08\u9762\u4e34\u7684\u884c\u653f\u8d1f\u62c5\u95ee\u9898\uff0c\u63d0\u5347\u56e2\u961f\u751f\u4ea7\u529b", "topic": "swe application"}}
{"id": "tldr.2601.30a7e02d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fagent-skills-rules-commands%3Futm_source=tldrdev/1/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/wSG7PZ3jIBuxRreapq10KLS2DUFTEgBQHUwh8okDRvs=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fagent-skills-rules-commands%3Futm_source=tldrdev/1/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/wSG7PZ3jIBuxRreapq10KLS2DUFTEgBQHUwh8okDRvs=439", "authors": ["TLDR Newsletter"], "title": "Agent Skills vs. Rules vs. Commands", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fagent-skills-rules-commands%3Futm_source=tldrdev/1/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/wSG7PZ3jIBuxRreapq10KLS2DUFTEgBQHUwh8okDRvs=439", "summary": "Agent Skills vs. Rules vs. Commands (10 minute read) Agent skills are markdown files that the AI only loads when it actually needs them (like lazy-loading, but for AI instructions). Rules are the hard rules that always apply (like \u201cdon't commit .env files\u201d). Commands are shortcuts you type when you want something specific (like `/release`).", "source": "tldr", "AI": {"tldr": "\u8be5\u6587\u7ae0\u533a\u5206\u4e86AI\u4ee3\u7406\u7684\u4e09\u79cd\u6307\u4ee4\u7c7b\u578b\uff1a\u6280\u80fd\uff08\u6309\u9700\u52a0\u8f7d\u7684Markdown\u6587\u4ef6\uff09\u3001\u89c4\u5219\uff08\u59cb\u7ec8\u9002\u7528\u7684\u786c\u6027\u7ea6\u675f\uff09\u548c\u547d\u4ee4\uff08\u7528\u6237\u8f93\u5165\u7684\u7279\u5b9a\u5feb\u6377\u6307\u4ee4\u3002", "motivation": "\u4e3a\u4e86\u6f84\u6e05AI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u4e0d\u540c\u7c7b\u578b\u7684\u6307\u4ee4\u548c\u7ea6\u675f\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f7f\u7528\u4ee3\u7406\u529f\u80fd\uff0c\u907f\u514d\u6df7\u6dc6\u6280\u80fd\u3001\u89c4\u5219\u548c\u547d\u4ee4\u7684\u6982\u5ff5\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u5206\u6790\u548c\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c06AI\u4ee3\u7406\u7684\u6307\u4ee4\u7cfb\u7edf\u5206\u4e3a\u4e09\u7c7b\uff1a\u6280\u80fd\uff08\u52a8\u6001\u52a0\u8f7d\u7684\u7279\u5b9a\u80fd\u529b\uff09\u3001\u89c4\u5219\uff08\u59cb\u7ec8\u751f\u6548\u7684\u7cfb\u7edf\u7ea6\u675f\uff09\u548c\u547d\u4ee4\uff08\u7528\u6237\u89e6\u53d1\u7684\u5feb\u6377\u64cd\u4f5c\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u6e05\u6670\u7684AI\u4ee3\u7406\u6307\u4ee4\u5206\u7c7b\u6846\u67b6\uff0c\u660e\u786e\u4e86\u6280\u80fd\u3001\u89c4\u5219\u548c\u547d\u4ee4\u5728\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u4e0d\u540c\u4f5c\u7528\u548c\u5b9e\u73b0\u65b9\u5f0f\u3002", "conclusion": "\u7406\u89e3\u8fd9\u4e09\u79cd\u6307\u4ee4\u7c7b\u578b\u7684\u533a\u522b\u5bf9\u4e8e\u6709\u6548\u4f7f\u7528AI\u4ee3\u7406\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u6bcf\u79cd\u7c7b\u578b\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u626e\u6f14\u4e0d\u540c\u7684\u89d2\u8272\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.37fb0458", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefactoring.fm%2Fp%2Fmaking-ai-agents-work-in-the-real%3Futm_source=tldrdev/1/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/-qhtLm66wDMzOebVOsGBIAQh1wJcUu79hb2SzUfCvsU=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefactoring.fm%2Fp%2Fmaking-ai-agents-work-in-the-real%3Futm_source=tldrdev/1/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/-qhtLm66wDMzOebVOsGBIAQh1wJcUu79hb2SzUfCvsU=439", "authors": ["TLDR Newsletter"], "title": "Making AI agents work in the real world \u2699\ufe0f", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefactoring.fm%2Fp%2Fmaking-ai-agents-work-in-the-real%3Futm_source=tldrdev/1/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/-qhtLm66wDMzOebVOsGBIAQh1wJcUu79hb2SzUfCvsU=439", "summary": "Making AI agents work in the real world \u2699\ufe0f (15 minute read) Many AI agent deployments fail to achieve material impact, often remaining stuck in pilot mode due to challenges like compounding errors in multi-step workflows. AI agents, unlike chatbots or copilots, take autonomous actions and require a robust system encompassing the model, orchestrator, and external tools, not just a capable \"brain.\" To succeed in production, teams must be realistic about agent capabilities, start with narrow and...", "source": "tldr", "AI": {"tldr": "AI\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u56e0\u591a\u6b65\u5de5\u4f5c\u6d41\u4e2d\u7684\u7d2f\u79ef\u9519\u8bef\u7b49\u95ee\u9898\u800c\u5931\u8d25\uff0c\u9700\u8981\u6784\u5efa\u5305\u542b\u6a21\u578b\u3001\u7f16\u6392\u5668\u548c\u5916\u90e8\u5de5\u5177\u7684\u5b8c\u6574\u7cfb\u7edf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5f3a\u5927\u7684\"\u5927\u8111\"", "motivation": "\u8bb8\u591aAI\u667a\u80fd\u4f53\u90e8\u7f72\u672a\u80fd\u4ea7\u751f\u5b9e\u8d28\u6027\u5f71\u54cd\uff0c\u7ecf\u5e38\u505c\u7559\u5728\u8bd5\u70b9\u9636\u6bb5\uff0c\u4e3b\u8981\u6311\u6218\u5305\u62ec\u591a\u6b65\u5de5\u4f5c\u6d41\u4e2d\u7684\u7d2f\u79ef\u9519\u8bef\u3001\u7f3a\u4e4f\u5b8c\u6574\u7684\u7cfb\u7edf\u67b6\u6784\u7b49", "method": "\u6784\u5efa\u5305\u542b\u6a21\u578b\u3001\u7f16\u6392\u5668\u548c\u5916\u90e8\u5de5\u5177\u7684\u5b8c\u6574AI\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4ece\u72ed\u7a84\u548c\u5177\u4f53\u7684\u5e94\u7528\u573a\u666f\u5f00\u59cb\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u65b9\u6cd5\uff0c\u800c\u975e\u4e00\u6b21\u6027\u6784\u5efa\u590d\u6742\u7cfb\u7edf", "result": "\u901a\u8fc7\u6784\u5efa\u5b8c\u6574\u7684\u7cfb\u7edf\u67b6\u6784\u548c\u91c7\u7528\u6e10\u8fdb\u5f0f\u90e8\u7f72\u7b56\u7565\uff0c\u53ef\u4ee5\u63d0\u9ad8AI\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6210\u529f\u7387\u548c\u5f71\u54cd\u529b", "conclusion": "AI\u667a\u80fd\u4f53\u8981\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u9700\u8981\u8d85\u8d8a\u5355\u7eaf\u4f9d\u8d56\u5f3a\u5927\u6a21\u578b\uff0c\u6784\u5efa\u5305\u542b\u7f16\u6392\u3001\u5de5\u5177\u96c6\u6210\u548c\u9519\u8bef\u5904\u7406\u7684\u5b8c\u6574\u7cfb\u7edf\uff0c\u5e76\u91c7\u7528\u52a1\u5b9e\u3001\u6e10\u8fdb\u5f0f\u7684\u90e8\u7f72\u7b56\u7565", "topic": "agent analysis"}}
{"id": "tldr.2601.574ed086", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr_ai%26utm_medium=sponsorship%26utm_campaign=tldr_ai/2/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/K_aa9jNRNGmh5Gb6Oyvwjs3D9dOF1mwKG8KAzkVDs3Q=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr_ai%26utm_medium=sponsorship%26utm_campaign=tldr_ai/2/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/K_aa9jNRNGmh5Gb6Oyvwjs3D9dOF1mwKG8KAzkVDs3Q=441", "authors": ["TLDR Newsletter"], "title": "AI is writing more code. Someone still has to review it.", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr_ai%26utm_medium=sponsorship%26utm_campaign=tldr_ai/2/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/K_aa9jNRNGmh5Gb6Oyvwjs3D9dOF1mwKG8KAzkVDs3Q=441", "summary": "AI is writing more code. Someone still has to review it. (Sponsor) That review step is where bugs slip through, context gets lost, and teams slow down.Greptile reviews each PR with full repo context and learns your team's conventions over time from comments, reactions, and what gets merged. It flags issues and suggests fixes that match your team, not generic best practices. \u2705 Now integrated with Claude Code: install via /plugin. \u2705 Free for open source. \u2705 Trusted by engineering teams at NVIDIA...", "source": "tldr", "AI": {"tldr": "Greptile\u662f\u4e00\u6b3eAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u5229\u7528\u5b8c\u6574\u4ed3\u5e93\u4e0a\u4e0b\u6587\u5b66\u4e60\u56e2\u961f\u89c4\u8303\uff0c\u81ea\u52a8\u6807\u8bb0\u95ee\u9898\u5e76\u63d0\u4f9b\u7b26\u5408\u56e2\u961f\u4e60\u60ef\u7684\u4fee\u590d\u5efa\u8bae", "motivation": "\u968f\u7740AI\u7f16\u5199\u4ee3\u7801\u589e\u591a\uff0c\u4eba\u5de5\u4ee3\u7801\u5ba1\u67e5\u6210\u4e3a\u74f6\u9888\uff0c\u5bb9\u6613\u9057\u6f0fbug\u3001\u4e22\u5931\u4e0a\u4e0b\u6587\u3001\u964d\u4f4e\u56e2\u961f\u6548\u7387\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u6539\u8fdb\u5ba1\u67e5\u6d41\u7a0b", "method": "\u901a\u8fc7\u5206\u6790\u5b8c\u6574\u4ed3\u5e93\u4e0a\u4e0b\u6587\uff0c\u4ece\u56e2\u961f\u8bc4\u8bba\u3001\u53cd\u5e94\u548c\u5408\u5e76\u8bb0\u5f55\u4e2d\u5b66\u4e60\u56e2\u961f\u89c4\u8303\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u4ee3\u7801\u5ba1\u67e5\u5efa\u8bae", "result": "\u5df2\u96c6\u6210\u5230Claude Code\u63d2\u4ef6\u4e2d\uff0c\u5f00\u6e90\u9879\u76ee\u514d\u8d39\u4f7f\u7528\uff0c\u88abNVIDIA\u7b49\u5de5\u7a0b\u56e2\u961f\u91c7\u7528", "conclusion": "Greptile\u901a\u8fc7AI\u9a71\u52a8\u7684\u4e2a\u6027\u5316\u4ee3\u7801\u5ba1\u67e5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u6548\u7387\u548c\u8d28\u91cf\u95ee\u9898", "topic": "swe application"}}
{"id": "tldr.2601.7a004109", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flmsys.org%2Fblog%2F2026-01-21-novita-glm4%2F%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/tw4kFFK_Yz_c5DbbK1f9ioZtIZiLUJWQe_0V0RGrLIs=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flmsys.org%2Fblog%2F2026-01-21-novita-glm4%2F%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/tw4kFFK_Yz_c5DbbK1f9ioZtIZiLUJWQe_0V0RGrLIs=441", "authors": ["TLDR Newsletter"], "title": "GLM4-MoE Inference with SGLang", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flmsys.org%2Fblog%2F2026-01-21-novita-glm4%2F%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/tw4kFFK_Yz_c5DbbK1f9ioZtIZiLUJWQe_0V0RGrLIs=441", "summary": "GLM4-MoE Inference with SGLang (11 minute read) Novita AI introduced performance optimizations for GLM4-MoE models using SGLang, achieving faster Time-to-First-Token and better token generation speed under agentic coding workloads.", "source": "tldr", "AI": {"tldr": "Novita AI\u4f7f\u7528SGLang\u4f18\u5316GLM4-MoE\u6a21\u578b\u63a8\u7406\uff0c\u5728\u4ee3\u7406\u7f16\u7801\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u9996\u6b21\u4ee4\u724c\u65f6\u95f4\u548c\u66f4\u597d\u7684\u4ee4\u724c\u751f\u6210\u901f\u5ea6", "motivation": "GLM4-MoE\u6a21\u578b\u5728\u4ee3\u7406\u7f16\u7801\u4efb\u52a1\u4e2d\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u51cf\u5c11\u9996\u6b21\u4ee4\u724c\u5ef6\u8fdf\u548c\u63d0\u9ad8\u4ee4\u724c\u751f\u6210\u901f\u5ea6\uff0c\u4ee5\u6539\u5584\u7528\u6237\u4f53\u9a8c\u548c\u7cfb\u7edf\u54cd\u5e94\u6027", "method": "\u91c7\u7528SGLang\u6846\u67b6\u5bf9GLM4-MoE\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u4f18\u5316\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u63a8\u7406\u4f18\u5316\u6280\u672f\u63d0\u5347\u6a21\u578b\u5728\u4ee3\u7406\u7f16\u7801\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6267\u884c\u6548\u7387", "result": "\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684Time-to-First-Token\u548c\u66f4\u597d\u7684token\u751f\u6210\u901f\u5ea6\uff0c\u5728\u4ee3\u7406\u7f16\u7801\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u63a8\u7406\u6027\u80fd", "conclusion": "SGLang\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316GLM4-MoE\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7406\u7f16\u7801\u573a\u666f\u4e0b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u541e\u5410\u91cf", "topic": "code agent"}}
{"id": "tldr.2601.356e1c1c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.substack.com%2Fp%2Fclaude-codes-3%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/ySHdE6Si8nUFwN5sOe5ZE1WZGdDWw9v3XZIQ18LN5Uw=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.substack.com%2Fp%2Fclaude-codes-3%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/ySHdE6Si8nUFwN5sOe5ZE1WZGdDWw9v3XZIQ18LN5Uw=441", "authors": ["TLDR Newsletter"], "title": "Claude Codes #3", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: 24 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.substack.com%2Fp%2Fclaude-codes-3%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/ySHdE6Si8nUFwN5sOe5ZE1WZGdDWw9v3XZIQ18LN5Uw=441", "summary": "Claude Codes #3 (24 minute read) This post contains a curated list of news, tutorials, tips, and articles on Claude Code. It covers recent upgrades, tools that complement Claude Code, and more. The post provides advice on how to skill up with Claude Code as well as predictions on where the technology is headed.", "source": "tldr", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8eClaude Code\u7684\u7efc\u8ff0\u6027\u6587\u7ae0\uff0c\u5305\u542b\u65b0\u95fb\u3001\u6559\u7a0b\u3001\u6280\u5de7\u548c\u5de5\u5177\u4ecb\u7ecd\uff0c\u65e8\u5728\u5e2e\u52a9\u8bfb\u8005\u63d0\u5347Claude Code\u6280\u80fd\u5e76\u4e86\u89e3\u6280\u672f\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u4e86\u89e3\u548c\u638c\u63e1Claude Code\u6280\u672f\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u5b66\u4e60\u8d44\u6e90\u548c\u884c\u4e1a\u52a8\u6001\uff0c\u4fc3\u8fdbAI\u7f16\u7a0b\u52a9\u624b\u6280\u672f\u7684\u5e94\u7528\u548c\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6574\u7406\u548c\u7cbe\u9009Claude Code\u76f8\u5173\u7684\u65b0\u95fb\u3001\u6559\u7a0b\u3001\u6280\u5de7\u548c\u5de5\u5177\uff0c\u6784\u5efa\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u5b66\u4e60\u8d44\u6e90\u96c6\u5408\uff0c\u5305\u62ec\u6280\u672f\u66f4\u65b0\u3001\u914d\u5957\u5de5\u5177\u4ecb\u7ecd\u4ee5\u53ca\u6280\u80fd\u63d0\u5347\u5efa\u8bae\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684Claude Code\u5b66\u4e60\u6307\u5357\uff0c\u5305\u542b\u6700\u65b0\u6280\u672f\u52a8\u6001\u3001\u5b9e\u7528\u5de5\u5177\u63a8\u8350\u3001\u6280\u80fd\u63d0\u5347\u65b9\u6cd5\u548c\u672a\u6765\u8d8b\u52bf\u9884\u6d4b\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b66\u4e60\u8d44\u6e90\u3002", "conclusion": "Claude Code\u4f5c\u4e3aAI\u7f16\u7a0b\u52a9\u624b\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b66\u4e60\u548c\u638c\u63e1\u76f8\u5173\u6280\u80fd\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7f16\u7a0b\u6548\u7387\uff0c\u8be5\u6587\u7ae0\u4e3a\u8fd9\u4e00\u5b66\u4e60\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002", "topic": "code agent"}}
{"id": "tldr.2601.ff26f33e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fdevin-review%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/QVSftfsEAmQcvG4W02ArMKLXdIPAw6GKQcWc2G4-sM8=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fdevin-review%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/QVSftfsEAmQcvG4W02ArMKLXdIPAw6GKQcWc2G4-sM8=441", "authors": ["TLDR Newsletter"], "title": "Devin Review: AI to Stop Slop", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fdevin-review%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/QVSftfsEAmQcvG4W02ArMKLXdIPAw6GKQcWc2G4-sM8=441", "summary": "Devin Review: AI to Stop Slop (4 minute read) Devin Review is a code review tool that uses AI and UX to scale human understanding of complex code diffs. It is currently free and works on any public or private GitHub PR. The tool helps in every step of the PR process. It allows developers to chat about changes without leaving the review.", "source": "tldr", "AI": {"tldr": "Devin Review\u662f\u4e00\u6b3e\u5229\u7528AI\u548c\u7528\u6237\u4f53\u9a8c\u8bbe\u8ba1\u6765\u6269\u5c55\u4eba\u7c7b\u5bf9\u590d\u6742\u4ee3\u7801\u5dee\u5f02\u7406\u89e3\u7684\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u652f\u6301\u4efb\u4f55GitHub PR\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u4e0d\u79bb\u5f00\u5ba1\u67e5\u754c\u9762\u7684\u60c5\u51b5\u4e0b\u8ba8\u8bba\u53d8\u66f4\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u5ba1\u67e5\u8fc7\u7a0b\u4e2d\u7406\u89e3\u590d\u6742\u4ee3\u7801\u5dee\u5f02\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4ee3\u7801\u5ba1\u67e5\u6548\u7387\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u8f7b\u677e\u5730\u8ba8\u8bba\u548c\u5ba1\u67e5\u4ee3\u7801\u53d8\u66f4\u3002", "method": "\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u8bbe\u8ba1\uff0c\u521b\u5efa\u96c6\u6210\u5230GitHub PR\u6d41\u7a0b\u4e2d\u7684\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u652f\u6301\u5f00\u53d1\u8005\u76f4\u63a5\u5728\u5ba1\u67e5\u754c\u9762\u4e2d\u8fdb\u884c\u804a\u5929\u8ba8\u8bba\u3002", "result": "\u76ee\u524d\u514d\u8d39\u63d0\u4f9b\uff0c\u652f\u6301\u4efb\u4f55\u516c\u5171\u6216\u79c1\u6709GitHub PR\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5b8c\u6210PR\u6d41\u7a0b\u7684\u6bcf\u4e00\u6b65\uff0c\u63d0\u4f9b\u65e0\u7f1d\u7684\u4ee3\u7801\u5ba1\u67e5\u4f53\u9a8c\u3002", "conclusion": "Devin Review\u901a\u8fc7AI\u8f85\u52a9\u7684\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u7b80\u5316\u4e86\u4ee3\u7801\u5ba1\u67e5\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u4e3a\u56e2\u961f\u534f\u4f5c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u652f\u6301\u3002", "topic": "code agent"}}
{"id": "tldr.2601.59557880", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fbuilding-agentic-ai%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-building-agentic-ai%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/ddn_Ojo8eb7pBB6iXz3LIGMuCvR6GSlPqkAdXLE28Wc=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fbuilding-agentic-ai%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-building-agentic-ai%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/ddn_Ojo8eb7pBB6iXz3LIGMuCvR6GSlPqkAdXLE28Wc=441", "authors": ["TLDR Newsletter"], "title": "A detailed guide to building agentic systems", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fbuilding-agentic-ai%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-building-agentic-ai%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/ddn_Ojo8eb7pBB6iXz3LIGMuCvR6GSlPqkAdXLE28Wc=441", "summary": "A detailed guide to building agentic systems (Sponsor) Is agentic AI your next step? Beware of getting bogged down with custom integrations. Algolia's white paper breaks down how to build AI agents on a deadline. Read it now", "source": "tldr", "AI": {"tldr": "Algolia\u53d1\u5e03\u767d\u76ae\u4e66\uff0c\u63d0\u4f9b\u6784\u5efaAI\u4ee3\u7406\u7cfb\u7edf\u7684\u8be6\u7ec6\u6307\u5357\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u622a\u6b62\u65e5\u671f\u524d\u5b8c\u6210\u9879\u76ee", "motivation": "AI\u4ee3\u7406\u7cfb\u7edf\u5f00\u53d1\u4e2d\u5e38\u9047\u5230\u5b9a\u5236\u96c6\u6210\u590d\u6742\u3001\u8017\u65f6\u7684\u95ee\u9898\uff0c\u9700\u8981\u5b9e\u7528\u7684\u6784\u5efa\u6307\u5357\u6765\u52a0\u901f\u5f00\u53d1\u8fdb\u7a0b", "method": "\u901a\u8fc7\u767d\u76ae\u4e66\u5f62\u5f0f\u63d0\u4f9b\u8be6\u7ec6\u7684\u6784\u5efa\u6307\u5357\uff0c\u5206\u6790\u5982\u4f55\u907f\u514d\u5b9a\u5236\u96c6\u6210\u7684\u56f0\u6270\uff0c\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u5f00\u53d1\u65b9\u6cd5", "result": "\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684AI\u4ee3\u7406\u7cfb\u7edf\u6784\u5efa\u6307\u5357\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u5b8c\u6210\u9879\u76ee\u5f00\u53d1", "conclusion": "AI\u4ee3\u7406\u7cfb\u7edf\u5f00\u53d1\u9700\u8981\u7cfb\u7edf\u5316\u7684\u6307\u5bfc\uff0cAlgolia\u7684\u767d\u76ae\u4e66\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2601.0ddbd066", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.detectionatscale.com%2Fp%2Fai-threat-hunting-mcp-workflows%3Futm_source=tldrinfosec/1/0100019be619f6bb-d58c16ae-c07d-4029-84c5-a5571612f3dc-000000/MMwwdxxpHx69MPX2zM6vuAEGh_oZMjEqFN75ha0Ufrg=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.detectionatscale.com%2Fp%2Fai-threat-hunting-mcp-workflows%3Futm_source=tldrinfosec/1/0100019be619f6bb-d58c16ae-c07d-4029-84c5-a5571612f3dc-000000/MMwwdxxpHx69MPX2zM6vuAEGh_oZMjEqFN75ha0Ufrg=441", "authors": ["TLDR Newsletter"], "title": "Threat Hunting with Claude Code and MCP", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.detectionatscale.com%2Fp%2Fai-threat-hunting-mcp-workflows%3Futm_source=tldrinfosec/1/0100019be619f6bb-d58c16ae-c07d-4029-84c5-a5571612f3dc-000000/MMwwdxxpHx69MPX2zM6vuAEGh_oZMjEqFN75ha0Ufrg=441", "summary": "Threat Hunting with Claude Code and MCP (7 minute read) This guide demonstrates how to use Claude Code with Model Context Protocol servers to compress days of manual threat hunting into hours by connecting AI agents directly to security data lakes for hypothesis-driven investigations. The workflow involves stakeholder alignment on three to five priority threats from threat models, then deploying reusable \"Skills\" that teach agents hunting methodologies, such as the pivot loop pattern for corr...", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528Claude Code\u548cMCP\u534f\u8bae\u8fdb\u884c\u5a01\u80c1\u72e9\u730e\uff0c\u5c06\u6570\u5929\u7684\u624b\u52a8\u8c03\u67e5\u538b\u7f29\u5230\u6570\u5c0f\u65f6\u5185\u5b8c\u6210", "motivation": "\u4f20\u7edf\u5a01\u80c1\u72e9\u730e\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\uff0c\u8017\u65f6\u6570\u5929\u3002\u901a\u8fc7AI\u4ee3\u7406\u76f4\u63a5\u8fde\u63a5\u5b89\u5168\u6570\u636e\u6e56\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8c03\u67e5\u6548\u7387", "method": "\u4f7f\u7528Claude Code\u4e0eModel Context Protocol\u670d\u52a1\u5668\uff0c\u901a\u8fc7\u53ef\u590d\u7528\u7684\"\u6280\u80fd\"\u6559\u5bfcAI\u4ee3\u7406\u5a01\u80c1\u72e9\u730e\u65b9\u6cd5\uff0c\u5982pivot loop\u6a21\u5f0f\u8fdb\u884c\u5173\u8054\u5206\u6790", "result": "\u80fd\u591f\u5c06\u6570\u5929\u7684\u5a01\u80c1\u72e9\u730e\u5de5\u4f5c\u538b\u7f29\u5230\u6570\u5c0f\u65f6\u5185\u5b8c\u6210\uff0c\u5b9e\u73b0\u5047\u8bbe\u9a71\u52a8\u7684\u8c03\u67e5", "conclusion": "AI\u4ee3\u7406\u4e0e\u5b89\u5168\u6570\u636e\u6e56\u7684\u76f4\u63a5\u8fde\u63a5\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u5a01\u80c1\u72e9\u730e\u6548\u7387\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u53ef\u590d\u7528\u7684\u6280\u80fd\u5b9e\u73b0\u5feb\u901f\u8c03\u67e5", "topic": "code agent"}}
