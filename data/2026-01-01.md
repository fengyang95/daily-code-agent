<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.LG](#cs.LG) [Total: 2]
- [tldr.article](#tldr.article) [Total: 6]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs](https://arxiv.org/abs/2512.24314)
*Shupeng Li,Weipeng Lu,Linyun Liu,Chen Lin,Shaofei Li,Zhendong Tan,Hanjun Zhong,Yucheng Zeng,Chenghao Zhu,Mengyue Liu,Daxiang Dong,Jianmin Wu,Yunting Xiao,Annan Li,Danyu Liu,Jingnan Zhang,Licen Liu,Dawei Yin,Dou Shen*

Main category: cs.CL

TL;DR: QianfanHuijinæ˜¯ä¸€ä¸ªé‡‘èé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼ï¼šæŒç»­é¢„è®­ç»ƒå·©å›ºçŸ¥è¯†åŸºç¡€ï¼Œç„¶åé€šè¿‡é‡‘èSFTã€é‡‘èæ¨ç†RLã€é‡‘èä»£ç†RLå’Œé€šç”¨RLçš„æ¸è¿›å¼åè®­ç»ƒæå‡èƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: é‡‘èæœåŠ¡çš„å¤æ‚æ€§æ—¥ç›Šå¢åŠ ï¼Œéœ€è¦æ¨¡å‹ä¸ä»…å…·å¤‡é¢†åŸŸçŸ¥è¯†ï¼Œè¿˜è¦æœ‰å¼ºå¤§çš„é‡‘èæ¨ç†å’Œä»£ç†èƒ½åŠ›ã€‚ç°æœ‰æ¨¡å‹å¦‚BloombergGPTå’ŒBaichuan-Financeä¸»è¦å…³æ³¨çŸ¥è¯†å¢å¼ºï¼Œæ— æ³•æ»¡è¶³å¯¹æ¨ç†å’Œä»£ç†èƒ½åŠ›çš„éœ€æ±‚ã€‚

Method: æå‡ºé€šç”¨çš„å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼ï¼š1) åœ¨é‡‘èè¯­æ–™ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼›2) æ¸è¿›å¼åè®­ç»ƒç®¡é“ï¼šé‡‘èSFT â†’ é‡‘èæ¨ç†RL â†’ é‡‘èä»£ç†RL â†’ é€šç”¨RLï¼ˆä¸ç°å®ä¸šåŠ¡åœºæ™¯å¯¹é½ï¼‰ã€‚

Result: QianfanHuijinåœ¨å¤šä¸ªæƒå¨é‡‘èåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œé’ˆå¯¹æ€§çš„æ¨ç†RLå’Œä»£ç†RLé˜¶æ®µåœ¨å„è‡ªèƒ½åŠ›ä¸Šå¸¦æ¥æ˜¾è‘—æå‡ã€‚

Conclusion: è¿™ç§ç»†ç²’åº¦ã€æ¸è¿›å¼çš„åè®­ç»ƒæ–¹æ³•æœ‰æœ›æˆä¸ºå„ç§å·¥ä¸šå¢å¼ºå‹LLMçš„ä¸»æµèŒƒå¼ï¼ŒéªŒè¯äº†ç ”ç©¶åŠ¨æœºçš„æœ‰æ•ˆæ€§ã€‚

Abstract: Domain-specific enhancement of Large Language Models (LLMs) within the financial context has long been a focal point of industrial application. While previous models such as BloombergGPT and Baichuan-Finance primarily focused on knowledge enhancement, the deepening complexity of financial services has driven a growing demand for models that possess not only domain knowledge but also robust financial reasoning and agentic capabilities. In this paper, we present QianfanHuijin, a financial domain LLM, and propose a generalizable multi-stage training paradigm for industrial model enhancement.
  Our approach begins with Continual Pre-training (CPT) on financial corpora to consolidate the knowledge base. This is followed by a fine-grained Post-training pipeline designed with increasing specificity: starting with Financial SFT, progressing to Finance Reasoning RL and Finance Agentic RL, and culminating in General RL aligned with real-world business scenarios. Empirical results demonstrate that QianfanHuijin achieves superior performance across various authoritative financial benchmarks. Furthermore, ablation studies confirm that the targeted Reasoning RL and Agentic RL stages yield significant gains in their respective capabilities. These findings validate our motivation and suggest that this fine-grained, progressive post-training methodology is poised to become a mainstream paradigm for various industrial-enhanced LLMs.

</details>


### [2] [Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline](https://arxiv.org/abs/2512.24933)
*Minjun Zhao,Xinyu Zhang,Shuai Zhang,Deyang Li,Ruifeng Shi*

Main category: cs.CL

TL;DR: ADOPTæ˜¯ä¸€ä¸ªç”¨äºå¤šæ­¥LLMç®¡é“çš„è‡ªé€‚åº”ä¾èµ–æ„ŸçŸ¥æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡æ¯ä¸ªLLMæ­¥éª¤ä¸æœ€ç»ˆä»»åŠ¡ç»“æœä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå®ç°ç²¾ç¡®çš„æ–‡æœ¬æ¢¯åº¦ä¼°è®¡å’Œèµ„æºåˆ†é…ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤šæ­¥LLMç®¡é“åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†å…¶æ€§èƒ½ä¸¥é‡ä¾èµ–äºæ¯ä¸€æ­¥ä½¿ç”¨çš„æç¤ºã€‚ç”±äºç¼ºä¹æ­¥éª¤çº§ç›‘ç£å’Œæ­¥éª¤é—´ä¾èµ–å…³ç³»ï¼Œè”åˆä¼˜åŒ–è¿™äº›æç¤ºéå¸¸å›°éš¾ã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯æç¤ºä¼˜åŒ–æ–¹æ³•åœ¨è¿™äº›æ¡ä»¶ä¸‹æ•ˆæœä¸ä½³ï¼Œå¾€å¾€äº§ç”Ÿæ¬¡ä¼˜æˆ–ä¸ç¨³å®šçš„æ›´æ–°ã€‚

Method: ADOPTæ¡†æ¶æ˜¾å¼å»ºæ¨¡æ¯ä¸ªLLMæ­¥éª¤ä¸æœ€ç»ˆä»»åŠ¡ç»“æœä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå®ç°ç±»ä¼¼äºè®¡ç®—è§£æå¯¼æ•°çš„ç²¾ç¡®æ–‡æœ¬æ¢¯åº¦ä¼°è®¡ã€‚å®ƒå°†æ–‡æœ¬æ¢¯åº¦ä¼°è®¡ä¸æ¢¯åº¦æ›´æ–°è§£è€¦ï¼Œå°†å¤šæç¤ºä¼˜åŒ–ç®€åŒ–ä¸ºçµæ´»çš„å•æç¤ºä¼˜åŒ–æ­¥éª¤ï¼Œå¹¶ä½¿ç”¨åŸºäºShapleyå€¼çš„æœºåˆ¶è‡ªé€‚åº”åˆ†é…ä¼˜åŒ–èµ„æºã€‚

Result: åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†å’Œå¤šæ ·åŒ–ç®¡é“ç»“æ„ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒADOPTæ˜¯æœ‰æ•ˆä¸”é²æ£’çš„ï¼Œå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æç¤ºä¼˜åŒ–åŸºçº¿æ–¹æ³•ã€‚

Conclusion: ADOPTä¸ºå¤šæ­¥LLMç®¡é“æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä¾èµ–æ„ŸçŸ¥çš„æ¢¯åº¦ä¼°è®¡å’Œè‡ªé€‚åº”èµ„æºåˆ†é…ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ç¼ºä¹æ­¥éª¤çº§ç›‘ç£å’Œæ­¥éª¤é—´ä¾èµ–æ¡ä»¶ä¸‹çš„ä¼˜åŒ–å›°éš¾ã€‚

Abstract: Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.

</details>


### [3] [Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time](https://arxiv.org/abs/2512.24574)
*Zhenyu Zhang,Xiaoxia Wu,Zhongzhu Zhou,Qingyang Wu,Yineng Zhang,Pragaash Ponnusamy,Harikaran Subbaraj,Jue Wang,Shuaiwen Leon Song,Ben Athiwaratkun*

Main category: cs.CL

TL;DR: CRESTæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶å¹²é¢„ç‰¹å®šæ³¨æ„åŠ›å¤´æ¥å¼•å¯¼LLMçš„æ¨ç†è¿‡ç¨‹ï¼Œå‡å°‘ä½æ•ˆçš„é“¾å¼æ€è€ƒï¼Œæé«˜å‡†ç¡®ç‡åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–é•¿é“¾å¼æ€è€ƒè§£å†³å¤æ‚ä»»åŠ¡ï¼Œä½†è¿™äº›æ¨ç†è½¨è¿¹é€šå¸¸æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿæˆ–ä¸ç¨³å®šçš„æ¨ç†ï¼ˆæ€è€ƒä¸è¶³æˆ–è¿‡åº¦æ€è€ƒï¼‰ã€‚éœ€è¦ä¸€ç§æ–¹æ³•æ¥å¼•å¯¼æ¨¡å‹è¿œç¦»ä½æ•ˆçš„æ¨ç†æ¨¡å¼ã€‚

Method: CRESTåŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼š(1)ç¦»çº¿æ ¡å‡†æ­¥éª¤ï¼Œè¯†åˆ«ä¸ç‰¹å®šè®¤çŸ¥è¡Œä¸ºï¼ˆå¦‚éªŒè¯å’Œå›æº¯ï¼‰ç›¸å…³çš„æ³¨æ„åŠ›å¤´ï¼Œå¹¶æ¨å¯¼å‡ºå¤´ç‰¹å®šçš„å¼•å¯¼å‘é‡ï¼›(2)æ¨ç†æ—¶ç¨‹åºï¼Œé€šè¿‡æ—‹è½¬éšè—è¡¨ç¤ºæ¥æŠ‘åˆ¶è¿™äº›å‘é‡æ–¹å‘ä¸Šçš„ç»„ä»¶ï¼Œä»è€Œè‡ªé€‚åº”åœ°æŠ‘åˆ¶éç”Ÿäº§æ€§æ¨ç†è¡Œä¸ºã€‚

Result: åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ä¸Šï¼ŒCRESTå°†å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾17.5%ï¼ŒåŒæ—¶å‡å°‘äº†37.6%çš„tokenä½¿ç”¨é‡ï¼Œå®ç°äº†æ›´å¿«ã€æ›´å¯é çš„LLMæ¨ç†ã€‚

Conclusion: CRESTæä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„é€”å¾„ï¼Œé€šè¿‡å¹²é¢„ç‰¹å®šæ³¨æ„åŠ›å¤´æ¥å¼•å¯¼LLMæ¨ç†è¿‡ç¨‹ï¼Œæ—¢èƒ½æé«˜å‡†ç¡®ç‡åˆèƒ½é™ä½è®¡ç®—æˆæœ¬ï¼Œä¸ºæ›´é«˜æ•ˆå¯é çš„LLMæ¨ç†æä¾›äº†æ–°æ–¹æ³•ã€‚

Abstract: Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control](https://arxiv.org/abs/2512.24955)
*Yongwei Zhang,Yuanzhe Xing,Quan Quan,Zhikun She*

Main category: cs.LG

TL;DR: MSACLæ˜¯ä¸€ä¸ªå°†æŒ‡æ•°ç¨³å®šæ€§ç†è®ºä¸æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å¤šæ­¥Lyapunovè¯ä¹¦å­¦ä¹ å®ç°å¯è¯æ˜çš„ç¨³å®šæ¨¡å‹è‡ªç”±å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨ç®€å•å¥–åŠ±ä¸‹å®ç°æŒ‡æ•°ç¨³å®šæ€§å’Œå¿«é€Ÿæ”¶æ•›ã€‚


<details>
  <summary>Details</summary>
Motivation: æ¨¡å‹è‡ªç”±å¼ºåŒ–å­¦ä¹ åœ¨å®ç°å¯è¯æ˜ç¨³å®šæ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹³è¡¡æ¢ç´¢ä¸ä¸¥æ ¼å®‰å…¨æ€§æ–¹é¢ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤æ‚çš„å¥–åŠ±å·¥ç¨‹ï¼Œç¼ºä¹ç†è®ºä¿è¯ã€‚

Method: æå‡ºMSACLæ¡†æ¶ï¼Œæ•´åˆæŒ‡æ•°ç¨³å®šæ€§ç†è®ºä¸æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¤šæ­¥Lyapunovè¯ä¹¦å­¦ä¹ ã€‚ä½¿ç”¨ç¦»ç­–ç•¥å¤šæ­¥æ•°æ®å­¦ä¹ æ»¡è¶³ç†è®ºç¨³å®šæ€§æ¡ä»¶çš„Lyapunovè¯ä¹¦ï¼Œå¼•å…¥æŒ‡æ•°ç¨³å®šæ€§æ ‡ç­¾(ESL)å’ŒÎ»åŠ æƒèšåˆæœºåˆ¶å¹³è¡¡å¤šæ­¥å­¦ä¹ çš„åå·®-æ–¹å·®æƒè¡¡ã€‚ç­–ç•¥ä¼˜åŒ–ç”±ç¨³å®šæ€§æ„ŸçŸ¥ä¼˜åŠ¿å‡½æ•°æŒ‡å¯¼ï¼Œç¡®ä¿å­¦ä¹ ç­–ç•¥ä¿ƒè¿›å¿«é€ŸLyapunovä¸‹é™ã€‚

Result: åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬ç¨³å®šåŒ–å’Œéçº¿æ€§è·Ÿè¸ªä»»åŠ¡ï¼‰ä¸­è¯„ä¼°ï¼ŒMSACLä¼˜äºæœ€å…ˆè¿›çš„åŸºäºLyapunovçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚åœ¨ç®€å•å¥–åŠ±ä¸‹å®ç°æŒ‡æ•°ç¨³å®šæ€§å’Œå¿«é€Ÿæ”¶æ•›ï¼Œå¯¹ä¸ç¡®å®šæ€§å…·æœ‰æ˜¾è‘—é²æ£’æ€§ï¼Œå¹¶èƒ½æ³›åŒ–åˆ°æœªè§è½¨è¿¹ã€‚æ•æ„Ÿæ€§åˆ†æç¡®å®šå¤šæ­¥è§†é‡n=20ä½œä¸ºè·¨ä¸åŒç³»ç»Ÿçš„ç¨³å¥é»˜è®¤å€¼ã€‚

Conclusion: MSACLé€šè¿‡å°†Lyapunovç†è®ºä¸ç¦»ç­–ç•¥actor-criticæ¡†æ¶è¿æ¥ï¼Œä¸ºå¯éªŒè¯çš„å®‰å…¨å­¦ä¹ æ§åˆ¶å¥ å®šäº†åŸºç¡€ã€‚æºä»£ç å’ŒåŸºå‡†ç¯å¢ƒå°†å…¬å¼€æä¾›ã€‚

Abstract: Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $Î»$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.

</details>


### [5] [ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning](https://arxiv.org/abs/2512.25023)
*Timo Kaufmann,Yannick Metz,Daniel Keim,Eyke HÃ¼llermeier*

Main category: cs.LG

TL;DR: æå‡ºResponseRankæ–¹æ³•ï¼Œåˆ©ç”¨ä»£ç†ä¿¡å·ï¼ˆå¦‚å“åº”æ—¶é—´ã€æ ‡æ³¨è€…ä¸€è‡´æ€§ï¼‰çš„ç›¸å¯¹å·®å¼‚æ¥æ¨æ–­åå¥½å¼ºåº¦æ’åºï¼Œé€šè¿‡å±€éƒ¨åˆ†å±‚æ§åˆ¶ç³»ç»Ÿæ€§å˜å¼‚ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸­æå‡æ ·æœ¬æ•ˆç‡å’Œé²æ£’æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»ŸRLHFä½¿ç”¨çš„äºŒå…ƒé€‰æ‹©åªèƒ½ä¼ è¾¾åå¥½æ–¹å‘ï¼Œæ— æ³•è¡¡é‡åå¥½å¼ºåº¦ã€‚è€Œå¼ºåº¦ä¿¡æ¯å¯¹äºä¸ç¡®å®šæ€§å†³ç­–å’Œåå¥½æ¨¡å‹æ³›åŒ–è‡³å…³é‡è¦ã€‚ç°æœ‰ä»£ç†ä¿¡å·ï¼ˆå“åº”æ—¶é—´ã€æ ‡æ³¨è€…ä¸€è‡´æ€§ï¼‰é€šå¸¸å™ªå£°å¤§ä¸”å­˜åœ¨æ··æ·†å› ç´ ã€‚

Method: æå‡ºResponseRankæ–¹æ³•ï¼š1) åˆ©ç”¨ä»£ç†ä¿¡å·çš„ç›¸å¯¹å·®å¼‚å¯¹æˆå¯¹æ¯”è¾ƒä¸­çš„å“åº”è¿›è¡Œåå¥½å¼ºåº¦æ’åºï¼›2) é€šè¿‡ç²¾å¿ƒæ„å»ºçš„åˆ†å±‚è¿›è¡Œå±€éƒ¨æ¯”è¾ƒï¼Œæ§åˆ¶ç³»ç»Ÿæ€§å˜å¼‚ï¼›3) æœ€å°åŒ–å¯¹å¼ºåº¦ä¿¡å·çš„å‡è®¾ï¼Œå®ç°é²æ£’å­¦ä¹ ã€‚

Result: åœ¨åˆæˆåå¥½å­¦ä¹ ï¼ˆæ¨¡æ‹Ÿå“åº”æ—¶é—´ï¼‰ã€è¯­è¨€å»ºæ¨¡ï¼ˆæ ‡æ³¨è€…ä¸€è‡´æ€§ï¼‰å’ŒRLæ§åˆ¶ä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿå›åˆå›æŠ¥ï¼‰ä¸­ï¼ŒResponseRankå±•ç°å‡ºæ”¹è¿›çš„æ ·æœ¬æ•ˆç‡å’Œé²æ£’æ€§ã€‚åŒæ—¶æå‡ºPDCæŒ‡æ ‡æ¥åˆ†ç¦»åŸºæ•°æ•ˆç”¨å­¦ä¹ ä¸åºæ•°å‡†ç¡®æ€§ã€‚

Conclusion: ResponseRankèƒ½å¤Ÿä»å™ªå£°å¼ºåº¦ä¿¡å·ä¸­é²æ£’åœ°å­¦ä¹ åå¥½å¼ºåº¦ï¼Œé€šè¿‡å±€éƒ¨ç›¸å¯¹æ¯”è¾ƒå…‹æœç³»ç»Ÿæ€§å˜å¼‚ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆæä¾›äº†æ›´ä¸°å¯Œçš„å¼ºåº¦ä¿¡æ¯åˆ©ç”¨æ–¹æ³•ã€‚

Abstract: Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [6] [Codex vs. Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuild.ms%2F2025%2F12%2F22%2Fcodex-vs-claude-code-today%2F%3Futm_source=tldrai/1/0100019b50b7bce7-fe11a932-93bb-4e4a-ba91-e551e912cd48-000000/B4U5xapJHmVAYy5IpFBXnBAl_mmDQLGRL-_v1eH9ft8=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: å¯¹æ¯”åˆ†æClaude Codeå’ŒCodexä¸¤æ¬¾AIä»£ç å·¥å…·ï¼Œå¼ºè°ƒé€‰æ‹©åº”æ ¹æ®ä¸ªäººå·¥ä½œæ–¹å¼å†³å®šï¼Œå»ºè®®ç”¨æˆ·å®é™…è¯•ç”¨ä»¥å‘ç°å„è‡ªçš„ä¼˜ç¼ºç‚¹


<details>
  <summary>Details</summary>
Motivation: å¸®åŠ©å¼€å‘è€…äº†è§£ä¸åŒAIä»£ç å·¥å…·çš„ç‰¹ç‚¹ï¼ŒæŒ‡å¯¼ä»–ä»¬æ ¹æ®è‡ªèº«å·¥ä½œæµç¨‹é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼Œè€Œä¸æ˜¯ç›²ç›®è·Ÿéšä¸»æµé€‰æ‹©

Method: é€šè¿‡å¯¹æ¯”åˆ†æçš„æ–¹æ³•ï¼Œå¼ºè°ƒå®é™…ä½¿ç”¨ä½“éªŒçš„é‡è¦æ€§ï¼Œå»ºè®®ç”¨æˆ·äº²è‡ªè¯•ç”¨Claude Codeå’ŒCodexä¸¤æ¬¾å·¥å…·

Result: æŒ‡å‡ºä¸¤æ¬¾å·¥å…·å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œæ²¡æœ‰ç»å¯¹çš„å¥½åä¹‹åˆ†ï¼Œé€‰æ‹©åº”åŸºäºä¸ªäººå·¥ä½œæ–¹å¼å’Œå®é™…ä½¿ç”¨ä½“éªŒ

Conclusion: AIå·¥å…·çš„é€‰æ‹©åº”ä¸ªæ€§åŒ–ï¼Œå¼€å‘è€…éœ€è¦é€šè¿‡å®é™…è¯•ç”¨Claude Codeå’ŒCodexæ¥å‘ç°å“ªæ¬¾å·¥å…·æ›´é€‚åˆè‡ªå·±çš„å·¥ä½œæµç¨‹

Abstract: Codex vs. Claude Code (Today) (5 minute read) There's no wrong choice when it comes to AI. The tool you choose should match how you work. Try out both Claude and Codex and see which one fits. Every AI tool has its strengths and weaknesses, and the only way to discover what they are is by using them.

</details>


### [7] [Stirrup](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FArtificialAnalysis%2FStirrup%3Futm_source=tldrai/1/0100019b50b7bce7-fe11a932-93bb-4e4a-ba91-e551e912cd48-000000/SCXQJmPmQMCo-Go-Sj-kBXFSsN1RsCUSLpXZvtLqP_8=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Stirrupæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œå…è®¸æ¨¡å‹è‡ªä¸»é€‰æ‹©å®Œæˆä»»åŠ¡çš„æ–¹æ³•ï¼Œå†…ç½®æœ€ä½³å®è·µå’Œå·¥å…·ï¼Œæ”¯æŒå®Œå…¨è‡ªå®šä¹‰ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰çš„æ™ºèƒ½ä½“æ¡†æ¶å¾€å¾€é™åˆ¶æ¨¡å‹çš„è‡ªä¸»æ€§å’Œçµæ´»æ€§ï¼Œéœ€è¦ä¸€ç§èƒ½è®©æ¨¡å‹è‡ªä¸»é€‰æ‹©æ–¹æ³•ã€åŒæ—¶æä¾›æ ‡å‡†åŒ–å·¥å…·å’Œæœ€ä½³å®è·µçš„æ¡†æ¶ã€‚

Method: é€šè¿‡æŠ€èƒ½ç³»ç»Ÿæ‰©å±•æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œæä¾›çµæ´»çš„å·¥å…·æ‰§è¡Œã€ä¸Šä¸‹æ–‡ç®¡ç†å·¥å…·ã€å¤šæä¾›å•†æ”¯æŒä»¥åŠå¤šæ¨¡æ€æ”¯æŒï¼Œæ„å»ºä¸€ä¸ªå®Œå…¨å¯å®šåˆ¶çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚

Result: å¼€å‘äº†Stirrupæ¡†æ¶ï¼Œå…·å¤‡æŠ€èƒ½ç³»ç»Ÿã€çµæ´»å·¥å…·æ‰§è¡Œã€ä¸Šä¸‹æ–‡ç®¡ç†ã€å¤šæä¾›å•†æ”¯æŒå’Œå¤šæ¨¡æ€èƒ½åŠ›ï¼Œä¸ºæ„å»ºæ™ºèƒ½ä½“æä¾›äº†æ ‡å‡†åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚

Conclusion: Stirrupæ¡†æ¶æˆåŠŸå®ç°äº†è®©æ¨¡å‹è‡ªä¸»é€‰æ‹©æ–¹æ³•çš„ç›®æ ‡ï¼ŒåŒæ—¶æä¾›äº†å¿…è¦çš„å·¥å…·å’Œæœ€ä½³å®è·µï¼Œä¸ºæ™ºèƒ½ä½“å¼€å‘æä¾›äº†çµæ´»ä¸”æ ‡å‡†åŒ–çš„å¹³å°ã€‚

Abstract: Stirrup (GitHub Repo) Stirrup is a framework for building agents that lets models choose their own approach to completing tasks. It has best practices and tools built in and is fully customizable. Stirrup features a skills system that extends agent capabilities, flexible tool execution, context management tools, flexible provider support, and multimodal support.

</details>


### [8] [Delve Shipmas](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdelve.co%2Fbook-demo%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=tldr-primary-dec26-25/2/0100019b5a6655f3-905a9bc6-b7be-4501-af53-a7c1c8093375-000000/DDfDISgxi995g3ex53sY5NP57GiUHkzm3fAHJnx6iCk=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Delveæ¨å‡ºCUAï¼Œä¸€ä¸ªè®¡ç®—æœºä½¿ç”¨AIä»£ç†ï¼Œè‡ªåŠ¨ä¸ºåˆè§„å®¡è®¡ç”Ÿæˆæˆªå›¾ï¼Œå‡å°‘äººå·¥æ“ä½œæ—¶é—´


<details>
  <summary>Details</summary>
Motivation: è§£å†³åˆè§„å®¡è®¡ä¸­æ‰‹åŠ¨æˆªå›¾ã€æ ‡è®°æ–‡ä»¶ç­‰é‡å¤æ€§å·¥ä½œè€—æ—¶çš„é—®é¢˜ï¼Œæé«˜å®¡è®¡æ•ˆç‡

Method: å¼€å‘è®¡ç®—æœºä½¿ç”¨AIä»£ç†ï¼ˆCUAï¼‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å¯¼èˆªä»ªè¡¨æ¿ã€æˆªå›¾ã€æ ‡è®°æ–‡ä»¶ï¼Œå®ç°åˆè§„å®¡è®¡è‡ªåŠ¨åŒ–

Result: æ¨å‡ºDelve CUAäº§å“ï¼Œä½œä¸ºDelve Shipmasæ´»åŠ¨çš„ä¸€éƒ¨åˆ†å‘å¸ƒï¼Œæ—¨åœ¨ç®€åŒ–åˆè§„å®¡è®¡æµç¨‹

Conclusion: CUAèƒ½å¤Ÿæ˜¾è‘—å‡å°‘åˆè§„å®¡è®¡ä¸­çš„äººå·¥æ“ä½œæ—¶é—´ï¼Œæé«˜å®¡è®¡æ•ˆç‡å’Œå‡†ç¡®æ€§

Abstract: Delve Shipmas Day 5: Automated Screenshots with Delve CUA (Sponsor) Welcome to the final day of Delve Shipmas ğŸ„ We've announced an AI copilot, AI-native VRM, an AI security questionnaire extension, all-new frameworks, and now: CUA.If you've ever spent hours: clicking through dashboards, taking screenshots, labeling files, and then re-doing it all again for the next audit.â€¦this one's for you. Today, Delve is launching Delve CUA: a computer-using AI agent that takes compliance screenshots for y...

</details>


### [9] [Claude How To](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fluongnv89%2Fclaude-howto%3Futm_source=tldrnewsletter/1/0100019b5a6655f3-905a9bc6-b7be-4501-af53-a7c1c8093375-000000/54bmQYtj-LLjs0GTzdWg9bQZPti7VFfUDgitHLuRuDY=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHubä»“åº“æä¾›Claude CodeåŠŸèƒ½çš„å®Œæ•´ä½¿ç”¨æŒ‡å—


<details>
  <summary>Details</summary>
Motivation: å¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨Claude Codeçš„å„é¡¹åŠŸèƒ½ï¼Œæé«˜ç¼–ç æ•ˆç‡

Method: åˆ›å»ºè¯¦ç»†çš„æ–‡æ¡£å’Œç¤ºä¾‹ä»£ç ï¼Œå±•ç¤ºClaude Codeçš„å„ç§åŠŸèƒ½å’Œä½¿ç”¨åœºæ™¯

Result: æä¾›äº†å…¨é¢çš„Claude Codeä½¿ç”¨æŒ‡å—ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€è°ƒè¯•ã€é‡æ„ç­‰åŠŸèƒ½çš„å…·ä½“æ“ä½œæ–¹æ³•

Conclusion: è¯¥èµ„æºä¸ºå¼€å‘è€…æä¾›äº†å®ç”¨çš„Claude Codeä½¿ç”¨å‚è€ƒï¼Œæœ‰åŠ©äºæå‡ç¼–ç¨‹å·¥ä½œæ•ˆç‡

Abstract: Claude How To (GitHub Repo) A complete guide to Claude Code features.

</details>


### [10] [A Guide to Claude Code 2.0 and Getting Better at Using Coding Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsankalp.bearblog.dev%2Fmy-experience-with-claude-code-20-and-how-to-get-better-at-using-coding-agents%2F%3Futm_source=tldrnewsletter/1/0100019b69d73709-696ca2d6-e8c3-45c1-add5-296c6e82c8b8-000000/pKaVPgy0ZQ4sWVsBun3Jo4UhgTviNiMN-pIsFsGkWpg=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: è¿™æ˜¯ä¸€ç¯‡å…³äºClaude Code 2.0çš„ä½¿ç”¨æŒ‡å—ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ä½¿ç”¨ç¼–ç ä»£ç†å·¥å…·ï¼Œæå‡ä¸ªäººä½¿ç”¨å’Œç”Ÿäº§çº§å·¥ç¨‹èƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: Claude Codeä»Šå¹´åœ¨CLIç¼–ç äº§å“ä½“éªŒä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†ç”¨æˆ·éœ€è¦ç†è§£å…¶å·¥ä½œåŸç†å’Œæœ€ä½³å®è·µæ‰èƒ½å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ã€‚ä½œè€…å¸Œæœ›é€šè¿‡åˆ†äº«æ€ç»´è¿‡ç¨‹å’Œç®€å•æŠ€å·§ï¼Œå¸®åŠ©ç”¨æˆ·æŒæ¡ç¼–ç ä»£ç†å·¥å…·çš„ä½¿ç”¨æ–¹æ³•ã€‚

Method: æ–‡ç« é€šè¿‡å±•ç¤ºClaude Codeçš„æ€ç»´è¿‡ç¨‹å’Œå…³é”®ä½¿ç”¨è¦ç‚¹ï¼Œæä¾›å®ç”¨æŒ‡å—ã€‚å¼ºè°ƒä»Claude Codeå­¦åˆ°çš„æŠ€èƒ½å¯ä»¥ç›´æ¥è¿ç§»åˆ°å…¶ä»–å·¥å…·ï¼Œæ¶µç›–ä¸ªäººä½¿ç”¨å’Œç”Ÿäº§çº§å·¥ç¨‹ä¸¤ä¸ªå±‚é¢ã€‚

Result: ç”¨æˆ·èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨Claude Codeï¼ŒæŒæ¡ç¼–ç ä»£ç†å·¥å…·çš„æ ¸å¿ƒåŸç†ï¼Œä»è€Œè·Ÿä¸Šç¼–ç ä»£ç†æŠ€æœ¯çš„å‘å±•è¶‹åŠ¿ã€‚

Conclusion: æŒæ¡Claude Codeçš„ä½¿ç”¨æŠ€å·§ä¸ä»…æœ‰åŠ©äºæå‡å½“å‰å·¥å…·çš„ä½¿ç”¨æ•ˆç‡ï¼Œè¿˜èƒ½ä¸ºä½¿ç”¨å…¶ä»–ç¼–ç ä»£ç†å·¥å…·æ‰“ä¸‹åŸºç¡€ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨å¿«é€Ÿå‘å±•çš„ç¼–ç ä»£ç†é¢†åŸŸä¿æŒç«äº‰åŠ›ã€‚

Abstract: A Guide to Claude Code 2.0 and Getting Better at Using Coding Agents (55 minute read) Claude Code dominated the CLI coding product experience this year. This guide shows readers the thought processes and simple things to keep in mind to get the most out of Claude Code. Learning how things work in Claude Code directly transfers to other tools, both in terms of personal usage and production-grade engineering. The post will help users keep up with coding agents in general.

</details>


### [11] [Clopus-Watcher: An autonomous monitoring agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdenislavgavrilov.com%2Fp%2Fclopus-watcher-an-autonomous-monitoring%3Futm_source=tldrnewsletter/1/0100019b6eff6ec8-65de829a-5877-420b-a59f-5c6a82a0ed70-000000/fXpv7wp6VInCqWkAHXYVK5d0S94Dcoz-Ok_FLiZ3kmg=437)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ä»‹ç»äº†ä¸€ä¸ªè‡ªä¸»ç›‘æ§ä»£ç†Clopus-Watcherï¼Œèƒ½å¤Ÿæ›¿ä»£äººå·¥on-callå·¥ç¨‹å¸ˆè¿›è¡Œ24/7ç³»ç»Ÿç›‘æ§ï¼Œè‡ªåŠ¨æ‰§è¡Œæ•…éšœæ¢å¤æ“ä½œ


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿ24/7äººå·¥on-callç›‘æ§å·¥ä½œè´Ÿæ‹…é‡ä¸”æˆæœ¬é«˜ï¼Œè€ŒAIæŠ€æœ¯å‘å±•ä½¿å¾—è‡ªåŠ¨åŒ–ç›‘æ§æˆä¸ºå¯èƒ½ï¼Œå¯ä»¥è§£æ”¾å·¥ç¨‹å¸ˆå¹¶æé«˜ç³»ç»Ÿå¯é æ€§

Method: å¼€å‘äº†ä¸€ä¸ªè‡ªä¸»ç›‘æ§ä»£ç†ï¼Œèƒ½å¤Ÿåƒon-callå·¥ç¨‹å¸ˆä¸€æ ·å·¥ä½œï¼šç›‘æ§ç³»ç»ŸçŠ¶æ€ã€å‚è€ƒæ–‡æ¡£æ‰§è¡Œæ¢å¤æ“ä½œã€å®æ–½å¤‡ä»½å’Œæ¢å¤è®¡åˆ’

Result: åˆ›å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œon-callå·¥ç¨‹å¸ˆä»»åŠ¡çš„AIä»£ç†ï¼Œå®ç°24/7è‡ªåŠ¨åŒ–ç³»ç»Ÿç›‘æ§å’Œæ•…éšœæ¢å¤

Conclusion: AIé©±åŠ¨çš„è‡ªä¸»ç›‘æ§ä»£ç†æœ‰æœ›å–ä»£ä¼ ç»Ÿäººå·¥on-callï¼Œä½¿24/7ç›‘æ§æˆä¸ºå†å²ï¼Œæé«˜è¿ç»´æ•ˆç‡å’Œç³»ç»Ÿå¯é æ€§

Abstract: Clopus-Watcher: An autonomous monitoring agent (8 minute read) AI will likely make 24/7 on-call a thing of the past. 24/7 monitoring is a lot simpler than the development process. There are often reference documents that engineers can follow to bring systems back up, and if they fail, there's always a backup and recovery plan in place. On-call jobs have always been more systematic. This post introduces an autonomous monitoring agent that does what an on-call engineer would do, but autonomousl...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use](https://arxiv.org/abs/2512.24565)
*Wenrui Liu,Zixiang Liu,Elsie Dai,Wenhan Yu,Lei Yu,Tong Yang*

Main category: cs.AI

TL;DR: æå‡ºäº†MCPAgentBenchåŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºäºMCPåè®®çš„LLMæ™ºèƒ½ä½“å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ŒåŒ…å«çœŸå®ä»»åŠ¡ã€æ¨¡æ‹Ÿå·¥å…·å’ŒåŠ¨æ€æ²™ç®±ç¯å¢ƒï¼Œæµ‹è¯•æ¨¡å‹åœ¨å¤æ‚å¤šæ­¥å·¥å…·è°ƒç”¨ä¸­çš„è¡¨ç°ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰MCPè¯„ä¼°é›†å­˜åœ¨ä¾èµ–å¤–éƒ¨MCPæœåŠ¡ã€ç¼ºä¹éš¾åº¦æ„ŸçŸ¥ç­‰é—®é¢˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„åŸºå‡†æ¥è¯„ä¼°æ™ºèƒ½ä½“çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚

Method: æ„å»ºåŸºäºçœŸå®MCPå®šä¹‰çš„æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ä»»åŠ¡å’Œæ¨¡æ‹ŸMCPå·¥å…·ï¼›é‡‡ç”¨åŠ¨æ€æ²™ç®±ç¯å¢ƒï¼Œæä¾›åŒ…å«å¹²æ‰°é¡¹çš„å·¥å…·å€™é€‰åˆ—è¡¨ï¼›å¼•å…¥ç»¼åˆæŒ‡æ ‡è¡¡é‡ä»»åŠ¡å®Œæˆç‡å’Œæ‰§è¡Œæ•ˆç‡ã€‚

Result: åœ¨å¤šç§æœ€æ–°ä¸»æµå¤§è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨å¤„ç†å¤æ‚å¤šæ­¥å·¥å…·è°ƒç”¨æ—¶å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚

Conclusion: MCPAgentBenchä¸ºè¯„ä¼°æ™ºèƒ½ä½“å·¥å…·ä½¿ç”¨èƒ½åŠ›æä¾›äº†æœ‰æ•ˆåŸºå‡†ï¼Œä»£ç å·²å¼€æºï¼Œæœ‰åŠ©äºæ¨åŠ¨ç›¸å…³ç ”ç©¶å‘å±•ã€‚

Abstract: Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.

</details>


### [13] [Iterative Deployment Improves Planning Skills in LLMs](https://arxiv.org/abs/2512.24940)
*Augusto B. CorrÃªa,Yoav Gelberg,Luckeciano C. Melo,Ilia Shumailov,AndrÃ© G. Pereira,Yarin Gal*

Main category: cs.AI

TL;DR: è¿­ä»£éƒ¨ç½²LLMå¹¶åŸºäºç”¨æˆ·ç²¾é€‰æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¯æ˜¾è‘—æ”¹å˜æ¨¡å‹ç‰¹æ€§ï¼Œåœ¨è§„åˆ’ä»»åŠ¡ä¸­å®ç°èƒ½åŠ›æå‡å’Œæ³›åŒ–ï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ç§éšå¼å¼ºåŒ–å­¦ä¹ æœºåˆ¶


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶è¿­ä»£éƒ¨ç½²LLMå¹¶åŸºäºç”¨æˆ·ç²¾é€‰æ•°æ®å¾®è°ƒå¯¹æ¨¡å‹ç‰¹æ€§çš„å½±å“ï¼Œæ¢ç´¢è¿™ç§æœºåˆ¶ä¸å¼ºåŒ–å­¦ä¹ çš„ç†è®ºè”ç³»åŠå…¶å¯¹AIå®‰å…¨å’Œè®­ç»ƒæ–¹æ³•çš„æ„ä¹‰

Method: é‡‡ç”¨è¿­ä»£éƒ¨ç½²æœºåˆ¶ï¼šæ¯æ¬¡éƒ¨ç½²LLMåï¼Œç”¨æˆ·ä»æ¨¡å‹è¾“å‡ºä¸­ç²¾é€‰æ•°æ®ï¼Œç”¨äºå¾®è°ƒä¸‹ä¸€è½®æ¨¡å‹ï¼›åœ¨å¤šä¸ªè§„åˆ’é¢†åŸŸæµ‹è¯•ï¼›è¿›è¡Œç†è®ºåˆ†æï¼Œè¯æ˜è¯¥æœºåˆ¶ç­‰ä»·äºå¤–å±‚å¾ªç¯çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

Result: åœ¨è§„åˆ’ä»»åŠ¡ä¸­è§‚å¯Ÿåˆ°æ˜¾è‘—æ”¹è¿›ï¼Œåç»­æ¨¡å‹å±•ç°å‡ºæ¶Œç°æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå‘ç°æ¯”åˆå§‹æ¨¡å‹é•¿å¾—å¤šçš„è§„åˆ’æ–¹æ¡ˆï¼›ç†è®ºåˆ†æè¡¨æ˜è¿­ä»£éƒ¨ç½²å®ç°äº†éšå¼å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ 

Conclusion: è¿­ä»£éƒ¨ç½²æœºåˆ¶æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ›¿ä»£è®­ç»ƒèŒƒå¼ï¼Œä¾èµ–æ•°æ®ç²¾é€‰è€Œéæ˜¾å¼å¥–åŠ±ï¼›å¯¹AIå®‰å…¨æœ‰é‡è¦å½±å“ï¼Œå› ä¸ºéšå¼å¥–åŠ±å‡½æ•°å¯èƒ½å¯¼è‡´æœªæ¥æ¨¡å‹éƒ¨ç½²çš„æ„å¤–ç‰¹æ€§

Abstract: We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.

</details>


### [14] [AMAP Agentic Planning Technical Report](https://arxiv.org/abs/2512.24957)
*Yulan Hu,Xiangwen Zhang,Sheng Ouyang,Hao Yi,Lu Xu,Qinglin Lang,Lide Tan,Xiang Cheng,Tianchen Ye,Zhicong Li,Ge Chen,Wenjin Yang,Zheng Pan,Shaopan Xiong,Siran Yang,Ju Huang,Yan Zhang,Jiamang Wang,Yong Liu,Yinfeng Huang,Tucheng Lin,Xin Li,Ning Guo*

Main category: cs.AI

TL;DR: STAgentæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæ—¶ç©ºç†è§£çš„æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å·¥å…·äº¤äº’å’Œåˆ†å±‚è®­ç»ƒæ–¹æ³•ï¼Œåœ¨ä¿æŒé€šç”¨èƒ½åŠ›çš„åŒæ—¶è§£å†³å¤æ‚çš„æ—¶ç©ºä»»åŠ¡ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„æ—¶ç©ºç†è§£ä»»åŠ¡ï¼ˆå¦‚å—é™å…´è¶£ç‚¹å‘ç°å’Œè¡Œç¨‹è§„åˆ’ï¼‰ä¸Šå­˜åœ¨å±€é™ï¼Œéœ€è¦ä¸“é—¨çš„æ™ºèƒ½ä½“æ¨¡å‹æ¥æœ‰æ•ˆå¤„ç†è¿™äº›éœ€è¦å¤šæ­¥éª¤æ¨ç†å’Œå·¥å…·äº¤äº’çš„åœºæ™¯ã€‚

Method: é‡‡ç”¨ä¸‰é˜¶æ®µæ–¹æ³•ï¼š1ï¼‰æ„å»ºæ”¯æŒ10+æ—¶ç©ºé¢†åŸŸå·¥å…·çš„ç¨³å®šå·¥å…·ç¯å¢ƒï¼›2ï¼‰åˆ†å±‚æ•°æ®ç­›é€‰æ¡†æ¶ï¼Œä»¥1:10,000çš„æ¯”ä¾‹ç­›é€‰é«˜è´¨é‡æŸ¥è¯¢ï¼›3ï¼‰çº§è”è®­ç»ƒç­–ç•¥ï¼šç§å­SFTé˜¶æ®µè¯„ä¼°æŸ¥è¯¢éš¾åº¦ï¼Œç¬¬äºŒSFTé˜¶æ®µå¾®è°ƒé«˜ç¡®å®šæ€§æŸ¥è¯¢ï¼Œæœ€ç»ˆRLé˜¶æ®µåˆ©ç”¨ä½ç¡®å®šæ€§æ•°æ®ã€‚

Result: STAgentåœ¨TravelBenchåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨å¹¿æ³›çš„é€šç”¨åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†å…¶é€šç”¨èƒ½åŠ›ï¼Œè¯æ˜äº†æ‰€æå‡ºçš„æ™ºèƒ½ä½“æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

Conclusion: STAgenté€šè¿‡ä¸“é—¨çš„å·¥å…·ç¯å¢ƒã€é«˜è´¨é‡æ•°æ®ç­›é€‰å’Œçº§è”è®­ç»ƒæ–¹æ³•ï¼ŒæˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªæ—¢èƒ½å¤„ç†å¤æ‚æ—¶ç©ºä»»åŠ¡åˆèƒ½ä¿æŒé€šç”¨èƒ½åŠ›çš„æ™ºèƒ½ä½“æ¨¡å‹ï¼Œä¸ºé¢†åŸŸç‰¹å®šçš„æ™ºèƒ½ä½“å¼€å‘æä¾›äº†æœ‰æ•ˆæ¡†æ¶ã€‚

Abstract: We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.

</details>


### [15] [Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings](https://arxiv.org/abs/2512.25055)
*Tianzhi He,Farrokh Jazizadeh*

Main category: cs.AI

TL;DR: æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å»ºç­‘èƒ½æºç®¡ç†AIä»£ç†æ¡†æ¶ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èƒ½æºç®¡ç†ï¼Œåœ¨è®¾å¤‡æ§åˆ¶ã€è®°å¿†ä»»åŠ¡ç­‰æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†æˆæœ¬ä¼°ç®—ç­‰å¤æ‚ä»»åŠ¡ä»éœ€æ”¹è¿›ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰èƒ½æºç®¡ç†ç³»ç»Ÿå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´æ™ºèƒ½ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„è‡ªä¸»æ•°æ®åˆ†æèƒ½åŠ›ï¼Œå¼€å‘èƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŸ¥è¯¢å¹¶æä¾›æ™ºèƒ½èƒ½æºç®¡ç†çš„AIä»£ç†ã€‚

Method: æå‡ºåŒ…å«æ„ŸçŸ¥ã€ä¸­å¤®æ§åˆ¶å’Œè¡ŒåŠ¨ä¸‰ä¸ªæ¨¡å—çš„æ¦‚å¿µæ¡†æ¶ï¼Œå½¢æˆé—­ç¯åé¦ˆç³»ç»Ÿã€‚ä½¿ç”¨120ä¸ªç”¨æˆ·æŸ¥è¯¢åœ¨å››ä¸ªçœŸå®ä½å®…èƒ½æºæ•°æ®é›†ä¸Šè¯„ä¼°åŸå‹æ€§èƒ½ï¼Œè¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å»¶è¿Ÿã€åŠŸèƒ½ã€èƒ½åŠ›ã€å‡†ç¡®æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚

Result: åŸå‹åœ¨è®¾å¤‡æ§åˆ¶å‡†ç¡®ç‡86%ã€è®°å¿†ç›¸å…³ä»»åŠ¡97%ã€è°ƒåº¦è‡ªåŠ¨åŒ–74%ã€èƒ½æºåˆ†æ77%æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†æˆæœ¬ä¼°ç®—ä»»åŠ¡å‡†ç¡®ç‡ä»…49%ã€‚é€šè¿‡ANOVAæµ‹è¯•éªŒè¯äº†æ¡†æ¶çš„æ™®é€‚æ€§ã€‚

Conclusion: è¯¥ç ”ç©¶ä¸ºåŸºäºLLMçš„BEMS AIä»£ç†è¯„ä¼°æä¾›äº†æ¡†æ¶ï¼Œå±•ç¤ºäº†åœ¨è‡ªç„¶è¯­è¨€äº¤äº’èƒ½æºç®¡ç†æ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶æŒ‡å‡ºäº†å“åº”å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

Abstract: This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype's performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.

</details>


### [16] [What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?](https://arxiv.org/abs/2512.24497)
*Basile Terver,Tsung-Yen Yang,Jean Ponce,Adrien Bardes,Yann LeCun*

Main category: cs.AI

TL;DR: è¯¥è®ºæ–‡æå‡ºJEPA-WMæ¨¡å‹ï¼Œé€šè¿‡åœ¨ä¸–ç•Œæ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡Œè§„åˆ’ï¼Œåœ¨ç‰©ç†ä»»åŠ¡ä¸­å®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œè¶…è¶Šäº†DINO-WMå’ŒV-JEPA-2-ACç­‰åŸºçº¿æ¨¡å‹ã€‚


<details>
  <summary>Details</summary>
Motivation: AIé¢†åŸŸé•¿æœŸå­˜åœ¨ä¸€ä¸ªæŒ‘æˆ˜ï¼šå¼€å‘èƒ½å¤Ÿè§£å†³å¹¿æ³›ç‰©ç†ä»»åŠ¡å¹¶èƒ½æ³›åŒ–åˆ°æ–°æœªè§ä»»åŠ¡å’Œç¯å¢ƒçš„æ™ºèƒ½ä½“ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ä¸–ç•Œæ¨¡å‹çš„çŠ¶æ€-åŠ¨ä½œè½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨è¾“å…¥ç©ºé—´ä¸­è¿›è¡Œè§„åˆ’ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜åœ¨è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡Œè§„åˆ’å¯èƒ½æ›´é«˜æ•ˆã€‚

Method: å°†è¿™ç±»æ–¹æ³•å®šä¹‰ä¸ºJEPA-WMï¼Œç³»ç»Ÿç ”ç©¶äº†æ¨¡å‹æ¶æ„ã€è®­ç»ƒç›®æ ‡å’Œè§„åˆ’ç®—æ³•ç­‰å…³é”®ç»„ä»¶ã€‚é€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®æœºå™¨äººæ•°æ®è¿›è¡Œå®éªŒï¼Œåˆ†æå„ç»„ä»¶å¯¹è§„åˆ’æˆåŠŸç‡çš„å½±å“ï¼Œæœ€ç»ˆæå‡ºä¼˜åŒ–æ¨¡å‹ã€‚

Result: æå‡ºçš„æ¨¡å‹åœ¨å¯¼èˆªå’Œæ“ä½œä»»åŠ¡ä¸Šéƒ½è¶…è¶Šäº†DINO-WMå’ŒV-JEPA-2-ACä¸¤ä¸ªåŸºå‡†æ¨¡å‹ï¼Œè¯æ˜äº†åœ¨è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡Œè§„åˆ’çš„ä¼˜åŠ¿ã€‚

Conclusion: JEPA-WMæ–¹æ³•é€šè¿‡åœ¨ä¸–ç•Œæ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡Œè§„åˆ’ï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°è§£å†³ç‰©ç†ä»»åŠ¡å¹¶å®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸ºAIæ™ºèƒ½ä½“å¼€å‘æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚

Abstract: A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.

</details>
