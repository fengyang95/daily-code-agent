<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.SE](#cs.SE) [Total: 9]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Teaching People LLM's Errors and Getting it Right](https://arxiv.org/abs/2512.21422)
*Nathan Stringham,Fateme Hashemi Chaleshtori,Xinyuan Yan,Zhichao Xu,Bei Wang,Ana Marasović*

Main category: cs.CL

TL;DR: 论文分析用户过度依赖LLM的原因，发现失败模式确实存在但难以自动发现，提出新的评估指标显示教学失败模式可有效减少过度依赖


<details>
  <summary>Details</summary>
Motivation: 用户经常在不该使用LLM的情况下过度依赖它们，部分原因是看到LLM能完成复杂任务而误以为它们不会在简单任务上出错。现有方法试图通过聚类失败模式来教学用户，但效果不佳，本文旨在探究原因

Method: 1) 检查失败模式是否存在：通过元标签分组评估LLM表现，识别显著且错误率高的组；2) 测试提示和嵌入方法能否发现已知失败模式；3) 提出新评估指标衡量用户利用失败模式预测LLM错误的能力，并进行用户研究

Result: 1) 确实存在显著的失败模式；2) 现有自动发现方法效果参差不齐；3) 使用新指标的用户研究显示教学失败模式有积极效果，而传统的人机协作准确率指标则无此效果

Conclusion: 教学失败模式是减少用户过度依赖的可行方法，但成功取决于更好的自动失败发现方法和使用本文提出的新评估指标

Abstract: People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.
  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.

</details>


### [2] [Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management](https://arxiv.org/abs/2512.21567)
*Changzhi Sun,Xiangyu Chen,Jixiang Luo,Dell Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 论文提出DAM框架，将LLM内存管理重构为不确定性下的序列决策问题，使用价值函数和不确定性估计评估操作，替代启发式方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM系统的外部内存管理主要依赖人工设计的启发式方法，难以预测内存决策的长期和不确定后果。内存的读写选择会影响未来的检索和下游行为，需要更系统的决策框架。

Method: 提出DAM（决策理论代理内存）框架，将内存管理分解为即时信息访问和分层存储维护。通过价值函数和不确定性估计评估候选操作，基于长期效用和风险估计的聚合策略进行决策仲裁。

Result: 论文的主要贡献不是新算法，而是原则性的重构框架，阐明了启发式方法的局限性，为未来不确定性感知内存系统研究奠定了基础。

Conclusion: 内存管理应被视为不确定性下的序列决策问题，DAM框架为此提供了理论基础，能够更好地处理内存效用的延迟性和未来交互依赖性。

Abstract: External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.

</details>


### [3] [MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles](https://arxiv.org/abs/2512.21708)
*Jing Han,Binwei Yan,Tianyu Guo,Zheyuan Bai,Mengyu Zheng,Hanting Chen,Ying Nie*

Main category: cs.CL

TL;DR: 提出了Mixture-of-Roles (MoR)框架，通过将智能体任务分解为推理者、执行者和总结者三个角色，使用专门的LoRA组进行参数高效微调，在多个LLM和智能体基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管最近在微调大语言模型以促进智能体任务方面取得了进展，但针对智能体的参数高效微调方法仍然缺乏探索。当前需要一种能够有效分解智能体能力并实现高效微调的方法。

Method: 1) 将智能体任务能力分解为三个角色：推理者（理解查询并决定下一步角色）、执行者（识别要调用的函数和参数）、总结者（向用户传递对话的提炼信息）。2) 提出Mixture-of-Roles框架，包含三个专门的LoRA组，每个组负责一个角色。3) 开发基于公开数据集的多角色数据生成管道，包含角色特定的内容补全和可靠性验证。

Result: 在多个LLM和智能体基准测试上进行了广泛实验和消融研究，证明了所提方法的有效性。项目已公开在https://mor-agent.github.io。

Conclusion: MoR框架通过角色分解和专门的LoRA组实现了智能体任务的参数高效微调，为智能体PEFT方法提供了新的解决方案。

Abstract: Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant Reason+Action paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification. We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io.

</details>


### [4] [Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments](https://arxiv.org/abs/2512.21817)
*Hong Su*

Main category: cs.CL

TL;DR: DeMe框架通过从隐藏目标、积累经验和环境反馈中提取显式装饰来修改LLM的方法生成路径，使IoT设备能适应未知或故障环境


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的IoT系统在遇到未见情况时缺乏系统生成新方法的能力，且依赖固定的设备特定逻辑，无法适应变化的环境条件

Method: 提出Method Decoration (DeMe)框架，通过从隐藏目标、积累学习方法和环境反馈中提取显式装饰来修改LLM的方法生成路径。装饰不是硬编码的，而是从通用行为原则、经验和观察到的环境差异中提取。支持预装饰、后装饰、中间步骤修改和步骤插入四种方式

Result: 实验结果表明，方法装饰使IoT设备在面对未知或故障操作条件时能够推导出更合适的方法

Conclusion: DeMe框架通过动态装饰LLM的方法生成路径，实现了上下文感知、安全对齐和环境自适应的方法生成，提升了IoT系统在动态环境中的适应能力

Abstract: Intelligent IoT systems increasingly rely on large language models (LLMs) to generate task-execution methods for dynamic environments. However, existing approaches lack the ability to systematically produce new methods when facing previously unseen situations, and they often depend on fixed, device-specific logic that cannot adapt to changing environmental conditions.In this paper, we propose Method Decoration (DeMe), a general framework that modifies the method-generation path of an LLM using explicit decorations derived from hidden goals, accumulated learned methods, and environmental feedback. Unlike traditional rule augmentation, decorations in DeMe are not hardcoded; instead, they are extracted from universal behavioral principles, experience, and observed environmental differences. DeMe enables the agent to reshuffle the structure of its method path-through pre-decoration, post-decoration, intermediate-step modification, and step insertion-thereby producing context-aware, safety-aligned, and environment-adaptive methods. Experimental results show that method decoration allows IoT devices to derive ore appropriate methods when confronting unknown or faulty operating conditions.

</details>


### [5] [SWE-RM: Execution-free Feedback For Software Engineering Agents](https://arxiv.org/abs/2512.21919)
*KaShun Shum,Binyuan Hui,Jiawei Chen,Lei Zhang,X. W.,Jiaxi Yang,Yuzhen Huang,Junyang Lin,Junxian He*

Main category: cs.CL

TL;DR: 本文提出SWE-RM奖励模型，通过混合专家架构解决编码代理中执行反馈稀疏性问题，显著提升软件工程代理在测试时扩展和强化学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前编码代理开发主要依赖单元测试等执行反馈，但这类反馈稀疏且无法有效区分成功/失败的轨迹。执行无关的奖励模型反馈能提供更细粒度信号，但在实际软件工程代理中尚未充分探索。研究发现测试时扩展性能相似的验证器在强化学习中表现差异很大，需要开发更稳健的奖励模型。

Method: 识别出对强化学习训练至关重要的分类准确性和校准两个关键方面，通过控制实验研究训练数据规模、策略混合、数据源组成等因素的影响。基于这些研究，提出SWE-RM奖励模型，采用混合专家架构，总参数量30B，推理时激活3B参数。

Result: SWE-RM显著提升软件工程代理在测试时扩展和强化学习中的性能。在SWE-Bench Verified上，将Qwen3-Coder-Flash准确率从51.6%提升至62.0%，Qwen3-Coder-Max从67.0%提升至74.6%，在开源模型中达到新的最先进性能。

Conclusion: 执行无关的奖励模型反馈能有效解决编码代理开发中的反馈稀疏性问题。通过识别分类准确性和校准等关键因素，并采用混合专家架构，可以开发出在测试时扩展和强化学习中均表现优异的稳健奖励模型。

Abstract: Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.

</details>


### [6] [Context as a Tool: Context Management for Long-Horizon SWE-Agents](https://arxiv.org/abs/2512.22087)
*Shukai Liu,Jian Yang,Bo Jiang,Yizhi Li,Jinyang Guo,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: CAT提出了一种新的上下文管理范式，将上下文维护作为可调用工具集成到智能体决策过程中，通过结构化工作空间和主动压缩机制解决长时交互中的上下文爆炸和语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的软件工程智能体在处理仓库级代码库的长时交互任务时，通常采用追加式上下文维护或被动触发压缩启发式方法，导致上下文爆炸、语义漂移和推理能力下降。

Method: 提出CAT上下文管理范式，建立结构化上下文工作空间（稳定任务语义、压缩长期记忆、高保真短期交互），使智能体能在适当时机主动压缩历史轨迹为可操作摘要。开发CAT-GENERATOR轨迹级监督框架，通过离线数据构建管道注入上下文管理动作，训练出上下文感知模型SWE-Compressor。

Result: 在SWE-Bench-Verified基准测试中，SWE-Compressor达到57.6%的解决率，显著优于基于ReAct的智能体和静态压缩基线，同时在有限上下文预算下保持稳定且可扩展的长时推理能力。

Conclusion: CAT范式通过将上下文管理提升为智能体决策过程中的可调用工具，有效解决了长时软件工程任务中的上下文管理挑战，显著提升了智能体的性能和可扩展性。

Abstract: Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent](https://arxiv.org/abs/2512.21578)
*Ali Sahami,Sudhanshu Garg,Andrew Wang,Chaitanya Kulkarni,Farhad Farahani,Sean Yun-Shiuan Chuang,Jian Wan,Srinivasan Manoharan,Uma Kona,Nitin Sharma,Linsey Pang,Prakhar Mehrotra,Jessica Clark,Mark Moyou*

Main category: cs.AI

TL;DR: PayPal与NVIDIA合作，使用NeMo框架微调Nemotron小型语言模型，优化其Commerce Agent中的搜索与发现代理，显著降低了延迟和成本，同时保持系统性能。


<details>
  <summary>Details</summary>
Motivation: PayPal Commerce Agent中的检索组件占用了超过50%的总响应时间，存在性能瓶颈。需要优化搜索与发现代理以提升整体系统效率，降低延迟和成本。

Method: 使用NVIDIA NeMo框架对Nemotron小型语言模型进行微调，采用llama3.1-nemotron-nano-8B-v1架构，通过LoRA技术进行系统化的超参数扫描（学习率、优化器、余弦退火调度、LoRA秩等）。

Result: 微调后的Nemotron SLM有效解决了检索组件的关键性能问题，在保持或提升整体系统性能的同时，显著改善了延迟和成本指标。

Conclusion: 该研究展示了NeMo框架在商业特定代理优化中的首次应用，为生产环境中的电子商务多代理系统优化提供了可扩展的框架。

Abstract: We present the development and optimization of PayPal's Commerce Agent, powered by NEMO-4-PAYPAL, a multi-agent system designed to revolutionize agentic commerce on the PayPal platform. Through our strategic partnership with NVIDIA, we leveraged the NeMo Framework for LLM model fine-tuning to enhance agent performance. Specifically, we optimized the Search and Discovery agent by replacing our base model with a fine-tuned Nemotron small language model (SLM).
  We conducted comprehensive experiments using the llama3.1-nemotron-nano-8B-v1 architecture, training LoRA-based models through systematic hyperparameter sweeps across learning rates, optimizers (Adam, AdamW), cosine annealing schedules, and LoRA ranks. Our contributions include: (1) the first application of NVIDIA's NeMo Framework to commerce-specific agent optimization, (2) LLM powered fine-tuning strategy for retrieval-focused commerce tasks, (3) demonstration of significant improvements in latency and cost while maintaining agent quality, and (4) a scalable framework for multi-agent system optimization in production e-commerce environments. Our results demonstrate that the fine-tuned Nemotron SLM effectively resolves the key performance issue in the retrieval component, which represents over 50\% of total agent response time, while maintaining or enhancing overall system performance.

</details>


### [8] [AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design](https://arxiv.org/abs/2512.21613)
*Zhishuai Zhang,Xintian Li,Shilong Liu,Aodong Zhang,Lu Jie,Nan Sun*

Main category: cs.AI

TL;DR: 提出AMS-IO-Agent，一个基于LLM的领域专用代理，用于模拟和混合信号集成电路的结构感知I/O子系统生成，将自然语言设计意图转化为工业级设计交付物。


<details>
  <summary>Details</summary>
Motivation: 传统AMS IC设计流程中，I/O子系统生成需要大量手动工作，设计周期长且容易出错。需要将自然语言设计意图与工业级设计交付物连接起来，提高设计自动化水平。

Method: 开发了包含结构化领域知识库和设计意图结构化两个关键能力的框架。知识库捕获可重用约束和设计惯例，设计意图结构化将模糊用户意图转换为可验证的逻辑步骤，使用JSON和Python作为中间格式。

Result: 在AMS-IO-Bench基准测试中，达到超过70%的DRC+LVS通过率，将设计周转时间从小时级缩短到分钟级，优于基线LLM。代理生成的I/O环在28nm CMOS流片中成功制造并验证。

Conclusion: 这是首个报道的人机协作AMS IC设计，其中基于LLM的代理完成了非平凡子任务，输出直接用于硅片制造，证明了该方法在实际AMS IC设计流程中的有效性。

Abstract: In this paper, we propose AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware input/output (I/O) subsystem generation in analog and mixed-signal (AMS) integrated circuits (ICs). The central contribution of this work is a framework that connects natural language design intent with industrial-level AMS IC design deliverables. AMS-IO-Agent integrates two key capabilities: (1) a structured domain knowledge base that captures reusable constraints and design conventions; (2) design intent structuring, which converts ambiguous user intent into verifiable logic steps using JSON and Python as intermediate formats. We further introduce AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. On this benchmark, AMS-IO-Agent achieves over 70\% DRC+LVS pass rate and reduces design turnaround time from hours to minutes, outperforming the baseline LLM. Furthermore, an agent-generated I/O ring was fabricated and validated in a 28 nm CMOS tape-out, demonstrating the practical effectiveness of the approach in real AMS IC design flows. To our knowledge, this is the first reported human-agent collaborative AMS IC design in which an LLM-based agent completes a nontrivial subtask with outputs directly used in silicon.

</details>


### [9] [Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design](https://arxiv.org/abs/2512.21623)
*Takahide Suzuki,Kazuki Nakanishi,Takashi Fujiwara,Hideyuki Shimizu*

Main category: cs.AI

TL;DR: OrchestRA是一个人类在环的多智能体平台，将生物学、化学和药理学统一为自主发现引擎，通过自主执行模拟和推理结果来驱动迭代优化，将药物发现从随机搜索转变为可编程的循证工程学科。


<details>
  <summary>Details</summary>
Motivation: 当前治疗发现面临专业领域碎片化和计算设计与生理验证之间的执行差距等挑战，现有生成式AI模型通常作为被动助手而非自主执行者，需要更自主的发现平台。

Method: OrchestRA采用人类在环的多智能体架构，包含：1) Orchestrator协调器；2) Biologist Agent利用大规模知识图谱进行深度推理识别靶点；3) Chemist Agent自主检测结构口袋进行从头设计或药物重定位；4) Pharmacologist Agent通过PBPK模拟评估候选药物。建立动态反馈循环，药代动力学和毒性特征直接触发结构重新优化。

Result: 平台建立了动态反馈循环，将药代动力学和毒性特征直接与结构重新优化相连接，实现了治疗设计的民主化，将药物发现从随机搜索转变为可编程的循证工程学科。

Conclusion: OrchestRA通过无缝集成自主执行与人类指导，实现了治疗设计的民主化，将药物发现从随机搜索转变为可编程的循证工程学科。

Abstract: Therapeutic discovery remains a formidable challenge, impeded by the fragmentation of specialized domains and the execution gap between computational design and physiological validation. Although generative AI offers promise, current models often function as passive assistants rather than as autonomous executors. Here, we introduce OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine. Unlike static code generators, our agents actively execute simulations and reason the results to drive iterative optimization. Governed by an Orchestrator, a Biologist Agent leverages deep reasoning over a massive knowledge graph (>10 million associations) to pinpoint high-confidence targets; a Chemist Agent autonomously detects structural pockets for de novo design or drug repositioning; and a Pharmacologist Agent evaluates candidates via rigorous physiologically based pharmacokinetic (PBPK) simulations. This architecture establishes a dynamic feedback loop where pharmacokinetic and toxicity profiles directly trigger structural reoptimization. By seamlessly integrating autonomous execution with human guidance, OrchestRA democratizes therapeutic design, transforming drug discovery from a stochastic search to a programmable evidence-based engineering discipline.

</details>


### [10] [Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning](https://arxiv.org/abs/2512.21699)
*Eranga Bandara,Tharaka Hewa,Ross Gore,Sachin Shetty,Ravi Mukkamala,Peter Foytik,Abdul Rahman,Safdar H. Bouk,Xueping Liang,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 提出一种基于多模型共识和推理层治理的负责任可解释AI代理架构，通过异构代理独立生成候选输出，再由专用推理代理进行结构化整合，确保安全性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理系统在自主性增强的同时，面临可解释性、问责制、鲁棒性和治理方面的关键挑战，现有实现往往强调功能和可扩展性，但缺乏理解决策原理和执行责任制的机制。

Method: 提出RAI和XAI代理架构，采用异构LLM和VLM代理从共享输入上下文独立生成候选输出，暴露不确定性和分歧，然后由专用推理代理进行结构化整合，强制执行安全和策略约束，减轻幻觉和偏见。

Result: 在多个实际AI代理工作流中评估该架构，证明共识驱动的推理提高了鲁棒性、透明度和跨不同应用领域的操作信任。

Conclusion: 该工作为设计既自主可扩展又负责任可解释的AI代理系统提供了实用指导，通过架构设计实现责任和可解释性。

Abstract: Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.

</details>


### [11] [Accelerating Scientific Discovery with Autonomous Goal-evolving Agents](https://arxiv.org/abs/2512.21782)
*Yuanqi Du,Botao Yu,Tianyu Liu,Tony Shen,Junwu Chen,Jan G. Rittig,Kunyang Sun,Yikun Zhang,Zhangde Song,Bo Zhou,Cassandra Masschelein,Yingze Wang,Haorui Wang,Haojun Jia,Chao Zhang,Hongyu Zhao,Martin Ester,Teresa Head-Gordon,Carla P. Gomes,Huan Sun,Chenru Duan,Philippe Schwaller,Wengong Jin*

Main category: cs.AI

TL;DR: 本文提出SAGA框架，通过自动化目标函数设计来改进科学发现代理，采用双层架构：外层LLM代理分析优化结果并提出新目标，内层执行当前目标下的解决方案优化。


<details>
  <summary>Details</summary>
Motivation: 当前科学发现代理主要依赖科学家指定的定量目标函数进行优化，但这些目标函数往往只是不完美的代理指标。对于科学领域的重大挑战，自动化目标函数设计是一个核心但尚未满足的需求。

Method: 提出科学自主目标演化代理(SAGA)，采用双层架构：外层LLM代理分析优化结果、提出新目标并将其转换为可计算评分函数；内层在当前目标下执行解决方案优化。这种设计能够系统探索目标空间及其权衡。

Result: 在抗生素设计、无机材料设计、功能性DNA序列设计和化学过程设计等多个应用领域展示，自动化目标制定能够显著提高科学发现代理的有效性。

Conclusion: 自动化目标函数设计是科学发现代理的关键改进方向，SAGA框架通过双层架构实现了目标空间的系统探索，为科学发现提供了更有效的自动化方法。

Abstract: There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey](https://arxiv.org/abs/2512.21347)
*Vítor Mateus de Brito,Kleinner Farias*

Main category: cs.SE

TL;DR: 对46名行业专业人士的实证调查显示，LLM在软件工程中受到积极评价，能加速技术问题解决、改善文档支持和代码标准化，但也存在认知依赖、安全风险和技术自主性侵蚀等担忧。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件工程中的深入应用，理解其实际使用情况变得至关重要。本研究旨在通过实证调查填补学术讨论与真实软件开发实践之间的差距。

Method: 对46名具有不同教育背景和经验水平的行业专业人士进行问卷调查，收集关于LLM在软件工程中采用情况的实证数据。

Result: 调查结果显示：1）对LLM持积极态度，特别是在加速技术问题解决、改善文档支持和增强源代码标准化方面；2）同时存在对认知依赖、安全风险和技术自主性侵蚀的担忧。

Conclusion: LLM在软件工程中具有显著价值，但需要批判性和监督性使用。研究为开发者和研究人员提供了实用见解，并呼吁未来研究关注LLM的认知、伦理和组织影响。

Abstract: The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.

</details>


### [13] [CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation](https://arxiv.org/abs/2512.21351)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore-Evo扩展了CosmoCore，通过进化算法增强代码生成任务的适应性和新颖性，将RL轨迹视为"基因组"进行变异和选择，在分布偏移环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成代理在遇到分布偏移环境（如API变更、新库）时适应性有限，需要突破训练模式的限制，产生新颖解决方案。受人类进化中自然选择和适应机制的启发，需要增强代理的适应性和新颖性。

Method: 基于CosmoCore的情感梦境回放RL框架，引入进化算法：将RL轨迹视为"基因组"，在夜间回放阶段进行变异和选择；增强梦境队列，包括高适应度轨迹的变异和企业调优的适应度函数（效率、合规性、可扩展性）。

Result: 在HumanEval变体、BigCodeBench和自定义PySpark管道模拟等扩展基准测试中，CosmoCore-Evo相比原始CosmoCore和PPO、REAMER等基线，解决方案新颖性提高35%，适应速度加快25%。消融实验证实进化组件在弥合LLM代理感知差距中的作用。

Conclusion: 进化算法能有效增强代码生成代理的适应性和新颖性，特别是在分布偏移环境中。CosmoCore-Evo框架为LLM代理提供了突破训练模式限制的新途径。

Abstract: Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.

</details>


### [14] [Multi-Agent LLM Committees for Autonomous Software Beta Testing](https://arxiv.org/abs/2512.21352)
*Sumanth Bharadwaj Hachalli Karanam,Dhiwahar Adhithya Kennady*

Main category: cs.SE

TL;DR: 提出多智能体委员会框架，通过视觉增强的LLM协作进行软件测试，相比单智能体基线显著提升任务成功率，支持实时持续集成测试。


<details>
  <summary>Details</summary>
Motivation: 传统软件beta测试成本高、耗时长，单智能体LLM方法存在幻觉和不一致行为问题，需要更可靠的自动化测试解决方案。

Method: 采用多智能体委员会框架，包含多样化视觉增强LLM，通过三轮投票协议达成共识，结合模型多样性、角色驱动行为变化和视觉界面理解，系统探索Web应用。

Result: 多智能体委员会在84次实验运行中达到89.5%总体任务成功率，2-4智能体配置达到91.7-100%成功率，相比单智能体基线提升13.7-22.0个百分点。在WebShop上达到74.7%成功率（GPT-3基线为50.1%），在OWASP安全测试中覆盖8/10漏洞类别，bug检测F1分数达0.91。

Conclusion: 多智能体委员会框架显著提升软件测试效果，支持实时持续集成测试，开源实现促进可重复研究和实际部署。

Abstract: Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.

</details>


### [15] [AInsteinBench: Benchmarking Coding Agents on Scientific Repositories](https://arxiv.org/abs/2512.21373)
*Titouan Duston,Shuo Xin,Yang Sun,Daoguang Zan,Aoyan Li,Shulin Xin,Kai Shen,Yixiao Chen,Qiming Sun,Ge Zhang,Jiashuo Liu,Huan Zhou,Jingkai Liu,Zhichen Pu,Yuanheng Wang,Bo-Xuan Ge,Xin Tong,Fei Ye,Zhi-Chao Zhao,Wen-Biao Han,Zhoujian Cao,Yueran Zhao,Weiluo Ren,Qingshen Long,Yuxiao Liu,Anni Huang,Yidi Du,Yuanyuan Rong,Jiahao Peng*

Main category: cs.SE

TL;DR: AInsteinBench是一个用于评估LLM代理在真实科研软件生态系统中作为科学计算开发代理能力的大规模基准测试，基于六个主流科学代码库的实际pull request任务。


<details>
  <summary>Details</summary>
Motivation: 现有科学推理基准侧重于概念知识，软件工程基准强调通用功能实现和问题解决，缺乏评估LLM在真实科研开发环境中端到端能力的方法。

Method: 从六个广泛使用的科学代码库（量子化学、量子计算、分子动力学、数值相对论、流体动力学、化学信息学）中提取维护者编写的pull request任务，通过多阶段筛选和专家评审确保科学挑战性、充分测试覆盖和适当难度。

Result: 创建了一个包含可执行环境评估、科学意义失败模式和测试驱动验证的基准测试，能够衡量模型超越表面代码生成、掌握计算科学研究核心能力的情况。

Conclusion: AInsteinBench填补了现有基准的空白，为评估LLM在真实科研软件开发环境中的能力提供了新的标准，推动模型从简单代码生成向科研开发核心能力发展。

Abstract: We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.

</details>


### [16] [What Makes a GitHub Issue Ready for Copilot?](https://arxiv.org/abs/2512.21426)
*Mohammed Sayagh*

Main category: cs.SE

TL;DR: 该研究构建了32项详细标准来评估GitHub问题的质量，以使其更适合AI代理处理，并开发了可解释的机器学习模型来预测问题能否成功生成合并的PR。


<details>
  <summary>Details</summary>
Motivation: AI代理（如Copilot）在编码任务中表现依赖于输入质量，但GitHub问题往往不够清晰或范围不明确，导致实现失败。现有最佳实践建议有限且过于高层，需要更详细的质量评估标准。

Method: 1. 构建32项详细标准来衡量GitHub问题质量；2. 比较导致合并PR与关闭PR的问题特征；3. 建立可解释的机器学习模型预测问题能否生成合并PR。

Result: 成功合并的PR通常来自：更简短、范围明确、提供清晰指导和相关工件提示、包含实现指导的问题。包含外部引用（配置、上下文设置、依赖或外部API）的问题合并率较低。模型的中位AUC达到72%。

Conclusion: 研究揭示了编写高质量GitHub问题的关键指标，强调了在AI协作时代将问题编写作为一等软件工程活动的重要性，并提供了可操作的工具帮助用户改进问题质量。

Abstract: AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.

</details>


### [17] [Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors](https://arxiv.org/abs/2512.21431)
*Hridya Dhulipala,Xiaokai Rong,Tien N. Nguyen*

Main category: cs.SE

TL;DR: Cerberus是一个无需实际执行的预测性覆盖引导测试框架，使用LLMs生成触发运行时错误的输入，并通过代码覆盖预测和错误检测来发现代码片段中的运行时异常。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，需要在无需实际执行的情况下检测代码片段中的运行时错误和异常，特别是在将在线代码片段集成到代码库之前进行安全检测。

Method: Cerberus采用两阶段反馈循环：第一阶段同时增加代码覆盖率和检测运行时错误；当覆盖率达到100%或最大值时，第二阶段专注于检测运行时错误。框架使用LLMs生成测试输入，并进行代码覆盖预测和错误检测，无需实际执行代码。

Result: 实证评估表明，Cerberus在（不）完整代码片段上比传统和基于学习的测试框架表现更好，能更高效地生成高覆盖率测试用例，发现更多运行时错误。

Conclusion: Cerberus通过LLMs驱动的预测性测试框架，实现了无需执行的代码覆盖引导测试，在检测运行时错误方面优于现有方法。

Abstract: In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.

</details>


### [18] [Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code](https://arxiv.org/abs/2512.21591)
*Shuo Sun,Shixin Zhang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的仓库级Python类型推断方法，通过类型与依赖关系的协同演化实现准确推断


<details>
  <summary>Details</summary>
Motivation: Python的动态类型机制虽然灵活，但容易导致运行时类型错误，现有类型推断工具主要处理孤立代码片段，难以应对仓库级复杂依赖关系

Method: 构建实体依赖图(EDG)建模仓库级类型依赖，通过迭代方式协同演化类型和依赖关系，采用类型检查器在环策略实时验证和修正推断结果

Result: 在12个复杂Python仓库上评估，TypeSim得分0.89，TypeExact得分0.84，分别比最强基线提升27%和40%，减少了92.7%的工具引入新类型错误

Conclusion: 该方法在自动化、可靠的Python类型标注方面取得了显著进展，为实际Python开发提供了有效的仓库级类型推断解决方案

Abstract: Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \methodName~significantly outperformed prior works, achieving a \textit{TypeSim} score of 0.89 and a \textit{TypeExact} score of 0.84, representing a 27\% and 40\% relative improvement over the strongest baseline. More importantly, \methodName~removed new type errors introduced by the tool by 92.7\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.

</details>


### [19] [How Do Agents Perform Code Optimization? An Empirical Study](https://arxiv.org/abs/2512.21757)
*Huiyun Peng,Antonio Zhong,Ricardo Andrés Calvo Méndez,Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: 首个比较AI代理与人类性能优化提交的实证研究，发现AI生成的PR更少包含显式性能验证，但优化模式与人类相似。


<details>
  <summary>Details</summary>
Motivation: 性能优化是软件开发中关键但具有挑战性的方面，需要深入理解系统行为、算法权衡和仔细的代码修改。尽管AI编码代理在代码生成和错误修复方面取得了进展，但对其在真实世界性能优化任务上的表现知之甚少。

Method: 对AIDev数据集中的324个AI生成和83个人类编写的性能优化PR进行实证研究，分析采纳率、可维护性、优化模式和验证实践。

Result: AI编写的性能PR包含显式性能验证的可能性显著低于人类编写的PR（45.7% vs. 63.6%，p=0.007）。AI编写的PR主要使用与人类相同的优化模式。

Conclusion: 讨论了AI代理代码优化的局限性和机会，为推进代理式代码优化提供了见解。

Abstract: Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\% vs. 63.6\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.

</details>


### [20] [Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development](https://arxiv.org/abs/2512.21818)
*Brian Bowers,Smita Khapre,Jugal Kalita*

Main category: cs.SE

TL;DR: 论文分析多智能体系统在软件工程中的安全漏洞，发现coder-reviewer-tester架构比coder和coder-tester架构更安全但效率较低，通过添加安全分析代理可提升效率和安全，但安全代理本身仍易受高级代码注入攻击。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI和多智能体系统即将在工业和社会中占据主导地位，这些系统从被动内容生成转向主动多任务能力。然而，由于其自主设计和缺乏人工干预，这些系统无法自行识别和应对攻击，存在严重安全风险。

Method: 提出用于软件工程实现阶段的多智能体系统架构，并建立全面的威胁模型。分析不同架构（coder、coder-tester、coder-reviewer-tester）的脆弱性，评估添加安全分析代理的效果，并演示高级代码注入攻击。

Result: coder-reviewer-tester架构比coder和coder-tester架构更具弹性但编码效率较低。添加安全分析代理可缓解效率损失并实现更好的弹性。然而，安全分析代理本身易受高级代码注入攻击，通过嵌入有毒少样本示例可将攻击成功率从0%提升至71.95%。

Conclusion: 多智能体系统虽然能准确生成代码，但存在严重安全漏洞，特别是代码注入攻击。即使添加安全分析代理，仍可能被高级攻击绕过。需要更强大的安全机制来保护这些自主系统。

Abstract: Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [21] [Hardening Atlas Against Prompt Injection](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FDEARHN/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/K8B4Pk92d75a9tIOCcO0iz2XoHrp1ahC7GWv9AxxERI=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI正在加强其AI浏览器Atlas对抗提示注入攻击的防护，但承认这仍然是一个未解决且持续的威胁


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在开放网络上的能力扩展，恶意指令嵌入网页内容操纵代理行为的提示注入攻击成为严重安全威胁，需要持续防护

Method: OpenAI正在改进缓解技术来保护Atlas AI浏览器，包括检测和防御嵌入在网页内容中的恶意指令

Result: 虽然防护技术有所改进，但提示注入攻击仍然是一个未解决且持续存在的安全威胁

Conclusion: 提示注入攻击是AI代理安全领域的一个持久挑战，需要持续的研究和防护措施，特别是在代理能力不断扩展的背景下

Abstract: Hardening Atlas Against Prompt Injection (13 minute read) This post details OpenAI's ongoing efforts to secure its AI browser, Atlas, against prompt injection attacks - malicious instructions embedded in web content that manipulate agent behavior. While mitigation techniques are improving, the company acknowledged prompt injection remains an unsolved and persistent threat, particularly as agent capabilities expand on the open web.

</details>


### [22] [Async Coding Agents "From Scratch"](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenanderson.work%2Fblog%2Fasync-coding-agents%2F%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/XtcMOOL8sJtfu00KhA0dFtGqPE7SmxhiHjZBTHVgs2o=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文指出构建异步编码代理相对容易，企业仅凭云端沙箱代理连接Slack已无法形成差异化优势，因此需要训练自有SWE代理和辅助模型来改进系统。


<details>
  <summary>Details</summary>
Motivation: 当前市场上编码代理产品同质化严重，许多公司仅提供云端沙箱代理连接Slack的服务，这种模式容易被复制，缺乏技术壁垒。企业需要寻找真正的差异化竞争优势。

Method: 文章未详细描述具体技术方法，但暗示企业应通过训练自有SWE（软件工程）代理和辅助模型来改进其代理系统，建立技术护城河。

Result: 分析表明，简单的异步编码代理构建相对容易，导致市场竞争加剧。企业意识到仅靠基础功能无法维持竞争优势，必须投资于更先进的代理训练。

Conclusion: 编码代理市场正在从简单的云端服务向更复杂的技术栈演进，企业需要通过训练自有模型和构建更强大的代理系统来建立真正的技术优势。

Abstract: Async Coding Agents "From Scratch" (10 minute read) It's pretty easy to homebrew your own asynchronous coding agent. This means that businesses selling coding agents can no longer differentiate themselves by only running sandboxed agents in the cloud that connect to Slack. Companies working on coding agents likely realize this and are doing everything they can to train their own SWE agents and auxiliary models to improve their harnesses.

</details>


### [23] [We removed 80% of our agent's tools](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fwe-removed-80-percent-of-our-agents-tools%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/kvyXXX_eLqnRDy3IYp086IvgDXO8bjA8lFxqPupGun0=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Vercel团队发现，过度复杂的文本转SQL代理（包含专门工具、复杂提示工程和上下文管理）效果不佳，而简化为仅执行任意bash命令的单一工具后，成功率从80%提升至100%


<details>
  <summary>Details</summary>
Motivation: 解决传统文本转SQL代理的脆弱性、速度慢和维护成本高的问题，探索简化代理架构是否能提高性能和可靠性

Method: 将原本包含专门工具、复杂提示工程和上下文管理的文本转SQL代理简化为仅使用单一工具（执行任意bash命令）的极简架构

Result: 简化后的代理成功率从80%提升至100%，同时变得更简单、更可靠

Conclusion: 在某些情况下，极简的代理架构（如单一bash命令执行工具）可能比复杂的专门化工具系统更有效，简化设计可以同时提高成功率和降低维护成本

Abstract: We removed 80% of our agent's tools (4 minute read) Vercel spent months building a sophisticated internal text-to-SQL agent with specialized tools, heavy prompt engineering, and careful context management. It kind of worked, but it was fragile, slow, and required constant maintenance. The team then deleted most of it and stripped the agent down to a single tool that executed arbitrary bash commands. Its agent got simpler and better at the same time: it had a 100% success rate instead of 80%.

</details>


### [24] [LLM code quality leaderboard: How does your preferred model score?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fthe-coding-personalities-of-leading-llms%2Fleaderboard%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-llm-leaderboard25%26utm_content=newsletter-tldrai-secondary-llmleaderboard-251223-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/Ro1C0MOXjNYCWDArM6YiKWKMLjLi_HEVvzwJTLzYH10=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sonar发布LLM代码质量排行榜，评估GPT-5.2 High和Gemini 3.0 Pro等模型在代码生成中的结构质量、安全性和可靠性表现


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码生成中广泛应用，需要系统评估不同模型生成的代码质量，包括结构质量、安全性和可维护性等方面，为开发者选择合适模型提供参考

Method: 创建LLM代码质量排行榜，通过系统化评估框架测试GPT-5.2 High、Gemini 3.0 Pro等主流模型，分析代码的结构质量、安全性和可靠性指标

Result: 研究揭示了不同LLM在代码生成中的权衡关系，某些模型在结构质量上表现优异，而其他模型在安全性方面更强，提供了模型选择的量化参考

Conclusion: Sonar的LLM排行榜成为理解AI生成代码真实质量的权威资源，帮助开发者根据具体需求选择最合适的代码生成模型

Abstract: LLM code quality leaderboard: How does your preferred model score? (Sponsor) Interested to learn how different LLMs perform for coding? New research on models like GPT-5.2 High and Gemini 3.0 Pro reveals trade-offs in structural quality and security. Learn more about the reliability, security, and maintainability of code written by the latest models with Sonar's LLM Leaderboard—the definitive resource to understand the true quality of AI-generated code.

</details>


### [25] [Agent Skills for Context Engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmuratcankoylan%2FAgent-Skills-for-Context-Engineering%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/-PsvoncJ4CUnsc34hIrQCCwr8qKyq-TsZp5lXQVMq84=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub仓库提供用于构建生产级AI代理系统的Agent Skills集合，分为基础技能、架构技能和运维技能三类，支持任何允许自定义指令的代理平台


<details>
  <summary>Details</summary>
Motivation: 为AI代理系统开发提供标准化的技能集合，解决生产环境中代理系统构建的复杂性和一致性需求，促进高效上下文使用

Method: 创建结构化的技能分类体系：基础技能、架构技能、运维技能，每个技能都针对高效上下文使用进行设计，支持跨平台兼容

Result: 提供了全面的Agent Skills集合，支持生产级AI代理系统构建，可在任何支持技能或自定义指令的代理平台上使用

Conclusion: 该技能集合为构建生产级AI代理系统提供了标准化、可复用的组件，有助于提高开发效率和系统质量

Abstract: Agent Skills for Context Engineering (GitHub Repo) This repository contains a comprehensive collection of Agent Skills for building production-grade AI agent systems. They are categorized into Foundational skills, Architectural skills, and Operational skills. Each skill is structured for efficient context use. The patterns work on any agent platform that supports skills or allows custom instructions.

</details>


### [26] [Cursor Expands Agent Hooks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fhooks-partners%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/qvOILFxEu0TLJ2kCuFEzSS5dtrX7Y2kYl8G782cWWZk=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor宣布与安全和平台供应商合作，扩展其代理钩子系统，使组织能够观察、修改或阻止代理循环的各个阶段，支持治理、依赖扫描、密钥管理和代理安全等用例。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在软件开发中的广泛应用，需要更好的控制和治理机制来确保代理的安全性、合规性和可靠性。组织需要能够监控和干预代理行为的能力，以应对安全风险、管理依赖、保护敏感信息。

Method: 通过建立合作伙伴关系，将Cursor的代理钩子系统与安全和平台供应商集成。这些钩子允许在代理循环的各个阶段插入观察、修改或阻止功能，提供细粒度的控制机制。

Result: 成功扩展了代理钩子系统，支持多种实际用例，包括治理、依赖扫描、密钥管理和代理安全。通过集成合作伙伴的解决方案，为组织提供了更强大的代理控制和监控能力。

Conclusion: 代理钩子系统为AI代理的安全部署提供了重要基础设施，通过可扩展的集成框架，使组织能够在保持开发效率的同时确保代理行为的安全性和合规性。

Abstract: Cursor Expands Agent Hooks (3 minute read) Cursor has announced partnerships to integrate its agent hook system with security and platform vendors. These hooks allow organizations to observe, modify, or block stages of the agent loop, supporting use cases like governance, dependency scanning, secrets management, and agent safety.

</details>


### [27] [Announcing advanced governance capabilities for Vertex AI Agent Builder](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fai-machine-learning%2Fnew-enhanced-tool-governance-in-vertex-ai-agent-builder%2F%3Futm_source=tldrai/1/0100019b4b924458-2d845cd0-5afb-4732-ab94-fb15a2c8e6ca-000000/sh6-lRCLDVaX3Fdj9FehWIwWNPr02X1PZC-caUto38U=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google为Vertex AI Agent Builder推出高级治理功能，增强Agent Engine以管理短期和长期记忆


<details>
  <summary>Details</summary>
Motivation: 提升AI代理的治理能力，使代理能够更好地管理和利用记忆信息，提高代理的智能水平和应用效果

Method: 在Vertex AI Agent Builder中集成高级治理功能，扩展Agent Engine以支持短期和长期记忆管理

Result: 增强了AI代理的记忆管理能力，使代理能够更有效地处理上下文信息并维持长期状态

Conclusion: 这些新功能将提升AI代理的智能水平和实用性，为开发者提供更强大的代理构建工具

Abstract: Announcing advanced governance capabilities for Vertex AI Agent Builder (5 minute read) Google announced advanced governance features for Vertex AI Agent Builder, enhancing its Agent Engine to manage both short-term and long-term memory.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [A Reinforcement Learning Approach to Synthetic Data Generation](https://arxiv.org/abs/2512.21395)
*Natalia Espinosa-Dice,Nicholas J. Jackson,Chao Yan,Aaron Lee,Bradley A. Malin*

Main category: cs.LG

TL;DR: RLSyn：将合成数据生成重构为强化学习问题，在小样本生物医学数据场景中表现优于传统生成模型


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法（如GAN、扩散模型）需要大量数据和复杂训练，限制了在小样本生物医学研究中的应用。需要开发更数据高效的方法来促进医疗数据共享同时保护患者隐私。

Method: 将合成数据生成重构为强化学习问题，提出RLSyn框架：将数据生成器建模为患者记录上的随机策略，使用近端策略优化（PPO）和判别器生成的奖励进行优化，实现更稳定和数据高效的训练。

Result: 在两个生物医学数据集（AI-READI和MIMIC-IV）上评估，RLSyn在隐私性、实用性和保真度方面表现优异。在较小的AI-READI数据集上优于扩散模型和GANs，在MIMIC-IV上与扩散模型相当并优于GANs。

Conclusion: 强化学习为合成生物医学数据生成提供了原则性和有效的替代方案，特别适用于数据稀缺场景，有助于促进医疗数据共享同时保护患者隐私。

Abstract: Synthetic data generation (SDG) is a promising approach for enabling data sharing in biomedical studies while preserving patient privacy. Yet, state-of-the-art generative models often require large datasets and complex training procedures, limiting their applicability in small-sample settings. In this work, we reframe SDG as a reinforcement learning (RL) problem and introduce RLSyn, a novel framework that models the data generator as a stochastic policy over patient records and optimizes it using Proximal Policy Optimization with discriminator-derived rewards, yielding more stable and data-efficient training. We evaluate RLSyn on two biomedical datasets - AI-READI and MIMIC-IV- and benchmark it against state-of-the-art generative adversarial networks (GANs) and diffusion-based methods across extensive privacy, utility, and fidelity evaluations. RL-Syn performs comparably to diffusion models and outperforms GANs on MIMIC-IV, while outperforming both diffusion models and GANs on the smaller AI-READI dataset. These results demonstrate that reinforcement learning provides a principled and effective alternative for synthetic biomedical data generation, particularly in data-scarce regimes.

</details>


### [29] [dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.21446)
*Shirui Chen,Jiantao Jiao,Lillian J. Ratliff,Banghua Zhu*

Main category: cs.LG

TL;DR: dUltra是一个基于强化学习的框架，通过优化解掩码策略实现扩散语言模型的高效并行解码，在数学推理和代码生成任务中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散语言模型（MDLMs）即使采用复杂采样策略，每次前向传播也只能解码不到5个token，采样速度与自回归模型+推测解码方案相当，限制了其优势。现有的基于蒸馏的加速方法存在离策略问题，且性能受限于基础模型的样本质量。

Method: 提出dUltra框架，基于组相对策略优化（GRPO）进行在线强化学习，学习高效的并行解码解掩码策略。引入解掩码规划头，在独立伯努利分布下预测每个token的解掩码概率。联合优化基础扩散LLM和解掩码顺序规划器，使用可验证奖励、蒸馏奖励和解掩码步骤数作为奖励信号。

Result: 在数学推理和代码生成任务中，dUltra在准确率-效率权衡方面超越了最先进的启发式和蒸馏基线方法，朝着实现扩散模型对自回归模型的"扩散优势"迈进。

Conclusion: dUltra通过强化学习优化解掩码策略，显著提升了掩码扩散语言模型的并行解码效率，在保持准确性的同时减少了推理步骤，为实现扩散模型的实际优势提供了有效途径。

Abstract: Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy'' over autoregressive models.

</details>


### [30] [RLLaVA: An RL-central Framework for Language and Vision Assistants](https://arxiv.org/abs/2512.21450)
*Lei Zhao,Zihao Ma,Boyu Lin,Yuhe Liu,Wenjun Wu,Lei Huang*

Main category: cs.LG

TL;DR: RLLaVA是一个将强化学习算法逻辑与模型架构和分布式执行解耦的框架，支持研究人员用最少代码实现新RL算法，并兼容多种RL方法和视觉语言模型，能在单张24GB GPU上高效训练1B-7B模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态助手训练中，强化学习框架通常与特定模型架构和训练引擎紧密耦合，限制了研究灵活性和资源效率。需要一种解耦的框架，使研究人员能轻松实现新RL算法，同时支持多种视觉语言模型，并在有限硬件资源上高效训练。

Method: 提出基于马尔可夫决策过程(MDP)的RL-central框架RLLaVA，将RL算法逻辑与模型架构、分布式执行解耦。框架支持多种RL方法和视觉语言模型，保持对特定训练和推理引擎的不可知性，实现资源高效训练。

Result: RLLaVA能在单张24GB GPU上端到端训练4B规模模型，支持1B-7B模型训练。在多模态和智能体任务实验中，使用RLLaVA训练的模型性能持续超越基础模型，与其他专门设计的RL框架竞争力相当。

Conclusion: RLLaVA框架通过解耦设计实现了RL算法的灵活实现和资源高效训练，为多模态助手研究提供了可扩展且实用的工具，代码已开源。

Abstract: We present an RL-central framework for Language and Vision Assistants (RLLaVA) with its formulation of Markov decision process (MDP). RLLaVA decouples RL algorithmic logic from model architecture and distributed execution, supporting researchers in implementing new RL algorithms with minimal code, and to plug in a broad family of RL methods and vision-language models (VLMs) while remaining agnostic to specific training and inference engines. RLLaVA makes resource-efficient training of 1B--7B models feasible on common GPUs; notably, 4B-scale models can be trained end-to-end with full-parameter updates on a single 24GB GPU. Experiments on multi-modal and agentic tasks demonstrate that RLLaVA has task extensibility, and the models trained with it consistently improve performance over base models, competitive with other specially engineered RL frameworks. The code is available at https://github.com/TinyLoopX/RLLaVA.

</details>


### [31] [Generative Actor Critic](https://arxiv.org/abs/2512.21527)
*Aoyang Qin,Deqian Kong,Wei Wang,Ying Nian Wu,Song-Chun Zhu,Sirui Xie*

Main category: cs.LG

TL;DR: GAC是一种新颖的强化学习框架，将策略评估重构为学习轨迹与回报的联合分布生成模型，将策略改进重构为对该模型的推理，显著提升了离线到在线学习的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法主要关注估计或最大化期望回报，在利用在线经验微调离线预训练模型时面临挑战。需要一种能够有效结合离线预训练和在线学习的框架。

Method: 提出生成式行动者-评论家（GAC）框架：1）策略评估：学习轨迹τ和回报y的联合分布p(τ,y)；2）策略改进：对学习到的生成模型进行推理。具体实现基于连续潜在计划向量的潜变量模型，开发了两种推理策略：利用（优化潜在计划以最大化期望回报）和探索（基于动态调整的目标回报采样潜在计划）。

Result: 在Gym-MuJoCo和Maze2D基准测试中，GAC展现出强大的离线性能，并且离线到在线的改进显著优于现有最先进方法，即使在缺乏逐步奖励的情况下也能有效工作。

Conclusion: GAC通过将序列决策解耦为生成建模和推理，提供了一种灵活且有效的强化学习框架，特别适合离线到在线的学习场景，能够同时支持利用和探索策略。

Abstract: Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing \textit{policy evaluation} as learning a generative model of the joint distribution over trajectories and returns, $p(τ, y)$, and \textit{policy improvement} as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both \textit{exploitation}, by optimizing latent plans to maximize expected returns, and \textit{exploration}, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC's strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.

</details>


### [32] [Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations](https://arxiv.org/abs/2512.21586)
*Xin Liu,Haoran Li,Dongbin Zhao*

Main category: cs.LG

TL;DR: BCV-LR：通过潜在表示从视频进行行为克隆的无监督框架，仅需少量交互即可实现样本高效的视觉策略学习


<details>
  <summary>Details</summary>
Motivation: 人类能够从少量视频中高效提取知识和学习技能，但自主智能体难以复制这一过程，因为视觉输入复杂、缺乏动作或奖励信号、交互步骤有限

Method: 通过自监督任务从高维视频输入中提取动作相关潜在特征，利用基于动态的无监督目标预测连续帧间的潜在动作，在线微调并对齐到真实动作空间进行策略行为克隆

Result: 在24/28个任务中超越最先进的ILV基线和强化学习方法（提供环境奖励），在某些任务上达到专家级性能，仅需少量交互

Conclusion: 首次证明视频能够支持极其样本高效的视觉策略学习，无需任何其他专家监督

Abstract: Humans can efficiently extract knowledge and learn skills from the videos within only a few trials and errors. However, it poses a big challenge to replicate this learning process for autonomous agents, due to the complexity of visual input, the absence of action or reward signals, and the limitations of interaction steps. In this paper, we propose a novel, unsupervised, and sample-efficient framework to achieve imitation learning from videos (ILV), named Behavior Cloning from Videos via Latent Representations (BCV-LR). BCV-LR extracts action-related latent features from high-dimensional video inputs through self-supervised tasks, and then leverages a dynamics-based unsupervised objective to predict latent actions between consecutive frames. The pre-trained latent actions are fine-tuned and efficiently aligned to the real action space online (with collected interactions) for policy behavior cloning. The cloned policy in turn enriches the agent experience for further latent action finetuning, resulting in an iterative policy improvement that is highly sample-efficient.
  We conduct extensive experiments on a set of challenging visual tasks, including both discrete control and continuous control. BCV-LR enables effective (even expert-level on some tasks) policy performance with only a few interactions, surpassing state-of-the-art ILV baselines and reinforcement learning methods (provided with environmental rewards) in terms of sample efficiency across 24/28 tasks. To the best of our knowledge, this work for the first time demonstrates that videos can support extremely sample-efficient visual policy learning, without the need to access any other expert supervision.

</details>


### [33] [Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search](https://arxiv.org/abs/2512.21648)
*Maximilian Weichart*

Main category: cs.LG

TL;DR: 提出Inverse-RPO方法，系统性地从任何无先验UCB推导出带先验的UCT策略，应用于UCB-V得到两个新的方差感知树策略，在多个基准测试中优于PUCT且不增加计算成本。


<details>
  <summary>Details</summary>
Motivation: MCTS在强化学习中结合规划与学习，AlphaZero的成功部分归功于PUCT中引入先验项。虽然存在比UCB1理论保证更强的替代UCB，但将其扩展到带先验的UCT一直很困难，因为PUCT是经验性而非原理性推导的。

Method: 基于MCTS作为正则化策略优化问题的视角，提出Inverse-RPO通用方法，系统地从任何无先验UCB推导出带先验的UCT。将此方法应用于方差感知的UCB-V，得到两个新的带先验树策略，并扩展mctx库支持方差感知UCT。

Result: 实验表明，这些方差感知的带先验UCT在多个基准测试中优于PUCT，且不增加额外计算成本。代码修改量小，便于进一步研究。

Conclusion: Inverse-RPO为系统推导带先验UCT提供了通用框架，方差感知UCT在性能上超越PUCT，为MCTS搜索策略设计提供了新的理论指导。

Abstract: Monte Carlo Tree Search (MCTS) has profoundly influenced reinforcement learning (RL) by integrating planning and learning in tasks requiring long-horizon reasoning, exemplified by the AlphaZero family of algorithms. Central to MCTS is the search strategy, governed by a tree policy based on an upper confidence bound (UCB) applied to trees (UCT). A key factor in the success of AlphaZero is the introduction of a prior term in the UCB1-based tree policy PUCT, which improves exploration efficiency and thus accelerates training. While many alternative UCBs with stronger theoretical guarantees than UCB1 exist, extending them to prior-based UCTs has been challenging, since PUCT was derived empirically rather than from first principles. Recent work retrospectively justified PUCT by framing MCTS as a regularized policy optimization (RPO) problem. Building on this perspective, we introduce Inverse-RPO, a general methodology that systematically derives prior-based UCTs from any prior-free UCB. Applying this method to the variance-aware UCB-V, we obtain two new prior-based tree policies that incorporate variance estimates into the search. Experiments indicate that these variance-aware prior-based UCTs outperform PUCT across multiple benchmarks without incurring additional computational cost. We also provide an extension of the mctx library supporting variance-aware UCTs, showing that the required code changes are minimal and intended to facilitate further research on principled prior-based UCTs. Code: github.com/Max-We/inverse-rpo.

</details>


### [34] [An Information Theoretic Perspective on Agentic System Design](https://arxiv.org/abs/2512.21720)
*Shizhe He,Avanika Narayan,Ishan S. Khare,Scott W. Linderman,Christopher Ré,Dan Biderman*

Main category: cs.LG

TL;DR: 该论文提出了一种信息论框架来分析和优化基于压缩器-预测器架构的智能语言模型系统，通过互信息量化压缩质量，发现扩大压缩器规模比扩大预测器规模更有效。


<details>
  <summary>Details</summary>
Motivation: 当前基于压缩器-预测器架构的智能语言模型系统设计缺乏理论指导，压缩器和预测器的选择对下游性能的影响不明确，需要昂贵的任务特定配对扫描来评估效果。

Method: 将压缩器LM视为噪声信道，引入简单的互信息估计器来量化上下文与其压缩之间的信息量，提出任务无关的压缩质量评估方法。在五个数据集和三个模型家族上进行全面的实证分析。

Result: 互信息能强预测下游性能，与具体任务无关。更大的压缩器不仅更准确，而且更token高效，每个token传递更多比特信息。扩大压缩器规模比扩大预测器规模更有效，使本地压缩器能与更小的云端预测器配对。

Conclusion: 信息论框架为智能系统设计提供了理论基础，通过互信息量化压缩质量可实现任务无关的系统优化。扩大压缩器规模是提高性能的关键策略，能在保持准确性的同时显著降低API成本。

Abstract: Agentic language model (LM) systems power modern applications like "Deep Research" and "Claude Code," and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller "compressor" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger "predictor" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is $1.6\times$ more accurate, $4.6\times$ more concise, and conveys $5.5\times$ more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover $99\%$ of frontier-LM accuracy at $26\%$ of API costs.

</details>


### [35] [A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting](https://arxiv.org/abs/2512.22101)
*Shuyu Gan,Renxiang Wang,James Mooney,Dongyeop Kang*

Main category: cs.LG

TL;DR: A2P-Vis是一个两阶段多智能体系统，能够将原始数据集自动转换为高质量的数据可视化报告，解决了当前AI代理在生成多样化视觉证据和构建连贯专业报告方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前自动化端到端数据科学流程的AI代理存在两个主要问题：难以生成有洞察力且多样化的视觉证据，以及无法将这些证据组织成连贯专业的报告。这限制了自动化数据分析在实际应用中的实用性。

Method: 采用两阶段多智能体流水线：1) 数据分析器：执行数据剖析、提出多样化可视化方向、生成和执行绘图代码、通过可读性检查器过滤低质量图表、自动生成并评分候选洞察（基于深度、正确性、特异性、深度和可操作性）；2) 报告生成器：组织主题顺序、基于排名靠前的洞察编写图表支撑的叙述、撰写合理的过渡段落、修订文档以确保清晰度和一致性。

Result: 系统能够将原始数据转换为经过筛选的材料（图表+验证过的洞察）和可读的叙述，无需人工干预，生成符合出版标准的连贯报告。

Conclusion: 通过将质量保证的数据分析器与叙述性报告生成器相结合，A2P-Vis实现了端到端的协同分析操作化，提高了自动化数据分析在实际应用中的实用性。

Abstract: Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.

</details>


### [36] [A Comedy of Estimators: On KL Regularization in RL Training of LLMs](https://arxiv.org/abs/2512.21852)
*Vedant Shah,Johan Obando-Ceron,Vineet Jain,Brian Bartoldson,Bhavya Kailkhura,Sarthak Mittal,Glen Berseth,Pablo Samuel Castro,Yoshua Bengio,Nikolay Malkin,Moksh Jain,Siddarth Venkatraman,Aaron Courville*

Main category: cs.LG

TL;DR: 本文系统分析了LLM强化学习中KL散度估计器的配置对训练稳定性和性能的影响，发现无偏梯度估计器能带来更好的域内和域外任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习训练中广泛使用KL散度正则化，但实践中各种KL估计器的配置方式缺乏系统研究，且现有方法常导致目标函数与实现之间的不一致，影响训练效果。

Method: 分析不同KL估计器配置的梯度特性，通过RL微调Qwen2.5-7B、Llama-3.1-8B-Instruct和Qwen3-4B-Instruct-2507等模型，在域内和域外任务上评估不同配置的性能。

Result: 在在线策略设置中，有偏梯度估计器会导致训练不稳定，而无偏梯度估计器在域内和域外任务上都表现更好；在离线策略设置中，KL正则化有助于稳定异步设置带来的训练不稳定性。

Conclusion: KL估计器的配置选择对LLM强化学习训练至关重要，使用无偏梯度估计器能提高训练稳定性和模型性能，特别是在域外任务上的泛化能力。

Abstract: The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.

</details>
