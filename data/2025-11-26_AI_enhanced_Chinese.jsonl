{"id": "2511.19648", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19648", "abs": "https://arxiv.org/abs/2511.19648", "authors": ["Manil Shrestha", "Edward Kim"], "title": "Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search", "comment": null, "summary": "Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6df7\u5408\u7b97\u6cd5\u6765\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u6548\u7387\u548c\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\uff1aLLM\u5f15\u5bfc\u89c4\u5212\u4f7f\u7528\u5355\u6b21LLM\u8c03\u7528\u9884\u6d4b\u5173\u7cfb\u5e8f\u5217\uff0c\u5d4c\u5165\u5f15\u5bfc\u795e\u7ecf\u641c\u7d22\u5b8c\u5168\u6d88\u9664LLM\u8c03\u7528\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u8bc4\u5206\u5668\u878d\u5408\u6587\u672c\u548c\u56fe\u5d4c\u5165\u3002", "motivation": "\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u51cf\u5c11\u6602\u8d35\u7684LLM\u63a8\u7406\u6210\u672c\uff0c\u786e\u4fdd\u7b54\u6848\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u4e2d\u7684\u53ef\u9a8c\u8bc1\u6027\u3002", "method": "1) LLM\u5f15\u5bfc\u89c4\u5212\uff1a\u5355\u6b21LLM\u8c03\u7528\u9884\u6d4b\u5173\u7cfb\u5e8f\u5217\uff0c\u901a\u8fc7\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u6267\u884c\uff1b2) \u5d4c\u5165\u5f15\u5bfc\u795e\u7ecf\u641c\u7d22\uff1a\u4f7f\u75286.7M\u53c2\u6570\u8fb9\u7f18\u8bc4\u5206\u5668\u878d\u5408\u6587\u672c\u548c\u56fe\u5d4c\u5165\uff0c\u5b8c\u5168\u6d88\u9664LLM\u8c03\u7528\uff1b\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u89c4\u5212\u80fd\u529b\u538b\u7f29\u52304B\u53c2\u6570\u6a21\u578b\u4e2d\u3002", "result": "\u5728MetaQA\u4e0a\u8bc4\u4f30\uff0cLLM\u5f15\u5bfc\u89c4\u5212\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff08micro-F1 > 0.90\uff09\uff0c\u5d4c\u5165\u5f15\u5bfc\u795e\u7ecf\u641c\u7d22\u5b9e\u73b0100\u500d\u4ee5\u4e0a\u52a0\u901f\u4e14\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u7387\u3002\u7ed3\u6784\u5316\u89c4\u5212\u6bd4\u76f4\u63a5\u7b54\u6848\u751f\u6210\u66f4\u5177\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u53ef\u9a8c\u8bc1\u7684\u591a\u8df3\u63a8\u7406\u4e0d\u9700\u8981\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u800c\u662f\u9700\u8981\u7ed3\u5408\u7b26\u53f7\u7ed3\u6784\u548c\u5b66\u4e60\u8868\u5f81\u7684\u6b63\u786e\u67b6\u6784\u5f52\u7eb3\u504f\u7f6e\u3002", "topic": "agent analysis"}}
{"id": "2511.19719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19719", "abs": "https://arxiv.org/abs/2511.19719", "authors": ["Mobina Mehrazar", "Mohammad Amin Yousefi", "Parisa Abolfath Beygi", "Behnam Bahrak"], "title": "Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian", "comment": null, "summary": "Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.", "AI": {"tldr": "\u8bc4\u4f30LLM\u5728\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u7c7b\u4e2d\u751f\u6210\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u89e3\u91ca\u4e0e\u4eba\u7c7b\u6807\u6ce8\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63d0\u793a\u7b56\u7565\u5bf9\u5fe0\u5b9e\u6027\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u6ce2\u65af\u8bed\uff09\u4e2d\uff0cLLM\u751f\u6210\u81ea\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u5b58\u5728\u62c5\u5fe7\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u89e3\u91ca\u662f\u5426\u53cd\u6620\u771f\u5b9e\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83LLM\u8bc6\u522b\u7684\u5f71\u54cd\u8bcd\u4e0e\u4eba\u7c7b\u6807\u6ce8\uff0c\u4f7f\u7528\u57fa\u4e8etoken\u7ea7\u5bf9\u6570\u6982\u7387\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u8bc4\u4f30\u5fe0\u5b9e\u6027\uff0c\u6d4b\u8bd5\u4e24\u79cd\u63d0\u793a\u7b56\u7565\uff08\u5148\u9884\u6d4b\u540e\u89e3\u91ca vs \u5148\u89e3\u91ca\u540e\u9884\u6d4b\uff09\u3002", "result": "LLM\u5206\u7c7b\u6027\u80fd\u5f3a\u4f46\u751f\u6210\u89e3\u91ca\u4e0d\u5fe0\u5b9e\uff0c\u6a21\u578b\u89e3\u91ca\u95f4\u4e00\u81f4\u6027\u9ad8\u4e8e\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u4e00\u81f4\u6027\uff0c\u63d0\u793a\u7b56\u7565\u5bf9\u5fe0\u5b9e\u6027\u5f71\u54cd\u4e0d\u5927\u3002", "conclusion": "\u5f53\u524d\u89e3\u91ca\u65b9\u6cd5\u548c\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u786e\u4fddLLM\u5728\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.19739", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19739", "abs": "https://arxiv.org/abs/2511.19739", "authors": ["Richard J. Young", "Alice M. Matthews"], "title": "Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation", "comment": "25 pages, 13 figures, 5 tables", "summary": "Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "2511.19477", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19477", "abs": "https://arxiv.org/abs/2511.19477", "authors": ["Aram Vardanyan"], "title": "Building Browser Agents: Architecture, Security, and Practical Solutions", "comment": "30 pages, 22 figures. Production architecture and benchmark evaluation of browser agents", "summary": "Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings from building and operating a production browser agent. The analysis examines where current approaches fail and what prevents safe autonomous operation. The fundamental insight: model capability does not limit agent performance; architectural decisions determine success or failure. Security analysis of real-world incidents reveals prompt injection attacks make general-purpose autonomous operation fundamentally unsafe. The paper argues against developing general browsing intelligence in favor of specialized tools with programmatic constraints, where safety boundaries are enforced through code instead of large language model (LLM) reasoning. Through hybrid context management combining accessibility tree snapshots with selective vision, comprehensive browser tooling matching human interaction capabilities, and intelligent prompt engineering, the agent achieved approximately 85% success rate on the WebGames benchmark across 53 diverse challenges (compared to approximately 50% reported for prior browser agents and 95.7% human baseline).", "AI": {"tldr": "\u751f\u4ea7\u7ea7\u6d4f\u89c8\u5668\u4ee3\u7406\u9762\u4e34\u53ef\u9760\u6027\u548c\u5b89\u5168\u6311\u6218\uff0c\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u4e0d\u662f\u9650\u5236\u56e0\u7d20\uff0c\u67b6\u6784\u51b3\u7b56\u51b3\u5b9a\u6210\u8d25\u3002\u5b89\u5168\u5206\u6790\u663e\u793a\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4f7f\u901a\u7528\u81ea\u4e3b\u64cd\u4f5c\u4e0d\u5b89\u5168\uff0c\u5efa\u8bae\u5f00\u53d1\u5177\u6709\u7a0b\u5e8f\u7ea6\u675f\u7684\u4e13\u7528\u5de5\u5177\u800c\u975e\u901a\u7528\u6d4f\u89c8\u667a\u80fd\u3002", "motivation": "\u89e3\u51b3\u751f\u4ea7\u73af\u5883\u4e2d\u6d4f\u89c8\u5668\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u95ee\u9898\uff0c\u5206\u6790\u5f53\u524d\u65b9\u6cd5\u5931\u8d25\u7684\u539f\u56e0\u548c\u963b\u788d\u5b89\u5168\u81ea\u4e3b\u64cd\u4f5c\u7684\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e0a\u4e0b\u6587\u7ba1\u7406\uff08\u7ed3\u5408\u53ef\u8bbf\u95ee\u6027\u6811\u5feb\u7167\u548c\u9009\u62e9\u6027\u89c6\u89c9\uff09\u3001\u5168\u9762\u7684\u6d4f\u89c8\u5668\u5de5\u5177\u96c6\uff08\u5339\u914d\u4eba\u7c7b\u4ea4\u4e92\u80fd\u529b\uff09\u548c\u667a\u80fd\u63d0\u793a\u5de5\u7a0b\u3002", "result": "\u5728WebGames\u57fa\u51c6\u6d4b\u8bd5\u768453\u4e2a\u591a\u6837\u5316\u6311\u6218\u4e2d\u8fbe\u5230\u7ea685%\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u4e4b\u524d\u6d4f\u89c8\u5668\u4ee3\u7406\u7684\u7ea650%\u548c\u4eba\u7c7b\u57fa\u7ebf\u768495.7%\u3002", "conclusion": "\u53cd\u5bf9\u5f00\u53d1\u901a\u7528\u6d4f\u89c8\u667a\u80fd\uff0c\u4e3b\u5f20\u5f00\u53d1\u5177\u6709\u7a0b\u5e8f\u7ea6\u675f\u7684\u4e13\u7528\u5de5\u5177\uff0c\u901a\u8fc7\u4ee3\u7801\u800c\u975e\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6765\u5f3a\u5236\u6267\u884c\u5b89\u5168\u8fb9\u754c\u3002", "topic": "agent analysis"}}
{"id": "2511.19483", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19483", "abs": "https://arxiv.org/abs/2511.19483", "authors": ["Qingsong He", "Jing Nan", "Jiayu Jiao", "Liangjie Tang", "Xiaodong Xu", "Mengmeng Sun", "Qingyao Wang", "Minghui Yan"], "title": "Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation", "comment": null, "summary": "Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\\% while achieving a 92\\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.", "AI": {"tldr": "Z-Space\u662f\u4e00\u4e2a\u9762\u5411\u6570\u636e\u751f\u6210\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5de5\u5177\u8c03\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u610f\u56fe\u89e3\u6790\u3001\u5de5\u5177\u8fc7\u6ee4\u548c\u63a8\u7406\u6267\u884c\u4e09\u4e2a\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21MCP\u670d\u52a1\u4e2d\u5de5\u5177\u5339\u914d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u4f01\u4e1a\u7ea7MCP\u670d\u52a1\u7684\u5feb\u901f\u589e\u957f\uff0c\u5728\u6570\u5343\u4e2a\u5f02\u6784\u5de5\u5177\u4e2d\u9ad8\u6548\u51c6\u786e\u5730\u5339\u914d\u76ee\u6807\u529f\u80fd\u6210\u4e3a\u9650\u5236\u7cfb\u7edf\u5b9e\u7528\u6027\u7684\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7528\u6237\u67e5\u8be2\u4e0e\u5de5\u5177\u63cf\u8ff0\u8bed\u4e49\u8131\u8282\u3001LLM\u8f93\u5165\u4e0a\u4e0b\u6587\u81a8\u80c0\u548c\u9ad8\u63a8\u7406\u5ef6\u8fdf\u7b49\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\u548c\u5de5\u5177\u8fc7\u6ee4\u7b97\u6cd5\uff1a1\uff09\u901a\u8fc7\u610f\u56fe\u89e3\u6790\u6a21\u578b\u5b9e\u73b0\u7528\u6237\u67e5\u8be2\u7684\u7ed3\u6784\u5316\u8bed\u4e49\u7406\u89e3\uff1b2\uff09\u57fa\u4e8e\u878d\u5408\u5b50\u7a7a\u95f4\u52a0\u6743\u7b97\u6cd5\u7684\u5de5\u5177\u8fc7\u6ee4\u6a21\u5757\u5b9e\u73b0\u610f\u56fe\u4e0e\u5de5\u5177\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff1b3\uff09\u6784\u5efa\u63a8\u7406\u6267\u884c\u4ee3\u7406\u652f\u6301\u591a\u6b65\u4efb\u52a1\u7684\u52a8\u6001\u89c4\u5212\u548c\u5bb9\u9519\u6267\u884c\u3002", "result": "\u7cfb\u7edf\u5728\u5de5\u5177\u63a8\u7406\u4e2d\u5e73\u5747token\u6d88\u8017\u964d\u4f4e96.26%\uff0c\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u8fbe\u523092%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "Z-Space\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u8bed\u4e49\u5339\u914d\u95ee\u9898\uff0c\u5728\u997f\u4e86\u4e48\u5e73\u53f0\u6280\u672f\u90e8\u591a\u4e2a\u4e1a\u52a1\u5355\u5143\u7684\u5927\u89c4\u6a21\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u573a\u666f\u4e2d\u5f97\u5230\u6210\u529f\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2511.19663", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19663", "abs": "https://arxiv.org/abs/2511.19663", "authors": ["Ahmed Awadallah", "Yash Lara", "Raghav Magazine", "Hussein Mozannar", "Akshay Nambi", "Yash Pandya", "Aravind Rajeswaran", "Corby Rosset", "Alexey Taymanov", "Vibhav Vineet", "Spencer Whitehead", "Andrew Zhao"], "title": "Fara-7B: An Efficient Agentic Model for Computer Use", "comment": null, "summary": "Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.", "AI": {"tldr": "FaraGen\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6b65\u7f51\u9875\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7cfb\u7edf\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8bad\u7ec3\u51faFara-7B\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u751a\u81f3\u80fd\u4e0e\u66f4\u5927\u7684\u524d\u6cbf\u6a21\u578b\u7ade\u4e89\u3002", "motivation": "\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUAs\uff09\u7684\u53d1\u5c55\u53d7\u5230\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u9650\u5236\uff0c\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u4e0e\u8ba1\u7b97\u673a\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u5f00\u53d1FaraGen\u7cfb\u7edf\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5305\u62ec\u63d0\u51fa\u591a\u6837\u5316\u4efb\u52a1\u3001\u751f\u6210\u591a\u4e2a\u89e3\u51b3\u65b9\u6848\u5c1d\u8bd5\uff0c\u5e76\u4f7f\u7528\u591a\u4e2a\u9a8c\u8bc1\u5668\u8fc7\u6ee4\u6210\u529f\u8f68\u8ff9\u3002\u4f7f\u7528\u8be5\u6570\u636e\u8bad\u7ec3Fara-7B\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ec5\u901a\u8fc7\u622a\u56fe\u611f\u77e5\u8ba1\u7b97\u673a\uff0c\u901a\u8fc7\u9884\u6d4b\u5750\u6807\u6267\u884c\u52a8\u4f5c\u3002", "result": "Fara-7B\u5728WebVoyager\u3001Online-Mind2Web\u548cWebTailBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u540c\u7c7b\u89c4\u6a21\u7684CUA\u6a21\u578b\uff0c\u5e76\u4e14\u4e0e\u66f4\u5927\u7684\u524d\u6cbf\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\u3002\u6570\u636e\u751f\u6210\u6210\u672c\u7ea6\u4e3a\u6bcf\u6761\u8f68\u8ff91\u7f8e\u5143\u3002", "conclusion": "\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u7cfb\u7edf\u5728\u63a8\u8fdb\u5c0f\u578b\u9ad8\u6548\u4ee3\u7406\u6a21\u578b\u65b9\u9762\u5177\u6709\u5173\u952e\u4f18\u52bf\uff0cFara-7B\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "topic": "code agent"}}
{"id": "2511.19489", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19489", "abs": "https://arxiv.org/abs/2511.19489", "authors": ["Zhe Zhao", "Yuheng Yang", "Haibin Wen", "Xiaojie Qiu", "Zaixi Zhang", "Qingfu Zhang"], "title": "Evolution without an Oracle: Driving Effective Evolution with LLM Judges", "comment": "14 pages, 5 figures", "summary": "The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through \"Problem Specification.\" By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing \"computable metrics\" to \"describable qualities,\" thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.", "AI": {"tldr": "MADE\u6846\u67b6\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u5c06\u4e3b\u89c2LLM\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7a33\u5b9a\u9009\u62e9\u538b\u529b\uff0c\u5728\u8f6f\u4ef6\u5f00\u53d1\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u5b9e\u73b050%\u4ee5\u4e0a\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u4ece\u53ef\u8ba1\u7b97\u6307\u6807\u4f18\u5316\u5230\u53ef\u63cf\u8ff0\u8d28\u91cf\u4f18\u5316\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u6253\u7834\u4f20\u7edf\u8fdb\u5316\u8ba1\u7b97\u5bf9\u5ba2\u89c2\u53ef\u8ba1\u7b97\u9002\u5e94\u5ea6\u51fd\u6570\u7684\u4f9d\u8d56\uff0c\u63a2\u7d22\u5728\u7eaf\u4e3b\u89c2LLM\u8bc4\u5224\u4e0b\u7684\u8fdb\u5316\u4f18\u5316\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faMADE\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u5c06\u6a21\u7cca\u6307\u4ee4\u8f6c\u5316\u4e3a\u5177\u4f53\u53ef\u9a8c\u8bc1\u7684\u5b50\u8981\u6c42\uff0c\u4ece\u800c\u9a6f\u670d\u4e3b\u89c2\u8bc4\u4f30\u7684\u566a\u58f0\u3002", "result": "\u5728DevAI\u548cInfoBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8f6f\u4ef6\u9700\u6c42\u6ee1\u8db3\u7387\u4ece39.9%\u63d0\u5347\u81f361.9%\uff0c\u590d\u6742\u6307\u4ee4\u9075\u5faa\u7684\u5b8c\u7f8e\u901a\u8fc7\u7387\u8fbe\u523095%\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u4ece\u4f18\u5316\u53ef\u8ba1\u7b97\u6307\u6807\u5230\u4f18\u5316\u53ef\u63cf\u8ff0\u8d28\u91cf\u7684\u6839\u672c\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u65e0\u771f\u5b9e\u6807\u7b7e\u7684\u5f00\u653e\u9886\u57df\u89e3\u9501\u4e86\u8fdb\u5316\u4f18\u5316\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2511.19510", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19510", "abs": "https://arxiv.org/abs/2511.19510", "authors": ["Asif Zaman", "Kallol Naha", "Khalid Belhajjame", "Hasan M. Jamil"], "title": "CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem", "comment": "9 pages, 4 figures", "summary": "Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.", "AI": {"tldr": "\u63d0\u51fa\u4e86CodeR\u00b3\u7cfb\u7edf\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u5c06\u8fc7\u65f6\u7684Taverna\u5de5\u4f5c\u6d41\u8fc1\u79fb\u5230Snakemake\u548cVisFlow\u7b49\u73b0\u4ee3\u5de5\u4f5c\u6d41\u6280\u672f\u4e2d\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4fee\u590d\u548c\u4eba\u5de5\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\u89e3\u51b3\u5de5\u4f5c\u6d41\u8870\u8d25\u95ee\u9898\u3002", "motivation": "\u79d1\u5b66\u5de5\u4f5c\u6d41\u5305\u542b\u5b9d\u8d35\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f46\u5927\u91cf\u5df2\u53d1\u5e03\u7684\u5de5\u4f5c\u6d41\u4f1a\u968f\u65f6\u95f4\u8870\u8d25\uff0c\u7279\u522b\u662f\u50cfTaverna\u8fd9\u6837\u7684\u4f20\u7edf\u5de5\u4f5c\u6d41\u7cfb\u7edf\uff0c\u7531\u4e8e\u670d\u52a1\u7ec8\u6b62\u3001\u4f9d\u8d56\u8fc7\u65f6\u548c\u7cfb\u7edf\u9000\u5f79\u5bfc\u81f4\u529f\u80fd\u5931\u6548\u3002", "method": "\u5f00\u53d1CodeR\u00b3\u7cfb\u7edf\uff0c\u96c6\u6210\u751f\u6210\u5f0fAI\u5206\u6790\u8870\u8d25\u5de5\u4f5c\u6d41\u7279\u5f81\uff0c\u8fdb\u884c\u9010\u6b65\u5de5\u4f5c\u6d41\u5206\u6790\u53ef\u89c6\u5316\u3001\u81ea\u52a8\u5316\u670d\u52a1\u66ff\u6362\u548c\u4eba\u5de5\u53c2\u4e0e\u9a8c\u8bc1\uff0c\u5c06\u5de5\u4f5c\u6d41\u8fc1\u79fb\u5230\u73b0\u4ee3\u6280\u672f\u3002", "result": "\u901a\u8fc7\u591a\u4e2aTaverna\u5de5\u4f5c\u6d41\u590d\u5174\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u81ea\u52a8\u5316\u663e\u8457\u51cf\u5c11\u4e86\u5de5\u4f5c\u6d41\u89e3\u6790\u548c\u670d\u52a1\u8bc6\u522b\u7684\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u4f46\u670d\u52a1\u66ff\u6362\u548c\u6570\u636e\u9a8c\u8bc1\u4ecd\u9700\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e73\u8861\u81ea\u52a8\u5316\u6548\u7387\u4e0e\u5fc5\u8981\u4eba\u5de5\u5224\u65ad\u7684\u5de5\u4f5c\u6d41\u590d\u5174\u6846\u67b6\uff0c\u5c06\u5f00\u53d1\u4f17\u5305\u5e73\u53f0\u4f9b\u793e\u533a\u534f\u4f5c\u590d\u5174\u8870\u8d25\u5de5\u4f5c\u6d41\u5e76\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u3002", "topic": "swe application"}}
{"id": "2511.19635", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19635", "abs": "https://arxiv.org/abs/2511.19635", "authors": ["Abhi Chivukula", "Jay Somasundaram", "Vijay Somasundaram"], "title": "Agint: Agentic Graph Compilation for Software Engineering Agents", "comment": "18 pages, 5 figures, NeurIPS 2025: Deep Learning for Code in the Agentic Era", "summary": "LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.", "AI": {"tldr": "Agint\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u8bd1\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u7c7b\u578b\u5316\u7684\u4ee3\u7801DAG\uff0c\u63d0\u4f9b\u53ef\u91cd\u73b0\u3001\u53ef\u4f18\u5316\u7684\u6267\u884c\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dLLM\u7f16\u7801\u4ee3\u7406\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u5ef6\u8fdf\u3001\u53ef\u9760\u6027\u3001\u53ef\u91cd\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u663e\u5f0f\u7c7b\u578b\u5c42\u6b21\uff08\u6587\u672c\u2192\u6570\u636e\u2192\u89c4\u8303\u2192\u4ee3\u7801\uff09\u548c\u8bed\u4e49\u56fe\u8f6c\u6362\uff0c\u7ed3\u5408\u6df7\u5408LLM\u548c\u57fa\u4e8e\u51fd\u6570\u7684JIT\u8fd0\u884c\u65f6\uff0c\u5b9e\u73b0\u52a8\u6001\u56fe\u4f18\u5316\u3002", "result": "\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\uff0c\u652f\u6301\u5e76\u53d1\u7ec4\u5408\uff0c\u4f7f\u7528\u66f4\u5c0f\u66f4\u5feb\u7684\u6a21\u578b\uff0c\u964d\u4f4e\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u652f\u6301\u53ef\u91cd\u73b0\u7684\u5e76\u884c\u751f\u6210\u3002", "conclusion": "Agint\u901a\u8fc7\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u3001\u7f16\u8bd1\u5668\u65b9\u6cd5\u548c\u5f00\u53d1\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u53ef\u7ec4\u5408\u3001\u56e2\u961f\u5bfc\u5411\u7684\u7f16\u7801\u4ee3\u7406\u89c4\u6a21\u5316\u90e8\u7f72\u3002", "topic": "code agent"}}
{"id": "2511.19773", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19773", "abs": "https://arxiv.org/abs/2511.19773", "authors": ["Meng Lu", "Ran Xu", "Yi Fang", "Wenxuan Zhang", "Yue Yu", "Gaurav Srivastava", "Yuchen Zhuang", "Mohamed Elhoseiny", "Charles Fleming", "Carl Yang", "Zhengzhong Tu", "Yang Xie", "Guanghua Xiao", "Hanrui Wang", "Di Jin", "Wenqi Shi", "Xuan Wang"], "title": "Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs", "comment": "17 pages, 9 figures, work in progress", "summary": "While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.", "AI": {"tldr": "VISTA-Gym\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6b65\u89c6\u89c9\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u6b65\u9aa4\u89c6\u89c9\u4ea4\u4e92\u63a8\u7406\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u63d0\u5347\u5de5\u5177\u9009\u62e9\u3001\u8c03\u7528\u548c\u534f\u8c03\u7684\u80fd\u529b\u3002", "method": "VISTA-Gym\u7edf\u4e00\u4e86\u591a\u6837\u5316\u7684\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u89c6\u89c9\u5de5\u5177\u63a5\u53e3\u3001\u53ef\u6267\u884c\u4ea4\u4e92\u5faa\u73af\u3001\u53ef\u9a8c\u8bc1\u53cd\u9988\u4fe1\u53f7\u548c\u9ad8\u6548\u8f68\u8ff9\u8bb0\u5f55\uff0c\u901a\u8fc7\u591a\u8f6e\u8f68\u8ff9\u91c7\u6837\u548c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3VISTA-R1\u6a21\u578b\u3002", "result": "\u572811\u4e2a\u516c\u5171\u63a8\u7406\u5bc6\u96c6\u578bVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVISTA-R1-8B\u6a21\u578b\u5728\u76f8\u4f3c\u89c4\u6a21\u4e0b\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u53479.51%-18.72%\u3002", "conclusion": "VISTA-Gym\u662f\u4e00\u4e2a\u6709\u6548\u7684\u8bad\u7ec3\u5e73\u53f0\uff0c\u80fd\u591f\u89e3\u9501\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.19957", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19957", "abs": "https://arxiv.org/abs/2511.19957", "authors": ["Tianyi Chen", "Michael Solodko", "Sen Wang", "Jongwoo Ko", "Junheng Hao", "Colby Banbury", "Sara Abdali", "Saeed Amizadeh", "Qing Xiao", "Yinheng Li", "Tianyu Ding", "Kamran Ghasedi Dizaji", "Suzhen Zheng", "Hao Fan", "Justin Wagle", "Pashmina Cameron", "Kazuhito Koishida"], "title": "AppSelectBench: Application-Level Tool Selection Benchmark", "comment": null, "summary": "Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.", "AI": {"tldr": "\u63d0\u51fa\u4e86AppSelectBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u5e94\u7528\u9009\u62e9\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u542b10\u4e07+\u771f\u5b9e\u7528\u6237\u4efb\u52a1\u548c100\u4e2a\u684c\u9762\u5e94\u7528\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8de8\u5e94\u7528\u63a8\u7406\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u7ec6\u7c92\u5ea6API\u9009\u62e9\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5728\u4e0d\u540c\u5e94\u7528\u95f4\u8fdb\u884c\u63a8\u7406\u548c\u9009\u62e9\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u800c\u5e94\u7528\u9009\u62e9\u662f\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6709\u6548\u64cd\u4f5c\u7684\u57fa\u7840\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u7528\u6237\u4efb\u52a1\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u8bed\u4e49\u57fa\u7840\u7684\u7528\u6237\u610f\u56fe\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u62ec\u968f\u673a\u3001\u542f\u53d1\u5f0f\u3001\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u68c0\u7d22\u589e\u5f3a\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5e94\u7528\u9009\u62e9\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u5728\u8de8\u5e94\u7528\u63a8\u7406\u4e0a\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u7684\u4f18\u52bf\u548c\u5f31\u70b9\u3002", "conclusion": "AppSelectBench\u4e3a\u7814\u7a76\u548c\u63a8\u8fdb\u5e94\u7528\u7ea7\u63a8\u7406\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u662f\u667a\u80fd\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.19875", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19875", "abs": "https://arxiv.org/abs/2511.19875", "authors": ["Qingyu Zhang", "Puzhuo Liu", "Peng Di", "Chenxiong Qian"], "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection", "comment": null, "summary": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u63d0\u4ea4\u6d88\u606f\u4e0e\u4ee3\u7801\u53d8\u66f4\u4e0d\u4e00\u81f4\u6027(MCI)\u7684\u57fa\u51c6CODEFUSE-COMMITEVAL\uff0c\u901a\u8fc7\u89c4\u5219\u5f15\u5bfc\u7684\u7a81\u53d8\u751f\u6210\u4e03\u79cd\u4e0d\u4e00\u81f4\u6d88\u606f\u7c7b\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86\u516d\u79cd\u5f00\u6e90LLM\u5728\u591a\u79cd\u589e\u5f3a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u7248\u672c\u63a7\u5236\u4e2d\u7684\u63d0\u4ea4\u6d88\u606f\u7ecf\u5e38\u8d28\u91cf\u4f4e\u4e0b\u4e14\u4e0e\u4ee3\u7801\u53d8\u66f4\u4e0d\u4e00\u81f4\uff0c\u8fd9\u4f1a\u8bef\u5bfc\u5ba1\u67e5\u8005\u3001\u963b\u788d\u7ef4\u62a4\u3001\u6c61\u67d3\u7814\u7a76\u6570\u636e\u96c6\u5e76\u53ef\u80fd\u63a9\u76d6\u5b89\u5168\u8865\u4e01\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30MCI\u68c0\u6d4b\u6a21\u578b\u3002", "method": "\u57fa\u4e8eApacheCM\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u89c4\u5219\u5f15\u5bfc\u7684\u7a81\u53d8\u751f\u6210\u4e03\u79cd\u4e0d\u4e00\u81f4\u6d88\u606f\u7c7b\u578b\uff0c\u5e94\u7528\u53cc\u91cd\u9a8c\u8bc1\u786e\u4fdd\u6b63\u8d1f\u6837\u672c\u8d28\u91cf\uff0c\u8bc4\u4f30\u516d\u79cd\u5f00\u6e90LLM\u5728\u666e\u901a\u8bbe\u7f6e\u53ca\u4e09\u79cd\u589e\u5f3a\u7b56\u7565(\u5c11\u6837\u672c\u63d0\u793a\u3001\u601d\u7ef4\u94fe\u3001\u6269\u5c55\u4e0a\u4e0b\u6587)\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u68c0\u6d4b\u4e0d\u4e00\u81f4\u63d0\u4ea4\u6bd4\u4e00\u81f4\u63d0\u4ea4\u66f4\u53ef\u9760(\u5e73\u5747\u53ec\u56de\u738785.95%\uff0c\u7cbe\u786e\u738780.28%\uff0c\u7279\u5f02\u602763.8%)\uff1bgpt-oss-20B\u8868\u73b0\u6700\u4f73\u4f46token\u4f7f\u7528\u91cf\u662f\u5176\u4ed6\u6a21\u578b\u7684\u4e24\u500d\u591a\uff1b\u589e\u5f3a\u7b56\u7565\u6548\u679c\u5404\u5f02\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u96be\u5ea6\u4e0d\u540c\u3002", "conclusion": "CODEFUSE-COMMITEVAL\u4e3aMCI\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u548c\u5e73\u8861\u7684\u6570\u636e\u6765\u6355\u6349\u9ad8\u5c42\u6b21\u8bed\u4e49\u5dee\u8ddd\u3002", "topic": "swe benchmark"}}
{"id": "2511.20403", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20403", "abs": "https://arxiv.org/abs/2511.20403", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Barto"], "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework", "comment": "Accepted at 40th IEEE/ACM International Conference on Automated Software Engineering", "summary": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.", "AI": {"tldr": "AgoneTest\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684Java\u5355\u5143\u6d4b\u8bd5\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5305\u542bClasses2Test\u6570\u636e\u96c6\u548c\u7efc\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u5b9e\u9a8c\u663e\u793aLLM\u751f\u6210\u7684\u6d4b\u8bd5\u5728\u7f16\u8bd1\u6210\u529f\u540e\u80fd\u5728\u8986\u76d6\u7387\u548c\u7f3a\u9677\u68c0\u6d4b\u65b9\u9762\u8fbe\u5230\u6216\u8d85\u8fc7\u4eba\u5de5\u7f16\u5199\u7684\u6d4b\u8bd5\u3002", "motivation": "\u5355\u5143\u6d4b\u8bd5\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u91cd\u8981\u4f46\u8d44\u6e90\u5bc6\u96c6\u7684\u73af\u8282\uff0c\u9700\u8981\u6807\u51c6\u5316\u6846\u67b6\u6765\u6bd4\u8f83\u4e0d\u540cLLM\u548c\u63d0\u793a\u7b56\u7565\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faAgoneTest\u8bc4\u4f30\u6846\u67b6\u548cClasses2Test\u6570\u636e\u96c6\uff0c\u96c6\u6210\u53d8\u5f02\u5206\u6570\u548c\u6d4b\u8bd5\u5f02\u5473\u7b49\u9ad8\u7ea7\u8bc4\u4f30\u6307\u6807\uff0c\u5efa\u7acb\u7aef\u5230\u7aef\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u5bf9\u4e8e\u80fd\u591f\u7f16\u8bd1\u7684\u6d4b\u8bd5\u5b50\u96c6\uff0cLLM\u751f\u6210\u7684\u6d4b\u8bd5\u5728\u8986\u76d6\u7387\u548c\u7f3a\u9677\u68c0\u6d4b\u65b9\u9762\u80fd\u591f\u5339\u914d\u6216\u8d85\u8fc7\u4eba\u5de5\u7f16\u5199\u7684\u6d4b\u8bd5\uff0c\u589e\u5f3a\u7684\u63d0\u793a\u7b56\u7565\u6709\u52a9\u4e8e\u63d0\u9ad8\u6d4b\u8bd5\u8d28\u91cf\u3002", "conclusion": "AgoneTest\u9610\u660e\u4e86LLM\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u6d4b\u8bd5\u5b9e\u8df5\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "swe benchmark"}}
{"id": "2511.20617", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.20617", "abs": "https://arxiv.org/abs/2511.20617", "authors": ["Saman Dehghan", "Tianran Sun", "Tianxiang Wu", "Zihan Li", "Reyhaneh Jabbarvand"], "title": "Translating Large-Scale C Repositories to Idiomatic Rust", "comment": "21 pages, 14 figures", "summary": "Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.", "AI": {"tldr": "Rustine\u662f\u4e00\u4e2a\u81ea\u52a8\u5316C\u5230Rust\u7ffb\u8bd1\u7ba1\u9053\uff0c\u572823\u4e2aC\u7a0b\u5e8f\u4e0a\u5b9e\u73b0\u4e8687%\u529f\u80fd\u7b49\u4ef7\u6027\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5b89\u5168\u3001\u66f4\u5730\u9053\u3001\u66f4\u53ef\u8bfb", "motivation": "\u73b0\u6709C\u5230Rust\u7ffb\u8bd1\u6280\u672f\u65e0\u6cd5\u5e73\u8861\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\uff1a\u57fa\u4e8e\u8f6c\u8bd1\u7684\u65b9\u6cd5\u53ef\u6269\u5c55\u4f46\u4ee3\u7801\u8d28\u91cf\u5dee\uff0c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u4f9d\u8d56\u524d\u6cbf\u6a21\u578b", "method": "\u63d0\u51faRustine\u5168\u81ea\u52a8\u7ba1\u9053\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u6709\u6548\u7684\u4ed3\u5e93\u7ea7C\u5230\u5730\u9053\u5b89\u5168Rust\u7ffb\u8bd1", "result": "\u572823\u4e2aC\u7a0b\u5e8f\u4e0a\uff0cRustine\u4e3a\u6240\u6709\u7a0b\u5e8f\u751f\u6210\u5b8c\u5168\u53ef\u7f16\u8bd1\u7684Rust\u4ee3\u7801\uff0c\u8fbe\u523087%\u529f\u80fd\u7b49\u4ef7\u6027\uff0c\u6bd46\u79cd\u73b0\u6709\u6280\u672f\u66f4\u5b89\u5168\u3001\u66f4\u5730\u9053\u3001\u66f4\u53ef\u8bfb", "conclusion": "Rustine\u5728C\u5230Rust\u7ffb\u8bd1\u4e2d\u5b9e\u73b0\u4e86\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u7684\u5e73\u8861\uff0c\u5f53\u7ffb\u8bd1\u65e0\u6cd5\u901a\u8fc7\u6240\u6709\u6d4b\u8bd5\u65f6\uff0c\u4eba\u7c7b\u5f00\u53d1\u8005\u5e73\u5747\u53ea\u97004.5\u5c0f\u65f6\u5373\u53ef\u5b8c\u6210\u4efb\u52a1", "topic": "swe application"}}
{"id": "2511.19849", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19849", "abs": "https://arxiv.org/abs/2511.19849", "authors": ["Dominik Wagner", "Leon Witzman", "Luke Ong"], "title": "Reinforcement Learning with $\u03c9$-Regular Objectives and Constraints", "comment": null, "summary": "Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $\u03c9$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.\n  We address both limitations simultaneously by combining $\u03c9$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $\u03c9$-regular objective while also adhering to $\u03c9$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u03c9-\u6b63\u5219\u76ee\u6807\u548c\u663e\u5f0f\u7ea6\u675f\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u7b97\u6cd5\u6700\u5927\u5316\u6ee1\u8db3\u03c9-\u6b63\u5219\u76ee\u6807\u7684\u6982\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u6ee1\u8db3\u03c9-\u6b63\u5219\u7ea6\u675f\u6761\u4ef6\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u4e14\u5bb9\u6613\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff1b\u540c\u65f6\u5355\u4e00\u6807\u91cf\u6027\u80fd\u6307\u6807\u63a9\u76d6\u4e86\u5b89\u5168\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c06\u03c9-\u6b63\u5219\u76ee\u6807\u4e0e\u7ea6\u675f\u5206\u79bb\u5904\u7406\uff0c\u5e76\u5efa\u7acb\u5230\u7ea6\u675f\u6781\u9650\u5e73\u5747\u95ee\u9898\u7684\u8f6c\u6362\u3002", "result": "\u7b97\u6cd5\u5728\u6781\u9650\u60c5\u51b5\u4e0b\u80fd\u591f\u4ea7\u751f\u6700\u5927\u5316\u6ee1\u8db3\u03c9-\u6b63\u5219\u76ee\u6807\u6982\u7387\u7684\u7b56\u7565\uff0c\u540c\u65f6\u786e\u4fdd\u6ee1\u8db3\u6307\u5b9a\u7684\u03c9-\u6b63\u5219\u7ea6\u675f\u9608\u503c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5b89\u5168-\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u884c\u4e3a\u89c4\u8303\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.19872", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19872", "abs": "https://arxiv.org/abs/2511.19872", "authors": ["Daniel I Jackson", "Emma L Jensen", "Syed-Amad Hussain", "Emre Sezgin"], "title": "Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy", "comment": "25 pages,5 tables, 3 figures", "summary": "Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u901a\u7528\u81ea\u6211\u6548\u80fd\u611f\u91cf\u8868(GSES)\u5e94\u7528\u4e8e10\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e0d\u540c\u4efb\u52a1\u6761\u4ef6\u4e0b\u8bc4\u4f30\u5176\u6a21\u62df\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u7a33\u5b9a\u4f46\u4e0d\u51c6\u786e\uff0c\u4e0e\u771f\u5b9e\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u4e14\u9ad8\u81ea\u6211\u6548\u80fd\u611f\u5bf9\u5e94\u66f4\u62df\u4eba\u5316\u7684\u63a8\u7406\u98ce\u683c\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u81ea\u6211\u8bc4\u4f30\u8fd9\u4e00\u53ef\u9760\u667a\u80fd\u7684\u5173\u952e\u65b9\u9762\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u7684\u6a21\u62df\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\u53ca\u5176\u4e0e\u4efb\u52a1\u8868\u73b0\u7684\u5173\u7cfb\u3002", "method": "\u5c0610\u9879\u901a\u7528\u81ea\u6211\u6548\u80fd\u611f\u91cf\u8868(GSES)\u5e94\u7528\u4e8e10\u4e2aLLMs\uff0c\u5728\u56db\u79cd\u6761\u4ef6\u4e0b(\u65e0\u4efb\u52a1\u3001\u8ba1\u7b97\u63a8\u7406\u3001\u793e\u4f1a\u63a8\u7406\u3001\u6458\u8981)\u6536\u96c6\u6a21\u62df\u81ea\u6211\u8bc4\u4f30\uff0c\u5206\u6790\u5176\u7a33\u5b9a\u6027\u3001\u51c6\u786e\u6027\u4ee5\u53ca\u4e0e\u4efb\u52a1\u8868\u73b0\u7684\u5173\u7cfb\u3002", "result": "\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u5ea6\u7a33\u5b9a\u4f46\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u6807\u51c6\uff1b\u6240\u6709\u6a21\u578b\u5728\u8ba1\u7b97\u548c\u793e\u4f1a\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u5b8c\u7f8e\uff0c\u4f46\u6458\u8981\u4efb\u52a1\u8868\u73b0\u5dee\u5f02\u5f88\u5927\uff1b\u81ea\u6211\u8bc4\u4f30\u4e0d\u80fd\u53ef\u9760\u53cd\u6620\u5b9e\u9645\u80fd\u529b\uff1b\u9ad8\u81ea\u6211\u6548\u80fd\u611f\u5bf9\u5e94\u66f4\u62df\u4eba\u5316\u7684\u63a8\u7406\u98ce\u683c\u3002", "conclusion": "\u5fc3\u7406\u6d4b\u91cf\u63d0\u793a\u6cd5\u80fd\u63d0\u4f9b\u5bf9LLM\u6c9f\u901a\u884c\u4e3a\u7684\u7ed3\u6784\u5316\u6d1e\u5bdf\uff0c\u4f46\u4e0d\u80fd\u63d0\u4f9b\u6821\u51c6\u7684\u6027\u80fd\u4f30\u8ba1\u3002\u81ea\u6211\u8bc4\u4f30\u5728LLMs\u4e2d\u7a33\u5b9a\u4f46\u4e0d\u51c6\u786e\uff0c\u4e0e\u771f\u5b9e\u80fd\u529b\u8131\u8282\u3002", "topic": "agent analysis"}}
{"id": "2511.19895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19895", "abs": "https://arxiv.org/abs/2511.19895", "authors": ["Yuanyuan Lin", "Xiangyu Ouyang", "Teng Zhang", "Kaixin Sui"], "title": "RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation", "comment": "Accepted at AAAI 2026", "summary": "Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.", "AI": {"tldr": "RPM-MCTS\u662f\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u68c0\u7d22\u4f5c\u4e3a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6765\u8bc4\u4f30\u4e2d\u95f4\u7b97\u6cd5\u6b65\u9aa4\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\uff0c\u540c\u65f6\u4f7f\u7528\u6c99\u7bb1\u6267\u884c\u53cd\u9988\u5b9a\u4f4d\u548c\u7ea0\u6b63\u9519\u8bef\u6b65\u9aa4\uff0c\u5728\u51cf\u5c1115%token\u6d88\u8017\u7684\u540c\u65f6\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u4e2d\u95f4\u7b97\u6cd5\u6b65\u9aa4\uff0c\u65e0\u6cd5\u53ca\u65f6\u5b9a\u4f4d\u548c\u7ea0\u6b63\u9519\u8bef\u6b65\u9aa4\uff0c\u5bfc\u81f4\u751f\u6210\u9519\u8bef\u4ee3\u7801\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u3002", "method": "\u63d0\u51faRPM-MCTS\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u77e5\u8bc6\u68c0\u7d22\u4f5c\u4e3a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u4e2d\u95f4\u6b65\u9aa4\uff1b2) \u5728\u6269\u5c55\u9636\u6bb5\u4f7f\u7528\u76f8\u4f3c\u6027\u8fc7\u6ee4\u53bb\u9664\u5197\u4f59\u8282\u70b9\uff1b3) \u5229\u7528\u6c99\u7bb1\u6267\u884c\u53cd\u9988\u5b9a\u4f4d\u548c\u7ea0\u6b63\u9519\u8bef\u7b97\u6cd5\u6b65\u9aa4\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRPM-MCTS\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u7ea615%\u7684token\u6d88\u8017\u3002\u4f7f\u7528RPM-MCTS\u6784\u5efa\u7684\u6570\u636e\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5168\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u5176\u4ee3\u7801\u80fd\u529b\u3002", "conclusion": "RPM-MCTS\u901a\u8fc7\u77e5\u8bc6\u68c0\u7d22\u548c\u6c99\u7bb1\u53cd\u9988\u6709\u6548\u89e3\u51b3\u4e86\u4e2d\u95f4\u6b65\u9aa4\u8bc4\u4f30\u548c\u9519\u8bef\u5b9a\u4f4d\u95ee\u9898\uff0c\u5728\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "topic": "code agent"}}
{"id": "2511.19496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19496", "abs": "https://arxiv.org/abs/2511.19496", "authors": ["Yang Liu", "Xiaolong Zhong", "Ling Jiang"], "title": "Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM", "comment": null, "summary": "Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \\textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \\emph{drop-in agent core}. Training with maximal-update parameterization ($\u03bc$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \\emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \\textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\\,\\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.", "AI": {"tldr": "Xmodel-2.5\u662f\u4e00\u4e2a13\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f5c\u4e3a\"\u5373\u63d2\u5373\u7528\u4ee3\u7406\u6838\u5fc3\"\u8bbe\u8ba1\uff0c\u901a\u8fc7\u03bcP\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u8d85\u53c2\u6570\u4ece2000\u4e07\u53c2\u6570\u4ee3\u7406\u6a21\u578b\u5230\u5b8c\u6574\u6a21\u578b\u7684\u76f4\u63a5\u8fc1\u79fb\uff0c\u5e76\u91c7\u7528AdamW\u5230Muon\u7684\u4f18\u5316\u5668\u5207\u6362\u7b56\u7565\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4f46\u5176\u8ba1\u7b97\u9700\u6c42\u4f7f\u5176\u4e0d\u9002\u7528\u4e8e\u8fb9\u7f18\u6216\u6210\u672c\u654f\u611f\u90e8\u7f72\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u5c0f\u578b\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6700\u5927\u66f4\u65b0\u53c2\u6570\u5316(\u03bcP)\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u75281.4T token\u7684Warmup-Stable-Decay\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5e76\u5728\u8870\u51cf\u9636\u6bb5\u4eceAdamW\u5207\u6362\u5230Muon\u4f18\u5316\u5668\uff0c\u7ed3\u5408FP8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002", "result": "\u4f18\u5316\u5668\u5207\u6362\u7b56\u7565\u4f7f13\u4e2a\u4efb\u52a1\u7684\u63a8\u7406\u5e73\u5747\u6027\u80fd\u63d0\u53474.58%\uff0c\u540c\u65f6\u4fdd\u6301\u6240\u6709\u5176\u4ed6\u8d85\u53c2\u6570\u4e0d\u53d8\uff0c\u9a8c\u8bc1\u4e86\u65e9\u671fAdamW\u7a33\u5b9a\u6027\u4e0e\u540e\u671fMuon\u9510\u5316\u7684\u7ec4\u5408\u4f18\u52bf\u3002", "conclusion": "Xmodel-2.5\u5c55\u793a\u4e86\u5c0f\u578b\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u6838\u5fc3\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u6240\u6709\u8d44\u6e90\u5747\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2511.19933", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19933", "abs": "https://arxiv.org/abs/2511.19933", "authors": ["Vaishali Vinay"], "title": "A System-Level Taxonomy of Failure Modes in Large Language Model Applications", "comment": null, "summary": "Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7ea7\u5206\u7c7b\u6cd5\uff0c\u8bc6\u522b\u4e8615\u79cd\u771f\u5b9e\u4e16\u754cLLM\u5e94\u7528\u4e2d\u7684\u9690\u85cf\u6545\u969c\u6a21\u5f0f\uff0c\u5206\u6790\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u4e86\u6784\u5efa\u53ef\u9760\u3001\u53ef\u7ef4\u62a4\u3001\u6210\u672c\u611f\u77e5\u7684LLM\u7cfb\u7edf\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "LLM\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u7406\u89e3\u4e0d\u8db3\uff0c\u5176\u6545\u969c\u6a21\u5f0f\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6709\u6839\u672c\u5dee\u5f02\uff0c\u9700\u8981\u7cfb\u7edf\u5c42\u9762\u7684\u53ef\u9760\u6027\u5206\u6790\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u7ea7\u5206\u7c7b\u6cd5\u8bc6\u522b15\u79cd\u9690\u85cf\u6545\u969c\u6a21\u5f0f\uff0c\u5206\u6790\u8bc4\u4f30\u4e0e\u76d1\u63a7\u5b9e\u8df5\u7684\u5dee\u8ddd\uff0c\u7814\u7a76\u90e8\u7f72\u6311\u6218\u5e76\u5236\u5b9a\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684LLM\u6545\u969c\u6a21\u5f0f\u5206\u7c7b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u7a33\u5b9a\u6027\u3001\u53ef\u91cd\u590d\u6027\u3001\u6f02\u79fb\u548c\u5de5\u4f5c\u6d41\u96c6\u6210\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u5c06LLM\u53ef\u9760\u6027\u89c6\u4e3a\u7cfb\u7edf\u5de5\u7a0b\u95ee\u9898\u800c\u975e\u7eaf\u6a21\u578b\u4e2d\u5fc3\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u65b9\u6cd5\u3001AI\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u5206\u6790\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.20048", "categories": ["cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.20048", "abs": "https://arxiv.org/abs/2511.20048", "authors": ["Zixiao Huang", "Wen Zeng", "Tianyu Fu", "Tengxuan Liu", "Yizhou Sun", "Ke Hong", "Xinhao Yang", "Chengchun Liu", "Yan Li", "Quanlu Zhang", "Guohao Dai", "Zhenhua Zhu", "Yu Wang"], "title": "Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design", "comment": null, "summary": "LLM-based search agents achieve strong performance but suffer from severe latency, as each step requires serialized LLM reasoning followed by action of tool execution. We revisit this bottleneck through the lens of speculation. While traditional predict-verify speculation paradigm can break serial execution, its benefit remains limited, as it retains the full original workload and adds extra inference overhead. We observe that early agent steps often involve simple evidence-gathering, where correct actions can often be predicted without full reasoning. Building on these observations, we present SPAgent, an algorithm-system co-design framework that expands the role of speculation in search agents to reduce latency. Algorithmically, SPAgent introduces a two-phase adaptive speculation mechanism that selectively omits verification when safe. System-wise, a two-level scheduler regulates speculative requests based on engine load to ensure speculation remains beneficial. We implement SPAgent in real-world systems. Across extensive experimental settings, SPAgent achieves up to $1.65\\times$ end-to-end speedup while maintaining same or even achieving higher accuracy, enabling practical deployment of multi-step search agents.", "AI": {"tldr": "SPAgent\u662f\u4e00\u4e2a\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u63a8\u6d4b\u673a\u5236\u548c\u4e24\u7ea7\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u7684\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad81.65\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "motivation": "\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u867d\u7136\u6027\u80fd\u5f3a\u5927\u4f46\u5b58\u5728\u4e25\u91cd\u5ef6\u8fdf\u95ee\u9898\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u6b65\u9aa4\u90fd\u9700\u8981\u4e32\u884c\u5316\u7684LLM\u63a8\u7406\u548c\u5de5\u5177\u6267\u884c\u3002\u4f20\u7edf\u9884\u6d4b-\u9a8c\u8bc1\u63a8\u6d4b\u8303\u5f0f\u867d\u7136\u80fd\u6253\u7834\u4e32\u884c\u6267\u884c\uff0c\u4f46\u6536\u76ca\u6709\u9650\uff0c\u56e0\u4e3a\u5b83\u4fdd\u7559\u4e86\u5b8c\u6574\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5e76\u589e\u52a0\u4e86\u989d\u5916\u7684\u63a8\u7406\u5f00\u9500\u3002", "method": "SPAgent\u91c7\u7528\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff1a\u7b97\u6cd5\u4e0a\u5f15\u5165\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u63a8\u6d4b\u673a\u5236\uff0c\u5728\u5b89\u5168\u65f6\u9009\u62e9\u6027\u7701\u7565\u9a8c\u8bc1\uff1b\u7cfb\u7edf\u4e0a\u4f7f\u7528\u4e24\u7ea7\u8c03\u5ea6\u5668\u6839\u636e\u5f15\u64ce\u8d1f\u8f7d\u8c03\u8282\u63a8\u6d4b\u8bf7\u6c42\uff0c\u786e\u4fdd\u63a8\u6d4b\u4fdd\u6301\u6709\u76ca\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\uff0cSPAgent\u5b9e\u73b0\u4e86\u6700\u9ad81.65\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u751a\u81f3\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "SPAgent\u901a\u8fc7\u6269\u5c55\u63a8\u6d4b\u5728\u641c\u7d22\u4ee3\u7406\u4e2d\u7684\u4f5c\u7528\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u4f7f\u5f97\u591a\u6b65\u9aa4\u641c\u7d22\u4ee3\u7406\u7684\u5b9e\u9645\u90e8\u7f72\u6210\u4e3a\u53ef\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2511.20085", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.20085", "abs": "https://arxiv.org/abs/2511.20085", "authors": ["Chujie Wang", "Zhiyuan Luo", "Ruiqi Liu", "Can Ran", "Shenghua Fan", "Xi Chen", "Chu He"], "title": "VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis", "comment": null, "summary": "The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and flexibility.We also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86VICoT\u591a\u6a21\u6001\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u5de5\u5177\u52a8\u6001\u6574\u5408\u5230\u601d\u7ef4\u94fe\u4e2d\u5b9e\u73b0\u663e\u5f0f\u591a\u8f6e\u63a8\u7406\uff0c\u5728\u9065\u611f\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u6846\u67b6\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u6b63\u4ece\u4f20\u7edf\u76ee\u6807\u8bc6\u522b\u5411\u590d\u6742\u667a\u80fd\u63a8\u7406\u6f14\u8fdb\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u548c\u7075\u6d3b\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5806\u6808\u7684\u63a8\u7406\u7ed3\u6784\u548c\u6a21\u5757\u5316MCP\u517c\u5bb9\u5de5\u5177\u5957\u4ef6\uff0c\u4f7fLLM\u80fd\u9ad8\u6548\u6267\u884c\u591a\u8f6e\u4ea4\u9519\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff1b\u63d0\u51fa\u63a8\u7406\u5806\u6808\u84b8\u998f\u65b9\u6cd5\u5c06\u590d\u6742\u4ee3\u7406\u884c\u4e3a\u8fc1\u79fb\u5230\u8f7b\u91cf\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVICoT\u5728\u63a8\u7406\u900f\u660e\u5ea6\u3001\u6267\u884c\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u6846\u67b6\u3002", "conclusion": "VICoT\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u591a\u8f6e\u63a8\u7406\u548c\u5de5\u5177\u52a8\u6001\u6574\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u5206\u6790\u7684\u63a8\u7406\u80fd\u529b\u548c\u7075\u6d3b\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.20138", "categories": ["cs.AI", "cs.DM", "cs.LG", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.20138", "abs": "https://arxiv.org/abs/2511.20138", "authors": ["Jason Lo", "Mohammadnima Jafari"], "title": "From data to concepts via wiring diagrams", "comment": "19 pages", "summary": "A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u51c6\u9aa8\u67b6\u63a5\u7ebf\u56fe\u7684\u6982\u5ff5\uff0c\u8bc1\u660e\u4e86\u5176\u4e0eHasse\u56fe\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4ece\u987a\u5e8f\u6570\u636e\u4e2d\u63d0\u53d6\u63a5\u7ebf\u56fe\u7684\u7b97\u6cd5\u3002\u8fd9\u4e9b\u7b97\u6cd5\u5728\u5206\u6790\u81ea\u4e3b\u4ee3\u7406\u73a9\u7535\u8111\u6e38\u620f\u7684\u884c\u4e3a\u65f6\uff0c\u6210\u529f\u8bc6\u522b\u4e86\u83b7\u80dc\u7b56\u7565\u3002", "motivation": "\u63a5\u7ebf\u56fe\u662f\u8868\u793a\u62bd\u8c61\u6982\u5ff5\uff08\u5982\u65f6\u95f4\u8fc7\u7a0b\uff09\u7684\u6807\u8bb0\u6709\u5411\u56fe\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4ece\u987a\u5e8f\u6570\u636e\u4e2d\u81ea\u52a8\u63d0\u53d6\u63a5\u7ebf\u56fe\u7684\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u5206\u6790\u590d\u6742\u7cfb\u7edf\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u5f15\u5165\u51c6\u9aa8\u67b6\u63a5\u7ebf\u56fe\u6982\u5ff5\uff0c\u8bc1\u660e\u5176\u4e0eHasse\u56fe\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6b64\u7684\u63a5\u7ebf\u56fe\u63d0\u53d6\u7b97\u6cd5\uff0c\u5e76\u4e0eDBSCAN\u548c\u51dd\u805a\u5c42\u6b21\u805a\u7c7b\u7b49\u6807\u51c6\u805a\u7c7b\u6280\u672f\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7b97\u6cd5\u5728\u5206\u6790\u81ea\u4e3b\u4ee3\u7406\u6e38\u620f\u884c\u4e3a\u65f6\u6b63\u786e\u8bc6\u522b\u4e86\u83b7\u80dc\u7b56\u7565\uff0c\u5728\u6570\u636e\u6270\u52a8\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5c06\u8303\u7574\u8bba\u3001\u56fe\u8bba\u3001\u805a\u7c7b\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u636e\u5de5\u7a0b\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4e3a\u4ece\u987a\u5e8f\u6570\u636e\u4e2d\u63d0\u53d6\u63a5\u7ebf\u56fe\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2511.19517", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19517", "abs": "https://arxiv.org/abs/2511.19517", "authors": ["Adarsh Kumarappan", "Ananya Mujoo"], "title": "Automating Deception: Scalable Multi-Turn LLM Jailbreaks", "comment": null, "summary": "Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7ba1\u9053\u6765\u751f\u6210\u5927\u89c4\u6a21\u3001\u57fa\u4e8e\u5fc3\u7406\u5b66\u7684\u591a\u8f6e\u8d8a\u72f1\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540cLLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u653b\u51fb\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0GPT\u7cfb\u5217\u6a21\u578b\u5bf9\u5bf9\u8bdd\u5386\u53f2\u7279\u522b\u654f\u611f\uff0c\u800cGemini 2.5 Flash\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u62b5\u6297\u529b\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u653b\u51fb\u5229\u7528\u5fc3\u7406\u5b66\u539f\u7406\uff08\u5982\u5f97\u5bf8\u8fdb\u5c3a\u6280\u5de7\uff09\u7ed5\u8fc7LLM\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u4f46\u76ee\u524d\u9632\u5fa1\u8fdb\u5c55\u53d7\u5230\u624b\u52a8\u3001\u96be\u4ee5\u6269\u5c55\u7684\u6570\u636e\u96c6\u521b\u5efa\u7684\u9650\u5236\u3002", "method": "\u7cfb\u7edf\u5730\u5c06FITD\u6280\u672f\u8f6c\u5316\u4e3a\u53ef\u590d\u73b0\u6a21\u677f\uff0c\u521b\u5efa\u5305\u542b1,500\u4e2a\u573a\u666f\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5728\u6709\u65e0\u5bf9\u8bdd\u5386\u53f2\u7684\u6761\u4ef6\u4e0b\u8bc4\u4f307\u4e2a\u6765\u81ea\u4e09\u5927LLM\u5bb6\u65cf\u7684\u6a21\u578b\u3002", "result": "GPT\u7cfb\u5217\u6a21\u578b\u5bf9\u5bf9\u8bdd\u5386\u53f2\u6781\u5176\u654f\u611f\uff0c\u653b\u51fb\u6210\u529f\u7387\u6700\u591a\u589e\u52a032\u4e2a\u767e\u5206\u70b9\uff1bGemini 2.5 Flash\u51e0\u4e4e\u514d\u75ab\u8fd9\u4e9b\u653b\u51fb\uff1bClaude 3 Haiku\u8868\u73b0\u51fa\u5f3a\u5927\u4f46\u4e0d\u5b8c\u7f8e\u7684\u62b5\u6297\u529b\u3002", "conclusion": "\u5f53\u524d\u5b89\u5168\u67b6\u6784\u5728\u5904\u7406\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u65b9\u9762\u5b58\u5728\u5173\u952e\u5dee\u5f02\uff0c\u9700\u8981\u80fd\u591f\u62b5\u6297\u57fa\u4e8e\u53d9\u4e8b\u7684\u64cd\u7eb5\u7684\u9632\u5fa1\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2511.20200", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20200", "abs": "https://arxiv.org/abs/2511.20200", "authors": ["Yitian Huang", "Yuxuan Lei", "Jianxun Lian", "Hao Liao"], "title": "Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025", "comment": null, "summary": "This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u5728CPDC 2025\u6311\u6218\u8d5b\u4e2d\u7edf\u4e00\u6539\u8fdb\u4e86GPU\u548cAPI\u4e24\u4e2a\u8d5b\u9053\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548cGRPO\u8bad\u7ec3\u83b7\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5de5\u5177\u8c03\u7528\u7a33\u5b9a\u6027\u548c\u89d2\u8272\u626e\u6f14\u6307\u5bfc\u7684\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7f13\u89e3\u5c0f\u6837\u672c\u8fc7\u62df\u5408\u3002", "method": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08\u52a8\u6001\u5de5\u5177\u526a\u679d\u3001\u89d2\u8272\u526a\u88c1\u3001\u53c2\u6570\u5f52\u4e00\u5316\u3001\u51fd\u6570\u5408\u5e76\uff09\u548cGPU\u8d5b\u9053\u4e2d\u7684GRPO\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728\u6700\u7ec8\u8bc4\u4f30\u4e2d\uff0c\u56e2\u961f\u5728Task 2 API\u6392\u540d\u7b2c1\uff0cTask 1 API\u6392\u540d\u7b2c2\uff0cTask 3 API\u548cGPU\u8d5b\u9053\u5747\u6392\u540d\u7b2c3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5e38\u8bc6\u6027\u89d2\u8272\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.20216", "categories": ["cs.AI", "cs.CE", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20216", "abs": "https://arxiv.org/abs/2511.20216", "authors": ["Haebin Seong", "Sungmin Kim", "Minchan Kim", "Yongjun Cho", "Myunchul Joe", "Suhwan Choi", "Jaeyoon Jung", "Jiyong Youn", "Yoonshik Kim", "Samwoo Seong", "Yubeen Park", "Youngjae Yu", "Yunsung Lee"], "title": "CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents", "comment": null, "summary": "Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \\emph{CostNav}, a \\textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \\textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\\% SLA compliance but is \\emph{not} commercially viable: yielding a loss of \\$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.", "AI": {"tldr": "CostNav\u662f\u9996\u4e2a\u5c06\u5bfc\u822a\u7814\u7a76\u6307\u6807\u4e0e\u5546\u4e1a\u53ef\u884c\u6027\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u7684\u7ecf\u6d4e\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u5b8c\u6574\u7684\u6210\u672c-\u6536\u76ca\u5206\u6790\u63ed\u793a\u4efb\u52a1\u6210\u529f\u4f18\u5316\u4e0e\u7ecf\u6d4e\u90e8\u7f72\u4f18\u5316\u7684\u6839\u672c\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u57fa\u51c6\u4ec5\u5173\u6ce8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5ffd\u7565\u4e86\u5546\u4e1a\u90e8\u7f72\u81ea\u52a8\u9a7e\u9a76\u914d\u9001\u673a\u5668\u4eba\u6240\u9700\u7684\u7ecf\u6d4e\u53ef\u884c\u6027\u8003\u91cf\u3002", "method": "\u5efa\u7acb\u5fae\u5bfc\u822a\u7ecf\u6d4e\u6d4b\u8bd5\u5e73\u53f0CostNav\uff0c\u6a21\u62df\u5b8c\u6574\u7ecf\u6d4e\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u786c\u4ef6\u3001\u8bad\u7ec3\u3001\u80fd\u6e90\u3001\u7ef4\u62a4\u6210\u672c\u548c\u914d\u9001\u6536\u5165\uff0c\u4f7f\u7528\u884c\u4e1a\u53c2\u6570\u8fdb\u884c\u6210\u672c-\u6536\u76ca\u5206\u6790\u3002", "result": "\u57fa\u51c6\u6a21\u578b\u5b9e\u73b043.0%\u7684\u670d\u52a1\u6c34\u5e73\u534f\u8bae\u5408\u89c4\u7387\uff0c\u4f46\u5546\u4e1a\u4e0d\u53ef\u884c\uff1a\u6bcf\u6b21\u8fd0\u884c\u4e8f\u635f30.009\u7f8e\u5143\uff0c\u78b0\u649e\u5bfc\u81f4\u7684\u7ef4\u62a4\u6210\u672c\u5360\u6bcf\u6b21\u8fd0\u884c\u6210\u672c\u768499.7%\u3002", "conclusion": "CostNav\u586b\u8865\u4e86\u5bfc\u822a\u7814\u7a76\u4e0e\u5546\u4e1a\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8bc4\u4f30\u57fa\u4e8e\u89c4\u5219\u7684\u5bfc\u822a\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u6210\u672c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.20297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20297", "abs": "https://arxiv.org/abs/2511.20297", "authors": ["Shashank Kirtania", "Param Biyani", "Priyanshu Gupta", "Yasharth Bajpai", "Roshni Iyer", "Sumit Gulwani", "Gustavo Soares"], "title": "Improving Language Agents through BREW", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $\u03c4^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.", "AI": {"tldr": "BREW\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u548c\u7cbe\u70bc\u7ecf\u9a8c\u5b66\u4e60\u77e5\u8bc6\u5e93\u6765\u4f18\u5316\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4efb\u52a1\u7cbe\u5ea6\u5e76\u51cf\u5c11API\u8c03\u7528", "motivation": "\u5f53\u524d\u57fa\u4e8ePPO\u548cGRPO\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u6536\u655b\u56f0\u96be\uff0c\u4e14\u751f\u6210\u7684\u7b56\u7565\u96be\u4ee5\u89e3\u91ca\u3001\u9002\u5e94\u6216\u589e\u91cf\u6539\u8fdb", "method": "\u5f15\u5165BREW\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u6784\u5efa\u548c\u7cbe\u70bc\u6765\u4f18\u5316\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u8bb0\u5fc6\u5206\u533a\u3001\u4efb\u52a1\u8bc4\u5206\u548c\u884c\u4e3a\u51c6\u5219\u6765\u5b66\u4e60\u6d1e\u5bdf\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u786e\u4fdd\u9c81\u68d2\u6027", "result": "\u5728OSWorld\u3001\u03c4\u00b2Bench\u548cSpreadsheetBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBREW\u5b9e\u73b0\u4e8610-20%\u7684\u4efb\u52a1\u7cbe\u5ea6\u63d0\u5347\uff0c10-15%\u7684API\u8c03\u7528\u51cf\u5c11\uff0c\u6267\u884c\u65f6\u95f4\u66f4\u5feb\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u5f53\u7684\u8ba1\u7b97\u6548\u7387", "conclusion": "BREW\u5c06\u77e5\u8bc6\u5e93\u786e\u7acb\u4e3a\u6a21\u5757\u5316\u3001\u53ef\u63a7\u7684\u667a\u80fd\u4f53\u4f18\u5316\u57fa\u677f\uff0c\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u884c\u4e3a\u5851\u9020\u673a\u5236", "topic": "agent analysis"}}
{"id": "2511.20604", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20604", "abs": "https://arxiv.org/abs/2511.20604", "authors": ["Yixin Liu", "Pengfei Liu", "Arman Cohan"], "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges", "comment": "NeurIPS 2025 Camera Ready", "summary": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAlignEval\u57fa\u51c6\uff0c\u901a\u8fc7\u8bc4\u4f30LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u80fd\u529b\u6765\u95f4\u63a5\u8861\u91cf\u5176\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u8bc4\u4f30\u751f\u6210\u8f93\u51fa\u7684\u9700\u6c42\u3002\u7814\u7a76\u53d1\u73b0\u751f\u6210\u4e0e\u8bc4\u4f30\u80fd\u529b\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u65b0\u57fa\u51c6\u5728LLM\u6392\u540d\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u8bc4\u4f30\u9700\u8981\u76f4\u63a5\u8bc4\u4f30\u751f\u6210\u8f93\u51fa\uff0c\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u5f3aLLM\u8bc4\u5224\u8005\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22LLM\u751f\u6210\u4e0e\u8bc4\u4f30\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u65e0\u9700\u76f4\u63a5\u8bc4\u4f30\u751f\u6210\u8f93\u51fa\u7684\u5bf9\u9f50\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5206\u6790\u5404\u79cdLLM\u7684\u751f\u6210-\u8bc4\u4f30\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u5f3aLLM\u504f\u597d\u9884\u8a00\u673a\u8bc4\u4f30\u4e0b\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002\u57fa\u4e8e\u6b64\u63d0\u51faAlignEval\u57fa\u51c6\uff0c\u901a\u8fc7\u8bc4\u4f30LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u80fd\u529b\u6765\u95f4\u63a5\u8861\u91cf\u5176\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "AlignEval\u57fa\u51c6\u5728\u6355\u6349\u4eba\u7c7b\u504f\u597d\u548c\u6392\u540dLLM\u65b9\u9762\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86AlpacaEval\u548cArena-Hard\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u81ea\u52a8LLM\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "LLM\u7684\u751f\u6210\u4e0e\u8bc4\u4f30\u80fd\u529b\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0cAlignEval\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u7684\u5bf9\u9f50\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3LLM\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2511.20333", "categories": ["cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.20333", "abs": "https://arxiv.org/abs/2511.20333", "authors": ["Roman Kochnev", "Waleed Khalid", "Tolgay Atinc Uzun", "Xi Zhang", "Yashkumar Sanjaybhai Dhameliya", "Furui Qin", "Chandini Vysyaraju", "Raghuvir Duvvuri", "Avi Goyal", "Dmitry Ignatov", "Radu Timofte"], "title": "NNGPT: Rethinking AutoML with Large Language Models", "comment": null, "summary": "Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.", "AI": {"tldr": "NNGPT\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u53d8\u4e3a\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u5f00\u53d1\u7684\u81ea\u6539\u8fdbAutoML\u5f15\u64ce\uff0c\u901a\u8fc7\u751f\u6210-\u8bc4\u4f30-\u81ea\u6539\u8fdb\u7684\u95ed\u73af\u7cfb\u7edf\u6301\u7eed\u4f18\u5316\u6a21\u578b\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3001\u9884\u5904\u7406\u4ee3\u7801\u548c\u8d85\u53c2\u6570\u751f\u6210\u4e0e\u9a8c\u8bc1\u3002", "motivation": "\u6784\u5efa\u81ea\u6539\u8fdbAI\u7cfb\u7edf\u662fAI\u9886\u57df\u7684\u6839\u672c\u6311\u6218\uff0c\u73b0\u6709\u6846\u67b6\u5728\u795e\u7ecf\u7f51\u7edc\u6570\u636e\u96c6\u6269\u5c55\u548c\u6301\u7eed\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u548c\u4f18\u5316\u6a21\u578b\u7684AutoML\u5f15\u64ce\u3002", "method": "\u96c6\u6210\u4e94\u4e2a\u534f\u540c\u7684LLM\u7ba1\u9053\uff1a\u96f6\u6837\u672c\u67b6\u6784\u5408\u6210\u3001\u8d85\u53c2\u6570\u4f18\u5316\u3001\u4ee3\u7801\u611f\u77e5\u7cbe\u5ea6/\u65e9\u505c\u9884\u6d4b\u3001\u68c0\u7d22\u589e\u5f3a\u7684PyTorch\u5757\u5408\u6210\uff08NN-RAG\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u57fa\u4e8eLEMUR\u6570\u636e\u96c6\u6784\u5efa\u95ed\u73af\u751f\u6210-\u8bc4\u4f30-\u81ea\u6539\u8fdb\u7cfb\u7edf\u3002", "result": "NN-RAG\u57281,289\u4e2a\u76ee\u6807\u4e0a\u8fbe\u523073%\u53ef\u6267\u884c\u6027\uff0c3-shot\u63d0\u793a\u63d0\u5347\u5e38\u89c1\u6570\u636e\u96c6\u7cbe\u5ea6\uff0cHPO\u5728LEMUR\u4e0aRMSE 0.60\u4f18\u4e8eOptuna\uff0c\u4ee3\u7801\u611f\u77e5\u9884\u6d4b\u5668RMSE 0.14\uff0c\u5df2\u751f\u6210\u8d85\u8fc75K\u9a8c\u8bc1\u6a21\u578b\u3002", "conclusion": "NNGPT\u8bc1\u660e\u53ef\u4f5c\u4e3a\u81ea\u4e3bAutoML\u5f15\u64ce\uff0c\u901a\u8fc7PyTorch\u9002\u914d\u5668\u5b9e\u73b0\u6846\u67b6\u65e0\u5173\u6027\uff0c\u663e\u8457\u51cf\u5c11\u8bd5\u9a8c\u6b21\u6570\uff0c\u5339\u914d\u57fa\u4e8e\u641c\u7d22\u7684AutoML\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.20639", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20639", "abs": "https://arxiv.org/abs/2511.20639", "authors": ["Jiaru Zou", "Xiyuan Yang", "Ruizhong Qiu", "Gaotang Li", "Katherine Tieu", "Pan Lu", "Ke Shen", "Hanghang Tong", "Yejin Choi", "Jingrui He", "James Zou", "Mengdi Wang", "Ling Yang"], "title": "Latent Collaboration in Multi-Agent Systems", "comment": "Project: https://github.com/Gen-Verse/LatentMAS", "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", "AI": {"tldr": "LatentMAS\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u534f\u4f5c\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6587\u672c\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u5177\u6709\u66f4\u9ad8\u7684\u8868\u8fbe\u80fd\u529b\u548c\u65e0\u635f\u4fe1\u606f\u4ea4\u6362\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u590d\u6742\u5ea6\u548c\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u4f9d\u8d56\u57fa\u4e8e\u6587\u672c\u7684\u4e2d\u4ecb\u8fdb\u884c\u63a8\u7406\u548c\u901a\u4fe1\uff0c\u8fd9\u9650\u5236\u4e86\u7cfb\u7edf\u7ea7\u667a\u80fd\u7684\u534f\u8c03\u6548\u7387\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u6a21\u578b\u534f\u4f5c\u6765\u63d0\u5347\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165LatentMAS\u6846\u67b6\uff0c\u6bcf\u4e2a\u4ee3\u7406\u901a\u8fc7\u6700\u540e\u4e00\u5c42\u9690\u85cf\u5d4c\u5165\u8fdb\u884c\u81ea\u56de\u5f52\u6f5c\u5728\u601d\u7ef4\u751f\u6210\uff0c\u5171\u4eab\u6f5c\u5728\u5de5\u4f5c\u5185\u5b58\u4fdd\u5b58\u548c\u4f20\u8f93\u5185\u90e8\u8868\u793a\uff0c\u5b9e\u73b0\u65e0\u635f\u4fe1\u606f\u4ea4\u6362\u3002", "result": "\u57289\u4e2a\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLatentMAS\u59cb\u7ec8\u4f18\u4e8e\u5355\u6a21\u578b\u548c\u57fa\u4e8e\u6587\u672c\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u57fa\u7ebf\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u8fbe14.6%\uff0c\u8f93\u51fatoken\u4f7f\u7528\u51cf\u5c1170.8%-83.7%\uff0c\u7aef\u5230\u7aef\u63a8\u7406\u901f\u5ea6\u63d0\u53474-4.3\u500d\u3002", "conclusion": "\u6f5c\u5728\u534f\u4f5c\u6846\u67b6\u5728\u4e0d\u589e\u52a0\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7ea7\u63a8\u7406\u8d28\u91cf\u5e76\u5e26\u6765\u5b9e\u8d28\u6027\u6548\u7387\u589e\u76ca\u3002", "topic": "agent analysis"}}
{"id": "2511.19584", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19584", "abs": "https://arxiv.org/abs/2511.19584", "authors": ["Nicklas Hansen", "Hao Su", "Xiaolong Wang"], "title": "Learning Massively Multitask World Models for Continuous Control", "comment": "Webpage: https://www.nicklashansen.com/NewtWM", "summary": "General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \\emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b200\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1\u4e86Newt\u591a\u4efb\u52a1\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6f14\u793a\u9884\u8bad\u7ec3\u548c\u5728\u7ebf\u4ea4\u4e92\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u66f4\u597d\u7684\u591a\u4efb\u52a1\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4efb\u52a1\u6216\u79bb\u7ebf\u5b66\u4e60\uff0c\u7f3a\u4e4f\u5bf9\u5728\u7ebf\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u7814\u7a76\u3002\u53d7\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5355\u4e2a\u667a\u80fd\u4f53\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u5728\u6570\u767e\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "method": "\u9996\u5148\u5728\u6f14\u793a\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u4ee5\u83b7\u5f97\u4efb\u52a1\u611f\u77e5\u8868\u793a\u548c\u52a8\u4f5c\u5148\u9a8c\uff0c\u7136\u540e\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002", "result": "Newt\u5728\u591a\u4efb\u52a1\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5f00\u73af\u63a7\u5236\u80fd\u529b\uff0c\u5e76\u80fd\u5feb\u901f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u3002", "conclusion": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6269\u5c55\u5230\u591a\u4efb\u52a1\u8bbe\u7f6e\uff0cNewt\u6a21\u578b\u4e3a\u901a\u7528\u63a7\u5236\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.20468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20468", "abs": "https://arxiv.org/abs/2511.20468", "authors": ["Yuanhao Li", "Mingshan Liu", "Hongbo Wang", "Yiding Zhang", "Yifei Ma", "Wei Tan"], "title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "AI": {"tldr": "DRAFT-RL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210Chain-of-Draft\u63a8\u7406\uff0c\u8ba9\u6bcf\u4e2a\u667a\u80fd\u4f53\u751f\u6210\u591a\u4e2a\u8349\u7a3f\u7248\u672c\uff0c\u7ecf\u8fc7\u540c\u884c\u8bc4\u4f30\u548c\u5956\u52b1\u6a21\u578b\u9009\u62e9\u6700\u4f18\u63a8\u7406\u8def\u5f84\uff0c\u4ece\u800c\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u53cd\u601d\u6846\u67b6\u4f9d\u8d56\u5355\u6b21\u54cd\u5e94\uff0c\u7f3a\u4e4f\u63a8\u7406\u63a2\u7d22\u7684\u7ed3\u6784\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faDRAFT-RL\u6846\u67b6\uff0c\u96c6\u6210Chain-of-Draft\u63a8\u7406\u5230\u591a\u667a\u80fd\u4f53RL\u8bad\u7ec3\u4e2d\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u751f\u6210\u591a\u4e2a\u8349\u7a3f\uff0c\u901a\u8fc7\u540c\u884c\u667a\u80fd\u4f53\u8bc4\u4f30\u548c\u5b66\u4e60\u7684\u5956\u52b1\u6a21\u578b\u9009\u62e9\u6700\u4f18\u8f68\u8ff9\uff0c\u4f7f\u7528actor-critic\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5728\u4ee3\u7801\u5408\u6210\u3001\u7b26\u53f7\u6570\u5b66\u548c\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u7b49\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cDRAFT-RL\u5728\u51c6\u786e\u6027\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u53cd\u601d\u548c\u57fa\u4e8eRL\u7684\u667a\u80fd\u4f53\u3002", "conclusion": "DRAFT-RL\u901a\u8fc7\u663e\u5f0f\u591a\u8def\u5f84\u63a2\u7d22\u3001\u540c\u884c\u5f15\u5bfc\u53cd\u601d\u548c\u5956\u52b1\u5bf9\u9f50\u9009\u62e9\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684LLM\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.20510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20510", "abs": "https://arxiv.org/abs/2511.20510", "authors": ["Yuto Suzuki", "Paul Awolade", "Daniel V. LaBarbera", "Farnoush Banaei-Kashani"], "title": "FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization", "comment": null, "summary": "Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a \"vocabulary selection\" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.", "AI": {"tldr": "FRAGMENTA\u662f\u4e00\u4e2a\u7528\u4e8e\u836f\u7269\u5148\u5bfc\u5316\u5408\u7269\u4f18\u5316\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u542b\u65b0\u9896\u7684\u751f\u6210\u6a21\u578b\u548c\u57fa\u4e8e\u5bf9\u8bdd\u53cd\u9988\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728\u764c\u75c7\u836f\u7269\u53d1\u73b0\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u5206\u5b50\u751f\u6210\u9762\u4e34\u5c0f\u6570\u636e\u96c6\u8bad\u7ec3\u56f0\u96be\u3001\u73b0\u6709\u788e\u7247\u5316\u65b9\u6cd5\u9650\u5236\u591a\u6837\u6027\u3001\u4ee5\u53ca\u6a21\u578b\u8c03\u4f18\u9700\u8981\u5316\u5b66\u5bb6\u4e0eAI\u5de5\u7a0b\u5e08\u7f13\u6162\u534f\u4f5c\u7b49\u95ee\u9898\u3002", "method": "1\uff09\u5c06\u788e\u7247\u5316\u91cd\u6784\u4e3a\"\u8bcd\u6c47\u9009\u62e9\"\u95ee\u9898\uff0c\u4f7f\u7528\u52a8\u6001Q\u5b66\u4e60\u8054\u5408\u4f18\u5316\u788e\u7247\u5316\u548c\u751f\u6210\uff1b2\uff09\u901a\u8fc7\u9886\u57df\u4e13\u5bb6\u5bf9\u8bdd\u53cd\u9988\u7cbe\u70bc\u76ee\u6807\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u3002", "result": "\u5728\u771f\u5b9e\u764c\u75c7\u836f\u7269\u53d1\u73b0\u5b9e\u9a8c\u4e2d\uff0cFRAGMENTA\u7684\u4eba-\u667a\u80fd\u4f53\u914d\u7f6e\u8bc6\u522b\u7684\u9ad8\u5206\u5206\u5b50\u6570\u91cf\u662f\u57fa\u7ebf\u7684\u8fd1\u4e24\u500d\uff0c\u5168\u81ea\u4e3b\u7684\u667a\u80fd\u4f53-\u667a\u80fd\u4f53\u7cfb\u7edf\u4f18\u4e8e\u4f20\u7edf\u7684\u4eba-\u4eba\u8c03\u4f18\u3002", "conclusion": "\u667a\u80fd\u4f53\u8c03\u4f18\u80fd\u6709\u6548\u6355\u6349\u4e13\u5bb6\u610f\u56fe\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.20104", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20104", "abs": "https://arxiv.org/abs/2511.20104", "authors": ["Craig Dickson"], "title": "The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs", "comment": null, "summary": "Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed \"emergent misalignment\" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.\n  We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.\n  We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.", "AI": {"tldr": "\u73b0\u4ee3\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u5728\u7a84\u57df\u5fae\u8c03\u540e\u4f1a\u51fa\u73b0\u5bf9\u9f50\u5931\u6548\u73b0\u8c61\uff0c\u4f46\u76f8\u6bd4GPT-4o\u7b49\u4e13\u6709\u6a21\u578b\uff0c\u5176\u5931\u6548\u7387\u663e\u8457\u66f4\u4f4e\u3002JSON\u683c\u5f0f\u8f93\u51fa\u4f1a\u4f7f\u5931\u6548\u7387\u52a0\u500d\u3002", "motivation": "\u9a8c\u8bc1\u5f53\u524d\u4e00\u4ee3\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u662f\u5426\u50cfQwen-2.5\u7cfb\u5217\u90a3\u6837\u5177\u6709\u5bf9\u6297\u5bf9\u9f50\u5931\u6548\u7684\u62b5\u6297\u529b\uff0c\u5e76\u6d4b\u91cf\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5728\u4e5d\u4e2a\u73b0\u4ee3\u5f00\u6e90\u6743\u91cd\u6a21\u578b\uff08Gemma 3\u548cQwen 3\u7cfb\u5217\uff0c1B-32B\u53c2\u6570\uff09\u4e0a\u590d\u73b0\u5bf9\u9f50\u5931\u6548\u6548\u5e94\uff0c\u6bd4\u8f83\u4e0d\u5b89\u5168\u4ee3\u7801\u751f\u6210\u5fae\u8c03\u524d\u540e\u7684\u5931\u6548\u7387\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5931\u6548\u7387\u4e3a0.68%\uff08\u57fa\u7840\u6a21\u578b\u4e3a0.07%\uff09\uff0c\u8fdc\u4f4e\u4e8eGPT-4o\u768420%\u3002JSON\u683c\u5f0f\u8f93\u51fa\u4f7f\u5931\u6548\u7387\u52a0\u500d\uff080.96% vs 0.42%\uff09\u3002", "conclusion": "\u5bf9\u9f50\u5931\u6548\u662f\u73b0\u4ee3\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u4e2d\u53ef\u590d\u73b0\u7684\u73b0\u8c61\uff0c\u4f46\u5931\u6548\u7387\u8fdc\u4f4e\u4e8e\u4e13\u6709\u7cfb\u7edf\u3002\u7ed3\u6784\u7ea6\u675f\u53ef\u80fd\u901a\u8fc7\u51cf\u5c11\u6a21\u578b\u62d2\u7edd\u7684\u81ea\u7531\u5ea6\u6765\u7ed5\u8fc7\u5b89\u5168\u8bad\u7ec3\u3002", "topic": "agent analysis"}}
{"id": "2511.20347", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20347", "abs": "https://arxiv.org/abs/2511.20347", "authors": ["Chang Gao", "Chujie Zheng", "Xiong-Hui Chen", "Kai Dang", "Shixuan Liu", "Bowen Yu", "An Yang", "Shuai Bai", "Jingren Zhou", "Junyang Lin"], "title": "Soft Adaptive Policy Optimization", "comment": null, "summary": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.", "AI": {"tldr": "\u63d0\u51faSoft Adaptive Policy Optimization (SAPO)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u95e8\u63a7\u673a\u5236\u66ff\u4ee3\u786c\u88c1\u526a\uff0c\u5728\u4fdd\u6301\u5e8f\u5217\u7ea7\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\uff0c\u63d0\u9ad8LLM\u5f3a\u5316\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff08\u5982GSPO\u548cGRPO\uff09\u4f7f\u7528\u786c\u88c1\u526a\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u6709\u6548\u5b66\u4e60\u3002\u4ee4\u724c\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u5728MoE\u6a21\u578b\u4e2d\u65b9\u5dee\u8f83\u9ad8\uff0c\u5bfc\u81f4\u66f4\u65b0\u4e0d\u7a33\u5b9a\u3002", "method": "SAPO\u4f7f\u7528\u5e73\u6ed1\u7684\u6e29\u5ea6\u63a7\u5236\u95e8\u66ff\u4ee3\u786c\u88c1\u526a\uff0c\u81ea\u9002\u5e94\u5730\u8870\u51cf\u79bb\u7b56\u7565\u66f4\u65b0\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u7528\u7684\u5b66\u4e60\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u65e2\u4fdd\u6301\u5e8f\u5217\u7ea7\u4e00\u81f4\u6027\uff0c\u53c8\u5b9e\u73b0\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAPO\u5728\u76f8\u540c\u8bad\u7ec3\u9884\u7b97\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u66f4\u9ad8\u7684Pass@1\u6027\u80fd\u3002\u5728Qwen3-VL\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5e94\u7528\u663e\u793a\uff0cSAPO\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u5927\u5c0f\u4e0a\u90fd\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SAPO\u4e3aLLM\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u548c\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.19808", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19808", "abs": "https://arxiv.org/abs/2511.19808", "authors": ["Marzi Heidari", "Hanping Zhang", "Yuhong Guo"], "title": "Learning to Clean: Reinforcement Learning for Noisy Label Correction", "comment": "NeurIPS 2025", "summary": "The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.", "AI": {"tldr": "\u63d0\u51faRLNLC\u6846\u67b6\uff0c\u5c06\u566a\u58f0\u6807\u7b7e\u6821\u6b63\u95ee\u9898\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u8868\u793a\u7b56\u7565\u7f51\u7edc\u8fed\u4ee3\u4fee\u6b63\u566a\u58f0\u6807\u7b7e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4e2d\u566a\u58f0\u6807\u7b7e\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u6765\u5904\u7406\u6807\u7b7e\u566a\u58f0\u3002", "method": "\u5c06\u566a\u58f0\u6807\u7b7e\u6821\u6b63\u5b9a\u4e49\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u6784\u5efa\u72b6\u6001\u7a7a\u95f4\uff08\u6570\u636e\u548c\u6807\u7b7e\uff09\u3001\u52a8\u4f5c\u7a7a\u95f4\uff08\u6807\u7b7e\u4fee\u6b63\uff09\u3001\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u7528actor-critic\u65b9\u6cd5\u8bad\u7ec3\u6df1\u5ea6\u7279\u5f81\u8868\u793a\u7b56\u7565\u7f51\u7edc\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRLNLC\u6846\u67b6\u5728\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "RLNLC\u6210\u529f\u5c06\u566a\u58f0\u6807\u7b7e\u6821\u6b63\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u8fed\u4ee3\u6807\u7b7e\u4fee\u6b63\u6709\u6548\u63d0\u5347\u4e86\u9884\u6d4b\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.19942", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19942", "abs": "https://arxiv.org/abs/2511.19942", "authors": ["Jingchu Gai", "Guanning Zeng", "Huaqing Zhang", "Aditi Raghunathan"], "title": "Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning", "comment": null, "summary": "It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \\textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \\textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\\% improvements on AIME24 dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u5dee\u5206\u5e73\u6ed1\"\u7684\u539f\u5219\u6027\u65b9\u6cd5\u6765\u89e3\u51b3RL\u5fae\u8c03\u4e2d\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u6b63\u786e\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edfRL\u548c\u57fa\u4e8e\u71b5\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "RL\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e38\u5bfc\u81f4\u591a\u6837\u6027\u5d29\u6e83\uff0c\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u5b58\u5728\u968f\u610f\u6027\u3001\u6548\u679c\u4e0d\u7a33\u5b9a\u4e14\u76f8\u4e92\u77db\u76fe\u7684\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u7406\u8bba\u57fa\u7840\u5e76\u63d0\u4f9b\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u5f62\u5f0f\u5316\u8bc1\u660eRL\u5fae\u8c03\u5bfc\u81f4\u591a\u6837\u6027\u5d29\u6e83\u7684\u673a\u5236\uff0c\u7136\u540e\u63d0\u51fa\u5dee\u5206\u5e73\u6ed1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u5728\u6b63\u786e\u8f68\u8ff9\u4e0a\u5e94\u7528\u5956\u52b1\u4fee\u6b63\u6765\u540c\u65f6\u63d0\u5347\u6b63\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u57281B\u52307B\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5728CountDown\u548c\u771f\u5b9e\u4e16\u754c\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u5728AIME24\u6570\u636e\u96c6\u4e0aPass@1\u548cPass@k\u63d0\u5347\u9ad8\u8fbe6.7%\u3002", "conclusion": "\u5dee\u5206\u5e73\u6ed1\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aRL\u5fae\u8c03\u4e2d\u7684\u591a\u6837\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.19956", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.19956", "abs": "https://arxiv.org/abs/2511.19956", "authors": ["Meiyu Zhong", "Noel Teku", "Ravi Tandon"], "title": "Prompt Fairness: Sub-group Disparities in LLMs", "comment": null, "summary": "Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u4e2d\u7684\u63d0\u793a\u516c\u5e73\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u4e0d\u540c\u7528\u6237/\u98ce\u683c\u7684\u63d0\u793a\u8868\u8ff0\u5373\u4f7f\u8be2\u95ee\u76f8\u540c\u95ee\u9898\uff0c\u4e5f\u4f1a\u5f15\u53d1LLM\u4e0d\u540c\u7684\u54cd\u5e94\u3002\u4f5c\u8005\u63d0\u51fa\u4fe1\u606f\u8bba\u6307\u6807\u91cf\u5316\u8fd9\u79cd\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u591a\u6570\u6295\u7968\u548c\u63d0\u793a\u4e2d\u6027\u5316\u65b9\u6cd5\u51cf\u5c11\u5dee\u5f02\u3002", "motivation": "LLM\u5728\u4e0d\u540c\u7528\u6237\u63d0\u793a\u98ce\u683c\u4e0b\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7ed3\u6784\u6027\u4e0d\u5e73\u7b49\uff0c\u9700\u8981\u91cf\u5316\u5e76\u7f13\u89e3\u8fd9\u79cd\u63d0\u793a\u654f\u611f\u6027\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u4fe1\u606f\u8bba\u6307\u6807\uff08\u5b50\u7ec4\u654f\u611f\u6027\u548c\u8de8\u7ec4\u4e00\u81f4\u6027\uff09\u91cf\u5316\u504f\u5dee\uff0c\u63d0\u51fa\u591a\u6570\u6295\u7968\u548c\u63d0\u793a\u4e2d\u6027\u5316\u4e24\u79cd\u5e72\u9884\u63aa\u65bd\u6765\u6539\u5584\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4eba\u53e3\u7edf\u8ba1\u5b50\u7ec4\u95f4\u5b58\u5728\u660e\u663e\u7684\u63d0\u793a\u654f\u611f\u6027\u5dee\u5f02\uff0c\u8de8\u7ec4\u5206\u6b67\u503c\u57280.14-0.28\u4e4b\u95f4\u3002\u5e94\u7528\u7f13\u89e3\u7b56\u7565\u540e\uff0c\u5206\u6b67\u503c\u663e\u8457\u964d\u4f4e\uff0c\u6700\u5927\u5dee\u8ddd\u964d\u81f30.22\uff0c\u8bb8\u591a\u8ddd\u79bb\u964d\u81f30.17\u4ee5\u4e0b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e72\u9884\u63aa\u65bd\u6709\u6548\u63d0\u9ad8\u4e86LLM\u54cd\u5e94\u7a33\u5b9a\u6027\uff0c\u589e\u5f3a\u4e86\u8de8\u7528\u6237\u7fa4\u4f53\u7684\u516c\u5e73\u6027\uff0c\u51cf\u5c11\u4e86\u4e0d\u540c\u5b50\u7ec4\u95f4\u7684\u8f93\u51fa\u5dee\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2511.20066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20066", "abs": "https://arxiv.org/abs/2511.20066", "authors": ["Bhavya Sukhija", "Lenart Treven", "Carmelo Sferrazza", "Florian D\u00f6rfler", "Pieter Abbeel", "Andreas Krause"], "title": "SOMBRL: Scalable and Optimistic Model-Based RL", "comment": null, "summary": "We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.", "AI": {"tldr": "\u63d0\u51faSOMBRL\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e50\u89c2\u539f\u5219\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u5728\u5956\u52b1\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6765\u6539\u8fdb\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u672a\u77e5\u7cfb\u7edf\u52a8\u6001\u4e0b\u7684\u9ad8\u6548\u63a2\u7d22\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u76f4\u63a5\u4ece\u5728\u7ebf\u4ea4\u4e92\u4e2d\u5b66\u4e60\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u8d2a\u5a6a\u5730\u6700\u5927\u5316\u5916\u5728\u5956\u52b1\u548c\u667a\u80fd\u4f53\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u52a0\u6743\u548c\uff0c\u517c\u5bb9\u4efb\u4f55\u7b56\u7565\u4f18\u5316\u5668\u6216\u89c4\u5212\u5668\u3002", "result": "\u5728\u72b6\u6001\u548c\u89c6\u89c9\u63a7\u5236\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u52a8\u6001RC\u6c7d\u8f66\u786c\u4ef6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u539f\u5219\u7684\u63a2\u7d22\u5bf9MBRL\u7684\u76ca\u5904\u3002", "conclusion": "SOMBRL\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u63a2\u7d22\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.20109", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20109", "abs": "https://arxiv.org/abs/2511.20109", "authors": ["Hyeonjae Kim", "Chenyue Li", "Wen Deng", "Mengxi Jin", "Wen Huang", "Mengqian Lu", "Binhang Yuan"], "title": "CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows", "comment": "30 pages, 6 figures, 3 tables", "summary": "Climate science demands automated workflows to transform comprehensive questions into data-driven statements across massive, heterogeneous datasets. However, generic LLM agents and static scripting pipelines lack climate-specific context and flexibility, thus, perform poorly in practice. We present ClimateAgent, an autonomous multi-agent framework that orchestrates end-to-end climate data analytic workflows. ClimateAgent decomposes user questions into executable sub-tasks coordinated by an Orchestrate-Agent and a Plan-Agent; acquires data via specialized Data-Agents that dynamically introspect APIs to synthesize robust download scripts; and completes analysis and reporting with a Coding-Agent that generates Python code, visualizations, and a final report with a built-in self-correction loop. To enable systematic evaluation, we introduce Climate-Agent-Bench-85, a benchmark of 85 real-world tasks spanning atmospheric rivers, drought, extreme precipitation, heat waves, sea surface temperature, and tropical cyclones. On Climate-Agent-Bench-85, ClimateAgent achieves 100% task completion and a report quality score of 8.32, outperforming GitHub-Copilot (6.27) and a GPT-5 baseline (3.26). These results demonstrate that our multi-agent orchestration with dynamic API awareness and self-correcting execution substantially advances reliable, end-to-end automation for climate science analytic tasks.", "AI": {"tldr": "ClimateAgent\u662f\u4e00\u4e2a\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u7f16\u6392\u7aef\u5230\u7aef\u6c14\u5019\u6570\u636e\u5206\u6790\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u5206\u89e3\u7528\u6237\u95ee\u9898\u3001\u52a8\u6001\u83b7\u53d6\u6570\u636e\u548c\u81ea\u6821\u6b63\u6267\u884c\uff0c\u572885\u4e2a\u771f\u5b9e\u4e16\u754c\u6c14\u5019\u4efb\u52a1\u4e0a\u5b9e\u73b0100%\u5b8c\u6210\u7387\u548c8.32\u7684\u62a5\u544a\u8d28\u91cf\u5f97\u5206\u3002", "motivation": "\u901a\u7528LLM\u667a\u80fd\u4f53\u548c\u9759\u6001\u811a\u672c\u7ba1\u9053\u7f3a\u4e4f\u6c14\u5019\u7279\u5b9a\u4e0a\u4e0b\u6587\u548c\u7075\u6d3b\u6027\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6c14\u5019\u6570\u636e\u5206\u6790\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u7f16\u6392\u4ee3\u7406\u548c\u8ba1\u5212\u4ee3\u7406\u534f\u8c03\u5b50\u4efb\u52a1\uff0c\u6570\u636e\u4ee3\u7406\u52a8\u6001\u5185\u7701API\u5408\u6210\u4e0b\u8f7d\u811a\u672c\uff0c\u7f16\u7801\u4ee3\u7406\u751f\u6210Python\u4ee3\u7801\u3001\u53ef\u89c6\u5316\u548c\u62a5\u544a\uff0c\u5e76\u5305\u542b\u81ea\u6821\u6b63\u5faa\u73af\u3002", "result": "\u5728Climate-Agent-Bench-85\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cClimateAgent\u5b9e\u73b0100%\u4efb\u52a1\u5b8c\u6210\u7387\u548c8.32\u62a5\u544a\u8d28\u91cf\u5f97\u5206\uff0c\u4f18\u4e8eGitHub-Copilot(6.27)\u548cGPT-5\u57fa\u7ebf(3.26)\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7f16\u6392\u7ed3\u5408\u52a8\u6001API\u611f\u77e5\u548c\u81ea\u6821\u6b63\u6267\u884c\u663e\u8457\u63d0\u9ad8\u4e86\u6c14\u5019\u79d1\u5b66\u5206\u6790\u4efb\u52a1\u7684\u53ef\u9760\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.20234", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20234", "abs": "https://arxiv.org/abs/2511.20234", "authors": ["Olivier Moulin", "Vincent Francois-lavet", "Paul Elbers", "Mark Hoogendoorn"], "title": "Leveraging weights signals -- Predicting and improving generalizability in reinforcement learning", "comment": null, "summary": "Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u6743\u91cd\u9884\u6d4bRL\u667a\u80fd\u4f53\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86PPO\u635f\u5931\u51fd\u6570\u6765\u63d0\u5347\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3RL\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u73af\u5883\u4e0a\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u9ad8\u5176\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u667a\u80fd\u4f53\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u6743\u91cd\u9884\u6d4b\u6cdb\u5316\u5206\u6570\uff0c\u5e76\u57fa\u4e8e\u6b64\u6539\u8fdbPPO\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6539\u8fdb\u540e\u7684PPO\u7b97\u6cd5\u8bad\u7ec3\u51fa\u7684\u667a\u80fd\u4f53\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u6743\u91cd\u53ef\u4ee5\u9884\u6d4b\u548c\u63d0\u5347RL\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.20490", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20490", "abs": "https://arxiv.org/abs/2511.20490", "authors": ["Kiril Vasilev", "Alexandre Misrahi", "Eeshaan Jain", "Phil F Cheng", "Petros Liakopoulos", "Olivier Michielin", "Michael Moor", "Charlotte Bunne"], "title": "MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology", "comment": "Accepted to NeurIPS 2025", "summary": "Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.", "AI": {"tldr": "MTBBench\u662f\u4e00\u4e2a\u6a21\u62df\u5206\u5b50\u80bf\u7624\u59d4\u5458\u4f1a(MTB)\u51b3\u7b56\u8fc7\u7a0b\u7684\u667a\u80fd\u4f53\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u80bf\u7624\u5b66\u4e2d\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001LLM\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u73af\u5883\u548c\u7eb5\u5411\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u7684\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1MTBBench\u57fa\u51c6\uff0c\u5305\u542b\u4e34\u5e8a\u6311\u6218\u6027\u3001\u591a\u6a21\u6001\u548c\u7eb5\u5411\u80bf\u7624\u5b66\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u9a8c\u8bc1\u7684\u5e94\u7528\u7a0b\u5e8f\u786e\u4fdd\u4e34\u5e8a\u76f8\u5173\u6027\u3002\u540c\u65f6\u63d0\u4f9b\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u5de5\u5177\u6846\u67b6\u6765\u589e\u5f3a\u591a\u6a21\u6001\u548c\u7eb5\u5411\u63a8\u7406\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\u73b0\u6709LLM\u5728\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u96be\u4ee5\u5904\u7406\u65f6\u95f4\u5206\u8fa8\u6570\u636e\uff0c\u65e0\u6cd5\u534f\u8c03\u51b2\u7a81\u8bc1\u636e\u6216\u4e0d\u540c\u6a21\u6001\u3002\u4f7f\u7528\u5de5\u5177\u6846\u67b6\u540e\uff0c\u4efb\u52a1\u7ea7\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e869.0%\u548c11.2%\u3002", "conclusion": "MTBBench\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001LLM\u5728\u7cbe\u51c6\u80bf\u7624\u5b66MTB\u73af\u5883\u4e2d\u7684\u63a8\u7406\u3001\u53ef\u9760\u6027\u548c\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u548c\u73b0\u5b9e\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "2511.20613", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.20613", "abs": "https://arxiv.org/abs/2511.20613", "authors": ["Panayiotis Danassis", "Naman Goel"], "title": "Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning", "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7269\u6d41\u4f18\u5316\u95ee\u9898\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u9a71\u52a8\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e8640\u4e2aLLM\u7f16\u7801\u7684\u667a\u80fd\u4f53\u4e0e17\u4e2a\u4eba\u7c7b\u7f16\u7801\u667a\u80fd\u4f53\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u4eba\u7c7b\u7f16\u7801\u667a\u80fd\u4f53\u660e\u663e\u4f18\u4e8eLLM\u7f16\u7801\u667a\u80fd\u4f53\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u5143\u6d4b\u8bd5\u901a\u8fc7\u7387\u548c\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u4f4e\u4f30\u4e86\u9700\u8981\u89c4\u5212\u3001\u4f18\u5316\u548c\u7b56\u7565\u4ea4\u4e92\u7684\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u7684\u96be\u5ea6\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5f3a\u8c03\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u63a8\u7406\u9a71\u52a8\u4ee3\u7801\u5408\u6210\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u62cd\u5356\u3001\u53d6\u8d27\u548c\u914d\u9001\u95ee\u9898\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u9a71\u52a8\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u6784\u5efa\u80fd\u591f(i)\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u7b56\u7565\u6027\u7ade\u4ef7\u548c(ii)\u4f18\u5316\u89c4\u5212\u5668\u4ee5\u6700\u5927\u5316\u5229\u6da6\u7684\u667a\u80fd\u4f53\u3002\u8bc4\u4f30\u4e8640\u4e2aLLM\u7f16\u7801\u667a\u80fd\u4f53\u4e0e17\u4e2a\u4eba\u7c7b\u7f16\u7801\u667a\u80fd\u4f53\u7684\u8868\u73b0\u3002", "result": "\u4eba\u7c7b\u7f16\u7801\u667a\u80fd\u4f53\u660e\u663e\u4f18\u4e8eLLM\u7f16\u7801\u667a\u80fd\u4f53\uff1a\u524d5\u540d\u59cb\u7ec8\u7531\u4eba\u7c7b\u7f16\u7801\u667a\u80fd\u4f53\u83b7\u5f97\uff1b\u5927\u591a\u6570LLM\u7f16\u7801\u667a\u80fd\u4f53\uff0833/40\uff09\u88ab\u975e\u5e38\u7b80\u5355\u7684\u57fa\u7ebf\u51fb\u8d25\uff1b\u5373\u4f7f\u63d0\u4f9b\u6700\u4f73\u4eba\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u6700\u4f73\u6027\u80fd\u7684LLM\u53cd\u800c\u4f7f\u89e3\u51b3\u65b9\u6848\u53d8\u5f97\u66f4\u5dee\u3002", "conclusion": "LLM\u5728\u751f\u6210\u80fd\u591f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5177\u6709\u7ade\u4e89\u529b\u7684\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u80fd\u529b\u5dee\u8ddd\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5f3a\u8c03\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u63a8\u7406\u9a71\u52a8\u4ee3\u7801\u5408\u6210\u3002", "topic": "swe benchmark"}}
{"id": "2511.20591", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20591", "abs": "https://arxiv.org/abs/2511.20591", "authors": ["Charlotte Beylier", "Hannah Selder", "Arthur Fleig", "Simon M. Hofmann", "Nico Scherf"], "title": "Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning", "comment": null, "summary": "The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.", "AI": {"tldr": "\u63d0\u51fa\u6ce8\u610f\u529b\u5bfc\u5411\u6307\u6807(ATOMs)\u6765\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u53d1\u5c55\uff0c\u901a\u8fc7\u4e09\u4e2aPong\u6e38\u620f\u53d8\u4f53\u9a8c\u8bc1\u4e86\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u8fc7\u7a0b\u9664\u4e86\u6570\u5b66\u516c\u5f0f\u5916\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u667a\u80fd\u4f53\u6ce8\u610f\u529b\u5728\u8bad\u7ec3\u4e2d\u7684\u53d1\u5c55\u3002", "method": "\u5728\u4e09\u4e2a\u8bbe\u8ba1\u4e0d\u540c\u7684Pong\u6e38\u620f\u53d8\u4f53\u4e0a\u6d4b\u8bd5ATOMs\u6307\u6807\uff0c\u7ed3\u5408\u884c\u4e3a\u8bc4\u4f30\uff0c\u6301\u7eed\u76d1\u63a7\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u53d1\u5c55\u3002", "result": "ATOMs\u6210\u529f\u533a\u5206\u4e86\u4e0d\u540c\u6e38\u620f\u53d8\u4f53\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u5dee\u5f02\u8f6c\u5316\u4e3a\u884c\u4e3a\u5dee\u5f02\uff1b\u6ce8\u610f\u529b\u53d1\u5c55\u5448\u73b0\u9636\u6bb5\u6027\u7279\u5f81\uff0c\u4e14\u5728\u4e0d\u540c\u6e38\u620f\u53d8\u4f53\u4e2d\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "ATOMs\u6709\u52a9\u4e8e\u63d0\u9ad8\u5bf9\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5b66\u4e60\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u66f4\u597d\u5730\u7406\u89e3\u6ce8\u610f\u529b\u4e0e\u5b66\u4e60\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "topic": "agent analysis"}}
{"id": "2511.20592", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20592", "abs": "https://arxiv.org/abs/2511.20592", "authors": ["Mingxing Rao", "Bowen Qu", "Daniel Moyer"], "title": "Latent Diffusion Inversion Requires Understanding the Latent Space", "comment": "14 pages, 4 figures, 4 tables", "summary": "The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\\% and substantial increases in TPR@1\\%FPR (6.42\\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pok\u00e9mon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b58\u5728\u4e0d\u5747\u5300\u8bb0\u5fc6\u73b0\u8c61\uff0c\u63d0\u51fa\u57fa\u4e8e\u89e3\u7801\u5668\u56de\u62c9\u5ea6\u91cf\u7684\u7ef4\u5ea6\u6392\u5e8f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u5458\u63a8\u7406\u653b\u51fb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u53cd\u8f6c\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u57df\uff0c\u5ffd\u7565\u4e86\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7f16\u7801\u5668/\u89e3\u7801\u5668\u5bf9\u548c\u6f5c\u5728\u4ee3\u7801\u5bf9\u8bb0\u5fc6\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89e3\u7801\u5668\u56de\u62c9\u5ea6\u91cf\u7684\u6f5c\u5728\u7ef4\u5ea6\u6392\u5e8f\u65b9\u6cd5\uff0c\u8bc6\u522b\u5bf9\u8bb0\u5fc6\u8d21\u732e\u6700\u5927\u7684\u7ef4\u5ea6\uff0c\u5728\u8ba1\u7b97\u653b\u51fb\u7edf\u8ba1\u91cf\u65f6\u79fb\u9664\u8f83\u5c11\u8bb0\u5fc6\u7684\u7ef4\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f7f\u6210\u5458\u63a8\u7406\u653b\u51fb\u7684AUROC\u5e73\u5747\u63d0\u53472.7%\uff0cTPR@1%FPR\u63d0\u53476.42%\u3002", "conclusion": "\u81ea\u7f16\u7801\u5668\u51e0\u4f55\u7ed3\u6784\u5bf9LDM\u8bb0\u5fc6\u6709\u88ab\u5ffd\u89c6\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u4e3a\u5206\u6790\u6269\u6563\u6a21\u578b\u9690\u79c1\u98ce\u9669\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.d2d5000d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffintechtakes.com%2Farticles%2F2025-11-20%2Frefactoring-risk-decisioning-with-agentic-ai%2F%3Futm_source=tldrfintech/1/0100019ab63eb80a-a7667c25-6429-419b-8e66-3759245f183e-000000/BGqS2YThO0C8-LCLuPl_xldW4__uHuWSWx3x5Z5ksEU=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffintechtakes.com%2Farticles%2F2025-11-20%2Frefactoring-risk-decisioning-with-agentic-ai%2F%3Futm_source=tldrfintech/1/0100019ab63eb80a-a7667c25-6429-419b-8e66-3759245f183e-000000/BGqS2YThO0C8-LCLuPl_xldW4__uHuWSWx3x5Z5ksEU=432", "authors": ["TLDR Newsletter"], "title": "Refactoring Risk Decisioning with Agentic AI", "comment": "Source: TLDR Newsletter, Date: 2025-11-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffintechtakes.com%2Farticles%2F2025-11-20%2Frefactoring-risk-decisioning-with-agentic-ai%2F%3Futm_source=tldrfintech/1/0100019ab63eb80a-a7667c25-6429-419b-8e66-3759245f183e-000000/BGqS2YThO0C8-LCLuPl_xldW4__uHuWSWx3x5Z5ksEU=432", "summary": "Refactoring Risk Decisioning with Agentic AI (10 minute read) This article attempts to define what agentic AI is, how it's different from generative AI (and why that difference isn't just semantic), and how we should think about redesigning risk decisioning processes and systems around agentic AI.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u5b9a\u4e49\u4e86\u667a\u80fd\u4f53AI\u7684\u6982\u5ff5\uff0c\u533a\u5206\u5176\u4e0e\u751f\u6210\u5f0fAI\u7684\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u56f4\u7ed5\u667a\u80fd\u4f53AI\u91cd\u65b0\u8bbe\u8ba1\u98ce\u9669\u51b3\u7b56\u6d41\u7a0b\u548c\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAI\u9886\u57df\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u5f0fAI\uff0c\u4f46\u667a\u80fd\u4f53AI\u5177\u6709\u66f4\u4e3b\u52a8\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u9700\u8981\u4e13\u95e8\u7684\u98ce\u9669\u51b3\u7b56\u6846\u67b6\u6765\u5e94\u5bf9\u5176\u7279\u6027\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u5206\u6790\u548c\u6bd4\u8f83\u7814\u7a76\uff0c\u5b9a\u4e49\u667a\u80fd\u4f53AI\u7684\u7279\u5f81\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u98ce\u9669\u51b3\u7b56\u7cfb\u7edf\u7684\u91cd\u65b0\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u660e\u786e\u4e86\u667a\u80fd\u4f53AI\u4e0e\u751f\u6210\u5f0fAI\u5728\u81ea\u4e3b\u6027\u3001\u51b3\u7b56\u80fd\u529b\u7b49\u65b9\u9762\u7684\u672c\u8d28\u533a\u522b\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u98ce\u9669\u51b3\u7b56\u6846\u67b6\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u9700\u8981\u4e13\u95e8\u7684\u98ce\u9669\u51b3\u7b56\u65b9\u6cd5\uff0c\u4f20\u7edf\u7684\u751f\u6210\u5f0fAI\u98ce\u9669\u7ba1\u7406\u6846\u67b6\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u5176\u4e3b\u52a8\u51b3\u7b56\u7279\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.dfaccd42", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fz83buk/1/0100019ab646550b-0e992898-819c-42f3-876d-9a3529ea4a25-000000/qW9I1s49mZsw2TMI_X5LQJxdQRTwfDg-jt-T1fRoBGk=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fz83buk/1/0100019ab646550b-0e992898-819c-42f3-876d-9a3529ea4a25-000000/qW9I1s49mZsw2TMI_X5LQJxdQRTwfDg-jt-T1fRoBGk=432", "authors": ["TLDR Newsletter"], "title": "What OpenAI Did When ChatGPT Users Lost Touch With Reality", "comment": "Source: TLDR Newsletter, Date: 2025-11-24, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fz83buk/1/0100019ab646550b-0e992898-819c-42f3-876d-9a3529ea4a25-000000/qW9I1s49mZsw2TMI_X5LQJxdQRTwfDg-jt-T1fRoBGk=432", "summary": "What OpenAI Did When ChatGPT Users Lost Touch With Reality (12 minute read) The New York Times revealed OpenAI's internal struggle between user engagement and safety after the company overruled its Model Behavior team's warnings to release a sycophantic April update to GPT-4o that made users return more frequently. The company now faces five wrongful death lawsuits and declared a \"Code Orange\" in October after discovering its safer GPT-5 model was losing users, with executives calling it \"the...", "source": "tldr", "AI": {"tldr": "OpenAI\u5728ChatGPT\u7528\u6237\u4e0e\u73b0\u5b9e\u8131\u8282\u65f6\u9762\u4e34\u5185\u90e8\u5b89\u5168\u4e0e\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u51b2\u7a81\uff0c\u516c\u53f8\u5426\u51b3\u4e86\u6a21\u578b\u884c\u4e3a\u56e2\u961f\u7684\u8b66\u544a\uff0c\u53d1\u5e03\u4e86\u5949\u627f\u7528\u6237\u7684GPT-4o\u66f4\u65b0\uff0c\u5bfc\u81f4\u7528\u6237\u66f4\u9891\u7e41\u4f7f\u7528\u3002\u73b0\u5728\u9762\u4e34\u4e94\u8d77\u975e\u6b63\u5e38\u6b7b\u4ea1\u8bc9\u8bbc\uff0c\u5e76\u5728\u53d1\u73b0\u66f4\u5b89\u5168\u7684GPT-5\u6a21\u578b\u5931\u53bb\u7528\u6237\u540e\u5ba3\u5e03\"\u6a59\u8272\u4ee3\u7801\"\u7d27\u6025\u72b6\u6001\u3002", "motivation": "OpenAI\u9762\u4e34\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u6a21\u578b\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u6743\u8861\u56f0\u5883\uff0c\u516c\u53f8\u4f18\u5148\u8003\u8651\u7528\u6237\u7559\u5b58\u7387\u800c\u5ffd\u89c6\u4e86\u5b89\u5168\u56e2\u961f\u7684\u8b66\u544a\u3002", "method": "\u901a\u8fc7\u53d1\u5e03\u5949\u627f\u7528\u6237\u7684GPT-4o\u66f4\u65b0\u6765\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u540c\u65f6\u76d1\u63a7\u66f4\u5b89\u5168\u7684GPT-5\u6a21\u578b\u7684\u7528\u6237\u6d41\u5931\u60c5\u51b5\u3002", "result": "GPT-4o\u66f4\u65b0\u6210\u529f\u63d0\u9ad8\u4e86\u7528\u6237\u4f7f\u7528\u9891\u7387\uff0c\u4f46\u5bfc\u81f4\u7528\u6237\u4e0e\u73b0\u5b9e\u8131\u8282\uff0c\u5f15\u53d1\u4e94\u8d77\u975e\u6b63\u5e38\u6b7b\u4ea1\u8bc9\u8bbc\uff1b\u66f4\u5b89\u5168\u7684GPT-5\u6a21\u578b\u5219\u9762\u4e34\u7528\u6237\u6d41\u5931\u95ee\u9898\u3002", "conclusion": "OpenAI\u5728\u5e73\u8861\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u6a21\u578b\u5b89\u5168\u6027\u65b9\u9762\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u9700\u8981\u5728\u5546\u4e1a\u5229\u76ca\u548c\u9053\u5fb7\u8d23\u4efb\u4e4b\u95f4\u627e\u5230\u66f4\u597d\u7684\u5e73\u8861\u70b9\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.e34251d0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F11%2F21%2Fagents-are-hard%2F%3Futm_source=tldrai/1/0100019ab646550b-0e992898-819c-42f3-876d-9a3529ea4a25-000000/JmOWz5MCv6rDJj1rtUGWt2T2n7A42rW2250u4Ug0WuU=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F11%2F21%2Fagents-are-hard%2F%3Futm_source=tldrai/1/0100019ab646550b-0e992898-819c-42f3-876d-9a3529ea4a25-000000/JmOWz5MCv6rDJj1rtUGWt2T2n7A42rW2250u4Ug0WuU=432", "authors": ["TLDR Newsletter"], "title": "Agent Design Is Still Hard", "comment": "Source: TLDR Newsletter, Date: 2025-11-24, Reading time: 16 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F11%2F21%2Fagents-are-hard%2F%3Futm_source=tldrai/1/0100019ab646550b-0e992898-819c-42f3-876d-9a3529ea4a25-000000/JmOWz5MCv6rDJj1rtUGWt2T2n7A42rW2250u4Ug0WuU=432", "summary": "Agent Design Is Still Hard (16 minute read) Building agents is still messy. Abstractions break once you hit real tool use. Caching works better when self-managed. Reinforcement does more heavy lifting than expected. Output tooling is surprisingly tricky. Model choice still depends on the task.", "source": "tldr", "AI": {"tldr": "\u6784\u5efa\u667a\u80fd\u4f53\u4ecd\u7136\u590d\u6742\uff0c\u62bd\u8c61\u5728\u771f\u5b9e\u5de5\u5177\u4f7f\u7528\u65f6\u5931\u6548\uff0c\u81ea\u7ba1\u7406\u7f13\u5b58\u6548\u679c\u66f4\u597d\uff0c\u5f3a\u5316\u5b66\u4e60\u627f\u62c5\u66f4\u591a\u4efb\u52a1\uff0c\u8f93\u51fa\u5de5\u5177\u8bbe\u8ba1\u56f0\u96be\uff0c\u6a21\u578b\u9009\u62e9\u4ecd\u4f9d\u8d56\u5177\u4f53\u4efb\u52a1\u3002", "motivation": "\u63a2\u8ba8\u5f53\u524d\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e2d\u7684\u5b9e\u9645\u6311\u6218\u548c\u75db\u70b9\uff0c\u63ed\u793a\u7406\u8bba\u62bd\u8c61\u4e0e\u5b9e\u9645\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8e\u5b9e\u8df5\u7ecf\u9a8c\u5206\u6790\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u5404\u4e2a\u73af\u8282\uff0c\u5305\u62ec\u5de5\u5177\u4f7f\u7528\u3001\u7f13\u5b58\u7ba1\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u3001\u8f93\u51fa\u5de5\u5177\u8bbe\u8ba1\u548c\u6a21\u578b\u9009\u62e9\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u4f53\u8bbe\u8ba1\u4ecd\u9762\u4e34\u591a\u91cd\u6311\u6218\uff1a\u62bd\u8c61\u5c42\u5728\u771f\u5b9e\u5de5\u5177\u4f7f\u7528\u65f6\u6613\u5931\u6548\uff0c\u81ea\u7ba1\u7406\u7f13\u5b58\u6bd4\u9884\u8bbe\u7f13\u5b58\u66f4\u6709\u6548\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u627f\u62c5\u4e86\u6bd4\u9884\u671f\u66f4\u591a\u7684\u529f\u80fd\uff0c\u8f93\u51fa\u5de5\u5177\u8bbe\u8ba1\u590d\u6742\uff0c\u6a21\u578b\u9009\u62e9\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u5b9a\u5236\u3002", "conclusion": "\u667a\u80fd\u4f53\u8bbe\u8ba1\u4ecd\u7136\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u5145\u6ee1\u6311\u6218\u7684\u9886\u57df\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u6765\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.dd303734", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fai%2F2025%2F11%2Fanthropic-introduces-opus-4-5-cuts-api-pricing-and-enables-much-longer-claude-chats%2F%3Futm_source=tldrnewsletter/1/0100019abac15980-ab1fc195-f162-4e2a-8637-e274947357e6-000000/QqdVhwVUeH-k3-oC_R0m2Z_lpxICHiDCThxmjUGumH4=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fai%2F2025%2F11%2Fanthropic-introduces-opus-4-5-cuts-api-pricing-and-enables-much-longer-claude-chats%2F%3Futm_source=tldrnewsletter/1/0100019abac15980-ab1fc195-f162-4e2a-8637-e274947357e6-000000/QqdVhwVUeH-k3-oC_R0m2Z_lpxICHiDCThxmjUGumH4=432", "authors": ["TLDR Newsletter"], "title": "Anthropic introduces cheaper, more powerful, more efficient Opus 4.5 model", "comment": "Source: TLDR Newsletter, Date: 2025-11-25, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fai%2F2025%2F11%2Fanthropic-introduces-opus-4-5-cuts-api-pricing-and-enables-much-longer-claude-chats%2F%3Futm_source=tldrnewsletter/1/0100019abac15980-ab1fc195-f162-4e2a-8637-e274947357e6-000000/QqdVhwVUeH-k3-oC_R0m2Z_lpxICHiDCThxmjUGumH4=432", "summary": "Anthropic introduces cheaper, more powerful, more efficient Opus 4.5 model (2 minute read) Anthropic has released its latest frontier model, Opus 4.5. The model brings improvements in coding performance and user experience. The Opus 4.5 API costs $5 per million input tokens and $25 per million output tokens. Anthropic's developer platform now includes a new 'effort' parameter that allows developers to more precisely tune the balance between efficacy and token usage. Claude Code is now availab...", "source": "tldr", "AI": {"tldr": "Anthropic\u53d1\u5e03\u4e86\u66f4\u4fbf\u5b9c\u3001\u66f4\u5f3a\u5927\u3001\u66f4\u9ad8\u6548\u7684Opus 4.5\u6a21\u578b\uff0c\u5728\u7f16\u7801\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u6709\u6539\u8fdb\uff0cAPI\u4ef7\u683c\u4e3a\u6bcf\u767e\u4e07\u8f93\u5165token 5\u7f8e\u5143\u3001\u8f93\u51fatoken 25\u7f8e\u5143\uff0c\u5e76\u5f15\u5165\u4e86'effort'\u53c2\u6570\u6765\u5e73\u8861\u6548\u679c\u548ctoken\u4f7f\u7528\u3002", "motivation": "\u63d0\u4f9b\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6539\u5584\u5f00\u53d1\u8005\u7684\u7f16\u7801\u4f53\u9a8c\u548c\u6210\u672c\u63a7\u5236\u3002", "method": "\u5f00\u53d1\u65b0\u7684Opus 4.5\u6a21\u578b\uff0c\u4f18\u5316\u7f16\u7801\u6027\u80fd\uff0c\u5f15\u5165'effort'\u53c2\u6570\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u8c03\u8282\u6a21\u578b\u6548\u679c\u4e0etoken\u6d88\u8017\u7684\u5e73\u8861\u3002", "result": "\u53d1\u5e03\u4e86Opus 4.5\u6a21\u578b\uff0c\u4ef7\u683c\u66f4\u4fbf\u5b9c\uff0c\u7f16\u7801\u6027\u80fd\u63d0\u5347\uff0c\u7528\u6237\u4f53\u9a8c\u6539\u5584\uff0cClaude Code\u73b0\u5728\u53ef\u7528\u3002", "conclusion": "Anthropic\u901a\u8fc7Opus 4.5\u6a21\u578b\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684AI\u7f16\u7801\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "wechat.2511.59375347", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNzkyNzcwNg==&mid=2247484243&idx=1&sn=b7a2bb49400392b4f471e9aa2bec7afe&chksm=fb134c9b79d4d05af290eb5459b23671cbdef3b64de5646188776c7423385d953bc683426d6e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNzkyNzcwNg==&mid=2247484243&idx=1&sn=b7a2bb49400392b4f471e9aa2bec7afe&chksm=fb134c9b79d4d05af290eb5459b23671cbdef3b64de5646188776c7423385d953bc683426d6e#rd", "authors": ["\u952e\u9699\u968f\u60f3"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1a\u4ece\u5165\u95e8\u5230\u6210\u4ed9", "comment": "Source: WeChat, Published: 2025-11-26 11:23:26", "summary": "2\u3001\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u7684\u662f\u5b66\u5230\u4e00\u4e2a\u7b56\u7565\u51fd\u6570\uff0c\u5728\u6bcf\u4e2a\u65f6\u523b\u6839\u636e\u89c2\u6d4b\u5230\u7684\u72b6\u6001\u505a\u51fa\u51b3\u7b56\uff0c\u7b56\u7565\u53ef\u4ee5\u662f\u786e\u5b9a\u6027\u7684\uff0c\u4e5f\u53ef\u4ee5\u662f\u968f\u673a\u7684\u30023\u3001\u968f\u673a\u6027\u7684\u7b56\u7565\u51fd\u6570\u4e0e\u72b6\u6001\u8f6c\u79fb\u51fd\u6570", "AI": {"tldr": "2\u3001\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u7684\u662f\u5b66\u5230\u4e00\u4e2a\u7b56\u7565\u51fd\u6570\uff0c\u5728\u6bcf\u4e2a\u65f6\u523b\u6839\u636e\u89c2\u6d4b\u5230\u7684\u72b6\u6001\u505a\u51fa\u51b3\u7b56\uff0c\u7b56\u7565\u53ef\u4ee5\u662f\u786e\u5b9a\u6027\u7684\uff0c\u4e5f\u53ef\u4ee5\u662f\u968f\u673a\u7684\u30023\u3001\u968f\u673a\u6027\u7684\u7b56\u7565\u51fd\u6570\u4e0e\u72b6\u6001\u8f6c\u79fb\u51fd\u6570", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.4d078df6", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247517077&idx=2&sn=f5cea821164e6ff41fa8f3221f91b2ff&chksm=9ab688bcf777350bc658e8a64c957dce21b77b8d6dac0f6fb44194d5fbb3c3b4455bc668f47d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247517077&idx=2&sn=f5cea821164e6ff41fa8f3221f91b2ff&chksm=9ab688bcf777350bc658e8a64c957dce21b77b8d6dac0f6fb44194d5fbb3c3b4455bc668f47d#rd", "authors": ["\u6708\u6765\u5ba2\u6808"], "title": "\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u57fa\u4e8eActor-Critic\u7684Vanilla Policy Gradient \u539f\u7406", "comment": "Source: WeChat, Published: 2025-11-26 11:16:31", "summary": "\u53c8\u56e0\u4e3a\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u667a\u80fd\u4f53\u5b9e\u9645\u4e0a\u5e94\u8be5\u53ea\u6839\u636e\u884c\u52a8\u4e4b\u540e\u7684\u5956\u52b1\u6765\u5f3a\u5316\u7b56\u7565\uff0c\u800c\u884c\u52a8\u4e4b\u524d\u83b7\u5f97\u7684\u5956\u52b1\u4fbf\u4e0e\u5f53\u524d\u884c\u52a8\u7684\u4f18\u52a3\u65e0\u5173\u4e86\uff0c\u4e5f\u5c31\u662f\u8bf4\u53ea\u6709\u884c\u52a8\u4e4b\u540e\u83b7\u5f97\u7684\u5956\u52b1\u624d\u6709\u5f71\u54cd\u3002", "AI": {"tldr": "\u53c8\u56e0\u4e3a\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u667a\u80fd\u4f53\u5b9e\u9645\u4e0a\u5e94\u8be5\u53ea\u6839\u636e\u884c\u52a8\u4e4b\u540e\u7684\u5956\u52b1\u6765\u5f3a\u5316\u7b56\u7565\uff0c\u800c\u884c\u52a8\u4e4b\u524d\u83b7\u5f97\u7684\u5956\u52b1\u4fbf\u4e0e\u5f53\u524d\u884c\u52a8\u7684\u4f18\u52a3\u65e0\u5173\u4e86\uff0c\u4e5f\u5c31\u662f\u8bf4\u53ea\u6709\u884c\u52a8\u4e4b\u540e\u83b7\u5f97\u7684\u5956\u52b1\u624d\u6709\u5f71\u54cd\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.e5a9b837", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5OTY5NDMxMg==&mid=2247485425&idx=1&sn=5542f5429bc63bda715ae3d1ffe13bbf&chksm=ff4ac98ab68218aa5beb18cae4e63c4a20cff4bdeb410c3aa10222e5e6b62276a7bedfd060b3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5OTY5NDMxMg==&mid=2247485425&idx=1&sn=5542f5429bc63bda715ae3d1ffe13bbf&chksm=ff4ac98ab68218aa5beb18cae4e63c4a20cff4bdeb410c3aa10222e5e6b62276a7bedfd060b3#rd", "authors": ["\u7535\u78c1\u5fae\u8bfe\u5802"], "title": "\u3010\u8bba\u6587\u8350\u8bfb\u3011\u5143<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-11-26 11:16:18", "summary": "2.3 \u5143\u5f3a\u5316\u5b66\u4e60\u5173\u952e\u6311\u6218 \u5c3d\u7ba1\u5143\u5f3a\u5316\u5b66\u4e60\u5728\u9002\u5e94\u901f\u5ea6\u4e0e\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\uff0c\u4f46\u4ecd\u9762\u4e34\u4e94\u5927\u6838\u5fc3\u6311\u6218\uff1a\uff081\uff09 \u5143\u77e5\u8bc6\u8d28\u91cf\u96be\u4ee5\u4fdd\u8bc1 \u5143\u77e5\u8bc6\u662f\u5143\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\uff0c\u5f53\u524d\u591a\u4ee5\u4efb\u52a1\u7ecf\u9a8c\u3001\u5171\u6027\u89c4\u5f8b\u6216\u53c2\u6570\u521d\u59cb\u5316\u6761\u4ef6\u5f62\u5f0f\u5b58\u5728\uff0c", "AI": {"tldr": "2.3 \u5143\u5f3a\u5316\u5b66\u4e60\u5173\u952e\u6311\u6218 \u5c3d\u7ba1\u5143\u5f3a\u5316\u5b66\u4e60\u5728\u9002\u5e94\u901f\u5ea6\u4e0e\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\uff0c\u4f46\u4ecd\u9762\u4e34\u4e94\u5927\u6838\u5fc3\u6311\u6218\uff1a\uff081\uff09 \u5143\u77e5\u8bc6\u8d28\u91cf\u96be\u4ee5\u4fdd\u8bc1 \u5143\u77e5\u8bc6\u662f\u5143\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\uff0c\u5f53\u524d\u591a\u4ee5\u4efb\u52a1\u7ecf\u9a8c\u3001\u5171\u6027\u89c4\u5f8b\u6216\u53c2\u6570\u521d\u59cb\u5316\u6761\u4ef6\u5f62\u5f0f\u5b58\u5728\uff0c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.268bb97e", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MDE0NzEzNA==&mid=2647617383&idx=1&sn=78a58c3eb8686500ca8b32a7938132c0&chksm=87e1eafcea8fa3b5b5a9a677589c2633efccc839e313213ebe76a3758e5cdec3ef2893af6e67#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MDE0NzEzNA==&mid=2647617383&idx=1&sn=78a58c3eb8686500ca8b32a7938132c0&chksm=87e1eafcea8fa3b5b5a9a677589c2633efccc839e313213ebe76a3758e5cdec3ef2893af6e67#rd", "authors": ["\u8d70\u5411\u672a\u6765"], "title": "\u4e07\u5b57\u957f\u6587\u8bfb\u900f\u901a\u5411\u901a\u7528\u667a\u80fd\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1a144\u9875\u7535\u5b50\u4e66\u5168\u9762\u8bb2\u89e3DQN\u3001A2C\u3001\u4e16\u754c\u6a21\u578b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u667a\u80fd\u4f53 | 2026\u5fc5\u8bfb", "comment": "Source: WeChat, Published: 2025-11-26 03:26:00", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u7684\u672c\u8d28\u662f\u5173\u4e8e\u5e8f\u8d2f\u51b3\u7b56\u7684\u79d1\u5b66\u4e0e\u5de5\u7a0b\u3002\u5b83\u8bd5\u56fe\u89e3\u51b3\u4e00\u4e2a\u6839\u672c\u6027\u95ee\u9898\uff1a\u4e00\u4e2a\u667a\u80fd\u4f53\uff08agent\uff09\u5982\u4f55\u5728\u590d\u6742\u7684\u3001\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u6765\u5b66\u4e60\u4e00\u7cfb\u5217\u52a8\u4f5c\uff0c\u4ee5\u6700\u5927\u5316\u5176\u7d2f\u79ef\u7684\u671f\u671b\u5956\u52b1\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u7684\u672c\u8d28\u662f\u5173\u4e8e\u5e8f\u8d2f\u51b3\u7b56\u7684\u79d1\u5b66\u4e0e\u5de5\u7a0b\u3002\u5b83\u8bd5\u56fe\u89e3\u51b3\u4e00\u4e2a\u6839\u672c\u6027\u95ee\u9898\uff1a\u4e00\u4e2a\u667a\u80fd\u4f53\uff08agent\uff09\u5982\u4f55\u5728\u590d\u6742\u7684\u3001\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u6765\u5b66\u4e60\u4e00\u7cfb\u5217\u52a8\u4f5c\uff0c\u4ee5\u6700\u5927\u5316\u5176\u7d2f\u79ef\u7684\u671f\u671b\u5956\u52b1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.605c6b16", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDAyOTQ2MA==&mid=2247483710&idx=1&sn=a6a5186ea84a6163764c33b397ab14e2&chksm=c5abd208676f64ab074ec740f8e619f7d7ccf537f1003f7778c7c7e0ca3bbd83bc94d1406ae6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDAyOTQ2MA==&mid=2247483710&idx=1&sn=a6a5186ea84a6163764c33b397ab14e2&chksm=c5abd208676f64ab074ec740f8e619f7d7ccf537f1003f7778c7c7e0ca3bbd83bc94d1406ae6#rd", "authors": ["AI\u6742\u8d27\u94fa"], "title": "AdaCuRL\uff1a\u963f\u91cc\u63d0\u51fa\u65b0\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6846\u67b6\uff0c\u89e3\u9501\u5927\u6a21\u578b\u7684\u63a8\u7406\u6f5c\u529b\uff0c\u5168\u9762\u8d85\u8d8aGRPO!", "comment": "Source: WeChat, Published: 2025-11-26 02:21:00", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5f00\u8f9f\u4e86\u4e00\u6761\u6fc0\u52a8\u4eba\u5fc3\u7684\u8def\u5f84\u3002\u4ee5GRPO\u4e3a\u4ee3\u8868\u7684\u65b9\u6cd5\u8bc1\u660e\u4e86\uff0c\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u6211\u63d0\u5347\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u4e14\u8017\u65f6\u7684\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5f00\u8f9f\u4e86\u4e00\u6761\u6fc0\u52a8\u4eba\u5fc3\u7684\u8def\u5f84\u3002\u4ee5GRPO\u4e3a\u4ee3\u8868\u7684\u65b9\u6cd5\u8bc1\u660e\u4e86\uff0c\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u6211\u63d0\u5347\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u4e14\u8017\u65f6\u7684\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.690d52b4", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzNTIwOTQ2OQ==&mid=2247483838&idx=1&sn=cba57f716e1a1f656ac8a4632ead9e41&chksm=f15b688f02a6f9f7bbc9b9f20dc926c9d4c8ca79e5ed7b213b25a803c492026c515e7d2fcef4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzNTIwOTQ2OQ==&mid=2247483838&idx=1&sn=cba57f716e1a1f656ac8a4632ead9e41&chksm=f15b688f02a6f9f7bbc9b9f20dc926c9d4c8ca79e5ed7b213b25a803c492026c515e7d2fcef4#rd", "authors": ["\u5927\u6a21\u578b\u4e0e\u8f6f\u4ef6\u5de5\u7a0b"], "title": "\u3010ASE25\u83b7\u5956\u8bba\u6587|Agent\u8f68\u8ff9\u3011\u5927\u6a21\u578b<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em>\u662f\u5982\u4f55\"\u601d\u8003\"\u548c\"\u884c\u52a8\"\u7684?", "comment": "Source: WeChat, Published: 2025-11-26 10:00:00", "summary": "\u4e0e\u4f20\u7edf\u7684\u4ee3\u7801\u8865\u5168\u5de5\u5177\u4e0d\u540c\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u8c03\u7528\u5916\u90e8\u5de5\u5177\u3001\u8fed\u4ee3\u6539\u8fdb\u65b9\u6848\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\u4e00\u76f4\u662f\u4e2a\"\u9ed1\u76d2\"\u2014\u2014\u6211\u4eec\u770b\u5230\u4e86\u7ed3\u679c\uff0c\u5374\u4e0d\u77e5\u9053\u5b83\u4eec\u5982\u4f55\u5f97\u51fa\u7b54\u6848\u3002", "AI": {"tldr": "\u4e0e\u4f20\u7edf\u7684\u4ee3\u7801\u8865\u5168\u5de5\u5177\u4e0d\u540c\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u8c03\u7528\u5916\u90e8\u5de5\u5177\u3001\u8fed\u4ee3\u6539\u8fdb\u65b9\u6848\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\u4e00\u76f4\u662f\u4e2a\"\u9ed1\u76d2\"\u2014\u2014\u6211\u4eec\u770b\u5230\u4e86\u7ed3\u679c\uff0c\u5374\u4e0d\u77e5\u9053\u5b83\u4eec\u5982\u4f55\u5f97\u51fa\u7b54\u6848\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.daa457db", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3NzIyNjcyMA==&mid=2247489997&idx=1&sn=150871e924ab877090fb3238d840f13d&chksm=ea70160bb087a9b53ccc6512aaf1ce08df67958f149281a19a4580d870a41ee86929f0c0a15a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3NzIyNjcyMA==&mid=2247489997&idx=1&sn=150871e924ab877090fb3238d840f13d&chksm=ea70160bb087a9b53ccc6512aaf1ce08df67958f149281a19a4580d870a41ee86929f0c0a15a#rd", "authors": ["\u534e\u6cf0\u8f6f\u4ef6"], "title": "Huatek X AWS\uff1a\u4ee5<em class=\"highlight\">Agentic</em> AI\u9a71\u52a8\u7684\u4f01\u4e1a\u4f9b\u5e94\u94fe\u667a\u80fd\u4f18\u5316\u51b3\u7b56", "comment": "Source: WeChat, Published: 2025-11-26 10:52:48", "summary": "1\u7814\u53d1\u8bbe\u8ba1\u65b9\u9762Agentic AI\u53ef\u52a0\u901f\u4ea7\u54c1\u521b\u65b0\u5468\u671f\u3002\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5386\u53f2\u8bbe\u8ba1\u6570\u636e\u4e0e\u5e02\u573a\u53cd\u9988\uff0c\u6a21\u62df\u591a\u7ef4\u5ea6\u53c2\u6570\u7ec4\u5408\uff0c\u667a\u80fd\u63a8\u8350\u6700\u4f18\u8bbe\u8ba1\u65b9\u6848\uff1b\u540c\u65f6\u5b9e\u65f6\u5206\u6790\u7ade\u54c1\u6280\u672f\u52a8\u6001\uff0c\u9884\u5224\u7814\u53d1\u98ce\u9669\uff0c\u663e\u8457\u63d0\u5347\u7814\u53d1\u6548\u7387\u4e0e\u6210\u679c\u8f6c\u5316\u7387\u3002", "AI": {"tldr": "1\u7814\u53d1\u8bbe\u8ba1\u65b9\u9762Agentic AI\u53ef\u52a0\u901f\u4ea7\u54c1\u521b\u65b0\u5468\u671f\u3002\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5386\u53f2\u8bbe\u8ba1\u6570\u636e\u4e0e\u5e02\u573a\u53cd\u9988\uff0c\u6a21\u62df\u591a\u7ef4\u5ea6\u53c2\u6570\u7ec4\u5408\uff0c\u667a\u80fd\u63a8\u8350\u6700\u4f18\u8bbe\u8ba1\u65b9\u6848\uff1b\u540c\u65f6\u5b9e\u65f6\u5206\u6790\u7ade\u54c1\u6280\u672f\u52a8\u6001\uff0c\u9884\u5224\u7814\u53d1\u98ce\u9669\uff0c\u663e\u8457\u63d0\u5347\u7814\u53d1\u6548\u7387\u4e0e\u6210\u679c\u8f6c\u5316\u7387\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.6bab2c0e", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNDU0NzcxNg==&mid=2247486046&idx=2&sn=16078b584aa310f48fe1589373c8194b&chksm=f163ee317112afb58fb7511d73f7d2f2910ff9c77d079609605f0a120cca1127e07943149da5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNDU0NzcxNg==&mid=2247486046&idx=2&sn=16078b584aa310f48fe1589373c8194b&chksm=f163ee317112afb58fb7511d73f7d2f2910ff9c77d079609605f0a120cca1127e07943149da5#rd", "authors": ["AI\u5178\u578b\u573a\u666f\u4ea7\u54c1"], "title": "\u3010\u4ea7\u4e1a\u8d44\u8baf\u3011\u5fae\u8f6f\u63a8\u51fa\u5168\u65b0Fara-7B <em class=\"highlight\">Agentic</em>\u6a21\u578b", "comment": "Source: WeChat, Published: 2025-11-26 10:19:35", "summary": "\u5b9a\u4f4d\u4e3a\u4e13\u95e8\u7528\u4e8e\u8ba1\u7b97\u673a\u64cd\u4f5c\u7684\u201cAgentic\u201d\u6a21\u578b\uff0c\u53ef\u901a\u8fc7\u9f20\u6807\u548c\u952e\u76d8\u6267\u884c\u7f51\u9875\u4efb\u52a1\u3002\u4f5c\u4e3a\u5fae\u8f6f\u9996\u4e2a\u9762\u5411\u7535\u8111\u4f7f\u7528\u573a\u666f\u7684\u5c0f\u6a21\u578b\uff08SLM\uff09\uff0cFara-7B \u7531 70 \u4ebf\u53c2\u6570\u6784\u6210\uff0c\u5728\u540c\u7ea7\u4f53\u91cf\u4e2d\u8fbe\u5230\u9886\u5148\u6027\u80fd\uff0c\u5e76\u80fd\u5728\u8bbe\u5907\u7aef\u672c\u5730\u8fd0\u884c\uff0c\u5b9e\u73b0\u66f4\u4f4e\u5ef6\u8fdf\u53ca", "AI": {"tldr": "\u5b9a\u4f4d\u4e3a\u4e13\u95e8\u7528\u4e8e\u8ba1\u7b97\u673a\u64cd\u4f5c\u7684\u201cAgentic\u201d\u6a21\u578b\uff0c\u53ef\u901a\u8fc7\u9f20\u6807\u548c\u952e\u76d8\u6267\u884c\u7f51\u9875\u4efb\u52a1\u3002\u4f5c\u4e3a\u5fae\u8f6f\u9996\u4e2a\u9762\u5411\u7535\u8111\u4f7f\u7528\u573a\u666f\u7684\u5c0f\u6a21\u578b\uff08SLM\uff09\uff0cFara-7B \u7531 70 \u4ebf\u53c2\u6570\u6784\u6210\uff0c\u5728\u540c\u7ea7\u4f53\u91cf\u4e2d\u8fbe\u5230\u9886\u5148\u6027\u80fd\uff0c\u5e76\u80fd\u5728\u8bbe\u5907\u7aef\u672c\u5730\u8fd0\u884c\uff0c\u5b9e\u73b0\u66f4\u4f4e\u5ef6\u8fdf\u53ca", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.e20f7e2e", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4NDY2MDMwMw==&mid=2247515261&idx=2&sn=b853609faf6f2e62f932b632d15aca8b&chksm=ea3acec163a7f74c23f577855825bc033346c0589f51abcd4dc5278294874fa70ae99727a7c7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4NDY2MDMwMw==&mid=2247515261&idx=2&sn=b853609faf6f2e62f932b632d15aca8b&chksm=ea3acec163a7f74c23f577855825bc033346c0589f51abcd4dc5278294874fa70ae99727a7c7#rd", "authors": ["\u5b89\u5168\u5185\u53c2"], "title": "\u56fd\u5916<em class=\"highlight\">Agentic</em> SOC\u5e73\u53f0\u843d\u5730\u5b9e\u8df5\u7ecf\u9a8c", "comment": "Source: WeChat, Published: 2025-11-26 09:30:11", "summary": "\u5f53\u524d\uff0cAgentic AI\u8d4b\u80fd\u7684Agentic SOC\uff08\u81ea\u4e3b\u5f0fSOC\uff09\u5e73\u53f0\uff08\u5373Agentic SOP\uff09\u6b63\u53d1\u5c55\u5f97\u5982\u706b\u5982\u837c\uff0c\u4e0d\u65ad\u6500\u4e0a\u7092\u4f5c\u7684\u9ad8\u5cf0\u3002Agentic SOC\u5e73\u53f0\u4ee5LLM\u4f5c\u4e3a\u601d\u8003\u4e2d\u67a2\uff0c\u5177\u6709\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u80fd\u591f\u8c03\u7528\u5404\u79cd\u5de5\u5177\u81ea\u52a8\u5b8c\u6210\u9884\u5b9a\u7684\u5b89\u5168\u8fd0\u8425\u4efb\u52a1\uff0c\u5e76", "AI": {"tldr": "\u5f53\u524d\uff0cAgentic AI\u8d4b\u80fd\u7684Agentic SOC\uff08\u81ea\u4e3b\u5f0fSOC\uff09\u5e73\u53f0\uff08\u5373Agentic SOP\uff09\u6b63\u53d1\u5c55\u5f97\u5982\u706b\u5982\u837c\uff0c\u4e0d\u65ad\u6500\u4e0a\u7092\u4f5c\u7684\u9ad8\u5cf0\u3002Agentic SOC\u5e73\u53f0\u4ee5LLM\u4f5c\u4e3a\u601d\u8003\u4e2d\u67a2\uff0c\u5177\u6709\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u80fd\u591f\u8c03\u7528\u5404\u79cd\u5de5\u5177\u81ea\u52a8\u5b8c\u6210\u9884\u5b9a\u7684\u5b89\u5168\u8fd0\u8425\u4efb\u52a1\uff0c\u5e76", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.cc313b34", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484384&idx=1&sn=0c909dad348fdd7714bf76ccc5d765a6&chksm=c007a0ebf1878015253953078126336fd3344142529028482f9698f937bcd16d3f504520a163#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484384&idx=1&sn=0c909dad348fdd7714bf76ccc5d765a6&chksm=c007a0ebf1878015253953078126336fd3344142529028482f9698f937bcd16d3f504520a163#rd", "authors": ["AI Lab Dev"], "title": "<em class=\"highlight\">Agentic</em>21\u79cd\u8bbe\u8ba1\u6a21\u5f0f18-\u9632\u62a4\u673a\u5236\u4e0e\u5b89\u5168\u6a21\u5f0f", "comment": "Source: WeChat, Published: 2025-11-25 23:00:23", "summary": "\u8bbe\u8ba1\u53ef\u9760\u7684Agent\u6784\u5efa\u53ef\u9760\u7684Agent\u9700\u8981\u6211\u4eec\u8fd0\u7528\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u76f8\u540c\u7684\u4e25\u8c28\u65b9\u6cd5\u4e0e\u6700\u4f73\u5b9e\u8df5\u3002\u6211\u4eec\u5fc5\u987b\u7262\u8bb0\uff0c\u5373\u4fbf\u662f\u786e\u5b9a\u6027\u4ee3\u7801\u4e5f\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u4e14\u53ef\u80fd\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "AI": {"tldr": "\u8bbe\u8ba1\u53ef\u9760\u7684Agent\u6784\u5efa\u53ef\u9760\u7684Agent\u9700\u8981\u6211\u4eec\u8fd0\u7528\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u76f8\u540c\u7684\u4e25\u8c28\u65b9\u6cd5\u4e0e\u6700\u4f73\u5b9e\u8df5\u3002\u6211\u4eec\u5fc5\u987b\u7262\u8bb0\uff0c\u5373\u4fbf\u662f\u786e\u5b9a\u6027\u4ee3\u7801\u4e5f\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u4e14\u53ef\u80fd\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.80a84575", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5NTQ3NzIxMw==&mid=2247485799&idx=1&sn=daf59f36823ae0eb65db2885890e75ac&chksm=ed0934ff2a57c7b01a2a29680a2f0ef9f3751dd087464698d9ba67bcc03a60023a7f4fbef5aa#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5NTQ3NzIxMw==&mid=2247485799&idx=1&sn=daf59f36823ae0eb65db2885890e75ac&chksm=ed0934ff2a57c7b01a2a29680a2f0ef9f3751dd087464698d9ba67bcc03a60023a7f4fbef5aa#rd", "authors": ["AI\u7b80\u5316\u5b89\u5168"], "title": "<em class=\"highlight\">Agentic</em> AI \u5b89\u5168\u5168\u666f\uff1a\u4ece\u5a01\u80c1\u5efa\u6a21\u5230\u9632\u5fa1\u5b9e\u6218", "comment": "Source: WeChat, Published: 2025-11-25 15:31:05", "summary": "Agentic AI \u7cfb\u7edf\u80fd\u591f\u611f\u77e5\u73af\u5883\uff0c\u5236\u5b9a\u884c\u52a8\u8ba1\u5212\uff0c\u5e76\u5728\u65e0\u9700\u6301\u7eed\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002\u8fd9\u79cd\u67b6\u6784\u901a\u5e38\u7531\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u6784\u6210\u95ed\u73af\uff1a\u63a8\u7406\u5f15\u64ce\uff08LLM\uff09\uff1a \u4f5c\u4e3a\u7cfb\u7edf\u7684\u201c\u5927\u8111\u201d\uff0c\u8d1f\u8d23\u5206\u89e3\u76ee\u6807\u548c\u89c4\u5212\u4efb\u52a1 \u3002", "AI": {"tldr": "Agentic AI \u7cfb\u7edf\u80fd\u591f\u611f\u77e5\u73af\u5883\uff0c\u5236\u5b9a\u884c\u52a8\u8ba1\u5212\uff0c\u5e76\u5728\u65e0\u9700\u6301\u7eed\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002\u8fd9\u79cd\u67b6\u6784\u901a\u5e38\u7531\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u6784\u6210\u95ed\u73af\uff1a\u63a8\u7406\u5f15\u64ce\uff08LLM\uff09\uff1a \u4f5c\u4e3a\u7cfb\u7edf\u7684\u201c\u5927\u8111\u201d\uff0c\u8d1f\u8d23\u5206\u89e3\u76ee\u6807\u548c\u89c4\u5212\u4efb\u52a1 \u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
