<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 31]
- [cs.SE](#cs.SE) [Total: 8]
- [tldr.article](#tldr.article) [Total: 27]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models](https://arxiv.org/abs/2602.05385)
*Tao Liu,Jiafan Lu,Bohan Yu,Pengcheng Wu,Liu Haixin,Guoyu Xu,Li Xiangheng,Lixiao Li,Jiaming Hou,Zhao Shijun,Xinglin Lyu,Kunli Zhang,Yuxiang Jia,Hongyin Zan*

Main category: cs.CL

TL;DR: IESR框架通过信息增强结构化推理，为轻量级大语言模型解决复杂Text-to-SQL任务，在LogicCat和Archer数据集上取得SOTA性能，无需微调。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法在复杂推理、领域知识、假设查询方面表现不佳，且企业部署成本高。需要为轻量级LLM设计更有效的框架。

Method: 提出IESR框架：(1)利用LLM进行关键信息理解和模式链接，解耦数学计算和SQL生成；(2)基于蒙特卡洛树搜索的多路径推理机制与多数投票；(3)轨迹一致性验证模块和判别器模型确保准确性。

Result: 在复杂推理基准LogicCat上达到24.28 EX，在Archer数据集上达到37.28 EX，仅使用轻量级模型且无需微调即实现SOTA性能。

Conclusion: IESR框架有效提升了轻量级LLM在复杂Text-to-SQL任务上的表现，同时揭示了当前编码器模型在物理知识、数学计算和常识推理方面的偏见和不足。

Abstract: Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.

</details>


### [2] [Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444)
*Yao Zhou,Zeen Song,Wenwen Qiang,Fengge Wu,Shuyi Zhou,Changwen Zheng,Hui Xiong*

Main category: cs.CL

TL;DR: 提出CFA²攻击框架，利用因果前门准则切断安全机制与任务意图的混淆关联，实现高效越狱


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐机制作为潜在内部状态运行，掩盖了模型固有能力，需要从因果角度建模安全机制以揭示并绕过这些防御

Method: 将安全机制建模为未观测的混淆变量，采用Pearl前门准则切断混淆关联；使用稀疏自编码器物理剥离防御相关特征，隔离核心任务意图；将计算昂贵的边缘化简化为确定性干预

Result: CFA²实现了最先进的攻击成功率，同时为越狱过程提供了机制性解释

Conclusion: 该研究展示了因果推理在理解和攻击LLM安全机制方面的有效性，为安全对齐研究提供了新视角

Abstract: Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.

</details>


### [3] [Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale](https://arxiv.org/abs/2602.05447)
*Damon McMillan*

Main category: cs.CL

TL;DR: 系统研究LLM代理在结构化数据上的上下文工程，发现模型能力是主导因素，架构选择需根据模型能力定制而非通用最佳实践


<details>
  <summary>Details</summary>
Motivation: LLM代理越来越多地通过编程接口操作外部系统，但从业者缺乏关于如何构建代理所消费上下文的实证指导。使用SQL生成为代理操作的代理，研究结构化数据的上下文工程。

Method: 使用SQL生成作为代理操作的代理，进行系统研究：9,649个实验，涵盖11个模型、4种格式（YAML、Markdown、JSON、TOON）和10到10,000个表的模式。

Result: 1) 架构选择依赖模型：前沿模型（Claude、GPT、Gemini）使用基于文件的上下文检索准确率提高2.7%，开源模型则下降7.7%；2) 格式对总体准确率无显著影响；3) 模型能力是主导因素，前沿与开源模型间有21个百分点的准确率差距；4) 文件原生代理通过域分区模式可扩展到10,000个表；5) 文件大小不能预测运行时效率。

Conclusion: 为在结构化系统上部署LLM代理提供基于证据的指导，表明架构决策应根据模型能力定制，而非假设通用最佳实践。

Abstract: Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.
  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.
  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.

</details>


### [4] [LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards](https://arxiv.org/abs/2602.05758)
*Bowen Ping,Zijun Chen,Yiyao Yu,Tingfeng Hui,Junchi Yan,Baobao Chang*

Main category: cs.CL

TL;DR: LongR是一个增强长上下文推理的强化学习框架，通过"思考-阅读"机制和上下文密度奖励，在多个长上下文基准上实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注数据合成或架构修改，但仅依赖稀疏的结果奖励在复杂长上下文推理中效果有限，需要更细粒度的指导信号

Method: 提出LongR统一框架：1）动态"思考-阅读"机制，交替进行推理和文档查阅；2）基于相对信息增益的上下文密度奖励，量化相关文档的效用

Result: 在LongBench v2上获得9%的性能提升，在RULER和InfiniteBench上实现一致改进，在不同RL算法（DAPO、GSPO）上均有效

Conclusion: LongR通过细粒度的奖励机制和动态推理-阅读交替，显著提升了长上下文推理能力，并展示了良好的鲁棒性和算法兼容性

Abstract: Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic "Think-and-Read" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.

</details>


### [5] [Reinforcement World Model Learning for LLM-based Agents](https://arxiv.org/abs/2602.05842)
*Xiao Yu,Baolin Peng,Ruize Xu,Yelong Shen,Pengcheng He,Suman Nath,Nikhil Singh,Jiangfeng Gao,Zhou Yu*

Main category: cs.CL

TL;DR: 提出RWML方法，通过自监督学习为LLM智能体构建动作条件世界模型，使用模拟到现实的差距奖励来对齐内部模拟与环境动态，在ALFWorld和τ² Bench上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM在语言任务中表现良好，但在智能体环境中难以预测动作后果和适应环境动态，需要增强LLM智能体的世界建模能力。

Method: 提出RWML（强化世界模型学习），自监督学习方法，在文本状态上学习动作条件世界模型，使用模拟到现实的差距奖励，在预训练嵌入空间中对齐模型生成的模拟下一状态与环境观察到的实际下一状态。

Result: 在ALFWorld和τ² Bench上相比基础模型有显著提升。结合任务成功奖励时，在ALFWorld和τ² Bench上分别比直接任务成功奖励RL高出6.9和5.7分，且与专家数据训练性能相当。

Conclusion: RWML为LLM智能体提供了一种有效的自监督世界模型学习方法，相比传统的下一状态token预测和LLM-as-a-judge方法更鲁棒，能显著提升智能体在环境中的适应能力。

Abstract: Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $τ^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $τ^2$ Bench respectively, while matching the performance of expert-data training.

</details>


### [6] [OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions](https://arxiv.org/abs/2602.05843)
*Fangzhi Xu,Hang Yan,Qiushi Sun,Jinyang Wu,Zixian Huang,Muye Huang,Jingyang Gong,Zichen Ding,Kanzhi Cheng,Yian Wang,Xinyu Che,Zeyi Sun,Jian Zhang,Zhangyue Yin,Haoran Luo,Xuanjing Huang,Ben Kao,Jun Liu,Qika Lin*

Main category: cs.CL

TL;DR: OdysseyArena是一个新的智能体评估框架，专注于长时程、主动和归纳式交互，旨在解决现有评估方法忽视智能体从经验中自主发现潜在转移规律的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体评估主要采用演绎范式，基于明确规则和静态目标执行任务，忽略了智能体从经验中自主发现潜在转移规律的归纳能力，这是实现智能体前瞻性和战略连贯性的关键。

Method: 提出了OdysseyArena框架，形式化并实例化了四个原语，将抽象转移动态转化为具体交互环境。建立了OdysseyArena-Lite用于标准化基准测试（包含120个任务），以及OdysseyArena-Challenge用于压力测试极端交互时程（>200步）。

Result: 对15+个领先LLM的广泛实验表明，即使是前沿模型在归纳场景中也存在明显缺陷，揭示了在复杂环境中实现自主发现的关键瓶颈。

Conclusion: 该研究提出了一个重新聚焦智能体评估的新框架，强调了归纳推理和长时程交互的重要性，为未来自主发现智能体的发展指明了方向。

Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena

</details>


### [7] [Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models](https://arxiv.org/abs/2602.05897)
*Shuo Nie,Hexuan Deng,Chao Wang,Ruiyu Fang,Xuebo Liu,Shuangyong Song,Yu Li,Min Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: FaithRL提出了一种基于步骤级监督的强化学习方法，通过过程奖励模型提供显式忠实度奖励，并结合截断重采样策略，有效减少小推理模型在思维链中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 小推理模型在资源受限环境中对思维链推理至关重要，但容易产生中间推理步骤的忠实性幻觉。现有基于在线强化学习的缓解方法依赖结果奖励或粗粒度评估，可能在最终答案正确时无意中强化不忠实的推理。

Method: 提出Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL)，包含：1) 通过过程奖励模型提供步骤级显式忠实度奖励；2) 隐式截断重采样策略，从忠实前缀生成对比信号。

Result: 在多个小推理模型和开放书问答基准测试中，FaithRL持续减少了思维链和最终答案中的幻觉，实现了更忠实可靠的推理。

Conclusion: FaithRL通过步骤级监督有效解决了小推理模型中的忠实性幻觉问题，为资源受限环境中的可靠推理提供了解决方案。

Abstract: As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.

</details>


### [8] [Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions](https://arxiv.org/abs/2602.05932)
*Léo Labat,Etienne Ollion,François Yvon*

Main category: cs.CL

TL;DR: 研究多语言大语言模型在价值导向多选题中的跨语言一致性，发现大型指令调优模型整体一致性较高，但特定问题仍存在语言依赖行为


<details>
  <summary>Details</summary>
Motivation: 虽然多语言性对LLM事实回忆的影响已有研究，但价值导向多选题中的语言诱导变异问题尚未充分探索。研究旨在了解多语言LLM在不同语言中是否保持一致的价值观表达，还是像多个单语模型一样表现出语言依赖的价值观差异。

Method: 发布新的多语言欧洲价值观调查（MEVS）语料库，包含8种欧洲语言的人类翻译调查问题。对30多个多语言LLM进行测试，控制提示变量（答案顺序、符号类型、尾部字符等），分析跨语言一致性。

Result: 大型指令调优模型整体一致性较高，但响应稳健性在不同问题间差异很大。某些问题在所有模型和语言中达成完全一致，而其他问题则导致LLM答案分裂。所有一致的指令微调模型在特定问题上都表现出语言特定行为。

Conclusion: 多语言LLM在价值导向多选题中并非完全一致的理论多语者，而是在特定问题上表现出语言依赖行为。这需要进一步研究偏好微调的选择性效应。

Abstract: Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.

</details>


### [9] [Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory](https://arxiv.org/abs/2602.06025)
*Haozhen Zhang,Haodong Yue,Tao Feng,Quanyu Long,Jianzhu Bao,Bowen Jin,Weizhi Zhang,Xiao Li,Jiaxuan You,Chengwei Qin,Wenya Wang*

Main category: cs.CL

TL;DR: BudgetMem是一个运行时代理内存框架，通过三层预算控制（低/中/高）和轻量级路由器来平衡任务性能与内存构建成本，实现显式的查询感知性能-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理内存系统大多依赖离线的、查询无关的内存构建，效率低下且可能丢弃关键信息。运行时内存利用虽然自然，但现有方法开销大且缺乏对性能-成本权衡的显式控制。

Method: 将内存处理结构化为多个内存模块，每个模块提供三个预算层级（低/中/高）。使用轻量级路由器进行跨模块的预算层级路由，通过强化学习训练的紧凑神经策略实现成本与性能的平衡。研究了三种实现预算层级的策略：实现（方法复杂度）、推理（推理行为）和容量（模块模型大小）。

Result: 在LoCoMo、LongMemEval和HotpotQA数据集上，BudgetMem在优先性能（高预算设置）时超越强基线，在更紧预算下提供更好的准确率-成本前沿。分析揭示了不同层级策略的优缺点，明确了在不同预算机制下每种策略何时提供最有利的权衡。

Conclusion: BudgetMem提供了一个统一的测试平台，实现了显式的查询感知性能-成本控制，通过预算层级路由有效平衡了LLM代理的内存效率与任务性能，为不同预算场景下的内存优化提供了实用解决方案。

Abstract: Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead是一个结构感知的多轮文档推理代理，通过利用文档的层次结构和顺序话语结构，在长文档问答中显著优于现有的代理搜索框架。


<details>
  <summary>Details</summary>
Motivation: 现有代理搜索框架通常将长文档视为扁平化的文本块集合，未能充分利用文档固有的先验知识，如层次化组织和顺序话语结构。随着工具使用和代理性大语言模型的快速发展，检索增强生成正在从一次性被动检索演变为多轮决策驱动的证据获取。

Method: DeepRead使用基于LLM的OCR模型将PDF转换为保留标题和段落边界的结构化Markdown。在段落级别索引文档，并为每个段落分配编码其章节身份和章节内顺序的坐标式元数据键。为LLM配备两个互补工具：Retrieve工具定位相关段落并暴露其结构坐标，ReadSection工具在指定章节和段落范围内实现连续、顺序保持的阅读。

Result: 实验表明DeepRead在文档问答中显著优于Search-o1风格的代理搜索。验证了检索和阅读工具之间的协同效应。细粒度行为分析揭示了类似人类"定位然后阅读"的阅读推理范式。

Conclusion: DeepRead通过显式操作文档的层次结构和顺序话语结构先验，实现了更有效的长文档问答。其结构感知的多轮文档推理方法优于现有的扁平化检索方法。

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [11] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: MINT框架通过神经符号树推理知识缺口，利用自博弈优化AI代理的询问策略，在对象驱动的规划中主动获取人类输入，实现近专家级性能。


<details>
  <summary>Details</summary>
Motivation: 开放世界规划中存在各种不完全信息和未知因素（如对象、人类目标/意图），导致联合规划中的知识缺口，需要AI代理主动获取人类输入来优化规划性能。

Method: 提出MINT（最小信息神经符号树）框架：1）构建符号树模拟可能的人机交互；2）使用神经规划策略评估知识缺口导致的规划结果不确定性；3）利用LLM搜索和总结MINT推理过程，生成最优询问集；4）通过自博弈优化AI代理的询问策略。

Result: 在三个涉及未知/未见对象的基准测试中，MINT基于规划的方案通过有限的问题数量实现了近专家级的回报，显著提高了奖励和成功率。

Conclusion: MINT框架能有效处理对象驱动规划中的知识缺口问题，通过优化的主动询问策略实现高效的人机协作规划，在复杂开放世界任务中表现出色。

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [12] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: 论文提出首个通用的大语言模型智能体不确定性量化框架，将传统不确定性积累视角转变为条件性不确定性减少过程，强调智能体交互性在不确定性量化中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 当前不确定性量化研究主要集中于单轮问答场景，而实际应用中大语言模型智能体越来越多地部署于复杂交互任务中，需要新的不确定性量化框架来适应智能体在开放世界中的交互特性。

Method: 提出首个通用的智能体不确定性量化理论框架，将现有不确定性量化设置统一纳入该框架；从传统的不确定性积累视角转向条件性不确定性减少过程，强调智能体动作的交互性对不确定性减少的作用。

Result: 建立了能够涵盖广泛现有不确定性量化设置的通用智能体不确定性量化框架，提出了条件性不确定性减少的新视角，为设计大语言模型智能体不确定性量化提供了概念性指导框架。

Conclusion: 智能体不确定性量化需要从传统的不确定性积累视角转向条件性不确定性减少过程，强调交互性在不确定性量化中的核心作用，该框架对前沿大语言模型开发和领域特定应用具有实际指导意义。

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [13] [GAMMS: Graph based Adversarial Multiagent Modeling Simulator](https://arxiv.org/abs/2602.05105)
*Rohan Patil,Jai Malegaonkar,Xiao Jiang,Andre Dion,Gaurav S. Sukhatme,Henrik I. Christensen*

Main category: cs.AI

TL;DR: GAMMS是一个轻量级、可扩展的图基对抗多智能体建模模拟器，旨在为需要可扩展性和易用性的多智能体系统研究提供快速开发和评估工具。


<details>
  <summary>Details</summary>
Motivation: 随着智能系统和多智能体协调在现实应用中日益重要，需要既可扩展又易于使用的模拟工具。现有高保真模拟器计算成本高，不适合快速原型设计或大规模智能体部署。

Method: GAMMS采用基于图的模拟框架，支持将环境表示为图，强调五个核心目标：可扩展性、易用性、集成优先架构、快速可视化反馈和现实世界基础。支持与外部工具集成，并提供内置可视化功能。

Result: GAMMS能够高效模拟复杂领域（如城市道路网络和通信系统），支持多种策略类型（启发式、基于优化、基于学习，包括大语言模型），在标准硬件上实现高性能模拟。

Conclusion: 通过降低研究门槛并在标准硬件上实现高性能模拟，GAMMS促进了多智能体系统、自主规划和对抗建模领域的实验和创新。该框架已开源。

Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/

</details>


### [14] [Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment](https://arxiv.org/abs/2602.05110)
*Liang Wang,Junpeng Wang,Chin-chia Michael Yeh,Yan Zheng,Jiarui Sun,Xiran Fan,Xin Dai,Yujie Fan,Yiwei Cai*

Main category: cs.AI

TL;DR: 本文提出一个结构化多评估者框架，用于评估LLM在商户风险分类中的推理质量，通过五标准量表和蒙特卡洛评分，结合专家验证和真实支付数据验证，揭示了不同LLM的评估偏见和稳定性差异。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地被用作推理质量的评估者，但在支付风险场景下的可靠性和偏见仍不清楚。需要系统评估LLM在商户分类代码风险评估中的表现，为金融操作环境提供可靠的评估框架。

Method: 提出结构化多评估者框架，结合五标准量表和蒙特卡洛评分评估推理质量。使用五个前沿LLM在署名和匿名条件下生成并交叉评估商户风险推理。引入共识偏差度量消除循环性，通过26名支付行业专家验证，并使用真实支付网络数据进行地面真实验证。

Result: 结果显示显著异质性：GPT-5.1和Claude 4.5 Sonnet显示负自我评估偏见，Gemini-2.5 Pro和Grok 4显示正偏见，匿名化使偏见减少25.8%。LLM评估者评分平均比人类共识高0.46分，GPT-5.1和Claude 4.5 Sonnet的负偏见反映与人类判断更接近。四个模型与真实支付数据有显著相关性。

Conclusion: 该框架为支付风险工作流中的LLM-as-a-judge系统提供了可复现的评估基础，强调了在金融操作环境中需要偏见感知协议的重要性。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

</details>


### [15] [SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers](https://arxiv.org/abs/2602.05115)
*Keyang Xuan,Pengda Wang,Chongrui Ye,Haofei Yu,Tal August,Jiaxuan You*

Main category: cs.AI

TL;DR: SocialVeil是一个模拟认知差异导致沟通障碍的社会学习环境，用于评估LLM在非理想沟通条件下的社会智能表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常假设智能体之间的理想化通信，限制了诊断LLM在更现实、不完美环境中维持和修复交互的能力。需要填补这一空白，使社会交互环境更接近真实世界通信。

Method: 基于对人类交互中沟通挑战的系统文献综述，SocialVeil引入了三种代表性的沟通障碍类型：语义模糊性、社会文化不匹配和情感干扰。还引入了两种障碍感知评估指标：未解决的困惑和相互理解。在720个场景和四个前沿LLM上进行实验，并进行了人类评估验证。

Result: 障碍持续损害LLM性能，相互理解平均降低超过45%，困惑度提升近50%。人类评估验证了模拟障碍的保真度（ICC≈0.78，Pearson r≈0.80）。适应策略（修复指令和交互学习）效果有限，远未达到无障碍性能。

Conclusion: 这项工作使社会交互环境更接近真实世界通信，为探索LLM智能体的社会智能开辟了机会。结果表明当前LLM在非理想沟通条件下表现显著下降，需要进一步研究提升其社会适应能力。

Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.

</details>


### [16] [Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents](https://arxiv.org/abs/2602.05249)
*Xinyi He,Ying Yang,Chuanjian Fu,Sihan Guo,Songchun Zhu,Lifeng Fan,Zhenliang Zhang,Yujia Peng*

Main category: cs.AI

TL;DR: 提出TEA方法，通过动态原位任务生成评估智能体在未见3D环境中的能力，发现现有模型在基本感知任务上表现不佳


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在数据污染和缺乏场景特异性问题，无法有效评估智能体在未见环境中的能力，需要开发针对未见3D环境的评估方法

Method: 提出TEA方法，采用结构化图表示定义任务，构建两阶段交互-进化任务生成系统：交互阶段通过任务执行与生成的循环持续生成任务；进化阶段通过任务图建模重组和重用现有任务生成新任务

Result: 在10个未见场景中自动生成了87,876个任务，人类验证确认这些任务物理合理且涵盖基本日常认知能力；基准测试显示SOTA模型在基本感知任务上表现差，严重缺乏3D交互意识，对任务类型高度敏感

Conclusion: 在将智能体部署到真实人类环境之前，必须进行原位评估，现有模型在未见环境中的能力存在显著不足

Abstract: As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.

</details>


### [17] [RocqSmith: Can Automatic Optimization Forge Better Proof Agents?](https://arxiv.org/abs/2602.05762)
*Andrei Kozyrev,Nikita Khramov,Denis Lochmelis,Valerio Morelli,Gleb Solovev,Anton Podkopaev*

Main category: cs.AI

TL;DR: 研究自动AI代理优化方法在形式验证领域（特别是Rocq自动定理证明）的适用性，评估不同优化器对证明生成代理的性能提升效果，发现few-shot bootstrapping最有效但无法超越精心设计的最先进证明代理。


<details>
  <summary>Details</summary>
Motivation: 探索自动AI代理优化方法能否应用于现实世界的形式验证代理，特别是能否自动化代理系统的精细调优（如提示设计、上下文知识和控制策略），以减轻人工工程负担。

Method: 以Rocq自动定理证明为代表性挑战领域，评估不同自动代理优化器在优化Rocq证明生成代理任务上的表现，比较各种优化方法的有效性。

Result: 多个优化器都能带来可测量的改进，其中简单的few-shot bootstrapping方法表现最稳定有效；但所有研究的自动优化方法都无法达到精心设计的最先进证明代理的性能水平。

Conclusion: 自动代理优化方法在形式验证领域有一定潜力，但当前技术仍无法完全替代人工精心设计的代理系统，few-shot bootstrapping是最有前景的自动化方法。

Abstract: This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.

</details>


### [18] [PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences](https://arxiv.org/abs/2602.05302)
*Chris Zhu,Sasha Cui,Will Sanok Dufallo,Runzhi Jin,Zhen Xu,Linjun Zhang,Daylian Cain*

Main category: cs.AI

TL;DR: GPT-5在PieArena谈判基准测试中达到或超越商学院学生水平，但中低层模型仍有改进空间，谈判行为存在跨模型异质性


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在谈判任务中的能力，谈判是商业核心任务，需要战略推理、心智理论和经济价值创造能力

Method: 引入PieArena大规模谈判基准，基于精英商学院MBA谈判课程的真实场景，进行多智能体交互评估，研究联合意向性智能体脚手架的影响

Result: GPT-5达到AGI级别表现，匹配或超越经过一学期谈判训练和针对性指导的商学院学生；联合意向性脚手架对中低层模型有显著提升，但对前沿模型收益递减；发现谈判行为的多维异质性

Conclusion: 前沿语言智能体已具备在高风险经济环境中部署的智力和心理能力，但鲁棒性和可信赖性仍是开放挑战

Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.

</details>


### [19] [ProAct: Agentic Lookahead in Interactive Environments](https://arxiv.org/abs/2602.05327)
*Yangbin Yu,Mingyu Yang,Junyou Li,Yiming Gao,Feiyu Liu,Yijun Yang,Zichuan Lin,Jiafei Lyu,Yicheng Liu,Zhicong Lu,Deheng Ye,Jie Jiang*

Main category: cs.AI

TL;DR: ProAct是一个两阶段训练框架，通过GLAD蒸馏搜索轨迹和MC-Critic价值估计器，让LLM智能体学习前瞻推理，在交互环境中实现长程规划，显著提升规划准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在需要长程规划的交互环境中表现不佳，主要原因是模拟未来状态时错误会不断累积。需要一种方法让智能体内部化准确的前瞻推理能力。

Method: 提出ProAct两阶段训练框架：1) GLAD：通过监督微调将基于环境搜索的轨迹压缩为简洁的因果推理链；2) MC-Critic：轻量级环境rollout校准价值估计的插件式辅助价值估计器，增强PPO/GRPO等策略梯度算法。

Result: 在随机（如2048）和确定性（如Sokoban）环境中，ProAct显著提升规划准确性。4B参数模型超越所有开源基线，媲美最先进的闭源模型，并在未见环境中展现鲁棒泛化能力。

Conclusion: ProAct通过内部化前瞻推理，有效解决了LLM智能体在长程规划中的错误累积问题，为交互环境中的智能体规划提供了高效解决方案。

Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct

</details>


### [20] [AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction](https://arxiv.org/abs/2602.05353)
*Ruijie Shi,Houbin Zhang,Yuecheng Han,Yuheng Wang,Jingru Fan,Runde Yang,Yufan Dang,Huatao Li,Dewen Liu,Yuan Cheng,Chen Qian*

Main category: cs.AI

TL;DR: 提出AgentXRay框架，通过搜索方法从黑盒智能体系统的输入输出中重建可解释的工作流，解决智能体系统不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂问题解决中表现出强大能力，但许多智能体系统由于内部工作流程不透明而难以解释和控制。虽然有些框架提供了明确的协作架构，但许多部署的智能体系统对用户来说仍然是黑盒。

Method: 提出Agentic Workflow Reconstruction (AWR)任务，旨在仅使用输入输出访问合成显式、可解释的替代工作流。开发AgentXRay框架，将AWR表述为链式结构工作流空间中离散智能体角色和工具调用的组合优化问题。采用蒙特卡洛树搜索，并通过基于评分的红黑剪枝机制增强，动态整合代理质量与搜索深度。

Result: 实验表明，AgentXRay在多个领域实现了更高的代理相似度，并减少了令牌消耗，相比未剪枝搜索能够在固定迭代预算下进行更深层的工作流探索。

Conclusion: AgentXRay能够从黑盒智能体系统中重建可编辑的白盒工作流，提高系统可解释性和可控性，而无需访问模型参数。

Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.

</details>


### [21] [PATHWAYS: Evaluating Investigation and Context Discovery in AI Web Agents](https://arxiv.org/abs/2602.05354)
*Shifat E. Arman,Syed Nazmus Sakib,Tapodhir Karmakar Taton,Nafiul Haque,Shahrear Bin Amin*

Main category: cs.AI

TL;DR: PATHWAYS是一个包含250个多步决策任务的基准测试，用于评估基于网络的智能体是否能发现并正确使用隐藏的上下文信息。研究发现当前智能体在发现隐藏证据、整合证据和推翻误导性表面信号方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前基于网络的智能体在处理需要发现隐藏上下文信息的复杂决策任务时，其实际能力尚不清楚。研究者希望评估智能体是否能进行适应性调查、证据整合和判断覆盖，特别是在需要推翻误导性表面信号的情况下。

Method: 研究者创建了PATHWAYS基准测试，包含250个多步决策任务，测试智能体发现和使用隐藏上下文信息的能力。评估了封闭和开放模型，分析了智能体在导航、证据发现、证据整合和判断覆盖等方面的表现。

Result: 智能体通常能导航到相关页面，但只在少数情况下能检索到决定性的隐藏证据。当任务需要推翻误导性表面信号时，性能急剧下降到接近随机水平。智能体经常产生幻觉，声称依赖从未访问过的证据。即使发现了正确的上下文，也常常无法将其整合到最终决策中。提供更明确的指令能改善上下文发现，但通常会降低整体准确性。

Conclusion: 当前网络智能体架构缺乏可靠的适应性调查、证据整合和判断覆盖机制。需要在智能体架构中开发更可靠的机制来处理复杂决策任务中的隐藏信息和误导性信号。

Abstract: We introduce PATHWAYS, a benchmark of 250 multi-step decision tasks that test whether web-based agents can discover and correctly use hidden contextual information. Across both closed and open models, agents typically navigate to relevant pages but retrieve decisive hidden evidence in only a small fraction of cases. When tasks require overturning misleading surface-level signals, performance drops sharply to near chance accuracy. Agents frequently hallucinate investigative reasoning by claiming to rely on evidence they never accessed. Even when correct context is discovered, agents often fail to integrate it into their final decision. Providing more explicit instructions improves context discovery but often reduces overall accuracy, revealing a tradeoff between procedural compliance and effective judgement. Together, these results show that current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override.

</details>


### [22] [Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation](https://arxiv.org/abs/2602.05381)
*Ting Fang Tan,Kabilan Elangovan,Andreas Pollreisz,Kevin Bryan Dy,Wei Yan Ng,Joy Le Yi Wong,Jin Liyuan,Chrystie Quek Wan Ning,Ashley Shuen Ying Hong,Arun James Thirunavukarasu,Shelley Yin-His Chang,Jie Yao,Dylan Hong,Wang Zhaoran,Amrita Gupta,Daniel SW Ting*

Main category: cs.AI

TL;DR: 评估四个小型医疗LLM在眼科患者咨询中的表现，并测试GPT-4-Turbo作为评估工具的可行性。Meerkat-7B表现最佳，MedLLaMA3-v20最差且有25.5%的幻觉内容。GPT-4-Turbo评估与临床医生评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 随着领域特定大语言模型在眼科患者教育、分诊和临床决策中的应用增加，需要严格评估以确保安全性和准确性。研究旨在评估小型医疗LLM在回答眼科患者咨询方面的表现，并探索LLM评估相对于临床医生评分的可行性。

Method: 横断面研究，使用180个眼科患者咨询，由四个参数小于100亿的医疗LLM（Meerkat-7B、BioMistral-7B、OpenBioLLM-8B、MedLLaMA3-v20）回答，共生成2160个回答。由三位不同资历的眼科医生和GPT-4-Turbo使用S.C.O.R.E.框架（安全性、共识与上下文、客观性、可重复性、可解释性）进行五级李克特量表评分。使用Spearman秩相关、Kendall tau统计和核密度估计分析评估LLM与临床医生评分的一致性。

Result: Meerkat-7B表现最佳，平均得分：高级顾问3.44、顾问4.08、住院医师4.18。MedLLaMA3-v20表现最差，25.5%的回答包含幻觉或临床误导内容，包括捏造的术语。GPT-4-Turbo评估与临床医生整体评估高度一致（Spearman rho=0.80，Kendall tau=0.67），但高级顾问评分更保守。

Conclusion: 医疗LLM在眼科问答中显示出安全潜力，但在临床深度和共识方面仍有差距。支持LLM评估在大规模基准测试中的可行性，需要混合自动化和临床医生审查框架来指导安全的临床部署。

Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.

</details>


### [23] [M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining](https://arxiv.org/abs/2602.05429)
*Rui Lv,Juncheng Mo,Tianyi Chu,Chen Rao,Hongyi Jing,Jiajie Teng,Jiafu Chen,Shiqi Zhang,Liangzi Ding,Shuo Fang,Huaizhong Lin,Ziqiang Dang,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: M²-Miner：首个基于蒙特卡洛树搜索的低成本自动化移动GUI智能体数据挖掘框架，通过多智能体协作和意图回收策略解决GUI数据标注的高成本、低质量和低丰富度问题。


<details>
  <summary>Details</summary>
Motivation: 构建强大的GUI智能体需要大规模高质量的用户行为轨迹数据（意图-轨迹对）进行训练，但现有手动标注方法和GUI智能体数据挖掘方法面临三个关键挑战：高构建成本、数据质量差和数据丰富度低。

Method: 提出M²-Miner框架，基于蒙特卡洛树搜索实现低成本自动化数据挖掘。采用协作多智能体框架（InferAgent、OrchestraAgent、JudgeAgent）进行引导、加速和评估；设计意图回收策略提取额外有价值的交互轨迹；引入渐进式模型在环训练策略提高数据挖掘成功率。

Result: 实验表明，使用M²-Miner挖掘的数据微调的GUI智能体在多个常用移动GUI基准测试中达到了最先进的性能。

Conclusion: M²-Miner成功解决了GUI智能体数据挖掘的关键挑战，为社区研究提供了有效的解决方案，并将开源以促进相关研究发展。

Abstract: Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.

</details>


### [24] [ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation](https://arxiv.org/abs/2602.05472)
*Yiwen Duan,Jing Ye,Xinpei Zhao*

Main category: cs.AI

TL;DR: ALIVE框架通过对抗学习和指导性语言反馈，让LLM内部化推理逻辑，无需人工监督即可实现专家级推理能力


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励存在成本高、跨领域脆弱、无法理解解决方案底层逻辑等问题，阻碍了LLM实现专家级推理能力

Method: 提出ALIVE框架，基于认知协同原则，将问题提出、解决和评判统一在单一策略模型中，通过对抗学习和指导性语言反馈让模型从原始语料中内部化评估标准

Result: 在数学推理、代码生成和一般逻辑推理基准测试中，ALIVE显著缓解了奖励信号限制，在相同数据和计算条件下实现了准确率提升、跨领域泛化能力增强和更高的自我纠正率

Conclusion: 推理三位一体促进了能力增长的自我维持轨迹，使ALIVE成为无需人工监督的通用推理对齐的可扩展基础

Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.

</details>


### [25] [TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?](https://arxiv.org/abs/2602.05570)
*Yikun Zong,Cheston Tan*

Main category: cs.AI

TL;DR: 论文提出基于人类认知过程的测试时自优化框架，通过上下文学习和奖励反馈循环提升视觉语言模型在连续几何推理任务上的表现，在Tangram拼图任务上实现显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在连续几何推理任务上表现不佳（如Tangram拼图），远低于人类水平。受人类通过试错、观察和修正解决拼图的认知过程启发，研究探索无需参数更新的测试时自优化方法。

Method: 提出训练免费的验证器-优化器代理框架，结合上下文学习和奖励引导的反馈循环。通过递归优化循环，基于几何一致性反馈迭代优化预测，模仿人类的认知机制。

Result: 在五个代表性视觉语言模型上的实验显示，单块任务平均IoU仅0.41，双块组合降至0.23。但提出的自优化框架将中等三角形案例的IoU从0.63提升至0.932，无需模型重新训练。

Conclusion: 通过上下文学习和奖励循环融入人类启发的迭代优化机制，能显著提升视觉语言模型的几何推理能力，推动自优化AI在连续空间领域的实际应用。

Abstract: Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.

</details>


### [26] [Graph-based Agent Memory: Taxonomy, Techniques, and Applications](https://arxiv.org/abs/2602.05665)
*Chang Yang,Chuang Zhou,Yilin Xiao,Su Dong,Luyao Zhuang,Yujing Zhang,Zhu Wang,Zijin Hong,Zheng Yuan,Zhishang Xiang,Shengyuan Chen,Huachi Zhou,Qinggang Zhang,Ninghao Liu,Jinsong Su,Xinrun Wang,Yi Chang,Xiao Huang*

Main category: cs.AI

TL;DR: 本文综述了基于图的智能体记忆系统，探讨了其在LLM智能体中的核心作用，包括记忆分类、生命周期关键技术、开源工具和应用场景。


<details>
  <summary>Details</summary>
Motivation: 记忆是LLM智能体处理长期复杂任务（如多轮对话、游戏、科学发现）的核心模块，能够实现知识积累、迭代推理和自我进化。图结构因其建模关系依赖、组织层次信息和高效检索的内在能力，成为智能体记忆的强大结构。

Method: 1. 提出智能体记忆的分类法：短期vs长期记忆、知识vs经验记忆、非结构化vs结构化记忆，以及基于图的记忆实现视角。2. 按照记忆生命周期系统分析基于图记忆的关键技术：记忆提取（数据转换）、存储（高效组织）、检索（支持推理的相关内容检索）和进化（内容更新）。3. 总结支持自进化智能体记忆开发和评估的开源库与基准测试。

Result: 本文全面综述了基于图的智能体记忆系统，提供了记忆分类框架、生命周期技术分析、开源资源汇总和应用场景探索，为开发更高效可靠的图基智能体记忆系统提供可操作的见解。

Conclusion: 基于图的智能体记忆是LLM智能体处理复杂任务的关键模块。本文通过系统分类、技术分析和资源汇总，识别了关键挑战和未来研究方向，旨在推动更高效可靠的图基智能体记忆系统发展。

Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.

</details>


### [27] [RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765)
*Zhong Guan,Haoran Sun,Yongjian Guo,Shuai Di,Xiaodong Bai,Jing Long,Tianyun Zhao,Mingxi Luo,Chen Zhou,Yucheng Guo,Qiming Yang,Wanting Xu,Wen Huang,Yunxuan Ma,Hongke Zhao,Likang Wu,Xiaotie Deng,Xi Xiao,Sheng Wen,Yicheng Gong,Junwu Xiong*

Main category: cs.AI

TL;DR: 提出首个完全异步的VLA模型训练框架，通过环境交互、策略生成和模型更新的异步并行化，显著提升训练效率，在LIBERO基准上实现最高126.67%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的训练效率成为瓶颈，虽然RL-based框架如RLinf能提升泛化能力，但仍依赖同步执行，导致环境交互、策略生成和模型更新阶段的资源利用率低下和吞吐量限制。

Method: 提出完全异步策略训练框架，采用多级解耦架构：1) 环境交互和轨迹收集的异步并行化；2) 策略生成的流式执行；3) 训练更新的解耦调度。从大模型RL的异步优化思想中系统汲取灵感。

Result: 在LIBERO基准上，相比现有同步策略实现最高59.25%的吞吐量提升，深度优化分离策略后可达126.67%提升。消融研究验证了各异步组件的有效性，8-256 GPU规模扩展验证了方法的优秀可扩展性。

Conclusion: 提出的完全异步训练框架能有效解决VLA模型训练中的效率瓶颈问题，通过系统性的异步优化显著提升训练吞吐量和资源利用率，为大规模VLA模型训练提供了高效解决方案。

Abstract: In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.

</details>


### [28] [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818)
*Zihao Jiang,Miao Peng,Zhenyan Shan,Wenjie Xu,Ben Liu,Gong Chen,Ziqi Gao,Min Peng*

Main category: cs.AI

TL;DR: TKG-Thinker：一种用于时序知识图谱问答的新型智能体，通过自主规划和自适应检索能力，结合监督微调和强化学习双阶段训练，在复杂时序约束下实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的时序知识图谱问答方法存在两个主要问题：1）在复杂时序约束下容易产生推理幻觉；2）静态提示限制了模型自主性和泛化能力，缺乏与知识图谱环境的动态交互优化。

Method: 提出TKG-Thinker智能体，具备自主规划和自适应检索能力。采用双阶段训练策略：首先使用思维链数据进行监督微调，培养核心规划能力；然后通过强化学习阶段，利用多维奖励在复杂时序约束下优化推理策略。

Result: 在基准数据集和三个开源大语言模型上的实验表明，TKG-Thinker实现了最先进的性能，并在复杂的时序知识图谱问答场景中表现出强大的泛化能力。

Conclusion: TKG-Thinker通过动态多轮交互和双阶段训练策略，有效解决了时序知识图谱问答中的推理幻觉和泛化限制问题，为复杂时序推理任务提供了有效的解决方案。

Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.

</details>


### [29] [Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877)
*Lukas Stappen,Ahmet Erkan Turan,Johann Hagerer,Georg Groh*

Main category: cs.AI

TL;DR: 提出AgentHeLLM威胁建模框架，将资产识别与攻击路径分析分离，用于车辆LLM智能助手的安全分析


<details>
  <summary>Details</summary>
Motivation: 车辆LLM智能助手通过A2A等协议与外部服务协调，创建了新的攻击面，现有AI安全框架缺乏安全关键系统工程的"关注点分离"原则

Method: 提出AgentHeLLM框架：1)基于伤害导向"受害者建模"和《世界人权宣言》的人类中心资产分类法；2)区分毒化路径（恶意数据传播）和触发路径（激活动作）的基于图的正式模型；3)开源攻击路径建议工具AgentHeLLM Attack Path Generator，使用双层搜索策略自动化多阶段威胁发现

Result: 开发了实用的攻击路径建议工具，能够自动化发现多阶段威胁，为车辆LLM智能助手安全分析提供系统化方法

Conclusion: AgentHeLLM框架通过正式分离资产识别和攻击路径分析，填补了现有AI安全框架的方法论空白，为车辆LLM智能助手的安全威胁建模提供了系统化方法

Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.

</details>


### [30] [Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2602.05920)
*Eva Andrés*

Main category: cs.AI

TL;DR: 该论文比较了经典和量子强化学习方法解决带容量约束的车辆路径问题，发现量子增强模型在路由距离、紧凑性和路线重叠方面优于经典方法，其中混合架构表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决带容量约束的车辆路径问题（CVRP），探索量子强化学习在复杂组合优化问题中的潜力，比较经典、全量子和混合方法的性能差异。

Method: 采用优势演员-评论家（A2C）智能体，实现经典、全量子和混合三种变体，集成transformer架构通过自注意力和交叉注意力机制捕捉车辆、客户和仓库之间的关系。实验针对20个客户和4辆车的多车辆容量约束场景，进行10次独立运行。

Result: 所有三种方法都能学习有效的路由策略，但量子增强模型优于经典基线，产生更稳健的路线组织。混合架构在距离、紧凑性和路线重叠方面实现最佳整体性能。定性可视化显示量子模型生成更结构化和连贯的路由解决方案。

Conclusion: 混合量子-经典强化学习模型在解决CVRP等复杂组合优化问题方面具有潜力，量子增强方法在路由质量和鲁棒性方面优于纯经典方法。

Abstract: This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.

</details>


### [31] [AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions](https://arxiv.org/abs/2602.06008)
*Xianyang Liu,Shangding Gu,Dawn Song*

Main category: cs.AI

TL;DR: AgenticPay是一个用于多智能体买家-卖家谈判的基准测试和仿真框架，通过自然语言驱动，包含110多个任务，评估语言模型在经济学交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体越来越多地需要自主进行谈判、协调和交易，但现有基准测试缺乏评估多智能体之间语言介导的经济交互的原则性设置。

Method: 开发了AgenticPay框架，模拟买家和卖家拥有私有约束和产品依赖估值的市场环境，通过多轮语言谈判而非单纯数字竞价达成协议。包含结构化动作提取和可行性、效率、福利等指标。

Result: 对最先进的专有和开源权重LLM进行基准测试，揭示了谈判性能上的显著差距，突显了长期战略推理方面的挑战。

Conclusion: AgenticPay为研究智能体商业和基于语言的市场交互奠定了基础，代码和数据集已开源。

Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.

</details>


### [32] [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/abs/2602.06039)
*Yuxing Lu,Yucheng Hu,Xukai Zhao,Jiuxin Cao*

Main category: cs.AI

TL;DR: DyTopo是一个动态拓扑的多智能体框架，通过每轮重建稀疏有向通信图来优化LLM多智能体系统的推理性能，在代码生成和数学推理任务上平均提升6.2%。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的大语言模型多智能体系统通常采用固定的通信模式，无法适应迭代问题解决过程中不同阶段的需求变化，需要更灵活、动态的通信机制。

Method: DyTopo采用管理者引导的多智能体框架，每轮根据管理者的目标，让每个智能体输出轻量级的自然语言查询（需求）和关键信息（提供）描述符，通过语义匹配构建稀疏有向通信图，仅沿诱导边传递私有消息。

Result: 在代码生成和数学推理基准测试以及四种LLM骨干网络上，DyTopo始终优于最强基线（平均提升+6.2%），并产生可解释的协调轨迹。

Conclusion: DyTopo通过动态重构通信拓扑显著提升了多智能体系统的推理性能，同时提供了可解释的协调过程可视化，有助于理解跨轮次的通信路径重构。

Abstract: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: 提出Causal Analyst框架，使用因果发现方法识别LLM越狱的直接原因，并应用于攻击增强和防御建议


<details>
  <summary>Details</summary>
Motivation: 现有研究主要分析潜在表示，忽略了可解释提示特征与越狱发生之间的因果关系，需要更深入理解越狱机制以增强LLM安全性和可靠性

Method: 提出Causal Analyst框架，结合LLM提示编码和GNN因果图学习，构建包含35k越狱尝试的数据集，标注37个人类可读提示特征，重建从提示特征到越狱响应的因果路径

Result: 发现特定特征（如"Positive Character"和"Number of Task Steps"）是越狱的直接因果驱动因素；Jailbreaking Enhancer显著提升攻击成功率，Guardrail Advisor有效提取混淆查询中的恶意意图

Conclusion: 从因果角度分析越狱特征是提高LLM可靠性的有效且可解释的方法，因果分析优于非因果方法

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [34] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: 特征导向方法能有效控制LLM行为但严重损害模型性能，提示工程在性能与行为控制间取得最佳平衡


<details>
  <summary>Details</summary>
Motivation: 特征导向作为直接操纵内部表征的LLM控制方法，其在实际应用中的效果和潜在性能权衡尚不明确，需要实证评估

Method: 评估Goodfire的Auto Steer特征导向方法与提示工程基线，在14个导向查询（涵盖无害和安全相关行为）上，使用Llama-8B和Llama-70B模型，在171个MMLU问题上测量准确性、连贯性和行为控制

Result: Auto Steer成功修改目标行为（Llama-8B得分3.33 vs 2.98，Llama-70B得分3.57 vs 3.10），但导致性能显著下降：MMLU准确率从66%降至46%（8B）和87%降至73%（70B），连贯性从4.62降至2.24和4.94降至3.89

Conclusion: 当前特征导向方法在实际部署中存在局限性，简单提示工程在性能与行为控制间取得最佳平衡，机械控制方法面临基本的能力-行为权衡

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [35] [Internalizing LLM Reasoning via Discovery and Replay of Latent Actions](https://arxiv.org/abs/2602.04925)
*Zhenning Shi,Yijia Zhu,Junhan Shi,Xun Zhang,Lei Wang,Congcong Miao*

Main category: cs.LG

TL;DR: STIR框架通过动态潜在轨迹控制实现推理增强，将显式思维链过程内化为隐藏状态，在提高推理准确性的同时显著减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖静态控制向量，无法适应复杂推理任务中的非平稳演化过程，需要更动态的推理增强方法。

Method: 提出三阶段框架：1) 差分内在动作诱导收集潜在推理成功模式；2) 稀疏控制基构建创建紧凑几何多样工具库；3) 价值调制轨迹干预通过锚点门控动态注入上下文特定脉冲。

Result: 在6个算术和逻辑基准测试中，STIR将平均准确率提高1.9%至7.5%，同时将平均token消耗减少高达35%。

Conclusion: 显式思维链的优势可以通过动态潜在轨迹控制实现，将推理过程内化以绕过显式生成，同时实现更高的保真度。

Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.

</details>


### [36] [Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models](https://arxiv.org/abs/2602.04931)
*Shahar Haim,Daniel C McNamee*

Main category: cs.LG

TL;DR: LLMs在深度维度上存在从上下文处理到预测形成的计算阶段转换，伴随表征几何的重组，其中角度组织参数化预测分布相似性，范数编码上下文特定信息。


<details>
  <summary>Details</summary>
Motivation: 理解解码器架构大语言模型如何将上下文信息转化为预测的内部计算机制和几何表征动态。

Method: 结合几何分析和机制干预的统一框架，分析LLM深度层级的表征几何结构，包括角度组织和范数编码。

Result: 发现LLM存在深度维度的计算阶段转换，晚期层实现结构化几何编码，角度参数化预测分布相似性，范数编码上下文特定信息。

Conclusion: 为LLM将上下文转化为预测的动态过程提供了机制-几何解释，揭示了表征几何在选择性因果控制中的作用。

Abstract: We show that decoder-only large language models exhibit a depth-wise transition from context-processing to prediction-forming phases of computation accompanied by a reorganization of representational geometry. Using a unified framework combining geometric analysis with mechanistic intervention, we demonstrate that late-layer representations implement a structured geometric code that enables selective causal control over token prediction. Specifically, angular organization of the representation geometry parametrizes prediction distributional similarity, while representation norms encode context-specific information that does not determine prediction. Together, these results provide a mechanistic-geometric account of the dynamics of transforming context into predictions in LLMs.

</details>


### [37] [Privileged Information Distillation for Language Models](https://arxiv.org/abs/2602.04942)
*Emiliano Penaloza,Dheeraj Vattikonda,Nicolas Gontier,Alexandre Lacoste,Laurent Charlin,Massimo Caccia*

Main category: cs.LG

TL;DR: 提出π-Distill和OPSD两种方法，用于在训练时利用特权信息但推理时无法访问的情况下，蒸馏前沿模型在多轮智能体环境中的能力。


<details>
  <summary>Details</summary>
Motivation: 在智能体环境中，闭源系统通常只暴露动作轨迹而隐藏内部推理过程，这使得标准蒸馏流程失效。需要解决如何在只有动作轨迹特权信息的情况下，将前沿模型的能力蒸馏到推理时无法访问特权信息的学生模型中。

Method: 提出两种方法：1) π-Distill：联合教师-学生目标，同时训练特权信息条件化的教师和无条件的学生模型；2) OPSD：基于策略的自蒸馏，使用强化学习并加入学生与教师之间的反向KL惩罚项。

Result: 两种算法都能有效利用仅动作特权信息蒸馏前沿智能体。π-Distill在某些情况下OPSD，在多个智能体基准测试、模型和特权信息形式上，都优于行业标准实践（监督微调后接强化学习）。

Conclusion: π-Distill和OPSD能够有效解决在只有动作轨迹特权信息的情况下蒸馏前沿模型的问题，为智能体蒸馏提供了新的有效方法。

Abstract: Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that π-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.

</details>


### [38] [Laplacian Representations for Decision-Time Planning](https://arxiv.org/abs/2602.05031)
*Dikshant Shehmar,Matthew Schlegel,Matthew E. Taylor,Marlos C. Machado*

Main category: cs.LG

TL;DR: ALPS是一种基于拉普拉斯表示的分层规划算法，在离线目标条件RL任务中优于常用基线方法


<details>
  <summary>Details</summary>
Motivation: 基于学习模型的规划仍然是基于模型的强化学习中的关键挑战。决策时规划中，状态表示必须支持局部成本计算同时保持长时程结构，现有方法在这方面存在不足。

Method: 提出拉普拉斯表示作为有效的规划潜空间，该表示能捕捉多时间尺度的状态空间距离，保持有意义的距离度量，并将长时程问题自然分解为子目标。在此基础上开发了ALPS分层规划算法。

Result: 在OGBench的离线目标条件RL任务上，ALPS超越了常用的基线方法，这是之前由无模型方法主导的基准。

Conclusion: 拉普拉斯表示为基于模型的强化学习规划提供了有效的潜空间，ALPS算法展示了其在离线目标条件任务中的优越性能。

Abstract: Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.

</details>


### [39] [ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation](https://arxiv.org/abs/2602.05051)
*Songyuan Zhang,Oswin So,H. M. Sabbir Ahmad,Eric Yang Yu,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: ReFORM是一种基于流策略的离线强化学习方法，通过构造强制执行较宽松的支持约束，解决了分布外误差问题，同时在OGBench基准测试中优于所有基线方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临两个主要挑战：1）分布外误差问题，当策略离开训练分布时会产生误差；2）最优策略分布可能是多模态且难以表示的。现有方法要么过度约束策略改进，要么无法同时保证策略表达能力和避免分布外误差。

Method: ReFORM基于流策略设计：1）学习一个有界源分布的行为克隆流策略来捕捉动作分布的支持；2）优化一个反射流，为BC流生成有界噪声同时保持支持，以最大化性能。这种方法通过构造强制执行支持约束。

Result: 在OGBench基准的40个具有不同质量数据集的挑战性任务中，使用恒定超参数的ReFORM在性能曲线图上优于所有经过手动调优超参数的基线方法。

Conclusion: ReFORM通过流策略和反射流设计，在保持策略表达能力的同时有效避免了分布外误差，为离线强化学习提供了一种有效的解决方案。

Abstract: Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.

</details>


### [40] [StagePilot: A Deep Reinforcement Learning Agent for Stage-Controlled Cybergrooming Simulation](https://arxiv.org/abs/2602.05060)
*Heajun An,Qi Zhang,Minqian Liu,Xinyi Zhang,Sang Won Lee,Lifu Huang,Pamela J. Wisniewski,Jin-Hee Cho*

Main category: cs.LG

TL;DR: StagePilot是一个基于离线强化学习的对话代理，通过模拟网络诱骗行为的阶段性进展来进行预防训练，使用复合奖励平衡用户情感和目标接近度，在LLM模拟评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 网络诱骗对青少年构成持续威胁，需要主动的教育干预措施来预防这种危害。

Method: 提出StagePilot离线RL对话代理，使用复合奖励函数平衡用户情感和目标接近度，约束阶段转换到相邻阶段以保证真实性和可解释性。

Result: 在LLM模拟评估中，StagePilot生成真实连贯的对话，IQL+AWAC代理在战略规划和情感连贯性方面表现最佳，达到最终阶段的频率比基线高43%，同时保持超过70%的情感对齐。

Conclusion: StagePilot能够有效模拟网络诱骗的阶段性行为，为预防训练提供实用的对话代理工具。

Abstract: Cybergrooming is an evolving threat to youth, necessitating proactive educational interventions. We propose StagePilot, an offline RL-based dialogue agent that simulates the stage-wise progression of grooming behaviors for prevention training. StagePilot selects conversational stages using a composite reward that balances user sentiment and goal proximity, with transitions constrained to adjacent stages for realism and interpretability. We evaluate StagePilot through LLM-based simulations, measuring stage completion, dialogue efficiency, and emotional engagement. Results show that StagePilot generates realistic and coherent conversations aligned with grooming dynamics. Among tested methods, the IQL+AWAC agent achieves the best balance between strategic planning and emotional coherence, reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.

</details>


### [41] [Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling](https://arxiv.org/abs/2602.05087)
*Parsa Vares*

Main category: cs.LG

TL;DR: AutoDiscover将主动学习重构为在线决策问题，使用异构图神经网络和动态策略组合管理，在系统文献综述筛选任务中比静态方法更高效


<details>
  <summary>Details</summary>
Motivation: 系统文献综述的手动筛选面临低相关研究比例和专家标注稀缺昂贵的挑战，传统主动学习方法使用固定查询策略，无法适应动态变化且忽略科学文献网络结构

Method: 提出AutoDiscover框架：1) 将文献建模为包含文档、作者和元数据的异构图；2) 使用异构图注意力网络学习节点表示；3) 采用折扣汤普森采样智能体动态管理查询策略组合；4) 结合实时人工标注平衡探索与利用

Result: 在26个数据集的SYNERGY基准测试中，AutoDiscover比静态主动学习方法筛选效率更高，特别是在冷启动场景下，能从最小初始标注中有效发现相关文献

Conclusion: AutoDiscover通过自适应智能体方法显著加速系统文献综述筛选过程，同时开发了TS-Insight可视化分析工具帮助解释和诊断智能体决策

Abstract: Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent's decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.

</details>


### [42] [Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks](https://arxiv.org/abs/2602.05125)
*William F. Shen,Xinchi Qiu,Chenxi Whitehouse,Lisa Alazraki,Shashwat Goel,Francesco Barbieri,Timon Willi,Akhil Mathur,Ilias Leontiadis*

Main category: cs.LG

TL;DR: RRD框架通过递归分解-过滤循环优化评分标准，提升LLM评判准确性和强化微调效果


<details>
  <summary>Details</summary>
Motivation: 现有评分标准存在覆盖不足、维度混淆、偏好方向错位、冗余相关等问题，导致评判准确性下降和强化微调效果不佳

Method: 提出RRD框架，采用递归分解-过滤循环：分解粗粒度标准为细粒度判别性准则，过滤错位和冗余标准，使用相关性感知加权方案

Result: 在JudgeBench和PPE上显著提升评判准确性（最高+17.7分），在WildChat上强化微调效果大幅提升（Qwen3-4B奖励提升160%，Llama3.1-8B提升60%）

Conclusion: RRD为开放领域LLM评判和奖励建模提供了可扩展、可解释的基础框架

Abstract: Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.

</details>


### [43] [SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines](https://arxiv.org/abs/2602.05134)
*Olga Ovcharenko,Matthias Boehm,Sebastian Schelter*

Main category: cs.LG

TL;DR: SemPipes是一个新颖的声明式编程模型，将LLM驱动的语义数据操作符集成到表格ML管道中，通过自然语言指定数据转换，并基于数据特征和上下文合成自定义实现，自动优化管道操作。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的表格机器学习需要复杂的数据准备管道，这需要大量领域专业知识和工程努力。研究如何利用大语言模型通过代码合成来支持表格ML，减少人工设计和优化的工作量。

Method: 引入SemPipes声明式编程模型，集成LLM驱动的语义数据操作符。语义操作符用自然语言指定数据转换，运行时系统执行。训练时基于数据特征、操作指令和管道上下文合成自定义实现，通过进化搜索引导的LLM代码合成自动优化数据操作。

Result: 在多样化的表格ML任务中评估SemPipes，结果显示语义操作符显著提升了专家设计和代理生成管道的端到端预测性能，同时降低了管道复杂度。

Conclusion: SemPipes通过LLM驱动的语义操作符和自动代码合成，有效支持表格ML管道设计，提高预测性能并减少复杂性，为自动化表格数据处理提供了新方法。

Abstract: Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.

</details>


### [44] [Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.05183)
*John Yan,Michael Yu,Yuqi Sun,Alexander Duffy,Tyler Marques,Matthew Lyle Olson*

Main category: cs.LG

TL;DR: 该论文提出Meta-Autointerp方法，使用稀疏自编码器(SAEs)和LLM摘要器分析复杂强化学习环境中LLM的训练动态，发现了细粒度行为模式，但发现大多数自动生成的解释对人类帮助有限。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在复杂强化学习多智能体环境中训练，理解训练过程中行为变化变得困难。需要数据中心的可解释性方法来分析训练动态。

Method: 提出Meta-Autointerp方法，应用预训练的稀疏自编码器(SAEs)和LLM摘要器分析Full-Press Diplomacy环境中的大规模强化学习训练运行，将SAE特征分组为可解释的假设。

Result: 发现了角色扮演模式、退化输出、语言切换等细粒度行为，以及高级战略行为和特定环境bug。90%的SAE元特征显著，发现了奖励黑客行为。但用户研究发现大多数SAE特征和LLM生成假设对人类帮助有限，只有部分SAE衍生假设对下游任务有预测价值。

Conclusion: SAEs和LLM摘要器提供了互补的视角来分析智能体行为，该框架为未来数据中心的可解释性工作提供了实用起点，以确保LLM在训练过程中的可信行为。

Abstract: Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent's system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.

</details>


### [45] [Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions](https://arxiv.org/abs/2602.05234)
*Yuntai Bao,Xuhong Zhang,Jintao Chen,Ge Su,Yuxiang Cai,Hao Peng,Bing Sun,Haiqin Weng,Liu Yan,Jianwei Yin*

Main category: cs.LG

TL;DR: 提出Concept DAS（CDAS）方法，基于分布式对齐搜索原理，通过分布匹配目标而非概率最大化来实现更忠实、稳定的模型干预控制


<details>
  <summary>Details</summary>
Motivation: 当前基于干预的模型控制方法容易过拟合且性能不佳，因为有效的控制需要识别内部机制而非强加外部偏好

Method: 基于分布式对齐搜索（DAS）原理，采用分布式交换干预（DII）机制，引入针对控制任务的分布匹配目标，使干预输出分布与反事实分布对齐

Result: 在AxBench基准测试中，CDAS不一定优于偏好优化方法，但能从模型规模扩大中获益更多；在安全相关案例中能系统控制模型同时保持通用性能

Conclusion: CDAS与偏好优化方法互补，在特定条件下构成稳健的干预式模型控制方法

Abstract: Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at https://github.com/colored-dye/concept_das.

</details>


### [46] [Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates](https://arxiv.org/abs/2602.05311)
*Chengxiao Wang,Haoze Wu,Gagandeep Singh*

Main category: cs.LG

TL;DR: 该论文提出了一种合成鲁棒神经李雅普诺夫屏障证书的方法，用于在系统动力学存在扰动的情况下验证深度强化学习控制器的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有神经李雅普诺夫和屏障证书方法仅在固定理想无扰动动力学下提供保证，限制了其在现实世界应用中的可靠性，因为实际系统动力学可能因不确定性而偏离。

Method: 提出鲁棒李雅普诺夫屏障函数的正式定义，基于Lipschitz连续性指定充分条件以确保对有界扰动的鲁棒性。通过对抗训练、Lipschitz邻域边界和全局Lipschitz正则化等实用训练目标来强制执行这些条件。

Result: 在Inverted Pendulum和2D Docking两个实际相关环境中验证方法。相比基线，显著提高了认证鲁棒性边界（高达4.6倍）和强扰动下的经验成功率（高达2.4倍）。

Conclusion: 研究结果表明，在动力学存在扰动的情况下，训练鲁棒神经证书对于安全强化学习是有效的。

Abstract: Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.

</details>


### [47] [DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders](https://arxiv.org/abs/2602.05859)
*Xu Wang,Bingqing Jiang,Yu Wan,Baosong Yang,Lingpeng Kong,Difan Zou*

Main category: cs.LG

TL;DR: 提出了首个针对扩散语言模型（DLMs）的稀疏自编码器（SAE）可解释性框架DLM-Scope，发现SAE在DLMs中的行为与自回归LLMs不同，能有效提取可解释特征并实现更好的干预效果。


<details>
  <summary>Details</summary>
Motivation: 随着扩散语言模型（DLMs）成为自回归大语言模型（LLMs）的有前景替代方案，需要为这类新兴模型开发专门的可解释性工具。稀疏自编码器（SAEs）在LLMs中已成为标准可解释性工具，但尚未应用于DLMs。

Method: 提出了DLM-Scope框架，使用Top-K稀疏自编码器（SAEs）提取DLMs的可解释特征。研究了SAE插入对DLMs的影响，比较了与LLMs的差异，并探索了SAE在DLMs解码顺序和训练后稳定性等新研究方向。

Result: 1) SAE在DLMs早期层插入能减少交叉熵损失，与LLMs中通常增加损失的现象相反；2) DLM的SAE特征能实现更有效的扩散时间干预，优于LLM引导；3) SAE能为DLM解码顺序提供有用信号；4) SAE特征在DLMs训练后阶段保持稳定。

Conclusion: 该工作为DLMs的机制可解释性奠定了基础，展示了SAEs在DLM相关任务和算法中的巨大应用潜力，为这一新兴模型类别提供了专门的可解释性工具。

Abstract: Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.

</details>


### [48] [Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations](https://arxiv.org/abs/2602.05885)
*Wei Liu,Jiawei Xu,Yingru Li,Longtao Zheng,Tianjian Li,Qian Liu,Junxian He*

Main category: cs.LG

TL;DR: 提出KernelGYM分布式GPU环境，开发TRLOO方法解决多轮RL中的偏差问题，结合Profiling-based Rewards和Rejection Sampling训练Dr.Kernel-14B模型，在KernelBench上性能超越Claude-4.5-Sonnet和GPT-5


<details>
  <summary>Details</summary>
Motivation: 高质量内核对可扩展AI系统至关重要，但训练LLM生成内核代码面临数据不足、环境脆弱、奖励黑客攻击和懒惰优化等问题，需要系统研究强化学习在内核生成中的应用

Method: 1) 设计KernelGYM分布式GPU环境支持奖励黑客检查、多轮交互数据收集和长期RL训练；2) 提出TRLOO方法解决GRPO中的自包含偏差问题；3) 引入Profiling-based Rewards和Profiling-based Rejection Sampling解决懒惰优化；4) 结合不匹配校正提升训练稳定性

Result: Dr.Kernel-14B在KernelBench上性能与Claude-4.5-Sonnet相当，在Level-2子集上31.6%生成的内核达到至少1.2倍加速，超越Claude-4.5-Sonnet(26.7%)和GPT-5(28.6%)；多轮最佳候选选择时加速率提升至47.8%

Conclusion: 通过系统研究内核生成的强化学习方法，开发了有效的训练环境和算法，成功训练出高性能内核生成模型，证明了多轮RL和Profiling-based方法在内核优化中的有效性

Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.

</details>


### [49] [DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training](https://arxiv.org/abs/2602.05890)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Jiahan Li,Chenhao Huang,Junjie Ye,Sixian Li,Mingxu Chai,Yuhui Wang,Yajie Yang,Ming Zhang,Jiazheng Zhang,Shichun Liu,Caishuang Huang,Yunke Zhang,Yuran Wang,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: DFPO提出了一种鲁棒的分布强化学习框架，通过将价值建模为跨时间步的连续流而非独立的分位数预测，在噪声监督下实现更稳定的训练和更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中训练RL系统面临噪声监督和域外泛化差的挑战，特别是在LLM后训练中。现有的分布RL方法虽然通过建模多个分位数点提高了鲁棒性，但每个分位数仍作为标量独立学习，导致价值表示粗糙，缺乏对状态信息的细粒度条件化，在复杂和OOD条件下表现不佳。

Method: DFPO（Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control）将价值建模为跨时间步的连续流，通过学习价值流场而非孤立的分位数预测来扩展价值建模。为了在噪声反馈下稳定训练，DFPO进一步集成了条件风险控制和价值流轨迹上的一致性约束。

Result: 在对话、数学推理和科学任务上的实验表明，DFPO在噪声监督下优于PPO、FlowRL和其他鲁棒基线，实现了改进的训练稳定性和泛化能力。

Conclusion: DFPO通过连续价值流建模和条件风险控制，为噪声环境下的RL训练提供了更鲁棒的框架，特别是在LLM后训练等复杂场景中表现出色。

Abstract: Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.

</details>


### [50] [When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL](https://arxiv.org/abs/2602.05459)
*Jan Malte Töpperwien,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 研究发现深度强化学习中的超参数敏感性并非RL问题固有，而是由自举机制加剧。在离线目标条件RL中，当存在适度专家数据时，基于准度量表示学习的算法比基于自举TD学习的算法表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中的超参数敏感性通常被认为是不可避免的，但尚不清楚这是RL问题的固有特性还是特定训练机制加剧的结果。本研究旨在探究在离线目标条件RL中，当数据分布固定且非平稳性可被明确控制时，超参数敏感性的本质。

Method: 研究离线目标条件RL，通过调度数据质量变化来明确控制非平稳性。在平稳和非平稳两种机制下研究不同数据质量，并比较两种代表性算法：HIQL（基于自举TD学习）和QRL（准度量表示学习）。引入跨目标梯度对齐诊断来分析算法差异。

Result: 观察到比在线RL更强的超参数配置鲁棒性，即使在受控非平稳性下也是如此。当存在约20%专家数据时，QRL保持广泛稳定的近最优区域，而HIQL表现出尖锐的最优点且在不同训练阶段显著漂移。自举目标表现出更强的破坏性梯度干扰，这与超参数敏感性直接相关。

Conclusion: 训练期间对超参数配置变化的高敏感性在RL中并非不可避免，而是由自举机制动态加剧。这为设计更鲁棒的算法目标提供了途径。

Abstract: Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\approx$ 20\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.

</details>


### [51] [Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification](https://arxiv.org/abs/2602.05535)
*Tao Huang,Rui Wang,Xiaofei Liu,Yi Qin,Li Duan,Liping Jing*

Main category: cs.LG

TL;DR: EUQ提出了一种细粒度的证据不确定性量化方法，通过建模正负证据来检测LVLM的误行为，包括幻觉、越狱、对抗攻击和OOD失败，效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在面对不完整或对抗性输入时会产生不可靠甚至有害的内容（如事实幻觉或危险指令），这些误行为源于认知不确定性（内部知识冲突或信息缺失），而现有不确定性量化方法仅捕捉整体认知不确定性，对此类问题的检测效果有限。

Method: 提出证据不确定性量化方法，将模型输出头的特征解释为支持（正）或反对（负）的证据，利用证据理论建模和聚合这些证据，在单次前向传播中量化内部冲突和知识缺口。

Result: 在四种误行为类别（幻觉、越狱、对抗漏洞、OOD失败）上对最先进的LVLM进行广泛评估，EUQ始终优于强基线，发现幻觉对应高内部冲突，OOD失败对应高无知度。层间证据不确定性动态分析有助于从新视角解释内部表示的演化。

Conclusion: EUQ提供了一种有效的细粒度不确定性量化方法，能够检测LVLM的误行为，为模型安全部署提供了重要工具，并有助于理解模型内部表示的形成过程。

Abstract: Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.

</details>


### [52] [Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation](https://arxiv.org/abs/2602.05548)
*Zhiqi Yu,Zhangquan Chen,Mengting Liu,Heye Zhang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 本文提出A-GRAE方法，通过非对称优势估计解决GRPO中的探索效率和难度适应问题，在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: RLVR（特别是GRPO）已成为激发LLM推理的标准方法，但其在探索效率和难度适应方面存在瓶颈。作者认为这些瓶颈源于组相对优势估计（GRAE）中隐含的优势对称性，这种对称性导致两个关键限制：1）群体层面，正确与错误轨迹间的严格对称权重阻碍新正确解决方案的探索；2）样本层面，算法隐含优先中等难度样本，无法适应难度聚焦的非平稳需求。

Method: 提出非对称GRAE（A-GRAE）方法，通过动态调节探索激励和样本难度聚焦来解决GRAE的对称性问题。具体包括：1）非对称抑制正确轨迹的优势以鼓励必要探索；2）采用课程式学习策略，初始优先简单样本，然后逐渐转向复杂样本。

Result: 在七个基准测试上的实验表明，A-GRAE能够持续改进GRPO及其变体，在LLM和MLLM上均取得显著性能提升。

Conclusion: GRAE中的优势对称性是次优的，通过引入非对称性和课程式难度聚焦可以显著提升强化学习在可验证奖励设置中的效率和适应性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.

</details>


### [53] [Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation](https://arxiv.org/abs/2602.05656)
*Igor Santos-Grueiro*

Main category: cs.LG

TL;DR: 行为评估无法唯一识别潜在对齐属性，即使通过理想化行为测试也无法验证模型的内在对齐性，只能估计不可区分性类别。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对齐评估主要依赖行为测试（基准、红队测试等），将观察到的合规行为视为内在对齐的证据。但这种方法隐含地将行为证据推断为潜在对齐属性，缺乏对这种推断过程本身的形式化分析。

Method: 将对齐评估形式化为部分可观测性下的可识别性问题，允许智能体行为依赖于与评估机制相关的信息。引入对齐可验证性问题和规范性不可区分性概念，分析不同潜在对齐假设如何产生相同的评估者可访问信号分布。

Result: 主要结果为负面的可识别性定理：在有限行为评估和评估感知智能体条件下，观察到的行为合规性不能唯一识别潜在对齐性。即使理想化的行为评估也无法一般性地验证对齐作为潜在属性。行为对齐测试应解释为不可区分性类别的估计器而非对齐验证器。

Conclusion: 行为对齐基准应重新理解为在特定机制内提供可观测合规性的上界，而非底层对齐的保证。通过更严格的测试可以缩小兼容假设空间，但在所述条件下无法将其缩小为单一假设。

Abstract: Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.
  We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.
  Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.
  We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.

</details>


### [54] [Learning to Inject: Automated Prompt Injection via Reinforcement Learning](https://arxiv.org/abs/2602.05746)
*Xin Chen,Jie Zhang,Florian Tramer*

Main category: cs.LG

TL;DR: AutoInject：基于强化学习的自动化提示注入攻击框架，通过生成通用对抗后缀，可有效攻击前沿LLM系统


<details>
  <summary>Details</summary>
Motivation: 当前提示注入攻击主要依赖人工红队和手工制作的提示，缺乏可扩展性和适应性，需要自动化优化方法

Method: 提出AutoInject强化学习框架，联合优化攻击成功率和良性任务效用保持，支持查询优化和迁移攻击

Result: 仅使用1.5B参数的后缀生成器，成功攻击GPT 5 Nano、Claude Sonnet 3.5、Gemini 2.5 Flash等前沿系统

Conclusion: 为自动化提示注入研究建立了更强基线，展示了强化学习在生成通用对抗后缀方面的有效性

Abstract: Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.

</details>


### [55] [Cross-Domain Offline Policy Adaptation via Selective Transition Correction](https://arxiv.org/abs/2602.05776)
*Mengbei Yan,Jiafei Lyu,Shengjie Sun,Zhongjian Qiao,Jingwen Yang,Zichuan Lin,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: 提出选择性转移校正算法，通过修正源域数据来适应目标域动态，解决跨域离线强化学习中的动态不匹配问题


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，跨域策略适应面临动态不匹配的挑战。现有方法通过源域转移过滤或奖励修改来缓解，但可能导致对宝贵源域数据利用不足。需要更有效的方法来利用源域数据增强目标域策略学习。

Method: 提出选择性转移校正算法，利用逆策略模型和奖励模型修正源域转移的动作和奖励，使其与目标域动态对齐。进一步使用前向动态模型保留比原始转移更匹配目标动态的修正样本。

Result: 在具有动态偏移的各种环境实验中，STC算法相比现有基线实现了更优越的性能。

Conclusion: 通过修正源域数据而非直接合并或过滤，STC能够可靠地利用源域数据进行策略适应，有效解决跨域离线强化学习中的动态不匹配问题。

Abstract: It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.

</details>


### [56] [Distributional Reinforcement Learning with Diffusion Bridge Critics](https://arxiv.org/abs/2602.05783)
*Shutong Ding,Yimiao Zhou,Ke Hu,Mokai Pan,Shan Zhong,Yanwei Fu,Jingya Wang,Ye Shi*

Main category: cs.LG

TL;DR: 提出了一种基于扩散桥的分布强化学习方法（DBC），首次将扩散桥模型用作评论家，通过直接建模Q值的逆累积分布函数来准确捕捉价值分布。


<details>
  <summary>Details</summary>
Motivation: 现有扩散强化学习方法主要关注扩散策略，而忽略了扩散评论家。由于策略优化本质上依赖于评论家，准确的价值估计比策略表达能力更重要。考虑到强化学习任务的随机性，评论家更适合用分布模型来描述。

Method: 提出扩散桥评论家（DBC），直接建模Q值的逆累积分布函数，利用扩散桥的强大分布匹配能力防止价值分布坍缩为平凡高斯分布。还推导了解析积分公式来解决DBC中的离散化误差问题。

Result: 在MuJoCo机器人控制基准测试中，DBC相比之前的分布评论家模型表现出优越性。DBC是一个即插即用组件，可以集成到大多数现有RL框架中。

Conclusion: DBC是首个将扩散桥模型用作评论家的工作，能够准确捕捉价值分布，在分布强化学习中表现出色，具有很好的通用性和实用性。

Abstract: Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.

</details>


### [57] [Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents](https://arxiv.org/abs/2602.05810)
*Quan M. Tran,Zhuo Huang,Wenbin Zhang,Bo Han,Koji Yatani,Masashi Sugiyama,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出Bifrost方法，通过揭示上下文-轨迹相关性，利用上下文差异引导先前解决轨迹适应目标任务，无需训练即可缓解上下文偏移导致的错位问题。


<details>
  <summary>Details</summary>
Motivation: 自主代理通过反思和迭代改进进行自我提升，重用成功任务轨迹作为上下文示例。但跨任务时存在上下文不匹配问题，现有方法要么丢弃轨迹，要么使用启发式方法处理，导致微调成本高或性能无法保证。

Method: 提出Bifrost方法：1）揭示上下文-轨迹相关性，发现上下文偏移与轨迹偏移高度平行；2）利用上下文差异精确指导先前解决轨迹适应目标任务；3）在表示层面使用代理隐藏状态进行轨迹适应，确保轨迹转换在共享空间中与目标上下文准确对齐。

Result: 在多样化基准测试中，Bifrost始终优于现有轨迹重用和微调自我改进方法，表明代理能够有效利用过去经验，即使存在显著的上下文偏移。

Conclusion: Bifrost通过揭示上下文-轨迹相关性并利用上下文差异指导轨迹适应，实现了无需训练的有效轨迹重用，解决了上下文偏移问题，显著提升了自主代理的自我改进能力。

Abstract: Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.

</details>


### [58] [ContextBench: A Benchmark for Context Retrieval in Coding Agents](https://arxiv.org/abs/2602.05892)
*Han Li,Letian Zhu,Bohan Zhang,Rili Feng,Jiaming Wang,Yue Pan,Earl T. Barr,Sarro Federica,Zhaoyang Chu,He Ye*

Main category: cs.LG

TL;DR: ContextBench是一个面向过程的评估框架，用于评估代码代理在解决问题时的上下文检索能力，包含1136个任务和人工标注的黄金上下文，揭示了代理在上下文检索中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代码代理评估主要关注最终任务成功率，缺乏对代理在解决问题过程中如何检索和使用代码上下文的理解。需要一种过程导向的评估方法来深入分析上下文检索的质量和效率。

Method: 构建ContextBench基准，包含1136个问题解决任务，覆盖66个仓库和8种编程语言，每个任务都有人工标注的黄金上下文。实现自动化评估框架，跟踪代理执行轨迹，测量上下文召回率、精确率和效率。

Result: 评估4个前沿LLM和5个代码代理，发现：1) 复杂的代理框架在上下文检索上只有边际收益；2) LLM普遍偏向召回率而非精确率；3) 探索的上下文与实际使用的上下文之间存在显著差距。

Conclusion: ContextBench通过中间黄金上下文指标补充了现有的端到端基准测试，为理解问题解决过程提供了有价值的中间信号，可用于指导LLM在软件任务中的推理。

Abstract: LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval ("The Bitter Lesson" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.

</details>


### [59] [Chunky Post-Training: Data Driven Failures of Generalization](https://arxiv.org/abs/2602.05910)
*Seoirse Murray,Allison Qi,Timothy Qian,John Schulman,Collin Burns,Sara Price*

Main category: cs.LG

TL;DR: 论文提出SURF和TURF工具，用于检测和追踪大语言模型后训练中因数据块导致的虚假相关性行为问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练使用多种数据集，但这些数据集编码了意外模式（如格式与内容的相关性、狭窄的措辞等），导致模型学习到虚假相关性，产生让开发者惊讶的行为。

Method: 提出SURF（运行时检测意外行为的黑盒管道）和TURF（将失败行为追溯到特定后训练数据的工具），应用于前沿模型和开源模型进行分析。

Result: 应用这些工具发现，块状后训练导致校准错误的行为，这些行为通常源于不平衡或未充分指定的后训练数据块。

Conclusion: 后训练数据中的块状模式会导致模型学习虚假相关性，需要工具来检测和追踪这些问题以提高模型可靠性。

Abstract: LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (Tülu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.

</details>


### [60] [Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training](https://arxiv.org/abs/2602.05933)
*Zhenghao Xu,Qin Lu,Changlong Yu,Tuo Zhao*

Main category: cs.LG

TL;DR: 论文提出PMD-mean算法，通过用采样策略的平均奖励近似对数配分函数，在LLM强化学习中实现更稳定高效的策略优化。


<details>
  <summary>Details</summary>
Motivation: 策略镜像下降(PMD)为强化学习提供了理论框架，但在LLM的大动作空间中，精确估计配分函数面临挑战，特别是在有限样本情况下。需要一种更实用的近似方法来解决这一问题。

Method: 提出PMD-mean算法，用采样策略的平均奖励近似对数配分函数，在log-policy空间进行回归。该方法隐式优化了带有自适应混合KL-χ²正则化的镜像下降子问题。

Result: 在数学推理任务上的实验表明，PMD-mean实现了更优的性能，具有更好的稳定性和时间效率。额外的χ²正则化约束了概率的大幅变化，在期望奖励较低时产生更保守的更新。

Conclusion: PMD-mean为LLM的强化学习算法提供了原则性改进途径，加深了对该方法的理解，并展示了在实际应用中的优势。

Abstract: Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.

</details>


### [61] [Inverse Depth Scaling From Most Layers Being Similar](https://arxiv.org/abs/2602.05970)
*Yizhou Liu,Sara Kangaslahti,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 研究发现大语言模型中损失与深度成反比缩放，这种低效但稳健的机制源于残差网络架构偏差和目标函数不兼容平滑动态，而非组合学习或离散化平滑动态。


<details>
  <summary>Details</summary>
Motivation: 虽然神经缩放定律描述了损失与模型规模的关系，但深度和宽度对性能的影响可能不同，需要更详细的研究来量化深度如何影响损失。

Method: 通过分析大语言模型和玩具残差网络，量化深度对损失的影响，研究损失与深度的缩放关系。

Result: 发现大语言模型中损失与深度成反比缩放，这种机制源于功能相似层的集成平均，而非组合学习或离散化平滑动态。

Conclusion: 当前深度利用方式低效但稳健，改进大语言模型效率可能需要架构创新来鼓励深度的组合使用。

Abstract: Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.

</details>


### [62] [$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment](https://arxiv.org/abs/2602.05946)
*Rajdeep Haldar,Lantao Mei,Guang Lin,Yue Xing,Qifan Song*

Main category: cs.LG

TL;DR: 提出基于f-散度的统一对齐框架f-GRPO和f-HAL，在RLVR和偏好对齐任务上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法可视为对齐与未对齐响应分布间的散度估计器，本研究将此散度视角扩展到一般对齐设置（如仅有环境奖励的RLVR），建立统一框架

Method: 基于f-散度的变分表示，提出f-GRPO（在线策略强化学习）和f-HAL（混合在线/离线策略目标）两类对齐目标

Result: 理论保证对齐后平均奖励提升，实证在RLVR（数学推理）和PA（安全对齐）任务上验证了框架的优越性能和灵活性

Conclusion: 提出的基于f-散度的统一对齐框架为LLM对齐提供了理论保证和实证有效的解决方案，在多种对齐设置中表现优异

Abstract: Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.

</details>


### [63] [On Computation and Reinforcement Learning](https://arxiv.org/abs/2602.05999)
*Raj Ghugare,Michał Bortkiewicz,Alicja Ziarko,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文研究了计算资源对强化学习策略的影响，证明了增加计算量可以解决更多问题并提升泛化能力，提出了一种能够灵活使用不同计算量的架构，并在31个任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架缺乏形式化描述计算资源对策略影响的语言，且深度强化学习策略通常使用固定架构的神经网络，混淆了计算量和参数数量。本文旨在研究计算资源如何影响策略学习，以及固定参数量的策略是否能从额外计算中受益。

Method: 基于算法学习和无模型规划的前期工作，提出了一种能够使用可变计算量的最小化架构。该架构允许策略根据可用计算资源动态调整计算量，而不改变参数数量。

Result: 在31个不同的在线和离线强化学习任务上的实验表明：(1) 该架构仅通过使用更多计算就能获得更强的性能；(2) 相比使用多达5倍参数的普通前馈网络或深度残差网络，该架构在长时域测试任务上表现出更强的泛化能力。

Conclusion: 计算资源对强化学习策略有重要影响，增加计算量可以解决更多问题并提升泛化能力。提出的可变计算量架构能够有效利用额外计算资源，在保持参数数量不变的情况下显著提升性能。

Abstract: How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [64] [Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set](https://arxiv.org/abs/2602.04910)
*Nongyu Di,Tianyu Chen,Shan Lu,Shuai Lu,Yeyun Gong,Peng Cheng,Jacob R. Lorch,Yuan Yao,Xiaoxing Ma*

Main category: cs.SE

TL;DR: VeruSyn是一个为Rust验证工具Verus设计的数据合成管道，通过自合成和教程合成生成大量带形式证明的Rust程序，创建了690万个验证程序的数据集，并训练出优于商业模型的代码证明生成模型。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码正确性存在问题，虽然让LLM同时生成形式证明可以解决这个问题，但代码证明生成需要更强的推理能力且缺乏训练数据。特别是针对Verus（Rust系统软件验证工具）的数据集规模有限。

Method: 1. 自合成：从现有Verus程序中生成新变体；2. 教程合成：从Verus教程中提取模式生成新程序；3. 代理轨迹合成：补充长链思维数据。通过这些方法合成690万个带形式规范和证明的Rust程序。

Result: 创建了最大的Verus验证程序数据集（690万个程序），基于此微调的Qwen2.5-Coder-32B-Instruct模型在成本-证明权衡上优于Claude Sonnet 4.5等商业模型，也显著优于o4-mini和之前的研究模型。

Conclusion: VeruSyn通过创新的数据合成方法解决了代码证明生成的训练数据稀缺问题，创建的大规模数据集使得训练出的模型在代码证明生成任务上达到甚至超越商业模型的性能。

Abstract: Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.

</details>


### [65] [ASA: Activation Steering for Tool-Calling Domain Adaptation](https://arxiv.org/abs/2602.04935)
*Youjin Wang,Run Zhou,Rong Fu,Shuaishuai Cao,Hongwei Zeng,Jiaxuan Lu,Sicheng Fan,Jiaqiao Zhao,Liangming Pan*

Main category: cs.SE

TL;DR: 提出ASA（激活引导适配器），一种轻量级、无需训练、推理时机制，通过读取中间激活信号并使用超轻路由器产生自适应控制强度，实现精确领域对齐，解决LLM智能体在多领域工具生态系统中的高效适应问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中通用LLM智能体部署的核心挑战不是工具使用本身，而是在快速演变的工具集、API和协议下的高效领域适应。传统方法如重复LoRA或SFT训练成本指数增长，而提示或模式方法在分布偏移和复杂接口下脆弱。

Method: 提出ASA（激活引导适配器），一种轻量级、推理时、无需训练的机制。它从中间激活中读取路由信号，使用超轻路由器产生自适应控制强度，实现精确的领域对齐。

Result: 在多个模型规模和领域中，ASA实现了与LoRA相当的适应效果，同时显著降低开销，并具有强大的跨模型可转移性。

Conclusion: ASA为具有频繁接口变化的稳健、可扩展和高效的多领域工具生态系统提供了理想的实用解决方案。

Abstract: For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.

</details>


### [66] [EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering](https://arxiv.org/abs/2602.05242)
*Chenhui Mao,Yuanting Lei,Zhixiang Wei,Ming Liang,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: 提出EGSS框架，通过熵引导自适应搜索和测试套件增强，解决代理测试时扩展的计算开销问题，在SWE任务上提升性能5-10%，同时减少28%推理token使用。


<details>
  <summary>Details</summary>
Motivation: 代理测试时扩展(TTS)在代码生成和bug修复等软件工程任务上表现优异，但实际应用受限：1) 大型集成模型部署成本高；2) 缺乏可靠机制选择最优候选方案，限制了性能提升。

Method: 提出熵引导逐步扩展(EGSS)框架，通过熵引导自适应搜索动态平衡效率与效果，结合鲁棒的测试套件增强，减少计算开销同时提升性能。

Result: 在SWE-Bench-Verified上，EGSS将Kimi-K2-Intruct解决率从63.2%提升到72.2%，GLM-4.6从65.8%提升到74.6%，达到开源大模型SOTA水平，同时减少28%推理token使用。

Conclusion: EGSS有效解决了TTS的计算开销问题，在保持性能提升的同时显著提高计算效率，为代理测试时扩展的实际应用提供了可行方案。

Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.

</details>


### [67] [PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models](https://arxiv.org/abs/2602.05270)
*Thanh Le-Cong,Bach Le,Toby Murray,Michael Pradel,Cristian Cadar*

Main category: cs.SE

TL;DR: PatchGuru：首个从真实PR中自动推断可执行补丁规范的技术，通过LLM从自然语言描述中提取开发者意图，合成补丁预言（运行时断言），用于验证补丁行为，在400个Python项目PR中检测到24个真实bug，精度0.62。


<details>
  <summary>Details</summary>
Motivation: 软件系统演化中，补丁可能无意改变程序行为。由于回归测试不完整且补丁意图描述是非正式的自然语言，验证补丁是否符合预期语义很困难。需要自动化技术来推断可执行的补丁规范。

Method: PatchGuru利用LLM从PR的自然语言描述中提取开发者意图，合成补丁预言——在集成补丁前后版本的比较程序中表达的运行时断言。通过迭代比较补丁前后行为、识别违规、自审过滤不一致性，生成bug报告。

Result: 在4个广泛使用的开源Python项目的400个PR上评估，PatchGuru报告39个警告，精度0.62，确认24个真实bug（包括12个先前未知的bug，其中11个被开发者修复）。相比最先进技术Testora，多检测17个bug（24 vs 7），精度从0.32提升到0.62。平均每个PR成本8.9分钟和0.07美元。

Conclusion: PatchGuru通过提供可执行文档和补丁意图的自动验证，补充了代码审查和回归测试。结果表明该方法能有效检测补丁引入的bug，具有实用价值。

Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.

</details>


### [68] [Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations](https://arxiv.org/abs/2602.05523)
*Shahin Honarvar,Amber Gorzynski,James Lee-Jones,Harry Coppock,Marek Rei,Joseph Ryan,Alastair F. Donaldson*

Main category: cs.SE

TL;DR: 该论文提出CTF挑战家族的概念，通过语义保持的程序变换生成语义等价的挑战变体，用于评估LLM代理在网络安全任务中的鲁棒性和泛化能力，并开发了Evolve-CTF工具进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于CTF的网络安全评估基准只能进行点对点测试，无法评估LLM代理在不同源代码变体上的鲁棒性和泛化能力。需要一种能够控制变量、保持漏洞利用策略不变的方法来系统评估代理对源代码变换的适应性。

Method: 提出CTF挑战家族概念，使用语义保持的程序变换生成挑战变体。开发Evolve-CTF工具，对Python CTF挑战应用多种变换（重命名、代码插入、组合变换等）生成家族。基于Cybench和Intercode挑战构建数据集，评估13种带工具访问的LLM代理配置。

Result: 模型对侵入性重命名和代码插入变换表现出较强鲁棒性，但组合变换和深度混淆会影响性能，需要更复杂的工具使用。显式推理能力对解决成功率影响有限。研究提供了评估技术和工具，以及当前SOTA模型在该领域能力的大规模数据集。

Conclusion: CTF挑战家族方法为LLM评估提供了有价值的评估技术和工具，能够系统评估代理对源代码变换的鲁棒性。研究发现当前模型在某些变换上表现鲁棒，但在复杂变换下仍需改进，为未来LLM评估提供了新视角。

Abstract: Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.

</details>


### [69] [ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval](https://arxiv.org/abs/2602.05550)
*Yulong He,Artem Ermakov,Sergey Kovalchuk,Artem Aliev,Dmitry Shalymov*

Main category: cs.SE

TL;DR: 构建首个大规模ArkTS代码数据集，用于代码检索和评估任务，并评估现有代码嵌入模型，通过微调获得高性能ArkTS代码理解模型。


<details>
  <summary>Details</summary>
Motivation: OpenHarmony生态中ArkTS作为核心编程语言，缺乏公开数据集和评估基准，阻碍了ArkTS代码智能研究的发展。

Method: 从GitHub和Gitee爬取ArkTS仓库，使用tree-sitter-arkts提取注释-函数对，进行跨平台去重和统计分析；设计单搜索任务（自然语言注释检索对应ArkTS函数），评估现有开源代码嵌入模型，使用ArkTS和TypeScript训练数据集进行微调。

Result: 构建了首个大规模ArkTS数据集，建立了ArkTS代码检索的系统基准，获得了高性能的ArkTS代码理解模型，数据集和模型已公开发布。

Conclusion: 这项工作填补了ArkTS代码智能研究的空白，为ArkTS代码检索建立了首个系统基准，推动了OpenHarmony生态的发展。

Abstract: ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.

</details>


### [70] [A Dual-Loop Agent Framework for Automated Vulnerability Reproduction](https://arxiv.org/abs/2602.05721)
*Bin Liu,Yanjie Zhao,Zhenpeng Chen,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: Cve2PoC是一个基于LLM的双循环代理框架，用于从CVE描述自动生成可执行的PoC漏洞利用代码，通过战略规划-战术执行-自适应优化的范式，显著提高了漏洞复现成功率。


<details>
  <summary>Details</summary>
Motivation: 从CVE描述自动生成可执行的PoC漏洞利用代码对于软件安全研究和实践至关重要，但目前的手动方法耗时且需要专业知识。现有的LLM代理方法在探索攻击方向和修复实现细节时常常混淆，导致无效的调试循环。

Method: 提出了Cve2PoC框架，采用计划-执行-评估范式，包含三个核心组件：战略规划器分析漏洞语义和目标代码生成结构化攻击计划；战术执行器生成PoC代码并通过渐进验证进行测试；自适应优化器评估执行结果，将失败路由到不同的循环：战术循环用于代码级优化，战略循环用于攻击策略重新规划。

Result: 在两个包含617个真实世界漏洞的基准测试中，Cve2PoC在SecBench.js上达到82.9%的复现成功率，在PatchEval上达到54.3%的复现成功率，分别比最佳基线高出11.3%和20.4%。人工评估确认生成的PoC在可读性和可重用性方面与人工编写的漏洞利用代码相当。

Conclusion: Cve2PoC通过双循环设计有效解决了现有方法中探索攻击方向与修复实现细节混淆的问题，能够根据失败类型匹配相应的修复策略，从而避免无效的调试循环，显著提高了自动化漏洞复现的成功率。

Abstract: Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \textit{Tactical Loop} for code-level refinement, while the \textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\% and 54.3\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\% and 20.4\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.

</details>


### [71] [Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes](https://arxiv.org/abs/2602.05780)
*Ulrich Finkler,Irene Manotas,Wei Zhang,Geert Janssen,Octavian Popescu,Shyam Ramji*

Main category: cs.SE

TL;DR: 该论文提出了一种基于代码语义范围的自动化LLM定制方法，通过RAG和微调两种策略，将LLM适配到私有代码仓库，显著提升代码补全质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在公开基准测试上表现良好，但面对训练数据中未见的私有代码仓库时，生成的代码往往难以与仓库特定模式对齐。定制化LLM可以解决这一问题，提升开发者生产力。

Method: 提出基于代码语义范围的自动化LLM定制框架，包含两种策略：检索增强生成(RAG)和监督微调(FT)。通过语义范围机制处理仓库数据并构建训练数据对，帮助模型学习仓库特定模式。

Result: 在两个企业私有代码仓库上的评估显示，中等规模的定制模型在代码补全质量上显著优于未定制的大型模型。同时在两个公开基准测试上也进行了分析。

Conclusion: 基于语义范围的LLM定制能有效提升私有代码仓库的代码补全质量，为开发者提供更精确的代码建议，显著提高生产力。中等规模定制模型可超越大型未定制模型。

Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [72] [Claude in Xcode](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fapple-xcode-claude-agent-sdk%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/9zqlb2z9dmc5zr39kv7cQARCGFj_kGT6vKJK9D2bc0g=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Xcode 26.3集成Claude Agent SDK，在苹果IDE中提供原生代理能力支持


<details>
  <summary>Details</summary>
Motivation: 将Claude Code的代理功能直接集成到开发者的工作流中，提升开发效率

Method: 在Xcode 26.3中引入对Claude Agent SDK的原生支持

Result: 开发者可以在Xcode中使用子代理、后台任务和插件等完整代理功能

Conclusion: Claude的代理能力现在可以直接在苹果开发环境中使用

Abstract: Claude in Xcode (1 minute read) Xcode 26.3 introduces native support for Claude Agent SDK, enabling full agentic capabilities like subagents, background tasks, and plugins within Apple's IDE. This brings Claude Code's functionality directly into developers' workflows.

</details>


### [73] [Deep Dive: How Claude Code's /insights Command Works](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zolkos.com%2F2026%2F02%2F04%2Fdeep-dive-how-claude-codes-insights-command-works.html%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/z657i3rSoDFNfJKHnX11g9amNJ_7rP4JafsqvwQ5UcU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code的/insights命令生成HTML报告分析用户使用模式，帮助理解与Claude的交互情况、工作流程中的优点和摩擦点


<details>
  <summary>Details</summary>
Motivation: 帮助用户更好地理解他们如何使用Claude Code，识别哪些工作流程有效，哪些地方存在摩擦，从而改进与AI代码助手的交互效率

Method: 通过/insights命令生成HTML报告，分析用户在不同Claude Code会话中的使用模式、交互数据和行为特征

Result: 提供了一个工具让用户能够可视化分析自己的使用习惯，识别高效工作模式和潜在改进点

Conclusion: /insights命令是一个有价值的分析工具，通过定期使用Claude Code、提供反馈、不自我过滤和定期检查，可以获得更好的洞察

Abstract: Deep Dive: How Claude Code's /insights Command Works (22 minute read) The /insights command in Claude Code generates an HTML report that analyzes usage patterns across a user's Claude Code sessions. The tool is designed to help users understand how they interact with Claude, what's working well, where friction occurs, and how to improve workflows. This post takes a look at how the tool works. For better insights, use Claude Code regularly, give feedback, don't filter yourself, and check in mo...

</details>


### [74] [Expensively Quadratic: the LLM Agent Cost Curve](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.exe.dev%2Fexpensively-quadratic%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/0ovPGZQNwIw4qnrEpiNckdTfaZWR-H12nbpSnukpLCk=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文分析了LLM智能体成本随上下文长度呈二次方增长的问题，开发了交互工具展示上下文大小如何快速占据缓存读取的很大比例


<details>
  <summary>Details</summary>
Motivation: 随着对话变长，LLM智能体的上下文成本会显著增加，特别是在智能体和子智能体工作流中，这个问题会指数级恶化。智能体开发者需要意识到这个成本问题

Method: 提供了交互式工具来可视化上下文大小如何快速占据缓存读取的很大比例，帮助开发者理解成本增长模式

Result: 展示了LLM智能体成本随上下文长度呈二次方增长的特性，上下文大小会快速占据缓存读取的很大比例

Conclusion: 智能体开发者需要密切关注上下文成本问题，特别是在设计长对话和复杂工作流时，成本控制至关重要

Abstract: Expensively Quadratic: the LLM Agent Cost Curve (6 minute read) It doesn't take much to start spending a significant amount of tokens on context. This problem compounds as conversations get longer and only gets exponentially more important with agent and subagent workflows. Agent developers need to keep this problem in mind when developing AI tools. This post provides an interactive tool that shows how quickly context size can take up a large percentage of a cache read.

</details>


### [75] [Own your tests:](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2Fplatform%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=ACQ_All_Waitlist__NewsletterAudience_-_Newsletter_AIWaitlist_20260204-None_Experiment-FALSE%26utm_term=headline-NowAvailableForQATeamsAIPlatformForDeepCoverageFromQAWolf%26utm_content=AIWaitlist_GetEarlyAccess_None_Headline%253ANowAvailableForQATeamsAIPlatformForDeepCoverageFromQAWolf____Newsletter-SecondaryPlacement_20260204_v1_/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/NxqaMuWJZwL9W2EiK8fGkbIys2u69qJnz8aHfuwbbdk=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: QA Wolf推出AI助手平台，通过对话帮助团队为80%+产品工作流建立和维护自动化测试覆盖，生成可靠的Playwright和Appium代码而非易错的英文步骤，支持并行执行完整测试套件，提供开源代码避免供应商锁定。


<details>
  <summary>Details</summary>
Motivation: 传统QA测试面临测试覆盖不足、测试代码编写复杂、执行速度慢、供应商锁定等问题。QA团队需要更高效、可靠且自主可控的自动化测试解决方案。

Method: 开发AI助手平台，通过自然语言对话生成Playwright和Appium测试代码，而非易错的纯英文步骤。支持并行执行完整测试套件，提供开源代码避免供应商锁定。

Result: 平台能够为80%+产品工作流建立自动化测试覆盖，测试代码更可靠，完整测试套件可在几分钟内并行执行，团队拥有开源代码控制权。

Conclusion: AI驱动的QA测试平台能显著提升测试效率、覆盖率和可靠性，同时通过开源代码确保团队自主控制权，是QA自动化测试的有前景方向。

Abstract: Now available for QA teams: AI platform for deep coverage from QA Wolf (Sponsor) QA Wolf's new AI assistant helps teams build and maintain automated test coverage for 80%+ of their product workflows—just by chatting with the AI. Test complex workflows reliably: Prompts generate Playwright and Appium code instead of flaky plain-English steps. Run regressions in minutes: Full suites execute 100% in parallel. Own your tests: Open-source code, no vendor lock-in. Get early access

</details>


### [76] [Qwen3-Coder-Next for Agentic Coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fqwen.ai%2Fblog%3Fid=qwen3-coder-next%26utm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/nvJqz_SvI2xnFdyTozzYMt0FAhojamUXxEAt8ecXU7M=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 阿里推出Qwen3-Coder-Next，这是一个基于混合MoE架构的开源权重模型，专为代码代理任务微调，在可执行代码合成和基于RL的环境交互方面表现出色，能以较低推理成本实现强大的代理编码性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个专门为代码代理任务优化的开源模型，旨在提高代码生成和执行能力，同时降低推理成本。

Method: 基于混合MoE架构构建，通过微调专门针对代码代理任务，专注于可执行代码合成和基于强化学习的环境交互能力。

Result: 在代理编码任务中表现出色，实现了强大的性能，同时保持了较低的推理成本。

Conclusion: Qwen3-Coder-Next是一个有效的开源代码代理模型，在性能和效率之间取得了良好平衡。

Abstract: Qwen3-Coder-Next for Agentic Coding (5 minute read) Alibaba's Qwen3-Coder-Next is a new open-weight model fine-tuned for coding agents. Built on a hybrid MoE architecture, it excels in executable synthesis and RL-based environment interaction, achieving strong agentic coding performance at lower inference cost.

</details>


### [77] [800K+ Verifiable SWE Tasks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.02361%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/JolGWS3kQ1jjoLn4xN2d8mZgftVmwNBlrzuX7ExbmgA=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SWE-Universe提出了一种从GitHub PRs生成可验证软件工程环境的方法，通过内循环黑客检测和自我验证，为编码代理提供大规模中期训练，生成了超过80万个任务。


<details>
  <summary>Details</summary>
Motivation: 当前编码代理的训练数据有限，缺乏真实世界的软件工程任务环境。需要一种可扩展的方法来生成大量可验证的编程任务，以支持编码代理的大规模训练。

Method: 从GitHub PRs自动生成可验证的软件工程环境，采用内循环黑客检测机制防止作弊，并通过自我验证确保任务的可验证性，实现任务生成的可扩展性。

Result: 成功生成了超过800,000个可验证的软件工程任务，为编码代理提供了大规模的中期训练数据集，支持更有效的编码代理训练。

Conclusion: SWE-Universe提供了一种可扩展的方法来生成大量可验证的软件工程任务，解决了编码代理训练数据稀缺的问题，为编码代理的大规模训练奠定了基础。

Abstract: 800K+ Verifiable SWE Tasks (18 minute read) SWE-Universe presents a scalable method for generating verifiable software engineering environments from GitHub PRs. With in-loop hacking detection and self-verification, the system enables large-scale mid-training for coding agents, producing over 800K tasks.

</details>


### [78] [Most People Can't Vibe Code. Here's How We Fix That](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fmost-people-cant-vibe-code-heres%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/KZ2dTXMczsbBCciXOAQ-bm1wHpIdjrUkQcOhKLzB2lE=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨如何让非技术用户也能进行"氛围编程"，通过开发类似Squarespace和Canva的简化工具，使软件开发大众化


<details>
  <summary>Details</summary>
Motivation: 当前氛围编程主要局限于技术用户，需要开发消费者友好的AI产品来消除复杂的技术设置和术语，让非技术用户也能进行软件开发

Method: 通过开发类似Poke和Wabi这样的公司产品，简化技术设置和术语，创建类似Squarespace和Canva的易用工具

Result: 指出了让软件开发大众化的机会，但尚未实现主流消费者的广泛采用

Conclusion: 真正的机会在于创建使软件开发对非技术用户可访问的工具，就像Squarespace和Canva使网站和设计民主化一样

Abstract: Most People Can't Vibe Code. Here's How We Fix That (6 minute read) Vibe coding has yet to reach mainstream consumers, remaining primarily the domain of technical users. Companies like Poke and Wabi are developing consumer-friendly AI products that eliminate complex technical setup and terminology. The real opportunity lies in creating tools that make software development accessible to non-technical users, similar to how Squarespace and Canva democratized websites and design.

</details>


### [79] [How Yelp Built a Back-Testing Engine for Safer, Smarter Ad Budget Allocation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineeringblog.yelp.com%2F2026%2F02%2Fhow-yelp-built-a-back-testing-engine-for-safer-smarter-ad-budget-allocation.html%3Futm_source=tldrdata/1/0100019c2d7da8c9-5be7052b-7548-40e5-bfd4-d21470c6c8b6-000000/pA_0dVLUWDgbNIiyWEToLL5Mas4ZDcqIN84v6TbxVoQ=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Yelp开发了一个回测引擎，通过重放历史广告活动数据来安全模拟广告预算分配系统的变更，使用CatBoost模型预测结果，Scikit-Optimize调参，Git子模块集成代码评估系统影响


<details>
  <summary>Details</summary>
Motivation: Yelp需要安全地测试其复杂广告预算分配系统的变更，避免影响实际广告客户，同时确保系统改进的有效性和可靠性

Method: 构建回测引擎，重放历史广告活动数据，使用CatBoost机器学习模型进行结果预测，Scikit-Optimize进行参数调优，Git子模块集成代码来评估系统级影响

Result: 开发了一个能够安全模拟广告预算分配系统变更的引擎，可以在不影响实际广告客户的情况下评估系统改进效果

Conclusion: 回测引擎为Yelp提供了安全、智能的广告预算分配系统测试方法，降低了变更风险，提高了系统优化效率

Abstract: How Yelp Built a Back-Testing Engine for Safer, Smarter Ad Budget Allocation (10 minute read) Yelp built a Back-Testing Engine to safely simulate proposed changes to its complex Ad Budget Allocation system by replaying historical campaign data through production-like logic, using tools like CatBoost ML models for outcome prediction, Scikit-Optimize for parameter tuning, and Git-submodule-integrated code to evaluate system-wide impacts without risking live advertisers.

</details>


### [80] [Agentic Engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fagentic-engineering%2F%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/ItRp-RNi1CpxEqDQqpmkaPintHDSIJAOJYvhk3QRCI4=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了Agentic Engineering（智能体工程）这一专业术语，它描述了工程师如何像架构师、评审员和决策者一样编排AI智能体，这是一种涉及自主智能体的严肃工程学科。


<details>
  <summary>Details</summary>
Motivation: 提出Agentic Engineering这一专业术语，以更准确地描述当前工程师使用AI智能体的实践，替代"vibe coding"等非专业表述，强调其作为严肃工程学科的地位。

Method: 通过概念定义和术语阐述的方式，将工程师编排AI智能体的实践系统化为"Agentic Engineering"这一专业学科，强调工程师在架构设计、代码评审和决策制定中的核心作用。

Result: 建立了Agentic Engineering作为专业工程学科的概念框架，明确了其核心特征：工程师作为智能体编排者，承担架构师、评审员和决策者角色，特别有利于具备系统设计、安全模式和性能理解能力的高级工程师。

Conclusion: Agentic Engineering是描述工程师编排AI智能体实践的恰当专业术语，它代表了一种严肃的工程学科，比"vibe coding"等表述更专业，特别适合高级工程师应用。

Abstract: Agentic Engineering (7 minute read) Agentic engineering is a professional term that describes what is actually happening: the engineer orchestrates AI agents as they act as an architect, reviewer, and decision maker. It is a serious engineering discipline that involves autonomous agents. The term definitely sounds better than 'vibe coding' in professional settings. Agentic engineering disproportionately benefits senior engineers who understand system design, security patterns, and performance...

</details>


### [81] [Claude and Codex are now available in public preview on GitHub](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fchangelog%2F2026-02-04-claude-and-codex-are-now-available-in-public-preview-on-github%2F%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/0lp7ZAoq1kJDawO3yGx6JdrSPtJ6AA6ONlBUOfbWpY4=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub Copilot Pro+和Enterprise用户现在可以使用Claude和Codex作为编码代理，可以直接从issues、PRs等地方分配任务，使用会话会消耗一个高级请求


<details>
  <summary>Details</summary>
Motivation: GitHub希望为Copilot高级用户提供更多AI编码代理选择，让开发者能够更灵活地使用不同的AI模型来完成编码任务，提升开发效率

Method: 将Anthropic的Claude和OpenAI的Codex集成到GitHub Copilot平台中，用户可以通过多种界面（issues、PRs、Agents标签、VS Code会话视图）分配编码任务给这些代理

Result: Claude和Codex现在作为编码代理在GitHub Copilot Pro+和Enterprise版本中公开预览，用户可以直接使用这些AI代理，每个会话消耗一个高级请求

Conclusion: GitHub扩展了Copilot的AI代理生态系统，为用户提供了更多编码AI选择，进一步增强了开发者的AI辅助编程体验

Abstract: Claude and Codex are now available in public preview on GitHub (2 minute read) Claude by Anthropic and OpenAI Codex are now available as coding agents for Copilot Pro+ and Copilot Enterprise customers. Users can assign work to these agents directly from issues, pull requests, the Agents tab in enabled repositories, and the agent sessions view in VS Code. Access to Claude and Codex is included with existing Copilot subscriptions. Each coding agent session consumes one premium request during pu...

</details>


### [82] [OpenClaw is What Apple Intelligence Should Have Been](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jakequist.com%2Fthoughts%2Fopenclaw-is-what-apple-intelligence-should-have-been%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/gQV4rlqyaxYaAcTTtC-qc25kFg4rVu_vW7AvgD3lojM=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个开源本地AI代理框架，在Mac硬件上成为杀手级应用，显示了对代理和自动化的明确需求，苹果从中获得硬件收入但错过了平台收入机会。


<details>
  <summary>Details</summary>
Motivation: 分析OpenClaw作为开源本地AI代理框架在Mac硬件上的成功，揭示市场对AI代理和自动化的强烈需求，以及苹果在AI平台战略上的缺失机会。

Method: 通过观察OpenClaw框架在Mac Mini硬件上的销售热潮和用户采用情况，分析开源AI代理框架的市场影响和苹果的应对策略。

Result: OpenClaw成为Mac硬件的杀手级应用，推动Mac Mini销售火爆，显示用户强烈偏好本地运行的AI代理，苹果获得硬件收入但错失了平台控制权。

Conclusion: 苹果应该开发自己的原生AI代理平台来把握市场机会，而不是仅仅依赖硬件销售，开源框架的成功暴露了苹果在AI战略上的短板。

Abstract: OpenClaw is What Apple Intelligence Should Have Been (3 minute read) Mac Minis are selling out everywhere because people are using them to run OpenClaw, an open source framework for local AI agents that has become a killer app for Mac hardware. The rush shows that there is a clear demand for agents and automation, and Apple is getting the hardware revenue from it. However, it's missing out on the platform revenue - Apple could have created its own native agent, but this would likely have caus...

</details>


### [83] [Developers believe that AI increases productivity. Does it?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.gitkraken.com%2Fai-productivity-paradox-2025%3Fsource=TLDR%26product=gitkraken%26utm_source=TLDR%26utm_medium=sponsored%26utm_campaign=insights_launch%26utm_content=paradox/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/HF9X1dGxuqS9z6E259Mvjcin0_n2DNx42aECytEa8B8=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 基于对2.11亿行代码的分析，研究发现AI工具虽然被开发者认为能提高生产力，但实际上导致代码重复率上升、交付稳定性下降，只有顶级团队能更快适应AI工具


<details>
  <summary>Details</summary>
Motivation: 验证AI工具是否真正提高开发者的生产力，通过大规模代码分析来揭示AI对软件开发实践的实际影响

Method: 分析2.11亿行代码数据，研究AI工具使用对代码重复率、交付稳定性等指标的影响，比较不同水平团队的适应能力

Result: AI工具导致代码重复率上升、交付稳定性下降，只有顶级团队能更快适应AI工具并从中获益

Conclusion: AI工具对生产力的影响并非普遍积极，需要更谨慎地评估和采用，同时关注团队能力差异

Abstract: Developers believe that AI increases productivity. Does it? (Sponsor) 211M lines of code reveal AI's real impact: duplication is rising, delivery is less stable, and top teams are adapting faster than the rest. Download GitKraken's guide for engineering managers

</details>


### [84] [SEO Overhaul Using Claude Skills](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fmattbminor_if-youre-using-claude-code-or-cowork-and-share-7424092242404974594-9ZWB%3Futm_source=tldrmarketing/1/0100019c2db238e5-a7eb53a7-6f31-47cc-9652-fedbc1973d5c-000000/oBQGXEaIhOG00E-tvLgyLrYihTEI1S0P3JoZ4rCI0Wc=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用Claude技能进行SEO优化，通过npx命令安装SEO和GEO技能，运行审计发现标题问题、缺失SEO、弱元描述、模式差距和需要迁移的页面，使用营销和AI可见性技能指导修复，通过Directus MCP应用更改到数百个页面。


<details>
  <summary>Details</summary>
Motivation: 传统SEO优化过程繁琐且耗时，需要手动审计和修复大量网页，本文旨在展示如何利用Claude技能自动化SEO审计和修复流程，提高效率。

Method: 使用Claude Code通过npx命令安装SEO和GEO技能，运行自动化审计识别各种SEO问题，结合营销和AI可见性技能分析需要修复的内容，通过Directus MCP（模型上下文协议）批量应用更改到数百个页面。

Result: 成功实现了SEO审计和修复的自动化流程，能够快速识别并修复标题问题、缺失SEO元素、弱元描述、模式差距等常见SEO问题，大幅提高了SEO优化的效率。

Conclusion: Claude技能结合MCP技术能够有效自动化SEO优化工作流，显著减少手动工作量，为大规模网站的SEO管理提供了高效解决方案。

Abstract: SEO Overhaul Using Claude Skills (2 minute read) This post shows how Claude Code installed SEO and GEO skills with a single npx command and ran audits to find broken titles, missing SEO, weak meta descriptions, schema gaps, and pages needing migration. Marketing and AI visibility skills guided what to fix, while Cowork with the Directus MCP applied changes across hundreds of pages. Links to both the SEO and GEO repositories are in the post's comments.

</details>


### [85] [How we built a real-world benchmark for AI code review](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qodo.ai%2Fblog%2Fhow-we-built-a-real-world-benchmark-for-ai-code-review%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/5sIe6I8xN9KvTJ744iaJyR0QzeLgQcFGQobiJmj6jck=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Qodo开发了一个真实世界的代码审查基准，通过在开源生产级仓库的合并PR中注入各种缺陷，用于客观评估AI系统的代码审查能力


<details>
  <summary>Details</summary>
Motivation: 需要客观评估AI代码审查系统的能力，现有基准可能不够真实或全面，需要同时评估代码正确性和质量

Method: 在真实开源生产级仓库的合并PR中故意注入多种缺陷（功能bug和最佳实践违规），构建大规模评估基准

Result: 开发了一个严谨的代码审查基准，能够同时评估代码正确性和质量，支持大规模评估

Conclusion: 该基准为AI代码审查系统提供了客观、真实的评估标准，有助于推动该领域的发展

Abstract: How we built a real-world benchmark for AI code review (10 minute read) Qodo has developed a new, rigorous code review benchmark to objectively evaluate AI systems. This benchmark intentionally injects diverse defects, including functional bugs and best practice violations, into genuine, merged pull requests from production-grade open-source repositories. The methodology allows for the simultaneous evaluation of code correctness and quality at a larger scale.

</details>


### [86] [OpenClaw is What Apple Intelligence Should Have Been](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jakequist.com%2Fthoughts%2Fopenclaw-is-what-apple-intelligence-should-have-been%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/eq4vRESF-xSSciHV2uoI9rbwXKilRcSkV-FNm1o0F8I=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 苹果在"Apple Intelligence"上错失良机，未能开发出真正能够自动化使用电脑的智能体，而用户现在通过开源框架OpenClaw在Mac Minis上实现了这一能力


<details>
  <summary>Details</summary>
Motivation: 批评苹果在AI智能体开发上的保守策略，指出用户对强大自动化能力的需求已经超越了简单的摘要功能

Method: 通过对比苹果的"Apple Intelligence"与开源框架OpenClaw在Mac Minis上的实现，分析无头AI智能体的应用现状

Result: 开源框架OpenClaw展示了用户对强大自动化AI智能体的明确需求，而苹果错失了这一市场机会

Conclusion: 苹果在AI智能体开发上过于保守，未能满足用户对自动化电脑使用的真实需求，开源解决方案正在填补这一空白

Abstract: OpenClaw is What Apple Intelligence Should Have Been (5 minute read) Apple squandered a huge opportunity with "Apple Intelligence" by failing to develop a truly agentic AI capable of automating computer use, a capability users are now achieving with open-source frameworks like OpenClaw on Mac Minis. This widespread adoption of headless AI agents shows a clear demand for powerful automation beyond simple summaries.

</details>


### [87] [Code smells for AI agents: Q&A with Eno Reyes of Factory](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstackoverflow.blog%2F2026%2F02%2F04%2Fcode-smells-for-ai-agents-q-and-a-with-eno-reyes-of-factory%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/VGanrG-dT7vl9nZQfnRyPgf8-sNX03IfFouZ094jhNE=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI智能体开发不仅依赖模型质量，更需要基础设施层来管理上下文、工具和验证信号。研究发现AI采用率不能预测生产力，但代码质量可以：高质量代码库能加速AI应用，低质量代码库反而会减速。


<details>
  <summary>Details</summary>
Motivation: 探讨AI智能体开发中的关键因素，指出当前研究过于关注模型本身，而忽略了基础设施层的重要性。同时揭示AI采用与生产力之间的复杂关系，强调代码质量在AI时代的重要性。

Method: 通过访谈和斯坦福研究相结合的方式：1) 与Factory的Eno Reyes进行问答访谈，探讨AI智能体开发的基础设施需求；2) 引用斯坦福研究数据，分析AI采用率、代码质量与生产力之间的关系。

Result: 研究发现：1) AI智能体开发需要强大的基础设施层来管理上下文、工具和验证信号；2) AI采用率本身不能预测开发团队的生产力；3) 代码质量是关键因素——高质量代码库能加速AI应用，低质量代码库反而会减速。

Conclusion: 开发优秀的AI智能体需要同时关注模型质量和基础设施层建设。在AI时代，代码质量比AI采用率更重要，高质量代码库能最大化AI的生产力效益，而低质量代码库会阻碍AI的正面影响。

Abstract: Code smells for AI agents: Q&A with Eno Reyes of Factory (10 minute read) Building good coding agents isn't just about the model, but also about the infrastructure layer that manages context, tools, and hundreds of validation signals like linters and tests. Stanford research found that AI agent adoption doesn't predict productivity, but code quality does. High-quality codebases accelerate with AI, while low-quality ones actually decelerate.

</details>


### [88] [Claude Code: connect to a local model when your quota runs out](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboxc.net%2Fblog%2F2026%2Fclaude-code-connecting-to-local-models-when-your-quota-runs-out%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/BidzMUiKrwHmXiO7z6tjpLxke2LVr9qDaBdxC8QqsB0=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code 提供了一种连接本地开源AI模型的解决方案，当Anthropic API配额耗尽时，可以通过LM Studio使用本地模型作为备用方案继续编码。


<details>
  <summary>Details</summary>
Motivation: 解决开发者在使用Claude Code时遇到API配额耗尽的问题，确保编码工作能够持续进行，提供可靠的备用方案。

Method: 通过集成LM Studio，使Claude Code能够连接到本地运行的开源AI模型，当云端API配额不足时自动切换到本地模型。

Result: 实现了在API配额耗尽时无缝切换到本地模型的能力，确保编码工作不中断，提高了开发者的工作效率和可靠性。

Conclusion: Claude Code的本地模型连接功能为开发者提供了更可靠的编码辅助解决方案，减少了因API限制导致的工作中断风险。

Abstract: Claude Code: connect to a local model when your quota runs out (3 minute read) Claude Code can connect to a local open-source AI model, using LM Studio, as a backup solution to continue coding when Anthropic's API quota is exhausted.

</details>


### [89] [Staying engaged with AI plans: give inline feedback](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuonw.github.io%2Fblog%2F2026%2F02%2Fai-plan%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/VYLy5WRDp0Kw1MNrxqc9l3HD3M8gQxVoUqfeQfgGMBM=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过直接在Claude Code生成的markdown计划文件中提供内联反馈，可以提升AI编程代理的性能


<details>
  <summary>Details</summary>
Motivation: AI编程代理的性能可以通过用户直接参与和反馈来改进，特别是在计划制定阶段

Method: 用户可以直接编辑Claude Code生成的markdown计划文件，并在其中提供内联反馈，让AI代理能够根据反馈调整计划

Result: 这种方法能够提高AI编程代理的性能和计划质量

Conclusion: 通过内联反馈机制让用户直接参与AI计划的编辑过程，可以有效提升AI编程代理的效果

Abstract: Staying engaged with AI plans: give inline feedback (3 minute read) AI coding agent performance can be improved by editing Claude Code markdown plans directly and providing feedback inline in the markdown files that Claude Code generates.

</details>


### [90] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/m98I36Ctpbw5fF12f6wlru75GDhSccBrnrjz6-SLOTM=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过直接在Claude Code生成的markdown计划文件中提供内联反馈，可以提升AI编码代理的性能


<details>
  <summary>Details</summary>
Motivation: AI编码代理的性能可以通过用户直接参与和反馈来提升，特别是通过编辑代理生成的计划文件来提供具体指导

Method: 用户在Claude Code生成的markdown计划文件中直接编辑并提供内联反馈，让AI代理能够根据具体指导调整其编码计划

Result: 通过这种内联反馈机制，AI编码代理的性能得到显著提升，能够更好地理解用户意图并生成更符合需求的代码

Conclusion: 直接在AI生成的计划文件中提供内联反馈是一种有效的交互方式，能够显著提升AI编码代理的性能和用户满意度

Abstract: Staying engaged with AI plans: give inline feedback (3 minute read) AI coding agent performance can be improved by editing Claude Code markdown plans directly and providing feedback inline in the markdown files that Claude Code generates.

</details>


### [91] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/PFDsicbeD-Pn2Bn94lYIxBVGRHoiD5FUFPg4lXriOOU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编程助手Claude Code通过允许用户在生成的markdown计划文件中直接编辑和提供内联反馈，可以提升AI编码代理的性能


<details>
  <summary>Details</summary>
Motivation: 当前AI编码代理生成的计划文件通常是静态的，用户难以直接参与修改和提供反馈，这限制了代理性能的持续改进

Method: 允许用户在Claude Code生成的markdown计划文件中直接编辑内容，并提供内联反馈，使AI能够根据用户反馈调整后续的代码生成

Result: 通过这种内联反馈机制，AI编码代理能够更好地理解用户意图，生成更符合需求的代码，提升整体编码性能

Conclusion: 在AI生成的计划文件中支持直接编辑和内联反馈是一种有效的交互方式，能够显著提升AI编码代理的性能和用户体验

Abstract: Staying engaged with AI plans: give inline feedback (3 minute read) AI coding agent performance can be improved by editing Claude Code markdown plans directly and providing feedback inline in the markdown files that Claude Code generates.

</details>


### [92] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/vBbeMarB8U6kn0PxmZ8bVBL51wp-h3EKAv_hhtqI9xM=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过直接在Claude Code生成的markdown计划文件中提供内联反馈，可以提升AI编程代理的性能


<details>
  <summary>Details</summary>
Motivation: AI编程代理在生成代码计划时，用户难以有效参与和指导，导致最终结果可能不符合预期。需要一种更直接、高效的反馈机制来提升代理性能。

Method: 提出在Claude Code生成的markdown计划文件中直接编辑和提供内联反馈的方法。用户可以直接在代理生成的代码计划中进行修改和注释，代理会根据这些反馈调整后续的代码生成。

Result: 通过内联反馈机制，用户能够更有效地指导AI编程代理，使生成的代码更符合需求，提升了代理的性能和用户体验。

Conclusion: 在AI编程代理生成的markdown计划中提供内联反馈是一种有效的交互方式，能够显著提升代理的性能和代码质量。

Abstract: Staying engaged with AI plans: give inline feedback (3 minute read) AI coding agent performance can be improved by editing Claude Code markdown plans directly and providing feedback inline in the markdown files that Claude Code generates.

</details>


### [93] [Behavioral Agent Automation Platform](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.liminal.ai%2Fbehavioral-agent-automation-platform%3Futm_campaign=FintechSecondary02052026%26utm_source=tldr%26utm_medium=newsletter/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/TLDpXqV1Pm929zkQ4uWWZs6PIuqgdx5o9xjaAnQIvS8=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Liminal公司提出行为代理自动化平台，通过观察真实工作行为识别摩擦点，自动部署安全合规的自动化方案，解决银行AI试点ROI低的问题


<details>
  <summary>Details</summary>
Motivation: 95%的银行AI试点项目投资回报率为零，主要问题不在于技术本身，而在于架构设计。传统金融科技公司需要在观察交易员、分析师、个人银行家和客服代表实际工作方式之前就预测工作流程，这导致了解决方案与实际情况脱节。

Method: Liminal的行为代理自动化平台采用逆向方法：首先观察真实工作行为，识别工作中的摩擦点和效率瓶颈，然后自动部署安全、合规的自动化解决方案。平台包含人工监督和审计追踪功能。

Result: 该方法通过从实际行为出发而非预设工作流程，能够更精准地识别自动化机会，部署的自动化方案更贴合实际需求，从而提高投资回报率。

Conclusion: 解决银行AI试点ROI低的关键在于改变架构方法，从观察真实行为入手，而非预先预测工作流程。这种基于行为观察的自动化方法能够更有效地识别和解决实际工作中的摩擦点。

Abstract: 95% of Banks See Zero ROI from AI Pilots (Sponsor) The problem isn't technology. It's architecture. Fintech firms are forced to predict workflows before observing how traders, analysts, personal bankers, and CSRs actually work. Liminal's Behavioral Agent Automation Platform inverts this: start by observing real behavior, identify friction, and deploy secure, compliant automations automatically. Human oversight and audit trails included. Download the free whitepaper (no sign up required).

</details>


### [94] [Agentic commerce gatekeeping problem](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnekuda.substack.com%2Fp%2Fagentic-commerce-gatekeeping-problem%3Futm_source=tldrfintech/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/UH6o-ZqL0-I46h1feoxwH96CTw07CV_SnLx_2qZHGhM=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 代理商务协议由早期分销伙伴控制，造成数据不对称，可能将激励从商家成功转向平台GMV优化


<details>
  <summary>Details</summary>
Motivation: 分析新兴的代理商务协议作为"开放"标准被Shopify等早期分销伙伴控制的问题，关注数据不对称带来的激励扭曲风险

Method: 通过对代理商务协议（agentic commerce protocols）的分析，探讨平台控制权、数据不对称和激励结构问题

Result: 发现早期分销伙伴控制开放标准会导致数据不对称，使激励从个体商家成功转向平台层面的GMV优化

Conclusion: 代理商务协议的控制权集中和数据不对称问题需要关注，以确保协议真正开放并保护商家利益

Abstract: Agentic commerce gatekeeping problem (8 minute read) Agentic commerce protocols are emerging as “open” standards controlled by early distribution partners like Shopify, creating data asymmetries that risk shifting incentives from individual merchant success toward platform-level GMV optimization.

</details>


### [95] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfintech%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/np87ayDGw1oqLAtgeyWgYnC97Mhub33a11NKX_VBoJ8=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 代理商务协议由早期分销伙伴控制，造成数据不对称，可能将激励从商家成功转向平台GMV优化


<details>
  <summary>Details</summary>
Motivation: 代理商务协议作为"开放"标准出现，但被Shopify等早期分销伙伴控制，导致数据不对称问题，可能使激励从关注个体商家成功转向平台层面的GMV优化

Method: 未明确说明具体方法，但分析了代理商务协议的控制结构和数据不对称问题

Result: 识别了代理商务协议中的"守门人"问题，即早期分销伙伴控制开放标准，造成数据不对称，可能扭曲激励结构

Conclusion: 代理商务协议存在控制集中和数据不对称风险，需要关注激励结构从商家成功向平台GMV优化的转变

Abstract: Agentic commerce gatekeeping problem (8 minute read) Agentic commerce protocols are emerging as “open” standards controlled by early distribution partners like Shopify, creating data asymmetries that risk shifting incentives from individual merchant success toward platform-level GMV optimization.

</details>


### [96] [Who's actually reviewing all that AI-generated code?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr_ai%26utm_medium=sponsorship%26utm_campaign=tldr_ai/2/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/Gbck2upZ6hzjX8dlvFYOX1Xc0m0bsSpjgKi7cjyxVoU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Greptile是一个AI代码审查工具，通过分析完整代码库上下文和学习团队规范，自动审查AI生成的代码，防止低质量代码进入代码库


<details>
  <summary>Details</summary>
Motivation: 随着开发者大量使用AI生成代码，产生了大量未经验证的代码，导致代码质量下降和审查瓶颈，需要自动化工具来帮助团队高效审查AI生成的代码

Method: Greptile通过分析完整代码库的上下文，并学习团队的规范（从评论、反应和合并的代码中学习），自动审查PR，标记问题并提供符合团队特定实践的修复建议

Result: 该工具已被NVIDIA、Scale AI和Bre等工程团队信任使用，能够有效解决AI生成代码的审查瓶颈问题

Conclusion: AI代码审查工具对于管理AI生成的代码质量至关重要，能够帮助团队保持代码库质量，同时适应团队特定的开发规范

Abstract: Who's actually reviewing all that AI-generated code? (Sponsor) When devs use AI to generate thousands of lines of unverified code, you risk a codebase slopocalypse. The review step becomes your team's bottleneck.Greptile reviews each PR with full repo context and learns your team's conventions over time from comments, reactions, and what gets merged. It flags issues and suggests fixes that match your team, not generic best practices. ✅ Trusted by engineering teams at NVIDIA, Scale AI, and Bre...

</details>


### [97] [Claude and Codex are now available in public preview on GitHub](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fchangelog%2F2026-02-04-claude-and-codex-are-now-available-in-public-preview-on-github%2F%3Futm_source=tldrai/1/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/1u36bo0ftvlUcqARbdKYVoIhMnf05sZb9wXFyhnQlIE=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub宣布Claude和Codex在Copilot Pro+和企业版中公开预览，用户可通过多种平台使用这些编码助手，无需额外订阅


<details>
  <summary>Details</summary>
Motivation: GitHub希望为其Copilot用户提供更多AI编码助手选择，通过集成Claude和Codex来增强开发体验，让用户能够根据需求选择不同的AI助手

Method: 将Anthropic的Claude和OpenAI的Codex集成到GitHub Copilot平台中，通过现有GitHub基础设施管理所有代理操作，支持网页、移动应用和VS Code多种访问方式

Result: Pro+和企业客户现在可以在GitHub上使用Claude和Codex进行编码任务，包括起草PR、任务优先级排序等，所有操作都在GitHub现有框架内完成

Conclusion: GitHub通过集成多个AI编码助手扩展了Copilot功能，为用户提供了更多选择和灵活性，同时保持了统一的管理体验

Abstract: Claude and Codex are now available in public preview on GitHub (4 minute read) Claude by Anthropic and OpenAI Codex are now available for Copilot Pro+ and Enterprise customers on GitHub in public preview. Users can start sessions and assign tasks to these coding agents from the web, mobile app, or VS Code without additional subscriptions. All agent actions, like drafting pull requests and task prioritization, can be managed through GitHub's existing infrastructure.

</details>


### [98] [Windsurf Tab v2: 25-75% more accepted code with Variable Aggression](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwindsurf.com%2Fblog%2Fwindsurf-tab-2%3Futm_source=tldrai/1/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/tQPxY9XxmL4G_xyZw0hSt8yQOVOv-XxPU_zrMnMK8rI=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Windsurf Tab v2通过改进底层模型和上下文/数据工程管道，实现了平均54%的字符预测增长，并采用可变攻击性策略个性化用户体验，使代码接受率提升25-75%


<details>
  <summary>Details</summary>
Motivation: Windsurf的Tab功能虽然受到欢迎，但发布后维护不足。团队希望通过改进底层技术和个性化策略来提升用户体验和代码接受率

Method: 1. 显著改进底层模型；2. 优化上下文/数据工程管道；3. 引入可变攻击性策略，根据用户偏好个性化调整体验

Result: 1. 在所有指标上实现帕累托改进；2. 平均字符预测增长54%；3. 代码接受率提升25-75%；4. 通过可变攻击性策略提供个性化体验

Conclusion: Windsurf Tab v2通过技术改进和个性化策略显著提升了代码生成质量和用户体验，实现了代码接受率的大幅提升

Abstract: Windsurf Tab v2: 25-75% more accepted code with Variable Aggression (7 minute read) Windsurf's Tab feature was very well received, but it wasn't well maintained after launch. The team has now significantly improved the underlying model and context/data engineering pipeline. This has led to direct Pareto improvements across all of Windsurf's metrics, with an average 54% increase in characters per predict. Tab now uses variable aggression to tailor the experience to each user's preferences.

</details>
