<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [tldr.article](#tldr.article) [Total: 7]
- [wechat.article](#wechat.article) [Total: 19]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: ç ”ç©¶è¡¨æ˜LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¾ˆæ—©å°±å¯¹æœ€ç»ˆç»“æœåšå‡ºå†…éƒ¨æ‰¿è¯ºï¼Œå³ä½¿éœ€è¦æ›´é•¿çš„è¾“å‡ºæ‰èƒ½å¾—åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œæ­£ç¡®æ€§åœ¨å‰å‡ ä¸ªæ¨ç†tokenåå°±èƒ½é«˜åº¦é¢„æµ‹ã€‚


<details>
  <summary>Details</summary>
Motivation: æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é“¾å¼æ€ç»´æ¨ç†è¿‡ç¨‹ä¸­ä½•æ—¶å¯¹æœ€ç»ˆç»“æœåšå‡ºå†…éƒ¨æ‰¿è¯ºï¼Œä»¥åŠè¿™ç§æ—©æœŸæ‰¿è¯ºå¯¹æ¨¡å‹å¯è§£é‡Šæ€§å’Œæ¨ç†æ—¶æ§åˆ¶çš„å½±å“ã€‚

Method: é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹çš„å‰tä¸ªtokenåçš„éšè—çŠ¶æ€ä¸Šè®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ï¼Œåˆ†ææ¨¡å‹å¯¹æœ€ç»ˆæ­£ç¡®æ€§çš„é¢„æµ‹èƒ½åŠ›ã€‚

Result: å‘ç°æ¨¡å‹åœ¨ä»…å‡ ä¸ªæ¨ç†tokenåå°±èƒ½é«˜åº¦é¢„æµ‹æœ€ç»ˆæ­£ç¡®æ€§ï¼›å¯¹äºéš¾é¢˜ï¼Œé¢„æµ‹å‡†ç¡®ç‡ä¸‹é™æ­ç¤ºäº†é€‰æ‹©åå·®ï¼šéš¾é¢˜åœ¨é•¿CoTä¸­å æ¯”è¿‡é«˜ã€‚

Conclusion: æ¨ç†æ¨¡å‹çš„å†…éƒ¨è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›åœ¨ä»…å‡ ä¸ªtokenåå°±å‡ºç°ï¼Œè¿™å¯¹æ¨¡å‹å¯è§£é‡Šæ€§å’Œæ¨ç†æ—¶æ§åˆ¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [2] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASSæ˜¯ä¸€ä¸ªè½»é‡çº§ã€å¯è§£é‡Šçš„æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡æ¨¡å‹å†…åé¦ˆå¾ªç¯åŠ¨æ€è°ƒèŠ‚æ³¨æ„åŠ›å¤´ï¼Œå‡å°‘LLMçš„ä¸Šä¸‹æ–‡å¹»è§‰ï¼ŒåŒæ—¶æä¾›å¯¹æ¨¡å‹è¯æ®å¯¹é½æœºåˆ¶çš„ç§‘å­¦ç†è§£ã€‚


<details>
  <summary>Details</summary>
Motivation: LLMç»å¸¸ç”Ÿæˆæµç•…ä½†äº‹å®é”™è¯¯çš„é™ˆè¿°ï¼Œè¿™ç§å¤±è´¥æ¨¡å¼æºäºå®ƒä»¬åœ¨ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œå‚æ•°çŸ¥è¯†ä¹‹é—´åˆ†é…æ³¨æ„åŠ›çš„æ–¹å¼ã€‚ç†è§£å’Œå¼•å¯¼è¿™ç§å†…éƒ¨è¡Œä¸ºå¯¹äºå¯ä¿¡éƒ¨ç½²å’Œæ¨¡å‹æœºåˆ¶çš„ç§‘å­¦å¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚

Method: å¼•å…¥COMPASSæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡ä¾èµ–è¯„åˆ†(CRS)é‡åŒ–ä¸Šä¸‹æ–‡ä¾èµ–åº¦ï¼Œä½œä¸ºæ³¨æ„åŠ›å¤´å¦‚ä½•å°†ç”ŸæˆåŸºäºè¯æ®çš„åœ¨çº¿æ¢é’ˆã€‚ä½¿ç”¨è¿™ä¸ªå¯è§£é‡Šä¿¡å·ï¼ŒPIDæ§åˆ¶å™¨åŠ¨æ€è°ƒèŠ‚æ³¨æ„åŠ›å¤´ä»¥ä¿æŒäº‹å®ä¸€è‡´æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¤šè½®è§£ç ã€‚

Result: åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•(HotpotQAã€XSumã€HaluEvalã€RAGTruth)ä¸Šï¼ŒCOMPASSæŒç»­å‡å°‘ä¸Šä¸‹æ–‡å¹»è§‰ç‡(ç»å¯¹é™ä½2.8%åˆ°5.8%)ï¼ŒåŒæ—¶æ­ç¤ºäº†ä¸åŒæ³¨æ„åŠ›å¤´å¯¹è¯æ®å¯¹é½çš„è´¡çŒ®ã€‚

Conclusion: åé¦ˆé©±åŠ¨çš„å¯è§£é‡Šæ€§ä¸ºLLMè¡Œä¸ºçš„ç§‘å­¦ç†è§£æä¾›äº†ä¸€æ¡é€”å¾„ï¼ŒCOMPASSæ¡†æ¶åœ¨å‡å°‘å¹»è§‰çš„åŒæ—¶å¢å¼ºäº†å¯¹æ¨¡å‹æœºåˆ¶çš„ç†è§£ã€‚

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [3] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [4] [The Empowerment of Science of Science by Large Language Models: New Tools and Methods](https://arxiv.org/abs/2511.15370)
*Guoqiang Liang,Jingqian Gong,Mengxuan Li,Gege Lin,Shuo Zhang*

Main category: cs.CL

TL;DR: æœ¬æ–‡ä»ç”¨æˆ·è§’åº¦å…¨é¢å›é¡¾äº†æ”¯æ’‘å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æç¤ºå·¥ç¨‹ã€çŸ¥è¯†å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆã€å¾®è°ƒã€é¢„è®­ç»ƒå’Œå·¥å…·å­¦ä¹ ï¼Œå¹¶æ¢è®¨äº†LLMsåœ¨ç§‘å­¦è®¡é‡å­¦é¢†åŸŸçš„æ½œåœ¨åº”ç”¨å‰æ™¯ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆã€å›¾åƒè¯†åˆ«åŠå¤šæ¨¡æ€ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œæ­£æœç€AGIæ–¹å‘å‘å±•ï¼Œæˆä¸ºå…¨çƒæŠ€æœ¯ç«äº‰çš„æ ¸å¿ƒè®®é¢˜ã€‚

Method: é‡‡ç”¨æ–‡çŒ®ç»¼è¿°æ–¹æ³•ï¼Œç³»ç»Ÿæ¢³ç†LLMsçš„æ ¸å¿ƒæŠ€æœ¯å‘å±•å†ç¨‹ï¼Œå¹¶ç»“åˆç§‘å­¦è®¡é‡å­¦å‘å±•å†å²ï¼Œæå‡ºå‰ç»æ€§åº”ç”¨è§†è§’ã€‚

Result: æå‡ºäº†åŸºäºAIä»£ç†çš„ç§‘å­¦è¯„ä¼°æ¨¡å‹ï¼Œä»¥åŠåˆ©ç”¨LLMsè¿›è¡Œæ–°ç ”ç©¶å‰æ²¿æ£€æµ‹å’ŒçŸ¥è¯†å›¾è°±æ„å»ºçš„æ–°æ–¹æ³•ã€‚

Conclusion: LLMsåœ¨ç§‘å­¦è®¡é‡å­¦é¢†åŸŸå…·æœ‰å¹¿é˜”åº”ç”¨å‰æ™¯ï¼Œèƒ½å¤Ÿæ¨åŠ¨ç§‘å­¦è¯„ä¼°å’Œç ”ç©¶å‰æ²¿å‘ç°çš„å‘å±•ã€‚

Abstract: Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.

</details>


### [5] [DEPO: Dual-Efficiency Preference Optimization for LLM Agents](https://arxiv.org/abs/2511.15392)
*Sirui Chen,Mengshi Zhao,Lei Xu,Yuying Zhao,Beier Zhu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: DEPOæå‡ºäº†ä¸€ç§åŒæ•ˆç‡åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡è”åˆå¥–åŠ±ç®€æ´å“åº”å’Œæ›´å°‘è¡ŒåŠ¨æ­¥éª¤ï¼Œæ˜¾è‘—å‡å°‘LLMä»£ç†çš„tokenä½¿ç”¨å’Œæ­¥éª¤æ•°é‡ï¼ŒåŒæ—¶æå‡æ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: LLMä»£ç†æ¨ç†èƒ½åŠ›å¢å¼ºå¾€å¾€å¯¼è‡´æ€ç»´é“¾è¿‡é•¿ï¼Œå½±å“å®é™…åœºæ™¯ä¸­çš„äº¤äº’æ•ˆç‡ï¼Œä½†ç›®å‰ç¼ºä¹å¯¹LLMä»£ç†æ•ˆç‡çš„ç³»ç»Ÿå®šä¹‰ã€‚

Method: æå‡ºåŒæ•ˆç‡å®šä¹‰ï¼ˆæ­¥éª¤çº§æ•ˆç‡å’Œè½¨è¿¹çº§æ•ˆç‡ï¼‰ï¼Œå¹¶å¼€å‘DEPOæ–¹æ³•ï¼Œé€šè¿‡åå¥½ä¼˜åŒ–è”åˆå¥–åŠ±ç®€æ´å“åº”å’Œæ›´å°‘è¡ŒåŠ¨æ­¥éª¤ã€‚

Result: åœ¨WebShopå’ŒBabyAIä¸Šï¼ŒDEPOå‡å°‘tokenä½¿ç”¨è¾¾60.9%ï¼Œæ­¥éª¤å‡å°‘è¾¾26.9%ï¼Œæ€§èƒ½æå‡è¾¾29.3%ã€‚åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿèƒ½æ³›åŒ–ï¼Œä»…ç”¨25%æ•°æ®è®­ç»ƒä»ä¿æŒæ•ˆç‡å¢ç›Šã€‚

Conclusion: DEPOæœ‰æ•ˆè§£å†³äº†LLMä»£ç†æ•ˆç‡é—®é¢˜ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†äº¤äº’æ•ˆç‡ã€‚

Abstract: Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.

</details>


### [6] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: æå‡ºäº†NAMEGEnå¤šæ™ºèƒ½ä½“ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºè§£å†³ä¸­æ–‡èµ·åè¿™ä¸€çŸ­æ–‡æœ¬åˆ›æ„ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡è¿­ä»£çš„ç›®æ ‡æå–ã€åç§°ç”Ÿæˆå’Œè¯„ä¼°è¿‡ç¨‹æ¥æ»¡è¶³å¤šæ ·åŒ–éœ€æ±‚å¹¶ç”Ÿæˆå‡†ç¡®è§£é‡Šã€‚


<details>
  <summary>Details</summary>
Motivation: åˆ›æ„è‡ªç„¶è¯­è¨€ç”Ÿæˆé¢ä¸´å¤šç›®æ ‡çµæ´»æ€§å’Œè§£é‡Šå¤æ‚æ€§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­æ–‡æœ¬ç”Ÿæˆä¸­éš¾ä»¥åŒæ—¶æ»¡è¶³ä¸ªæ€§åŒ–ã€ç»†ç²’åº¦å’Œå¤šå…ƒåŒ–çš„ç”¨æˆ·éœ€æ±‚ï¼Œå¹¶ç†è§£éšå«æ„ä¹‰ä»¥å¢å¼ºç”¨æˆ·æ„ŸçŸ¥ã€‚

Method: NAMEGEnå¤šæ™ºèƒ½ä½“ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒ…å«ç›®æ ‡æå–ã€åç§°ç”Ÿæˆå’Œè¯„ä¼°ä¸‰ä¸ªè¿­ä»£æ­¥éª¤ï¼›æ„å»ºäº†17k+å¤å…¸è¯—æ­Œè¯­æ–™åº“å¢å¼ºç¾å­¦æ€§ï¼Œå¹¶å¼•å…¥CBNamesåŸºå‡†æµ‹è¯•ã€‚

Result: å®éªŒè¡¨æ˜NAMEGEnèƒ½æœ‰æ•ˆç”Ÿæˆæ»¡è¶³å¤šæ ·åŒ–ä¸ªæ€§åŒ–éœ€æ±‚çš„åˆ›æ„åç§°å¹¶æä¾›æœ‰æ„ä¹‰çš„è§£é‡Šï¼Œåœ¨å…­ä¸ªä¸åŒLLMåŸºçº¿çš„åŸºå‡†æ–¹æ³•ä¸­è¡¨ç°æœ€ä¼˜ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒã€‚

Conclusion: NAMEGEnæ¡†æ¶æˆåŠŸè§£å†³äº†åˆ›æ„çŸ­æ–‡æœ¬ç”Ÿæˆä¸­çš„å¤šç›®æ ‡ä¼˜åŒ–å’Œè§£é‡Šå¤æ‚æ€§æŒ‘æˆ˜ï¼Œä¸ºä¸ªæ€§åŒ–åˆ›æ„ç”Ÿæˆæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [7] [The productivity impact of coding agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fproductivity%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/btl4YJp2vnCBgQ27vAD_Kg_QgehD-hPOmmooLVvkVPA=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: å¯¹Cursorç”¨æˆ·çš„å¹¿æ³›ç ”ç©¶å‘ç°ï¼ŒAIç¼–ç åŠ©æ‰‹æˆä¸ºé»˜è®¤è®¾ç½®åï¼Œç»„ç»‡åˆå¹¶çš„PRæ•°é‡å¢åŠ äº†39%ï¼Œä¸”æ²¡æœ‰å¢åŠ ä»£ç å›æ»šæˆ–bugä¿®å¤ç‡ã€‚é«˜çº§å¼€å‘è€…æ›´é¢‘ç¹æ¥å—AIç”Ÿæˆçš„ä»£ç ï¼Œåœ¨ç¼–ç å‰è¿›è¡Œæ›´å¤šè§„åˆ’ï¼Œå¹¶èƒ½æ›´æœ‰æ•ˆåœ°ä½¿ç”¨AIåŠ©æ‰‹ã€‚


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶AIç¼–ç åŠ©æ‰‹å¯¹å¼€å‘è€…ç”Ÿäº§åŠ›çš„å®é™…å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨æˆä¸ºé»˜è®¤å·¥å…·åçš„ä½¿ç”¨æ•ˆæœå’Œä¸åŒç»éªŒæ°´å¹³å¼€å‘è€…çš„ä½¿ç”¨å·®å¼‚ã€‚

Method: å¯¹æ•°ä¸‡åCursorç”¨æˆ·è¿›è¡Œå¤§è§„æ¨¡ç ”ç©¶ï¼Œåˆ†æAIåŠ©æ‰‹æˆä¸ºé»˜è®¤è®¾ç½®å‰åçš„PRåˆå¹¶æ•°é‡ã€ä»£ç å›æ»šç‡ã€bugä¿®å¤ç‡ç­‰æŒ‡æ ‡ï¼Œå¹¶è°ƒæŸ¥ä¸åŒçº§åˆ«å¼€å‘è€…çš„ä½¿ç”¨æ¨¡å¼ã€‚

Result: ç»„ç»‡PRåˆå¹¶æ•°é‡å¢åŠ 39%ï¼Œæ²¡æœ‰è§‚å¯Ÿåˆ°ä»£ç å›æ»šæˆ–bugä¿®å¤ç‡ä¸Šå‡ã€‚é«˜çº§å¼€å‘è€…æ›´æ„¿æ„æ¥å—AIç”Ÿæˆçš„ä»£ç ï¼Œåœ¨ç¼–ç å‰è¿›è¡Œæ›´å¤šè§„åˆ’ï¼Œ61%çš„ç”¨æˆ·è¯·æ±‚æ˜¯é’ˆå¯¹ä»£ç å®ç°ã€‚

Conclusion: AIç¼–ç åŠ©æ‰‹èƒ½æ˜¾è‘—æå‡å¼€å‘æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹ç»éªŒä¸°å¯Œçš„å¼€å‘è€…æ•ˆæœæ›´æ˜æ˜¾ï¼Œä¸”ä¸ä¼šé™ä½ä»£ç è´¨é‡ã€‚

Abstract: The productivity impact of coding agents (3 minute read) A large study of tens of thousands of Cursor users found that after agents became the default, organizations merged 39% more PRs with no increase in revert or bug-fix rates. Senior developers accepted agent-written code more often, planned more before coding, and generally used agents more effectively, while most user requests (61%) were for code implementation

</details>


### [8] [Google Antigravity](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantigravity.google%2Fblog%2Fintroducing-google-antigravity%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/z4xOVIYNTf_SUcGgzLTrNKf7Q8NIDdH-MNvw_gx5AtE=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google Antigravityæ˜¯ä¸€ä¸ªé¢å‘æ™ºèƒ½ä½“çš„å¼€å‘å¹³å°ï¼Œé›†æˆäº†AIé©±åŠ¨çš„IDEå’Œèƒ½å¤Ÿè·¨å¤šä¸ªç•Œé¢è§„åˆ’æ‰§è¡Œå¤æ‚è½¯ä»¶ä»»åŠ¡çš„è‡ªä¸»æ™ºèƒ½ä½“ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¸ºäº†è§£å†³ä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸­æ™ºèƒ½ä½“åä½œå’Œä»»åŠ¡ç®¡ç†çš„å¤æ‚æ€§ï¼Œæä¾›æ›´é«˜è‡ªä¸»æ€§çš„ç¼–ç æ”¯æŒã€‚

Method: é‡‡ç”¨AIé©±åŠ¨çš„IDEä¸è‡ªä¸»æ™ºèƒ½ä½“ç›¸ç»“åˆï¼Œæ”¯æŒä»»åŠ¡çº§é€æ˜åº¦ã€å¼‚æ­¥æ™ºèƒ½ä½“ç®¡ç†ã€ç®€æ˜“åé¦ˆå’Œå†…ç½®å­¦ä¹ æœºåˆ¶ã€‚

Result: æ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿæ”¯æŒGemini 3ç­‰æ¨¡å‹çš„æ™ºèƒ½ä½“ä¼˜å…ˆå¼€å‘å¹³å°ã€‚

Conclusion: è¯¥å¹³å°é€šè¿‡é›†æˆAI IDEå’Œè‡ªä¸»æ™ºèƒ½ä½“ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„è½¯ä»¶å¼€å‘æµç¨‹å’Œæ›´é«˜ç¨‹åº¦çš„è‡ªåŠ¨åŒ–ã€‚

Abstract: Google Antigravity (Resource) Google Antigravity is an agent-first development platform that pairs an AI-powered IDE with autonomous agents capable of planning and executing complex software tasks across multiple surfaces. It adds task-level transparency, async agent management, easy feedback, and built-in learning to support higher-autonomy coding with models like Gemini 3.

</details>


### [9] [How Dash uses context engineering for smarter AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/CqTqgksp6mPGgf-OlqZOQD_u1cpTMSxy-TvUO5Zwlqk=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dropbox Dashä»æœç´¢ç³»ç»Ÿæ¼”å˜ä¸ºæ™ºèƒ½AIä»£ç†ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å·¥ç¨‹æ¥æ›´å¥½åœ°è§£é‡Šã€æ€»ç»“å’Œæ“ä½œä¿¡æ¯ï¼Œè§£å†³å·¥å…·è¿‡å¤šå’Œæ•°æ®è¿‡è½½å¯¼è‡´çš„'åˆ†æç˜«ç—ª'å’Œ'ä¸Šä¸‹æ–‡è…åŒ–'é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€å·¥å…·å’Œæ•°æ®çš„æ¿€å¢ï¼ŒAIç³»ç»Ÿé¢ä¸´'åˆ†æç˜«ç—ª'ï¼ˆä¿¡æ¯è¿‡å¤šéš¾ä»¥å†³ç­–ï¼‰å’Œ'ä¸Šä¸‹æ–‡è…åŒ–'ï¼ˆä¿¡æ¯è¿‡æ—¶æˆ–ä¸ç›¸å…³ï¼‰çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æ™ºèƒ½çš„ä¸Šä¸‹æ–‡ç®¡ç†æ–¹æ³•ã€‚

Method: é‡‡ç”¨ä¸Šä¸‹æ–‡å·¥ç¨‹æ–¹æ³•ï¼ŒåŒ…æ‹¬æ£€ç´¢æ•´åˆã€ç›¸å…³ä¸Šä¸‹æ–‡è¿‡æ»¤å’Œä¸“ç”¨ä»»åŠ¡ä»£ç†ï¼Œæ¥ä¼˜åŒ–AIçš„å†³ç­–è¿‡ç¨‹ã€‚

Result: Dashé€šè¿‡ä¸Šä¸‹æ–‡å·¥ç¨‹æˆåŠŸæå‡äº†AIå¯¹ä¿¡æ¯çš„è§£é‡Šã€æ€»ç»“å’Œæ“ä½œèƒ½åŠ›ï¼Œæ”¹å–„äº†å†³ç­–è´¨é‡ã€‚

Conclusion: ä¸Šä¸‹æ–‡å·¥ç¨‹æ˜¯è§£å†³AIç³»ç»Ÿä¸­ä¿¡æ¯è¿‡è½½å’Œå†³ç­–å›°éš¾çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¸Šä¸‹æ–‡ç®¡ç†å¯ä»¥æ˜¾è‘—æå‡AIä»£ç†çš„æ€§èƒ½ã€‚

Abstract: How Dash uses context engineering for smarter AI (7 minute read) Dropbox Dash has evolved from a search system into an agentic AI to better interpret, summarize, and act on information, requiring a shift towards context engineering. Dash curates context through retrieval consolidation, relevant context filtering, and specialized task agents to improve the AI's decision-making, addressing issues like "analysis paralysis" and "context rot" that arise with too many tools and excessive data.

</details>


### [10] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/LGVU2aOSDxKzzsz2GM1lMpF-AcZTL86kpSouHzFemNg=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google Kubernetes Engine (GKE) åœ¨10å‘¨å¹´ä¹‹é™…æ¨å‡ºé‡å¤§åˆ›æ–°ï¼Œæ”¯æŒAIå’Œæ™ºèƒ½ä½“å·¥ä½œè´Ÿè½½ï¼Œä»å®¹å™¨æ‰©å±•åˆ°æ™ºèƒ½ä½“å¹³å°ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€AIå’Œæ™ºèƒ½ä½“å·¥ä½œè´Ÿè½½çš„æ™®åŠï¼Œéœ€è¦ç»Ÿä¸€çš„å¹³å°æ¥æ”¯æŒè¿™äº›ç°ä»£å·¥ä½œè´Ÿè½½ï¼ŒGKEæ—¨åœ¨ä»å®¹å™¨å¹³å°æ¼”è¿›ä¸ºæ”¯æŒAIå’Œæ™ºèƒ½ä½“çš„ç»Ÿä¸€å¹³å°ã€‚

Method: é€šè¿‡GKEå¹³å°çš„åˆ›æ–°å‡çº§ï¼Œæ‰©å±•å…¶èƒ½åŠ›ä»¥æ”¯æŒAIæ¨¡å‹éƒ¨ç½²å’Œæ™ºèƒ½ä½“å·¥ä½œè´Ÿè½½ï¼Œæä¾›ç»Ÿä¸€çš„å¹³å°è§£å†³æ–¹æ¡ˆã€‚

Result: GKEæˆåŠŸå®ç°äº†ä»å®¹å™¨å¹³å°å‘æ”¯æŒAIå’Œæ™ºèƒ½ä½“å·¥ä½œè´Ÿè½½çš„ç»Ÿä¸€å¹³å°çš„è½¬å‹ï¼Œä¸ºç°ä»£å·¥ä½œè´Ÿè½½æä¾›å…¨é¢æ”¯æŒã€‚

Conclusion: GKEçš„æ¼”è¿›å±•ç¤ºäº†å®¹å™¨å¹³å°å‘AIå’Œæ™ºèƒ½ä½“å·¥ä½œè´Ÿè½½æ”¯æŒçš„æ‰©å±•æ½œåŠ›ï¼Œä¸ºç°ä»£åº”ç”¨éƒ¨ç½²æä¾›äº†ç»Ÿä¸€çš„åŸºç¡€è®¾æ–½ã€‚

Abstract: GKE: From containers to agents, the unified platform for every modern workload (5 minute read) Google Kubernetes Engine marked its 10th anniversary with major innovations to support AI and agentic workloads.

</details>


### [11] [Google Antigravity](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantigravity.google%2F%3Futm_source=tldrwebdev/1/0100019a9c1b9508-6fafb6a9-4866-48c1-9c25-83c80a0cd52c-000000/aRfKW5_33RVTctfYvipZXtST9gaDvjmcyhZR9pETlE8=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google Antigravityæ˜¯ä¸€ä¸ªåŸºäºGemini 3æ„å»ºçš„AIé©±åŠ¨çš„ä»£ç†å¼€å‘å¹³å°ï¼ŒåŒ…å«AIå¢å¼ºçš„IDEä½“éªŒå’Œé¢å‘å¤šä»£ç†ç¼–æ’çš„ä»£ç†ä¼˜å…ˆç•Œé¢ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¸ºå¼€å‘è€…æä¾›æ›´å¼ºå¤§çš„AIé©±åŠ¨å¼€å‘å·¥å…·ï¼Œé€šè¿‡é›†æˆå¤šä¸ªæ™ºèƒ½ä»£ç†æ¥æå‡è½¯ä»¶å¼€å‘æ•ˆç‡å’Œè´¨é‡ã€‚

Method: æ„å»ºåŸºäºGemini 3çš„AIå¹³å°ï¼Œæä¾›ä¸¤ç§ä¸»è¦ç•Œé¢ï¼šEditor viewï¼ˆAIå¢å¼ºçš„IDEä½“éªŒï¼‰å’ŒManager surfaceï¼ˆå¤šä»£ç†ç¼–æ’çš„ä»£ç†ä¼˜å…ˆç•Œé¢ï¼‰ã€‚

Result: å¼€å‘äº†Antigravityå¹³å°ï¼Œæ•´åˆäº†AIé©±åŠ¨çš„ä»£ç ç¼–è¾‘å’Œå¤šä»£ç†åä½œåŠŸèƒ½ã€‚

Conclusion: Antigravityä»£è¡¨äº†AIé©±åŠ¨è½¯ä»¶å¼€å‘çš„æ–°æ–¹å‘ï¼Œé€šè¿‡æ™ºèƒ½ä»£ç†å’ŒAIå¢å¼ºå·¥å…·æå‡å¼€å‘æ•ˆç‡ã€‚

Abstract: Google Antigravity (Website) Google's Antigravity is a new AI-powered agentic development platform built on Gemini 3. Antigravity has both an AI-powered IDE experience ("Editor view") and an agent-first interface for orchestrating multiple agents ("Manager surface").

</details>


### [12] [AI is writing your code, but who's reviewing it?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbdtechtalks.com%2F2025%2F11%2F17%2Fai-is-writing-your-code-but-whos-reviewing-it%3Futm_source=tldrwebdev/1/0100019a9c1b9508-6fafb6a9-4866-48c1-9c25-83c80a0cd52c-000000/yiL992ubdwpMQleLDg0W3c8GYTTRnBsHmWk-dqrF7z8=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AIä»£ç ç”Ÿæˆå·¥å…·äº§ç”Ÿäº†æŠ€æœ¯å€ºåŠ¡é—®é¢˜ï¼Œéœ€è¦AIä»£ç å®¡æŸ¥å·¥å…·æ¥æ£€æµ‹å®‰å…¨æ¼æ´ã€æ€§èƒ½é—®é¢˜å’Œæ¶æ„é—®é¢˜ï¼Œå®ç°æ™ºèƒ½åŒ–çš„è½¯ä»¶å¼€å‘å‘¨æœŸã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³AIä»£ç ç”Ÿæˆå·¥å…·äº§ç”Ÿçš„æŠ€æœ¯å€ºåŠ¡é—®é¢˜ï¼Œå¦‚ä»£ç è‡ƒè‚¿ã€bugå¤šã€æ¶æ„ä¸å½“å’ŒåŠŸèƒ½å†—ä½™ç­‰ã€‚

Method: ä½¿ç”¨AIé©±åŠ¨çš„ä»£ç å®¡æŸ¥å·¥å…·ï¼Œåƒè‡ªåŠ¨åŒ–é«˜çº§å·¥ç¨‹å¸ˆä¸€æ ·åˆ†æä»£ç ï¼Œæ£€æµ‹å®‰å…¨æ¼æ´ã€æ€§èƒ½é—®é¢˜å’Œæ¶æ„é—®é¢˜ã€‚

Result: å®ç°äº†æ™ºèƒ½åŒ–çš„è½¯ä»¶å¼€å‘å‘¨æœŸï¼Œå…¶ä¸­ä¸“é—¨çš„AIä»£ç†è´Ÿè´£ä»£ç å®¡æŸ¥ã€‚

Conclusion: AIä»£ç å®¡æŸ¥å·¥å…·æ˜¯è§£å†³AIç”Ÿæˆä»£ç è´¨é‡é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæå‡ä»£ç è´¨é‡å’Œå¼€å‘æ•ˆç‡ã€‚

Abstract: AI is writing your code, but who's reviewing it? (8 minute read) AI codegen tools are creating a new form of technical debt (â€œAI slopâ€): bloated, buggy code that doesn't have proper architecture and includes redundant functions. To address this problem, AI-powered code review tools act as automated senior engineers, analyzing code for security vulnerabilities, performance issues, and architectural problems. This leads to an agentic software development lifecycle where specialized AI agents ha...

</details>


### [13] [ğŸ†• StrongDM launches Leash: open-source policy enforcement for AI agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strongdm.com%2Fblog%2Fpolicy-enforcement-for-agentic-ai-with-leash%3Futm_source=tldrinfosec/1/0100019a9c711ace-b8efe0e9-e262-4896-b282-396a1bfed22d-000000/oWZp5GJknd6vp4h4hpVFehX2gkPJ-6zjXLia1e1MTt8=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: StrongDMæ¨å‡ºLeashï¼šä¸ºAIä»£ç†æä¾›å¼€æºç­–ç•¥æ‰§è¡Œæ¡†æ¶ï¼Œé€šè¿‡å†…æ ¸çº§æ§åˆ¶å®ç°ä½å»¶è¿Ÿçš„è¿è¡Œæ—¶è®¿é—®æ§åˆ¶


<details>
  <summary>Details</summary>
Motivation: AIä»£ç†æˆä¸ºå¢é•¿æœ€å¿«çš„èº«ä»½ç±»åˆ«ï¼Œéœ€è¦7x24å°æ—¶è¿æ¥åˆ°å·¥å…·å’Œç³»ç»Ÿï¼Œè¿™è¦æ±‚æ–°çš„è®¿é—®æ§åˆ¶æ–¹æ³•

Method: Leashåœ¨æ“ä½œç³»ç»Ÿå†…æ ¸å±‚é¢å·¥ä½œï¼ˆ<1mså»¶è¿Ÿå¼€é”€ï¼‰ï¼Œæ ¹æ®äººç±»å¯è¯»çš„é˜²æŠ¤æ è¯„ä¼°ä»£ç†æ´»åŠ¨

Result: Leashå°†è¿è¡Œæ—¶æ§åˆ¶æ‰©å±•åˆ°ä»£ç†ç³»ç»Ÿï¼Œæä¾›é«˜æ•ˆçš„å®‰å…¨ç­–ç•¥æ‰§è¡Œ

Conclusion: Leashä¸ºAIä»£ç†ç³»ç»Ÿæä¾›äº†å¿…è¦çš„è¿è¡Œæ—¶è®¿é—®æ§åˆ¶è§£å†³æ–¹æ¡ˆ

Abstract: ğŸ†• StrongDM launches Leash: open-source policy enforcement for AI agents (Sponsor) AI agents are the fastest-growing class of identity. They're connecting to tools and systems 24/7 - requiring a new approach to access control. Leash is StrongDM's new open-source project that extends runtime control to agentic systems. Leash works at kernel level (<1ms latency overhead) to evaluate agent activity against human-readable guardrails. Read the blog

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [14] [æ¸…åä¸å—æ´‹ç†å·¥æå‡ºVLA-RLï¼šç”¨<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>å¢å¼ºæœºå™¨äººå¤§æ¨¡å‹](http://mp.weixin.qq.com/s?__biz=MzU2ODgzMTM5NA==&mid=2247503881&idx=1&sn=17f1d05f1d78464ce56193ca4d8a7fe7&chksm=fd9d18459110a7f09a9ddafb6847100f21de0992d3b242c1fe214301f3652752c4629750b884#rd)
*CAAIè®¤çŸ¥ç³»ç»Ÿä¸ä¿¡æ¯å¤„ç†ä¸“å§”ä¼š*

Main category: wechat.article

TL;DR: è€Œè¿™æ­£æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ‹¿æ‰‹å¥½æˆã€‚é€šè¿‡åœ¨çº¿æ”¶é›†æ•°æ®å¹¶æ ¹æ®å¥–åŠ±ä¿¡å·è¿›è¡Œä¼˜åŒ–ï¼ŒRLèƒ½è®©æ™ºèƒ½ä½“å‘ç°æ¯”ç¤ºæ•™æ•°æ®æ›´ä¼˜çš„ç­–ç•¥ã€‚ä¸Šå›¾æ¸…æ™°åœ°å±•ç¤ºäº†VLA-RLä¸ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ æ–¹æ³•çš„åŒºåˆ«ï¼Œä»¥åŠå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: è€Œè¿™æ­£æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ‹¿æ‰‹å¥½æˆã€‚é€šè¿‡åœ¨çº¿æ”¶é›†æ•°æ®å¹¶æ ¹æ®å¥–åŠ±ä¿¡å·è¿›è¡Œä¼˜åŒ–ï¼ŒRLèƒ½è®©æ™ºèƒ½ä½“å‘ç°æ¯”ç¤ºæ•™æ•°æ®æ›´ä¼˜çš„ç­–ç•¥ã€‚ä¸Šå›¾æ¸…æ™°åœ°å±•ç¤ºäº†VLA-RLä¸ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ æ–¹æ³•çš„åŒºåˆ«ï¼Œä»¥åŠå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚

</details>


### [15] [æ·±æ€è€ƒæ—¶ä»£â€”â€”æ·±æ€è€ƒæ¨¡å‹ä¸<em class="highlight">å¼ºåŒ–å­¦ä¹ </em> | CCCFç²¾é€‰](http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651615664&idx=1&sn=c78426fb67016e1a6fbff6c8cde15849&chksm=bc49b50ebfe2d2bee413ca6574a7d76c5784fdb2842967e39cf15d5f01f7ae71f1f3194367d1#rd)
*ä¸­å›½è®¡ç®—æœºå­¦ä¼š*

Main category: wechat.article

TL;DR: åœ¨æœ¬æœŸâ€œæ·±æ€è€ƒæ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ â€ä¸“é¢˜ä¸­ï¼Œç»éªŒä¸°å¯Œçš„ä¸€çº¿ä¸“å®¶å­¦è€…ä»åŸºæœ¬æ¦‚å¿µã€æ–¹æ³•è¿­ä»£ã€è½¯ç¡¬ååŒç­‰å¤šä¸ªç»´åº¦å±•å¼€è®ºè¿°ï¼Œäº‰å–ä¸ºè¯»è€…å‘ˆç°ä¸€å¹…å…³äºæ·±æ€è€ƒæ—¶ä»£çš„ç«‹ä½“å…¨æ™¯å›¾ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: åœ¨æœ¬æœŸâ€œæ·±æ€è€ƒæ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ â€ä¸“é¢˜ä¸­ï¼Œç»éªŒä¸°å¯Œçš„ä¸€çº¿ä¸“å®¶å­¦è€…ä»åŸºæœ¬æ¦‚å¿µã€æ–¹æ³•è¿­ä»£ã€è½¯ç¡¬ååŒç­‰å¤šä¸ªç»´åº¦å±•å¼€è®ºè¿°ï¼Œäº‰å–ä¸ºè¯»è€…å‘ˆç°ä¸€å¹…å…³äºæ·±æ€è€ƒæ—¶ä»£çš„ç«‹ä½“å…¨æ™¯å›¾ã€‚

</details>


### [16] [ä¸Šäº¤åšå£«æœ€æ–°æ€è€ƒï¼šä»…ç”¨ä¸¤ä¸ªé—®é¢˜è®²æ¸…<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>](http://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&mid=2651739423&idx=1&sn=3f10f98a3df5e8ba3aaad52f0f40d8b7&chksm=bc467c2cc72f3dee8292377ba1d9feb49799faac39bacac725c8a8e20513051867c59d0194a2#rd)
*å¤§æ•°æ®æ–‡æ‘˜*

Main category: wechat.article

TL;DR: å¼ºåŒ–å­¦ä¹ çš„è¿‡ç¨‹ï¼Œæœ¬è´¨ä¸Šæ˜¯æ™ºèƒ½ä½“ä¸æ–­æ”¶é›†ç»éªŒã€å¹¶ç”¨è¿™äº›ç»éªŒæ”¹è¿›ç­–ç•¥çš„å¾ªç¯ã€‚ä¸åŒç®—æ³•çš„å·®å¼‚ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå®ƒä»¬ä¾èµ–ä»€ä¹ˆæ ·çš„æ•°æ®ã€‚æœ€ç›´æ¥çš„æ–¹å¼æ˜¯â€œåœ¨ç­–ç•¥å­¦ä¹ â€ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: å¼ºåŒ–å­¦ä¹ çš„è¿‡ç¨‹ï¼Œæœ¬è´¨ä¸Šæ˜¯æ™ºèƒ½ä½“ä¸æ–­æ”¶é›†ç»éªŒã€å¹¶ç”¨è¿™äº›ç»éªŒæ”¹è¿›ç­–ç•¥çš„å¾ªç¯ã€‚ä¸åŒç®—æ³•çš„å·®å¼‚ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå®ƒä»¬ä¾èµ–ä»€ä¹ˆæ ·çš„æ•°æ®ã€‚æœ€ç›´æ¥çš„æ–¹å¼æ˜¯â€œåœ¨ç­–ç•¥å­¦ä¹ â€ã€‚

</details>


### [17] [èš‚èšå¼€æºä¸‡äº¿å‚æ•°<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>é«˜æ€§èƒ½æƒé‡äº¤æ¢æ¡†æ¶ Awex](http://mp.weixin.qq.com/s?__biz=Mzg2MTg4ODc4Mg==&mid=2247492289&idx=1&sn=e1e3ef17fd24b3343af1c354b607fbb2&chksm=cf6e9c27dd73cefa36c89ad111af8cb118e65b9dd2de435916e0f6a178be6dd68b52913e20c2#rd)
*èš‚èšå¼€æº*

Main category: wechat.article

TL;DR: è¿‡å»å‡ å¹´ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼Œç®€ç§° RLï¼‰å·²ç»æˆä¸ºå¤§æ¨¡å‹åè®­ç»ƒçš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä» ChatGPT çš„ RLHFï¼Œåˆ° DeepSeek/Claude/Llama çš„åè®­ç»ƒä½“ç³»ï¼Œéƒ½ä¾èµ–å¼ºåŒ–å­¦ä¹ è®©æ¨¡å‹æ›´ç¬¦åˆäººç±»åå¥½ã€å…·å¤‡æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥æ‰©å¤§æ¨¡å‹æ™ºèƒ½è¾¹


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: è¿‡å»å‡ å¹´ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼Œç®€ç§° RLï¼‰å·²ç»æˆä¸ºå¤§æ¨¡å‹åè®­ç»ƒçš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä» ChatGPT çš„ RLHFï¼Œåˆ° DeepSeek/Claude/Llama çš„åè®­ç»ƒä½“ç³»ï¼Œéƒ½ä¾èµ–å¼ºåŒ–å­¦ä¹ è®©æ¨¡å‹æ›´ç¬¦åˆäººç±»åå¥½ã€å…·å¤‡æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥æ‰©å¤§æ¨¡å‹æ™ºèƒ½è¾¹

</details>


### [18] [æ¸…åä¸å—æ´‹ç†å·¥æå‡ºVLA-RLï¼šç”¨<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>å¢å¼ºæœºå™¨äººå¤§æ¨¡å‹ï¼Œå‘Šåˆ«æ¨¡ä»¿å­¦ä¹ ç“¶é¢ˆ](http://mp.weixin.qq.com/s?__biz=MzUzODkxNzQzMw==&mid=2247495855&idx=1&sn=796609bf779bda6c85fd04bb856cfd19&chksm=fba0e2f6ae31341731de2967231af84805d2db4ac455482186a6cb0b8e57fab968fb150c0817#rd)
*VLer*

Main category: wechat.article

TL;DR: è€Œè¿™æ­£æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ‹¿æ‰‹å¥½æˆã€‚é€šè¿‡åœ¨çº¿æ”¶é›†æ•°æ®å¹¶æ ¹æ®å¥–åŠ±ä¿¡å·è¿›è¡Œä¼˜åŒ–ï¼ŒRLèƒ½è®©æ™ºèƒ½ä½“å‘ç°æ¯”ç¤ºæ•™æ•°æ®æ›´ä¼˜çš„ç­–ç•¥ã€‚ä¸Šå›¾æ¸…æ™°åœ°å±•ç¤ºäº†VLA-RLä¸ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ æ–¹æ³•çš„åŒºåˆ«ï¼Œä»¥åŠå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: è€Œè¿™æ­£æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ‹¿æ‰‹å¥½æˆã€‚é€šè¿‡åœ¨çº¿æ”¶é›†æ•°æ®å¹¶æ ¹æ®å¥–åŠ±ä¿¡å·è¿›è¡Œä¼˜åŒ–ï¼ŒRLèƒ½è®©æ™ºèƒ½ä½“å‘ç°æ¯”ç¤ºæ•™æ•°æ®æ›´ä¼˜çš„ç­–ç•¥ã€‚ä¸Šå›¾æ¸…æ™°åœ°å±•ç¤ºäº†VLA-RLä¸ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ æ–¹æ³•çš„åŒºåˆ«ï¼Œä»¥åŠå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚

</details>


### [19] [ã€<em class="highlight">æ™ºèƒ½ä½“</em>å¼€å‘ã€‘ç³»ç»Ÿè§£æ„AI Agentä¸<em class="highlight">Agentic</em> AIçš„æ ¸å¿ƒå·®å¼‚](http://mp.weixin.qq.com/s?__biz=MzI3NDI4MzIyNQ==&mid=2247513505&idx=1&sn=27cf583b1b9fdac5569055dc354b2e19&chksm=ea5f84117d0b44c8fbb99336ef7887d8a4a8d35f7fd31a941e1af33b7ded1bd8ccfe5a8a3d39#rd)
*äº§ä¸šæ™ºèƒ½å®˜*

Main category: wechat.article

TL;DR: Agentic AIï¼ˆä»£ç†å¼AIï¼‰ï¼šç”±å¤šä¸ªä¸“ä¸šåŒ–æ™ºèƒ½ä½“ç»„æˆçš„ååŒç³»ç»Ÿï¼Œæ ¸å¿ƒèƒ½åŠ›æ˜¯ç›®æ ‡æ‹†è§£-ä»»åŠ¡åˆ†é…-ç»“æœæ•´åˆã€‚ç³»ç»ŸåŒ…å«ä¸­å¤®åè°ƒè€…ï¼ˆå¦‚å…ƒæ™ºèƒ½ä½“ï¼‰ï¼Œèƒ½åŠ¨æ€åˆ†é…å­ä»»åŠ¡ï¼Œé€šè¿‡å…±äº«è®°å¿†å®ç°å¤æ‚å·¥ä½œæµï¼ˆå¦‚è‡ªåŠ¨åŒ–ç ”ç©¶ã€å¤šæœºå™¨äººåè°ƒï¼‰


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: Agentic AIï¼ˆä»£ç†å¼AIï¼‰ï¼šç”±å¤šä¸ªä¸“ä¸šåŒ–æ™ºèƒ½ä½“ç»„æˆçš„ååŒç³»ç»Ÿï¼Œæ ¸å¿ƒèƒ½åŠ›æ˜¯ç›®æ ‡æ‹†è§£-ä»»åŠ¡åˆ†é…-ç»“æœæ•´åˆã€‚ç³»ç»ŸåŒ…å«ä¸­å¤®åè°ƒè€…ï¼ˆå¦‚å…ƒæ™ºèƒ½ä½“ï¼‰ï¼Œèƒ½åŠ¨æ€åˆ†é…å­ä»»åŠ¡ï¼Œé€šè¿‡å…±äº«è®°å¿†å®ç°å¤æ‚å·¥ä½œæµï¼ˆå¦‚è‡ªåŠ¨åŒ–ç ”ç©¶ã€å¤šæœºå™¨äººåè°ƒï¼‰

</details>


### [20] [Omdiaï¼š<em class="highlight">Agentic</em> AIå°†é©±åŠ¨ç”µä¿¡ç½‘ç»œè‡ªä¸»åŒ–ï¼Œ41%è¿è¥å•†èšç„¦ç½‘ç»œç®¡ç†](http://mp.weixin.qq.com/s?__biz=Mzk0NjMyMjA2NQ==&mid=2247499477&idx=1&sn=611099b295b40bb96d7a7e34de5912bb&chksm=c20e7f0f2bcc53b156a659a49e306407395ce85375e59ecbf94e542b3faf1b2119c14e6c9476#rd)
*Omdia*

Main category: wechat.article

TL;DR: Omdiaæœ€æ–°ç ”ç©¶æ˜¾ç¤ºï¼ŒAgentic AIï¼ˆè‡ªä¸»æ™ºèƒ½ä½“AIï¼‰æ­£æˆä¸ºæ¨åŠ¨ç”µä¿¡è¿è¥å•†ï¼ˆCSPï¼‰ç½‘ç»œè‡ªä¸»åŒ–çš„å…³é”®åŠ›é‡ã€‚æ ¹æ®Omdiaå‘å¸ƒçš„ã€ŠAgentic AIï¼š An Evolution with Transformative Potential for Telecom Operationsã€‹æŠ¥å‘Šï¼Œ41%çš„CSPè®¤ä¸ºç½‘ç»œç®¡ç†å°†æ˜¯Agentic AIå½±å“æœ€å¤§


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: Omdiaæœ€æ–°ç ”ç©¶æ˜¾ç¤ºï¼ŒAgentic AIï¼ˆè‡ªä¸»æ™ºèƒ½ä½“AIï¼‰æ­£æˆä¸ºæ¨åŠ¨ç”µä¿¡è¿è¥å•†ï¼ˆCSPï¼‰ç½‘ç»œè‡ªä¸»åŒ–çš„å…³é”®åŠ›é‡ã€‚æ ¹æ®Omdiaå‘å¸ƒçš„ã€ŠAgentic AIï¼š An Evolution with Transformative Potential for Telecom Operationsã€‹æŠ¥å‘Šï¼Œ41%çš„CSPè®¤ä¸ºç½‘ç»œç®¡ç†å°†æ˜¯Agentic AIå½±å“æœ€å¤§

</details>


### [21] [Gemini 3 Pro æ·±åº¦æŠ€æœ¯å‰–æï¼šé¢å‘æœªæ¥çš„ <em class="highlight">Agentic</em> é©å‘½](http://mp.weixin.qq.com/s?__biz=MzYyMzgyMDgyNg==&mid=2247484273&idx=1&sn=ffffc36e845a52244f8dd1b9b74125a5&chksm=fec3a0a970a6725f1ee19327979ccb748ed945f7905cf94832b8e38fb1dabf6dfddcd7eb493e#rd)
*æ™ºé€ ç¼–ç¨‹*

Main category: wechat.article

TL;DR: æœ¬å‘¨ Google DeepMind å‘å¸ƒçš„ Gemini 3 Pro æ¨¡å‹ï¼Œé€šè¿‡é©æ–°æ€§çš„ Agenticï¼ˆè‡ªä¸»æ™ºèƒ½ä½“ï¼‰èƒ½åŠ›å’Œè¶…æ·±åº¦æ¨ç†ï¼Œé‡æ–°ç¡®ç«‹äº† SOTA æ¨¡å‹çš„æ ‡å‡†ã€‚æœ¬æ–‡å°†ä»æ¶æ„ã€æ€§èƒ½ã€è¡Œä¸šç«äº‰å’Œå·¥ç¨‹å®è·µå››ä¸ªç»´åº¦ï¼Œä¸ºæŠ€æœ¯äººå‘˜æä¾›ä¸€ä¸ªå…¨é¢ã€æ·±å…¥çš„åˆ†æã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: æœ¬å‘¨ Google DeepMind å‘å¸ƒçš„ Gemini 3 Pro æ¨¡å‹ï¼Œé€šè¿‡é©æ–°æ€§çš„ Agenticï¼ˆè‡ªä¸»æ™ºèƒ½ä½“ï¼‰èƒ½åŠ›å’Œè¶…æ·±åº¦æ¨ç†ï¼Œé‡æ–°ç¡®ç«‹äº† SOTA æ¨¡å‹çš„æ ‡å‡†ã€‚æœ¬æ–‡å°†ä»æ¶æ„ã€æ€§èƒ½ã€è¡Œä¸šç«äº‰å’Œå·¥ç¨‹å®è·µå››ä¸ªç»´åº¦ï¼Œä¸ºæŠ€æœ¯äººå‘˜æä¾›ä¸€ä¸ªå…¨é¢ã€æ·±å…¥çš„åˆ†æã€‚

</details>


### [22] [AIæµè§ˆå™¨å½»åº•æ”¹å˜äº†æˆ‘ä»¬ä½¿ç”¨äº’è”ç½‘çš„æ–¹å¼](http://mp.weixin.qq.com/s?__biz=MzYzOTIzNDIxNg==&mid=2247483669&idx=1&sn=ce1cd75f72802fb63956c35ef81a62a4&chksm=f1f1e09fce1128fe56a61fb292a8e196ac84c3e9944c3fc1332d5869edd6c98b882660738074#rd)
*ä¸€æ¡è§é—»*

Main category: wechat.article

TL;DR: é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹Agenticæµè§ˆå¯èƒ½ä¼šç»™äº’è”ç½‘çš„æœªæ¥ã€ä¿¡æ¯ä»¥åŠäººç±»ä¸æœºå™¨çš„äº¤äº’æ–¹å¼å¸¦æ¥æ€æ ·çš„å½±å“ã€‚Agentic æµè§ˆæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿè¿‘å‡ ä¸ªæœˆæ¥ï¼Œå·²æœ‰å¤šæ¬¾æ”¯æŒAgentçš„ç½‘ç»œæµè§ˆå™¨å‘å…¬ä¼—å‘å¸ƒï¼šå…¶ä¸­åŒ…æ‹¬ç›®å‰ä»…é€‚ç”¨äº Mac ç³»ç»Ÿçš„ Chat


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹Agenticæµè§ˆå¯èƒ½ä¼šç»™äº’è”ç½‘çš„æœªæ¥ã€ä¿¡æ¯ä»¥åŠäººç±»ä¸æœºå™¨çš„äº¤äº’æ–¹å¼å¸¦æ¥æ€æ ·çš„å½±å“ã€‚Agentic æµè§ˆæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿè¿‘å‡ ä¸ªæœˆæ¥ï¼Œå·²æœ‰å¤šæ¬¾æ”¯æŒAgentçš„ç½‘ç»œæµè§ˆå™¨å‘å…¬ä¼—å‘å¸ƒï¼šå…¶ä¸­åŒ…æ‹¬ç›®å‰ä»…é€‚ç”¨äº Mac ç³»ç»Ÿçš„ Chat

</details>


### [23] [<em class="highlight">Agentic</em>21ç§è®¾è®¡æ¨¡å¼12-Exception Handling andRecovery](http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484332&idx=1&sn=9fe5dfa3bbbc82ba9d1394e73575315a&chksm=c03788e2cf94e1cf6dc497faf6289afc8e16f43022c0a1762d50cc97749f732aa55f32ccfc1f#rd)
*AI Lab Dev*

Main category: wechat.article

TL;DR: æ•°æ®å¤„ç†ä»£ç†ï¼šè´Ÿè´£å¤„ç†ä¸€æ‰¹æ–‡ä»¶çš„agentåœ¨å¤„ç†è¿‡ç¨‹ä¸­å¯èƒ½ä¼šé‡åˆ°æŸåçš„æ–‡ä»¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒåº”è¯¥è·³è¿‡è¿™äº›æŸåçš„æ–‡ä»¶ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶ï¼Œå¹¶åœ¨å¤„ç†å®ŒæˆåæŠ¥å‘Šè¢«è·³è¿‡çš„æ–‡ä»¶ï¼Œè€Œä¸ä¼šå› æ­¤åœæ­¢æ•´ä¸ªå¤„ç†æµç¨‹


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: æ•°æ®å¤„ç†ä»£ç†ï¼šè´Ÿè´£å¤„ç†ä¸€æ‰¹æ–‡ä»¶çš„agentåœ¨å¤„ç†è¿‡ç¨‹ä¸­å¯èƒ½ä¼šé‡åˆ°æŸåçš„æ–‡ä»¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒåº”è¯¥è·³è¿‡è¿™äº›æŸåçš„æ–‡ä»¶ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶ï¼Œå¹¶åœ¨å¤„ç†å®ŒæˆåæŠ¥å‘Šè¢«è·³è¿‡çš„æ–‡ä»¶ï¼Œè€Œä¸ä¼šå› æ­¤åœæ­¢æ•´ä¸ªå¤„ç†æµç¨‹

</details>


### [24] [é‡æ–°å®šä¹‰äººå·¥æ™ºèƒ½ï¼š<em class="highlight">Agentic</em> AI çš„å´›èµ·ä¸æœªæ¥](http://mp.weixin.qq.com/s?__biz=MzIzNDA0MDk0OA==&mid=2650380584&idx=1&sn=5d57e1ba5de6455c1ed9b557afea74ad&chksm=f12b5b825be5ca851b6e7799b8dece60bd8e2aa1ef408722e875459e88aa92c9c5a5aaaee8e4#rd)
*SAPHANA*

Main category: wechat.article

TL;DR: Agentic AI èƒ½ï¼šè‡ªä¸»ç†è§£ç›®æ ‡ï¼›åˆ¶å®šè®¡åˆ’ï¼›è°ƒç”¨å·¥å…·æˆ–APIï¼›ä¸äººç±»æˆ–å…¶ä»–AIåä½œï¼›ä¸æ–­åæ€å¹¶ä¼˜åŒ–ç­–ç•¥ã€‚â€œä»è¯­è¨€æ¨¡å‹åˆ°æ™ºèƒ½ä½“ï¼šAIçš„èƒ½åŠ›è·ƒè¿â€äºŒã€Agentic AI çš„æ ¸å¿ƒç»“æ„ï¼šèƒ½æ€è€ƒã€èƒ½è¡ŒåŠ¨ã€èƒ½å­¦ä¹ 


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: Agentic AI èƒ½ï¼šè‡ªä¸»ç†è§£ç›®æ ‡ï¼›åˆ¶å®šè®¡åˆ’ï¼›è°ƒç”¨å·¥å…·æˆ–APIï¼›ä¸äººç±»æˆ–å…¶ä»–AIåä½œï¼›ä¸æ–­åæ€å¹¶ä¼˜åŒ–ç­–ç•¥ã€‚â€œä»è¯­è¨€æ¨¡å‹åˆ°æ™ºèƒ½ä½“ï¼šAIçš„èƒ½åŠ›è·ƒè¿â€äºŒã€Agentic AI çš„æ ¸å¿ƒç»“æ„ï¼šèƒ½æ€è€ƒã€èƒ½è¡ŒåŠ¨ã€èƒ½å­¦ä¹ 

</details>


### [25] [ä¼ä¸šæ•°å­—åŒ–è½¬å‹ä¸­çš„<em class="highlight">å¤§æ¨¡å‹</em>ä¸æ™ºèƒ½ä½“ï¼šç ”ç©¶æ¡†æ¶ã€ç†è®ºè¿›å±•ä¸æœªæ¥æ–¹å‘](http://mp.weixin.qq.com/s?__biz=MzUzNDkwMzUzMA==&mid=2247502828&idx=1&sn=37509860f6288af76986e789511c9c86&chksm=fbd29225806f1544895d99ce00ad7d3250e7a59da6a0bf9ed4eff35485731759ba44ad3c1b92#rd)
*ç”¨æ•°è¯´*

Main category: wechat.article

TL;DR: å¤§è¯­è¨€æ¨¡å‹ ï¼ˆllmï¼‰ â€”â€” ä¼ä¸šçš„â€œè®¤çŸ¥å¼•æ“â€ï¼šllmæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé€šè¿‡æµ·é‡æ–‡æœ¬é¢„è®­ç»ƒè·å¾—çš„æ¦‚ç‡æ¨¡å‹ï¼Œå®ƒå…·å¤‡äº†é€šç”¨çš„è¯­è¨€ç†è§£ã€é€»è¾‘æ¨ç†å’Œå†…å®¹ç”Ÿæˆèƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: å¤§è¯­è¨€æ¨¡å‹ ï¼ˆllmï¼‰ â€”â€” ä¼ä¸šçš„â€œè®¤çŸ¥å¼•æ“â€ï¼šllmæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé€šè¿‡æµ·é‡æ–‡æœ¬é¢„è®­ç»ƒè·å¾—çš„æ¦‚ç‡æ¨¡å‹ï¼Œå®ƒå…·å¤‡äº†é€šç”¨çš„è¯­è¨€ç†è§£ã€é€»è¾‘æ¨ç†å’Œå†…å®¹ç”Ÿæˆèƒ½åŠ›ã€‚

</details>


### [26] [å…¨çƒé¡¶å°–<em class="highlight">å¤§æ¨¡å‹</em>Top 10æ€»ç»“](http://mp.weixin.qq.com/s?__biz=MzUwOTM2OTc4MA==&mid=2247487150&idx=1&sn=8b27b33c4ef6f5498b8caa5ee97f1adf&chksm=f8fc6df16754e16c12fc44b9967044bbbe434033a82cef2af03fb8ec614da7796999e2bff3e3#rd)
*çŸ¥æœæ—¥è®°*

Main category: wechat.article

TL;DR: å…¨çƒé¡¶å°–å¤§æ¨¡å‹top 10æ€»ç»“ å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªæ¢¯é˜Ÿï¼šâ€œé¢†å¤´ç¾Šâ€ã€â€œé¡¶çº§ç«äº‰è€…â€ å’Œ â€œç‰¹è‰²é²œæ˜çš„å¼ºè€…â€ã€‚ç¬¬ä¸€æ¢¯é˜Ÿï¼šå…¬è®¤çš„â€œé¢†å¤´ç¾Šâ€ è¿™äº›æ¨¡å‹åœ¨ç»å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­éƒ½ä½åˆ—å‰ä¸‰ï¼Œä»£è¡¨äº†å½“å‰å¤§æ¨¡å‹çš„æœ€é«˜æ°´å‡†ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: å…¨çƒé¡¶å°–å¤§æ¨¡å‹top 10æ€»ç»“ å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªæ¢¯é˜Ÿï¼šâ€œé¢†å¤´ç¾Šâ€ã€â€œé¡¶çº§ç«äº‰è€…â€ å’Œ â€œç‰¹è‰²é²œæ˜çš„å¼ºè€…â€ã€‚ç¬¬ä¸€æ¢¯é˜Ÿï¼šå…¬è®¤çš„â€œé¢†å¤´ç¾Šâ€ è¿™äº›æ¨¡å‹åœ¨ç»å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­éƒ½ä½åˆ—å‰ä¸‰ï¼Œä»£è¡¨äº†å½“å‰å¤§æ¨¡å‹çš„æœ€é«˜æ°´å‡†ã€‚

</details>


### [27] [<em class="highlight">å¤§æ¨¡å‹</em>å¼€å‘è·¯çº¿å®Œæ•´ç‰ˆï¼](http://mp.weixin.qq.com/s?__biz=Mzk2NDE4NDAwOA==&mid=2247487553&idx=1&sn=0324e731ba97d03656c40a1342721f5a&chksm=c5b3a22d59a3924e7eba0ccd6e24d7d9d9296b9a98bf9860c87e9116951a7a2330d9a3c9dafd#rd)
*å¤§æ¨¡å‹ä¹‹çª—*

Main category: wechat.article

TL;DR: #AIå·¥ä½œåŸç† #å¤§æ¨¡å‹ #RAG #ai #äººå·¥æ™ºèƒ½ #å¤§è¯­è¨€æ¨¡å‹ #æ™ºèƒ½ä½“ #å¾®è°ƒ #transformer #ç¼–ç¨‹ #LLM


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: #AIå·¥ä½œåŸç† #å¤§æ¨¡å‹ #RAG #ai #äººå·¥æ™ºèƒ½ #å¤§è¯­è¨€æ¨¡å‹ #æ™ºèƒ½ä½“ #å¾®è°ƒ #transformer #ç¼–ç¨‹ #LLM

</details>


### [28] [ã€æ”¶è—çº§ã€‘AI Agent ä»å…¥é—¨åˆ°ç²¾é€šï¼šå°ç™½ & ç¨‹åºå‘˜å¿…å­¦çš„<em class="highlight">å¤§æ¨¡å‹</em>åº”ç”¨æŒ‡å—](http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493392&idx=1&sn=65fbae1935394c7e54f035aef3c068fa&chksm=fa8b42df73858830545ff4710e845bac4132551ed244e62d0bfa8d730091cbe1129554bff3c1#rd)
*æ…•å®¹åƒè¯­*

Main category: wechat.article

TL;DR: å› æ­¤åœ¨æˆ‘çš„å®šä¹‰ä¸­ç›´æ¥çš„å¤§æ¨¡å‹å¯¹è¯æ¯”å¦‚ deepseekï¼Œchatgptï¼Œè¿™äº›å¹¶ä¸ç®—æ˜¯ AI Agentã€‚åŸºäºä¼ ç»Ÿæ„å›¾è¯†åˆ«æ¨¡å‹çš„ AI ç³»ç»Ÿï¼šåœ¨å¤§æ¨¡å‹ä¹‹å‰å°±æœ‰ä¸å°‘çš„æŠ€æœ¯å°è¯•é€šè¿‡ç®€å•æˆ–å¤æ‚çš„æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨æ¥è·¯ç”±ç”¨æˆ·çš„è¾“å…¥ï¼Œå¹¶å¯¼å‡ºåˆ°ä¸åŒçš„æ‰§è¡Œå™¨


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: å› æ­¤åœ¨æˆ‘çš„å®šä¹‰ä¸­ç›´æ¥çš„å¤§æ¨¡å‹å¯¹è¯æ¯”å¦‚ deepseekï¼Œchatgptï¼Œè¿™äº›å¹¶ä¸ç®—æ˜¯ AI Agentã€‚åŸºäºä¼ ç»Ÿæ„å›¾è¯†åˆ«æ¨¡å‹çš„ AI ç³»ç»Ÿï¼šåœ¨å¤§æ¨¡å‹ä¹‹å‰å°±æœ‰ä¸å°‘çš„æŠ€æœ¯å°è¯•é€šè¿‡ç®€å•æˆ–å¤æ‚çš„æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨æ¥è·¯ç”±ç”¨æˆ·çš„è¾“å…¥ï¼Œå¹¶å¯¼å‡ºåˆ°ä¸åŒçš„æ‰§è¡Œå™¨

</details>


### [29] [è¡Œä¸šæˆæœ | äººå·¥æ™ºèƒ½<em class="highlight">å¤§æ¨¡å‹</em>ç³»åˆ—å›½å®¶æ ‡å‡†è§£è¯»ï¼ˆä¸‰ï¼‰â€”â€”ã€Šäººå·¥æ™ºèƒ½ <em class="highlight">å¤§æ¨¡å‹</em> ç¬¬3éƒ¨åˆ†ï¼šæœåŠ¡èƒ½åŠ›æˆç†Ÿåº¦è¯„ä¼°ã€‹](http://mp.weixin.qq.com/s?__biz=Mzk0MzcxNDM2MQ==&mid=2247489185&idx=1&sn=81a774698cdb7bd009369c2213bb73d6&chksm=c282042066a150d36981d633640d8c844939454596a994684f134ca65fb44d12d1e7d547c576#rd)
*äº¤ç§‘æ™ºæ±‡*

Main category: wechat.article

TL;DR: ã€Šäººå·¥æ™ºèƒ½ å¤§æ¨¡å‹ ç¬¬2éƒ¨åˆ†ï¼šè¯„æµ‹æŒ‡æ ‡ä¸æ–¹æ³•ã€‹ç¡®ç«‹äº†äººå·¥æ™ºèƒ½å¤§æ¨¡å‹çš„è¯„æµ‹æŒ‡æ ‡ï¼Œæè¿°äº†äººå·¥æ™ºèƒ½å¤§æ¨¡å‹çš„è¯„æµ‹æ–¹æ³•ã€‚è¯¥æ ‡å‡†ä¸ºç¬¬3éƒ¨åˆ†ï¼Œç»™å‡ºäº†å¤§æ¨¡å‹æœåŠ¡èƒ½åŠ›æ¡†æ¶å’Œæˆç†Ÿåº¦ç­‰çº§ï¼Œæè¿°äº†å¤§æ¨¡å‹æœåŠ¡èƒ½åŠ›è¯„ä¼°æŒ‡æ ‡å’Œè¯„ä¼°æ–¹æ³•


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: ã€Šäººå·¥æ™ºèƒ½ å¤§æ¨¡å‹ ç¬¬2éƒ¨åˆ†ï¼šè¯„æµ‹æŒ‡æ ‡ä¸æ–¹æ³•ã€‹ç¡®ç«‹äº†äººå·¥æ™ºèƒ½å¤§æ¨¡å‹çš„è¯„æµ‹æŒ‡æ ‡ï¼Œæè¿°äº†äººå·¥æ™ºèƒ½å¤§æ¨¡å‹çš„è¯„æµ‹æ–¹æ³•ã€‚è¯¥æ ‡å‡†ä¸ºç¬¬3éƒ¨åˆ†ï¼Œç»™å‡ºäº†å¤§æ¨¡å‹æœåŠ¡èƒ½åŠ›æ¡†æ¶å’Œæˆç†Ÿåº¦ç­‰çº§ï¼Œæè¿°äº†å¤§æ¨¡å‹æœåŠ¡èƒ½åŠ›è¯„ä¼°æŒ‡æ ‡å’Œè¯„ä¼°æ–¹æ³•

</details>


### [30] [å„ç§<em class="highlight">å¤§æ¨¡å‹</em>æ¶æ„æ¯”è¾ƒ](http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498310&idx=1&sn=4c3fb3e9b0b333fe78af49c831379ac4&chksm=fd6ae44c22593b5cd1b4750feb509f0ea8eeeb1e5f4cbd7de0704413944954e4e57c12e42b26#rd)
*ä¸°å†œä¿¡æ¯*

Main category: wechat.article

TL;DR: ä¼ ç»Ÿçš„NLPåŸºå‡†æµ‹è¯•ï¼Œå¦‚MMLUï¼ˆå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼‰ï¼Œæ­£è¿…é€Ÿè¶‹äºé¥±å’Œï¼Œå¯¹äºåŒºåˆ†å‰æ²¿æ¨¡å‹çš„èƒ½åŠ›æ„ˆå‘æœ‰é™ã€‚ä¸æ­¤åŒæ—¶ï¼Œä¸€ç±»ä¸“æ³¨äºå¤æ‚æ¨ç†ï¼ˆå¦‚GPQAï¼Œ AIMEï¼‰å’Œæ™ºèƒ½ä½“æ‰§è¡Œï¼ˆå¦‚SWE - benchï¼Œ Terminal - benchï¼‰çš„æ–°åŸºå‡†ï¼Œå·²æˆä¸ºè¡¡é‡SOTA


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: ä¼ ç»Ÿçš„NLPåŸºå‡†æµ‹è¯•ï¼Œå¦‚MMLUï¼ˆå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼‰ï¼Œæ­£è¿…é€Ÿè¶‹äºé¥±å’Œï¼Œå¯¹äºåŒºåˆ†å‰æ²¿æ¨¡å‹çš„èƒ½åŠ›æ„ˆå‘æœ‰é™ã€‚ä¸æ­¤åŒæ—¶ï¼Œä¸€ç±»ä¸“æ³¨äºå¤æ‚æ¨ç†ï¼ˆå¦‚GPQAï¼Œ AIMEï¼‰å’Œæ™ºèƒ½ä½“æ‰§è¡Œï¼ˆå¦‚SWE - benchï¼Œ Terminal - benchï¼‰çš„æ–°åŸºå‡†ï¼Œå·²æˆä¸ºè¡¡é‡SOTA

</details>


### [31] [ä»èƒ½åŠ›åˆ°æ•ˆç‡ï¼Œå¤šç®¡é½ä¸‹æå‡<em class="highlight">å¤§æ¨¡å‹</em>æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ™ºèƒ½â€œå¯†åº¦â€](http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247772271&idx=1&sn=14ce17cb492fd8a91d29b699d40478a7&chksm=fa70e09e981e7c12aaa3b51aa06f384f7917d813026489a63844580cd7cf4bbca9ababc5894c#rd)
*DataFunTalk*

Main category: wechat.article

TL;DR: å¤§æ¨¡å‹çš„è¿™äº›ç‰¹ç‚¹æ¥è‡ªäºä¸¤ä¸ªå…³é”®èƒ½åŠ›ï¼šä¸€æ˜¯â€œæ¨ç†ä¸è§„åˆ’èƒ½åŠ›â€ï¼Œè®©å¤§æ¨¡å‹èƒ½å¤Ÿå¯¹å¤æ‚ä»»åŠ¡è¿›è¡Œæ¨ç†ï¼Œå°†å…¶åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤ï¼Œå¹¶è§„åˆ’è¡ŒåŠ¨é¡ºåºï¼Œè¿›ä¸€æ­¥ä½“ç°å¤§æ¨¡å‹åœ¨è°ƒç”¨å·¥å…·å’Œä¸ç¯å¢ƒäº’åŠ¨æ—¶çš„èƒ½åŠ›ï¼›


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: å¤§æ¨¡å‹çš„è¿™äº›ç‰¹ç‚¹æ¥è‡ªäºä¸¤ä¸ªå…³é”®èƒ½åŠ›ï¼šä¸€æ˜¯â€œæ¨ç†ä¸è§„åˆ’èƒ½åŠ›â€ï¼Œè®©å¤§æ¨¡å‹èƒ½å¤Ÿå¯¹å¤æ‚ä»»åŠ¡è¿›è¡Œæ¨ç†ï¼Œå°†å…¶åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤ï¼Œå¹¶è§„åˆ’è¡ŒåŠ¨é¡ºåºï¼Œè¿›ä¸€æ­¥ä½“ç°å¤§æ¨¡å‹åœ¨è°ƒç”¨å·¥å…·å’Œä¸ç¯å¢ƒäº’åŠ¨æ—¶çš„èƒ½åŠ›ï¼›

</details>


### [32] [å›½å¤–ä¸»æµäººå·¥æ™ºèƒ½<em class="highlight">å¤§æ¨¡å‹</em>è¯„æµ‹æ±‡æ€»](http://mp.weixin.qq.com/s?__biz=MjM5NDEwMzMyMA==&mid=2247483650&idx=3&sn=fbe88a2a37124280f439469e3e56e863&chksm=a72fc51f45b66eccb0fa0b0335e9cc70cd1af92af146bf49ad1a29e3fca2a23714ca6a938e17#rd)
*åŒ—æ–¹å¤§æ¨¡å‹*

Main category: wechat.article

TL;DR: æ ¹æ®2025å¹´æ–¯å¦ç¦HAIã€ŠAIæŒ‡æ•°æŠ¥å‘Šã€‹åŠè¡Œä¸šå®æµ‹æ•°æ®ï¼Œå›½å¤–ç»¼åˆèƒ½åŠ›é¢†å…ˆçš„å¤§æ¨¡å‹é›†ä¸­åœ¨é€šç”¨æ™ºèƒ½ã€é•¿æ–‡æœ¬å¤„ç†ã€å¤šæ¨¡æ€èåˆç­‰é¢†åŸŸã€‚å‰å››ç”²åˆ†åˆ«ä¸ºGPT-4 Turboã€Claude 3 Opusã€Gemini 1.5 Proã€LLaMA 3ï¼Œå„æ¨¡å‹æ ¸å¿ƒå·®å¼‚å¦‚ä¸‹ï¼š


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: æ ¹æ®2025å¹´æ–¯å¦ç¦HAIã€ŠAIæŒ‡æ•°æŠ¥å‘Šã€‹åŠè¡Œä¸šå®æµ‹æ•°æ®ï¼Œå›½å¤–ç»¼åˆèƒ½åŠ›é¢†å…ˆçš„å¤§æ¨¡å‹é›†ä¸­åœ¨é€šç”¨æ™ºèƒ½ã€é•¿æ–‡æœ¬å¤„ç†ã€å¤šæ¨¡æ€èåˆç­‰é¢†åŸŸã€‚å‰å››ç”²åˆ†åˆ«ä¸ºGPT-4 Turboã€Claude 3 Opusã€Gemini 1.5 Proã€LLaMA 3ï¼Œå„æ¨¡å‹æ ¸å¿ƒå·®å¼‚å¦‚ä¸‹ï¼š

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAIæ˜¯ä¸€ä¸ªç”¨äºæ£€æŸ¥å’Œæ‰°åŠ¨å¤šæ™ºèƒ½ä½“äº¤äº’ä¸­ä¿¡å¿µçŠ¶æ€çš„ç³»ç»Ÿçº§æ¡†æ¶ï¼Œé€šè¿‡è®°å½•å›æ”¾äº¤äº’ã€æŸ¥è¯¢æ™ºèƒ½ä½“ä¿¡å¿µå’Œæ¨ç†è¿‡ç¨‹ã€æ³¨å…¥åäº‹å®è¯æ®æ¥æµ‹è¯•ä¿¡å¿µç»“æ„å¯¹æ–°ä¿¡æ¯çš„å“åº”ã€‚


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶å¤šæ™ºèƒ½ä½“ç§‘å­¦æ¨ç†ä¸­çš„ä¿¡å¿µå½¢æˆå’Œè®¤çŸ¥å­¤å²›é—®é¢˜ï¼Œæä¾›ä¸€ç§å¯é‡ç°çš„æ–¹æ³•æ¥åˆ†ææ™ºèƒ½ä½“ä¿¡å¿µåŠ¨æ€ï¼Œè¿™åœ¨äººç±»ä¸“å®¶ä¸­éš¾ä»¥å®ç°ã€‚

Method: å¼€å‘äº†ä¸€ä¸ªè®°å½•å›æ”¾æ™ºèƒ½ä½“äº¤äº’çš„æ¡†æ¶ï¼Œæ”¯æŒå¸¦å¤–æŸ¥è¯¢æ™ºèƒ½ä½“ä¿¡å¿µå’Œæ¨ç†ï¼Œå¹¶èƒ½å¤Ÿæ³¨å…¥åäº‹å®è¯æ®ã€‚åº”ç”¨äºåŒ»ç–—æ¡ˆä¾‹æ¨¡æ‹Ÿå™¨ï¼Œå…¶ä¸­åŒ…å«å¤šæ™ºèƒ½ä½“å…±äº«è®°å¿†ï¼ˆæ—¶é—´æˆ³ç”µå­ç—…å†ï¼‰å’ŒæŒæœ‰çœŸå®å®éªŒå®¤ç»“æœçš„ç¥è°•æ™ºèƒ½ä½“ã€‚

Result: æ¨¡æ‹Ÿæ˜¾ç¤ºæ™ºèƒ½ä½“ä¿¡å¿µé€šå¸¸åæ˜ ç°å®ä¸–ç•Œå­¦ç§‘ç«‹åœºï¼ŒåŒ…æ‹¬è¿‡åº¦ä¾èµ–ç»å…¸ç ”ç©¶å’ŒæŠµæŠ—åè¯æ®ï¼Œè¿™äº›ä¿¡å¿µå¯ä»¥ä»¥äººç±»ä¸“å®¶æ— æ³•å®ç°çš„æ–¹å¼è¿›è¡Œè¿½è¸ªå’Œè´¨è¯¢ã€‚

Conclusion: Ask WhAIé€šè¿‡ä½¿è¿™äº›åŠ¨æ€å¯è§å’Œå¯æµ‹è¯•ï¼Œä¸ºç ”ç©¶å¤šæ™ºèƒ½ä½“ç§‘å­¦æ¨ç†ä¸­çš„ä¿¡å¿µå½¢æˆå’Œè®¤çŸ¥å­¤å²›æä¾›äº†ä¸€ç§å¯é‡ç°çš„æ–¹æ³•ã€‚

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [34] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆé”åº¦æ„ŸçŸ¥æœ€å°åŒ–(SAM)å’Œè½¯æ¼”å‘˜è¯„è®ºå®¶(SAC)ç®—æ³•çš„åˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºO-RANç½‘ç»œä¸­çš„èµ„æºç®¡ç†ï¼Œé€šè¿‡åŸºäºTDè¯¯å·®æ–¹å·®çš„é€‚åº”æ€§SAMæœºåˆ¶æé«˜é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¸‹ä¸€ä»£ç½‘ç»œé‡‡ç”¨O-RANæ¶æ„å®ç°åŠ¨æ€èµ„æºç®¡ç†ï¼Œä½†æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¾€å¾€ç¼ºä¹é²æ£’æ€§å’Œæ³›åŒ–æ€§ï¼Œéœ€è¦æ”¹è¿›ç®—æ³•æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚

Method: åœ¨åˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œå°†SACç®—æ³•ä¸SAMç»“åˆï¼Œå¼•å…¥åŸºäºTDè¯¯å·®æ–¹å·®çš„é€‚åº”æ€§é€‰æ‹©æ€§SAMæœºåˆ¶ï¼Œä»…å¯¹ç¯å¢ƒå¤æ‚åº¦é«˜çš„æ™ºèƒ½ä½“è¿›è¡Œæ­£åˆ™åŒ–ï¼ŒåŒæ—¶é‡‡ç”¨åŠ¨æ€Ïè°ƒåº¦æ–¹æ¡ˆä¼˜åŒ–æ¢ç´¢-åˆ©ç”¨æƒè¡¡ã€‚

Result: å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»ŸDRLæ–¹æ³•ï¼Œèµ„æºåˆ†é…æ•ˆç‡æå‡é«˜è¾¾22%ï¼Œå¹¶åœ¨å¤šæ ·åŒ–O-RANåˆ‡ç‰‡ä¸­å®ç°äº†ä¼˜è¶Šçš„QoSæ»¡æ„åº¦ã€‚

Conclusion: æ‰€æå‡ºçš„æ–¹æ³•é€šè¿‡é’ˆå¯¹æ€§æ­£åˆ™åŒ–ç­–ç•¥æœ‰æ•ˆæé«˜äº†è®­ç»ƒç¨³å®šæ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå­¦ä¹ æ•ˆç‡ï¼Œä¸ºO-RANç½‘ç»œèµ„æºç®¡ç†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $Ï$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [35] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: è¯¥è®ºæ–‡æå‡ºäº†Macro Action Quantization (MAQ)æ¡†æ¶ï¼Œé€šè¿‡å°†äººç±»æ¼”ç¤ºæç‚¼ä¸ºå®åŠ¨ä½œæ¥è®­ç»ƒç±»äººå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œåœ¨D4RL AdroitåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†äººç±»ç›¸ä¼¼åº¦ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è™½ç„¶åœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†å¾€å¾€äº§ç”Ÿä¸äººç±»è¡Œä¸ºç›¸æ¯”ä¸è‡ªç„¶çš„è¡Œä¸ºï¼Œè¿™å¼•å‘äº†å¯è§£é‡Šæ€§å’Œå¯ä¿¡èµ–æ€§çš„æ‹…å¿§ã€‚ç›®æ ‡æ˜¯è®¾è®¡èƒ½å¤Ÿäº§ç”Ÿç±»äººè¡Œä¸ºçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚

Method: å°†äººç±»ç›¸ä¼¼åº¦å»ºæ¨¡ä¸ºè½¨è¿¹ä¼˜åŒ–é—®é¢˜ï¼Œå¼•å…¥MAQæ¡†æ¶ï¼Œé€šè¿‡Vector-Quantized VAEå°†äººç±»æ¼”ç¤ºæç‚¼ä¸ºå®åŠ¨ä½œï¼Œå¹¶é‡‡ç”¨åé€€æ—¶åŸŸæ§åˆ¶ä½œä¸ºå¯å¤„ç†ä¸”é«˜æ•ˆçš„å®ç°æ–¹æ³•ã€‚

Result: åœ¨D4RL AdroitåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMAQæ˜¾è‘—æé«˜äº†äººç±»ç›¸ä¼¼åº¦ï¼Œå¢åŠ äº†è½¨è¿¹ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ç ”ç©¶ä¸­è·å¾—äº†æ‰€æœ‰RLæ™ºèƒ½ä½“ä¸­æœ€é«˜çš„ç±»äººæ’åã€‚

Conclusion: MAQå¯ä»¥è½»æ¾é›†æˆåˆ°å„ç§ç°æˆçš„RLç®—æ³•ä¸­ï¼Œä¸ºå­¦ä¹ ç±»äººRLæ™ºèƒ½ä½“å¼€è¾Ÿäº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [36] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLMæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºåŸºå› ç»„é—®ç­”ï¼Œé€šè¿‡æ¨¡å—åŒ–è®¾è®¡å’Œæ™ºèƒ½ä½“ä¸“ä¸šåŒ–ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ•ˆç‡ã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³GeneGPTä¾èµ–ä¸“æœ‰æ¨¡å‹å¸¦æ¥çš„å¯æ‰©å±•æ€§ã€è¿è¥æˆæœ¬ã€æ•°æ®éšç§å’Œæ³›åŒ–èƒ½åŠ›é—®é¢˜ï¼Œæ¢ç´¢å¼€æºæ¨¡å‹åœ¨åŸºå› ç»„é—®ç­”ä¸­çš„æ½œåŠ›ã€‚

Method: é‡‡ç”¨æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå¼•å…¥å·¥å…·è·¯ç”±ã€æŸ¥è¯¢ç”Ÿæˆå’Œå“åº”éªŒè¯çš„æ™ºèƒ½ä½“ä¸“ä¸šåŒ–ï¼Œå®ç°åè°ƒæ¨ç†å’ŒåŸºäºè§’è‰²çš„ä»»åŠ¡æ‰§è¡Œã€‚

Result: åœ¨90%ä»¥ä¸Šçš„åŸºå‡†ä»»åŠ¡ä¸­åŒ¹é…æˆ–è¶…è¶ŠGeneGPTï¼Œåœ¨Gene-Turingå’ŒGeneHopä¸Šåˆ†åˆ«è·å¾—0.849å’Œ0.830çš„å¹³å‡åˆ†ï¼Œå»¶è¿Ÿé™ä½40-50%ã€‚

Conclusion: å¼€æºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨åŸºå› ç»„é—®ç­”ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡æ•ˆç‡ã€‚

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [37] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue Oneæ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“çš„åä½œè¿­ä»£æ¥å‘ç°ã€ç”Ÿæˆå’ŒéªŒè¯é¢„æµ‹æ€§ç‰¹å¾ï¼Œåœ¨è¡¨æ ¼æ•°æ®ç‰¹å¾å·¥ç¨‹ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰åŸºäºLLMçš„è‡ªåŠ¨ç‰¹å¾æå–æ–¹æ³•å­˜åœ¨å•ä¸€ä½“æ¶æ„ã€ç®€å•é‡åŒ–åé¦ˆå’Œç¼ºä¹å¤–éƒ¨é¢†åŸŸçŸ¥è¯†æ•´åˆç­‰å±€é™æ€§ï¼Œéœ€è¦æ›´ç³»ç»ŸåŒ–çš„è§£å†³æ–¹æ¡ˆã€‚

Method: é‡‡ç”¨åˆ†æ•£å¼ä¸‰æ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆç§‘å­¦å®¶ã€æå–å™¨ã€æµ‹è¯•å™¨ï¼‰ï¼Œç»“åˆä¸°å¯Œçš„å®šæ€§åé¦ˆæœºåˆ¶å’Œ"æ³›æ»¥-ä¿®å‰ª"ç­–ç•¥ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç³»ç»Ÿæ•´åˆå¤–éƒ¨çŸ¥è¯†ã€‚

Result: åœ¨19ä¸ªåˆ†ç±»å’Œ9ä¸ªå›å½’æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶èƒ½å¤Ÿå‘ç°æ–°é¢–å¯æµ‹è¯•çš„å‡è®¾ï¼ˆå¦‚å¿ƒè‚Œæ•°æ®é›†ä¸­çš„æ–°ç”Ÿç‰©æ ‡å¿—ç‰©ï¼‰ã€‚

Conclusion: Rogue Oneä¸ä»…æä¾›ç»Ÿè®¡ä¸Šå¼ºå¤§çš„ç‰¹å¾ï¼Œè¿˜ç”Ÿæˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰ä¸”å¯è§£é‡Šçš„ç‰¹å¾ï¼Œå¯ä½œä¸ºç§‘å­¦å‘ç°çš„å·¥å…·ã€‚

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [38] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: æå‡ºäº†Octopusï¼šä¸€ç§å…·æœ‰å…­ç§èƒ½åŠ›åè°ƒçš„å¤šæ¨¡æ€ä»£ç†æ¨ç†æ–°èŒƒå¼ï¼Œèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢æ¨ç†è·¯å¾„å¹¶åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å­˜åœ¨æ¶æ„é™åˆ¶ï¼Œç¼ºä¹è‡ªä¸»æ¢ç´¢å¤šæ ·åŒ–æ¨ç†è·¯å¾„çš„èƒ½åŠ›ï¼Œæ— æ³•é€‚åº”åŠ¨æ€å˜åŒ–çš„ä»»åŠ¡éœ€æ±‚ï¼Œè€Œäººç±»å…·æœ‰äº’è¡¥çš„æ€ç»´èƒ½åŠ›ã€‚

Method: å®šä¹‰äº†å¤šæ¨¡æ€æ¨ç†çš„å…­ä¸ªæ ¸å¿ƒèƒ½åŠ›ï¼Œæ„å»ºäº†Octopus-Benchè¯„ä¼°åŸºå‡†ï¼ŒOctopusèƒ½å¤Ÿè‡ªä¸»æ¨ç†æ¢ç´¢å¹¶æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚

Result: å®éªŒç»“æœæ˜¾ç¤ºOctopusåœ¨Octopus-Benchçš„å¤§å¤šæ•°ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚

Conclusion: èƒ½åŠ›åè°ƒåœ¨ä»£ç†å¤šæ¨¡æ€æ¨ç†ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [39] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Novaæ˜¯ä¸€ä¸ªå—ã€Šæ–‡æ˜Vã€‹å¯å‘çš„ç»¼åˆæŒ‘æˆ˜ç¯å¢ƒï¼Œæ—¨åœ¨åŒæ—¶æµ‹è¯•å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ã€ä¿¡ç”¨åˆ†é…ã€è¡¨ç¤ºå­¦ä¹ ã€å·¨å¤§åŠ¨ä½œç©ºé—´ç­‰å¤šä¸ªç»å…¸æŒ‘æˆ˜ä¸­çš„ç»¼åˆèƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰RLç ”ç©¶ç¼ºä¹èƒ½å¤ŸåŒæ—¶æµ‹è¯•å¤šä¸ªäº¤äº’æŒ‘æˆ˜çš„ç¯å¢ƒï¼Œå¤§å¤šæ•°åŸºå‡†æµ‹è¯•åªæ˜¯ç®€å•èšåˆä¸ç›¸å…³çš„ä»»åŠ¡ï¼Œæ— æ³•è¯„ä¼°ä»£ç†åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚

Method: åŸºäºã€Šæ–‡æ˜Vã€‹æ¸¸æˆæ„å»ºç»¼åˆæŒ‘æˆ˜ç¯å¢ƒï¼Œè¦æ±‚ä»£ç†åœ¨å•ä¸€ç¯å¢ƒä¸­å¤„ç†å¤šä¸ªç›¸äº’å…³è”çš„RLæŒ‘æˆ˜ï¼Œè€Œéç®€å•çš„å¤šä»»åŠ¡èšåˆã€‚

Result: æå‡ºäº†Terra Novaç¯å¢ƒæ¡†æ¶ï¼Œå¼ºè°ƒå¯¹é›†æˆã€é•¿è§†é‡ç†è§£èƒ½åŠ›çš„æµ‹è¯•ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å¤šä»»åŠ¡åŸºå‡†ã€‚

Conclusion: Terra Novaä¸ºRLç ”ç©¶æä¾›äº†æ›´çœŸå®çš„ç»¼åˆæŒ‘æˆ˜æµ‹è¯•å¹³å°ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°ä»£ç†åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­çš„ç»¼åˆèƒ½åŠ›ã€‚

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [40] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: æœ¬æ–‡æå‡ºIPRï¼ˆäº¤äº’å¼ç‰©ç†æ¨ç†å™¨ï¼‰ï¼Œé€šè¿‡ä¸–ç•Œæ¨¡å‹æ¨æ¼”æ¥è¯„ä¼°å’Œå¼ºåŒ–VLMç­–ç•¥ï¼Œå¹¶å¼•å…¥PhysCodeç‰©ç†ä¸­å¿ƒåŠ¨ä½œç¼–ç ï¼Œåœ¨1000+æ¸¸æˆä¸­é¢„è®­ç»ƒåï¼Œåœ¨ä¸‰ä¸ªæ¨ç†çº§åˆ«ä¸Šè¡¨ç°ç¨³å¥ï¼Œæ•´ä½“åŒ¹é…GPT-5å¹¶åœ¨å¥½å¥‡å¿ƒçº§åˆ«è¶…è¶Šå®ƒã€‚


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶æ™ºèƒ½ä½“æ˜¯å¦èƒ½å¤Ÿåƒäººç±»ä¸€æ ·é€šè¿‡äº¤äº’å­¦ä¹ è·å¾—ç±»äººæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æ›´å¤šç»éªŒä¸­æŒç»­æ”¹è¿›ã€‚

Method: æå‡ºIPRæ¡†æ¶ï¼Œä½¿ç”¨ä¸–ç•Œæ¨¡å‹æ¨æ¼”æ¥è¯„åˆ†å’Œå¼ºåŒ–VLMç­–ç•¥ï¼Œå¼•å…¥PhysCodeç‰©ç†ä¸­å¿ƒåŠ¨ä½œç¼–ç ï¼Œåœ¨1000+å¼‚è´¨æ¸¸æˆä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚

Result: IPRåœ¨ä¸‰ä¸ªæ¨ç†çº§åˆ«ï¼ˆç”Ÿå­˜ã€å¥½å¥‡å¿ƒã€å®ç”¨æ€§ï¼‰è¡¨ç°ç¨³å¥ï¼Œæ•´ä½“æ€§èƒ½åŒ¹é…GPT-5ï¼Œåœ¨å¥½å¥‡å¿ƒçº§åˆ«è¶…è¶ŠGPT-5ï¼Œæ€§èƒ½éšè®­ç»ƒæ¸¸æˆå’Œäº¤äº’æ­¥éª¤å¢åŠ è€Œæå‡ï¼Œå¹¶èƒ½é›¶æ ·æœ¬è¿ç§»åˆ°æœªè§æ¸¸æˆã€‚

Conclusion: ç‰©ç†ä¸­å¿ƒçš„äº¤äº’æ˜¯æŒç»­æ”¹è¿›ç‰©ç†æ¨ç†çš„æœ‰æ•ˆè·¯å¾„ã€‚

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [41] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: æå‡ºäº†TIMæ¡†æ¶ï¼Œé€šè¿‡DeFiæ„å›¾åˆ†ç±»æ³•å’Œå¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿæ¥æ¨æ–­ç”¨æˆ·äº¤æ˜“æ„å›¾ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: DeFiäº¤æ˜“ä¸­ç”¨æˆ·æ„å›¾ç†è§£è‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æ·±åº¦è¯­ä¹‰æ´å¯Ÿï¼Œéœ€è¦è§£å†³å¤æ‚æ™ºèƒ½åˆçº¦äº¤äº’ã€å¤šå› ç´ å½±å“å’Œä¸é€æ˜æ—¥å¿—ç­‰é—®é¢˜ã€‚

Method: æ„å»ºåŸºäºæ‰æ ¹ç†è®ºçš„DeFiæ„å›¾åˆ†ç±»æ³•ï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼ŒåŒ…æ‹¬å…ƒçº§è§„åˆ’å™¨åŠ¨æ€åè°ƒé¢†åŸŸä¸“å®¶åˆ†è§£ä»»åŠ¡ï¼Œé—®é¢˜æ±‚è§£å™¨å¤„ç†å¤šæ¨¡æ€é“¾ä¸Š/é“¾ä¸‹æ•°æ®ï¼Œè®¤çŸ¥è¯„ä¼°å™¨å‡è½»LLMå¹»è§‰å¹¶ç¡®ä¿å¯éªŒè¯æ€§ã€‚

Result: å®éªŒè¡¨æ˜TIMæ˜¾è‘—ä¼˜äºæœºå™¨å­¦ä¹ æ¨¡å‹ã€å•ä¸€LLMå’Œå•ä¸€æ™ºèƒ½ä½“åŸºçº¿ï¼Œèƒ½å¤Ÿåˆ†ææ„å›¾æ¨ç†ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

Conclusion: è¯¥å·¥ä½œæœ‰åŠ©äºæ›´å¯é åœ°ç†è§£DeFiä¸­ç”¨æˆ·åŠ¨æœºï¼Œä¸ºå¤æ‚åŒºå—é“¾æ´»åŠ¨æä¾›æƒ…å¢ƒæ„ŸçŸ¥è§£é‡Šã€‚

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [42] [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593)
*Alexis Audran-Reiss,Jordi Armengol EstapÃ©,Karen Hambardzumyan,Amar Budhiraja,Martin Josifoski,Edan Toledo,Rishi Hazra,Despoina Magka,Michael Shvartsman,Parth Pathak,Justine T Kao,Lucia Cipolina-Kun,Bhavul Gauri,Jean-Christophe Gagnon-Audet,Emanuel Tewolde,Jenny Zhang,Taco Cohen,Yossi Adi,Tatiana Shavrina,Yoram Bachrach*

Main category: cs.AI

TL;DR: è¯¥è®ºæ–‡ç ”ç©¶äº†AIç ”ç©¶ä»£ç†ä¸­æ„æ€å¤šæ ·æ€§å¯¹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°åœ¨MLE-benchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ„æ€å¤šæ ·æ€§æ›´é«˜çš„ä»£ç†è¡¨ç°æ›´å¥½ï¼Œå¹¶é€šè¿‡æ§åˆ¶å®éªŒéªŒè¯äº†è¿™ä¸€å‘ç°ã€‚


<details>
  <summary>Details</summary>
Motivation: AIç ”ç©¶ä»£ç†æœ‰æœ›åŠ é€Ÿç§‘å­¦è¿›æ­¥ï¼Œä½†æˆåŠŸçš„å…³é”®å› ç´ å°šæœªå®Œå…¨ç†è§£ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨æ„æ€å¤šæ ·æ€§åœ¨ä»£ç†æ€§èƒ½ä¸­çš„ä½œç”¨ã€‚

Method: 1) åˆ†æMLE-benchä¸Šä¸åŒæ¨¡å‹å’Œä»£ç†æ¡†æ¶çš„è½¨è¿¹ï¼›2) è¿›è¡Œæ§åˆ¶å®éªŒè°ƒæ•´æ„æ€å¤šæ ·æ€§ç¨‹åº¦ï¼›3) ä½¿ç”¨é™¤æ ‡å‡†å¥–ç‰Œè¯„åˆ†å¤–çš„å…¶ä»–è¯„ä¼°æŒ‡æ ‡éªŒè¯ç»“æœã€‚

Result: ä¸åŒæ¨¡å‹å’Œä»£ç†æ¡†æ¶äº§ç”Ÿä¸åŒç¨‹åº¦çš„æ„æ€å¤šæ ·æ€§ï¼Œæ€§èƒ½æ›´é«˜çš„ä»£ç†å¾€å¾€å…·æœ‰æ›´é«˜çš„æ„æ€å¤šæ ·æ€§ã€‚æ§åˆ¶å®éªŒè¯å®æ›´é«˜çš„æ„æ€å¤šæ ·æ€§å¸¦æ¥æ›´å¼ºçš„æ€§èƒ½è¡¨ç°ã€‚

Conclusion: æ„æ€å¤šæ ·æ€§æ˜¯å½±å“AIç ”ç©¶ä»£ç†æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œæé«˜æ„æ€å¤šæ ·æ€§å¯ä»¥æ˜¾è‘—æ”¹å–„ä»£ç†åœ¨æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [43] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: æ¯”è¾ƒä¸¤ç§AIè¯„åˆ†æ–¹æ³•ï¼šç›´æ¥è¯„åˆ†æ³•ï¼ˆAIç›´æ¥åº”ç”¨è¯„åˆ†æ ‡å‡†ï¼‰å’Œåå‘è¯„åˆ†æ³•ï¼ˆAIå…ˆä¿®å¤é”™è¯¯å†æ¨æ–­åˆ†æ•°ï¼‰ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ç¼–ç¨‹ä½œä¸šè‡ªåŠ¨è¯„åˆ†ä¸­çš„æ•ˆæœã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³ç¼–ç¨‹ä½œä¸šäººå·¥è¯„åˆ†è€—æ—¶ä¸”ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæ¢ç´¢åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–è¯„åˆ†æ–¹æ³•ï¼Œæé«˜è¯„åˆ†çš„å®¢è§‚æ€§å’Œå¯æ‰©å±•æ€§ã€‚

Method: æå‡ºå¹¶æ¯”è¾ƒä¸¤ç§AIè¯„åˆ†æ–¹æ³•ï¼šç›´æ¥è¯„åˆ†æ³•å’Œåå‘è¯„åˆ†æ³•ã€‚ä½¿ç”¨åŸå§‹è¯„åˆ†æ ‡å‡†å’Œ10å€æ‰©å±•è¯„åˆ†æ ‡å‡†è¯„ä¼°å‡†ç¡®æ€§ï¼Œå¹¶ä¸äººç±»è¯„åˆ†è¿›è¡Œå¯¹æ¯”ã€‚è¿˜ä½¿ç”¨åˆæˆå­¦ç”Ÿä»£ç æµ‹è¯•ä¸€è‡´æ€§ã€‚

Result: ç›´æ¥æ–¹æ³•æ›´å¿«æ›´ç›´æ¥ï¼Œä½†åå‘æ–¹æ³•èƒ½æä¾›æ›´ç»†ç²’åº¦çš„è¯„ä¼°ã€‚ä¸¤ç§æ–¹æ³•éƒ½éœ€è¦ç²¾å¿ƒè®¾è®¡æç¤ºè¯ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†é…éƒ¨åˆ†åˆ†æ•°å’Œå¤„ç†é€»è¾‘é”™è¯¯æ—¶ã€‚

Conclusion: ä¸¤ç§æ–¹æ³•å„æœ‰ä¼˜åŠ¿ï¼Œéœ€è¦ç»“åˆä½¿ç”¨ã€‚æœªæ¥åº”å¼€å‘æ··åˆäººæœºè¯„åˆ†ç³»ç»Ÿï¼Œä»¥æé«˜è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹è¯„åˆ†çš„æ•ˆç‡ã€ä¸€è‡´æ€§å’Œå…¬å¹³æ€§ã€‚

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [44] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: æå‡ºäº†MermaidSeqBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMä»æ–‡æœ¬æç¤ºç”ŸæˆMermaidåºåˆ—å›¾çš„èƒ½åŠ›ï¼ŒåŒ…å«132ä¸ªäººå·¥éªŒè¯æ ·æœ¬ï¼Œé‡‡ç”¨æ··åˆæ–¹æ³•æ‰©å±•ï¼Œå¹¶ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤æ¨¡å‹è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰ç¼ºä¹ç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMåœ¨ç”Ÿæˆç»“æ„åŒ–å›¾è¡¨ï¼ˆç‰¹åˆ«æ˜¯Mermaidåºåˆ—å›¾ï¼‰æ–¹é¢çš„æ­£ç¡®æ€§ï¼Œéœ€è¦å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚

Method: é‡‡ç”¨æ··åˆæ–¹æ³•ï¼šäººå·¥æ ‡æ³¨ã€ä¸Šä¸‹æ–‡LLMæç¤ºå’ŒåŸºäºè§„åˆ™çš„å˜ä½“ç”Ÿæˆï¼Œæ„å»ºåŒ…å«132ä¸ªæ ·æœ¬çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤æ¨¡å‹è¿›è¡Œå¤šç»´åº¦è¯„ä¼°ã€‚

Result: è¯„ä¼°æ­ç¤ºäº†ä¸åŒæ¨¡å‹å’Œè¯„ä¼°æ¨¡å¼ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„èƒ½åŠ›å·®è·ï¼Œè¯æ˜äº†åŸºå‡†æµ‹è¯•çš„æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

Conclusion: è¯¥åŸºå‡†æµ‹è¯•ä¸ºç»“æ„åŒ–å›¾è¡¨ç”Ÿæˆç ”ç©¶æä¾›äº†åŸºç¡€ï¼Œå¹¶ä¸ºå¼€å‘æ›´ä¸¥æ ¼çš„ç»†ç²’åº¦è¯„ä¼°æ–¹æ³•å¥ å®šäº†åŸºç¡€ã€‚

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [45] [Effective Code Membership Inference for Code Completion Models via Adversarial Prompts](https://arxiv.org/abs/2511.15107)
*Yuan Jiang,Zehao Li,Shan Huang,Christoph Treude,Xiaohong Su,Tiantian Wang*

Main category: cs.SE

TL;DR: AdvPrompt-MIAæ˜¯ä¸€ç§é’ˆå¯¹ä»£ç è¡¥å…¨æ¨¡å‹çš„æˆå‘˜æ¨ç†æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä»£ç ç‰¹å®šçš„å¯¹æŠ—æ€§æç¤ºå’Œæ·±åº¦å­¦ä¹ ï¼Œè‡ªåŠ¨åŒºåˆ†è®­ç»ƒé›†æˆå‘˜å’Œéæˆå‘˜æ ·æœ¬ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰é»‘ç›’å’Œç°ç›’æˆå‘˜æ¨ç†æ”»å‡»ä¾èµ–æ˜‚è´µçš„ä»£ç†æ¨¡å‹æˆ–æ‰‹åŠ¨è®¾è®¡çš„å¯å‘å¼è§„åˆ™ï¼Œéš¾ä»¥æ•æ‰è¿‡å‚æ•°åŒ–ä»£ç è¯­è¨€æ¨¡å‹çš„å¤æ‚è®°å¿†æ¨¡å¼ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥è¯„ä¼°éšç§é£é™©ã€‚

Method: è®¾è®¡ä¸€ç³»åˆ—å¯¹æŠ—æ€§æç¤ºæ¥è¯±å¯¼å—å®³è€…ä»£ç æ¨¡å‹è¾“å‡ºå˜åŒ–ï¼Œé€šè¿‡æ¯”è¾ƒè¿™äº›è¾“å‡ºä¸çœŸå®è¡¥å…¨ç»“æœæ„å»ºç‰¹å¾å‘é‡ï¼Œè®­ç»ƒåˆ†ç±»å™¨è‡ªåŠ¨åŒºåˆ†æˆå‘˜å’Œéæˆå‘˜æ ·æœ¬ã€‚

Result: åœ¨Code Llama 7Bç­‰æ¨¡å‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨APPSå’ŒHumanEvalåŸºå‡†ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›åŸºçº¿ï¼ŒAUCæå‡é«˜è¾¾102%ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡å‹å’Œæ•°æ®é›†å¯è¿ç§»æ€§ã€‚

Conclusion: AdvPrompt-MIAèƒ½å¤Ÿæ•æ‰æ›´ä¸°å¯Œçš„è®°å¿†æ¨¡å¼ï¼Œå‡†ç¡®æ¨æ–­è®­ç»ƒé›†æˆå‘˜å…³ç³»ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼å’Œæ³›åŒ–èƒ½åŠ›ã€‚

Abstract: Membership inference attacks (MIAs) on code completion models offer an effective way to assess privacy risks by inferring whether a given code snippet was part of the training data. Existing black- and gray-box MIAs rely on expensive surrogate models or manually crafted heuristic rules, which limit their ability to capture the nuanced memorization patterns exhibited by over-parameterized code language models. To address these challenges, we propose AdvPrompt-MIA, a method specifically designed for code completion models, combining code-specific adversarial perturbations with deep learning. The core novelty of our method lies in designing a series of adversarial prompts that induce variations in the victim code model's output. By comparing these outputs with the ground-truth completion, we construct feature vectors to train a classifier that automatically distinguishes member from non-member samples. This design allows our method to capture richer memorization patterns and accurately infer training set membership. We conduct comprehensive evaluations on widely adopted models, such as Code Llama 7B, over the APPS and HumanEval benchmarks. The results show that our approach consistently outperforms state-of-the-art baselines, with AUC gains of up to 102%. In addition, our method exhibits strong transferability across different models and datasets, underscoring its practical utility and generalizability.

</details>


### [46] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: æå‡ºAutoSWæ„¿æ™¯â€”â€”ä¸€ç§è¿­ä»£å¼ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘èŒƒå¼ï¼Œé€šè¿‡åˆ†æ-è§„åˆ’-å®ç°-äº¤ä»˜å¾ªç¯ï¼Œè®©AIç³»ç»Ÿä½œä¸ºäººç±»ä¼™ä¼´å°†è‡ªç„¶è¯­è¨€æ„å›¾è½¬åŒ–ä¸ºå¯æ‰§è¡Œè½¯ä»¶ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰AIåœ¨è½¯ä»¶å¼€å‘ä¸­ä¸»è¦ä½œä¸ºå·¥å…·æˆ–åŠ©æ‰‹ï¼Œä»éœ€å¤§é‡äººå·¥å‚ä¸ã€‚ä½œè€…é¢„è§AIå°†å‚ä¸æ•´ä¸ªè½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼Œæ‰©å±•å…¨æ ˆå¼€å‘è¾¹ç•Œã€‚

Method: è®¾è®¡åˆ†æ-è§„åˆ’-å®ç°-äº¤ä»˜å¾ªç¯çš„AutoSWèŒƒå¼ï¼Œæ„å»ºè½»é‡çº§åŸå‹å¹¶æ‰§è¡Œä»£è¡¨æ€§æ¡ˆä¾‹ã€‚

Result: AutoSWèƒ½å¤ŸæˆåŠŸäº¤ä»˜å¯æ‰§è¡Œè½¯ä»¶ï¼Œä¸ºçœŸæ­£ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘æä¾›äº†å¯è¡Œæ–¹å‘ã€‚

Conclusion: AutoSWå±•ç¤ºäº†AIç³»ç»Ÿä½œä¸ºäººç±»ä¼™ä¼´å‚ä¸æ•´ä¸ªè½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å¯è¡Œæ€§ï¼Œæ˜¯å®ç°è½¯ä»¶è‡ªåŠ¨åŒ–å¼€å‘çš„é‡è¦è¿›å±•ã€‚

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [47] [Quantum-Guided Test Case Minimization for LLM-Based Code Generation](https://arxiv.org/abs/2511.15665)
*Huixiang Zhang,Mahzabeen Emu*

Main category: cs.SE

TL;DR: æå‡ºäº†ä¸€ä¸ªåŸºäºæµ‹è¯•é©±åŠ¨å¼€å‘(TDD)çš„æ¡†æ¶ï¼Œå°†ä»£ç è§„èŒƒè½¬åŒ–ä¸ºç»„åˆä¼˜åŒ–ä»»åŠ¡ï¼Œä½¿ç”¨é‡å­é€€ç«å™¨è§£å†³æµ‹è¯•ç”¨ä¾‹æœ€å°åŒ–é—®é¢˜ï¼Œæ˜¾è‘—æå‡ä»£ç ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç²¾ç¡®æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜æ•ˆç®€æ´ä»£ç æ˜¯è½¯ä»¶å·¥ç¨‹ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³ä»£ç è§„èŒƒåˆ°ä¼˜åŒ–ä»»åŠ¡çš„è½¬æ¢é—®é¢˜ã€‚

Method: æ¡†æ¶é¦–å…ˆæç¤ºLLMç”Ÿæˆæµ‹è¯•å¥—ä»¶ï¼Œç„¶åå°†æµ‹è¯•ç”¨ä¾‹æœ€å°åŒ–é—®é¢˜å»ºæ¨¡ä¸ºäºŒæ¬¡æ— çº¦æŸäºŒè¿›åˆ¶ä¼˜åŒ–æ¨¡å‹ï¼Œå…¼å®¹ç»å…¸æ±‚è§£å™¨å’Œé‡å­é€€ç«å™¨ç­‰æ–°å…´ç¡¬ä»¶ã€‚

Result: é‡å­é€€ç«è§£å†³æ ¸å¿ƒTCMä»»åŠ¡æ¯”æ¨¡æ‹Ÿé€€ç«å¿«16å€ï¼Œç«¯åˆ°ç«¯æ¡†æ¶å‡å°‘æ€»tokenæ¶ˆè€—36.5%ï¼Œæ˜¾è‘—æé«˜ä»£ç è´¨é‡ã€‚

Conclusion: å±•ç¤ºäº†ç”Ÿæˆå¼AIä¸ç»„åˆä¼˜åŒ–åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„å¼ºå¤§ååŒä½œç”¨ï¼Œçªæ˜¾äº†ç²¾ç¡®æ¨¡å‹åˆ¶å®šçš„é‡è¦æ€§ã€‚

Abstract: Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: æå‡ºäº†GTPOç®—æ³•ï¼Œä¸€ç§ä¸“é—¨ç”¨äºè®­ç»ƒLLMåœ¨å¤šè½®å·¥å…·é›†æˆæ¨ç†ä»»åŠ¡ä¸­çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç»†ç²’åº¦çš„å›åˆçº§å¥–åŠ±åˆ†é…ã€åŸºäºå›æŠ¥çš„ä¼˜åŠ¿ä¼°è®¡å’Œè‡ªç›‘ç£å¥–åŠ±å¡‘é€ æ¥è§£å†³ç°æœ‰æ–¹æ³•è®­ç»ƒåœæ»é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šè½®å·¥å…·é›†æˆæ¨ç†ä»»åŠ¡ä¸­é¢ä¸´è®­ç»ƒåœæ»é—®é¢˜ï¼Œå› ä¸ºè½¨è¿¹çº§å¥–åŠ±è¿‡äºç²—ç³™ï¼Œæ— æ³•ä¸ºå¤æ‚çš„å¤šè½®äº¤äº’æä¾›è¶³å¤Ÿçš„å­¦ä¹ ä¿¡å·ã€‚

Method: GTPOç®—æ³•åŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼šå›åˆçº§å¥–åŠ±åˆ†é…ã€åŸºäºå›æŠ¥çš„ä¼˜åŠ¿ä¼°è®¡ã€ä»¥åŠåˆ©ç”¨ç”Ÿæˆä»£ç è‡ªç›‘ç£ä¿¡å·æ¥ä¸°å¯Œç¨€ç–äºŒå…ƒç»“æœå¥–åŠ±çš„è‡ªç›‘ç£å¥–åŠ±å¡‘é€ ã€‚

Result: åœ¨å¤šæ ·åŒ–æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTPOå¹³å‡æ¯”GRPOæ–¹æ³•è¡¨ç°æå‡3.0%ã€‚

Conclusion: GTPOç®—æ³•æœ‰æ•ˆæ¨è¿›äº†å¤æ‚æ•°å­¦æ¨ç†åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ï¼Œä¸ºå¤šè½®å·¥å…·é›†æˆæ¨ç†ä»»åŠ¡æä¾›äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•ã€‚

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [49] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: æå‡ºåŸºäºTransformerå¼•å¯¼çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºä¼˜åŒ–eVTOLæ— äººæœºèµ·é£è½¨è¿¹ä»¥æœ€å°åŒ–èƒ½è€—ï¼Œç›¸æ¯”ä¼ ç»ŸDRLè®­ç»ƒæ•ˆç‡æå‡75%ï¼Œèƒ½è€—ä¼˜åŒ–å‡†ç¡®ç‡è¾¾åˆ°97.2%ã€‚


<details>
  <summary>Details</summary>
Motivation: eVTOLé£æœºæœ‰æœ›ç¼“è§£åŸå¸‚äº¤é€šæ‹¥å µï¼Œä½†éœ€è¦å¼€å‘æœ€ä¼˜èµ·é£è½¨è¿¹ä»¥æœ€å°åŒ–èƒ½è€—ã€‚ä¼ ç»Ÿæœ€ä¼˜æ§åˆ¶æ–¹æ³•å—é™äºé—®é¢˜ç»´åº¦å’Œå¤æ‚æ€§ï¼Œè€Œæ·±åº¦å¼ºåŒ–å­¦ä¹ é¢ä¸´è®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚

Method: æå‡ºTransformerå¼•å¯¼çš„DRLæ–¹æ³•ï¼Œä½¿ç”¨Transformeråœ¨æ¯ä¸ªæ—¶é—´æ­¥æ¢ç´¢çœŸå®çŠ¶æ€ç©ºé—´ï¼Œåº”ç”¨äºeVTOLæ— äººæœºæœ€ä¼˜èµ·é£è½¨è¿¹è®¾è®¡ï¼Œæ§åˆ¶å˜é‡ä¸ºåŠŸç‡å’Œæœºç¿¼è§’åº¦ã€‚

Result: Transformerå¼•å¯¼çš„DRLä»…éœ€4.57Ã—10^6æ—¶é—´æ­¥å®Œæˆè®­ç»ƒï¼Œæ˜¯ä¼ ç»ŸDRLæ‰€éœ€æ—¶é—´æ­¥çš„25%ï¼Œèƒ½è€—ä¼˜åŒ–å‡†ç¡®ç‡è¾¾åˆ°97.2%ï¼ˆä¼ ç»ŸDRLä¸º96.3%ï¼‰ã€‚

Conclusion: Transformerå¼•å¯¼çš„DRLåœ¨è®­ç»ƒæ•ˆç‡å’Œæœ€ä¼˜è®¾è®¡éªŒè¯æ–¹é¢å‡ä¼˜äºä¼ ç»ŸDRLæ–¹æ³•ã€‚

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [50] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: æå‡ºäº†GRPO-Verifç®—æ³•ï¼Œé€šè¿‡ç»Ÿä¸€æŸå¤±å‡½æ•°è”åˆä¼˜åŒ–è§£å†³æ–¹æ¡ˆç”Ÿæˆå’Œè‡ªæˆ‘éªŒè¯èƒ½åŠ›ï¼Œå¯è°ƒèŠ‚éªŒè¯ä¿¡å·æƒé‡


<details>
  <summary>Details</summary>
Motivation: å°½ç®¡é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œä½†LLMsä»éš¾ä»¥æŒç»­éªŒè¯è‡ªèº«æ¨ç†è½¨è¿¹ï¼Œéœ€è¦å¢å¼ºå…¶è‡ªæˆ‘éªŒè¯èƒ½åŠ›å¹¶æ¢ç´¢è¿™ç§èƒ½åŠ›æ˜¯å¦èƒ½è¿›ä¸€æ­¥æå‡æ¨ç†æ€§èƒ½

Method: GRPO-Verifç®—æ³•ï¼Œåœ¨ç»Ÿä¸€æŸå¤±å‡½æ•°ä¸­è”åˆä¼˜åŒ–è§£å†³æ–¹æ¡ˆç”Ÿæˆå’Œè‡ªæˆ‘éªŒè¯ï¼ŒåŒ…å«å¯è°ƒèŠ‚éªŒè¯ä¿¡å·æƒé‡çš„è¶…å‚æ•°

Result: å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶å¢å¼ºäº†è‡ªæˆ‘éªŒè¯èƒ½åŠ›

Conclusion: GRPO-Verifç®—æ³•æœ‰æ•ˆæå‡äº†LLMsçš„è‡ªæˆ‘éªŒè¯èƒ½åŠ›ï¼ŒåŒæ—¶ç»´æŒäº†æ¨ç†æ€§èƒ½

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [51] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVALé€šè¿‡å˜åˆ†è’¸é¦å°†æ©ç è‡ªå›å½’æ‰©æ•£æ¨¡å‹çš„æ‰©æ•£é“¾å‹ç¼©ä¸ºå•æ­¥ç”Ÿæˆï¼Œå®ç°30å€åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒæ ·æœ¬è´¨é‡ï¼Œå¹¶ä½¿å¼ºåŒ–å­¦ä¹ åè®­ç»ƒå˜å¾—å®ç”¨ã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³æ©ç è‡ªå›å½’æ‰©æ•£æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å…¶åˆ†å±‚æ¨ç†æœºåˆ¶ï¼ˆå¤–å¾ªç¯è§£æ©ç å’Œå†…å¾ªç¯æ‰©æ•£å»å™ªï¼‰ä¸ä»…å½±å“ç”Ÿæˆæ•ˆç‡ï¼Œè¿˜é˜»ç¢äº†å¼ºåŒ–å­¦ä¹ åè®­ç»ƒçš„å®é™…åº”ç”¨ã€‚

Method: æå‡ºMARVALæ¡†æ¶ï¼Œä½¿ç”¨åŸºäºåˆ†æ•°çš„å˜åˆ†ç›®æ ‡å°†æ©ç è‡ªå›å½’æ‰©æ•£æ¨¡å‹è’¸é¦ä¸ºå•æ­¥ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™çµæ´»çš„è‡ªå›å½’è§£æ©ç é¡ºåºï¼›å¹¶å¼€å‘MARVAL-RLè¿›è¡Œé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒã€‚

Result: åœ¨ImageNet 256*256ä¸Šï¼ŒMARVAL-Hugeè¾¾åˆ°FID 2.00ï¼Œç›¸æ¯”MAR-diffusionåŠ é€Ÿ30å€ä»¥ä¸Šï¼›MARVAL-RLåœ¨ImageNetæ•°æ®é›†ä¸ŠæŒç»­æå‡CLIPå’Œå›¾åƒå¥–åŠ±åˆ†æ•°ã€‚

Conclusion: MARVALä¸ºæ©ç è‡ªå›å½’æ‰©æ•£æ¨¡å‹çš„è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ æä¾›äº†é¦–ä¸ªå®ç”¨è·¯å¾„ï¼Œå®ç°äº†å¿«é€Ÿé‡‡æ ·å’Œæ›´å¥½çš„åå¥½å¯¹é½ã€‚

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [52] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: æå‡ºäº†è‡ªé€‚åº”è½¨è¿¹ç­–ç•¥ä¼˜åŒ–ï¼ˆATPOï¼‰ï¼Œé€šè¿‡åŠ¨æ€è¯†åˆ«å’Œé‡åˆ†é…æ¢¯åº¦æ›´æ–°åˆ°é«˜å½±å“åŠ›çš„å»å™ªæ­¥éª¤ï¼Œæ˜¾è‘—æå‡æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§å’Œè®­ç»ƒç¨³å®šæ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰åŸºäºè½¨è¿¹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å‡åŒ€åˆ†é…ç­–ç•¥æ¢¯åº¦åˆ°æ‰€æœ‰å»å™ªæ­¥éª¤ï¼Œéšå«å‡è®¾æ‰€æœ‰æ­¥éª¤åŒç­‰é‡è¦ã€‚æœ¬æ–‡æŒ‘æˆ˜è¿™ä¸€å‡è®¾ï¼Œå‘ç°è½¨è¿¹ä¸­å­˜åœ¨ç»“æ„åŒ–çš„"æ··æ·†åŒº"â€”â€”ä¸ç¡®å®šæ€§å’Œä¸ç¨³å®šçš„ç¬æ—¶å³°å€¼ï¼Œè¿™äº›åŒºåŸŸå¼ºçƒˆé¢„æµ‹æœ€ç»ˆæˆåŠŸæˆ–å¤±è´¥ã€‚

Method: ä½¿ç”¨åŸºäºç†µçš„ä¸ç¡®å®šæ€§ã€ç½®ä¿¡åº¦-è¾¹ç¼˜ä¸ç¡®å®šæ€§å’Œç†µå˜åŒ–ç‡ç­‰æ­¥éª¤çº§æŒ‡æ ‡åˆ†æè½¨è¿¹ï¼Œè¯†åˆ«é«˜å½±å“åŠ›æ­¥éª¤ã€‚æå‡ºATPOæ–¹æ³•ï¼Œé‡‡ç”¨æ··åˆRoEC+CMè§„åˆ™åŠ¨æ€é‡åˆ†é…æ¢¯åº¦æ›´æ–°åˆ°è¿™äº›å…³é”®æ­¥éª¤ï¼Œè€Œä¸æ”¹å˜RLç›®æ ‡ã€å¥–åŠ±æˆ–è®¡ç®—é¢„ç®—ã€‚

Result: ATPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨ç†å‡†ç¡®æ€§å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œè¯å®åˆ©ç”¨è½¨è¿¹åŠ¨æ€æ˜¯æ¨è¿›dLLM RLçš„å…³é”®ã€‚

Conclusion: é€šè¿‡åŠ¨æ€è¯†åˆ«å’Œé‡åˆ†é…æ¢¯åº¦æ›´æ–°åˆ°é«˜å½±å“åŠ›æ­¥éª¤ï¼ŒATPOæ–¹æ³•æœ‰æ•ˆæå‡äº†æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ€§èƒ½ï¼Œè¯æ˜äº†è½¨è¿¹åŠ¨æ€åˆ†æçš„é‡è¦æ€§ã€‚

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [53] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: æå‡ºEntroPICæ–¹æ³•ï¼Œé€šè¿‡æ¯”ä¾‹-ç§¯åˆ†æ§åˆ¶åŠ¨æ€è°ƒæ•´æ­£è´Ÿæ ·æœ¬çš„æŸå¤±ç³»æ•°ï¼Œç¨³å®šå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„ç†µå€¼ï¼Œé˜²æ­¢æ¨¡å‹é™·å…¥æ¬¡ä¼˜è¡Œä¸ºã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»´æŒé€‚å½“çš„ç†µæ°´å¹³ï¼Œå› ä¸ºæ­£è´Ÿæ ·æœ¬ä»¥ä¸åŒæ–¹å¼å½±å“ç†µå€¼å˜åŒ–ï¼Œå¯¼è‡´æ¢ç´¢ä¸ç¨³å®šå’Œè¿‡æ—©æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ã€‚

Method: åŸºäºæ¯”ä¾‹-ç§¯åˆ†æ§åˆ¶çš„è‡ªé€‚åº”æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´æ­£è´Ÿæ ·æœ¬çš„æŸå¤±ç³»æ•°ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç†µå€¼ã€‚

Result: å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½æˆåŠŸç»´æŒæœŸæœ›çš„ç†µæ°´å¹³ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹ç¨³å®šä¸”æœ€ä¼˜çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

Conclusion: EntroPICæ–¹æ³•èƒ½æœ‰æ•ˆæ§åˆ¶ç†µå€¼ï¼Œç¡®ä¿å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿æœŸè®­ç»ƒä¸­ä¿æŒç¨³å®šæ¢ç´¢å’ŒæŒç»­è¿›æ­¥ã€‚

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [54] [Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges](https://arxiv.org/abs/2511.15652)
*Kim N. Nolle,Ivana Dusparic,Rhodri Cusack,Vinny Cahill*

Main category: cs.LG

TL;DR: æœ¬æ–‡é€šè¿‡è‡ªåŠ¨é©¾é©¶ç¯å¢ƒä¸­çš„å®éªŒï¼Œæ­ç¤ºäº†æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆCRLï¼‰é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¯å¢ƒæŠ½è±¡ã€è¶…å‚æ•°æ•æ„Ÿæ€§ã€ç¾éš¾æ€§é—å¿˜å’Œç¥ç»ç½‘ç»œå®¹é‡åˆ©ç”¨é—®é¢˜ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚


<details>
  <summary>Details</summary>
Motivation: æŒç»­å­¦ä¹ åœ¨éå¹³ç¨³ç¯å¢ƒï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ï¼‰ä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œä½†ç›®å‰å°†æŒç»­å­¦ä¹ æˆåŠŸåº”ç”¨äºå¼ºåŒ–å­¦ä¹ ä»æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ï¼Œéœ€è¦æ¢ç´¢CRLçš„å®é™…æŒ‘æˆ˜ã€‚

Method: åœ¨è‡ªåŠ¨é©¾é©¶ç¯å¢ƒä¸­ï¼Œä½¿ç”¨PPOç®—æ³•è®©æ™ºèƒ½ä½“ä¾æ¬¡å­¦ä¹ å››ç§ä¸åŒè§’åº¦çš„åœè½¦åœºæ™¯ï¼Œæ„å»ºæŒç»­å­¦ä¹ ç¯å¢ƒè¿›è¡Œå®éªŒåˆ†æã€‚

Result: å®éªŒæ­ç¤ºäº†CRLçš„å››ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¯å¢ƒæŠ½è±¡è¡¨ç¤ºå›°éš¾ã€è¶…å‚æ•°è¿‡åº¦æ•æ„Ÿã€ç¾éš¾æ€§é—å¿˜é—®é¢˜ä»¥åŠç¥ç»ç½‘ç»œå®¹é‡åˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚

Conclusion: CRLé¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³ç¥ç»ç½‘ç»œåœ¨æŒç»­å­¦ä¹ ä¸­çš„é€‚ç”¨æ€§é—®é¢˜ï¼Œå¹¶å¼€å±•è®¡ç®—æœºç§‘å­¦ä¸ç¥ç»ç§‘å­¦ç­‰è·¨å­¦ç§‘ç ”ç©¶ã€‚

Abstract: Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.

</details>


### [55] [DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models](https://arxiv.org/abs/2511.15669)
*Cheng Yin,Yankai Lin,Wang Xu,Sikyuen Tam,Xiangrui Zeng,Zhiyuan Liu,Zhouping Yin*

Main category: cs.LG

TL;DR: DeepThinkVLAé€šè¿‡æ··åˆæ³¨æ„åŠ›è§£ç å™¨å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥è§£å†³VLAæ¨¡å‹ä¸­æ€ç»´ä¸åŠ¨ä½œçš„å†²çªï¼Œåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°97.0%çš„æˆåŠŸç‡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰VLAæ¨¡å‹ä½¿ç”¨å•ä¸€è‡ªå›å½’è§£ç å™¨åŒæ—¶å¤„ç†é¡ºåºæ¨ç†å’Œé«˜ç»´å¹¶è¡ŒåŠ¨ä½œï¼Œå¯¼è‡´è¿åŠ¨æ§åˆ¶æ€§èƒ½ä¸‹é™å’Œæ€ç»´-åŠ¨ä½œå› æœè”ç³»è–„å¼±ã€‚

Method: é‡‡ç”¨æ··åˆæ³¨æ„åŠ›è§£ç å™¨ï¼šç”¨å› æœæ³¨æ„åŠ›ç”Ÿæˆé¡ºåºCoTæ¨ç†ï¼Œç„¶ååˆ‡æ¢åˆ°åŒå‘æ³¨æ„åŠ›å¹¶è¡Œè§£ç åŠ¨ä½œå‘é‡ï¼›é…åˆä¸¤é˜¶æ®µè®­ç»ƒï¼šå…ˆç”¨SFTæ•™æˆåŸºç¡€æ¨ç†ï¼Œå†ç”¨RLé€šè¿‡ä»»åŠ¡æˆåŠŸå¥–åŠ±å¯¹é½æ¨ç†-åŠ¨ä½œåºåˆ—ã€‚

Result: åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°97.0%çš„æˆåŠŸç‡ï¼Œæ··åˆæ¶æ„æ¯”æ ‡å‡†è§£ç å™¨æå‡15.5%ï¼ŒRLé˜¶æ®µé¢å¤–æä¾›2%çš„æ€§èƒ½æå‡ã€‚

Conclusion: DeepThinkVLAé€šè¿‡æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„ååŒè®¾è®¡ï¼Œæœ‰æ•ˆè§£å†³äº†VLAæ¨¡å‹ä¸­æ€ç»´ä¸åŠ¨ä½œçš„å†²çªï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

Abstract: Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.

</details>


### [56] [The Impact of Quantization on Large Reasoning Model Reinforcement Learning](https://arxiv.org/abs/2511.15694)
*Medha Kumar,Zifei Xu,Xin Wang,Tristan Webb*

Main category: cs.LG

TL;DR: é‡åŒ–æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯¹å¤§æ¨ç†æ¨¡å‹æœ‰å®³ï¼Œè€Œè®­ç»ƒåé‡åŒ–å’ŒQLoRAæ–¹æ³•è¡¨ç°æ›´å¥½


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶é‡åŒ–å¦‚ä½•å½±å“å¤§æ¨ç†æ¨¡å‹ä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼Œå› ä¸ºè™½ç„¶é‡åŒ–åœ¨å¾®è°ƒèƒŒæ™¯ä¸‹å·²æœ‰ç ”ç©¶ï¼Œä½†åœ¨å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­çš„å½±å“å°šä¸æ˜ç¡®

Method: è¿›è¡Œç³»ç»Ÿå®éªŒï¼Œæ¯”è¾ƒè®­ç»ƒåé‡åŒ–ä¸é‡åŒ–æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨ç†æ€§èƒ½

Result: å‘ç°è®­ç»ƒåé‡åŒ–æ¨¡å‹ä¸é‡åŒ–æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ¨ç†æ€§èƒ½å·®è·ï¼Œé‡åŒ–æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯¹å­¦ä¹ è¿‡ç¨‹äº§ç”Ÿè´Ÿé¢å½±å“

Conclusion: é‡åŒ–æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯¹å¤§æ¨ç†æ¨¡å‹æœ‰å®³ï¼Œè€Œè®­ç»ƒåé‡åŒ–å’ŒQLoRAæ–¹æ³•èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½

Abstract: Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.

</details>
