<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [tldr.article](#tldr.article) [Total: 11]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.AI](#cs.AI) [Total: 49]
- [cs.LG](#cs.LG) [Total: 28]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [On Calibration of Large Language Models: From Response To Capability](https://arxiv.org/abs/2602.13540)
*Sin-Han Yang,Cheng-Kuang Wu,Chieh-Yen Lin,Yun-Nung Chen,Hung-yi Lee,Shao-Hua Sun*

Main category: cs.CL

TL;DR: 该论文提出能力校准（capability calibration）的概念，与传统响应级校准不同，它关注模型在查询上的期望准确率，解决了LLM解码随机性导致的单响应正确性与实际能力不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM校准工作主要关注响应级置信度，即估计单个生成输出的正确性。然而，这与许多实际场景不匹配，因为实际问题通常是"模型有多大可能解决一个查询"。这种不匹配源于现代LLM解码的随机性，单响应正确性无法反映底层模型能力。

Method: 引入能力校准概念，针对模型在查询上的期望准确率进行校准。正式区分能力校准与响应校准，建立实证评估框架，研究一系列置信度估计方法。

Result: 能力校准置信度在pass@k预测和推理预算分配方面表现更好，为多样化应用奠定了基础。理论与实证均表明能力校准与响应校准存在差异。

Conclusion: 能力校准比传统响应级校准更符合实际应用需求，能够更好地反映模型解决查询的整体能力，在多种应用场景中具有潜力。

Abstract: Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the correctness of a single generated output. However, this formulation is misaligned with many practical settings where the central question is how likely a model is to solve a query overall. We show that this mismatch results from the stochastic nature of modern LLM decoding, under which single-response correctness fails to reflect underlying model capability. To address this issue, we introduce capability calibration, which targets the model's expected accuracy on a query. We formally distinguish capability calibration from response calibration and show that the two differ both theoretically and empirically. We establish an empirical evaluation setup and study a range of confidence estimation methods. Our results demonstrate that capability-calibrated confidence improves pass@$k$ prediction and inference budget allocation, establishing a foundation with potential for diverse applications.

</details>


### [2] [Small Reward Models via Backward Inference](https://arxiv.org/abs/2602.13551)
*Yike Wang,Faeze Brahman,Shangbin Feng,Teng Xiao,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: FLIP是一种无参考、无评分标准的奖励建模方法，通过反向推理从响应推断指令，并将推断指令与原始指令的相似度作为奖励信号，在多个领域显著优于LLM-as-a-Judge基线。


<details>
  <summary>Details</summary>
Motivation: 当前奖励建模主要依赖大模型的强推理能力（LLM-as-a-Judge范式），或需要参考响应和明确评分标准，这限制了灵活性和可访问性。需要一种更灵活、无需参考的奖励建模方法。

Method: 提出FLIP方法，通过反向推理进行奖励建模：从给定的响应推断最可能产生该响应的指令，然后将推断指令与原始指令的相似度作为奖励信号。该方法无需参考响应或评分标准。

Result: 在四个领域使用13个小语言模型的评估显示，FLIP平均优于LLM-as-a-Judge基线79.6%。通过并行采样和GRPO训练，FLIP显著提升了下游任务性能。特别适用于长输出，对常见的奖励攻击具有鲁棒性。

Conclusion: FLIP通过利用验证-生成差距，在规模缩减的情况下实现了可靠的奖励建模，为奖励建模提供了一种灵活、有效的替代方案。

Abstract: Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.

</details>


### [3] [Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment](https://arxiv.org/abs/2602.13575)
*Jing Zhao,Ting Zhen,Junwei bao,Hongfei Jiang,Yang song*

Main category: cs.CL

TL;DR: Elo-Evolve：一个基于动态多智能体竞争的LLM对齐框架，通过成对竞争和自适应对手池实现更稳定高效的模型对齐


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法依赖将大量人类偏好数据压缩为静态绝对奖励函数，导致数据稀缺、噪声敏感和训练不稳定。需要一种更鲁棒的对齐方法。

Method: 引入Elo-Evolve框架，将对齐重新定义为自适应对手池中的动态多智能体竞争。关键创新：1）消除Bradley-Terry模型依赖，直接从成对竞争的二元输赢结果学习；2）实现Elo编排的对手选择，通过温度控制采样提供自动课程学习。

Result: 在Qwen2.5-7B模型上验证，使用Qwen2.5-14B、Qwen2.5-32B和Qwen3-8B作为对手。结果显示性能层次：基于点的方法 < 静态成对训练 < Elo-Evolve，在Alpaca Eval 2.0和MT-Bench上验证了成对比较和动态对手选择的渐进优势。

Conclusion: Elo-Evolve框架通过动态多智能体竞争和自适应对手选择，显著提高了LLM对齐的效率和稳定性，实现了4.5倍的噪声减少和更优的样本复杂度。

Abstract: Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods < static pairwise training < Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.

</details>


### [4] [On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis](https://arxiv.org/abs/2602.13713)
*Maciej Uberna,Michał Wawer,Jarosław A. Chudziak,Marcin Koszowy*

Main category: cs.CL

TL;DR: 本文提出一个多智能体框架，通过融入论证理论知识来识别话语中的重述策略功能，相比零样本基线在政治辩论数据集上实现近30%的宏F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在识别话语重述时只能检测表面相似性，无法捕捉重述的语用功能（如修辞作用）。需要将理论知识与计算模型结合，超越简单的复述检测，实现功能感知的论证话语分析。

Method: 1. 构建包含政治辩论标注的数据集，定义五种重述功能：弱化、强化、具体化、泛化、其他（D-I-S-G-O）
2. 设计并行多智能体系统：基于RAG融入论证理论的智能体 vs. 零样本基线智能体
3. 使用比较框架量化理论知识的效益

Result: RAG增强智能体在所有指标上显著优于基线，特别是在检测强化和泛化上下文方面优势明显。整体宏F1分数提升近30%，证明理论基础的融入对功能识别至关重要。

Conclusion: 理论根基不仅是有益的，而且是实现超越简单复述检测、迈向功能感知论证话语分析的必要条件。该多智能体架构为开发可扩展、理论知情的计算工具迈出重要一步。

Abstract: Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role within rhetorical discourse. This paper presents a comparative multi-agent framework designed to quantify the benefits of incorporating explicit theoretical knowledge for this task. We utilise an dataset of annotated political debates to establish a new standard encompassing four distinct rephrase functions: Deintensification, Intensification, Specification, Generalisation, and Other, which covers all remaining types (D-I-S-G-O). We then evaluate two parallel LLM-based agent systems: one enhanced by argumentation theory via Retrieval-Augmented Generation (RAG), and an identical zero-shot baseline. The results reveal a clear performance gap: the RAG-enhanced agents substantially outperform the baseline across the board, with particularly strong advantages in detecting Intensification and Generalisation context, yielding an overall Macro F1-score improvement of nearly 30\%. Our findings provide evidence that theoretical grounding is not only beneficial but essential for advancing beyond mere paraphrase detection towards function-aware analysis of argumentative discourse. This comparative multi-agent architecture represents a step towards scalable, theoretically informed computational tools capable of identifying rhetorical strategies in contemporary discourse.

</details>


### [5] [PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training](https://arxiv.org/abs/2602.13840)
*Yuhan Cheng,Hancheng Ye,Hai Helen Li,Jingwei Sun,Yiran Chen*

Main category: cs.CL

TL;DR: PrivAct是一个上下文隐私感知的多智能体学习框架，通过将隐私偏好嵌入到每个智能体中，直接在模型生成行为中内化上下文隐私保护，实现隐私合规的智能体行动。


<details>
  <summary>Details</summary>
Motivation: LLM智能体越来越多地部署在涉及敏感、上下文依赖信息的个性化任务中，由于上下文隐私的隐含性，智能体行动中可能出现隐私侵犯。现有方法依赖外部、推理时干预，这些方法脆弱、场景特定，并可能扩大隐私攻击面。

Method: 提出PrivAct框架，将上下文隐私保护直接内化到模型生成行为中，通过将隐私偏好嵌入到每个智能体中，实现系统范围的上下文完整性，并在隐私和帮助性之间取得更好的平衡。

Result: 在多个LLM骨干网络和基准测试上的实验表明，上下文隐私保护持续改进，泄漏率降低高达12.32%，同时保持相当的帮助性，并在不同多智能体拓扑结构中展示零样本泛化能力和鲁棒性。

Conclusion: PrivAct通过将隐私保护内化到智能体生成行为中，提供了一种更有效、更鲁棒的上下文隐私保护方法，优于现有的外部干预方法。

Abstract: Large language model (LLM) agents are increasingly deployed in personalized tasks involving sensitive, context-dependent information, where privacy violations may arise in agents' action due to the implicitness of contextual privacy. Existing approaches rely on external, inference-time interventions which are brittle, scenario-specific, and may expand the privacy attack surface. We propose PrivAct, a contextual privacy-aware multi-agent learning framework that internalizes contextual privacy preservation directly into models' generation behavior for privacy-compliant agentic actions. By embedding privacy preferences into each agent, PrivAct enhances system-wide contextual integrity while achieving a more favorable privacy-helpfulness tradeoff. Experiments across multiple LLM backbones and benchmarks demonstrate consistent improvements in contextual privacy preservation, reducing leakage rates by up to 12.32% while maintaining comparable helpfulness, as well as zero-shot generalization and robustness across diverse multi-agent topologies. Code is available at https://github.com/chengyh23/PrivAct.

</details>


### [6] [Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition](https://arxiv.org/abs/2602.14955)
*Varun Nathan,Shreyas Guha,Ayush Kumar*

Main category: cs.CL

TL;DR: 提出了一个面向联络中心工具感知规划生成的领域基准框架，用于评估LLM将业务查询分解为结构化/非结构化工具可执行步骤的能力，发现LLM在复杂查询和多步骤规划上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 联络中心需要将业务洞察查询分解为可执行的工具步骤（如Text2SQL、RAG等），但现有方法缺乏系统评估LLM在工具感知规划能力方面的基准，特别是在处理复杂查询和并行执行依赖关系方面存在挑战。

Method: 提出三方面贡献：1）基于参考的规划评估框架，包含七维度指标评估和一次性评估两种模式；2）数据整理方法，通过评估器->优化器循环迭代优化规划质量；3）对14个不同规模和家族的LLM进行大规模研究，评估其在有无规划谱系提示下的规划能力。

Result: LLM在复合查询和超过4步的规划上表现不佳（通常需要5-15步）；最佳总指标得分84.8%（Claude-3-7-Sonnet），最强一次性匹配率仅49.75%（o3-mini）。规划谱系整体收益有限，但对部分顶级模型有帮助，并提高了步骤可执行性。

Conclusion: 研究揭示了LLM在工具理解方面存在持续差距，特别是在工具提示对齐和工具使用完整性方面，较短的简单规划明显更容易。该框架为评估和改进联络中心数据分析查询的代理规划提供了可复现路径。

Abstract: We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage. Empirically, LLMs struggle on compound queries and on plans exceeding 4 steps (typically 5-15); the best total metric score reaches 84.8% (Claude-3-7-Sonnet), while the strongest one-shot match rate at the "A+" tier (Extremely Good, Very Good) is only 49.75% (o3-mini). Plan lineage yields mixed gains overall but benefits several top models and improves step executability for many. Our results highlight persistent gaps in tool-understanding, especially in tool-prompt alignment and tool-usage completeness, and show that shorter, simpler plans are markedly easier. The framework and findings provide a reproducible path for assessing and improving agentic planning with tools for answering data-analysis queries in contact-center settings.

</details>


### [7] [LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation](https://arxiv.org/abs/2602.14054)
*Jizheng Chen,Weiming Zhang,Xinyi Dai,Weiwen Liu,Kounianhua Du,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: LogitsCoder：通过轻量级logit级控制机制增强代码生成的思维链推理，解决现有方法中的"欠思考"和"过思考"问题，实现更高效高质量的代码生成。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间扩展方法（包括结构化树搜索）在探索推理路径时面临两大挑战：1）"欠思考" - 推理链过于浅显，无法捕捉问题的全部复杂性；2）"过思考" - 推理过于冗长，导致效率低下和计算成本增加。

Method: LogitsCoder框架通过轻量级logit级控制机制增强思维链推理：1）Logits Preference Decoding引导token选择朝向统计偏好的模式；2）Logits Rank Based Path Selection选择和聚合多样化的推理路径；3）Thoughts Aggregation整合推理链，实现深度与效率的平衡。

Result: 大量实验表明，LogitsCoder能够产生更高效、更高质量的推理链，在代码生成性能上优于基线方法。

Conclusion: LogitsCoder通过logit级控制机制有效解决了代码生成中的"欠思考"和"过思考"问题，实现了更连贯有效的推理链，提升了代码生成的质量和效率。

Abstract: Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend to be shallow and fail to capture the full complexity of problems; and (2) overthinking, where overly verbose reasoning leads to inefficiency and increased computational costs. To address these issues, we propose LogitsCoder, a novel framework that enhances chain-of-thought reasoning through lightweight, logit-level control mechanisms for code generation. LogitsCoder iteratively generates and refines reasoning steps by first steering token selection toward statistically preferred patterns via Logits Preference Decoding, then selecting and aggregating diverse reasoning paths using Logits Rank Based Path Selection and Thoughts Aggregation. This results in coherent and effective reasoning chains that balance depth and efficiency. Extensive experiments demonstrate that LogitsCoder produces more efficient and higher-quality reasoning chains, leading to superior code generation performance compared to baseline methods.

</details>


### [8] [Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric](https://arxiv.org/abs/2602.14069)
*Ruipeng Jia,Yunyi Yang,Yuxin Wu,Yongbo Gai,Siyuan Tao,Mengyu Zhou,Jianhe Lin,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.CL

TL;DR: OpenRS是一个基于明确原则的LLM-as-a-Judge框架，通过可检查的推理过程而非单一标量奖励来解决开放对齐中的泛化问题，使用自适应元准则和可验证准则来提供约束和奖励。


<details>
  <summary>Details</summary>
Motivation: 传统标量奖励模型将多维人类偏好压缩为单一不透明分数，造成信息瓶颈，导致开放对齐中的脆弱性和奖励黑客问题。作者认为稳健对齐本质上是一个原则泛化问题，奖励不应是内部化的学习函数，而应是基于可检查原则的显式推理过程。

Method: 提出Open Rubric System (OpenRS)，包含：1) Pairwise Adaptive Meta-Rubrics (PAMR)：根据候选回答的语义差异动态实例化准则；2) Pointwise Verifiable Rubrics (PVRs)：提供硬约束护栏和可验证奖励；3) 两级元准则精炼管道：自动化进化精炼通用原则，人机协同精炼领域原则；4) 作为奖励监督应用于成对RL训练。

Result: OpenRS避免了点加权标量化，通过准则级成对比较和外部偏好聚合，提高了开放环境下的判别能力，同时保持原则的一致性和可编辑性。

Conclusion: OpenRS将奖励从内部化函数转变为基于可检查原则的显式推理过程，为解决非可验证任务的稳健对齐问题提供了新框架，通过自适应准则和可验证组件实现了更好的泛化和抗黑客能力。

Abstract: Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.

</details>


### [9] [Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality](https://arxiv.org/abs/2602.14080)
*Nitay Calderon,Eyal Ben-David,Zorik Gekhman,Eran Ofek,Gal Yona*

Main category: cs.CL

TL;DR: 该论文提出了一种区分LLM事实性错误来源的框架：将错误分为知识缺失（空书架）和知识访问失败（丢失钥匙），并引入WikiProfile基准来评估事实的编码和可访问性。


<details>
  <summary>Details</summary>
Motivation: 现有的事实性评估方法将所有错误同等对待，无法区分错误是源于模型缺乏相关知识（空书架问题）还是无法访问已编码的知识（丢失钥匙问题）。这限制了我们对LLM知识能力的深入理解。

Method: 提出行为框架，在事实层面而非问题层面分析知识，将每个事实分类为：是否编码、可访问性（无法回忆、可直接回忆、需要推理计算）。构建WikiProfile基准，使用基于网络搜索的提示LLM自动构建，包含400万条来自13个LLM的响应。

Result: 前沿模型（GPT-5和Gemini-3）在基准上编码了95-98%的事实，表明编码接近饱和。但回忆仍是主要瓶颈：许多之前归因于知识缺失的错误实际上源于访问失败。这些失败具有系统性，尤其影响长尾事实和反向问题。思考（推理计算）能显著改善回忆。

Conclusion: 未来提升LLM事实性可能更依赖于改进模型如何利用已编码知识的方法，而非单纯扩大规模。访问机制和推理能力是关键改进方向。

Abstract: Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.

</details>


### [10] [A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing](https://arxiv.org/abs/2602.14158)
*Naeimeh Nourmohammadi,Md Meem Hossain,The Anh Han,Safina Showkat Ara,Zia Ush Shamszaman*

Main category: cs.CL

TL;DR: 提出一个多智能体医疗问答框架，结合LLM微调、证据检索、不确定性估计和偏见检测，以提高医疗问答的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗问答中表现出潜力，但临床应用受到验证能力弱、证据基础不足和置信度信号不可靠的限制。

Method: 分两阶段：1) 在MedQuAD医疗QA数据上微调三种LLM家族(GPT、LLaMA、DeepSeek R1)；2) 实现模块化多智能体管道，包括临床推理智能体、证据检索智能体、精炼智能体，以及安全机制(蒙特卡洛dropout、困惑度不确定性评分、偏见检测)。

Result: DeepSeek R1表现最佳(ROUGE-1 0.536, ROUGE-2 0.226, BLEU 0.098)，完整系统达到87%准确率，相关性约0.80，证据增强降低不确定性(困惑度4.13)，平均端到端延迟36.5秒。

Conclusion: 智能体专业化和验证层可以缓解单一模型的关键限制，为基于证据和偏见感知的医疗AI提供实用、可扩展的设计。

Abstract: Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.

</details>


### [11] [AD-Bench: A Real-World, Trajectory-Aware Advertising Analytics Benchmark for LLM Agents](https://arxiv.org/abs/2602.14257)
*Lingxiang Hu,Yiding Sun,Tianle Xia,Wenwei Li,Ming Xu,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: AD-Bench是一个针对广告营销领域的LLM智能体评估基准，基于真实业务需求构建，包含三个难度等级，实验显示现有模型在复杂场景下仍有显著能力差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估基准主要局限于理想化模拟环境，无法满足广告营销等专业领域的实际需求。这些领域的任务更复杂，通常需要与专业营销工具进行多轮交互。

Method: 基于真实用户营销分析需求构建基准，由领域专家提供可验证的参考答案和对应的工具调用轨迹。将请求分为三个难度等级（L1-L3），评估智能体在多轮、多工具协作下的能力。

Result: 在AD-Bench上，Gemini-3-Pro在整体上达到Pass@1=68.0%和Pass@3=83.0%，但在L3难度下性能显著下降至Pass@1=49.4%和Pass@3=62.1%，轨迹覆盖率为70.1%，表明即使最先进的模型在复杂广告营销分析场景中仍有显著能力差距。

Conclusion: AD-Bench为评估和改进广告营销智能体提供了现实的基准，揭示了当前模型在复杂专业场景中的局限性，并为未来研究提供了方向。

Abstract: While Large Language Model (LLM) agents have achieved remarkable progress in complex reasoning tasks, evaluating their performance in real-world environments has become a critical problem. Current benchmarks, however, are largely restricted to idealized simulations, failing to address the practical demands of specialized domains like advertising and marketing analytics. In these fields, tasks are inherently more complex, often requiring multi-round interaction with professional marketing tools. To address this gap, we propose AD-Bench, a benchmark designed based on real-world business requirements of advertising and marketing platforms. AD-Bench is constructed from real user marketing analysis requests, with domain experts providing verifiable reference answers and corresponding reference tool-call trajectories. The benchmark categorizes requests into three difficulty levels (L1-L3) to evaluate agents' capabilities under multi-round, multi-tool collaboration. Experiments show that on AD-Bench, Gemini-3-Pro achieves Pass@1 = 68.0% and Pass@3 = 83.0%, but performance drops significantly on L3 to Pass@1 = 49.4% and Pass@3 = 62.1%, with a trajectory coverage of 70.1%, indicating that even state-of-the-art models still exhibit substantial capability gaps in complex advertising and marketing analysis scenarios. AD-Bench provides a realistic benchmark for evaluating and improving advertising marketing agents, the leaderboard and code can be found at https://github.com/Emanual20/adbench-leaderboard.

</details>


### [12] [Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook](https://arxiv.org/abs/2602.14299)
*Ming Li,Xirui Li,Tianyi Zhou*

Main category: cs.CL

TL;DR: 该研究首次对AI智能体社会进行大规模系统诊断，发现虽然全局语义快速稳定，但个体保持高多样性且词汇持续更新，同时个体惯性强、相互影响弱，导致无法形成稳定的集体影响力锚点。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型智能体在网络环境中日益增多，需要研究AI智能体社会是否经历类似人类社会的收敛动态，探索规模化和交互密度是否足以引发社会化过程。

Method: 引入定量诊断框架分析AI智能体社会的动态演化，测量语义稳定性、词汇更新率、个体惯性、影响力持久性和集体共识等指标，在Moltbook平台上进行大规模系统诊断。

Result: 系统处于动态平衡：全局语义平均快速稳定，个体保持高多样性和持续词汇更新；但个体惯性强，对交互伙伴适应响应弱，影响力短暂无持久超级节点，缺乏共享社会记忆导致无法形成稳定集体影响力锚点。

Conclusion: 规模和交互密度不足以诱导社会化，为下一代AI智能体社会提供可操作的设计和分析原则，强调需要超越单纯规模扩展来促进真正的社会化过程。

Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.

</details>


### [13] [Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation](https://arxiv.org/abs/2602.14770)
*Shiwei Hong,Lingyao Li,Ethan Z. Rong,Chenxinran Shen,Zhicong Lu*

Main category: cs.CL

TL;DR: 研究探讨社区讨论对LLM喜剧写作的影响，通过多智能体沙盒实验发现，包含社区讨论的条件在75.6%的情况下优于基线，显著提升了写作质量和社交响应


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单轮交互和局部反馈，缺乏对在线社区中持久公共接收效果的考察。本研究旨在测试广播社区讨论是否能改善单口喜剧写作质量

Method: 采用受控多智能体沙盒实验设计：实验组记录、过滤、存储批评家和观众讨论作为社会记忆，并在后续生成中检索使用；对照组则省略讨论。共进行50轮实验（250对独白），由5位专家标注者使用A/B偏好和15项评分标准进行评估

Result: 讨论条件在75.6%的情况下获胜，显著提升了工艺/清晰度（Δ=0.440）和社交响应（Δ=0.422），偶尔增加了攻击性幽默

Conclusion: 社区讨论能有效改善LLM喜剧写作质量，特别是提升写作技巧和社交相关性，但需注意可能增加攻击性内容的风险

Abstract: Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.

</details>


### [14] [A Geometric Analysis of Small-sized Language Model Hallucinations](https://arxiv.org/abs/2602.14778)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CL

TL;DR: 该研究从几何视角分析小规模语言模型的幻觉问题，发现同一提示的真实回答在嵌入空间中呈现更紧密的聚类，基于此提出仅需30-50个标注即可分类大量回答的高效传播方法，F1分数超过90%。


<details>
  <summary>Details</summary>
Motivation: 幻觉（流畅但事实错误的回答）是语言模型可靠性的主要挑战，特别是在多步骤或智能体设置中。传统方法主要关注知识中心和单回答评估，需要从新视角理解幻觉现象。

Method: 采用几何视角分析幻觉，提出假设：模型对同一提示生成的多个回答中，真实回答在嵌入空间中呈现更紧密的聚类。证明该假设后，利用几何洞察开发标签高效传播方法，仅需30-50个标注即可分类大量回答。

Result: 验证了真实回答在嵌入空间中聚类更紧密的假设，实现了可分离性。提出的传播方法仅需少量标注（30-50个）就能达到90%以上的F1分数，显著提高了幻觉检测效率。

Conclusion: 从嵌入空间的几何视角理解幻觉问题，补充了传统的知识中心和单回答评估范式，为未来研究开辟了新方向，提供了高效检测幻觉的实用方法。

Abstract: Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.
  This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.
  Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.

</details>


### [15] [Overthinking Loops in Agents: A Structural Risk via MCP Tools](https://arxiv.org/abs/2602.14798)
*Yohan Lee,Jisoo Jang,Seoyeon Choi,Sangyeop Kim,Seungtaek Choi*

Main category: cs.CL

TL;DR: 恶意MCP工具服务器可通过诱导过度思考循环攻击LLM代理，造成资源放大和任务性能下降


<details>
  <summary>Details</summary>
Motivation: 当前基于文本元数据选择和链接第三方工具的LLM代理存在供应链攻击面，恶意工具服务器可诱导看似正常的工具调用形成循环轨迹，造成资源浪费

Method: 形式化结构性过度思考攻击，区别于token级冗长，实现14个恶意工具分布在三个服务器上，触发重复、强制细化和分心等攻击模式

Result: 攻击导致严重的资源放大（最高142.4倍token），可降低任务结果质量，解码时简洁控制无法可靠防止循环诱导

Conclusion: 防御措施应关注工具调用结构而非仅token，需要结构性推理来防止此类攻击

Abstract: Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack surface: a malicious MCP tool server can be co-registered alongside normal tools and induce overthinking loops, where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-to-end tokens and latency without any single step looking abnormal. We formalize this as a structural overthinking attack, distinguishable from token-level verbosity, and implement 14 malicious tools across three servers that trigger repetition, forced refinement, and distraction. Across heterogeneous registries and multiple tool-capable models, the attack causes severe resource amplification (up to $142.4\times$ tokens) and can degrade task outcomes. Finally, we find that decoding-time concision controls do not reliably prevent loop induction, suggesting defenses should reason about tool-call structure rather than tokens alone.

</details>


### [16] [BFS-PO: Best-First Search for Large Reasoning Models](https://arxiv.org/abs/2602.14917)
*Fiorenzo Parascandolo,Wenhui Tan,Enver Sangineto,Ruihua Song,Rita Cucchiara*

Main category: cs.CL

TL;DR: BFS-PO是一种基于最佳优先搜索的强化学习算法，通过回溯机制寻找最短正确答案，减少大型推理模型的过思考现象，同时提高准确率并缩短回答长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如OpenAI o1和DeepSeek-R1）在推理任务中表现出色，但产生了计算成本显著增加和冗长输出的问题，即"过思考"现象。强化学习算法（如GRPO/DAPO）往往会加剧这种趋势。

Method: 提出BFS-PO算法，采用最佳优先搜索探索策略，通过基于最大熵节点的回溯机制寻找最短正确答案。在训练过程中生成逐渐缩短的响应，学习产生简洁的推理链。

Result: 在不同基准测试和基础LRMs上，BFS-PO能够同时提高LRM的准确率并缩短其回答长度。

Conclusion: BFS-PO算法有效缓解了大型推理模型的过思考问题，通过优化推理链长度实现了准确性和效率的双重提升。

Abstract: Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.

</details>


### [17] [Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System](https://arxiv.org/abs/2602.14970)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: 本文评估了LLM在客服中心质量保证系统中的公平性，发现存在系统性偏见，特别是身份、上下文和行为风格方面，公平性提示仅能带来有限改进。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地应用于客服中心质量保证系统，但其基于网络规模训练数据的特性可能引入人口统计学和行为偏见，扭曲员工绩效评估，需要系统性的公平性评估。

Method: 采用反事实公平性评估框架，在身份、上下文和行为风格三个类别共13个维度上评估18个LLM。使用反事实翻转率(CFR)和平均绝对分数差异(MASD)量化公平性，基于3000个真实客服对话记录进行评估。

Result: 发现系统性差异：CFR在5.4%到13.0%之间，MASD在置信度、积极和改进分数上存在一致偏移。更大、对齐更好的模型偏见较小，但公平性不跟踪准确性。上下文历史表现提示导致最严重的公平性退化(CFR达16.4%)，语言身份线索是持续偏见来源。公平性提示仅带来适度改进。

Conclusion: LLM在高风险员工评估部署前需要标准化的公平性审计流程，当前模型存在系统性偏见，公平性提示效果有限。

Abstract: Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.

</details>


### [18] [Cold-Start Personalization via Training-Free Priors from Structured World Models](https://arxiv.org/abs/2602.15012)
*Avinandan Bose,Shuyue Stella Li,Faeze Brahman,Pang Wei Koh,Simon Shaolei Du,Yulia Tsvetkov,Maryam Fazel,Lin Xiao,Asli Celikyilmaz*

Main category: cs.CL

TL;DR: Pep框架通过离线学习偏好相关结构和在线贝叶斯推理，在冷启动个性化中实现高效偏好获取，相比强化学习方法需要更少交互且能更好适应用户响应。


<details>
  <summary>Details</summary>
Motivation: 冷启动个性化需要在没有用户历史数据的情况下通过交互推断用户偏好。核心挑战是路由问题：每个任务有数十个偏好维度，但个体用户只关心少数几个，且哪些维度重要取决于具体用户。在有限的提问预算下，无结构的提问会错过重要维度。强化学习是自然选择，但在多轮设置中其终端奖励无法利用偏好数据的因子化、按标准结构，实践中学习到的策略会退化为忽略用户响应的静态问题序列。

Method: 提出Pep框架，将冷启动偏好获取分解为离线结构学习和在线贝叶斯推理。Pep从完整用户档案中离线学习偏好相关性的结构化世界模型，然后在线进行无需训练的贝叶斯推理来选择信息量大的问题并预测完整偏好档案，包括从未询问过的维度。该框架在下游求解器之间是模块化的，只需要简单的信念模型。

Result: 在医疗、数学、社会和常识推理任务中，Pep在生成响应与用户陈述偏好之间的对齐度达到80.8%，而RL方法为68.5%，且交互次数减少3-5倍。当两个用户对同一问题给出不同答案时，Pep改变后续问题的概率为39-62%，而RL为0-28%。Pep仅需约1万个参数，而RL需要80亿个参数。

Conclusion: 冷启动偏好获取的瓶颈在于利用偏好数据因子化结构的能力。Pep通过离线学习结构和在线贝叶斯推理，在少量交互中实现了更好的个性化效果，且模型参数远少于传统RL方法。

Abstract: Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [19] [GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-ai-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/2/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/tj7yq1c_ynw5EzKxuClhc3loXnvBsw5iUQ67m30-YOA=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub Agentic Workflows是一个自动化仓库管理工具，使用AI代理自动处理问题分类、CI故障调查修复、测试改进等任务，具有强安全防护设计。


<details>
  <summary>Details</summary>
Motivation: 解决开发者日常重复性工作负担，让开发者能够专注于核心开发任务，通过自动化代理在开发者休息时处理仓库维护工作，提高开发效率。

Method: 采用安全优先的设计原则，构建具有强防护机制的自动化工作流系统，集成现有的编码代理技术，实现问题自动分类、CI故障调查与修复、测试改进建议等自动化功能。

Result: 开发者可以每天醒来就看到问题已分类、CI故障已调查并提供了修复方案、测试改进的PR已准备好，显著减少手动维护时间，提高开发效率。

Conclusion: GitHub Agentic Workflows通过自动化代理工作流显著提升开发效率，让开发者每天都能获得先发优势，专注于更有价值的开发工作。

Abstract: GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles (Sponsor) ☀️ Imagine waking up to your issues triaged, your CI failures investigated with fixes for you to review, and two fresh PRs proposing improvements to your tests. All of that while you were sleeping.Give yourself a headstart, every day. Create your first agentic workflow today.

</details>


### [20] [Create your first agentic workflow today](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-ai-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/2/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/tj7yq1c_ynw5EzKxuClhc3loXnvBsw5iUQ67m30-YOA=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub推出代理工作流平台，通过AI代理自动化代码仓库管理任务，包括问题分类、CI失败调查修复、测试改进等，提供安全护栏和优先安全设计


<details>
  <summary>Details</summary>
Motivation: 解决开发者日常重复性工作负担，让开发者在休息时也能自动处理仓库维护任务，提高开发效率和代码质量

Method: 基于GitHub平台构建代理工作流系统，集成现有AI编码代理，采用安全优先设计原则和强护栏机制

Result: 能够实现问题自动分类、CI失败自动调查修复、测试改进建议等自动化任务，为开发者提供每日工作先发优势

Conclusion: GitHub代理工作流为开发者提供自动化仓库管理解决方案，通过AI代理提升开发效率，同时确保安全性

Abstract: GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles (Sponsor) ☀️ Imagine waking up to your issues triaged, your CI failures investigated with fixes for you to review, and two fresh PRs proposing improvements to your tests. All of that while you were sleeping.Give yourself a headstart, every day. Create your first agentic workflow today.

</details>


### [21] [Real-World Agent Evaluation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fopenenv-turing%3Futm_source=tldrai/1/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/W14gs6ri5e_cX2AnXTetgOb1VNEEj9Srh5bIRvmP9Hc=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenEnv是一个开源框架，通过gym风格API和MCP工具接口标准化智能体与真实有状态环境的交互，使用生产级日历环境展示了在访问控制和长时程推理等现实约束下评估工具使用智能体的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前智能体评估通常缺乏与真实、有状态环境的交互，难以评估在现实约束（如访问控制、长时程推理）下的性能，需要标准化框架来促进真实世界智能体评估。

Method: 开发OpenEnv开源框架，采用gym风格API和MCP工具接口标准化智能体与环境交互，构建生产级日历环境作为示例，展示在现实约束下的评估挑战。

Result: 成功创建了标准化框架，通过日历环境案例展示了真实世界智能体评估的复杂性，特别是在访问控制、状态管理和长时程任务推理方面的挑战。

Conclusion: OpenEnv为真实世界智能体评估提供了标准化基础设施，揭示了在现实约束下评估工具使用智能体的重要性，为未来研究和开发奠定了基础。

Abstract: Real-World Agent Evaluation (11 minute read) OpenEnv is an open-source framework from Meta and Hugging Face that standardizes how agents interact with real, stateful environments via a gym-style API and MCP tool interface. A production-grade calendar environment illustrated the challenges of evaluating tool-using agents under realistic constraints such as access control and long-horizon reasoning.

</details>


### [22] [Math Research Agent from DeepMind](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.10177%3Futm_source=tldrai/1/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/U0ieRsyKL9ooccz2IE71186_0ptEJstRkPILWpBKUGc=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: DeepMind开发了Aletheia数学研究智能体，基于增强版Gemini Deep Think，通过迭代生成和验证长程证明，并利用工具支持进行数学研究。


<details>
  <summary>Details</summary>
Motivation: 开发能够自主进行数学研究的AI系统，解决数学证明生成和验证的挑战，特别是处理长程、复杂的数学证明问题。

Method: 使用增强版Gemini Deep Think作为核心模型，采用迭代式证明生成和验证框架，结合多种工具支持（如定理证明器、符号计算等）来辅助数学研究过程。

Result: 开发出Aletheia数学研究智能体，能够处理长程数学证明，通过工具支持提高证明生成和验证的准确性和效率。

Conclusion: Aletheia代表了数学研究AI的重要进展，展示了大型语言模型与工具结合在复杂数学问题解决中的潜力。

Abstract: Math Research Agent from DeepMind (34 minute read) DeepMind introduced Aletheia, a math research agent powered by an advanced version of Gemini Deep Think that iteratively generates and verifies long-horizon proofs using intensive tool support.

</details>


### [23] [On cognitive debt](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.natemeyvis.com%2Fon-cognitive-debt%2F%3Futm_source=tldrnewsletter/1/0100019c66335a94-028c14eb-82c0-4dab-82c4-270158ceb4fd-000000/c-spkPAZ1MitLatuwMo9xzzfGeucgVKVU181rubJ6nU=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI生成的代码库存在"认知债务"风险，即无人理解其工作原理，导致难以扩展、观察和调试。但研究发现，在控制项目规模和范围后，传统代码库的认知债务问题至少与AI代码库同样严重甚至更糟。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成代码带来的"认知债务"问题，即代码库变得无人理解、难以维护的风险，并比较AI代码与传统代码在这一问题上的差异。

Method: 通过分析AI驱动代码库的现状，并与传统代码库进行比较研究，控制项目规模和范围等变量，评估认知债务的程度。

Result: AI代码库确实存在认知债务问题，但研究发现传统代码库的认知债务问题至少同样严重甚至更糟，这一事实被传统工程实践中的其他因素所掩盖。

Conclusion: 认知债务是软件开发中的普遍问题，不仅限于AI生成代码。虽然AI代码库面临这一风险，但传统代码库同样存在严重问题，需要更全面的工程实践来应对。

Abstract: On cognitive debt (6 minute read) Cognitive debt is the idea that AI-generated codebases are at risk of falling into a state where nobody knows how they work, leaving them inextensible, unobservable, and hard to debug. This is already happening in AI-driven codebases. However, when controlling for the size and scope of a project, cognitive debt tends to be at least as bad, if not worse, in pre-AI code bases than in AI code bases. This fact is obscured because a lot of traditional engineering ...

</details>


### [24] [klaw](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fklawsh%2Fklaw.sh%3Futm_source=tldrnewsletter/1/0100019c66335a94-028c14eb-82c0-4dab-82c4-270158ceb4fd-000000/Mathxq-I0r3vN7xTxuYxtwZhsQhIC9g1ciwvi3VS4Vw=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: klaw是一个企业级AI智能体编排工具，用于管理、监控和扩展AI智能体工作流至数百个智能体规模


<details>
  <summary>Details</summary>
Motivation: 随着企业AI智能体应用的增加，需要有效管理大规模智能体部署，包括监控、协调和扩展

Method: 开发企业级AI智能体编排工具，提供智能体管理、监控和扩展功能

Result: 实现了能够支持数百个AI智能体规模的企业级编排系统

Conclusion: klaw工具解决了企业大规模AI智能体部署的管理挑战

Abstract: klaw (GitHub Repo) klaw is a tool for enterprise AI agent orchestration that allows users to manage, monitor, and scale their AI workforce to hundreds of agents.

</details>


### [25] [64.4% of product roadmaps already include agentic AI. Developers aren't sure what it actually is](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI-feb-16%26utm_content=/2/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/sHlGH6q2FlkyrTjeH2SaZhQhaE4a8UlOaaOUfmwryzo=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 64.4%的产品路线图已包含智能体AI，但开发者对其实际含义不确定。调查显示85%认为三年内智能体AI将成为标配，67%的团队已在构建或部署相关工作流。


<details>
  <summary>Details</summary>
Motivation: 智能体AI已成为产品路线图的重要组成部分，但开发者对其定义、自主性、监督范围和实际应用存在困惑。研究旨在了解智能体AI在开发社区中的认知现状、采用情况和未来预期。

Method: 通过对1000多名开发者和产品领导者进行调查，收集关于智能体AI在路线图中的现状、团队采用情况、开发挑战和未来预期的数据。

Result: 64.4%的产品路线图已包含智能体AI；85%的受访者认为三年内智能体AI将成为标配；67%的团队已在构建或部署智能体工作流；开发者对智能体AI的实际含义和最佳实践存在不确定性。

Conclusion: 智能体AI已从"是否采用"转向"如何实施"的阶段，成为产品开发的关键组成部分。虽然采用率很高，但行业需要更清晰的定义、最佳实践和工具来支持开发者有效实施智能体AI解决方案。

Abstract: 64.4% of product roadmaps already include agentic AI. Developers aren't sure what it actually is (Sponsor) The discussion around agentic AI is no longer "if" but "how" - as teams grapple with decisions around autonomy, oversight, and scope. Nylas asked 1,000+ devs and product leaders why agentic AI is so roadmap-friendly, and the results are surprising: 85% say agentic AI will be table stakes within three years. 67% of teams are already building or shipping agentic workflows - although there'...

</details>


### [26] [The rise of agentic platforms: Scaling beyond automation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fthe-rise-of-agentic-platforms-scaling-beyond-automation%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/KXOudM4FM5XyUt11q6flcdYvR2nm_k-JFsPhGuKy3W0=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了代理平台工程作为平台演进的下一阶段，通过集成目标驱动、上下文感知的AI代理来实现有界自主性，将人类角色转向监督和治理。


<details>
  <summary>Details</summary>
Motivation: 当前平台自动化存在局限性，需要更智能的系统来处理复杂操作任务，同时保持人类监督，推动平台从简单自动化向有界自主性演进。

Method: 采用代理平台工程方法，集成目标驱动、上下文感知的AI代理，使其能够在定义约束内进行推理、决策和执行操作，实现有界自主性。

Result: 代理平台工程成为平台演进的新阶段，能够处理复杂操作任务，同时将人类角色从直接操作转向监督和治理，实现更高效的系统管理。

Conclusion: 代理平台工程代表了平台发展的新方向，通过有界自主性平衡AI能力与人类监督，为复杂系统管理提供了更有效的解决方案。

Abstract: The rise of agentic platforms: Scaling beyond automation (8 minute read) Agentic Platform Engineering, which integrates goal-driven, context-aware AI agents to reason, decide, and execute actions within defined constraints, is emerging as the next phase in platform evolution. This approach aims to achieve bounded autonomy, enabling platforms to handle complex operational tasks while shifting human roles to oversight and governance.

</details>


### [27] [Azure Boards integration with GitHub Copilot includes custom agent support](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevblogs.microsoft.com%2Fdevops%2Fazure-boards-integration-with-github-copilot-includes-custom-agent-support%2F%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/kjVa9-Sxdk25Wa1Nv1btgg7tQ7yyPynETuze19rWayg=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub Copilot for Azure Boards新增自定义代理支持，允许团队在仓库或组织级别定义定制代理，从而标准化从工作项创建PR的工作流程。


<details>
  <summary>Details</summary>
Motivation: 现有的GitHub Copilot与Azure Boards集成缺乏灵活性，团队需要能够根据特定工作流程定制代理，以实现更标准化的开发流程和提升团队协作效率。

Method: 通过在仓库或组织级别引入自定义代理定义功能，允许团队创建和配置专门的代理。这些代理可以在从Azure Boards工作项直接创建拉取请求时被选择和使用。

Result: 团队现在能够定义和使用定制化的代理，这些代理针对特定的开发工作流程进行优化，从而实现了更高效、更标准化的从工作项到代码变更的转换过程。

Conclusion: 自定义代理支持增强了GitHub Copilot与Azure Boards集成的灵活性和实用性，使团队能够更好地标准化开发工作流程，提高协作效率和代码质量。

Abstract: Azure Boards integration with GitHub Copilot includes custom agent support (5 minute read) GitHub Copilot Coding Agent for Azure Boards is adding support for custom agents defined at the repository or organization level. Teams will be able to standardize workflows by selecting tailored agents when creating pull requests directly from work items.

</details>


### [28] [Monty](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fpydantic%2Fmonty%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/IhYdhFHalwVt6L_rbbDlfO4eEkfT0Gx9nt3LNZnkowE=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Monty是一个新的最小化、安全的Python解释器，专为AI代理安全快速执行LLM生成的代码而设计，具有微秒级启动时间，避免了传统容器沙箱的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统容器沙箱启动慢、复杂度高，无法满足AI代理快速执行LLM生成代码的需求。需要一种更轻量、更安全、启动更快的代码执行环境。

Method: 开发了一个最小化的Python解释器Monty，专注于安全性和快速启动，采用轻量级架构避免传统容器化方案的复杂性。

Result: 实现了微秒级启动时间，为AI代理提供了安全快速的代码执行环境，支持Pydantic AI的"代码模式"。

Conclusion: Monty为AI代理执行LLM生成代码提供了一种高效安全的解决方案，显著优于传统容器沙箱方法。

Abstract: Monty (GitHub Repo) Monty is a new minimal and secure Python interpreter that allows AI agents to safely and quickly execute LLM-generated code. This experimental project aims to power "code-mode" in Pydantic AI by offering microsecond startup times and avoiding the complexity of traditional container-based sandboxes.

</details>


### [29] [AWS Transform custom: AI-driven Java modernization to reduce tech debt](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Fdevops%2Faws-transform-custom-ai-driven-java-modernization-to-reduce-tech-debt%2F%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/kK2zHhPBmCr1sRlAjna6EmfLVkdvtjx_RqbQBGwJBwU=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AWS Transform Custom 使用代理AI和AWS托管转换，自动化Java应用现代化（如Java 8到21），通过CLI应用验证模式减少手动重构，同时支持自定义转换。


<details>
  <summary>Details</summary>
Motivation: 减少技术债务，自动化Java应用现代化过程，解决手动升级Java版本时的大量重构工作。

Method: 使用代理AI和AWS管理的转换模式，通过命令行界面(CLI)自动化应用验证的现代化模式，同时允许用户根据需要添加自定义转换。

Result: 能够大规模现代化Java应用程序，自动化升级Java版本（如从Java 8到21），显著减少手动重构工作量。

Conclusion: AWS Transform Custom提供了一种可扩展的Java应用现代化解决方案，结合AI驱动和托管转换，有效降低技术债务。

Abstract: AWS Transform custom: AI-driven Java modernization to reduce tech debt (7 minute read) AWS Transform custom uses agentic AI and AWS-managed transformations to modernize Java applications at scale, automating upgrades like Java 8 to 21. It reduces manual refactoring by applying vetted patterns through a CLI while allowing custom transformations when needed.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [A Survey of Code Review Benchmarks and Evaluation Practices in Pre-LLM and LLM Era](https://arxiv.org/abs/2602.13377)
*Taufiqul Islam Khan,Shaowei Wang,Haoxiang Zhang,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 该论文对代码审查基准进行了全面调查，分析了99篇研究论文，提出了一个多级分类法，揭示了从传统方法向端到端生成式同行评审的转变趋势。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码审查领域的应用日益增多，现有基准和评估方法缺乏系统性理解，阻碍了该领域的发展。当前数据集分散、设计差异大，且无法清晰评估实际的审查能力。

Method: 对2015-2025年间99篇研究论文（58篇前LLM时代，41篇LLM时代）进行系统性分析，提取关键元数据（数据集、评估指标、数据源、目标任务），并提出了一个包含5个领域和18个细粒度任务的多级分类法。

Result: 研究发现明显的趋势转变：向端到端生成式同行评审发展，多语言覆盖增加，独立的变更理解任务减少。同时识别了当前基准的局限性，包括任务覆盖范围有限、缺乏动态运行时评估等。

Conclusion: 该调查为开发更现实、更全面的LLM代码审查基准提供了结构化基础，并提出了未来研究方向，包括更广泛的任务覆盖、动态运行时评估和基于分类法的细粒度评估。

Abstract: Code review is a critical practice in modern software engineering, helping developers detect defects early, improve code quality, and facilitate knowledge sharing. With the rapid advancement of large language models (LLMs), a growing body of work has explored automated support for code review. However, progress in this area is hindered by the lack of a systematic understanding of existing benchmarks and evaluation practices. Current code review datasets are scattered, vary widely in design, and provide limited insight into what review capabilities are actually being assessed. In this paper, we present a comprehensive survey of code review benchmarks spanning both the Pre-LLM and LLM eras (2015--2025). We analyze 99 research papers (58 Pre-LLM era and 41 LLM era) and extract key metadata, including datasets, evaluation metrics, data sources, and target tasks. Based on this analysis, we propose a multi-level taxonomy that organizes code review research into five domains and 18 fine-grained tasks. Our study reveals a clear shift toward end-to-end generative peer review, increasing multilingual coverage, and a decline in standalone change understanding tasks. We further identify limitations of current benchmarks and outline future directions, including broader task coverage, dynamic runtime evaluation, and taxonomy-guided fine-grained assessment. This survey provides a structured foundation for developing more realistic and comprehensive benchmarks for LLM-based code review.

</details>


### [31] [InEx-Bug: A Human Annotated Dataset of Intrinsic and Extrinsic Bugs in the NPM Ecosystem](https://arxiv.org/abs/2602.13400)
*Tanner Wright,Adams Chen,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: InEx-Bug是一个手动标注的GitHub问题数据集，包含377个NPM仓库的问题，将缺陷分类为内部缺陷、外部依赖/环境问题、非缺陷或未知类型，并分析不同类型缺陷的解决时间和模式差异。


<details>
  <summary>Details</summary>
Motivation: 现有缺陷数据集未能区分问题来源是项目内部还是外部依赖/环境因素，这限制了理解软件缺陷根本原因和生态系统稳定性维护的能力。

Method: 创建InEx-Bug数据集，手动标注103个NPM仓库的377个GitHub问题，分类为内部缺陷、外部依赖/环境问题、非缺陷或未知类型，并收集时间序列和行为元数据（维护者参与、代码变更、重新开放模式等）。

Result: 内部缺陷解决更快（中位数8.9天 vs 10.2天），关闭率更高（92% vs 78%），更常需要代码变更（57% vs 28%）；外部缺陷重新开放率更高（12% vs 4%），复发延迟更长（中位数157天 vs 87天）。

Conclusion: InEx-Bug数据集为研究NPM生态系统中的内部和外部缺陷提供了基础，揭示了不同类型缺陷在解决时间、模式和影响方面的显著差异。

Abstract: Understanding the causes of software defects is essential for reliable software maintenance and ecosystem stability. However, existing bug datasets do not distinguish between issues originating within a project from those caused by external dependencies or environmental factors. In this paper we present InEx-Bug, a manually annotated dataset of 377 GitHub issues from 103 NPM repositories, categorizing issues as Intrinsic (internal defect), Extrinsic (dependency/environment issue), Not-a-Bug, or Unknown. Beyond labels, the dataset includes rich temporal and behavioral metadata such as maintainer participation, code changes, and reopening patterns. Analyses show Intrinsic bugs resolve faster (median 8.9 vs 10.2 days), are close more often (92% vs 78%), and require code changes more frequently (57% vs 28%) compared to Extrinsic bugs. While Extrinsic bugs exhibit higher reopen rates (12% vs 4%) and delayed recurrence (median 157 vs 87 days). The dataset provides a foundation for further studying Intrinsic and Extrinsic defects in the NPM ecosystem.

</details>


### [32] [Execution-State-Aware LLM Reasoning for Automated Proof-of-Vulnerability Generation](https://arxiv.org/abs/2602.13574)
*Haoyu Li,Xijia Che,Yanhao Wang,Xiaojing Liao,Luyi Xing*

Main category: cs.SE

TL;DR: DrillAgent是一个基于LLM的代理框架，通过迭代假设-验证-精炼过程生成漏洞证明，结合语义推理和程序执行反馈，显著提升C/C++漏洞的自动化PoV生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有PoV生成方法存在局限：定向模糊测试难以满足复杂语义约束，而基于LLM的方法缺乏具体执行行为的接地，导致无法生成精确的漏洞证明。需要一种结合语义推理和动态执行反馈的方法来提升自动化漏洞验证能力。

Method: DrillAgent将PoV生成重构为迭代的假设-验证-精炼过程，通过LLM进行语义推理，结合具体程序状态反馈，分析目标代码假设输入，观察执行行为，并将低级执行跟踪转换为源代码级约束，形成闭环设计。

Result: 在真实世界C/C++漏洞基准测试SEC-bench上评估，DrillAgent在固定预算约束下显著优于现有LLM代理基线，比最佳基线多解决52.8%的CVE任务。

Conclusion: DrillAgent证明了执行状态感知推理对于复杂软件系统中可靠PoV生成的必要性，通过结合LLM语义能力和动态执行反馈，有效提升了自动化漏洞验证的准确性和可靠性。

Abstract: Proof-of-Vulnerability (PoV) generation is a critical task in software security, serving as a cornerstone for vulnerability validation, false positive reduction, and patch verification. While directed fuzzing effectively drives path exploration, satisfying complex semantic constraints remains a persistent bottleneck in automated exploit generation. Large Language Models (LLMs) offer a promising alternative with their semantic reasoning capabilities; however, existing LLM-based approaches lack sufficient grounding in concrete execution behavior, limiting their ability to generate precise PoVs.
  In this paper, we present DrillAgent, an agentic framework that reformulates PoV generation as an iterative hypothesis-verification-refinement process. To bridge the gap between static reasoning and dynamic execution, DrillAgent synergizes LLM-based semantic inference with feedback from concrete program states. The agent analyzes the target code to hypothesize inputs, observes execution behavior, and employs a novel mechanism to translate low-level execution traces into source-level constraints. This closed-loop design enables the agent to incrementally align its input generation with the precise requirements of the vulnerability. We evaluate DrillAgent on SEC-bench, a large-scale benchmark of real-world C/C++ vulnerabilities. Experimental results show that DrillAgent substantially outperforms state-of-the-art LLM agent baselines under fixed budget constraints, solving up to 52.8% more CVE tasks than the best-performing baseline. These results highlight the necessity of execution-state-aware reasoning for reliable PoV generation in complex software systems.

</details>


### [33] [From What to How: Bridging User Requirements with Software Development Using Large Language Models](https://arxiv.org/abs/2602.13611)
*Xiao He,Ru Chen,Jialun Cao*

Main category: cs.SE

TL;DR: DesBench是一个面向软件设计的基准测试，用于评估LLM在代码生成、面向对象建模和验收测试设计三个任务上的表现，发现LLM在软件设计方面仍面临显著挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试主要关注代码实现，忽视了同样重要的软件设计环节。本文旨在探究两个关键问题：LLM能否处理软件设计？LLM能否按照特定设计编写代码？

Method: 提出DesBench基准测试，包含30个手工创建的Java项目，涵盖需求文档、设计模型、实现代码和验收测试。评估了7个最先进的LLM（包括DeepSeek R1、Qwen2.5和GPT模型）在三个任务上的表现：设计感知代码生成、面向对象建模、验收测试设计。

Result: 1) 代码生成：LLM在仅有高层设计或无设计时难以生成正确实现；2) 面向对象建模：LLM能准确识别对象和类，但在定义操作和类间关系方面存在困难；3) 验收测试：LLM从功能需求生成的测试用例在代码覆盖率方面与人工编写相当。

Conclusion: LLM在软件设计管理方面仍存在明显局限性，需要进一步研究适合LLM开发的新设计方法和语言。

Abstract: Recently, large language models (LLMs) are extensively utilized to enhance development efficiency, leading to numerous benchmarks for evaluating their performance. However, these benchmarks predominantly focus on implementation, overlooking the equally critical aspect of software design. This gap raises two pivotal questions: (1) Can LLMs handle software design? (2) Can LLMs write code following the specific designs? To investigate these questions, this paper proposes DesBench, a design-aware benchmark for evaluating LLMs on three software design-related tasks: design-aware code generation, object-oriented modeling, and the design of acceptance test cases. DesBench comprises 30 manually crafted Java projects that include requirement documents, design models, implementations, and acceptance tests, amounting to a total of 30 design models, 194 Java classes, and 737 test cases. We evaluated seven state-of-the-art LLMs, including three DeepSeek R1, two Qwen2.5, and two GPT models, using DesBench. The results reveal that LLMs remain significantly challenged by the intricacies of software design: (1) For code generation, LLMs struggle to produce correct implementations when provided with only high-level or no designs. (2) In object-oriented modeling, while LLMs can accurately identify objects and classes, they face challenges in defining operations and inter-class relationships. (3) Acceptance test cases generated by LLMs from functional requirements achieve code coverage quality comparable to those written by humans. Our research highlights the current limitations of LLMs in managing software design and calls for further investigation into new design methodologies and languages suitable for LLM-based development.

</details>


### [34] [ARC: Compiling Hundreds of Requirement Scenarios into A Runnable Web System](https://arxiv.org/abs/2602.13723)
*Weiyu Kong,Yun Lin,Xiwen Teoh,Duc-Minh Nguyen,Ruofei Ren,Jiaxin Chang,Haoxu Hu,Haoyu Chen*

Main category: cs.SE

TL;DR: ARC是一种从多模态DSL文档直接生成可运行Web系统的代理需求编译技术，通过双向测试驱动循环实现需求到代码的转换，相比基线方法通过50.6%更多的GUI测试。


<details>
  <summary>Details</summary>
Motivation: LLM在处理大规模多模态需求时性能显著下降，经常产生错误实现或遗漏约束，需要一种能够从复杂需求文档直接生成可运行系统的技术。

Method: 提出代理需求编译(ARC)技术，采用双向测试驱动代理循环：自上而下的架构阶段将需求分解为可验证接口，自下而上的实现阶段生成满足测试的代码，保持需求、设计和代码之间的严格可追溯性。

Result: 从50-200个多模态场景的文档中生成了6个可运行Web系统，相比最先进基线平均通过50.6%更多的GUI测试。用户研究表明新手用户平均5.6小时可为复杂系统（如1万行票务系统）编写DSL文档。

Conclusion: ARC能够有效将非平凡需求规范转换为可维护、可运行的软件，展示了从多模态需求文档直接生成完整软件系统的可行性。

Abstract: Large Language Models (LLMs) have improved programming efficiency, but their performance degrades significantly as requirements scale; when faced with multi-modal documents containing hundreds of scenarios, LLMs often produce incorrect implementations or omit constraints. We propose Agentic Requirement Compilation (ARC), a technique that moves beyond simple code generation to requirement compilation, enabling the creation of runnable web systems directly from multi-modal DSL documents. ARC generates not only source code but also modular designs for UI, API, and database layers, enriched test suites (unit, modular, and integration), and detailed traceability for software maintenance. Our approach employs a bidirectional test-driven agentic loop: a top-down architecture phase decomposes requirements into verifiable interfaces, followed by a bottom-up implementation phase where agents generate code to satisfy those tests. ARC maintains strict traceability across requirements, design, and code to facilitate intelligent asset reuse. We evaluated ARC by generating six runnable web systems from documents spanning 50-200 multi-modal scenarios. Compared to state-of-the-art baselines, ARC-generated systems pass 50.6% more GUI tests on average. A user study with 21 participants showed that novice users can successfully write DSL documents for complex systems, such as a 10K-line ticket-booking system, in an average of 5.6 hours. These results demonstrate that ARC effectively transforms non-trivial requirement specifications into maintainable, runnable software.

</details>


### [35] [CodeGlance: Understanding Code Reasoning Challenges in LLMs through Multi-Dimensional Feature Analysis](https://arxiv.org/abs/2602.13962)
*Yunkun Wang,Xuanhe Zhang,Junxiao Han,Chen Zhi,Shuiguang Deng*

Main category: cs.SE

TL;DR: CodeGlance是一个多维度代码推理基准测试，评估LLM在三种现实场景下的表现：内在逻辑推理、API交互推理和未知函数推理，揭示了未知函数推理对小型模型的显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理研究主要关注孤立代码片段，忽略了涉及外部API交互和未知函数的现实场景复杂性，这阻碍了我们理解LLM在不同编程上下文中的真正挑战。

Method: 创建CodeGlance基准测试，包含三个现实场景：内在逻辑推理、API交互推理和未知函数推理。系统评估7个最先进的LLM，分析代码复杂性特征（执行跟踪长度、API调用次数、控制流复杂性），并研究常见增强策略（CoT、文档检索、代码搜索）的效果。

Result: 未知函数推理对小型模型构成显著挑战，Qwen2.5-3b在未知函数上仅达到6.0%准确率，而在熟悉API上为37.5%。识别出执行跟踪长度、API调用次数和控制流复杂性等关键代码复杂性特征会影响推理难度。不同增强策略的效果因挑战来源（逻辑复杂性vs知识差距）而异。

Conclusion: 研究结果为开发更强大的代码推理系统和在现实软件开发中部署基于LLM的编程助手提供了可操作的指导，强调了考虑现实场景复杂性的重要性。

Abstract: In modern software development, developers frequently need to understand code behavior at a glance -- whether reviewing pull requests, debugging issues, or navigating unfamiliar codebases. This ability to reason about dynamic program behavior is fundamental to effective software engineering and increasingly supported by Large Language Models (LLMs). However, existing studies on code reasoning focus primarily on isolated code snippets, overlooking the complexity of real-world scenarios involving external API interactions and unfamiliar functions. This gap hinders our understanding of what truly makes code reasoning challenging for LLMs across diverse programming contexts.
  We present CodeGlance, a multi-dimensional benchmark investigating code reasoning challenges across three realistic scenarios: intrinsic logic reasoning, API interaction reasoning, and unseen function reasoning. Through systematic evaluation of 7 state-of-the-art LLMs, we reveal that unseen function reasoning poses significant challenges especially for smaller models, with Qwen2.5-3b achieving only 6.0\% accuracy on unseen functions compared to 37.5\% on familiar APIs. We identify critical code complexity features -- including execution trace length, API invocation count, and control flow complexity -- that significantly impact code reasoning difficulty across scenarios. We further investigate how common augmentation strategies, including CoT, document retrieval, and code search, can improve reasoning performance, finding that their effectiveness varies substantially depending on whether challenges stem from logical complexity or knowledge gaps. These findings provide actionable guidance for developing more capable code reasoning systems and deploying LLM-based programming assistants in real-world software development.

</details>


### [36] [ATTest: Agent-Driven Tensor Testing for Deep Learning Library Modules](https://arxiv.org/abs/2602.13987)
*Zhengyu Zhan,Ye Shang,Jiawei Liu,Chunrong Fang,Quanjun Zhang,Zhenyu Chen*

Main category: cs.SE

TL;DR: ATTest是一个基于智能体的张量测试框架，用于深度学习库的模块级单元测试生成，通过七阶段流水线显著提升了分支覆盖率。


<details>
  <summary>Details</summary>
Motivation: 深度学习库的单元测试面临挑战：复杂的数值语义和隐式张量约束使得传统基于搜索的软件测试存在语义盲区，而大语言模型在处理跨文件上下文和不稳定的代码修改方面存在困难。

Method: ATTest采用智能体驱动的七阶段流水线，包括约束提取和迭代的"生成-验证-修复"循环，以保持测试稳定性并缓解上下文窗口饱和问题。

Result: 在PyTorch和TensorFlow上的评估显示，ATTest显著优于现有基线方法（如PynguinML），平均分支覆盖率分别达到55.60%和54.77%。

Conclusion: 智能体驱动的工作流程能够弥合数值库中的语义差距，同时确保可审计的测试合成，为深度学习库测试提供了有效解决方案。

Abstract: The unit testing of Deep Learning (DL) libraries is challenging due to complex numerical semantics and implicit tensor constraints. Traditional Search-Based Software Testing (SBST) often suffers from semantic blindness, failing to satisfy the constraints of high-dimensional tensors, whereas Large Language Models (LLMs) struggle with cross-file context and unstable code modifications. This paper proposes ATTest, an agent-driven tensor testing framework for module-level unit test generation. ATTest orchestrates a seven-stage pipeline, which encompasses constraint extraction and an iterative "generation-validation-repair" loop, to maintain testing stability and mitigate context-window saturation. An evaluation on PyTorch and TensorFlow demonstrates that ATTest significantly outperforms state-of-the-art baselines such as PynguinML, achieving an average branch coverage of 55.60% and 54.77%, respectively. The results illustrate how agent-driven workflows bridge the semantic gap in numerical libraries while ensuring auditable test synthesis. Source code: https://github.com/iSEngLab/ATTest.git

</details>


### [37] [LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces](https://arxiv.org/abs/2602.14337)
*Yukang Feng,Jianwen Sun,Zelai Yang,Jiaxin Ai,Chuanhao Li,Zizhen Li,Fanrui Zhang,Kang He,Rui Ma,Jifan Lin,Jie Sun,Yang Xiao,Sizhuo Zhou,Wenxiao Wu,Yiming Liu,Pengfei Liu,Yu Qiao,Shenglin Zhang,Kaipeng Zhang*

Main category: cs.SE

TL;DR: LongCLI-Bench是一个新的长时程CLI任务基准，从1000多个计算机科学作业和真实工作流中精选20个高质量任务，涵盖四种工程类别，采用双重测试协议和步骤级评分，发现现有SOTA代理成功率低于20%，早期阶段常出现关键失败。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助编程基准存在三个主要局限：任务时程短、GitHub数据污染、缺乏细粒度评估指标，无法有效评估现实软件工程所需的长时程规划和执行能力。

Method: 从1000多个计算机科学作业和真实工作流中精选20个高质量长时程任务，涵盖四种工程类别（从零开始、功能添加、错误修复、重构）。提出双重测试协议：需求满足度（fail-to-pass）和回归避免（pass-to-pass），并采用步骤级评分精确定位执行失败。

Result: 实验显示即使最先进的代理在LongCLI-Bench中通过率也低于20%。步骤级分析表明大多数任务在完成度不到30%时就停滞，关键失败常发生在早期阶段。自我纠正仅带来边际改善，而通过计划注入和交互指导的人机协作能显著提高性能。

Conclusion: 未来研究必须强调协同人机工作流的开发，同时提升代理的规划和执行能力，以克服长时程任务性能的关键挑战。

Abstract: Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.

</details>


### [38] [An Empirical Study of the Evolution of GitHub Actions Workflows](https://arxiv.org/abs/2602.14572)
*Pooya Rostami Mazrae,Alexandre Decan,Tom Mens,Mairieli Wessel*

Main category: cs.SE

TL;DR: 对GitHub Actions工作流变更的混合方法分析：通过定性分析439个修改的工作流文件识别7种概念变更类型，定量分析49K+仓库的267K+变更历史和3.4M+工作流文件版本，发现仓库中位数为3个工作流文件，每周7.3%的文件被修改，变更通常较小，主要涉及任务配置和规范，未发现LLM工具对工作流创建和维护频率的显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究GitHub Actions作为GitHub集成的CI/CD工具在实际使用中的演变模式，了解工作流文件的变更频率、类型和规模，探索自动化开发工作流的维护实践和挑战。

Method: 混合方法研究：1）对439个修改的工作流文件进行初步定性分析，识别7种概念变更类型；2）对2019年11月至2025年8月期间49K+ GitHub仓库的267K+工作流变更历史和3.4M+工作流文件版本进行定量分析。

Result: 1）仓库中位数为3个工作流文件；2）每周7.3%的工作流文件被修改；3）约四分之三的变更只包含单个修改；4）大多数变更涉及工作流作业中的任务配置和任务规范；5）未发现LLM编码工具或其他重大技术变革对工作流创建和维护频率的明确影响。

Conclusion: 需要改进工具支持细粒度维护任务，包括更广泛采用依赖管理和基于AI的支持，以确保和维持工作流的安全性和质量。

Abstract: CI/CD practices play a significant role during collaborative software development by automating time-consuming and repetitive tasks such as testing, building, quality checking, dependency and security management. GitHub Actions, the CI/CD tool integrated into GitHub, allows repository maintainers to automate development workflows. We conducted a mixed methods analysis of GitHub Actions workflow changes over time. Through a preliminary qualitative analysis of 439 modified workflow files we identified seven types of conceptual changes to workflows. Next, we performed a quantitative analysis over 49K+ GitHub repositories totaling 267K+ workflow change histories and 3.4M+ workflow file versions from November 2019 to August 2025. This analysis revealed that repositories contain a median of three workflow files, and 7.3% of all workflow files are being changed every week. The changes made to workflows tend to be small, with about three-quarters containing only a single change. The large majority of the observed changes have to do with task configuration and task specification in workflow jobs. We did not find any conclusive evidence of the effect of LLM coding tools or other major technological changes on workflow creation and workflow maintenance frequency. Our findings highlight the need for improved tooling to support fine-grained maintenance tasks, such as a broader adoption of dependency management and AI-based support for ensuring and sustaining workflow security and quality.

</details>


### [39] [Automated Classification of Source Code Changes Based on Metrics Clustering in the Software Development Process](https://arxiv.org/abs/2602.14591)
*Evgenii Kniazev*

Main category: cs.SE

TL;DR: 基于代码变更指标聚类的自动化分类方法，通过k-means算法和11个源代码指标实现变更分类，专家只需将聚类结果映射到预定义类别，显著减少代码审查时间。


<details>
  <summary>Details</summary>
Motivation: 软件开发过程中需要大量时间进行代码变更审查，传统手动分类方法效率低下。本文旨在通过自动化方法减少代码变更分类所需的人工工作量，提高审查效率。

Method: 采用两阶段方法：1) 使用k-means算法和余弦相似度对每个代码变更的11个指标向量进行聚类；2) 专家将聚类结果映射到预定义的变更类别。指标包括代码行数、圈复杂度、文件数量、接口变更和结构变更等。

Result: 在5个软件系统（包括Subversion和NHibernate两个开源项目）上验证，分类纯度P_C = 0.75 ± 0.05，熵E_C = 0.37 ± 0.06（显著性水平0.05），表明方法有效且分类质量良好。

Conclusion: 提出的自动化代码变更分类方法能显著减少代码审查时间，通过聚类和专家映射的结合实现了有效的变更分类，为软件开发过程提供了实用的自动化工具。

Abstract: This paper presents an automated method for classifying source code changes during the software development process based on clustering of change metrics. The method consists of two steps: clustering of metric vectors computed for each code change, followed by expert mapping of the resulting clusters to predefined change classes. The distribution of changes into clusters is performed automatically, while the mapping of clusters to classes is carried out by an expert. Automation of the distribution step substantially reduces the time required for code change review. The k-means algorithm with a cosine similarity measure between metric vectors is used for clustering. Eleven source code metrics are employed, covering lines of code, cyclomatic complexity, file counts, interface changes, and structural changes. The method was validated on five software systems, including two open-source projects (Subversion and NHibernate), and demonstrated classification purity of P_C = 0.75 +/- 0.05 and entropy of E_C = 0.37 +/- 0.06 at a significance level of 0.05.

</details>


### [40] [Consistent or Sensitive? Automated Code Revision Tools Against Semantics-Preserving Perturbations](https://arxiv.org/abs/2602.14595)
*Shirin Pirouzkhah,Souhaila Serbout,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 本文研究了自动代码修订工具在语义等价代码变体上的不一致性问题，发现即使代码语义相同，ACR工具生成正确修订的能力可能下降高达45.3%，且目前缓解策略效果有限。


<details>
  <summary>Details</summary>
Motivation: 自动代码修订工具在实际应用中的实用性取决于其处理语义等价代码变体的能力，即一致性。然而，ACR工具的概率性质常常损害一致性，导致对相同问题的不同代码变体产生分歧的修订。

Method: 设计了9种语义保持扰动类型，应用于2032个真实GitHub项目的Java方法，生成超过10K个扰动变体进行评估。使用这些扰动评估了5个最先进的基于Transformer的ACR工具的一致性。

Result: ACR工具在语义等价代码上生成正确修订的能力可能下降高达45.3%。扰动越接近目标区域，ACR工具越可能无法生成正确修订。尝试的注意力引导启发式缓解策略仅带来边际改进。

Conclusion: ACR工具在处理语义等价代码变体时存在严重的一致性问题，当前缓解策略效果有限，这仍然是一个开放的研究问题。

Abstract: Automated Code Revision (ACR) tools aim to reduce manual effort by automatically generating code revisions based on reviewer feedback. While ACR tools have shown promising performance on historical data, their real-world utility depends on their ability to handle similar code variants expressing the same issue - a property we define as consistency. However, the probabilistic nature of ACR tools often compromises consistency, which may lead to divergent revisions even for semantically equivalent code variants. In this paper, we investigate the extent to which ACR tools maintain consistency when presented with semantically equivalent code variants. To do so, we first designed nine types of semantics-preserving perturbations (SPP) and applied them to 2032 Java methods from real-world GitHub projects, generating over 10K perturbed variants for evaluation. Then we used these perturbations to evaluate the consistency of five state-of-the-art transformer-based ACR tools. We found that the ACR tools' ability to generate correct revisions can drop by up to 45.3%, when presented with semantically equivalent code. The closer the perturbation is to this targeted region, the more likely an ACR tool is to fail to generate the correct revision. We explored potential mitigation strategies that modify the input representation, but found that these attention-guiding heuristics yielded only marginal improvements, thus leaving the solution to this problem as an open research question.

</details>


### [41] [Configuring Agentic AI Coding Tools: An Exploratory Study](https://arxiv.org/abs/2602.14690)
*Matthias Galster,Seyedmoein Mohsenimofidi,Jai Lal Lulla,Muhammad Auwal Abubakar,Christoph Treude,Sebastian Baltes*

Main category: cs.SE

TL;DR: 该论文系统分析了Claude Code、GitHub Copilot等代理式AI编码工具的配置机制，通过对2926个GitHub仓库的实证研究，发现Context Files主导配置，高级机制采用较浅，不同工具形成不同配置文化。


<details>
  <summary>Details</summary>
Motivation: 随着代理式AI编码工具具备超越对话内容生成的自主能力，开发者需要通过版本控制的仓库级工件（如Markdown和JSON文件）来配置这些工具。然而，目前缺乏对这些配置机制的系统分析，包括它们的采用情况和实际使用模式。

Method: 论文对Claude Code、GitHub Copilot、Cursor、Gemini和Codex等代理式AI编码工具的配置机制进行了系统分析，识别出8种配置机制。通过对2926个GitHub仓库的实证研究，考察了这些机制的采用情况，并重点深入分析了Context Files、Skills和Subagents这三种跨工具可用的机制。

Result: 研究发现三个主要趋势：1) Context Files在配置中占主导地位，AGENTS.md成为跨工具的互操作标准；2) Skills和Subagents等高级机制采用较浅，大多数仓库只定义一两个工件，Skills主要依赖静态指令而非可执行工作流；3) 不同工具形成不同的配置文化，Claude Code用户使用最广泛的机制。

Conclusion: 这些发现为纵向和实验研究建立了实证基线，有助于理解随着代理式AI编码工具的成熟，配置策略如何演变以及如何影响代理性能。

Abstract: Agentic AI coding tools with autonomous capabilities beyond conversational content generation increasingly automate repetitive and time-consuming software development tasks. Developers can configure these tools through versioned repository-level artifacts such as Markdown and JSON files. In this paper, we present a systematic analysis of configuration mechanisms for agentic AI coding tools, covering Claude Code, GitHub Copilot, Cursor, Gemini, and Codex. We identify eight configuration mechanisms and, in an empirical study of 2,926 GitHub repositories, examine whether and how they are adopted. We then explore Context Files, Skills, and Subagents, that is, three mechanisms available across tools, in more detail. Our findings reveal three trends. First, Context Files dominate the configuration landscape and are often the sole mechanism in a repository, with AGENTS$.$md emerging as an interoperable standard across tools. Second, advanced mechanisms such as Skills and Subagents are only shallowly adopted: most repositories define only one or two artifacts, and Skills predominantly rely on static instructions rather than executable workflows. Third, distinct configuration cultures are forming around different tools, with Claude Code users employing the broadest range of mechanisms. These findings establish an empirical baseline for longitudinal and experimental research on how configuration strategies evolve and affect agent performance as agentic AI coding tools mature.

</details>


### [42] [Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions](https://arxiv.org/abs/2602.14878)
*Mohammed Mehedi Hasan,Hao Li,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对856个MCP工具进行实证研究，发现97.1%的工具描述存在缺陷，改进描述可提升任务成功率但增加执行步骤，需要在性能与成本间权衡。


<details>
  <summary>Details</summary>
Motivation: MCP标准中，基础模型代理依赖自然语言工具描述来选择和使用工具，但这些描述中的缺陷可能导致代理错误选择工具或传递错误参数，其普遍性和影响尚不明确。

Method: 从文献中识别工具描述的六个组件，开发评分标准，基于此形式化工具描述缺陷，使用基于基础模型的扫描器分析103个MCP服务器中的856个工具，并进行描述增强实验。

Result: 97.1%的工具描述至少存在一个缺陷，56%未清晰说明目的；增强所有组件描述使任务成功率中位数提升5.85个百分点，部分目标完成率提升15.12%，但执行步骤增加67.46%，16.67%的情况性能下降；组件消融显示紧凑变体可保持可靠性同时减少token开销。

Conclusion: 工具描述质量对代理性能有显著影响，但改进描述需要在性能提升与执行成本间权衡，紧凑的描述变体可在保持可靠性的同时提高效率。

Abstract: The Model Context Protocol (MCP) standardizes how Foundation Model (FM)-based agents interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.
  To address this, we conduct the first large-scale empirical study of 856 tools spread across 103 MCP servers, assessing their description quality and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These findings highlight a trade-off between agent performance and cost, as well as the context sensitivity of the performance gain. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique](https://arxiv.org/abs/2602.13213)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.AI

TL;DR: 论文提出了一种用于商业保险核保的决策否定式人机协同代理系统，采用对抗性自我批判机制作为安全架构，在保持人类最终决策权的同时显著降低AI幻觉率并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 商业保险核保是劳动密集型过程，需要人工审查大量文档。现有AI解决方案缺乏全面的推理能力和确保在受监管高风险环境中可靠性的内部机制，完全自动化在需要人类判断和问责的场景中既不实际也不可取。

Method: 开发了一个决策否定式人机协同代理系统，包含对抗性自我批判机制作为安全架构。系统中，批判代理在将建议提交给人类审查员之前挑战主代理的结论。同时建立了决策否定代理潜在错误的正式分类法。

Result: 在500个专家验证的核保案例中，对抗性批判机制将AI幻觉率从11.3%降至3.8%，决策准确率从92%提高到96%。系统通过设计强制执行人类对所有约束性决策的最终权威。

Conclusion: 对抗性自我批判支持在受监管领域更安全的AI部署，为人类监督不可或缺场景下的负责任集成提供了模型。该框架解决了受监管工作流中AI安全的关键缺口。

Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. At the same time, the framework enforces strict human authority over all binding decisions by design. These findings indicate that adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable.

</details>


### [44] [BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors](https://arxiv.org/abs/2602.13214)
*Lingfeng Li,Yunlong Lu,Yuefei Zhang,Jingyu Yao,Yixin Zhu,KeYuan Cheng,Yongyi Wang,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: BotzoneBench：通过固定技能校准的游戏AI层次结构来评估LLM战略推理能力的基准框架，实现线性时间绝对技能测量和跨时间稳定可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估基准主要关注静态推理任务，无法捕捉动态战略能力。现有的基于游戏的LLM-vs-LLM锦标赛评估方法存在计算成本高、依赖瞬时模型池、缺乏稳定性能锚点等问题，需要建立可扩展的评估框架来测量LLM的战略推理能力。

Method: 基于Botzone平台的竞争基础设施，构建BotzoneBench基准，在八个多样化游戏（从确定性完美信息棋盘游戏到随机不完美信息纸牌游戏）中评估LLM。通过将LLM评估锚定到固定技能校准的游戏AI层次结构，实现线性时间的绝对技能测量。

Result: 通过系统评估177,047个状态-动作对和五个旗舰模型，揭示了显著的性能差异和不同的战略行为。表现最佳的模型在多个领域达到了与中高等级专业游戏AI相当的熟练度。

Conclusion: 这种锚定评估范式可以推广到任何具有明确定义技能层次的领域，为评估交互式AI能力建立了可扩展和可重用的框架。

Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

</details>


### [45] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic是一个基于逻辑推理的RLVR扩展框架，通过生成-验证-修复循环自动合成可执行的生成器-验证器程序对，实现任务家族级别的扩展，显著提升多个基准性能。


<details>
  <summary>Details</summary>
Motivation: 可验证训练信号的扩展是RLVR的关键瓶颈。逻辑推理是天然基底，但现有合成方法依赖专家代码或固定模板，只能进行实例级扰动，限制了任务家族级别的扩展。

Method: 提出SSLogic代理元合成框架，通过迭代合成和修复可执行的生成器-验证器程序对，在封闭的生成-验证-修复循环中实现连续家族演化。引入多门验证协议，结合多策略一致性检查和对抗性盲审，确保任务可靠性。

Result: 从400个种子家族开始，经过两轮演化扩展到953个家族和21,389个可验证实例（从5,718个）。在相同训练步数下，SSLogic演化数据相比种子基线在多个基准上获得一致提升：SynLogic +5.2，BBEH +1.4，AIME25 +3.0，Brumo25 +3.7。

Conclusion: SSLogic框架成功实现了任务家族级别的可扩展性，通过自动化合成和验证机制显著提升了RLVR的训练信号规模和质量。

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [46] [Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains](https://arxiv.org/abs/2602.13235)
*Yuqi Xiong,Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu*

Main category: cs.AI

TL;DR: Lang2Act提出通过自涌现语言工具链实现细粒度视觉感知与推理，替代传统VRAG的固定外部工具，使用两阶段RL训练框架提升VLM性能4%以上。


<details>
  <summary>Details</summary>
Motivation: 现有VRAG框架依赖预定义外部工具，将视觉感知与推理过程分离，导致视觉信息损失，特别是在图像裁剪等操作中。需要更灵活、细粒度的视觉感知增强方法。

Method: 提出Lang2Act框架：1）让VLM自探索高质量动作构建可重用语言工具箱；2）优化VLM利用这些语言工具进行下游推理。采用两阶段强化学习训练框架。

Result: 实验结果表明Lang2Act显著增强VLM的视觉感知能力，性能提升超过4%。代码和数据已开源。

Conclusion: Lang2Act通过自涌现语言工具链实现了更细粒度的视觉感知与推理，避免了传统VRAG框架的视觉信息损失问题，为VLM增强提供了新思路。

Abstract: Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.

</details>


### [47] [AST-PAC: AST-guided Membership Inference for Code](https://arxiv.org/abs/2602.13240)
*Roham Koohestani,Ali Al-Kaswan,Jonathan Katzy,Maliheh Izadi*

Main category: cs.AI

TL;DR: 该论文探索了代码大语言模型中的成员推理攻击方法，提出了基于抽象语法树的改进方法AST-PAC，用于检测未经授权的训练数据使用。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型常在受限许可的大规模代码数据集上训练，这带来了数据治理和版权挑战。成员推理攻击可作为审计机制检测模型中的未授权数据使用，但现有方法在代码领域效果有限。

Method: 研究评估了Loss Attack和Polarized Augment Calibration等方法在3B-7B参数代码模型上的效果。针对PAC方法在代码语法上的局限性，提出了AST-PAC方法，利用抽象语法树扰动生成语法有效的校准样本。

Result: PAC通常优于Loss基准，但其效果依赖于忽略代码严格语法的增强策略，导致在大型复杂文件上性能下降。AST-PAC在语法规模增大时表现改善，但在小文件和字母数字丰富代码上表现不足。

Conclusion: 研究发现需要语法感知和规模自适应的校准方法，作为代码语言模型可靠来源审计的前提条件，为未来工作提供了方向。

Abstract: Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.

</details>


### [48] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: DPBench是一个基于哲学家就餐问题的基准测试，用于评估LLM在资源竞争下的协调能力，发现LLM在顺序决策时表现良好，但在同时决策时死锁率超过95%


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多智能体系统中的部署增加，目前缺乏测试它们在资源竞争下协调能力的基准测试，需要评估LLM在并发资源访问场景中的协调表现

Method: 基于哲学家就餐问题设计DPBench基准测试，包含8种不同条件（决策时机、群体规模、通信），测试GPT-5.2、Claude Opus 4.5和Grok 4.1等LLM的协调能力

Result: LLM在顺序决策设置中能有效协调，但在需要同时决策时失败严重，死锁率在某些条件下超过95%；通信不仅不能解决问题，反而可能增加死锁率

Conclusion: 需要并发资源访问的多智能体LLM系统可能需要外部协调机制，而不能依赖LLM自发的协调能力；DPBench作为开源基准发布

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [49] [MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems](https://arxiv.org/abs/2602.13258)
*Deepak Babu Piskala*

Main category: cs.AI

TL;DR: MAPLE提出将LLM代理的记忆、学习和个性化分解为三个独立组件，通过专门化架构实现更好的用户适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理系统将记忆、学习和个性化视为统一能力，这种架构混淆限制了代理对个体用户的适应能力。需要将这些机制分解为独立组件，各自拥有不同基础设施、时间尺度和优化策略。

Method: 提出MAPLE框架，将系统分解为三个专用子代理：Memory负责存储和检索基础设施；Learning从累积交互中异步提取智能；Personalization在有限上下文预算内实时应用学习到的知识。每个组件都有专门工具和明确定义的接口。

Result: 在MAPLE-Personas基准测试中，该分解方法相比无状态基线实现了14.6%的个性化分数提升（p < 0.01, Cohen's d = 0.95），并将特征融入率从45%提高到75%。

Conclusion: 通过将记忆、学习和个性化分解为独立组件，MAPLE框架显著提升了LLM代理的个性化适应能力，使代理能够真正学习和适应用户。

Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.

</details>


### [50] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST通过代理强化学习让基础模型能够并行生成相同权重的克隆，在固定推理预算下优化计算效率，提升数学推理和长上下文问答的性能


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型需要更多测试时计算来提升性能，但串行推理或无协调的并行采样在固定推理预算下计算效率低下，需要更智能的计算资源分配方法

Method: 提出SELFCEST方法，通过代理强化学习训练基础模型，使其能够在并行上下文中生成相同权重的克隆，在全局任务奖励下进行端到端训练，学习控制器来分配生成和上下文预算

Result: 在具有挑战性的数学推理基准和长上下文多跳问答任务中，SELFCEST在匹配推理预算下相比单体基线提高了准确率-成本帕累托前沿，并在两个领域都表现出分布外泛化能力

Conclusion: SELFCEST通过智能并行计算分配机制，在固定推理预算下显著提升了语言模型的性能效率，为计算资源受限场景下的模型推理优化提供了有效解决方案

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [51] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench是一个多领域基准测试，用于评估模型在渐进丰富信息设置下的时序推理能力，揭示强预测性能不一定反映真正的时序理解或上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚强大的预测性能是否反映真正的时序理解能力，还是仅仅是在上下文和事件驱动条件下的推理能力。需要区分模型是真正理解时间模式，还是仅仅依赖统计模式。

Method: 提出TemporalBench基准测试，采用四层任务分类法：历史结构解释、无上下文预测、上下文时序推理、事件条件预测，覆盖零售、医疗、能源和物理系统四个真实领域。通过控制对未来目标和上下文信息的访问，进行诊断分析。

Result: 实验显示强大的数值预测准确性并不能可靠地转化为鲁棒的上下文或事件感知时序推理能力。现有代理框架表现出分散的优势和系统性的失败模式，这些在仅关注预测的基准测试中基本被隐藏。

Conclusion: 需要超越传统预测基准，开发能够真正理解时间模式、对齐外部上下文并在条件变化时适应预测的时序推理模型。TemporalBench为这一目标提供了诊断工具。

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [52] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench是一个统一的基准测试，评估11种提示范式在4个LLM家族中的表现，使用ETHICS、Scruples、WildJailbreak和新的ETHICS-Contrast测试，通过UMSS指标平衡准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 提示设计显著影响大语言模型的道德能力和安全对齐，但现有实证比较在不同数据集和模型之间是碎片化的，需要统一的评估框架。

Method: 引入ProMoral-Bench基准，评估11种提示范式在4个LLM家族中的表现，使用ETHICS、Scruples、WildJailbreak和新的ETHICS-Contrast测试，提出统一道德安全评分（UMSS）作为评估指标。

Result: 紧凑的示例引导支架优于复杂的多阶段推理，提供更高的UMSS分数和更强的鲁棒性，且token成本更低。多轮推理在扰动下脆弱，而少样本示例能持续增强道德稳定性和越狱抵抗能力。

Conclusion: ProMoral-Bench为原则性、成本效益高的提示工程建立了标准化框架，表明简洁的示例引导方法在道德安全对齐方面更有效。

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [53] [AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines](https://arxiv.org/abs/2602.14296)
*Yifan Wu,Yiran Peng,Yiyu Chen,Jianhao Ruan,Zijie Zhuang,Cheng Yang,Jiayi Zhang,Man Chen,Yenchi Tseng,Zhaoyang Yu,Liang Chen,Yuyao Zhai,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: AutoWebWorld：通过将网页建模为有限状态机并用代码智能体生成交互式网站，实现可控可验证的网页环境合成，大幅降低高质量训练数据收集成本。


<details>
  <summary>Details</summary>
Motivation: 自主网页GUI智能体的性能依赖于高质量训练数据，但收集真实网站交互轨迹成本高且难以验证，状态转换隐含导致依赖不一致的外部验证器。

Method: 将网页环境建模为有限状态机（FSM），使用代码智能体将FSM转换为交互式网站，实现显式定义所有状态、动作和转换规则，支持程序化验证。

Result: 以每条轨迹0.04美元的成本生成11,663条已验证轨迹，训练7B网页GUI智能体在WebVoyager上15步内超越所有基线，观察到合成数据量与WebVoyager、Online-Mind2Web性能呈明确缩放规律。

Conclusion: AutoWebWorld通过合成可控可验证的网页环境解决了高质量训练数据收集难题，显著提升真实世界性能，且数据量与性能呈缩放规律。

Abstract: The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.

</details>


### [54] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: 提出了Mirror框架，通过AI代理辅助伦理审查，包含EthicsLLM基础模型和两种工作模式，显著提升了伦理评估的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现代研究伦理审查系统面临大规模跨学科科学实践带来的结构性伦理风险压力，现有制度审查能力有限，而大语言模型直接应用存在伦理推理能力不足、与监管结构整合弱、真实审查材料隐私限制等问题。

Method: 开发Mirror代理框架，核心是EthicsLLM基础模型（在EthicsQA数据集上微调），支持两种模式：Mirror-ER通过可执行规则库自动化快速审查，Mirror-CR通过专家代理、伦理秘书代理和首席研究员代理的协调交互模拟完整委员会审议。

Result: 实证评估表明，Mirror相比强大的通用大语言模型，显著提高了伦理评估的质量、一致性和专业性。

Conclusion: Mirror框架为AI辅助伦理审查提供了有效解决方案，通过整合伦理推理、结构化规则解释和多代理审议，能够支持快速审查和完整委员会评估两种模式。

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [55] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: ReusStdFlow框架通过"提取-存储-构建"范式解决企业Agentic AI中的可重用性困境和结构幻觉问题，将异构DSL解构为标准模块化工作流片段，使用图+向量双知识架构，基于RAG策略智能组装工作流，在200个真实n8n工作流上实现超90%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决企业Agentic AI中的"可重用性困境"和结构幻觉问题，企业数字资产通常以异构、平台特定的领域特定语言(DSL)形式存在，难以标准化和重用。

Method: 提出ReusStdFlow框架，采用"提取-存储-构建"范式：1) 将异构DSL解构为标准模块化工作流片段；2) 使用图数据库+向量数据库的双知识架构协同检索拓扑结构和功能语义；3) 基于检索增强生成(RAG)策略智能组装工作流。

Result: 在200个真实世界的n8n工作流上进行测试，系统在提取和构建两方面都实现了超过90%的准确率。

Conclusion: 该框架为企业数字资产的自动化重组和高效重用提供了标准化解决方案。

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [56] [Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol](https://arxiv.org/abs/2602.13320)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出首个分析MCP代理中错误累积的理论框架，证明累积失真呈线性增长且高概率偏差有界，为可信代理系统提供部署原则


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的AI代理越来越多地使用外部工具进行高风险决策，需要理解错误如何在顺序工具调用中传播以确保系统可靠性

Method: 开发混合失真度量（结合离散事实匹配和连续语义相似度），建立顺序工具交互中错误传播的鞅集中界理论框架

Result: 理论证明累积失真呈线性增长，高概率偏差有界于O(√T)；实验验证失真跟踪线性趋势，语义加权减少80%失真，每约9步重新接地可控制错误

Conclusion: 错误传播的集中性保证了可预测的系统行为，排除了指数级故障模式，为可信代理系统提供了可操作的部署原则

Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context Protocol (MCP) agents, proving that cumulative distortion exhibits linear growth and high-probability deviations bounded by $O(\sqrt{T})$. This concentration property ensures predictable system behavior and rules out exponential failure modes. We develop a hybrid distortion metric combining discrete fact matching with continuous semantic similarity, then establish martingale concentration bounds on error propagation through sequential tool interactions. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate our theoretical predictions, showing empirical distortion tracks the linear trend with deviations consistently within $O(\sqrt{T})$ envelopes. Key findings include: semantic weighting reduces distortion by 80\%, and periodic re-grounding approximately every 9 steps suffices for error control. We translate these concentration guarantees into actionable deployment principles for trustworthy agent systems.

</details>


### [57] [Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts](https://arxiv.org/abs/2602.13367)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Xiyun Xu,Yang Song,Yiming Jia,Yuntao Wen,Yunzhi Xu,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.AI

TL;DR: Nanbeige4.1-3B是一个仅30亿参数的多功能语言模型，首次在开源小模型中同时实现强大的代理行为、代码生成和通用推理能力，性能超越同类甚至更大规模模型。


<details>
  <summary>Details</summary>
Motivation: 当前小规模语言模型(SLM)通常难以同时具备多种能力，要么专注于特定任务，要么能力有限。作者希望开发一个仅30亿参数但能同时实现代理行为、代码生成和通用推理的统一模型，重新定义小模型的潜力。

Method: 1. 结合点对点和配对奖励建模提升推理和偏好对齐；2. 为代码生成设计复杂度感知的强化学习奖励，优化正确性和效率；3. 深度搜索中进行复杂数据合成，并在训练中融入回合级监督；4. 实现稳定的长时程工具交互，支持多达600个工具调用回合。

Result: Nanbeige4.1-3B显著超越同规模模型(Nanbeige4-3B-2511和Qwen3-4B)，甚至在某些方面优于更大模型(Qwen3-30B-A3B)。模型能可靠执行长达600个工具调用回合的复杂问题解决，展示了小模型同时具备广泛能力和强专业化的潜力。

Conclusion: 该研究表明小模型能够同时实现广泛能力和强专业化，重新定义了30亿参数模型的潜力。通过创新的训练方法和架构设计，小规模语言模型也能在多任务场景中表现出色。

Abstract: We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.

</details>


### [58] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: 该论文提出了Morality Chains形式化方法和MoralityGym基准，用于评估AI代理在冲突性、层次化人类规范中的道德对齐能力。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在冲突性、层次化人类规范中的道德对齐是AI安全、道德哲学和认知科学交叉领域的关键挑战。现有方法缺乏对复杂道德决策的系统评估框架。

Method: 1. 提出Morality Chains形式化方法，将道德规范表示为有序的道义约束；2. 创建MoralityGym基准，包含98个伦理困境问题，以电车困境风格的Gymnasium环境呈现；3. 将任务解决与道德评估解耦；4. 引入新的道德度量标准。

Result: 使用安全强化学习方法进行基线测试，揭示了现有方法在道德决策方面的关键局限性，表明需要更原则性的伦理决策方法。

Conclusion: 该工作为开发在复杂现实世界中行为更可靠、透明和道德的AI系统奠定了基础，将心理学和哲学洞见整合到规范敏感推理的评估中。

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [59] [On-Policy Supervised Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.13407)
*Anhao Zhao,Ziyang Chen,Junlong Tong,Yingqi Fan,Fanghua Ye,Shuhao Li,Yunpu Ma,Wenjie Li,Xiaoyu Shen*

Main category: cs.AI

TL;DR: 提出on-policy SFT方法，通过简化奖励设计（仅使用截断式长度惩罚）将多目标RL优化问题转化为在自生成数据上的监督微调，在保持准确率的同时大幅缩短推理链长度并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型通常使用强化学习训练，但添加多奖励目标（如同时优化正确性和简洁性）会使训练复杂化、不稳定且产生次优权衡。作者质疑这种复杂性的必要性，希望通过简化方法获得更好的效果。

Method: 通过原理分析发现KL正则化和组归一化在多奖励场景下存在根本性错位问题。移除这些复杂组件，将奖励简化为基于截断的长度惩罚，将优化问题转化为在自生成数据（同时满足正确性和简洁性）上的监督微调，称为on-policy SFT。

Result: 在五个基准测试中，on-policy SFT始终定义准确率-效率的帕累托前沿，推理链长度减少高达80%的同时保持原始准确率，优于复杂的RL方法。训练效率显著提升：GPU内存使用减少50%，收敛速度加快70%。

Conclusion: 研究表明，通过简化奖励设计和移除不必要的复杂组件，可以将多目标RL优化问题有效转化为监督微调，在保持性能的同时大幅提升训练效率和推理简洁性。

Abstract: Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.

</details>


### [60] [OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage](https://arxiv.org/abs/2602.13477)
*Akshat Naik,Jay Culligan,Yarin Gal,Philip Torr,Rahaf Aljundi,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 论文研究了多智能体系统中流行的编排器架构的安全漏洞，发现了一种名为OMNI-LEAK的新型攻击向量，能够通过单次间接提示注入泄露敏感数据，即使存在数据访问控制机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体能力增强，多智能体系统将成为实用范式。先前研究主要关注单智能体安全风险，缺乏对多智能体系统的威胁建模，特别是在存在基本工程防护措施（如访问控制）的情况下。

Method: 通过红队测试一个代表未来可能用例的具体编排器设置，研究流行的编排器模式（中心智能体分解和委派任务给专业智能体）的安全漏洞。测试前沿模型对不同类别攻击的易感性。

Result: 发现了OMNI-LEAK攻击向量，能够通过单次间接提示注入泄露敏感数据，即使存在数据访问控制。发现推理和非推理模型都易受攻击，即使攻击者缺乏实现细节的内部知识。

Conclusion: 安全研究需要从单智能体扩展到多智能体设置，以减少现实世界隐私泄露、财务损失的风险，维护公众对AI智能体的信任。

Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.

</details>


### [61] [SPILLage: Agentic Oversharing on the Web](https://arxiv.org/abs/2602.13516)
*Jaechul Roh,Eugene Bagdasarian,Hamed Haddadi,Ali Shahin Shamsabadi*

Main category: cs.AI

TL;DR: 该论文提出了"自然代理过度分享"概念，即网络代理在完成任务时无意中泄露用户任务无关信息，并引入SPILLage框架从内容和行为两个维度分析过度分享现象。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的代理开始在开放网络上自动化用户任务，它们经常访问用户资源（如电子邮件和日历）。与受控聊天机器人环境不同，网络代理在"野外"行动，与第三方互动并留下行动痕迹。因此需要研究网络代理在完成任务时如何处理用户资源，特别是如何防止无意中泄露任务无关的用户信息。

Method: 引入SPILLage框架，从渠道（内容vs行为）和直接性（显式vs隐式）两个维度表征过度分享。在实时电商网站上对180个任务进行基准测试，使用真实标注区分任务相关和任务无关属性。进行了1,080次运行，涵盖两个代理框架和三个骨干LLM，并评估了提示级缓解措施的效果。

Result: 过度分享现象普遍存在，行为过度分享比内容过度分享多5倍。提示级缓解措施下这种现象持续存在甚至恶化。然而，在执行前移除任务无关信息可将任务成功率提高高达17.9%，表明减少过度分享能提高任务成功率。

Conclusion: 保护网络代理中的隐私是一个根本性挑战，需要更广泛的"输出"视角，不仅要考虑代理输入什么，还要考虑它们在网络上的行为。减少过度分享不仅能保护隐私，还能提高任务成功率。

Abstract: LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act "in the wild", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of "output" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.

</details>


### [62] [REMem: Reasoning with Episodic Memory in Language Agent](https://arxiv.org/abs/2602.13530)
*Yiheng Shu,Saisri Padmaja Jonnalagedda,Xiang Gao,Bernal Jiménez Gutiérrez,Weijian Qi,Kamalika Das,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: REMem是一个两阶段框架，通过构建混合记忆图并利用智能检索器进行迭代检索，显著提升了语言智能体在情景记忆方面的表现，在多个基准测试中优于现有记忆系统。


<details>
  <summary>Details</summary>
Motivation: 人类擅长在时空背景下记忆具体经历并进行跨事件推理（情景记忆能力），而当前语言智能体的记忆主要是语义性的，无法有效回忆和推理交互历史。现有工作往往忽视情景性、缺乏显式事件建模，或过度强调简单检索而非复杂推理。

Method: REMem采用两阶段框架：1）离线索引阶段，将经验转换为混合记忆图，灵活链接时间感知的要点和事实；2）在线推理阶段，使用智能检索器配合精心设计的工具，在记忆图上进行迭代检索。

Result: 在四个情景记忆基准测试中，REMem显著优于Mem0和HippoRAG 2等最先进的记忆系统，在情景回忆和推理任务上分别实现了3.4%和13.4%的绝对提升。此外，REMem对不可回答的问题表现出更稳健的拒绝行为。

Conclusion: REMem通过显式建模情景记忆和复杂推理能力，有效解决了语言智能体在情景记忆方面的局限性，为构建更接近人类记忆能力的智能体提供了有前景的方向。

Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.

</details>


### [63] [OpAgent: Operator Agent for Web Navigation](https://arxiv.org/abs/2602.13559)
*Yuyu Guo,Wenjie Yang,Siyuan Yang,Ziyang Liu,Cheng Chen,Yuan Wei,Yun Hu,Yang Huang,Guoliang Hao,Dongsheng Yuan,Jianming Wang,Xin Chen,Hang Yu,Lei Lei,Peng Di*

Main category: cs.AI

TL;DR: 提出一个在线强化学习WebAgent，通过分层多任务微调、在线强化学习训练和模块化操作代理框架，在WebArena上实现71.6%的SOTA成功率。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态数据集的监督微调或离线强化学习方法存在严重分布偏移问题，无法适应真实网站的复杂性和动态性，需要能够直接与无约束网站交互的在线强化学习方案。

Method: 1) 分层多任务微调：按规划、执行、基础三类功能原语整理数据集，训练视觉语言模型；2) 在线强化学习：开发在线交互环境，使用混合奖励机制（WebJudge整体评估+RDT进度奖励）；3) 操作代理框架：包含规划器、基础器、反思器、总结器的模块化系统。

Result: 强化学习增强模型在WebArena上达到38.1%成功率（pass@5），优于所有现有单体基线；模块化OpAgent框架进一步提升到71.6%的SOTA成功率。

Conclusion: 在线强化学习与模块化代理框架的结合能有效解决真实网站任务的复杂性和动态性，显著提升WebAgent的性能和鲁棒性。

Abstract: To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \textbf{71.6\%}.

</details>


### [64] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: LLMs在决策任务中表现出对人类专家反馈的显著顺从，即使专家意见错误，这种顺从也强于对其他LLM反馈的响应，显示出类似人类的信誉敏感社会影响模式。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在遇到社会信息（如其他代理的回答、工具输出或人类建议）时是否表现出类似人类的社会影响模式，特别是是否更倾向于接受人类专家的反馈而非其他LLM的反馈。

Method: 通过三个二元决策任务（阅读理解、多步推理、道德判断），向四个指令调优的LLM呈现标注为朋友、人类专家或其他LLM来源的先前回答。操纵群体正确性并改变群体规模。第二个实验引入单个人类与单个LLM之间的直接分歧。

Result: 在所有任务中，模型显著更倾向于顺从标注为人类专家的回答，即使该信号是错误的，并且向专家反馈修正答案的意愿强于向其他LLM反馈。专家框架作为当代LLMs的强先验。

Conclusion: LLMs表现出一种信誉敏感的社会影响形式，这种影响在不同决策领域中具有普遍性，专家框架对LLMs产生强烈影响，类似于人类的社会影响模式。

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [65] [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)
*Yi Li,Lianjie Cao,Faraz Ahmed,Puneet Sharma,Bingzhe Li*

Main category: cs.AI

TL;DR: Hippocampus是一个代理AI记忆管理系统，使用紧凑二进制签名进行语义搜索，通过动态小波矩阵压缩和共同索引，实现超快速搜索，显著降低检索延迟和存储开销。


<details>
  <summary>Details</summary>
Motivation: 代理AI需要持久记忆来存储超出LLM有限上下文窗口的用户特定历史。现有记忆系统使用密集向量数据库或知识图谱遍历（或混合），存在高检索延迟和存储扩展性差的问题。

Method: 引入Hippocampus系统，使用紧凑二进制签名进行语义搜索，无损令牌ID流进行精确内容重建。核心是动态小波矩阵（DWM），压缩和共同索引两个流，支持压缩域中的超快速搜索，避免昂贵的密集向量或图计算。

Result: 评估显示Hippocampus将端到端检索延迟降低高达31倍，将每查询令牌占用减少高达14倍，同时在LoCoMo和LongMemEval基准测试中保持准确性。

Conclusion: Hippocampus的设计随记忆大小线性扩展，适用于长视野代理部署，提供高效、可扩展的记忆管理解决方案。

Abstract: Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\times$ and cuts per-query token footprint by up to 14$\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.

</details>


### [66] [Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval](https://arxiv.org/abs/2602.13639)
*Linlin Wang,Tianqing Zhu,Laiqiao Qin,Longxiang Gao,Wanlei Zhou*

Main category: cs.AI

TL;DR: 论文提出了一种基于熵的自适应指导框架，用于解决异构多智能体系统中认知不匹配问题，通过动态调整指导强度来提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 异构多智能体系统中，强模型和弱模型之间的能力差异导致认知不匹配问题，使得强-弱协作效果不佳，甚至不如弱-弱组合。需要解决这种认知失衡来提升异构协作效果。

Method: 提出基于熵的自适应指导框架，通过多维度熵度量（表达、不确定性、结构、连贯性、相关性）量化弱智能体的理解程度，动态调整指导强度（轻度、中度、重度）。结合检索增强生成机制保留成功协作经验。

Result: 在GSM8K、MBPP和CVRP三个基准数据集上的实验表明，该方法能持续提升异构协作的效果和稳定性，自适应指导不仅缓解了认知失衡，还建立了更稳健、可扩展的多智能体协作路径。

Conclusion: 异构多智能体系统中的认知不匹配是关键瓶颈，基于熵的自适应指导框架能有效解决这一问题，为更稳健、协作的多智能体智能系统提供了可扩展的解决方案。

Abstract: With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.

</details>


### [67] [HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating](https://arxiv.org/abs/2602.13665)
*Weibin Liao,Jian-guang Lou,Haoyi Xiong*

Main category: cs.AI

TL;DR: HyFunc是一个消除AI代理系统中计算冗余的框架，通过混合模型级联和动态模板技术，显著降低推理延迟同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI代理系统存在三种计算冗余：每次请求都处理大量函数描述、使用大模型生成整个可预测的token序列、生成固定的样板参数语法，这些导致高推理延迟，阻碍实时应用。

Method: HyFunc采用混合模型级联：大模型将用户意图提炼为单个"软token"，指导轻量级检索器选择相关函数，并引导较小的前缀调优模型生成最终调用。使用"动态模板"技术在扩展的vLLM引擎中动态注入样板参数语法。

Result: 在未见过的BFCL基准数据集上，HyFunc达到0.828秒的推理延迟，优于所有基线模型；性能达到80.1%，超过所有参数规模相当的模型。

Conclusion: HyFunc在效率和性能之间取得了良好平衡，为AI代理系统提供了更高效的范式。

Abstract: While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single "soft token." This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our "dynamic templating" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.

</details>


### [68] [PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning](https://arxiv.org/abs/2602.13691)
*Yu Li,Guangfeng Cai,Shengtian Yang,Han Luo,Shuo Han,Xu He,Dong Li,Lei Feng*

Main category: cs.AI

TL;DR: PhGPO提出了一种基于信息素引导的策略优化方法，通过从历史成功轨迹中学习工具转换模式来改进LLM代理的长时程工具规划能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在复杂任务的多步骤工具规划中面临组合爆炸问题，即使找到正确的工具使用路径，也仅作为即时奖励，无法为后续训练提供可复用的信息。

Method: 受蚁群优化启发，从历史成功轨迹中学习轨迹级的工具转换模式（信息素），然后使用学习到的信息素来引导策略优化，为工具规划提供显式且可复用的指导。

Result: 综合实验结果表明PhGPO方法的有效性，能够改进长时程工具规划。

Conclusion: 历史成功轨迹包含可复用的工具转换模式，通过信息素引导的策略优化可以显著提升LLM代理的长时程工具规划能力。

Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.

</details>


### [69] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent是一个可配置的多智能体研究框架，用于在复杂实验环境中进行自动化科学探索，通过树形工作流管理假设生成和系统回溯，结合进化选择、分层反思和记忆压缩机制，在组合优化和协同驾驶场景中超越进化基线。


<details>
  <summary>Details</summary>
Motivation: 自动化科学发现在复杂实验驱动领域需要超越简单的程序迭代变异，需要结构化的假设管理、环境交互和有原则的反思机制。

Method: 提出OR-Agent框架：1) 基于树形工作流组织研究，显式建模分支假设生成和系统回溯；2) 进化-系统构思机制，统一进化选择研究起点、全面研究计划生成和协调探索；3) 分层优化启发式反思系统，包括短期实验反思（语言梯度）、长期反思（语言动量）和记忆压缩（正则化机制）。

Result: 在经典组合优化基准（旅行商、容量车辆路径、装箱、定向、多重背包问题）和基于模拟的协同驾驶场景中，OR-Agent超越了强大的进化基线，同时提供了一个通用、可扩展和可检查的AI辅助科学发现框架。

Conclusion: OR-Agent通过结构化的研究管理、进化-系统构思机制和分层反思系统，为自动化科学探索提供了一个有原则的架构，在复杂实验环境中表现出色，代码和数据已开源。

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [70] [From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents](https://arxiv.org/abs/2602.13855)
*Razeen A Rasheed,Somnath Banerjee,Animesh Mukherjee,Rima Hazra*

Main category: cs.AI

TL;DR: 论文提出随着AI研究生成变得廉价，可审计性成为瓶颈，主张将声明级可审计性作为深度研究代理的核心设计目标，并引入AAR标准和语义溯源框架来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理能快速生成科学报告，但验证成本高昂。随着研究生成变得廉价，主要风险从孤立事实错误转向声明-证据链接薄弱、缺失或误导的科学风格输出，可审计性成为瓶颈。

Method: 提出声明级可审计性作为核心设计目标，识别长期失败模式（目标漂移、瞬态约束、不可验证推理），引入AAR标准（溯源覆盖度、溯源健全性、矛盾透明度、审计工作量），并主张语义溯源与协议化验证。

Result: 建立了可审计性作为可测试指标的框架，提出了具体的测量维度（AAR标准），并设计了语义溯源系统架构，支持在合成过程中而非发布后进行持续验证。

Conclusion: 随着AI研究生成成本降低，可审计性成为关键瓶颈，需要将声明级可审计性作为首要设计目标，通过AAR标准和语义溯源框架实现可测试的审计能力。

Abstract: A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.

</details>


### [71] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: 提出MOC-2HER方法，通过双重目标回放机制解决稀疏奖励多目标环境中分层强化学习的性能问题，在机器人操作任务中达到90%成功率


<details>
  <summary>Details</summary>
Motivation: 现有分层强化学习方法（如Option-Critic和MOC）在稀疏奖励的多目标环境中表现不佳，特别是在需要将动作与时间上遥远结果关联的场景中。对于物体操作任务，奖励依赖于物体到达目标而非智能体直接交互，这使得HRL智能体难以发现如何与物体交互。

Method: 首先提出MOC-HER，将Hindsight Experience Replay机制集成到MOC框架中。然后引入Dual Objectives Hindsight Experience Replay (2HER)，创建两组虚拟目标：除了基于物体最终状态重新标记目标（标准HER），还从智能体效应器位置生成目标，奖励智能体既与物体交互又完成任务。

Result: 在机器人操作环境中，MOC-2HER达到高达90%的成功率，而MOC和MOC-HER的成功率均低于11%。双重目标重新标记策略在稀疏奖励多目标任务中表现出显著效果。

Conclusion: 提出的双重目标回放机制有效解决了分层强化学习在稀疏奖励多目标环境中的性能瓶颈，特别是在物体操作任务中，通过同时奖励智能体交互行为和任务完成，显著提升了学习效率和成功率。

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [72] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: 该论文提出了一套简单、计算成本低且任务无关的指标，用于检测和区分思维链推理中的三种病理模式：事后合理化、编码推理和内化推理。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是现代LLM架构的基础，也是AI安全的关键干预点。然而，思维链推理可能存在病理模式，这些病理会妨碍其监控有效性。先前研究已识别出三种病理模式，但缺乏有效的检测和区分方法。

Method: 1. 创建了一套具体的指标来理解和区分三种思维链病理模式；2. 这些指标简单易实现、计算成本低且任务无关；3. 为了验证方法，专门训练了能故意展示特定病理模式的模型生物；4. 开发了实用的工具包来评估思维链病理。

Result: 开发出了能够有效检测和区分三种思维链病理模式的指标和工具包，这些工具可以直接应用于训练时监控。

Conclusion: 该研究为评估思维链推理的病理模式提供了实用的工具包，对训练时监控具有直接意义，有助于提高LLM推理的安全性和可解释性。

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [73] [HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling](https://arxiv.org/abs/2602.13933)
*Xiaochen Zhao,Kaikai Wang,Xiaowen Zhang,Chen Yao,Aili Wang*

Main category: cs.AI

TL;DR: HyMem提出了一种混合内存架构，通过多粒度内存表示实现动态按需调度，在长对话中平衡效率与性能，相比全上下文减少92.6%计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM代理在短文本中表现良好，但在长对话中因内存管理效率低下而表现不佳。现有方法面临效率与效果的根本权衡：内存压缩可能丢失复杂推理所需的关键细节，而保留原始文本则引入不必要的计算开销。

Method: HyMem采用双粒度存储方案配合动态两级检索系统：轻量级模块构建摘要级上下文用于高效响应生成，而基于LLM的深度模块仅针对复杂查询选择性激活，并通过反思机制进行迭代推理优化。

Result: 在LOCOMO和LongMemEval基准测试中表现出色，优于全上下文方法，同时减少92.6%的计算成本，在长期内存管理中实现了效率与性能的最优平衡。

Conclusion: HyMem通过混合内存架构和多粒度表示，成功解决了LLM代理在长对话中的内存管理挑战，实现了效率与性能的平衡，为长期记忆管理建立了新的技术标准。

Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

</details>


### [74] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: 提出两种基于统计原理的早期停止方法，通过监控生成过程中的不确定性信号来减少LLM在推理任务中的过度思考问题


<details>
  <summary>Details</summary>
Motivation: LLM在推理能力上虽有显著提升，但有时会过度思考，特别是在面对不确定、定义不清或模糊查询时，生成不必要的推理步骤，影响效率和可靠性

Method: 提出两种统计原理的早期停止方法：1）参数化方法：将不确定性关键词的到达时间建模为更新过程，应用序列测试进行停止决策；2）非参数化方法：为定义良好的查询提供有限样本保证，确保不会过早停止

Result: 实验评估表明，基于不确定性的早期停止方法能提高LLM推理的效率和可靠性，尤其在数学推理任务上观察到显著改进

Conclusion: 通过监控不确定性信号实施早期停止是解决LLM过度思考问题的有效策略，能同时提升推理效率和可靠性

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [75] [FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning](https://arxiv.org/abs/2602.14035)
*Jinzi Zou,Bolin Wang,Liang Li,Shuo Zhang,Nuo Xu,Junzhou Zhao*

Main category: cs.AI

TL;DR: FloCA是一个零样本流程图导向对话代理，使用LLM进行意图理解和响应生成，同时将流程图推理委托给外部工具执行拓扑约束的图执行，确保跨对话轮次的忠实和逻辑一致的节点转换。


<details>
  <summary>Details</summary>
Motivation: 流程图导向对话系统需要引导用户通过多轮决策或操作流程，但现有LLM方法存在两个限制：1) 缺乏明确的机制来表示和推理流程图拓扑结构；2) 容易产生幻觉，导致不忠实的流程图推理。

Method: 提出FloCA框架，将LLM用于意图理解和响应生成，同时将流程图推理委托给外部工具执行拓扑约束的图执行，确保节点转换的忠实性和逻辑一致性。

Result: 在FLODIAL和PFDial数据集上的广泛实验突出了现有LLM方法的瓶颈，并证明了FloCA的优越性。

Conclusion: FloCA通过分离意图理解和流程图推理，解决了LLM在流程图导向对话中的局限性，实现了更忠实和逻辑一致的推理。

Abstract: Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.

</details>


### [76] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: FluxMem是一个自适应记忆组织框架，为LLM智能体提供多种记忆结构，通过学习基于交互特征选择合适结构，并引入三级记忆层次和概率门控机制，在长时程交互中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统存在两个关键问题：采用一刀切的记忆结构，且未将记忆结构选择建模为上下文自适应决策，这限制了处理异构交互模式的能力并导致性能不佳。

Method: 提出FluxMem统一框架，为智能体配备多种互补记忆结构，基于交互级特征学习选择合适结构（使用下游响应质量和记忆利用率的离线监督）。引入三级记忆层次和基于Beta混合模型的概率门控进行分布感知记忆融合，替代脆弱的相似度阈值。

Result: 在两个长时程基准测试PERSONAMEM和LoCoMo上，平均分别提升9.18%和6.14%。

Conclusion: FluxMem通过自适应记忆组织和概率门控机制，显著提升了LLM智能体在长时程交互中的记忆管理能力，解决了现有记忆系统的局限性。

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [77] [GUI-GENESIS: Automated Synthesis of Efficient Environments with Verifiable Rewards for GUI Agent Post-Training](https://arxiv.org/abs/2602.14093)
*Yuan Cao,Dezhi Ran,Mengzhou Wu,Yuzhe Guo,Xin Chen,Ang Li,Gang Cao,Gong Zhi,Hao Yu,Linyi Li,Wei Yang,Tao Xie*

Main category: cs.AI

TL;DR: GUI-GENESIS框架自动合成高效GUI训练环境，将真实应用重构为轻量级Web环境，使用代码原生奖励，显著降低延迟和成本，提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 在交互环境中训练GUI代理对发展泛化和长程规划能力至关重要，但在真实应用上训练存在高延迟、可复现性差、依赖噪声视觉代理的不可验证奖励等问题。

Method: 使用多模态代码模型将真实应用重构为轻量级Web环境，并配备代码原生奖励（可执行断言），提供确定性奖励信号，消除视觉估计噪声。

Result: GUI-GENESIS将环境延迟降低10倍，每轮训练成本节省超过28,000美元。训练出的代理在真实任务上比基础模型提升14.54%，比真实世界RL基线提升3.27%。

Conclusion: GUI-GENESIS为GUI代理训练提供了高效、低成本、可验证的解决方案，同时发现模型可以合成自己尚无法解决的环境，为自我改进代理开辟了路径。

Abstract: Post-training GUI agents in interactive environments is critical for developing generalization and long-horizon planning capabilities. However, training on real-world applications is hindered by high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies. To address the limitations, we present GUI-GENESIS, the first framework to automatically synthesize efficient GUI training environments with verifiable rewards. GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards, executable assertions that provide deterministic reward signals and eliminate visual estimation noise. Extensive experiments show that GUI-GENESIS reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Notably, agents trained with GUI-GENESIS outperform the base model by 14.54% and even real-world RL baselines by 3.27% on held-out real-world tasks. Finally, we observe that models can synthesize environments they cannot yet solve, highlighting a pathway for self-improving agents.

</details>


### [78] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: 论文系统评估了28个LLM模型的隐写推理能力，发现当前模型在复杂数学任务上尚无法维持隐藏推理，但在简化计数任务中Claude Opus 4.5达到92%准确率，揭示了潜在的隐写风险。


<details>
  <summary>Details</summary>
Motivation: 监控思维链推理是LLM代理的基础安全技术，但如果模型学会隐藏推理过程，这种监控就会失效。研究旨在评估隐写思维链的潜在风险，为风险评估和部署策略提供依据。

Method: 系统评估28个模型（从过去代际到当前前沿）的隐写能力，测量监控规避、拒绝率、编码保真度和隐藏任务准确性，在四个数据集上比较隐写藏头诗与普通推理和填充标记基线。

Result: 当前模型无法在复杂数学和算术任务中维持隐藏推理。但在简化计数实验中，Claude Opus 4.5在隐藏任务上达到92%准确率。GPT-5.2在罕见情况下（<1%）可能同时拒绝和遵守隐写指令。

Conclusion: 研究强调了持续评估隐写风险的必要性，提供了预先检测和预防隐藏推理的方法论，这些隐藏推理可能助长未对齐的阴谋和欺骗行为。

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [79] [ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI](https://arxiv.org/abs/2602.14135)
*Haibo Tong,Feifei Zhao,Linghao Feng,Ruoyu Wu,Ruolin Chen,Lu Jia,Zhou Zhao,Jindong Li,Tenglong Li,Erliang Lin,Shuai Yang,Enmeng Lu,Yinqian Sun,Qian Zhang,Zizhe Ruan,Zeyang Yue,Ping Wu,Huangrui Li,Chengyi Sun,Yi Zeng*

Main category: cs.AI

TL;DR: 提出了一个名为"ForesightSafety Bench"的全面AI安全评估框架，涵盖94个风险维度，对20多个主流大模型进行系统评估，揭示了前沿AI在多个安全支柱上的广泛漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全评估系统存在关键限制：风险维度受限、前沿风险检测失败。滞后的安全基准和对齐技术难以应对前沿AI模型带来的复杂挑战。

Method: 提出ForesightSafety Bench框架，从7个基础安全支柱扩展到高级具身AI安全、AI4Science安全、社会与环境AI风险、灾难性和存在性风险，以及8个关键工业安全领域，形成94个细化风险维度。

Result: 积累了数万个结构化风险数据和评估结果，对20多个主流先进大模型进行系统评估，识别出关键风险模式和能力边界。发现前沿AI在多个支柱上存在广泛安全漏洞，特别是在风险性自主代理、AI4Science安全、具身AI安全、社会AI安全以及灾难性和存在性风险方面。

Conclusion: 建立了一个广泛涵盖、层次清晰、动态演进的AI安全评估框架，为应对前沿AI带来的复杂安全挑战提供了系统性的评估工具。

Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.

</details>


### [80] [Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning](https://arxiv.org/abs/2602.14160)
*Chaeeun Lee,T. Michael Yates,Pasquale Minervini,T. Ian Simpson*

Main category: cs.AI

TL;DR: 提出一种基于工具代理的强化学习框架，用于基因-疾病有效性评估任务，通过过程级监督确保推理遵循临床标准路径，同时通过分层多智能体系统实现高效协调。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要基于异构证据进行细致推理并提供可追溯的论证。现有LLM多智能体系统主要优化结果准确性，但忽略了符合临床标准的过程基础推理。基因-疾病有效性评估是这一问题的典型案例，专家需要综合多样生物医学证据来确定基因是否与疾病有因果关系。

Method: 引入工具代理强化学习框架，包含两个目标：(1)过程级监督确保推理遵循有效临床路径；(2)通过分层多智能体系统实现高效协调。在ClinGen数据集上评估，比较仅结果奖励与过程+结果奖励两种设置下的性能。

Result: 仅使用结果奖励时，MAS与GRPO训练的Qwen3-4B监督代理将最终结果准确率从基础模型的0.195提升到0.732，但过程对齐性较差（0.392 F1）。使用过程+结果奖励时，MAS与GRPO训练监督代理获得更高的结果准确率（0.750），同时显著提高过程保真度至0.520 F1。

Conclusion: 过程级监督对于确保临床推理符合标准路径至关重要，结合过程+结果奖励的多智能体系统能在保持高准确率的同时显著改善过程对齐性，为临床决策支持系统提供了更可靠的解决方案。

Abstract: Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.

</details>


### [81] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: 该论文提出了一种分阶段知识注入方法，通过地球科学文本QA预训练来引导超高分辨率遥感图像的视觉推理，显著提升了多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感图像的多模态推理面临视觉证据获取的瓶颈：需要在巨大的像素空间中定位微小的任务相关区域。虽然基于缩放工具的强化学习提供了一条路径，但标准强化学习在没有结构化领域先验的情况下难以在这些广阔视觉空间中导航。

Method: 提出分阶段知识注入方法：1）使用可扩展的知识图谱验证的地球科学文本QA进行冷启动，注入推理结构；2）在SFT期间对相同的硬UHR图像-文本示例进行"预热"，以稳定和增强后续基于工具的强化学习。

Result: 该方法在XLRS-Bench上达到了60.40%的Pass@1，显著优于更大的通用模型（如GPT-5.2、Gemini 3.0 Pro、Intern-S1），建立了新的最先进水平。

Conclusion: 高质量的地球科学文本QA是UHR视觉推理增益的主要驱动力，尽管缺乏图像，但领域特定文本注入了指导视觉证据检索所需的概念、机制解释和决策规则。

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [82] [REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents](https://arxiv.org/abs/2602.14234)
*Zheng Chu,Xiao Wang,Jack Hong,Huiming Fan,Yuqi Huang,Yue Yang,Guohai Xu,Chenxiao Zhao,Cheng Xiang,Shengchao Hu,Dongdong Kuang,Ming Liu,Bing Qin,Xing Yu*

Main category: cs.AI

TL;DR: REDSearcher是一个统一的搜索智能体优化框架，通过协同设计复杂任务合成、中期训练和后期训练，解决了高质量搜索轨迹稀疏和奖励信号不足的问题，在文本和多模态搜索基准上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从通用知识引擎转向现实世界问题解决者时，面临深度搜索任务优化的挑战。主要瓶颈在于高质量搜索轨迹和奖励信号的极端稀疏性，这源于可扩展的长时域任务构建的困难以及涉及外部工具调用的交互密集型rollout的高成本。

Method: 1) 将任务合成构建为双约束优化问题，通过图拓扑和证据分散精确控制任务难度；2) 引入工具增强查询以鼓励主动工具使用而非被动回忆；3) 中期训练中加强核心原子能力（知识、规划和函数调用）；4) 构建本地模拟环境以实现快速、低成本的强化学习实验迭代。

Result: 在文本和多模态搜索智能体基准测试中，该方法实现了最先进的性能。同时将发布10K高质量复杂文本搜索轨迹、5K多模态轨迹和1K文本RL查询集，以及代码和模型检查点。

Conclusion: REDSearcher通过协同设计任务合成、训练和环境模拟，有效解决了搜索智能体优化中的关键挑战，为长时域搜索智能体的未来研究提供了有价值的资源和框架。

Abstract: Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.

</details>


### [83] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAIL使用模仿学习和逆强化学习从（可能次优的）演示轨迹中学习每个候选目标的目标导向策略，以更准确地识别智能体目标，特别是在次优和有偏行为场景下。


<details>
  <summary>Details</summary>
Motivation: 现有目标识别方法通常依赖于最优目标导向策略表示，但这可能与智能体的真实行为不同，阻碍了准确识别其目标。需要能够处理次优和有偏行为的目标识别方法。

Method: GRAIL（目标识别对齐通过模仿学习）结合模仿学习和逆强化学习，直接从演示轨迹中学习每个候选目标的目标导向策略。通过单次前向传递使用每个学习到的策略对观察到的部分轨迹进行评分。

Result: 在评估的领域中，GRAIL在系统有偏最优行为下将F1分数提高了0.5以上，在次优行为下获得约0.1-0.3的增益，在有噪声最优轨迹下提升高达0.4，同时在完全最优设置下保持竞争力。

Conclusion: GRAIL为在不确定环境中解释智能体目标提供了可扩展且鲁棒的模型，能够更好地处理次优、有偏和噪声行为。

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [84] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: PIR（先例知情推理）通过自适应选择相关先例并在测试时内部化解决方案模式，将LLM推理从自我探索转变为有指导的学习，显著缩短推理链同时保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理过程通常存在效率低下的长思维链，包含冗余的自我探索和验证，这会增加计算成本甚至降低性能。受人类利用过去相关案例约束搜索空间、减少试错推理模式的启发，需要将LLM推理范式从详尽的自我探索转变为有指导的从先例中学习。

Method: 提出PIR框架，包含两个核心组件：1）自适应先例选择（APS）为每个问题和LLM构建紧凑的先例集，基于语义相似度和模型困惑度的联合评分进行排序，并自适应调整先例数量以最大化困惑度降低；2）测试时经验内部化（TEI）在测试时对先例知情指令进行学习，更新轻量级适配器以内部化解决方案模式，并在后续推理中作为先验使用。

Result: 在数学推理、科学问答和代码生成等任务上的实验表明，PIR能够一致地缩短推理链，同时保持或提高最终准确性，在LLM中实现了出色的准确性与效率权衡。

Conclusion: PIR通过将LLM推理从自我探索转变为有指导的从先例中学习，有效解决了推理效率问题，在多个任务上实现了推理链缩短和准确性保持/提升的良好平衡。

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [85] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5](https://arxiv.org/abs/2602.14457)
*Dongrui Liu,Yi Yu,Jie Zhang,Guanxu Chen,Qihao Lin,Hanxi Zhu,Lige Huang,Yijin Zhou,Peng Wang,Shuai Shao,Boxuan Zhang,Zicheng Liu,Jingwei Sun,Yu Li,Yuejin Xie,Jiaxuan Guo,Jia Xu,Chaochao Lu,Bowen Zhou,Xia Hu,Jing Shao*

Main category: cs.AI

TL;DR: 该论文提出了一个前沿AI风险管理框架，评估了五个关键风险维度：网络攻击、说服操纵、战略欺骗、失控AI研发和自我复制，并提出了相应的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能力快速发展和智能体AI的普及，需要全面评估前沿AI模型带来的前所未有的风险，以应对新兴威胁。

Method: 采用更新的细粒度风险评估方法，针对五个关键维度设计复杂场景：网络攻击（更复杂场景）、说服操纵（LLM间说服评估）、战略欺骗（新兴错位实验）、失控AI研发（智能体"错误进化"）、自我复制（资源受限场景），并监控OpenClaw在Moltbook上的安全表现。

Result: 提出了系列稳健的缓解策略来应对这些新兴威胁，为前沿AI的安全部署提供了初步技术和可操作路径。

Conclusion: 这项工作反映了对AI前沿风险的当前理解，并呼吁采取集体行动来缓解这些挑战，为安全部署前沿AI提供了框架。

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.

</details>


### [86] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC是一个结合形式化验证和可解释性的工具，用于分析医疗强化学习策略，特别针对脓毒症治疗优化，通过构建可达状态空间和临床标签实现可验证性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的强化学习策略通常不透明且难以验证，标准概率模型检查器无法处理大规模MDP，也无法解释策略决策原因，这限制了在临床环境中的安全部署。

Method: COOL-MC包装了Storm模型检查器，但增加了三个关键功能：1) 仅构建训练策略诱导的可达状态空间，生成可验证的离散时间马尔可夫链；2) 自动用临床有意义的原子命题标记状态；3) 将可解释性方法与PCTL查询集成，揭示决策驱动特征。

Result: 在ICU-Sepsis MDP基准测试中，COOL-MC建立了硬边界验证，训练了达到最优生存概率的安全RL策略，并通过PCTL验证和可解释性分析发现策略主要依赖历史用药而非患者实时状况，这是标准评估无法发现的弱点。

Conclusion: COOL-MC可作为临床医生在部署前调查和调试脓毒症治疗策略的工具，展示了形式化验证与可解释性结合在医疗决策中的价值。

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [87] [Arbor: A Framework for Reliable Navigation of Critical Conversation Flows](https://arxiv.org/abs/2602.14643)
*Luís Silva,Diogo Gonçalves,Catarina Farinha,Clara Matos,Luís Ungaro*

Main category: cs.AI

TL;DR: Arbor框架通过将决策树导航分解为专门的节点级任务，解决了LLM在结构化工作流中的遵循问题，显著提升了临床分诊的准确性、降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗分诊等高风险领域难以严格遵循结构化工作流。单一提示方法随着提示长度增加会出现指令遵循退化、中间丢失效应和上下文窗口溢出等问题。

Method: 提出Arbor框架，将决策树导航分解为专门的节点级任务：将决策树标准化为边列表表示并存储；运行时通过DAG编排机制迭代检索当前节点的出边，通过专用LLM调用评估有效转换，并将响应生成委托给单独的推理步骤。

Result: 在10个基础模型上使用真实临床分诊对话进行评估：平均轮次准确率提升29.4个百分点，每轮延迟降低57.1%，每轮成本平均降低14.4倍。架构分解减少了对内在模型能力的依赖，使较小模型能够匹配或超越单一提示基线下的较大模型。

Conclusion: Arbor框架通过分解决策树导航，显著提升了LLM在结构化工作流中的性能，降低了成本和对大型模型的依赖，为高风险领域的应用提供了有效解决方案。

Abstract: Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.

</details>


### [88] [Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs](https://arxiv.org/abs/2602.14697)
*Lunjun Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: E-SPL提出了一种结合强化学习和系统提示进化的方法，通过并行选择多个系统提示进行rollout，对模型权重进行RL更新，同时使用LLM驱动的变异和交叉进化系统提示种群，实现了上下文和权重的联合优化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要通过两种机制自我改进：上下文更新的自我反思和权重更新的强化学习。本文旨在提出一种能够联合改进模型上下文和模型权重的方法，实现更有效的自主自我改进。

Method: E-SPL在每次RL迭代中并行选择多个系统提示进行rollout，对每个系统提示条件下的模型权重应用RL更新，同时通过LLM驱动的变异和交叉对系统提示种群进行进化更新。每个系统提示都有基于相对性能更新的TrueSkill评分用于进化选择。

Result: 在从易到难（AIME→BeyondAIME）的泛化设置中，E-SPL将RL成功率从38.8%提升到45.1%，同时优于反射提示进化（40.0%）。结果表明，强化学习与系统提示进化的结合在样本效率和泛化方面带来了一致的增益。

Conclusion: E-SPL方法通过将声明性知识编码在提示中、程序性知识编码在权重中，实现了上下文和权重的联合优化，在推理和代理任务上表现出改进的性能，展示了强化学习与系统提示进化结合的有效性。

Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>


### [89] [WebWorld: A Large-Scale World Model for Web Agent Training](https://arxiv.org/abs/2602.14721)
*Zikai Xiao,Jianhong Tu,Chuhang Zou,Yuxin Zuo,Zhi Li,Peng Wang,Bowen Yu,Fei Huang,Junyang Lin,Zuozhu Liu*

Main category: cs.AI

TL;DR: WebWorld是一个大规模开放网络模拟器，通过100万+网络交互训练，支持推理、多格式数据和30+步骤的长时程模拟，在WebArena上提升9.2%，性能接近GPT-4o，并展示跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有网络代理需要大量轨迹进行泛化，但现实世界训练受到网络延迟、速率限制和安全风险的约束。现有模拟器局限于封闭环境且轨迹数量有限，无法满足大规模训练需求。

Method: 使用可扩展的数据管道在100万+开放网络交互上进行训练，支持推理、多格式数据和长时程模拟。引入WebWorld-Bench进行内在评估，使用双指标覆盖九个维度。通过合成轨迹训练模型，并评估其作为世界模型的推理时搜索能力。

Result: WebWorld模拟性能与Gemini-3-Pro相当。Qwen3-14B在WebWorld合成轨迹上训练后，在WebArena上提升9.2%，性能接近GPT-4o。作为世界模型，其推理时搜索能力优于GPT-5。此外，WebWorld在代码、GUI和游戏环境中展示跨领域泛化能力。

Conclusion: WebWorld是第一个大规模开放网络模拟器，通过可扩展训练方法实现了高性能的网络代理训练，提供了世界模型构建的可复制方案，并展示了强大的跨领域泛化潜力。

Abstract: Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.

</details>


### [90] [Position: Introspective Experience from Conversational Environments as a Path to Better Learning](https://arxiv.org/abs/2602.14910)
*Claudiu Cristian Musat,Jackson Tolins,Diego Antognini,Jingling Li,Martin Klissarov,Tom Duerig*

Main category: cs.AI

TL;DR: 该论文提出通过语言自我反思和高质量社交互动来发展AI推理能力，而非依赖规模扩展。核心观点包括：社会互动生成私人思维、对话式内省经验促进意义建构、对话质量决定推理深度。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法将推理视为规模扩展的涌现属性，但作者认为这种范式存在问题。他们主张从维果茨基发展心理学出发，强调语言自我反思和高质量社交互动对于发展稳健推理能力的重要性。

Method: 基于维果茨基发展心理学理论框架，提出三个核心立场：1）私人思维的社会起源：从对话环境中学习；2）对话式内省经验：通过意义建构将原始数据转化为可学习的叙事；3）对话质量即数据质量：对话的多样性和严谨性决定推理深度。

Result: 论文提出了一个理论框架，认为优化对话式脚手架是发展下一代通用智能的主要杠杆。对话质量而非数据规模成为决定AI推理深度和测试时计算效率的关键因素。

Conclusion: 通过语言自我反思和高质量社交互动发展AI推理能力是下一代通用智能的关键。优化对话式脚手架，而非单纯扩大模型规模，是实现稳健推理能力的主要途径。

Abstract: Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.

</details>


### [91] [Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation](https://arxiv.org/abs/2602.15019)
*Alisa Vinogradova,Vlad Vinogradov,Luba Greenwood,Ilya Yasny,Dmitry Kobyzev,Shoman Kasbekar,Kong Nguyen,Dmitrii Radkevich,Roman Doronin,Andrey Doronichev*

Main category: cs.AI

TL;DR: 提出用于药物资产侦察的基准测试方法和自学习Bioptic Agent，在跨语言、异构数据源中实现高召回率、无幻觉的资产发现，显著优于现有AI系统。


<details>
  <summary>Details</summary>
Motivation: 生物制药创新格局已变：大量新药资产源自美国以外地区，主要通过区域性和非英语渠道披露。超过85%的专利申请来自美国以外，中国占全球近一半；非美国学术产出也在增长。中国约占全球药物开发的30%，涉及1200+候选药物。在这种高风险环境中，未能发现"雷达下"资产会给投资者和业务开发团队带来数十亿美元风险，资产侦察成为覆盖关键竞争，速度和完整性决定价值。然而当前深度研究AI代理在跨异构多语言源实现高召回发现且无幻觉方面仍落后于人类专家。

Method: 提出药物资产侦察的基准测试方法和经过调优的树状自学习Bioptic Agent，旨在实现完整、无幻觉的侦察。构建具有挑战性的完整性基准，使用多语言多代理流程：复杂用户查询配对主要在美国中心雷达之外的ground-truth资产。为反映真实交易复杂性，从专家投资者、BD和VC专业人士收集筛选查询，并用作先验条件生成基准查询。使用LLM-as-judge评估进行评分，校准到专家意见。

Result: Bioptic Agent达到79.7% F1分数，显著优于Claude Opus 4.6（56.2%）、Gemini 3 Pro + Deep Research（50.6%）、GPT-5.2 Pro（46.6%）、Perplexity Deep Research（44.2%）和Exa Websets（26.9%）。性能随额外计算资源增加而显著提升，支持更多计算带来更好结果的观点。

Conclusion: 提出的Bioptic Agent在药物资产侦察任务中显著优于现有AI系统，证明了自学习树状架构在跨语言、异构数据源中实现高召回、无幻觉发现的优势，为高风险生物制药投资环境提供了有效的解决方案。

Abstract: Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface "under-the-radar" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.
  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [92] [Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?](https://arxiv.org/abs/2602.13626)
*Mingqiao Zhang,Qiyao Peng,Yumeng Wang,Chunyuan Liu,Hongtao Liu*

Main category: cs.LG

TL;DR: 论文发现LLM推荐系统中存在基准数据泄露问题，即LLM在预训练或微调时接触并可能记忆了基准数据集，导致性能指标虚高，无法反映真实模型能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推荐系统中的广泛应用，评估可靠性面临关键挑战。本文识别并研究了一个先前被忽视的问题：基于LLM的推荐系统中的基准数据泄露现象。

Method: 通过模拟不同的数据泄露场景，对基础模型进行持续预训练，使用包含领域内和领域外用户-物品交互的战略性混合语料库。实验验证数据泄露对推荐性能的影响。

Result: 实验揭示了数据泄露的双重效应：当泄露数据与领域相关时，会导致显著但虚假的性能提升，误导性地夸大模型能力；而领域不相关的泄露通常会降低推荐准确性。

Conclusion: 数据泄露是基于LLM推荐系统中一个关键且先前未被考虑的因素，可能影响真实模型性能。需要更严谨的评估方法来避免这种偏差。

Abstract: The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.

</details>


### [93] [GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization](https://arxiv.org/abs/2602.13921)
*Juntong Wang,Libin Chen,Xiyuan Wang,Shijia Kang,Haotong Yang,Da Zheng,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出了首个用于仓库级bug定位的GNN基准测试GREPO，包含86个Python仓库和47294个bug修复任务，GNN方法在性能上显著优于传统信息检索基线方法。


<details>
  <summary>Details</summary>
Motivation: 仓库级bug定位是关键的软件工程挑战，标准LLM由于上下文窗口限制无法处理整个代码仓库，现有检索方法（如关键词匹配、文本相似度、简单图启发式）效果有限，而GNN虽然能建模复杂的仓库级依赖关系，但缺乏专用基准测试阻碍了其应用。

Method: 引入GREPO基准测试，包含86个Python仓库和47294个bug修复任务，提供可直接用于GNN处理的图数据结构，并评估了多种GNN架构的性能。

Result: GNN架构在bug定位任务上表现出色，性能显著优于传统的信息检索基线方法。

Conclusion: GNN在bug定位方面具有巨大潜力，GREPO为未来研究提供了基础资源，有助于推动该领域的发展。

Abstract: Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.

</details>


### [94] [A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning](https://arxiv.org/abs/2602.13937)
*Dat Le,Duc-Cuong Le,Anh-Son Nguyen,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.LG

TL;DR: iML是一个基于多智能体的AutoML框架，通过代码引导的规划、模块化实现和可验证集成，解决了传统AutoML黑盒问题和LLM智能体幻觉问题，显著提升了机器学习工程任务的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统AutoML框架缺乏灵活性和透明度，而基于LLM的智能体存在幻觉逻辑和逻辑纠缠问题，导致运行时故障难以恢复。需要一种新的架构范式来提升AutoML的可靠性和工程实用性。

Method: iML采用多智能体框架，包含三个核心创新：1) 代码引导规划：基于自主经验分析生成战略蓝图消除幻觉；2) 代码模块化实现：将预处理和建模解耦为专门组件，通过严格接口契约管理；3) 代码可验证集成：通过动态契约验证和迭代自校正确保物理可行性。

Result: 在MLE-BENCH上实现85%的有效提交率和45%的竞赛奖牌率，平均标准化性能得分(APS)为0.77。在iML-BENCH上比其他方法提升38%-163%的APS。即使在简化任务描述下仍保持70%的成功率。

Conclusion: iML通过代码引导的模块化架构有效弥合了随机生成与可靠工程之间的差距，为实现真正的AutoML迈出了重要一步，展示了在复杂现实工程任务中的强大潜力。

Abstract: Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as "black boxes", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.

</details>


### [95] [Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting](https://arxiv.org/abs/2602.13802)
*Xiaoyu Tao,Mingyue Cheng,Chuang Jiang,Tian Gao,Huanjian Zhang,Yaguo Liu*

Main category: cs.LG

TL;DR: Cast-R1将时间序列预测重新定义为顺序决策问题，通过基于记忆的状态管理机制和工具增强的智能体工作流实现自主证据收集、推理和迭代预测优化。


<details>
  <summary>Details</summary>
Motivation: 传统模型中心的时间序列预测方法通常将预测视为从历史观测到未来值的单次映射，在复杂和动态变化的环境中表现不佳，因为它们缺乏自主获取信息证据、推理潜在未来变化或通过迭代决策过程修正预测的能力。

Method: 提出Cast-R1框架，将预测重新定义为顺序决策问题。采用基于记忆的状态管理机制维护决策相关信息，通过工具增强的智能体工作流自主与模块化工具包交互，包括提取统计特征、调用轻量级预测模型进行决策支持、执行基于推理的预测，并通过自我反思迭代优化预测。采用两阶段学习策略：监督微调结合多轮强化学习，配合课程学习方案逐步增加任务难度。

Result: 在多个真实世界时间序列数据集上的广泛实验证明了Cast-R1的有效性。

Conclusion: 这项工作为时间序列建模的智能体范式探索提供了实际步骤，展示了将预测重新定义为顺序决策问题的潜力。

Abstract: Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.

</details>


### [96] [AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning](https://arxiv.org/abs/2602.13807)
*Xiaoyu Tao,Yuchong Wu,Mingyue Cheng,Ze Guo,Tian Gao*

Main category: cs.LG

TL;DR: AnomaMind是一个基于智能体的时间序列异常检测框架，将异常检测重构为序列决策过程，通过多轮工具交互、自适应特征准备和自我反思来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将异常检测视为固定的判别预测任务，而非证据驱动的诊断过程，难以处理上下文依赖性强或模式多样的异常情况。这些限制源于缺乏自适应特征准备、推理感知检测和迭代优化。

Method: 提出AnomaMind框架，采用结构化工作流程：从粗到细逐步定位异常区间，通过多轮工具交互进行自适应特征准备，通过自我反思优化异常决策。核心设计是混合推理机制：通用模型负责自主工具交互和自我反思优化，而核心异常检测决策通过强化学习在工作流程级反馈下学习。

Result: 在多样化设置下的广泛实验表明，AnomaMind能持续提升异常检测性能。

Conclusion: AnomaMind通过将异常检测重构为序列决策过程，结合工具增强和强化学习，有效解决了现有方法在上下文依赖和模式多样性方面的局限性。

Abstract: Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.

</details>


### [97] [Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation](https://arxiv.org/abs/2602.13810)
*Guojian Zhan,Letian Tao,Pengcheng Wang,Yixiao Wang,Yiheng Li,Yuxin Chen,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: MVP是一种新的生成式策略函数，通过建模平均速度场实现最快的一步动作生成，在机器人操作任务中达到SOTA性能，同时大幅提升训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的策略在建模复杂动作分布时面临表达能力和计算负担之间的权衡，通常通过流步骤数量来控制。需要一种既能保持高表达能力又能实现快速动作生成的策略函数。

Method: 提出平均速度策略（MVP），建模平均速度场实现最快的一步动作生成。引入瞬时速度约束（IVC）来确保高表达能力，该设计作为关键边界条件，提高学习精度和策略表达能力。

Result: 在Robomimic和OGBench的多个挑战性机器人操作任务中达到最先进的成功率。相比现有基于流的策略基线，在训练和推理速度上有显著提升。

Conclusion: MVP通过建模平均速度场和引入瞬时速度约束，成功解决了流策略在表达能力和计算效率之间的权衡问题，实现了高效且表达力强的策略学习。

Abstract: Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.

</details>


### [98] [Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning](https://arxiv.org/abs/2602.13934)
*Zhimin Zhao*

Main category: cs.LG

TL;DR: 论文提出基于信息结构的五级可学习性层次，解释为什么代码生成比强化学习更可靠，并指出ML进展上限更多取决于任务是否可学习而非模型规模。


<details>
  <summary>Details</summary>
Motivation: 代码生成比强化学习进展更可靠，主要因为代码具有使学习成为可能的信息结构。代码在每个token提供密集、局部、可验证的反馈，而大多数强化学习问题不具备这种特性。这种反馈质量的差异不是二元的而是渐进的。

Method: 提出基于信息结构的五级可学习性层次，建立计算问题的三个属性（可表达性、可计算性、可学习性）之间的形式化区分，分析它们的成对关系，并提供一个统一的模板来明确结构差异。

Result: 分析表明为什么代码的监督学习可以可预测地扩展而强化学习不能，以及为什么仅靠扩展就能解决剩余ML挑战的常见假设值得审视。

Conclusion: 机器学习进展的上限更多取决于任务是否可学习，而不是模型规模。代码的可学习性结构使其比强化学习问题更容易扩展。

Abstract: Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.

</details>


### [99] [Experiential Reinforcement Learning](https://arxiv.org/abs/2602.13949)
*Taiwei Shi,Sihao Chen,Bowen Jiang,Linxin Song,Longqi Yang,Jieyu Zhao*

Main category: cs.LG

TL;DR: ERL（经验强化学习）通过在强化学习中嵌入经验-反思-巩固循环，将稀疏延迟的环境反馈转化为结构化行为修正，提升学习效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型从环境奖励或反馈中学习时，反馈通常是稀疏和延迟的。模型需要从观察到的失败中推断如何改变未来行为，这具有挑战性。需要一种将反馈转化为结构化行为修正的方法。

Method: ERL训练范式在强化学习过程中嵌入经验-反思-巩固循环：模型生成初始尝试→接收环境反馈→产生反思→指导改进的第二次尝试→成功被强化并内化到基础策略中。

Result: 在稀疏奖励控制环境和智能推理基准测试中，ERL持续优于强强化学习基线，在复杂多步环境中提升高达81%，在工具使用推理任务中提升高达11%。

Conclusion: 将显式自我反思整合到策略训练中，为将反馈转化为持久行为改进提供了实用机制，改善探索、稳定优化，且部署时无需额外推理成本。

Abstract: Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.

</details>


### [100] [QuRL: Efficient Reinforcement Learning with Quantized Rollout](https://arxiv.org/abs/2602.13953)
*Yuhang Li,Reena Elangovan,Xin Dong,Priyadarshini Panda,Brucek Khailany*

Main category: cs.LG

TL;DR: 提出QuRL方法，使用量化演员加速RL训练中的rollout过程，解决训练崩溃和权重更新问题，实现20-80%的加速


<details>
  <summary>Details</summary>
Motivation: 在RLVR训练推理大语言模型时，自回归解码的rollout过程占训练时间高达70%，成为效率瓶颈，需要加速方法

Method: 提出量化强化学习(QuRL)，使用量化演员加速rollout。包含两个关键技术：1) 基于全精度和量化演员策略比率的自适应裁剪范围(ACR)，防止长期训练崩溃；2) 不变缩放技术，减少量化噪声，增强权重更新

Result: 在DeepScaleR和DAPO上进行INT8和FP8量化实验，实现20%到80%的rollout加速

Conclusion: QuRL通过量化演员有效加速RL训练中的rollout过程，解决了训练崩溃和权重更新问题，显著提升训练效率

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.

</details>


### [101] [When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift](https://arxiv.org/abs/2602.14161)
*Max Fomin*

Main category: cs.LG

TL;DR: 论文提出了Leave-One-Dataset-Out (LODO)评估方法，揭示传统训练测试分割严重高估了LLM提示攻击检测性能，发现现有防护系统在间接攻击上表现不佳（7-37%检测率），并展示了稀疏自编码器特征分析能提供更可靠的解释。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理处理越来越多来自电子邮件、文档、工具输出和外部API的不受信任数据，检测提示注入和越狱攻击变得至关重要。然而，当前的评估实践和生产系统存在根本性限制，无法准确衡量模型在真实分布外场景下的泛化能力。

Method: 1) 使用包含有害请求、越狱、间接提示注入和提取攻击的18个多样化数据集构建基准；2) 提出Leave-One-Dataset-Out (LODO)评估协议来测量真实分布外泛化；3) 分析稀疏自编码器(SAE)特征系数以理解分类器泛化失败原因；4) 系统比较生产级防护系统(PromptGuard 2, LlamaGuard)和LLM-as-judge方法。

Result: 1) 传统训练测试分割严重高估性能：聚合指标显示AUC膨胀8.4个百分点，每数据集准确率差距从1%到25%；2) 28%的顶级特征是数据集依赖的捷径特征；3) 现有防护系统在间接攻击上表现不佳：检测率仅7-37%；4) PromptGuard 2和LlamaGuard因架构限制无法评估代理工具注入；5) LODO稳定的SAE特征能过滤数据集伪影，提供更可靠的解释。

Conclusion: LODO评估协议是提示攻击检测研究的适当标准，能揭示传统评估方法掩盖的真实泛化缺陷。现有防护系统在间接攻击和代理工具注入场景中存在严重不足，需要新的架构和方法来应对这些挑战。

Abstract: Detecting prompt injection and jailbreak attacks is critical for deploying LLM-based agents safely. As agents increasingly process untrusted data from emails, documents, tool outputs, and external APIs, robust attack detection becomes essential. Yet current evaluation practices and production systems have fundamental limitations. We present a comprehensive analysis using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. We propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, revealing that the standard practice of train-test splits from the same dataset sources severely overestimates performance: aggregate metrics show an 8.4 percentage point AUC inflation, but per-dataset gaps range from 1% to 25% accuracy-exposing heterogeneous failure modes. To understand why classifiers fail to generalize, we analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, finding that 28% of top features are dataset-dependent shortcuts whose class signal depends on specific dataset compositions rather than semantic content. We systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on our benchmark, finding all three fail on indirect attacks targeting agents (7-37% detection) and that PromptGuard 2 and LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. Finally, we show that LODO-stable SAE features provide more reliable explanations for classifier decisions by filtering dataset artifacts. We release our evaluation framework at https://github.com/maxf-zn/prompt-mining to establish LODO as the appropriate protocol for prompt attack detection research.

</details>


### [102] [Evaluating LLMs in Finance Requires Explicit Bias Consideration](https://arxiv.org/abs/2602.14233)
*Yaxuan Kong,Hoyoung Lee,Yoontae Hwang,Alejandro Lopez-Lira,Bradford Levy,Dhagash Mehta,Qingsong Wen,Chanyeol Choi,Yongjae Lee,Stefan Zohren*

Main category: cs.LG

TL;DR: 该论文指出金融LLM应用中存在五种常见偏见，这些偏见会夸大性能、污染回测结果，并使得报告结果对部署声明毫无价值。作者提出了结构有效性框架和评估清单来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地集成到金融工作流程中，但评估实践未能跟上。金融特定偏见会夸大性能、污染回测结果，并使得报告结果对部署声明毫无价值，需要系统性的解决方案。

Method: 识别了金融LLM应用中的五种常见偏见（前瞻性偏见、幸存者偏见、叙事偏见、目标偏见和成本偏见），审查了2023-2025年的164篇论文，提出了结构有效性框架和评估清单。

Result: 研究发现没有单一偏见在超过28%的研究中被讨论，表明金融LLM系统中的偏见问题被严重忽视。提出了结构有效性框架和评估清单作为解决方案。

Conclusion: 金融LLM系统中的偏见需要明确关注，在支持任何部署声明之前应强制执行结构有效性。提出的框架和清单为偏见诊断和未来系统设计提供了最低要求。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.

</details>


### [103] [Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection](https://arxiv.org/abs/2602.14251)
*Pinqiao Wang,Sheng Li*

Main category: cs.LG

TL;DR: 提出MAD多智能体辩论框架，将表格异常检测中不同模型的异质性分歧作为核心信号，通过数学协调层解决分歧，提高检测鲁棒性


<details>
  <summary>Details</summary>
Motivation: 表格异常检测通常使用单一检测器或静态集成，但表格数据的强性能来自异构模型族（树集成、深度表格网络、表格基础模型），这些模型在分布偏移、缺失值和罕见异常情况下经常产生分歧，需要系统化处理这些分歧

Method: MAD框架包含多个ML检测器智能体，每个智能体输出归一化异常分数、置信度和结构化证据，由LLM批评家增强。协调器将这些信息转换为有界损失，通过指数梯度规则更新智能体影响力，生成最终辩论异常分数和可审计的辩论轨迹

Result: 在多样化表格异常基准测试中，相比基线方法展现出更好的鲁棒性，并提供更清晰的模型分歧轨迹。框架具有后悔保证，可通过符合性校准控制交换性下的误报率

Conclusion: MAD是一个统一的智能体框架，能够恢复现有方法（如专家混合门控和学习专家建议聚合），通过处理模型分歧作为核心信号，提高了表格异常检测的鲁棒性和可解释性

Abstract: Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement

</details>


### [104] [KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning](https://arxiv.org/abs/2602.14293)
*Kris Shengjun Dong,Sahil Modi,Dima Nikiforov,Sana Damani,Edward Lin,Siva Kumar Sastry Hari,Christos Kozyrakis*

Main category: cs.LG

TL;DR: KernelBlaster是一个基于记忆增强上下文强化学习的CUDA代码优化框架，通过构建可检索的知识库帮助LLM代理学习经验，实现跨GPU架构的高性能优化。


<details>
  <summary>Details</summary>
Motivation: CUDA代码在跨代GPU架构上的优化面临挑战，传统编译器受限于固定启发式方法，而微调LLM成本高昂。现有代理工作流缺乏从先前探索中聚合知识的能力，导致采样偏差和次优解。

Method: 提出MAIC-RL框架，构建可检索的持久CUDA知识库，采用基于配置文件的文本梯度代理流程进行CUDA生成和优化，系统探索超越简单重写的高潜力优化策略。

Result: 在KernelBench三个级别上相比PyTorch基线分别获得1.43x、2.50x和1.50x的几何平均加速比，并开源了包含测试工具、验证组件和可复现评估流程的框架。

Conclusion: KernelBlaster通过记忆增强和强化学习有效提升了LLM代理的CUDA优化能力，能够系统探索复杂优化空间，实现跨GPU架构的高性能代码生成。

Abstract: Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.

</details>


### [105] [Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows](https://arxiv.org/abs/2602.14295)
*Edwin Chen,Zulekha Bibi*

Main category: cs.LG

TL;DR: MLAT是一种设计模式，将预训练机器学习模型作为可调用工具集成到LLM代理工作流中，使代理能按需调用定量预测并在上下文中推理输出。


<details>
  <summary>Details</summary>
Motivation: 传统管道将ML推理作为静态预处理步骤，无法让LLM根据对话上下文动态决定何时以及如何使用定量预测。需要一种方法让LLM能够灵活调用ML模型作为工具，实现定量估计与上下文推理的结合。

Method: 提出MLAT设计模式，将ML模型作为一级工具暴露给LLM代理。开发PitchCraft系统作为验证案例，包含两个代理：研究代理通过并行工具调用收集情报，起草代理通过工具调用XGBoost定价模型并生成结构化输出。

Result: 定价模型在70个真实和人工验证合成数据上训练，在保留数据上达到R^2=0.807，平均绝对误差3688美元。系统将提案生成时间从数小时减少到10分钟以内。

Conclusion: MLAT框架适用于需要定量估计与上下文推理结合的领域，将ML模型作为工具集成到LLM工作流中，实现了灵活的动态调用和推理。

Abstract: We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.
  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.
  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.

</details>


### [106] [Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study](https://arxiv.org/abs/2602.14322)
*Hani Beirami,M M Manjurul Islam*

Main category: cs.LG

TL;DR: 将形式化时序逻辑规范与强化学习控制结合，通过符合性STL防护层提升航空航天应用中的安全性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 在航空航天应用中，强化学习控制的安全性和鲁棒性至关重要。需要确保RL代理在复杂环境中遵守关键的安全规范，特别是在面对模型失配、执行器限制和测量噪声等挑战时。

Method: 使用AeroBench F-16仿真基准，训练PPO代理进行发动机油门调节和空速跟踪。将控制目标编码为信号时序逻辑(STL)要求，在每次机动的最后几秒内保持空速在规定范围内。引入符合性STL防护层，通过在线符合性预测过滤RL代理的动作。

Result: 实验比较了三种设置：(1)PPO基线，(2)PPO+基于规则的STL防护层，(3)PPO+符合性防护层。在名义条件和压力场景下，符合性防护层在保持STL满足的同时维持接近基线的性能，并提供比传统防护层更强的鲁棒性保证。

Conclusion: 形式化规范监控与数据驱动的RL控制相结合，可以显著提高自主飞行控制在挑战性环境中的可靠性。符合性STL防护层在安全性和性能之间取得了良好平衡。

Abstract: We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent's actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.

</details>


### [107] [Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning](https://arxiv.org/abs/2602.14338)
*Zhi Zhang,Zhen Han,Costas Mavromatis,Qi Zhu,Yunyi Zhang,Sheng Guan,Dingmin Wang,Xiong Zhou,Shuai Wang,Soji Adeshina,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: AERO是一种改进GRPO的强化学习方法，通过自适应采样策略、选择性拒绝和贝叶斯后验来避免零梯度信号，在保持性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在强化学习后训练中，当一组rollouts结果完全一致（全对或全错）时，会产生零梯度信号，浪费计算资源。需要一种更高效的优化策略。

Method: 提出AERO方法：1）自适应rollout策略动态调整采样数量；2）选择性拒绝策略修剪低质量rollouts；3）贝叶斯后验防止零优势死区。

Result: 在三个模型配置上，AERO在相同总rollout预算下减少约48%总训练计算量，缩短约45%每步训练时间，同时保持或提升Pass@8和Avg@8性能。

Conclusion: AERO提供了一种实用、可扩展且计算高效的强化学习对齐策略，显著提升训练效率而不牺牲模型性能。

Abstract: Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.

</details>


### [108] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2602.14344)
*Mathias Jackermeier,Mattia Giuri,Jacques Cloete,Alessandro Abate*

Main category: cs.LG

TL;DR: 该论文提出一种基于线性时序逻辑（LTL）的层次化神经架构，通过布尔公式序列和注意力机制来增强多任务强化学习中指令跟随的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在多任务强化学习中，现有方法虽然能训练通用策略，但难以有效捕捉LTL规范中丰富的逻辑和时间结构，导致在零样本执行未见任务时表现不佳。

Method: 提出基于任务有限自动机构建布尔公式序列来条件化策略，采用层次化神经架构编码公式的逻辑结构，并引入注意力机制让策略能够推理未来子目标。

Result: 在多种复杂环境中的实验表明，该方法具有强大的泛化能力和优越的性能表现。

Conclusion: 通过结构化任务表示学习，能够有效提升多任务强化学习中指令跟随的泛化性能，特别是在处理复杂LTL规范时表现突出。

Abstract: We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.

</details>


### [109] [WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control](https://arxiv.org/abs/2602.14351)
*Mehran Aghabozorgi,Alireza Moazeni,Yanshu Zhang,Ke Li*

Main category: cs.LG

TL;DR: WIMLE是一种基于模型的强化学习方法，通过扩展隐式最大似然估计来学习随机多模态世界模型，使用集成和潜在采样估计预测不确定性，并通过置信度加权合成转移来稳定学习。


<details>
  <summary>Details</summary>
Motivation: 基于模型的强化学习虽然承诺高样本效率，但在实践中表现不佳，主要原因是：模型误差累积、单模态世界模型平均了多模态动态、过度自信的预测导致学习偏差。

Method: 将隐式最大似然估计扩展到基于模型的强化学习框架中，学习随机多模态世界模型（无需迭代采样），通过集成和潜在采样估计预测不确定性。在训练中，根据预测置信度对每个合成转移进行加权，保留有用的模型展开同时减少不确定预测带来的偏差。

Result: 在DeepMind Control、MyoSuite和HumanoidBench的40个连续控制任务中，WIMLE实现了优越的样本效率和竞争性或更好的渐近性能。在Humanoid-run任务上，样本效率比最强竞争者提高50%以上；在HumanoidBench上解决了14个任务中的8个（BRO解决4个，SimbaV2解决5个）。

Conclusion: IMLE基础的多模态和不确定性感知加权对于稳定的基于模型的强化学习具有重要价值。

Abstract: Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.

</details>


### [110] [Broken Chains: The Cost of Incomplete Reasoning in LLMs](https://arxiv.org/abs/2602.14444)
*Ian Su,Gaurav Purushothaman,Jey Narayan,Ruhika Goel,Kevin Zhu,Sunishchal Dev,Yash More,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 研究比较了不同推理模态（代码、自然语言、混合、无推理）在token预算限制下的性能表现，发现代码推理在资源受限时表现更稳健，而截断的推理链反而会损害模型性能。


<details>
  <summary>Details</summary>
Motivation: 推理专用模型（如GPT-5.1、DeepSeek-V3.2）在推理过程中消耗大量计算资源，推理token成本高昂。需要了解在token预算受限的情况下，不同的推理模态（代码、自然语言、混合或无推理）如何影响模型性能。

Method: 提出一个框架，强制模型仅通过代码、注释、两者结合或不进行推理来思考，然后系统性地将token预算削减到最优水平的10%、30%、50%和70%。在四个前沿模型（GPT-5.1、Gemini 3 Flash、DeepSeek-V3.2、Grok 4.1）上评估数学基准（AIME、GSM8K、HMMT）。

Result: 1) 截断推理会损害性能：DeepSeek-V3.2在无推理时达到53%准确率，但在50%预算下的截断CoT仅17%；2) 代码推理表现稳健：Gemini的注释推理崩溃到0%，而代码推理保持43-47%；3) 混合推理表现不如单一模态；4) 稳健性模型依赖：Grok在30%预算下保持80-90%准确率，而OpenAI和DeepSeek崩溃到7-27%。

Conclusion: 不完整的推理链会误导模型，这对在资源受限环境下部署推理专用系统具有重要意义。代码推理在token约束下表现更稳健，而混合推理和截断的自然语言推理可能损害性能。

Abstract: Reasoning-specialized models like OpenAI's 5.1 and DeepSeek-V3.2 allocate substantial inference compute to extended chain-of-thought (CoT) traces, yet reasoning tokens incur significant costs. How do different reasoning modalities of code, natural language, hybrid, or none do perform under token constraints? We introduce a framework that constrains models to reason exclusively through code, comments, both, or neither, then systematically ablates token budgets to 10\%, 30\%, 50\%, and 70\% of optimal. We evaluate four frontier models (GPT-5.1, Gemini 3 Flash, DeepSeek-V3.2, Grok 4.1) across mathematical benchmarks (AIME, GSM8K, HMMT). Our findings reveal: (1) \textbf{truncated reasoning can hurt} as DeepSeek-V3.2 achieves 53\% with no reasoning but only 17\% with truncated CoT at 50\% budget; (2) \textbf{code degrades gracefully} as Gemini's comments collapse to 0\% while code maintains 43-47\%; (3) \textbf{hybrid reasoning underperforms} single modalities; (4) \textbf{robustness is model-dependent} as Grok maintains 80-90\% at 30\% budget where OpenAI and DeepSeek collapse to 7-27\%. These results suggest incomplete reasoning chains actively mislead models, with implications for deploying reasoning-specialized systems under resource constraints.

</details>


### [111] [LACONIC: Length-Aware Constrained Reinforcement Learning for LLM](https://arxiv.org/abs/2602.14468)
*Chang Liu,Yiran Zhao,Lawrence Liu,Yaoqi Ye,Csaba Szepesvári,Lin F. Yang*

Main category: cs.LG

TL;DR: LACONIC是一种强化学习方法，通过在训练中引入长度成本来强制控制LLM输出长度，在保持任务性能的同时减少50%以上输出长度。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练LLM时会产生过长的响应，增加推理延迟和计算开销。现有长度控制方法依赖固定的启发式奖励调整，可能与任务目标不一致且需要脆弱的手动调优。

Method: 提出LACONIC方法，在训练过程中强制执行目标token预算。使用增强的目标函数更新策略模型，结合任务奖励和基于长度的成本。通过自适应调整成本规模来平衡简洁性和任务性能。

Result: 在数学推理模型和数据集上，LACONIC保持或提高pass@1性能，同时减少超过50%的输出长度。在通用知识和多语言基准测试中保持域外性能，减少44%的token使用。

Conclusion: LACONIC能够在不改变推理过程且部署开销最小的情况下，实现鲁棒的长度控制并保持任务奖励，为RL调优提供了有效的长度控制解决方案。

Abstract: Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.

</details>


### [112] [Fluid-Agent Reinforcement Learning](https://arxiv.org/abs/2602.14559)
*Shishir Sharma,Doina Precup,Theodore J. Perkins*

Main category: cs.LG

TL;DR: 提出流体智能体环境框架，允许智能体动态创建其他智能体，解决现实世界中智能体数量不固定、未知的问题，并通过实验验证MARL算法在该框架下的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中智能体数量既非固定也非预先已知，且智能体可以决定创建其他智能体（如细胞分裂、公司分拆部门）。传统多智能体强化学习主要研究固定数量智能体在环境中的交互，无法处理这种动态变化。

Method: 提出流体智能体环境框架，允许智能体动态创建其他智能体。为流体智能体游戏提出博弈论解决方案概念，并在该框架下实证评估多种MARL算法性能。实验包括Predator-Prey和Level-Based Foraging的流体变体，以及新引入的环境。

Result: 实验证明该框架能够产生根据环境需求动态调整规模的智能体团队。流体性能够解锁固定群体设置中无法观察到的新颖解决方案策略。

Conclusion: 流体智能体环境框架为处理动态智能体数量问题提供了有效方法，使智能体团队能够根据环境需求自适应调整规模，扩展了MARL在现实世界应用中的能力。

Abstract: The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.

</details>


### [113] [RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch](https://arxiv.org/abs/2602.14578)
*Isam Vrce,Andreas Kassler,Gökçe Aydos*

Main category: cs.LG

TL;DR: 该论文首次将N:M结构化稀疏应用于强化学习，提出RNM-TD3框架，在保持硬件加速兼容性的同时，在50%-75%稀疏度下性能优于稠密模型。


<details>
  <summary>Details</summary>
Motivation: 现有DRL稀疏化方法多为非结构化细粒度稀疏，限制了硬件加速机会；结构化粗粒度稀疏虽能硬件加速，但通常降低性能且增加剪枝复杂度。需要平衡压缩、性能和硬件效率的稀疏化方案。

Method: 提出RNM-TD3框架，在离策略RL（TD3）中为所有网络实施行级N:M稀疏化训练，保持与支持N:M稀疏矩阵运算的加速器兼容。

Result: 在连续控制基准测试中，RNM-TD3在50%-75%稀疏度（如2:4和1:4）下性能优于稠密模型，在Ant环境中2:4稀疏度下性能提升达14%。即使在87.5%稀疏度（1:8）下仍保持竞争力，同时实现潜在训练加速。

Conclusion: N:M结构化稀疏在RL中有效平衡了压缩、性能和硬件效率，为DRL模型的高效部署提供了有前景的解决方案。

Abstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.

</details>


### [114] [Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow](https://arxiv.org/abs/2602.14587)
*Minh Nguyen*

Main category: cs.LG

TL;DR: 提出一种解耦的连续时间演员-评论家算法，通过交替更新解决标准离散时间RL在连续时间控制问题中的局限性，在连续控制和交易任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界控制问题（金融、机器人）在连续时间中演化，具有非均匀、事件驱动的决策。标准离散时间RL基于固定步长Bellman更新，在这种设置下存在困难：当时间间隔缩小时，Q函数会坍缩为价值函数V，消除动作排序。现有连续时间方法通过优势率函数q重新引入动作信息，但通过复杂的鞅损失或正交约束强制执行最优性，对测试过程选择敏感，并将V和q纠缠成难以可靠训练的大型复杂优化问题。

Method: 提出一种新颖的解耦连续时间演员-评论家算法，采用交替更新：q从V的扩散生成器学习，V通过基于哈密顿量的价值流更新，该价值流在无穷小时间步下仍保持信息性（标准max/softmax备份会失效）。理论上通过新的概率论证证明严格收敛，绕过了生成器基哈密顿量在sup-norm下缺乏Bellman式收缩的挑战。

Result: 在具有挑战性的连续控制基准测试和真实世界交易任务中，该方法优于先前的连续时间和领先的离散时间基线，在单个季度内实现21%的利润——几乎是第二佳方法的两倍。

Conclusion: 该方法成功解决了连续时间RL中的关键挑战，通过解耦V和q的优化，提供了一种更稳定、更有效的训练框架，在理论和实证上都表现出优越性能。

Abstract: Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>


### [115] [Symmetry in language statistics shapes the geometry of model representations](https://arxiv.org/abs/2602.15029)
*Dhruva Karkada,Daniel J. Korchinski,Andres Nava,Matthieu Wyart,Yasaman Bahri*

Main category: cs.LG

TL;DR: 论文证明语言统计中的平移对称性导致LLM表示中出现简单几何结构，这种结构在统计扰动下依然稳健，源于潜在的连续隐变量控制。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络表示的成功已被广泛认可，但其基本性质仍未被充分理解。一个显著现象是LLM表示中出现的简单几何结构（如月份形成圆形、年份形成一维流形），需要解释这些结构为何出现及其稳健性。

Method: 首先证明语言统计具有平移对称性（如同现概率仅取决于时间间隔），然后证明这种对称性导致高维词嵌入模型中的几何结构。进一步研究当同现统计被强烈扰动时的稳健性，提出由潜在连续隐变量集体控制的解释框架。在词嵌入模型、文本嵌入模型和LLM中进行实证验证。

Result: 理论证明平移对称性确实导致观察到的几何结构。实验表明这些结构在同现统计被强烈扰动时依然保持稳健，甚至在中等嵌入维度下也是如此。验证了潜在连续隐变量控制的理论框架。

Conclusion: 语言统计的平移对称性是LLM表示中出现简单几何结构的基础原因，这些结构的稳健性源于潜在连续隐变量的集体控制，为理解神经网络表示的性质提供了理论框架。

Abstract: Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.

</details>


### [116] [Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment](https://arxiv.org/abs/2602.14844)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出Interactionless Inverse Reinforcement Learning和Alignment Flywheel方法，解决AI对齐中的Alignment Waste问题，将安全目标与代理策略解耦，创建可检查、可编辑、模型无关的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在结构性缺陷，将安全目标与代理策略纠缠在一起。RLHF和DPO等方法产生不透明、一次性使用的对齐产物（Alignment Waste），需要更可持续、可验证的工程解决方案。

Method: 1. Interactionless Inverse Reinforcement Learning：将对齐产物学习与策略优化解耦，生成可检查、可编辑、模型无关的奖励模型；2. Alignment Flywheel：人机交互生命周期，通过自动化审计和精化迭代强化奖励模型。

Result: 该方法将安全从一次性消耗转变为持久、可验证的工程资产，解决了Alignment Waste问题，提供了更透明、可持续的AI对齐框架。

Conclusion: 提出的架构解决了当前AI对齐方法的结构性缺陷，通过解耦对齐产物学习和策略优化，创建了可检查、可编辑的奖励模型，并通过迭代强化过程实现可持续的安全工程。

Abstract: AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.

</details>


### [117] [Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849)
*Bardia Mohammadi,Nearchos Potamitis,Lars Klein,Akhil Arora,Laurent Bindschaedler*

Main category: cs.LG

TL;DR: Atomix为LLM智能体工具调用提供进度感知的事务语义，通过epoch标记、资源前沿跟踪和进度谓词提交机制，确保失败、推测或竞争情况下的安全回滚和隔离性。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在外部系统上执行操作时，工具调用会立即产生效果。当发生故障、推测执行或资源竞争时，失败的分支可能会泄漏意外的副作用，且无法安全回滚。需要一种机制来确保工具调用的原子性和隔离性。

Method: Atomix运行时为智能体工具调用提供进度感知的事务语义：1) 为每个调用标记epoch；2) 跟踪每个资源的前沿状态；3) 仅当进度谓词指示安全时才提交；4) 可缓冲的效果延迟执行，外部化效果在回滚时进行补偿。

Result: 在真实工作负载中进行故障注入测试：1) 事务重试提高了任务成功率；2) 前沿门控提交在推测执行和竞争情况下增强了隔离性。

Conclusion: Atomix通过进度感知的事务语义解决了LLM智能体工具调用中的副作用泄漏和回滚问题，提高了系统的可靠性和隔离性。

Abstract: LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.

</details>


### [118] [On the Learning Dynamics of RLVR at the Edge of Competence](https://arxiv.org/abs/2602.14872)
*Yu Huang,Zixin Wen,Yuejie Chi,Yuting Wei,Aarti Singh,Yingbin Liang,Yuxin Chen*

Main category: cs.LG

TL;DR: 本文提出一个理论框架，解释强化学习与可验证奖励（RLVR）如何帮助大型推理模型克服长时程推理障碍，关键在于数据难度谱的平滑性决定学习动态。


<details>
  <summary>Details</summary>
Motivation: 尽管基于最终结果的奖励（RLVR）在大型推理模型中取得了突破，但人们仍不清楚这种仅依赖最终结果的奖励如何帮助克服长时程推理障碍。本文旨在理解RLVR在组合推理任务中的训练动态机制。

Method: 开发了一个关于transformer在组合推理任务上强化学习训练动态的理论框架，使用有限群上的傅里叶分析工具，并通过合成实验验证预测机制。

Result: 理论分析表明：当数据难度谱存在急剧不连续性时，学习会出现grokking型相变，产生长时间平台期；而平滑的难度谱会产生接力效应，使模型能力稳步提升。实验验证了这些预测机制。

Conclusion: RLVR通过梯度信号在能力边缘提升模型性能，适当设计的数据混合可以产生可扩展的收益。难度谱的平滑性是决定学习动态的关键因素。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.

</details>


### [119] [Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems](https://arxiv.org/abs/2602.14901)
*Pramit Saha,Joshua Strong,Mohammad Alsharid,Divyanshu Mishra,J. Alison Noble*

Main category: cs.LG

TL;DR: ToolSelect：一种基于注意力神经过程的模型选择方法，用于医疗代理系统中从异构专家模型池中自适应选择最佳专家模型，在胸部X光任务上显著优于10种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗代理系统中，单个任务通常没有唯一的"最佳"模型，而是多个专家模型在不同数据样本上表现各异。现有方法缺乏可靠的模型选择机制，无法从异构专家模型池中为给定查询选择最合适的模型。

Method: 提出ToolSelect方法：1）使用注意力神经过程构建选择器，以查询和每个模型的行为摘要为条件；2）通过最小化任务条件选择损失的一致替代来学习模型选择；3）首次构建胸部X光代理环境ToolSelectBench，包含17个疾病检测、19个报告生成、6个视觉定位和13个VQA模型，共1448个查询。

Result: ToolSelect在四个不同任务家族上一致优于10种SOTA方法，证明了其在异构专家模型池中选择最佳模型的优越性。

Conclusion: ToolSelect为医疗代理系统提供了一种有效的模型选择机制，能够自适应地从多个专家模型中选择最适合特定查询的模型，解决了实际应用中模型选择的关键问题。

Abstract: Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single "best" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.

</details>
