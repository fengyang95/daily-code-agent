{"id": "2512.23742", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23742", "abs": "https://arxiv.org/abs/2512.23742", "authors": ["Guangxi Fan", "Tianliang Ma", "Xuguang Sun", "Xun Wang", "Kain Lu Low", "Leilai Shao"], "title": "AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization", "comment": "7 pages, 7 figures, 2 tables", "summary": "With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.", "AI": {"tldr": "AgenticTCAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u534a\u5bfc\u4f53\u5668\u4ef6\u8bbe\u8ba1\u4e0e\u4f18\u5316\uff0c\u901a\u8fc7\u4e13\u5bb6\u6784\u5efa\u7684\u5f00\u6e90TCAD\u6570\u636e\u96c6\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u57282\u7eb3\u7c73\u7eb3\u7c73\u7247FET\u8bbe\u8ba1\u4e2d\u6bd4\u4eba\u7c7b\u4e13\u5bb6\u5feb40\u500d\u8fbe\u5230IRDS-2024\u89c4\u683c\u3002", "motivation": "\u968f\u7740\u5148\u8fdb\u6280\u672f\u8282\u70b9\u7684\u6301\u7eed\u7f29\u653e\uff0c\u8bbe\u8ba1-\u6280\u672f\u534f\u540c\u4f18\u5316\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46TCAD\u4eff\u771f\u9886\u57df\u7f3a\u4e4f\u5f00\u6e90\u8d44\u6e90\uff0c\u963b\u788d\u4e86\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u6548\u7684TCAD\u4ee3\u7801\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5668\u4ef6\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3002", "method": "1) \u6784\u5efa\u4e13\u5bb6\u7b56\u5212\u7684\u5f00\u6e90TCAD\u6570\u636e\u96c6\uff1b2) \u5fae\u8c03\u9886\u57df\u7279\u5b9a\u7684TCAD\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff1b3) \u63d0\u51faAgenticTCAD\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u5668\u4ef6\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3002", "result": "\u57282\u7eb3\u7c73\u7eb3\u7c73\u7247FET\u8bbe\u8ba1\u4e2d\uff0cAgenticTCAD\u57284.2\u5c0f\u65f6\u5185\u8fbe\u5230\u56fd\u9645\u5668\u4ef6\u4e0e\u7cfb\u7edf\u8def\u7ebf\u56fe(IRDS)-2024\u7684\u5668\u4ef6\u89c4\u683c\uff0c\u800c\u4eba\u7c7b\u4e13\u5bb6\u4f7f\u7528\u5546\u4e1a\u5de5\u5177\u9700\u89817.1\u5929\uff0c\u6548\u7387\u63d0\u5347\u7ea640\u500d\u3002", "conclusion": "AgenticTCAD\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u534a\u5bfc\u4f53\u5668\u4ef6\u8bbe\u8ba1\u4e0e\u4f18\u5316\u7684\u6548\u7387\uff0c\u4e3a\u5148\u8fdb\u6280\u672f\u8282\u70b9\u7684\u8bbe\u8ba1-\u6280\u672f\u534f\u540c\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2512.23747", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23747", "abs": "https://arxiv.org/abs/2512.23747", "authors": ["Abhinav Parmar", "Abhisek Panigrahi", "Abhishek Kumar Dwivedi", "Abhishek Bhattacharya", "Adarsh Ramachandra", "Aditya Choudhary", "Aditya Garg", "Aditya Raj", "Alankrit Bhatt", "Alpesh Yadav", "Anant Vishnu", "Ananthu Pillai", "Ankush Kumar", "Aryan Patnaik", "Aswatha Narayanan S", "Avanish Raj Singh", "Bhavya Shree Gadda", "Brijesh Pankajbhai Kachhadiya", "Buggala Jahnavi", "Chidurala Nithin Krishna", "Chintan Shah", "Chunduru Akshaya", "Debarshi Banerjee", "Debrup Dey", "Deepa R.", "Deepika B G", "Faiz ur Rahman", "Gagan Gayari", "Gudhi Jagadeesh Kumar Naidu", "Gursimar Singh", "Harshal Tyagi", "Harshini K", "James Mani Vathalloor", "Jayarama Nettar", "Jayashree Gajjam", "Joe Walter Sugil George", "Kamalakara Sri Krishna Tadepalli", "Kamalkumar Rathinasamy", "Karan Chaurasia", "Karthikeyan S", "Kashish Arora", "Kaushal Desai", "Khushboo Buwade", "Kiran Manjrekar", "Malikireddy Venkata Sai Likhitha", "Manjunath A", "Mitali Mahavir Bedmutha", "Mohammed Rafee Tarafdar", "Nikhil Tiwari", "Nikitha K Gigi", "Pavan Ravikumar", "Pendyala Swarnanjali", "Piyush Anand", "Prakash Chandrasekar", "Prasanna Bhalchandra Gawade", "Prasanth Sivan", "Preeti Khurana", "Priyanshi Babbar", "Rajab Ali Mondal", "Rajesh Kumar Vissapragada", "Rajeshwari Ganesan", "Rajeswari Koppisetti", "Ramjee R.", "Ramkumar Thiruppathisamy", "Rani G. S.", "S Reka", "Samarth Gupta", "Sandeep Reddy Kothakota", "Sarathy K", "Sathyanarayana Sampath Kumar", "Saurabh Kumar", "Shashank Khasare", "Shenbaga Devi Venkatesh Kumar", "Shiva Rama Krishna Parvatham", "Shoeb Shaikh", "Shrishanmathi A", "Shubham Pathak", "Sree Samhita Koppaka", "Sreenivasa Raghavan K S", "Sreeram Venkatasubramanian", "Suprabha Desai Bojja", "Swetha R", "Syed Ahmed", "Chinmai Harshitha Thota", "Tushar Yadav", "Veeravelly Kusumitha", "V V S S Prasanth Patnaik", "Vidya Sri Sesetti", "Vijayakeerthi K", "Vikram Raj Bakshi", "Vinay K K", "Vinoth Kumar Loganathan", "Vipin Tiwari", "Vivek Kumar Shrivastav", "V Venkata Sri Datta Charan", "Wasim Akhtar Khan"], "title": "State-of-the-art Small Language Coder Model: Mify-Coder", "comment": null, "summary": "We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.", "AI": {"tldr": "Mify-Coder\u662f\u4e00\u4e2a2.5B\u53c2\u6570\u7684\u4ee3\u7801\u6a21\u578b\uff0c\u901a\u8fc7\u8ba1\u7b97\u6700\u4f18\u7b56\u7565\u57284.2T tokens\u4e0a\u8bad\u7ec3\uff0c\u5728\u4ee3\u7801\u751f\u6210\u548c\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u8bc1\u660e\u7d27\u51d1\u6a21\u578b\u53ef\u4ee5\u8fbe\u5230\u524d\u6cbf\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u6700\u4f18\u7b56\u7565\u548c\u6570\u636e\u4f18\u5316\uff0c\u8ba9\u5c0f\u578b\u4ee3\u7801\u6a21\u578b\u8fbe\u5230\u4e0e\u5927\u578b\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u90e8\u7f72\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u57fa\u4e8eMify-2.5B\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u8ba1\u7b97\u6700\u4f18\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u4eba\u5de5\u6570\u636e\u548c\u901a\u8fc7\u667a\u80fd\u63d0\u793a\u751f\u6210\u7684\u5408\u6210\u6570\u636e\uff0c\u4f7f\u7528LLM\u8d28\u91cf\u8fc7\u6ee4\u589e\u5f3a\u6570\u636e\u5bc6\u5ea6\uff0c\u63a2\u7d22CPT-SFT\u76ee\u6807\u3001\u6570\u636e\u6df7\u5408\u548c\u91c7\u6837\u52a8\u6001\u3002", "result": "Mify-Coder\u5728\u6807\u51c6\u7f16\u7801\u548c\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8fbe\u5230\u53ef\u6bd4\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u91cf\u5316\u53d8\u4f53\u53ef\u5728\u6807\u51c6\u684c\u9762\u73af\u5883\u90e8\u7f72\u800c\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u3002", "conclusion": "\u901a\u8fc7\u539f\u5219\u6027\u7684\u6570\u636e\u548c\u8ba1\u7b97\u7eaa\u5f8b\uff0c\u5c0f\u578b\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u5b89\u5168\u6027\u5408\u89c4\uff0c\u7d27\u51d1\u6a21\u578b\u80fd\u591f\u5339\u914d\u524d\u6cbf\u7ea7\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u5de5\u4f5c\u6d41\u4e2d\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2512.23880", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.23880", "abs": "https://arxiv.org/abs/2512.23880", "authors": ["Xu Huang", "Junwu Chen", "Yuxing Fei", "Zhuohan Li", "Philippe Schwaller", "Gerbrand Ceder"], "title": "CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution", "comment": null, "summary": "Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from \"LLM + tool use\" to \"LLM + skill acquisition\". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.", "AI": {"tldr": "CASCADE\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u548c\u81ea\u6211\u53cd\u601d\u7b49\u5143\u6280\u80fd\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u80fd\u591f\u638c\u63e1\u590d\u6742\u7684\u5916\u90e8\u5de5\u5177\u5e76\u7f16\u7801\u77e5\u8bc6\uff0c\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4ece\"LLM+\u5de5\u5177\u4f7f\u7528\"\u5230\"LLM+\u6280\u80fd\u83b7\u53d6\"\u7684\u8f6c\u53d8\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u5de5\u5177\u6216\u8106\u5f31\u7684\u5de5\u5177\u751f\u6210\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u548c\u9002\u5e94\u65b0\u5de5\u5177\u7684\u6846\u67b6\u3002", "method": "CASCADE\u6846\u67b6\u901a\u8fc7\u4e24\u79cd\u5143\u6280\u80fd\u5b9e\u73b0\uff1a1) \u6301\u7eed\u5b66\u4e60\uff08\u901a\u8fc7\u7f51\u9875\u641c\u7d22\u548c\u4ee3\u7801\u63d0\u53d6\uff09\uff1b2) \u81ea\u6211\u53cd\u601d\uff08\u901a\u8fc7\u5185\u7701\u548c\u77e5\u8bc6\u56fe\u8c31\u63a2\u7d22\uff09\u3002\u6846\u67b6\u652f\u6301\u4eba\u7c7b-\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8bb0\u5fc6\u5de9\u56fa\uff0c\u79ef\u7d2f\u53ef\u6267\u884c\u7684\u6280\u80fd\u3002", "result": "\u5728SciSkillBench\u57fa\u51c6\u6d4b\u8bd5\uff08116\u4e2a\u6750\u6599\u79d1\u5b66\u548c\u5316\u5b66\u7814\u7a76\u4efb\u52a1\uff09\u4e0a\uff0c\u4f7f\u7528GPT-5\u7684CASCADE\u8fbe\u523093.3%\u7684\u6210\u529f\u7387\uff0c\u800c\u6ca1\u6709\u6f14\u5316\u673a\u5236\u7684\u57fa\u7ebf\u53ea\u670935.4%\u3002\u6846\u67b6\u5728\u8ba1\u7b97\u5206\u6790\u3001\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u548c\u8bba\u6587\u9009\u62e9\u6027\u590d\u73b0\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CASCADE\u901a\u8fc7\u79ef\u7d2f\u53ef\u8de8\u667a\u80fd\u4f53\u548c\u79d1\u5b66\u5bb6\u5171\u4eab\u7684\u53ef\u6267\u884c\u6280\u80fd\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u7684AI\u8f85\u52a9\u79d1\u5b66\u7814\u7a76\uff0c\u4ee3\u8868\u4e86\u4ece\"LLM+\u5de5\u5177\u4f7f\u7528\"\u5411\"LLM+\u6280\u80fd\u83b7\u53d6\"\u7684\u91cd\u8981\u8f6c\u53d8\u3002", "topic": "code agent"}}
{"id": "2512.23713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23713", "abs": "https://arxiv.org/abs/2512.23713", "authors": ["Jahidul Islam", "Md Ataullha", "Saiful Azad"], "title": "PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents", "comment": "6 Pages", "summary": "LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\\% on the development set and 71.6\\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.", "AI": {"tldr": "BanglaCodeAct\uff1a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u63d0\u793a\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\u7684\u5b5f\u52a0\u62c9\u8bed\u5230Python\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u5f00\u6e90\u591a\u8bed\u8a00LLM\u5728Thought-Code-Observation\u5faa\u73af\u4e2d\u5b9e\u73b0\u52a8\u6001\u751f\u6210\u3001\u6d4b\u8bd5\u548c\u4f18\u5316\u4ee3\u7801\u3002", "motivation": "LLM\u5728\u82f1\u8bed\u63d0\u793a\u7684\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fd9\u79cd\u8fdb\u5c55\u5c1a\u672a\u6269\u5c55\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u5230Python\u4ee3\u7801\u751f\u6210\u7684\u9700\u6c42\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faBanglaCodeAct\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5f00\u6e90\u591a\u8bed\u8a00LLM\u5728Thought-Code-Observation\u5faa\u73af\u4e2d\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u63d0\u793a\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\uff0c\u52a8\u6001\u751f\u6210\u3001\u6d4b\u8bd5\u548c\u4f18\u5316\u4ee3\u7801\u3002", "result": "\u5728mHumanEval\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u591a\u4e2a\u5c0f\u578b\u5f00\u6e90LLM\uff0cQwen3-8B\u7ed3\u5408BanglaCodeAct\u8868\u73b0\u6700\u4f73\uff1a\u5f00\u53d1\u96c6pass@1\u51c6\u786e\u738794.0%\uff0c\u76f2\u6d4b\u96c671.6%\uff0c\u4e3a\u5b5f\u52a0\u62c9\u8bed\u5230Python\u7ffb\u8bd1\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b5f\u52a0\u62c9\u8bed\u5230Python\u4ee3\u7801\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u63a8\u7406\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ef\u9760\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002", "topic": "code agent"}}
{"id": "2512.23844", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23844", "abs": "https://arxiv.org/abs/2512.23844", "authors": ["Tao Dong", "Harini Sampath", "Ja Young Lee", "Sherry Y. Shi", "Andrew Macvean"], "title": "From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering", "comment": null, "summary": "As Large Language Models (LLMs) evolve from code generators into collaborative partners for software engineers, our methods for evaluation are lagging. Current benchmarks, focused on code correctness, fail to capture the nuanced, interactive behaviors essential for successful human-AI partnership. To bridge this evaluation gap, this paper makes two core contributions. First, we present a foundational taxonomy of desirable agent behaviors for enterprise software engineering, derived from an analysis of 91 sets of user-defined agent rules. This taxonomy defines four key expectations of agent behavior: Adhere to Standards and Processes, Ensure Code Quality and Reliability, Solving Problems Effectively, and Collaborating with the User.\n  Second, recognizing that these expectations are not static, we introduce the Context-Adaptive Behavior (CAB) Framework. This emerging framework reveals how behavioral expectations shift along two empirically-derived axes: the Time Horizon (from immediate needs to future ideals), established through interviews with 15 expert engineers, and the Type of Work (from enterprise production to rapid prototyping, for example), identified through a prompt analysis of a prototyping agent. Together, these contributions offer a human-centered foundation for designing and evaluating the next generation of AI agents, moving the field's focus from the correctness of generated code toward the dynamics of true collaborative intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30AI\u4ee3\u7801\u52a9\u624b\u7684\u65b0\u6846\u67b6\uff0c\u5173\u6ce8\u534f\u4f5c\u884c\u4e3a\u800c\u975e\u4ec5\u4ee3\u7801\u6b63\u786e\u6027\u3002\u901a\u8fc7\u5206\u6790\u7528\u6237\u5b9a\u4e49\u7684\u4ee3\u7406\u89c4\u5219\uff0c\u5efa\u7acb\u4e86\u4f01\u4e1a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u671f\u671b\u4ee3\u7406\u884c\u4e3a\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u884c\u4e3a\u6846\u67b6\u6765\u7406\u89e3\u884c\u4e3a\u671f\u671b\u5982\u4f55\u968f\u65f6\u95f4\u548c\u5de5\u4f5c\u7c7b\u578b\u53d8\u5316\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u65b9\u6cd5\u6ede\u540e\uff0c\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u6b63\u786e\u6027\uff0c\u672a\u80fd\u6355\u6349\u6210\u529f\u4eba\u673a\u534f\u4f5c\u6240\u9700\u7684\u7ec6\u5fae\u4ea4\u4e92\u884c\u4e3a\u3002\u9700\u8981\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u53cd\u6620AI\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u534f\u4f5c\u4f19\u4f34\u7684\u5b9e\u9645\u9700\u6c42\u3002", "method": "1. \u5206\u679091\u7ec4\u7528\u6237\u5b9a\u4e49\u7684\u4ee3\u7406\u89c4\u5219\uff0c\u5efa\u7acb\u4f01\u4e1a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u671f\u671b\u4ee3\u7406\u884c\u4e3a\u7684\u5206\u7c7b\u6cd5\uff1b2. \u901a\u8fc715\u4f4d\u4e13\u5bb6\u5de5\u7a0b\u5e08\u8bbf\u8c08\u5efa\u7acb\u65f6\u95f4\u7ef4\u5ea6\uff1b3. \u901a\u8fc7\u539f\u578b\u4ee3\u7406\u7684\u63d0\u793a\u5206\u6790\u786e\u5b9a\u5de5\u4f5c\u7c7b\u578b\u7ef4\u5ea6\uff1b4. \u63d0\u51fa\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u884c\u4e3a\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u5305\u542b\u56db\u4e2a\u5173\u952e\u671f\u671b\u7684\u5206\u7c7b\u6cd5\uff1a\u9075\u5b88\u6807\u51c6\u548c\u6d41\u7a0b\u3001\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3001\u6709\u6548\u89e3\u51b3\u95ee\u9898\u3001\u4e0e\u7528\u6237\u534f\u4f5c\u3002\u5efa\u7acb\u4e86\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u884c\u4e3a\u6846\u67b6\uff0c\u63ed\u793a\u884c\u4e3a\u671f\u671b\u5982\u4f55\u6cbf\u4e24\u4e2a\u7ecf\u9a8c\u63a8\u5bfc\u7684\u8f74\u53d8\u5316\uff1a\u65f6\u95f4\u8303\u56f4\uff08\u4ece\u5373\u65f6\u9700\u6c42\u5230\u672a\u6765\u7406\u60f3\uff09\u548c\u5de5\u4f5c\u7c7b\u578b\uff08\u4ece\u4f01\u4e1a\u751f\u4ea7\u5230\u5feb\u901f\u539f\u578b\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u548c\u8bc4\u4f30\u4e0b\u4e00\u4ee3AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u57fa\u7840\uff0c\u5c06\u9886\u57df\u7126\u70b9\u4ece\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u8f6c\u5411\u771f\u6b63\u534f\u4f5c\u667a\u80fd\u7684\u52a8\u6001\u7279\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.24040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24040", "abs": "https://arxiv.org/abs/2512.24040", "authors": ["Natchaya Temyingyong", "Daman Jain", "Neeraj Kumarsahu", "Prabhat Kumar", "Rachata Phondi", "Wachiravit Modecrua", "Krittanon Kaewtawee", "Krittin Pachtrachai", "Touchapon Kraisingkorn"], "title": "ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment", "comment": "22 pages, 1 figure", "summary": "Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.", "AI": {"tldr": "ROAD\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5c06\u5931\u8d25\u65e5\u5fd7\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u51b3\u7b56\u6811\u534f\u8bae\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u96c6\u5373\u53ef\u4f18\u5316LLM\u63d0\u793a\uff0c\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd", "motivation": "\u73b0\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\uff0cLLM\u4ee3\u7406\u5f00\u53d1\u521d\u671f\u901a\u5e38\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u53ea\u6709\u6df7\u4e71\u7684\u751f\u4ea7\u65e5\u5fd7\u548c\u4e0d\u65ad\u6f14\u5316\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4f20\u7edf\u57fa\u4e8e\u6807\u6ce8\u6570\u636e\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4e0d\u9002\u7528", "method": "ROAD\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u5206\u6790\u5668\u8fdb\u884c\u6839\u56e0\u5206\u6790\uff0c\u4f18\u5316\u5668\u8fdb\u884c\u6a21\u5f0f\u805a\u5408\uff0c\u6559\u7ec3\u8fdb\u884c\u7b56\u7565\u6574\u5408\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u5931\u8d25\u65e5\u5fd7\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u51b3\u7b56\u6811\u534f\u8bae", "result": "\u5728\u5b66\u672f\u57fa\u51c6\u548c\u751f\u4ea7\u77e5\u8bc6\u7ba1\u7406\u5f15\u64ce\u4e0a\uff0cROAD\u4ec5\u75283\u6b21\u81ea\u52a8\u8fed\u4ee3\u5c31\u4f7f\u6210\u529f\u7387\u63d0\u53475.6%\uff0873.6%\u523079.2%\uff09\uff0c\u641c\u7d22\u51c6\u786e\u7387\u63d0\u53473.8%\uff1b\u5728\u96f6\u552e\u9886\u57df\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u4ee3\u7406\u6027\u80fd\u76f8\u5bf9\u57fa\u7ebf\u63d0\u5347\u7ea619%", "conclusion": "\u6a21\u62df\u4eba\u7c7b\u5de5\u7a0b\u5faa\u73af\uff08\u5931\u8d25\u5206\u6790\u548c\u4fee\u590d\uff09\u4e3a\u90e8\u7f72\u53ef\u9760LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u66ff\u4ee3\u8d44\u6e90\u5bc6\u96c6\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3", "topic": "code agent"}}
{"id": "2512.23875", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23875", "abs": "https://arxiv.org/abs/2512.23875", "authors": ["Mohsen Hesamolhokama", "Behnam Rohani", "Amirahmad Shafiee", "MohammadAmin Fazli", "Jafar Habibi"], "title": "From Illusion to Insight: Change-Aware File-Level Software Defect Prediction Using Agentic AI", "comment": null, "summary": "Much of the reported progress in file-level software defect prediction (SDP) is, in reality, nothing but an illusion of accuracy. Over the last decades, machine learning and deep learning models have reported increasing performance across software versions. However, since most files persist across releases and retain their defect labels, standard evaluation rewards label-persistence bias rather than reasoning about code changes. To address this issue, we reformulate SDP as a change-aware prediction task, in which models reason over code changes of a file within successive project versions, rather than relying on static file snapshots. Building on this formulation, we propose an LLM-driven, change-aware, multi-agent debate framework. Our experiments on multiple PROMISE projects show that traditional models achieve inflated F1, while failing on rare but critical defect-transition cases. In contrast, our change-aware reasoning and multi-agent debate framework yields more balanced performance across evolution subsets and significantly improves sensitivity to defect introductions. These results highlight fundamental flaws in current SDP evaluation practices and emphasize the need for change-aware reasoning in practical defect prediction. The source code is publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u4f20\u7edf\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u5b58\u5728\u51c6\u786e\u6027\u5e7b\u89c9\uff0c\u63d0\u51fa\u57fa\u4e8e\u4ee3\u7801\u53d8\u5316\u7684\u9884\u6d4b\u4efb\u52a1\u548cLLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u7f3a\u9677\u5f15\u5165\u7684\u654f\u611f\u6027\u3002", "motivation": "\u4f20\u7edf\u6587\u4ef6\u7ea7\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u5b58\u5728\u51c6\u786e\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u56e0\u4e3a\u5927\u591a\u6570\u6587\u4ef6\u5728\u7248\u672c\u95f4\u6301\u7eed\u5b58\u5728\u4e14\u4fdd\u6301\u7f3a\u9677\u6807\u7b7e\uff0c\u6807\u51c6\u8bc4\u4f30\u5956\u52b1\u7684\u662f\u6807\u7b7e\u6301\u7eed\u6027\u504f\u5dee\u800c\u975e\u5bf9\u4ee3\u7801\u53d8\u5316\u7684\u63a8\u7406\u3002", "method": "\u5c06SDP\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53d8\u5316\u611f\u77e5\u7684\u9884\u6d4b\u4efb\u52a1\uff0c\u6a21\u578b\u57fa\u4e8e\u8fde\u7eed\u9879\u76ee\u7248\u672c\u4e2d\u6587\u4ef6\u7684\u4ee3\u7801\u53d8\u5316\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u975e\u4f9d\u8d56\u9759\u6001\u6587\u4ef6\u5feb\u7167\u3002\u63d0\u51faLLM\u9a71\u52a8\u7684\u3001\u53d8\u5316\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2aPROMISE\u9879\u76ee\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u4f20\u7edf\u6a21\u578b\u83b7\u5f97\u865a\u9ad8\u7684F1\u5206\u6570\uff0c\u4f46\u5728\u5173\u952e\u7f3a\u9677\u8f6c\u6362\u6848\u4f8b\u4e0a\u5931\u8d25\uff1b\u800c\u63d0\u51fa\u7684\u53d8\u5316\u611f\u77e5\u63a8\u7406\u548c\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u5728\u6f14\u5316\u5b50\u96c6\u4e0a\u8868\u73b0\u66f4\u5747\u8861\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u7f3a\u9677\u5f15\u5165\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u5f53\u524dSDP\u8bc4\u4f30\u5b9e\u8df5\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u5f3a\u8c03\u5728\u5b9e\u9645\u7f3a\u9677\u9884\u6d4b\u4e2d\u9700\u8981\u53d8\u5316\u611f\u77e5\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2512.24077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24077", "abs": "https://arxiv.org/abs/2512.24077", "authors": ["Chunhui Wan", "Xunan Dai", "Zhuo Wang", "Minglei Li", "Yanpeng Wang", "Yinan Mao", "Yu Lan", "Zhiwen Xiao"], "title": "LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm", "comment": null, "summary": "The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike \"blind\" mutation operators, LoongFlow integrates LLMs into a cognitive \"Plan-Execute-Summarize\" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.", "AI": {"tldr": "LoongFlow\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06LLM\u96c6\u6210\u5230\"\u8ba1\u5212-\u6267\u884c-\u603b\u7ed3\"\u8ba4\u77e5\u8303\u5f0f\u4e2d\uff0c\u5c06\u8fdb\u5316\u641c\u7d22\u6620\u5c04\u4e3a\u63a8\u7406\u5bc6\u96c6\u578b\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u8fdb\u5316\u6548\u7387\u5e76\u53d1\u73b0\u66f4\u4f18\u89e3\u3002", "motivation": "\u4f20\u7edf\u8fdb\u5316\u65b9\u6cd5\u5728\u4ece\u9759\u6001LLM\u5411\u81ea\u6539\u8fdb\u4ee3\u7406\u8fc7\u6e21\u65f6\u5b58\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4\u4ee3\u7801\u7a7a\u95f4\u4e2d\u5bb9\u6613\u8fc7\u65e9\u6536\u655b\u548c\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faLoongFlow\u6846\u67b6\uff0c\u96c6\u6210LLM\u5230\"\u8ba1\u5212-\u6267\u884c-\u603b\u7ed3\"\u8ba4\u77e5\u8303\u5f0f\uff0c\u91c7\u7528\u6df7\u5408\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\u7ed3\u5408\u591a\u5c9b\u6a21\u578b\u3001MAP-Elites\u548c\u81ea\u9002\u5e94\u73bb\u5c14\u5179\u66fc\u9009\u62e9\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728AlphaEvolve\u57fa\u51c6\u6d4b\u8bd5\u548cKaggle\u7ade\u8d5b\u4e2d\uff0cLoongFlow\u6bd4\u9886\u5148\u57fa\u7ebf\uff08\u5982OpenEvolve\u3001ShinkaEvolve\uff09\u8fdb\u5316\u6548\u7387\u63d0\u5347\u9ad8\u8fbe60%\uff0c\u5e76\u80fd\u53d1\u73b0\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LoongFlow\u5728\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\uff0c\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u751f\u6210\u4e13\u5bb6\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.23982", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23982", "abs": "https://arxiv.org/abs/2512.23982", "authors": ["Hung-Fu Chang", "MohammadShokrolah Shirazi", "Lizhou Cao", "Supannika Koolmanojwong Mobasser"], "title": "Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education", "comment": "21 pages, 5 figures", "summary": "Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u679057\u4e2aYouTube\u89c6\u9891\uff0c\u63a2\u8ba8\u4e86LLM\u7f16\u7a0b\u5de5\u5177\u5728\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u3001\u76f8\u5173\u98ce\u9669\u53ca\u5bf9\u5f00\u53d1\u5de5\u4f5c\u6d41\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u5bf9\u8ba1\u7b97\u673a\u6559\u80b2\u7684\u542f\u793a\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e2a\u4f53\u5c42\u9762\u6216\u6559\u80b2\u73af\u5883\u4e2d\u7684AI\u7f16\u7a0b\uff0c\u7f3a\u4e4f\u5bf9\u5de5\u4e1a\u4ece\u4e1a\u8005\u89c6\u89d2\u7684\u6df1\u5165\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e86\u89e3LLM\u7f16\u7a0b\u5de5\u5177\u5728\u4e13\u4e1a\u5b9e\u8df5\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3001\u76f8\u5173\u98ce\u9669\u4ee5\u53ca\u5f00\u53d1\u5de5\u4f5c\u6d41\u7684\u8f6c\u53d8\u3002", "method": "\u5bf92024\u5e74\u672b\u81f32025\u5e74\u95f4\u53d1\u5e03\u768457\u4e2a\u7cbe\u9009YouTube\u89c6\u9891\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u8fd9\u4e9b\u89c6\u9891\u8bb0\u5f55\u4e86\u4ece\u4e1a\u8005\u7684\u53cd\u601d\u548c\u7ecf\u9a8c\u3002\u7ecf\u8fc7\u7b5b\u9009\u548c\u8d28\u91cf\u8bc4\u4f30\u540e\uff0c\u5206\u6790\u6bd4\u8f83LLM\u7f16\u7a0b\u4e0e\u4f20\u7edf\u7f16\u7a0b\u3001\u8bc6\u522b\u65b0\u5174\u98ce\u9669\u5e76\u63cf\u8ff0\u6f14\u53d8\u4e2d\u7684\u5de5\u4f5c\u6d41\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u5b9a\u4e49\u4e86AI\u7f16\u7a0b\u5b9e\u8df5\u3001\u663e\u8457\u7684\u751f\u4ea7\u529b\u63d0\u5347\u548c\u964d\u4f4e\u5165\u95e8\u95e8\u69db\u3002\u4ece\u4e1a\u8005\u62a5\u544a\u5f00\u53d1\u74f6\u9888\u8f6c\u5411\u4ee3\u7801\u5ba1\u67e5\uff0c\u5e76\u5173\u6ce8\u4ee3\u7801\u8d28\u91cf\u3001\u53ef\u7ef4\u62a4\u6027\u3001\u5b89\u5168\u6f0f\u6d1e\u3001\u4f26\u7406\u95ee\u9898\u3001\u57fa\u7840\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u9000\u5316\u4ee5\u53ca\u521d\u7ea7\u5de5\u7a0b\u5e08\u51c6\u5907\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u8ba8\u8bba\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u7684\u542f\u793a\uff0c\u4e3b\u5f20\u8bfe\u7a0b\u6539\u9769\u5e94\u8f6c\u5411\u95ee\u9898\u89e3\u51b3\u3001\u67b6\u6784\u601d\u7ef4\u3001\u4ee3\u7801\u5ba1\u67e5\u4ee5\u53ca\u65e9\u671f\u6574\u5408LLM\u5de5\u5177\u7684\u9879\u76ee\u5f0f\u5b66\u4e60\u3002\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5de5\u4e1a\u89c6\u89d2\u7684AI\u7f16\u7a0b\u89c2\u70b9\uff0c\u5e76\u4e3a\u6559\u80b2\u5b9e\u8df5\u4e0e\u5feb\u901f\u6f14\u53d8\u7684\u4e13\u4e1a\u73b0\u5b9e\u5bf9\u9f50\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "swe application"}}
{"id": "2512.23717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23717", "abs": "https://arxiv.org/abs/2512.23717", "authors": ["Shenzhe Zhu"], "title": "HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate", "comment": null, "summary": "Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.", "AI": {"tldr": "HarmTransform\uff1a\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u6709\u5bb3\u67e5\u8be2\u8f6c\u5316\u4e3a\u66f4\u9690\u853d\u7684\u5f62\u5f0f\uff0c\u4ee5\u6539\u8fdbLLM\u5b89\u5168\u5bf9\u9f50\u8bad\u7ec3\u6570\u636e", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u673a\u5236\u4e3b\u8981\u5173\u6ce8\u660e\u663e\u6709\u5bb3\u5185\u5bb9\uff0c\u4f46\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u9690\u853d\u7684\u6539\u5199\u65b9\u5f0f\u4fdd\u7559\u6076\u610f\u610f\u56fe\u800c\u663e\u5f97\u65e0\u5bb3\uff0c\u73b0\u6709\u5b89\u5168\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u663e\u8457\u7a7a\u767d", "method": "\u5f15\u5165HarmTransform\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2a\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8fed\u4ee3\u6279\u8bc4\u548c\u7cbe\u70bc\uff0c\u7cfb\u7edf\u5730\u5c06\u6709\u5bb3\u67e5\u8be2\u8f6c\u5316\u4e3a\u66f4\u9690\u853d\u7684\u5f62\u5f0f", "result": "\u5b9e\u9a8c\u8868\u660eHarmTransform\u5728\u751f\u6210\u6709\u6548\u67e5\u8be2\u8f6c\u6362\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u8fa9\u8bba\u4e5f\u5e26\u6765\u4e3b\u9898\u504f\u79fb\u548c\u4e0d\u5fc5\u8981\u590d\u6742\u6027\u7b49\u6311\u6218", "conclusion": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u5728\u751f\u6210\u5168\u9762\u5b89\u5168\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u65e2\u6709\u524d\u666f\u4e5f\u6709\u5c40\u9650\u6027\uff0c\u9700\u8981\u5e73\u8861\u8f6c\u6362\u6548\u679c\u4e0e\u590d\u6742\u6027", "topic": "agent analysis"}}
{"id": "2512.24113", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24113", "abs": "https://arxiv.org/abs/2512.24113", "authors": ["Jiaxin Hu", "Tao Wang", "Bingsan Yang", "Hongrun Wang"], "title": "CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation", "comment": "9 pages, 6 figures", "summary": "Large Language Models (LLMs) have demonstrated a remarkable capacity in understanding user preferences for recommendation systems. However, they are constrained by several critical challenges, including their inherent \"Black-Box\" characteristics, susceptibility to knowledge hallucination, and limited online learning capacity. These factors compromise their trustworthiness and adaptability. Conversely, cognitive architectures such as Soar offer structured and interpretable reasoning processes, yet their knowledge acquisition is notoriously laborious. To address these complementary challenges, we propose a novel cognitive recommender agent called CogRec which synergizes the strengths of LLMs with the Soar cognitive architecture. CogRec leverages Soar as its core symbolic reasoning engine and leverages an LLM for knowledge initialization to populate its working memory with production rules. The agent operates on a Perception-Cognition-Action(PCA) cycle. Upon encountering an impasse, it dynamically queries the LLM to obtain a reasoned solution. This solution is subsequently transformed into a new symbolic production rule via Soar's chunking mechanism, thereby enabling robust online learning. This learning paradigm allows the agent to continuously evolve its knowledge base and furnish highly interpretable rationales for its recommendations. Extensive evaluations conducted on three public datasets demonstrate that CogRec demonstrates significant advantages in recommendation accuracy, explainability, and its efficacy in addressing the long-tail problem.", "AI": {"tldr": "CogRec\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0eSoar\u8ba4\u77e5\u67b6\u6784\u7684\u63a8\u8350\u4ee3\u7406\uff0c\u901a\u8fc7\u611f\u77e5-\u8ba4\u77e5-\u884c\u52a8\u5faa\u73af\u5b9e\u73b0\u53ef\u89e3\u91ca\u63a8\u8350\u548c\u5728\u7ebf\u5b66\u4e60", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b58\u5728\u9ed1\u76d2\u7279\u6027\u3001\u77e5\u8bc6\u5e7b\u89c9\u548c\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u800c\u8ba4\u77e5\u67b6\u6784\u5982Soar\u867d\u7136\u63a8\u7406\u8fc7\u7a0b\u7ed3\u6784\u5316\u53ef\u89e3\u91ca\uff0c\u4f46\u77e5\u8bc6\u83b7\u53d6\u56f0\u96be\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u89e3\u51b3\u4e92\u8865\u6311\u6218", "method": "\u63d0\u51faCogRec\u8ba4\u77e5\u63a8\u8350\u4ee3\u7406\uff0c\u4ee5Soar\u4f5c\u4e3a\u6838\u5fc3\u7b26\u53f7\u63a8\u7406\u5f15\u64ce\uff0c\u5229\u7528LLM\u8fdb\u884c\u77e5\u8bc6\u521d\u59cb\u5316\u586b\u5145\u5de5\u4f5c\u8bb0\u5fc6\u4e2d\u7684\u4ea7\u751f\u5f0f\u89c4\u5219\u3002\u91c7\u7528\u611f\u77e5-\u8ba4\u77e5-\u884c\u52a8\u5faa\u73af\uff0c\u9047\u5230\u50f5\u5c40\u65f6\u52a8\u6001\u67e5\u8be2LLM\u83b7\u53d6\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7Soar\u7684\u5206\u5757\u673a\u5236\u5c06\u89e3\u51b3\u65b9\u6848\u8f6c\u5316\u4e3a\u65b0\u7684\u7b26\u53f7\u4ea7\u751f\u5f0f\u89c4\u5219", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cCogRec\u5728\u63a8\u8350\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u89e3\u51b3\u957f\u5c3e\u95ee\u9898\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf", "conclusion": "CogRec\u6210\u529f\u7ed3\u5408\u4e86LLM\u548cSoar\u8ba4\u77e5\u67b6\u6784\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u548c\u5f3a\u5927\u7684\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "topic": "agent analysis"}}
{"id": "2512.24156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24156", "abs": "https://arxiv.org/abs/2512.24156", "authors": ["Evgenii Rudakov", "Jonathan Shock", "Benjamin Ultan Cowley"], "title": "Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks", "comment": null, "summary": "We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u57fa\u65b9\u6cd5\u89e3\u51b3ARC-AGI-3\u4ea4\u4e92\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u89c6\u89c9\u5e27\u5904\u7406\u4e0e\u7cfb\u7edf\u5316\u72b6\u6001\u7a7a\u95f4\u63a2\u7d22\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u524d\u6cbfLLM\u4ee3\u7406", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u65e0\u6cd5\u53ef\u9760\u89e3\u51b3ARC-AGI-3\u4e2d\u7684\u4ea4\u4e92\u63a8\u7406\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u4ee3\u7406\u901a\u8fc7\u6709\u9650\u4ea4\u4e92\u63a8\u65ad\u4efb\u52a1\u673a\u5236\u5e76\u9002\u5e94\u9012\u589e\u590d\u6742\u5ea6", "method": "\u7ed3\u5408\u89c6\u89c9\u5e27\u5904\u7406\u4e0e\u7cfb\u7edf\u5316\u72b6\u6001\u7a7a\u95f4\u63a2\u7d22\uff1a1) \u5c06\u89c6\u89c9\u5e27\u5206\u5272\u4e3a\u6709\u610f\u4e49\u7ec4\u4ef6\uff1b2) \u57fa\u4e8e\u89c6\u89c9\u663e\u8457\u6027\u4f18\u5148\u9009\u62e9\u52a8\u4f5c\uff1b3) \u7ef4\u62a4\u5df2\u63a2\u7d22\u72b6\u6001\u548c\u8f6c\u79fb\u7684\u6709\u5411\u56fe\uff1b4) \u8ddf\u8e2a\u8bbf\u95ee\u72b6\u6001\u548c\u6d4b\u8bd5\u52a8\u4f5c\uff0c\u4f18\u5148\u9009\u62e9\u5230\u8fbe\u672a\u6d4b\u8bd5\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u6700\u77ed\u8def\u5f84", "result": "\u5728ARC-AGI-3\u9884\u89c8\u6311\u6218\u4e2d\uff0c\u4e2d\u4f4d\u89e3\u51b352\u4e2a\u5173\u5361\u4e2d\u768430\u4e2a\uff0c\u5728\u79c1\u6709\u6392\u884c\u699c\u6392\u540d\u7b2c3\uff0c\u663e\u8457\u4f18\u4e8e\u524d\u6cbfLLM\u4ee3\u7406", "conclusion": "\u5373\u4f7f\u65e0\u9700\u5b66\u4e60\uff0c\u663e\u5f0f\u56fe\u7ed3\u6784\u63a2\u7d22\u4e5f\u53ef\u4f5c\u4e3a\u4ea4\u4e92\u63a8\u7406\u7684\u5f3a\u57fa\u7ebf\uff0c\u7a81\u663e\u4e86\u5728\u7a00\u758f\u53cd\u9988\u73af\u5883\u4e2d\u7cfb\u7edf\u5316\u72b6\u6001\u8ddf\u8e2a\u548c\u52a8\u4f5c\u4f18\u5148\u7ea7\u7684\u91cd\u8981\u6027", "topic": "agent analysis"}}
{"id": "2512.24183", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24183", "abs": "https://arxiv.org/abs/2512.24183", "authors": ["Nan Jia", "Wangchao Sang", "Pengfei Lin", "Xiangping Chen", "Yuan Huang", "Yi Liu", "Mingliang Li"], "title": "CoHalLo: code hallucination localization via probing hidden layer vector", "comment": null, "summary": "The localization of code hallucinations aims to identify specific lines of code containing hallucinations, helping developers to improve the reliability of AI-generated code more efficiently. Although recent studies have adopted several methods to detect code hallucination, most of these approaches remain limited to coarse-grained detection and lack specialized techniques for fine-grained hallucination localization. This study introduces a novel method, called CoHalLo, which achieves line-level code hallucination localization by probing the hidden-layer vectors from hallucination detection models. CoHalLo uncovers the key syntactic information driving the model's hallucination judgments and locates the hallucinating code lines accordingly. Specifically, we first fine-tune the hallucination detection model on manually annotated datasets to ensure that it learns features pertinent to code syntactic information. Subsequently, we designed a probe network that projects high-dimensional latent vectors onto a low-dimensional syntactic subspace, generating vector tuples and reconstructing the predicted abstract syntax tree (P-AST). By comparing P-AST with the original abstract syntax tree (O-AST) extracted from the input AI-generated code, we identify the key syntactic structures associated with hallucinations. This information is then used to pinpoint hallucinated code lines. To evaluate CoHalLo's performance, we manually collected a dataset of code hallucinations. The experimental results show that CoHalLo achieves a Top-1 accuracy of 0.4253, Top-3 accuracy of 0.6149, Top-5 accuracy of 0.7356, Top-10 accuracy of 0.8333, IFA of 5.73, Recall@1% Effort of 0.052721, and Effort@20% Recall of 0.155269, which outperforms the baseline methods.", "AI": {"tldr": "CoHalLo\uff1a\u4e00\u79cd\u901a\u8fc7\u63a2\u6d4b\u5e7b\u89c9\u68c0\u6d4b\u6a21\u578b\u7684\u9690\u85cf\u5c42\u5411\u91cf\u6765\u5b9e\u73b0\u884c\u7ea7\u4ee3\u7801\u5e7b\u89c9\u5b9a\u4f4d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u9884\u6d4b\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\u548c\u539f\u59cb\u62bd\u8c61\u8bed\u6cd5\u6811\u6765\u8bc6\u522b\u5e7b\u89c9\u4ee3\u7801\u884c\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u505c\u7559\u5728\u7c97\u7c92\u5ea6\u68c0\u6d4b\u5c42\u9762\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u7ec6\u7c92\u5ea6\u5e7b\u89c9\u5b9a\u4f4d\u6280\u672f\uff0c\u96be\u4ee5\u5e2e\u52a9\u5f00\u53d1\u8005\u9ad8\u6548\u5b9a\u4f4dAI\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u5177\u4f53\u5e7b\u89c9\u884c\u3002", "method": "1\uff09\u5728\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5e7b\u89c9\u68c0\u6d4b\u6a21\u578b\uff0c\u4f7f\u5176\u5b66\u4e60\u4ee3\u7801\u8bed\u6cd5\u7279\u5f81\uff1b2\uff09\u8bbe\u8ba1\u63a2\u6d4b\u7f51\u7edc\u5c06\u9ad8\u7ef4\u6f5c\u5728\u5411\u91cf\u6295\u5f71\u5230\u4f4e\u7ef4\u8bed\u6cd5\u5b50\u7a7a\u95f4\uff0c\u751f\u6210\u5411\u91cf\u5143\u7ec4\u5e76\u91cd\u5efa\u9884\u6d4b\u62bd\u8c61\u8bed\u6cd5\u6811\uff1b3\uff09\u901a\u8fc7\u6bd4\u8f83\u9884\u6d4bAST\u548c\u539f\u59cbAST\u8bc6\u522b\u5173\u952e\u8bed\u6cd5\u7ed3\u6784\uff0c\u5b9a\u4f4d\u5e7b\u89c9\u4ee3\u7801\u884c\u3002", "result": "CoHalLo\u5728\u624b\u52a8\u6536\u96c6\u7684\u4ee3\u7801\u5e7b\u89c9\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1aTop-1\u51c6\u786e\u73870.4253\uff0cTop-3\u51c6\u786e\u73870.6149\uff0cTop-5\u51c6\u786e\u73870.7356\uff0cTop-10\u51c6\u786e\u73870.8333\uff0cIFA\u4e3a5.73\uff0cRecall@1% Effort\u4e3a0.052721\uff0cEffort@20% Recall\u4e3a0.155269\uff0c\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CoHalLo\u901a\u8fc7\u63a2\u6d4b\u5e7b\u89c9\u68c0\u6d4b\u6a21\u578b\u7684\u9690\u85cf\u5c42\u5411\u91cf\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u884c\u7ea7\u4ee3\u7801\u5e7b\u89c9\u5b9a\u4f4d\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u4ee3\u7801\u53ef\u9760\u6027\u6539\u8fdb\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "2512.23739", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23739", "abs": "https://arxiv.org/abs/2512.23739", "authors": ["Michaela Levi-Richter", "Reuth Mirsky", "Oren Glickman"], "title": "Break Out the Silverware -- Semantic Understanding of Stored Household Items", "comment": "Poster presented at the Israeli Seminar on Computational Linguistics 2025", "summary": "``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots' cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.\n  Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants' kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.\n  To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.\n  We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5b58\u50a8\u5bb6\u5ead\u7269\u54c1\u6311\u6218\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u670d\u52a1\u673a\u5668\u4eba\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86NOAM\u6df7\u5408\u4ee3\u7406\u7ba1\u9053\uff0c\u7ed3\u5408\u89c6\u89c9\u573a\u666f\u7406\u89e3\u548cLLM\u63a8\u7406\u6765\u9884\u6d4b\u9690\u85cf\u7269\u54c1\u7684\u5b58\u50a8\u4f4d\u7f6e\u3002", "motivation": "\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u662f\u7f3a\u4e4f\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u6765\u63a8\u65ad\u65e5\u5e38\u7269\u54c1\uff08\u901a\u5e38\u9690\u85cf\u5728\u62bd\u5c49\u3001\u6a71\u67dc\u7b49\u4e0d\u53ef\u89c1\u4f4d\u7f6e\uff09\u7684\u5b58\u50a8\u4f4d\u7f6e\u3002\u73b0\u6709\u6280\u672f\u5728\u89c6\u89c9\u548c\u64cd\u4f5c\u65b9\u9762\u6709\u8fdb\u5c55\uff0c\u4f46\u5728\u8ba4\u77e5\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u63d0\u51faNOAM\uff08\u975e\u53ef\u89c1\u7269\u54c1\u5206\u914d\u6a21\u578b\uff09\u6df7\u5408\u4ee3\u7406\u7ba1\u9053\uff1a1\uff09\u5c06\u89c6\u89c9\u8f93\u5165\u8f6c\u6362\u4e3a\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u53ef\u89c1\u5bb9\u5668\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff1b2\uff09\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u63a8\u7406\u6700\u53ef\u80fd\u7684\u9690\u85cf\u5b58\u50a8\u4f4d\u7f6e\uff1b3\uff09\u8bbe\u8ba1\u4e24\u4e2a\u6570\u636e\u96c6\uff1a100\u4e2a\u771f\u5b9e\u53a8\u623f\u7269\u54c1-\u56fe\u50cf\u5bf9\u548c6500\u4e2a\u5e26\u5b58\u50a8\u591a\u8fb9\u5f62\u6807\u6ce8\u7684\u516c\u5f00\u53a8\u623f\u56fe\u50cf\u3002", "result": "NOAM\u76f8\u6bd4\u968f\u673a\u9009\u62e9\u3001\u89c6\u89c9\u8bed\u8a00\u7ba1\u9053\uff08Grounding-DINO + SAM\uff09\u3001\u591a\u6a21\u6001\u6a21\u578b\uff08Gemini\u3001GPT-4o\u7b49\uff09\u548c\u4eba\u7c7b\u8868\u73b0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u8868\u73b0\u3002", "conclusion": "NOAM\u5c55\u793a\u4e86\u7ed3\u5408\u7ed3\u6784\u5316\u573a\u666f\u7406\u89e3\u548cLLM\u63a8\u7406\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u90e8\u7f72\u5177\u6709\u8ba4\u77e5\u80fd\u529b\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u6700\u4f73\u5b9e\u8df5\uff0c\u63a8\u52a8\u4e86\u670d\u52a1\u673a\u5668\u4eba\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.23770", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23770", "abs": "https://arxiv.org/abs/2512.23770", "authors": ["Ankit Kanwar", "Dominik Wagner", "Luke Ong"], "title": "Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions", "comment": null, "summary": "Reinforcement learning (RL) in safety-critical domains requires agents to maximise rewards while strictly adhering to safety constraints. Existing approaches, such as Lagrangian and projection-based methods, often either fail to ensure near-zero safety violations or sacrifice reward performance in the face of hard constraints. We propose Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a new trust-region algorithm for hard-constrained RL. SB-TRPO adaptively biases policy updates towards constraint satisfaction while still seeking reward improvement. Concretely, it performs trust-region updates using a convex combination of the natural policy gradients of cost and reward, ensuring a fixed fraction of optimal cost reduction at each step. We provide a theoretical guarantee of local progress towards safety, with reward improvement when gradients are suitably aligned. Experiments on standard and challenging Safety Gymnasium tasks show that SB-TRPO consistently achieves the best balance of safety and meaningful task completion compared to state-of-the-art methods.", "AI": {"tldr": "SB-TRPO\u662f\u4e00\u79cd\u65b0\u7684\u4fe1\u4efb\u533a\u57df\u7b97\u6cd5\uff0c\u7528\u4e8e\u786c\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5c06\u7b56\u7565\u66f4\u65b0\u504f\u5411\u7ea6\u675f\u6ee1\u8db3\uff0c\u540c\u65f6\u5bfb\u6c42\u5956\u52b1\u6539\u8fdb\uff0c\u5728\u5b89\u5168\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u4e4b\u95f4\u5b9e\u73b0\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u62c9\u683c\u6717\u65e5\u6cd5\u548c\u6295\u5f71\u6cd5\uff09\u5f80\u5f80\u65e0\u6cd5\u786e\u4fdd\u8fd1\u4e4e\u96f6\u7684\u5b89\u5168\u8fdd\u89c4\uff0c\u6216\u8005\u5728\u9762\u5bf9\u786c\u7ea6\u675f\u65f6\u727a\u7272\u5956\u52b1\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6700\u5927\u5316\u5956\u52b1\u53c8\u80fd\u4e25\u683c\u9075\u5b88\u5b89\u5168\u7ea6\u675f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSafety-Biased Trust Region Policy Optimisation (SB-TRPO)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u4fe1\u4efb\u533a\u57df\u7b97\u6cd5\u3002\u5b83\u4f7f\u7528\u6210\u672c\u548c\u5956\u52b1\u7684\u81ea\u7136\u7b56\u7565\u68af\u5ea6\u7684\u51f8\u7ec4\u5408\u8fdb\u884c\u4fe1\u4efb\u533a\u57df\u66f4\u65b0\uff0c\u786e\u4fdd\u6bcf\u4e00\u6b65\u90fd\u6709\u56fa\u5b9a\u6bd4\u4f8b\u7684\u6700\u4f18\u6210\u672c\u51cf\u5c11\u3002\u7b97\u6cd5\u81ea\u9002\u5e94\u5730\u5c06\u7b56\u7565\u66f4\u65b0\u504f\u5411\u7ea6\u675f\u6ee1\u8db3\uff0c\u540c\u65f6\u4ecd\u5bfb\u6c42\u5956\u52b1\u6539\u8fdb\u3002", "result": "\u5728\u6807\u51c6\u4e14\u5177\u6709\u6311\u6218\u6027\u7684Safety Gymnasium\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSB-TRPO\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u59cb\u7ec8\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u548c\u6709\u610f\u4e49\u7684\u4efb\u52a1\u5b8c\u6210\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "SB-TRPO\u4e3a\u786c\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u786e\u4fdd\u5b89\u5168\u6027\u7684\u540c\u65f6\u4fdd\u6301\u5956\u52b1\u6027\u80fd\uff0c\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.24560", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24560", "abs": "https://arxiv.org/abs/2512.24560", "authors": ["David Gros", "Prem Devanbu"], "title": "Localized Calibrated Uncertainty in Code Language Models", "comment": null, "summary": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u4f4dLLM\u751f\u6210\u4ee3\u7801\u4e2d\u4e0e\u7528\u6237\u610f\u56fe\u4e0d\u5339\u914d\u90e8\u5206\u7684\u6280\u672f\uff0c\u901a\u8fc7\u521b\u5efa\"\u6700\u5c0f\u610f\u56fe\u5bf9\u9f50\u8865\u4e01\"\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u767d\u76d2\u63a2\u6d4b\u3001\u9ed1\u76d2\u53cd\u601d\u548c\u81ea\u4e00\u81f4\u6027\u7b49\u65b9\u6cd5\u5728\u9884\u6d4b\u4ee3\u7801\u7f16\u8f91\u4f4d\u7f6e\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "LLM\u751f\u6210\u7684\u4ee3\u7801\u53ef\u80fd\u504f\u79bb\u7528\u6237\u610f\u56fe\uff0c\u9700\u8981\u76d1\u7763\u548c\u7f16\u8f91\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u9700\u8981\u6280\u672f\u6765\u5b9a\u4f4d\u751f\u6210\u4ee3\u7801\u4e2d\u53ef\u80fd\u4e0d\u5339\u914d\u7684\u90e8\u5206\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u9ad8\u6548\u5730\u5ba1\u67e5\u548c\u4fee\u6539\u4ee3\u7801\u3002", "method": "1) \u521b\u5efa\"\u6700\u5c0f\u610f\u56fe\u5bf9\u9f50\u8865\u4e01\"\u6570\u636e\u96c6\uff0c\u5305\u542b\u4fee\u590d\u540e\u7684LLM\u751f\u6210\u7a0b\u5e8f\uff1b2) \u4f7f\u7528\u6d4b\u8bd5\u7528\u4f8b\u9a8c\u8bc1\u6b63\u786e\u6027\uff1b3) \u6bd4\u8f83\u591a\u79cd\u6280\u672f\uff1a\u767d\u76d2\u63a2\u6d4b\uff08\u63d0\u51fa\u9ad8\u6548\u4efb\u610f\u8de8\u5ea6\u67e5\u8be2\u6280\u672f\uff09\u3001\u9ed1\u76d2\u53cd\u601d\u548c\u81ea\u4e00\u81f4\u6027\u65b9\u6cd5\uff1b4) \u8bc4\u4f30\u8fd9\u4e9b\u6280\u672f\u4e3a\u4ee3\u7801\u90e8\u5206\u5206\u914d\u6821\u51c6\u6982\u7387\u7684\u80fd\u529b\uff0c\u9884\u6d4b\u54ea\u4e9b\u4ee3\u7801\u884c\u4f1a\u88ab\u7f16\u8f91\u3002", "result": "\u4f7f\u7528\u5c0f\u578b\u76d1\u7763\u6a21\u578b\u7684\u63a2\u9488\u80fd\u591f\u5b9e\u73b0\u8f83\u4f4e\u7684\u6821\u51c6\u8bef\u5dee\uff0cBrier\u6280\u80fd\u8bc4\u5206\u7ea6\u4e3a0.2\uff0c\u5728\u9884\u6d4b\u7531\u5927\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u7f16\u8f91\u884c\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002\u63a2\u9488\u4ec5\u5728\u4ee3\u7801\u4e0a\u8bad\u7ec3\uff0c\u4f46\u5728\u5141\u8bb8\u65b0\u7684\u6982\u7387\u7f29\u653e\u65f6\uff0c\u663e\u793a\u51fa\u5411\u81ea\u7136\u8bed\u8a00\u9519\u8bef\u6cdb\u5316\u7684\u8ff9\u8c61\u3002", "conclusion": "\u63d0\u51fa\u7684\u6280\u672f\u80fd\u591f\u6709\u6548\u5b9a\u4f4dLLM\u751f\u6210\u4ee3\u7801\u4e2d\u4e0e\u7528\u6237\u610f\u56fe\u4e0d\u5339\u914d\u7684\u90e8\u5206\uff0c\u5c0f\u578b\u76d1\u7763\u6a21\u578b\u63a2\u9488\u5728\u9884\u6d4b\u7f16\u8f91\u4f4d\u7f6e\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002\u8fd9\u4e9b\u6280\u672f\u4e0eAI\u76d1\u7763\u548c\u63a7\u5236\u76f8\u5173\uff0c\u663e\u793a\u51fa\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2512.24461", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24461", "abs": "https://arxiv.org/abs/2512.24461", "authors": ["Seohui Bae", "Jeonghye Kim", "Youngchul Sung", "Woohyung Lim"], "title": "Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents", "comment": null, "summary": "In this paper, we propose a test-time adaptive agent that performs exploratory inference through posterior-guided belief refinement without relying on gradient-based updates or additional training for LLM agent operating under partial observability. Our agent maintains an external structured belief over the environment state, iteratively updates it via action-conditioned observations, and selects actions by maximizing predicted information gain over the belief space. We estimate information gain using a lightweight LLM-based surrogate and assess world alignment through a novel reward that quantifies the consistency between posterior belief and ground-truth environment configuration. Experiments show that our method outperforms inference-time scaling baselines such as prompt-augmented or retrieval-enhanced LLMs, in aligning with latent world states with significantly lower integration overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u540e\u9a8c\u5f15\u5bfc\u7684\u4fe1\u5ff5\u7cbe\u5316\u8fdb\u884c\u63a2\u7d22\u6027\u63a8\u7406\uff0c\u65e0\u9700\u68af\u5ea6\u66f4\u65b0\u6216\u989d\u5916\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684LLM\u667a\u80fd\u4f53\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\uff0cLLM\u667a\u80fd\u4f53\u9700\u8981\u6709\u6548\u63a8\u65ad\u9690\u85cf\u7684\u4e16\u754c\u72b6\u6001\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u68af\u5ea6\u66f4\u65b0\u6216\u989d\u5916\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u7075\u6d3b\u3002", "method": "\u667a\u80fd\u4f53\u7ef4\u62a4\u5916\u90e8\u7ed3\u6784\u5316\u4fe1\u5ff5\u8868\u793a\uff0c\u901a\u8fc7\u52a8\u4f5c\u6761\u4ef6\u89c2\u5bdf\u8fed\u4ee3\u66f4\u65b0\uff0c\u9009\u62e9\u6700\u5927\u5316\u4fe1\u5ff5\u7a7a\u95f4\u4fe1\u606f\u589e\u76ca\u7684\u52a8\u4f5c\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u4ee3\u7406\u4f30\u8ba1\u4fe1\u606f\u589e\u76ca\uff0c\u5e76\u901a\u8fc7\u540e\u9a8c\u4fe1\u5ff5\u4e0e\u771f\u5b9e\u73af\u5883\u914d\u7f6e\u7684\u4e00\u81f4\u6027\u6765\u8bc4\u4f30\u4e16\u754c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6f5c\u5728\u4e16\u754c\u72b6\u6001\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u63a8\u7406\u65f6\u6269\u5c55\u57fa\u7ebf\uff08\u5982\u63d0\u793a\u589e\u5f3a\u6216\u68c0\u7d22\u589e\u5f3a\u7684LLMs\uff09\uff0c\u4e14\u96c6\u6210\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u901a\u8fc7\u540e\u9a8c\u5f15\u5bfc\u7684\u4fe1\u5ff5\u7cbe\u5316\uff0c\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u63a2\u7d22\u63a8\u7406\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2512.24570", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24570", "abs": "https://arxiv.org/abs/2512.24570", "authors": ["Shiqi Kuang", "Zhao Tian", "Tao Xiao", "Dong Wang", "Junjie Chen"], "title": "On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.", "AI": {"tldr": "\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u4e94\u79cd\u5e38\u7528\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u6280\u672f\u53ca\u5176\u7ec4\u5408\u5bf9LLM\u4ee3\u7801\u751f\u6210\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6570\u636e\u5408\u6210\u5bf9\u529f\u80fd\u6b63\u786e\u6027\u6700\u6709\u6548\uff0c\u800c\u6570\u636e\u5408\u6210+\u6570\u636e\u91cd\u6784\u7ec4\u5408\u6574\u4f53\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u8bb8\u591a\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u6280\u672f\u88ab\u63d0\u51fa\u7528\u4e8e\u63d0\u9ad8LLM\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u8fd9\u4e9b\u6280\u672f\u7684\u6574\u4f53\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u56db\u4e2aLLM\u4e0a\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e94\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u6280\u672f\u53ca\u5176\u4e24\u4e24\u7ec4\u5408\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u3001\u6570\u636e\u91cd\u6784\u3001\u6570\u636e\u6e05\u6d17\u3001\u6570\u636e\u9009\u62e9\u7b49\u3002", "result": "\u6570\u636e\u5408\u6210\u5728\u63d0\u9ad8\u529f\u80fd\u6b63\u786e\u6027\u548c\u51cf\u5c11\u4ee3\u7801\u5f02\u5473\u65b9\u9762\u6700\u6709\u6548\uff1b\u5927\u591a\u6570\u7ec4\u5408\u4e0d\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4f46\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\uff1b\u6570\u636e\u5408\u6210+\u6570\u636e\u91cd\u6784\u7ec4\u5408\u6574\u4f53\u8868\u73b0\u6700\u5f3a\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u548c\u7ec4\u5408\u7b56\u7565\u8fdb\u884c\u7cfb\u7edf\u7406\u89e3\u7684\u7b2c\u4e00\u6b65\uff0c\u4e3a\u672a\u6765LLM\u4ee3\u7801\u751f\u6210\u7684\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "code agent"}}
{"id": "2512.24630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24630", "abs": "https://arxiv.org/abs/2512.24630", "authors": ["Md Nahidul Islam Opu", "Shahidul Islam", "Muhammad Asaduzzaman", "Shaiful Chowdhury"], "title": "How Do Agentic AI Systems Address Performance Optimizations? A BERTopic-Based Analysis of Pull Requests", "comment": null, "summary": "LLM-based software engineering is influencing modern software development. In addition to correctness, prior studies have also examined the performance of software artifacts generated by AI agents. However, it is unclear how exactly the agentic AI systems address performance concerns in practice. In this paper, we present an empirical study of performance-related pull requests generated by AI agents. Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics grouped into 10 higher-level categories. Our results show that AI agents apply performance optimizations across diverse layers of the software stack and that the type of optimization significantly affects pull request acceptance rates and review times. We also found that performance optimization by AI agents primarily occurs during the development phase, with less focus on the maintenance phase. Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86AI\u4ee3\u7406\u751f\u6210\u7684\u6027\u80fd\u76f8\u5173PR\uff0c\u8bc6\u522b\u4e8652\u4e2a\u6027\u80fd\u4e3b\u9898\u548c10\u4e2a\u9ad8\u5c42\u7c7b\u522b\uff0c\u53d1\u73b0AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u6808\u5404\u5c42\u5e94\u7528\u6027\u80fd\u4f18\u5316\uff0c\u4e14\u4f18\u5316\u7c7b\u578b\u663e\u8457\u5f71\u54cdPR\u63a5\u53d7\u7387\u548c\u5ba1\u67e5\u65f6\u95f4\u3002", "motivation": "\u867d\u7136LLM\u8f6f\u4ef6\u5de5\u7a0b\u5df2\u5f71\u54cd\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\uff0c\u4e14\u5148\u524d\u7814\u7a76\u5173\u6ce8AI\u751f\u6210\u8f6f\u4ef6\u7684\u6b63\u786e\u6027\u548c\u6027\u80fd\uff0c\u4f46AI\u4ee3\u7406\u7cfb\u7edf\u5728\u5b9e\u8df5\u4e2d\u5982\u4f55\u5177\u4f53\u89e3\u51b3\u6027\u80fd\u95ee\u9898\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u8bc1\u5206\u6790AI\u4ee3\u7406\u751f\u6210\u7684\u6027\u80fd\u76f8\u5173PR\uff0c\u4e86\u89e3\u5176\u6027\u80fd\u4f18\u5316\u884c\u4e3a\u3002", "method": "\u4f7f\u7528LLM\u8f85\u52a9\u68c0\u6d4b\u548cBERTopic\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u5bf9AI\u4ee3\u7406\u751f\u6210\u7684\u6027\u80fd\u76f8\u5173PR\u8fdb\u884c\u5206\u6790\uff0c\u8bc6\u522b\u6027\u80fd\u4f18\u5316\u4e3b\u9898\u5e76\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u8bc6\u522b\u51fa52\u4e2a\u6027\u80fd\u76f8\u5173\u4e3b\u9898\uff0c\u5206\u4e3a10\u4e2a\u9ad8\u5c42\u7c7b\u522b\uff1bAI\u4ee3\u7406\u5728\u8f6f\u4ef6\u6808\u5404\u5c42\u5e94\u7528\u6027\u80fd\u4f18\u5316\uff1b\u4f18\u5316\u7c7b\u578b\u663e\u8457\u5f71\u54cdPR\u63a5\u53d7\u7387\u548c\u5ba1\u67e5\u65f6\u95f4\uff1b\u6027\u80fd\u4f18\u5316\u4e3b\u8981\u5728\u5f00\u53d1\u9636\u6bb5\u8fdb\u884c\uff0c\u7ef4\u62a4\u9636\u6bb5\u5173\u6ce8\u8f83\u5c11\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbAI\u4ee3\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u4f18\u5316\u884c\u4e3a\u53ca\u5ba1\u67e5\u7ed3\u679c\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bc1\u636e\uff0c\u63ed\u793a\u4e86AI\u4ee3\u7406\u5728\u6027\u80fd\u4f18\u5316\u65b9\u9762\u7684\u5b9e\u9645\u8868\u73b0\u548c\u5c40\u9650\u6027\u3002", "topic": "swe application"}}
{"id": "2512.24505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24505", "abs": "https://arxiv.org/abs/2512.24505", "authors": ["Samuel Golladay", "Majid Bani-Yaghoub"], "title": "Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems", "comment": "7 pages, submitted to ACM Transactions on Intelligent Systems and Technology", "summary": "Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86GPT-4o-mini\u3001Gemini-2.0-Flash\u548cDeepSeek-V3\u5728\u5bc6\u82cf\u91cc\u5927\u5b66\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0DeepSeek-V3\u5728\u6240\u6709\u7c7b\u522b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6240\u6709\u6a21\u578b\u5728\u51e0\u4f55\u95ee\u9898\u4e0a\u8868\u73b0\u90fd\u8f83\u5f31\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u9519\u8bef\u6a21\u5f0f\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u4f7f\u7528\u76f8\u540c\u6570\u636e\u96c6\u8bc4\u4f30LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u9650\u5236\u4e86\u7ed3\u679c\u7684\u666e\u9002\u6027\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u591a\u6837\u5316\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790LLMs\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u5bc6\u82cf\u91cc\u5927\u5b66\u6570\u5b66\u7ade\u8d5b\u4e2d\u7684\u5fae\u79ef\u5206\u3001\u89e3\u6790\u51e0\u4f55\u548c\u79bb\u6563\u6570\u5b66\u95ee\u9898\uff0c\u6d4b\u8bd5GPT-4o-mini\u3001Gemini-2.0-Flash\u548cDeepSeek-V3\u4e09\u4e2a\u9886\u5148LLMs\u3002\u5c06\u6a21\u578b\u56de\u7b54\u4e0e\u5df2\u77e5\u6b63\u786e\u7b54\u6848\u6bd4\u8f83\uff0c\u5206\u6790\u51c6\u786e\u7387\uff0c\u5e76\u6df1\u5165\u5206\u6790\u63a8\u7406\u8fc7\u7a0b\u4ee5\u63a2\u7d22\u9519\u8bef\u6a21\u5f0f\u3002", "result": "DeepSeek-V3\u5728\u5fae\u79ef\u5206\u3001\u89e3\u6790\u51e0\u4f55\u548c\u79bb\u6563\u6570\u5b66\u4e09\u4e2a\u7c7b\u522b\u4e2d\u8868\u73b0\u6700\u4f73\u3002\u6240\u6709\u4e09\u4e2aLLMs\u5728\u51e0\u4f55\u95ee\u9898\u4e0a\u8868\u73b0\u663e\u8457\u8f83\u5f31\u3002DeepSeek-V3\u7684\u4e3b\u8981\u9519\u8bef\u662f\u8ba1\u7b97\u548c\u903b\u8f91\u9519\u8bef\uff0cGPT-4o-mini\u5e38\u89c1\u903b\u8f91\u548c\u65b9\u6cd5\u76f8\u5173\u9519\u8bef\uff0cGemini\u5219\u503e\u5411\u4e8e\u4e0d\u5b8c\u6574\u63a8\u7406\u548c\u4ed3\u4fc3\u7ed3\u8bba\u3002", "conclusion": "\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6570\u5b66\u7ade\u8d5b\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30LLMs\u80fd\u591f\u63d0\u4f9b\u5bf9\u5176\u72ec\u7279\u9519\u8bef\u6a21\u5f0f\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u5e76\u7a81\u663e\u7ed3\u6784\u5316\u63a8\u7406\uff08\u7279\u522b\u662f\u5728\u51e0\u4f55\u9886\u57df\uff09\u4e2d\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2512.24532", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24532", "abs": "https://arxiv.org/abs/2512.24532", "authors": ["Amir Tahmasbi", "Sadegh Majidi", "Kazem Taram", "Aniket Bera"], "title": "From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning", "comment": null, "summary": "Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5\u63d0\u5347LLM\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff1a\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5b66\u4e60\u57fa\u7840\u7a7a\u95f4\u53d8\u6362\uff08\u65cb\u8f6c\u3001\u5e73\u79fb\u3001\u7f29\u653e\uff09\uff0c\u518d\u51bb\u7ed3\u6a21\u578b\u5e76\u8bad\u7ec3LoRA\u9002\u914d\u5668\u5b66\u4e60\u591a\u6b65\u89c4\u5212\u7b56\u7565\uff0c\u5728ASCII\u827a\u672f\u73af\u5883\u4e2d\u8fdb\u884c\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u8bed\u8a00\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u53d8\u6362\u548c\u591a\u6b65\u89c4\u5212\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5bfc\u822a\u548c\u89c4\u5212\u7b49\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "1. \u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff1a\u5728\u57fa\u7840\u7a7a\u95f4\u53d8\u6362\uff08\u65cb\u8f6c\u3001\u5e73\u79fb\u3001\u7f29\u653e\uff09\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u638c\u63e1\u7a7a\u95f4\u7269\u7406\u77e5\u8bc6\uff1b2. \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff1a\u51bb\u7ed3\u7269\u7406\u611f\u77e5\u6a21\u578b\uff0c\u5728GRPO\u6846\u67b6\u4e0b\u8bad\u7ec3\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\uff0c\u5b66\u4e60\u5c06\u57fa\u7840\u6784\u5efa\u5757\u7ec4\u5408\u6210\u591a\u6b65\u89c4\u5212\u7b56\u7565\uff1b3. \u6784\u5efaASCII\u827a\u672f\u6570\u636e\u96c6\u548c\u5bf9\u5e94\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\uff08\u6709\u663e\u5f0f\u72b6\u6001\u66f4\u65b0\uff09\u548c\u9759\u6001\u73af\u5883\uff08\u4f9d\u8d56\u5185\u90e8\u72b6\u6001\uff09\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff08\u901a\u7528\u9aa8\u5e72\u6a21\u578b\u3001\u7269\u7406\u611f\u77e5\u6a21\u578b\u548c\u7aef\u5230\u7aefRL\u6a21\u578b\uff09\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u8bad\u7ec3\u66f4\u7a33\u5b9a\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6790\u9a8c\u8bc1\u4e86\u5fae\u8c03\u786e\u5b9e\u63d0\u5347\u4e86\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u5c06\u7a7a\u95f4\u63a8\u7406\u5206\u89e3\u4e3a\u539f\u5b50\u6784\u5efa\u5757\u53ca\u5176\u7ec4\u5408\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5bfc\u822a\u548c\u89c4\u5212\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.24601", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24601", "abs": "https://arxiv.org/abs/2512.24601", "authors": ["Alex L. Zhang", "Tim Kraska", "Omar Khattab"], "title": "Recursive Language Models", "comment": "9 pages, 33 with Appendix", "summary": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.", "AI": {"tldr": "\u63d0\u51fa\u9012\u5f52\u8bed\u8a00\u6a21\u578b\uff08RLMs\uff09\u4f5c\u4e3a\u5904\u7406\u8d85\u957f\u63d0\u793a\u7684\u63a8\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u68c0\u67e5\u3001\u5206\u89e3\u548c\u9012\u5f52\u8c03\u7528\u5904\u7406\u63d0\u793a\u7247\u6bb5\uff0c\u5b9e\u73b0\u8d85\u51fa\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u8f93\u5165\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\uff0c\u65e0\u6cd5\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u63d0\u793a\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9LLMs\u80fd\u591f\u5904\u7406\u8d85\u51fa\u5176\u539f\u59cb\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u957f\u63d0\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u8bed\u8a00\u6a21\u578b\uff08RLMs\uff09\u63a8\u7406\u7b56\u7565\uff1a\u5c06\u957f\u63d0\u793a\u89c6\u4e3a\u5916\u90e8\u73af\u5883\uff0c\u8ba9LLM\u7a0b\u5e8f\u5316\u5730\u68c0\u67e5\u3001\u5206\u89e3\u63d0\u793a\uff0c\u5e76\u9012\u5f52\u8c03\u7528\u81ea\u8eab\u5904\u7406\u63d0\u793a\u7247\u6bb5\u3002\u8fd9\u662f\u4e00\u79cd\u901a\u7528\u7684\u63a8\u7406\u65f6\u6269\u5c55\u65b9\u6cd5\u3002", "result": "RLMs\u6210\u529f\u5904\u7406\u8d85\u51fa\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u8f93\u5165\uff0c\u5728\u56db\u4e2a\u4e0d\u540c\u7684\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\uff0c\u5373\u4f7f\u5bf9\u4e8e\u8f83\u77ed\u7684\u63d0\u793a\uff0c\u4e5f\u663e\u8457\u4f18\u4e8e\u57fa\u7840LLMs\u548c\u5e38\u89c1\u7684\u957f\u4e0a\u4e0b\u6587\u6846\u67b6\uff0c\u540c\u65f6\u6bcf\u4e2a\u67e5\u8be2\u7684\u6210\u672c\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "RLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u4f7fLLMs\u80fd\u591f\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u63d0\u793a\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u7684\u540c\u65f6\u63a7\u5236\u6210\u672c\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2512.24858", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24858", "abs": "https://arxiv.org/abs/2512.24858", "authors": ["Ke Ma", "Jianjun Huang", "Wei You", "Bin Liang", "Jingzheng Wu", "Yanjun Wu", "Yuanjun Gong"], "title": "Feature Slice Matching for Precise Bug Detection", "comment": "Accepted by FSE2026", "summary": "Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.", "AI": {"tldr": "MATUS\u901a\u8fc7\u7279\u5f81\u5207\u7247\u548c\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u6765\u68c0\u6d4b\u8f6f\u4ef6bug\uff0c\u80fd\u6709\u6548\u6291\u5236\u76ee\u6807\u4ee3\u7801\u4e2d\u7684\u566a\u58f0\u5e72\u6270\uff0c\u5728Linux\u5185\u6838\u4e2d\u53d1\u73b031\u4e2a\u672a\u77e5bug", "motivation": "\u73b0\u6709\u57fa\u4e8e\u51fd\u6570\u76f8\u4f3c\u5ea6\u7684bug\u68c0\u6d4b\u65b9\u6cd5\u53d7\u65e0\u5173\u8bed\u53e5\u7684\u566a\u58f0\u5e72\u6270\u5f71\u54cd\u6027\u80fd\uff0c\u73b0\u6709\u5de5\u4f5c\u672a\u80fd\u6709\u6548\u6d88\u9664\u76ee\u6807\u4ee3\u7801\u4e2d\u7684\u566a\u58f0", "method": "\u4ecebuggy\u67e5\u8be2\u548c\u76ee\u6807\u4ee3\u7801\u4e2d\u63d0\u53d6\u7279\u5f81\u5207\u7247\u8868\u793a\u8bed\u4e49\u7279\u5f81\uff0c\u5229\u7528buggy\u4ee3\u7801\u7684\u5148\u9a8c\u77e5\u8bc6\u6307\u5bfc\u76ee\u6807\u5207\u7247\uff0c\u7aef\u5230\u7aef\u5730\u786e\u5b9a\u5207\u7247\u6807\u51c6\uff0c\u901a\u8fc7\u5411\u91cf\u76f8\u4f3c\u5ea6\u6bd4\u8f83\u7279\u5f81\u5207\u7247", "result": "MATUS\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u5177\u6709bug\u68c0\u6d4b\u4f18\u52bf\u4e14\u6548\u7387\u53ef\u63a5\u53d7\uff0c\u5728Linux\u5185\u6838\u4e2d\u53d1\u73b031\u4e2a\u672a\u77e5bug\uff0c\u5168\u90e8\u5f97\u5230\u5185\u6838\u5f00\u53d1\u8005\u786e\u8ba4\uff0c\u5176\u4e2d11\u4e2a\u88ab\u5206\u914d\u4e86CVE\u7f16\u53f7", "conclusion": "MATUS\u901a\u8fc7\u6291\u5236\u76ee\u6807\u566a\u58f0\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684bug\u68c0\u6d4b\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u7684bug\u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027", "topic": "swe benchmark"}}
{"id": "2512.24609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24609", "abs": "https://arxiv.org/abs/2512.24609", "authors": ["Dong Qiu", "Duo Xu", "Limengxi Yue"], "title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization", "comment": "Accepted by IEEE ICFTIC 2025", "summary": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7Dec-POMDP\u5efa\u6a21\u534f\u4f5c\uff0c\u91c7\u7528CTDE\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u534f\u4f5c\u5199\u4f5c\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524dLLM\u5728\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7f3a\u4e4f\u534f\u4f5c\u610f\u8bc6\uff0c\u96be\u4ee5\u4f18\u5316\u5168\u5c40\u6027\u80fd\u3002\u9700\u8981\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u534f\u8c03\u95ee\u9898\u3002", "method": "\u5c06\u534f\u4f5c\u5efa\u6a21\u4e3a\u5206\u6563\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(Dec-POMDP)\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c(CTDE)\u6846\u67b6\u3002\u63d0\u51faGroup Relative Policy Optimization(GRPO)\u7b97\u6cd5\uff0c\u5728\u8bad\u7ec3\u65f6\u5229\u7528\u5168\u5c40\u4fe1\u53f7\u8054\u5408\u4f18\u5316\u667a\u80fd\u4f53\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u7b80\u5316\u7684\u8054\u5408\u5956\u52b1\u51fd\u6570\u5e73\u8861\u4efb\u52a1\u8d28\u91cf\u3001\u901f\u5ea6\u548c\u534f\u8c03\u6210\u672c\u3002", "result": "\u5728\u534f\u4f5c\u5199\u4f5c\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a\u4efb\u52a1\u5904\u7406\u901f\u5ea6\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u9ad83\u500d\uff1b\u5199\u4f5c\u4efb\u52a1\u8fbe\u523098.7%\u7684\u7ed3\u6784/\u98ce\u683c\u4e00\u81f4\u6027\uff1b\u7f16\u7a0b\u4efb\u52a1\u8fbe\u523074.6%\u7684\u6d4b\u8bd5\u901a\u8fc7\u7387\u3002\u4e00\u81f4\u4f18\u4e8e\u5f3a\u5927\u591a\u667a\u80fd\u4f53LLM\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u7684\u53ef\u9760\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3aLLM\u667a\u80fd\u4f53\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.23966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23966", "abs": "https://arxiv.org/abs/2512.23966", "authors": ["Chen Zhang", "Yang Bai", "Jiahuan Li", "Anchun Gui", "Keheng Wang", "Feifan Liu", "Guanyu Wu", "Yuwei Jiang", "Defei Bu", "Li Wei", "Haihang Jing", "Hongyin Tang", "Xin Chen", "Xiangzhou Huang", "Fengcun Li", "Rongxiang Weng", "Yulei Qian", "Yifan Lu", "Yerui Sun", "Jingang Wang", "Yuchen Xie", "Xunliang Cai"], "title": "Efficient Context Scaling with LongCat ZigZag Attention", "comment": "10 pages, 3 figures, 3 tables", "summary": "We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.", "AI": {"tldr": "LoZA\u662f\u4e00\u79cd\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6848\uff0c\u53ef\u5c06\u73b0\u6709\u5168\u6ce8\u610f\u529b\u6a21\u578b\u8f6c\u6362\u4e3a\u7a00\u758f\u7248\u672c\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u5904\u7406\u901f\u5ea6", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u5168\u6ce8\u610f\u529b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9884\u586b\u5145\u5bc6\u96c6\u578b\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u548c\u89e3\u7801\u5bc6\u96c6\u578b\uff08\u5982\u5de5\u5177\u96c6\u6210\u63a8\u7406\uff09\u4efb\u52a1\u4e2d", "method": "\u63d0\u51faLongCat ZigZag Attention (LoZA)\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u5168\u6ce8\u610f\u529b\u6a21\u578b\u8f6c\u6362\u4e3a\u7a00\u758f\u7248\u672c\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5904\u7406", "result": "LoZA\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff0c\u5e94\u7528\u4e8eLongCat-Flash\u6a21\u578b\u540e\u5f97\u5230LongCat-Flash-Exp\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u9ad8\u8fbe100\u4e07\u4e2atoken\uff0c\u652f\u6301\u957f\u671f\u63a8\u7406\u548c\u957f\u89c6\u91ce\u667a\u80fd\u4f53\u80fd\u529b", "conclusion": "LoZA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6848\uff0c\u80fd\u591f\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6548\u7387\uff0c\u4e3a\u957f\u671f\u63a8\u7406\u548c\u667a\u80fd\u4f53\u5e94\u7528\u63d0\u4f9b\u652f\u6301", "topic": "agent analysis"}}
{"id": "2512.24613", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24613", "abs": "https://arxiv.org/abs/2512.24613", "authors": ["Zheyu Shi", "Dong Qiu", "Shanlong Yu"], "title": "Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning", "comment": "Accepted by IEEE ITCA 2025", "summary": "This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7fa4\u4f53\u5ba1\u8bae\u7684\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u5c42\u89d2\u8272\u67b6\u6784\uff08\u751f\u6210\u3001\u9a8c\u8bc1\u3001\u6574\u5408\uff09\u548c\u81ea\u535a\u5f08\u673a\u5236\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u548c\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5355\u4e00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u89d2\u8272\u67b6\u6784\uff1a\u89c2\u70b9\u751f\u6210\u4ee3\u7406\u4ea7\u751f\u591a\u6837\u5316\u63a8\u7406\u89c6\u89d2\uff0c\u8bc1\u636e\u9a8c\u8bc1\u4ee3\u7406\u68c0\u7d22\u5916\u90e8\u77e5\u8bc6\u5e76\u91cf\u5316\u4e8b\u5b9e\u652f\u6301\u5ea6\uff0c\u4e00\u81f4\u6027\u4ef2\u88c1\u4ee3\u7406\u6574\u5408\u903b\u8f91\u4e00\u81f4\u7684\u7ed3\u8bba\u3002\u5f15\u5165\u81ea\u535a\u5f08\u673a\u5236\u6269\u5c55\u591a\u8def\u5f84\u63a8\u7406\u8f68\u8ff9\uff0c\u68c0\u7d22\u589e\u5f3a\u6a21\u5757\u52a8\u6001\u8865\u5145\u5916\u90e8\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u7ed3\u5408\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u903b\u8f91\u8fde\u8d2f\u6027\u7684\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u5e94\u7528\u6539\u8fdb\u7684\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u534f\u4f5c\u8bad\u7ec3\u3002", "result": "\u5728HotpotQA\u4e0a\u591a\u8df3\u63a8\u7406\u51c6\u786e\u7387\u63d0\u534716.8%\uff0c\u57282WikiMultihopQA\u4e0a\u63d0\u534714.3%\uff0c\u5728MeetingBank\u4e0a\u63d0\u534719.2%\uff0c\u4e00\u81f4\u6027\u63d0\u534721.5%\u3002\u76f8\u6bd4\u4e3b\u6d41\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.24615", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24615", "abs": "https://arxiv.org/abs/2512.24615", "authors": ["Yuchen Shi", "Yuzheng Cai", "Siqi Cai", "Zihan Xu", "Lichao Chen", "Yulei Qin", "Zhijian Zhou", "Xiang Fei", "Chaofan Qiu", "Xiaoyu Tan", "Gang Li", "Zongyi Li", "Haojia Lin", "Guocan Cai", "Yong Mao", "Yunsheng Wu", "Ke Li", "Xing Sun"], "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "comment": null, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \\textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \\textbf{Workflow} mode for standard tasks and a \\textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \\textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \\textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "AI": {"tldr": "Youtu-Agent\u662f\u4e00\u4e2a\u6a21\u5757\u5316LLM\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u548c\u6301\u7eed\u6f14\u5316\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u914d\u7f6e\u6210\u672c\u9ad8\u3001\u80fd\u529b\u9759\u6001\u7684\u95ee\u9898\uff0c\u652f\u6301\u5de5\u4f5c\u6d41\u548c\u5143\u4ee3\u7406\u4e24\u79cd\u751f\u6210\u6a21\u5f0f\uff0c\u5e76\u5305\u542b\u5b9e\u8df5\u548c\u5f3a\u5316\u5b66\u4e60\u4e24\u79cd\u4f18\u5316\u6a21\u5757\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u9ad8\u914d\u7f6e\u6210\u672c\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u4ee3\u7406\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u5177\u96c6\u6210\u548c\u63d0\u793a\u5de5\u7a0b\u5de5\u4f5c\uff1b2\uff09\u9759\u6001\u80fd\u529b\uff0c\u5df2\u90e8\u7f72\u4ee3\u7406\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\u3002", "method": "\u63d0\u51faYoutu-Agent\u6846\u67b6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u914d\u7f6e\u7cfb\u7edf\uff08\u89e3\u8026\u6267\u884c\u73af\u5883\u3001\u5de5\u5177\u96c6\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\uff09\uff0c\u652f\u6301\u4e24\u79cd\u751f\u6210\u8303\u5f0f\uff1a\u5de5\u4f5c\u6d41\u6a21\u5f0f\u7528\u4e8e\u6807\u51c6\u4efb\u52a1\uff0c\u5143\u4ee3\u7406\u6a21\u5f0f\u7528\u4e8e\u590d\u6742\u975e\u6807\u51c6\u9700\u6c42\uff08\u80fd\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u3001\u63d0\u793a\u548c\u914d\u7f6e\uff09\u3002\u5efa\u7acb\u6df7\u5408\u7b56\u7565\u4f18\u5316\u7cfb\u7edf\uff1a1\uff09\u4ee3\u7406\u5b9e\u8df5\u6a21\u5757\u901a\u8fc7\u4e0a\u4e0b\u6587\u4f18\u5316\u79ef\u7d2f\u7ecf\u9a8c\uff1b2\uff09\u4ee3\u7406RL\u6a21\u5757\u4e0e\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\u96c6\u6210\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728WebWalkerQA\uff0871.47%\uff09\u548cGAIA\uff0872.8%\uff09\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff1b\u81ea\u52a8\u5316\u751f\u6210\u7ba1\u9053\u5de5\u5177\u5408\u6210\u6210\u529f\u7387\u8d85\u8fc781%\uff1b\u5b9e\u8df5\u6a21\u5757\u5728AIME 2024/2025\u4e0a\u5206\u522b\u63d0\u53472.7%\u548c5.4%\uff1bAgent RL\u8bad\u7ec3\u57287B LLMs\u4e0a\u5b9e\u73b040%\u52a0\u901f\uff0c\u5728\u6570\u5b66\u548c\u901a\u7528/\u591a\u8df3QA\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u5347\u7f16\u7801/\u63a8\u7406\u80fd\u529b35%\u548c\u641c\u7d22\u80fd\u529b21%\u3002", "conclusion": "Youtu-Agent\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u548c\u6301\u7eed\u6f14\u5316\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u6846\u67b6\u7684\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u53ef\u91cd\u7528\u3001\u53ef\u81ea\u9002\u5e94\u4f18\u5316\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "topic": "code agent"}}
{"id": "2512.24014", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24014", "abs": "https://arxiv.org/abs/2512.24014", "authors": ["Sijia Chen", "Di Niu"], "title": "iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning", "comment": "9 pages, 6 figures. The source code is publicly available at https://github.com/AgenticFinLab/latent-planning", "summary": "Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.", "AI": {"tldr": "iCLP\u6846\u67b6\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9690\u5f0f\u89c4\u5212\uff0c\u5728\u8bed\u8a00\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7d27\u51d1\u7684\u6f5c\u5728\u8ba1\u5212\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u663e\u5f0f\u6587\u672c\u8ba1\u5212\u7684\u63a8\u7406\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5927\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u751f\u6210\u4e0d\u51c6\u786e\u7684\u8ba1\u5212\uff1b2\uff09\u4efb\u52a1\u7279\u5b9a\u95ee\u9898\u7684\u9ad8\u5ea6\u591a\u6837\u6027\u4f7f\u5f97\u751f\u6210\u6709\u6548\u8ba1\u5212\u5177\u6709\u6311\u6218\u6027\u3002\u53d7\u4eba\u7c7b\u9690\u5f0f\u8ba4\u77e5\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u9690\u5f0f\u89c4\u5212\u7684\u65b9\u6cd5\u3002", "method": "iCLP\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a1\uff09\u4ece\u73b0\u6709\u7684\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\u4e2d\u63d0\u53d6\u663e\u5f0f\u8ba1\u5212\uff1b2\uff09\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u81ea\u7f16\u7801\u5668\u548c\u7801\u672c\u5b66\u4e60\u8fd9\u4e9b\u8ba1\u5212\u7684\u79bb\u6563\u8868\u793a\uff08\u6f5c\u5728\u8ba1\u5212\uff09\uff1b3\uff09\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u5bf9\u6f5c\u5728\u8ba1\u5212\u548c\u76f8\u5e94\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u6a21\u578b\u5b66\u4f1a\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u9690\u5f0f\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\uff0ciCLP\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u89c4\u5212\uff0c\u540c\u65f6\u5728\u8bed\u8a00\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "iCLP\u901a\u8fc7\u5c06\u663e\u5f0f\u8ba1\u5212\u538b\u7f29\u4e3a\u6f5c\u5728\u8868\u793a\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u9690\u5f0f\u89c4\u5212\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u663e\u5f0f\u6587\u672c\u8ba1\u5212\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u8fd8\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u9ad8\u6548\u53ef\u9760\u7684AI\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "code agent"}}
{"id": "2512.24873", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24873", "abs": "https://arxiv.org/abs/2512.24873", "authors": ["Weixun Wang", "XiaoXiao Xu", "Wanhe An", "Fangwen Dai", "Wei Gao", "Yancheng He", "Ju Huang", "Qiang Ji", "Hanqi Jin", "Xiaoyang Li", "Yang Li", "Zhongwen Li", "Shirong Lin", "Jiashun Liu", "Zenan Liu", "Tao Luo", "Dilxat Muhtar", "Yuanbin Qu", "Jiaqiang Shi", "Qinghui Sun", "Yingshui Tan", "Hao Tang", "Runze Wang", "Yi Wang", "Zhaoguo Wang", "Yanan Wu", "Shaopan Xiong", "Binchen Xu", "Xander Xu", "Yuchi Xu", "Qipeng Zhang", "Xixia Zhang", "Haizhou Zhao", "Jie Zhao", "Shuaibing Zhao", "Baihui Zheng", "Jianhui Zheng", "Suhang Zheng", "Yanni Zhu", "Mengze Cai", "Kerui Cao", "Xitong Chen", "Yue Dai", "Lifan Du", "Tao Feng", "Tao He", "Jin Hu", "Yijie Hu", "Ziyu Jiang", "Cheng Li", "Xiang Li", "Jing Liang", "Chonghuan Liu", "ZhenDong Liu", "Haodong Mi", "Yanhu Mo", "Junjia Ni", "Shixin Pei", "Jingyu Shen", "XiaoShuai Song", "Cecilia Wang", "Chaofan Wang", "Kangyu Wang", "Pei Wang", "Tao Wang", "Wei Wang", "Ke Xiao", "Mingyu Xu", "Tiange Xu", "Nan Ya", "Siran Yang", "Jianan Ye", "Yaxing Zang", "Duo Zhang", "Junbo Zhang", "Boren Zheng", "Wanxi Deng", "Ling Pan", "Lin Qu", "Wenbo Su", "Jiamang Wang", "Wei Wang", "Hu Wei", "Minggang Wu", "Cheng Yu", "Bing Zhao", "Zhicheng Zheng", "Bo Zheng"], "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "comment": "36 pages, 15 figures", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "AI": {"tldr": "ALE\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u542bROLL\u6743\u91cd\u4f18\u5316\u6846\u67b6\u3001ROCK\u8f68\u8ff9\u751f\u6210\u73af\u5883\u548ciFlow CLI\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5de5\u5177\u3002\u57fa\u4e8eALE\u8bad\u7ec3\u7684ROME\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u57fa\u7840\u8bbe\u65bd\uff0c\u96be\u4ee5\u652f\u6301\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u8f6e\u64cd\u4f5c\u3001\u89c2\u5bdf\u7ed3\u679c\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faALE\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1) ROLL\u7528\u4e8e\u6743\u91cd\u4f18\u5316\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff1b2) ROCK\u7528\u4e8e\u8f68\u8ff9\u751f\u6210\u7684\u6c99\u7bb1\u73af\u5883\u7ba1\u7406\u5668\uff1b3) iFlow CLI\u7528\u4e8e\u9ad8\u6548\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u667a\u80fd\u4f53\u6846\u67b6\u3002\u5f00\u53d1\u4e86ROME\u6a21\u578b\uff0c\u91c7\u7528\u6570\u636e\u5408\u6210\u534f\u8bae\u548cIPA\uff08\u57fa\u4e8e\u4ea4\u4e92\u7684\u7b56\u7565\u5bf9\u9f50\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u4ea4\u4e92\u5757\u800c\u975e\u5355\u4e2atoken\u6765\u5206\u914d\u4fe1\u7528\u3002", "result": "ROME\u5728SWE-bench Verified\u548cTerminal Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86ALE\u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u6027\u3002\u540c\u65f6\u63d0\u51fa\u4e86Terminal Bench Pro\u57fa\u51c6\uff0c\u5177\u6709\u66f4\u597d\u7684\u89c4\u6a21\u548c\u6c61\u67d3\u63a7\u5236\u3002", "conclusion": "ALE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u751f\u4ea7\u6d41\u7a0b\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2512.23978", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23978", "abs": "https://arxiv.org/abs/2512.23978", "authors": ["Tinglong Dai", "David Simchi-Levi", "Michelle Xiao Wu", "Yao Xie"], "title": "Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems", "comment": "Authors are listed alphabetically", "summary": "Generative artificial intelligence (GenAI) is shifting from conversational assistants toward agentic systems -- autonomous decision-making systems that sense, decide, and act within operational workflows. This shift creates an autonomy paradox: as GenAI systems are granted greater operational autonomy, they should, by design, embody more formal structure, more explicit constraints, and stronger tail-risk discipline. We argue stochastic generative models can be fragile in operational domains unless paired with mechanisms that provide verifiable feasibility, robustness to distribution shift, and stress testing under high-consequence scenarios. To address this challenge, we develop a conceptual framework for assured autonomy grounded in operations research (OR), built on two complementary approaches. First, flow-based generative models frame generation as deterministic transport characterized by an ordinary differential equation, enabling auditability, constraint-aware generation, and connections to optimal transport, robust optimization, and sequential decision control. Second, operational safety is formulated through an adversarial robustness lens: decision rules are evaluated against worst-case perturbations within uncertainty or ambiguity sets, making unmodeled risks part of the design. This framework clarifies how increasing autonomy shifts OR's role from solver to guardrail to system architect, with responsibility for control logic, incentive protocols, monitoring regimes, and safety boundaries. These elements define a research agenda for assured autonomy in safety-critical, reliability-sensitive operational domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u786e\u4fdd\u751f\u6210\u5f0fAI\u81ea\u4e3b\u7cfb\u7edf\u5b89\u5168\u6027\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5c06\u8fd0\u7b79\u5b66\u65b9\u6cd5\u5e94\u7528\u4e8eAI\u81ea\u4e3b\u51b3\u7b56\u7cfb\u7edf\uff0c\u901a\u8fc7\u6d41\u6a21\u578b\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u89e3\u51b3\u81ea\u4e3b\u6027\u6096\u8bba\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u4ece\u5bf9\u8bdd\u52a9\u624b\u8f6c\u5411\u81ea\u4e3b\u51b3\u7b56\u7cfb\u7edf\uff0c\u8fd9\u4ea7\u751f\u4e86\u81ea\u4e3b\u6027\u6096\u8bba\uff1a\u7cfb\u7edf\u83b7\u5f97\u66f4\u5927\u81ea\u4e3b\u6743\u65f6\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u5f62\u5f0f\u5316\u7ed3\u6784\u3001\u660e\u786e\u7ea6\u675f\u548c\u5c3e\u90e8\u98ce\u9669\u63a7\u5236\u3002\u968f\u673a\u751f\u6210\u6a21\u578b\u5728\u64cd\u4f5c\u9886\u57df\u53ef\u80fd\u8106\u5f31\uff0c\u9700\u8981\u53ef\u9a8c\u8bc1\u7684\u53ef\u884c\u6027\u3001\u5206\u5e03\u504f\u79fb\u9c81\u68d2\u6027\u548c\u9ad8\u540e\u679c\u573a\u666f\u538b\u529b\u6d4b\u8bd5\u673a\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fd0\u7b79\u5b66\u7684\u786e\u4fdd\u81ea\u4e3b\u6027\u6982\u5ff5\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\uff0c\u5c06\u751f\u6210\u89c6\u4e3a\u786e\u5b9a\u6027\u4f20\u8f93\uff0c\u652f\u6301\u53ef\u5ba1\u8ba1\u6027\u3001\u7ea6\u675f\u611f\u77e5\u751f\u6210\uff0c\u8fde\u63a5\u6700\u4f18\u4f20\u8f93\u3001\u9c81\u68d2\u4f18\u5316\u548c\u5e8f\u5217\u51b3\u7b56\u63a7\u5236\uff1b2) \u901a\u8fc7\u5bf9\u6297\u9c81\u68d2\u6027\u89c6\u89d2\u5236\u5b9a\u64cd\u4f5c\u5b89\u5168\u6027\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u6216\u6a21\u7cca\u96c6\u5185\u8bc4\u4f30\u51b3\u7b56\u89c4\u5219\u5bf9\u6297\u6700\u574f\u60c5\u51b5\u6270\u52a8\u3002", "result": "\u8be5\u6846\u67b6\u9610\u660e\u4e86\u589e\u52a0\u81ea\u4e3b\u6027\u5982\u4f55\u5c06\u8fd0\u7b79\u5b66\u7684\u89d2\u8272\u4ece\u6c42\u89e3\u5668\u8f6c\u53d8\u4e3a\u62a4\u680f\u518d\u5230\u7cfb\u7edf\u67b6\u6784\u5e08\uff0c\u8d1f\u8d23\u63a7\u5236\u903b\u8f91\u3001\u6fc0\u52b1\u534f\u8bae\u3001\u76d1\u63a7\u673a\u5236\u548c\u5b89\u5168\u8fb9\u754c\u3002\u8fd9\u4e9b\u8981\u7d20\u5b9a\u4e49\u4e86\u5b89\u5168\u5173\u952e\u3001\u53ef\u9760\u6027\u654f\u611f\u64cd\u4f5c\u9886\u57df\u4e2d\u786e\u4fdd\u81ea\u4e3b\u6027\u7684\u7814\u7a76\u8bae\u7a0b\u3002", "conclusion": "\u4e3a\u786e\u4fdd\u751f\u6210\u5f0fAI\u81ea\u4e3b\u7cfb\u7edf\u5728\u64cd\u4f5c\u5de5\u4f5c\u6d41\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u9700\u8981\u7ed3\u5408\u8fd0\u7b79\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u6a21\u578b\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u6846\u67b6\u89e3\u51b3\u81ea\u4e3b\u6027\u6096\u8bba\uff0c\u5efa\u7acb\u53ef\u9a8c\u8bc1\u3001\u9c81\u68d2\u4e14\u80fd\u5e94\u5bf9\u9ad8\u540e\u679c\u573a\u666f\u7684\u81ea\u4e3b\u51b3\u7b56\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2512.24289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24289", "abs": "https://arxiv.org/abs/2512.24289", "authors": ["Jonathan Schmoll", "Adam Jatowt"], "title": "Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs", "comment": null, "summary": "The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u6b27\u76df\u5206\u7c7b\u6cd5\u5408\u89c4\u8bc4\u4f30\u7684\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u5728\u5b9a\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5728\u5b9a\u91cf\u4efb\u52a1\u4e0a\u5b8c\u5168\u5931\u8d25\uff0c\u53ef\u4f5c\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u7684\u8f85\u52a9\u5de5\u5177\u800c\u975e\u5b8c\u5168\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6b27\u76df\u5206\u7c7b\u6cd5\u5408\u89c4\u8fc7\u7a0b\u9700\u8981\u5927\u91cf\u4eba\u5de5\u548c\u8d44\u6e90\uff0cLLM\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u8def\u5f84\uff0c\u4f46\u7f3a\u4e4f\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u963b\u788d\u4e86\u76f8\u5173\u7814\u7a76\u3002", "method": "\u4ece190\u4efd\u4f01\u4e1a\u62a5\u544a\u4e2d\u6784\u5efa\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u7ecf\u6d4e\u6d3b\u52a8\u6570\u636e\u548c\u5b9a\u91cfKPI\u6307\u6807\uff0c\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9LLM\u5728\u6838\u5fc3\u5408\u89c4\u5de5\u4f5c\u6d41\u4e2d\u8fdb\u884c\u9996\u6b21\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "LLM\u5728\u8bc6\u522b\u7ecf\u6d4e\u6d3b\u52a8\u7684\u5b9a\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u591a\u6b65\u667a\u80fd\u4f53\u6846\u67b6\u7565\u5fae\u63d0\u5347\u7cbe\u5ea6\uff1b\u5728\u9884\u6d4b\u8d22\u52a1KPI\u7684\u5b9a\u91cf\u4efb\u52a1\u4e0a\u5b8c\u5168\u5931\u8d25\uff1b\u53d1\u73b0\u7b80\u6d01\u5143\u6570\u636e\u6bd4\u5b8c\u6574\u975e\u7ed3\u6784\u5316\u62a5\u544a\u8868\u73b0\u66f4\u597d\u7684\u6096\u8bba\uff1b\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5206\u6570\u6821\u51c6\u4e0d\u4f73\u3002", "conclusion": "LLM\u5c1a\u672a\u51c6\u5907\u597d\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u4f46\u53ef\u4f5c\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u7684\u5f3a\u5927\u8f85\u52a9\u5de5\u5177\uff1b\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5f00\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2512.24063", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24063", "abs": "https://arxiv.org/abs/2512.24063", "authors": ["Haoyue Bai", "Yiyou Sun", "Wenjie Hu", "Shi Qiu", "Maggie Ziyu Huan", "Peiyang Song", "Robert Nowak", "Dawn Song"], "title": "How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns", "comment": null, "summary": "Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u8ba1\u7b97\u3001\u4e8b\u5b9e\u68c0\u7d22\u3001\u6a21\u62df\u3001\u679a\u4e3e\u548c\u8bca\u65ad\u7b49\u6838\u5fc3\u6280\u80fd\uff0c\u7528\u4e8e\u7cbe\u7ec6\u5206\u6790SFT\u548cRL\u8c03\u4f18\u5728LLM\u4e2d\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "LLMs\u5728\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u8c03\u4f18\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6cdb\u5316\u884c\u4e3a\uff1aSFT\u5f80\u5f80\u7f29\u5c0f\u80fd\u529b\u8303\u56f4\uff0c\u800cRL\u503e\u5411\u4e8e\u4fdd\u6301\u80fd\u529b\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u7c97\u7c92\u5ea6\u51c6\u786e\u5ea6\u6307\u6807\uff0c\u65e0\u6cd5\u6df1\u5165\u7406\u89e3\u8fd9\u79cd\u5dee\u5f02\u7684\u539f\u56e0\u3002", "method": "1. \u5f15\u5165\u65b0\u57fa\u51c6\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u8ba1\u7b97\u3001\u4e8b\u5b9e\u68c0\u7d22\u3001\u6a21\u62df\u3001\u679a\u4e3e\u548c\u8bca\u65ad\u7b49\u539f\u5b50\u6838\u5fc3\u6280\u80fd\uff1b2. \u7ed3\u5408\u5206\u5e03\u5dee\u5f02\u548c\u53c2\u6570\u7edf\u8ba1\u7b49\u4f4e\u5c42\u7edf\u8ba1\u6a21\u5f0f\u5206\u6790\uff1b3. \u63d0\u51fa\u5143\u63a2\u6d4b\u6846\u67b6\uff0c\u8ffd\u8e2a\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u6a21\u578b\u884c\u4e3a\u3002", "result": "RL\u8c03\u4f18\u6a21\u578b\u4fdd\u6301\u66f4\u7a33\u5b9a\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u62b5\u6297\u63a8\u7406\u6280\u80fd\u5d29\u6e83\uff1bSFT\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u660e\u663e\u7684\u6f02\u79fb\uff0c\u8fc7\u5ea6\u62df\u5408\u8868\u9762\u6a21\u5f0f\u3002\u57fa\u51c6\u63d0\u4f9b\u4e86\u5bf9\u6570\u5b66\u3001\u79d1\u5b66\u63a8\u7406\u548c\u975e\u63a8\u7406\u4efb\u52a1\u4e2d\u6cdb\u5316\u6f14\u53d8\u7684\u7cbe\u7ec6\u7814\u7a76\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7406\u89e3LLM\u4e2d\u63a8\u7406\u7684\u672c\u8d28\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u4fc3\u8fdb\u5e7f\u6cdb\u3001\u7a33\u5065\u6cdb\u5316\u7684\u8bad\u7ec3\u7b56\u7565\u6307\u660e\u4e86\u539f\u5219\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2512.24103", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24103", "abs": "https://arxiv.org/abs/2512.24103", "authors": ["Bernd Bohnet", "Pierre-Alexandre Kamienny", "Hanie Sedghi", "Dilan Gorur", "Pranjal Awasthi", "Aaron Parisi", "Kevin Swersky", "Rosanne Liu", "Azade Nova", "Noah Fiedel"], "title": "Enhancing LLM Planning Capabilities through Intrinsic Self-Critique", "comment": null, "summary": "We demonstrate an approach for LLMs to critique their \\emph{own} answers with the goal of enhancing their performance that leads to significant improvements over established planning benchmarks. Despite the findings of earlier research that has cast doubt on the effectiveness of LLMs leveraging self critique methods, we show significant performance gains on planning datasets in the Blocksworld domain through intrinsic self-critique, without external source such as a verifier. We also demonstrate similar improvements on Logistics and Mini-grid datasets, exceeding strong baseline accuracies. We employ a few-shot learning technique and progressively extend it to a many-shot approach as our base method and demonstrate that it is possible to gain substantial improvement on top of this already competitive approach by employing an iterative process for correction and refinement. We illustrate how self-critique can significantly boost planning performance. Our empirical results present new state-of-the-art on the class of models considered, namely LLM model checkpoints from October 2024. Our primary focus lies on the method itself, demonstrating intrinsic self-improvement capabilities that are applicable regardless of the specific model version, and we believe that applying our method to more complex search techniques and more capable models will lead to even better performance.", "AI": {"tldr": "LLM\u901a\u8fc7\u5185\u5728\u81ea\u6211\u6279\u8bc4\u63d0\u5347\u89c4\u5212\u6027\u80fd\uff0c\u5728Blocksworld\u3001Logistics\u548cMini-grid\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u65b0\u7684SOTA\u7ed3\u679c", "motivation": "\u5c3d\u7ba1\u65e9\u671f\u7814\u7a76\u5bf9LLM\u81ea\u6211\u6279\u8bc4\u65b9\u6cd5\u7684\u6709\u6548\u6027\u8868\u793a\u6000\u7591\uff0c\u4f46\u672c\u6587\u65e8\u5728\u8bc1\u660e\u901a\u8fc7\u5185\u5728\u81ea\u6211\u6279\u8bc4\uff08\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u5728\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "method": "\u91c7\u7528\u5c11\u6837\u672c\u5b66\u4e60\u6280\u672f\u5e76\u9010\u6b65\u6269\u5c55\u5230\u591a\u6837\u672c\u65b9\u6cd5\u4f5c\u4e3a\u57fa\u7840\uff0c\u7136\u540e\u901a\u8fc7\u8fed\u4ee3\u4fee\u6b63\u548c\u7cbe\u70bc\u8fc7\u7a0b\u8fdb\u884c\u6539\u8fdb\uff0c\u5b9e\u73b0\u5185\u5728\u81ea\u6211\u6279\u8bc4", "result": "\u5728Blocksworld\u3001Logistics\u548cMini-grid\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u51c6\u786e\u7387\uff0c\u57282024\u5e7410\u6708\u7684LLM\u6a21\u578b\u68c0\u67e5\u70b9\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684SOTA", "conclusion": "\u81ea\u6211\u6279\u8bc4\u80fd\u663e\u8457\u63d0\u5347\u89c4\u5212\u6027\u80fd\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5185\u5728\u81ea\u6211\u6539\u8fdb\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u7248\u672c\uff0c\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u641c\u7d22\u6280\u672f\u548c\u66f4\u5f3a\u5927\u6a21\u578b\u5c06\u5e26\u6765\u66f4\u597d\u6027\u80fd", "topic": "agent analysis"}}
{"id": "2512.24138", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24138", "abs": "https://arxiv.org/abs/2512.24138", "authors": ["Haoran He", "Yuxiao Ye", "Jie Liu", "Jiajun Liang", "Zhiyong Wang", "Ziyang Yuan", "Xintao Wang", "Hangyu Mao", "Pengfei Wan", "Ling Pan"], "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking", "comment": "17 pages. Project: https://tinnerhrhe.github.io/gardo_project", "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.", "AI": {"tldr": "GARDO\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u60e9\u7f5a\u9ad8\u4e0d\u786e\u5b9a\u6027\u6837\u672c\u3001\u81ea\u9002\u5e94\u66f4\u65b0\u53c2\u8003\u6a21\u578b\u3001\u4ee5\u53ca\u591a\u6837\u6027\u5956\u52b1\u653e\u5927\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u3001\u63a2\u7d22\u4e0d\u8db3\u548c\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u7531\u4e8e\u4ee3\u7406\u5956\u52b1\u4e0e\u771f\u5b9e\u76ee\u6807\u4e0d\u5339\u914d\uff0c\u5e38\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\uff08\u4ee3\u7406\u5206\u6570\u4e0a\u5347\u4f46\u771f\u5b9e\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff09\u3001\u63a2\u7d22\u4e0d\u8db3\uff08\u53c2\u8003\u7b56\u7565\u901a\u5e38\u6b21\u4f18\uff09\u548c\u6a21\u5f0f\u5d29\u6e83\uff08\u751f\u6210\u591a\u6837\u6027\u4e0b\u964d\uff09\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faGARDO\u6846\u67b6\uff1a1\uff09\u9009\u62e9\u6027\u6b63\u5219\u5316\uff1a\u4ec5\u60e9\u7f5a\u9ad8\u4e0d\u786e\u5b9a\u6027\u6837\u672c\uff1b2\uff09\u81ea\u9002\u5e94\u6b63\u5219\u5316\uff1a\u5b9a\u671f\u66f4\u65b0\u53c2\u8003\u6a21\u578b\u4ee5\u5339\u914d\u5728\u7ebf\u7b56\u7565\u80fd\u529b\uff1b3\uff09\u591a\u6837\u6027\u611f\u77e5\u4f18\u5316\uff1a\u5bf9\u9ad8\u8d28\u91cf\u4e14\u9ad8\u591a\u6837\u6027\u7684\u6837\u672c\u653e\u5927\u5956\u52b1\u3002", "result": "\u5728\u591a\u79cd\u4ee3\u7406\u5956\u52b1\u548c\u672a\u89c1\u6307\u6807\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGARDO\u80fd\u6709\u6548\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u3001\u589e\u5f3a\u751f\u6210\u591a\u6837\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6837\u672c\u6548\u7387\u6216\u63a2\u7d22\u80fd\u529b\uff0c\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GARDO\u901a\u8fc7\u667a\u80fd\u7684\u6b63\u5219\u5316\u7b56\u7565\u548c\u591a\u6837\u6027\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578bRL\u5fae\u8c03\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5956\u52b1\u9ed1\u5ba2\u7f13\u89e3\u3001\u591a\u6837\u6027\u4fdd\u6301\u548c\u9ad8\u6548\u63a2\u7d22\u7684\u5e73\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.24618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24618", "abs": "https://arxiv.org/abs/2512.24618", "authors": ["Junru Lu", "Jiarui Qin", "Lingfeng Qiao", "Yinghui Li", "Xinyi Dai", "Bo Ke", "Jianfeng He", "Ruizhi Qiao", "Di Yin", "Xing Sun", "Yunsheng Wu", "Yinsong Liu", "Shuangyin Liu", "Mingkong Tang", "Haodong Lin", "Jiayi Kuang", "Fanxu Meng", "Xiaojuan Tang", "Yunjia Xi", "Junjie Huang", "Haotong Yang", "Zhenyi Shen", "Yangning Li", "Qianwen Zhang", "Yifei Yu", "Siyu An", "Junnan Dong", "Qiufeng Wang", "Jie Wang", "Keyu Chen", "Wei Wen", "Taian Guo", "Zhifeng Shen", "Daohai Yu", "Jiahao Li", "Ke Li", "Zongyi Li", "Xiaoyu Tan"], "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "comment": "57 pages, 26 figures", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "AI": {"tldr": "Youtu-LLM\u662f\u4e00\u4e2a1.96B\u53c2\u6570\u7684\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4ece\u5934\u9884\u8bad\u7ec3\u800c\u975e\u84b8\u998f\uff0c\u5728\u7d27\u51d1\u67b6\u6784\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u652f\u6301128k\u4e0a\u4e0b\u6587\uff0c\u5728\u901a\u7528\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u5c0f\u578b\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u84b8\u998f\u6280\u672f\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u667a\u80fd\u4f53\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u5177\u5907\u539f\u751f\u667a\u80fd\u4f53\u667a\u80fd\u7684\u6a21\u578b\uff0c\u80fd\u591f\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6267\u884c\u590d\u6742\u7684\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\u3002", "method": "1. \u91c7\u7528\u5bc6\u96c6\u591a\u6f5c\u5728\u6ce8\u610f\u529b(MLA)\u67b6\u6784\u548cSTEM\u5bfc\u5411\u8bcd\u6c47\u8868\uff0c\u652f\u6301128k\u4e0a\u4e0b\u6587\u7a97\u53e3\uff1b2. \u8bbe\u8ba1\"\u5e38\u8bc6-STEM-\u667a\u80fd\u4f53\"\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u4f7f\u7528\u7ea611T token\u6570\u636e\uff1b3. \u9488\u5bf9\u667a\u80fd\u4f53\u4efb\u52a1\u91c7\u7528\u591a\u6837\u5316\u6570\u636e\u6784\u9020\u65b9\u6848\uff0c\u5408\u6210\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u3002", "result": "Youtu-LLM\u57282B\u4ee5\u4e0b\u6a21\u578b\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6c34\u5e73\uff1a\u5728\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u66f4\u5927\u6a21\u578b\u7ade\u4e89\uff0c\u5728\u667a\u80fd\u4f53\u7279\u5b9a\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u8bc1\u660e\u8f7b\u91cf\u7ea7\u6a21\u578b\u53ef\u4ee5\u5177\u5907\u5f3a\u5927\u7684\u5185\u5728\u667a\u80fd\u4f53\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u53ef\u4ee5\u5177\u5907\u5f3a\u5927\u7684\u539f\u751f\u667a\u80fd\u4f53\u667a\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u667a\u80fd\u4f53\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.24661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24661", "abs": "https://arxiv.org/abs/2512.24661", "authors": ["Casey O. Barkan", "Sid Black", "Oliver Sourbut"], "title": "Do Large Language Models Know What They Are Capable Of?", "comment": "23 pages, 8 figures", "summary": "We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.", "AI": {"tldr": "LLM\u4ee3\u7406\u666e\u904d\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u81ea\u8eab\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4e14\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u81ea\u4fe1\u4f1a\u52a0\u5267\u3002\u867d\u7136\u90e8\u5206LLM\u80fd\u4ece\u5931\u8d25\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u964d\u4f4e\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f46\u6574\u4f53\u4e0a\u7f3a\u4e4f\u5bf9\u81ea\u8eab\u80fd\u529b\u7684\u8ba4\u77e5\u9650\u5236\u4e86\u5176\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76LLM\u662f\u5426\u80fd\u9884\u6d4b\u81ea\u8eab\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4ee5\u53ca\u8fd9\u79cd\u9884\u6d4b\u80fd\u529b\u662f\u5426\u4f1a\u968f\u7740\u4efb\u52a1\u8fdb\u5c55\u800c\u6539\u5584\u3002\u540c\u65f6\u63a2\u7a76LLM\u80fd\u5426\u4ece\u4e0a\u4e0b\u6587\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u5728\u5931\u8d25\u6210\u672c\u9ad8\u7684\u573a\u666f\u4e2d\u505a\u51fa\u66f4\u597d\u7684\u4efb\u52a1\u9009\u62e9\u51b3\u7b56\u3002", "method": "\u6d4b\u8bd5\u591a\u4e2aLLM\u5728\u5355\u6b65\u548c\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u9884\u6d4b\u80fd\u529b\uff0c\u8bc4\u4f30\u5176\u9884\u6d4b\u7684\u533a\u5206\u5ea6\u3002\u7814\u7a76LLM\u5728\u7ecf\u5386\u5931\u8d25\u7ecf\u9a8c\u540e\u662f\u5426\u8c03\u6574\u9884\u6d4b\uff0c\u5e76\u5206\u6790\u5176\u51b3\u7b56\u662f\u5426\u7406\u6027\uff08\u57fa\u4e8e\u5176\u4f30\u8ba1\u7684\u6210\u529f\u6982\u7387\uff09\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684LLM\u90fd\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f46\u5927\u591a\u6570\u5177\u6709\u4f18\u4e8e\u968f\u673a\u7684\u533a\u5206\u80fd\u529b\u3002\u65b0\u6a21\u578b\u548c\u66f4\u5927\u6a21\u578b\u4e0d\u4e00\u5b9a\u6709\u66f4\u597d\u7684\u533a\u5206\u80fd\u529b\uff08Claude\u6a21\u578b\u9664\u5916\uff09\u3002\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\uff0c\u591a\u4e2a\u524d\u6cbfLLM\u7684\u8fc7\u5ea6\u81ea\u4fe1\u4f1a\u968f\u7740\u4efb\u52a1\u8fdb\u5c55\u800c\u52a0\u5267\u3002\u90e8\u5206LLM\u80fd\u4ece\u5931\u8d25\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u964d\u4f4e\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u6539\u5584\u51b3\u7b56\uff0c\u4f46\u5e76\u975e\u6240\u6709\u6a21\u578b\u90fd\u80fd\u505a\u5230\u3002\u6709\u8da3\u7684\u662f\uff0c\u6240\u6709LLM\u7684\u51b3\u7b56\u90fd\u8fd1\u4f3c\u7406\u6027\uff08\u57fa\u4e8e\u5176\u4f30\u8ba1\u6982\u7387\uff09\uff0c\u4f46\u8fc7\u5ea6\u4e50\u89c2\u7684\u4f30\u8ba1\u5bfc\u81f4\u51b3\u7b56\u8d28\u91cf\u5dee\u3002", "conclusion": "\u5f53\u524dLLM\u4ee3\u7406\u7f3a\u4e4f\u5bf9\u81ea\u8eab\u80fd\u529b\u7684\u8ba4\u77e5\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u51b3\u7b56\u80fd\u529b\u3002\u7814\u7a76\u7ed3\u679c\u5bf9AI\u6ee5\u7528\u548c\u9519\u4f4d\u98ce\u9669\u6709\u91cd\u8981\u542f\u793a\uff0c\u56e0\u4e3aLLM\u7684\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\u4f1a\u5f71\u54cd\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.24684", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24684", "abs": "https://arxiv.org/abs/2512.24684", "authors": ["Maoyuan Li", "Zhongsheng Wang", "Haoyuan Li", "Jiamou Liu"], "title": "R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory", "comment": "Accepteed by AAMAS 2026 full paper", "summary": "We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.", "AI": {"tldr": "R-Debater\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bba\u8bc1\u8bb0\u5fc6\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u591a\u8f6e\u8fa9\u8bba\u3002\u5b83\u901a\u8fc7\u68c0\u7d22\u6848\u4f8b\u8bc1\u636e\u548c\u5148\u524d\u8fa9\u8bba\u52a8\u4f5c\uff0c\u7ed3\u5408\u89d2\u8272\u667a\u80fd\u4f53\u6765\u751f\u6210\u8fde\u8d2f\u7684\u8fa9\u8bba\u8bdd\u8bed\uff0c\u5728ORCHID\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f3aLLM\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u8fa9\u8bba\u7cfb\u7edf\u5728\u4fdd\u6301\u7acb\u573a\u4e00\u81f4\u6027\u3001\u56de\u5e94\u5bf9\u624b\u4ee5\u53ca\u4f7f\u7528\u8bc1\u636e\u652f\u6301\u4e3b\u5f20\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u4f5c\u8005\u53d7\u5230\u4fee\u8f9e\u5b66\u548c\u8bb0\u5fc6\u7814\u7a76\u7684\u542f\u53d1\uff0c\u8ba4\u4e3a\u8fa9\u8bba\u5e94\u8be5\u662f\u56de\u5fc6\u548c\u8c03\u6574\u5148\u524d\u8bba\u8bc1\u7684\u8fc7\u7a0b\uff0c\u9700\u8981\u7cfb\u7edf\u80fd\u591f\u8de8\u8f6e\u6b21\u4fdd\u6301\u8fde\u8d2f\u6027\u3002", "method": "R-Debater\u6574\u5408\u4e86\u8fa9\u8bba\u77e5\u8bc6\u5e93\uff08\u7528\u4e8e\u68c0\u7d22\u6848\u4f8b\u8bc1\u636e\u548c\u5148\u524d\u8fa9\u8bba\u52a8\u4f5c\uff09\u548c\u57fa\u4e8e\u89d2\u8272\u7684\u667a\u80fd\u4f53\uff08\u7528\u4e8e\u8de8\u8f6e\u6b21\u7ec4\u5408\u8fde\u8d2f\u8bdd\u8bed\uff09\u3002\u7cfb\u7edf\u5728ORCHID\u8fa9\u8bba\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6784\u5efa\u4e86\u5305\u542b1000\u4e2a\u6761\u76ee\u7684\u68c0\u7d22\u8bed\u6599\u5e93\u548c32\u4e2a\u8fa9\u8bba\u7684\u6d4b\u8bd5\u96c6\u3002", "result": "\u5728\u4e24\u4e2a\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff1a1\uff09\u4e0b\u4e00\u8bdd\u8bed\u751f\u6210\uff08\u901a\u8fc7InspireScore\u8bc4\u4f30\u4e3b\u89c2\u3001\u903b\u8f91\u548c\u4e8b\u5b9e\u7ef4\u5ea6\uff09\uff1b2\uff09\u5bf9\u6297\u6027\u591a\u8f6e\u6a21\u62df\uff08\u901a\u8fc7Debatrix\u8bc4\u4f30\u8bba\u8bc1\u3001\u6765\u6e90\u3001\u8bed\u8a00\u548c\u6574\u4f53\u7ef4\u5ea6\uff09\u3002R-Debater\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u5f97\u5206\u4e0a\u90fd\u8d85\u8d8a\u4e86\u5f3aLLM\u57fa\u7ebf\u300220\u4f4d\u7ecf\u9a8c\u4e30\u5bcc\u7684\u8fa9\u624b\u53c2\u4e0e\u7684\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u786e\u8ba4\u4e86\u7cfb\u7edf\u7684\u4e00\u81f4\u6027\u548c\u8bc1\u636e\u4f7f\u7528\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u68c0\u7d22\u57fa\u7840\u548c\u7ed3\u6784\u5316\u89c4\u5212\u7684R-Debater\u80fd\u591f\u751f\u6210\u66f4\u52a0\u5fe0\u5b9e\u3001\u7acb\u573a\u5bf9\u9f50\u4e14\u8de8\u8f6e\u6b21\u8fde\u8d2f\u7684\u8fa9\u8bba\u3002\u8fd9\u8868\u660e\u5c06\u68c0\u7d22\u57fa\u7840\u4e0e\u7ed3\u6784\u5316\u89c4\u5212\u76f8\u7ed3\u5408\u5bf9\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u8fa9\u8bba\u7cfb\u7edf\u662f\u6709\u6548\u7684\u3002", "topic": "agent analysis"}}
{"id": "2512.24693", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24693", "abs": "https://arxiv.org/abs/2512.24693", "authors": ["Wenzhe Li", "Shujian Zhang", "Wenxuan Zhou", "John Lambert", "Chi Jin", "Andrew Hard", "Rajiv Mathews", "Lun Wang"], "title": "MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models", "comment": null, "summary": "Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \\textit{training} techniques, effective automated \\textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \\textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \\textbf{MU}lti-\\textbf{S}tep \\textbf{I}nstruction \\textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.", "AI": {"tldr": "\u63d0\u51faMUSIC\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u6bd4\u8bad\u7ec3\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u5956\u52b1\u6a21\u578b\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u5355\u8f6e\u8bc4\u4f30\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u8d28\u91cf\u8bc4\u4f30\u5bf9LLM\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u4f46\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u57fa\u4e8e\u5355\u8f6e\u5bf9\u6bd4\u7684\u504f\u597d\u6570\u636e\u96c6\u65e0\u6cd5\u6355\u6349\u591a\u8f6e\u4ea4\u4e92\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u8f6e\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMUSIC\uff08\u591a\u6b65\u6307\u4ee4\u5bf9\u6bd4\uff09\u65e0\u76d1\u7763\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5408\u6210\u8de8\u591a\u8f6e\u5bf9\u8bdd\u7684\u5bf9\u6bd4\u5bf9\u8bdd\u5bf9\uff0c\u5728Skywork\u504f\u597d\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u57fa\u4e8eGemma-2-9B-Instruct\u7684\u591a\u8f6e\u5956\u52b1\u6a21\u578b\u3002", "result": "MUSIC\u589e\u5f3a\u7684\u5956\u52b1\u6a21\u578b\u5728\u591a\u9879\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0e\u9ad8\u7ea7\u4e13\u6709LLM\u8bc4\u5224\u5728\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u4e0a\u5bf9\u9f50\u5ea6\u66f4\u9ad8\uff0c\u4e14\u4e0d\u5f71\u54cd\u6807\u51c6\u5355\u8f6e\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6027\u80fd\u3002", "conclusion": "\u591a\u8f6e\u5bf9\u6bd4\u8bad\u7ec3\u5bf9\u6784\u5efa\u9c81\u68d2\u7684\u591a\u8f6e\u5956\u52b1\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0cMUSIC\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u8d28\u91cf\uff0c\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u53cd\u9988\u4fe1\u53f7\u3002", "topic": "agent analysis"}}
{"id": "2512.24885", "categories": ["cs.CL", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.24885", "abs": "https://arxiv.org/abs/2512.24885", "authors": ["Hengli Li", "Zhaoxin Yu", "Qi Shen", "Chenxi Li", "Mengmeng Wang", "Tinglang Wu", "Yipeng Kang", "Yuxuan Wang", "Song-Chun Zhu", "Zixia Jia", "Zilong Zheng"], "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts", "comment": "Accepted by AAMAS 2026", "summary": "Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.", "AI": {"tldr": "BEDA\u6846\u67b6\u901a\u8fc7\u5c06\u4fe1\u5ff5\u4f30\u8ba1\u8f6c\u5316\u4e3a\u751f\u6210\u7ea6\u675f\uff0c\u5728\u6218\u7565\u5bf9\u8bdd\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u5bf9\u6297\u3001\u5408\u4f5c\u548c\u8c08\u5224\u4e09\u79cd\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6218\u7565\u5bf9\u8bdd\u7cfb\u7edf\u867d\u7136\u80fd\u51c6\u786e\u4f30\u8ba1\u4fe1\u5ff5\uff0c\u4f46\u7f3a\u4e4f\u4f7f\u7528\u8fd9\u4e9b\u4fe1\u5ff5\u8fdb\u884c\u751f\u6210\u7684\u539f\u5219\u6027\u673a\u5236\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5c06\u4fe1\u5ff5\u4f30\u8ba1\u8f6c\u5316\u4e3a\u751f\u6210\u7ea6\u675f\u6765\u6307\u5bfc\u5bf9\u8bdd\u884c\u4e3a\u3002", "method": "\u63d0\u51faBEDA\u6846\u67b6\uff0c\u9996\u5148\u5f62\u5f0f\u5316\u5bf9\u6297\u548c\u5bf9\u9f50\u4e24\u79cd\u6838\u5fc3\u5bf9\u8bdd\u884c\u4e3a\uff0c\u901a\u8fc7\u6982\u7387\u7ea6\u675f\u64cd\u4f5c\u5316\u8fd9\u4e9b\u884c\u4e3a\u3002\u6846\u67b6\u5305\u542b\u4e16\u754c\u96c6\u5408\u3001\u4fe1\u5ff5\u4f30\u8ba1\u5668\u548c\u6761\u4ef6\u751f\u6210\u5668\uff0c\u540e\u8005\u6839\u636e\u63a8\u65ad\u7684\u4fe1\u5ff5\u9009\u62e9\u884c\u4e3a\u5e76\u751f\u6210\u4e00\u81f4\u7684\u8bed\u53e5\u3002", "result": "\u5728\u4e09\u79cd\u8bbe\u7f6e\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1a\u5728CKBG\uff08\u5bf9\u6297\uff09\u4e2d\uff0c\u6210\u529f\u7387\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u4e0a\u81f3\u5c11\u63d0\u53475.0\u4e2a\u767e\u5206\u70b9\uff0c\u4f7f\u7528GPT-4.1-nano\u65f6\u63d0\u534720.6\u4e2a\u767e\u5206\u70b9\uff1b\u5728Mutual Friends\uff08\u5408\u4f5c\uff09\u4e2d\u5e73\u5747\u63d0\u53479.3\u4e2a\u767e\u5206\u70b9\uff1b\u5728CaSiNo\uff08\u8c08\u5224\uff09\u4e2d\u8fbe\u6210\u76f8\u5bf9\u4e8e\u6240\u6709\u57fa\u7ebf\u7684\u6700\u4f18\u4ea4\u6613\u3002", "conclusion": "\u5c06\u4fe1\u5ff5\u4f30\u8ba1\u4f5c\u4e3a\u7ea6\u675f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u901a\u7528\u7684\u673a\u5236\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u7684\u6218\u7565\u5bf9\u8bdd\u3002", "topic": "agent analysis"}}
{"id": "2512.24818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24818", "abs": "https://arxiv.org/abs/2512.24818", "authors": ["Shulun Chen", "Runlong Zhou", "Zihan Zhang", "Maryam Fazel", "Simon S. Du"], "title": "Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback", "comment": "28 pages", "summary": "Aligning large language models (LLMs) with human preferences has proven effective for enhancing model capabilities, yet standard preference modeling using the Bradley-Terry model assumes transitivity, overlooking the inherent complexity of human population preferences. Nash learning from human feedback (NLHF) addresses this by framing non-transitive preferences as a two-player zero-sum game, where alignment reduces to finding the Nash equilibrium (NE). However, existing algorithms typically rely on regularization, incurring unavoidable bias when computing the duality gap in the original game. In this work, we provide the first convergence guarantee for Optimistic Multiplicative Weights Update ($\\mathtt{OMWU}$) in NLHF, showing that it achieves last-iterate linear convergence after a burn-in phase whenever an NE with full support exists, with an instance-dependent linear convergence rate to the original NE, measured by duality gaps. Compared to prior results in Wei et al. (2020), we do not require the assumption of NE uniqueness. Our analysis identifies a novel marginal convergence behavior, where the probability of rarely played actions grows exponentially from exponentially small values, enabling exponentially better dependence on instance-dependent constants than prior results. Experiments corroborate the theoretical strengths of $\\mathtt{OMWU}$ in both tabular and neural policy classes, demonstrating its potential for LLM applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u4e50\u89c2\u4e58\u6027\u6743\u91cd\u66f4\u65b0(OMWU)\u7b97\u6cd5\u89e3\u51b3\u975e\u4f20\u9012\u504f\u597d\u4e0b\u7684Nash\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988(NLHF)\u95ee\u9898\uff0c\u9996\u6b21\u8bc1\u660e\u4e86OMWU\u5728\u5b58\u5728\u5168\u652f\u6491Nash\u5747\u8861\u65f6\u7684\u7ebf\u6027\u6536\u655b\u6027\uff0c\u65e0\u9700\u5747\u8861\u552f\u4e00\u6027\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eBradley-Terry\u6a21\u578b\u7684\u504f\u597d\u5efa\u6a21\u5047\u8bbe\u4f20\u9012\u6027\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u7fa4\u4f53\u504f\u597d\u7684\u590d\u6742\u6027\u3002NLHF\u5c06\u975e\u4f20\u9012\u504f\u597d\u5efa\u6a21\u4e3a\u4e24\u4eba\u96f6\u548c\u535a\u5f08\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u4f9d\u8d56\u6b63\u5219\u5316\uff0c\u5728\u8ba1\u7b97\u539f\u59cb\u535a\u5f08\u5bf9\u5076\u95f4\u9699\u65f6\u4f1a\u4ea7\u751f\u4e0d\u53ef\u907f\u514d\u7684\u504f\u5dee\u3002", "method": "\u91c7\u7528\u4e50\u89c2\u4e58\u6027\u6743\u91cd\u66f4\u65b0(OMWU)\u7b97\u6cd5\u89e3\u51b3NLHF\u95ee\u9898\uff0c\u5c06\u975e\u4f20\u9012\u504f\u597d\u5bf9\u9f50\u5efa\u6a21\u4e3a\u5bfb\u627eNash\u5747\u8861\u3002\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86OMWU\u5728\u5b58\u5728\u5168\u652f\u6491Nash\u5747\u8861\u65f6\u7684\u6536\u655b\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u8fb9\u9645\u6536\u655b\u884c\u4e3a\u3002", "result": "\u9996\u6b21\u8bc1\u660e\u4e86OMWU\u5728NLHF\u4e2d\u7684\u6536\u655b\u6027\uff1a\u5728\u9884\u70ed\u9636\u6bb5\u540e\u5b9e\u73b0\u6700\u7ec8\u8fed\u4ee3\u7ebf\u6027\u6536\u655b\uff0c\u5177\u6709\u5b9e\u4f8b\u4f9d\u8d56\u7684\u7ebf\u6027\u6536\u655b\u901f\u7387\u3002\u76f8\u6bd4\u4e4b\u524d\u5de5\u4f5c\uff0c\u65e0\u9700Nash\u5747\u8861\u552f\u4e00\u6027\u5047\u8bbe\uff0c\u4e14\u5bf9\u5b9e\u4f8b\u4f9d\u8d56\u5e38\u6570\u6709\u6307\u6570\u7ea7\u66f4\u597d\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "OMWU\u4e3aNLHF\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u5904\u7406\u975e\u4f20\u9012\u504f\u597d\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2512.24827", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24827", "abs": "https://arxiv.org/abs/2512.24827", "authors": ["Raul D. Steleac", "Mohan Sridharan", "David Abel"], "title": "Discovering Coordinated Joint Options via Inter-Agent Relative Dynamics", "comment": null, "summary": "Temporally extended actions improve the ability to explore and plan in single-agent settings. In multi-agent settings, the exponential growth of the joint state space with the number of agents makes coordinated behaviours even more valuable. Yet, this same exponential growth renders the design of multi-agent options particularly challenging. Existing multi-agent option discovery methods often sacrifice coordination by producing loosely coupled or fully independent behaviours. Toward addressing these limitations, we describe a novel approach for multi-agent option discovery. Specifically, we propose a joint-state abstraction that compresses the state space while preserving the information necessary to discover strongly coordinated behaviours. Our approach builds on the inductive bias that synchronisation over agent states provides a natural foundation for coordination in the absence of explicit objectives. We first approximate a fictitious state of maximal alignment with the team, the \\textit{Fermat} state, and use it to define a measure of \\textit{spreadness}, capturing team-level misalignment on each individual state dimension. Building on this representation, we then employ a neural graph Laplacian estimator to derive options that capture state synchronisation patterns between agents. We evaluate the resulting options across multiple scenarios in two multi-agent domains, showing that they yield stronger downstream coordination capabilities compared to alternative option discovery methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u9009\u9879\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u72b6\u6001\u62bd\u8c61\u548c\u8d39\u9a6c\u72b6\u6001\u8fd1\u4f3c\u6765\u53d1\u73b0\u5f3a\u534f\u8c03\u884c\u4e3a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u4e0b\u6e38\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0c\u8054\u5408\u72b6\u6001\u7a7a\u95f4\u968f\u667a\u80fd\u4f53\u6570\u91cf\u5448\u6307\u6570\u589e\u957f\uff0c\u4f7f\u5f97\u534f\u8c03\u884c\u4e3a\u8bbe\u8ba1\u7279\u522b\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u727a\u7272\u534f\u8c03\u6027\uff0c\u4ea7\u751f\u677e\u6563\u8026\u5408\u6216\u5b8c\u5168\u72ec\u7acb\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u8054\u5408\u72b6\u6001\u62bd\u8c61\u65b9\u6cd5\u538b\u7f29\u72b6\u6001\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u7559\u53d1\u73b0\u5f3a\u534f\u8c03\u884c\u4e3a\u6240\u9700\u4fe1\u606f\u3002\u9996\u5148\u8fd1\u4f3c\u6700\u5927\u5bf9\u9f50\u7684\u8d39\u9a6c\u72b6\u6001\uff0c\u5b9a\u4e49\u6269\u5c55\u5ea6\u5ea6\u91cf\u56e2\u961f\u7ea7\u4e0d\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u7136\u540e\u4f7f\u7528\u795e\u7ecf\u56fe\u62c9\u666e\u62c9\u65af\u4f30\u8ba1\u5668\u63a8\u5bfc\u6355\u83b7\u667a\u80fd\u4f53\u95f4\u72b6\u6001\u540c\u6b65\u6a21\u5f0f\u7684\u9009\u9879\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u667a\u80fd\u4f53\u9886\u57df\u7684\u591a\u4e2a\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u76f8\u6bd4\u5176\u4ed6\u9009\u9879\u53d1\u73b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u5f3a\u7684\u4e0b\u6e38\u534f\u8c03\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u72b6\u6001\u62bd\u8c61\u548c\u8d39\u9a6c\u72b6\u6001\u8fd1\u4f3c\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u534f\u8c03\u9009\u9879\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u534f\u8c03\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.70ffbace", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavegriffith.substack.com%2Fp%2Fclaude-code-sees-like-a-software%3Futm_source=tldrnewsletter/1/0100019b4af34c38-387c2a2b-1bd9-404a-ab9b-37772c7f8978-000000/xfPv3ofFw_dvwPF0ukj_oXNqJ3yWXhrN_dKCLCLE3sY=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavegriffith.substack.com%2Fp%2Fclaude-code-sees-like-a-software%3Futm_source=tldrnewsletter/1/0100019b4af34c38-387c2a2b-1bd9-404a-ab9b-37772c7f8978-000000/xfPv3ofFw_dvwPF0ukj_oXNqJ3yWXhrN_dKCLCLE3sY=437", "authors": ["TLDR Newsletter"], "title": "Claude Code Sees Like A Software Architect", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavegriffith.substack.com%2Fp%2Fclaude-code-sees-like-a-software%3Futm_source=tldrnewsletter/1/0100019b4af34c38-387c2a2b-1bd9-404a-ab9b-37772c7f8978-000000/xfPv3ofFw_dvwPF0ukj_oXNqJ3yWXhrN_dKCLCLE3sY=437", "summary": "Claude Code Sees Like A Software Architect (10 minute read) Claude Code shipped native Language Server Protocol support last week, enabling the IDE to actually understand code.", "source": "tldr", "AI": {"tldr": "Claude Code\u65b0\u589e\u539f\u751f\u8bed\u8a00\u670d\u52a1\u5668\u534f\u8bae\u652f\u6301\uff0c\u4f7fIDE\u80fd\u771f\u6b63\u7406\u89e3\u4ee3\u7801", "motivation": "\u4f20\u7edfIDE\u5bf9\u4ee3\u7801\u7684\u7406\u89e3\u6709\u9650\uff0c\u9700\u8981\u589e\u5f3a\u4ee3\u7801\u667a\u80fd\u5206\u6790\u80fd\u529b\u4ee5\u63d0\u5347\u5f00\u53d1\u6548\u7387", "method": "\u5b9e\u73b0\u539f\u751f\u8bed\u8a00\u670d\u52a1\u5668\u534f\u8bae\u652f\u6301\uff0c\u8ba9IDE\u5177\u5907\u6df1\u5ea6\u4ee3\u7801\u7406\u89e3\u548c\u5206\u6790\u80fd\u529b", "result": "Claude Code\u73b0\u5728\u80fd\u50cf\u8f6f\u4ef6\u67b6\u6784\u5e08\u4e00\u6837\u7406\u89e3\u4ee3\u7801\u7ed3\u6784\u3001\u4f9d\u8d56\u5173\u7cfb\u548c\u8bbe\u8ba1\u6a21\u5f0f", "conclusion": "\u539f\u751fLSP\u652f\u6301\u663e\u8457\u63d0\u5347\u4e86IDE\u7684\u4ee3\u7801\u667a\u80fd\u5206\u6790\u80fd\u529b\uff0c\u4f7f\u5f00\u53d1\u5de5\u5177\u66f4\u63a5\u8fd1\u4e13\u4e1a\u8f6f\u4ef6\u67b6\u6784\u5e08\u7684\u89c6\u89d2", "topic": "code agent"}}
{"id": "tldr.2512.bf5a9843", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.kierangill.xyz%2Foversight-and-guidance%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/Nzko7PXp3_d1hdhLqCsE7M0TfAme9p2uVAKD2f9Id84=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.kierangill.xyz%2Foversight-and-guidance%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/Nzko7PXp3_d1hdhLqCsE7M0TfAme9p2uVAKD2f9Id84=437", "authors": ["TLDR Newsletter"], "title": "Scaling LLMs to larger codebases", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.kierangill.xyz%2Foversight-and-guidance%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/Nzko7PXp3_d1hdhLqCsE7M0TfAme9p2uVAKD2f9Id84=437", "summary": "Scaling LLMs to larger codebases (12 minute read) Scaling LLMs within large codebases requires investments in both guidance and oversight. Guidance focuses on providing LLMs with high-quality context, such as prompt libraries and well-structured, modular codebases, to allow for efficient \"one-shot\" code generation without needing a lot of rework. Oversight refers to human engineers who validate LLM choices, making sure of architectural integrity and aligned solutions.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u4e2d\u6269\u5c55LLMs\u9700\u8981\u6295\u8d44\u4e8e\u6307\u5bfc\uff08\u63d0\u4f9b\u9ad8\u8d28\u91cf\u4e0a\u4e0b\u6587\uff09\u548c\u76d1\u7763\uff08\u4eba\u5de5\u9a8c\u8bc1\uff09\u4e24\u65b9\u9762\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u4e00\u6b21\u6027\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u968f\u7740LLMs\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u5982\u4f55\u5c06\u5176\u6709\u6548\u6269\u5c55\u5230\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u5f53\u524dLLMs\u5728\u5904\u7406\u5927\u89c4\u6a21\u4ee3\u7801\u65f6\u9762\u4e34\u4e0a\u4e0b\u6587\u8d28\u91cf\u4e0d\u8db3\u548c\u7f3a\u4e4f\u76d1\u7763\u7684\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u4ee3\u7801\u751f\u6210\u7684\u6548\u7387\u548c\u67b6\u6784\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\uff1a1\uff09\u6307\u5bfc\u65b9\u9762\uff1a\u901a\u8fc7\u63d0\u793a\u5e93\u548c\u7ed3\u6784\u5316\u3001\u6a21\u5757\u5316\u7684\u4ee3\u7801\u5e93\u4e3aLLMs\u63d0\u4f9b\u9ad8\u8d28\u91cf\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u9ad8\u6548\u7684\u4e00\u6b21\u6027\u4ee3\u7801\u751f\u6210\uff1b2\uff09\u76d1\u7763\u65b9\u9762\uff1a\u7531\u4eba\u5de5\u5de5\u7a0b\u5e08\u9a8c\u8bc1LLM\u7684\u51b3\u7b56\uff0c\u786e\u4fdd\u67b6\u6784\u5b8c\u6574\u6027\u548c\u89e3\u51b3\u65b9\u6848\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5e2e\u52a9LLMs\u5728\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u4e2d\u66f4\u6709\u6548\u5730\u5de5\u4f5c\uff0c\u51cf\u5c11\u91cd\u590d\u5de5\u4f5c\uff0c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u4eba\u5de5\u76d1\u7763\u786e\u4fdd\u6280\u672f\u51b3\u7b56\u7684\u5408\u7406\u6027\u548c\u67b6\u6784\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u6210\u529f\u5c06LLMs\u6269\u5c55\u5230\u5927\u578b\u4ee3\u7801\u5e93\u9700\u8981\u5e73\u8861\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u76d1\u7763\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6307\u5bfc\u548c\u9a8c\u8bc1\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u5f00\u53d1\u6548\u7387\u7684\u540c\u65f6\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf\u548c\u67b6\u6784\u4e00\u81f4\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2512.13396bbf", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fread.engineerscodex.com%2Fp%2Feveryone-is-a-staff-engineer-now%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/z9DcZ01NiSu5op7U3sse5qXEJDNyUgU7c7-lcY39jlY=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fread.engineerscodex.com%2Fp%2Feveryone-is-a-staff-engineer-now%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/z9DcZ01NiSu5op7U3sse5qXEJDNyUgU7c7-lcY39jlY=437", "authors": ["TLDR Newsletter"], "title": "Everyone is a Staff Engineer Now", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fread.engineerscodex.com%2Fp%2Feveryone-is-a-staff-engineer-now%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/z9DcZ01NiSu5op7U3sse5qXEJDNyUgU7c7-lcY39jlY=437", "summary": "Everyone is a Staff Engineer Now (8 minute read) AI coding agents have become so good that they have fundamentally transformed software engineering, making code implementation inexpensive. This means engineers are increasingly expected to focus on higher-level skills like architectural judgment, system-level thinking, and managing complex contexts across multiple domains. Devs will need to start adapting their workflows, including planning and steering AI agents and developing habits to maint...", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7801\u4ee3\u7406\u5df2\u6781\u5927\u964d\u4f4e\u4ee3\u7801\u5b9e\u73b0\u6210\u672c\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u9700\u8f6c\u5411\u66f4\u9ad8\u5c42\u6b21\u7684\u67b6\u6784\u5224\u65ad\u3001\u7cfb\u7edf\u7ea7\u601d\u7ef4\u548c\u591a\u9886\u57df\u590d\u6742\u4e0a\u4e0b\u6587\u7ba1\u7406\u80fd\u529b", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u80fd\u529b\u5927\u5e45\u63d0\u5347\uff0c\u4f7f\u5f97\u4ee3\u7801\u5b9e\u73b0\u53d8\u5f97\u5ec9\u4ef7\uff0c\u4f20\u7edf\u7f16\u7a0b\u6280\u80fd\u4ef7\u503c\u4e0b\u964d\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u9002\u5e94\u65b0\u7684\u5de5\u4f5c\u8303\u5f0f", "method": "\u901a\u8fc7\u5206\u6790AI\u7f16\u7801\u4ee3\u7406\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5de5\u7a0b\u5e08\u9700\u8981\u8f6c\u53d8\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u89c4\u5212\u548c\u5f15\u5bfcAI\u4ee3\u7406\uff0c\u57f9\u517b\u65b0\u4e60\u60ef\u6765\u7ef4\u6301\u4ee3\u7801\u8d28\u91cf", "result": "\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6839\u672c\u6027\u8f6c\u53d8\uff0c\u5de5\u7a0b\u5e08\u89d2\u8272\u4ece\u4ee3\u7801\u5b9e\u73b0\u8005\u8f6c\u5411\u67b6\u6784\u5e08\u548c\u7cfb\u7edf\u601d\u8003\u8005\uff0c\u9700\u8981\u65b0\u7684\u6280\u80fd\u7ec4\u5408\u548c\u5de5\u4f5c\u65b9\u5f0f", "conclusion": "AI\u7f16\u7801\u4ee3\u7406\u7684\u8fdb\u6b65\u8981\u6c42\u6240\u6709\u5de5\u7a0b\u5e08\u5177\u5907\u9ad8\u7ea7\u67b6\u6784\u80fd\u529b\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u9002\u5e94\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u6280\u80fd\u53d1\u5c55\u8def\u5f84", "topic": "code agent"}}
{"id": "tldr.2512.26960eb0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F12%2F22%2Fa-year-of-vibes%2F%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/iwzzDcq49FZrdrXyGtTVZOv9TrQpNc-5cZekeMudzHA=437", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F12%2F22%2Fa-year-of-vibes%2F%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/iwzzDcq49FZrdrXyGtTVZOv9TrQpNc-5cZekeMudzHA=437", "authors": ["TLDR Newsletter"], "title": "A Year Of Vibes", "comment": "Source: TLDR Newsletter, Date: 2025-12-23, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F12%2F22%2Fa-year-of-vibes%2F%3Futm_source=tldrdev/1/0100019b4b1c6d21-17968b35-54cd-4308-975c-6c0bcf53281a-000000/iwzzDcq49FZrdrXyGtTVZOv9TrQpNc-5cZekeMudzHA=437", "summary": "A Year Of Vibes (12 minute read) In 2025, this dev left Sentry, launched a new company, and shifted his programming approach to embrace hands-off agentic coding with tools like Claude Code. He became deeply integrated with AI agents for tasks from code generation to daily organization. However, he's sometimes worried about emergent human-like tendencies of LLMs, questioning terms like \"agent.\u201d", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u8005\u57282025\u5e74\u79bb\u5f00Sentry\u540e\u521b\u7acb\u65b0\u516c\u53f8\uff0c\u5168\u9762\u91c7\u7528AI\u4ee3\u7406\u7f16\u7a0b\uff08\u5982Claude Code\uff09\uff0c\u4ece\u4ee3\u7801\u751f\u6210\u5230\u65e5\u5e38\u7ec4\u7ec7\u90fd\u6df1\u5ea6\u96c6\u6210AI\u4ee3\u7406\uff0c\u4f46\u5bf9LLMs\u8868\u73b0\u51fa\u7c7b\u4eba\u503e\u5411\u611f\u5230\u62c5\u5fe7", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u5728\u5b9e\u9645\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u7684\u6df1\u5ea6\u96c6\u6210\u5e94\u7528\uff0c\u6d4b\u8bd5AI\u8f85\u52a9\u7f16\u7a0b\u7684\u6781\u9650\uff0c\u540c\u65f6\u5173\u6ce8AI\u7cfb\u7edf\u53ef\u80fd\u51fa\u73b0\u7684\u7c7b\u4eba\u884c\u4e3a\u503e\u5411", "method": "\u91c7\u7528\u5b9e\u8df5\u5bfc\u5411\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u771f\u5b9e\u9879\u76ee\u548c\u5de5\u4f5c\u6d41\u7a0b\u5168\u9762\u96c6\u6210AI\u4ee3\u7406\u5de5\u5177\uff08\u7279\u522b\u662fClaude Code\uff09\uff0c\u4ece\u4ee3\u7801\u751f\u6210\u5230\u65e5\u5e38\u4efb\u52a1\u7ba1\u7406\u90fd\u4f9d\u8d56AI\u8f85\u52a9", "result": "\u6210\u529f\u5b9e\u73b0AI\u4ee3\u7406\u6df1\u5ea6\u96c6\u6210\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u89c2\u5bdf\u5230LLMs\u8868\u73b0\u51fa\u4ee4\u4eba\u4e0d\u5b89\u7684\u7c7b\u4eba\u503e\u5411\uff0c\u5bf9\"\u4ee3\u7406\"\u8fd9\u4e00\u672f\u8bed\u4ea7\u751f\u8d28\u7591", "conclusion": "AI\u4ee3\u7406\u7f16\u7a0b\u5de5\u5177\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u9700\u8981\u8b66\u60d5AI\u7cfb\u7edf\u53ef\u80fd\u51fa\u73b0\u7684\u7c7b\u4eba\u884c\u4e3a\u503e\u5411\uff0c\u5bf9AI\u4e0e\u4eba\u7c7b\u5173\u7cfb\u7684\u8fb9\u754c\u9700\u8981\u66f4\u6df1\u5165\u7684\u601d\u8003", "topic": "code agent"}}
{"id": "tldr.2601.d5537173", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=CQmI4XKTa0U%26t=12s%26utm_source=tldrdata/1/0100019b793de7bf-3b24424d-a063-413a-8dd5-b7c16df11556-000000/rHyDbE7aSqHtuNmYatoxiLWIGnc9Cwy5g3WiZEyfhmk=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=CQmI4XKTa0U%26t=12s%26utm_source=tldrdata/1/0100019b793de7bf-3b24424d-a063-413a-8dd5-b7c16df11556-000000/rHyDbE7aSqHtuNmYatoxiLWIGnc9Cwy5g3WiZEyfhmk=438", "authors": ["TLDR Newsletter"], "title": "How AI Will Change Software Engineering", "comment": "Source: TLDR Newsletter, Date: 2026-01-01, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=CQmI4XKTa0U%26t=12s%26utm_source=tldrdata/1/0100019b793de7bf-3b24424d-a063-413a-8dd5-b7c16df11556-000000/rHyDbE7aSqHtuNmYatoxiLWIGnc9Cwy5g3WiZEyfhmk=438", "summary": "How AI Will Change Software Engineering (110 minute video) LLMs are a once-in-a-career shift like assembly to high-level languages, but bigger in one way: software becomes non-deterministic (probabilistic outputs), forcing new engineering habits. AI is great for fast prototyping, navigating unfamiliar stacks, and understanding legacy code, but unsafe for blind \u201cvibe coding,\u201d which breaks the learning loop. Treat AI output like a PR from a dodgy but productive teammate: review hard, test relen...", "source": "tldr", "AI": {"tldr": "AI\u5c06\u5f7b\u5e95\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u4f7f\u8f6f\u4ef6\u5f00\u53d1\u4ece\u786e\u5b9a\u6027\u8f6c\u5411\u6982\u7387\u6027\uff0c\u9700\u8981\u65b0\u7684\u5de5\u7a0b\u4e60\u60ef\u3002AI\u64c5\u957f\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3001\u7406\u89e3\u9057\u7559\u4ee3\u7801\uff0c\u4f46\u4e0d\u80fd\u76f2\u76ee\u4f9d\u8d56\uff0c\u9700\u8981\u4e25\u683c\u5ba1\u67e5\u548c\u6d4b\u8bd5\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\uff0c\u5c31\u50cf\u6c47\u7f16\u8bed\u8a00\u5230\u9ad8\u7ea7\u8bed\u8a00\u7684\u8f6c\u53d8\u4e00\u6837\u91cd\u8981\uff0c\u4f46\u5f71\u54cd\u66f4\u5927\uff0c\u56e0\u4e3a\u8f6f\u4ef6\u53d8\u5f97\u975e\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u89c6\u9891\u8bb2\u5ea7\u5f62\u5f0f\uff0c\u5206\u6790AI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u63d0\u51fa\u5c06AI\u8f93\u51fa\u89c6\u4e3a\u9700\u8981\u4e25\u683c\u5ba1\u67e5\u7684\u4ee3\u7801\u63d0\u4ea4\uff0c\u5f3a\u8c03\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u7684\u91cd\u8981\u6027\u3002", "result": "AI\u5728\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3001\u7406\u89e3\u4e0d\u719f\u6089\u7684\u4ee3\u7801\u6808\u548c\u9057\u7559\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u76f2\u76ee\u4f9d\u8d56\u4f1a\u5bfc\u81f4\u5b66\u4e60\u5faa\u73af\u4e2d\u65ad\uff0c\u9700\u8981\u65b0\u7684\u5de5\u7a0b\u5b9e\u8df5\u6765\u7ba1\u7406\u6982\u7387\u6027\u8f93\u51fa\u3002", "conclusion": "\u8f6f\u4ef6\u5de5\u7a0b\u9700\u8981\u9002\u5e94AI\u5e26\u6765\u7684\u6839\u672c\u6027\u53d8\u5316\uff0c\u5efa\u7acb\u65b0\u7684\u5de5\u7a0b\u4e60\u60ef\u6765\u7ba1\u7406\u975e\u786e\u5b9a\u6027\u8f6f\u4ef6\uff0c\u5c06AI\u89c6\u4e3a\u9700\u8981\u4e25\u683c\u76d1\u7763\u7684\"\u591a\u4ea7\u4f46\u4e0d\u53ef\u9760\"\u7684\u56e2\u961f\u6210\u5458\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.e5aae17c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpzNUXP/1/0100019b794d35da-0cf0af27-0e60-490e-82cd-198246df9836-000000/uldpwKR2mjAthpkwjidByvS8qQ9VCcwHqF7l6ofzum8=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpzNUXP/1/0100019b794d35da-0cf0af27-0e60-490e-82cd-198246df9836-000000/uldpwKR2mjAthpkwjidByvS8qQ9VCcwHqF7l6ofzum8=438", "authors": ["TLDR Newsletter"], "title": "How I code with agents, without being 'technical'", "comment": "Source: TLDR Newsletter, Date: 2026-01-01, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpzNUXP/1/0100019b794d35da-0cf0af27-0e60-490e-82cd-198246df9836-000000/uldpwKR2mjAthpkwjidByvS8qQ9VCcwHqF7l6ofzum8=438", "summary": "How I code with agents, without being 'technical' (18 minute read) Ben Tossell, who works at Factory, a company developing a frontier software development agent, has spent 3 billion tokens in four months through an agent without writing any code. None of the code was read, but Tossell read the agent output religiously, which led to him picking up a ton of knowledge around how code works, how projects work, where things fail, and where they succeed. Tossell has shipped several projects using h...", "source": "tldr", "AI": {"tldr": "\u975e\u6280\u672f\u4eba\u5458\u901a\u8fc7AI\u4ee3\u7406\u8fdb\u884c\u8f6f\u4ef6\u5f00\u53d1\uff0c\u65e0\u9700\u7f16\u5199\u4ee3\u7801\u4f46\u901a\u8fc7\u76d1\u63a7\u4ee3\u7406\u8f93\u51fa\u5b66\u4e60\u7f16\u7a0b\u77e5\u8bc6\u5e76\u6210\u529f\u4ea4\u4ed8\u9879\u76ee", "motivation": "\u63a2\u7d22\u975e\u6280\u672f\u4eba\u5458\u5982\u4f55\u5229\u7528AI\u4ee3\u7406\u8fdb\u884c\u8f6f\u4ef6\u5f00\u53d1\uff0c\u964d\u4f4e\u7f16\u7a0b\u95e8\u69db\uff0c\u8ba9\u4e0d\u5177\u5907\u4f20\u7edf\u7f16\u7a0b\u6280\u80fd\u7684\u4eba\u4e5f\u80fd\u53c2\u4e0e\u8f6f\u4ef6\u9879\u76ee", "method": "\u4f7f\u7528\u524d\u6cbf\u7684\u8f6f\u4ef6\u5f00\u53d1\u4ee3\u7406\uff08Factory\u516c\u53f8\u4ea7\u54c1\uff09\uff0c\u901a\u8fc7\u5927\u91cftoken\u4ea4\u4e92\u8ba9AI\u4ee3\u7406\u751f\u6210\u4ee3\u7801\uff0c\u7528\u6237\u4e0d\u7f16\u5199\u4ee3\u7801\u4f46\u5bc6\u5207\u76d1\u63a7\u4ee3\u7406\u8f93\u51fa\uff0c\u4ece\u4e2d\u5b66\u4e60\u7f16\u7a0b\u77e5\u8bc6", "result": "\u57284\u4e2a\u6708\u5185\u6d88\u801730\u4ebftoken\uff0c\u6210\u529f\u4ea4\u4ed8\u591a\u4e2a\u9879\u76ee\uff0c\u7528\u6237\u901a\u8fc7\u76d1\u63a7\u4ee3\u7406\u8f93\u51fa\u83b7\u5f97\u4e86\u5927\u91cf\u5173\u4e8e\u4ee3\u7801\u5de5\u4f5c\u539f\u7406\u3001\u9879\u76ee\u7ba1\u7406\u3001\u5931\u8d25\u4e0e\u6210\u529f\u56e0\u7d20\u7684\u77e5\u8bc6", "conclusion": "AI\u4ee3\u7406\u4f7f\u975e\u6280\u672f\u4eba\u5458\u80fd\u591f\u53c2\u4e0e\u8f6f\u4ef6\u5f00\u53d1\uff0c\u901a\u8fc7\u76d1\u63a7\u4ee3\u7406\u8f93\u51fa\u53ef\u4ee5\u5b66\u4e60\u7f16\u7a0b\u77e5\u8bc6\uff0c\u8fd9\u79cd\u6a21\u5f0f\u53ef\u80fd\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u7684\u5de5\u4f5c\u65b9\u5f0f", "topic": "code agent"}}
{"id": "tldr.2601.c293a119", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cbc.ca%2Fnews%2Fbusiness%2Fmeta-manus-acquisition-two-billion-explained-9.7030180%3Futm_source=tldrmarketing/1/0100019b79729bdf-1e85403a-a01b-4ed6-926f-15755a377d9f-000000/VZ_AM3bOFeQYRGNNZadESNtejoYmjwEm70yNaDJswck=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cbc.ca%2Fnews%2Fbusiness%2Fmeta-manus-acquisition-two-billion-explained-9.7030180%3Futm_source=tldrmarketing/1/0100019b79729bdf-1e85403a-a01b-4ed6-926f-15755a377d9f-000000/VZ_AM3bOFeQYRGNNZadESNtejoYmjwEm70yNaDJswck=438", "authors": ["TLDR Newsletter"], "title": "Meta just acquired a Chinese-founded AI startup for $2B. Here's why that matters", "comment": "Source: TLDR Newsletter, Date: 2026-01-01, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cbc.ca%2Fnews%2Fbusiness%2Fmeta-manus-acquisition-two-billion-explained-9.7030180%3Futm_source=tldrmarketing/1/0100019b79729bdf-1e85403a-a01b-4ed6-926f-15755a377d9f-000000/VZ_AM3bOFeQYRGNNZadESNtejoYmjwEm70yNaDJswck=438", "summary": "Meta just acquired a Chinese-founded AI startup for $2B. Here's why that matters (4 minute read) Manus makes agentic AI that can make decisions and complete tasks with minimal prompting and generates revenue through subscriptions. Meta plans to integrate Manus into WhatsApp, Instagram, and Facebook to create an AI companion that handles chat, payments, and other tasks, keeping users engaged longer.", "source": "tldr", "AI": {"tldr": "Meta\u4ee520\u4ebf\u7f8e\u5143\u6536\u8d2d\u4e2d\u56fd\u80cc\u666f\u7684AI\u521d\u521b\u516c\u53f8Manus\uff0c\u8ba1\u5212\u5c06\u5176\u667a\u80fd\u4ee3\u7406AI\u96c6\u6210\u5230WhatsApp\u3001Instagram\u548cFacebook\u4e2d\uff0c\u6253\u9020\u80fd\u591f\u5904\u7406\u804a\u5929\u3001\u652f\u4ed8\u7b49\u4efb\u52a1\u7684AI\u4f34\u4fa3\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "Meta\u5e0c\u671b\u901a\u8fc7\u6536\u8d2dManus\u83b7\u5f97\u5148\u8fdb\u7684\u667a\u80fd\u4ee3\u7406AI\u6280\u672f\uff0c\u5c06\u5176\u96c6\u6210\u5230\u4e3b\u8981\u793e\u4ea4\u5e73\u53f0\u4e2d\uff0c\u521b\u5efa\u80fd\u591f\u81ea\u4e3b\u51b3\u7b56\u548c\u5b8c\u6210\u4efb\u52a1\u7684AI\u4f34\u4fa3\uff0c\u4ece\u800c\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u3001\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5e76\u5728AI\u52a9\u624b\u9886\u57df\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "method": "\u901a\u8fc7\u6536\u8d2dManus\u8fd9\u5bb6\u4e13\u6ce8\u4e8e\u667a\u80fd\u4ee3\u7406AI\u7684\u521d\u521b\u516c\u53f8\uff0c\u83b7\u5f97\u5176\u9700\u8981\u6700\u5c11\u63d0\u793a\u5c31\u80fd\u505a\u51fa\u51b3\u7b56\u548c\u5b8c\u6210\u4efb\u52a1\u7684\u6280\u672f\uff0c\u7136\u540e\u5c06\u8be5\u6280\u672f\u96c6\u6210\u5230Meta\u7684\u4e09\u5927\u793e\u4ea4\u5e73\u53f0\uff08WhatsApp\u3001Instagram\u3001Facebook\uff09\u4e2d\u3002", "result": "Meta\u4ee520\u4ebf\u7f8e\u5143\u5b8c\u6210\u5bf9Manus\u7684\u6536\u8d2d\uff0c\u83b7\u5f97\u4e86\u5148\u8fdb\u7684\u667a\u80fd\u4ee3\u7406AI\u6280\u672f\uff0c\u8ba1\u5212\u5c06\u5176\u6574\u5408\u5230\u6838\u5fc3\u793e\u4ea4\u4ea7\u54c1\u4e2d\uff0c\u521b\u5efa\u80fd\u591f\u5904\u7406\u804a\u5929\u3001\u652f\u4ed8\u7b49\u591a\u79cd\u4efb\u52a1\u7684AI\u4f34\u4fa3\u3002", "conclusion": "\u8fd9\u6b21\u6536\u8d2d\u6807\u5fd7\u7740Meta\u5728\u667a\u80fd\u4ee3\u7406AI\u9886\u57df\u7684\u91cd\u8981\u5e03\u5c40\uff0c\u901a\u8fc7\u5c06\u5148\u8fdb\u7684AI\u6280\u672f\u96c6\u6210\u5230\u793e\u4ea4\u5e73\u53f0\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u5e73\u53f0\u7c98\u6027\uff0c\u540c\u65f6\u4e3aMeta\u5728AI\u52a9\u624b\u7ade\u4e89\u4e2d\u5360\u636e\u6709\u5229\u4f4d\u7f6e\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.2ed7fc8c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdima.day%2Fblog%2Fbuild-software-build-users%2F%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/F7453JOKu3IHazfi1J6xXN7hwpV4swUY_c-tLXc8G28=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdima.day%2Fblog%2Fbuild-software-build-users%2F%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/F7453JOKu3IHazfi1J6xXN7hwpV4swUY_c-tLXc8G28=438", "authors": ["TLDR Newsletter"], "title": "Build Software. Build Users", "comment": "Source: TLDR Newsletter, Date: 2026-01-01, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdima.day%2Fblog%2Fbuild-software-build-users%2F%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/F7453JOKu3IHazfi1J6xXN7hwpV4swUY_c-tLXc8G28=438", "summary": "Build Software. Build Users (5 minute read) True software quality is often missed because engineers fail to deeply understand user needs. A possible solution is to \"vibe code users\" first, before coding the software itself, by creating detailed, LLM-driven user agents that simulate target users and their interactions. This iterative process involves building these agentic users with defined profiles and \"happy paths,\" then developing software, and finally allowing the simulated users to provi...", "source": "tldr", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u521b\u5efaLLM\u9a71\u52a8\u7684\u7528\u6237\u4ee3\u7406\u6765\u6a21\u62df\u76ee\u6807\u7528\u6237\u53ca\u5176\u4ea4\u4e92\uff0c\u5728\u7f16\u7801\u524d\u5148\"\u611f\u77e5\u7528\u6237\"\uff0c\u901a\u8fc7\u8fed\u4ee3\u8fc7\u7a0b\u6784\u5efa\u5177\u6709\u5b9a\u4e49\u914d\u7f6e\u6587\u4ef6\u548c\"\u5feb\u4e50\u8def\u5f84\"\u7684\u4ee3\u7406\u7528\u6237\uff0c\u7136\u540e\u5f00\u53d1\u8f6f\u4ef6\uff0c\u6700\u540e\u8ba9\u6a21\u62df\u7528\u6237\u63d0\u4f9b\u53cd\u9988\u4ee5\u6539\u8fdb\u8f6f\u4ef6\u8d28\u91cf", "motivation": "\u5de5\u7a0b\u5e08\u5e38\u5e38\u56e0\u4e3a\u672a\u80fd\u6df1\u5165\u7406\u89e3\u7528\u6237\u9700\u6c42\u800c\u9519\u8fc7\u771f\u6b63\u7684\u8f6f\u4ef6\u8d28\u91cf\u3002\u5f53\u524d\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u5bf9\u7528\u6237\u9700\u6c42\u7684\u6df1\u523b\u6d1e\u5bdf\uff0c\u5bfc\u81f4\u6700\u7ec8\u4ea7\u54c1\u4e0e\u7528\u6237\u5b9e\u9645\u9700\u6c42\u4e0d\u5339\u914d", "method": "\u63d0\u51fa\"vibe code users\"\u65b9\u6cd5\uff1a\u5728\u7f16\u7801\u8f6f\u4ef6\u4e4b\u524d\uff0c\u5148\u521b\u5efa\u8be6\u7ec6\u7684LLM\u9a71\u52a8\u7684\u7528\u6237\u4ee3\u7406\u6765\u6a21\u62df\u76ee\u6807\u7528\u6237\u53ca\u5176\u4ea4\u4e92\u3002\u8fd9\u662f\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\uff1a1) \u6784\u5efa\u5177\u6709\u5b9a\u4e49\u914d\u7f6e\u6587\u4ef6\u548c\"\u5feb\u4e50\u8def\u5f84\"\u7684\u4ee3\u7406\u7528\u6237\uff1b2) \u57fa\u4e8e\u8fd9\u4e9b\u6a21\u62df\u7528\u6237\u5f00\u53d1\u8f6f\u4ef6\uff1b3) \u8ba9\u6a21\u62df\u7528\u6237\u63d0\u4f9b\u53cd\u9988\u4ee5\u6539\u8fdb\u8f6f\u4ef6", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u7528\u6237\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u65e9\u671f\u9636\u6bb5\u63d0\u4f9b\u7528\u6237\u89c6\u89d2\uff0c\u5e2e\u52a9\u5de5\u7a0b\u5e08\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u9700\u6c42\uff0c\u4ece\u800c\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c", "conclusion": "\u901a\u8fc7LLM\u9a71\u52a8\u7684\u7528\u6237\u4ee3\u7406\u6a21\u62df\u76ee\u6807\u7528\u6237\uff0c\u53ef\u4ee5\u5728\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u66f4\u65e9\u3001\u66f4\u6df1\u5165\u5730\u7406\u89e3\u7528\u6237\u9700\u6c42\uff0c\u4ece\u800c\u63d0\u5347\u6700\u7ec8\u8f6f\u4ef6\u4ea7\u54c1\u7684\u8d28\u91cf\u548c\u7528\u6237\u6ee1\u610f\u5ea6", "topic": "swe application"}}
{"id": "tldr.2601.6546d93d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbits.logic.inc%2Fp%2Fai-is-forcing-us-to-write-good-code%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/U5DZ3gpCrEKU6wmQ96R934IzkUNrG6rHTVkMWe0PpbE=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbits.logic.inc%2Fp%2Fai-is-forcing-us-to-write-good-code%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/U5DZ3gpCrEKU6wmQ96R934IzkUNrG6rHTVkMWe0PpbE=438", "authors": ["TLDR Newsletter"], "title": "AI Is Forcing Us To Write Good Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-01, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbits.logic.inc%2Fp%2Fai-is-forcing-us-to-write-good-code%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/U5DZ3gpCrEKU6wmQ96R934IzkUNrG6rHTVkMWe0PpbE=438", "summary": "AI Is Forcing Us To Write Good Code (10 minute read) AI agents require a higher standard of code quality. Previously \"optional\" best practices are now essential requirements for effective operation. Thoughtful file organization and extensive use of end-to-end type systems are also necessary for guiding agents, reducing errors, and improving context loading.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u9700\u8981\u66f4\u9ad8\u7684\u4ee3\u7801\u8d28\u91cf\u6807\u51c6\uff0c\u4e4b\u524d\"\u53ef\u9009\"\u7684\u6700\u4f73\u5b9e\u8df5\u73b0\u5728\u6210\u4e3a\u6709\u6548\u64cd\u4f5c\u7684\u5fc5\u8981\u6761\u4ef6", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u66f4\u9ad8\u7684\u4ee3\u7801\u8d28\u91cf\u6765\u786e\u4fdd\u4ee3\u7406\u7684\u6709\u6548\u8fd0\u884c", "method": "\u5f3a\u8c03\u826f\u597d\u7684\u6587\u4ef6\u7ec4\u7ec7\u548c\u7aef\u5230\u7aef\u7c7b\u578b\u7cfb\u7edf\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4ee5\u6307\u5bfcAI\u4ee3\u7406\u3001\u51cf\u5c11\u9519\u8bef\u5e76\u6539\u5584\u4e0a\u4e0b\u6587\u52a0\u8f7d", "result": "AI\u4ee3\u7406\u8feb\u4f7f\u5f00\u53d1\u8005\u91c7\u7528\u66f4\u4e25\u683c\u7684\u4ee3\u7801\u8d28\u91cf\u6807\u51c6\uff0c\u5c06\u4e4b\u524d\u53ef\u9009\u7684\u6700\u4f73\u5b9e\u8df5\u53d8\u4e3a\u5fc5\u8981\u8981\u6c42", "conclusion": "AI\u6b63\u5728\u63a8\u52a8\u8f6f\u4ef6\u5f00\u53d1\u5411\u66f4\u9ad8\u8d28\u91cf\u6807\u51c6\u8f6c\u53d8\uff0c\u826f\u597d\u7684\u4ee3\u7801\u7ec4\u7ec7\u548c\u7c7b\u578b\u7cfb\u7edf\u5bf9AI\u4ee3\u7406\u64cd\u4f5c\u81f3\u5173\u91cd\u8981", "topic": "code agent"}}
{"id": "tldr.2601.4a599c50", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F31%2Fthe-year-in-llms%2F%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/40mcEnEYUbQX1c_cbyjrMZFKk45fH3uMwibT2H2D7yo=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F31%2Fthe-year-in-llms%2F%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/40mcEnEYUbQX1c_cbyjrMZFKk45fH3uMwibT2H2D7yo=438", "authors": ["TLDR Newsletter"], "title": "2025: The year in LLMs", "comment": "Source: TLDR Newsletter, Date: 2026-01-01, Reading time: 41 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F31%2Fthe-year-in-llms%2F%3Futm_source=tldrdev/1/0100019b7975830a-ba0cccd3-0d70-4358-8103-2204a0a651c8-000000/40mcEnEYUbQX1c_cbyjrMZFKk45fH3uMwibT2H2D7yo=438", "summary": "2025: The year in LLMs (41 minute read) 2025 was when LLMs learned to reason, allowing models to tackle complex, multi-step tasks, which in turn drove the widespread adoption of highly capable AI agents. \"Coding agents\u201d came to life, autonomously writing, executing, and debugging code across command-line interfaces and even mobile phones. The competitive landscape shifted a lot as Chinese open-weight models dominated capability rankings, and Google's Gemini made large strides with new models ...", "source": "tldr", "AI": {"tldr": "2025\u5e74LLMs\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u53d6\u5f97\u7a81\u7834\uff0c\u63a8\u52a8\u4e86AI\u4ee3\u7406\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7279\u522b\u662f\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u81ea\u4e3b\u7f16\u5199\u3001\u6267\u884c\u548c\u8c03\u8bd5\u4ee3\u7801\uff0c\u540c\u65f6\u4e2d\u56fd\u5f00\u6e90\u6a21\u578b\u5728\u80fd\u529b\u6392\u540d\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d", "motivation": "\u603b\u7ed32025\u5e74LLMs\u9886\u57df\u7684\u5173\u952e\u53d1\u5c55\uff0c\u7a81\u51fa\u63a8\u7406\u80fd\u529b\u7684\u7a81\u7834\u3001AI\u4ee3\u7406\u7684\u666e\u53ca\u3001\u7f16\u7801\u4ee3\u7406\u7684\u51fa\u73b0\u4ee5\u53ca\u7ade\u4e89\u683c\u5c40\u7684\u53d8\u5316", "method": "\u8fd9\u662f\u4e00\u7bc7\u7efc\u8ff0\u6027\u6587\u7ae0\uff0c\u901a\u8fc7\u56de\u987e\u548c\u5206\u67902025\u5e74LLMs\u9886\u57df\u7684\u4e3b\u8981\u8d8b\u52bf\u548c\u53d1\u5c55\uff0c\u603b\u7ed3\u5e74\u5ea6\u8fdb\u5c55", "result": "LLMs\u5b66\u4f1a\u4e86\u63a8\u7406\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\uff1b\u7f16\u7801\u4ee3\u7406\u5b9e\u73b0\u81ea\u4e3b\u7f16\u7a0b\uff1b\u4e2d\u56fd\u5f00\u6e90\u6a21\u578b\u5728\u80fd\u529b\u6392\u540d\u4e2d\u5360\u636e\u4e3b\u5bfc\uff1bGoogle\u7684Gemini\u6a21\u578b\u53d6\u5f97\u91cd\u5927\u8fdb\u5c55", "conclusion": "2025\u5e74\u662fLLMs\u53d1\u5c55\u7684\u91cd\u8981\u8f6c\u6298\u70b9\uff0c\u63a8\u7406\u80fd\u529b\u7684\u7a81\u7834\u63a8\u52a8\u4e86AI\u4ee3\u7406\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6539\u53d8\u4e86\u884c\u4e1a\u7ade\u4e89\u683c\u5c40", "topic": "code agent"}}
